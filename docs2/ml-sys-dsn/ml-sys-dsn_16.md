# 13 集成

### 本章涵盖

+   API 设计

+   发布周期

+   操作系统

+   覆盖和回退

如我们之前所声称的，你能做的最糟糕的事情就是构建一个系统，然后只是把它放在架子上而不是让它上线。我们俩在我们的职业生涯中至少都遇到过这样的问题，这不是我们推荐的经历。

一个新手错误的想法是认为集成是一次性的事件或项目的一个阶段。这是一个反模式：你不能只是为未来的集成分配几周时间，然后在真空中开始构建系统。实际上，它是一个从项目一开始就开始，直到系统退役才结束的持续过程。更重要的是，当系统的生命周期结束时，它需要一定的解集成努力，确保没有任何直接或间接的用户会受到关闭系统的影响。适当的集成是您系统成功的关键，使您更容易获得反馈并改进。各种元素集成到您的系统中的越平滑，反馈循环就越短，您可以实施的迭代就越快。

在本章中，我们讨论如何高效地集成您的系统，重点关注技术方面。

## 13.1 API 设计

API 设计是集成过程中的关键部分。它可能被视为系统与其用户之间的合同，但这是你在签署之前需要彻底阅读的合同。这是因为一旦 API 设计被设置，即使它不是一成不变的，系统仍在开发中，更改 API 设计也将是代价高昂的。

如果你是一位在机器学习（ML）方面经验丰富的读者，你可能觉得可以跳过这一节，仅仅因为你已经知道如何设计 API 并且已经多次这样做。这是一个合理的说法，因为我们不会教你 REST 和 RPC 之间的区别，或者如何一般性地设计 API。此外，关于这个主题有大量的优秀书籍和文章（例如，可以在[`news.ycombinator.com/item?id=24383180`](https://news.ycombinator.com/item?id=24383180)找到一些推荐的资料集合）。相反，我们将专注于关键方面，并突出 ML 系统特有的陷阱。

如果我们要挑选出好的 API 的两个属性，我们会选择*简单性*和*可预测性*。Butler Lampson 有一句经典的软件名言，甚至被称为“软件工程的基本定理”：“我们可以通过引入一个额外的间接层来解决任何问题。”

这句话的一个变体是，任何编程问题都可以通过一层抽象来解决，除了太多抽象的问题。所以 API 的简单性是找到正确抽象的艺术，这种抽象不会泄露太多底层实现细节。

简单性的关键作用在于其使 API 更易于学习和使用，而无需深入了解内部结构。一个典型的机器学习系统通常有许多处理程序和参数，总是有向外部用户暴露它们的诱惑。这导致了解决方案过于复杂，调用方法需要提供多个参数，最终很难理解它们的含义以及它们是如何相互关联的。更好的方法是在简单的界面后面隐藏复杂性，并提供一些参数数量较少的方法。用户会为此感到感激。

然而，隐藏所有参数也不是最好的主意（见图 13.1）。为系统行为提供定制方式很重要，尤其是在调试目的上。想象一下，在一个深夜的值班期间，你正在调试一个有十几个参数的系统，而你无法修改任何一个参数。这不是一个愉快的体验！在这些情况下，建议合理的默认值并提供一种覆盖它们的方式总是很有意义的。

![figure](img/CH13_F01_Babushkin.png)

##### 图 13.1 过度配置与不足配置

##### 阿尔谢尼的营火故事

我曾经为一家公司工作，该公司为外部开发者提供 API。我负责构建一个全新的端点，该端点将镜像底层的分类系统。在早期阶段，该 API 似乎非常简单——只需接受一个对象作为输入，并从预定义的分类法中输出一个标签。

尽管基线准确度并不完美，我的一个同事建议返回一个标签列表而不是单个标签。这是一个合理的建议，我毫不犹豫地实施了它。然而，实践表明，即使在标签不正确的情况下（他们的使用模式无法使用第二个标签或更多），用户也只需要一个标签。坏消息是标签列表已经暴露在 API 中，如果不破坏兼容性，很难将其移除。因此，API 无端变得过于复杂，许多用户对此表示了不满：“为什么你返回一个标签列表，而它总是包含单个项目？”过于草率的 API 设计决策导致了一个次优的解决方案，后来很难修复。

可预测性是良好 API 的另一个关键属性。我们已经讨论了机器学习系统通常是非确定性的，除非它们被强制有意地（请参阅第十章）。这对于 API 来说是一个更加关键的因素，因为它们必须是确定性和可预测的。确保相同的输入总是产生相同的输出是很重要的。当然，有一些算法是非确定性的（例如，带有温度采样的文本生成），但这只是例外，证明了规则。

总是有强制实现**确定性行为**的可能性。一个简单的例子就是从参数中获取随机种子（如果没有指定，可以选择自己的种子）。顺便说一句，虽然许多机器学习库使用全局状态的随机种子，但在 Google 新近出现的数值计算库 JAX 中则不行。其设计表明，你必须明确传递随机状态，正是为了这个原因——强制实现完全可重复性。更多信息请见 [`mng.bz/lrQ2`](https://mng.bz/lrQ2)。

非确定性的另一个来源可能是输入数据，包括一些隐含的数据，如当前时间（它也应该通过参数提供）。

让我们看看两个使用时间作为输入并返回具有一定随机性结果的预测函数的实现。以下列表只有一个显式参数，而其输出取决于三个参数，这意味着输出是不可重复的。

##### 列表 13.1 一个平庸的 `predict` 函数设计

```py
def predict(features):
    time = datetime.now()
    return model.predict(features, time, seed=42)
```

与前面的例子不同，以下列表中显示的函数调用者控制着影响输出的所有参数。

##### 列表 13.2 一个更好的 `predict` 函数设计

```py
def predict(features, time=None, seed=42):
    if time is None:
        time = datetime.now()
    return model.predict(features, time, seed)
```

注释：虽然我们在这些例子中使用 Python，但高级抽象的基本原理保持不变。想象一下这个函数是 HTTP API 的一部分，其中 `time` 和 `seed` 是新引入的查询参数。在这种情况下，将应用相同的原理。

可预测性的一个特定方面是**兼容性**。当谈论兼容性时，工程师通常指的是向后兼容或向前兼容。向后兼容意味着 API 的新版本与旧版本兼容（旧代码可以在不进行任何更改的情况下与新版本的 API 一起使用）。向前兼容则意味着 API 的旧版本与新版本兼容（新代码可以在不进行任何更改的情况下与旧版本的 API 一起使用）。

在机器学习系统的背景下，兼容性也与底层模型的版本有关。通常有一个常见的做法是给模型打版本号，并在初始化时提供一个请求特定版本模型的方法。

##### 列表 13.3 将模型版本添加到 API 中

```py
class Model:
    def __init__(self, version):
        self.version = version
        self.model = load_model(version)

    def predict(self, features, time=None, seed=42):
        if time is None:
            time = datetime.now()
        return self.model.predict(features, time, seed)
```

这个例子过于简化，你可能需要一个更高级的解决方案来处理复杂的系统。如果你想了解更多关于模型注册模式的信息，可以阅读相关材料（例如，[`neptune.ai/blog/ml-model-registry`](https://neptune.ai/blog/ml-model-registry)）。

版本控制很棘手。一种反模式可能是更新模型而不提升版本，这会导致系统行为的变化而没有任何通知。有很多场景中，更新模型被视为破坏性变更，必须在版本中反映出来。更重要的是，这不仅适用于模型，还适用于管道的任何方面——数据输入/输出（IO）、预处理、后处理等。一些更改甚至不是有意为之：你可以更新依赖项并得到不同的结果，从而隐式地破坏兼容性。

##### 阿列克谢的篝火故事

更新 Python 版本可能需要多长时间？这个问题看起来很简单，但事实上，我不得不艰难地学习到它并非如此。现实的重击如此之重，以至于让我陷入了一个兔子洞。

我工作的系统需要完全兼容。哎呀，当我提升 Python 版本（同时保持其他一切静态！）导致几个输出不匹配时，我感到非常惊讶。经过几次二分查找迭代后，我意识到问题与图像读取库有关。为 Python 3.5 构建的同一版本库与 Python 3.6 版本的库行为略有不同，因此，可以用一个版本库读取的文件与另一个版本库不兼容。

但这怎么可能发生呢？看起来这个库使用了一个用 C 实现的低级 JPEG 库；同时，Python 库的不同构建版本——即使版本相同——也使用了不同的底层 C 库版本，因为它们使用了在构建机器上全局安装的版本。在每种情况下找到使用的版本并不容易，需要一些硬核软件考古（挖掘开源库 6 年前的构建日志，从中找到线索，并最终重现相同的构建）。

再次强调，差异并不显著；然而，这足以破坏兼容性，因为模型是在使用库的一个版本读取的图像上训练的，并且它们对输入的敏感性太高。用户不太可能注意到差异，但这仍然是一个破坏性变更——本不应该发生的事情，因为这是由于用户协议。

虽然阿列克谢的经验展示了在单一系统中保持兼容性的挑战，但以下来自瓦列里伊的故事突出了当版本控制问题并非出现在系统内部，而是在我们与第三方解决方案互动的节点上时，可能会变成一个更大的难题。

##### 瓦列里伊的篝火故事

有一次，我需要为一个金融机构实施 KYC（了解你的客户）解决方案。这个解决方案的一部分需要验证用户上传的 ID 文件，并确保用户的脸不在现有用户中。换句话说，这是一项关于用户唯一性的监管要求约束。作为一个熟悉构建或购买权衡的人（请参阅第三章），我使用了一个大型流行供应商的面部识别解决方案。

系统很简单：以文档作为输入，找到一张脸，计算一个向量，确保数据库中没有类似的向量，然后让用户注册；否则，让客户支持团队手动验证情况。系统一直运行良好，直到有一天不幸的是客户支持团队因误报而工作量过大。经过调查，发现供应商的 API 版本已被修复——除非不小心将其取消设置。结果，API 的新版本返回了隐式不兼容的结果，导致了这一事件。

这种失败由于其隐含性质而非常危险。外部 API 突然更改字段名称可能会导致中断，幸运的是，这种中断很容易被发现和修复。当模型版本更改时，你最初可能不会注意到——它之前返回的是浮点向量，现在返回的是类似的向量，中断可以通过正确配置的测试/监控设置或在下游任务（在这种情况下是客户验证）退化后检测到。

没有什么比在持续集成（CI）上运行的一套适当的测试更能帮助捕捉这类问题。

### 13.1.1 API 实践

几年来，行业已经发展出多种与 API 协同工作的实践。我们提到了一些我们认为效率较高的实践。它们可能并不一定专门针对机器学习系统，但它们通常与它们相关（见图 13.2）：

+   *至少设计 API 的两层结构。* 这里，我们谈论的是外部和内部层，其中前者在逻辑上是后者的子集，但并不一定遵循相同的协议。外部 API 暴露给用户或其他组件，而内部 API 被外部 API 使用。只要内部 API 不对用户暴露，就可以在不破坏兼容性的情况下进行更改。反过来，外部 API 是内部 API 的子集，应该考虑到兼容性进行设计。它有助于分离关注点，使外部 API 在兼容性方面更简单、更容易维护，同时使内部 API 更灵活、更容易更改。

+   *尽可能将 API 的机器学习和 IO 组件分离。* 当机器学习服务是无状态的并且因此是幂等的时，它最容易维护。这并不总是可能的，但这是一个值得追求的良好实践。这种方法不仅对维护有用，而且对可扩展性也有用：IO 和 ML 组件可以独立扩展，考虑到它们有不同的要求（例如，ML 组件通常受 CPU 或 GPU 限制），这是一个很好的特性。此外，它简化了系统的演变：您可以在不触及 IO 组件的情况下部署 ML 组件的新版本，并在 A/B 测试或逐步推出期间同时使用两个 ML 组件一段时间。

+   *为您的 API 构建一个客户端库以简化使用。* 不论是为外部用户还是您的团队成员，拥有一个客户端库可以降低入门门槛，从而简化调试过程并加快实验速度。它也是一个实现 API 直接不包含的实践的好地方，例如推荐的重试、超时等。

+   *考虑嵌入功能开关*（也称为功能标志）或您组织使用的任何其他替代方案。机器学习系统通常在风险环境中运行，并且始终有一种方法可以禁用模型的新版本或切换到回退解决方案，以防出现问题。功能开关不是 API 的一部分，但它们有效地充当了绕过系统行为的手段，而无需重新部署或更改 API/客户端行为。

![figure](img/CH13_F02_Babushkin.png)

##### 图 13.2 分层 API 结构更易于维护、测试和开发。

## 13.2 发布周期

机器学习系统的发布周期通常与常规软件类似。然而，有两个主要区别：

+   机器学习系统更难以测试。

+   训练一个新的模型（即使是完全自动化的流程）通常比编译代码和构建其他工件需要更多的时间。

让我们详细阐述这些观点。

由于测试复杂性，仅仅运行测试并不总是足够的，而且仍然可能发生回归。让我们暂时谈谈软件。一旦常规软件更新，通常只需要运行测试来确保一切按预期工作。但是，如果我们谈论机器学习模型，情况就完全不同了，因为许多改进都伴随着代价。当机器学习模型更新时，即使我们有代表性的测试数据集和良好的软件相关部分的测试覆盖率，我们仍然不能保证这些更改不会引发有害的结果。例如，假设最终测试集中有 100 个样本，新模型在之前被标记为错误的 3 个样本上提高了性能，但在其他样本上引入了两个新的错误。整体性能有所提高，但这种变化是否足够好以至于可以发布？

现实生活中的场景充满了类似的例子，这意味着许多发布都需要人工介入。这类似于测试用户体验的改变，其中员工应该检查这些改变是否带来了足够的利益。评估改进和退步之间的权衡通常比仅仅检查 UI 动画是否按预期工作要复杂得多。人工介入的方法可能因系统而异。在某些情况下，ML 工程师负责；在其他情况下，可能是一位产品经理或外部领域专家，或者这项工作甚至可以委托给提供大量用户和聚合反馈的众包平台。

有一些更简单的情况，某种 AutoML 允许模型（不是整个系统——只是模型）自动发布，无需额外审查。想象一下，你正在构建一个具有强大功能的先进文本编辑器：一个模仿作者风格的自动完成功能（哦，我们希望我们在写这本书的时候就有这样一个功能！）。这个软件需要为每个用户运行一个自定义模型（可能是在大型基础模型之上），收集新用户的写作并定期更新模型，而不需要人工参与，这似乎是一个好的做法；否则，它将无法扩展。这样的场景需要更高的偏执测试水平，以覆盖尽可能多的悲观路径，并在出现故障的可能性时禁用模型更新。

好吧，让我们假设测试不是问题。假设你在预处理代码中发现了错误；你确信修复它是好的解决方案，不会使事情变得更糟，并且先进的测试工具集可以帮助你确保这真的是这样。对于常规软件来说，这意味着你可以直接修复错误，运行测试，并部署新版本；通常，这不会花费太多时间。但对于 ML 系统来说，情况并非如此，因为你需要重新训练模型。我们不会使用超大规模的例子，但根据我们的经验，训练了几周的模式并不少见，所以第二天发布修复是不可能的。

在实践中，这意味着 ML 系统的发布周期通常比常规软件要长；然而，这可能会变化很大——从每天多次到一年一次，中间有多次变化。在设计系统时，请务必考虑这一点。长的发布周期意味着系统应该更加可靠，并且需要进行广泛的测试，而短的发布周期则允许系统更加敏捷，并允许进行更多的实验。

多组件系统可以为不同的组件使用不同的发布周期。想象一个包含四个组件的简单搜索引擎：一个包含要搜索的文档的索引，一个轻量级的过滤器用于初步选择，一个重型模型用于最终排名，以及一个 API 层来公开搜索结果。文档可以随时添加到索引中（无需发布），索引代码库很少被修改，过滤器层和 API 层是纯非机器学习软件，更容易测试和构建，因此它们可以更频繁地发布。反过来，排名模型每两周训练和发布一次，并附加额外的验证。这使我们能够对非机器学习组件更加敏捷，对机器学习组件更加稳定。

存在一系列与发布相关的技术，包括蓝绿部署（见图 13.3）和金丝雀部署（见图 13.4）。它们可能略有不同，但它们背后的核心思想是同时让两个或更多系统在生产环境中运行。一旦部署了新版本，新用户就会被发送到新版本，而旧版本仍然在运行。在蓝绿部署中，更改是离散的（所有用户都切换到蓝色或绿色版本），而在金丝雀部署中，发布是细粒度的，新版本仅用于一小部分现有用户。这使我们能够在生产环境中测试新版本，并在出现问题时更容易回滚。这并不特定于机器学习系统，但也可以应用于它们。机器学习系统还可以使用的一种技术是分解（例如，某些组件如模型可以与金丝雀部署一起发布；相比之下，其他组件如 API 层可以以更传统的方式发布）。

![figure](img/CH13_F03_Babushkin.png)

##### 图 13.3 蓝绿部署

![figure](img/CH13_F04_Babushkin.png)

##### 图 13.4 金丝雀部署

金丝雀部署不应与我们在第十二章中讨论的 A/B 测试混淆。虽然在技术上它们可能看起来像类似的概念（系统的多个实例都是活跃的，并且流量以适当的分割发送到它们），但它们的意图是不同的。A/B 测试用于评估系统不同版本的性能，而金丝雀部署用于在完全切换之前测试系统的最新版本。在 A/B 测试中，我们希望比较不同版本的系统性能，测试时间基于统计显著性；保留选项 A 或选项 B 都是可行的。在金丝雀部署中，我们希望确保新版本足够好，可以供所有用户使用，并且我们希望尽可能快地完全切换到它。

虽然大型企业通常倾向于有更严格的政策和更长的发布周期，但这并不总是如此：通过技术和组织方法来缩短迭代之间的时间差距是一个崇高的目标。这也是 DevOps 文化的一个重要方面，机器学习系统也不例外。如果你对这个话题感兴趣，我们建议阅读《凤凰项目》这本书。这本书不是关于机器学习系统的，甚至不是一本非常技术性的书（更像是“商业寓言”），但它是一本关于 DevOps 文化和如何在现实世界中应用它的优秀读物。

创业公司和成熟的科技巨头通常更加敏捷，发布周期也更短，但这也会有所变化。阿尔谢尼曾为一家初创公司工作，周五晚上晚些时候部署是常见的做法（有时这会导致需要工程师解决故障，而这些工程师当时已经在享受一杯美酒）。在另一家更成熟的初创公司中，发布周期非常灵活；每个工程师都可以随时部署他们的组件，但在部署前会有一个简单的护栏警告，如果时间不合适（例如，午餐后周五）。

那些有幸安排发布的人应该意识到所有依赖关系：系统如何影响其他系统，以及其他系统如何影响它。最大的故障通常发生在基础设施层面，或者是在不属于同一团队的系统或组件之间。与其他团队进行适当的沟通以避免此类问题是任何高级工程师必备的技能。不幸的是，这种技能及其重要性的理解往往需要付出一定的代价（通常是在大故障之后）。

阿尔谢尼遇到的最大故障与一个日志记录器配置有关（在构建机器学习系统时，你通常不会太在意这类配置）。一些与机器学习相关的负载发生在线程中，当试图追踪行为时，他设计了一个复杂的日志记录器来在线程间保持请求的 ID。它在某个环境中运行良好，后来被部署到另一个环境中，那里的工程师对系统的控制力要小得多。当缺陷暴露出来时，这是一个黑暗的时刻：问题只能在有 1,000 次特定类型的请求之后发生，而这种请求在之前的环境中从未发生过。理解根本原因或回滚新版本花了一些时间，因此这次事件成为了一个很好的教训，即在发布过程中引入更多的检查。

## 13.3 操作系统

仅构建系统并将其直接与其他需要其输出的组件集成，从产品逻辑的角度来看，这永远是不够的。任何系统都需要额外的连接来保证健康运行，这些连接既与技术相关，也与非技术相关。有些是为了使维护和运营更加顺畅（从工程和产品两个角度来看）；有些是由于与系统相关的隐含的非功能性需求引起的（例如法律或隐私问题）。让我们列举一些。

### 13.3.1 技术相关连接

CI 通常是整个基础设施中首先设置的元素。它有助于识别和解决集成问题，同时促进更顺畅和更快的开发过程。CI 的典型任务包括运行测试（单元或集成）和构建将用于下游（例如，用于进一步部署）的工件。然而，CI 级别可能还有其他需求，例如安全测试、性能测试、成本分析（“这个发布是否需要我们启动更多的云服务器？”）、部署到测试环境、代码风格检查、报告关键指标等等。

需要存储但不是系统数据一部分的两个主要事物是日志和指标。通常，公司中日志和指标存储、聚合和监控的方法是通用的，你只需要遵循常规方式。我们将在下一章中对此话题进行更详细的阐述。

系统的性能可能会出现故障，这并不令人惊讶。因此，系统应该连接到公司使用的警报和事件管理平台，这样值班人员就会意识到潜在的故障并做出适当的反应。

但他们具体是如何反应的呢？在这里，你可能需要准备特定的食谱，描述预期的故障模式以及如何处理它们。此外，可能还有一套额外的工具来帮助灭火，例如配置管理面板、系统特定的仪表板等等，这些内容我们将在第十六章中进行介绍。再次强调，通常 ML 和非 ML 系统之间有一个公司标准，所以你很可能只需要适应现有的软件工具集，而不需要重新发明轮子。

设计一个系统需要考虑系统的整个生命周期，而不仅仅是顺利的路径，并且保持一点偏执是有帮助的。

### 13.3.2 非技术相关连接

除了操作的技术方面之外，还有一些非技术方面应该考虑。它们通常与客户成功或合规性相关，并且并不总是显而易见。如果用户希望所有个人数据都按照通用数据保护条例（回想第十一章，想象他们的数据可以传播得多深）被清除，我们应该怎么办？是否有法规强制模型可解释，以及在不牺牲模型性能的情况下遵循它的最佳方式是什么？如果高级管理人员或初创投资者在系统中遇到错误并感到愤怒怎么办？我们如何在难以重现的场景中调试系统的行为（例如，缺陷只能通过上述高级管理人员的用户账户重现）？在系统发布之前，所有这些问题都应该得到解答，并且这些答案至少应该在设计阶段简要反映出来。否则，后续的更改可能过于昂贵而难以实施。通常需要构建额外的组件，例如一些用户模拟机制或用于数据管理和模型可解释性的管理面板，这可能需要大量的项目时间和需要其他团队（例如法律或合规性团队了解法规或网络开发团队构建所需的仪表板）。

根据我们的经验，所有这些额外的连接和考虑通常比核心系统本身花费更多的时间，公司规模越大，所需努力就越多。鉴于当前趋势，在不久的将来不太可能有所改善，因为全球范围内正在应用更多与机器学习和隐私相关的法规。

## 13.4 覆盖和后备方案

您的系统可能有一个合法的失败原因。这样的原因的一个例子可能是外部依赖：您从第三方 API 中提取了一块数据，而在某个时刻，它就不再可用了。这就是您可能想要有一个后备解决方案的情况之一。

**后备方案**是指在主计划或解决方案失败或不可用时可以使用的备用计划或替代解决方案。我们使用它来确保即使在主机器学习模型因任何原因失败的情况下，系统仍然可以运行并做出决策。

这在用于关键任务或即使最短的中断也可能导致重大后果的行业中尤为重要。例如，对于用于预测制造环境中设备故障的模型，后备方案可能是至关重要的，确保即使在主模型遇到问题时，生产也能继续进行。

使用后备方案的另一个原因是，当主系统无法提供令人满意的答案或自信的预测，或者当主模型的输出超出可接受范围时，提供一个替代解决方案。

实现后备方案有相当多的不同方法。一个常见的方法是使用一个辅助的机器学习模型，这个模型可以在不同的数据集或使用不同的算法上训练。它可能是一个更简单的基线解决方案，如我们在第八章中回顾的，或者是一个双模型设置。在这个设置中，第一个模型仅使用稳定特征构建，而第二个模型使用更大的特征集来纠正主模型的输出。这些模型可以一起使用，根据输入数据或预定的规则选择一个模型的输出。或者，可以为“核心”模型设置输入特征漂移监控（见第十四章）以检测关键的变化。

另一个选项是使用基于规则的系统作为后备方案，当模型不可用或表现不佳时，它可以提供稳定且可预测的响应。也有可能结合使用这些方法，例如使用基于规则的系统处理简单情况，而使用机器学习模型处理更复杂的情况（然而，这本身也引入了额外的复杂性和断点）。

与基线一样，一个简单的常数也可以作为我们的后备方案。最后，有时后备解决方案是回复一个明确的错误信息。

后备解决方案应该始终有一个计划来激活后备方案，并在机器学习模型和后备系统之间切换。它可以是自动的（由监控事件触发）、手动的或混合的，具体取决于用例。

一种自定义的后备类型是覆盖。这是一种在模型发出不良预测信号时手动覆盖模型输出的方式。一个例子可能是在模型的预测超出可接受范围或模型置信度太低时，放弃模型的输出并使用一个常数代替。使用覆盖的另一个原因与发布周期有关。例如，客户抱怨模型在非常特定的场景中失败。理想情况下，我们需要确保这个场景在训练数据中得到体现，重新训练模型，运行所有检查，然后部署它。但是，正如我们之前讨论的，这可能需要一段时间。因此，我们可以使用基于规则的策略覆盖模型在这个特定场景的输出，让客户满意，并在下一个版本中正确处理它。

覆盖（override）的缺点是它们不透明且容易被遗忘。因此，有一个跟踪它们的方法以及一个如何妥善处理它们的计划是很重要的；否则，它们可能会变成技术债务。拥有许多覆盖的好处是，覆盖集合可以通过*多源弱监督*来改进模型——这是一种在未标记数据上使用“标记函数”进行标记的技术（这些启发式方法并不完美，但易于实现）。标记函数提供了一个噪声数据集，这成为模型训练的基础。关于这项技术的更多细节可以在亚历山大·拉特纳等人撰写的论文中找到，“数据编程：快速创建大型训练集”([`arxiv.org/abs/1605.07723`](https://arxiv.org/abs/1605.07723))和“使用多任务弱监督训练复杂模型”([`arxiv.org/abs/1810.02840`](https://arxiv.org/abs/1810.02840))。多源弱监督的概念因名为 Snorkel 的流行库而受到行业认可([`www.snorkel.org/`](https://www.snorkel.org/))。

阿尔谢尼的同事曾经实施了一个更优雅的解决方案，帮助解决了长期发布周期的限制。他处理了一个命名实体识别问题，有时模型无法识别对客户重要的某些实体。然而，由于训练时间过长，在发现问题后重新训练新模型不是一个选择。因此，他基于知识库实现了一个解决方案：在运行模型之前，将输入文本与知识库进行核对，如果找到了可能的实体，则将其用作模型的提示。这允许团队在不重新训练的情况下解决问题。非技术人员可以在一分钟内将样本添加到知识库中，因此可以迅速解决问题。该解决方案在博客文章中有更详细的描述([`mng.bz/BgG1`](https://mng.bz/BgG1))。这个案例与覆盖有些相似，但它增加了模型的输入，而不是输出。

## 13.5 设计文档：集成

超级大零售和 PhotoStock Inc.的集成方法旨在创建用户友好、快速和高效的机制，用于库存预测或搜索结果。

### 13.5.1 超级大零售的集成

超级大零售的集成策略旨在提供无缝、动态且高度响应的预测系统，以帮助管理库存。

#### 设计文档：超级大零售

#### X. 集成

#### i. 回退策略

在面对不可预见的情况时，回退（fallback）对于保持运营效率至关重要。超级大零售采用了多级回退系统：

+   *主要回退**——*主要模型在最重要的特征子集上训练。如果没有在这个子集中检测到特征漂移/问题，它将被使用。

+   *二级回退**—*我们的下一层回退涉及 SARIMA 或 Prophet 等时间序列模型，我们在第 4.4 节中探讨了这些模型。这些模型对外部特征的依赖性较低，如果发生漂移，可以提供更稳健的预测。

+   *三级回退**—*作为最后的手段，我们会预测与上周数据类似的销售额，并对预期事件和假日进行修改。

系统会监控数据漂移和质量问题，触发警报，自动切换到适当的回退，以确保尽可能准确的预测。

#### ii. API 设计

+   *HTTP API 处理器**—*此组件将管理请求和响应，以结构化的 JSON 格式与用户进行交互。

+   *模型 API**—*这将直接从模型中提取预测。

请求格式是

```py
GET /predictions?query=<query_string>&parameters=<parameters>&version=
↪<version>
&limit=<limit>&request_id=<request_id>&sku=<sku>&entity_id=<entity_id>
&group=<group_type>
```

响应格式是

```py
{
    "predictions": [
        {
            "sku": <sku_id>,
            "demand": <demand>,
            "entity": <entity_id>,
            "period": <time_period_for_demand>,
        },
        ...
    ]
}
```

#### iii. 发布周期

##### A. 包装器发布与模型发布

在我们的集成策略中，包装器的发布和模型的发布代表两个不同的过程。以下是对每个过程的细微差别。

对于包装器（基础设施）的发布，我们应该考虑以下因素：

+   *频率和时间表.* 释放通常比模型的频率低。由于需求模式可能会在夜间发生变化，因此能够通过训练将这些模式纳入模型中非常重要。

+   *依赖项.* 基础设施发布主要依赖于软件更新、第三方服务或系统要求。这些领域的任何变化都可能需要新的发布。

+   *测试.* 综合集成测试是必须的，以确保所有组件协同工作。同时，确保向后兼容性也非常关键，以避免现有服务中断。

+   *推出.* 这通常采用标准的软件部署策略。根据变更的性质，蓝色-绿色部署可能并不总是必要的，特别是如果变更不是面向用户的，并且不影响批量作业。

+   *监控.* 重点将放在系统健康、正常运行时间、响应时间和任何错误率上。

对于模型的发布，我们应该考虑以下因素：

+   *频率和时间表**—*模型发布更频繁，并与新数据的可用性、数据模式的变化或建模技术的重大改进相关联。

+   *依赖项**—*这些主要依赖于新训练数据的质量和数量。数据模式的变化或新数据源的引入可能会触发模型的更新。

+   *测试**—*在推出之前，模型会经过严格的离线验证。一旦验证通过，它可能会在影子模式下进行测试，其中其预测与当前模型并行运行，但不会被使用。这有助于在没有任何风险的情况下，比较和验证新模型在真实世界场景中的性能。

+   *发布**—*在引入新模型时，不仅仅是部署模型文件。还需要确保任何预处理步骤、特征工程和其他管道与模型期望的一致。

+   *监控.* 主要关注模型性能指标。同时，关注数据漂移也是必要的。详见第十四章。

##### B. 包装器和模型发布之间的相互作用

在基础设施更新会影响模型（例如，数据管道的变化）的情况下，两个发布之间的协调变得至关重要。此外，模型架构的任何重大变化可能需要更新包装器以适应这些变化。通过将它们视为独立的过程，同时确保它们协调一致，我们保持系统的稳定性，并持续提高其能力。

#### iv. 运营问题

反馈对于持续改进至关重要。应向内部用户提供一个包含覆盖功能的反馈机制。这不仅有助于改进预测，还让业务用户根据实时洞察获得控制和适应性。

#### v. 非工程考虑因素

集成策略还将考虑非工程因素——例如

+   *管理面板*—对于管理和获取系统高级概述至关重要

+   *与公司级仪表板的集成*—为了实现公司范围内的可见性和决策

+   *附加报告*—对于深入了解和分析至关重要

+   *覆盖*—考虑到不可预见或独特情况下的手动调整的必要功能

此外，公司使用的标准 CI 工具以及典型的调度器将被集成，以保持一致性和优化工作流程。

#### vi. 部署

由于我们的受众主要是内部客户和频繁的批量作业，因此目前没有绿色-蓝色或金丝雀部署的迫切需要。由于没有终端用户流量，这种分阶段部署的需求不存在，简化了我们的发布策略。

### 13.5.2 PhotoStock Inc.的集成

PhotoStock Inc.的集成策略专注于提供最相关的搜索结果，无论搜索查询的复杂性如何，同时保持快速响应。

#### 设计文档：PhotoStock Inc.

#### X. 集成

#### i. API 设计

我们的搜索引擎需要暴露一个 HTTP API 处理器，它接受一个查询字符串+可选的附加过滤器（例如，价格、收藏、分辨率、作者等）并返回与查询匹配的图片 ID 列表，按相关性排序。除了这些以产品为中心的参数外，我们还需要传递更多技术参数，如`version`、`limit`和`request_id`。

处理器仅用于内部，不会公开暴露，因此由于它在一个私有网络中运行，我们不需要担心身份验证和授权。

我们无法使服务完全无状态（我们需要拥有索引和覆盖），但所有与查询相关的元数据都应该由后端服务处理，因为它们已经存储了其他用户的元数据。

在底层，我们将使用简单的级联来缩小搜索结果。我们首先根据可选的过滤器进行过滤，然后从嵌入空间中获取查询字符串的最近邻，最后根据相关性对结果进行排序。

我们考虑使用 Qdrant ([`qdrant.tech/`](https://qdrant.tech/))作为一个能够大规模过滤和获取候选者的快速向量数据库；然而，公司之前尚未使用过它，因此在使用生产环境之前，我们可能需要对其进行适当的测试。或者，如果需要，我们还可以考虑使用其他向量数据库。

请求格式是

```py
```

GET /search?query=<query_string>&filters=<filters>&version=<version>

&limit=<limit>&request_id=<request_id>

```py
```

响应格式是

```py
```

{

    "results": [

        {

            "id": <photo_id>,

            "score": <score>

        },

        ...

    ]

}

```py
```

请求和响应都使用 JSON 格式，这是我们内部 API 中使用的默认格式。请求和响应的结构目前简单直接，但在需要时可以扩展。

底层 API 应该按照以下方式分层：

+   HTTP API 处理器仅作为底层 API 的代理，不包含任何业务逻辑；它只是解析请求，将其传递给底层 API，将响应封装成 JSON 格式，并处理错误。

+   向量数据库 API 负责根据查询字符串的嵌入进行过滤和获取候选者。

+   模型 API 负责从字符串中提取嵌入并评分候选者。

+   排名 API 负责根据相关性对候选者进行排序并应用可能的覆盖。

#### ii. 发布周期

我们假设模型更新将相对较少，因为训练需要花费大量时间，我们也不期望数据随时间发生显著变化。我们可以预期每 1 到 2 个月发布一个新的模型和相关 API，而大多数热更新将仅与索引和覆盖有关。

索引是搜索引擎的核心，它将需要定期更新（针对数据，而不是软件）。我们可以每天添加新项目（例如，通过每晚运行的批处理作业），并在需要时准备更新索引（例如，删除被禁止的图像或根据 VIP 用户的特殊请求添加新图像）。

#### iii. 运营关注点

许多内部用户可以对搜索结果提供大量反馈，因此我们需要为他们提供适当的工具。一开始，我们可以在内部 PhotoStock Inc.用户的搜索结果页面上添加一个“报告不良匹配”按钮。这将向数据网关发送请求，因此我们将照片 ID、搜索引擎结果页位置和查询字符串保存到数据湖中。然后我们可以在模型重新训练阶段、错误分析和手动覆盖期间使用这些数据。未来，我们可以考虑为一些外部认证用户提供类似的功能（例如，我们信任的顶级买家）。

#### iv. 覆盖和回退

作为回退，我们将使用现有的基于 Elasticsearch 的搜索引擎。虽然它在相关性方面可能不如新设定的那么好，但它仍然是一个不错的搜索引擎。

关于覆盖，我们可能对某些查询有手动覆盖，这些可以存储在一个单独的数据库中。这种情况可能发生在对流行/关键查询的相关性较差时，我们无法及时用模型修复。目前，它可能是一个简单的键值存储，其中键是查询字符串的正则表达式，值是照片 ID 的列表，我们将使用这些 ID 在搜索引擎结果页面上。这种解决方案的可扩展性不高，但对于第一个版本来说足够稳固。我们可能希望在将来为管理这些覆盖提供一个简单的用户界面。

与“不良照片”相关的另一种可能的覆盖类型，我们希望从搜索结果中隐藏（例如，通过审查的裸露/暴力照片）。然而，如果我们突然意识到某个图像不再适合我们的搜索结果，我们可以简单地从索引中删除它。

## 摘要

+   记住，集成不是一个一次性的事件或项目的某个阶段，而是一个从项目开始到系统退役的持续过程。

+   在为您的系统选择 API 时，您应该寻找的两个主要品质是简洁性和可预测性。

+   我们认为对于机器学习系统而言，有效的 API 实践包括至少设计两层 API，在可能的情况下将 API 的机器学习和 IO 组件分离，为 API 构建客户端库，以及嵌入功能开关或其替代方案。

+   有一个回退方案或替代方案，可以在主要计划或解决方案失败或不可用时使用。
