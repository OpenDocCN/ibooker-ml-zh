# 10 训练管道

### 本章节涵盖

+   训练管道的本质

+   您可以使用哪些工具和平台来构建和维护训练管道

+   训练管道的可扩展性和可配置性

+   测试管道的方法

有一个经验法则可以区分经验丰富的机器学习（ML）工程师和新手：要求他们用一句话描述一个工作系统的训练过程。新手往往关注模型，而有些经验丰富的人会包括数据处理。成熟的工程师通常描述管道——最终产生训练好的 ML 模型所需的一系列阶段。在本章中，我们将以 ML 工程师的视角来分析这些步骤，并讨论如何相互连接和编排它们。

## 10.1 训练管道：你是谁？

想象一家小型比萨连锁公司。它在当地市场取得了成功，但那些理解软件正在吞噬世界（这句话来自马克·安德森的文章，[`a16z.com/why-software-is-eating-the-world/`](https://a16z.com/why-software-is-eating-the-world/))以及一切都在数字化的老板们知道还有更多的市场份额可以抢占。因此，在 COVID 大流行之前，它就下注于数字化，雇佣了几位工程师来构建移动应用、简单的客户关系管理计划和多个内部软件系统。换句话说，这家公司没有科技巨头的规模或胃口，但它遵循主要趋势，并知道如何现在投资软件以在将来获得显著的利润。只需提及，它的应用帮助公司在 2020 年大流行中幸存下来。

现在，随着公司跟随趋势，人工智能热潮全面展开，雇佣 Jane 这位年轻而有潜力的机器学习工程师也就不足为奇了。她的兴趣在机器学习方面是无可否认的。入职后，CTO 将她的第一个问题交给她解决：构建一个 AI 驱动的助手，帮助比萨师傅对每个订单的比萨底部的组件进行基本视觉评估，如列出和计算组件数量。

在这家比萨公司中，软件开发生命周期至今尚未包括机器学习系统。因此，工程经理 Alex 要求 Jane 准备一个模型和一小段代码片段，展示如何运行它；内部系统团队将处理其余部分。

快进几个月后：Jane 收集了一个小数据集并训练了一个模型，在最初的测试中一切看起来都很正常，所以 Alex 的团队设法将其包装成一个服务。但在部署前，产品经理带来了多个新食谱，并说模型也应该能够支持这些。这并不复杂，只需要添加一些更多数据，更改标签映射，并重新训练模型——听起来好像不会对部署时间表产生太大影响。然而，经过讨论，Jane 和 Alex 认识到即使假设新的数据集已经准备好，这也需要额外几个月的时间。这里出了什么问题？Jane 执行了所有训练模型所需的步骤——手动验证数据集，在 Jupyter Notebook 环境中应用大量数据处理和清洗步骤，多次中断训练模型，临时性地与厨师和客户满意度团队验证结果，将训练好的模型上传到公司的共享存储，并将链接发送回 Alex。

注意：她做了所有正确的事情，但采取了一种临时的方法，没有适当的努力使这些步骤在一个单一的、透明的流程中可重复。

通过这个例子，我们想表明机器学习不仅仅是训练一个模型，还包括构建一个流水线，以便以可重复的方式准备模型和其他工件。在本章中，我们将讨论流水线中的步骤，如何编排它们，以及如何使它们可重复。

### 10.1.1 训练流水线与推理流水线

在机器学习领域，“流水线”这个术语被用于许多不同的上下文中。通常，人们将流水线视为一系列有序的步骤和过程。每个步骤都是一个程序，它接受一些输入，执行一些操作，并产生一些输出。一个步骤的输出是下一个步骤的输入。更正式地说，我们通常可以将流水线描述为步骤的有向无环图（DAG）。

要使事情更加复杂，模型本身通常是一个另一种类型的流水线。例如，一个简单的逻辑回归分类器通常通过一个特征缩放步骤来增强，从而形成一个至少包含两个步骤的流水线。通常，还会有基本的特征工程（例如，对分类变量进行独热编码），因此即使是最简单的模型也具有流水线的特性。其他模态，如图像、文本、音频等，需要额外的预处理步骤。例如，一个典型的图像分类模型是一个包含图像读取、归一化、调整大小以及模型本身的流水线。如果我们转向自然语言处理，流水线几乎总是从文本分词等步骤开始。总的来说，围绕“流水线”这个术语本身存在很多模糊和混淆的空间。为了使事情更清晰，我们将使用以下术语。

*训练管道*指的是用于训练模型的管道。它是一系列步骤的 DAG（有向无环图），这些步骤接收完整的数据集和可选的元数据作为输入，并产生一个训练好的模型作为输出。它比模型本身是一个更高层次的抽象（见图 10.1）。

![figure](img/CH10_F01_Babushkin.png)

##### 图 10.1 表示训练管道的 DAG 方案

*推理管道*指的是用于在生产环境中运行模型或作为训练管道一部分的管道（例如，使用梯度下降训练神经网络需要多个推理步骤，每个步骤都是一个管道）。它是一系列步骤的 DAG，这些步骤接收原始数据作为输入，并产生预测作为输出。它比训练管道是一个更低层次的抽象（见图 10.2）。

![figure](img/CH10_F02_Babushkin.png)

##### 图 10.2 表示管道中训练步骤的方案

在本章中，我们将重点关注训练管道，而推理管道将在第十五章中讨论。在最高层次上，典型的训练管道包括以下步骤：

1.  数据获取

1.  预处理

1.  模型训练

1.  模型评估和测试

1.  后处理

1.  报告生成

1.  艺术品打包

让我们简要地分解每个步骤。

*数据获取*是管道中的第一步。它负责从源下载数据，并使其对后续步骤可用。如第六章所述，我们不将自己视为数据工程专家，因此不会详细讨论数据获取和存储。

*预处理*通常是管道中的第二步。这是一个非常通用的术语，对于不同的任务可能有不同的含义。一般来说，预处理是一组为准备数据以供模型训练而执行的操作。虽然我们为了本书的结构将训练和推理管道分开，但在实践中这种区分可能有些模糊。例如，你可以在训练模型之前完全预处理原始数据集，或者将其作为单个模型推理的一部分。在这种情况下，我们讨论的是特定于训练的预处理。特征选择是此类预处理的一个例子：我们只在训练之前执行它，并将选定的特征冻结在后续步骤中。*模型训练*是训练管道的核心。这是一个接收预处理数据并产生训练模型的步骤；它通常是管道中最长且最复杂的步骤（尤其是在基于深度学习的系统中）。

模型训练完成后，它可以被*评估和测试*。这些不同的方面旨在回答同一个问题：模型有多好？评估是计算指标的一个步骤，而测试是一系列检查，以确保模型按预期工作。

*后处理*是在评估和测试之后执行的一个步骤。它是一组执行以准备模型部署的动作。在这里，我们可以将模型转换为支持的目标平台格式，如果适用，应用后训练量化或其他优化，准备供人类评估的任务，等等。值得注意的是，后处理和评估可以互换。例如，我们可以在将模型转换为目标格式之前评估模型，或者相反，我们可以在将模型转换为目标格式后使用部署格式进行评估。

*工件打包*是流程中的最后一步。它负责将模型和其他工件（例如，包含预处理参数的配置文件）打包成易于部署到生产环境的格式。这里的目的是简化进一步的部署并分离训练和部署流程。理想情况下，输出应该尽可能与训练流程无关。例如，模型被导出为 ONNX 这样的通用格式以供后端服务或 CoreML 以供 iOS 服务使用，所有配置文件都导出为 JSON 这样的通用格式，并且训练流程中的任何变化都应尽可能少地影响部署。否则，部署流程将与训练流程紧密耦合，并在每次训练流程更新后需要许多更改，从而成为快速模型开发和相关实验的障碍。

*报告*是工件的一种特殊情况。这是一个通用术语，可以与许多事物相关联，包括包含验证/测试指标的基本表格、各种类型的错误分析（参见第九章）、额外的可视化和其他辅助信息。虽然这些工件不是直接用于部署的，但它们对于考虑训练成功至关重要；没有责任心的工程师在至少简要查看适当的报告之前不会发布新训练的模型。我们看到的唯一例外是 AutoML 场景的一种变体，当根据用户请求自动训练许多新模型时。在这种情况下，手动验证并不总是可能的；因此，工程师只能审查可疑的异常值。我们将在第十三章中讨论发布周期的话题。

一些这些工件与实验跟踪和可重复性相关，对于有多个贡献者或参与方的项目来说，这些是至关重要的。当研究人员独立研究他们自己的问题时，他们可以使用简单的笔记本或文本文件来跟踪他们所有的想法和实验。然而，当一组研究人员共同研究同一个问题时，他们需要一个更结构化的方式来跟踪、比较和重现他们的实验。为所有实验建立一个集中式存储库是帮助实现这一目标的工具之一。

## 10.2 工具和平台

与训练管道相关的工具和实践，以及机器学习系统设计背景下的推理管道、部署管道和监控服务，通常归因于机器学习操作（MLOps）。鉴于 MLOps 是一个相对较新的领域，平台和工具还没有形成良好的标准。其中一些相对容易识别（MLflow、Kubeflow、BentoML、AWS Sagemaker、Google Vertex AI、Azure ML），一些正在获得关注，而一些还处于早期开发阶段。

在这本书中，我们不希望突出任何特定的平台或工具，因此我们不会详细讨论它们。鉴于 MLOps 领域的变革速度，我们目前对工具和平台的理解在本书出版时很可能已经过时。相反，我们将专注于所有平台和工具共有的原则和实践。在最简单的情况下，你可以使用通用的非机器学习工具实现完整的训练管道——例如，通过创建一系列与 shell 脚本连接的 Python 脚本。然而，在实践中很少是这样：通常有一些特定的机器学习工具引入了抽象并简化了管道实现。大多数 MLOps 工具都是“有偏见的”，这意味着使用它们强烈暗示了特定的代码结构方式。从长远来看，这提高了训练管道代码的一致性，并使其在未来更容易维护（参见第十六章）。通常需要从训练管道平台获得的一些典型功能包括

+   *解决依赖关系—*由于管道是一系列步骤的 DAG，因此解决步骤之间的依赖关系并按正确顺序运行它们很重要。

+   *可重现性—*给定一组参数和管道版本（例如，由 git 提交指定），管道应该每次都产生相同的结果。

+   *与计算资源集成（如云提供商或 Kubernetes 安装）*—例如，用户应该能够在特定的计算实例上运行作业（例如，具有 X 个 CPU 核心和 N 个 GPU 的虚拟机）或在一组实例的集群上。

+   *工件存储—*一旦训练管道运行完毕，其工件应该可用。实验跟踪可以被视为此功能的一个子集。

+   *缓存中间结果—*只要管道中的许多步骤都是计算密集型的，缓存中间结果以节省资源和时间就很重要。

除了功能之外，还重要的是要提及一些实践者从平台中需要的非功能性需求，包括成本效益和数据隐私（尤其是在医疗保健或法律等敏感领域）。

这里需要强调的是，功能不一定需要由同一个平台提供。例如，你可以使用一个通用平台来运行流水线，并使用一个定制的工具来进行实验跟踪，因为市场解决方案无法满足定制需求。有时这只是一个成本优化的问题。阿森尼曾在一家公司工作，该公司使用两种工具的混合体，因为其中一种提供了许多有用的功能，并且整体上为开发者提供了良好的体验，而另一种则与提供最便宜 GPU 的云服务提供商集成。在这种情况下，花一些时间进行集成并在训练成本上节省大量资金是合理的。

选择合适的工具取决于问题规模和公司的基础设施。FAANG 级别的公司通常拥有自己的机器学习平台和工具，能够在适当的规模上运行，而较小的公司通常更倾向于使用一套开源工具和云服务。每个工具都有自己的采用成本，因此选择适合特定问题的工具非常重要。据我们所知，没有一种适合所有情况的解决方案，这与许多其他更为成熟的软件工程问题不同（见图 10.3）。

![figure](img/CH10_F03_Babushkin.png)

##### 图 10.3 使用适当的框架进行训练管道通常是良好的实践，尽管有时保持简单就足够了。

## 10.3 可扩展性

可扩展性可能是某些问题的训练管道的关键属性。如果我们处理的是数千个样本的数据集，那么这并不是一个重大的问题，因为即使是单台机器也可能能够处理它。然而，当涉及到大型数据集时，情况就改变了。什么构成了大型数据集？这取决于问题和数据类型（1 百万张表格记录与 1 百万个视频剪辑相比微不足道），但如果我们必须选择一个标准“对于不适合单台机器 RAM 的数据集”呢？

当前数据集的大小不应与未来预期使用的数据集大小混淆。我们可能会遇到冷启动问题（见第六章），即使有数千个样本也可能在系统开发的初始阶段具有显著优势。然而，在未来，它可能增长几个数量级，如果你想要使用所有数据，你需要能够处理它。

虽然在大型数据集上训练模型没有一劳永逸的解决方案，但有两种经典的软件工程方法可以实现扩展。这些是垂直扩展和水平扩展（见图 10.4）。

![figure](img/CH10_F04_Babushkin.png)

##### 图 10.4 垂直扩展与水平扩展

*垂直扩展*意味着升级您的硬件或用更强大的节点替换训练机器。这种方法的最大的优点在于其简单性；增加更多资源（尤其是在使用云计算资源的情况下，这通常是情况）非常容易。然而，缺点是垂直扩展的局限性。比如说，您将机器的 RAM 加倍甚至四倍，并将 GPU 升级到最新一代。如果还不够，在垂直扩展方法内您能做的事情就很少了。

*水平扩展*涉及在多台机器之间分配负载。水平扩展的第一级是使用多 GPU 机器；在这种情况下，机器学习工程师通常需要修改管道代码，因为大部分繁重的工作已经由训练框架完成。然而，这并不是真正的水平扩展，因为我们仍在谈论一台机器。真正的水平扩展涉及使用多台机器并将负载在他们之间分配。如今，这种扩展通常也由框架提供，但这种方法更复杂，通常在实施过程中需要更多的工程努力。微软的 DeepSpeed、Hugging Face 的 Accelerate 以及起源于 Uber 的 Horovod 就是这类框架的例子。

一种特定的机器学习扩展方法是子采样：如果您的数据集过于庞大，对其进行子采样并减少所需的计算资源可能是合理的。最直接的子采样方法适用于大多数问题，涉及去除重复的样本。然而，还有更激进的方法：基于简单距离函数（例如，字符串的 Levenshtein 距离）的近重复样本下采样，以及基于内部 ID（例如，每个用户不超过 X 个样本）或人工 ID（例如，使用简单方法对整个数据集进行聚类并保持每个聚类不超过 X 个样本）的下采样。

在机器学习管道中，扩展通常需要更改其他管道参数：例如，批大小受 GPU 内存限制，较大的批大小会导致收敛更快，而学习率计划取决于批大小和预期的收敛计划。因此，改变参数的能力很重要。

## 10.4 可配置性

当机器学习工程师设计训练管道的可配置性时，存在一个光谱，每边都有两种不良做法：配置不足和配置过度。

*配置不足*意味着管道的可配置性不足，这使得改变模型的架构、数据集、预处理步骤等变得困难。事物在这里和那里以复杂的方式硬编码，很难理解管道的工作方式或改变最简单的方面。这是机器学习发展早期的一个典型问题。当管道小而简单时，理解和改变都很容易。因此，没有软件工程背景的研究人员可能会觉得引入适当的软件抽象是不必要的，从而导致越来越多的代码被无结构地添加。这种反模式在研究人员中很常见。

*配置过度*同样不理想。典型的机器学习管道有许多与数据集处理、模型架构、特征工程和训练过程相关的超参数。在现实中，很难预测所有可能的使用案例和可更改的参数，并且缺乏经验的开发者可能会试图覆盖所有可能的情况，并尽可能多地引入抽象。在某个时候，这些额外的抽象层只会增加复杂性。请注意，在本节中，我们交替使用“训练管道的参数”和“模型的超参数”。仅作提醒，超参数是在训练过程中没有学习到的模型参数，而是由用户设置的。

在以下列表中过度配置的代码示例中，我们可以看到多级子配置层次结构如何使事情复杂化。

##### 列表 10.1 多级子编码层次结构

```py
def train():
   ...
   batch_size = 32
   ... 
   learning_rate = 3e-4
   ...
   model.train(data, batch_size, loss_fn)
   ...   

^ example of underconfigured code: things are too rigid  

class Config(BaseConfig):
    def __init__(self):
        self.data_config = DataConfig()
        self.model_config = ModelConfig()
        self.training_config = TrainingConfig()
        self.inference_config = InferenceConfig()
        self.environment_config = EnvironmentConfig()

class DataConfig(BaseConfig):
    def __init__(self):
        self.train_data_config = TrainDataConfig()
        self.validation_data_config = ValidationDataConfig()
        self.test_data_config = TestDataConfig()

config = Config(
    data_config=DataConfig(
        train_data_config=TrainDataConfig(
            ...
        ),
        validation_data_config=ValidationDataConfig(
            ...
        ),
        test_data_config=TestDataConfig(
            ...
        ),
    ),
    ...
)
```

为了在两种极端之间找到一个良好的平衡，你应该估计各种参数被改变的概率。例如，数据集被更新几乎是 100%确定的事情，而模型内部的激活函数被改变的可能性并不大。因此，在忽略激活函数的同时，使数据集可配置是合理的。可变参数因各种管道而异，因此找到良好平衡的唯一方法就是考虑你会在接下来的几个月内认为的低垂之果的潜在实验。我们推荐的一个有助于基于深度学习管道的有用指南是由谷歌团队提供的：[`github.com/google-research/tuning_playbook`](https://github.com/google-research/tuning_playbook)。

在初步决定哪些超参数可调整后，确定调整策略非常重要。当计算资源有限时，手工实验更可取。当资源充足时，应用自动超参数调整方法（如简单的随机搜索或更高级的贝叶斯优化）是有意义的。超参数调整工具（例如，Hyperopt、Optuna 和 scikit-optimize）可以是机器学习平台的一部分，并可能规定配置文件应该如何看起来。

根据我们的经验，广泛的超参数调整更适合小数据集，因为在合理的时间内可以运行大量的实验。当单个实验需要数周时间时，更实际的做法是依靠机器学习工程师的直觉手动运行几个实验。值得注意的是，使用小数据集进行的实验可能有助于建立这种直觉，尽管并非每个结论都能推广到大规模的训练运行中。

找到适当的方式来配置训练管道非常重要（见图 10.5）。

![figure](img/CH10_F05_Babushkin.png)

##### 图 10.5 管道需要适当的配置以实现最佳性能

最典型的做法可能是分配一个单独的文件（通常是用像 YAML 或 TOML 这样的特定语言编写的），其中包含所有可更改的值。另一种流行的方法是使用像 Hydra ([`hydra.cc/`](https://hydra.cc/))这样的库。我们见过的一个反模式是将配置分散在具有相同参数的多个训练管道文件中，这些文件具有不同的优先级水平（例如，批大小可以从文件 X 中读取，但如果未指定，则尝试从文件 Y 中获取）。在实验阶段可能会出错，特别是如果实验是由不太熟悉这个特定管道的缺乏经验的工程师进行的。

## 10.5 测试

我们在机器学习管道中经常看到的一个常见问题是缺乏测试。这并不奇怪，因为测试机器学习管道并不是一件容易的事情。在构建常规软件系统时，我们可以通过运行它并检查输出来进行测试。然而，运行训练管道可能需要几天时间，显然我们不可能在每次实施更改后都再次运行它。另一个问题，如前所述，是机器学习管道通常配置不足，这使得它们难以单独测试。最后，考虑到可能的超参数数量，在合理的时间内测试所有可能的组合几乎是不可能的。简而言之，向机器学习管道引入测试是一项具有挑战性的任务。但这是值得做的！

适当的测试有三个目的：

+   在引入更改的同时避免回归错误

+   通过尽早捕捉缺陷来提高迭代速度

+   改进整体管道设计，因为它迫使工程师找到适当的可配置性平衡

我们对测试机器学习管道的建议是结合对整个管道的高级别烟雾测试和对其最重要的单个组件的低级别单元测试。

烟雾测试应该尽可能快，这样你就可以在数据集的小子集上运行它，运行少量 epoch，也许使用模型的简化版本。它应该检查管道运行无误并产生合理的输出——例如，它确保在这个玩具数据集上损失正在减少。以下列表展示了训练管道烟雾测试的简化示例。

##### 列表 10.2 训练管道的烟雾测试可能看起来像什么

```py
from unittest.mock import patch, Mock
import torch
from training_pipeline import train, get_config

class DummyResnet(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.model = torch.nn.Sequential(torch.nn.AdaptiveAvgPool2d(1),
                                         torch.nn.Conv2d(3, 2048, 1))

    def forward(self, x):
        return self.model(x).squeeze(-1).squeeze(-1)

def test_train_pipeline():
    config = get_config()
    config["dataset_path"] = "/path/to/fixture"
    config["num_epochs"] = 1

    mock = Mock(wraps=DummyResnet)
    with patch('training_pipeline.models.Resnet', mock):
        result = train(config)
        assert mock.call_count == 1
        assert result['train_loss'] < .5
        assert result['val_loss'] < 1
```

这样的烟雾测试显著提高了迭代速度，从而简化了实验和调试。然而，也存在一个缺点。像任何集成测试一样，它们需要大量的维护工作。这是因为几乎任何重大的管道更改都可能影响代码。低级别的单元测试应该覆盖管道的各个组件。拥有几个这样的单元测试或甚至没有它们并不罕见——如果你没有它们，也没有什么可耻的。然而，我们建议至少覆盖最敏感的组件。这样一个敏感组件的例子可能是最终的模型转换——想象一下，模型是用 Pytorch 训练的，后来被部署到 iOS（并使用 CoreML 运行）以及后端（并使用 ONNX 运行）。确保模型被正确转换并且转换过程没有引入任何变化非常重要，这意味着转换模型的输出应该与原始模型相同。

### 10.5.1 基于属性的测试

另一组测试适用于训练好的模型，其灵感来源于基于属性的测试方法。基于属性的测试是一种软件测试方法，它涉及为函数或系统生成随机输入，然后验证对于所有输入某些属性或不变量是否成立。与编写具有预定输入和预期输出的特定测试用例不同，基于属性的测试侧重于定义系统应满足的一般属性，然后自动生成测试用例以验证这些属性。

在机器学习项目的背景下，基于属性的测试可以用来确保最终训练好的模型按预期行为并满足某些属性。以下是一些在机器学习项目中可以测试的属性示例：

+   *一致性**—*给定相同的输入数据，模型应该始终如一地产生相同的输出或预测，无论执行多少次：

![figure](img/babushkin-ch10-eqs-0x.png)

+   *单调性**—*在简单的机器学习模型中，输出应该相对于某些输入特征单调递增或递减。基于属性的测试可以用来验证模型的输出遵循预期的单调行为：

![figure](img/babushkin-ch10-eqs-1x.png)

单调性通常在各种价格预测模型中是预期的。例如，如果其他特征固定，房屋的价格应该随着其面积的增大而增加。

+   *变换不变性**—*某些机器学习模型应该在输入数据的特定变换（如缩放或旋转）下保持不变。基于属性的测试可以用来检查当输入数据以特定方式变换时，模型的输出是否保持不变：

![figure](img/babushkin-ch10-eqs-2x.png)

其中 g 是一个预期的转换。这可能是对图像进行旋转或缩放，将实体更改为其同义词进行自然语言处理，改变声音的音量，等等。

+   *鲁棒性**—*模型应能抵抗输入数据中的小扰动。可以使用基于属性的测试来验证当输入数据被小量扰动时，模型的输出不会发生显著变化：

![figure](img/babushkin-ch10-eqs-3x.png)

+   *否定**—*当输入数据被反转时，模型应提供相反的预测。可以使用基于属性的测试来验证当输入数据被否定时，模型的输出是预期输出的相反。最简单的例子是情感分析，其中模型通常应该预测负面情感，如果将单词“爱”替换为“恨”：

![figure](img/babushkin-ch10-eqs-4x.png)

我们已经在 5.2.1 节中介绍了一个非常类似的概念。区别在于，在一种情况下，我们期望结果存在一些变化（并且我们想要测量它），而在另一种情况下，我们期望严格的致性（因此我们想要断言它）。使用一些数据样本作为固定值并为他们编写基于属性的测试是确保模型按预期行为并保持其可靠性的好方法。

通常不会在设计文档中包含一个精确的测试列表；然而，我们建议提前考虑它并在文档中提及。设计文档通常用作实现的参考，因此提及测试是有用的。

注意：如果您对机器学习测试感兴趣，我们推荐阅读 Arseny 关于该主题的深入评论的幻灯片：[`arseny.info/reliable_ML`](https://arseny.info/reliable_ML)。

## 10.6 设计文档：训练管道

随着我们继续为我们的虚构业务制作两个独立的设计文档的工作，现在是时候介绍 Supermegaretail 和 PhotoStock Inc.的训练管道了。

### 10.6.1 Supermegaretail 的训练管道

让我们看看 Supermegaretail 可能的管道可能是什么样子。

#### 设计文档：Supermegaretail

#### VII. 训练管道

#### i. 概述

Supermegaretail 的需求预测模型旨在预测特定时间段内特定商店中特定商品的需求。为了实现这一点，我们需要一个训练管道，它可以预处理数据，训练模型，并评估其性能。我们假设该管道应该是可扩展的并且易于维护，并允许对各种模型架构、特征工程技术和超参数进行实验。

#### ii. 工具集

管道建议的工具是

+   Python 作为主要编程语言，因其多功能性和丰富的数据处理和机器学习生态系统

+   用于并行和分布式计算的 Spark

+   PyTorch 用于深度学习模型

+   MLflow 用于跟踪实验和管理机器学习生命周期

+   Docker 用于容器化和可重复性

+   AWS Sagemaker 或 Google Cloud AI Platform 用于基于云的训练和部署

#### iii. 数据预处理

数据预处理阶段应包括

+   *数据清洗*—处理缺失值、删除重复项和纠正错误数据点

+   *特征工程*—从现有特征创建新特征，例如汇总销售数据、提取时间特征（如星期几、月份等）以及整合外部数据（例如，假日、天气和促销活动）

+   *数据归一化*—将数值特征缩放到标准范围

+   *训练-测试分割*—将数据集分割成训练集和验证集，确保它们在时间上不重叠，以防止数据泄露

#### iv. 模型训练

模型训练阶段应适应各种模型架构和配置，包括

+   *基线模型*—简单的预测方法，如移动平均、指数平滑和自回归积分移动平均

+   *机器学习模型*—决策树、随机森林、梯度提升机和支持向量机

+   *深度学习模型*—循环神经网络、长短期记忆网络和转换器（如有需要！）

我们还应该实现一个超参数调整机制，例如网格搜索或贝叶斯优化，以找到最佳模型配置。

#### v. 模型评估

应使用我们之前推导出的指标来评估模型性能，例如 1.5、25、50、75、95 和 99 分位数指标，以及作为是和权重等于 SKU 价格的权重。它计算为点估计值，具有 95%置信区间（使用自助法或交叉验证），加上标准指标，如平均绝对误差（MAE）、平均平方误差（MSE）或均方根误差（RMSE）。我们还应包括针对 Supermegaretail 业务需求的特定自定义指标，例如过剩库存和缺货情况的成本。（见*验证*章节。）

#### vi. 实验跟踪和模型管理

使用像 MLflow 这样的工具，我们应该跟踪和管理实验，包括

+   模型参数和超参数

+   输入数据和特征工程技术

+   评估指标和性能

+   模型工件，如训练模型权重和序列化模型

#### vii. 持续集成和部署

训练管道应集成到 Supermegaretail 现有的 CI/CD 基础设施中。这包括定期设置自动训练和评估，确保使用最新数据更新模型，并以最小的人工干预将更新的模型部署到生产中。

#### viii. 监控和维护

我们应该监控模型在生产中的性能，并设置对预期性能的重大偏差的警报。这将使我们能够及早发现问题，并在必要时触发重新训练或模型更新（见第十四章）。

#### ix. 未来工作和实验

训练流水线应该足够灵活，以适应未来的实验，例如整合额外的数据源、尝试新的模型架构以及调整损失函数以优化特定的业务目标。

### 10.6.2 PhotoStock Inc.的训练流水线

现在我们回到 PhotoStock Inc.，在那里我们被要求构建一个智能内部搜索引擎以提升正确结果的输出。

#### 设计文档：PhotoStock Inc.

#### VII. 训练流水线

多模态排名模型是 PhotoStock Inc.搜索引擎的核心组件，我们需要一个训练流水线来训练这个模型。如前所述，我们有一个基于预训练 CLIP 模型的坚实基础。然而，我们需要在我们的数据集上对其进行微调，这是一个图像和文本描述的组合。虽然数据集最初可能不会很大，但它可能会在后期增长，因此我们需要使流水线具有一定的可扩展性。我们假设我们可以从在单个顶级 GPU 上训练它开始，但希望将来能够将其扩展到单台机器上的多个 GPU。目前我们不追求完全分布式训练。

我们建议以下工具集用于流水线：

+   *PyTorch*——默认的深度学习框架，因为它是全球最受欢迎的，并且拥有大量的社区支持。

+   *PyTorch Lightning*——一个高级框架，用于简化训练循环并使其更具可重复性。

+   *Flyte*——一个工作流程管理工具，因为它已经在公司用于数据工程工作，我们可以重用一些现有的代码。

+   *AWS Sagemaker*——一个训练平台，因为 AWS 已经在公司使用，并且很容易与 Flyte 集成。

+   *Tensorboard*——一个简单的训练指标可视化工具。

+   *Docker*——一个容器化工具，使流水线更加便携和可重复。

流水线的输出应该是两个模型：一个文本编码器和图像编码器。两者都应转换为静态图表示（ONNX）并保存到 S3。此外，我们还应该输出用于推理（提示生成、图像预处理、距离函数）的训练参数列表。最后，每次运行都应该生成一个包含训练指标的报告。所有工件应在运行后保存到 S3。

我们预计将在以下领域进行积极的实验：

+   *要微调的内容*——一些组件、整个模型，或者两者的组合，并使用自定义调度器。

+   *增强技术*——我们可以为图像和文本使用不同的增强技术。

+   *各种损失函数*——为不同的组件分配不同的权重。

+   *CLIP 模型系列的多种骨干网络*——例如，基于卷积或基于 transformer 的图像编码器；对于哪一个更好，没有强烈的直觉，因此我们需要对两者都进行实验。

+   *为文本编码器生成文本提示的方法*——它必须是图像描述、标签等的组合。

+   *预处理图像输入的方法*——例如，调整大小、裁剪、填充参数。

## 摘要

+   记住，机器学习不仅仅是训练一个模型。其支柱之一是构建一个管道，允许以可重复的方式准备模型和其他工件。

+   虽然训练管道和推理管道之间的区别可能看起来有些模糊且难以区分，但我们建议以下定义：训练管道用于训练模型本身，而推理管道用于在生产环境中运行模型或作为训练管道的一部分。

+   典型训练管道的生命周期包括七个连续步骤，从数据获取、预处理、训练、评估和测试模型到后处理、工件打包和报告生成。

+   到目前为止，在处理管道时还没有确立的平台和工具的标准。然而，在通用机器学习中存在经过时间考验的解决方案，您可以根据您设计的系统类型找到合适的解决方案。

+   在某个时候，您将面临在两种管道扩展方法之间进行选择——垂直扩展或水平扩展。前者更简单且易于实现，但受限于机器的潜在最大性能。然而，后者却为提高硬件性能提供了更大的机会。

+   尝试找到一种方法，使您的管道在可配置性方面保持平衡。如果您陷入任一极端（欠配置或过配置），您的管道将要么过于僵化且难以改变，要么对于既定的目标来说过于复杂。

+   不要忽视测试您的管道！它将帮助您在引入更改时避免回归错误，通过早期捕捉缺陷来提高迭代速度，并改善整体设计，因为它将迫使您找到适当的可配置性平衡。
