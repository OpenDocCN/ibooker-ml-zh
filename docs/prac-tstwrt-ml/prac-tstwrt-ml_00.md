# 序言

我们生活在一个机器学习系统（ML）在医学、法律和国防等越来越重要的领域中被使用的世界。模型决策可能导致数百万甚至数十亿美元的经济收益或损失。由于其决策和后果的高风险性质，这些 ML 系统的可信性至关重要。当 ML 系统不安全、可能无法预测地失败、在样本群体中表现差异显著和/或难以解释其决策时，这可能是一个问题。我们撰写这本书是为了帮助你的 ML 模型在现实世界中独立立足。

# 在生产中实施机器学习

如果你正在阅读这本书，你可能已经意识到 ML 的巨大重要性。无论应用领域如何，ML 技术都影响着我们生活的方方面面。Google Brain 的联合创始人 Andrew Ng 在描述 AI 为“新电力”时并没有夸大其词。毕竟，我们手头上的东西最好可以描述为一个通用函数逼近器。就像电力一样，如果处理不当，ML 可能会很危险。就像高压线碰撞到镀铝膜气球一样，ML 失败的案例可能是意外和令人恐惧的。

在现实世界中部署 ML 应用与在封闭环境中工作模型大不相同。学术数据集通常不包含真实世界数据的全部变化。将来我们的模型互动的数据可能与过去的数据不同，特别是如果有人在获取这些数据时走捷径。这可能包含各种可能让模型学习的偏见，从而使部署它的人陷入棘手的伦理和/或法律境地。即使在这些方面一切顺利，你也不是安全的。黑客每年变得更加复杂，最终可能会找出如何通过查询你部署的模型来窃取敏感数据的方法。

尽管前景并非全是悲观与绝望。有许多经过深入研究的最佳实践，用于管理数据集，无论是真实世界的数据还是合成数据。有很多方法可以衡量新进数据与已有数据的差异。正如有办法发现和修复机器学习中的偏见一样，也有新的方法使你的机器学习流水线在一般情况下更易于解释和理解。至于安全性和健壮性，全球一些最大的机器学习公司正在发布工具包，帮助你隐藏敏感模型细节，避免外人窥探。

本书讨论了修复你的 ML 流水线象征性接线的所有方法，从经典解决方案到前沿技术。

# 变压器的收敛

在我们开始写作这本书的不久之前，即在 2010 年末和 2020 年初，一个名为“变压器”的深度学习模型架构已经在自然语言处理（NLP）领域引起轰动。随着时间推移，变压器的采用速度只有加快。这种方法迅速成为计算机视觉、表格数据处理甚至强化学习的标准工具。这与 2010 年代初期的深度学习工作方式有了巨大的不同，当时每个任务和领域都有独特和明确的架构，这使得计算机视觉专家很难完全理解 NLP 研究（同样，NLP 研究人员也很难深入理解计算机视觉方法）。

变压器是一种机器学习架构，最早出现在 2017 年的论文“Attention Is All You Need”中。¹ 在以往的神经网络方法中，如卷积神经网络（CNN）和循环神经网络（RNN），系统首先集中在输入数据的局部区域，然后才逐步扩展到整体。相比之下，使用变压器模型，输入数据的每个元素都与其他每个元素连接（或者关注）。这种方法意味着变压器可以理解其训练的整个数据集。

变压器能够在整个数据集中的数据点之间建立连接，这是其有用性的关键。变压器模型已经成为诸如问答、文本预测和翻译等任务的领先者。最近，这种应用已经扩展到超越自然语言处理的视觉领域，如图像分类。² 变压器在这些领域的普及是最近的现象，但很明显，它将在未来继续发展。

尽管变压器不适用于每一个问题（例如，许多情况下，计算和内存消耗较少的方法效果最佳），但鉴于这一领域的最新趋势，我们把基于变压器的模型作为本书的重点。

# 大规模且高能力的机器学习模型的爆发

变压器已经变得无处不在，并且它们已经被用来将许多人手中的 AI 系统赋予了科幻般的能力，这在仅仅十年前似乎还是不可思议的。2019 年，OpenAI 发布了 GPT-3，这是一个语言模型，能够生成在许多情况下与人类写作的文本难以区分的文本。即使公司们正在围绕这些模型构建他们的产品，³ 我们仍然在发现新的能力。例如，在 2022 年，人们发现可以极大地提升 GPT-3 在像 MultiArith（从 17.7% 提升到 78.7% 准确率）和 GSM8K（从 10.4% 提升到 40.7% 准确率）这样的推理基准上的性能。这种惊人的能力飞跃是如何实现的呢？简单地说，只需在每个答案之前预填写 `"让我们一步一步地思考"` 的提示，就能促使 GPT-3 完成答案。⁴ 但是奇怪的不仅仅在于此，因为这种提示可能导致语言模型输出的推理步骤并不一定能得出答案（你需要进一步的提示和查询才能得到实际的答案）。⁵^,⁶

在我们撰写本书的同时，另一个显著的机器学习模型是 StableDiffusion，这是一个文本到图像的模型，可以根据文本描述生成图像。它是通过与文本到图像模型（如 [OpenAI 的 DALL·E 2](https://oreil.ly/DCZPc)，[Google 的 Imagen](https://oreil.ly/TSU2A)，[Google 的 Parti](https://oreil.ly/ZLAUJ)，以及 [MidJourney](https://oreil.ly/LOIJO)）相同的方式进行训练的，因此其输出的质量大致相似。与其他模型不同的是，该模型的底层代码和完整模型权重都已公开发布。这种能力强大的模型的发布对机器学习安全社区来说是一件大事。它违背了将高能力的机器学习模型保持私密，直到评估其后果和安全性的伦理。在 StableDiffusion 的情况下，作者们发布了多种减少伤害的工具，与发布高能力模型同时进行。⁷^,⁸ 虽然这种最佳实践应受到鼓励，但也突显了许多机器学习安全倡议在资源匮乏方面的问题，即使是对于风险较低的机器学习模型和流水线。

毕竟，我们看到了许多类似的新的图像/语言模型从 Google、DeepMind、OpenAI 和 Microsoft 等竞争公司和团队中涌现出来。由于这些项目是并行构建的，并且有着可比较的结果，新思想的产生并不是瓶颈。在某些情况下，这可能意味着进展不会因为一个团队或组织选择退出而放缓，这会产生反常的激励机制。一个团队可能决定通过不对其文本或图像生成工具施加限制来取得领先优势。虽然大型组织的团队因为这些安全问题而开发产品进展缓慢，但很难阻止这些团队中的工程师加入到想要更快推进产品的初创公司中去。由于这些类似项目是并行开发的，看来保密不再像过去那样提供如此多的保护。

因此，看起来确保安全性被考虑的最有前途的方法之一是让组织尽可能公开他们对安全风险的看法以及他们对这些风险的解决方案的建议。⁹ 正是出于这个原因，我们写了这本书。

# 为什么我们写这本书

作为既进行过 ML 研究又成功部署 ML 系统的人们，我们注意到，初步建立静态数据集的 ML 模型与部署之间的差距很大。这个差距的主要部分在于缺乏可信度。有很多种方式，开发中工作的 ML 模型在生产中可能会失败。许多大公司都有专门的负责 AI 和安全的团队来分析他们当前和潜在未来 ML 系统的潜在风险和后果。¹⁰ 不幸的是，大多数使用 ML 的团队和公司没有足够的带宽来做到这一点。即使存在这样的团队，在这些团队中，它们通常都资源不足，模型开发周期可能过快，以至于安全团队担心竞争对手会先发布类似的模型。

我们写这本书的目的是降低理解如何创建值得信赖的 ML 模型的门槛。尽管已经有很多关于这个主题的标题，但我们希望创建一个对没有机器学习研究背景的人也可以理解的资源，教授框架和思考可信度的方法，以及一些评估和改进模型可信度的方法。

+   可以复制粘贴到你自己项目中的代码块

+   链接列表到开源项目和资源

+   链接到深入的代码教程，其中许多可以在浏览器中探索

尽管经验是无法替代的，但为了积累经验，你需要知道从何处开始。这本书旨在为将你的机器学习应用程序发布到嘈杂、混乱、有时敌对的真实世界中提供所需的基础。这项工作依赖于无数其他研究人员、工程师等的成果，我们希望这项工作能够帮助那些致力于部署 ML 系统的人们翻译部分工作。

# 本书的目标读者

本书适合所有当前使用机器学习模型并希望确保他们的劳动成果在释放到真实世界时不会造成意外伤害的人。

本书的主要受众是具有一定机器学习基础的工程师和数据科学家。书中的部分内容应该对非工程师也是可理解的，比如具有 ML 概念理解的产品经理和高管。你们中的一些人可能正在构建比之前的工作或学术中更重要决策的 ML 系统。我们假设你们对深度学习的基础知识和 Python 的代码示例有所了解。

初次阅读将使工程师对信任度有坚实的理解，并了解它如何适用于你正在使用的 ML 系统。随着你在 ML 职业生涯的继续，你可以回头并从书中调整和适应代码片段，以评估和确保你系统的信任度方面。

# AI 安全与对齐

有一个广泛的研究领域专注于 AI 安全和 AI 对齐问题。*AI 对齐* 是如何制造符合人类意愿且没有意外副作用的 AI 系统的问题。这是*AI 安全* 的一个子集，涉及减轻 AI 系统可能出现的更广泛问题空间。这些问题范围从无法修正地延续社会偏见，到被人类用于战争、欺诈或网络犯罪领域，再到展现出任何文化或从属都不会希望的行为。

AI 对齐被视为解决 AI 安全风险的*解决方案*，因为它涉及让 AI 完全理解并可靠尊重人类的价值观。关于值得信赖的机器学习从理论和/或学术角度来看，有大量的文献。

一个问题是，很多这样的文章试图清晰地定义心理学（例如，*意图*、*欲望*、*目标*和*动机*）和哲学（例如，*价值体系*和*效用*）术语，但是这些定义对于实际负责构建 AI 系统的工程师来说几乎没有用处。也许有一天人类会建造一个真正模拟人脑到神经元和突触水平的 AI 系统，在那种情况下，这些哲学和心理学描述符将会很有用。然而，从作者之一先前作为湿实验室神经科学家的经验来看，现代神经网络与人脑几乎没有什么共同之处。真实的活体神经元不像逻辑门，通常需要大约 1 万个耦合和非线性微分方程来描述它们的行为。模拟单个神经元通常是一个整个专用人工神经网络的任务，而不仅仅是一个单一的权重和偏差。¹¹我们现在还不清楚，是否能够找到一种数学上的方法来证明我们的 AI 系统不会对人类造成伤害。然而，正如 Cohere、OpenAI 和 Al21 Labs 等组织所展示的，¹²仍然有很多事情可以做来预防常见问题并建立最佳实践。

另一个挑战是，很多 AI 安全文献集中在像人工通用智能（AGI）和自我改进的 AI 系统这样的假设未来场景上。¹³^,¹⁴这并不完全脱离现实世界。在撰写本书期间，世界已经见证了像 OpenAI 的 DALL·E 2（可以仅通过文本提示合成高质量图像）和 DeepMind 的 Gato（一个单一的“通用”转换器模型，可以解决语言任务、视觉任务和强化学习任务）这样的 AI 模型的发布。¹⁵针对这些发布，预测市场更新了它们关于类似于 AI 安全文献中预测的通用 AI 代理可能出现的时间的估计，变得比以往更快。¹⁶现在更容易想象涉及与人类价值不一致的强 AI 系统的灾难性场景。

但是，AI 安全并不是某种模糊而不祥的未来需要担心的事情。它是当下需要担心的事情。未对齐的 AI 系统的危险是一个非常现实的威胁，即使是具有较少通用 AI 系统也是如此。威胁包括由 AI 交易机器人驱动的闪电市场崩盘（参见 17），通过在算法中将正号改为负号来改造药物发现 AI 以制造化学武器（参见 18），以及具有 AI 启用的面部识别能力的小型作战无人机可用于种族清洗（参见 19）。然而，关于超智能 AI 单体的可能行为的高层哲学论证，尽管是真实的，但在没有更详细的指导如何诊断和纠正这些问题的情况下将是无用的。

###### 提示

尽管世界是否会以“纸夹子最大化器”风格的事件结束尚不清楚，但这样的极端情景对于记住优化功能如果不小心监控可能会变得灾难性是一个有用的心理工具。

对于更好地评估 AI 安全性声明和研究的心理工具，我们建议阅读 José Luis Ricón Fernández de la Puente 在 Nintil 博客上的[“Set Sail For Fail? On AI Risk”](https://oreil.ly/Li3Xm)。

由于关于这一主题的辩论没有短缺，我们决定通过编制一个充满实用工具和代码片段的资源来帮助采取工程方法的人们。我们不试图制定“信任”的明确定义，而是假定读者在看到时知道“信任”。我们重点列出一些更常见的可能导致某人不信任机器学习系统的实际失败案例，并提供一些工具来帮助您避免这些陷阱。您需要修复赋予人类生活巨大权力的任意或狭隘 AI 系统的实用工具。

修复这类问题的好消息是，通常有解决方案，其可行性远远超过只希望猴子的手掌上不会出现任何负面后果。（参见 20）

# 使用 HuggingFace PyTorch 进行 AI 模型

在本书的代码示例中，我们大量使用 HuggingFace 的 Transformers 库。除了少数例外，我们主要关注这些模型在 PyTorch 中的实现。这个在 Meta 开发的框架（参见 21）根据许多与其他框架（如 TensorFlow 和 JAX）编写的机器学习模型相同的数学原理运行。虽然其他框架的代码示例可能有所不同，但底层原理是相同的。

在写作过程中，HuggingFace 作为分享 AI 模型参数的工具已经越来越受欢迎。这始于语言模型，但已扩展到计算机视觉模型、文本到图像模型、音频模型甚至强化学习模型（参见 22）。

###### 警告

请务必信任您从 HuggingFace 下载模型的作者。有一段时间，HuggingFace 使用 Python pickle 模块下载模型。正如 YouTuber Yannic Kilcher 在他的视频中解释的那样，几乎任何任意可执行代码都可以存储在 pickle 文件中。这可能包括恶意代码，正如 HuggingFace 的 [完全无害模型](https://oreil.ly/ovsWe) 概念所示。

此安全漏洞的修复是 [torch-save 补丁](https://oreil.ly/ib7qu)。自视频发布以来，HuggingFace 已修复了此漏洞，并在网站上添加了有关任意代码执行的警告。请始终仔细检查您信任的模型作者。

# 基础

为了帮助您从本书中获得最大收益，以下是一些基础术语的定义以及进一步信息的链接：

词嵌入

词嵌入是词的向量表示形式，使得一个词被映射到编码其语义含义的向量。一些流行的嵌入包括 GloVe 和 Word2Vec。

语言模型

语言模型是学习在给定上下文中预测令牌概率的模型。它们可以是自回归模型或掩码语言模型。自回归模型将到特定时间步的令牌作为上下文，而掩码语言模型则从正在预测的令牌之前和之后的上下文中获取信息。²³

注意

在各种机器学习模型中，注意力是一种技术，用于衡量在当前步骤的表示的嵌入中考虑每个令牌的程度²⁴。

# 本书中使用的约定

本书中使用了以下排版约定：

*斜体*

指示新术语、URL、电子邮件地址、文件名和文件扩展名。

`常量宽度`

用于程序列表以及在段落内引用程序元素，如变量或函数名称、数据库、数据类型、环境变量、语句和关键字。

**`常量宽度粗体`**

显示用户应直接输入的命令或其他文本。

*`常量宽度斜体`*

显示应由用户提供的值或根据上下文确定的值替换的文本。

###### 提示

此元素表示提示或建议。在第七章和第八章中，它表示练习或提示。

###### 注意

此元素表示一般注释。

###### 警告

此元素指示警告或注意事项。

# 使用代码示例

可以在 [*https://github.com/matthew-mcateer/practicing_trustworthy_machine_learning*](https://github.com/matthew-mcateer/practicing_trustworthy_machine_learning) 下载补充材料（代码示例、练习等）。

如果您有技术问题或在使用示例代码时遇到问题，请发送电子邮件至*bookquestions@oreilly.com*。

本书旨在帮助您完成工作。一般来说，如果本书提供示例代码，您可以在您的程序和文档中使用它。除非您重复使用大部分代码，否则无需获得我们的许可。例如，编写一个使用本书多个代码片段的程序不需要许可。出售或分发 O’Reilly 书籍中的示例代码需要许可。引用本书并引用示例代码回答问题不需要许可。将本书大量示例代码整合到您产品的文档中需要许可。

我们赞赏，但通常不需要署名。署名通常包括标题、作者、出版商和 ISBN。例如：“*《可信机器学习实践》*，作者 Yada Pruksachatkun、Matthew McAteer 和 Subhabrata Majumdar（O’Reilly）。版权所有 2023 Yada Pruksachatkun、Matthew McAteer 和 Subhabrata Majumdar，978-1-098-12027-6。”

如果您认为您使用的示例代码超出了公平使用或上述许可，请随时与我们联系，电子邮件至*permissions@oreilly.com*。

# O’Reilly 在线学习

###### 注意

40 多年来，[*O’Reilly Media*](https://oreilly.com)已为公司提供技术和商业培训、知识和见解，帮助其取得成功。

我们独特的专家和创新者网络通过书籍、文章和我们的在线学习平台分享他们的知识和专长。O’Reilly 的在线学习平台让您随需应变地访问现场培训课程、深入学习路径、交互式编码环境，以及来自 O’Reilly 和其他 200 多家出版商的大量文本和视频。欲了解更多信息，请访问[*https://oreilly.com*](https://oreilly.com)。

# 如何联系我们

请将有关本书的评论和问题寄给出版商：

+   O’Reilly Media, Inc.

+   1005 Gravenstein Highway North

+   加利福尼亚州塞巴斯托波尔市 95472

+   800-998-9938（美国或加拿大）

+   707-829-0515（国际或本地）

+   707-829-0104（传真）

由于信任主题的复杂性，尽管我们试图包含该领域的一些主要主题，但创建一个完全全面的资源是不可能的。另外，由于本书的受众，我们采取了比学术界更简单、更对话式的教学方法。

如果您在本书中发现事实错误（或特别明显的遗漏），请告知我们。我们不仅会乐意纠正错误，还会在下一版的致谢中首先提到报告任何特定错误的人。

我们有这本书的网页，列出勘误、示例和任何额外信息。您可以访问这个页面：[*https://oreil.ly/ptml*](https://oreil.ly/ptml)。

发送电子邮件至*bookquestions@oreilly.com*，以评论或提出关于本书的技术问题。

关于我们的书籍和课程的新闻和信息，请访问[*https://oreilly.com*](https://oreilly.com)。

在 LinkedIn 上找到我们：[*https://linkedin.com/company/oreilly-media*](https://linkedin.com/company/oreilly-media)。

关注我们的 Twitter：[*https://twitter.com/oreillymedia*](https://twitter.com/oreillymedia)。

观看我们的 YouTube 频道：[*https://youtube.com/oreillymedia*](https://youtube.com/oreillymedia)。

# 致谢

我们要感谢 Divesh Shrivastava、Kush Varshney、Jiahao Chen、Vinay Prabhu、Josh Albrecht、Kanjun Qiu、Chelsea Sierra Voss、Jwala Dhamala、Trista Cao、Andrew Trask、Yonah Borns-Weil、Alexander Ziller、Antonio Lopardo、Benjamin Szymkow、Bobby Wagner、Emma Bluemke、Jean-Mickael Nounahon、Jonathan Passerat-Palmbach、Kritika Prakash、Nick Rose、Théo Ryffel、Zarreen Naowal Reza 和 Georgios Kaissis，因为他们审阅了我们的章节。如果您也有兴趣进行正式审阅，请告知我们！

¹ 阿希什·瓦斯瓦尼等人，[“注意力机制是你所需要的一切”](https://oreil.ly/ASQqB)，*NeurIPS 会议论文* (2017).

² 韩凯等人，[“视觉 Transformer 综述”](https://arxiv.org/abs/2012.12556)，*IEEE 模式识别与机器智能期刊* (2022)。

³ 马修·麦克阿提尔的博客提供了[公司基于 GPT-3 构建的例子](https://oreil.ly/OY8lh)。

⁴ 小岛武志等人，[“大型语言模型是零-shot 推理者”](https://arxiv.org/abs/2205.11916)，*arXiv 预印本* (2022).

⁵ 参见 DeepMind 附属的安东尼娅·克雷斯威尔等人，讨论使用提示进行可解释的组合推理：[“选择推理：利用大型语言模型进行可解释逻辑推理”](https://arxiv.org/abs/2205.09712)，*arXiv 预印本* (2022).

⁶ 这甚至还没有涉及到在互联网上可访问的书籍中讨论提示工程的可能后果，因此这些书籍可能会成为未来大型语言模型（如 GPT-3 的后继者）的训练数据的一部分。

⁷ 参见 Stability.ai 关于他们的 Deep Fake 检测倡议的[推特公告](https://oreil.ly/quHFV)，使用了新的 OpenCLIP 模型等技术。

⁸ 除了类似 StableDiffusion 的文本到图像模型外，其他组织也在采用类似的方法发布大型模型。Meta AI 发布了与 GPT-3 规模相当的 1750 亿参数的[开源预训练 Transformer](https://oreil.ly/aqf6R)。

⁹ 这还有一个额外的效果，让潜在的叛徒知道他们正在叛逆，增加了实施不安全 AI 系统的声誉成本，同时减少了减少 AI 风险的成本。

¹⁰ 例如，2021 年 DeepMind 的伦理团队发表了名为《语言模型的伦理和社会风险》的论文，而 OpenAI 在 2022 年 3 月更新了他们关于 AI 安全的[立场](https://oreil.ly/89Jqu)。

¹¹ Allison Whitten，《一个神经元的计算复杂性有多高？》，《Quanta Magazine》，2021 年 9 月 2 日。这篇文章总结了 David Beniaguev 等人的论文《单个皮层神经元是深度人工神经网络》（2021），[链接](https://oreil.ly/OSMJP)。

¹² Cohere 团队，《部署语言模型的最佳实践》，《co:here》，2022 年 6 月 2 日，[链接](https://oreil.ly/3rFni)。

¹³ 尼克·博斯特罗姆的《超智能》描绘了这样一种情景：这样的系统可能来自任何一个各种 AI 研究实验室，然后超出人类控制的能力，[链接](https://oreil.ly/7OAxP)。

¹⁴ 受欢迎的网络散文家 Gwern 撰写了《看起来你正试图掌控世界》的[短篇小说](https://oreil.ly/hzDHT)，旨在帮助读者想象 AI 研究在当前技术水平下可能引发灾难的情景。

¹⁵ 参见 DeepMind 的项目页面，了解他们称之为《通用代理程序》的[示范和实例](https://oreil.ly/bczvt)。

¹⁶ 查看 Metaculus 预测市场关于与 AI 相关问题的[预测随时间变化的图表](https://oreil.ly/k7kWZ)。

¹⁷ David Pogue，《算法交易导致闪崩》，《Yahoo! Finance》，2018 年 2 月 6 日，[链接](https://oreil.ly/zEyj0)。

¹⁸ Justine Calma，《AI 在短短六小时内建议了 40,000 种新的可能化学武器》，《The Verge》，2022 年 3 月 17 日，[链接](https://oreil.ly/mJLpv)。

¹⁹ Stuart Russell 等，《为什么你应该担心 *Slaughterbots*—一个回应》，《IEEE Spectrum》，2018 年 1 月 23 日，[链接](https://oreil.ly/4uwnu)。

²⁰ 参见此解释，关于故意用 38 英里的光纤电缆减慢交易速度的股票交易所：Tom Scott，《[如何减缓股票交易所](https://youtu.be/d8BcCLLX4N4)》，视频，2019 年 2 月 4 日。

²¹ Meta 宣布 PyTorch 将移交给独立的 PyTorch 基金会（由 Linux 基金会孵化）：《[宣布 PyTorch 基金会：AI 框架的新时代](https://oreil.ly/0t10a)》，2022 年 9 月 12 日。

²² 欲了解更多信息，请参阅 Stable-Baselines3，最受欢迎的深度强化学习库，与 HuggingFace Hub 集成的公告：《[欢迎 Stable-baselines3 加入 Hugging Face Hub](https://oreil.ly/eigfk)》，2022 年 1 月 21 日。

²³ 欲了解更多关于各种类型的 NLP 语言模型，详见 Devyanshu Shukla，《[自然语言处理中的语言模型简介](https://oreil.ly/KNmLp)》，*Medium*（博客），2020 年 3 月 16 日。

²⁴ Lilian Weng，《[关注？注意！](https://oreil.ly/ZB0n5)》，*Lil’Log*（博客），2018 年 6 月 24 日。
