# 第二章：人：人类在环路中

> “人们担心计算机会变得太聪明并接管世界，但真正的问题是它们太愚蠢，它们已经接管了世界。”
> 
> 佩德罗·多明戈斯

自其成立以来，人们越来越倾向于赋予人工智能和机器学习越来越多的代理权。然而，这不应该是当今部署机器学习的组织的目标。由于我们正在看到的所有人工智能事件，我们坚信这项技术还不够成熟。相反，目标应该是确保人类参与到基于机器学习的决策中去。人类的参与至关重要，因为正如上面的引用所强调的那样，一个非常普遍的错误是企业认为他们的负责任的机器学习职责仅仅在于技术实施。本章节介绍了企业在构建机器学习基础设施时必须考虑的许多人类因素。我们将从组织文化开始，然后转向讨论从业者和消费者如何更多地参与机器学习系统的内部运作。章节最后强调了一些与负责任地实践机器学习相关的员工活动和数据新闻的最新例子。

# 负责任的机器学习文化

一个组织的机器学习文化是负责任机器学习的一个重要方面。本节将讨论责任、自我使用、有效挑战以及人口和职业多样性的文化概念。我们还将讨论那句看似陈词滥调的话：“快速前进并破坏事物”。

## 责任

成功缓解机器学习风险的关键是真正的责任制。问问自己：“谁追踪我组织中机器学习的开发和使用方式？谁负责审计我们的机器学习系统？我们有人工智能事件响应计划吗？”对于今天的许多组织来说，答案可能是“没有人”，“没有”。如果在一个机器学习系统失败或遭到攻击时没有人的工作受到威胁，那么可能该组织中真正没有人关心机器学习的风险。这是许多领先金融机构现在雇佣首席模型风险官的主要原因。较小的组织可能无法抽出一个全职员工来监控机器学习模型风险。但是，有一个负责任和被追责的个人或团队仍然至关重要，如果机器学习系统表现不佳。根据我们的经验，如果一个组织假设每个人都对机器学习风险和人工智能事件负责，那么现实情况可能是没有人真正负责。

## 自我使用

*Dogfooding* 是软件工程的一个术语，指的是一个组织使用自己的软件，即“吃自己的狗食”。在负责任的 ML 的背景下，dogfooding 带来了额外的 Alpha 或 Prealpha 测试层次，通常在从认为是 ML 黄金热潮中获利的疯狂冲刺中被忽视。更重要的是，dogfooding 可以将法律和风险问题置于前沿。如果一个组织开发了一个操作方式违反他们自己的隐私政策、或者旨在欺骗或操纵的 ML 系统，那么参与 dogfooding 的员工可能会觉得这是不可接受的，并提出关注。Dogfooding 可以将黄金法则引入 ML 中：如果你不会在自己身上使用一个 ML 系统，那么你可能不应该在其他人身上使用它。我们将在下一节讨论多样性，但在这里值得一提的是，如果您的团队更加多样化，dogfooding 更有可能检测到更广泛的可挑剔（或有问题的）特性。

## 人口统计和专业多样性

许多人已经[记录了因 ML 工程师在 ML 系统的培训或结果中未考虑人口统计多样性而可能产生的不幸结果](https://oreil.ly/SA0ow)。解决这些疏忽的一个潜在解决方案是增加 ML 团队的人口统计多样性，而其[当前令人遗憾的水平](https://oreil.ly/xT8nF)。另一种也可以帮助缓解 ML 风险的多样性是专业经验的多样性。根据威斯康星大学的卡尔·布罗曼教授所说，“如果你在分析数据，[你正在做统计学](https://oreil.ly/-qoYp)” 。ML 在很大程度上是一个数据分析领域，因此它是一个统计学的学科。尽管 Kaggle 的排行榜青睐于单一模型的单一结果，但 ML 系统通常受益于其他数据分析学科（如统计学、计量经济学或心理测量学）的视角。这些领域有丰富的学习历史，可以在几乎任何 ML 项目上发挥作用。安全人员是 ML 项目的另一个有用的技术补充，因为 ML 可能涉及数据隐私和安全问题。

发展具有深度跨学科专业经验团队，在部署机器学习时非常宝贵。许多最成功的量化投资和咨询公司，如麦肯锡或文艺复兴技术公司，以他们如何组建来自物理学、生物学、医学、天文学等领域极其多样化技术背景的精英团队为傲。法律、合规和审计人员在机器学习项目中也可能是必要的。机器学习项目可能会触犯法律、法规或企业隐私政策。从一开始就涉及监督专业人员是评估和减轻这些风险的好方法。机器学习还可能挑战伦理底线，而很少有机器学习工程师具备管理项目穿越不明确伦理道德领域所需的教育或经验。将专业伦理学家引入机器学习系统的开发中，有助于在问题出现时管理道德问题。

## 文化有效挑战

有效挑战的概念源于模型治理的实践。在构建复杂的机器学习系统时，有效挑战大致表明，确保良好结果的最佳途径之一是积极挑战和质疑机器学习开发过程中的步骤。关于有效挑战还有更多技术方面的内容，将在第四章中详细讨论。但是，鼓励严肃质疑机器学习设计选择的文化，更有可能在问题扩大成人工智能事件之前发现问题。当然，文化有效挑战不能滥用，并且必须适用于所有开发机器学习系统的人员，即使是所谓的“摇滚明星”工程师和数据科学家。根据我们的经验，文化有效挑战实践应该是有结构的，例如每周会议上会质疑和讨论替代的设计和实施选择。

## 快速前进与破坏

“快速前进并破坏”这句话深深印在许多顶级工程师和数据科学家的心中。不幸的是，当你快速前进并破坏时，事情往往会出问题。如果你在娱乐应用和其广告的领域工作，这可能并不是什么大问题。但假设你在医学、人力资源、信贷借贷、刑事司法、军事或其他高风险应用领域使用机器学习，快速前进并破坏可能会违法或毁了人们的生活。从业者必须意识到其工作的后果和下游风险，而不是为了过时的格言而赛跑追求结果。

传统的模型治理实践提供了防范这些故障的选项，如严格的验证和监控。然而，这些做法需要大量的人力、时间和技术资源。对于处于商业压力下需要快速行动的年轻或较小的组织来说，标准的模型治理可能不可行。常识表明，当迅速行动和打破事物，以及没有传统模型治理时，人工智能事件更有可能发生。因此，如果您需要快速行动和打破事物，AI 事件响应计划对您的组织至关重要。通过 AI 事件响应，正如第三章所讨论的那样，没有资源严格监督机器学习项目的较小组织可以将有限资源用于可以使其快速行动，但也可以面对人工智能事件的方式。从长远来看，为复杂系统故障做好准备可能最终是最快的发展策略。

# 进入循环

现在我们已经涉及了一些关于负责任的机器学习的文化方面，本节将描述从业者或管理者可以采取的具体步骤，以便更好地控制机器学习系统。截至今天，人类在成功实施机器学习系统中仍然扮演着重要角色。正如本章开头的引用所强调的那样，许多决策者和从业者可能对当今的机器学习系统过于信任。除了有效挑战机器学习设计选择之外，人类对机器学习系统的详细审查是另一种可行的风险缓解策略。模型治理的清单和文档是模型治理的基础，许多最近的人工智能和机器学习最佳实践指南强调了对机器学习系统进行人工审计的必要性。当然，所有这些都需要深入了解数据和问题领域的人员，并且需要构建机器学习系统以便与这些领域专家进行交互。没有领域专业知识，机器学习系统可能会被训练在不正确的数据上，结果可能会被误解，审计也会变得不那么有意义，数据或编程错误可能会演变成全面的人工智能事件。机器学习系统通常也应设计为允许用户提供有意义的反馈，特别是对基于机器学习的决策进行申诉和覆盖，并在必要时切断开关！

## 机器学习系统的人工审计

随着技术的进步，使得更多幕后透明性、更好的区分和安全性测试机器学习系统成为可能，现在可以促进人类对机器学习的理解和信任。这些技术将在第四章中进行讨论，但这些技术仍然需要人们来部署。这些新技术最好的用途之一是对机器学习系统进行人工审计。在一篇[最近的论文](https://oreil.ly/2znTg)中，谷歌的研究人员提出了机器学习模型审计的框架。他们还提出了关于[模型](https://oreil.ly/Fqgug)和[数据](https://oreil.ly/pzVBW)的基本样本文档。这些发展是在多年的金融服务垂直模型治理的基础上进行的，其中治理、有效挑战、模型清单、模型文档、模型验证以及多个技术和决策者审查级别已经成为高风险预测模型应用的规范。

您和您的组织可以做什么来促进机器学习系统的人工审计？基本的做法相对简单：

+   创建机器学习系统清单

+   指定负责的执行官

+   进行机器学习系统的行政和技术审查

+   部署机器学习系统之前要求技术和行政签字

+   仔细记录、验证和监控所有机器学习系统

当你准备好超越这些基本步骤时，请查看谷歌研究的参考论文，并查看公共模型风险管理论坛的资源，例如[北美首席风险官理事会](https://oreil.ly/myISS)。

## 领域专业知识

许多人通过**Pandora 推荐算法**或类似的内容首次接触到人类专家参与的概念，这最终演变成了一个价值数十亿美元的专家标注和机器学习系统决策审查的产业。更一般地说，机器学习在现实世界中的成功几乎总是需要来自深刻理解问题领域的人类的参与。当然，这些专家可以帮助进行特征选择和工程，以及解释机器学习的结果。但专家们还可以作为一种理智检查的机制。例如，如果你正在开发医学机器学习系统，你应该咨询医生和其他医疗专业人员。普通的数据科学家们如何理解医学数据的微妙和复杂性，以及这些数据训练系统的结果？他们可能做不到，这会导致系统部署时的人工智能事件。在这方面，社会科学也值得特别关注。一些组织被描述为[“科技对社会科学的悄然殖民”](https://oreil.ly/ImONd)，它们正在进行一些不明智的机器学习项目，这些项目要么[取代了专业社会科学家会做出的决策](https://oreil.ly/8SEp5)，要么使用了被实际社会科学家谴责的做法，比如[用于犯罪风险评估的面部识别技术](https://oreil.ly/KYOb5)。

## 用户与机器学习的互动

由于机器学习始终是通过软件进行的，因此您组织中的人们可能会通过软件与机器学习结果和成果进行交互。为了最大限度地发挥影响，非技术人员和决策者用户需要理解并采取行动机器学习系统的结果。不幸的是，许多机器学习系统和软件包只生成由高度技术的研究人员和从业者设计的数值输出或视觉效果。在最好的情况下，这限制了组织内能够使用人工智能和机器学习技术的人数。在最坏的情况下，人们可能会误解设计不良的输出，导致流程失败、客户不满意，甚至人工智能事件的发生。在构建机器学习系统时，考虑到将需要与系统交互的不同用户和角色是明智的选择。您的组织可能还应该有合格的用户互动专业人员，以帮助为这些不同的机器学习系统用户构建清晰易懂和有用的界面。

## 用户吸引力和操作者覆盖

如果一台计算机[错误地让你呆在监狱里](https://oreil.ly/ZnvS9)怎么办？如果一台计算机[错误地指控你犯罪](https://oreil.ly/7xhyz)怎么办？如果一台计算机[让你或亲人错失了梦想中的大学](https://oreil.ly/lElj1)怎么办？您可能希望知道为什么，您可能希望有权利申诉这些决策。从机器学习系统操作者的角度来看，操作者甚至可能希望告诉您决策的制定过程。（在某些情况下，操作者可能有法律义务提供这些信息。）或者操作者可能希望拥有覆盖个别机器学习系统决策的能力。如果所涉及的机器学习系统基于黑箱算法，其操作者可能无法告诉您该决策是如何做出的，他们可能无法及时检查或覆盖这些决策。考虑到“所有模型都是错误的”，至少在某个时刻，所有这些似乎都是灾难的根源。¹在最坏的情况下，黑箱机器学习系统将（不可避免地）发布错误预测，也许速度很快会有很多错误决策。这些错误决策会伤害消费者或公众，而机器学习系统操作者将面临声誉上，如果不是法规上的损害。

这个主题在数据隐私圈中也被称为[“干预性”](https://oreil.ly/VjIS7)，目前已经被相当了解。因此，您可以采取措施防止您组织的机器学习系统做出不可申诉，可能违法的黑箱决策：

+   使用可解释的机器学习模型或可靠的事后解释技术（最好两者兼有）

+   正确记录这些系统中使用的过程

+   在部署前，对机器学习系统解释能力功能进行细致测试

申诉、覆盖和干预能力的基础问题是透明度。因此，您的组织应了解机器学习决策的制定过程，使操作者能够逻辑地覆盖——消费者能够申诉——机器学习系统的决策。理想情况下，机器学习系统应该为消费者和公众，特别是受到机器学习系统影响的人群，提供整体透明度。这甚至可以涉及用户探测这些系统，提取个体决策背后的推理，并在必要时否定决策。这些类型的申诉和覆盖机制还可以阻止不可避免的机器学习系统错误演变为严重的人工智能事件。

## 断开开关

一篇最近的《福布斯》文章标题问道，[“人工智能会有关机开关吗？”](https://oreil.ly/858_v) 如果你的组织希望减少机器学习和人工智能的风险，我们希望你的机器学习系统的答案是，“是的”。机器学习系统可以非常快速地做出决策——比人类快得多。因此，如果你的机器学习系统出现严重问题，你将希望能够快速关闭它。但是你又如何知道你的机器学习系统是否在出现问题？机器学习系统应该监控多种问题，包括不准确性、不稳定性、歧视、私人数据泄露和安全漏洞。

一旦发现了严重问题，问题就变成了，你能关闭机器学习系统吗？机器学习系统的输出通常会进入到下游的业务流程中，有时还包括其他机器学习系统。这些系统和业务流程可能具有使命关键性，例如用于信用核准或电子零售产品推荐的机器学习系统。要关闭一个机器学习系统，你不仅需要正确的技术知识和人员，还需要了解该模型在更广泛组织流程中的位置。在进行中的人工智能事故中，不是开始考虑关闭一个致命缺陷的机器学习系统的好时机。因此，杀死进程和杀死开关是你的机器学习系统文档和人工智能事故响应计划的重要补充（参见第三章）。这样，当关闭一个机器学习系统的时机到来时，你的组织可以准备好做出明智的决定。

# 走向核心：公众抗议、数据新闻和白帽黑客攻击

有时候，仅仅在组织文化的限制内工作或融入到机器学习系统的循环中是不够的。有时，组织在技术上的不负责任会导致员工、记者、研究人员或其他人感到有必要采取极端行动。本章剩余部分将讨论一些最近和相关的示例，包括罢工、抗议、调查数据新闻和白帽黑客攻击。

近年来，技术巨头的员工举行抗议活动，表达对公司关于[虚假信息](https://oreil.ly/3aCWH)，[气候变化](https://oreil.ly/PusPE)，[性骚扰](https://oreil.ly/aRPdB)及其他重大问题政策的不满。作为大多数科技公司最宝贵的资产，高技能员工的抗议活动似乎确实引起了公司的重视，无论是通过回应抗议者的要求还是对抗议者采取报复行动。近年来，出现了一种令人兴奋的新型机器学习监督方式；最能描述它的可能是极端数据新闻和白帽黑客的混合体。这些行动的催化剂似乎是 2016 年 ProPublica 对称为 COMPAS 的犯罪风险评估工具进行分析。在实质上是一场模型提取攻击中，ProPublica 的记者制作了一个粗略副本的 COMPAS 专有训练数据和黑盒逻辑，并利用这一分析对[算法犯罪风险评估中的歧视问题](https://oreil.ly/C99fu)提出了严重指控。尽管分析结果在科学上存在争议，但这项工作引起了广泛关注，使算法歧视问题备受关注，而许可了 COMPAS 的公司后来更名。

在商业技术的外部监督的另一个例子中，麻省理工学院的研究人员在[性别偏差](https://oreil.ly/eEVkJ)项目下进行了多个商业面部识别工具的种族和性别歧视测试。该研究结果于 2018 年公开，并且非常严重。当天的一些领先面部识别技术在白人男性上表现良好，但在有色人种女性上表现非常糟糕。一旦性别偏差的结果公开，公司们不得不要么纠正他们明显的歧视系统，要么为其辩护。尽管大多数公司选择迅速解决问题，[亚马逊选择为其 Rekognition 系统辩护](https://oreil.ly/jWW8l)，并继续向执法部门授权使用。面对不断增长的公众抗议声，[亚马逊](https://oreil.ly/OEsIu)和[IBM](https://oreil.ly/aUPXl)——在性别偏差研究中均有提及——在 2020 年夏季取消了他们的监控面部识别计划。这种揭露机器学习问题的公开尝试继续进行，且可能越来越频繁。2020 年 1 月，沃尔玛的员工向记者爆料称，他们认为[防盗机器学习系统](https://oreil.ly/h-emE)存在错误，并在 COVID-19 大流行期间不必要地增加了顾客和员工的接触频率。2020 年 2 月，专注于监督大科技公司的非营利新闻组织 The Markup 在一篇类似于原始 COMPAS 揭露的分析中，声称[全球保险](https://oreil.ly/IJzRT)利用算法向其最高付费客户收取更高的费率，实质上创建了一个“受骗者清单”。

幸运的是，正如从原始 COMPAS 新闻报道和性别阴影项目的影响可以看出的那样，公众和政府对机器学习风险的日益关注似乎至少在某种程度上影响了组织对 ML 的使用。组织们逐渐意识到，至少在某些情况下，这些类型的事件可能会损害品牌声誉，即使不会影响到财务状况。当我们将讨论重点转移到第三章中的流程和第四章中的技术时，请记住，截至今日，人仍然是几乎任何 ML 或 AI 技术部署中的关键因素。无论是通过培养责任文化，更深入地参与 ML 系统的内部运作，还是采取更激进的行动，你确实可以改变 ML 系统对我们世界的影响。

¹ 著名统计学家乔治·博克斯被认为说过：“[所有模型都是错的，但有些是有用的](https://oreil.ly/0cJgu)”。
