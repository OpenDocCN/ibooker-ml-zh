# 第十二章：如何在高风险机器学习中取得成功

虽然人工智能和机器学习已经研究了几十年，并且在某些领域几乎同样长时间使用，但我们在广泛经济中采用机器学习的初期阶段。机器学习往往是不成熟的，有时是高风险的技术。机器学习令人兴奋并且充满了希望，但它不是魔法，从事机器学习的人也没有神奇的超能力。我们和我们的机器学习技术都有可能失败。如果我们希望成功，我们需要积极应对系统风险。

整本书提出了技术风险缓解措施和一些治理方法。这一最后一章旨在为您留下一些常识性建议，这些建议应能使您能够在机器学习中处理更困难的问题。然而，我们的建议可能并不容易实现。解决难题几乎总是需要艰苦的工作。用机器学习解决难题也不例外。在高风险技术项目中如何取得成功？通常不是通过快速行动和破坏事物。虽然快速行动和破坏事物在有缺陷的社交应用和简单游戏中可能表现得足够好，但这不是我们登上月球、安全飞行在全球各地的喷气机、推动我们的经济或制造微处理器的方式。高风险的机器学习与其他每个学科一样，都需要对安全和质量做出严肃的承诺。

如果我们处于机器学习采纳的早期阶段，那么我们正处于机器学习风险管理的黎明时期。直到 2022 年，国家标准与技术研究所（NIST）才发布了其首个 AI 风险管理框架的草案。遵循这一指导及其他指南，并与我们的实际经验和本书内容保持一致，我们认为在高风险的机器学习环境中取得成功的一种重要方式是对机器学习系统和数据科学家应用治理，并建立透明、经过测试的、公平和安全的技术。然而，除了这些过程和技术目标外，我们还想分享一些超出这些目标的建议和教训。在本章中，我们提出了对多样性、公平性、包容性、可访问性、科学方法、评估已发表声明、外部标准以及其他几个常识性指南的额外考虑，以帮助我们全面管理风险，并提高我们在重要机器学习项目中成功的机会。

###### 注意

要在高风险的机器学习应用中成功，不仅需要技术能力和工具，还需要以下内容：

+   拥有多元化视角的团队

+   理解何时以及如何应用科学实验与软件工程方法

+   评估已发表结果和声明的能力

+   应用权威的外部标准的能力

+   常识

这一章讨论了多年来我们学到的关键社会技术教训，以便读者能够在下一个重要项目中以超越治理、代码和数学的专业知识快速起步。

# 谁在房间里？

从 ML 项目的最早阶段开始，即关于项目会议的会议，或者组织开始讨论 ML 采纳时，多样化的人员参与是一项基本的风险控制措施。要理解其中原因，可以考虑 Twitter 之前的 ML 伦理、透明度和问责（META）团队曾显示出平台的几个特征可能存在偏见，这至少部分是由于参与系统开发的人员类型所致。尽管这并非 ML 问题，但 Twitter 最初的 140 字符限制被视为激励英语使用者进行简洁对话，但正如在[“给您更多表达空间”](https://oreil.ly/pRNEZ)中讨论的那样，字符限制对平台的某些用户确实存在问题。这些问题最初设计者多为英语使用者并未立即显现。至于 ML，最近 Twitter 针对现已停用的图像裁剪器提出的 META 偏见悬赏显示出对少数在 ML 工程团队中很少得到充分代表的人群存在偏见，例如[非拉丁脚本用户（例如阿拉伯语）的偏见，对白发人群的偏见](https://oreil.ly/MEXn9)，以及对[戴宗教头饰者的偏见](https://oreil.ly/yhNGv)。只有通过全球用户社区的参与，Twitter 才发现了这些具体问题。

###### 警告

多样性、公平性、包容性和可访问性是严肃的伦理、法律、商业和 ML 表现考量。看看周围的房间（或视频通话）。如果每个人看起来都一样或具有相同的技术背景，那么我们可能存在增加风险的重大盲点，并且可能会错过可以改善我们模型的重要视角。请参考[NIST SP1270](https://oreil.ly/OAw2q)获取有关在 ML 中增加多样性、公平性、包容性和可访问性的额外想法、资源和缓解措施。

最近的研究题为[“偏见程序员？还是偏见数据？操作化 AI 伦理的实地实验”](https://oreil.ly/bl7xW)可能揭示了在编码 ML 模型时这些偏见是如何产生的。在这项研究中，ML 模型的预测错误与开发者的人口统计学特征相关。不同类型的人倾向于有不同的盲点。参与 ML 项目的人种类越多，他们的盲点重叠就越少，整个团队的视角也更广泛。从我们 ML 努力的最开始，拥有职业和人口统计多样性非常关键，因为规划或治理的盲点会像糟糕的模型或差劲的测试一样毁掉 ML 系统。此外，人们普遍理解的是，[多样性可以推动财务表现](https://oreil.ly/xTeoX)。多样性不仅仅是风险管理，也关乎更好的业务表现。

学者和从业者已经在思考如何改善 ML 领域的多样性，许多人确信[雇佣更多多样化的团队可以建立更少偏见的 AI](https://oreil.ly/gx8Rp)，或者至少在问诸如[“人本 AI 如何对抗机器和人类的偏见？”](https://oreil.ly/7YC_J)等重要问题。然而，我们必须诚实地承认，今天在 ML 领域中缺乏多样性。根据 AI Now 报告“歧视性系统：AI 中的性别、种族和权力”，80%的 AI 教授是男性，“女性在 Facebook 的 AI 研究人员中占 15%，而在 Google 仅占 10%”，而仅“2.5%的 Google 员工是黑人，而 Facebook 和微软的比例分别为 4%”。尽管这可能需要延长时间表，与同事和利益相关者进行教育和学习，更多的会议和电子邮件交流，以及可能对我们自身偏见和盲点的艰难认知，但在组织的 ML 旅程的起点上，拥有一个职业和人口统计多样化的团队常常会导致更好的 ML 系统表现和更少的总体风险。

如果我们发现自己所在的团队过于同质化，我们必须与我们的管理者沟通，并参与面试过程，帮助实现更好的多样性和包容性。我们可以询问关于对模型进行外部审计的可能性，或者获取其他类型的外部专业知识，这些知识可以提供多元化的观点。此外，可以参考 NIST SP1270 提供的权威建议，该建议由许多领先的专家审阅，旨在增强 ML 领域的多样性、公平性、包容性和可访问性。

下次我们启动项目时，我们可以更好地包括具有更多不同人口统计背景的从业者，还有法律或监督人员，传统的统计学家和经济学家，用户体验研究人员，客户或利益相关者的声音，以及其他可以扩展我们对系统及其结果看法的人。而且，如果我们真的对从某个特定 ML 项目中出现的偏见或其他危害感到担忧，我们可以考虑联系我们内部的法律团队——特别是产品或数据隐私顾问——或者进行举报，如果我们组织内存在相关的保护措施的话。

# 科学与工程的对立

部署高风险的 ML 系统更像是一项科学实验，而不是机械的工程任务。尽管我们听到软件、硬件、容器化和监控解决方案如何帮助我们实现 ML 的运营，但 ML 系统并不保证可运行。毕竟，我们并不像建造一张桌子或甚至一辆汽车那样，可以假设只要按照一套说明就能工作。在 ML 中，我们可以按照这本书或其他任何被认为权威的指导所告诉我们的做，但系统仍然可能因为许多原因失败。至少其中一个原因是，构建 ML 系统往往涉及许多我们经常视为假设的假设，其中主要假设是我们能够在现实世界中实现系统预期效果。

###### 警告

AI 和 ML 的大部分仍然是一个不断发展的社会技术科学，尚未准备好仅使用软件工程技术进行直接产品化。

一般来说，作为数据科学家，我们似乎忘记了需要仔细应用科学方法，以便在高风险部署中取得成功的良好机会，因为我们经常进行隐式实验。我们倾向于认为，如果我们能把工程做对，项目就会成功。这样做过于信任从观察数据中检测到的相关性，通常也过于信任训练数据本身，后者通常存在偏见且经常不准确。坦率地说，我们通常很难产生可重现的结果。而且，当我们确实提出更正式的假设时，通常是关于使用哪种算法。然而，对于高风险的 ML 系统，我们应该对系统预期的实际世界结果提出正式假设。

## 数据科学方法

我们在数据科学工作流程中多次看到了这些基本反模式，甚至在我们自己的工作中也是如此，我们给它起了个名字：[数据科学方法](https://oreil.ly/22Zmt)。许多 ML 项目的成功似乎是建立在使用“正确”的技术基础上的，更糟糕的是，如果我们这样做，那么我们就不会失败。我们向看过数据科学方法的大多数同事展示后，他们都同意这听起来太过熟悉了。

阅读以下步骤，思考一下您参与过的数据科学团队和项目。以下是数据科学方法的运作方式：

1.  假设我们会赚数百万美元。

1.  安装 GPU，下载 Python。

1.  从互联网或某些业务流程的排放中收集不准确、有偏见的数据。

1.  屈服于确认偏见：

    1.  研究收集的数据形成假设（即使用哪些`X`、`y`和 ML 算法）。

    1.  使用基本上相同的数据从假设生成到测试我们的假设。

    1.  使用能够很好地拟合几乎任何一组松散相关的`X`和`y`的高容量学习算法来测试我们的假设。

    1.  不断修改我们的假设，直到结果“好”为止。

1.  不用担心复制；我们都很好，兄弟。

数据科学方法无法导致我们的系统展示其预期的现实世界目的，除非碰巧。换句话说，数据科学方法无法提供证据支持或反驳关于我们系统体内结果的正式假设。尽管听起来很疯狂，但如果我们想要在高风险部署中系统地增加成功的机会，我们必须改变我们的整体机器学习方法。我们不能假设我们会成功（或发财）。事实上，如果我们想成功，我们可能应该更加对抗，假设成功非常困难，我们现在所做的任何事情都不会奏效。我们应该不断地寻找我们方法和实验设置中的漏洞。

尽管选择正确的工具对成功至关重要，正确掌握基本科学更为重要——主要因为通常并不存在“正确”的技术。回想起 C++的发明者**比雅尼·斯特劳斯特鲁普**[经常说](https://oreil.ly/J9uWR)：“声称自己拥有完美编程语言的人要么是推销员，要么是傻子，或者两者兼而有之。”像生活中的许多事物一样，技术更多地是关于权衡而非寻找完美工具。

我们还必须质疑机器学习中的基本思想和方法。观察数据中的相关性，几乎所有机器学习模型都依赖的现象，可能是毫无意义、伪相关或错误的。统计学家和其他经验科学家长期以来就已经意识到了这个问题。如果我们在相同数据上进行大规模的网格搜索或其他重复比较，那么模型中的数百万、数十亿或数万亿个参数可能会发现大数据集中的相关模式，这可能并不重要。此外，我们必须质疑我们正在使用的数据的客观性和准确性。仅仅因为一个数据集是数字化的或大规模的，并不意味着它以我们需要的方式包含了机器学习模型可以学习的信息。最后的关键是缺乏可重复性。如果我们应用数据科学方法，难怪数据科学和机器学习存在众所周知的可重复性问题。重现实验设置和繁琐的技术步骤已经够难了，但是要求其他人像我们一样应用确认偏见和其他——通常未记录的——实验设计错误来复制我们有缺陷的结果几乎是不可能的。

## 科学方法

尽管数据科学方法通常令人兴奋、快速且简单，但我们必须找到方法将传统的科学方法应用于我们的工作，尤其是对于高风险的机器学习系统。本小节的步骤仅展示了将传统科学方法应用于机器学习项目的一种方式。阅读以下步骤时，请将其与数据科学方法进行比较。注意避免确认偏见、结果与技术的对比、收集适当的数据以及重现结果的重点：

1.  发展一个可信的直觉（例如，基于先前的实验或文献回顾）。

1.  记录我们的假设（即，我们机器学习系统预期的真实世界结果）。

1.  收集适当的数据（例如，使用实验设计方法）。

1.  测试假设：机器学习系统对治疗组是否具有预期的体内效应。

    1.  使用 [CEM](https://oreil.ly/RxSH-) 或 [FLAME](https://oreil.ly/AFb4z) 从收集的观察数据中构建对照组和治疗组，或设计一个双盲随机实验。

    1.  测试治疗组在体内是否存在统计学显著效应。

1.  复现。

这个提议代表了大多数数据科学工作流程的彻底改变，因此我们将更详细地讨论每个步骤。首先，我们试图基于深入的文献回顾或过去成功的实验来设计我们的系统。然后我们将我们的假设记录在一个公共的地方，比如 GitHub 仓库，这样其他人如果改变它们就会注意到。那个假设应该是有意义的（即具有构建有效性），关于系统预期的真实世界结果，并且不是关于例如 XGBoost 能否击败 LightGBM。我们应该尽量避免使用现有的任何数据。相反，我们应该尝试使用合适的数据。这可能意味着收集特定的数据，并与统计学家或调查专家合作，以确保我们收集的数据遵循已知的实验设计原则。此外，我们必须提醒自己，我们的验证和测试错误度量看起来很好并不一定重要；如果我们正在进行数据分析，并参与使用网格搜索进行多个子实验，我们正在过拟合测试数据。虽然我们重视测试数据中的积极结果，但我们的重点应该是在真实世界中测量显著的治疗效果。我们的系统是否做到了我们说它会做的事情？我们能以某种可信的方式测量这一点吗，比如使用粗略的精确匹配来创建治疗和对照组，配合 A/B 测试和统计假设检验，用系统处理过的人和未处理过的人？最后，我们尽量不要假设我们的系统真的有效，直到其他人，比如熟练的模型验证者，重现我们的结果。

我们承认这种数据科学上的激烈变化最多只是理想化的，但当我们接近高风险的机器学习项目时，我们需要尝试。我们有责任避免高风险机器学习项目的数据科学方法，因为系统失败会迅速而大规模地影响真正的人类。

# 评估已发布的结果和声明。

另一个阻止我们应用科学方法的问题是，在 ML 的兴奋中，我们可能已经忘记了如何验证所有这些声称的发表。我们获取信息的许多来源——Medium、Substack、Quora、LinkedIn、Twitter 和其他社交定向平台——通常不是经过同行评审的出版物。仅仅因为在 Medium 或 Substack 上发布是有趣的（我们确实这样做了），它们是学习新事物的方便场所，但我们必须记住，这些平台上的任何人都可以说任何事情。除非它们直接重申了更可信出版物中发布的结果或在其他独立实验中得到了证实，否则我们应对社交媒体上报告的结果持怀疑态度。

此外，像 arXiv 这样的预印本服务并未经过同行评审。如果我们在那里找到了有趣的东西，我们应该查看是否实际上在尊重的期刊中发表，或者至少在会议记录中发表后再考虑采取行动。即使对于经过同行评审的期刊出版物或教科书，我们也应该花时间独立理解和验证这些主张。如果一篇论文上所有的引用都是已经揭穿的伪科学，那是一个非常糟糕的信号。最后，我们确实承认，我们的经验告诉我们，学术方法通常必须适应现实世界的应用。但是，还是建议在稳固的学术研究基础上建立，而不是在博客和社交媒体帖子的流变沙上建立。

###### 警告

博客、新闻通讯和社交媒体内容通常不是权威科学和工程信息的来源。

一些资金充足的科技公司的研究团队可能也在推动被视为研究成就与工程成就的界限。请花一点时间思考语言模型（LMs），通常是技术研究团队的宝贵 AI 成就。即使是一支资金适中的学术研究团队能否重建其中一个模型？我们知道使用了哪些训练数据，或者我们看到了代码吗？这些系统难道没有经常[失败](https://oreil.ly/4blT4)吗？传统上，科学上被接受的研究结果应该是可重复的，或者至少是可验证的。虽然我们不怀疑技术公司发布的 LMs 的基准分数，但我们质疑它们是否足够具有意义地可重复、可验证或透明，以便被视为研究而非工程成就。

###### 注意

读者应理解，尽管这本书经过了编辑和技术审查，但它并没有经过同行评审。这是我们试图将该书与 NIST AI 风险管理框架等外部标准对齐的另一个原因。

此外，由于 ML 是一个商业领域——在这个领域中，大量研究和 ML 工程的目标是在商业解决方案中实施，并且许多研究人员被从学术界挖掘到高薪的工业工程工作中——我们必须诚实地面对利益冲突。如果一家公司计划销售某项技术，我们应该对其报道的结果持谨慎态度。当一家技术公司计划销售技术时，它发表关于 AI 系统的惊人结果可能不是完全可信的声明。如果这些发布的结果没有经过外部、独立和客观的同行评审，情况会更糟。坦率地说，公司 X 说他们自己的技术很棒并不真实可信，无论白皮书有多长或 LaTeX 模板看起来多像 NeurIPS 论文。我们必须小心处理商业实体和 ML 供应商自报的结果。

有很多夸大和花哨的广告。在获取关于下一个项目的想法或者只是试图理解什么是炒作和什么是真实的时候，我们应该更加慎重。虽然通过关注广受引用的学术期刊和教科书可能会错过一些新的想法，但我们将更清楚地了解到实际可能发生的事情。我们也更有可能基于坚实且可重复的想法而不是炒作或市场宣传来规划我们的下一个项目。因为相比于演示、博客文章或低风险应用，高风险 ML 应用的成功更难伪造，所以即使我们花费更长时间开始并且我们的计划听起来并不那么令人激动，我们在长远来看更有可能成功。最终，在解决难题时真正的成功比更多的演示、博客文章和在琐碎用例中的成功更为重要。

# 应用外部标准

长期以来，关于人工智能（AI）和机器学习（ML）的标准大多缺失。但现在不同了。标准开始被定义。如果我们在处理复杂的 ML 问题时，如果对自己诚实，我们希望得到帮助和建议。获取高风险 ML 项目帮助和建议的好地方是权威标准。在本节中，我们将重点关注来自美国联邦储备银行（FRB）、NIST、欧盟 AI 法案和国际标准化组织（ISO）的标准，以及我们认为如何最好地利用它们。FRB 模型风险管理（MRM）指南和 NIST AI 风险管理框架（RMF）都非常注重文化和流程，尽管 NIST 还涉及一些技术细节。欧盟 AI 法案的附件对于定义和文档非常有帮助，ISO 也提供了许多定义和良好的技术建议。这些资源帮助我们思考许多不同类型的风险和风险缓解，并帮助确保在高风险 ML 项目中我们没有忽视明显的事情：

模型风险管理指南

我们在本书早些时候就赞美了[“模型风险管理监管指南”](https://oreil.ly/Gy_ol)，我们现在再次赞美一下。只是不要期望这些指导为低级技术提供建议。在尝试为组织建立治理或风险管理结构时，请参考这些指导。可以从这些指导中获得的通用经验包括以下内容：

文化至上。

如果一个组织的文化不尊重风险管理，那么风险管理就不起作用。

风险管理从最高层开始。

董事会和高级管理人员必须在 ML 风险管理中积极参与。

文档是基本的风险控制措施。

写出我们的模型工作原理，以便他人可以审查我们的思路。

测试应是独立且高声望的功能。

测试人员应被授权暂停或终止开发工作。

必须激励人们参与风险管理。

这对于免费来说太难了。

此外，如果读者希望通过 ML 风险管理大开眼界，请查阅[*“监理指南：模型风险管理”*](https://oreil.ly/jR7Wl)，特别是内部控制问卷。这些是银行监管机构在进行监管检查时要经历的步骤，我们建议您仅出于可能性的艺术目的偷看一下，并记住这只是大银行为控制其 ML 风险而预期做的部分工作。此外，这些风险控制对于 NIST AI RMF 具有重大影响，该框架多次引用了监督指导和*监理手册*。熟悉这些资源很重要，因为它们可能塑造您所在行业、部门或垂直领域的未来监管或风险管理指导。这些资源本身——监督指导和*监理手册*——也可能会继续缓慢发展。

NIST AI 风险管理框架

[NIST AI 风险管理框架](https://oreil.ly/8yGFz)在银行业实践的 MRM 指导基础上进行了有意义的扩展。在银行业实践 MRM 时，模型风险管理人员通常可以指望银行中的其他职能部门关注隐私、安全和公平性问题，从而能够主要专注于系统性能。RMF 将这些以及其他可信特征——有效性、可靠性、安全性、偏见管理、安全性、弹性、透明度、问责制、可解释性、可解读性和隐私——统一纳入 AI 风险管理的旗帜下，对于非银行机构更为现实。

AI RMF 提供跨所有这些愿望的高层建议，并且重要的是，它明确指出它们都是相互关联的。与 MRM 指南不同，RMF 强调多样性和包容性作为风险控制，并将类似事件响应和漏洞悬赏这样的网络安全风险控制引入 AI 风险控制范畴。NIST 指南也分为多个文件和互动网站。虽然[核心 RMF 文件](https://oreil.ly/q27WB)提供了更高层次的指导，但许多额外的资源深入探讨了技术和风险管理细节。例如，[AI 风险管理手册](https://oreil.ly/hd5oV)提供了有关风险管理的详尽指导，以及相关文档建议和参考资料。相关文件，如 NIST SP1270 和 NISTIR 8367，《人工智能中可解释性和可理解性的心理基础》，提供了特定主题的极其有用和详细的指导。RMF 是一个长期项目。请期待未来几年出现更多高质量的风险管理建议。

欧盟 AI 法案附件

前往此处查看高层次定义，包括高风险机器学习的定义以及文档建议。[欧盟 AI 法案](https://oreil.ly/5WVMj)的附件 I 为 AI 提供了一个坚实的定义。我们需要统一和达成一致的风险管理定义。这很重要，因为如果一个政策或测试应该适用于组织中的所有 AI 系统，我们可以预期至少有一组或一个人会通过声称他们不从事 AI 来逃避要求。附件 III 描述了被视为高风险的具体应用，例如生物识别身份验证、基础设施管理、教育、就业、政府或公共事业服务、信用评分、执法、移民和边境控制以及刑事司法。最后，附件 IV 为 ML 系统应该记录的内容提供了良好的指导。如果我们的组织偏向于庞大的 MRM 文件和最小化的模型卡之间的某个地方，我们将欣赏到附件还为 ML 系统文档提出了良好的框架。请注意，AI 法案截至本书出版时为草案法规，但通过的可能性被认为很大。

ISO AI 标准

不断发展的[ISO AI 标准](https://oreil.ly/BxcQz)是寻找低级技术指导和大量技术定义的好地方。虽然许多标准仍在开发中，但像[ISO/IEC PRF TS 4213—​机器学习分类性能评估](https://oreil.ly/bMczF)、[ISO/IEC TR 24029-1:2021—​神经网络稳健性评估](https://oreil.ly/AvPNZ)和[ISO/IEC TR 29119-11:2020—​AI 系统测试指南](https://oreil.ly/MwV_T)等已经可以使用。这些可用的标准确实有助于确保我们的技术方法是完整和彻底的。与本节讨论的其他指南不同，ISO 标准通常不是免费的。但它们也不是特别昂贵，比 AI 事件要便宜得多。随着时间的推移，监视 ISO AI 标准的增量完善会为额外有价值的技术指导和风险管理资源提供支持。

###### 注意

应用外部标准，如 ISO 和 NIST 的标准，可以提高我们的工作质量，并在不可避免发生问题时增加防御性。

还有其他组织提供的标准，比如电气和电子工程师学会(IEEE)、美国国家标准学会(ANSI)或经济合作与发展组织(OECD)的标准，这些标准可能也适合您的组织。关于这些标准，还有一件事要记住，应用它们不仅有助于我们做得更好，而且在审查时也有助于我们证明我们的选择。如果我们在机器学习的高风险工作中，应该期待审查和监督。用这些标准来证明我们的工作流程和风险控制会比基于我们自己编造或在博客或社交网站上找到的东西更好。简而言之，使用这些标准使我们和我们的工作看起来更好，因为它们已知能够提升技术水平。

# 常识性风险缓解

我们在高风险的机器学习项目上花费的时间越多，我们对什么可能出错和什么可能顺利的直觉就越发展。这一部分详细的建议可能可以在一些标准或权威指南中找到，但我们是通过艰辛的方式学到的。这些观点是一系列常识性建议的集合，应该有助于加速从业者在处理高风险机器学习系统时的直觉。它们可能看起来基础或显而易见，但是让我们坚持这些艰难赢得的经验教训并不容易。市场总是在推动我们更快地前进、测试更少、对风险做更少的事情。这对于低风险应用可能没问题，但对于严肃的使用场景来说，放慢节奏并思考是值得的。我们在这里详细描述的步骤有助于阐明我们为什么以及如何做到这一点。基本上，我们应该在编码之前思考，测试我们的代码，并为这些过程提供足够的时间和资源：

简单开始。

使用基于深度学习、堆叠泛化或其他复杂技术的复杂机器学习系统可能会令人兴奋，尤其是在高风险应用中。然而，除非问题需要那种复杂度水平，否则我们不应这样做。复杂性往往意味着更多的故障模式和更少的透明度。透明度较少通常意味着系统更难修复和审查。在处理高风险项目时，我们必须权衡失败可能性及其带来的损害与我们玩弄高科技的愿望之间的关系。有时候，最好从更简单、更清晰理解的方法开始，然后随着时间的推移逐步迭代到更复杂的解决方案。

避免过去的设计失败。

不要重复过去的机器学习失败。在处理高风险问题时，我们应该回顾过去类似问题的失败尝试。这正是[AI 事故数据库](https://oreil.ly/VlclU)的变革论点之一。这是我们应该查看的几个资源之一，以帮助我们避免过去的机器学习错误。我们还应该在组织内部询问。可能有人之前就尝试过解决我们正在尝试解决的问题，尤其是如果这是一个重要的问题。

为风险管理分配时间和资源。

风险管理需要时间、人力、资金和其他资源。建立系统演示的团队可能不够大或者广泛，以至于无法构建系统的生产版本并管理其风险。如果我们正在处理高风险的机器学习系统，我们需要更多的资源进行强化工程、测试、文档编制、处理用户反馈以及风险评审。我们还需要更多的时间。组织、管理者以及数据科学家往往低估了即使是构建平凡机器学习系统所需的时间。如果你在处理高风险的机器学习项目，你可能需要延长项目周期，甚至可能是低风险系统所需周期的几倍。

应用标准的软件质量方法。

我们以前说过，现在再说一遍。没有理由让机器学习系统免于标准软件质量保证流程。对于高风险系统，我们可能需要应用全方位的软件质量保证措施：单元测试、集成测试、功能测试、混乱测试、随机攻击等等。如果你需要回顾这些技术如何应用于机器学习系统，可以参考第三章。

限制软件、硬件和网络依赖。

每一个我们使用的第三方软件，无论是开源还是专有的，都会增加我们系统的风险。我们并不能总是知道这些依赖项是如何管理风险的。它们是否安全？是否公平？是否符合数据隐私法？这些都很难确认。同样的概念也适用于网络依赖性。我们连接的机器是否安全？它们是否始终可用？答案是，至少在较长的时间内，可能并不是。虽然专用硬件通常比第三方软件和额外的网络连接带来更少的安全和故障风险，但它确实增加了复杂性。增加的复杂性通常默认会增加风险。减少和简化软件、硬件和网络依赖性可能会减少意外情况，必要的变更管理流程以及所需的风险管理资源。

限制多个机器学习系统之间的连接。

如果一个机器学习系统的风险难以列举，那么当我们开始构建基于机器学习的决策或技术流程时会发生什么？结果可能极不可预测。在连接机器学习系统到像互联网这样的大型网络或将多个机器学习系统连接在一起时要小心。这两种情况都可能导致意外的伤害，甚至系统性的故障。

限制系统输出，以避免可预见的事件发生。

如果某个机器学习系统的某些结果可预见地存在问题，比如允许自动驾驶汽车加速到每小时 200 英里，我们不必坐视不管，让我们的系统做出错误的决策。使用业务规则、模型断言、数值限制或其他保障措施，防止系统做出可预见的错误决策。

记住游戏并非现实世界。

数据科学竞赛的排行榜根据单一指标排名模型，没有考虑到方差或现实世界中的权衡，不足以评估现实世界的决策。机器学习系统在游戏中成功也不是。仅因为一个机器学习系统在游戏中成功，并不意味着它在现实世界中也会成功。在游戏中，我们知道所有的规则，规则不会改变。在某些情况下，我们可以访问与游戏相关的所有可能数据，例如所有可能的结果或所有可能的移动。但这并不现实。在现实世界中，我们不知道所有的规则，系统的规则可以急剧而迅速地改变。我们也无法获得所有需要做出良好决策所需的数据。一个机器学习系统在游戏中的成功可能是一个巨大的研究成就，但对于部署在世界上的高风险社会技术机器学习系统来说可能是无关紧要的。

仔细监控无监督或自我更新的系统。

无监督系统、在没有地面真相的情况下训练的系统以及自更新系统（例如，强化、适应或在线学习）本质上具有更高风险。在部署这些系统之前，很难理解无监督系统是否表现足够良好，并且难以预测自更新系统可能的行为。虽然所有机器学习系统都应该进行监控，但部署到高风险应用程序的无监督和自更新系统需要实时监控性能、偏差和安全问题。这类监控也应尽快在检测到问题时提醒人类，并且这些系统可能需要配备关闭开关。

理解人类受试者的伦理和法律义务。

鉴于许多机器学习部署涉及收集敏感数据或本身就是对人类用户进行的隐含或显性实验，我们应熟悉我们组织的机构审查委员会（IRB）政策，[人类实验的基本指南](https://oreil.ly/1ptk7)，以及进行人类实验的其他法律和伦理义务。

限制匿名使用。

如果系统不需要匿名使用，则要求用户在使用之前进行身份验证或以其他方式证明其身份可以显著减少涉及系统的黑客攻击、滥用和其他不良行为。

对 AI 生成的内容应用水印。

向任何 AI 生成的内容添加显著标记、字符和声音可以帮助识别它，并减少这类内容被用于欺骗行为的风险。

知道何时不使用机器学习。

机器学习并不是解决所有问题的万能药。事实上，有一大类问题我们知道它并不擅长解决。在[预测生活结果](https://oreil.ly/UyX10)方面，机器学习并不比人类或简单模型表现更优，而人类和简单模型在这方面也表现不佳。从视频中，机器学习也无法准确预测谁在工作中表现良好，根据[NIST](https://oreil.ly/1QY4W)的说法。包括阿尔温德·纳拉亚南在内的知名机器学习研究人员，指出了机器学习在预测犯罪再犯率、执法和发现恐怖分子方面的问题。机器学习在理解或预测许多人类和社会结果方面表现并不理想。虽然这些问题有趣且具有高价值，但我们不应该试图用机器学习解决它们，除非我们知道一些 NIST 和美国国家科学院尚不知道的事情。社会结果并不是机器学习系统已知存在问题的唯一领域。在深入涉及高风险机器学习系统之前，记得查看过去的失败案例。

###### 注意

在你下一个重要的机器学习项目中，不要害怕询问关于设计、时机、资源、成果和用户的基础性问题。

通过结合这些常识性控制措施与增加的人口和专业多样性，更好地坚持科学方法，更严格地评估已发表声明，应用权威外部标准，以及在前几章中提到的所有治理和技术好处，您应该能够在困难的机器学习应用中取得更好的结果。当然，要获取所有这些额外工作的支持并找到时间来完成这一切并不容易。不要试图做得太多。回想一下第一章和风险管理基础知识。尝试理解您最严重的风险是什么，并首先减轻它们。

# 结论

本书从管理构建和维护机器学习系统的人员的教训开始。然后讨论了如何通过可解释模型和可解释人工智能使机器学习模型更加易于理解。它概述了通过模型调试和安全方法使机器学习模型对人们更加可信的方法，并强调了如何使它们对人们更加公平。这种对人们的关注并非偶然。技术是关于人的。除了某种类型的人类利益外，几乎没有理由去开发技术，而机器在受到伤害时不会感到痛苦、愤怒和悲伤，人们会。此外，根据我们的判断，人类仍然比计算机更聪明。过去十年的机器学习主要是关于使用几乎没有人类输入训练的大规模不可解释模型的成功，我们怀疑现在是时候在某种程度上把钟摆摆回来了。未来几年的许多机器学习成功将涉及法律和监管合规性、改进人类与机器学习的互动、风险管理和切实的业务成果。将最大化人类利益和最小化伤害作为您的高风险机器学习项目的核心，您将取得更多成功。

# 资源

进一步阅读

+   [欧盟 AI 法案附件](https://oreil.ly/CcERN)

+   [ISO AI 标准](https://oreil.ly/cUmGz)

+   [NIST AI 风险管理框架](https://oreil.ly/fN5BS)

+   [NIST SP1270：“向标准化人工智能中的偏见识别与管理迈进”](https://oreil.ly/udvYe)

+   [“监管模型风险管理指导”](https://oreil.ly/IuzZx)
