# 第七章。自编码器

本书的前六章探讨了如何利用无监督学习进行降维和聚类，我们讨论的概念帮助我们构建了检测异常和基于相似性分割群组的应用程序。

然而，无监督学习能够做的远不止这些。无监督学习在特征提取方面表现出色，特征提取是一种从原始特征集生成新特征表示的方法；新的特征表示称为*学习表示*，并用于提高监督学习问题的性能。换句话说，特征提取是无监督学习到监督学习的手段。

自编码器是特征提取的一种形式。它们使用*前馈、非递归神经网络*执行*表示学习*。表示学习是涉及神经网络的整个机器学习分支的核心部分。

在自编码器中——它们是一种表示学习的形式——神经网络的每一层学习原始特征的表示，后续层基于前面层学到的表示进行构建。逐层递进，自编码器从简单的表示学习逐步建立更为复杂的表示，形成所谓的层次概念，并且这些概念变得越来越抽象。

输出层是原始特征的最终新学习表示。然后，可以将这种学习表示用作监督学习模型的输入，以改进泛化误差。

但在我们过多深入之前，让我们先介绍神经网络以及 Python 框架 TensorFlow 和 Keras。

# 神经网络

在其根本上，神经网络执行表示学习，即神经网络的每一层从前一层学习到一个表示。通过逐层构建更加细致和详细的表示，神经网络可以完成非常惊人的任务，如计算机视觉、语音识别和机器翻译。

神经网络有两种形式——浅层和深层。浅层网络有少量层，而深层网络有许多层。深度学习因其使用深度（多层）神经网络而得名。浅层神经网络并不特别强大，因为表示学习的程度受到层次较少的限制。另一方面，深度学习非常强大，目前是机器学习中最热门的领域之一。

明确一点，使用神经网络进行浅层和深层学习只是整个机器学习生态系统的一部分。使用神经网络和传统机器学习之间的主要区别在于，神经网络自动执行了大部分特征表示，而在传统机器学习中则是手动设计的。

神经网络具有*输入层*、一个或多个*隐藏层*和一个*输出层*。隐藏层的数量定义了神经网络的*深度*。您可以将这些隐藏层视为中间计算；这些隐藏层共同允许整个神经网络执行复杂的函数逼近。

每个层次有一定数量的*节点*（也称为*神经元*或*单元*）组成该层。然后，每层的节点连接到下一层的节点。在训练过程中，神经网络确定分配给每个节点的最佳权重。

除了增加更多的层次外，我们还可以向神经网络添加更多节点，以增加神经网络模拟复杂关系的能力。这些节点被输入到一个*激活函数*中，该函数决定了当前层的值被馈送到神经网络的下一层。常见的激活函数包括*线性*、*sigmoid*、*双曲正切*和*修正线性单元（ReLU）*激活函数。最终的激活函数通常是*softmax 函数*，它输出输入观察值属于某个类的概率。这对于分类问题非常典型。

神经网络可能还包括*偏置节点*；这些节点始终是常量值，并且与前一层的节点不连接。相反，它们允许激活函数的输出向上或向下偏移。通过隐藏层（包括节点、偏置节点和激活函数），神经网络试图学习正确的函数逼近，以便将输入层映射到输出层。

在监督学习问题中，这相当直观。输入层表示馈送到神经网络的特征，输出层表示分配给每个观察的标签。在训练过程中，神经网络确定了在整个神经网络中哪些*权重*有助于最小化每个观察的预测标签与真实标签之间的误差。在无监督学习问题中，神经网络通过各个隐藏层学习输入层的表示，但不受标签的指导。

神经网络非常强大，能够模拟复杂的非线性关系，这是传统机器学习算法难以处理的。总体来说，这是神经网络的一个伟大特性，但也存在潜在风险。因为神经网络能够建模如此复杂的非线性关系，它们也更容易过拟合，这是在设计使用神经网络的机器学习应用时需要注意和解决的问题。¹

尽管有多种类型的神经网络，比如*递归神经网络*（数据可以在任何方向上流动，用于语音识别和机器翻译）和*卷积神经网络*（用于计算机视觉），我们将专注于更为直接的前馈神经网络，其中数据仅向一个方向移动：向前。

我们还必须进行更多的超参数优化，以使神经网络表现良好——包括选择成本函数、用于最小化损失的算法、起始权重的初始化类型、用于训练神经网络的迭代次数（即周期数）、每次权重更新前要喂入的观察次数（即批量大小）以及在训练过程中移动权重的步长（即学习率）。

## TensorFlow

在介绍自动编码器之前，让我们先探索一下*TensorFlow*，这是我们用来构建神经网络的主要库。TensorFlow 是一个开源软件库，用于高性能数值计算，最初由 Google Brain 团队为内部使用开发。在 2015 年 11 月，它作为开源软件发布。²

TensorFlow 可在许多操作系统上使用（包括 Linux、macOS、Windows、Android 和 iOS），并且可以在多个 CPU 和 GPU 上运行，使得软件在快速性能方面非常具有可扩展性，并且可以部署到桌面、移动、网络和云端用户。

TensorFlow 的美妙之处在于用户可以在 Python 中定义神经网络——或者更普遍地说，定义计算图——然后使用 C++ 代码运行这个神经网络，这比 Python 快得多。

TensorFlow 还能够*并行化*计算，将整个操作序列分解为多个部分，并在多个 CPU 和 GPU 上并行运行。对于像 Google 为其核心操作（如搜索）运行的大规模机器学习应用程序来说，这样的性能非常重要。

尽管有其他能够实现类似功能的开源库，TensorFlow 已经成为最受欢迎的一个，部分原因是 Google 的品牌。

### TensorFlow 示例

在我们继续之前，让我们建立一个 TensorFlow 计算图并运行一个计算。我们将导入 TensorFlow，使用 TensorFlow API 定义几个变量（类似于我们在之前章节中使用的 Scikit-Learn API），然后计算这些变量的值：

```py
import tensorflow as tf

b = tf.constant(50)
x = b * 10
y = x + b

with tf.Session() as sess:
    result = y.eval()
    print(result)
```

很重要的一点是，这里有两个阶段。首先，我们构建计算图，定义了 b、x 和 y。然后，通过调用 `tf.Session()` 执行计算图。在调用之前，CPU 和/或 GPU 不会执行任何计算。而是仅仅存储计算的指令。执行此代码块后，您将如预期看到结果为“550”。

后面，我们将使用 TensorFlow 构建实际的神经网络。

## Keras

Keras 是一个开源软件库，提供在 TensorFlow 之上运行的高级 API。它为 TensorFlow 提供了一个更加用户友好的接口，使数据科学家和研究人员能够比直接使用 TensorFlow 命令更快速、更轻松地进行实验。Keras 的主要作者也是一位 Google 工程师，弗朗索瓦·朱勒。

当我们开始使用 TensorFlow 构建模型时，我们将亲自动手使用 Keras 并探索其优势。

# 自编码器：编码器和解码器

现在我们已经介绍了神经网络及其在 Python 中的流行库——TensorFlow 和 Keras，让我们来构建一个自编码器，这是最简单的无监督学习神经网络之一。

自编码器包括两部分，一个*编码器*和一个*解码器*。编码器将输入的特征集通过表示学习转换为不同的表示，解码器将这个新学到的表示转换回原始格式。

自编码器的核心概念与我们在第三章中学习的降维概念类似。类似于降维，自编码器不会记忆原始观察和特征，这将是所谓的*恒等函数*。如果它学到了确切的恒等函数，那么自编码器就没有用处。相反，自编码器必须尽可能接近但不完全复制原始观察，使用新学到的表示；换句话说，自编码器学习了恒等函数的近似。

由于自编码器受到约束，它被迫学习原始数据的最显著特性，捕获数据的基础结构；这与降维中发生的情况类似。约束是自编码器的一个非常重要的属性——约束迫使自编码器智能地选择要捕获的重要信息和要丢弃的不相关或较不重要的信息。

自编码器已经存在几十年了，你可能已经怀疑它们已广泛用于降维和自动特征工程/学习。如今，它们经常用于构建*生成模型*，例如*生成对抗网络*。

# 不完全自编码器

在自编码器中，我们最关心的是编码器，因为这个组件是学习原始数据新表示的组件。这个新表示是从原始特征和观察得到的新特征集。

我们将自编码器的编码器函数称为*h = f(x)*，它接收原始观察*x*并使用函数*f*中捕获的新学到的表示输出*h*。解码器函数使用编码器函数重建原始观察，其形式为*r = g(h)*。

如您所见，解码器函数将编码器的输出*h*馈入并使用其重构函数*g*重构观察结果，称为*r*。如果做得正确，*g(f(x))*不会在所有地方完全等于*x*，但会足够接近。

我们如何限制编码器函数来近似*x*，以便它只能学习*x*的最显著属性而不是精确复制它？

我们可以约束编码器函数的输出*h*，使其维数少于*x*。这被称为*欠完备*自编码器，因为编码器的维数少于原始输入的维数。这再次类似于降维中发生的情况，其中我们接收原始输入维度并将其减少到一个更小的集合。

在这种方式下受限制，自编码器试图最小化我们定义的一个*损失函数*，使得解码器近似地使用编码器的输出重构观察结果后的重构误差尽可能小。重要的是要意识到隐藏层是维度受限的地方。换句话说，编码器的输出比原始输入的维数少。但解码器的输出是重构的原始数据，因此与原始输入具有相同数量的维数。

当解码器为线性且损失函数为均方误差时，欠完备自编码器学习的是与 PCA 相同类型的新表示，PCA 是我们在第三章介绍的一种降维方法。然而，如果编码器和解码器函数是非线性的，自编码器可以学习更复杂的非线性表示。这才是我们最关心的。但要注意——如果自编码器被赋予了太多的容量和自由度来建模复杂的、非线性的表示，它将简单地记住/复制原始观察结果，而不是从中提取最显著的信息。因此，我们必须有意义地限制自编码器，以防止这种情况发生。

# 过完备自编码器

如果编码器在比原始输入维度更多的维度上学习表示，那么自编码器被认为是*过完备*的。这样的自编码器简单地复制原始观察结果，并且不像欠完备自编码器那样被迫有效而紧凑地捕获原始分布的信息。话虽如此，如果我们采用某种形式的*正则化*，对神经网络学习不必要复杂函数进行惩罚，过完备自编码器可以成功用于降维和自动特征工程。

与欠完备自编码器相比，*正则化超完备自编码器*更难成功设计，但可能更强大，因为它们可以学习到更复杂但不过度复杂的表示，从而更好地近似原始观察结果而不是精确复制它们。

简而言之，表现良好的自编码器是那些学习到新表示，这些表示足够接近原始观察结果但并非完全相同的自编码器。为了做到这一点，自编码器本质上学习了一个新的概率分布。

# 密集自编码器 vs. 稀疏自编码器

如果你还记得，在第三章中，我们有密集（正常）和稀疏版本的降维算法。自编码器的工作原理类似。到目前为止，我们只讨论了输出密集最终矩阵的普通自编码器，以便少数特征具有有关原始数据的最显著信息。然而，我们可能希望输出一个稀疏的最终矩阵，以便捕获的信息更好地分布在自编码器学习到的特征之间。

为了做到这一点，我们需要在自编码器中包括不仅作为一部分的*重构误差*，还要包括*稀疏惩罚*，以便自编码器必须考虑最终矩阵的稀疏性。稀疏自编码器通常是超完备的——隐藏层的单元数比输入特征的数量多，但只有很小一部分隐藏单元被允许同时处于活动状态。这样定义的*稀疏自编码器*将输出一个具有更多零值的最终矩阵，所捕获的信息将更好地分布在学习到的特征中。

对于某些机器学习应用，稀疏自编码器具有更好的性能，并且学习到的表示也与正常（密集）自编码器略有不同。稍后，我们将使用真实示例来看看这两种类型的自编码器之间的区别。

# 去噪自编码器

正如你现在所知，自编码器能够从原始输入数据中学习新的（并且改进的）表示，捕获最显著的元素，但忽略原始数据中的噪音。

在某些情况下，我们可能希望设计的自编码器更积极地忽略数据中的噪声，特别是如果我们怀疑原始数据在某种程度上被损坏。想象一下在白天嘈杂的咖啡店里记录两个人之间的对话。我们希望将对话（信号）与背景嘈杂声（噪音）隔离开来。又或者，想象一下由于低分辨率或某种模糊效果而导致图像有颗粒感或失真的数据集。我们希望将核心图像（信号）与失真（噪音）隔离开来。

针对这些问题，我们可以设计一个*去噪自编码器*，它接收损坏的数据作为输入，并训练以尽可能地输出原始未损坏的数据。当然，尽管这并不容易，但这显然是自编码器应用于解决现实问题的一个非常强大的应用。

# 变分自编码器

到目前为止，我们已经讨论了使用自编码器来学习原始输入数据的新表示（通过编码器），以最小化新重构数据（通过解码器）与原始输入数据之间的重构误差。

在这些示例中，编码器的大小是固定的，为*n*，其中*n*通常比原始维度的数量小——换句话说，我们训练了一个欠完备自编码器。或者*n*可能大于原始维度的数量——一个过完备自编码器——但通过使用正则化惩罚、稀疏性惩罚等进行约束。但在所有这些情况下，编码器输出一个固定大小为*n*的单个向量。

一种替代的自编码器被称为*变分自编码器*，其编码器输出两个向量而不是一个：一个均值向量*mu*和一个标准差向量*sigma*。这两个向量形成随机变量，使得*mu*和*sigma*的第*i*个元素对应于第*i*个随机变量的*均值*和*标准差*。通过编码器形成这种随机输出，变分自编码器能够基于其从输入数据中学到的知识在连续空间中进行采样。

变分自编码器不仅局限于它训练过的示例，还可以进行泛化并输出新的示例，即使它可能以前从未见过完全相似的示例。这非常强大，因为现在变分自编码器可以生成看起来属于从原始输入数据学习的分布中的新合成数据。像这样的进展导致了一个完全新的和趋势的无监督学习领域，被称为生成建模，其中包括*生成对抗网络*。使用这些模型，可以生成合成图像、语音、音乐、艺术等，为 AI 生成数据开辟了无限可能。

# 结论

在本章中，我们介绍了神经网络及其流行的开源库 TensorFlow 和 Keras。我们还探讨了自编码器及其从原始输入数据学习新表示的能力。变种包括稀疏自编码器、去噪自编码器和变分自编码器，等等。

在第八章中，我们将使用本章讨论的技术构建实际应用程序。

在我们继续之前，让我们重新思考一下为什么自动特征提取如此重要。如果没有自动提取特征的能力，数据科学家和机器学习工程师将不得不手工设计可能在解决现实世界问题中重要的特征。这是非常耗时的，并且会极大地限制人工智能领域的进展。

实际上，直到 Geoffrey Hinton 和其他研究人员开发出使用神经网络自动学习新特征的方法——从 2006 年开始引发了深度学习革命——涉及计算机视觉、语音识别、机器翻译等问题一直大多数难以解决。

一旦自动编码器和其他神经网络变种被用来自动从输入数据中提取特征，许多这些问题就变得可以解决，导致过去十年间机器学习领域的一些重大突破。

通过在第八章中的自动编码器的实际应用中，你将看到自动特征提取的力量。

¹ 这个过程被称为正则化。

² 欲了解更多有关 TensorFlow 的信息，请查阅[网站](https://www.tensorflow.org/)。
