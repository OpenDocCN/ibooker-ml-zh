# 第六章：公平性、隐私和道德机器学习系统

作者：艾琳·尼尔森

本章讨论了在创建或部署机器学习系统时涉及的道德考虑和法律义务相关的主题。我们无法在这些主题上提供详尽的指导，但这份资源可以为您指明正确的方向。在本章结束时，您应该对机器学习部署的基本道德考虑有了良好的理解，以及可以让您开始在与您自己工作最直接相关的领域进行更深入教育的具体语言和概念类别。

###### 注意

编者注：当我们整理 MLOps 需要真正了解的主题列表时，公平性、隐私和人工智能以及机器学习系统中的道德问题位居榜首。然而，我们也知道，一个具有强烈行业背景的作者团队很难提供关于这些复杂问题的真正无偏见的观点。因此，我们邀请了艾琳·尼尔森（Aileen Nielsen），《实用公平性》（O'Reilly，2020）的作者，独立撰写这一章节。虽然我们就清晰性提供了草稿反馈，但这里的观点完全是她的，她对本章有完全的编辑控制权。你将直接从一个世界级专家那里获取这些信息！

我们还应该从一开始就指出，人工智能和机器学习中的公平性和道德仍然是高度争议的话题。事实上，目前一个合理的立场是，在计算系统中促进公平性的一种相当可行的方法 *不* 使用人工智能/机器学习。然而，对于那些发现自己被迫这样做，或者相信在特定使用案例中可以克服算法解决方案怀疑的人来说，本章提供了理解如何正确执行的起点。

此外，需要认识到，在某些情况下，现有或传统的问题解决方案在算法前时代（例如，指定人类决策者，或没有明确的决策者）并不总是很好。¹ 例如，大量经验研究表明，种族影响法官的判决决策，甚至店员关于是否允许顾客退货的非正式决策。因此，当我们考虑人工智能和机器学习中的公平性和道德时，我们也必须承认，在某些情况下，它们可能确实相对于人类决策者有所改进。因此，尽管本章中可能会发现一些忧愁和悲观情绪，但我们也从一开始就认识到，虽然尚无明确的解决方案能够使人工智能和机器学习在全球范围内保持公平和正义，但有些算法的使用确实在提高整体公平性方面取得了巨大成功。

本章节中，你会发现专注于特定热门话题的部分，特别是公平性、隐私和负责任人工智能。我们认识到这些话题在公众意识和关注度以及工业和学术界的关注度方面的重要性。在这些领域正在发生许多发展，我们希望能够为你提供一个良好的初始背景，使你能从本章节中获取这些话题的信息。在本章中，我们还提供了关于如何在你的组织中考虑重构工作的笔记，以实际方式增强你在 AI/ML 工作中的公平性和道德性。

# 公平性（也称为抗击偏见）

*算法公平性*及其各种变体，长期以来一直是机器学习中的热门话题。当你阅读关于这个主题的内容时，最常见的是公平性被用作直接与偏见相关的概念——即公平性被定义为没有偏见，而偏见则是不公平的状态。²

长期以来，机器学习研究人员、法律学者和活动家们一直对机器学习可能会延续现有社会偏见，甚至创造新的偏见形式表示关注。在这些讨论中，许多人强调，这可能发生的原因是用于训练机器学习系统的数据可能来自有偏见的系统或以有偏见的方式收集。简而言之，人们提出了类似于“垃圾进，垃圾出”的观点。

但你应当注意到，“垃圾进”并不一定指的是糟糕的数据。垃圾指的是由于不具代表性的训练数据或算法偏见而导致的*结果*。这是一个关键点：重要的是要意识到偏见的根本源头可以来自机器学习流程中的多个步骤，包括糟糕的数据，也包括糟糕的建模选择。以下是一些常被引用的偏见来源的非详尽列表：

抽样偏见

在抽样偏见中，数据收集过程本身就带有系统性偏见。一个常见的例子是，白人和黑人美国人之间的大麻使用率被认为大致相等，但是关于大麻持有的逮捕率对黑人美国人远高于白人美国人。这几乎可以肯定是由于抽样偏见造成的（除其他原因外）。由于种族主义在个体决策和系统层面上的显现，黑人美国人要比白人美国人更有可能因大麻持有而被警方搜索（有些人可能熟悉相关的种族主义表现，[开车时是黑人](https://oreil.ly/4w8oQ)）。

*与机器学习相关的例子：*

犯罪预测算法，例如那些用来影响警察巡逻分配的算法，很可能受到抽样偏差的影响。³ 在许多国家和大陆，警务工作往往集中在低社会经济地位的社区。由于这种过程产生的数据在某些地区或人口群体中过度抽样犯罪，因此很可能在不同社区中误代表犯罪的基础率。然而，这种以偏见方式抽样的数据，又为更多机器学习模型分配未来警务巡逻创建了新的偏见输入。由于这种算法，警察往往会过于频繁地回到同一地区，因为其他地区没有以同样的方式过度抽样。

不平等对待

偏见可能源于对个体明确地进行不同对待。当没有合理理由时，政府和某些私营部门的规范区域都禁止不平等对待。在几乎所有关于基于种族的不平等对待的例子中（如种族隔离学校或只雇佣白人的政策），法院都没有找到正当理由支持这种歧视（民权运动后）。在基于性别的不平等对待中，法院有时认为为性别不同的理由（例如对体能测试的不同表现门槛）可能由于强制性利益而被证明是正当的。

*ML 相关示例：*

机器学习算法常常因其发现超越人类能力的模式而受到赞誉。然而，有时这些模式只是显而易见的性别歧视。亚马逊曾开发但未部署⁴ 一种内部招聘算法，该算法对就读女子学院的女性应聘者应用了强烈的负参数。⁵ 因此，我们看到了算法在做出决策时使用无关因素，直接对一个群体（在这种情况下是去女子学院的女性）进行歧视的案例。如果这种算法被使用，这将看起来像是一种不平等对待的案例。

制度性偏见

与前述例子相比，这种偏见的来源在识别和减轻方面可能更具挑战性。广义上来说，我们可以将系统偏见视为许多因素的集合，这些因素很可能在个别情况下不能作为解释特征被识别，但在集合层面上，显然影响个人结果的差异，这些差异是由于结构性限制（这些限制已经融入到我们的社会、教育和就业系统中，等等）。人工智能系统无法脱离其构建的社会背景，因此没有理解背景的人工智能可能会加剧系统偏见。事实上，即使是人工智能应用的特定问题本身，通常也被视为系统偏见的表现。例如，一些人问为什么长期以来积极使用预测警务来预测社会经济弱势群体中的犯罪，而用于识别或预测警察不当行为的算法系统相对较少。

*与机器学习相关的例子：*

在培训机器学习系统时，有时会选择某些特征，因为它们似乎提供了“常识”示例，例如某些雇主或教育机构可能概念化了这种观念的“优点”。例如，中产阶级高中学生经常被告知要参加课外活动，以显示他们是高度积极的领导者。然而，如果在算法或人类决策中使用有关参与课外活动的数据，没有额外的背景信息，这将促成一个奖励中产阶级学生的系统，因为他们能够接触到这样的可能性。这种系统同样会使得无法参加这些活动的低收入学生处于劣势，因为他们需要工作或履行家庭义务。因此，对于不熟悉不同社会背景并且不了解系统偏见后果的人来说，所谓的“常识”一旦融入到分析中，可能看起来相当偏见。

多数暴政

这种偏见的来源与使用的培训过程形式相关。在许多常用的建模系统中，数值上的多数类别通常通过总数对训练损失影响最大。因此，如果建模过程没有考虑到这一点，许多类型的模型实际上会偏向于多数群体，并且相比于少数群体，更直接地将错误最小化。同样的担忧有时会激励设计政治系统，以保护少数群体的利益，而不是仅仅依靠多数规则。

*与机器学习相关的例子：*

众所周知，使用不涉及相对平等特征分布的数据来训练机器学习系统（不平衡数据集）可能会非常具有挑战性，如果我们希望为所有类别实现良好的性能。当处理包含不同类人群的数据集时，这种情况很可能会发生，因为大多数数据集在性别、种族或其他人类多样性形式（如地理或语言）上都存在某种形式的数据不平衡。因此，建模人类行为应该特别注意确保模型对每个人都公平对待，而不是根据事实上只对大多数类别中的某个人相对准确的原型进行建模。

不幸的是，这些偏见形式（及其他许多形式）在广泛的机器学习应用中仍然频繁出现，即使有关算法公平性的媒体关注日益增加。这种情况存在许多原因。社会偏见普遍存在，并不总是在个体层面上显现出来。某人可能认为自己在做出不带偏见的决定，既因为他们没有关于做出决定的系统所有细节，也因为他们自己的无意识偏见。这不是一个可以通过一章书解决的问题，但却是一章书可以让读者意识到并且有动力采取行动的问题。

机器学习/人工智能研究社区一直在开发方法和技术，系统地识别和纠正这些偏见。试图开发负责任和公平的人工智能系统的从业者应该了解这些新兴工具可能会有所帮助。更重要的是，机器学习/人工智能很可能提供一种前进的方式，使系统比使用不负责任的人类更加公平，只要机器学习的开发是谨慎的，并在针对适当问题时带有适当的保障。

## 公平性的定义

在机器学习社区中，我们尚未围绕公平性形成单一的定义，但存在一些常见类别，这些类别具有直观和吸引人的描述。例如，一些关于*公平性*的定义强调个体公平。这些公平性的概念认为，对于除了无关因素（种族、性别等）外“相同”的两个个体，这些个体应该被同等对待。⁶ 其他公平性的定义强调机器学习应该在群体层面上看待公平性。也就是说，错误率应该在各个群体中相同且质量相同，并且总体上性能水平在各个群体中也应该同样高。还有其他的公平性定义可能会更加复杂，并试图建立因果机制来理解是什么驱使个体成功或失败，然后再试图算法地对其进行分类。

两种常见的公平性类别是直观和吸引人的组公平和基于校准的评估。这些远非公平性的唯一概念，但我们选择这两种讨论的原因有两个。首先，每个定义都很直观，并且具有很强的吸引力。其次，每个都突出了公平的两个不同概念。

如其名，*组公平* 强调公平作为基础上组进行比较的问题。它强调应该至少部分地基于他们身份的敏感属性来审视个体的结果，尤其是法律保护的属性，如性别和种族，但也包括一些可能不受法律保护但某些人认为在道德上重要的属性，如经济地位。这样做有很好的理由，包括我们在看世界时，这些特征通常在实际结果中影响深远。

另一方面，*校准* 强调个体公平，这是我们大多数人都认为相当直观和吸引人的另一个价值观。同样，我们坚信个体应该根据他们是谁以及他们作为个体的所做所为来对待，而不是基于他们来自何处。

在哲学上看来，这两种公平性定义似乎不需要互相对立。在一个完美的世界中，群体和个体都得到公平对待，我们会得到相同的结果。不幸的是，在我们不完美的世界中，由于各种原因，包括基本的数学限制，情况并非如此。（参见 7）因此，至少目前，从业者必须选择他们的公平定义。正如我们将要描述的那样，不同的用例将有不同合理的公平定义。

在不同情况下应用不同的公平概念可能看起来很奇怪，成熟的读者将意识到我们在现实生活中同样应用相同的原则。考虑美国，一个强烈市场驱动的经济体。在很大程度上——即使在我们当前的民粹主义时代——在劳动市场上，存在着普遍的个性化公平观念。美国人似乎认为人们应该在开放市场上得到他们能得到的东西，尽管对工资不要过低有些担忧（因此有法定最低工资），同样对于最高支付率过高也有一些担忧（通过对不断增加的不平等和亿万富翁阶级崛起的担忧表明）。另一方面，当涉及到医疗保健时，大多数美国人认为每个人都应该获得良好的医疗护理，而不管个体健康或先天遗传的差异。

看起来（至少对我们这些坐在家里的人类学家来说），在某种程度上，美国人似乎对经济利益的分配方式感到舒适，可能在某种程度上是基于天生的能力，但当涉及医疗福利的分配时则不然。因此，事实上，普通人似乎确实常常在生活的不同领域应用不同的公平观念，我们认为在 ML 的不同上下文或使用案例中，同样可能是如此。

###### 警告

似乎 ML 伦理社区应该试图在*公平* 的单一定义上达成一致。当我们看到公平的不同定义在实践中是分歧甚至矛盾的时候，这种观点似乎尤为正确。目前，鉴于行业和该领域的成熟程度，几乎可以肯定的是，目标并不是收敛于一个单一的公平定义。这有两个原因。

首先，我们不需要这样做。目前部署在现实世界中的大多数模型的技术水平仍然在任何度量标准下基本上是不公平的（或至少是未经审查的公平）。我们有许多有用的公平定义可以选择，并且选择一个或几个来努力实现使我们有机会立即取得快速进展，而不需要等待进一步的理论或法律发展。

第二，从结构上看，目前 AI 和 ML 理论领域主要由社会中主导群体的人控制。这些人最有可能忽视 AI 和 ML 系统可能对无权或代表性不足群体产生负面影响的方式。毫无疑问，这些人不应该成为未来将公正定义为排斥其他一切的仲裁者。

*组平等* 公平性定义要求算法性能的相关比率在各种群体中相同。例如，一个组平等要求可能要求所有族裔的雇佣率相同，但也可能要求在诊断[低氧](https://oreil.ly/wU0mJ)时，无论皮肤颜色如何，准确度也相同。这个想法很引人注目和直观，因为它描绘了我们许多人希望看到的社会图景——一个不幸或机会平等分配给社区的社会。

与组平等相反，*校准* 公平性定义要求 ML 模型对所有个体工作效果一样好。虽然这不能直接衡量，但可以衡量的是，对于任何群体，模型得分对个体意味着相同。因此，一个以校准为导向的公平定义将要求对于任何群体，ML 得分的含义都是相同的。

这听起来令人生畏和高度技术化，但通过一个具体的例子可能更容易理解。校准的常见示例是用于许多美国州刑事法院的《矫正罪犯管理倾向评估替代刑罚》（Correctional Offender Management Profiling for Alternative Sanctions）或[COMPAS](https://oreil.ly/KEVqN)再犯评分算法。因为已经显示该算法在种族类别上正确校准，因此对于黑人和白人数据对象来说，相同的 COMPAS 分数（比如 0.5）意味着相同的事情：即，一个白人罪犯和一个黑人罪犯，如果有相同的风险评分，则有相同的再犯概率。⁸ 以通俗的英语理解，这也似乎很直观——即模型分数应该对每个人都有相同的含义，无论他们属于哪个群体。

你可能会想知道，公平有多种定义是否是一个问题。毕竟，在其他情境中，描述世界的正确和直观方式有很多，那么为什么在公平的情况下不行呢？困难在于——至少在我们目前生活的世界中——这些公平的定义会发生冲突。COMPAS 算法是一个引人注目的例子。该算法并非一定是随意实施的，而实际上有时是作为减少刑事司法系统偏见运动的一部分而采用的。该算法通过被认为是确保公平和非歧视的黄金标准的校准检查。

然而，《非营利新闻室》（ProPublica）的记者后来证明，尽管进行了这种校准，该算法在标记黑人被告为高风险时有更高的虚警率，并且在标记白人被告为低风险时有更高的漏警率。换句话说，与白人相比，黑人被告更有可能被标记为有高概率再犯的可能性，但实际上未再犯；而白人更可能被标记为低概率再犯，但实际上再犯。

这引发了公众的强烈抗议，促使许多数学家和计算机科学家致力于解决这个问题。然而，学者们很快意识到，他们的目标——制定一个同时展示校准和统计平等的风险评分——在大多数真实世界数据中是不可能的。统计平等和校准不能在同一时间数学上满足，除非每个群体的事件基础率相同。然而，在现实世界中，基础率的平等是一个很少满足的条件。⁹

在数学上，不同的公平性定义在现实世界的条件下不可能同时满足。我们必须决定在机器学习环境中追求哪些公平性目标，并决定是否专注于特定的公平性指标甚至可能减少朝向本质视角的倾向，这本来可能更有助于增强公平性和其他伦理价值。

目前，我们认为对于特定的机器学习工具，选择和强调特定的公平性指标可能是合理的，同时要认识到不可能同时满足所有直观和规范上期望的公平性概念。

我们不希望完美成为良好的敌人。相反，从业者们发现，有多种公平性定义比没有更具操作性，并且努力朝着公平性指标的方向努力，广泛来说也可能增强许多其他形式的公平性。更重要的是，一些研究人员意识到，不同的情况可能需要不同的公平性指标，考虑到算法特定用例的相对伤害或政策目标，因此有兴趣的读者可以找到如何根据特定任务选择公平性指标的有用指导，考虑到各种错误可能导致的后果。

## 争取公平性

具体地说，经典的机器学习设置有三种工作模式，以朝向更公平（更少偏见）的结果。在这里，我们简要介绍它们的工作原理，以便您可以通过概览的方式探索文献，并了解不同方法的利弊：

预处理

这些方法在数据而非模型中进行干预。预处理方法采用多种方法来减少机器学习训练输入中的不公平性。最直接的是，数据点可以被重新标记。例如，一些方法，比如[Kamiran 和 Calders (2011)](https://oreil.ly/I2fDD)，提供了识别应重新标记数据的方式，因为数据表明存在偏见结果。¹⁰

另一个较少激进的方法是寻求降低有关个体所属群体信息的数据表示；例如，[Zemel 等人](https://oreil.ly/vW9fa)（2013）首倡的方法建议描述数据，使得无法准确猜测个体的敏感属性。因为这些方法看待数据为干预点，它们不依赖于模型。一个经验法则是，**在机器学习管道的尽早阶段介入最有可能产生最佳结果**（并为后续的额外干预留下选项）。

但是，对预处理方法存在有效的关注。激进更改数据标签挑战了数据驱动学科（如机器学习），从根本上删除信息。同样，寻求“转换”数据而不是直接操纵标签的方法，在故意删除或更改数据时也挑战了数据驱动方法的基本原则。此外，很难确定由于更改数据而导致模型变化的具体原因，因为在大多数情况下，将无法清楚地知道所有影响重新标记少数数据点的事实。

在处理中

这些方法干预模型训练过程。这可以表现为任何实际步骤训练模型时受公平性考虑的影响方式。在许多情况下，这已通过调整模型训练中使用的损失函数来解决。可以添加各种“惩罚”，以反映由偏见结果造成的公平成本，正如[Kamishima 等人](https://oreil.ly/rEc4P)（2011）的工作所反映的那样。这些方法类似于正则化技术。

另一种方法类似于预处理中描述的方法（学习公平的表示），其动机在于从模型对敏感属性的知识中移除识别信息。同时训练对抗模型和感兴趣的模型，使得对抗模型的目标是猜测输出与哪个敏感类别相关联。感兴趣的模型被训练为一项任务，但也被优化以减少其输出传输给对抗模型的信息量。其中一些方法，例如[Zhang 等人](https://oreil.ly/SVmNt)（2018）的工作，可以是模型无关的。

后处理

这些方法干预模型的标签而不是直接干预模型本身。通过这种方式，它们根据达到某些目标来纠正模型的输出结果。后处理的一个例子是引入随机化，例如[Hardt 等人（2016）](https://oreil.ly/b9EhM)，在没有这种随机化的情况下，假阴性可能在不同群体之间不同（这是应用组平等的一个例子）。

另一个后处理的例子是为不同的群体设置不同的阈值，例如[信用评分](https://oreil.ly/DzYYL)或大学入学分数，以便使用分数做出的预测或决策对不同群体同样准确。¹¹因为这些方法在模型运行后介入，所以它们是模型无关的。

选择哪种干预方法取决于多种因素。在某些情况下，组织可能需要应用特定的干预阶段，因为这是他们可以控制的干预方式。例如，一个组织可能已经获得了一个预训练的神经网络，他们将对其进行微调。由于该组织无法访问原始训练数据或方法，后处理可能是一个更可行的选择。

另一方面，另一个组织可能会发现后处理提供的选项在规范上存在问题，因为它们违反了一些人们持有的某些基本价值观。例如，人们可能对为不同群体明确设定不同的分数截止点感到不适，他们也可能对用随机数字替换那些算法输出的确实可能是正确的内容的想法感到不适。

到目前为止，监管机构或著名的伦理领袖尚无关于最佳干预方式的明确指导方针¹²，我们认为这将需要进行高度依赖于情境的分析。但高度情境化的案例决策需求并不异于组织运作的其他要素或影响他人生活的决策。算法将有助于提高效率和一致性，但在所有情况下都没有一个单一的算法公平解决方案。

## 公平作为一个过程而不是一个终点

这是一系列棘手的话题，了解到 AI 可能会造成如此多的伤害可能会极大地泄气。但不要因此不再去思考精心设计的 ML/AI 系统能够带来的所有优势。关于公平性的消息并不一定是坏消息。

让我们考虑两个问题：

+   就公平而言，你不可能让每个人都满意。在任何算法公平性挑战中，都没有“完全”公平的解决方案（至少目前还没有被确认出来）。

+   同样地，即使你确定了公平的特定定义（即，定义一个度量并不保证你能使模型完全符合该度量，或指示你应该如何尝试这样做），也没有完美的强制公平的方式。

让我们从你在 ML 建模中做出选择的角度来反驳这两点：

+   你有许多良好的公平定义可以选择。

+   任何朝着公平目标努力的过程都比忽视这些问题的世界要好——因为忽视并没有带来好的结果。

将公平视为一个过程而不是一个特定的终点可能会有所帮助。对于那些希望通过构建机器学习算法来解决业务问题并前进的人来说，这可能令人失望。现实更加复杂。产品需要维护有很多原因。世界在变化，因此部署条件也在变化，因此模型必须不断发展。公平性并不例外。

## 一个快速的法律声明

在这项工作中提供法律指导远超出范围，这正是你公司法律部门的职责！尽管算法公平性似乎是一个新课题，但歧视的概念在法律中的重要性和明确性已经有了几个世纪的历史。今天最前沿的话题，如种族和性别平等，同样在几十年来一直在法律中占据着突出的地位。特别是在被理解为人类生活核心的市民生活领域，如就业、教育、医疗、住房和金融信贷服务的访问。如果您从事涉及这些核心领域的机器学习工作，您很可能需要对公平性 — 尤其是您国家反歧视法律中的公平性 — 深感关切。

# 隐私

*隐私* 是一个在学术界证明极其难以定义的概念。因此，隐私措施在未来保护方面尤为困难。大数据的兴起正是一个明显的例子。

专家曾认为去标识化数据是适当和安全的。这些数据会移除被认为是可识别信息的内容，如姓名或地址，这些信息很容易与一个人匹配。然而，与特定人的身份看似不直接相关的其他因素的信息仍然被发布，如生日、种族或邮政编码。

随着大数据的出现，许多数据集被编制，通常涉及重叠的人群，因此可以使用不同的数据集一起使用来识别脱身信息中的人们。例如，对于大多数美国人来说，仅凭知道他们的生日、邮政编码和性别就可能在去标识化数据集中识别他们¹³。此外，世界越来越充满新的数据集，有时是由于数据泄露，而其他时候是由于人们自愿分享关于自己的信息 — 这些信息变得非常容易获取[对于任何随意的恶心跟踪者](https://oreil.ly/AOVNs)。我们可能希望认为世界是一个匿名的地方，但今天情况并非如此。

这里描述了隐私的两个关键理念，既涉及到它们与隐私概念的关系，也涉及到如何将含有个人信息的现有数据集转化为可以使用或发布而不会泄露个体身份的数据集：

k-匿名性

k-匿名性的理念是，在给定的数据集中，对于任何感兴趣的组合类别，至少应有* k *个个体（外部指定）属于任何给定的桶。例如，我们可以将 k-匿名性应用于按城镇列出个体的数据集，要求数据被分桶，以确保对于任何邮政编码/出生信息/性别类别，至少有 10 个个体。可能有许多实现这一目标的方法。两个潜在的机会之一是报告月份或年份级别的生日，另一个是只报告邮政编码的前四位数字而不是全部五位。

这种隐私和预防措施的概念本质上将隐私概念化为在数据集中不处于太小的群体中。[建议的大小](https://oreil.ly/zl6MO) 在医疗领域至少为 5，尽管已经报道的*k*大小远大于 50。*k*的适当大小将取决于特定领域，这可能涉及数据的敏感性、能够使用具有大*k*的有用数据集的可能性，以及考虑其他已知可能可链接数据集的再识别可能性。

差分隐私

这种数学方法向数据添加噪声，以便可以就可能性（或更重要的是，不可能性）在访问到这些噪声化数据时推断出关于特定个体的信息做出概率性保证。差分隐私的理念是，一个人的包含或不包含对数据集的操作（例如计算平均值或其他统计数据）产生影响，使得基于聚合报告可以推断出数据集的信息。例如，如果报告一个班级的平均年龄和组的大小，可以知道该个体的年龄。然而，如果应用了差分隐私，在概率上将不可能以预定的精度水平和给定的查询预算推断出该个体的年龄。^（14）差分隐私可以广泛应用，不仅适用于聚合统计数据的计算，还适用于 ML 模型的训练，通过确保模型的输出或训练不依赖于特定数据点的包含。

这些隐私概念可能显得相当技术性，但与真实的隐私问题相关，例如管理负责和保护隐私的开放数据集的发布，或确保法律权利具有有意义的技术转化。例如，欧盟的 GDPR 赋予个人数据删除权利。然而，如果一个机器学习模型已经被训练过，删除权利可能并不完全具有意义。例如，个人删除其数据的权利是否意味着他们也能强迫一家公司证明其机器学习模型也已经“遗忘”他们的数据？¹⁵

一些人已经探讨了如何通过差分隐私训练模型来解决这一问题。然而，技术挑战依然存在，因为在保证差分隐私和高性能之间达到平衡是相当具有技术挑战性的。¹⁶

## 保护隐私的方法

先前描述的 k-匿名性和差分隐私的定义和场景涉及非常具体的隐私和计算措施。事实上，隐私本身是一个高度技术化和专业化的领域，最好由专家来实施。¹⁷ 然而，有各种易于实施且有意义的可访问的增强隐私措施。你应该将它们包含在自己的工作流程中，并可能需要根据实际情况进行定制。

系统管理员和涉及合规问题的人员可能已经熟悉这些概念。然而，对于数据科学家和机器学习工程师来说，这些概念有时会非常陌生。我希望它们将成为基本项目规范和日常实践的一部分，今后会得到重视。

### 技术措施

以下是一些保护隐私的技术措施：

访问控制

保护隐私和减少威胁的一个关键方法是实施强大的访问控制。数据库中关于个人的任何数据都应被视为“需要知道”的资源，机器学习工程师应该根据具体目的请求访问，而不是自由访问或浏览数据。同样，访问权限应定期审查，以确保员工没有因为已无有效理由而继续访问数据。

访问日志

跟踪谁在何时访问特定形式的数据。这样可以了解数据使用模式，看到某人可能在不适当地访问数据，并在后来被提出不适当使用的指控时保留证据。对这些日志的分析也可能表明数据存储模式可以重构以减少不同使用模式可以访问的数据量。例如，如果一个 ML 模型需要访问敏感数据表的一个列，考虑将该数据列拆分出来，而不是授予访问完整表格的额外但不必要的信息访问权限。

数据最小化

应尽量减少数据的收集和使用。数据不应仅仅因为“将来可能有用”而收集。只有在存在该数据的即时用例时（最好对被收集数据的个体有一些益处），才应记录数据。

数据分离

需要用于合法业务用途的数据应与不太可能对创建 ML 模型有用的敏感数据（如姓名和地址）分开。例如，为了预测用户的点击，似乎没有理由知道用户的姓名或地址。¹⁸ 因此，没有理由将这些信息存储在可能对特定预测任务有用的信息（如过去的浏览历史或人口统计信息）中。¹⁹ 正如之前所述，研究您的数据访问日志可以帮助您识别可以重构以最小化数据暴露给 ML 应用程序的方式。

### 机构措施

这里也有一些机构措施可供选择：

道德培训

每个人进入新领域时都需要道德培训，为设计 ML 产品也是如此。 ML 工程师应该得到——但通常不会得到——全面审查，不仅包括一般的道德培训（如数据中的偏见可能性），还包括为特定用例构建算法时的领域特定培训。太多时候，组织没有任何关于隐私或伦理更广泛讨论或培训，即使基础培训，无论多么“俗气”，也有助于突显问题。

数据访问指南

除了已经描述的技术措施外，制定明确的规则，即使在何时访问数据和使用数据以及明确禁止的用例方面，也是有意义的。缺乏明确和明确的伦理规则可能导致缺乏问责制的机构文化。组织应在一个显而易见和可访问的地方制定清晰的数据访问和适当数据使用指南。

隐私设计

隐私设计是一组设计原则，适用于任何数字产品，包括机器学习流水线和机器学习驱动的产品。隐私设计的概念是隐私不应仅仅是附加到已存在流程的最后阶段。相反，隐私应从设计考虑的开始就内在于其中，并且应该是所有工作阶段中要考虑和关注的问题。隐私设计可以为在机器学习开发的所有元素中确保隐私提供一种灵活但全面的方式。

尽管这些可能都看起来是基本和显而易见的观念，但很少有组织，无论是大型还是小型，采取这些基本措施，甚至追求任何类似隐私或公平议程的内容。无论您是在初创公司、学术机构还是大型企业，几乎肯定都可以为增强您组织的机器学习流水线中的隐私做出贡献。

简而言之，有多种方法可以通过在您的组织中采取具体步骤来保护隐私。不幸的是，很少有人采取这些步骤，但它们可以简单而有效。通过在您自己的工作流程中采取这些简单措施，您将无所失（而有很多可以获益）。

## 一个快速的法律声明

超出本工作范围的内容是对通常影响数字产品的隐私法律进行详尽审查。在这里，我们重点介绍与隐私和机器学习相关的几类关键法律：

数据泄露通知法律

数据泄露通知法律要求那些持有数据的人在数据泄露中数据受到损害时通知他们持有数据的人。为确保合规性，这类法律通常对数据持有者在意识到数据泄露但未进行适当通知时施加严格处罚。这类法律有时也适用于本应意识到数据泄露的数据持有者，以确保企业不能简单选择保持无知。迄今的实证研究表明，这些法律并未阻止数据泄露随时间变得越来越普遍，甚至呈指数增长。尽管如此，这类法律在提供消费者何时因其个人数据曝光而处于最高风险的通知方面仍然非常有用。

数据保护和个人隐私法律

数据保护法律中最显著的例子是欧盟的 GDPR。全球许多国家都有数据保护法律，赋予人们基本权利，例如知道在线场所收集了哪些关于他们的数据，这些场所如何处理这些数据，以及这些数据是否正确。一些法律甚至进一步赋予消费者选择退出数据收集的权利，或者甚至赋予消费者删除数据或阻止数据销售的权利。不幸的是，实证研究表明，这类法律似乎在面向消费者的应用中被广泛忽视。尽管如此，这类法律为消费者提供了积极的方式，可以采取措施保护其隐私。

反不正当和欺骗行为的法律

在美国的背景下，这类更普遍的消费者保护法律至关重要，因为美国缺乏全面的国家个人数据隐私制度。美国联邦贸易委员会，作为消费者保护执法的重要来源，有时会采取行动，发现公司在业务实践中未能遵守公平或诚实的基本期望，以保护消费者隐私。这种情况的一个关键例子是那些甚至不尊重他们自己制定并放在网站上的隐私政策条款的公司。因此，至少有必要确保，在您的组织构建数据收集和机器学习建模能力时，这些实践与面向公众的隐私政策和服务条款一致。

# 负责任的人工智能

*负责任的人工智能*已经成为我们在训练或部署机器学习系统时应考虑的伦理关切的总称。这是一个日益增长的领域，可以安全地假设，无论是工业界还是学术界，都还没有完全掌握应由负责任的人工智能处理的伤害范围。在这里，我们在这一框架下讨论了一些近年来引起广泛关注的附加问题。然而，我们强调这里的问题只是一些亮点。我们并不打算提供负责任的人工智能价值观的详尽清单。

## 解释

ML 模型*解释*是分析和展示关于 ML 系统的信息以描述该系统运作方式的过程。这一过程及其使模型易于人类理解的目标通常简称为*可解释性*，在 ML 方面是一个关键领域。许多人希望理解 ML 系统为何以某种方式工作，这是理解其对其影响的特定结果的一部分的愿望。²⁰

出于技术和伦理原因，希望有能够“解释”ML 模型工作方式或为何在特定案例中达成特定结果的方法是可取的。解释 AI 的技术动机与控制模型质量以及通过模型了解数据有关。开发模型解释的技术人员可以深入了解为什么他们的模型运行良好或不佳，并可能了解到关于数据基础领域的一些内容。寻求可解释 ML 的伦理学家和倡导者出于不同的原因有类似的关注。他们发现了解模型是如何得出结论的对于那些受其影响的人来说是有帮助的，甚至可以在 ML 的结论不合理时提出质疑。

然而，解释并不是一件简单的事情。解释可以服务于许多目的，因此，根据所需目的和准备的受众，解释所包含的信息可能多种多样。从这个角度看，模型解释可能会感觉很像高度训练的专业人士（如医生或律师）在尝试向特定受众提供建议或诊断真实世界情况时面临的问题。例如，当医生解释推荐的治疗及其相关风险时，他们会根据受众调整解释。他们可能会向一位同行医生提供一个解释，向一个已知拥有生物工程学位的患者提供另一个不那么技术性但仍严谨的解释，并向患有痴呆症但仍可能能够作出自己医疗决定的人提供另一个解释。因此，我们可以看出，解释取决于那个解释的受众或消费者。

提供的解释还取决于其*目的*。如果机器学习解释的目的是检查模型质量，例如确保模型出于正确原因做出正确决策，则可能更喜欢全局解释。*全局*解释指出模型通常如何工作以及为什么会遵循某些一般决策规则。另一方面，如果解释的目的是让某个特定被拒绝信用（或遭遇其他不良结果）的人了解为什么以一种方式，可以帮助她将来改善机会，那么此人会希望得到局部解释。*局部*解释解释了为什么特定的人被拒绝以及可能在将来采取的最具可操作性的调整。

我们暂时不会详细讨论解释。在这里，我们希望您了解以下关键点：

+   模型的解释没有单一正确的解释。

+   解释需要根据受众和目的进行调整。

+   解释需要有用，有时还需要采取行动。

在具体实施方面，您在开始了解机器学习解释技术时可能需要采取的最低步骤如下：

+   至少，对于最终用户来说，知道算法使用的输入是有帮助的（即使这些最小信息通常不可用）。如果您能提供相关特征重要性的列表，那就更好了。如果能够将其放在显著位置，并用易于理解的语言编写，以便普通人可以看到和理解模型，那将是最有意义的。

+   另一种获取直观和具体信息的简单方法是生成测试输入，也许是反事实配对，并查看它们的情况。例如，对于某些用例，某些反事实配对不应该影响决策（例如，性别反事实应该不会改变信用决策），而对于其他用例，它们很可能会（例如，体重反事实可能经常会改变医疗干预决策）。这些可以构成基本的“嗅探”测试，同时也是为决策主体提供示例解释的一种方式。

+   考虑你是否能够提供全局（解释整体模型）或局部（解释特定 ML 模型决策/分类）的解释，并探索一些实现这些解释的技术。某种技术是否特别适合您的受众以及您的 ML 产品正在做出的决策类型？

随着各种解释技术的出现，一些研究人员提供了有关什么系统可能为不同目的服务并且适合使用的有用指导。²¹ 最重要的考虑因素是确定解释的最终用户的复杂程度以及提供解释的目的。从这里开始，通常可以识别至少一个，通常更多的技术选项，一些是开源的并已由专家实施。²²

## 效果

ML *效果* — 即，ML 产品实际上达到其预期目标，并且出于正确的原因 — 是负责任地部署 ML 的关键。然而，正如凯西·奥尼尔在她的《*数学毁灭武器*》（Crown，2016）中所强调的那样，ML 的一个特别令人担忧的元素是，在许多部署场景中，ML 可能会成为一个自我实现的预言：ML 看起来可能有效，但这可能完全是基于错误的原因。

考虑一下，就像奥尼尔所做的那样，用于招聘目的的 ML 产品，其中候选人可能会被标记为自动拒绝。也许 ML 算法正确地认为某人会是一位糟糕的雇员，但很有可能，我们永远不会知道某人是否会成为一位优秀的雇员，因为我们没有雇用他们。但是，如果各种 ML 算法都使用相同的逻辑来不雇用特定候选人，那么该候选人永远不会有机会获得一份工作，我们也永远无法真正知道该候选人是否能够胜任工作（系统性缺失数据）。

如果某人被一个机器学习算法以某种特定方式标记，有时这个标记会被信任以解决问题，即使人类应该在一定程度上进行监督。²³ 在就业场景中，某人可能被标记为不合格的候选人，并且因为使用算法辅助决策的人认为他们不适合被雇用。也许这个求职者之所以不被雇用，并不是因为他们实际上是不合适的选项，而是因为算法标记他们为不合格选项。也许在许多潜在雇主中使用相同或类似的算法使得这种情况变得更糟。也许这个求职者将面临许多潜在雇主都在使用的相同算法，因此可能面临延长甚至无限期的失业。在某些时候，他们长期的失业本身可能会成为机器学习算法使用的另一个标志来反对雇佣某人的因素。

这个问题不仅仅限于 O’Neil 在潜在轶事中提到的情景。立法者们也很关注。例如，在最近提出的[2021 年算法公正和在线平台透明法案](https://oreil.ly/iVwyj)中，美国参议员埃德·马基和美国众议员多丽丝·松井提议，只有安全有效的算法才能合法。算法的有效性将通过展示机器学习算法“具有产生其期望或预期结果的能力”来确立。²⁴

鉴于这项法律或类似的有效性要求，我们可以思考一下 O’Neil 提到的自我实现招聘算法的例子可能如何发展。招聘机器学习算法的设计者应该展示该算法实际上预测了谁会是一个优秀的员工，而不仅仅是谁不太可能被聘用。从某种角度来看，这就是 *外部有效性* —— 即，算法驱动的发现或结论是否可以转化为一个重要的泛化到真实世界的概念。展示有效性的要求类似于算法的逻辑具有外部有效性，即它在合理程度上具有普遍性。

当然，自我实现的算法并不是与算法有效性相关的唯一问题。与这一概念相关的其他一些术语在此简要描述：

鲁棒性

算法只有在能够抵抗可预见的攻击或滥用，并且设计为限制或防止这种可预见的滥用时，才能发挥有效作用。

验证的性能

模型必须在其部署的使用案例中发挥作用。每当在新情况下部署模型时，如在新的人群中或者只是在长时间内应用时，都应检查模型的性能是否良好。

逻辑

尽管有些人因为这些模型可以“发现人类看不见的模式”而庆祝 ML，有时候识别出的模式却毫无意义，或者由于错误的原因而有意义。如果特定输入的相关逻辑不通过基本的“嗅觉测试”，那么现在是提出问题的时候了。要求一定程度的逻辑也与与效果相关的负责任 AI 目标保持一致。

## 社会和文化的适当性

用于负责任地使用 ML 的另一个考虑因素是技术在社会情境中或在社会影响方面的一般可接受性。机器作为观察者或决策者的所有角色是否都适用？例如，您想通过算法被告知将死于可怕的疾病吗？同样，您希望您的孩子被 ML 产品“监视”还是由人类保姆？也许答案是您不在乎，也许答案是您在乎。有时候，人们拒绝算法产品是因为与人类尊严或社会尊重相关的想法，而不是对算法产品的安全性问题。也就是说，仅仅因为某事可以很好地自动化，并不保证人们在与算法交互时会感到受到尊重。

关于人类尊严的这些文化关切可能适用于 ML 产品的用户，而不仅仅是对象。例如，考虑最近的 Twitter 负责任 AI 研究，在这项研究中，团队最终得出结论，使一个功能公平（在这种情况下是一个照片裁剪功能）的最佳方式是[删除](https://oreil.ly/YsWXs)这个功能，而不是完善它，部分原因是为了鼓励发布照片的人的自主权和代理权。有时候，最好的解决方案是移除一个技术“解决方案”。

# 负责任 AI 沿着 ML 管道

我们之前讨论过的与负责任 AI 相关的各种具体问题将在现实世界的 ML 管道中必然重叠。在本节中，我们包括特定的考虑点，涉及您应该根据您所处 ML 管道的位置向自己和团队提出的实际问题。

## 用例头脑风暴

如果您正在进行潜在用例的头脑风暴，也许是因为您看到一个新的商业机会或看到获取新数据的可能性，您应该考虑与您的潜在项目相关的以下基本问题：

+   这是一个可能会损害隐私的用例吗？如果是，您将如何从一开始就采取预防措施来构建隐私保护？

+   这是一个涉及到人类尊严或社会期望基本关注点的用例吗？这可能会对算法方法的适用范围增加额外的限制吗？

+   机器学习决策或分类是否是一个重要决策，公平性应特别受到保护，如果是，是否有迹象表明这可以实现？什么构成*重要*会有所不同，但有时这些被理解为具有法律效力或类似法律效果的决策（如招聘或教育）。另一种定义*重要*的方式是首先确定你自己组织中那些会对决策结果产生最重大影响的领域的模型。

## 数据收集和清洗

在管道的这一阶段，你已经确定了一个使用案例，并正在寻找数据并为建模做准备。现在，应解决以下问题：

+   数据是否以一种尊重知情同意的方式获取？²⁵ 你是否以合理的信息透明方式向受试者披露将使用数据的目的？

+   是否以促进隐私并最大程度减少意外披露的方式存储数据？

+   你是否进行了数据的探索性分析，以查找数据中的潜在偏差？

## 模型创建和训练

现在你已经收到数据，是时候进行一些建模了：

+   你是否制定了积极的计划来监测和解决偏见？你将如何根据偏见可能带来的潜在危害以及你的使用案例的特定规范价值或法律限制，选择各种公平干预形式？

+   你是否以一种方式进行训练，可以减少训练模型的数据泄漏，并增强对[恶意攻击](https://arxiv.org/abs/1412.6572)的鲁棒性？

+   在你的损失函数中，是否考虑了不同错误造成的相对危害程度，而不是懒惰地将所有错误都设置为相同的损失值？

+   在选择模型架构（例如，不透明的神经网络与可解释的线性函数）时，是否通过理解准确性和可解释性对你的特定应用的相对重要性来做出选择？

+   如果你的领域有类似具有强预测价值的科学法则，你是否在模型架构和培训选择中包含了这些领域知识？

## 模型验证和质量评估

在管道的这个阶段，你可能会收到一个被告知在准确性方面已经达到最佳的模型。你的工作是以理性的方式检查并决定是否批准此模型继续前进，或者将其送回进行训练：

+   你是否询问过模型是否是在代理上训练的，如果是，是否有数据可证明使用该代理的合理性？

+   你是否在现实挑战情况下，充分测试了模型，使用了公正选择的保留数据集？基本准确性仍然是道德义务，也是商业目标和技术指标。

+   你能否全局识别和理解推动模型的逻辑，并在需要时生成个体解释？（有关更多信息，请参见第九章。）

## 模型部署

现在是时候让模型按计划使用了：

+   你是否已经建立了监控程序，持续评估系统在实际使用中的性能？这样的性能评估可以根据传统性能指标（如准确性）以及公平性指标（如前面讨论的那样）进行评估。

+   你是否已经建立了事先的标准来评估模型是否按预期工作？²⁶

+   你是否尝试在线模式运行模型，以便您可以提前观察其如何表现，即使是在实际产品发布之前的反事实情况？或者您可以维护一个影子模型，在用户看不到结果的情况下对生产数据进行预测。这可以作为理解可能的部署性能的中间步骤，而无需承担实际部署所带来的风险。

## 市场产品

无论模型是用于内部还是外部使用，它们都需要符合公平性和隐私保护的同样标准。但是，直接由外部用户访问的模型通常有一组额外的必须满足的要求。

如果您的模型最终目标是直接由人类用户访问，请考虑以下几点：

+   你是否会提供救济或挑战决定的方法？如果会，具体是怎样的方法？

+   你是否会提供关于机器学习系统如何运行的解释，甚至可能为个人提供关于他们成果形成方式的具体指导？²⁷

+   你将如何检测到未预料到的事件或问题？你将如何了解你目前不知道的那些你不知道的内容？

# 结论

我们已经审查了许多涉及设计、训练和部署现实世界机器学习系统的公平性、隐私和其他伦理考量。这些话题都非常复杂。本章应该为您和您的组织提供了一些关键问题的示例，您应该在设计和部署机器学习系统时考虑这些问题。要进一步了解，您可以找到许多优秀的学习资料和学术研究论文，涵盖所有这些话题，供您获取更多信息。²⁸

与此同时，这种复杂性不应使您气馁！我们建议采取以下步骤：

+   在工作场所创建基本的机构规则和保障措施。

+   与计算技术相比，人类可读的变更可能更容易理解和更透明。

+   组织变革鼓励人们在会议上提出伦理关切。公平最好不要在黑暗角落中。试着举手提出你的公平关切。从小事情开始（数据访问、追索接口、统计平等），随着时间推移，你可以逐步提高到更复杂的目标。随着时间的推移，组织甚至可以更进一步，例如创建一个*红队*，有意识地和积极地尝试操纵产品或识别有害结果。

+   伦理指南也可以作为启动新项目或批准已完成产品发布的高层次但实用的检查清单。

+   在产品初始阶段定期解决公平、隐私和伦理问题。

+   大多数机器学习中的公平、隐私和其他伦理问题都源于产品的概念和创建阶段。在考虑一个项目是否甚至是一个好主意时，开始提出相关问题是非常重要的。

+   你是否在合法和道德的方式获得了公平的数据？来自[Gebru 等人 (2021)](https://oreil.ly/6Vo2R)的指导可以为记录你的数据及其适当使用提供一个很好的框架。

+   是否有适当的工作条件来维护数据的安全和隐私？

+   如果你成功实现你提出的目标，这对整个世界来说会是一个好事（或至少是中立的）吗？

+   创建负责任人工智能实践的良性循环。

+   学到的越多，保持负责任的人工智能关注的时间越长，你的机器学习管道和机器学习产品将反映良好的公平实践。

+   如果你能承诺每周学习一点关于负责任人工智能的内容，并且在自己的工作中慢慢实施它，在一年结束时你会注意到显著的进展。

祝你好运，因为你开始采取实际步骤，让世界变得更美好，从你的机器学习产品开始（有时，包括不使用它们）。

¹ 指定的人类决策者的例子包括法官作为法律裁决的正式决策者，或大学教授作为评分决策的正式决策者。没有明确决策者的例子包括“扁平”组织，在这些组织中有时不清楚谁持有最终决策权。

² 一般而言，我们对公平概念的不适当缩窄表示反对，并且为了明确沟通，我们解决了许多其他问题，我们认为这些问题与*公平*相关，在“负责任人工智能”部分中，我们重点讨论了作为与偏见相关的*公平*的主流用法。

³ [“机器偏见”](https://oreil.ly/38he8)由 Julia Angwin 等人撰写，是 ProPublica 发布的开创性研究，揭示了这种偏见。这项工作讨论了在美国刑事司法系统中使用算法风险评分的情况，对理解 AI 可能造成的伤害以及发起负责任 AI 运动具有基础性意义。文章讨论了在美国刑事司法系统中使用算法风险评分的情况。这些有偏见的风险评分最常用于确定在负担过重的刑事司法系统中，被告是否应该在等待审判期间获得自由。在这种情况下，被预测的因素应该是被告是否会出庭并且在此期间是否会犯罪，而不是一般将来的任何时候。目前尚不清楚“将来是否会被指控犯罪”的单一预测能否在保释、判刑和假释决策中有用。本文讨论了稍后在本章节中详细讨论的 COMPAS 算法。

⁴ 这是一些因素之一，导致一些政府，尤其是纽约市政府，提议对在招聘中使用的 AI 进行规范。参见 Tom Simonite 的[“纽约市提议规范招聘中使用的算法”](https://oreil.ly/xjN4P)。

⁵ 关于歧视是“品味导向”还是“统计导向”的辩论更为广泛。一些人认为决定就读女子学院可能在某种程度上表明与雇佣决策相关的人格类型。然而，这样的假设似乎不太可能适用。有兴趣了解更多的人可以查阅法律和经济学中关于歧视机制和动机的研究文献，包括不平等对待。

⁶ 我们在这个句子中使用引号，因为当然，定义“相同”或“同等的价值”的内容本身就是一种判断的行为，而不是客观的真理。

⁷ 为了清楚地展示校准与群体平等之间的冲突，强烈推荐阅读 Alexandra Chouldechova 的[“具有不同影响的公平预测：关于刑事再犯预测工具偏见的研究”](https://arxiv.org/abs/1610.07524)。

⁸ 参见上文脚注中提到的[Chouldechova 文章](https://arxiv.org/abs/1610.07524)。

⁹ 如果我们广泛寻找和解释关于算法公平性的问题，其他不可能定理与公平性关切相关。例如，阿罗不可能定理表明，在特定形式的投票中，三个直观的公平性标准不能同时满足。这三个标准，粗略地说，分别是：（1）在两个选项之间普遍共享的个人偏好必然会转化为反映该偏好的选举结果，（2）稳定的个人偏好会导致稳定的选举结果，以及（3）没有单一选民会拥有决定选举结果的权力。因此，无论是跨学科还是技术或组织机制，我们并非总能在公平性上一举两得。非常感谢 Niall Murphy 提供的这个例子。

¹⁰ 例如，来自受青睐群体的个人，他们由于他们的优点而获得了难以置信的有利结果。

¹¹ 有时这可以理解为校正缺乏校准的方式。例如，有充分证据表明，在美国使用的信用评分在不同种族群体中的还款意愿方面似乎并不相同。因此，完全依赖评分而不调整阈值的银行可能会无意中标记出不同种族群体的信用申请者存在不同的假阴性率。然而，为不同群体设定不同的阈值必然引起其他法律和道德上的关切。因此，读者可以看到，在现实世界中，公平性确实是非常具有挑战性的。

¹² 在本书即将出版的时刻（2022 年 8 月），欧洲已经针对数字环境中公平性等一系列问题提出了一系列开创性的立法。尤其值得注意的是，[欧盟于 2021 年 4 月发布了一项提议的 AI 监管框架](https://oreil.ly/b5DZn)，该框架将实施基于风险的一系列规定，不同的 AI 使用案例将有不同的法律要求。该提议法律还将禁止某些高风险使用情形，包括社会信用评分。同样地，[中国于 2022 年 3 月实施了全面的新 AI 法规](https://oreil.ly/U5xVq)，以规范包括个人信息价格歧视和内容聚合算法在内的多种常见损害消费者利益的做法。

¹³ 早期的重新识别案例可以参见 Nate Anderson 的[“‘Anonymized’ Data Really Isn’t—And Here’s Why Not”](https://oreil.ly/7nFhr)，描述了 Latanya Sweeney 在这一主题上的工作。

¹⁴ 多次查询差分私密数据集可能会导致违反差分隐私保证。这是因为这些保证仅在给定的隐私预算下有效，该预算限制了可以进行的查询次数，同时确保差分隐私保证。

¹⁵ 理论上，我们可以在每次将培训数据标记为删除时完全重新训练模型。实际上，这将是极其浪费和不切实际的——也可能是不必要的。在这个领域还需要做更多工作，以了解技术需求、可持续性问题和个人隐私权如何成功共存。

¹⁶ 差分隐私模式未能达到最高精度水平的一个原因与用于创建差分隐私模型的方法有关。一个关键技术是注入噪音，这从定义上来说会降低准确性。

¹⁷ 一个推荐的起点是[TensorFlow Privacy](https://github.com/tensorflow/privacy)，其中包括差分私密模型的训练算法。

¹⁸ 单独的问题是，当点击预测是一个有用的活动，而当它引起令人担忧的 AI 问题时。毫无疑问，预测某人的偏好或兴趣具有益处，就像许多点击预测的例子一样。另一方面，关于人类偏好的可变性以及在线环境中行为和偏好操纵的具体场景的增加研究，也指出了混淆代表偏好*适应*（给予人们他们想要的东西）与偏好*操纵*（让人们想要你所拥有的东西）的危险。后者显然存在大量的公平性问题，并且与关于*黑暗模式*的增长文献有关，这些是数字设计模式，倾向于引导数字产品的用户做违背他们利益的事情（但符合那些产品设计者的利益）。

¹⁹ 当然，包含人口统计信息可能会带来问题，因为一些数据集可能在看似匿名信息与 PII 之间具有强烈的预测关系，这应该考虑在数据分离方式中。

²⁰ 了解负责 AI 的可解释性的核心性的一个好的起点，但同时也要理解把握正确的复杂性，是 Brent Mittelstadt 等人的[“在 AI 中解释解释”](https://oreil.ly/OOeoT)。

²¹ 由 Kasun Amarasinghe 等人撰写的[“公共政策中的可解释机器学习：用例、空白和研究方向”](https://arxiv.org/pdf/2010.14374.pdf)提供了一个极好且易于访问的示例，涉及哪些解释类型可能是有用的考虑因素。

²² IBM 的[AI 可解释性 360 库](https://aix360.mybluemix.net)是一个易于使用的开源库，包括多种前沿研究方法用于模型解释。该工具包提供了 API 以及通过可解释性 API 应用库方法的多种示例用例教程。

²³ 以波兰将分类算法推广至失业求职者分类为极端案例。理论上，工作人员可以偏离初步的算法评估。实际上，他们在 0.58%的情况下偏离算法标签。详见 Jędrzej Niklas 等人的[“波兰失业人员概况”](https://oreil.ly/eBvcv)。

²⁴ 法案的完整引用如下：“如果在线平台采用或以其他方式利用算法过程，且已采取合理措施确保算法过程能够产生其预期或预期结果，则算法过程有效。”

²⁵ 值得注意的是，定义*同意*可能会很复杂。毕竟，没有人喜欢欧盟一般数据保护条例（GDPR）后广泛出现的所有弹出同意通知。在这里，*同意*应该以广义而非狭义的词义来解释。

²⁶ 最好事先明确这些，这样你就不能在事后“作弊”或以其他方式合理化表现不佳。这不一定要严格遵循这些标准，但即使随时间重新评估这些标准，它也会使你对什么构成良好工作保持诚实。

²⁷ 如前所述，模型卡是实现这一目标的一种系统。

²⁸ 作为毫不掩饰的自我推广，可以考虑阅读 Aileen Nielsen 的关于实用 ML 伦理的书籍长篇介绍：[*实用公平性*](https://oreil.ly/tsjGP)，另一本 O'Reilly 出版的书籍。
