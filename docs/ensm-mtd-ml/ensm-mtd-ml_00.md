# 前置内容

## 前言

从前，我是一名研究生，在充满不尽人意的科研方向和不确定的未来中迷失方向。然后，我偶然发现了一篇题为“支持向量机：炒作还是赞美？”的杰出文章。在 21 世纪初，支持向量机（SVMs）当然是当时领先的机器学习技术。

在这篇文章中，作者们（其中一位后来成为我的博士导师）采取了一种相当简约的方法来解释相当复杂的话题——SVMs，将直觉和几何与理论和应用交织在一起。这篇文章给我留下了深刻的印象，它不仅点燃了我对机器学习终身的好奇心，还让我着迷于理解这些方法在底层是如何工作的。确实，第一章的标题是对那篇对我的生活产生深远影响的文章的致敬。

就像 SVMs 一样，集成方法今天被广泛认为是领先的机器学习技术。但许多人没有意识到的是，几十年来，某种集成方法或另一种方法一直被认为是最佳水平：20 世纪 90 年代的 bagging，21 世纪初的随机森林和 boosting，2010 年代的梯度提升，以及 2020 年代的 XGBoost。在最佳机器学习模型不断变化的世界上，集成方法似乎确实值得这样的炒作。

我很幸运，在过去十年中，我训练了许多种集成模型，将它们应用于工业，并撰写了关于它们的学术论文。在这本书中，我试图展示尽可能多的这些集成方法：一些你肯定听说过的，以及一些你应该真正了解的新方法。

这本书的初衷并不是仅仅是一本带有逐步指令和剪切粘贴代码的教程（尽管你也可以这样使用它）。网上有数十个这样的优秀教程，它们可以让你瞬间开始处理你的数据集。相反，我使用一种沉浸式的方法来讨论每种新的方法，这种方法灵感来源于我第一次阅读的那篇机器学习论文，并在大学课堂中作为研究生讲师期间进行了完善。

我一直觉得，要深入理解一个技术主题，将其拆解、拆分并尝试重新组合是很有帮助的。在这本书中，我采用了同样的方法：我们将拆解集成方法并（重新）自己创造它们。我们将调整它们并探究它们的变化。通过这样做，我们将确切地看到是什么让它们运转！

我希望这本书能帮助你揭开那些技术和算法细节的神秘面纱，并让你进入集成思维模式，无论是为了你的课堂项目、Kaggle 竞赛，还是生产质量的软件应用。

## 致谢

我从未想过一本关于集成方法的书籍本身会变成一个由家人、朋友、同事和合作者组成的集成努力，他们从构思到完成都与这本书有很大关系。

致 Brian Sawyer，感谢你让我提出这本书的想法，感谢你相信这个项目，感谢你的耐心，以及保持我按计划进行：感谢你给我这个机会去做我一直想做的事情。

致我的第一位发展编辑 Katherine Olstein、第二位发展编辑 Karen Miller 和技术发展编辑 Alain Couniot：当我开始时，我对这本书的样貌有一个愿景，你们帮助让它变得更好。感谢你们细致入微的审阅，感谢你们敏锐的编辑，以及你们总是挑战我成为一个更好的作家。你们的努力与这本书的最终质量有很大关系。

致 Manish Jain：感谢你逐行仔细校对代码。致 Marija Tudor：感谢你设计这个绝对出色的封面（我认为这是这本书最好的部分），按照我的要求将其设计成橙色，并且从封面到封底进行了排版。感谢 Manning 出版社的校对和生产团队：感谢你们卓越的工艺——这本书看起来完美——审稿编辑 Mihaela Batinic、生产编辑 Kathleen Rossland、校对编辑 Julie McNamee 和校对员 Katie Tennant。

致我的审稿人，Al Krinker、Alain Lompo、Biswanath Chowdhury、Chetan Saran Mehra、Eric Platon、Gustavo A. Patino、Joaquin Beltran、Lucian Mircea Sasu、Manish Jain、McHugson Chambers、Ninoslav Cerkez、Noah Flynn、Oliver Korten、Or Golan、Peter V. Henstock、Philip Best、Sergio Govoni、Simon Seyag、Stephen John Warnett、Subhash Talluri、Todd Cook 和 Xiangbo Mao：感谢你们精彩的反馈，以及一些真正出色的洞察和评论。我尽量吸收了你们的所有建议（我真的这么做了），其中许多已经融入到这本书中。

致在早期访问期间阅读这本书并留下许多评论、更正和鼓励话语的读者——你知道你是谁——感谢你的支持！

致我的导师，Kristin Bennett、Jong-Shi Pang、Jude Shavlik、Sriraam Natarajan 和 Maneesh Singh，他们在我的学生、博士后、教授和专业人士的不同阶段深刻地塑造了我的思考：感谢你们教我如何用机器学习思考，如何用机器学习说话，以及如何用机器学习构建。你们的大部分智慧和许多教训都体现在这本书中。Kristin，我希望你喜欢第一章的标题。

致 Jenny 和 Guilherme de Oliveira，感谢你们多年来建立的友谊，尤其是在这场大流行期间，当时这本书的大部分内容都在撰写中：感谢你们让我保持理智。我永远珍视我们 2020 年那个夏天和秋天的下午和晚上，在你们的小后院里，我们的避难所。

致我的父母，Vijaya 和 Shivakumar，以及我的兄弟，Anupam：感谢你们一直相信我，并一直支持我，即使相隔数万公里。我知道你们为我感到骄傲。这本书终于完成了，现在我们可以做那些我们一直谈论的其他事情了……至少在我开始写下一本书之前。

致我的妻子、最好的朋友和最大的支持者，Kristine：你是我无尽的安慰和鼓励的源泉，尤其是在事情变得艰难的时候。感谢你和我一起头脑风暴，感谢你和我一起校对，感谢茶点和零食，感谢 Gus，感谢你牺牲所有周末（有时甚至是工作日夜晚）来陪我写作。感谢你一直支持我，一直在我身边，从未怀疑过我能做到这一点。我爱你！

## 关于本书

学习集成方法从未有过更好的时机。本书涵盖的模型分为三大类：

+   *基础集成方法*——大家耳熟能详的经典方法，包括历史性的集成技术，如 Bagging、随机森林和 AdaBoost

+   *最先进的集成方法*——现代集成时代的经过检验的强大工具，是许多真实世界、在生产中的预测、推荐和搜索系统的核心

+   *新兴集成方法*——最新方法，刚刚从研究熔炉中出炉，用于处理新的需求，如可解释性和可理解性

每一章都将介绍不同的集成技术，采用三管齐下的方法。首先，通过逐步可视化学习过程，你将了解每个集成方法的*直觉*。其次，你将*实现*每个集成方法的基本版本，以全面理解算法的细节。第三，你将学习如何实际应用强大的集成库和工具。

大多数章节还附带了自己在真实世界数据上的案例研究，这些数据来自手写数字预测、推荐系统、情感分析、需求预测等领域。这些案例研究在适当的时候解决了几种真实世界问题，包括预处理和特征工程、超参数选择、高效训练技术和有效模型评估。

### 应该阅读这本书的人

本书面向广泛的读者：

+   对使用集成方法从数据中获取最佳实际应用效果感兴趣的数据科学家

+   构建、评估和部署基于集成、生产就绪的应用程序和管道的 MLOps 和 DataOps 工程师

+   希望将本书作为学习资源或作为补充教科书的实际参考的数据科学和机器学习的学生

+   使用本书作为学习关于使用集成方法探索无限建模可能性的入门点的 Kagglers 和数据科学爱好者

本书**不是**机器学习和数据科学的入门书。本书假设你有一些基本的机器学习工作知识，并且你已经使用或尝试过至少一种基本的学习技术（例如，决策树）。

假设读者具备基本的 Python 工作知识。示例、可视化和章节案例研究都使用 Python 和 Jupyter Notebooks。了解其他常用 Python 包，如 NumPy（用于数学计算）、pandas（用于数据处理）和 Matplotlib（用于可视化）是有用的，但不是必需的。实际上，你可以通过示例和案例研究来学习如何使用这些包。

### 本书是如何组织的：一个路线图

本书分为三部分，共九章。第一部分是对集成方法的温和介绍，第二部分介绍并解释了几个重要的集成方法，第三部分涵盖了高级主题。

第一部分，“集成的基础”，介绍了集成方法以及为什么你应该关注它们。本部分还包含本书其余部分涵盖的集成方法的路线图：

+   第一章讨论了集成方法和基本集成术语。它还介绍了拟合与复杂度权衡（或称为偏差-方差权衡，更为正式的称呼）。你将在本章构建你的第一个集成。

第二部分，“基本集成方法”，涵盖了几个重要的集成方法家族，其中许多被认为是“基本”的，并且在现实世界的应用中得到了广泛使用。在每一章中，你将学习如何从头开始实现不同的集成方法，了解它们的工作原理以及如何将它们应用于现实世界问题：

+   第二章以并行集成方法开始我们的旅程，具体来说是并行同质集成。涵盖的集成方法包括 bagging、随机森林、pasting、随机子空间、随机补丁和 Extra Trees。

+   第三章继续旅程，引入了更多的并行集成，但本章的重点在于并行异构集成。涵盖的集成方法包括通过多数投票组合基础模型、通过加权组合、Dempster-Shafer 预测融合以及通过堆叠进行元学习。

+   第四章介绍了另一类集成方法——顺序自适应集成，特别是将许多弱模型提升为一个强大模型的基本概念。涵盖的集成方法包括 AdaBoost 和 LogitBoost。

+   第五章建立在提升的基础概念之上，并涵盖了另一个基本的顺序集成方法——梯度提升，它将梯度下降与提升相结合。本章讨论了如何使用 scikit-learn 和 LightGBM 训练梯度提升集成。

+   第六章继续探讨使用牛顿提升的顺序集成方法，牛顿提升是梯度提升的一个高效且有效扩展，它结合了牛顿下降和提升。本章讨论了如何使用 XGBoost 训练牛顿提升集成。

第三部分，“野外的集成：将集成方法应用于您的数据”，展示了如何将集成方法应用于许多场景，包括具有连续和计数值标签的数据集以及具有分类特征的数据集。您还将学习如何解释您的集成并解释其预测：

+   第七章展示了我们如何为不同类型的回归问题和广义线性模型训练集成，其中训练标签是连续值或计数值。本章涵盖了线性回归、泊松回归、伽马回归和 Tweedie 回归的并行和顺序集成。

+   第八章确定了使用非数值特征学习时的挑战，特别是分类特征和编码方案，这些方案将帮助我们为这类数据训练有效的集成。本章还讨论了两个重要的实际问题：数据泄露和预测偏移。最后，我们将看到如何使用有序提升和 CatBoost 克服这些问题。

+   第九章从集成方法的角度覆盖了新兴且非常重要的可解释人工智能（Explainable AI）主题。本章介绍了可解释性的概念以及为什么它很重要。还讨论了几种常见的黑盒可解释性方法，包括排列特征重要性、部分依赖图、代理方法、局部可解释模型无关解释（Locally Interpretable Model-Agnostic Explanation）、Shapley 值和 SHapley 加性解释（SHapley Additive exPlanations）。还介绍了玻璃盒集成方法、可解释提升机（Explainable Boosting Machines）和 InterpretML 包。

+   序言以进一步探索和阅读的额外主题结束了我们的旅程。

虽然本书的大部分章节可以独立阅读，但第七章、第八章和第九章建立在本书的第二部分基础上。

### 关于代码

本书中的所有代码和示例都是用 Python 3 编写的。代码组织成 Jupyter Notebooks，可在在线 GitHub 仓库（[`github.com/gkunapuli/ensemble-methods-notebooks`](https://github.com/gkunapuli/ensemble-methods-notebooks)）和 Manning 网站（[www.manning.com/books/ensemble-methods-for-machine-learning](https://www.manning.com/books/ensemble-methods-for-machine-learning)）上下载。您可以从本书的 liveBook（在线）版本中获取可执行的代码片段，网址为[`livebook.manning.com/book/ensemble-methods-for-machine-learning`](https://livebook.manning.com/book/ensemble-methods-for-machine-learning)。

本书还使用了几个 Python 科学和可视化库，包括 NumPy ([`numpy.org/`](https://numpy.org/))、SciPy ([`scipy.org/`](https://scipy.org/))、pandas ([`pandas.pydata.org/`](https://pandas.pydata.org/))和 Matplotlib ([`matplotlib.org/`](https://matplotlib.org/))。代码还使用了几个 Python 机器学习和集成方法库，包括 scikit-learn ([`scikit-learn.org/stable/`](https://scikit-learn.org/stable/))、LightGBM ([`lightgbm.readthedocs.io/`](https://lightgbm.readthedocs.io/))、XGBoost ([`xgboost.readthedocs.io/`](https://xgboost.readthedocs.io/))、CatBoost ([`catboost.ai/`](https://catboost.ai/))和 InterpretML ([`interpret.ml/`](https://interpret.ml/))。

本书包含许多源代码示例，既有编号列表中的，也有与普通文本并行的。在这两种情况下，源代码都使用固定宽度字体格式化，如这样，以将其与普通文本区分开来。在许多情况下，原始源代码已被重新格式化；我们添加了换行并重新调整了缩进，以适应书中的可用页面空间。此外，当代码在文本中描述时，源代码中的注释通常已被从列表中删除。许多列表都伴随着代码注释，突出显示重要概念。

### liveBook 讨论论坛

购买《机器学习集成方法》包括免费访问 Manning 的在线阅读平台 liveBook。使用 liveBook 的独家讨论功能，您可以在全球范围内或针对特定章节或段落添加评论。为自己做笔记、提问和回答技术问题以及从作者和其他用户那里获得帮助都非常简单。要访问论坛，请访问[`livebook.manning.com/book/ensemble-methods-for-machine-learning/discussion`](https://livebook.manning.com/book/ensemble-methods-for-machine-learning/discussion)。您还可以在[`livebook.manning.com/discussion`](https://livebook.manning.com/discussion)了解更多关于 Manning 论坛和行为准则的信息。

Manning 对我们读者的承诺是提供一个场所，让读者之间以及读者与作者之间可以进行有意义的对话。这不是对作者参与特定数量活动的承诺，作者对论坛的贡献仍然是自愿的（且未付费）。我们建议您尝试向作者提出一些挑战性的问题，以免他的兴趣转移！只要本书有售，论坛和先前讨论的存档将可通过出版社的网站访问。

## 关于作者

![FM_UN01_Kunapuli](img/FM_UN01_Kunapuli.png)

戈塔姆·库纳普利在学术界和机器学习行业拥有超过 15 年的经验。他的工作专注于人机交互学习、基于知识和采纳建议的学习算法，以及针对困难机器学习问题的可扩展学习。戈塔姆为多个应用领域开发了几个新颖的算法，包括社交网络分析、文本和自然语言处理、计算机视觉、行为挖掘、教育数据挖掘、保险和金融分析以及生物医学应用。他还发表了关于关系域和失衡数据中集成方法的论文。

## 关于封面插图

《机器学习集成方法》封面上的图像是“胡安或中国音乐家”，或“胡安或中国音乐家”，来自雅克·格拉塞·德·圣索沃尔的收藏，1788 年出版。每一幅插图都是手工精细绘制和着色的。

在那些日子里，仅凭人们的服饰就能轻易识别出他们居住的地方以及他们的职业或社会地位。曼宁通过基于几个世纪前丰富多样的地域文化的书封面来庆祝计算机行业的创新精神和主动性，这些文化通过如这一系列图片的图片被重新带回生活。
