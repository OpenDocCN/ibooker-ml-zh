- en: 8 Baseline solution
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 基准解决方案
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: What is the baseline?
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是基准？
- en: Constant baselines
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 恒定基准
- en: Model baselines and feature baselines
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型基准和特征基准
- en: A variety of deep learning baselines
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种深度学习基准
- en: Baseline comparison
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基准比较
- en: Everything should be made as simple as possible, but not simpler.— Albert Einstein
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一切都应该尽可能简单，但不能过于简单。—— 阿尔伯特·爱因斯坦
- en: 'When we start to think about the building blocks of our future machine learning
    (ML) system, the essential part of it, or core component of it, seems to be a
    model built using ML techniques. In some sense, this is so true that we may even
    think that “this is it: this is the primary point where I should spend most of
    my time, energy, and creative power.”'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始思考我们未来机器学习（ML）系统的构建块时，它的核心部分，或者说核心组件，似乎是一个使用机器学习技术构建的模型。在某种程度上，这是如此真实，以至于我们甚至可能会认为：“这就是它：这是我应该花费大部分时间、精力和创造力的主要点。”
- en: But in reality, this may turn out to be a trap that the majority of ML projects
    fall into and get bogged down in without ever reaching production. An ML model
    is not necessarily the most important thing in the context of an ML system and
    its design document. Although the temptation is great, you should always keep
    in mind that it is extremely easy to spend a lot of time, team effort, and, more
    importantly, money on building a cool, modern, and sophisticated AI model that
    doesn’t ever bring any value to users and your company. A mediocre model in production
    is usually better than a great model on paper.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但在现实中，这可能会变成大多数机器学习项目陷入的陷阱，它们在从未达到生产阶段的情况下陷入困境。在机器学习系统的背景下，机器学习模型并不一定是最重要的东西，以及其设计文档。尽管诱惑很大，但你应该始终记住，花费大量时间、团队努力，更重要的是，金钱来构建一个酷、现代且复杂的AI模型，而这个模型却从未为用户和你的公司带来任何价值。在生产中的平庸模型通常比纸上的优秀模型要好。
- en: One of the first versions of this book’s title was *Machine Learning System
    Design That Works*, and it corresponds to the primary goal of any ML project,
    which is to build a system that will work; only then, when it brings profit, will
    we start to iteratively improve it while gradually increasing its complexity (if
    needed). In this chapter, we will discuss the baseline solution, the first step
    in bringing our system to life. We will cover why baselines are needed, as well
    as the purpose of building them. We will go all the way from constant baselines
    to sophisticated specialized models and also through various feature baselines.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书最初的一个版本标题是《有效的机器学习系统设计》，这与任何机器学习项目的首要目标相对应，即构建一个能够工作的系统；只有当它带来利润时，我们才会开始迭代改进它，逐渐增加其复杂性（如果需要）。在本章中，我们将讨论基准解决方案，这是使我们的系统生命化的第一步。我们将讨论为什么需要基准，以及构建它们的目的。我们将从恒定基准到复杂的专用模型，以及各种特征基准进行探讨。
- en: '8.1 Baseline: What are you?'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 基准：你是谁？
- en: 'A baseline is the simplest possible (but working!) version of a model, feature
    set, or anything else in your system. It’s the minimum viable product (MVP) in
    the world of ML systems that brings value from the start without yet diving into
    complexity. Let’s elaborate a bit more on the MVP analogy by outlining key goals
    that may equally apply to both:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 基准是系统中最简单（但可行！）的模型、特征集或其他任何东西的版本。在机器学习系统世界中，它是最小可行产品（MVP），从一开始就带来价值，而尚未深入复杂性。让我们通过概述可能同样适用于两者的关键目标来进一步阐述MVP类比：
- en: '*Reduce the maximum risk with the lowest amount of time, cost, and effort invested
    in a product*. At the beginning of the product’s life, it is still unclear whether
    the market needs it, what use cases the product will have, whether the economy
    will converge, and so on. To a large extent, these risks are peculiar to ML products,
    too. In a way, a baseline (or MVP) is the easiest way to test a hypothesis that
    lies at the heart of your product.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*以最低的时间、成本和努力投入，降低产品的最大风险*。在产品的生命初期，市场是否需要它，产品将有哪些用例，经济是否会收敛等等，这些都还不清楚。在很大程度上，这些风险也特属于机器学习产品。从某种意义上说，基准（或MVP）是测试产品核心假设的最简单方法。'
- en: '*Get early feedback*. This is the fail-fast principle cut down to the product
    scale. If the whole idea of your ML system is wrong, you can see it at an early
    stage, rethink the entire plan, rewrite the design document with new knowledge,
    and start anew.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*获取早期反馈*。这是将快速失败原则缩小到产品规模。如果你的机器学习系统的整个想法是错误的，你可以在早期阶段看到，重新思考整个计划，用新的知识重新编写设计文档，并重新开始。'
- en: '*Bring user value as soon as possible*. Each company aims to generate revenue
    by making its customers happy. If we can bring value to customers early with a
    baseline and then update it stage by stage while generating a predictable amount
    of money, why not do this? It will leave everyone in the equation satisfied.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*尽快带来用户价值*。每个公司都希望通过让客户满意来创造收入。如果我们可以通过基线尽早为顾客带来价值，然后在逐步生成可预测收入的同时更新它，为什么不这样做呢？这将使方程中的每个人都很满意。'
- en: 'These three points form the grand basis of similarities between a baseline
    and an MVP. However, there are three more purely baseline-specific goals:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个点构成了基线和MVP之间相似性的基础。然而，还有三个纯粹是基线特定的目标：
- en: '*A placeholder to check that components work properly*—Baselines are like smoke
    tests. As Cem Kaner, James Bach, and Brett Pettichord once said in their *Lessons
    Learned in Software Testing*, the phrase “smoke test” comes from electronic hardware
    testing. You plug in a new board and turn on the power. If you see smoke coming
    from the board, turn off the power. You don’t have to do any more testing.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一个检查组件是否正常工作的占位符*——基线就像烟雾测试。正如Cem Kaner、James Bach和Brett Pettichord在他们的《软件测试经验教训》一书中所说，“烟雾测试”这个短语来自电子硬件测试。你插入一块新板并打开电源。如果你看到板子上冒烟，就关掉电源。你不需要做更多的测试。'
- en: First you want to check whether the system works and, second, whether it works
    correctly. To “compile” the whole system, you don’t need a powerful ML model.
    You need something that predicts something with the required format, optionally,
    based on something. Why not choose the easiest possible alternative?
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，你需要检查系统是否工作，其次，它是否正确工作。要“编译”整个系统，你不需要一个强大的机器学习模型。你需要的是能够以所需格式预测某些东西的东西，可选地，基于某些东西。为什么不选择最简单可能的替代方案呢？
- en: '*A thing to compare with*—Shall we go further and think about how much our
    investment in the model could pay in the future? The baseline is a “base line.”
    It is the origin of the coordinate plane—something we compare new models with
    in terms of some metrics.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一个用于比较的东西*——我们是否可以进一步思考我们对模型的投资在未来能带来多少回报？基线是一个“基线”。它是坐标平面的起点——我们在某些指标上将其与新模型进行比较。'
- en: When working in the industry, the model’s performance is not the only metric
    by which we compare models. Others require effort, interpretability, maintainability,
    and so on. We’ll cover them in section 8.5\.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在工业界工作的时候，模型的性能并不是我们比较模型的唯一指标。其他方面还需要努力、可解释性、可维护性等等。我们将在第8.5节中讨论它们。
- en: '*A fallback answer*—Unlike MVP, when we move on to its second and subsequent
    versions, we don’t throw out the baseline completely. It is good practice when
    it lives in parallel with the sophisticated model. The system switches to the
    baseline response when this primary model goes south while making a prediction.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一个回退答案*——与MVP不同，当我们继续进行其第二版和后续版本时，我们不会完全丢弃基线。当它与复杂的模型并行存在时，这是一种良好的实践。当这个主要模型在预测时出现问题时，系统会切换到基线响应。'
- en: 'So what are the advantages of a well-chosen baseline? Simplicity automatically
    brings a lot of pros with it: it is robust, not prone to unexpected behavior and
    overfitting (due to fewer degrees of freedom), easy to build and maintain, and
    not too pressing on computing resources. Consequently, baselines are easy to scale.
    As an additional bonus, from non-ML colleagues’ perspective, simple models are
    easier to interpret and make it less difficult to understand what is going on
    under the hood. It can help increase trust in our ML product, which can be critical
    when the stakes are high. However, simplicity is not a goal by itself but a valuable
    property.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，一个精心选择的基线有什么优势呢？简单性自动带来很多优点：它稳健，不易出现意外行为和过拟合（由于自由度较少），易于构建和维护，对计算资源的需求不是太高。因此，基线易于扩展。作为额外的奖励，从非机器学习同事的角度来看，简单的模型更容易解释，并使理解底层发生的事情变得更加容易。这有助于增加对我们机器学习产品的信任，这在风险很高时可能是关键的。然而，简单性本身并不是目标，而是一个有价值的属性。
- en: If we think of our ML system as a Lego model, a baseline is an opportunity to
    assemble all the other blocks as fast as possible. Still, we encourage you to
    make your system as modular (i.e., “orthogonal”) as possible by design. This will
    make later updates easier, including the transition to more complex models and
    features (initial design doesn’t dictate how fast you can update a system in the
    future). The initial system should be simple and agile, not trivial or restricted,
    with a baseline.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将我们的机器学习系统比作乐高模型，基线就是一个尽可能快地组装其他所有模块的机会。尽管如此，我们仍然鼓励您通过设计使您的系统尽可能模块化（即，“正交”）。这将使后续的更新更加容易，包括过渡到更复杂的模型和功能（初始设计并不决定您未来更新系统速度的快慢）。初始系统应该是简单且灵活的，而不是微不足道的或受限的，并包含基线。
- en: 'Still, despite all the advantages baselines provide without requiring a lot,
    they are not used as often as they deserve. The bitter truth is that, unfortunately,
    complexity sells better. There is a brilliant article from Eugene Yan that we
    strongly recommend reading. It’s called “Simplicity Is an Advantage But Sadly
    Complexity Sells Better” ([https://eugeneyan.com/writing/simplicity/](https://eugeneyan.com/writing/simplicity/)),
    highlighting the main reasons why many choose complexity over simplicity, which
    are:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基线提供了许多优势，而且不需要太多，但它们的使用频率并没有像应有的那样高。不幸的真相是，复杂性更受欢迎。Eugene Yan有一篇出色的文章，我们强烈推荐阅读。它叫做“简单是一种优势，但遗憾的是复杂性更受欢迎”([https://eugeneyan.com/writing/simplicity/](https://eugeneyan.com/writing/simplicity/))，强调了很多人选择复杂性而不是简单的主要原因，包括：
- en: Complexity signals effort.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复杂性表明了努力。
- en: Complexity signals mastery.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复杂性表明了精通。
- en: Complexity signals innovation.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复杂性表明了创新。
- en: Complexity signals more features.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复杂性表明了更多功能。
- en: This leads to complexity bias, where we give undue credit to and favor complex
    ideas and systems over simpler solutions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了复杂性偏差，我们过度赞扬并偏爱复杂的思想和系统，而不是更简单的解决方案。
- en: 'Of course, baselines are not a silver bullet, and there are reasonable cases
    when baselines are not necessary or are even irrelevant:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，基线不是万能的灵丹妙药，在某些情况下，基线可能不是必需的，甚至是不相关的：
- en: '*Accuracy is crucial*. In many cases, an error of a couple of percentage points
    will not even be noticed. But if we can’t afford decreased quality—for example,
    in some medical applications like cancer detection or when dealing with autonomous
    cars—a baseline will be a bad life-saving rope. In this case, explicit switching
    to manual control would be a better idea.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*准确性至关重要*。在许多情况下，几个百分点的错误甚至不会被注意到。但如果我们无法承受质量的下降——例如，在某些医疗应用中，如癌症检测或处理自动驾驶汽车时——基线将是一个糟糕的救命绳索。在这种情况下，明确切换到手动控制可能是一个更好的主意。'
- en: '*There is a high degree of certainty*. We clearly understand what the user
    wants (for example, based on a competitor’s experience), or we have our own experience
    of implementing identical systems. In this case, we don’t need to reinvent the
    wheel and waste our time on gradual iterations if we have a plan already proven
    in battle and can just copy-paste the system.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*确定性很高*。我们清楚地了解用户的需求（例如，基于竞争对手的经验），或者我们有实施相同系统的自身经验。在这种情况下，如果我们已经有了在实战中证明有效的计划，并且可以简单地复制粘贴系统，我们就不需要重新发明轮子，也不需要在逐步迭代上浪费时间。'
- en: '*We are rebuilding an already-working system*. Suppose we already have a working
    search engine based on the deep semantic similarity model (DSSM) architecture.
    The whole pipeline is already implemented and tested. So when it consistently
    brings value to users, it is a good time for optimization in terms of speed and
    accuracy—for example, by switching to a Transformer-based model. It is not the
    right place to think about baselines because the old version is effectively a
    baseline.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*我们正在重建一个已经工作的系统*。假设我们已经有了一个基于深度语义相似性模型（DSSM）架构的工作搜索引擎。整个流程已经实现并经过测试。因此，当它持续为用户提供价值时，就是从速度和准确性方面进行优化的时候——例如，通过切换到基于Transformer的模型。这不是考虑基线的正确地方，因为旧版本实际上就是一个基线。'
- en: Still, we believe that even if complexity at an early stage can be justified
    in certain cases, it can’t be the go-to solution by default because it incentivizes
    people to make things unnecessarily complicated; it encourages the “not invented
    here” mindset, where people prefer to build from scratch and avoid reusing existing
    components even though it saves time and effort, and it wastes time and resources
    while often leading to poorer outcomes.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们相信，尽管在某些情况下，早期的高复杂性可以找到合理的理由，但它不能成为默认的解决方案，因为它会激励人们使事情变得不必要地复杂；它鼓励“没有发明在这里”的心态，人们宁愿从头开始构建，即使这样做可以节省时间和精力，也会浪费时间和资源，而且往往会导致较差的结果。
- en: That is why we believe a baseline solution is the first thing to do, with incremental
    improvements where and when needed.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正因如此，我们相信基线解决方案是首先要做的事情，在需要和可能的地方进行逐步改进。
- en: 8.2 Constant baselines
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 常数基线
- en: 'A good metaphor for a baseline is building a bridge: sometimes you don’t need
    a team of bridge construction engineers, huge budgets, plans, or years to build
    it. Sometimes you just need a stably fixed log. A baseline is the very log that
    allows you to connect components and solve a given task at a minimal scale—in
    the case of a baseline, a temporary, primitive, easy-to-build solution (see figure
    8.1).'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 基线的良好隐喻是建造一座桥梁：有时你不需要一支桥梁建筑工程师团队、巨大的预算、计划或数年时间去建造它。有时你只需要一个稳定固定的日志。基线就是那个允许你以最小规模连接组件并解决给定任务的日志——在基线的情况下，一个临时、原始、易于构建的解决方案（见图8.1）。
- en: 'The idea we want to convey before going into detail is simple: build a lean,
    operable ML system first, and improve it later. Think of the complexity of possible
    solutions as a continuum. Choose an appropriate initial point in this range based
    on the effort–accuracy tradeoff, and move ahead. Don’t spend too much time on
    modeling unless it’s necessary.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细说明之前，我们想要传达的想法很简单：首先构建一个精简、可操作的机器学习系统，然后再对其进行改进。将可能的解决方案的复杂性视为一个连续体。根据努力-准确性权衡，在这个范围内选择一个合适的初始点，然后继续前进。除非必要，否则不要在建模上花费太多时间。
- en: '![figure](../Images/CH08_F01_Babushkin.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F01_Babushkin.png)'
- en: Figure 8.1 Before building a complicated model, start with a primitive baseline,
    which may well be the most appropriate foundation for your future ML system.
  id: totrans-40
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.1 在构建复杂模型之前，先从一个原始基线开始，这可能是你未来机器学习系统的最合适的基础。
- en: Keeping in mind that analogy, let’s start the discussion with the most spartan
    solutions that look like a log bridge. When we initiate a search for a suitable
    baseline, we often ask ourselves, “What is the most straightforward ML model that
    could solve the problem?” or “What is the right ML model to start from?” but frequently
    these questions turn out to be the wrong ones to ask. We believe the right one
    could sound like, “Do we need ML at all to solve this problem?”
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 记住这个类比，让我们从最斯巴达式的解决方案开始讨论，这些解决方案看起来像一根桥梁的木材。当我们开始寻找合适的基线时，我们经常问自己，“最直接解决这个问题的机器学习模型是什么？”或者“从哪里开始选择正确的机器学习模型？”但这些问题往往被证明是错误的。我们认为正确的问题可能是，“我们是否真的需要机器学习来解决这个问题？”
- en: Sometimes we either don’t even need ML for the problem or at least should not
    reinvent the wheel on our own and can instead use a third-party vendor. We already
    discussed this alternative in chapter 3 (section 3.2).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候我们甚至不需要机器学习来解决这个问题，或者至少我们不应该自己重新发明轮子，而可以使用第三方供应商。我们已经在第3章（第3.2节）讨论了这种替代方案。
- en: 'But let’s say we decided to build our own model. Good modeling starts with
    no model at all: with trying to hack a defined metric by picking the most trivial
    and lazy solution from the solution space. It will be the very first approximation
    of our problem. You can argue that a constant baseline represents a model by itself.
    With a constant baseline, we approximate all dependencies and interactions by
    a constant.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 但假设我们决定构建自己的模型。良好的建模始于没有任何模型：通过从解决方案空间中选择最简单、最懒惰的解决方案来尝试破解一个定义的指标。这将是我们问题的第一个近似。你可以争论说，一个常数基线本身就代表了一个模型。使用常数基线，我们通过一个常数来近似所有的依赖关系和交互作用。
- en: 'To immediately give an idea of what we are talking about, here are a couple
    of examples that you already know:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了立即给出我们正在谈论的内容，这里有一些你已经知道的例子：
- en: For regression tasks, constant baselines are average or median predictions (in
    time-series forecasting, you can take both by last day/week/month/year) by the
    last available value (e.g., for the corresponding user or item). Also, this could
    be some user-defined constant that maximizes the metric.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于回归任务，常量基线是最后可用值（例如，对于相应的用户或项目）的平均或中位数预测（在时间序列预测中，你可以取最后一天/周/月/年的值）。此外，这也可以是用户定义的某个常量，该常量最大化了指标。
- en: For classification tasks, it will be prediction by the major class (let’s say,
    in the antifraud problem, we can assume that there is no fraud at all) or the
    constant prediction of the probability of a positive class.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于分类任务，这将是通过主要类别进行预测（例如，在反欺诈问题中，我们可以假设根本不存在欺诈）或对正类概率的常量预测。
- en: For ranking, this could be either a random order of documents or sorting based
    on an irrelevant numerical property like document ID or a simple heuristic like
    “number of queried keywords contained in an item description.”
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于排序，这可以是文档的随机顺序，或者基于无关的数值属性（如文档ID）或简单的启发式方法（如“包含在项目描述中的查询关键字数量”）的排序。
- en: In a way, a constant baseline is like the first term in the Taylor Series or
    the mean predictor as the first base estimator in gradient boosting (see figure
    8.2). Neither even depends on variable x; they already do (although roughly) something
    related to our problem—no more, no less.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在某种程度上，常量基线就像泰勒级数的第一项或梯度提升中的第一个基估计器的均值预测器（见图8.2）。它们都不依赖于变量x；它们已经（虽然粗略地）与我们的问题相关——不多，也不少。
- en: '![figure](../Images/CH08_F02_Babushkin.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F02_Babushkin.png)'
- en: Figure 8.2 A constant baseline is like the first term of the Taylor Series—the
    simplest approximation that sets the foundation for more complex models.
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.2 常量基线就像泰勒级数的第一项——这是为更复杂模型奠定基础的简单近似。
- en: 8.2.1 Why do we need constant baselines?
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 我们为什么需要常量基线？
- en: There are two goals for building such a baseline.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 建立这样的基线有两个目标。
- en: The first one is benchmarking. It is helpful to get a baseline value of a selected
    metric for a random prediction. A simple sanity check compares your model against
    simple rules of thumb. Indeed, it would be sad to do 2 weeks of hardcore ML modeling
    and then finally implement the most straightforward possible baseline in 5 minutes
    that beats your model. It sounds ridiculous, but the situation is quite common
    in real life.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个目标是基准测试。对于随机预测，获取所选指标的一个基线值是有帮助的。一个简单的合理性检查是将你的模型与简单的经验法则进行比较。确实，如果你花了两周时间进行艰苦的机器学习建模，然后最终在5分钟内实现了最简单的基线，并且这个基线击败了你的模型，那将是一件令人难过的事情。这听起来很荒谬，但在现实生活中这种情况相当普遍。
- en: There’s a cool story from Valerii about this case. He was lucky enough to work
    with an engineer who is a wonderful person and a great specialist. Once, she won
    an ML competition with the goal of predicting some factory time series with just
    a constant baseline—or, as she likes to correct him, a stepwise constant. Conducting
    ML competitions is usually a very straightforward process. Participants have a
    labeled dataset and an unlabeled dataset. Their goal is to build a model using
    the labeled dataset that will output predictions to the unlabeled dataset that
    are the closest to the actual ones (available to the organizers only). Now imagine
    the frustration of other participants who have been engineering dozens of features
    for months and tuning parameters of their gradient-boosting models.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个案例，有一个来自Valerii的有趣故事。他非常幸运，能够与一位既是一个出色的人也是一个伟大的专家的工程师一起工作。有一次，她通过仅使用常量基线——或者，如她喜欢纠正他的那样，逐步常量——赢得了预测某些工厂时间序列的机器学习竞赛。举办机器学习竞赛通常是一个非常直接的过程。参与者有一个标记的数据集和一个未标记的数据集。他们的目标是使用标记的数据集构建一个模型，该模型将对未标记的数据集进行预测，这些预测与实际值（仅对组织者可用）最接近。现在想象一下其他参与者们的挫败感，他们已经为几个月来工程化了数十个特征，并调整了他们梯度提升模型的参数。
- en: This case inspired us to look for and start with the simplest models, and this
    is something we’d like to encourage everyone to do. Don’t limit yourself to them,
    though. It will give you a much more adequate understanding of your metric and
    target values from the beginning so you can get a vision of what can be done with
    given data and what cannot.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这个案例激发了我们寻找并从最简单的模型开始，这是我们希望鼓励每个人去做的事情。不过，不要局限于它们。这将从一开始就让你对指标和目标值有一个更恰当的理解，这样你就可以对给定数据可以做什么以及不能做什么有一个愿景。
- en: The second goal of constant baselines is to provide a bulletproof fallback.
    If your real ML model could not make a prediction during runtime, due to some
    raised error, because of running into response-time constraints, or because there
    is no history for calculating features (aka the cold start problem with new users
    and new items)—or it simply goes crazy (which sometimes happens)—your ML service
    should return at least something. So, in this case, a constant baseline is all
    you need.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 恒定基线的第二个目标是提供一个万无一失的回退方案。如果你的真实机器学习模型在运行时无法进行预测，由于某些错误、遇到响应时间限制、没有历史数据来计算特征（即新用户和新物品的冷启动问题）——或者它简单地变得疯狂（这有时会发生）——你的机器学习服务至少应该返回一些内容。因此，在这种情况下，恒定基线就足够了。
- en: At the same time, we can easily imagine situations where a constant baseline
    is too primitive and brings no value at all. So the simplest usable baseline should
    be more complicated, represented as a set of heuristics/regular expressions or
    as a shallow model. Constant baselines tend to work fine for simple regression/classification
    problems on tabular or small text data; however, it is impossible to build a chatbot
    or voice recognition system with a constant baseline.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，我们可以轻松想象出一些情况，其中恒定基线过于原始，毫无价值。因此，最简单的可用基线应该更复杂，表现为一组启发式规则/正则表达式或浅层模型。恒定基线通常适用于简单的回归/分类问题，尤其是在表格或小型文本数据上；然而，使用恒定基线构建聊天机器人或语音识别系统是不可能的。
- en: 8.3 Model baselines and feature baselines
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 模型基线和特征基线
- en: 'If we move further along the complexity scale, our next stop is rule-based
    models, although it is not always possible to draw a clear line between constant
    baselines and rule-based ones, as in most cases we can define the last one as
    a constant on top of some grouping. But there is also another well-known and illustrative
    example of rule-based baselines: to start solving natural language processing
    problems with just regular expressions.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们进一步沿着复杂度尺度前进，我们的下一个停靠点是规则基础模型，尽管在大多数情况下，我们无法在恒定基线和规则基础基线之间划出一条清晰的界限，因为我们可以将后者定义为一组之上的恒定值。但还有一个众所周知且具有说明性的规则基础基线例子：仅使用正则表达式开始解决自然语言处理问题。
- en: 'A couple of years ago, Arseny worked for a taxi aggregator company, where he
    was involved in developing a service that would predict the time it would take
    for the nearest car to get to a client. The problem was apparent: if we overpredicted,
    the client could decide not to wait and would look for another service; if we
    underpredicted, the client would wait longer than we promised, which would mean
    we disappointed them.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 几年前，Arseny在一家出租车聚合公司工作，他参与开发了一个预测最近车辆到达客户所需时间的服务。问题很明显：如果我们预测过高，客户可能会决定不去等待并寻找其他服务；如果我们预测过低，客户会等待比我们承诺的时间更长，这意味着我们让他们失望。
- en: 'Arseny’s colleague, who was a senior engineer at the time, treated it like
    a standard regression task and started with models like “always predict 5 minutes”
    or “if borough == ‘manhattan’: return 4.” Long story short: these types of baselines
    were hard to beat with hardcore ML alchemy, and ironically, the latter was even
    in production for a while as a fallback.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Arseny的同事，当时是一名高级工程师，将其视为一个标准的回归任务，并从像“总是预测5分钟”或“如果区域等于‘曼哈顿’：返回4”这样的模型开始。简而言之：这些类型的基线很难被硬核机器学习魔法打败，而且讽刺的是，后者甚至作为回退方案在一段时间内投入了生产。
- en: 'The “if district == X: return Y” model is an excellent example of a rule-based
    baseline. We can generate similar models by taking the mean/median/mode of some
    category or several categories—in our example, the median by location.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: “如果区域等于X则返回Y”模型是规则基础基线的优秀例子。我们可以通过取某些类别或几个类别的平均值/中位数/众数来生成类似的模型——在我们的例子中，通过位置取中位数。
- en: The further we go down the progression, the more complex our model gets and
    the more connections between objects’ properties and the labels it can find.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们沿着进步的方向前进，我们的模型变得更加复杂，并且能够找到更多对象属性与标签之间的联系。
- en: 'A typical sequence of baselines in an ML problem would begin with the following:
    constant baseline, rule-based baseline, and linear model (see figure 8.3). We
    need something more sophisticated and specialized only if these baselines are
    insufficient for our task.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个机器学习问题中，典型的基线序列可能开始于以下内容：恒定基线、规则基础基线和线性模型（见图8.3）。只有当这些基线不足以满足我们的任务时，我们才需要更复杂和专业的解决方案。
- en: '![figure](../Images/CH08_F03_Babushkin.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F03_Babushkin.png)'
- en: Figure 8.3 A typical sequence of baselines at the early stage of designing your
    model
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.3 设计模型早期阶段的典型基线序列
- en: For example, when building a recommender system, we start with some constant
    retrieval, then try collaborative filtering (e.g., alternate least squares), factorization
    machines, and, finally, deep learning (e.g., deep structured semantic model) if
    needed.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在构建推荐系统时，我们从一个恒定的检索开始，然后尝试协同过滤（例如交替最小二乘法），因子分解机，如果需要的话，最后是深度学习（例如深度结构化语义模型）。
- en: Whatever problem you face, be aware of simple approaches in this field. They
    don’t necessarily perform worse than more sophisticated ones.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你面临什么问题，都要注意这个领域的简单方法。它们不一定比更复杂的方法表现差。
- en: A vivid example of that can be found in the paper called “Are We Really Making
    Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches”
    by Maurizio Ferrari Dacrema et al. ([https://arxiv.org/abs/1907.06902](https://arxiv.org/abs/1907.06902))
    that took RecSys’ Best Paper Award in 2019 ([https://recsys.acm.org/best-papers/](https://recsys.acm.org/best-papers/)).
    The paper is notable for unveiling strikingly impressive stats. The research group
    examined 18 algorithms that had been presented at top-level research conferences
    in the preceding years. After studying these algorithms, only seven of them could
    be reproduced with reasonable effort. Things got even more interesting when it
    turned out that six out of seven algorithms could often be outperformed using
    relatively simple heuristic methods (e.g., those based on nearest-neighbor or
    graph-based techniques). The only remaining algorithm clearly outclassed baselines;
    however, it could not consistently outperform a well-tuned nonneural linear ranking
    method.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一个生动的例子可以在Maurizio Ferrari Dacrema等人撰写的论文“Are We Really Making Much Progress?
    A Worrying Analysis of Recent Neural Recommendation Approaches”中找到，这篇论文获得了2019年RecSys最佳论文奖([https://arxiv.org/abs/1907.06902](https://arxiv.org/abs/1907.06902))。该论文因其揭示的令人印象深刻的数据而备受瞩目。研究小组检查了在过去几年顶级研究会议上展示的18个算法。在研究了这些算法之后，只有七个算法在合理的努力下可以重现。当发现其中七个算法中的六个通常可以使用相对简单的启发式方法（例如基于最近邻或图技术的方法）来超越时，事情变得更加有趣。唯一剩下的算法明显优于基线；然而，它并不能始终超越一个调优良好的非神经网络线性排名方法。
- en: 'A nonexhaustive list of examples from the already-mentioned Eugene Yan article
    includes the following:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 从已经提到的Eugene Yan文章中列出的非详尽例子包括以下内容：
- en: Tree-based models (random forest, gradient boosting) in most cases beat deep
    neural networks on tabular data, especially on small/medium datasets (say, < 1
    million) ([https://arxiv.org/abs/2207.08815](https://arxiv.org/abs/2207.08815)).
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大多数情况下，基于树的模型（随机森林、梯度提升）在表格数据上优于深度神经网络，尤其是在小型/中型数据集（例如，小于100万）上([https://arxiv.org/abs/2207.08815](https://arxiv.org/abs/2207.08815))。
- en: Greedy algorithms outperform graph neural networks on combinatorial graph problems
    ([https://arxiv.org/abs/2206.13211](https://arxiv.org/abs/2206.13211)).
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在组合图问题上，贪婪算法优于图神经网络([https://arxiv.org/abs/2206.13211](https://arxiv.org/abs/2206.13211))。
- en: Simple averaging is often not worse than complex optimizers on multitask learning
    problems ([https://arxiv.org/abs/2201.04122](https://arxiv.org/abs/2201.04122)).
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多任务学习问题上，简单的平均通常不比复杂的优化器差([https://arxiv.org/abs/2201.04122](https://arxiv.org/abs/2201.04122))。
- en: A dot product of embeddings outperforms neural collaborative filtering in item
    recommendation and retrieval ([https://arxiv.org/abs/2005.09683](https://arxiv.org/abs/2005.09683)).
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入的乘积在项目推荐和检索中优于神经网络协同过滤([https://arxiv.org/abs/2005.09683](https://arxiv.org/abs/2005.09683))。
- en: 'Up to now, we have been talking about baselines focusing on models. But what
    about features? Features are effectively a part of the model and sometimes are
    even the most important part of it. In classic ML, we have to engineer features
    manually, and choosing features for a baseline is based on the same principles;
    we start with a small group of essential features (most likely, those that are
    easier to calculate). There are two ways of adding new features:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在谈论关注模型的基线。但特征呢？特征实际上是模型的一部分，有时甚至是模型最重要的部分。在经典机器学习中，我们必须手动构建特征，为基线选择特征是基于相同的原则；我们从一小组基本特征（最可能的是那些更容易计算的）开始。有两种方法可以添加新特征：
- en: Engineering new features, which is challenging and time-consuming and requires
    building new ETL pipelines
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建新特征，这是一个具有挑战性和耗时的工作，需要构建新的ETL管道
- en: Generating features that derive from ones that already exist
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从已存在的特征派生新特征
- en: 'The sequence of baseline features we need to try should look like this: the
    original minimum set of features, all sorts of interactions and counters, then
    embeddings, and then something more complicated (see figure 8.4).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要尝试的基线特征序列应如下所示：原始的最小特征集，各种交互和计数器，然后是嵌入，接着是更复杂的东西（见图8.4）。
- en: '![figure](../Images/CH08_F04_Babushkin.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F04_Babushkin.png)'
- en: 'Figure 8.4 A sequence of baseline features you need to try: from the simplest
    to the more complicated'
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.4 需要尝试的基线特征序列：从最简单到更复杂
- en: What are the properties of a good bunch of features to start from? The answer
    is exactly the same as for the models, and we’ll discuss it further.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 从哪里开始的一组好的特征有哪些属性？答案是和模型完全一样，我们将在后面进一步讨论。
- en: For a typical problem usually solved with deep learning methods, there can be
    a simple baseline built with shallow models. As we recall, deep learning is a
    part of representation learning, which means that instead of handcrafting features,
    we delegate this work to a neural network. However, for some problems like image
    or text classification, you can apply naive approaches (rule-based or linear model-based).
    For example, naive Bayes was a very strong baseline in a natural language processing
    world before BERT-like architectures emerged. For computer vision, some problems
    can be solved by using a histogram of pixel color (or even just mean/median value!)
    as features for a linear model. Having said that, for most scenarios, starting
    with a simple deep learning model—either foundational in a few-shot setup or a
    trained one—can be a better choice because these models have already proven themselves
    in a wide variety of tasks.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于通常用深度学习方法解决的问题，可以使用简单的基线，通过浅层模型构建。正如我们回忆的那样，深度学习是表示学习的一部分，这意味着我们不是手工制作特征，而是将这项工作委托给神经网络。然而，对于一些像图像或文本分类这样的问题，你可以应用朴素的方法（基于规则或基于线性模型）。例如，在BERT-like架构出现之前，朴素贝叶斯在自然语言处理领域是一个非常强大的基线。对于计算机视觉，一些问题可以通过使用像素颜色的直方图（甚至只是均值/中值！）作为线性模型的特征来解决。话虽如此，对于大多数场景，从简单的深度学习模型开始——无论是在少量样本设置中的基础模型还是在训练好的模型——可能是一个更好的选择，因为这些模型已经在各种任务中证明了自己的能力。
- en: Arseny once designed a take-home exercise for candidates, where they were provided
    with a script solving anomaly detection problems on a simple image dataset. The
    script contained two baselines—one with a neural network and one with a color
    histogram—and candidates were instructed to improve either of them to beat some
    metric. Both baselines already performed on a similar level, and both were implemented
    specifically poorly, so the candidates had room for improvement. The majority
    of candidates preferred working on a more complicated deep learning solution,
    and only the most experienced of them noticed that it was possible to reach the
    required result with a single line of code changed for a histogram-based baseline.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 阿尔谢尼曾经为候选人设计了一个带回家的练习题，其中他们得到了一个在简单图像数据集上解决异常检测问题的脚本。该脚本包含两个基线——一个使用神经网络，另一个使用颜色直方图——并且候选人被指示改进其中的任何一个以击败某些指标。这两个基线已经在一个相似的水平上表现，并且都实现得特别糟糕，因此候选人有改进的空间。大多数候选人更喜欢工作在更复杂的深度学习解决方案上，而只有其中经验最丰富的候选人注意到，只需对基于直方图的基线进行一行代码的更改，就可以达到所需的结果。
- en: 8.4 Variety of deep learning baselines
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 深度学习基线的多样性
- en: When a problem is not trivial and suggests the use of deep learning because
    of the data structure (which can be applicable to most computer vision or language
    processing problems), the variety of baselines is slightly different. The most
    common are reusing pretrained models and training/fine-tuning the simplest models.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当问题不是微不足道的，并且由于数据结构（这可以适用于大多数计算机视觉或语言处理问题）而建议使用深度学习时，基线的多样性略有不同。最常见的是重用预训练模型和训练/微调最简单的模型。
- en: Reusing pretrained models is a common practice if a problem is not unique and
    there is a model that has been trained on a similar task. For example, if we want
    to train a model that can recognize breeds of pets, we can reuse a model that
    was trained on an ImageNet dataset. ImageNet is a dataset that contains images
    of 1,000 classes, and more than 100 of them are dog breeds. So, once your goal
    is to recognize cats and dogs, you can reuse a model that was trained on ImageNet
    dataset without retraining it. This is a common practice for many generic problems
    like speech recognition, object detection, text classification, sentiment analysis,
    end so on.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果问题不是独特的，并且有一个在类似任务上训练过的模型，那么重用预训练模型是一种常见的做法。例如，如果我们想训练一个能够识别宠物品种的模型，我们可以重用在一个ImageNet数据集上训练过的模型。ImageNet是一个包含1,000个类别的图像数据集，其中超过100个是狗的品种。所以，一旦你的目标是识别猫和狗，你就可以重用ImageNet数据集上训练过的模型，而无需重新训练。这对于许多通用问题（如语音识别、目标检测、文本分类、情感分析等）来说是一种常见的做法。
- en: A slightly more advanced version of this approach is reusing features (also
    known as *embeddings* or *representations*) from a pretrained model to train a
    simple shallow model. For example, you can take a pretrained model that was trained
    on an ImageNet dataset and use its representations from the last backbone layer
    (before the final classification layer) to train a simple linear model that will
    classify images into a custom set of classes. This approach is especially useful
    when the dataset is small and the final task is more or less trivial (e.g., classification),
    so training large models from scratch is not likely to work. This approach is
    also known as a specific case of transfer learning. We have seen cases where such
    a baseline was literally unbeatable, and no fancy models were able to outperform
    it.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的稍微高级版本是重用预训练模型中的特征（也称为*嵌入*或*表示*）来训练一个简单的浅层模型。例如，你可以使用在ImageNet数据集上训练的预训练模型，并使用其最后骨干层（在最终分类层之前）的表示来训练一个简单的线性模型，该模型将图像分类到自定义的类别集合中。这种方法在数据集较小且最终任务或多或少是平凡的（例如，分类）时特别有用，因此从头开始训练大型模型不太可能奏效。这种方法也被称为迁移学习的一个特例。我们见过一些案例，其中这样的基线实际上是无敌的，而且没有花哨的模型能够超越它。
- en: An even more specific version of using pretraining models is using pretrained
    models or third-party APIs capable of zero-shot or few-shot performance (meaning
    they require no or a few training samples to provide a result). A glorified example
    of such an API is the GPT family, but there are many APIs available for different
    tasks—for example, all major cloud vendors have a long list of AI solutions, such
    as Amazon Rekognition or Google Cloud Vision AI in the computer vision niche;
    detailed information about them is out of the scope of this book.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练模型的一个更具体的版本是使用能够实现零样本或少量样本性能（意味着它们需要没有或少量训练样本来提供结果）的预训练模型或第三方API。一个这样的API的华丽例子是GPT家族，但有许多API可用于不同的任务——例如，所有主要云供应商都有一个长长的AI解决方案列表，例如计算机视觉领域的Amazon
    Rekognition或Google Cloud Vision AI；关于它们的详细信息超出了本书的范围。
- en: 'Using a third-party API by a major vendor as a baseline has a nice side effect:
    it is a bargaining chip in negotiations, such as selling a software product to
    a big enterprise or proving a startup tech is solid to potential investors. Potential
    customers may not know what a good metric is for a given problem (recall chapter
    5), but comparing your tech to an AWS solution frames the problem in a proper
    way. Arseny knows at least three startups that have used this approach, bragging
    about how they beat alternatives such as Amazon, Google, and OpenAI. In all three
    cases, the companies’ claims were legit, and that’s expected, as major vendors
    aim to tailor one-size-fits-all solutions, while startups can offer more niche
    ML systems doing one thing just great.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 使用主要供应商的第三方API作为基线有一个很好的副作用：它是谈判中的筹码，例如向大型企业销售软件产品或向潜在投资者证明初创科技是可靠的。潜在客户可能不知道给定问题的好指标是什么（回忆第5章），但将你的技术与AWS解决方案进行比较，可以恰当地界定问题。Arseny至少知道三家使用过这种方法的公司，吹嘘他们如何击败了Amazon、Google和OpenAI等替代方案。在所有三个案例中，公司的声明都是合法的，这是预期的，因为主要供应商旨在提供一刀切解决方案，而初创公司可以提供更专业的ML系统，只做一件事就做得很好。
- en: Finally, if none of these options work, you can try to train a simple model
    from scratch. However, it is recommended to avoid recent state-of-the-art models
    and use something simpler and time proven. Recent models are often more “capricious”
    during training, while some older “stars” are already more researched, and recipes
    for stable training are well-known. A popular example of such models is the ResNet
    family for vision and the BERT family for natural language processing. Our personal
    heuristic is to start with models that are at least 2 years old, but it is not
    a strict rule. It depends on the level of innovation required for the system,
    as discussed in chapter 3.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果这些选项都不起作用，你可以尝试从头开始训练一个简单的模型。然而，建议避免使用最新的最先进模型，而使用更简单且经过时间考验的模型。最新的模型在训练过程中往往更加“反复无常”，而一些较老的“明星”模型已经被深入研究，稳定的训练方法已知。这类模型的流行例子是用于视觉的ResNet系列和用于自然语言处理的BERT系列。我们个人的经验法则是从至少2年前的模型开始，但这不是一条严格的规定。它取决于系统所需的创新水平，如第3章所述。
- en: 'It’s worth noting that there are multiple shades of fine-tuning between “training
    a shallow model on top of a pretrained model” and “training a model from scratch.”
    Choosing the right degree of fine-tuning is very case-specific and may require
    some experiments. For example, when training a text classification model based
    on BERT, you can gradually complicate the scope of training:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，在“在预训练模型之上训练浅层模型”和“从头开始训练模型”之间有多个微调的层次。选择合适的微调程度非常具体，可能需要进行一些实验。例如，当基于BERT训练文本分类模型时，你可以逐渐复杂化训练范围：
- en: Train only the last layer.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只训练最后一层。
- en: Train some blocks using adapter methods such as low-rank adaptation ([https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)).
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用低秩适应（[https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)）等适配器方法训练一些块。
- en: Train some encoder blocks.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练一些编码器块。
- en: Train normalization layers.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练标准化层。
- en: Train embedding layer.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练嵌入层。
- en: Train full model.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练完整模型。
- en: Train full model + tokenizer.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练完整模型 + 标准化器。
- en: This variety leads to the question, “How do we choose a proper baseline?”
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这种多样性导致了一个问题，“我们如何选择一个合适的基线？”
- en: 8.5 Baseline comparison
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.5 基线比较
- en: Let’s examined various features and model baselines, starting with the trivial
    ones. We can answer the central question, which is how to decide when to stop
    adding complexity and how to determine a suitable baseline for our system. There
    are multiple factors we should consider simultaneously; some of them are
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查各种特征和模型基线，从最简单的开始。我们可以回答核心问题，即何时停止增加复杂性以及如何确定我们系统的合适基线。我们应该同时考虑多个因素；其中一些是
- en: Accuracy
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确性
- en: Effort (mostly, time of development)
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 努力程度（主要是开发时间）
- en: Interpretability
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可解释性
- en: Computation time
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算时间
- en: The most fundamental is the tradeoff between a model’s accuracy (or other ML
    metric) and the effort it requires. The first component in the equation is accuracy.
    When you move from a constant baseline to a rule-based baseline, from a rule-based
    one to a linear model, or from original features to their aggregations and ratios,
    you already start to get a feeling for what increase in metrics these small changes
    give. Is it responsive or not? How difficult is it to significantly surpass your
    constant baseline? Is it reasonable to invest more time attempting to gain more
    accuracy?
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的是模型准确度（或其他机器学习指标）与其所需努力之间的权衡。方程中的第一个组成部分是准确度。当你从恒定基线移动到基于规则的基线，从基于规则的基线到线性模型，或者从原始特征到它们的聚合和比率时，你已经开始感受到这些小变化对指标增加的影响。它是敏感的还是不敏感的？显著超越你的恒定基线有多难？投资更多时间尝试获得更高准确度是否合理？
- en: In some sense, as an ML engineer, you do backpropagation by getting “feedback”
    from your training loop and updating your understanding of the problem with its
    data and accuracy distribution across the solution space.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在某种意义上，作为一个机器学习工程师，你通过从训练循环中获得“反馈”来进行反向传播，并使用其数据和解决方案空间中的准确度分布来更新你对问题的理解。
- en: The second component is effort. By “effort,” we mostly mean time and computing
    resources. No ML project has an infinite budget and, hence, an infinite amount
    of time. We consider the time required to implement a new model (or feature),
    train it, debug it, and test it. You should also pay attention to all the attendant
    complications and pitfalls that may arise on the way, especially infrastructural
    ones.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个组成部分是努力。通过“努力”，我们主要指的是时间和计算资源。没有机器学习项目有无限的预算和，因此，无限的时间。我们考虑实现一个新模型（或特征）、训练它、调试它和测试它所需的时间。你也应该注意可能出现的所有伴随的复杂性和陷阱，尤其是基础设施方面的。
- en: Let’s examine a constant baseline. It takes almost no time to implement, and
    it provides us with the lowest accuracy. So we will map it into the (0, 0)-point
    in time-accuracy coordinates (as shown in figure 8.5).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考察一个恒定的基线。实现它几乎不需要时间，但它提供的准确性最低。因此，我们将它在时间-准确性坐标中的（0，0）点进行映射（如图8.5所示）。
- en: '![figure](../Images/CH08_F05_Babushkin.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH08_F05_Babushkin.png)'
- en: Figure 8.5 Simple baselines are easy to build but sacrifice final system metrics
    (example for a time-series prediction).
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.5简单的基线容易构建，但牺牲了最终系统指标（时间序列预测的示例）。
- en: Let’s take a look at the linear model. It requires more effort but also most
    likely provides us with better accuracy. We will probably find the corresponding
    point to the right and higher than the previous one, and so on. On the other hand,
    it is important to understand that as the model improves and evolves (and therefore
    gains in complexity), the cost–accuracy ratio begins to decrease. A striking example
    of such a drop in efficiency is gradient boosting, which we mentioned earlier.
    Based on our estimates and experience, gradient boosting requires more input than
    all the simpler models you would use at the earlier stages put together while
    giving no significant increase in accuracy.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看线性模型。它需要更多的努力，但也很可能提供更好的准确性。我们可能会找到对应于右侧和高于上一个点的点，依此类推。另一方面，重要的是要理解，随着模型改进和演变（因此复杂性增加），成本-准确性比开始下降。这种效率下降的一个显著例子是前面提到的梯度提升。根据我们的估计和经验，梯度提升需要的输入比你在早期阶段使用的所有更简单的模型加在一起还要多，而准确性却没有显著提高。
- en: We should estimate how long it would take to try a more complex model each time
    and how much additional accuracy it could provide. Once we understand that the
    next step requires too much effort for almost no significant score improvement,
    we should stop. This “early stopping threshold” differs depending on the concrete
    domain and problem.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该估计尝试更复杂模型需要多长时间以及它可能提供多少额外的准确性。一旦我们了解下一步需要太多的努力而几乎没有任何显著的分数提升，我们就应该停止。这个“早期停止阈值”取决于具体的领域和问题。
- en: But what if something goes wrong or some additional change in the model is required?
    What model would be easier to debug or update?
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果出了问题或者模型需要一些额外的变化呢？哪个模型更容易调试或更新？
- en: On the left of the spectrum, we have linear regression with an exact form solution.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在光谱的左侧，我们有具有精确形式解的线性回归。
- en: A deep neural network with sophisticated training and inference pipelines is
    on the right.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右侧是一个具有复杂训练和推理管道的深度神经网络。
- en: Which one would you prefer to face?
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 你更愿意面对哪一个？
- en: Maintenance, which we will touch upon in more detail in chapter 16, includes
    the amount of additional work that is necessary for debugging implemented features
    or a model. We could count maintenance as a part of the extra effort the more
    complex baseline requires.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 维护，我们将在第16章中更详细地讨论，包括调试实现的功能或模型所需的额外工作量。我们可以将维护视为更复杂基线所需的额外努力的一部分。
- en: Another essential property of a baseline is computation time. How does the computation
    time of our model and its features affect the response time? Does our baseline
    meet the service-level agreement? Is it the natural limit for solution space,
    especially when dealing with real-time systems? But even with no real-time requirements,
    computation time also determines how fast we will iterate during more thorough
    experimentation in the future.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 基线的另一个基本属性是计算时间。我们的模型及其特征的计算时间如何影响响应时间？我们的基线是否符合服务水平协议？它是解决方案空间的自然极限，尤其是在处理实时系统时吗？但即使没有实时需求，计算时间也决定了我们在未来更彻底的实验中迭代的速度。
- en: 'Finally, we have interpretability. This parameter matters when we deal with
    the very first iteration of any ML system, especially for other teammates. When
    we deal, for example, with sensitive or medical data, it becomes a safety problem,
    too, not just a question of trust in the predictions of our model only. The general
    pattern is trivial: the simpler the baseline, the easier it is to explain how
    it works.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有可解释性。当处理任何ML系统的第一次迭代时，这个参数很重要，特别是对于其他队友。当我们处理敏感或医疗数据时，它也成为了一个安全问题，而不仅仅是模型预测的信任问题。一般的模式很简单：基线越简单，解释其工作原理就越容易。
- en: We’ll discuss this topic in detail later in chapter 11.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第11章中详细讨论这个话题。
- en: '8.6 Design document: Baselines'
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.6 设计文档：基线
- en: As long as baselines can be part of your design document, we are going to fill
    this gap for our fictional companies, Supermegaretail and PhotoStock, Inc.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 只要基线可以成为您的设计文档的一部分，我们将为我们的虚构公司Supermegaretail和PhotoStock，Inc.填补这个空白。
- en: 8.6.1 Baselines for Supermegaretail
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.6.1 Supermegaretail的基线
- en: Let’s start with the forecast system. Here, seasonality will be a huge factor
    when choosing a prediction model, so we can’t but consider it in our design document.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从预测系统开始。在这里，季节性在选择预测模型时将是一个巨大的因素，所以我们不能不在我们的设计文档中考虑它。
- en: 'Design document: Supermegaretail'
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设计文档：Supermegaretail
- en: V. Baseline solution
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V. 基线解决方案
- en: i. Constant baseline
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: i. 常量基线
- en: As a constant baseline for Supermegaretail’s demand forecasting system, we plan
    to use the actual value of the previous day per SKU per grocery. Knowing that
    data sometimes could appear with delay and that grocery sales experience strong
    weekly seasonality, we will go 1 full week back instead of going 1 day back. As
    a result, our prediction for a specific item on September 8, 2022, will be the
    actual sales value for this item on September 1, 2022.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 作为Supermegaretail需求预测系统的常量基线，我们计划使用每个SKU每个杂货的前一天的实际值。考虑到数据有时会延迟出现，以及杂货销售存在强烈的每周季节性，我们将后退1整个星期，而不是后退1天。因此，我们对2022年9月8日特定商品的预测将是该商品在2022年9月1日的实际销售价值。
- en: ii. Advanced constant baseline
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ii. 高级常量基线
- en: Chapter 5 mentioned quantile losses of 1.5th, 25th, 50th, 75th, 95th, and 99th
    percentiles. We can calculate the same with our baseline using a yearly window.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 第5章提到了1.5分位数、25分位数、50分位数、75分位数、95分位数和99分位数的分位数损失。我们可以使用我们的基线使用年度窗口来计算相同的值。
- en: iii. Linear model baseline
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: iii. 线性模型基线
- en: We will use a basic set of features to use linear regression with quantile loss;
    for a start, we can only use target variables but with multiple lags and aggregations
    like sum/min/max/avg/median or corresponding quantiles for the last 7/14/30/60/90/180
    days or different rolling windows of different sizes. The same magic could be
    done with other dynamic data beyond sales date, like price, revenue, average check,
    or number of unique customers.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一组基本特征来使用具有分位数损失的线性回归；一开始，我们只能使用目标变量，但带有多个滞后和聚合，如总和/最小值/最大值/平均值/中位数或对应于过去7/14/30/60/90/180天或不同大小的滚动窗口的最后几个分位数。同样的魔法也可以用于其他动态数据，如销售日期之外的价格、收入、平均账单或独特客户的数量。
- en: iv. Time-series-specific baseline
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: iv. 时间序列特定基线
- en: '![figure](../Images/CH08_UN01_Babushkin.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH08_UN01_Babushkin.png)'
- en: Autoregressive integrated moving average (ARIMA) and seasonal ARIMA (SARIMA)
    are both autoregressive algorithms for forecasting; the second one considers any
    seasonality patterns.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归积分移动平均（ARIMA）和季节性ARIMA（SARIMA）都是用于预测的自回归算法；后者考虑了任何季节性模式。
- en: 'Both require fine-tuning multiple hyperparameters to provide satisfying accuracy.
    To avoid this, we may prefer a state-of-the-art forecasting procedure that works
    great out-of-the-box and is called Prophet ([https://github.com/facebook/prophet](https://github.com/facebook/prophet)).
    The nice advantage of Prophet is that it’s robust and doesn’t require a lot of
    preprocessing: outliers, missing values, shifts, and trends are handled automatically.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法都需要调整多个超参数以提供令人满意的准确性。为了避免这种情况，我们可能更喜欢一种最先进的预测程序，它可以直接使用，称为Prophet ([https://github.com/facebook/prophet](https://github.com/facebook/prophet))。Prophet的优点是它很稳健，不需要太多的预处理：异常值、缺失值、位移和趋势都会自动处理。
- en: v. Feature baselines
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: v. 特征基线
- en: What additional information can some baselines and possible future models benefit
    from?
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 什么额外信息可以让一些基线和可能的未来模型受益？
- en: We will include extra static info about products (brand, category), shops (geo
    features), and context (time-based features, seasonality, day of the week)—all
    of them with preprocessing and encoding appropriate for a chosen model.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将包括关于产品（品牌、类别）、商店（地理特征）和上下文（基于时间的特征、季节性、星期几）的额外静态信息——所有这些都有适合所选模型的预处理和编码。
- en: Features that are also suitable for the baseline are counters and interactions.
    Examples include
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 适用于基线的特征还包括计数器和交互。例如包括
- en: The difference between current and average price (absolute and relative)
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前价格与平均价格之间的差异（绝对和相对）
- en: 'Penetration: the ratio of product sales to sales of a category (of levels 1,
    2, 3) for rolling windows of different sizes'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 渗透率：产品销售额与类别（1级、2级、3级）销售额的比率，对于不同大小的滚动窗口
- en: Number of days since the last purchase
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自上次购买以来的天数
- en: Number of unique customers
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 独特客户数量
- en: 8.6.2 Baselines for PhotoStock Inc.
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.6.2 PhotoStock Inc.的基线
- en: Now we switch to the PhotoStock Inc. case, where we are building an advanced
    search engine set to provide better, more accurate results and eventually increase
    sales.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们转向PhotoStock Inc.的案例，在那里我们正在构建一个高级搜索引擎，旨在提供更好、更准确的结果，并最终增加销售额。
- en: 'Design document: PhotoStock Inc'
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设计文档：PhotoStock Inc.
- en: V. Baseline solution
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V. 基线解决方案
- en: We suggest three approaches to our baseline model for the PhotoStock Inc. search
    engine problem.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议对PhotoStock Inc.搜索引擎问题中的基线模型采用三种方法。
- en: i. Non-ML solution as a baseline
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: i. 非机器学习解决方案作为基线
- en: Currently, PhotoStock Inc. uses a simple non-ML solution for its search engine.
    It is a keyword-based search engine with the ElasticSearch database capable of
    fuzzy search. It doesn’t require any training, and it is already deployed to production,
    so it is a solid candidate for a baseline model.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，PhotoStock Inc.为其搜索引擎使用的是一个简单的非机器学习解决方案。它是一个基于关键字的搜索引擎，使用ElasticSearch数据库，可以进行模糊搜索。它不需要任何训练，并且已经部署到生产环境中，因此它是一个很好的基线模型候选。
- en: 'It has two drawbacks, though: it doesn’t use images in the search, only metadata
    (e.g., tags, descriptions, etc.), and it’s not too easy to embed it into a new
    ML pipeline for comparison. However, it’s still very useful to have it as a baseline
    model because it will allow us to compare the performance of an ML model with
    the performance of a non-ML solution.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，它有两个缺点：它不使用图像进行搜索，只使用元数据（例如，标签、描述等），并且将其嵌入到新的机器学习管道中进行比较并不太容易。然而，将其作为基线模型仍然非常有用，因为它将允许我们比较机器学习模型与非机器学习解决方案的性能。
- en: ii. Simple ML solution as a baseline
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ii. 简单的机器学习解决方案作为基线
- en: Following the previous example, we can use a simple ML model as a baseline.
    It will not use images but only metadata. Such a model can use queries and metadata
    as raw input, transform them into features using a naive term frequency—inverse
    document frequency (TF-IDF) vectorizer, and then use a simple linear model to
    predict a relevance score. On top of that, it will be easy to implement and train,
    and its outstanding simplicity can help with early-stage debugging and understanding
    of the problem.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 沿用之前的例子，我们可以使用一个简单的机器学习模型作为基线。它将不会使用图像，而只使用元数据。这样的模型可以使用查询和元数据作为原始输入，使用朴素词频-逆文档频率（TF-IDF）向量器将它们转换为特征，然后使用简单的线性模型来预测相关性得分。除此之外，它易于实现和训练，其卓越的简单性有助于早期阶段的调试和理解问题。
- en: iii. Pretrained model as a baseline
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: iii. 预训练模型作为基线
- en: Finally, we can use a pretrained model as a baseline. Given the problem’s origin,
    we need a solution capable of unifying visual and text domains, and the most famous
    one is CLIP ([https://openai.com/index/clip/](https://openai.com/index/clip/)).
    CLIP was released in 2021 and proved to be useful across various tasks. There
    are also several CLIP successors available, so they can be reviewed for future
    iterations if needed.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用预训练模型作为基线。鉴于问题的起源，我们需要一个能够统一视觉和文本领域的解决方案，最著名的是CLIP ([https://openai.com/index/clip/](https://openai.com/index/clip/))。CLIP于2021年发布，并在各种任务中证明是有用的。还有几个CLIP的后续版本可用，如果需要，可以在未来的迭代中对其进行审查。
- en: CLIP, in a nutshell, is an image encoder and a text encoder trained to predict
    which images were paired with proper text descriptions. It was trained on a huge
    dataset and thus demonstrates reasonable performance on a variety of tasks. CLIP
    is open source and is distributed under the MIT license, so it can be used for
    commercial purposes.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP，简而言之，是一个图像编码器和文本编码器，经过训练以预测哪些图像与适当的文本描述配对。它在一个庞大的数据集上进行了训练，因此在各种任务上表现出合理的性能。CLIP是开源的，并且根据MIT许可证分发，因此可以用于商业目的。
- en: 'To make it work for our use case, we can start by using its output for a pair
    of queries (images) as a relevancy score. This approach doesn’t use metadata and
    text descriptions, so it can be either combined with a previous approach or developed
    further to use a two-component score—for example:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使它适用于我们的用例，我们可以从使用其输出作为一对查询（图像）的相关性分数开始。这种方法不使用元数据和文本描述，因此它可以与先前的某种方法结合使用，或者进一步发展以使用两个组件的分数——例如：
- en: relevancy_score = distance(query, image) + distance(query, description)
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: relevancy_score = distance(query, image) + distance(query, description)
- en: Both distances can be computed using CLIP—one using both text and image encoders
    and the other using only a text encoder.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个距离都可以使用CLIP来计算——一个使用文本和图像编码器，另一个仅使用文本编码器。
- en: As a first step, we may avoid any training at all. As for the next steps, we
    can start fine-tuning the model or its components on our dataset.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，我们可能完全避免任何训练。至于下一步，我们可以在我们的数据集上开始微调模型或其组件。
- en: Summary
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Consider baselines an integral point of ML system design, as they effectively
    solve technical, ML, and product-related problems (interconnecting components,
    setting up a metric to compare with, and understanding product UX with a weak
    model inside).
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将基线视为机器学习系统设计的一个基本点，因为它们有效地解决了技术、机器学习和产品相关的问题（连接组件、设置一个用于比较的指标，以及通过一个弱模型理解产品用户体验）。
- en: Even though baselines are perceived as something as easy as ABC, the skill of
    recognizing where to start, both in terms of features and models, is underestimated.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管基线被认为像ABC一样简单，但识别从哪些特征和模型开始的能力，却被低估了。
- en: 'As you dive deeper into your project, your common progression will lean toward
    the following progression: constant baseline, then rule-based baseline, then linear
    model, then something more complicated.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着你深入到你的项目，你的常见进展将倾向于以下进展：恒定基线，然后是规则基线，然后是线性模型，然后是更复杂的东西。
- en: While building a complex model from the start may seem tempting, always consider
    kicking off with a constant baseline; this will save you resources and time and
    point to whether you’re moving in the right direction with minimum expenses.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然从一开始就构建一个复杂的模型可能很有吸引力，但始终考虑从恒定的基线开始；这将节省你的资源和时间，并指出你是否以最低的成本朝着正确的方向前进。
- en: When a problem implies the use of deep learning, the most common practice is
    reusing pretrained models, training/fine-tuning the simplest models, or training
    a simple model from scratch if neither of the first two approaches works.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当一个问题暗示了使用深度学习时，最常见的方法是重用预训练模型，训练/微调最简单的模型，或者如果前两种方法都不奏效，从头开始训练一个简单的模型。
- en: When choosing between various baseline options, consider accuracy, effort (mostly
    time of development), interpretability, and computation time as key factors, with
    the accuracy–effort tradeoff being especially important.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在选择各种基线选项时，应将准确性、努力程度（主要是开发时间）、可解释性和计算时间视为关键因素，其中准确性与努力程度的权衡尤为重要。
- en: As your model evolves and gains in complexity, the cost accuracy inevitably
    decreases. This is especially the case with switching to gradient boosting, which,
    as practice shows, requires more input than all previous models put together while
    giving no significant increase in accuracy.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着模型的发展和复杂度的增加，准确性的成本不可避免地会降低。这尤其适用于转向梯度提升，实践表明，它需要的输入比所有之前的模型加起来还要多，而准确性的提升却并不显著。
