<html><head></head><body><section data-pdf-bookmark="Chapter 9. Imbalanced Classes" data-type="chapter" epub:type="chapter"><div class="chapter" id="idm46066897679576">&#13;
<h1><span class="label">Chapter 9. </span>Imbalanced Classes</h1>&#13;
&#13;
&#13;
<p><a data-primary="imbalanced classes, managing" data-type="indexterm" id="ix_ch09-asciidoc0"/>If you are classifying data, and the classes are not relatively balanced&#13;
in size, the bias toward more popular classes can carry over into your&#13;
model. For example, if you have 1 positive case and 99 negative cases, you can&#13;
get 99% accuracy simply by classifying everything as negative. There&#13;
are various options for dealing with <em>imbalanced classes</em>.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Use a Different Metric" data-type="sect1"><div class="sect1" id="idm46066896472008">&#13;
<h1>Use a Different Metric</h1>&#13;
&#13;
<p><a data-primary="imbalanced classes, managing" data-secondary="using metric other than accuracy" data-type="indexterm" id="idm46066896470776"/><a data-primary="metrics" data-secondary="imbalanced classes" data-type="indexterm" id="idm46066896469768"/>One hint is to use a measure other than accuracy (AUC is a good&#13;
choice) for calibrating models. Precision and recall are also&#13;
better options when the target sizes are different. However, there&#13;
are other options to consider as well.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Tree-based Algorithms and Ensembles" data-type="sect1"><div class="sect1" id="idm46066896468200">&#13;
<h1>Tree-based Algorithms and Ensembles</h1>&#13;
&#13;
<p><a data-primary="imbalanced classes, managing" data-secondary="tree-based algorithms" data-type="indexterm" id="idm46066896467064"/><a data-primary="tree-based algorithms" data-type="indexterm" id="idm46066896466024"/>Tree-based models may perform better depending on the distribution of&#13;
the smaller class. If they tend to be clustered, they can be&#13;
classified easier.</p>&#13;
&#13;
<p><a data-primary="ensemble methods" data-type="indexterm" id="idm46066896464808"/><a data-primary="imbalanced classes, managing" data-secondary="ensemble methods" data-type="indexterm" id="idm46066896464104"/>Ensemble methods can further aid in pulling out&#13;
the minority classes. Bagging and boosting are options&#13;
found in tree models like random forests and Extreme Gradient Boosting (XGBoost).</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Penalize Models" data-type="sect1"><div class="sect1" id="idm46066896462552">&#13;
<h1>Penalize Models</h1>&#13;
&#13;
<p><a data-primary="imbalanced classes, managing" data-secondary="penalizing models" data-type="indexterm" id="idm46066896461384"/><a data-primary="scikit-learn" data-secondary="class_weight parameter" data-type="indexterm" id="idm46066896460392"/>Many scikit-learn classification models support the <code>class_weight</code> parameter. Setting&#13;
this to <code>'balanced'</code> will attempt to regularize minority classes and&#13;
incentivize the model to classify them correctly. Alternatively, you can&#13;
grid search and specify the weight options by passing in a dictionary&#13;
mapping class to weight (give higher weight to smaller classes).</p>&#13;
&#13;
<p><a data-primary="XGBoost" data-secondary="max_delta_step parameter" data-type="indexterm" id="idm46066896457960"/>The <a href="https://xgboost.readthedocs.io">XGBoost</a> library has the <code>max_delta_step</code> parameter, which can be set&#13;
from 1 to 10 to make the update step more conservative. It also has the&#13;
<code>scale_pos_weight</code> parameter that sets the ratio of negative to&#13;
positive samples (for binary classes). Also, the <code>eval_metric</code> should be set to&#13;
<code>'auc'</code> rather than the default value of <code>'error'</code> for classification.</p>&#13;
&#13;
<p><a data-primary="k-nearest neighbor (KNN)" data-secondary="weights parameter" data-type="indexterm" id="idm46066896453416"/>The KNN model has a <code>weights</code> parameter that can bias neighbors that are&#13;
closer. If the minority class samples are close together,&#13;
setting this parameter to <code>'distance'</code> may improve performance.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Upsampling Minority" data-type="sect1"><div class="sect1" id="idm46066896451064">&#13;
<h1>Upsampling Minority</h1>&#13;
&#13;
<p><a data-primary="imbalanced classes, managing" data-secondary="upsampling the minority class" data-type="indexterm" id="idm46066896449624"/><a data-primary="minority class" data-secondary="upsampling" data-type="indexterm" id="idm46066896448568"/><a data-primary="sklearn" data-secondary="upsampling minority class" data-type="indexterm" id="idm46066896447624"/><a data-primary="upsampling" data-type="indexterm" id="idm46066896446664"/>You can upsample the minority class in a couple of ways. Here is an sklearn&#13;
implementation:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.utils</code> <code class="kn">import</code> <code class="n">resample</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">mask</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">survived</code> <code class="o">==</code> <code class="mi">1</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">surv_df</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="n">mask</code><code class="p">]</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">death_df</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="o">~</code><code class="n">mask</code><code class="p">]</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">df_upsample</code> <code class="o">=</code> <code class="n">resample</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">surv_df</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">replace</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">n_samples</code><code class="o">=</code><code class="nb">len</code><code class="p">(</code><code class="n">death_df</code><code class="p">),</code>&#13;
<code class="gp">... </code>    <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">df2</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">([</code><code class="n">death_df</code><code class="p">,</code> <code class="n">df_upsample</code><code class="p">])</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">df2</code><code class="o">.</code><code class="n">survived</code><code class="o">.</code><code class="n">value_counts</code><code class="p">()</code>&#13;
<code class="go">1    809</code>&#13;
<code class="go">0    809</code>&#13;
<code class="go">Name: survived, dtype: int64</code></pre>&#13;
&#13;
<p><a data-primary="imbalanced-learn" data-secondary="upsampling minority class" data-type="indexterm" id="idm46066896444360"/>We can also use the imbalanced-learn library to randomly sample with replacement:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">imblearn.over_sampling</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">RandomOverSampler</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">ros</code> <code class="o">=</code> <code class="n">RandomOverSampler</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">X_ros</code><code class="p">,</code> <code class="n">y_ros</code> <code class="o">=</code> <code class="n">ros</code><code class="o">.</code><code class="n">fit_sample</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">y_ros</code><code class="p">)</code><code class="o">.</code><code class="n">value_counts</code><code class="p">()</code>&#13;
<code class="go">1    809</code>&#13;
<code class="go">0    809</code>&#13;
<code class="go">dtype: int64</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Generate Minority Data" data-type="sect1"><div class="sect1" id="idm46066896323752">&#13;
<h1>Generate Minority Data</h1>&#13;
&#13;
<p><a data-primary="Adaptive Synthetic (ADASYN)" data-type="indexterm" id="idm46066896211048"/><a data-primary="imbalanced classes, managing" data-secondary="generating minority data" data-type="indexterm" id="idm46066896210408"/><a data-primary="minority class" data-secondary="generating new samples of" data-type="indexterm" id="idm46066896209528"/><a data-primary="Synthetic Minority Over-sampling Technique (SMOTE)" data-type="indexterm" id="idm46066896208616"/>The imbalanced-learn library can also generate new samples of minority&#13;
classes with both the Synthetic Minority Over-sampling Technique (SMOTE)&#13;
and Adaptive Synthetic (ADASYN) sampling approach algorithms. SMOTE&#13;
works by choosing one of its k-nearest neighbors, connecting a line to&#13;
one of them, and choosing a point along that line. ADASYN is similar to&#13;
SMOTE, but generates more samples from those that are harder to learn.&#13;
The classes in imbanced-learn are named <code>over_sampling.SMOTE</code> and&#13;
<code>over_sampling.ADASYN</code>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Downsampling Majority" data-type="sect1"><div class="sect1" id="idm46066896206152">&#13;
<h1>Downsampling Majority</h1>&#13;
&#13;
<p><a data-primary="downsampling" data-type="indexterm" id="idm46066896204680"/><a data-primary="imbalanced classes, managing" data-secondary="downsampling majority classes" data-type="indexterm" id="idm46066896203976"/><a data-primary="majority classes" data-type="indexterm" id="idm46066896203000"/><a data-primary="sklearn" data-secondary="downsampling majority classes" data-type="indexterm" id="idm46066896202328"/>Another method to balance classes is to downsample majority classes. Here is an sklearn example:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.utils</code> <code class="kn">import</code> <code class="n">resample</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">mask</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">survived</code> <code class="o">==</code> <code class="mi">1</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">surv_df</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="n">mask</code><code class="p">]</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">death_df</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="o">~</code><code class="n">mask</code><code class="p">]</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">df_downsample</code> <code class="o">=</code> <code class="n">resample</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">death_df</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">replace</code><code class="o">=</code><code class="bp">False</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">n_samples</code><code class="o">=</code><code class="nb">len</code><code class="p">(</code><code class="n">surv_df</code><code class="p">),</code>&#13;
<code class="gp">... </code>    <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">df3</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">([</code><code class="n">surv_df</code><code class="p">,</code> <code class="n">df_downsample</code><code class="p">])</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">df3</code><code class="o">.</code><code class="n">survived</code><code class="o">.</code><code class="n">value_counts</code><code class="p">()</code>&#13;
<code class="go">1    500</code>&#13;
<code class="go">0    500</code>&#13;
<code class="go">Name: survived, dtype: int64</code></pre>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>Don’t use replacement when downsampling.</p>&#13;
</div>&#13;
&#13;
<p><a data-primary="imbalanced-learn" data-secondary="downsampling algorithms" data-type="indexterm" id="idm46066896292232"/>The imbalanced-learn library also implements various downsampling&#13;
algorithms:</p>&#13;
<dl>&#13;
<dt><code>ClusterCentroids</code></dt>&#13;
<dd>&#13;
<p>This class uses K-means to synthesize data&#13;
with the <span class="keep-together">centroids.</span></p>&#13;
</dd>&#13;
<dt><code>RandomUnderSampler</code></dt>&#13;
<dd>&#13;
<p>This class randomly selects samples.</p>&#13;
</dd>&#13;
<dt><code>NearMiss</code></dt>&#13;
<dd>&#13;
<p>This class uses nearest neighbors to downsample.</p>&#13;
</dd>&#13;
<dt><code>TomekLink</code></dt>&#13;
<dd>&#13;
<p>This class downsamples by removing samples that are close to each other.</p>&#13;
</dd>&#13;
<dt><code>EditedNearestNeighbours</code></dt>&#13;
<dd>&#13;
<p>This class removes samples that have neighbors that are either not in the majority or all of the same class.</p>&#13;
</dd>&#13;
<dt><code>RepeatedNearestNeighbours</code></dt>&#13;
<dd>&#13;
<p>This class repeatedly calls the&#13;
<code>EditedNearestNeighbours</code>.</p>&#13;
</dd>&#13;
<dt><code>AllKNN</code></dt>&#13;
<dd>&#13;
<p>This class is similar but increases the number of nearest neighbors during the iterations of downsampling.</p>&#13;
</dd>&#13;
<dt><code>CondensedNearestNeighbour</code></dt>&#13;
<dd>&#13;
<p>This class picks one sample of the class to be downsampled, then iterates through the other samples of the class, and if KNN doesn’t misclassify, it adds that sample.</p>&#13;
</dd>&#13;
<dt><code>OneSidedSelection</code></dt>&#13;
<dd>&#13;
<p>This classremoves noisy samples.</p>&#13;
</dd>&#13;
<dt><code>NeighbourhoodCleaningRule</code></dt>&#13;
<dd>&#13;
<p>This class uses <code>EditedNearestNeighbours</code> results and applies KNN to it.</p>&#13;
</dd>&#13;
<dt><code>InstanceHardnessThreshold</code></dt>&#13;
<dd>&#13;
<p>This class trains a model, then removes samples with low probabilities.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>All of these classes support the <code>.fit_sample</code> method.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Upsampling Then Downsampling" data-type="sect1"><div class="sect1" id="idm46066896096536">&#13;
<h1>Upsampling Then Downsampling</h1>&#13;
&#13;
<p><a data-primary="imbalanced classes, managing" data-secondary="upsampling then downsampling" data-type="indexterm" id="idm46066896095368"/><a data-primary="imbalanced-learn" data-secondary="upsampling then downsampling" data-type="indexterm" id="idm46066896094312"/>The imbalanced-learn library implements <code>SMOTEENN</code> and <code>SMOTETomek</code>,&#13;
which both upsample and then apply downsampling to clean up the data.<a data-startref="ix_ch09-asciidoc0" data-type="indexterm" id="idm46066896092328"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>