- en: Chapter 9\. Dimensionality Reduction Using Feature Extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 9.0 Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is common to have access to thousands and even hundreds of thousands of features.
    For example, in [Chapter 8](ch08.xhtml#handling-images) we transformed a 256 ×
    256–pixel color image into 196,608 features. Furthermore, because each of these
    pixels can take one of 256 possible values, our observation can take 256^(196608)
    different configurations. Many machine learning algorithms have trouble learning
    from such data, because it will never be practical to collect enough observations
    for the algorithms to operate correctly. Even in more tabular, structured datasets
    we can easily end up with thousands of features after the feature engineering
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, not all features are created equal, and the goal of *feature extraction*
    for dimensionality reduction is to transform our set of features, *p[original]*,
    such that we end up with a new set, *p[new]*, where *p[original] > p[new]*, while
    still keeping much of the underlying information. Put another way, we reduce the
    number of features with only a small loss in our data’s ability to generate high-quality
    predictions. In this chapter, we will cover a number of feature extraction techniques
    to do just this.
  prefs: []
  type: TYPE_NORMAL
- en: One downside of the feature extraction techniques we discuss is that the new
    features we generate will not be interpretable by humans. They will contain as
    much or nearly as much ability to train our models but will appear to the human
    eye as a collection of random numbers. If we wanted to maintain our ability to
    interpret our models, dimensionality reduction through *feature selection* is
    a better option (and will be discussed in [Chapter 10](ch10.xhtml#dimensionality-reduction-using-feature-selection)).
    During feature selection we remove features we deem unimportant but keep other
    features as they currently are. Although this may not let us keep information
    from all features as feature extraction does, it leaves the features we don’t
    drop intact—and therefore fully interpretable by humans during analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1 Reducing Features Using Principal Components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given a set of features, you want to reduce the number of features while retaining
    the variance (important information) in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use principal component analysis with scikit’s `PCA`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Principal component analysis* (PCA) is a popular linear dimensionality reduction
    technique. PCA projects observations onto the (hopefully fewer) principal components
    of the feature matrix that retain the most *variance* in the data, which, practically,
    means we retain information. PCA is an unsupervised technique, meaning that it
    does not use the information from the target vector and instead only considers
    the feature matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: For a mathematical description of how PCA works, see the external resources
    listed at the end of this recipe. However, we can understand the intuition behind
    PCA using a simple example. In [Figure 9-1](#figure0901), our data contains two
    features, <math display="inline"><msub><mi>x</mi><mn>1</mn></msub></math> and
    <math display="inline"><msub><mi>x</mi><mn>2</mn></msub></math>. Looking at the
    visualization, it should be clear that observations are spread out like a cigar,
    with a lot of length and very little height. More specifically, we can say that
    the variance of the “length” is significantly greater than the “height.” Instead
    of length and height, we refer to the “direction” with the most variance as the
    first principal component and the “direction” with the second-most variance as
    the second principal component (and so on).
  prefs: []
  type: TYPE_NORMAL
- en: If we wanted to reduce our features, one strategy would be to project all observations
    in our two-dimensional space onto the one-dimensional principal component. We
    would lose the information captured in the second principal component, but in
    some situations that would be an acceptable trade-off. This is PCA.
  prefs: []
  type: TYPE_NORMAL
- en: '![mpc2 0901](assets/mpc2_0901.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-1\. The first and second principal components of PCA
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: PCA is implemented in scikit-learn using the `PCA` class. `n_components` has
    two operations, depending on the argument provided. If the argument is greater
    than 1, `pca` will return that many features. This leads to the question of how
    to select the optimal number of features. Fortunately, if the argument to `n_components`
    is between 0 and 1, `pca` returns the minimum number of features that retain that
    much variance. It’s common to use values of 0.95 and 0.99, meaning 95% and 99%
    of the variance of the original features has been retained, respectively. `whiten=True`
    transforms the values of each principal component so that they have zero mean
    and unit variance. Another parameter and argument is `svd_solver="randomized"`,
    which implements a stochastic algorithm to find the first principal components
    in often significantly less time.
  prefs: []
  type: TYPE_NORMAL
- en: The output of our solution shows that PCA enables us to reduce our dimensionality
    by 10 features while still retaining 99% of the information (variance) in the
    feature matrix.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[scikit-learn documentation: PCA](https://oreil.ly/OT_gN)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Principal Component Analysis with Linear Algebra, Jeff Jauregui](https://oreil.ly/Uns61)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 9.2 Reducing Features When Data Is Linearly Inseparable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You suspect you have linearly inseparable data and want to reduce the dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use an extension of principal component analysis that uses kernels to allow
    for nonlinear dimensionality reduction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PCA is able to reduce the dimensionality of our feature matrix (i.e., the number
    of features). Standard PCA uses linear projection to reduce the features. If the
    data is *linearly separable* (i.e., you can draw a straight line or hyperplane
    between different classes) then PCA works well. However, if your data is not linearly
    separable (i.e., you can only separate classes using a curved decision boundary),
    the linear transformation will not work as well. In our solution we used scikit-learn’s
    `make_circles` to generate a simulated dataset with a target vector of two classes
    and two features. `make_circles` makes linearly inseparable data; specifically,
    one class is surrounded on all sides by the other class, as shown in [Figure 9-2](#figure0902).
  prefs: []
  type: TYPE_NORMAL
- en: '![mpc2 0902](assets/mpc2_0902.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-2\. The first principal component projected on linearly inseparable
    data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If we used linear PCA to reduce the dimensions of our data, the two classes
    would be linearly projected onto the first principal component such that they
    would become intertwined, as shown in [Figure 9-3](#figure0903).
  prefs: []
  type: TYPE_NORMAL
- en: '![mpc2 0903](assets/mpc2_0903.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-3\. The first principal component of linearly inseparable data without
    kernel PCA
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Ideally, we would want a transformation that would reduce the dimensions and
    make the data linearly separable. Kernel PCA can do both, as shown in [Figure 9-4](#figure0904).
  prefs: []
  type: TYPE_NORMAL
- en: '![mpc2 0904](assets/mpc2_0904.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-4\. The first principal component of linearly inseparable data *with*
    kernel PCA
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Kernels allow us to project the linearly inseparable data into a higher dimension
    where it is linearly separable; this is called the “kernel trick.” Don’t worry
    if you don’t understand the details of the kernel trick; just think of kernels
    as different ways of projecting the data. There are a number of kernels we can
    use in scikit-learn’s `kernelPCA` class, specified using the `kernel` parameter.
    A common kernel to use is the Gaussian radial basis function kernel `rbf`, but
    other options are the polynomial kernel (`poly`) and sigmoid kernel (`sigmoid`).
    We can even specify a linear projection (`linear`), which will produce the same
    results as standard PCA.
  prefs: []
  type: TYPE_NORMAL
- en: One downside of kernel PCA is that we need to specify a number of parameters.
    For example, in [Recipe 9.1](#reducing-features-using-principal-components) we
    set `n_components` to `0.99` to make `PCA` select the number of components to
    retain 99% of the variance. We don’t have this option in kernel PCA. Instead we
    have to define the number of components (e.g., `n_components=1`). Furthermore,
    kernels come with their own hyperparameters that we will have to set; for example,
    the radial basis function requires a `gamma` value.
  prefs: []
  type: TYPE_NORMAL
- en: So how do we know which values to use? Through trial and error. Specifically,
    we can train our machine learning model multiple times, each time with a different
    kernel or different value of the parameter. Once we find the combination of values
    that produces the highest quality predicted values, we are done. This is a common
    theme in machine learning, and we will learn about this strategy in depth in [Chapter 12](ch12.xhtml#model-selection).
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[scikit-learn documentation on Kernel PCA](https://oreil.ly/SCAX-)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Kernel Tricks and Nonlinear Dimensionality Reduction via RBF Kernel PCA](https://oreil.ly/ktm5Z)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 9.3 Reducing Features by Maximizing Class Separability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to reduce the number of features to be used by a classifier by maximizing
    the separation between the classes.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Try *linear discriminant analysis* (LDA) to project the features onto component
    axes that maximize the separation of classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use `explained_variance_ratio_` to view the amount of variance explained
    by each component. In our solution the single component explained over 99% of
    the variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LDA is a classification that is also a popular technique for dimensionality
    reduction. LDA works similarly to PCA in that it projects our feature space onto
    a lower-dimensional space. However, in PCA we were interested only in the component
    axes that maximize the variance in the data, while in LDA we have the additional
    goal of maximizing the differences between classes. In [Figure 9-5](#figure0905),
    we have data comprising two target classes and two features. If we project the
    data onto the y-axis, the two classes are not easily separable (i.e., they overlap),
    while if we project the data onto the x-axis, we are left with a feature vector
    (i.e., we reduced our dimensionality by one) that still preserves class separability.
    In the real world, of course, the relationship between the classes will be more
    complex and the dimensionality will be higher, but the concept remains the same.
  prefs: []
  type: TYPE_NORMAL
- en: '![mpc2 0905](assets/mpc2_0905.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-5\. LDA attempts to maximize the difference between our classes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In scikit-learn, LDA is implemented using `LinearDiscriminantAnalysis`, which
    includes a parameter, `n_components`, indicating the number of features we want
    returned. To figure out what argument value to use with `n_components` (e.g.,
    how many parameters to keep), we can take advantage of the fact that `explained_variance_ratio_`
    tells us the variance explained by each outputted feature and is a sorted array.
    For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Specifically, we can run `LinearDiscriminantAnalysis` with `n_components` set
    to `None` to return the ratio of variance explained by every component feature,
    then calculate how many components are required to get above some threshold of
    variance explained (often 0.95 or 0.99):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Comparison of LDA and PCA 2D Projection of Iris Dataset](https://oreil.ly/evGsx)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Linear Discriminant Analysis](https://oreil.ly/uOB81)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 9.4 Reducing Features Using Matrix Factorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have a feature matrix of nonnegative values and want to reduce the dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use *nonnegative matrix factorization* (NMF) to reduce the dimensionality of
    the feature matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'NMF is an unsupervised technique for linear dimensionality reduction that *factorizes*
    (i.e., breaks up into multiple matrices whose product approximates the original
    matrix) the feature matrix into matrices representing the latent relationship
    between observations and their features. Intuitively, NMF can reduce dimensionality
    because in matrix multiplication, the two factors (matrices being multiplied)
    can have significantly fewer dimensions than the product matrix. Formally, given
    a desired number of returned features, *r*, NMF factorizes our feature matrix
    such that:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi mathvariant="bold">V</mi> <mo>≈</mo> <mi mathvariant="bold">W</mi>
    <mi mathvariant="bold">H</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where **V** is our *n × d* feature matrix (i.e., *d* features, *n* observations),
    **W** is an *n* × *r* matrix, and **H** is an *r* × *d* matrix. By adjusting the
    value of *r* we can set the amount of dimensionality reduction desired.
  prefs: []
  type: TYPE_NORMAL
- en: One major requirement of NMF is that, as the name implies, the feature matrix
    cannot contain negative values. Additionally, unlike PCA and other techniques
    we have examined, NMF does not provide the explained variance of the outputted
    features. Thus, the best way for us to find the optimum value of `n_components`
    is by trying a range of values to find the one that produces the best result in
    our end model (see [Chapter 12](ch12.xhtml#model-selection)).
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Non-negative matrix factorization, Wikipedia](https://oreil.ly/HJ_Qe)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 9.5 Reducing Features on Sparse Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have a sparse feature matrix and want to reduce the dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use *Truncated Singular Value Decomposition* (TSVD):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TSVD is similar to PCA and, in fact, PCA often uses nontruncated *Singular Value
    Decomposition* (SVD) in one of its steps. Given *d* features, SVD will create
    factor matrices that are *d* × *d*, whereas TSVD will return factors that are
    *n* × *n*, where *n* is previously specified by a parameter. The practical advantage
    of TSVD is that, unlike PCA, it works on sparse feature matrices.
  prefs: []
  type: TYPE_NORMAL
- en: 'One issue with TSVD: because of how it uses a random number generator, the
    signs of the output can flip between fittings. An easy workaround is to use `fit`
    only once per preprocessing pipeline, then use `transform` multiple times.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As with linear discriminant analysis, we have to specify the number of features
    (components) we want to output. This is done with the `n_components` parameter.
    A natural question is: what is the optimum number of components? One strategy
    is to include `n_components` as a hyperparameter to optimize during model selection
    (i.e., choose the value for `n_components` that produces the best-trained model).
    Alternatively, because TSVD provides the ratio of the original feature matrix’s
    variance explained by each component, we can select the number of components that
    explain a desired amount of variance (95% and 99% are common values). For example,
    in our solution, the first three outputted components explain approximately 30%
    of the original data’s variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can automate the process by creating a function that runs TSVD with `n_components`
    set to one less than the number of original features and then calculate the number
    of components that explain a desired amount of the original data’s variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[scikit-learn documentation: TruncatedSVD](https://oreil.ly/nD1pF)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
