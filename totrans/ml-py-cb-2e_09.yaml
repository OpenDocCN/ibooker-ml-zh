- en: Chapter 9\. Dimensionality Reduction Using Feature Extraction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 9 章\. 使用特征提取进行降维
- en: 9.0 Introduction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9.0 介绍
- en: It is common to have access to thousands and even hundreds of thousands of features.
    For example, in [Chapter 8](ch08.xhtml#handling-images) we transformed a 256 ×
    256–pixel color image into 196,608 features. Furthermore, because each of these
    pixels can take one of 256 possible values, our observation can take 256^(196608)
    different configurations. Many machine learning algorithms have trouble learning
    from such data, because it will never be practical to collect enough observations
    for the algorithms to operate correctly. Even in more tabular, structured datasets
    we can easily end up with thousands of features after the feature engineering
    process.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我们可以接触到成千上万的特征。例如，在 [第 8 章](ch08.xhtml#handling-images) 中，我们将一个 256 ×
    256 像素的彩色图像转换成了 196,608 个特征。此外，因为每个像素可以取 256 个可能的值，我们的观察可以有 256^(196608) 种不同的配置。许多机器学习算法在学习这样的数据时会遇到困难，因为收集足够的观察数据使算法能够正确运行是不现实的。即使在更结构化的表格数据集中，经过特征工程处理后，我们很容易就能得到数千个特征。
- en: Fortunately, not all features are created equal, and the goal of *feature extraction*
    for dimensionality reduction is to transform our set of features, *p[original]*,
    such that we end up with a new set, *p[new]*, where *p[original] > p[new]*, while
    still keeping much of the underlying information. Put another way, we reduce the
    number of features with only a small loss in our data’s ability to generate high-quality
    predictions. In this chapter, we will cover a number of feature extraction techniques
    to do just this.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，并非所有的特征都是相同的，*特征提取* 的目标是为了降低维度，将我们的特征集合 *p[original]* 转换成一个新的集合 *p[new]*，使得
    *p[original] > p[new]*，同时保留大部分底层信息。换句话说，我们通过仅有少量数据丢失来减少特征数目，而数据仍能生成高质量的预测。在本章中，我们将涵盖多种特征提取技术来实现这一目标。
- en: One downside of the feature extraction techniques we discuss is that the new
    features we generate will not be interpretable by humans. They will contain as
    much or nearly as much ability to train our models but will appear to the human
    eye as a collection of random numbers. If we wanted to maintain our ability to
    interpret our models, dimensionality reduction through *feature selection* is
    a better option (and will be discussed in [Chapter 10](ch10.xhtml#dimensionality-reduction-using-feature-selection)).
    During feature selection we remove features we deem unimportant but keep other
    features as they currently are. Although this may not let us keep information
    from all features as feature extraction does, it leaves the features we don’t
    drop intact—and therefore fully interpretable by humans during analysis.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论的特征提取技术的一个缺点是，我们生成的新特征对人类来说是不可解释的。它们将具有与训练模型几乎相同的能力，但在人眼中看起来像是一组随机数。如果我们希望保留解释模型的能力，通过*特征选择*进行降维是更好的选择（将在
    [第 10 章](ch10.xhtml#dimensionality-reduction-using-feature-selection) 中讨论）。在特征选择期间，我们会移除我们认为不重要的特征，但保留其他特征。虽然这可能不像特征提取那样保留所有特征的信息，但它保留了我们不删除的特征——因此在分析过程中完全可解释。
- en: 9.1 Reducing Features Using Principal Components
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9.1 使用主成分减少特征
- en: Problem
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Given a set of features, you want to reduce the number of features while retaining
    the variance (important information) in the data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组特征，你希望减少特征数目同时保留数据中的方差（重要信息）。
- en: Solution
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use principal component analysis with scikit’s `PCA`:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 scikit 的 `PCA` 进行主成分分析：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Discussion
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: '*Principal component analysis* (PCA) is a popular linear dimensionality reduction
    technique. PCA projects observations onto the (hopefully fewer) principal components
    of the feature matrix that retain the most *variance* in the data, which, practically,
    means we retain information. PCA is an unsupervised technique, meaning that it
    does not use the information from the target vector and instead only considers
    the feature matrix.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*主成分分析* (PCA) 是一种流行的线性降维技术。PCA 将观察结果投影到特征矩阵的（希望较少的）主成分上，这些主成分保留了数据中最多的*方差*，从实际上来说，我们保留了信息。PCA
    是一种无监督技术，意味着它不使用目标向量的信息，而只考虑特征矩阵。'
- en: For a mathematical description of how PCA works, see the external resources
    listed at the end of this recipe. However, we can understand the intuition behind
    PCA using a simple example. In [Figure 9-1](#figure0901), our data contains two
    features, <math display="inline"><msub><mi>x</mi><mn>1</mn></msub></math> and
    <math display="inline"><msub><mi>x</mi><mn>2</mn></msub></math>. Looking at the
    visualization, it should be clear that observations are spread out like a cigar,
    with a lot of length and very little height. More specifically, we can say that
    the variance of the “length” is significantly greater than the “height.” Instead
    of length and height, we refer to the “direction” with the most variance as the
    first principal component and the “direction” with the second-most variance as
    the second principal component (and so on).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解PCA的工作原理的数学描述，请参阅本食谱末尾列出的外部资源。然而，我们可以使用一个简单的例子来理解PCA的直觉。在[图9-1](#figure0901)中，我们的数据包含两个特征，<math
    display="inline"><msub><mi>x</mi><mn>1</mn></msub></math>和<math display="inline"><msub><mi>x</mi><mn>2</mn></msub></math>。通过观察可视化结果，应该清楚地看到观察结果像雪茄一样散开，长度很长，高度很低。更具体地说，我们可以说“长度”的方差显著大于“高度”的方差。我们将“方向”中的最大方差称为第一主成分，“方向”中的第二大方差称为第二主成分（依此类推）。
- en: If we wanted to reduce our features, one strategy would be to project all observations
    in our two-dimensional space onto the one-dimensional principal component. We
    would lose the information captured in the second principal component, but in
    some situations that would be an acceptable trade-off. This is PCA.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要减少特征，一个策略是将我们二维空间中的所有观察投影到一维主成分上。我们会丢失第二主成分中捕获的信息，但在某些情况下，这是可以接受的权衡。这就是PCA。
- en: '![mpc2 0901](assets/mpc2_0901.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![mpc2 0901](assets/mpc2_0901.png)'
- en: Figure 9-1\. The first and second principal components of PCA
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-1\. PCA的第一和第二主成分
- en: PCA is implemented in scikit-learn using the `PCA` class. `n_components` has
    two operations, depending on the argument provided. If the argument is greater
    than 1, `pca` will return that many features. This leads to the question of how
    to select the optimal number of features. Fortunately, if the argument to `n_components`
    is between 0 and 1, `pca` returns the minimum number of features that retain that
    much variance. It’s common to use values of 0.95 and 0.99, meaning 95% and 99%
    of the variance of the original features has been retained, respectively. `whiten=True`
    transforms the values of each principal component so that they have zero mean
    and unit variance. Another parameter and argument is `svd_solver="randomized"`,
    which implements a stochastic algorithm to find the first principal components
    in often significantly less time.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: PCA在scikit-learn中是通过`PCA`类实现的。`n_components`有两个操作，取决于提供的参数。如果参数大于1，`pca`将返回那么多特征。这引出了如何选择最优特征数量的问题。幸运的是，如果`n_components`的参数在0到1之间，`pca`将返回保留原始特征方差百分之多少的最小特征数。通常使用0.95和0.99的值，分别表示保留原始特征方差的95%和99%。`whiten=True`将每个主成分的值转换为具有零均值和单位方差。另一个参数和参数是`svd_solver="randomized"`，它实现了一种随机算法，通常能够在更短的时间内找到第一个主成分。
- en: The output of our solution shows that PCA enables us to reduce our dimensionality
    by 10 features while still retaining 99% of the information (variance) in the
    feature matrix.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的解决方案的输出显示，PCA使我们能够通过减少10个特征的维度，同时仍保留特征矩阵中99%的信息（方差）。
- en: See Also
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[scikit-learn documentation: PCA](https://oreil.ly/OT_gN)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[scikit-learn文档：PCA](https://oreil.ly/OT_gN)'
- en: '[Principal Component Analysis with Linear Algebra, Jeff Jauregui](https://oreil.ly/Uns61)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[线性代数中的主成分分析，Jeff Jauregui](https://oreil.ly/Uns61)'
- en: 9.2 Reducing Features When Data Is Linearly Inseparable
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9.2 在数据线性不可分时减少特征
- en: Problem
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You suspect you have linearly inseparable data and want to reduce the dimensions.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 您怀疑您的数据是线性不可分的，并希望降低维度。
- en: Solution
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use an extension of principal component analysis that uses kernels to allow
    for nonlinear dimensionality reduction:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用使用核函数的主成分分析的扩展，以实现非线性降维：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Discussion
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: PCA is able to reduce the dimensionality of our feature matrix (i.e., the number
    of features). Standard PCA uses linear projection to reduce the features. If the
    data is *linearly separable* (i.e., you can draw a straight line or hyperplane
    between different classes) then PCA works well. However, if your data is not linearly
    separable (i.e., you can only separate classes using a curved decision boundary),
    the linear transformation will not work as well. In our solution we used scikit-learn’s
    `make_circles` to generate a simulated dataset with a target vector of two classes
    and two features. `make_circles` makes linearly inseparable data; specifically,
    one class is surrounded on all sides by the other class, as shown in [Figure 9-2](#figure0902).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: PCA能够减少我们特征矩阵的维度（即特征数量）。标准PCA使用线性投影来减少特征。如果数据是*线性可分*的（即你可以在不同类别之间画一条直线或超平面），那么PCA效果很好。然而，如果你的数据不是线性可分的（即你只能使用曲线决策边界来分离类别），线性变换效果就不那么好了。在我们的解决方案中，我们使用了scikit-learn的`make_circles`来生成一个模拟数据集，其中包含两个类别和两个特征的目标向量。`make_circles`生成线性不可分的数据；具体来说，一个类别被另一个类别包围在所有方向上，如[图 9-2](#figure0902)所示。
- en: '![mpc2 0902](assets/mpc2_0902.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![mpc2 0902](assets/mpc2_0902.png)'
- en: Figure 9-2\. The first principal component projected on linearly inseparable
    data
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-2\. 线性不可分数据上投影的第一个主成分
- en: If we used linear PCA to reduce the dimensions of our data, the two classes
    would be linearly projected onto the first principal component such that they
    would become intertwined, as shown in [Figure 9-3](#figure0903).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用线性PCA来降低数据维度，那么这两个类别将线性投影到第一个主成分上，使它们变得交织在一起，如[图 9-3](#figure0903)所示。
- en: '![mpc2 0903](assets/mpc2_0903.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![mpc2 0903](assets/mpc2_0903.png)'
- en: Figure 9-3\. The first principal component of linearly inseparable data without
    kernel PCA
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-3\. 线性不可分数据的第一个主成分，没有核PCA
- en: Ideally, we would want a transformation that would reduce the dimensions and
    make the data linearly separable. Kernel PCA can do both, as shown in [Figure 9-4](#figure0904).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望进行一种转换，能够减少维度并使数据线性可分。核PCA可以做到这两点，如[图 9-4](#figure0904)所示。
- en: '![mpc2 0904](assets/mpc2_0904.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![mpc2 0904](assets/mpc2_0904.png)'
- en: Figure 9-4\. The first principal component of linearly inseparable data *with*
    kernel PCA
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-4\. 带有核PCA的线性不可分数据的第一个主成分
- en: Kernels allow us to project the linearly inseparable data into a higher dimension
    where it is linearly separable; this is called the “kernel trick.” Don’t worry
    if you don’t understand the details of the kernel trick; just think of kernels
    as different ways of projecting the data. There are a number of kernels we can
    use in scikit-learn’s `kernelPCA` class, specified using the `kernel` parameter.
    A common kernel to use is the Gaussian radial basis function kernel `rbf`, but
    other options are the polynomial kernel (`poly`) and sigmoid kernel (`sigmoid`).
    We can even specify a linear projection (`linear`), which will produce the same
    results as standard PCA.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 核函数允许我们将线性不可分的数据投影到一个更高维度，使其线性可分；这被称为“核技巧”。如果你不理解核技巧的细节也不要担心；只需把核函数看作是投影数据的不同方法。在scikit-learn的`kernelPCA`类中，我们可以使用多种核，通过`kernel`参数指定。一个常用的核是高斯径向基函数核`rbf`，但其他选项包括多项式核（`poly`）和sigmoid核（`sigmoid`）。我们甚至可以指定一个线性投影（`linear`），它将产生与标准PCA相同的结果。
- en: One downside of kernel PCA is that we need to specify a number of parameters.
    For example, in [Recipe 9.1](#reducing-features-using-principal-components) we
    set `n_components` to `0.99` to make `PCA` select the number of components to
    retain 99% of the variance. We don’t have this option in kernel PCA. Instead we
    have to define the number of components (e.g., `n_components=1`). Furthermore,
    kernels come with their own hyperparameters that we will have to set; for example,
    the radial basis function requires a `gamma` value.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 核PCA的一个缺点是我们需要指定一些参数。例如，在[第9.1节](#reducing-features-using-principal-components)中，我们将`n_components`设置为`0.99`，以便使`PCA`选择保留99%方差的成分数量。在核PCA中我们没有这个选项。相反，我们必须定义成分的数量（例如`n_components=1`）。此外，核函数自带它们自己的超参数，我们需要设置；例如，径向基函数需要一个`gamma`值。
- en: So how do we know which values to use? Through trial and error. Specifically,
    we can train our machine learning model multiple times, each time with a different
    kernel or different value of the parameter. Once we find the combination of values
    that produces the highest quality predicted values, we are done. This is a common
    theme in machine learning, and we will learn about this strategy in depth in [Chapter 12](ch12.xhtml#model-selection).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何知道使用哪些值？通过试错。具体而言，我们可以多次训练我们的机器学习模型，每次使用不同的核函数或不同的参数值。一旦找到产生最高质量预测值组合的值，我们就完成了。这是机器学习中的一个常见主题，我们将在[第12章](ch12.xhtml#model-selection)深入学习这一策略。
- en: See Also
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[scikit-learn documentation on Kernel PCA](https://oreil.ly/SCAX-)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kernel PCA的scikit-learn文档](https://oreil.ly/SCAX-)'
- en: '[Kernel Tricks and Nonlinear Dimensionality Reduction via RBF Kernel PCA](https://oreil.ly/ktm5Z)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过RBF Kernel PCA进行核技巧和非线性降维](https://oreil.ly/ktm5Z)'
- en: 9.3 Reducing Features by Maximizing Class Separability
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9.3 通过最大化类别可分性来减少特征
- en: Problem
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to reduce the number of features to be used by a classifier by maximizing
    the separation between the classes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望通过最大化类别之间的分离来减少分类器使用的特征数量。
- en: Solution
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Try *linear discriminant analysis* (LDA) to project the features onto component
    axes that maximize the separation of classes:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试*线性判别分析*（LDA）将特征投影到最大化类别分离的组件轴上：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can use `explained_variance_ratio_` to view the amount of variance explained
    by each component. In our solution the single component explained over 99% of
    the variance:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`explained_variance_ratio_`来查看每个组件解释的方差量。在我们的解决方案中，单个组件解释了超过99%的方差：
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Discussion
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: LDA is a classification that is also a popular technique for dimensionality
    reduction. LDA works similarly to PCA in that it projects our feature space onto
    a lower-dimensional space. However, in PCA we were interested only in the component
    axes that maximize the variance in the data, while in LDA we have the additional
    goal of maximizing the differences between classes. In [Figure 9-5](#figure0905),
    we have data comprising two target classes and two features. If we project the
    data onto the y-axis, the two classes are not easily separable (i.e., they overlap),
    while if we project the data onto the x-axis, we are left with a feature vector
    (i.e., we reduced our dimensionality by one) that still preserves class separability.
    In the real world, of course, the relationship between the classes will be more
    complex and the dimensionality will be higher, but the concept remains the same.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: LDA是一个分类技术，也是一种流行的降维技术。LDA的工作方式类似于PCA，即将我们的特征空间投影到较低维空间上。然而，在PCA中，我们只对最大化数据方差的组件轴感兴趣，而在LDA中，我们还有额外的目标是最大化类别之间的差异。在[图9-5](#figure0905)中，我们有包含两个目标类别和两个特征的数据。如果我们将数据投影到y轴上，两个类别不容易分开（即它们重叠），而如果我们将数据投影到x轴上，我们将得到一个特征向量（即我们通过减少一个维度），它仍然保留了类别可分性。在现实世界中，当然，类别之间的关系会更复杂，维度会更高，但概念保持不变。
- en: '![mpc2 0905](assets/mpc2_0905.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![mpc2 0905](assets/mpc2_0905.png)'
- en: Figure 9-5\. LDA attempts to maximize the difference between our classes
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-5\. LDA试图最大化我们类别之间的差异
- en: 'In scikit-learn, LDA is implemented using `LinearDiscriminantAnalysis`, which
    includes a parameter, `n_components`, indicating the number of features we want
    returned. To figure out what argument value to use with `n_components` (e.g.,
    how many parameters to keep), we can take advantage of the fact that `explained_variance_ratio_`
    tells us the variance explained by each outputted feature and is a sorted array.
    For example:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，LDA使用`LinearDiscriminantAnalysis`实现，其中包括一个参数`n_components`，表示我们希望返回的特征数量。要确定`n_components`参数值（例如，要保留多少个参数），我们可以利用`explained_variance_ratio_`告诉我们每个输出特征解释的方差，并且是一个排序数组。例如：
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Specifically, we can run `LinearDiscriminantAnalysis` with `n_components` set
    to `None` to return the ratio of variance explained by every component feature,
    then calculate how many components are required to get above some threshold of
    variance explained (often 0.95 or 0.99):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 具体地，我们可以运行`LinearDiscriminantAnalysis`，将`n_components`设置为`None`，返回每个组件特征解释的方差比率，然后计算需要多少个组件才能超过一定阈值的方差解释（通常是0.95或0.99）：
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: See Also
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Comparison of LDA and PCA 2D Projection of Iris Dataset](https://oreil.ly/evGsx)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LDA和PCA在鸢尾花数据集的2D投影比较](https://oreil.ly/evGsx)'
- en: '[Linear Discriminant Analysis](https://oreil.ly/uOB81)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[线性判别分析](https://oreil.ly/uOB81)'
- en: 9.4 Reducing Features Using Matrix Factorization
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9.4 使用矩阵分解减少特征
- en: Problem
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You have a feature matrix of nonnegative values and want to reduce the dimensionality.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 您有一个非负值的特征矩阵，并希望降低其维度。
- en: Solution
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use *nonnegative matrix factorization* (NMF) to reduce the dimensionality of
    the feature matrix:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*非负矩阵分解*（NMF）来降低特征矩阵的维度：
- en: '[PRE12]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Discussion
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'NMF is an unsupervised technique for linear dimensionality reduction that *factorizes*
    (i.e., breaks up into multiple matrices whose product approximates the original
    matrix) the feature matrix into matrices representing the latent relationship
    between observations and their features. Intuitively, NMF can reduce dimensionality
    because in matrix multiplication, the two factors (matrices being multiplied)
    can have significantly fewer dimensions than the product matrix. Formally, given
    a desired number of returned features, *r*, NMF factorizes our feature matrix
    such that:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: NMF是一种用于线性降维的无监督技术，*分解*（即将原始矩阵分解为多个矩阵，其乘积近似原始矩阵）特征矩阵为表示观察和其特征之间潜在关系的矩阵。直观地说，NMF可以降低维度，因为在矩阵乘法中，两个因子（相乘的矩阵）的维度可以显著少于乘积矩阵。正式地说，给定所需的返回特征数*r*，NMF将分解我们的特征矩阵，使之：
- en: <math display="block"><mrow><mi mathvariant="bold">V</mi> <mo>≈</mo> <mi mathvariant="bold">W</mi>
    <mi mathvariant="bold">H</mi></mrow></math>
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi mathvariant="bold">V</mi> <mo>≈</mo> <mi mathvariant="bold">W</mi>
    <mi mathvariant="bold">H</mi></mrow></math>
- en: where **V** is our *n × d* feature matrix (i.e., *d* features, *n* observations),
    **W** is an *n* × *r* matrix, and **H** is an *r* × *d* matrix. By adjusting the
    value of *r* we can set the amount of dimensionality reduction desired.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，**V**是我们的*n × d*特征矩阵（即*d*个特征，*n*个观察值），**W**是一个*n* × *r*的矩阵，**H**是一个*r* ×
    *d*的矩阵。通过调整*r*的值，我们可以设置所需的降维量。
- en: One major requirement of NMF is that, as the name implies, the feature matrix
    cannot contain negative values. Additionally, unlike PCA and other techniques
    we have examined, NMF does not provide the explained variance of the outputted
    features. Thus, the best way for us to find the optimum value of `n_components`
    is by trying a range of values to find the one that produces the best result in
    our end model (see [Chapter 12](ch12.xhtml#model-selection)).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: NMF的一个主要要求是，正如其名称所示，特征矩阵不能包含负值。此外，与我们已经研究过的PCA和其他技术不同，NMF不提供输出特征的解释方差。因此，我们找到`n_components`的最佳值的最佳方式是尝试一系列值，找到在我们最终模型中产生最佳结果的值（见[第12章](ch12.xhtml#model-selection)）。
- en: See Also
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Non-negative matrix factorization, Wikipedia](https://oreil.ly/HJ_Qe)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[非负矩阵分解，维基百科](https://oreil.ly/HJ_Qe)'
- en: 9.5 Reducing Features on Sparse Data
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9.5 在稀疏数据上减少特征
- en: Problem
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You have a sparse feature matrix and want to reduce the dimensionality.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 您有一个稀疏的特征矩阵，并希望降低其维度。
- en: Solution
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use *Truncated Singular Value Decomposition* (TSVD):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*截断奇异值分解*（TSVD）：
- en: '[PRE14]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Discussion
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: TSVD is similar to PCA and, in fact, PCA often uses nontruncated *Singular Value
    Decomposition* (SVD) in one of its steps. Given *d* features, SVD will create
    factor matrices that are *d* × *d*, whereas TSVD will return factors that are
    *n* × *n*, where *n* is previously specified by a parameter. The practical advantage
    of TSVD is that, unlike PCA, it works on sparse feature matrices.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: TSVD与PCA类似，并且事实上，PCA在其步骤中经常使用非截断的*奇异值分解*（SVD）。给定*d*个特征，SVD将创建*d* × *d*的因子矩阵，而TSVD将返回*n*
    × *n*的因子，其中*n*是由参数预先指定的。TSVD的实际优势在于，与PCA不同，它适用于稀疏特征矩阵。
- en: 'One issue with TSVD: because of how it uses a random number generator, the
    signs of the output can flip between fittings. An easy workaround is to use `fit`
    only once per preprocessing pipeline, then use `transform` multiple times.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: TSVD的一个问题是：由于它如何使用随机数生成器，输出的符号在拟合过程中可能会翻转。一个简单的解决方法是仅在预处理流水线中使用`fit`一次，然后多次使用`transform`。
- en: 'As with linear discriminant analysis, we have to specify the number of features
    (components) we want to output. This is done with the `n_components` parameter.
    A natural question is: what is the optimum number of components? One strategy
    is to include `n_components` as a hyperparameter to optimize during model selection
    (i.e., choose the value for `n_components` that produces the best-trained model).
    Alternatively, because TSVD provides the ratio of the original feature matrix’s
    variance explained by each component, we can select the number of components that
    explain a desired amount of variance (95% and 99% are common values). For example,
    in our solution, the first three outputted components explain approximately 30%
    of the original data’s variance:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 与线性判别分析类似，我们必须指定要输出的特征（组件）的数量。这通过参数`n_components`来完成。一个自然的问题是：什么是最佳的组件数量？一种策略是将`n_components`作为超参数包含在模型选择中进行优化（即选择使模型训练效果最佳的`n_components`值）。另一种方法是因为TSVD提供了原始特征矩阵每个组件解释的方差比例，我们可以选择解释所需方差的组件数量（常见值为95%和99%）。例如，在我们的解决方案中，前三个输出的组件大约解释了原始数据约30%的方差：
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can automate the process by creating a function that runs TSVD with `n_components`
    set to one less than the number of original features and then calculate the number
    of components that explain a desired amount of the original data’s variance:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过创建一个函数来自动化这个过程，该函数将`n_components`设置为原始特征数减一，然后计算解释原始数据所需方差量的组件数量：
- en: '[PRE18]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: See Also
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[scikit-learn documentation: TruncatedSVD](https://oreil.ly/nD1pF)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[scikit-learn文档：截断SVD](https://oreil.ly/nD1pF)'
