<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 5. Handling Categorical Data" data-type="chapter" epub:type="chapter"><div class="chapter" id="handling-categorical-data">
<h1><span class="label">Chapter 5. </span>Handling Categorical Data</h1>
<section data-pdf-bookmark="5.0 Introduction" data-type="sect1"><div class="sect1" id="id129">
<h1>5.0 Introduction</h1>
<p>It is often useful to <a data-primary="categorical data" data-type="indexterm" id="ix_categ_data_ch5"/>measure objects not in terms of their quantity but in terms of some quality. We frequently represent qualitative information in categories such as gender, colors, or brand of car. However, not all categorical data is the same. Sets of categories with no intrinsic ordering are <a data-primary="nominal categories" data-type="indexterm" id="id1226"/>called <em>nominal</em>. Examples of nominal categories include:</p>
<ul>
<li>
<p>Blue, Red, Green</p>
</li>
<li>
<p>Man, Woman</p>
</li>
<li>
<p>Banana, Strawberry, Apple</p>
</li>
</ul>
<p>In contrast, when a set of categories has some <a data-primary="ordinal features" data-type="indexterm" id="id1227"/>natural ordering we refer to it as <em>ordinal</em>. For example:</p>
<ul>
<li>
<p>Low, Medium, High</p>
</li>
<li>
<p>Young, Old</p>
</li>
<li>
<p>Agree, Neutral, Disagree</p>
</li>
</ul>
<p>Furthermore, categorical information is often represented in data as a
vector or column of strings (e.g., <code>"Maine"</code>, <code>"Texas"</code>, <code>"Delaware"</code>). The problem is that most machine learning algorithms require
inputs to be numerical values.</p>
<p>The <a data-primary="k-nearest neighbors (KNN)" data-secondary="numerical data requirement" data-type="indexterm" id="id1228"/>k-nearest neighbors algorithm is an example of an algorithm that requires numerical data. One step in
the algorithm is calculating the distances between observations—​often
using Euclidean <span class="keep-together">distance:</span></p>
<div data-type="equation">
<math display="block">
<msqrt>
<mrow>
<msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </msubsup>
<msup><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi> </msub><mo>-</mo><msub><mi>y</mi> <mi>i</mi> </msub><mo>)</mo></mrow> <mn>2</mn> </msup>
</mrow>
</msqrt>
</math>
</div>
<p>where <math display="inline"><mi>x</mi></math> and <math display="inline"><mi>y</mi></math> are two observations and
subscript <math display="inline"><mi>i</mi></math> denotes the value for the observations’
<math display="inline"><mi>i</mi></math>th feature. However, the distance calculation obviously
is impossible if the value of <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math> is a string (e.g., <code>"Texas"</code>). Instead, we need to convert the string into some numerical format so that it can be input into the Euclidean distance equation. Our goal is to transform the data in a way that properly captures the
information in the categories (ordinality, relative intervals between categories, etc.). In this chapter we will cover techniques for
making this transformation as well as overcoming other challenges often
encountered when handling categorical data.</p>
</div></section>
<section data-pdf-bookmark="5.1 Encoding Nominal Categorical Features" data-type="sect1"><div class="sect1" id="encoding-nominal-categorical-features">
<h1>5.1 Encoding Nominal Categorical Features</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id130">
<h2>Problem</h2>
<p>You have a feature with <a data-primary="nominal features, encoding" data-type="indexterm" id="ix_nom_categ_data"/><a data-primary="categorical data" data-secondary="nominal features" data-type="indexterm" id="ix_categ_data_nom"/>nominal classes that has no intrinsic ordering (e.g., apple, pear, banana), and you want to encode the feature into numerical values.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id131">
<h2>Solution</h2>
<p>One-hot <a data-primary="one-hot encoding" data-type="indexterm" id="ix_onehot_encode"/><a data-primary="LabelBinarizer" data-type="indexterm" id="id1229"/>encode the feature using scikit-learn’s <code>LabelBinarizer</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import libraries</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">LabelBinarizer</code><code class="p">,</code> <code class="n">MultiLabelBinarizer</code>

<code class="c1"># Create feature</code>
<code class="n">feature</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code><code class="s2">"Texas"</code><code class="p">],</code>
                    <code class="p">[</code><code class="s2">"California"</code><code class="p">],</code>
                    <code class="p">[</code><code class="s2">"Texas"</code><code class="p">],</code>
                    <code class="p">[</code><code class="s2">"Delaware"</code><code class="p">],</code>
                    <code class="p">[</code><code class="s2">"Texas"</code><code class="p">]])</code>

<code class="c1"># Create one-hot encoder</code>
<code class="n">one_hot</code> <code class="o">=</code> <code class="n">LabelBinarizer</code><code class="p">()</code>

<code class="c1"># One-hot encode feature</code>
<code class="n">one_hot</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">feature</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([[0, 0, 1],
       [1, 0, 0],
       [0, 0, 1],
       [0, 1, 0],
       [0, 0, 1]])</pre>
<p>We can use the <code>classes_</code> attribute to output the classes:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View feature classes</code>
<code class="n">one_hot</code><code class="o">.</code><code class="n">classes_</code></pre>
<pre data-type="programlisting">array(['California', 'Delaware', 'Texas'],
      dtype='&lt;U10')</pre>
<p>If we want to <a data-primary="inverse_transform method" data-type="indexterm" id="id1230"/>reverse the one-hot encoding, we can use
<code>inverse_transform</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Reverse one-hot encoding</code>
<code class="n">one_hot</code><code class="o">.</code><code class="n">inverse_transform</code><code class="p">(</code><code class="n">one_hot</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">feature</code><code class="p">))</code></pre>
<pre data-type="programlisting">array(['Texas', 'California', 'Texas', 'Delaware', 'Texas'],
      dtype='&lt;U10')</pre>
<p>We can even use pandas to one-hot encode the feature:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import library</code>
<code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>

<code class="c1"># Create dummy variables from feature</code>
<code class="n">pd</code><code class="o">.</code><code class="n">get_dummies</code><code class="p">(</code><code class="n">feature</code><code class="p">[:,</code><code class="mi">0</code><code class="p">])</code></pre>
<table>
<thead>
<tr>
<th/>
<th>California</th>
<th>Delaware</th>
<th>Texas</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<th>1</th>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<th>2</th>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<th>3</th>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<th>4</th>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>One helpful feature of scikit-learn is the ability to handle a situation where each
observation lists multiple classes:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create multiclass feature</code>
<code class="n">multiclass_feature</code> <code class="o">=</code> <code class="p">[(</code><code class="s2">"Texas"</code><code class="p">,</code> <code class="s2">"Florida"</code><code class="p">),</code>
                      <code class="p">(</code><code class="s2">"California"</code><code class="p">,</code> <code class="s2">"Alabama"</code><code class="p">),</code>
                      <code class="p">(</code><code class="s2">"Texas"</code><code class="p">,</code> <code class="s2">"Florida"</code><code class="p">),</code>
                      <code class="p">(</code><code class="s2">"Delaware"</code><code class="p">,</code> <code class="s2">"Florida"</code><code class="p">),</code>
                      <code class="p">(</code><code class="s2">"Texas"</code><code class="p">,</code> <code class="s2">"Alabama"</code><code class="p">)]</code>

<code class="c1"># Create multiclass one-hot encoder</code>
<code class="n">one_hot_multiclass</code> <code class="o">=</code> <code class="n">MultiLabelBinarizer</code><code class="p">()</code>

<code class="c1"># One-hot encode multiclass feature</code>
<code class="n">one_hot_multiclass</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">multiclass_feature</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([[0, 0, 0, 1, 1],
       [1, 1, 0, 0, 0],
       [0, 0, 0, 1, 1],
       [0, 0, 1, 1, 0],
       [1, 0, 0, 0, 1]])</pre>
<p>Once again, we can see the classes with the <code>classes_</code> method:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View classes</code>
<code class="n">one_hot_multiclass</code><code class="o">.</code><code class="n">classes_</code></pre>
<pre data-type="programlisting">array(['Alabama', 'California', 'Delaware', 'Florida', 'Texas'], dtype=object)</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id132">
<h2>Discussion</h2>
<p>We might think the proper strategy is to assign each class a numerical
value (e.g., Texas = 1, California = 2). However, when our classes
have no intrinsic ordering (e.g., Texas isn’t “less” than California), our numerical values erroneously create an ordering that is not present.</p>
<p>The proper strategy is to create a binary feature for each class in the
original feature. This is often <a data-primary="dummying (one-hot encoding)" data-type="indexterm" id="id1231"/>called <em>one-hot encoding</em> (in machine
learning literature) or <em>dummying</em> (in statistical and research literature). Our solution’s feature was a vector containing three classes (i.e., Texas, California, and Delaware). In one-hot encoding, each class becomes its own feature with 1s when the class appears and 0s otherwise. Because our feature had three classes, one-hot encoding returned three binary features (one for each class). By using one-hot encoding we can capture the membership of an observation in a class while preserving the notion that the class lacks any sort of hierarchy.</p>
<p>Finally, it is often recommended that
after one-hot encoding a feature, we drop one of the one-hot encoded
features in the resulting matrix to avoid linear dependence.<a data-primary="" data-startref="ix_categ_data_nom" data-type="indexterm" id="id1232"/><a data-primary="" data-startref="ix_nom_categ_data" data-type="indexterm" id="id1233"/><a data-primary="" data-startref="ix_onehot_encode" data-type="indexterm" id="id1234"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1235">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/xjBhG">Dummy Variable Trap in Regression Models, Algosome</a></p>
</li>
<li>
<p><a href="https://oreil.ly/CTdpG">Dropping one of the columns when using one-hot encoding, Cross Validated</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="5.2 Encoding Ordinal Categorical Features" data-type="sect1"><div class="sect1" id="encoding-ordinal-categorical-features">
<h1>5.2 Encoding Ordinal Categorical Features</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id133">
<h2>Problem</h2>
<p>You have an <a data-primary="ordinal features" data-type="indexterm" id="ix_ord_categ_data"/><a data-primary="categorical data" data-secondary="ordinal features" data-type="indexterm" id="ix_categ_data_ord"/>ordinal categorical feature (e.g., high, medium, low), and you want to transform it into numerical values.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id134">
<h2>Solution</h2>
<p>Use the pandas <a data-primary="replace method" data-type="indexterm" id="id1236"/>DataFrame <code>replace</code> method to transform string labels to
numerical equivalents:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>

<code class="c1"># Create features</code>
<code class="n">dataframe</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">({</code><code class="s2">"Score"</code><code class="p">:</code> <code class="p">[</code><code class="s2">"Low"</code><code class="p">,</code> <code class="s2">"Low"</code><code class="p">,</code> <code class="s2">"Medium"</code><code class="p">,</code> <code class="s2">"Medium"</code><code class="p">,</code> <code class="s2">"High"</code><code class="p">]})</code>

<code class="c1"># Create mapper</code>
<code class="n">scale_mapper</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"Low"</code><code class="p">:</code><code class="mi">1</code><code class="p">,</code>
                <code class="s2">"Medium"</code><code class="p">:</code><code class="mi">2</code><code class="p">,</code>
                <code class="s2">"High"</code><code class="p">:</code><code class="mi">3</code><code class="p">}</code>

<code class="c1"># Replace feature values with scale</code>
<code class="n">dataframe</code><code class="p">[</code><code class="s2">"Score"</code><code class="p">]</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="n">scale_mapper</code><code class="p">)</code></pre>
<pre data-type="programlisting">0    1
1    1
2    2
3    2
4    3
Name: Score, dtype: int64</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id135">
<h2>Discussion</h2>
<p>Often we have a feature with classes that have some kind of natural
ordering. A famous example is the Likert scale:</p>
<ul>
<li>
<p>Strongly Agree</p>
</li>
<li>
<p>Agree</p>
</li>
<li>
<p>Neutral</p>
</li>
<li>
<p>Disagree</p>
</li>
<li>
<p>Strongly Disagree</p>
</li>
</ul>
<p>When encoding the feature for use in machine learning, we need to
transform the ordinal classes into numerical values that maintain the
notion of ordering. The most common approach is to create a dictionary that
maps the string label of the class to a number and then apply that map
to the feature.</p>
<p>It is important that our choice of numeric values is based on our prior
information on the ordinal classes. In our solution, <code>high</code> is literally
three times larger than <code>low</code>. This is fine in many instances but can
break down if the assumed intervals between the classes are not equal:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">dataframe</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">({</code><code class="s2">"Score"</code><code class="p">:</code> <code class="p">[</code><code class="s2">"Low"</code><code class="p">,</code>
                                    <code class="s2">"Low"</code><code class="p">,</code>
                                    <code class="s2">"Medium"</code><code class="p">,</code>
                                    <code class="s2">"Medium"</code><code class="p">,</code>
                                    <code class="s2">"High"</code><code class="p">,</code>
                                    <code class="s2">"Barely More Than Medium"</code><code class="p">]})</code>

<code class="n">scale_mapper</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"Low"</code><code class="p">:</code><code class="mi">1</code><code class="p">,</code>
                <code class="s2">"Medium"</code><code class="p">:</code><code class="mi">2</code><code class="p">,</code>
                <code class="s2">"Barely More Than Medium"</code><code class="p">:</code><code class="mi">3</code><code class="p">,</code>
                <code class="s2">"High"</code><code class="p">:</code><code class="mi">4</code><code class="p">}</code>

<code class="n">dataframe</code><code class="p">[</code><code class="s2">"Score"</code><code class="p">]</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="n">scale_mapper</code><code class="p">)</code></pre>
<pre data-type="programlisting">0    1
1    1
2    2
3    2
4    4
5    3
Name: Score, dtype: int64</pre>
<p>In this example, the distance between <code>Low</code> and <code>Medium</code> is the same as
the distance between <code>Medium</code> and <code>Barely More Than Medium</code>, which is
almost certainly not accurate. The best approach is to be conscious
about the numerical values mapped to classes:<a data-primary="" data-startref="ix_categ_data_ord" data-type="indexterm" id="id1237"/><a data-primary="" data-startref="ix_ord_categ_data" data-type="indexterm" id="id1238"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="n">scale_mapper</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"Low"</code><code class="p">:</code><code class="mi">1</code><code class="p">,</code>
                <code class="s2">"Medium"</code><code class="p">:</code><code class="mi">2</code><code class="p">,</code>
                <code class="s2">"Barely More Than Medium"</code><code class="p">:</code><code class="mf">2.1</code><code class="p">,</code>
                <code class="s2">"High"</code><code class="p">:</code><code class="mi">3</code><code class="p">}</code>

<code class="n">dataframe</code><code class="p">[</code><code class="s2">"Score"</code><code class="p">]</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="n">scale_mapper</code><code class="p">)</code></pre>
<pre data-type="programlisting">0    1.0
1    1.0
2    2.0
3    2.0
4    3.0
5    2.1
Name: Score, dtype: float64</pre>
</div></section>
</div></section>
<section data-pdf-bookmark="5.3 Encoding Dictionaries of Features" data-type="sect1"><div class="sect1" id="encoding-dictionaries-of-features">
<h1>5.3 Encoding Dictionaries of Features</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id136">
<h2>Problem</h2>
<p>You have a <a data-primary="dictionaries of features, encoding" data-type="indexterm" id="ix_dict_feat_encode"/><a data-primary="categorical data" data-secondary="dictionaries of features" data-type="indexterm" id="ix_categ_data_dict_feat"/>dictionary and want to convert it into a feature <a data-primary="matrices" data-secondary="converting data from dictionary to feature matrix" data-type="indexterm" id="ix_matrix_conv_data_dict"/><a data-primary="DictVectorizer" data-type="indexterm" id="ix_dict_vector"/>matrix.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id137">
<h2>Solution</h2>
<p>Use <code>DictVectorizer</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import library</code>
<code class="kn">from</code> <code class="nn">sklearn.feature_extraction</code> <code class="kn">import</code> <code class="n">DictVectorizer</code>

<code class="c1"># Create dictionary</code>
<code class="n">data_dict</code> <code class="o">=</code> <code class="p">[{</code><code class="s2">"Red"</code><code class="p">:</code> <code class="mi">2</code><code class="p">,</code> <code class="s2">"Blue"</code><code class="p">:</code> <code class="mi">4</code><code class="p">},</code>
             <code class="p">{</code><code class="s2">"Red"</code><code class="p">:</code> <code class="mi">4</code><code class="p">,</code> <code class="s2">"Blue"</code><code class="p">:</code> <code class="mi">3</code><code class="p">},</code>
             <code class="p">{</code><code class="s2">"Red"</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s2">"Yellow"</code><code class="p">:</code> <code class="mi">2</code><code class="p">},</code>
             <code class="p">{</code><code class="s2">"Red"</code><code class="p">:</code> <code class="mi">2</code><code class="p">,</code> <code class="s2">"Yellow"</code><code class="p">:</code> <code class="mi">2</code><code class="p">}]</code>

<code class="c1"># Create dictionary vectorizer</code>
<code class="n">dictvectorizer</code> <code class="o">=</code> <code class="n">DictVectorizer</code><code class="p">(</code><code class="n">sparse</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>

<code class="c1"># Convert dictionary to feature matrix</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">dictvectorizer</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">data_dict</code><code class="p">)</code>

<code class="c1"># View feature matrix</code>
<code class="n">features</code></pre>
<pre data-type="programlisting">array([[ 4.,  2.,  0.],
       [ 3.,  4.,  0.],
       [ 0.,  1.,  2.],
       [ 0.,  2.,  2.]])</pre>
<p>By default <code>DictVectorizer</code> outputs a <a data-primary="sparse matrix" data-secondary="encoding dictionaries of features" data-type="indexterm" id="ix_sparse_matrix_dict"/>sparse matrix that only stores
elements with a value other than 0. This can be very helpful when we have massive matrices (often encountered in <a data-primary="natural language processing (NLP)" data-type="indexterm" id="id1239"/><a data-primary="NLP (natural language processing)" data-type="indexterm" id="id1240"/>natural language processing) and want to minimize the memory requirements. We can force <code>DictVectorizer</code> to output a dense matrix using <code>sparse=False</code>.</p>
<p>We can get the names of each generated feature using the
<code>get_feature_names</code> method:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Get feature names</code>
<code class="n">feature_names</code> <code class="o">=</code> <code class="n">dictvectorizer</code><code class="o">.</code><code class="n">get_feature_names</code><code class="p">()</code>

<code class="c1"># View feature names</code>
<code class="n">feature_names</code></pre>
<pre data-type="programlisting">['Blue', 'Red', 'Yellow']</pre>
<p>While not necessary, for the sake of illustration we can create a pandas
DataFrame to view the output better:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import library</code>
<code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>

<code class="c1"># Create dataframe from features</code>
<code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="n">feature_names</code><code class="p">)</code></pre>
<table>
<thead>
<tr>
<th/>
<th>Blue</th>
<th>Red</th>
<th>Yellow</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>4.0</td>
<td>2.0</td>
<td>0.0</td>
</tr>
<tr>
<th>1</th>
<td>3.0</td>
<td>4.0</td>
<td>0.0</td>
</tr>
<tr>
<th>2</th>
<td>0.0</td>
<td>1.0</td>
<td>2.0</td>
</tr>
<tr>
<th>3</th>
<td>0.0</td>
<td>2.0</td>
<td>2.0</td>
</tr>
</tbody>
</table>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id138">
<h2>Discussion</h2>
<p>A dictionary is a popular data structure used by many programming
languages; however, machine learning algorithms expect the data to be in
the form of a matrix. We can accomplish this using scikit-learn’s
<code>DictVectorizer</code>.</p>
<p>This is a common situation when working with natural language
processing. For example, we might have a collection of documents and for
each document we have a dictionary containing the number of times every
word appears in the document. Using <code>DictVectorizer</code>, we can easily
create a feature matrix where every feature is the number of times a
word appears in each document:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create word count dictionaries for four documents</code>
<code class="n">doc_1_word_count</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"Red"</code><code class="p">:</code> <code class="mi">2</code><code class="p">,</code> <code class="s2">"Blue"</code><code class="p">:</code> <code class="mi">4</code><code class="p">}</code>
<code class="n">doc_2_word_count</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"Red"</code><code class="p">:</code> <code class="mi">4</code><code class="p">,</code> <code class="s2">"Blue"</code><code class="p">:</code> <code class="mi">3</code><code class="p">}</code>
<code class="n">doc_3_word_count</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"Red"</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s2">"Yellow"</code><code class="p">:</code> <code class="mi">2</code><code class="p">}</code>
<code class="n">doc_4_word_count</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"Red"</code><code class="p">:</code> <code class="mi">2</code><code class="p">,</code> <code class="s2">"Yellow"</code><code class="p">:</code> <code class="mi">2</code><code class="p">}</code>

<code class="c1"># Create list</code>
<code class="n">doc_word_counts</code> <code class="o">=</code> <code class="p">[</code><code class="n">doc_1_word_count</code><code class="p">,</code>
                   <code class="n">doc_2_word_count</code><code class="p">,</code>
                   <code class="n">doc_3_word_count</code><code class="p">,</code>
                   <code class="n">doc_4_word_count</code><code class="p">]</code>

<code class="c1"># Convert list of word count dictionaries into feature matrix</code>
<code class="n">dictvectorizer</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">doc_word_counts</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([[ 4.,  2.,  0.],
       [ 3.,  4.,  0.],
       [ 0.,  1.,  2.],
       [ 0.,  2.,  2.]])</pre>
<p>In our toy example there are only three unique words (<code>Red</code>, <code>Yellow</code>,
<code>Blue</code>) so there are only three features in our matrix; however, you can
imagine that if each document was actually a book in a university
library our feature matrix would be very large (and then we would want
to set <code>sparse</code> to <code>True</code>).<a data-primary="" data-startref="ix_categ_data_dict_feat" data-type="indexterm" id="id1241"/><a data-primary="" data-startref="ix_dict_feat_encode" data-type="indexterm" id="id1242"/><a data-primary="" data-startref="ix_dict_vector" data-type="indexterm" id="id1243"/><a data-primary="" data-startref="ix_matrix_conv_data_dict" data-type="indexterm" id="id1244"/><a data-primary="" data-startref="ix_sparse_matrix_dict" data-type="indexterm" id="id1245"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1246">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/zu5hU">How to Create Dictionaries in Python</a></p>
</li>
<li>
<p><a href="https://oreil.ly/5nAsU">SciPy Sparse Matrices</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="5.4 Imputing Missing Class Values" data-type="sect1"><div class="sect1" id="imputing-missing-class-values">
<h1>5.4 Imputing Missing Class Values</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id139">
<h2>Problem</h2>
<p>You have a categorical feature containing <a data-primary="missing data" data-secondary="imputing missing values" data-type="indexterm" id="ix_miss_data_imput_miss_val2"/><a data-primary="classes" data-secondary="imputing missing values" data-type="indexterm" id="ix_class_imput_val"/><a data-primary="imputing missing values" data-type="indexterm" id="ix_imput_miss_val2"/><a data-primary="categorical data" data-secondary="missing class values" data-type="indexterm" id="ix_categ_data_miss_val"/>missing values that you want
to replace with <a data-primary="predictions and predicting" data-secondary="imputing missing class values" data-type="indexterm" id="ix_predict_miss_val"/>predicted values.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id140">
<h2>Solution</h2>
<p>The ideal solution is to train a machine learning classifier algorithm
to predict the missing values, commonly a <a data-primary="KNeighborsClassifier" data-type="indexterm" id="id1247"/>k-nearest neighbors (KNN)
classifier:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn.neighbors</code> <code class="kn">import</code> <code class="n">KNeighborsClassifier</code>

<code class="c1"># Create feature matrix with categorical feature</code>
<code class="n">X</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code><code class="mi">0</code><code class="p">,</code> <code class="mf">2.10</code><code class="p">,</code> <code class="mf">1.45</code><code class="p">],</code>
              <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mf">1.18</code><code class="p">,</code> <code class="mf">1.33</code><code class="p">],</code>
              <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mf">1.22</code><code class="p">,</code> <code class="mf">1.27</code><code class="p">],</code>
              <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="o">-</code><code class="mf">0.21</code><code class="p">,</code> <code class="o">-</code><code class="mf">1.19</code><code class="p">]])</code>

<code class="c1"># Create feature matrix with missing values in the categorical feature</code>
<code class="n">X_with_nan</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code><code class="n">np</code><code class="o">.</code><code class="n">nan</code><code class="p">,</code> <code class="mf">0.87</code><code class="p">,</code> <code class="mf">1.31</code><code class="p">],</code>
                       <code class="p">[</code><code class="n">np</code><code class="o">.</code><code class="n">nan</code><code class="p">,</code> <code class="o">-</code><code class="mf">0.67</code><code class="p">,</code> <code class="o">-</code><code class="mf">0.22</code><code class="p">]])</code>

<code class="c1"># Train KNN learner</code>
<code class="n">clf</code> <code class="o">=</code> <code class="n">KNeighborsClassifier</code><code class="p">(</code><code class="mi">3</code><code class="p">,</code> <code class="n">weights</code><code class="o">=</code><code class="s1">'distance'</code><code class="p">)</code>
<code class="n">trained_model</code> <code class="o">=</code> <code class="n">clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code><code class="mi">1</code><code class="p">:],</code> <code class="n">X</code><code class="p">[:,</code><code class="mi">0</code><code class="p">])</code>

<code class="c1"># Predict class of missing values</code>
<code class="n">imputed_values</code> <code class="o">=</code> <code class="n">trained_model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_with_nan</code><code class="p">[:,</code><code class="mi">1</code><code class="p">:])</code>

<code class="c1"># Join column of predicted class with their other features</code>
<code class="n">X_with_imputed</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">hstack</code><code class="p">((</code><code class="n">imputed_values</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code><code class="mi">1</code><code class="p">),</code> <code class="n">X_with_nan</code><code class="p">[:,</code><code class="mi">1</code><code class="p">:]))</code>

<code class="c1"># Join two feature matrices</code>
<code class="n">np</code><code class="o">.</code><code class="n">vstack</code><code class="p">((</code><code class="n">X_with_imputed</code><code class="p">,</code> <code class="n">X</code><code class="p">))</code></pre>
<pre data-type="programlisting">array([[ 0.  ,  0.87,  1.31],
       [ 1.  , -0.67, -0.22],
       [ 0.  ,  2.1 ,  1.45],
       [ 1.  ,  1.18,  1.33],
       [ 0.  ,  1.22,  1.27],
       [ 1.  , -0.21, -1.19]])</pre>
<p>An alternative solution is to fill in missing values with the feature’s
most frequent value:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">sklearn.impute</code> <code class="kn">import</code> <code class="n">SimpleImputer</code>

<code class="c1"># Join the two feature matrices</code>
<code class="n">X_complete</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">vstack</code><code class="p">((</code><code class="n">X_with_nan</code><code class="p">,</code> <code class="n">X</code><code class="p">))</code>

<code class="n">imputer</code> <code class="o">=</code> <code class="n">SimpleImputer</code><code class="p">(</code><code class="n">strategy</code><code class="o">=</code><code class="s1">'most_frequent'</code><code class="p">)</code>

<code class="n">imputer</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_complete</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([[ 0.  ,  0.87,  1.31],
       [ 0.  , -0.67, -0.22],
       [ 0.  ,  2.1 ,  1.45],
       [ 1.  ,  1.18,  1.33],
       [ 0.  ,  1.22,  1.27],
       [ 1.  , -0.21, -1.19]])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id141">
<h2>Discussion</h2>
<p>When we have missing values in a categorical feature, our best solution
is to open our toolbox of machine learning algorithms to predict the
values of the missing observations. We can accomplish this by treating
the feature with the missing values as the target vector and the other
features as the feature matrix. A commonly used algorithm is KNN
(discussed in depth in <a data-type="xref" href="ch15.xhtml#k-nearest-neighbors">Chapter 15</a>), which assigns to the missing
value the most frequent class of the <em>k</em> nearest observations.</p>
<p>Alternatively, we can fill in missing values with the most frequent
class of the feature or even discard the observations with missing values. While less sophisticated than KNN, these options are much more
scalable to larger data. In any case, it is advisable to include a
binary feature indicating which observations contain imputed values.<a data-primary="" data-startref="ix_categ_data_miss_val" data-type="indexterm" id="id1248"/><a data-primary="" data-startref="ix_class_imput_val" data-type="indexterm" id="id1249"/><a data-primary="" data-startref="ix_imput_miss_val2" data-type="indexterm" id="id1250"/><a data-primary="" data-startref="ix_miss_data_imput_miss_val2" data-type="indexterm" id="id1251"/><a data-primary="" data-startref="ix_predict_miss_val" data-type="indexterm" id="id1252"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1253">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/joZ6J">scikit-learn documentation: Imputation of Missing Values</a></p>
</li>
<li>
<p><a href="https://oreil.ly/TcvOf">Overcoming Missing Values in a Random Forest Classifier</a></p>
</li>
<li>
<p><a href="https://oreil.ly/kDFEC">A Study of K-Nearest Neighbour as an Imputation Method</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="5.5 Handling Imbalanced Classes" data-type="sect1"><div class="sect1" id="handling-imbalanced-classes-ch05">
<h1>5.5 Handling Imbalanced Classes</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id486">
<h2>Problem</h2>
<p>You have a target vector with highly <a data-primary="imbalanced classes, handling" data-secondary="categorical data" data-type="indexterm" id="ix_imb_class_categ_data"/><a data-primary="categorical data" data-secondary="imbalanced classes" data-type="indexterm" id="ix_categ_data_imb_class"/>imbalanced classes, and you want to make adjustments so that you can handle the class imbalance.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id142">
<h2>Solution</h2>
<p>Collect more data. If that isn’t possible, change the metrics used to
evaluate your model. If that doesn’t work, consider using a model’s
built-in class weight parameters (if available), downsampling, or
upsampling. We cover evaluation metrics in a later chapter, so for now
let’s focus on class weight parameters, downsampling, and 
<span class="keep-together">upsampling.</span></p>
<p>To demonstrate our solutions, we need to create some data with imbalanced classes. Fisher’s Iris dataset contains three balanced classes of 50 observations, each indicating the species of flower (<em>Iris setosa</em>, <em>Iris virginica</em>, and <em>Iris versicolor</em>). To unbalance the dataset, we remove 40 of the 50 <em>Iris setosa</em> observations and then merge the <em>Iris virginica</em> and <em>Iris versicolor</em> classes. The end result is a binary target vector indicating if an observation is an <em>Iris setosa</em> flower or not. The result is 10 observations of <em>Iris setosa</em> (class 0) and 100 observations of not <em>Iris setosa</em> (class 1):</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">RandomForestClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_iris</code>

<code class="c1"># Load iris data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">load_iris</code><code class="p">()</code>

<code class="c1"># Create feature matrix</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>

<code class="c1"># Create target vector</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Remove first 40 observations</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">features</code><code class="p">[</code><code class="mi">40</code><code class="p">:,:]</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">target</code><code class="p">[</code><code class="mi">40</code><code class="p">:]</code>

<code class="c1"># Create binary target vector indicating if class 0</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">where</code><code class="p">((</code><code class="n">target</code> <code class="o">==</code> <code class="mi">0</code><code class="p">),</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Look at the imbalanced target vector</code>
<code class="n">target</code></pre>
<pre data-type="programlisting">array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])</pre>
<p>Many algorithms in scikit-learn offer a <a data-primary="parameters" data-secondary="imbalanced classes solution" data-type="indexterm" id="id1254"/><a data-primary="RandomForestClassifier" data-type="indexterm" id="id1255"/>parameter to weight classes
during training to counteract the effect of their imbalance. While we
have not covered it yet, <code>RandomForestClassifier</code> is a popular
classification algorithm and includes a <code>class_weight</code> parameter; learn more about the <code>RandomForestClassifier</code> in <a data-type="xref" href="ch14.xhtml#training-a-random-forest-classifier">Recipe 14.4</a>. You
can pass an argument explicitly specifying the desired class weights:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create weights</code>
<code class="n">weights</code> <code class="o">=</code> <code class="p">{</code><code class="mi">0</code><code class="p">:</code> <code class="mf">0.9</code><code class="p">,</code> <code class="mi">1</code><code class="p">:</code> <code class="mf">0.1</code><code class="p">}</code>

<code class="c1"># Create random forest classifier with weights</code>
<code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">class_weight</code><code class="o">=</code><code class="n">weights</code><code class="p">)</code></pre>
<pre data-type="programlisting">RandomForestClassifier(class_weight={0: 0.9, 1: 0.1})</pre>
<p>Or you can pass <code>balanced</code>, which automatically creates weights inversely
proportional to class frequencies:</p>
<pre class="less_space pagebreak-before" data-code-language="python" data-type="programlisting"><code class="c1"># Train a random forest with balanced class weights</code>
<code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">class_weight</code><code class="o">=</code><code class="s2">"balanced"</code><code class="p">)</code></pre>
<pre data-type="programlisting">RandomForestClassifier(class_weight='balanced')</pre>
<p>Alternatively, we can downsample the majority class or upsample the
minority class. In <em>downsampling</em>, we <a data-primary="downsampling, imbalanced class problem" data-type="indexterm" id="id1256"/>randomly sample without replacement from the majority class (i.e., the class with more observations) to create a new subset of observations equal in size to the minority class. For example, if the minority class has 10 observations, we will randomly select 10 observations from the majority class and use those 20 observations as our data. Here we do exactly that using our unbalanced iris data:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Indicies of each class's observations</code>
<code class="n">i_class0</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">where</code><code class="p">(</code><code class="n">target</code> <code class="o">==</code> <code class="mi">0</code><code class="p">)[</code><code class="mi">0</code><code class="p">]</code>
<code class="n">i_class1</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">where</code><code class="p">(</code><code class="n">target</code> <code class="o">==</code> <code class="mi">1</code><code class="p">)[</code><code class="mi">0</code><code class="p">]</code>

<code class="c1"># Number of observations in each class</code>
<code class="n">n_class0</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">i_class0</code><code class="p">)</code>
<code class="n">n_class1</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">i_class1</code><code class="p">)</code>

<code class="c1"># For every observation of class 0, randomly sample</code>
<code class="c1"># from class 1 without replacement</code>
<code class="n">i_class1_downsampled</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">(</code><code class="n">i_class1</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="n">n_class0</code><code class="p">,</code> <code class="n">replace</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>

<code class="c1"># Join together class 0's target vector with the</code>
<code class="c1"># downsampled class 1's target vector</code>
<code class="n">np</code><code class="o">.</code><code class="n">hstack</code><code class="p">((</code><code class="n">target</code><code class="p">[</code><code class="n">i_class0</code><code class="p">],</code> <code class="n">target</code><code class="p">[</code><code class="n">i_class1_downsampled</code><code class="p">]))</code></pre>
<pre data-type="programlisting">array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])</pre>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Join together class 0's feature matrix with the</code>
<code class="c1"># downsampled class 1's feature matrix</code>
<code class="n">np</code><code class="o">.</code><code class="n">vstack</code><code class="p">((</code><code class="n">features</code><code class="p">[</code><code class="n">i_class0</code><code class="p">,:],</code> <code class="n">features</code><code class="p">[</code><code class="n">i_class1_downsampled</code><code class="p">,:]))[</code><code class="mi">0</code><code class="p">:</code><code class="mi">5</code><code class="p">]</code></pre>
<pre data-type="programlisting">array([[ 5. ,  3.5,  1.3,  0.3],
       [ 4.5,  2.3,  1.3,  0.3],
       [ 4.4,  3.2,  1.3,  0.2],
       [ 5. ,  3.5,  1.6,  0.6],
       [ 5.1,  3.8,  1.9,  0.4]])</pre>
<p>Our other option is to upsample the minority class. In <em>upsampling</em>, for
every <a data-primary="upsampling, imbalanced class problem" data-type="indexterm" id="id1257"/>observation in the majority class, we randomly select an
observation from the minority class with replacement. The result is
the same number of observations from the minority and majority classes.
Upsampling is implemented very similarly to downsampling, just in reverse:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># For every observation in class 1, randomly sample from class 0 with</code>
<code class="c1"># replacement</code>
<code class="n">i_class0_upsampled</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">(</code><code class="n">i_class0</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="n">n_class1</code><code class="p">,</code> <code class="n">replace</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>

<code class="c1"># Join together class 0's upsampled target vector with class 1's target vector</code>
<code class="n">np</code><code class="o">.</code><code class="n">concatenate</code><code class="p">((</code><code class="n">target</code><code class="p">[</code><code class="n">i_class0_upsampled</code><code class="p">],</code> <code class="n">target</code><code class="p">[</code><code class="n">i_class1</code><code class="p">]))</code></pre>
<pre data-type="programlisting">array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])</pre>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Join together class 0's upsampled feature matrix with class 1's feature matrix</code>
<code class="n">np</code><code class="o">.</code><code class="n">vstack</code><code class="p">((</code><code class="n">features</code><code class="p">[</code><code class="n">i_class0_upsampled</code><code class="p">,:],</code> <code class="n">features</code><code class="p">[</code><code class="n">i_class1</code><code class="p">,:]))[</code><code class="mi">0</code><code class="p">:</code><code class="mi">5</code><code class="p">]</code></pre>
<pre data-type="programlisting">array([[ 5. ,  3.5,  1.6,  0.6],
       [ 5. ,  3.5,  1.6,  0.6],
       [ 5. ,  3.3,  1.4,  0.2],
       [ 4.5,  2.3,  1.3,  0.3],
       [ 4.8,  3. ,  1.4,  0.3]])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id143">
<h2>Discussion</h2>
<p>In the real world, imbalanced classes are everywhere—​most visitors
don’t click the buy button, and many types of cancer are thankfully rare.
For this reason, handling imbalanced classes is a common activity in
machine learning.</p>
<p>Our best strategy is simply to collect more observations—​especially
observations from the minority class. However, often this is just not
possible, so we have to resort to other options.</p>
<p>A second strategy is to use a model evaluation metric better
suited to imbalanced classes. Accuracy is often used as a metric for
evaluating the performance of a model, but when imbalanced classes
are present, accuracy can be ill suited. For example, if only 0.5% of
observations have some rare cancer, then even a naive model that
<span class="keep-together">predicts</span> nobody has cancer will be 99.5% accurate. Clearly this is not ideal. Some better metrics we discuss in later chapters are confusion matrices, precision, recall, <em>F<sub>1</sub></em> scores, and ROC curves.</p>
<p>A third strategy is to use the class weighing parameters included in
implementations of some models. This allows the algorithm to
adjust for imbalanced classes. Fortunately, many scikit-learn classifiers
have a <code>class_weight</code> parameter, making it a good option.<a data-primary="" data-startref="ix_categ_data_ch5" data-type="indexterm" id="id1258"/><a data-primary="" data-startref="ix_categ_data_imb_class" data-type="indexterm" id="id1259"/><a data-primary="" data-startref="ix_imb_class_categ_data" data-type="indexterm" id="id1260"/></p>
<p>The fourth and fifth strategies are related: downsampling and
upsampling. In downsampling we create a random subset of the majority
class of equal size to the minority class. In upsampling we repeatedly
sample with replacement from the minority class to make it of equal size
as the majority class. The decision between using downsampling and
upsampling is context-specific, and in general we should try both to see
which produces better results.</p>
</div></section>
</div></section>
</div></section></div></body></html>