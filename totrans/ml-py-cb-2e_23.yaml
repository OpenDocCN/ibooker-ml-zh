- en: Chapter 23\. Saving, Loading, and Serving Trained Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 23.0 Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last 22 chapters and around 200 recipes, we have covered how to take
    raw data and use machine learning to create well-performing predictive models.
    However, for all our work to be worthwhile, we eventually need to *do something*
    with our model, such as integrate it with an existing software application. To
    accomplish this goal, we need to be able to save our models after training, load
    them when they are needed by an application, and then make requests to that application
    to get predictions.
  prefs: []
  type: TYPE_NORMAL
- en: ML models are typically deployed in simple web servers and designed to take
    input data and return predictions. This makes the model available to any client
    on the same network, so other services (such as UIs, users, etc.) can use the
    ML model to make predictions wherever they are in real time. An example use case
    would be using ML for item search on an ecommerce website, where an ML model would
    be served that takes in data about users and listings, and returns a likelihood
    of the user purchasing that listing. The search results need to be available in
    real time and available to the ecommerce application that is responsible for taking
    user searches and coordinating results for the user.
  prefs: []
  type: TYPE_NORMAL
- en: 23.1 Saving and Loading a scikit-learn Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have a trained scikit-learn model and want to save it and load it elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Save the model as a pickle file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the model is saved, we can use scikit-learn in our destination application
    (e.g., web application) to load the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'And use it to make predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step in using a model in production is to save that model as a file
    that can be loaded by another application or workflow. We can accomplish this
    by saving the model as a pickle file, a Python-specific data format that enables
    us to serialize Python objects and write them out to files. Specifically, to save
    the model we use `joblib`, which is a library extending pickle for cases when
    we have large NumPy arrays—​a common occurrence for trained models in scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'When saving scikit-learn models, be aware that saved models might not be compatible
    between versions of scikit-learn; therefore, it can be helpful to include the
    version of scikit-learn used in the model in the filename:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 23.2 Saving and Loading a TensorFlow Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have a trained TensorFlow model and want to save it and load it elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Save the model using the TensorFlow `saved_model` format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then load the model either in another application or for additional
    training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Although we didn’t use TensorFlow significantly throughout the course of this
    book, it is useful to know how to save and load TensorFlow models. Unlike scikit-learn,
    which uses the Python-native `pickle` format, TensorFlow provides its own method
    of saving and loading models. The `saved_model` format creates a directory that
    stores the model and all information necessary to load it back in and make predictions
    in protocol buffer format (which uses the *.pb* file extension):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: While we won’t go into this format in depth, it is the standard way of saving,
    loading, and serving models trained in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Serialization and Saving Keras Models](https://oreil.ly/CDPvo)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TensorFlow Saved Model Format](https://oreil.ly/StpSL)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 23.3 Saving and Loading a PyTorch Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have a trained PyTorch model and want to save it and load it elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the `torch.save` and `torch.load` functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Though we used a similar formula in [Chapter 21](ch21.xhtml#neural-networks)
    to checkpoint our training progress, here we see how the same approach can be
    used to load a model back into memory to make predictions. The `model.pt` that
    we save the model in is actually just a dictionary that contains the model parameters.
    We saved the model state in the dictionary key `model_state_dict`; to load the
    model back in, we re-initialize our network and load the state of the model using
    `network.load_state_dict`.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PyTorch tutorial: Saving and Loading Models](https://oreil.ly/WO3X1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 23.4 Serving scikit-learn Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to serve your trained scikit-learn model using a web server.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Build a Python Flask application that loads the model trained earlier in this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Make sure you have Flask installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'And then run the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can make predictions to the application and get results by submitting
    data points to the endpoints using `curl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this example, we used Flask, a popular open source library for building web
    frameworks in Python. We define one route, `/predict`, that takes JSON data in
    a POST request and returns a dictionary containing the predictions. Though this
    server is not production-ready (see the Flask warning about using a development
    server), we can easily extend and serve this code with a more production-ready
    web framework to move it to production.
  prefs: []
  type: TYPE_NORMAL
- en: 23.5 Serving TensorFlow Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to serve your trained TensorFlow model using a web server.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the open source TensorFlow Serving framework and Docker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorFlow Serving is an open source serving solution optimized for TensorFlow
    models. By simply providing the model path, we get an HTTP and gRPC server out
    of the box with additional useful features for developers.
  prefs: []
  type: TYPE_NORMAL
- en: The `docker run` command runs a container using the public `tensorflow/serving`
    image and mounts the `saved_model` path of our current working directory (`$(pwd)/saved_model`)
    to `/models/saved_model/1` inside our container. This automatically loads the
    model we saved earlier in this chapter into a running Docker container we can
    send prediction queries to.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you go to [*http://localhost:8501/v1/models/saved_model*](http://localhost:8501/v1/models/saved_model)
    in your web browser, you should see the JSON result shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `/metadata` route at [*http://localhost:8501/v1/models/saved_model/metadata*](http://localhost:8501/v1/models/saved_model/metadata)
    will return more information about the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We can make predictions to the REST endpoint using `curl` and passing the variables
    (this neural network takes 10 features):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[TensorFlow documentation: Serving Models](https://oreil.ly/5ZEQo)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 23.6 Serving PyTorch Models in Seldon
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to serve a trained PyTorch model for real-time predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Serve the model using the Seldon Core Python wrapper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'And run it with Docker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While there are many different ways we can serve a PyTorch model, here we choose
    the Seldon Core Python wrapper. Seldon Core is a popular framework for serving
    models in production and has a number of useful features that make it easier to
    use and more scalable than a Flask application. It allows us to write a simple
    class (above we use `MyModel`), while the Python library takes care of all the
    server components and endpoints. We can then run the service using the `seldon-core-microservice`
    command, which starts a REST server, gRPC server, and even exposes a metrics endpoint.
    To make a prediction to the service, we can call the service with the following
    endpoint on port 9000:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Seldon Core Python Package](https://oreil.ly/FTofY)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TorchServe documentation](https://oreil.ly/fjmrE)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
