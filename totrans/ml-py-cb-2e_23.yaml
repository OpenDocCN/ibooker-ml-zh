- en: Chapter 23\. Saving, Loading, and Serving Trained Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 23 章\. 保存、加载和提供训练好的模型
- en: 23.0 Introduction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 23.0 简介
- en: In the last 22 chapters and around 200 recipes, we have covered how to take
    raw data and use machine learning to create well-performing predictive models.
    However, for all our work to be worthwhile, we eventually need to *do something*
    with our model, such as integrate it with an existing software application. To
    accomplish this goal, we need to be able to save our models after training, load
    them when they are needed by an application, and then make requests to that application
    to get predictions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的 22 章和大约 200 个示例中，我们已经涵盖了如何使用机器学习从原始数据创建性能良好的预测模型。然而，为了使我们所有的工作变得有价值，最终我们需要*对我们的模型采取行动*，比如将其集成到现有的软件应用程序中。为了实现这个目标，我们需要能够在训练后保存我们的模型，在应用程序需要时加载它们，然后请求该应用程序获取预测结果。
- en: ML models are typically deployed in simple web servers and designed to take
    input data and return predictions. This makes the model available to any client
    on the same network, so other services (such as UIs, users, etc.) can use the
    ML model to make predictions wherever they are in real time. An example use case
    would be using ML for item search on an ecommerce website, where an ML model would
    be served that takes in data about users and listings, and returns a likelihood
    of the user purchasing that listing. The search results need to be available in
    real time and available to the ecommerce application that is responsible for taking
    user searches and coordinating results for the user.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型通常部署在简单的 Web 服务器上，旨在接收输入数据并返回预测结果。这使得模型能够在同一网络上的任何客户端中使用，因此其他服务（如 UI、用户等）可以实时使用
    ML 模型进行预测。例如，在电子商务网站上使用 ML 进行商品搜索时，将提供一个 ML 模型，该模型接收关于用户和列表的数据，并返回用户购买该列表的可能性。搜索结果需要实时可用，并且可供负责接收用户搜索并协调用户结果的电子商务应用程序使用。
- en: 23.1 Saving and Loading a scikit-learn Model
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 23.1 保存和加载 scikit-learn 模型
- en: Problem
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You have a trained scikit-learn model and want to save it and load it elsewhere.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 您有一个训练好的 scikit-learn 模型，想要在其他地方保存和加载它。
- en: Solution
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Save the model as a pickle file:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型保存为 pickle 文件：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Once the model is saved, we can use scikit-learn in our destination application
    (e.g., web application) to load the model:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型保存完成，我们可以在目标应用程序（例如 Web 应用程序）中使用 scikit-learn 加载该模型：
- en: '[PRE2]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And use it to make predictions:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 并使用它进行预测：
- en: '[PRE3]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Discussion
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: The first step in using a model in production is to save that model as a file
    that can be loaded by another application or workflow. We can accomplish this
    by saving the model as a pickle file, a Python-specific data format that enables
    us to serialize Python objects and write them out to files. Specifically, to save
    the model we use `joblib`, which is a library extending pickle for cases when
    we have large NumPy arrays—​a common occurrence for trained models in scikit-learn.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型用于生产环境的第一步是将该模型保存为可以被另一个应用程序或工作流加载的文件。我们可以通过将模型保存为 pickle 文件来实现这一点，pickle
    是一种 Python 特定的数据格式，使我们能够序列化 Python 对象并将其写入文件。具体来说，为了保存模型，我们使用 `joblib`，这是一个扩展
    pickle 的库，用于处理我们在 scikit-learn 中经常遇到的大型 NumPy 数组。
- en: 'When saving scikit-learn models, be aware that saved models might not be compatible
    between versions of scikit-learn; therefore, it can be helpful to include the
    version of scikit-learn used in the model in the filename:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在保存 scikit-learn 模型时，请注意保存的模型可能在不同版本的 scikit-learn 之间不兼容；因此，在文件名中包含使用的 scikit-learn
    版本可能会有所帮助：
- en: '[PRE5]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 23.2 Saving and Loading a TensorFlow Model
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 23.2 保存和加载 TensorFlow 模型
- en: Problem
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You have a trained TensorFlow model and want to save it and load it elsewhere.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 您有一个训练好的 TensorFlow 模型，想要在其他地方保存和加载它。
- en: Solution
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Save the model using the TensorFlow `saved_model` format:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 的 `saved_model` 格式保存模型：
- en: '[PRE7]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can then load the model either in another application or for additional
    training:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以在另一个应用程序中加载该模型，或用于进一步的训练：
- en: '[PRE9]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Discussion
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Although we didn’t use TensorFlow significantly throughout the course of this
    book, it is useful to know how to save and load TensorFlow models. Unlike scikit-learn,
    which uses the Python-native `pickle` format, TensorFlow provides its own method
    of saving and loading models. The `saved_model` format creates a directory that
    stores the model and all information necessary to load it back in and make predictions
    in protocol buffer format (which uses the *.pb* file extension):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在本书的整个过程中我们并没有大量使用 TensorFlow，但了解如何保存和加载 TensorFlow 模型仍然是有用的。与使用 Python 原生的
    `pickle` 格式不同，TensorFlow 提供了自己的保存和加载模型的方法。`saved_model` 格式创建一个存储模型和加载所需所有信息的目录，以便以协议缓冲区格式（使用
    *.pb* 文件扩展名）加载模型并进行预测：
- en: '[PRE10]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: While we won’t go into this format in depth, it is the standard way of saving,
    loading, and serving models trained in TensorFlow.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们不会深入探讨这种格式，但这是在 TensorFlow 中保存、加载和提供训练模型的标准方式。
- en: See Also
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Serialization and Saving Keras Models](https://oreil.ly/CDPvo)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[序列化和保存 Keras 模型](https://oreil.ly/CDPvo)'
- en: '[TensorFlow Saved Model Format](https://oreil.ly/StpSL)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TensorFlow 保存模型格式](https://oreil.ly/StpSL)'
- en: 23.3 Saving and Loading a PyTorch Model
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 23.3 保存和加载 PyTorch 模型
- en: Problem
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You have a trained PyTorch model and want to save it and load it elsewhere.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个训练好的 PyTorch 模型，并希望在其他地方保存和加载它。
- en: Solution
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use the `torch.save` and `torch.load` functions:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `torch.save` 和 `torch.load` 函数：
- en: '[PRE12]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Discussion
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Though we used a similar formula in [Chapter 21](ch21.xhtml#neural-networks)
    to checkpoint our training progress, here we see how the same approach can be
    used to load a model back into memory to make predictions. The `model.pt` that
    we save the model in is actually just a dictionary that contains the model parameters.
    We saved the model state in the dictionary key `model_state_dict`; to load the
    model back in, we re-initialize our network and load the state of the model using
    `network.load_state_dict`.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在 [第 21 章](ch21.xhtml#neural-networks) 中使用了类似的公式来检查点我们的训练进度，但在这里我们看到相同的方法如何用于将模型加载回内存以进行预测。我们保存模型的
    `model.pt` 实际上只是一个包含模型参数的字典。我们在字典键 `model_state_dict` 中保存了模型状态；为了将模型加载回来，我们重新初始化我们的网络，并使用
    `network.load_state_dict` 加载模型的状态。
- en: See Also
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[PyTorch tutorial: Saving and Loading Models](https://oreil.ly/WO3X1)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch 教程：保存和加载模型](https://oreil.ly/WO3X1)'
- en: 23.4 Serving scikit-learn Models
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 23.4 提供 scikit-learn 模型
- en: Problem
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to serve your trained scikit-learn model using a web server.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要使用 Web 服务器提供你训练好的 scikit-learn 模型。
- en: Solution
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Build a Python Flask application that loads the model trained earlier in this
    chapter:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个 Python Flask 应用程序，加载本章早期训练的模型：
- en: '[PRE14]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Make sure you have Flask installed:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 确保已安装 Flask：
- en: '[PRE15]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'And then run the application:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然后运行应用程序：
- en: '[PRE16]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, we can make predictions to the application and get results by submitting
    data points to the endpoints using `curl`:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过向端点提交数据点来对应用程序进行预测，并通过 `curl` 获取结果：
- en: '[PRE18]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Discussion
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: In this example, we used Flask, a popular open source library for building web
    frameworks in Python. We define one route, `/predict`, that takes JSON data in
    a POST request and returns a dictionary containing the predictions. Though this
    server is not production-ready (see the Flask warning about using a development
    server), we can easily extend and serve this code with a more production-ready
    web framework to move it to production.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们使用了 Flask，这是一个流行的用于构建 Python Web 框架的开源库。我们定义了一个路由 `/predict`，该路由接受 POST
    请求中的 JSON 数据，并返回包含预测结果的字典。尽管这个服务器并非准备用于生产环境（请参阅 Flask 关于使用开发服务器的警告），我们可以很容易地使用更适合生产环境的
    Web 框架扩展和提供此代码以将其移至生产环境。
- en: 23.5 Serving TensorFlow Models
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 23.5 提供 TensorFlow 模型
- en: Problem
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to serve your trained TensorFlow model using a web server.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要使用 Web 服务器提供你训练好的 TensorFlow 模型。
- en: Solution
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use the open source TensorFlow Serving framework and Docker:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用开源 TensorFlow Serving 框架和 Docker：
- en: '[PRE20]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Discussion
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: TensorFlow Serving is an open source serving solution optimized for TensorFlow
    models. By simply providing the model path, we get an HTTP and gRPC server out
    of the box with additional useful features for developers.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving 是一个针对 TensorFlow 模型优化的开源服务解决方案。通过简单提供模型路径，我们就能获得一个 HTTP 和
    gRPC 服务器，并附带开发者所需的额外有用功能。
- en: The `docker run` command runs a container using the public `tensorflow/serving`
    image and mounts the `saved_model` path of our current working directory (`$(pwd)/saved_model`)
    to `/models/saved_model/1` inside our container. This automatically loads the
    model we saved earlier in this chapter into a running Docker container we can
    send prediction queries to.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker run` 命令使用公共 `tensorflow/serving` 镜像运行容器，并将我们当前工作目录的 `saved_model` 路径
    (`$(pwd)/saved_model`) 挂载到容器内部的 `/models/saved_model/1`。这样就会自动将我们之前在本章保存的模型加载到正在运行的
    Docker 容器中，我们可以向其发送预测查询。'
- en: 'If you go to [*http://localhost:8501/v1/models/saved_model*](http://localhost:8501/v1/models/saved_model)
    in your web browser, you should see the JSON result shown here:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在网络浏览器中转到 [*http://localhost:8501/v1/models/saved_model*](http://localhost:8501/v1/models/saved_model)，你应该看到这里显示的
    JSON 结果：
- en: '[PRE21]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The `/metadata` route at [*http://localhost:8501/v1/models/saved_model/metadata*](http://localhost:8501/v1/models/saved_model/metadata)
    will return more information about the model:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*http://localhost:8501/v1/models/saved_model/metadata*](http://localhost:8501/v1/models/saved_model/metadata)
    的 `/metadata` 路由将返回有关模型的更多信息：
- en: '[PRE22]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can make predictions to the REST endpoint using `curl` and passing the variables
    (this neural network takes 10 features):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `curl` 向 REST 端点进行预测，并传递变量（此神经网络使用 10 个特征）：
- en: '[PRE23]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: See Also
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[TensorFlow documentation: Serving Models](https://oreil.ly/5ZEQo)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TensorFlow 文档：模型服务](https://oreil.ly/5ZEQo)'
- en: 23.6 Serving PyTorch Models in Seldon
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 23.6 在 Seldon 中为 PyTorch 模型提供服务
- en: Problem
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to serve a trained PyTorch model for real-time predictions.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望为实时预测提供经过训练的 PyTorch 模型。
- en: Solution
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Serve the model using the Seldon Core Python wrapper:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Seldon Core Python 包装器提供模型服务：
- en: '[PRE25]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'And run it with Docker:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 并使用 Docker 运行它：
- en: '[PRE26]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Discussion
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'While there are many different ways we can serve a PyTorch model, here we choose
    the Seldon Core Python wrapper. Seldon Core is a popular framework for serving
    models in production and has a number of useful features that make it easier to
    use and more scalable than a Flask application. It allows us to write a simple
    class (above we use `MyModel`), while the Python library takes care of all the
    server components and endpoints. We can then run the service using the `seldon-core-microservice`
    command, which starts a REST server, gRPC server, and even exposes a metrics endpoint.
    To make a prediction to the service, we can call the service with the following
    endpoint on port 9000:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以使用多种方式为 PyTorch 模型提供服务，但在这里我们选择了 Seldon Core Python 包装器。Seldon Core 是一个流行的用于在生产环境中为模型提供服务的框架，具有许多有用的功能，使其比
    Flask 应用程序更易于使用和更可扩展。它允许我们编写一个简单的类（上面我们使用 `MyModel`），而 Python 库则负责所有服务器组件和端点。然后我们可以使用
    `seldon-core-microservice` 命令运行服务，该命令启动一个 REST 服务器、gRPC 服务器，甚至公开一个指标端点。要向服务进行预测，我们可以在端口
    9000 上调用以下端点：
- en: '[PRE28]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You should see the following output:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下输出：
- en: '[PRE29]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: See Also
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Seldon Core Python Package](https://oreil.ly/FTofY)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Seldon Core Python 包](https://oreil.ly/FTofY)'
- en: '[TorchServe documentation](https://oreil.ly/fjmrE)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TorchServe 文档](https://oreil.ly/fjmrE)'
