<html><head></head><body><section data-pdf-bookmark="Chapter 15. Bias in Recommendation Systems" data-type="chapter" epub:type="chapter"><div class="chapter" id="Diversity">&#13;
<h1><span class="label">Chapter 15. </span>Bias in Recommendation Systems</h1>&#13;
&#13;
&#13;
<p>We’ve<a data-primary="recommendation systems" data-secondary="bias in" data-type="indexterm" id="RSbias15"/> spent much time in this book dissecting how to improve our recommendations, making them more personalized and relevant to an individual user. Along the way, you’ve learned that latent relationships between users and user personas encode important information about shared preferences. Unfortunately, all of this has a serious downside: bias.</p>&#13;
&#13;
<p>For<a data-primary="bias" data-secondary="types of" data-type="indexterm" id="id1128"/> the purposes of our discussion, we’ll talk about the two most important kinds of bias for recommendation systems:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Overly redundant or self-similar sets of recommendations</p>&#13;
</li>&#13;
<li>&#13;
<p>Stereotypes learned by AI systems</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>First, we’ll delve into the crucial element of diversity in recommendation outputs. As critical as it is for a recommendation system to offer relevant choices to users, ensuring a variety of recommendations is also essential. Diversity not only safeguards against overspecialization but also promotes novel and serendipitous discoveries, enriching the overall user experience.</p>&#13;
&#13;
<p>The balance between relevance and diversity is delicate and can be tricky. This balance challenges the algorithm to go beyond merely echoing users’ past behavior and encourages an exploration of new territories, hopefully providing a more holistically positive experience with the content.</p>&#13;
&#13;
<p>This kind of bias is primarily a technical challenge; how do we satisfy the multiobjectives of diverse recommendations and highly relevant ones?</p>&#13;
&#13;
<p>We’ll consider the intrinsic and extrinsic biases in recommendation systems as an often unintended yet significant consequence of both the underlying algorithms and the data they learn from. Systemic biases in data collection or algorithmic design can result in prejudiced outputs, leading to ethical and fairness issues. Moreover, they may create echo chambers or filter bubbles, curtailing users’ exposure to a broader range of content and inadvertently reinforcing preexisting beliefs.</p>&#13;
&#13;
<p>At the end of this chapter, we will discuss the risks and provide resources to learn more about them. We are not experts in AI fairness and bias, but all ML practitioners should understand and seriously consider these topics. We aim to provide an introduction and signposts.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Diversification of Recommendations" data-type="sect1"><div class="sect1" id="id163">&#13;
<h1>Diversification of Recommendations</h1>&#13;
&#13;
<p>Our<a data-primary="bias" data-secondary="diversification of recommendations" data-type="indexterm" id="Bdivers15"/><a data-primary="diversification of recommendations" data-type="indexterm" id="divrec15"/> first investment into fighting bias is to explicitly target more diversity in our recommendation outputs. We’ll briefly cover two of the many goals you may pursue: intra-list diversity and serendipitous recommendations.</p>&#13;
&#13;
<p><em>Intra-list diversity</em> attempts<a data-primary="intra-list diversity" data-type="indexterm" id="id1129"/> to ensure that there are a variety of types of items within a single recommendation list. The idea is to minimize similarity between the recommended items to reduce overspecialization and encourage exploration. High intra-list diversity within a set of recommendations increases the user’s exposure to many items they may like; however, the recommendations for any particular interest will be shallower, reducing the recall.</p>&#13;
&#13;
<p><em>Serendipitous recommendations</em> are<a data-primary="serendipitous recommendations" data-type="indexterm" id="id1130"/> both surprising and interesting to the user. These are often items that the user might not have discovered independently or that are generally far less popular in the system. Serendipity can be introduced into the recommendation process by injecting nonobvious or unexpected choices—even if those have a  relatively lower affinity score with the user—to improve overall serendipity. In an ideal world, these serendipitous choices are high affinity relative to other items of their popularity, so they’re the “best of the outside choices.”</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Improving Diversity" data-type="sect2"><div class="sect2" id="id164">&#13;
<h2>Improving Diversity</h2>&#13;
&#13;
<p>Now that we have our measures of diversity, we can explicitly attempt to improve them. Importantly, by adding diversity metrics as one of our objectives, we will potentially sacrifice performance on things like recall or NDCG. It can be useful to think of this as a Pareto problem, or to impose a lower bound on ranking metric performance that you’ll accept in pursuit of diversity.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>In<a data-primary="Pareto problem" data-type="indexterm" id="id1131"/> a <em>Pareto problem</em>, you have two priorities that often trade off with each other. In many areas of ML, and more generally applied mathematics, certain outcomes have a natural tension. Diversity in recommendations is an important example of a Pareto problem in recommendation systems, but it’s not the only one. In <a data-type="xref" href="ch14.html#HardRanking">Chapter 14</a>, you briefly saw global optimization, which is an extreme case of trade-offs.</p>&#13;
</div>&#13;
&#13;
<p>One simple approach to improve diversity metrics is<a data-primary="reranking" data-type="indexterm" id="id1132"/> <em>reranking</em>: a post-processing step in which the initially retrieved recommendation list is reordered to enhance diversity. Various algorithms for re-ranking consider not just the relevance scores but also the dissimilarity among the items in the recommendation list. Re-ranking is a strategy that can operationalize any external loss function, so using it for diversity is a straightforward approach.</p>&#13;
&#13;
<p>Another strategy is to break out of the closed loop of recommendation feedback that we discussed in the section <a data-type="xref" href="ch10.html#propensity">“Propensity Weighting for Recommendation System Evaluation”</a>. As in multiarmed bandit problems,<a data-primary="explore-exploit recommenders" data-type="indexterm" id="id1133"/> <em>explore-exploit trade-offs</em> can choose between exploiting what the model knows the user will like and exploring less certain options that may yield higher rewards. This trade-off can be used in recommendation systems to ensure diversity by occasionally choosing to <em>explore</em> and recommend less obvious choices. To implement a system like this, we can use affinity as a reward estimate and propensity as an exploitation measure.</p>&#13;
&#13;
<p>Instead of using these posterior strategies, an alternative is to <em>incorporate diversity as an objective in the learning process</em> or include a<a data-primary="loss functions" data-secondary="including diversity regularization terms in" data-type="indexterm" id="id1134"/><a data-primary="regularization" data-type="indexterm" id="id1135"/> diversity regularization term in the loss function. Multiobjective loss including pairwise similarity as a regularizer can help train the model to learn diverse sets of recommendations. You previously saw that kinds of regularization can coach the training process to minimize certain behaviors. One regularization<a data-primary="low-rank methods" data-secondary="dot-product similarity" data-type="indexterm" id="id1136"/><a data-primary="dot-product similarity" data-type="indexterm" id="id1137"/><a data-primary="similarity" data-secondary="dot-product similarity" data-type="indexterm" id="id1138"/> term that can be used explicitly is<a data-primary="similarity" data-secondary="among recommendations" data-type="indexterm" id="id1139"/> <em>similarity among recommendations</em>; the dot product of each embedding vector in the recommendations to each other can approximate this self-similarity. Let <math alttext="script upper R equals left-parenthesis upper R 1 comma upper R 2 comma ellipsis comma upper R Subscript k Baseline right-parenthesis">&#13;
  <mrow>&#13;
    <mi>ℛ</mi>&#13;
    <mo>=</mo>&#13;
    <mo>(</mo>&#13;
    <msub><mi>R</mi> <mn>1</mn> </msub>&#13;
    <mo>,</mo>&#13;
    <msub><mi>R</mi> <mn>2</mn> </msub>&#13;
    <mo>,</mo>&#13;
    <mo>...</mo>&#13;
    <mo>,</mo>&#13;
    <msub><mi>R</mi> <mi>k</mi> </msub>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math> be the list of embeddings for the recommendations, and then consider <math alttext="script upper R">&#13;
  <mi>ℛ</mi>&#13;
</math> as a column matrix—with each row a recommendation. Calculating <math alttext="script upper R">&#13;
  <mi>ℛ</mi>&#13;
</math>’s Gramian<a data-primary="Gramian regularization" data-type="indexterm" id="id1140"/> would yield all our dot-product similarity calculations, and thus we can regularize by this term with appropriate hyperparameter weighting. Note that this differs from our previous Gramian regularization because we’re considering the recommendations for only an individual query in this case.</p>&#13;
&#13;
<p>Finally, we can use rankings from multiple domains to boost recommendation diversity. By integrating various ranking measures, the recommendation system can suggest items from outside the user’s “mode,” thus broadening the range of recommendations. A vibrant discipline exists around<a data-primary="multimodal recommendations" data-type="indexterm" id="id1141"/><a data-primary="recommendation systems" data-secondary="multimodal recommendations" data-type="indexterm" id="id1142"/><a data-primary="PinnerSage paper" data-type="indexterm" id="id1143"/> multimodal recommendations, with the <a href="https://oreil.ly/KOQK2">PinnerSage paper</a> from Pinterest a particularly impressive implementation. In many of the works about multimodal recommendations, the retrieval step returns too many recommendations near to the user’s query vector. This forces self-similarity among the retrieved list. Multimodality forces multiple query vectors to be used for each request, allowing a built-in diversity.</p>&#13;
&#13;
<p>Let’s look at another perspective on item self-similarity and think about how the pairwise relationships between items can be used to this end.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Applying Portfolio Optimization" data-type="sect2"><div class="sect2" id="id165">&#13;
<h2>Applying Portfolio Optimization</h2>&#13;
&#13;
<p><em>Portfolio optimization</em>, a<a data-primary="portfolio optimization" data-type="indexterm" id="id1144"/><a data-primary="optimization" data-secondary="portfolio optimization" data-type="indexterm" id="id1145"/> concept borrowed from finance, can be an effective approach to enhance diversity in recommendation systems. The goal here is to create a “portfolio” of recommended items that balances the two key parameters: relevance and diversity.</p>&#13;
&#13;
<p>At its heart, portfolio optimization is about risk (in our case, relevance) and return (diversity). Here’s a basic approach for applying this optimization to recommendation systems:</p>&#13;
<ol>&#13;
<li>&#13;
<p>Formulate an item representation such that the distance in the space is a good measure of similarity. This is in line with our previous discussions about what makes a good latent space.</p>&#13;
</li>&#13;
<li>&#13;
<p>Calculate pairwise distance between items. You can do this by using whatever distance metric that enriches your latent space. It is important to calculate these pairwise distances across all items retrieved and be ready for consideration to return. Note that how you aggregate these distributions of distances can be nuanced.</p>&#13;
</li>&#13;
<li>&#13;
<p>Evaluate affinity for the retrieved set. Note that calibrated affinity scores will perform better as they provide a more realistic estimate of return.</p>&#13;
</li>&#13;
<li>&#13;
<p>Solve the optimization problem. Solving the problem will yield a weight for each item that balances the trade-off between relevance and diversity. Items with higher weights are more valuable in terms of both diversity and relevance, and they should be prioritized in the recommendation list. Mathematically, the problem looks like this:</p>&#13;
<div data-type="equation">&#13;
<math alttext="less-than u l c l a s s equals quotation-mark s i m p l e l i s t quotation-mark greater-than less-than l i greater-than upper M a x i m i z e left-parenthesis w Superscript upper T Baseline asterisk r minus lamda asterisk w Superscript upper T Baseline asterisk upper C asterisk w right-parenthesis less-than slash l i greater-than less-than slash u l greater-than" display="block">&#13;
  <mrow>&#13;
    <mi>M</mi>&#13;
    <mi>a</mi>&#13;
    <mi>x</mi>&#13;
    <mi>i</mi>&#13;
    <mi>m</mi>&#13;
    <mi>i</mi>&#13;
    <mi>z</mi>&#13;
    <mi>e</mi>&#13;
    <mo>(</mo>&#13;
    <msup><mi>w</mi> <mi>T</mi> </msup>&#13;
    <mo>*</mo>&#13;
    <mi>r</mi>&#13;
    <mo>-</mo>&#13;
    <mi>λ</mi>&#13;
    <mo>*</mo>&#13;
    <msup><mi>w</mi> <mi>T</mi> </msup>&#13;
    <mo>*</mo>&#13;
    <mi>C</mi>&#13;
    <mo>*</mo>&#13;
    <mi>w</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Here, <math alttext="w">&#13;
  <mi>w</mi>&#13;
</math> is a vector representing the weights (i.e., the proportion of each item in the recommendation list), <math alttext="r">&#13;
  <mi>r</mi>&#13;
</math> is the relevance score vector, <math alttext="upper C">&#13;
  <mi>C</mi>&#13;
</math> is the covariance matrix (which captures the diversity), and <math alttext="lamda">&#13;
  <mi>λ</mi>&#13;
</math> is a parameter to balance relevance and diversity. The constraint here is that the sum of the weights equals 1.</p>&#13;
&#13;
<p>Remember, the hyperparameter <math alttext="lamda">&#13;
  <mi>λ</mi>&#13;
</math> trades off between relevance and diversity. This makes it a critical part of this process and may require experimentation or tuning based on the specific needs of your system and its users. This would be straightforward via hyperparameter optimization in one of many packages such as Weights &amp; Biases.<a data-primary="" data-startref="Bdivers15" data-type="indexterm" id="id1146"/><a data-primary="" data-startref="divrec15" data-type="indexterm" id="id1147"/></p>&#13;
</li>&#13;
&#13;
</ol>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Multiobjective Functions" data-type="sect1"><div class="sect1" id="id166">&#13;
<h1>Multiobjective Functions</h1>&#13;
&#13;
<p>Another<a data-primary="bias" data-secondary="multiobjective functions" data-type="indexterm" id="id1148"/><a data-primary="multiobjective functions" data-type="indexterm" id="id1149"/> related approach to diversity is to rank based on a multiobjective loss. Instead of the ranking stage being purely personalization affinity, introducing a second (or more!) ranking term can dramatically improve diversity.</p>&#13;
&#13;
<p>The simplest approach here is something similar to what you learned in <a data-type="xref" href="ch14.html#HardRanking">Chapter 14</a>: hard ranking. A business rule that may apply to diversity is limiting each item category to only one item. This is the simplest case of multiobjective ranking because sorting by a categorical column and selecting the max in each group will achieve explicit diversity with respect to that covariate. Let’s move on to something more subtle.</p>&#13;
&#13;
<p>In <a href="https://oreil.ly/OREt2">“Stitching Together Spaces for Query-Based Recommendations”</a>, one of this book’s authors worked with coauthor Ian Horn to implement a multiobjective recommendation system that balanced both personalization and relevance to an image-retrieval problem.</p>&#13;
&#13;
<p>The goal was to provide personalized recommendations for clothing that were similar to clothes in an image the user uploaded. This means there are two latent spaces:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The latent space of personalized clothes to a user</p>&#13;
</li>&#13;
<li>&#13;
<p>The latent space of images of clothing</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>To solve this problem, we first had to make a decision: what was more important for relevance? Personalization or image similarity? Because the product was centered around a photo-upload experience, we chose image similarity. However, we had another fact to consider: each uploaded image contained several pieces of clothing. As is popular in computer vision, we first segmented the model into several items and then treated each item as its own query<a data-primary="anchor-items" data-type="indexterm" id="id1150"/> (which we called <em>anchor-items</em>). This meant our image-similarity retrieval was multimodal, as we searched with several different query vectors. After we gathered them all, we had to make one final ranking—a multiobjective ranking for image similarity and personalization. The loss function we optimized is shown here:</p>&#13;
<div data-type="equation">&#13;
<math alttext="s Subscript i Baseline equals alpha times left-parenthesis 1 minus d Subscript i Baseline right-parenthesis plus left-parenthesis 1 minus alpha right-parenthesis times a Subscript i" display="block">&#13;
  <mrow>&#13;
    <msub><mi>s</mi> <mi>i</mi> </msub>&#13;
    <mo>=</mo>&#13;
    <mi>α</mi>&#13;
    <mo>×</mo>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mn>1</mn>&#13;
      <mo>-</mo>&#13;
      <msub><mi>d</mi> <mi>i</mi> </msub>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>+</mo>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mn>1</mn>&#13;
      <mo>-</mo>&#13;
      <mi>α</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>×</mo>&#13;
    <msub><mi>a</mi> <mi>i</mi> </msub>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>The <math alttext="alpha">&#13;
  <mi>α</mi>&#13;
</math> is a hyperparameter that represents the weighting, <math alttext="d Subscript i">&#13;
  <msub><mi>d</mi> <mi>i</mi> </msub>&#13;
</math> is the image distance, and <math alttext="a Subscript i">&#13;
  <msub><mi>a</mi> <mi>i</mi> </msub>&#13;
</math> is the personalization. We learn α experimentally. The last step was to impose some hard ranking to ensure that one recommendation came from each anchor.</p>&#13;
&#13;
<p class="less_space pagebreak-before">So let’s sum this up:</p>&#13;
<ol>&#13;
<li>&#13;
<p>We used two latent spaces with distances to provide rankings.</p>&#13;
</li>&#13;
<li>&#13;
<p>We did multimodal retrieval via image segmentation.</p>&#13;
</li>&#13;
<li>&#13;
<p>We retrieved using only one of the rankings.</p>&#13;
</li>&#13;
<li>&#13;
<p>Our final ranking was multiobjective, with hard ranking utilizing all our latent spaces and business logic.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>This allowed our recommendations to be <em>diverse</em> in the sense that they achieved relevance in several areas of the query that corresponded to different items.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Predicate Pushdown" data-type="sect1"><div class="sect1" id="PredicatePushdown">&#13;
<h1>Predicate Pushdown</h1>&#13;
&#13;
<p>You<a data-primary="bias" data-secondary="predicate pushdown" data-type="indexterm" id="id1151"/><a data-primary="predicate pushdown" data-type="indexterm" id="id1152"/><a data-primary="optimization" data-secondary="predicate pushdown" data-type="indexterm" id="id1153"/> may be happy and comfortable applying these metrics during serving—after all, that’s the title for this part of the book—but before we move on from this topic, we should talk about an edge case that can have quite disastrous consequences. When you impose the hard rules from <a data-type="xref" href="ch14.html#HardRanking">Chapter 14</a> and the diversity expectations discussed earlier in this chapter, and do a little multiobjective ranking, sometimes you arrive at…​no recommendations.</p>&#13;
&#13;
<p>Say you start by retrieving <em>k</em> items, but after the sufficiently diverse combinations that also satisfy business rules, there’s simply nothing left. You might say, “I’ll just retrieve more items; let’s crank up <em>k</em>!” But this has some serious issues: it can really increase latency, depress match quality, and throw off your ranking model that is more tuned to lower-cardinality sets.</p>&#13;
&#13;
<p>A common experience, especially with diversity, is that different modes for the retrieval have vastly different match scores. To take an example from our fashion recommender world: all jeans might be a better match than any shirt we have, but if you’re looking for diverse categories of clothes to recommend, no matter how big the <em>k</em>, you’ll potentially be missing out on shirts.</p>&#13;
&#13;
<p>One solution to this problem is <em>predicate pushdown</em>. This optimization technique is used in databases, specifically in the context of data retrieval. The main idea of predicate pushdown is to filter data as early as possible in the data-retrieval process, to reduce the amount of data that needs to be processed later in the query execution plan.</p>&#13;
&#13;
<p>For traditional databases, you see predicate pushdown applied, for example, as “apply my query’s <code>where</code> clause in the database to cut down on I/O.” It may achieve this by explicitly pulling the relevant columns to check the <code>where</code> clause first, and then getting the row IDs from those that pass before executing the rest of the query.</p>&#13;
&#13;
<p>How does this help us in our case? The simple idea is if your vector store also has features for the vectors, you can include the feature comparisons as part of retrieval. Let’s take an overly simple example: assume your items have a categorical feature called <code>color</code>, and for good diverse recommendations you want a nice set of at least three colors in your five recommendations. To achieve this, you can do a top-<em>k</em> search across each of the colors in your store (the downside is that your retrieval is <em>C</em> times as large, where <em>C</em> is the number of colors that exist) and then do ranking and diversity on the union of these sets. This has a much higher likelihood of surviving your diversity rule in the eventual recommendations. This is great! We expect that latency is relatively low in retrieval, so this tax of extra retrievals isn’t bad if we know where to look.</p>&#13;
&#13;
<p>This optimization technique can be applied on quite complicated predicates if your vector store is set up well for the kinds of filters you wish to impose.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Fairness" data-type="sect1"><div class="sect1" id="id168">&#13;
<h1>Fairness</h1>&#13;
&#13;
<p>Fairness<a data-primary="bias" data-secondary="fairness" data-type="indexterm" id="id1154"/><a data-primary="fairness" data-type="indexterm" id="id1155"/><a data-primary="machine learning (ML)" data-secondary="fairness in" data-type="indexterm" id="id1156"/><a data-primary="recommendation systems" data-secondary="fairness in" data-type="indexterm" id="id1157"/> in ML in general is a particularly nuanced subject that is ill-served by short summaries. The following topics are important, and we invite you to consider the robust references included here:</p>&#13;
<dl>&#13;
<dt>Nudging</dt>&#13;
<dd>&#13;
<p>Fairness<a data-primary="nudging" data-type="indexterm" id="id1158"/> does not need to be only “equal probabilities for all outcomes”; it can be fair with respect to a specific covariate. Nudging via a recommender—i.e., recommending items to emphasize certain behavior or buying patterns—can increase fairness. Consider the work by Karlijn Dinnissen and Christine Bauer from Spotify about <a href="https://oreil.ly/fit3j">using nudging to improve gender representation in music recommendations</a>.</p>&#13;
</dd>&#13;
<dt>Filter bubbles</dt>&#13;
<dd>&#13;
<p>Filter bubbles<a data-primary="filter bubbles" data-type="indexterm" id="id1159"/><a data-primary="collaborative filtering (CF)" data-secondary="filter bubbles" data-type="indexterm" id="id1160"/> are a downside of extreme collaborative filtering: a group of users begin liking similar recommendations, the system learns that they should receive similar recommendations, and the feedback loop perpetuates this. For a deep look into not only the concept but also mitigation strategies, consider <a href="https://oreil.ly/2jyeJ">“Mitigating the Filter Bubble While Maintaining Relevance”</a> by Zhaolin Gao et al.</p>&#13;
</dd>&#13;
<dt>High risk</dt>&#13;
<dd>&#13;
<p>Not<a data-primary="high-risk applications" data-type="indexterm" id="id1161"/> all applications of AI are equal in risk. Some domains are particularly harmful when AI systems are poorly guardrailed. For a general overview of the most high-risk circumstances and mitigation, consult <a class="orm:hideurl" href="https://learning.oreilly.com/library/view/machine-learning-for/9781098102425/"><em>Machine Learning for High-Risk Applications</em></a>  by Patrick Hall et al. (O’Reilly).</p>&#13;
</dd>&#13;
<dt>Trustworthiness</dt>&#13;
<dd>&#13;
<p>Explainable models<a data-primary="trustworthiness" data-type="indexterm" id="id1162"/> is a popular mitigation strategy for risky applications of AI. While explainability does not <em>solve</em> the problem, it frequently provides a path toward identification and resolution. For a deep dive on this, <a class="orm:hideurl" href="https://learning.oreilly.com/library/view/practicing-trustworthy-machine/9781098120269/"><em>Practicing Trustworthy Machine Learning</em></a>  by Yada Pruksachatkun et al. (O’Reilly) provides tools and techniques.</p>&#13;
</dd>&#13;
<dt>Fairness in recommendations</dt>&#13;
<dd>&#13;
<p>Because recommendation systems are so obviously susceptible to issues of AI fairness, much has been written on the topic. Each of the major social media giants has employed teams working in AI safety. One particular highlight is the Twitter Responsible AI team led by Rumman Chowdhury. You can read about the team’s work in <a href="https://oreil.ly/uvFep">“Can Auditing Eliminate Bias from Algorithms?”</a> by <span class="keep-together">Alfred Ng.</span><a data-primary="" data-startref="RSbias15" data-type="indexterm" id="id1163"/></p>&#13;
</dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id320">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>While these techniques provide pathways to enhance diversity, it’s important to remember to strike a balance between diversity and relevance. The exact method or combination of methods used may vary depending on the specific use case, the available data, the intricacies of the user base, and the kind of feedback you’re collecting. As you implement recommendation systems, think about which aspects are the most key in your diversity problem.</p>&#13;
</div></section>&#13;
</div></section></body></html>