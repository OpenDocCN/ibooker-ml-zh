["```py\n>>> from sklearn.cluster import KMeans\n>>> X_std = preprocessing.StandardScaler().fit_transform(\n...     X\n... )\n>>> km = KMeans(2, random_state=42)\n>>> km.fit(X_std)\nKMeans(algorithm='auto', copy_x=True,\n init='k-means', max_iter=300,\n n_clusters=2, n_init=10, n_jobs=1,\n precompute_distances='auto',\n random_state=42, tol=0.0001, verbose=0)\n```", "```py\n>>> X_km = km.predict(X)\n>>> X_km\narray([1, 1, 1, ..., 1, 1, 1], dtype=int32)\n```", "```py\n>>> inertias = []\n>>> sizes = range(2, 12)\n>>> for k in sizes:\n...     k2 = KMeans(random_state=42, n_clusters=k)\n...     k2.fit(X)\n...     inertias.append(k2.inertia_)\n>>> fig, ax = plt.subplots(figsize=(6, 4))\n>>> pd.Series(inertias, index=sizes).plot(ax=ax)\n>>> ax.set_xlabel(\"K\")\n>>> ax.set_ylabel(\"Inertia\")\n>>> fig.savefig(\"images/mlpr_1801.png\", dpi=300)\n```", "```py\n>>> from sklearn import metrics\n>>> inertias = []\n>>> sils = []\n>>> chs = []\n>>> dbs = []\n>>> sizes = range(2, 12)\n>>> for k in sizes:\n...     k2 = KMeans(random_state=42, n_clusters=k)\n...     k2.fit(X_std)\n...     inertias.append(k2.inertia_)\n...     sils.append(\n...         metrics.silhouette_score(X, k2.labels_)\n...     )\n...     chs.append(\n...         metrics.calinski_harabasz_score(\n...             X, k2.labels_\n...         )\n...     )\n...     dbs.append(\n...         metrics.davies_bouldin_score(\n...             X, k2.labels_\n...         )\n...     )\n>>> fig, ax = plt.subplots(figsize=(6, 4))\n>>> (\n...     pd.DataFrame(\n...         {\n...             \"inertia\": inertias,\n...             \"silhouette\": sils,\n...             \"calinski\": chs,\n...             \"davis\": dbs,\n...             \"k\": sizes,\n...         }\n...     )\n...     .set_index(\"k\")\n...     .plot(ax=ax, subplots=True, layout=(2, 2))\n... )\n>>> fig.savefig(\"images/mlpr_1802.png\", dpi=300)\n```", "```py\n>>> from yellowbrick.cluster.silhouette import (\n...     SilhouetteVisualizer,\n... )\n>>> fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n>>> axes = axes.reshape(4)\n>>> for i, k in enumerate(range(2, 6)):\n...     ax = axes[i]\n...     sil = SilhouetteVisualizer(\n...         KMeans(n_clusters=k, random_state=42),\n...         ax=ax,\n...     )\n...     sil.fit(X_std)\n...     sil.finalize()\n...     ax.set_xlim(-0.2, 0.8)\n>>> plt.tight_layout()\n>>> fig.savefig(\"images/mlpr_1803.png\", dpi=300)\n```", "```py\n>>> from scipy.cluster import hierarchy\n>>> fig, ax = plt.subplots(figsize=(6, 4))\n>>> dend = hierarchy.dendrogram(\n...     hierarchy.linkage(X_std, method=\"ward\")\n... )\n>>> fig.savefig(\"images/mlpr_1804.png\", dpi=300)\n```", "```py\n>>> from scipy.cluster import hierarchy\n>>> fig, ax = plt.subplots(figsize=(6, 4))\n>>> dend = hierarchy.dendrogram(\n...     hierarchy.linkage(X_std, method=\"ward\"),\n...     truncate_mode=\"lastp\",\n...     p=20,\n...     show_contracted=True,\n... )\n>>> fig.savefig(\"images/mlpr_1805.png\", dpi=300)\n```", "```py\n>>> from sklearn.cluster import (\n...     AgglomerativeClustering,\n... )\n>>> ag = AgglomerativeClustering(\n...     n_clusters=4,\n...     affinity=\"euclidean\",\n...     linkage=\"ward\",\n... )\n>>> ag.fit(X)\n```", "```py\n>>> km = KMeans(n_clusters=2)\n>>> km.fit(X_std)\n>>> labels = km.predict(X_std)\n>>> (\n...     X.assign(cluster=labels, survived=y)\n...     .groupby(\"cluster\")\n...     .agg([\"mean\", \"var\"])\n...     .T\n... )\ncluster                 0         1\npclass     mean  0.526538 -1.423831\n var   0.266089  0.136175\nage        mean -0.280471  0.921668\n var   0.653027  1.145303\nsibsp      mean -0.010464 -0.107849\n var   1.163848  0.303881\nparch      mean  0.387540  0.378453\n var   0.829570  0.540587\nfare       mean -0.349335  0.886400\n var   0.056321  2.225399\nsex_male   mean  0.678986  0.552486\n var   0.218194  0.247930\nembarked_Q mean  0.123548  0.016575\n var   0.108398  0.016345\nembarked_S mean  0.741288  0.585635\n var   0.191983  0.243339\nsurvived   mean  0.596685  0.299894\n var   0.241319  0.210180\n```", "```py\n.style.background_gradient(cmap='RdBu', axis=1)\n```", "```py\n>>> fig, ax = plt.subplots(figsize=(6, 4))\n... (\n...     X.assign(cluster=labels, survived=y)\n...     .groupby(\"cluster\")\n...     .mean()\n...     .T.plot.bar(ax=ax)\n... )\n>>> fig.savefig(\n...     \"images/mlpr_1806.png\",\n...     dpi=300,\n...     bbox_inches=\"tight\",\n... )\n```", "```py\n>>> fig, ax = plt.subplots(figsize=(6, 4))\n>>> sns.scatterplot(\n...     \"PC1\",\n...     \"PC2\",\n...     data=X.assign(\n...         PC1=X_pca[:, 0],\n...         PC2=X_pca[:, 1],\n...         cluster=labels,\n...     ),\n...     hue=\"cluster\",\n...     alpha=0.5,\n...     ax=ax,\n... )\n>>> fig.savefig(\n...     \"images/mlpr_1807.png\",\n...     dpi=300,\n...     bbox_inches=\"tight\",\n... )\n```", "```py\n>>> (\n...     X.assign(cluster=label)\n...     .groupby(\"cluster\")\n...     .age.describe()\n...     .T\n... )\ncluster           0           1\ncount    362.000000  947.000000\nmean       0.921668   -0.280471\nstd        1.070188    0.808101\nmin       -2.160126   -2.218578\n25%        0.184415   -0.672870\n50%        0.867467   -0.283195\n75%        1.665179    0.106480\nmax        4.003228    3.535618\n```", "```py\n>>> dt = tree.DecisionTreeClassifier()\n>>> dt.fit(X, labels)\n>>> for col, val in sorted(\n...     zip(X.columns, dt.feature_importances_),\n...     key=lambda col_val: col_val[1],\n...     reverse=True,\n... ):\n...     print(f\"{col:10}{val:10.3f}\")\npclass         0.902\nage            0.074\nsex_male       0.016\nembarked_S     0.003\nfare           0.003\nparch          0.003\nsibsp          0.000\nembarked_Q     0.000\n```", "```py\n>>> dot_data = StringIO()\n>>> tree.export_graphviz(\n...     dt,\n...     out_file=dot_data,\n...     feature_names=X.columns,\n...     class_names=[\"0\", \"1\"],\n...     max_depth=2,\n...     filled=True,\n... )\n>>> g = pydotplus.graph_from_dot_data(\n...     dot_data.getvalue()\n... )\n>>> g.write_png(\"images/mlpr_1808.png\")\n```"]