- en: Chapter 4\. Handling Numerical Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 4.0 Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Quantitative data is the measurement of something—​whether class size, monthly
    sales, or student scores. The natural way to represent these quantities is numerically
    (e.g., 29 students, $529,392 in sales). In this chapter, we will cover numerous
    strategies for transforming raw numerical data into features purpose-built for
    machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Rescaling a Feature
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to rescale the values of a numerical feature to be between two values.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use scikit-learn’s `MinMaxScaler` to rescale a feature array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Rescaling* is a common preprocessing task in machine learning. Many of the
    algorithms described later in this book will assume all features are on the same
    scale, typically 0 to 1 or –1 to 1\. There are a number of rescaling techniques,
    but one of the simplest is called *min-max scaling*. Min-max scaling uses the
    minimum and maximum values of a feature to rescale values to within a range. Specifically,
    min-max calculates:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msubsup><mi>x</mi> <mi>i</mi> <mo>'</mo></msubsup>
    <mo>=</mo> <mfrac><mrow><msub><mi>x</mi> <mi>i</mi></msub> <mo>-</mo><mtext>min</mtext><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow>
    <mrow><mtext>max</mtext><mo>(</mo><mi>x</mi><mo>)</mo><mo>-</mo><mtext>min</mtext><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'where <math display="inline"><mi>x</mi></math> is the feature vector, <math
    display="inline"><msub><mi>x</mi><mi>i</mi></msub></math> is an individual element
    of feature <math display="inline"><mi>x</mi></math>, and <math display="inline"><msubsup><mi>x</mi>
    <mi>i</mi> <mo>''</mo></msubsup></math> is the rescaled element. In our example,
    we can see from the outputted array that the feature has been successfully rescaled
    to between 0 and 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: scikit-learn’s `MinMaxScaler` offers two options to rescale a feature. One option
    is to use `fit` to calculate the minimum and maximum values of the feature, and
    then use `transform` to rescale the feature. The second option is to use `fit_transform`
    to do both operations at once. There is no mathematical difference between the
    two options, but there is sometimes a practical benefit to keeping the operations
    separate because it allows us to apply the same transformation to different *sets*
    of the data.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Feature scaling, Wikipedia](https://oreil.ly/f2WiM)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[About Feature Scaling and Normalization, Sebastian Raschka](https://oreil.ly/Da0AH)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4.2 Standardizing a Feature
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to transform a feature to have a mean of 0 and a standard deviation
    of 1.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'scikit-learn’s `StandardScaler` performs both transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A common alternative to the min-max scaling discussed in [Recipe 4.1](#rescaling-a-feature)
    is rescaling of features to be approximately standard normally distributed. To
    achieve this, we use standardization to transform the data such that it has a
    mean, <math display="inline"><mover accent="true"><mi>x</mi> <mo>¯</mo></mover></math>,
    of 0 and a standard deviation, <math display="inline"><mi>σ</mi></math>, of 1\.
    Specifically, each element in the feature is transformed so that:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msubsup><mi>x</mi> <mi>i</mi> <mo>'</mo></msubsup>
    <mo>=</mo> <mfrac><mrow><msub><mi>x</mi> <mi>i</mi></msub> <mo>-</mo><mover accent="true"><mi>x</mi>
    <mo>¯</mo></mover></mrow> <mi>σ</mi></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math display="inline"><msubsup><mi>x</mi> <mi>i</mi> <mo>'</mo></msubsup></math>
    is our standardized form of <math display="inline"><msub><mi>x</mi> <mi>i</mi></msub></math>
    . The transformed feature represents the number of standard deviations of the
    original value from the feature’s mean value (also called a *z-score* in statistics).
  prefs: []
  type: TYPE_NORMAL
- en: Standardization is a common go-to scaling method for machine learning preprocessing
    and, in my experience, is used more often than min-max scaling. However, it depends
    on the learning algorithm. For example, principal component analysis often works
    better using standardization, while min-max scaling is often recommended for neural
    networks (both algorithms are discussed later in this book). As a general rule,
    I’d recommend defaulting to standardization unless you have a specific reason
    to use an alternative.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the effect of standardization by looking at the mean and standard
    deviation of our solution’s output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If our data has significant outliers, it can negatively impact our standardization
    by affecting the feature’s mean and variance. In this scenario, it is often helpful
    to instead rescale the feature using the median and quartile range. In scikit-learn,
    we do this using the `RobustScaler` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 4.3 Normalizing Observations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to rescale the feature values of observations to have unit norm (a
    total length of 1).
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use `Normalizer` with a `norm` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many rescaling methods (e.g., min-max scaling and standardization) operate on
    features; however, we can also rescale across individual observations. `Normalizer`
    rescales the values on individual observations to have unit norm (the sum of their
    lengths is 1). This type of rescaling is often used when we have many equivalent
    features (e.g., text classification when every word or *n*-word group is a feature).
  prefs: []
  type: TYPE_NORMAL
- en: '`Normalizer` provides three norm options with Euclidean norm (often called
    L2) being the default argument:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mfenced close="∥" open="∥"><mi>x</mi></mfenced>
    <mn>2</mn></msub> <mo>=</mo> <msqrt><mrow><msup><msub><mi>x</mi> <mn>1</mn></msub>
    <mn>2</mn></msup> <mo>+</mo> <msup><msub><mi>x</mi> <mn>2</mn></msub> <mn>2</mn></msup>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msup><msub><mi>x</mi> <mi>n</mi></msub> <mn>2</mn></msup></mrow></msqrt></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math display="inline"><mi>x</mi></math> is an individual observation
    and <math display="inline"><msub><mi>x</mi><mi>n</mi></msub></math> is that observation’s
    value for the <math display="inline"><mi>n</mi></math>th feature.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can specify Manhattan norm (L1):'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mfenced close="∥" open="∥"><mi>x</mi></mfenced>
    <mn>1</mn></msub> <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <mfenced close="|" open="|" separators=""><msub><mi>x</mi>
    <mi>i</mi></msub></mfenced> <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Intuitively, L2 norm can be thought of as the distance between two points in
    New York for a bird (i.e., a straight line), while L1 can be thought of as the
    distance for a human walking on the street (walk north one block, east one block,
    north one block, east one block, etc.), which is why it is called “Manhattan norm”
    or “Taxicab norm.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Practically, notice that `norm="l1"` rescales an observation’s values so they
    sum to 1, which can sometimes be a desirable quality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 4.4 Generating Polynomial and Interaction Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to create polynomial and interaction features.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Even though some choose to create polynomial and interaction features manually,
    scikit-learn offers a built-in method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The `degree` parameter determines the maximum degree of the polynomial. For
    example, `degree=2` will create new features raised to the second power:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>,</mo> <msup><msub><mi>x</mi> <mn>1</mn></msub> <mn>2</mn></msup>
    <mo>,</mo> <msup><msub><mi>x</mi> <mn>1</mn></msub> <mn>2</mn></msup> <mo>,</mo>
    <msup><msub><mi>x</mi> <mn>2</mn></msub> <mn>2</mn></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'while `degree=3` will create new features raised to the second and third power:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>,</mo> <msup><msub><mi>x</mi> <mn>1</mn></msub> <mn>2</mn></msup>
    <mo>,</mo> <msup><msub><mi>x</mi> <mn>2</mn></msub> <mn>2</mn></msup> <mo>,</mo>
    <msup><msub><mi>x</mi> <mn>1</mn></msub> <mn>3</mn></msup> <mo>,</mo> <msup><msub><mi>x</mi>
    <mn>2</mn></msub> <mn>3</mn></msup> <mo>,</mo> <msup><msub><mi>x</mi> <mn>1</mn></msub>
    <mn>2</mn></msup> <mo>,</mo> <msup><msub><mi>x</mi> <mn>1</mn></msub> <mn>3</mn></msup>
    <mo>,</mo> <msup><msub><mi>x</mi> <mn>2</mn></msub> <mn>3</mn></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, by default `PolynomialFeatures` includes interaction features:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>x</mi> <mn>1</mn></msub> <msub><mi>x</mi>
    <mn>2</mn></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'We can restrict the features created to only interaction features by setting
    `interaction_only` to `True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Polynomial features are often created when we want to include the notion that
    there exists a nonlinear relationship between the features and the target. For
    example, we might suspect that the effect of age on the probability of having
    a major medical condition is not constant over time but increases as age increases.
    We can encode that nonconstant effect in a feature, <math display="inline"><mi>x</mi></math>,
    by generating that feature’s higher-order forms (<math display="inline"><msup><mi>x</mi><mn>2</mn></msup></math>,
    <math display="inline"><msup><mi>x</mi><mn>3</mn></msup></math>, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, often we run into situations where the effect of one feature
    is dependent on another feature. A simple example would be if we were trying to
    predict whether or not our coffee was sweet, and we had two features: (1) whether
    or not the coffee was stirred, and (2) whether or not we added sugar. Individually,
    each feature does not predict coffee sweetness, but the combination of their effects
    does. That is, a coffee would only be sweet if the coffee had sugar and was stirred.
    The effects of each feature on the target (sweetness) are dependent on each other.
    We can encode that relationship by including an interaction feature that is the
    product of the individual features.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Transforming Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to make a custom transformation to one or more features.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In scikit-learn, use `FunctionTransformer` to apply a function to a set of
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We can create the same transformation in pandas using `apply`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '|  | feature_1 | feature_2 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 12 | 13 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 12 | 13 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 12 | 13 |'
  prefs: []
  type: TYPE_TB
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is common to want to make some custom transformations to one or more features.
    For example, we might want to create a feature that is the natural log of the
    values of a different feature. We can do this by creating a function and then
    mapping it to features using either scikit-learn’s `FunctionTransformer` or pandas’
    `apply`. In the solution we created a very simple function, `add_ten`, which added
    10 to each input, but there is no reason we could not define a much more complex
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Detecting Outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to identify extreme observations.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Detecting outliers is unfortunately more of an art than a science. However,
    a common method is to assume the data is normally distributed and, based on that
    assumption, “draw” an ellipse around the data, classifying any observation inside
    the ellipse as an inlier (labeled as `1`) and any observation outside the ellipse
    as an outlier (labeled as `-1`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In these arrays, values of -1 refer to outliers whereas values of 1 refer to
    inliers. A major limitation of this approach is the need to specify a `contamination`
    parameter, which is the proportion of observations that are outliers—​a value
    that we don’t know. Think of `contamination` as our estimate of the cleanliness
    of our data. If we expect our data to have few outliers, we can set `contamination`
    to something small. However, if we believe that the data is likely to have outliers,
    we can set it to a higher value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of looking at observations as a whole, we can instead look at individual
    features and identify extreme values in those features using interquartile range
    (IQR):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: IQR is the difference between the first and third quartile of a set of data.
    You can think of IQR as the spread of the bulk of the data, with outliers being
    observations far from the main concentration of data. Outliers are commonly defined
    as any value 1.5 IQRs less than the first quartile, or 1.5 IQRs greater than the
    third quartile.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is no single best technique for detecting outliers. Instead, we have a
    collection of techniques all with their own advantages and disadvantages. Our
    best strategy is often trying multiple techniques (e.g., both `EllipticEnvelope`
    and IQR-based detection) and looking at the results as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: If at all possible, we should look at observations we detect as outliers and
    try to understand them. For example, if we have a dataset of houses and one feature
    is number of rooms, is an outlier with 100 rooms really a house or is it actually
    a hotel that has been misclassified?
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Three Ways to Detect Outliers (and the source of the IQR function used in
    this recipe)](https://oreil.ly/wlwmH)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4.7 Handling Outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have outliers in your data that you want to identify and then reduce their
    impact on the data distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Typically we can use three strategies to handle outliers. First, we can drop
    them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Price | Bathrooms | Square_Feet |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 534433 | 2.0 | 1500 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 392333 | 3.5 | 2500 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 293222 | 2.0 | 1500 |'
  prefs: []
  type: TYPE_TB
- en: 'Second, we can mark them as outliers and include “Outlier” as a feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Price | Bathrooms | Square_Feet | Outlier |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 534433 | 2.0 | 1500 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 392333 | 3.5 | 2500 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 293222 | 2.0 | 1500 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 4322032 | 116.0 | 48000 | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'Finally, we can transform the feature to dampen the effect of the outlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Price | Bathrooms | Square_Feet | Outlier | Log_Of_Square_Feet |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 534433 | 2.0 | 1500 | 0 | 7.313220 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 392333 | 3.5 | 2500 | 0 | 7.824046 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 293222 | 2.0 | 1500 | 0 | 7.313220 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 4322032 | 116.0 | 48000 | 1 | 10.778956 |'
  prefs: []
  type: TYPE_TB
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to detecting outliers, there is no hard-and-fast rule for handling them.
    How we handle them should be based on two aspects. First, we should consider what
    makes them outliers. If we believe they are errors in the data, such as from a
    broken sensor or a miscoded value, then we might drop the observation or replace
    outlier values with `NaN` since we can’t trust those values. However, if we believe
    the outliers are genuine extreme values (e.g., a house [mansion] with 200 bathrooms),
    then marking them as outliers or transforming their values is more appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: Second, how we handle outliers should be based on our goal for machine learning.
    For example, if we want to predict house prices based on features of the house,
    we might reasonably assume the price for mansions with over 100 bathrooms is driven
    by a different dynamic than regular family homes. Furthermore, if we are training
    a model to use as part of an online home loan web application, we might assume
    that our potential users will not include billionaires looking to buy a mansion.
  prefs: []
  type: TYPE_NORMAL
- en: So what should we do if we have outliers? Think about why they are outliers,
    have an end goal in mind for the data, and, most importantly, remember that not
    making a decision to address outliers is itself a decision with implications.
  prefs: []
  type: TYPE_NORMAL
- en: 'One additional point: if you do have outliers, standardization might not be
    appropriate because the mean and variance might be highly influenced by the outliers.
    In this case, use a rescaling method more robust against outliers, like `RobustScaler`.'
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[`RobustScaler` documentation](https://oreil.ly/zgm-1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4.8 Discretizating Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have a numerical feature and want to break it up into discrete bins.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Depending on how we want to break up the data, there are two techniques we
    can use. First, we can binarize the feature according to some threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Second, we can break up numerical features according to multiple thresholds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the arguments for the `bins` parameter denote the left edge of each
    bin. For example, the `20` argument does not include the element with the value
    of 20, only the two values smaller than 20\. We can switch this behavior by setting
    the parameter `right` to `True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Discretization can be a fruitful strategy when we have reason to believe that
    a numerical feature should behave more like a categorical feature. For example,
    we might believe there is very little difference in the spending habits of 19-
    and 20-year-olds, but a significant difference between 20- and 21-year-olds (the
    age in the United States when young adults can consume alcohol). In that example,
    it could be useful to break up individuals in our data into those who can drink
    alcohol and those who cannot. Similarly, in other cases it might be useful to
    discretize our data into three or more bins.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the solution, we saw two methods of discretization—​scikit-learn’s `Binarizer`
    for two bins and NumPy’s `digitize` for three or more bins—​however, we can also
    use `digitize` to binarize features like `Binarizer` by specifying only a single
    threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[`digitize` documentation](https://oreil.ly/KipXX)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4.9 Grouping Observations Using Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to cluster observations so that similar observations are grouped together.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you know that you have *k* groups, you can use k-means clustering to group
    similar observations and output a new feature containing each observation’s group
    membership:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '|  | feature_1 | feature_2 | group |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | –9.877554 | –3.336145 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | –7.287210 | –8.353986 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | –6.943061 | –7.023744 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | –7.440167 | –8.791959 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | –6.641388 | –8.075888 | 2 |'
  prefs: []
  type: TYPE_TB
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are jumping ahead of ourselves a bit and will go into much more depth about
    clustering algorithms later in the book. However, I wanted to point out that we
    can use clustering as a preprocessing step. Specifically, we use unsupervised
    learning algorithms like k-means to cluster observations into groups. The result
    is a categorical feature with similar observations being members of the same group.
  prefs: []
  type: TYPE_NORMAL
- en: 'Don’t worry if you did not understand all of that: just file away the idea
    that clustering can be used in preprocessing. And if you really can’t wait, feel
    free to flip to [Chapter 19](ch19.xhtml#clustering) now.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.10 Deleting Observations with Missing Values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to delete observations containing missing values.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deleting observations with missing values is easy with a clever line of NumPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can drop missing observations using pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '|  | feature_1 | feature_2 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1.1 | 11.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2.2 | 22.2 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 3.3 | 33.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 4.4 | 44.4 |'
  prefs: []
  type: TYPE_TB
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most machine learning algorithms cannot handle any missing values in the target
    and feature arrays. For this reason, we cannot ignore missing values in our data
    and must address the issue during preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest solution is to delete every observation that contains one or more
    missing values, a task quickly and easily accomplished using NumPy or pandas.
  prefs: []
  type: TYPE_NORMAL
- en: That said, we should be very reluctant to delete observations with missing values.
    Deleting them is the nuclear option, since our algorithm loses access to the information
    contained in the observation’s nonmissing values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as important, depending on the cause of the missing values, deleting observations
    can introduce bias into our data. There are three types of missing data:'
  prefs: []
  type: TYPE_NORMAL
- en: Missing completely at random (MCAR)
  prefs: []
  type: TYPE_NORMAL
- en: 'The probability that a value is missing is independent of everything. For example,
    a survey respondent rolls a die before answering a question: if she rolls a six,
    she skips that question.'
  prefs: []
  type: TYPE_NORMAL
- en: Missing at random (MAR)
  prefs: []
  type: TYPE_NORMAL
- en: The probability that a value is missing is not completely random but depends
    on the information captured in other features. For example, a survey asks about
    gender identity and annual salary, and women are more likely to skip the salary
    question; however, their nonresponse depends only on information we have captured
    in our gender identity feature.
  prefs: []
  type: TYPE_NORMAL
- en: Missing not at random (MNAR)
  prefs: []
  type: TYPE_NORMAL
- en: The probability that a value is missing is not random and depends on information
    not captured in our features. For example, a survey asks about annual salary,
    and women are more likely to skip the salary question, and we do not have a gender
    identity feature in our data.
  prefs: []
  type: TYPE_NORMAL
- en: It is sometimes acceptable to delete observations if they are MCAR or MAR. However,
    if the value is MNAR, the fact that a value is missing is itself information.
    Deleting MNAR observations can inject bias into our data because we are removing
    observations produced by some unobserved systematic effect.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Identifying the 3 Types of Missing Data](https://oreil.ly/sz9Fx)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Missing-Data Imputation](https://oreil.ly/swU2j)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4.11 Imputing Missing Values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have missing values in your data and want to impute them via a generic method
    or prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can impute missing values using k-nearest neighbors (KNN) or the scikit-learn
    `SimpleImputer` class. If you have a small amount of data, predict and impute
    the missing values using k-nearest neighbors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can use scikit-learn’s `SimpleImputer` class from the `imputer`
    module to fill in missing values with the feature’s mean, median, or most frequent
    value. However, we will typically get worse results than with KNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are two main strategies for replacing missing data with substitute values,
    each of which has strengths and weaknesses. First, we can use machine learning
    to predict the values of the missing data. To do this we treat the feature with
    missing values as a target vector and use the remaining subset of features to
    predict missing values. While we can use a wide range of machine learning algorithms
    to impute values, a popular choice is KNN. KNN is addressed in depth in [Chapter 15](ch15.xhtml#k-nearest-neighbors),
    but the short explanation is that the algorithm uses the *k* nearest observations
    (according to some distance metric) to predict the missing value. In our solution
    we predicted the missing value using the five closest observations.
  prefs: []
  type: TYPE_NORMAL
- en: The downside to KNN is that in order to know which observations are the closest
    to the missing value, it needs to calculate the distance between the missing value
    and every single observation. This is reasonable in smaller datasets but quickly
    becomes problematic if a dataset has millions of observations. In such cases,
    approximate nearest neighbors (ANN) is a more feasible approach. We will discuss
    ANN in [Recipe 15.5](ch15.xhtml#finding-approximate-nearest-neighbors).
  prefs: []
  type: TYPE_NORMAL
- en: An alternative and more scalable strategy than KNN is to fill in the missing
    values of numerical data with the mean, median, or mode. For example, in our solution
    we used scikit-learn to fill in missing values with a feature’s mean value. The
    imputed value is often not as close to the true value as when we used KNN, but
    we can scale mean-filling to data containing millions of observations more easily.
  prefs: []
  type: TYPE_NORMAL
- en: If we use imputation, it is a good idea to create a binary feature indicating
    whether the observation contains an imputed value.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[scikit-learn documentation: Imputation of Missing Values](https://oreil.ly/1M4bn)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Study of K-Nearest Neighbour as an Imputation Method](https://oreil.ly/012--)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
