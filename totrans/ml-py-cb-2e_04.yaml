- en: Chapter 4\. Handling Numerical Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章 处理数值数据
- en: 4.0 Introduction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4.0 引言
- en: Quantitative data is the measurement of something—​whether class size, monthly
    sales, or student scores. The natural way to represent these quantities is numerically
    (e.g., 29 students, $529,392 in sales). In this chapter, we will cover numerous
    strategies for transforming raw numerical data into features purpose-built for
    machine learning algorithms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 定量数据是某物的测量——无论是班级规模、月销售额还是学生分数。表示这些数量的自然方式是数值化（例如，29名学生、销售额为529,392美元）。在本章中，我们将介绍多种策略，将原始数值数据转换为专门用于机器学习算法的特征。
- en: 4.1 Rescaling a Feature
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4.1 重新调整特征
- en: Problem
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You need to rescale the values of a numerical feature to be between two values.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要将数值特征的值重新缩放到两个值之间。
- en: Solution
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use scikit-learn’s `MinMaxScaler` to rescale a feature array:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用scikit-learn的`MinMaxScaler`来重新调整特征数组：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Discussion
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: '*Rescaling* is a common preprocessing task in machine learning. Many of the
    algorithms described later in this book will assume all features are on the same
    scale, typically 0 to 1 or –1 to 1\. There are a number of rescaling techniques,
    but one of the simplest is called *min-max scaling*. Min-max scaling uses the
    minimum and maximum values of a feature to rescale values to within a range. Specifically,
    min-max calculates:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*重新缩放* 是机器学习中常见的预处理任务。本书后面描述的许多算法将假定所有特征在同一尺度上，通常是0到1或-1到1。有许多重新缩放技术，但最简单的之一称为*最小-最大缩放*。最小-最大缩放使用特征的最小值和最大值将值重新缩放到一个范围内。具体来说，最小-最大缩放计算：'
- en: <math display="block"><mrow><msubsup><mi>x</mi> <mi>i</mi> <mo>'</mo></msubsup>
    <mo>=</mo> <mfrac><mrow><msub><mi>x</mi> <mi>i</mi></msub> <mo>-</mo><mtext>min</mtext><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow>
    <mrow><mtext>max</mtext><mo>(</mo><mi>x</mi><mo>)</mo><mo>-</mo><mtext>min</mtext><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></mrow></math>
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msubsup><mi>x</mi> <mi>i</mi> <mo>'</mo></msubsup>
    <mo>=</mo> <mfrac><mrow><msub><mi>x</mi> <mi>i</mi></msub> <mo>-</mo><mtext>min</mtext><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow>
    <mrow><mtext>max</mtext><mo>(</mo><mi>x</mi><mo>)</mo><mo>-</mo><mtext>min</mtext><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></mrow></math>
- en: 'where <math display="inline"><mi>x</mi></math> is the feature vector, <math
    display="inline"><msub><mi>x</mi><mi>i</mi></msub></math> is an individual element
    of feature <math display="inline"><mi>x</mi></math>, and <math display="inline"><msubsup><mi>x</mi>
    <mi>i</mi> <mo>''</mo></msubsup></math> is the rescaled element. In our example,
    we can see from the outputted array that the feature has been successfully rescaled
    to between 0 and 1:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math display="inline"><mi>x</mi></math>是特征向量，<math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math>是特征<math
    display="inline"><mi>x</mi></math>的单个元素，<math display="inline"><msubsup><mi>x</mi>
    <mi>i</mi> <mo>'</mo></msubsup></math>是重新调整的元素。在我们的例子中，我们可以从输出的数组中看到，特征已成功重新调整为0到1之间：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: scikit-learn’s `MinMaxScaler` offers two options to rescale a feature. One option
    is to use `fit` to calculate the minimum and maximum values of the feature, and
    then use `transform` to rescale the feature. The second option is to use `fit_transform`
    to do both operations at once. There is no mathematical difference between the
    two options, but there is sometimes a practical benefit to keeping the operations
    separate because it allows us to apply the same transformation to different *sets*
    of the data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn的`MinMaxScaler`提供了两种重新调整特征的选项。一种选项是使用`fit`来计算特征的最小值和最大值，然后使用`transform`来重新调整特征。第二个选项是使用`fit_transform`来同时执行这两个操作。这两个选项在数学上没有区别，但有时将操作分开会有实际的好处，因为这样可以将相同的转换应用于不同的*数据集*。
- en: See Also
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Feature scaling, Wikipedia](https://oreil.ly/f2WiM)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[特征缩放，维基百科](https://oreil.ly/f2WiM)'
- en: '[About Feature Scaling and Normalization, Sebastian Raschka](https://oreil.ly/Da0AH)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关于特征缩放和归一化，Sebastian Raschka](https://oreil.ly/Da0AH)'
- en: 4.2 Standardizing a Feature
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4.2 标准化特征
- en: Problem
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to transform a feature to have a mean of 0 and a standard deviation
    of 1.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望将一个特征转换为具有均值为0和标准差为1。
- en: Solution
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'scikit-learn’s `StandardScaler` performs both transformations:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn的`StandardScaler`执行这两个转换：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Discussion
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'A common alternative to the min-max scaling discussed in [Recipe 4.1](#rescaling-a-feature)
    is rescaling of features to be approximately standard normally distributed. To
    achieve this, we use standardization to transform the data such that it has a
    mean, <math display="inline"><mover accent="true"><mi>x</mi> <mo>¯</mo></mover></math>,
    of 0 and a standard deviation, <math display="inline"><mi>σ</mi></math>, of 1\.
    Specifically, each element in the feature is transformed so that:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于[问题 4.1](#rescaling-a-feature)中讨论的最小-最大缩放的常见替代方案是将特征重新缩放为近似标准正态分布。为了实现这一目标，我们使用标准化来转换数据，使其均值<math
    display="inline"><mover accent="true"><mi>x</mi> <mo>¯</mo></mover></math>为0，标准差<math
    display="inline"><mi>σ</mi></math>为1。具体来说，特征中的每个元素都被转换，以便：
- en: <math display="block"><mrow><msubsup><mi>x</mi> <mi>i</mi> <mo>'</mo></msubsup>
    <mo>=</mo> <mfrac><mrow><msub><mi>x</mi> <mi>i</mi></msub> <mo>-</mo><mover accent="true"><mi>x</mi>
    <mo>¯</mo></mover></mrow> <mi>σ</mi></mfrac></mrow></math>
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msubsup><mi>x</mi> <mi>i</mi> <mo>'</mo></msubsup>
    <mo>=</mo> <mfrac><mrow><msub><mi>x</mi> <mi>i</mi></msub> <mo>-</mo><mover accent="true"><mi>x</mi>
    <mo>¯</mo></mover></mrow> <mi>σ</mi></mfrac></mrow></math>
- en: where <math display="inline"><msubsup><mi>x</mi> <mi>i</mi> <mo>'</mo></msubsup></math>
    is our standardized form of <math display="inline"><msub><mi>x</mi> <mi>i</mi></msub></math>
    . The transformed feature represents the number of standard deviations of the
    original value from the feature’s mean value (also called a *z-score* in statistics).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math display="inline"><msubsup><mi>x</mi> <mi>i</mi> <mo>'</mo></msubsup></math>
    是 <math display="inline"><msub><mi>x</mi> <mi>i</mi></msub></math> 的标准化形式。转换后的特征表示原始值与特征均值之间的标准偏差数（在统计学中也称为
    *z-score*）。
- en: Standardization is a common go-to scaling method for machine learning preprocessing
    and, in my experience, is used more often than min-max scaling. However, it depends
    on the learning algorithm. For example, principal component analysis often works
    better using standardization, while min-max scaling is often recommended for neural
    networks (both algorithms are discussed later in this book). As a general rule,
    I’d recommend defaulting to standardization unless you have a specific reason
    to use an alternative.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化是机器学习预处理中常见的缩放方法，在我的经验中，它比最小-最大缩放更常用。但这取决于学习算法。例如，主成分分析通常在使用标准化时效果更好，而对于神经网络，则通常建议使用最小-最大缩放（这两种算法稍后在本书中讨论）。作为一个一般规则，我建议除非有特定原因使用其他方法，否则默认使用标准化。
- en: 'We can see the effect of standardization by looking at the mean and standard
    deviation of our solution’s output:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查看解决方案输出的平均值和标准偏差来看到标准化的效果：
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If our data has significant outliers, it can negatively impact our standardization
    by affecting the feature’s mean and variance. In this scenario, it is often helpful
    to instead rescale the feature using the median and quartile range. In scikit-learn,
    we do this using the `RobustScaler` method:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的数据存在显著的异常值，它可能通过影响特征的均值和方差而对我们的标准化产生负面影响。在这种情况下，通常可以通过使用中位数和四分位距来重新调整特征，从而提供帮助。在
    scikit-learn 中，我们使用 `RobustScaler` 方法来实现这一点：
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 4.3 Normalizing Observations
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4.3 规范化观测值
- en: Problem
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to rescale the feature values of observations to have unit norm (a
    total length of 1).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望将观测值的特征值重新调整为单位范数（总长度为1）。
- en: Solution
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use `Normalizer` with a `norm` argument:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用带有 `norm` 参数的 `Normalizer`：
- en: '[PRE9]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Discussion
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Many rescaling methods (e.g., min-max scaling and standardization) operate on
    features; however, we can also rescale across individual observations. `Normalizer`
    rescales the values on individual observations to have unit norm (the sum of their
    lengths is 1). This type of rescaling is often used when we have many equivalent
    features (e.g., text classification when every word or *n*-word group is a feature).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 许多重新调整方法（例如，最小-最大缩放和标准化）作用于特征，但我们也可以跨个体观测值进行重新调整。`Normalizer` 将单个观测值上的值重新调整为单位范数（它们长度的总和为1）。当存在许多等效特征时（例如，在文本分类中，每个单词或
    *n*-word 组合都是一个特征时），通常会使用这种重新调整。
- en: '`Normalizer` provides three norm options with Euclidean norm (often called
    L2) being the default argument:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`Normalizer` 提供三种范数选项，其中欧几里德范数（通常称为L2）是默认参数：'
- en: <math display="block"><mrow><msub><mfenced close="∥" open="∥"><mi>x</mi></mfenced>
    <mn>2</mn></msub> <mo>=</mo> <msqrt><mrow><msup><msub><mi>x</mi> <mn>1</mn></msub>
    <mn>2</mn></msup> <mo>+</mo> <msup><msub><mi>x</mi> <mn>2</mn></msub> <mn>2</mn></msup>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msup><msub><mi>x</mi> <mi>n</mi></msub> <mn>2</mn></msup></mrow></msqrt></mrow></math>
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mfenced close="∥" open="∥"><mi>x</mi></mfenced>
    <mn>2</mn></msub> <mo>=</mo> <msqrt><mrow><msup><msub><mi>x</mi> <mn>1</mn></msub>
    <mn>2</mn></msup> <mo>+</mo> <msup><msub><mi>x</mi> <mn>2</mn></msub> <mn>2</mn></msup>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msup><msub><mi>x</mi> <mi>n</mi></msub> <mn>2</mn></msup></mrow></msqrt></mrow></math>
- en: where <math display="inline"><mi>x</mi></math> is an individual observation
    and <math display="inline"><msub><mi>x</mi><mi>n</mi></msub></math> is that observation’s
    value for the <math display="inline"><mi>n</mi></math>th feature.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math display="inline"><mi>x</mi></math> 是一个单独的观测值，<math display="inline"><msub><mi>x</mi><mi>n</mi></msub></math>
    是该观测值在第 <math display="inline"><mi>n</mi></math> 个特征上的值。
- en: '[PRE11]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Alternatively, we can specify Manhattan norm (L1):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以指定曼哈顿范数（L1）：
- en: <math display="block"><mrow><msub><mfenced close="∥" open="∥"><mi>x</mi></mfenced>
    <mn>1</mn></msub> <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <mfenced close="|" open="|" separators=""><msub><mi>x</mi>
    <mi>i</mi></msub></mfenced> <mo>.</mo></mrow></math>
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mfenced close="∥" open="∥"><mi>x</mi></mfenced>
    <mn>1</mn></msub> <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <mfenced close="|" open="|" separators=""><msub><mi>x</mi>
    <mi>i</mi></msub></mfenced> <mo>.</mo></mrow></math>
- en: '[PRE13]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Intuitively, L2 norm can be thought of as the distance between two points in
    New York for a bird (i.e., a straight line), while L1 can be thought of as the
    distance for a human walking on the street (walk north one block, east one block,
    north one block, east one block, etc.), which is why it is called “Manhattan norm”
    or “Taxicab norm.”
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，L2 范数可以被视为鸟在纽约两点之间的距离（即直线距离），而L1 范数可以被视为在街道上行走的人的距离（向北走一块，向东走一块，向北走一块，向东走一块，等等），这就是为什么它被称为“曼哈顿范数”或“出租车范数”的原因。
- en: 'Practically, notice that `norm="l1"` rescales an observation’s values so they
    sum to 1, which can sometimes be a desirable quality:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，注意到 `norm="l1"` 将重新调整观测值的值，使其总和为1，这在某些情况下是一种可取的质量：
- en: '[PRE15]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 4.4 Generating Polynomial and Interaction Features
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4.4 生成多项式和交互特征
- en: Problem
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to create polynomial and interaction features.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望创建多项式和交互特征。
- en: Solution
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Even though some choose to create polynomial and interaction features manually,
    scikit-learn offers a built-in method:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有些人选择手动创建多项式和交互特征，scikit-learn 提供了一个内置方法：
- en: '[PRE17]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `degree` parameter determines the maximum degree of the polynomial. For
    example, `degree=2` will create new features raised to the second power:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 参数`degree`确定多项式的最大次数。例如，`degree=2`将创建被提升到二次幂的新特征：
- en: <math display="block"><mrow><msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>,</mo> <msup><msub><mi>x</mi> <mn>1</mn></msub> <mn>2</mn></msup>
    <mo>,</mo> <msup><msub><mi>x</mi> <mn>1</mn></msub> <mn>2</mn></msup> <mo>,</mo>
    <msup><msub><mi>x</mi> <mn>2</mn></msub> <mn>2</mn></msup></mrow></math>
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>,</mo> <msup><msub><mi>x</mi> <mn>1</mn></msub> <mn>2</mn></msup>
    <mo>,</mo> <msup><msub><mi>x</mi> <mn>1</mn></msub> <mn>2</mn></msup> <mo>,</mo>
    <msup><msub><mi>x</mi> <mn>2</mn></msub> <mn>2</mn></msup></mrow></math>
- en: 'while `degree=3` will create new features raised to the second and third power:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 而`degree=3`将创建被提升到二次和三次幂的新特征：
- en: <math display="block"><mrow><msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>,</mo> <msup><msub><mi>x</mi> <mn>1</mn></msub> <mn>2</mn></msup>
    <mo>,</mo> <msup><msub><mi>x</mi> <mn>2</mn></msub> <mn>2</mn></msup> <mo>,</mo>
    <msup><msub><mi>x</mi> <mn>1</mn></msub> <mn>3</mn></msup> <mo>,</mo> <msup><msub><mi>x</mi>
    <mn>2</mn></msub> <mn>3</mn></msup> <mo>,</mo> <msup><msub><mi>x</mi> <mn>1</mn></msub>
    <mn>2</mn></msup> <mo>,</mo> <msup><msub><mi>x</mi> <mn>1</mn></msub> <mn>3</mn></msup>
    <mo>,</mo> <msup><msub><mi>x</mi> <mn>2</mn></msub> <mn>3</mn></msup></mrow></math>
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>,</mo> <msup><msub><mi>x</mi> <mn>1</mn></msub> <mn>2</mn></msup>
    <mo>,</mo> <msup><msub><mi>x</mi> <mn>2</mn></msub> <mn>2</mn></msup> <mo>,</mo>
    <msup><msub><mi>x</mi> <mn>1</mn></msub> <mn>3</mn></msup> <mo>,</mo> <msup><msub><mi>x</mi>
    <mn>2</mn></msub> <mn>3</mn></msup> <mo>,</mo> <msup><msub><mi>x</mi> <mn>1</mn></msub>
    <mn>2</mn></msup> <mo>,</mo> <msup><msub><mi>x</mi> <mn>1</mn></msub> <mn>3</mn></msup>
    <mo>,</mo> <msup><msub><mi>x</mi> <mn>2</mn></msub> <mn>3</mn></msup></mrow></math>
- en: 'Furthermore, by default `PolynomialFeatures` includes interaction features:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，默认情况下，`PolynomialFeatures`包括交互特征：
- en: <math display="block"><mrow><msub><mi>x</mi> <mn>1</mn></msub> <msub><mi>x</mi>
    <mn>2</mn></msub></mrow></math>
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>x</mi> <mn>1</mn></msub> <msub><mi>x</mi>
    <mn>2</mn></msub></mrow></math>
- en: 'We can restrict the features created to only interaction features by setting
    `interaction_only` to `True`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将`interaction_only`设置为`True`来限制仅创建交互特征：
- en: '[PRE19]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Discussion
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Polynomial features are often created when we want to include the notion that
    there exists a nonlinear relationship between the features and the target. For
    example, we might suspect that the effect of age on the probability of having
    a major medical condition is not constant over time but increases as age increases.
    We can encode that nonconstant effect in a feature, <math display="inline"><mi>x</mi></math>,
    by generating that feature’s higher-order forms (<math display="inline"><msup><mi>x</mi><mn>2</mn></msup></math>,
    <math display="inline"><msup><mi>x</mi><mn>3</mn></msup></math>, etc.).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们希望包括特征与目标之间存在非线性关系时，通常会创建多项式特征。例如，我们可能怀疑年龄对患重大医疗状况的概率的影响并非随时间恒定，而是随年龄增加而增加。我们可以通过生成该特征的高阶形式（<math
    display="inline"><msup><mi>x</mi><mn>2</mn></msup></math>，<math display="inline"><msup><mi>x</mi><mn>3</mn></msup></math>等）来编码这种非恒定效果。
- en: 'Additionally, often we run into situations where the effect of one feature
    is dependent on another feature. A simple example would be if we were trying to
    predict whether or not our coffee was sweet, and we had two features: (1) whether
    or not the coffee was stirred, and (2) whether or not we added sugar. Individually,
    each feature does not predict coffee sweetness, but the combination of their effects
    does. That is, a coffee would only be sweet if the coffee had sugar and was stirred.
    The effects of each feature on the target (sweetness) are dependent on each other.
    We can encode that relationship by including an interaction feature that is the
    product of the individual features.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们经常遇到一种情况，即一个特征的效果取决于另一个特征。一个简单的例子是，如果我们试图预测我们的咖啡是否甜，我们有两个特征：(1)咖啡是否被搅拌，以及(2)是否添加了糖。单独来看，每个特征都不能预测咖啡的甜度，但它们的效果组合起来却可以。也就是说，只有当咖啡既加了糖又被搅拌时，咖啡才会变甜。每个特征对目标（甜度）的影响取决于彼此之间的关系。我们可以通过包含一个交互特征，即两个个体特征的乘积来编码这种关系。
- en: 4.5 Transforming Features
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4.5 特征转换
- en: Problem
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to make a custom transformation to one or more features.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望对一个或多个特征进行自定义转换。
- en: Solution
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'In scikit-learn, use `FunctionTransformer` to apply a function to a set of
    features:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，使用`FunctionTransformer`将一个函数应用到一组特征上：
- en: '[PRE21]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can create the same transformation in pandas using `apply`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`apply`在pandas中创建相同的转换：
- en: '[PRE23]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '|  | feature_1 | feature_2 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | feature_1 | feature_2 |'
- en: '| --- | --- | --- |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0 | 12 | 13 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 12 | 13 |'
- en: '| 1 | 12 | 13 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 12 | 13 |'
- en: '| 2 | 12 | 13 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 12 | 13 |'
- en: Discussion
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: It is common to want to make some custom transformations to one or more features.
    For example, we might want to create a feature that is the natural log of the
    values of a different feature. We can do this by creating a function and then
    mapping it to features using either scikit-learn’s `FunctionTransformer` or pandas’
    `apply`. In the solution we created a very simple function, `add_ten`, which added
    10 to each input, but there is no reason we could not define a much more complex
    function.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 通常希望对一个或多个特征进行一些自定义转换。例如，我们可能想创建一个特征，其值是另一个特征的自然对数。我们可以通过创建一个函数，然后使用scikit-learn的`FunctionTransformer`或pandas的`apply`将其映射到特征来实现这一点。在解决方案中，我们创建了一个非常简单的函数`add_ten`，它为每个输入加了10，但我们完全可以定义一个复杂得多的函数。
- en: 4.6 Detecting Outliers
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4.6 检测异常值
- en: Problem
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to identify extreme observations.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望识别极端观察结果。
- en: Solution
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Detecting outliers is unfortunately more of an art than a science. However,
    a common method is to assume the data is normally distributed and, based on that
    assumption, “draw” an ellipse around the data, classifying any observation inside
    the ellipse as an inlier (labeled as `1`) and any observation outside the ellipse
    as an outlier (labeled as `-1`):'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 检测异常值很遗憾更像是一种艺术而不是一种科学。然而，一种常见的方法是假设数据呈正态分布，并基于该假设在数据周围“画”一个椭圆，将椭圆内的任何观察结果归类为内围值（标记为`1`），将椭圆外的任何观察结果归类为异常值（标记为`-1`）：
- en: '[PRE24]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In these arrays, values of -1 refer to outliers whereas values of 1 refer to
    inliers. A major limitation of this approach is the need to specify a `contamination`
    parameter, which is the proportion of observations that are outliers—​a value
    that we don’t know. Think of `contamination` as our estimate of the cleanliness
    of our data. If we expect our data to have few outliers, we can set `contamination`
    to something small. However, if we believe that the data is likely to have outliers,
    we can set it to a higher value.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些数组中，值为-1表示异常值，而值为1表示内围值。这种方法的一个主要局限性是需要指定一个`contamination`参数，它是异常值观察值的比例，这是我们不知道的值。将`contamination`视为我们对数据清洁程度的估计。如果我们预计数据中有很少的异常值，我们可以将`contamination`设置为较小的值。但是，如果我们认为数据可能有异常值，我们可以将其设置为较高的值。
- en: 'Instead of looking at observations as a whole, we can instead look at individual
    features and identify extreme values in those features using interquartile range
    (IQR):'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以不将观察结果作为一个整体来看待，而是可以查看单个特征，并使用四分位距（IQR）来识别这些特征中的极端值：
- en: '[PRE26]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: IQR is the difference between the first and third quartile of a set of data.
    You can think of IQR as the spread of the bulk of the data, with outliers being
    observations far from the main concentration of data. Outliers are commonly defined
    as any value 1.5 IQRs less than the first quartile, or 1.5 IQRs greater than the
    third quartile.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: IQR是一组数据的第一和第三四分位数之间的差异。您可以将IQR视为数据的主要集中区域的扩展，而异常值是远离数据主要集中区域的观察结果。异常值通常定义为第一四分位数的1.5倍IQR小于或第三四分位数的1.5倍IQR大于的任何值。
- en: Discussion
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: There is no single best technique for detecting outliers. Instead, we have a
    collection of techniques all with their own advantages and disadvantages. Our
    best strategy is often trying multiple techniques (e.g., both `EllipticEnvelope`
    and IQR-based detection) and looking at the results as a whole.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 没有单一的最佳技术来检测异常值。相反，我们有一系列技术，各有优缺点。我们最好的策略通常是尝试多种技术（例如，`EllipticEnvelope`和基于IQR的检测）并综合查看结果。
- en: If at all possible, we should look at observations we detect as outliers and
    try to understand them. For example, if we have a dataset of houses and one feature
    is number of rooms, is an outlier with 100 rooms really a house or is it actually
    a hotel that has been misclassified?
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可能的话，我们应该查看我们检测到的异常值，并尝试理解它们。例如，如果我们有一个房屋数据集，其中一个特征是房间数量，那么房间数量为100的异常值是否真的是一座房子，还是实际上是一个被错误分类的酒店？
- en: See Also
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Three Ways to Detect Outliers (and the source of the IQR function used in
    this recipe)](https://oreil.ly/wlwmH)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[检测异常值的三种方法（以及此配方中使用的IQR函数的来源）](https://oreil.ly/wlwmH)'
- en: 4.7 Handling Outliers
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4.7 处理异常值
- en: Problem
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You have outliers in your data that you want to identify and then reduce their
    impact on the data distribution.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 您的数据中存在异常值，您希望识别并减少其对数据分布的影响。
- en: Solution
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Typically we can use three strategies to handle outliers. First, we can drop
    them:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 通常我们可以采用三种策略来处理异常值。首先，我们可以放弃它们：
- en: '[PRE28]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '|  | Price | Bathrooms | Square_Feet |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | 价格 | 浴室 | 平方英尺 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0 | 534433 | 2.0 | 1500 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 534433 | 2.0 | 1500 |'
- en: '| 1 | 392333 | 3.5 | 2500 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 392333 | 3.5 | 2500 |'
- en: '| 2 | 293222 | 2.0 | 1500 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 293222 | 2.0 | 1500 |'
- en: 'Second, we can mark them as outliers and include “Outlier” as a feature:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们可以将它们标记为异常值，并将“异常值”作为特征包含在内：
- en: '[PRE29]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '|  | Price | Bathrooms | Square_Feet | Outlier |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | 价格 | 浴室 | 平方英尺 | 异常值 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 534433 | 2.0 | 1500 | 0 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 534433 | 2.0 | 1500 | 0 |'
- en: '| 1 | 392333 | 3.5 | 2500 | 0 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 392333 | 3.5 | 2500 | 0 |'
- en: '| 2 | 293222 | 2.0 | 1500 | 0 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 293222 | 2.0 | 1500 | 0 |'
- en: '| 3 | 4322032 | 116.0 | 48000 | 1 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 4322032 | 116.0 | 48000 | 1 |'
- en: 'Finally, we can transform the feature to dampen the effect of the outlier:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以转换特征以减轻异常值的影响：
- en: '[PRE30]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '|  | Price | Bathrooms | Square_Feet | Outlier | Log_Of_Square_Feet |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | 价格 | 浴室 | 平方英尺 | 异常值 | 平方英尺的对数 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 0 | 534433 | 2.0 | 1500 | 0 | 7.313220 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 534433 | 2.0 | 1500 | 0 | 7.313220 |'
- en: '| 1 | 392333 | 3.5 | 2500 | 0 | 7.824046 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 392333 | 3.5 | 2500 | 0 | 7.824046 |'
- en: '| 2 | 293222 | 2.0 | 1500 | 0 | 7.313220 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 293222 | 2.0 | 1500 | 0 | 7.313220 |'
- en: '| 3 | 4322032 | 116.0 | 48000 | 1 | 10.778956 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 4322032 | 116.0 | 48000 | 1 | 10.778956 |'
- en: Discussion
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Similar to detecting outliers, there is no hard-and-fast rule for handling them.
    How we handle them should be based on two aspects. First, we should consider what
    makes them outliers. If we believe they are errors in the data, such as from a
    broken sensor or a miscoded value, then we might drop the observation or replace
    outlier values with `NaN` since we can’t trust those values. However, if we believe
    the outliers are genuine extreme values (e.g., a house [mansion] with 200 bathrooms),
    then marking them as outliers or transforming their values is more appropriate.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于检测异常值，处理它们没有硬性规则。我们处理它们应该基于两个方面。首先，我们应该考虑它们为何成为异常值。如果我们认为它们是数据中的错误，比如来自损坏传感器或错误编码的值，那么我们可能会删除该观测值或将异常值替换为`NaN`，因为我们不能信任这些值。然而，如果我们认为异常值是真实的极端值（例如，一个有200个浴室的豪宅），那么将它们标记为异常值或转换它们的值更为合适。
- en: Second, how we handle outliers should be based on our goal for machine learning.
    For example, if we want to predict house prices based on features of the house,
    we might reasonably assume the price for mansions with over 100 bathrooms is driven
    by a different dynamic than regular family homes. Furthermore, if we are training
    a model to use as part of an online home loan web application, we might assume
    that our potential users will not include billionaires looking to buy a mansion.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们处理异常值的方式应该基于我们在机器学习中的目标。例如，如果我们想根据房屋特征预测房价，我们可能合理地假设拥有超过100个浴室的豪宅的价格受到不同动态的驱动，而不是普通家庭住宅。此外，如果我们正在训练一个在线住房贷款网站应用程序的模型，我们可能会假设我们的潜在用户不包括寻求购买豪宅的亿万富翁。
- en: So what should we do if we have outliers? Think about why they are outliers,
    have an end goal in mind for the data, and, most importantly, remember that not
    making a decision to address outliers is itself a decision with implications.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 那么如果我们有异常值应该怎么办？考虑它们为何成为异常值，设定数据的最终目标，最重要的是记住，不处理异常值本身也是一种带有影响的决策。
- en: 'One additional point: if you do have outliers, standardization might not be
    appropriate because the mean and variance might be highly influenced by the outliers.
    In this case, use a rescaling method more robust against outliers, like `RobustScaler`.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 另外一点：如果存在异常值，标准化可能不合适，因为异常值可能会严重影响均值和方差。在这种情况下，应该使用对异常值更具鲁棒性的重新缩放方法，比如`RobustScaler`。
- en: See Also
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[`RobustScaler` documentation](https://oreil.ly/zgm-1)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`RobustScaler` 文档](https://oreil.ly/zgm-1)'
- en: 4.8 Discretizating Features
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4.8 特征离散化
- en: Problem
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You have a numerical feature and want to break it up into discrete bins.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 您有一个数值特征，并希望将其分割成离散的箱子。
- en: Solution
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Depending on how we want to break up the data, there are two techniques we
    can use. First, we can binarize the feature according to some threshold:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 根据数据分割方式的不同，我们可以使用两种技术。首先，我们可以根据某个阈值对特征进行二值化：
- en: '[PRE31]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Second, we can break up numerical features according to multiple thresholds:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们可以根据多个阈值分割数值特征：
- en: '[PRE33]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Note that the arguments for the `bins` parameter denote the left edge of each
    bin. For example, the `20` argument does not include the element with the value
    of 20, only the two values smaller than 20\. We can switch this behavior by setting
    the parameter `right` to `True`:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`bins` 参数的参数表示每个箱的左边缘。例如，`20` 参数不包括值为20的元素，只包括比20小的两个值。我们可以通过将参数 `right`
    设置为 `True` 来切换这种行为：
- en: '[PRE35]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Discussion
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Discretization can be a fruitful strategy when we have reason to believe that
    a numerical feature should behave more like a categorical feature. For example,
    we might believe there is very little difference in the spending habits of 19-
    and 20-year-olds, but a significant difference between 20- and 21-year-olds (the
    age in the United States when young adults can consume alcohol). In that example,
    it could be useful to break up individuals in our data into those who can drink
    alcohol and those who cannot. Similarly, in other cases it might be useful to
    discretize our data into three or more bins.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有理由认为数值特征应该表现得更像分类特征时，离散化可以是一种有效的策略。例如，我们可能认为19岁和20岁的人的消费习惯几乎没有什么差异，但20岁和21岁之间存在显著差异（美国的法定饮酒年龄）。在这种情况下，将数据中的个体分为可以饮酒和不能饮酒的人可能是有用的。同样，在其他情况下，将数据离散化为三个或更多的箱子可能是有用的。
- en: 'In the solution, we saw two methods of discretization—​scikit-learn’s `Binarizer`
    for two bins and NumPy’s `digitize` for three or more bins—​however, we can also
    use `digitize` to binarize features like `Binarizer` by specifying only a single
    threshold:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决方案中，我们看到了两种离散化的方法——scikit-learn的`Binarizer`用于两个区间和NumPy的`digitize`用于三个或更多的区间——然而，我们也可以像使用`Binarizer`那样使用`digitize`来对功能进行二值化，只需指定一个阈值：
- en: '[PRE37]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: See Also
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '[`digitize` documentation](https://oreil.ly/KipXX)'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`digitize` documentation](https://oreil.ly/KipXX)'
- en: 4.9 Grouping Observations Using Clustering
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4.9 使用聚类对观测进行分组
- en: Problem
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to cluster observations so that similar observations are grouped together.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望将观测聚类，以便将相似的观测分组在一起。
- en: Solution
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'If you know that you have *k* groups, you can use k-means clustering to group
    similar observations and output a new feature containing each observation’s group
    membership:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您知道您有*k*个组，您可以使用k均值聚类来将相似的观测分组，并输出一个新的特征，其中包含每个观测的组成员资格：
- en: '[PRE39]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '|  | feature_1 | feature_2 | group |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  | 功能_1 | 功能_2 | 组 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0 | –9.877554 | –3.336145 | 0 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 0 | –9.877554 | –3.336145 | 0 |'
- en: '| 1 | –7.287210 | –8.353986 | 2 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 1 | –7.287210 | –8.353986 | 2 |'
- en: '| 2 | –6.943061 | –7.023744 | 2 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 2 | –6.943061 | –7.023744 | 2 |'
- en: '| 3 | –7.440167 | –8.791959 | 2 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 3 | –7.440167 | –8.791959 | 2 |'
- en: '| 4 | –6.641388 | –8.075888 | 2 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 4 | –6.641388 | –8.075888 | 2 |'
- en: Discussion
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: We are jumping ahead of ourselves a bit and will go into much more depth about
    clustering algorithms later in the book. However, I wanted to point out that we
    can use clustering as a preprocessing step. Specifically, we use unsupervised
    learning algorithms like k-means to cluster observations into groups. The result
    is a categorical feature with similar observations being members of the same group.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍微超前一点，并且将在本书的后面更深入地讨论聚类算法。但是，我想指出，我们可以将聚类用作预处理步骤。具体来说，我们使用无监督学习算法（如k均值）将观测分成组。结果是一个分类特征，具有相似观测的成员属于同一组。
- en: 'Don’t worry if you did not understand all of that: just file away the idea
    that clustering can be used in preprocessing. And if you really can’t wait, feel
    free to flip to [Chapter 19](ch19.xhtml#clustering) now.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有理解所有这些，不要担心：只需将聚类可用于预处理的想法存档。如果您真的等不及，现在就可以翻到[第19章](ch19.xhtml#clustering)。
- en: 4.10 Deleting Observations with Missing Values
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4.10 删除具有缺失值的观测
- en: Problem
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You need to delete observations containing missing values.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要删除包含缺失值的观测。
- en: Solution
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Deleting observations with missing values is easy with a clever line of NumPy:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 NumPy 的巧妙一行代码轻松删除具有缺失值的观测：
- en: '[PRE40]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Alternatively, we can drop missing observations using pandas:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以使用 pandas 删除缺失的观测：
- en: '[PRE42]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '|  | feature_1 | feature_2 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|  | 功能_1 | 功能_2 |'
- en: '| --- | --- | --- |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0 | 1.1 | 11.1 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1.1 | 11.1 |'
- en: '| 1 | 2.2 | 22.2 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2.2 | 22.2 |'
- en: '| 2 | 3.3 | 33.3 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 3.3 | 33.3 |'
- en: '| 3 | 4.4 | 44.4 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 4.4 | 44.4 |'
- en: Discussion
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Most machine learning algorithms cannot handle any missing values in the target
    and feature arrays. For this reason, we cannot ignore missing values in our data
    and must address the issue during preprocessing.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习算法无法处理目标和特征数组中的任何缺失值。因此，我们不能忽略数据中的缺失值，必须在预处理过程中解决这个问题。
- en: The simplest solution is to delete every observation that contains one or more
    missing values, a task quickly and easily accomplished using NumPy or pandas.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的解决方案是删除包含一个或多个缺失值的每个观测，可以使用 NumPy 或 pandas 快速轻松地完成此任务。
- en: That said, we should be very reluctant to delete observations with missing values.
    Deleting them is the nuclear option, since our algorithm loses access to the information
    contained in the observation’s nonmissing values.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们应该非常不情愿地删除具有缺失值的观测。删除它们是核心选项，因为我们的算法失去了观测的非缺失值中包含的信息。
- en: 'Just as important, depending on the cause of the missing values, deleting observations
    can introduce bias into our data. There are three types of missing data:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 同样重要的是，根据缺失值的原因，删除观测可能会向我们的数据引入偏差。有三种类型的缺失数据：
- en: Missing completely at random (MCAR)
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 完全随机缺失（MCAR）
- en: 'The probability that a value is missing is independent of everything. For example,
    a survey respondent rolls a die before answering a question: if she rolls a six,
    she skips that question.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失值出现的概率与一切无关。例如，调查对象在回答问题之前掷骰子：如果她掷出六点，她会跳过那个问题。
- en: Missing at random (MAR)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 随机缺失（MAR）
- en: The probability that a value is missing is not completely random but depends
    on the information captured in other features. For example, a survey asks about
    gender identity and annual salary, and women are more likely to skip the salary
    question; however, their nonresponse depends only on information we have captured
    in our gender identity feature.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 值缺失的概率并非完全随机，而是依赖于其他特征捕获的信息。例如，一项调查询问性别身份和年薪，女性更有可能跳过薪水问题；然而，她们的未响应仅依赖于我们在性别身份特征中捕获的信息。
- en: Missing not at random (MNAR)
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失非随机（MNAR）
- en: The probability that a value is missing is not random and depends on information
    not captured in our features. For example, a survey asks about annual salary,
    and women are more likely to skip the salary question, and we do not have a gender
    identity feature in our data.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 值缺失的概率并非随机，而是依赖于我们特征未捕获的信息。例如，一项调查询问年薪，女性更有可能跳过薪水问题，而我们的数据中没有性别身份特征。
- en: It is sometimes acceptable to delete observations if they are MCAR or MAR. However,
    if the value is MNAR, the fact that a value is missing is itself information.
    Deleting MNAR observations can inject bias into our data because we are removing
    observations produced by some unobserved systematic effect.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据是MCAR或MAR，有时可以接受删除观测值。但是，如果值是MNAR，缺失本身就是信息。删除MNAR观测值可能会在数据中引入偏差，因为我们正在删除由某些未观察到的系统效应产生的观测值。
- en: See Also
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '[Identifying the 3 Types of Missing Data](https://oreil.ly/sz9Fx)'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[识别三种缺失数据类型](https://oreil.ly/sz9Fx)'
- en: '[Missing-Data Imputation](https://oreil.ly/swU2j)'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[缺失数据填补](https://oreil.ly/swU2j)'
- en: 4.11 Imputing Missing Values
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4.11 填补缺失值
- en: Problem
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You have missing values in your data and want to impute them via a generic method
    or prediction.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 您的数据中存在缺失值，并希望通过通用方法或预测来填补它们。
- en: Solution
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'You can impute missing values using k-nearest neighbors (KNN) or the scikit-learn
    `SimpleImputer` class. If you have a small amount of data, predict and impute
    the missing values using k-nearest neighbors:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用k最近邻（KNN）或scikit-learn的`SimpleImputer`类来填补缺失值。如果数据量较小，请使用KNN进行预测和填补缺失值：
- en: '[PRE43]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Alternatively, we can use scikit-learn’s `SimpleImputer` class from the `imputer`
    module to fill in missing values with the feature’s mean, median, or most frequent
    value. However, we will typically get worse results than with KNN:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以使用scikit-learn的`imputer`模块中的`SimpleImputer`类，将缺失值用特征的均值、中位数或最频繁的值填充。然而，通常情况下，与KNN相比，我们通常会获得更差的结果：
- en: '[PRE45]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Discussion
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: There are two main strategies for replacing missing data with substitute values,
    each of which has strengths and weaknesses. First, we can use machine learning
    to predict the values of the missing data. To do this we treat the feature with
    missing values as a target vector and use the remaining subset of features to
    predict missing values. While we can use a wide range of machine learning algorithms
    to impute values, a popular choice is KNN. KNN is addressed in depth in [Chapter 15](ch15.xhtml#k-nearest-neighbors),
    but the short explanation is that the algorithm uses the *k* nearest observations
    (according to some distance metric) to predict the missing value. In our solution
    we predicted the missing value using the five closest observations.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 替换缺失数据的两种主要策略都有各自的优势和劣势。首先，我们可以使用机器学习来预测缺失数据的值。为此，我们将带有缺失值的特征视为目标向量，并使用其余子集特征来预测缺失值。虽然我们可以使用各种机器学习算法来填补值，但一个流行的选择是KNN。在[第15章](ch15.xhtml#k-nearest-neighbors)深入讨论了KNN，简而言之，该算法使用*k*个最近的观测值（根据某个距离度量）来预测缺失值。在我们的解决方案中，我们使用了五个最接近的观测值来预测缺失值。
- en: The downside to KNN is that in order to know which observations are the closest
    to the missing value, it needs to calculate the distance between the missing value
    and every single observation. This is reasonable in smaller datasets but quickly
    becomes problematic if a dataset has millions of observations. In such cases,
    approximate nearest neighbors (ANN) is a more feasible approach. We will discuss
    ANN in [Recipe 15.5](ch15.xhtml#finding-approximate-nearest-neighbors).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: KNN的缺点在于为了知道哪些观测值最接近缺失值，需要计算缺失值与每个观测值之间的距离。在较小的数据集中这是合理的，但是如果数据集有数百万个观测值，则很快会变得问题重重。在这种情况下，近似最近邻（ANN）是一个更可行的方法。我们将在[第15.5节](ch15.xhtml#finding-approximate-nearest-neighbors)讨论ANN。
- en: An alternative and more scalable strategy than KNN is to fill in the missing
    values of numerical data with the mean, median, or mode. For example, in our solution
    we used scikit-learn to fill in missing values with a feature’s mean value. The
    imputed value is often not as close to the true value as when we used KNN, but
    we can scale mean-filling to data containing millions of observations more easily.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 与 KNN 相比，一种可替代且更可扩展的策略是用平均值、中位数或众数填补数值数据的缺失值。例如，在我们的解决方案中，我们使用 scikit-learn
    将缺失值填充为特征的均值。填充的值通常不如我们使用 KNN 时接近真实值，但我们可以更轻松地将均值填充应用到包含数百万观察值的数据中。
- en: If we use imputation, it is a good idea to create a binary feature indicating
    whether the observation contains an imputed value.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用填充，创建一个二进制特征指示观察是否包含填充值是一个好主意。
- en: See Also
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '[scikit-learn documentation: Imputation of Missing Values](https://oreil.ly/1M4bn)'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[scikit-learn 文档：缺失值的填充](https://oreil.ly/1M4bn)'
- en: '[A Study of K-Nearest Neighbour as an Imputation Method](https://oreil.ly/012--)'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[K-最近邻居作为填充方法的研究](https://oreil.ly/012--)'
