["```py\n# Load libraries\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn import datasets\n\n# Load the data\ndigits = datasets.load_digits()\n\n# Standardize the feature matrix\nfeatures = StandardScaler().fit_transform(digits.data)\n\n# Create a PCA that will retain 99% of variance\npca = PCA(n_components=0.99, whiten=True)\n\n# Conduct PCA\nfeatures_pca = pca.fit_transform(features)\n\n# Show results\nprint(\"Original number of features:\", features.shape[1])\nprint(\"Reduced number of features:\", features_pca.shape[1])\n```", "```py\nOriginal number of features: 64\nReduced number of features: 54\n```", "```py\n# Load libraries\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.datasets import make_circles\n\n# Create linearly inseparable data\nfeatures, _ = make_circles(n_samples=1000, random_state=1, noise=0.1, factor=0.1)\n\n# Apply kernel PCA with radius basis function (RBF) kernel\nkpca = KernelPCA(kernel=\"rbf\", gamma=15, n_components=1)\nfeatures_kpca = kpca.fit_transform(features)\n\nprint(\"Original number of features:\", features.shape[1])\nprint(\"Reduced number of features:\", features_kpca.shape[1])\n```", "```py\nOriginal number of features: 2\nReduced number of features: 1\n```", "```py\n# Load libraries\nfrom sklearn import datasets\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n# Load Iris flower dataset:\niris = datasets.load_iris()\nfeatures = iris.data\ntarget = iris.target\n\n# Create and run an LDA, then use it to transform the features\nlda = LinearDiscriminantAnalysis(n_components=1)\nfeatures_lda = lda.fit(features, target).transform(features)\n\n# Print the number of features\nprint(\"Original number of features:\", features.shape[1])\nprint(\"Reduced number of features:\", features_lda.shape[1])\n```", "```py\nOriginal number of features: 4\nReduced number of features: 1\n```", "```py\nlda.explained_variance_ratio_\n```", "```py\narray([0.9912126])\n```", "```py\nlda.explained_variance_ratio_\n```", "```py\narray([0.9912126])\n```", "```py\n# Create and run LDA\nlda = LinearDiscriminantAnalysis(n_components=None)\nfeatures_lda = lda.fit(features, target)\n\n# Create array of explained variance ratios\nlda_var_ratios = lda.explained_variance_ratio_\n\n# Create function\ndef select_n_components(var_ratio, goal_var: float) -> int:\n    # Set initial variance explained so far\n    total_variance = 0.0\n\n    # Set initial number of features\n    n_components = 0\n\n    # For the explained variance of each feature:\n    for explained_variance in var_ratio:\n\n        # Add the explained variance to the total\n        total_variance += explained_variance\n\n        # Add one to the number of components\n        n_components += 1\n\n        # If we reach our goal level of explained variance\n        if total_variance >= goal_var:\n            # End the loop\n            break\n\n    # Return the number of components\n    return n_components\n\n# Run function\nselect_n_components(lda_var_ratios, 0.95)\n```", "```py\n1\n```", "```py\n# Load libraries\nfrom sklearn.decomposition import NMF\nfrom sklearn import datasets\n\n# Load the data\ndigits = datasets.load_digits()\n\n# Load feature matrix\nfeatures = digits.data\n\n# Create, fit, and apply NMF\nnmf = NMF(n_components=10, random_state=4)\nfeatures_nmf = nmf.fit_transform(features)\n\n# Show results\nprint(\"Original number of features:\", features.shape[1])\nprint(\"Reduced number of features:\", features_nmf.shape[1])\n```", "```py\nOriginal number of features: 64\nReduced number of features: 10\n```", "```py\n# Load libraries\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import TruncatedSVD\nfrom scipy.sparse import csr_matrix\nfrom sklearn import datasets\nimport numpy as np\n\n# Load the data\ndigits = datasets.load_digits()\n\n# Standardize feature matrix\nfeatures = StandardScaler().fit_transform(digits.data)\n\n# Make sparse matrix\nfeatures_sparse = csr_matrix(features)\n\n# Create a TSVD\ntsvd = TruncatedSVD(n_components=10)\n\n# Conduct TSVD on sparse matrix\nfeatures_sparse_tsvd = tsvd.fit(features_sparse).transform(features_sparse)\n\n# Show results\nprint(\"Original number of features:\", features_sparse.shape[1])\nprint(\"Reduced number of features:\", features_sparse_tsvd.shape[1])\n```", "```py\nOriginal number of features: 64\nReduced number of features: 10\n```", "```py\n# Sum of first three components' explained variance ratios\ntsvd.explained_variance_ratio_[0:3].sum()\n```", "```py\n0.3003938537287226\n```", "```py\n# Create and run a TSVD with one less than number of features\ntsvd = TruncatedSVD(n_components=features_sparse.shape[1]-1)\nfeatures_tsvd = tsvd.fit(features)\n\n# List of explained variances\ntsvd_var_ratios = tsvd.explained_variance_ratio_\n\n# Create a function\ndef select_n_components(var_ratio, goal_var):\n    # Set initial variance explained so far\n    total_variance = 0.0\n\n    # Set initial number of features\n    n_components = 0\n\n    # For the explained variance of each feature:\n    for explained_variance in var_ratio:\n\n        # Add the explained variance to the total\n        total_variance += explained_variance\n\n        # Add one to the number of components\n        n_components += 1\n\n        # If we reach our goal level of explained variance\n        if total_variance >= goal_var:\n            # End the loop\n            break\n\n    # Return the number of components\n    return n_components\n\n# Run function\nselect_n_components(tsvd_var_ratios, 0.95)\n```", "```py\n40\n```"]