<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 18. Naive Bayes" data-type="chapter" epub:type="chapter"><div class="chapter" id="naive-bayes">
<h1><span class="label">Chapter 18. </span>Naive Bayes</h1>
<section data-pdf-bookmark="18.0 Introduction" data-type="sect1"><div class="sect1" id="id367">
<h1>18.0 Introduction</h1>
<p><em>Bayes’ theorem</em> is the <a data-primary="Bayes’ theorem" data-type="indexterm" id="id1855"/><a data-primary="classification and classifiers" data-secondary="naive Bayes classifier" data-type="indexterm" id="ix_class_naive_bayes"/><a data-primary="naive Bayes classifier" data-type="indexterm" id="ix_naive_bayes_class"/>premier method for understanding the probability
of some event, <math display="inline"><mi>P</mi>
<mrow>
<mo>(</mo>
<mi>A</mi>
<mo>∣</mo>
<mi>B</mi>
<mo>)</mo>
</mrow></math>, given some new information,
<math display="inline"><mi>P</mi>
<mrow>
<mo>(</mo>
<mi>B</mi>
<mo>∣</mo>
<mi>A</mi>
<mo>)</mo>
</mrow></math>, and a prior belief in the probability of the
event, <math display="inline"><mi>P</mi>
<mrow>
<mo>(</mo>
<mi>A</mi>
<mo>)</mo>
</mrow></math>:</p>
<div data-type="equation">
<math display="block">
<mrow>
<mi>P</mi>
<mrow>
<mo>(</mo>
<mi>A</mi>
<mo>∣</mo>
<mi>B</mi>
<mo>)</mo>
</mrow>
<mo>=</mo>
<mfrac><mrow><mi>P</mi><mo>(</mo><mi>B</mi><mo>∣</mo><mi>A</mi><mo>)</mo><mspace width="0.166667em"/><mi>P</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow> <mrow><mi>P</mi><mo>(</mo><mi>B</mi><mo>)</mo></mrow></mfrac>
</mrow>
</math>
</div>
<p>The Bayesian method’s popularity has skyrocketed in the last decade, increasingly rivaling traditional frequentist applications in academia,
government, and business. In machine learning, one application of Bayes’
theorem to classification comes in the form of the <em>naive Bayes
classifier</em>. Naive Bayes classifiers combine a number of desirable
qualities in practical machine learning into a single classifier. These
include:</p>
<ul>
<li>
<p>An intuitive approach</p>
</li>
<li>
<p>The ability to work with small data</p>
</li>
<li>
<p>Low computation costs for training and prediction</p>
</li>
<li>
<p>Often solid results in a variety of settings</p>
</li>
</ul>
<p>Specifically, a naive Bayes classifier is based on:</p>
<div data-type="equation">
<math display="block">
<mrow>
<mi>P</mi>
<mrow>
<mo>(</mo>
<mi>y</mi>
<mo>∣</mo>
<msub><mi>x</mi> <mn>1</mn> </msub>
<mo>,</mo>
<mo>…</mo>
<mo>,</mo>
<msub><mi>x</mi> <mi>j</mi> </msub>
<mo>)</mo>
</mrow>
<mo>=</mo>
<mfrac><mrow><mi>P</mi><mrow><mo>(</mo><msub><mi>x</mi> <mn>1</mn> </msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>x</mi> <mi>j</mi> </msub><mo>∣</mo><mi>y</mi><mo>)</mo></mrow><mi>P</mi><mrow><mo>(</mo><mi>y</mi><mo>)</mo></mrow></mrow> <mrow><mi>P</mi><mo>(</mo><msub><mi>x</mi> <mn>1</mn> </msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>x</mi> <mi>j</mi> </msub><mo>)</mo></mrow></mfrac>
</mrow>
</math>
</div>
<p class="less_space pagebreak-before">where:</p>
<ul>
<li>
<p><math display="inline"><mi>P</mi>
<mrow>
<mo>(</mo>
<mi>y</mi>
<mo>∣</mo>
<msub><mi>x</mi> <mn>1</mn> </msub>
<mo>,</mo>
<mo>…</mo>
<mo>,</mo>
<msub><mi>x</mi> <mi>j</mi> </msub>
<mo>)</mo>
</mrow></math> is <a data-primary="posterior, naive Bayes" data-type="indexterm" id="id1856"/>called the <em>posterior</em> and is the probability that an observation is class <math display="inline"><mi>y</mi></math> given the observation’s values for the <math display="inline"><mi>j</mi></math> features,
<math display="inline">
<msub><mi>x</mi> <mn>1</mn> </msub>
<mo>,</mo>
<mo>…</mo>
<mo>,</mo>
<msub><mi>x</mi> <mi>j</mi> </msub>
</math>.</p>
</li>
<li>
<p><math display="inline"><mi>P</mi><mrow><mo>(</mo><msub><mi>x</mi> <mn>1</mn> </msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>x</mi> <mi>j</mi> </msub><mo>∣</mo><mi>y</mi><mo>)</mo></mrow></math> is <a data-primary="likelihood, naive Bayes" data-type="indexterm" id="id1857"/>called <em>likelihood</em> and is the likelihood of an observation’s values for features <math display="inline">
<msub><mi>x</mi> <mn>1</mn> </msub>
<mo>,</mo>
<mo>…</mo>
<mo>,</mo>
<msub><mi>x</mi> <mi>j</mi> </msub>
</math> given their class, <math display="inline"><mi>y</mi></math>.</p>
</li>
<li>
<p><math display="inline"><mi>P</mi><mrow><mo>(</mo><mi>y</mi><mo>)</mo></mrow></math> is <a data-primary="prior, naive Bayes" data-type="indexterm" id="id1858"/>called the <em>prior</em> and is our belief for the probability of class <math display="inline"><mi>y</mi></math> before looking at the data.</p>
</li>
<li>
<p><math display="inline"><mi>P</mi><mo>(</mo><msub><mi>x</mi> <mn>1</mn> </msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>x</mi> <mi>j</mi> </msub><mo>)</mo></math>  is <a data-primary="marginal probability, naive Bayes" data-type="indexterm" id="id1859"/>called the <em>marginal probability</em>.</p>
</li>
</ul>
<p>In naive Bayes, we compare an observation’s posterior values for each
possible class. Specifically, because the marginal probability is
constant across these comparisons, we compare the numerators of the
posterior for each class. For each observation, the class with the
greatest posterior numerator becomes the predicted class,
<math display="inline"><mover accent="true"><mi>y</mi><mo>^</mo></mover></math>.</p>
<p>There are two important things to note about naive Bayes classifiers.
First, for each feature in the data, we have to assume the statistical
distribution of the likelihood, <math display="inline"><mrow><mi>P</mi><mo>(</mo><msub><mi>x</mi> <mi>j</mi> </msub><mo>∣</mo><mi>y</mi><mo>)</mo></mrow></math>. The common distributions are the normal (Gaussian), multinomial, and Bernoulli distributions. The distribution chosen is often determined by the nature of features (continuous, binary, etc.). Second, naive Bayes gets its name because we assume that each feature, and its resulting
likelihood, is independent. This “naive” assumption is frequently wrong
yet in practice does little to prevent building high-quality
classifiers.</p>
<p>In this chapter we will cover using scikit-learn to train three types of
naive Bayes classifiers using three different likelihood distributions. Afterwards, we will learn to calibrate the predictions from naive Bayes models to make them interpretable.</p>
</div></section>
<section data-pdf-bookmark="18.1 Training a Classifier for Continuous Features" data-type="sect1"><div class="sect1" id="training-a-classifier-for-continuous-features">
<h1>18.1 Training a Classifier for Continuous Features</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id368">
<h2>Problem</h2>
<p>You have only <a data-primary="naive Bayes classifier" data-secondary="continuous features" data-type="indexterm" id="ix_nb_classif_cont"/><a data-primary="continuous features, training naive Bayes classifier for" data-type="indexterm" id="ix_contin_feat_bayes"/>continuous features and you want to train a naive Bayes
classifier.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id369">
<h2>Solution</h2>
<p>Use a <a data-primary="GaussianNB" data-type="indexterm" id="ix_gauss_nb"/>Gaussian naive Bayes classifier in scikit-learn:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.naive_bayes</code> <code class="kn">import</code> <code class="n">GaussianNB</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create Gaussian naive Bayes object</code>
<code class="n">classifer</code> <code class="o">=</code> <code class="n">GaussianNB</code><code class="p">()</code>

<code class="c1"># Train model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">classifer</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id370">
<h2>Discussion</h2>
<p>The most common type of naive Bayes classifier is the <em>Gaussian naive
Bayes</em>. In Gaussian naive Bayes, we assume that the likelihood of the
feature values <math display="inline"><mi>x</mi></math>, given an observation is of class
<math display="inline"> <mi>y</mi></math>, follows a normal distribution:</p>
<div data-type="equation">
<math display="block">
<mrow>
<mi>p</mi>
<mrow>
<mo>(</mo>
<msub><mi>x</mi> <mi>j</mi> </msub>
<mo>∣</mo>
<mi>y</mi>
<mo>)</mo>
</mrow>
<mo>=</mo>
<mfrac><mn>1</mn> <msqrt><mrow><mn>2</mn><mi>π</mi><msup><msub><mi>σ</mi> <mi>y</mi></msub> <mn>2</mn> </msup></mrow></msqrt></mfrac>
<mspace width="0.166667em"/>
<msup><mi>e</mi> <mrow><mo>-</mo><mfrac><msup><mrow><mo>(</mo><msub><mi>x</mi> <mi>j</mi> </msub><mo>-</mo><msub><mi>μ</mi> <mi>y</mi> </msub><mo>)</mo></mrow> <mn>2</mn> </msup> <mrow><mn>2</mn><msup><msub><mi>σ</mi> <mi>y</mi></msub> <mn>2</mn> </msup></mrow></mfrac></mrow> </msup>
</mrow>
</math>
</div>
<p>where <math display="inline"><msup><msub><mi>σ</mi> <mi>y</mi></msub> <mn>2</mn> </msup></math> and <math display="inline"><msub><mi>μ</mi> <mi>y</mi></msub></math> are the variance and mean values of feature <math display="inline"><msub><mi>x</mi> <mi>j</mi></msub></math> for class <math display="inline"><mi>y</mi></math>. Because of the assumption of the normal distribution, Gaussian naive Bayes is best used in cases where all our features are continuous.</p>
<p>In scikit-learn, we train a Gaussian naive Bayes like any other model
using <code>fit</code>, and in turn can then make predictions about the class of an
observation:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create new observation</code>
<code class="n">new_observation</code> <code class="o">=</code> <code class="p">[[</code> <code class="mi">4</code><code class="p">,</code>  <code class="mi">4</code><code class="p">,</code>  <code class="mi">4</code><code class="p">,</code>  <code class="mf">0.4</code><code class="p">]]</code>

<code class="c1"># Predict class</code>
<code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">new_observation</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([1])</pre>
<p>One of the interesting aspects of naive Bayes classifiers is that they
allow us to assign a prior belief over the respected target classes.
We can do this using the <code>GaussianNB priors</code> parameter, which takes in
a list of the probabilities assigned to each class of the target vector:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create Gaussian naive Bayes object with prior probabilities of each class</code>
<code class="n">clf</code> <code class="o">=</code> <code class="n">GaussianNB</code><code class="p">(</code><code class="n">priors</code><code class="o">=</code><code class="p">[</code><code class="mf">0.25</code><code class="p">,</code> <code class="mf">0.25</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">])</code>

<code class="c1"># Train model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">classifer</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
<p>If we do not add any argument to the <code>priors</code> parameter, the prior is
adjusted based on the data.</p>
<p class="less_space pagebreak-before">Finally, note that the raw predicted probabilities from Gaussian naive
Bayes (outputted using <code>predict_proba</code>) are not calibrated. That is,
they should not be believed. If we want to create useful predicted
probabilities, we will need to calibrate them using an isotonic
regression or a related method.<a data-primary="" data-startref="ix_contin_feat_bayes" data-type="indexterm" id="id1860"/><a data-primary="" data-startref="ix_gauss_nb" data-type="indexterm" id="id1861"/><a data-primary="" data-startref="ix_nb_classif_cont" data-type="indexterm" id="id1862"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1863">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/9yqSw">How the Naive Bayes Classifier Works in Machine Learning</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="18.2 Training a Classifier for Discrete and Count Features" data-type="sect1"><div class="sect1" id="training-a-classifier-for-discrete-and-count-features">
<h1>18.2 Training a Classifier for Discrete and Count Features</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id371">
<h2>Problem</h2>
<p>Given <a data-primary="naive Bayes classifier" data-secondary="discrete and count features" data-type="indexterm" id="ix_nb_classif_disc"/><a data-primary="discrete and count features, naive Bayes classifier" data-type="indexterm" id="ix_disc_count_nb"/>discrete or count data, you need to train a naive Bayes
classifier.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id372">
<h2>Solution</h2>
<p>Use a <a data-primary="MultinomialNB" data-type="indexterm" id="ix_multinomial_nb"/>multinomial naive Bayes classifier:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn.naive_bayes</code> <code class="kn">import</code> <code class="n">MultinomialNB</code>
<code class="kn">from</code> <code class="nn">sklearn.feature_extraction.text</code> <code class="kn">import</code> <code class="n">CountVectorizer</code>

<code class="c1"># Create text</code>
<code class="n">text_data</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="s1">'I love Brazil. Brazil!'</code><code class="p">,</code>
                      <code class="s1">'Brazil is best'</code><code class="p">,</code>
                      <code class="s1">'Germany beats both'</code><code class="p">])</code>

<code class="c1"># Create bag of words</code>
<code class="n">count</code> <code class="o">=</code> <code class="n">CountVectorizer</code><code class="p">()</code>
<code class="n">bag_of_words</code> <code class="o">=</code> <code class="n">count</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">text_data</code><code class="p">)</code>

<code class="c1"># Create feature matrix</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">bag_of_words</code><code class="o">.</code><code class="n">toarray</code><code class="p">()</code>

<code class="c1"># Create target vector</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="mi">0</code><code class="p">,</code><code class="mi">0</code><code class="p">,</code><code class="mi">1</code><code class="p">])</code>

<code class="c1"># Create multinomial naive Bayes object with prior probabilities of each class</code>
<code class="n">classifer</code> <code class="o">=</code> <code class="n">MultinomialNB</code><code class="p">(</code><code class="n">class_prior</code><code class="o">=</code><code class="p">[</code><code class="mf">0.25</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">])</code>

<code class="c1"># Train model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">classifer</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id373">
<h2>Discussion</h2>
<p><em>Multinomial naive Bayes</em> works similarly to Gaussian naive Bayes, but
the features are assumed to be multinomially distributed. In practice,
this means that this classifier is commonly used when we have discrete
data (e.g., movie ratings ranging from 1 to 5). One of the most common uses of multinomial naive Bayes is text classification using bags of words or <math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math> approaches (see Recipes <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch06.xhtml#encoding-text-as-a-bag-of-words">6.9</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch06.xhtml#weighting-word-importance">6.10</a>).</p>
<p>In our solution, we created a toy text dataset of three observations and converted the text strings into a bag-of-words feature matrix and an
accompanying target vector. We then used <code>MultinomialNB</code> to train a
model while defining the prior probabilities for the two classes
(pro-<code>brazil</code> and pro-<code>germany</code>).</p>
<p><code>MultinomialNB</code> works similarly to <code>GaussianNB</code>; models are trained using
<code>fit</code>, and observations can be predicted using <code>predict</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create new observation</code>
<code class="n">new_observation</code> <code class="o">=</code> <code class="p">[[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">]]</code>

<code class="c1"># Predict new observation's class</code>
<code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">new_observation</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([0])</pre>
<p>If <code>class_prior</code> is not specified, prior probabilities are learned using
the data. However, if we want a uniform distribution to be used as
the prior, we can set <code>fit_prior=False</code>.</p>
<p>Finally, <code>MultinomialNB</code> contains an additive smoothing hyperparameter,
<code>alpha</code>, that should be tuned. The default value is <code>1.0</code>, with <code>0.0</code>
meaning no smoothing takes place.<a data-primary="" data-startref="ix_disc_count_nb" data-type="indexterm" id="id1864"/><a data-primary="" data-startref="ix_multinomial_nb" data-type="indexterm" id="id1865"/><a data-primary="" data-startref="ix_nb_classif_disc" data-type="indexterm" id="id1866"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="18.3 Training a Naive Bayes Classifier for Binary Features" data-type="sect1"><div class="sect1" id="training-a-naive-bayes-classifier-for-binary-features">
<h1>18.3 Training a Naive Bayes Classifier for Binary Features</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id547">
<h2>Problem</h2>
<p>You have <a data-primary="binary feature data" data-secondary="naive Bayes classifier training with" data-type="indexterm" id="id1867"/><a data-primary="naive Bayes classifier" data-secondary="binary features" data-type="indexterm" id="id1868"/>binary feature data and need to train a naive Bayes classifier.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id374">
<h2>Solution</h2>
<p>Use a <a data-primary="BernouilliNB" data-type="indexterm" id="id1869"/>Bernoulli naive Bayes classifier:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn.naive_bayes</code> <code class="kn">import</code> <code class="n">BernoulliNB</code>

<code class="c1"># Create three binary features</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">randint</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="mi">3</code><code class="p">))</code>

<code class="c1"># Create a binary target vector</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">randint</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code><code class="o">.</code><code class="n">ravel</code><code class="p">()</code>

<code class="c1"># Create Bernoulli naive Bayes object with prior probabilities of each class</code>
<code class="n">classifer</code> <code class="o">=</code> <code class="n">BernoulliNB</code><code class="p">(</code><code class="n">class_prior</code><code class="o">=</code><code class="p">[</code><code class="mf">0.25</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">])</code>

<code class="c1"># Train model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">classifer</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id375">
<h2>Discussion</h2>
<p>The <em>Bernoulli naive Bayes</em> classifier assumes that all our features are
binary, such that they take only two values (e.g., a nominal categorical
feature that has been one-hot encoded). Like its multinomial cousin,
Bernoulli naive Bayes is often used in text classification, when our
feature matrix is simply the presence or absence of a word in a
document. <a data-primary="MultinomialNB" data-type="indexterm" id="id1870"/>Furthermore, like <code>MultinomialNB</code>, <code>BernoulliNB</code> has an
additive smoothing hyperparameter, <code>alpha</code>, we will want to tune using
model selection techniques. Finally, if we want to use priors, we can use
the <code>class_prior</code> parameter with a list containing the prior
probabilities for each class. If we want to specify a uniform prior, we
can set <code>fit_prior=False</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">model_uniform_prior</code> <code class="o">=</code> <code class="n">BernoulliNB</code><code class="p">(</code><code class="n">class_prior</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code> <code class="n">fit_prior</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code></pre>
</div></section>
</div></section>
<section data-pdf-bookmark="18.4 Calibrating Predicted Probabilities" data-type="sect1"><div class="sect1" id="calibrating-predicted-probabilities">
<h1>18.4 Calibrating Predicted Probabilities</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id376">
<h2>Problem</h2>
<p>You want to <a data-primary="predictions and predicting" data-secondary="naive Bayes predicted probabilities" data-type="indexterm" id="ix_pred_nb_prob"/><a data-primary="naive Bayes classifier" data-secondary="calibrating predicted probabilities" data-type="indexterm" id="ix_nb_calib_pred_poss"/><a data-primary="CalibratedClassifierCV" data-type="indexterm" id="ix_calib_class_cv"/><a data-primary="calibrating predicted probabilities" data-type="indexterm" id="ix_calib_pred_poss"/><a data-primary="cross-validation (CV) of ML models" data-secondary="CalibratedClassifierCV" data-type="indexterm" id="ix_cv_calib_pred_poss"/>calibrate the predicted probabilities from naive Bayes
classifiers so they are interpretable.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id1871">
<h2>Solution</h2>
<p>Use <code>CalibratedClassifierCV</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.naive_bayes</code> <code class="kn">import</code> <code class="n">GaussianNB</code>
<code class="kn">from</code> <code class="nn">sklearn.calibration</code> <code class="kn">import</code> <code class="n">CalibratedClassifierCV</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create Gaussian naive Bayes object</code>
<code class="n">classifer</code> <code class="o">=</code> <code class="n">GaussianNB</code><code class="p">()</code>

<code class="c1"># Create calibrated cross-validation with sigmoid calibration</code>
<code class="n">classifer_sigmoid</code> <code class="o">=</code> <code class="n">CalibratedClassifierCV</code><code class="p">(</code><code class="n">classifer</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">method</code><code class="o">=</code><code class="s1">'sigmoid'</code><code class="p">)</code>

<code class="c1"># Calibrate probabilities</code>
<code class="n">classifer_sigmoid</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>

<code class="c1"># Create new observation</code>
<code class="n">new_observation</code> <code class="o">=</code> <code class="p">[[</code> <code class="mf">2.6</code><code class="p">,</code>  <code class="mf">2.6</code><code class="p">,</code>  <code class="mf">2.6</code><code class="p">,</code>  <code class="mf">0.4</code><code class="p">]]</code>

<code class="c1"># View calibrated probabilities</code>
<code class="n">classifer_sigmoid</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">new_observation</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([[0.31859969, 0.63663466, 0.04476565]])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id377">
<h2>Discussion</h2>
<p>Class <a data-primary="predict_proba method" data-type="indexterm" id="id1872"/>probabilities are a common and useful part of machine learning
models. In scikit-learn, most learning algorithms allow us to see the
predicted probabilities of class membership using <code>predict_proba</code>. This
can be extremely useful if, for instance, we want to predict a
certain class only if the model predicts the probability that the
class is over 90%. However, some models, including naive Bayes
classifiers, output probabilities that are not based on the real world.
That is, <code>predict_proba</code> might predict an observation has a 0.70 chance
of being a certain class, when the reality is that it is 0.10 or 0.99.
Specifically in naive Bayes, while the ranking of predicted
probabilities for the different target classes is valid, the raw
predicted probabilities tend to take on extreme values close to 0 and 1.</p>
<p>To obtain meaningful predicted probabilities we need conduct what is
called <em>calibration</em>. In scikit-learn we can use the
<code>CalibratedClassifierCV</code> class to create well-calibrated predicted
probabilities using k-fold cross-validation. In <code>CalibratedClassifierCV</code>,
the training sets are used to train the model, and the test set is used
to calibrate the predicted probabilities. The returned predicted
probabilities are the average of the k-folds.</p>
<p>Using our solution we can see the difference between raw and well-calibrated predicted probabilities. In our solution, we created a
Gaussian naive Bayes classifier. If we train that classifier and then
predict the class probabilities for a new observation, we can see very
extreme probability estimates:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Train a Gaussian naive Bayes then predict class probabilities</code>
<code class="n">classifer</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">new_observation</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([[2.31548432e-04, 9.99768128e-01, 3.23532277e-07]])</pre>
<p>However if, after we calibrate the predicted probabilities (which we did in our solution), we get very different results:<a data-primary="" data-startref="ix_class_naive_bayes" data-type="indexterm" id="id1873"/><a data-primary="" data-startref="ix_naive_bayes_class" data-type="indexterm" id="id1874"/><a data-primary="" data-startref="ix_calib_class_cv" data-type="indexterm" id="id1875"/><a data-primary="" data-startref="ix_calib_pred_poss" data-type="indexterm" id="id1876"/><a data-primary="" data-startref="ix_cv_calib_pred_poss" data-type="indexterm" id="id1877"/><a data-primary="" data-startref="ix_nb_calib_pred_poss" data-type="indexterm" id="id1878"/><a data-primary="" data-startref="ix_pred_nb_prob" data-type="indexterm" id="id1879"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View calibrated probabilities</code>
<code class="n">array</code><code class="p">([[</code><code class="mf">0.31859969</code><code class="p">,</code> <code class="mf">0.63663466</code><code class="p">,</code> <code class="mf">0.04476565</code><code class="p">]])</code></pre>
<pre data-type="programlisting">array([[ 0.31859969,  0.63663466,  0.04476565]])</pre>
<p><code>CalibratedClassifierCV</code> offers two <a data-primary="isotonic regression" data-type="indexterm" id="id1880"/><a data-primary="Platt’s sigmoid model" data-type="indexterm" id="id1881"/>calibration methods—​Platt’s
sigmoid model and isotonic regression—​defined by the <code>method</code>
parameter. While we don’t have the space to go into the specifics,
because isotonic regression is nonparametric it tends to overfit when
sample sizes are very small (e.g., 100 observations). In our solution we
used the Iris dataset with 150 observations and therefore used the
Platt’s sigmoid model.</p>
</div></section>
</div></section>
</div></section></div></body></html>