<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 9. Dimensionality Reduction Using Feature Extraction" data-type="chapter" epub:type="chapter"><div class="chapter" id="dimensionality-reduction-using-feature-extraction">
<h1><span class="label">Chapter 9. </span>Dimensionality Reduction <span class="keep-together">Using Feature Extraction</span></h1>
<section data-pdf-bookmark="9.0 Introduction" data-type="sect1"><div class="sect1" id="id230">
<h1>9.0 Introduction</h1>
<p>It is <a data-primary="feature extraction" data-type="indexterm" id="ix_feat_ext_ch9"/>common to have access to thousands and even hundreds of thousands
of features. For example, in <a data-type="xref" href="ch08.xhtml#handling-images">Chapter 8</a> we transformed a 256	× 256–pixel color image into 196,608 features. Furthermore, because each of these pixels can take one of 256 possible values, our observation can take 256<sup>196608</sup> different configurations. Many machine learning algorithms have trouble learning from such data, because it will never be practical to collect enough observations for the algorithms to operate correctly. Even in more tabular, structured datasets we can easily end up with thousands of features after the feature engineering process.</p>
<p>Fortunately, not all features are created equal, and the goal of <em>feature
extraction</em> for <a data-primary="dimensionality reduction" data-seealso="feature extraction; feature selection" data-type="indexterm" id="id1459"/>dimensionality reduction is to transform our set of
features, <em>p<sub>original</sub></em>, such that we end up with a new set, <em>p<sub>new</sub></em>, where <em>p<sub>original</sub> &gt; p<sub>new</sub></em>,
while still keeping much of the underlying information. Put another
way, we reduce the number of features with only a small loss in our
data’s ability to generate high-quality predictions. In this chapter, we
will cover a number of feature extraction techniques to do just this.</p>
<p>One downside of the feature extraction techniques we discuss is that the
new features we generate will not be interpretable by humans. They will
contain as much or nearly as much ability to train our models but will
appear to the human eye as a collection of random numbers. If we
wanted to maintain our ability to interpret our models, <a data-primary="feature selection" data-type="indexterm" id="id1460"/>dimensionality
reduction through <em>feature selection</em> is a better option (and will be discussed in <a data-type="xref" href="ch10.xhtml#dimensionality-reduction-using-feature-selection">Chapter 10</a>). During feature selection we remove features we deem unimportant but keep other features as they currently are. Although this may not let us keep information from all features as feature extraction does, it leaves the features we don’t drop intact—and therefore fully interpretable by humans during analysis.</p>
</div></section>
<section data-pdf-bookmark="9.1 Reducing Features Using Principal Components" data-type="sect1"><div class="sect1" id="reducing-features-using-principal-components">
<h1>9.1 Reducing Features Using Principal Components</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id231">
<h2>Problem</h2>
<p>Given a set of <a data-primary="feature extraction" data-secondary="principal component approach" data-type="indexterm" id="ix_feat_ext_pc_app"/>features, you want to reduce the number of features while
retaining the <a data-primary="variance in data, managing" data-seealso="feature extraction; feature selection" data-type="indexterm" id="id1461"/>variance (important information) in the data.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id232">
<h2>Solution</h2>
<p>Use <a data-primary="PCA (principal component analysis)" data-type="indexterm" id="ix_pca_prin_comp_anal"/><a data-primary="principal component analysis (PCA)" data-type="indexterm" id="ix_prin_comp_anal_pca"/><a data-primary="dimensionality reduction" data-secondary="principal component analysis" data-type="indexterm" id="ix_dim_reduct_pca"/>principal component analysis with scikit’s <code>PCA</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>
<code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">PCA</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>

<code class="c1"># Load the data</code>
<code class="n">digits</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_digits</code><code class="p">()</code>

<code class="c1"># Standardize the feature matrix</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">digits</code><code class="o">.</code><code class="n">data</code><code class="p">)</code>

<code class="c1"># Create a PCA that will retain 99% of variance</code>
<code class="n">pca</code> <code class="o">=</code> <code class="n">PCA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mf">0.99</code><code class="p">,</code> <code class="n">whiten</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>

<code class="c1"># Conduct PCA</code>
<code class="n">features_pca</code> <code class="o">=</code> <code class="n">pca</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Show results</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Original number of features:"</code><code class="p">,</code> <code class="n">features</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Reduced number of features:"</code><code class="p">,</code> <code class="n">features_pca</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code></pre>
<pre data-type="programlisting">Original number of features: 64
Reduced number of features: 54</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id233">
<h2>Discussion</h2>
<p><em>Principal component analysis</em> (PCA) is a popular linear dimensionality
reduction technique. PCA projects observations onto the (hopefully
fewer) principal components of the feature matrix that retain the most
<em>variance</em> in the data, which, practically, means we retain information. PCA is an unsupervised technique, meaning that it does not use the information from the target vector and instead only considers the
feature matrix.</p>
<p>For a mathematical description of how PCA works, see the external
resources listed at the end of this recipe. However, we can understand
the intuition behind PCA using a simple example. In <a data-type="xref" href="#figure0901">Figure 9-1</a>, our data contains two features, <math display="inline"><msub><mi>x</mi><mn>1</mn></msub></math> and <math display="inline"><msub><mi>x</mi><mn>2</mn></msub></math>. Looking at the visualization, it should be clear that observations are spread out like a cigar, with a lot of length and very little height. More specifically, we can say that the variance of the “length” is significantly greater than the “height.” Instead of length and height, we refer to the “direction” with the most variance as the first principal component and the “direction” with the second-most variance as the second principal component (and so on).</p>
<p>If we wanted to reduce our features, one strategy would be to project
all observations in our two-dimensional space onto the one-dimensional principal component. We
would lose the information captured in the second principal component,
but in some situations that would be an acceptable trade-off. This
is PCA.</p>
<figure><div class="figure" id="figure0901">
<img alt="mpc2 0901" height="597" src="assets/mpc2_0901.png" width="600"/>
<h6><span class="label">Figure 9-1. </span>The first and second principal components of PCA</h6>
</div></figure>
<p class="fix_tracking">
PCA is implemented in scikit-learn using the <code>PCA</code> class. <code>n_components</code> has two operations, depending on the argument provided. If the argument is greater than 1, <code>pca</code> will return that many features. This leads to the question of how to select the optimal number of features. Fortunately, if the argument to <code>n_components</code> is between 0 and 1, <code>pca</code> returns the minimum number of features that retain that much variance. It’s common to use values of 0.95 and 0.99, meaning 95% and 99% of the variance of the original features has been retained, respectively. <code>whiten=True</code> transforms the values of each principal component so that they have zero mean and unit variance. Another parameter and argument is <code>svd_solver="randomized"</code>, which implements a stochastic algorithm to find the first principal components in often significantly less time.
</p>
<p>The output of our solution shows that PCA enables us to reduce our
dimensionality by 10 features while still retaining 99% of the
information (variance) in the feature matrix.<a data-primary="" data-startref="ix_feat_ext_pc_app" data-type="indexterm" id="id1462"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1463">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/OT_gN">scikit-learn documentation: PCA</a></p>
</li>
<li>
<p><a href="https://oreil.ly/Uns61">Principal Component Analysis with Linear Algebra, Jeff Jauregui</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="9.2 Reducing Features When Data Is Linearly Inseparable" data-type="sect1"><div class="sect1" id="reducing-features-when-data-is-linearly-inseparable">
<h1>9.2 Reducing Features When Data Is Linearly Inseparable</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id234">
<h2>Problem</h2>
<p>You suspect you have <a data-primary="linearly inseparable data, reducing features" data-type="indexterm" id="ix_lin_insep_feat_ext"/><a data-primary="feature extraction" data-secondary="linearly inseparable data" data-type="indexterm" id="ix_feat_ext_lin_insep"/>linearly inseparable data and want to reduce the
dimensions.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id235">
<h2>Solution</h2>
<p>Use an extension of principal component analysis that uses kernels to
allow for <a data-primary="nonlinear dimensionality reduction" data-type="indexterm" id="id1464"/>nonlinear dimensionality reduction:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">PCA</code><code class="p">,</code> <code class="n">KernelPCA</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_circles</code>

<code class="c1"># Create linearly inseparable data</code>
<code class="n">features</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="n">make_circles</code><code class="p">(</code><code class="n">n_samples</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">noise</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code> <code class="n">factor</code><code class="o">=</code><code class="mf">0.1</code><code class="p">)</code>

<code class="c1"># Apply kernel PCA with radius basis function (RBF) kernel</code>
<code class="n">kpca</code> <code class="o">=</code> <code class="n">KernelPCA</code><code class="p">(</code><code class="n">kernel</code><code class="o">=</code><code class="s2">"rbf"</code><code class="p">,</code> <code class="n">gamma</code><code class="o">=</code><code class="mi">15</code><code class="p">,</code> <code class="n">n_components</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="n">features_kpca</code> <code class="o">=</code> <code class="n">kpca</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="nb">print</code><code class="p">(</code><code class="s2">"Original number of features:"</code><code class="p">,</code> <code class="n">features</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Reduced number of features:"</code><code class="p">,</code> <code class="n">features_kpca</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code></pre>
<pre data-type="programlisting">Original number of features: 2
Reduced number of features: 1</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id236">
<h2>Discussion</h2>
<p>PCA is able to reduce the dimensionality of our feature matrix (i.e., the
number of features). Standard PCA uses linear projection to reduce the
features. If the <a data-primary="linearly separable data" data-type="indexterm" id="id1465"/>data is <em>linearly separable</em> (i.e., you can draw a
straight line or hyperplane between different classes) then PCA works
well. However, if your data is not linearly separable (i.e., you can only
separate classes using a curved decision boundary), the linear
transformation will not work as well. In our solution we used
<a data-primary="make_circles" data-type="indexterm" id="id1466"/>scikit-learn’s <code>make_circles</code> to generate a simulated dataset with a
target vector of two classes and two features. <code>make_circles</code> makes
linearly inseparable data; specifically, one class is surrounded on all
sides by the other class, as shown in <a data-type="xref" href="#figure0902">Figure 9-2</a>.</p>
<figure><div class="figure" id="figure0902">
<img alt="mpc2 0902" height="597" src="assets/mpc2_0902.png" width="600"/>
<h6><span class="label">Figure 9-2. </span>The first principal component projected on linearly inseparable data</h6>
</div></figure>
<p>If we used linear PCA to reduce the dimensions of our data, the two
classes would be linearly projected onto the first principal component
such that they would become intertwined, as shown in <a data-type="xref" href="#figure0903">Figure 9-3</a>.</p>
<figure><div class="figure" id="figure0903">
<img alt="mpc2 0903" height="108" src="assets/mpc2_0903.png" width="600"/>
<h6><span class="label">Figure 9-3. </span>The first principal component of linearly inseparable data without kernel PCA</h6>
</div></figure>
<p>Ideally, we would want a transformation that would reduce the
dimensions and make the data linearly separable. Kernel PCA can do
both, as shown in <a data-type="xref" href="#figure0904">Figure 9-4</a>.</p>
<figure><div class="figure" id="figure0904">
<img alt="mpc2 0904" height="83" src="assets/mpc2_0904.png" width="600"/>
<h6><span class="label">Figure 9-4. </span>The first principal component of linearly inseparable data <em>with</em> kernel PCA</h6>
</div></figure>
<p>Kernels allow us to project the linearly inseparable data into a higher
dimension where it is linearly separable; this is <a data-primary="kernel trick" data-type="indexterm" id="id1467"/>called the “kernel
trick.” Don’t worry if you don’t understand the details of the kernel
trick; just think of <a data-primary="kernel functions" data-type="indexterm" id="id1468"/>kernels as different ways of projecting the data.
There are a number of kernels we can use in <a data-primary="kernelPCA" data-type="indexterm" id="id1469"/>scikit-learn’s <code>kernelPCA</code> class,
specified using the <code>kernel</code> parameter. A common kernel to use is the
Gaussian radial basis function kernel <code>rbf</code>, but other options are the
polynomial kernel (<code>poly</code>) and sigmoid kernel (<code>sigmoid</code>). We can even
specify a linear projection (<code>linear</code>), which will produce the same
results as standard PCA.</p>
<p>One downside of kernel PCA is that we need to specify a number of parameters. For example, in <a data-type="xref" href="#reducing-features-using-principal-components">Recipe 9.1</a> we set <code>n_components</code>
to <code>0.99</code> to make <code>PCA</code> select the number of components to retain 99% of
the variance. We don’t have this option in kernel PCA. Instead we have
to define the number of components (e.g., <code>n_components=1</code>). Furthermore, kernels come with their own hyperparameters that we will have to set; for example, the radial basis function requires a <code>gamma</code> value.</p>
<p>So how do we know which values to use? Through trial and error.
Specifically, we can train our machine learning model multiple times,
each time with a different kernel or different value of the parameter.
Once we find the combination of values that produces the highest
quality predicted values, we are done. This is a common theme in machine learning, and we will learn about this strategy in depth in <a data-type="xref" href="ch12.xhtml#model-selection">Chapter 12</a>.<a data-primary="" data-startref="ix_dim_reduct_pca" data-type="indexterm" id="id1470"/><a data-primary="" data-startref="ix_pca_prin_comp_anal" data-type="indexterm" id="id1471"/><a data-primary="" data-startref="ix_prin_comp_anal_pca" data-type="indexterm" id="id1472"/><a data-primary="" data-startref="ix_feat_ext_lin_insep" data-type="indexterm" id="id1473"/><a data-primary="" data-startref="ix_lin_insep_feat_ext" data-type="indexterm" id="id1474"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1475">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/SCAX-">scikit-learn documentation on Kernel PCA</a></p>
</li>
<li>
<p><a href="https://oreil.ly/ktm5Z">Kernel Tricks and Nonlinear Dimensionality Reduction via RBF Kernel PCA</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="9.3 Reducing Features by Maximizing Class Separability" data-type="sect1"><div class="sect1" id="reducing-features-by-maximizing-class-separability">
<h1>9.3 Reducing Features by Maximizing Class Separability</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id237">
<h2>Problem</h2>
<p>You want to reduce the number of features to be used by a <a data-primary="feature extraction" data-secondary="maximizing class separability" data-type="indexterm" id="ix_feat_ext_max_class_sep"/><a data-primary="maximizing class separability, reducing features" data-type="indexterm" id="ix_max_class_sep"/><a data-primary="classes" data-secondary="maximizing separability, reducing features" data-type="indexterm" id="ix_class_max_sep"/>classifier by maximizing the separation between the classes.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id238">
<h2>Solution</h2>
<p>Try <em>linear discriminant analysis</em> (LDA) to <a data-primary="linear discriminant analysis (LDA)" data-type="indexterm" id="ix_lin_disc_anal_lda"/><a data-primary="LDA (linear discriminant analysis)" data-type="indexterm" id="ix_lda_lin_disc_anal"/><a data-primary="dimensionality reduction" data-secondary="linear discriminant analysis" data-type="indexterm" id="ix_dim_reduct_lda"/>project the features onto
component axes that maximize the separation of classes:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.discriminant_analysis</code> <code class="kn">import</code> <code class="n">LinearDiscriminantAnalysis</code>

<code class="c1"># Load Iris flower dataset:</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create and run an LDA, then use it to transform the features</code>
<code class="n">lda</code> <code class="o">=</code> <code class="n">LinearDiscriminantAnalysis</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="n">features_lda</code> <code class="o">=</code> <code class="n">lda</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Print the number of features</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Original number of features:"</code><code class="p">,</code> <code class="n">features</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Reduced number of features:"</code><code class="p">,</code> <code class="n">features_lda</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code></pre>
<pre data-type="programlisting">Original number of features: 4
Reduced number of features: 1</pre>
<p>We can use <code>explained_variance_ratio_</code> to view the amount of variance
explained by each component. In our solution the single component
explained over 99% of the variance:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">lda</code><code class="o">.</code><code class="n">explained_variance_ratio_</code></pre>
<pre data-type="programlisting">array([0.9912126])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id239">
<h2>Discussion</h2>
<p>LDA is a classification that is also a popular technique for
dimensionality reduction. LDA works similarly to PCA in that it projects our feature space onto a
lower-dimensional space. However, in PCA we were interested only in the
component axes that maximize the variance in the data, while in LDA we
have the additional goal of maximizing the differences between classes.
In <a data-type="xref" href="#figure0905">Figure 9-5</a>, we have data comprising two target classes and
two features. If we project the data onto the y-axis, the two classes
are not easily separable (i.e., they overlap), while if we project the
data onto the x-axis, we are left with a feature vector (i.e., we reduced our dimensionality by one) that still preserves class separability. In the real world, of course, the relationship between the classes will be more complex and the dimensionality will be higher, but the concept remains the same.</p>
<figure><div class="figure" id="figure0905">
<img alt="mpc2 0905" height="280" src="assets/mpc2_0905.png" width="600"/>
<h6><span class="label">Figure 9-5. </span>LDA attempts to maximize the difference between our classes</h6>
</div></figure>
<p class="less_space pagebreak-before">In scikit-learn, LDA is implemented using <code>LinearDiscriminantAnalysis</code>,
which includes a parameter, <code>n_components</code>, indicating the number of features we want returned. To figure out what argument value to use with <code>n_components</code> (e.g., how many parameters to keep), we can take advantage of the fact that 
<span class="keep-together"><code>explained_variance_ratio_</code></span> tells us the variance explained by each outputted feature and is a sorted array. For example:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">lda</code><code class="o">.</code><code class="n">explained_variance_ratio_</code></pre>
<pre data-type="programlisting">array([0.9912126])</pre>
<p>Specifically, we can run <code>LinearDiscriminantAnalysis</code> with
<code>n_components</code> set to <code>None</code> to return the ratio of variance explained by every component feature, then calculate how many components are required to get above some threshold of variance explained (often 0.95 or 0.99):<a data-primary="" data-startref="ix_class_max_sep" data-type="indexterm" id="id1476"/><a data-primary="" data-startref="ix_dim_reduct_lda" data-type="indexterm" id="id1477"/><a data-primary="" data-startref="ix_feat_ext_max_class_sep" data-type="indexterm" id="id1478"/><a data-primary="" data-startref="ix_lda_lin_disc_anal" data-type="indexterm" id="id1479"/><a data-primary="" data-startref="ix_lin_disc_anal_lda" data-type="indexterm" id="id1480"/><a data-primary="" data-startref="ix_max_class_sep" data-type="indexterm" id="id1481"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create and run LDA</code>
<code class="n">lda</code> <code class="o">=</code> <code class="n">LinearDiscriminantAnalysis</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="kc">None</code><code class="p">)</code>
<code class="n">features_lda</code> <code class="o">=</code> <code class="n">lda</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>

<code class="c1"># Create array of explained variance ratios</code>
<code class="n">lda_var_ratios</code> <code class="o">=</code> <code class="n">lda</code><code class="o">.</code><code class="n">explained_variance_ratio_</code>

<code class="c1"># Create function</code>
<code class="k">def</code> <code class="nf">select_n_components</code><code class="p">(</code><code class="n">var_ratio</code><code class="p">,</code> <code class="n">goal_var</code><code class="p">:</code> <code class="nb">float</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">int</code><code class="p">:</code>
    <code class="c1"># Set initial variance explained so far</code>
    <code class="n">total_variance</code> <code class="o">=</code> <code class="mf">0.0</code>

    <code class="c1"># Set initial number of features</code>
    <code class="n">n_components</code> <code class="o">=</code> <code class="mi">0</code>

    <code class="c1"># For the explained variance of each feature:</code>
    <code class="k">for</code> <code class="n">explained_variance</code> <code class="ow">in</code> <code class="n">var_ratio</code><code class="p">:</code>

        <code class="c1"># Add the explained variance to the total</code>
        <code class="n">total_variance</code> <code class="o">+=</code> <code class="n">explained_variance</code>

        <code class="c1"># Add one to the number of components</code>
        <code class="n">n_components</code> <code class="o">+=</code> <code class="mi">1</code>

        <code class="c1"># If we reach our goal level of explained variance</code>
        <code class="k">if</code> <code class="n">total_variance</code> <code class="o">&gt;=</code> <code class="n">goal_var</code><code class="p">:</code>
            <code class="c1"># End the loop</code>
            <code class="k">break</code>

    <code class="c1"># Return the number of components</code>
    <code class="k">return</code> <code class="n">n_components</code>

<code class="c1"># Run function</code>
<code class="n">select_n_components</code><code class="p">(</code><code class="n">lda_var_ratios</code><code class="p">,</code> <code class="mf">0.95</code><code class="p">)</code></pre>
<pre data-type="programlisting">1</pre>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1482">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/evGsx">Comparison of LDA and PCA 2D Projection of Iris Dataset</a></p>
</li>
<li>
<p><a href="https://oreil.ly/uOB81">Linear Discriminant Analysis</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="9.4 Reducing Features Using Matrix Factorization" data-type="sect1"><div class="sect1" id="reducing-features-using-matrix-factorization">
<h1>9.4 Reducing Features Using Matrix Factorization</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id1483">
<h2>Problem</h2>
<p class="fix_tracking2">You have a feature matrix of nonnegative values and want to reduce the
dimensionality.
</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id240">
<h2>Solution</h2>
<p>Use <em>nonnegative matrix factorization</em> (NMF) <a data-primary="feature extraction" data-secondary="matrix factorization" data-type="indexterm" id="ix_feat_ext_matrix_factor"/><a data-primary="matrix factorization" data-type="indexterm" id="ix_matrix_factor"/>to <a data-primary="nonnegative matrix factorization (NMF)" data-type="indexterm" id="ix_nonneg_matrix_factor"/><a data-primary="NMF (nonnegative matrix factorization)" data-type="indexterm" id="ix_nonneg_matrix_factor_rev"/>reduce the dimensionality
of the feature matrix:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">NMF</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>

<code class="c1"># Load the data</code>
<code class="n">digits</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_digits</code><code class="p">()</code>

<code class="c1"># Load feature matrix</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">digits</code><code class="o">.</code><code class="n">data</code>

<code class="c1"># Create, fit, and apply NMF</code>
<code class="n">nmf</code> <code class="o">=</code> <code class="n">NMF</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">4</code><code class="p">)</code>
<code class="n">features_nmf</code> <code class="o">=</code> <code class="n">nmf</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Show results</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Original number of features:"</code><code class="p">,</code> <code class="n">features</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Reduced number of features:"</code><code class="p">,</code> <code class="n">features_nmf</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code></pre>
<pre data-type="programlisting">Original number of features: 64
Reduced number of features: 10</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id241">
<h2>Discussion</h2>
<p>NMF is an <a data-primary="factorization" data-type="indexterm" id="id1484"/>unsupervised technique for linear dimensionality reduction
that <em>factorizes</em> (i.e., breaks up into multiple matrices whose product
approximates the original matrix) the feature matrix into matrices
representing the latent relationship between observations and their
features. Intuitively, NMF can reduce dimensionality because in matrix
multiplication, the two factors (matrices being multiplied) can have
significantly fewer dimensions than the product matrix. Formally, given
a desired number of returned features, <em>r</em>, NMF factorizes
our feature matrix such that:</p>
<div data-type="equation">
<math display="block">
<mrow>
<mi mathvariant="bold">V</mi>
<mo>≈</mo>
<mi mathvariant="bold">W</mi>
<mi mathvariant="bold">H</mi>
</mrow>
</math>
</div>
<p>where <strong>V</strong> is our <em>n	× d</em> feature matrix (i.e., <em>d</em> features, <em>n</em>
observations), <strong>W</strong> is an <em>n</em>	× <em>r</em> matrix, and <strong>H</strong> is an <em>r</em>	× <em>d</em> matrix. By adjusting the value of <em>r</em> we can set the amount of dimensionality reduction desired.</p>
<p>One major requirement of NMF is that, as the name implies, the feature
matrix cannot contain negative values. Additionally, unlike PCA and
other techniques we have examined, NMF does not provide the
explained variance of the outputted features. Thus, the best way for us
to find the optimum value of <code>n_components</code> is by trying a range of values to find the one that produces the best result in our end model (see
<a data-type="xref" href="ch12.xhtml#model-selection">Chapter 12</a>).<a data-primary="" data-startref="ix_nonneg_matrix_factor" data-type="indexterm" id="id1485"/><a data-primary="" data-startref="ix_nonneg_matrix_factor_rev" data-type="indexterm" id="id1486"/><a data-primary="" data-startref="ix_feat_ext_matrix_factor" data-type="indexterm" id="id1487"/><a data-primary="" data-startref="ix_matrix_factor" data-type="indexterm" id="id1488"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1489">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/HJ_Qe">Non-negative matrix factorization, Wikipedia</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="9.5 Reducing Features on Sparse Data" data-type="sect1"><div class="sect1" id="reducing-features-on-sparse-data">
<h1>9.5 Reducing Features on Sparse Data</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id499">
<h2>Problem</h2>
<p>You have a <a data-primary="sparse matrix" data-secondary="reducing features on" data-type="indexterm" id="ix_sparse_feat_ext"/><a data-primary="feature extraction" data-secondary="sparse data" data-type="indexterm" id="ix_feat_ext_sparse"/>sparse feature matrix and want to reduce the dimensionality.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id242">
<h2>Solution</h2>
<p>Use <em>Truncated Singular Value Decomposition</em> (TSVD)<a data-primary="Truncated Singular Value Decomposition (TSVD)" data-type="indexterm" id="ix_trunc_sing_val_dec_tsvd"/>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>
<code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">TruncatedSVD</code>
<code class="kn">from</code> <code class="nn">scipy.sparse</code> <code class="kn">import</code> <code class="n">csr_matrix</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>

<code class="c1"># Load the data</code>
<code class="n">digits</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_digits</code><code class="p">()</code>

<code class="c1"># Standardize feature matrix</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">digits</code><code class="o">.</code><code class="n">data</code><code class="p">)</code>

<code class="c1"># Make sparse matrix</code>
<code class="n">features_sparse</code> <code class="o">=</code> <code class="n">csr_matrix</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Create a TSVD</code>
<code class="n">tsvd</code> <code class="o">=</code> <code class="n">TruncatedSVD</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">10</code><code class="p">)</code>

<code class="c1"># Conduct TSVD on sparse matrix</code>
<code class="n">features_sparse_tsvd</code> <code class="o">=</code> <code class="n">tsvd</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_sparse</code><code class="p">)</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">features_sparse</code><code class="p">)</code>

<code class="c1"># Show results</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Original number of features:"</code><code class="p">,</code> <code class="n">features_sparse</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Reduced number of features:"</code><code class="p">,</code> <code class="n">features_sparse_tsvd</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code></pre>
<pre data-type="programlisting">Original number of features: 64
Reduced number of features: 10</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id243">
<h2>Discussion</h2>
<p>TSVD is similar to PCA and, in fact, PCA often uses <a data-primary="Singular Value Decomposition (SVD)" data-type="indexterm" id="id1490"/><a data-primary="SVD (Singular Value Decomposition)" data-type="indexterm" id="id1491"/>nontruncated <em>Singular Value Decomposition</em> (SVD) in one of its steps. Given <em>d</em> features, SVD will create factor matrices that are <em>d</em>	× <em>d</em>, whereas TSVD will return factors that are <em>n</em>	× <em>n</em>, where <em>n</em> is previously specified by a parameter. The practical advantage of TSVD is that, unlike PCA, it works on sparse feature matrices.</p>
<p>One issue with TSVD: because of how it uses a random number
generator, the signs of the output can flip between fittings. An easy
workaround is to use <code>fit</code> only once per preprocessing pipeline, then use <code>transform</code> multiple times.</p>
<p>As with linear discriminant analysis, we have to specify the number of
features (components) we want to output. This is done with the
<code>n_components</code> parameter. A natural question is: what is the
optimum number of components? One strategy is to include <code>n_components</code> as a hyperparameter to optimize during model selection (i.e., choose the value for <code>n_components</code> that produces the best-trained model). Alternatively, because TSVD provides the ratio of the original feature matrix’s variance explained by each component, we can select the number of components that explain a desired amount of variance (95% and 99% are common values). For example, in our solution, the first three outputted components explain approximately 30% of the original data’s variance:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Sum of first three components' explained variance ratios</code>
<code class="n">tsvd</code><code class="o">.</code><code class="n">explained_variance_ratio_</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="mi">3</code><code class="p">]</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code></pre>
<pre data-type="programlisting">0.3003938537287226</pre>
<p>We can automate the process by creating a function that runs TSVD with

<span class="keep-together"><code>n_components</code></span> set to one less than the number of original features and then calculate the number of components that explain a desired amount of the original data’s variance:<a data-primary="" data-startref="ix_feat_ext_ch9" data-type="indexterm" id="id1492"/><a data-primary="" data-startref="ix_feat_ext_sparse" data-type="indexterm" id="id1493"/><a data-primary="" data-startref="ix_sparse_feat_ext" data-type="indexterm" id="id1494"/><a data-primary="" data-startref="ix_trunc_sing_val_dec_tsvd" data-type="indexterm" id="id1495"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create and run a TSVD with one less than number of features</code>
<code class="n">tsvd</code> <code class="o">=</code> <code class="n">TruncatedSVD</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="n">features_sparse</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">-</code><code class="mi">1</code><code class="p">)</code>
<code class="n">features_tsvd</code> <code class="o">=</code> <code class="n">tsvd</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># List of explained variances</code>
<code class="n">tsvd_var_ratios</code> <code class="o">=</code> <code class="n">tsvd</code><code class="o">.</code><code class="n">explained_variance_ratio_</code>

<code class="c1"># Create a function</code>
<code class="k">def</code> <code class="nf">select_n_components</code><code class="p">(</code><code class="n">var_ratio</code><code class="p">,</code> <code class="n">goal_var</code><code class="p">):</code>
    <code class="c1"># Set initial variance explained so far</code>
    <code class="n">total_variance</code> <code class="o">=</code> <code class="mf">0.0</code>

    <code class="c1"># Set initial number of features</code>
    <code class="n">n_components</code> <code class="o">=</code> <code class="mi">0</code>

    <code class="c1"># For the explained variance of each feature:</code>
    <code class="k">for</code> <code class="n">explained_variance</code> <code class="ow">in</code> <code class="n">var_ratio</code><code class="p">:</code>

        <code class="c1"># Add the explained variance to the total</code>
        <code class="n">total_variance</code> <code class="o">+=</code> <code class="n">explained_variance</code>

        <code class="c1"># Add one to the number of components</code>
        <code class="n">n_components</code> <code class="o">+=</code> <code class="mi">1</code>

        <code class="c1"># If we reach our goal level of explained variance</code>
        <code class="k">if</code> <code class="n">total_variance</code> <code class="o">&gt;=</code> <code class="n">goal_var</code><code class="p">:</code>
            <code class="c1"># End the loop</code>
            <code class="k">break</code>

    <code class="c1"># Return the number of components</code>
    <code class="k">return</code> <code class="n">n_components</code>

<code class="c1"># Run function</code>
<code class="n">select_n_components</code><code class="p">(</code><code class="n">tsvd_var_ratios</code><code class="p">,</code> <code class="mf">0.95</code><code class="p">)</code></pre>
<pre data-type="programlisting">40</pre>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1496">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/nD1pF">scikit-learn documentation: TruncatedSVD</a></p>
</li>
</ul>
</div></section>
</div></section>
</div></section></div></body></html>