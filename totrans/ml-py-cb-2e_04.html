<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 4. Handling Numerical Data" data-type="chapter" epub:type="chapter"><div class="chapter" id="handling-numerical-data">
<h1><span class="label">Chapter 4. </span>Handling Numerical Data</h1>
<section data-pdf-bookmark="4.0 Introduction" data-type="sect1"><div class="sect1" id="id100">
<h1>4.0 Introduction</h1>
<p>Quantitative <a data-primary="numerical data" data-type="indexterm" id="ix_num_data_ch4"/>data is the measurement of something—​whether class size,
monthly sales, or student scores. The natural way to represent these
quantities is numerically (e.g., 29 students, $529,392 in sales).
In this chapter, we will cover numerous strategies for transforming raw
numerical data into features purpose-built for machine learning
algorithms.</p>
</div></section>
<section data-pdf-bookmark="4.1 Rescaling a Feature" data-type="sect1"><div class="sect1" id="rescaling-a-feature">
<h1>4.1 Rescaling a Feature</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id101">
<h2>Problem</h2>
<p>You need to <a data-primary="rescaling numerical features" data-type="indexterm" id="ix_rescale_num_feat"/><a data-primary="numerical data" data-secondary="rescaling numerical features" data-type="indexterm" id="ix_num_data_rescale"/><a data-primary="preprocessing data" data-secondary="and rescaling a numerical feature" data-secondary-sortas="rescaling a numerical feature" data-type="indexterm" id="ix_preproc_data_rescale"/>rescale the values of a numerical feature to be between two
values.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id102">
<h2>Solution</h2>
<p>Use scikit-learn’s <code>MinMaxScaler</code> to <a data-primary="MinMaxScaler" data-type="indexterm" id="ix_min_max_scaler"/>rescale a feature array:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">preprocessing</code>

<code class="c1"># Create feature</code>
<code class="n">feature</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code><code class="o">-</code><code class="mf">500.5</code><code class="p">],</code>
                    <code class="p">[</code><code class="o">-</code><code class="mf">100.1</code><code class="p">],</code>
                    <code class="p">[</code><code class="mi">0</code><code class="p">],</code>
                    <code class="p">[</code><code class="mf">100.1</code><code class="p">],</code>
                    <code class="p">[</code><code class="mf">900.9</code><code class="p">]])</code>

<code class="c1"># Create scaler</code>
<code class="n">minmax_scale</code> <code class="o">=</code> <code class="n">preprocessing</code><code class="o">.</code><code class="n">MinMaxScaler</code><code class="p">(</code><code class="n">feature_range</code><code class="o">=</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>

<code class="c1"># Scale feature</code>
<code class="n">scaled_feature</code> <code class="o">=</code> <code class="n">minmax_scale</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">feature</code><code class="p">)</code>

<code class="c1"># Show feature</code>
<code class="n">scaled_feature</code></pre>
<pre data-type="programlisting">array([[ 0.        ],
       [ 0.28571429],
       [ 0.35714286],
       [ 0.42857143],
       [ 1.        ]])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id103">
<h2>Discussion</h2>
<p><em>Rescaling</em> is a common preprocessing task in machine learning. Many of
the algorithms described later in this book will assume all features are on the same scale, typically 0 to 1 or 	–1 to 1. There are a number of rescaling techniques, but one of the simplest is called <em>min-max scaling</em>. Min-max scaling uses the minimum and maximum values of a feature to rescale values to within a range. Specifically, min-max
calculates:</p>
<div data-type="equation">
<math display="block">
<mrow>
<msubsup><mi>x</mi> <mi>i</mi> <mo>'</mo> </msubsup>
<mo>=</mo>
<mfrac><mrow><msub><mi>x</mi> <mi>i</mi> </msub><mo>-</mo><mtext>min</mtext><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow> <mrow><mtext>max</mtext><mo>(</mo><mi>x</mi><mo>)</mo><mo>-</mo><mtext>min</mtext><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac>
</mrow>
</math>
</div>
<p>where <math display="inline"><mi>x</mi></math> is the feature vector, <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math> is an
individual element of feature <math display="inline"><mi>x</mi></math>, and <math display="inline"><msubsup><mi>x</mi> <mi>i</mi> <mo>'</mo> </msubsup></math> is the rescaled element. In our example, we can see from the outputted
array that the feature has been successfully rescaled to between 0 and
1:</p>
<pre data-type="programlisting">array([[ 0.        ],
      [ 0.28571429],
      [ 0.35714286],
      [ 0.42857143],
      [ 1.        ]])</pre>
<p>scikit-learn’s <code>MinMaxScaler</code> offers two options to rescale a feature.
One <a data-primary="fit operation" data-type="indexterm" id="id1160"/><a data-primary="transform operation" data-type="indexterm" id="id1161"/><a data-primary="fit_transform function" data-type="indexterm" id="id1162"/>option is to use <code>fit</code> to calculate the minimum and maximum values of the feature, and then use <code>transform</code> to rescale the feature. The second option is to use <code>fit_transform</code> to do both operations at once. There is no mathematical difference between the two options, but there is sometimes a practical benefit to keeping the operations separate because it allows us to apply the same transformation to different <em>sets</em> of the data.<a data-primary="" data-startref="ix_min_max_scaler" data-type="indexterm" id="id1163"/><a data-primary="" data-startref="ix_num_data_rescale" data-type="indexterm" id="id1164"/><a data-primary="" data-startref="ix_preproc_data_rescale" data-type="indexterm" id="id1165"/><a data-primary="" data-startref="ix_rescale_num_feat" data-type="indexterm" id="id1166"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1167">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/f2WiM">Feature scaling, Wikipedia</a></p>
</li>
<li>
<p><a href="https://oreil.ly/Da0AH">About Feature Scaling and Normalization, Sebastian Raschka</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="4.2 Standardizing a Feature" data-type="sect1"><div class="sect1" id="standardizing-a-feature">
<h1>4.2 Standardizing a Feature</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id483">
<h2>Problem</h2>
<p>You want to <a data-primary="numerical data" data-secondary="standardizing numerical features" data-type="indexterm" id="ix_num_data_stand_feat"/>transform a feature to have a mean of 0 and a standard
deviation of 1.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id104">
<h2>Solution</h2>
<p>scikit-learn’s <code>StandardScaler</code> performs<a data-primary="StandardScaler" data-type="indexterm" id="ix_stand_scaler"/> both transformations:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">preprocessing</code>

<code class="c1"># Create feature</code>
<code class="n">x</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code><code class="o">-</code><code class="mf">1000.1</code><code class="p">],</code>
              <code class="p">[</code><code class="o">-</code><code class="mf">200.2</code><code class="p">],</code>
              <code class="p">[</code><code class="mf">500.5</code><code class="p">],</code>
              <code class="p">[</code><code class="mf">600.6</code><code class="p">],</code>
              <code class="p">[</code><code class="mf">9000.9</code><code class="p">]])</code>

<code class="c1"># Create scaler</code>
<code class="n">scaler</code> <code class="o">=</code> <code class="n">preprocessing</code><code class="o">.</code><code class="n">StandardScaler</code><code class="p">()</code>

<code class="c1"># Transform the feature</code>
<code class="n">standardized</code> <code class="o">=</code> <code class="n">scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>

<code class="c1"># Show feature</code>
<code class="n">standardized</code></pre>
<pre data-type="programlisting">array([[-0.76058269],
       [-0.54177196],
       [-0.35009716],
       [-0.32271504],
       [ 1.97516685]])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id105">
<h2>Discussion</h2>
<p>A common alternative to the min-max scaling discussed in <a data-type="xref" href="#rescaling-a-feature">Recipe 4.1</a> is rescaling of features to be approximately standard normally distributed. To achieve this, we use standardization to transform the data such that it has a mean, <math display="inline"><mover accent="true"><mi>x</mi> <mo>¯</mo></mover></math>, of 0 and a standard deviation, <math display="inline"><mi>σ</mi></math>, of 1. Specifically, each element in the feature is transformed so that:</p>
<div data-type="equation">
<math display="block">
<mrow>
<msubsup><mi>x</mi> <mi>i</mi> <mo>'</mo> </msubsup>
<mo>=</mo>
<mfrac><mrow><msub><mi>x</mi> <mi>i</mi> </msub><mo>-</mo><mover accent="true"><mi>x</mi> <mo>¯</mo></mover></mrow> <mi>σ</mi></mfrac>
</mrow>
</math>
</div>
<p>where <math display="inline"> <msubsup><mi>x</mi> <mi>i</mi> <mo>'</mo> </msubsup></math> is our standardized form of
<math display="inline"> <msub><mi>x</mi> <mi>i</mi> </msub></math>. The transformed feature represents the number of standard deviations of the original value from the feature’s mean value (also called a <em>z-score</em> in statistics).</p>
<p>Standardization is a common go-to scaling method for machine learning
preprocessing and, in my experience, is used more often than min-max scaling.
However, it depends on the learning algorithm. For example, principal
component analysis often works better using standardization, while
min-max scaling is often recommended for neural networks (both
algorithms are discussed later in this book). As a general rule, I’d
recommend defaulting to standardization unless you have a specific
reason to use an alternative.</p>
<p>We can see the effect of standardization by looking at the mean and
standard deviation of our solution’s output:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Print mean and standard deviation</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Mean:"</code><code class="p">,</code> <code class="nb">round</code><code class="p">(</code><code class="n">standardized</code><code class="o">.</code><code class="n">mean</code><code class="p">()))</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Standard deviation:"</code><code class="p">,</code> <code class="n">standardized</code><code class="o">.</code><code class="n">std</code><code class="p">())</code></pre>
<pre data-type="programlisting">Mean: 0.0
Standard deviation: 1.0</pre>
<p>If our data has <a data-primary="RobustScaler" data-type="indexterm" id="id1168"/>significant outliers, it can negatively impact our
standardization by affecting the feature’s mean and variance. In this
scenario, it is often helpful to instead rescale the feature using the
median and quartile range. In scikit-learn, we do this using the <code>RobustScaler</code> method:<a data-primary="" data-startref="ix_num_data_stand_feat" data-type="indexterm" id="id1169"/><a data-primary="" data-startref="ix_stand_scaler" data-type="indexterm" id="id1170"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create scaler</code>
<code class="n">robust_scaler</code> <code class="o">=</code> <code class="n">preprocessing</code><code class="o">.</code><code class="n">RobustScaler</code><code class="p">()</code>

<code class="c1"># Transform feature</code>
<code class="n">robust_scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">x</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([[ -1.87387612],
       [ -0.875     ],
       [  0.        ],
       [  0.125     ],
       [ 10.61488511]])</pre>
</div></section>
</div></section>
<section data-pdf-bookmark="4.3 Normalizing Observations" data-type="sect1"><div class="sect1" id="normalizing-observations">
<h1>4.3 Normalizing Observations</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id106">
<h2>Problem</h2>
<p>You want to rescale the <a data-primary="numerical data" data-secondary="normalizing observations" data-type="indexterm" id="ix_num_data_normal_observ"/><a data-primary="observations" data-secondary="normalizing" data-type="indexterm" id="ix_observ_norm"/><a data-primary="normalizing observations (Normalizer)" data-type="indexterm" id="ix_normal_observ"/>feature values of observations to have unit norm
(a total length of 1).</p>
</div></section>
<section class="less_space pagebreak-before" data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id1171">
<h2>Solution</h2>
<p>Use <code>Normalizer</code> with a <code>norm</code> argument:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">Normalizer</code>

<code class="c1"># Create feature matrix</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code><code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">],</code>
                     <code class="p">[</code><code class="mf">1.1</code><code class="p">,</code> <code class="mf">3.4</code><code class="p">],</code>
                     <code class="p">[</code><code class="mf">1.5</code><code class="p">,</code> <code class="mf">20.2</code><code class="p">],</code>
                     <code class="p">[</code><code class="mf">1.63</code><code class="p">,</code> <code class="mf">34.4</code><code class="p">],</code>
                     <code class="p">[</code><code class="mf">10.9</code><code class="p">,</code> <code class="mf">3.3</code><code class="p">]])</code>

<code class="c1"># Create normalizer</code>
<code class="n">normalizer</code> <code class="o">=</code> <code class="n">Normalizer</code><code class="p">(</code><code class="n">norm</code><code class="o">=</code><code class="s2">"l2"</code><code class="p">)</code>

<code class="c1"># Transform feature matrix</code>
<code class="n">normalizer</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([[ 0.70710678,  0.70710678],
       [ 0.30782029,  0.95144452],
       [ 0.07405353,  0.99725427],
       [ 0.04733062,  0.99887928],
       [ 0.95709822,  0.28976368]])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id107">
<h2>Discussion</h2>
<p>Many rescaling methods (e.g., min-max scaling and standardization)
operate on features; however, we can also rescale across individual
observations. <code>Normalizer</code> rescales the values on individual
observations to have unit norm (the sum of their lengths is 1). This
type of rescaling is often used when we have many equivalent features
(e.g., text classification when every word or <em>n</em>-word group is a feature).</p>
<p><code>Normalizer</code> provides three norm options with Euclidean norm (often
called L2) being the default argument:</p>
<div data-type="equation">
<math display="block">
<mrow>
<msub><mfenced close="∥" open="∥"><mi>x</mi></mfenced> <mn>2</mn> </msub>
<mo>=</mo>
<msqrt>
<mrow>
<msup><msub><mi>x</mi> <mn>1</mn></msub> <mn>2</mn> </msup>
<mo>+</mo>
<msup><msub><mi>x</mi> <mn>2</mn></msub> <mn>2</mn> </msup>
<mo>+</mo>
<mo>⋯</mo>
<mo>+</mo>
<msup><msub><mi>x</mi> <mi>n</mi></msub> <mn>2</mn> </msup>
</mrow>
</msqrt>
</mrow>
</math>
</div>
<p>where <math display="inline"><mi>x</mi></math> is an individual observation and <math display="inline"><msub><mi>x</mi><mi>n</mi></msub></math> is that observation’s value for the <math display="inline"><mi>n</mi></math>th feature.</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Transform feature matrix</code>
<code class="n">features_l2_norm</code> <code class="o">=</code> <code class="n">Normalizer</code><code class="p">(</code><code class="n">norm</code><code class="o">=</code><code class="s2">"l2"</code><code class="p">)</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Show feature matrix</code>
<code class="n">features_l2_norm</code></pre>
<pre data-type="programlisting">array([[ 0.70710678,  0.70710678],
       [ 0.30782029,  0.95144452],
       [ 0.07405353,  0.99725427],
       [ 0.04733062,  0.99887928],
       [ 0.95709822,  0.28976368]])</pre>
<p>Alternatively, we can specify Manhattan norm (L1):</p>
<div data-type="equation">
<math display="block">
<mrow>
<msub><mfenced close="∥" open="∥"><mi>x</mi></mfenced> <mn>1</mn> </msub>
<mo>=</mo>
<munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </munderover>
<mfenced close="|" open="|" separators="">
<msub><mi>x</mi> <mi>i</mi> </msub>
</mfenced>
<mo>.</mo>
</mrow>
</math>
</div>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Transform feature matrix</code>
<code class="n">features_l1_norm</code> <code class="o">=</code> <code class="n">Normalizer</code><code class="p">(</code><code class="n">norm</code><code class="o">=</code><code class="s2">"l1"</code><code class="p">)</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Show feature matrix</code>
<code class="n">features_l1_norm</code></pre>
<pre data-type="programlisting">array([[ 0.5       ,  0.5       ],
       [ 0.24444444,  0.75555556],
       [ 0.06912442,  0.93087558],
       [ 0.04524008,  0.95475992],
       [ 0.76760563,  0.23239437]])</pre>
<p>Intuitively, L2 norm can be thought of as the distance between two points in New York for a bird (i.e., a straight line), while L1 can be thought of as the distance for a human walking on the street (walk north one block, east one block, north one block, east one block, etc.), which is why it is called “Manhattan norm” or “Taxicab norm.”<a data-primary="" data-startref="ix_normal_observ" data-type="indexterm" id="id1172"/><a data-primary="" data-startref="ix_num_data_normal_observ" data-type="indexterm" id="id1173"/><a data-primary="" data-startref="ix_observ_norm" data-type="indexterm" id="id1174"/></p>
<p>Practically, notice that <code>norm="l1"</code> rescales an observation’s values so they sum to 1, which can sometimes be a desirable quality:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Print sum</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Sum of the first observation</code><code class="se">\'</code><code class="s2">s values:"</code><code class="p">,</code>
   <code class="n">features_l1_norm</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]</code> <code class="o">+</code> <code class="n">features_l1_norm</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">])</code></pre>
<pre data-type="programlisting">Sum of the first observation's values: 1.0</pre>
</div></section>
</div></section>
<section data-pdf-bookmark="4.4 Generating Polynomial and Interaction Features" data-type="sect1"><div class="sect1" id="generating-polynomial-and-interaction-features">
<h1>4.4 Generating Polynomial and Interaction Features</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id108">
<h2>Problem</h2>
<p>You want to create <a data-primary="interaction features, generating" data-type="indexterm" id="ix_interact_feat"/><a data-primary="numerical data" data-secondary="generating polynomial and interaction features" data-type="indexterm" id="ix_num_data_poly_inter"/>polynomial and interaction features.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id109">
<h2>Solution</h2>
<p>Even though some choose to create <a data-primary="PolynomialFeatures" data-type="indexterm" id="ix_poly_nom_feat"/>polynomial and interaction features
manually, scikit-learn offers a built-in method:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">PolynomialFeatures</code>

<code class="c1"># Create feature matrix</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code><code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">],</code>
                     <code class="p">[</code><code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">],</code>
                     <code class="p">[</code><code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">]])</code>

<code class="c1"># Create PolynomialFeatures object</code>
<code class="n">polynomial_interaction</code> <code class="o">=</code> <code class="n">PolynomialFeatures</code><code class="p">(</code><code class="n">degree</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">include_bias</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>

<code class="c1"># Create polynomial features</code>
<code class="n">polynomial_interaction</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([[ 2.,  3.,  4.,  6.,  9.],
       [ 2.,  3.,  4.,  6.,  9.],
       [ 2.,  3.,  4.,  6.,  9.]])</pre>
<p>The <code>degree</code> parameter determines the maximum degree of the polynomial.
For example, <code>degree=2</code> will create new features raised to the second
power:</p>
<div data-type="equation">
<math display="block">
<mrow>
<msub><mi>x</mi> <mn>1</mn> </msub>
<mo>,</mo>
<mspace width="2.84544pt"/>
<msub><mi>x</mi> <mn>2</mn> </msub>
<mo>,</mo>
<mspace width="2.84544pt"/>
<msup><msub><mi>x</mi> <mn>1</mn></msub> <mn>2</mn> </msup>
<mo>,</mo>
<mspace width="2.84544pt"/>
<msup><msub><mi>x</mi> <mn>1</mn></msub> <mn>2</mn> </msup>
<mo>,</mo>
<mspace width="2.84544pt"/>
<msup><msub><mi>x</mi> <mn>2</mn></msub> <mn>2</mn> </msup>
</mrow>
</math>
</div>
<p>while <code>degree=3</code> will create new features raised to the second and third power:</p>
<div data-type="equation">
<math display="block">
<mrow>
<msub><mi>x</mi> <mn>1</mn> </msub>
<mo>,</mo>
<mspace width="2.84544pt"/>
<msub><mi>x</mi> <mn>2</mn> </msub>
<mo>,</mo>
<mspace width="2.84544pt"/>
<msup><msub><mi>x</mi> <mn>1</mn></msub> <mn>2</mn> </msup>
<mo>,</mo>
<mspace width="2.84544pt"/>
<msup><msub><mi>x</mi> <mn>2</mn></msub> <mn>2</mn> </msup>
<mo>,</mo>
<mspace width="2.84544pt"/>
<msup><msub><mi>x</mi> <mn>1</mn></msub> <mn>3</mn> </msup>
<mo>,</mo>
<mspace width="2.84544pt"/>
<msup><msub><mi>x</mi> <mn>2</mn></msub> <mn>3</mn> </msup>
<mo>,</mo>
<mspace width="2.84544pt"/>
<msup><msub><mi>x</mi> <mn>1</mn></msub> <mn>2</mn> </msup>
<mo>,</mo>
<mspace width="2.84544pt"/>
<msup><msub><mi>x</mi> <mn>1</mn></msub> <mn>3</mn> </msup>
<mo>,</mo>
<mspace width="2.84544pt"/>
<msup><msub><mi>x</mi> <mn>2</mn></msub> <mn>3</mn> </msup>
</mrow>
</math>
</div>
<p>Furthermore, by default <code>PolynomialFeatures</code> includes interaction
features:</p>
<div data-type="equation">
<math display="block">
<mrow>
<mspace width="7.11317pt"/>
<msub><mi>x</mi> <mn>1</mn> </msub>
<msub><mi>x</mi> <mn>2</mn> </msub>
</mrow>
</math>
</div>
<p>We can restrict the features created to only interaction features by
setting 
<span class="keep-together"><code>interaction_only</code></span> to <code>True</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">interaction</code> <code class="o">=</code> <code class="n">PolynomialFeatures</code><code class="p">(</code><code class="n">degree</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>
              <code class="n">interaction_only</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">include_bias</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>

<code class="n">interaction</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([[ 2.,  3.,  6.],
       [ 2.,  3.,  6.],
       [ 2.,  3.,  6.]])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id110">
<h2>Discussion</h2>
<p>Polynomial features are often created when we want to include the notion that there exists a nonlinear relationship between the features and the target. For example, we might suspect that the effect of age on the probability of having a major medical condition is not constant over time but increases as age increases. We can encode that nonconstant effect in a feature, <math display="inline"><mi>x</mi></math>, by generating that feature’s
higher-order forms (<math display="inline"><msup><mi>x</mi><mn>2</mn></msup></math>, <math display="inline"><msup><mi>x</mi><mn>3</mn></msup></math>, etc.).</p>
<p>Additionally, often we run into situations where the effect of one
feature is dependent on another feature. A simple example would be if we were trying to predict whether or not our coffee was sweet, and we had two features: (1) whether or not the coffee was stirred, and (2) whether or not we
added sugar. Individually, each feature does not predict coffee
sweetness, but the combination of their effects does. That is, a coffee
would only be sweet if the coffee had sugar and was stirred. The effects of each feature on the target (sweetness) are dependent on each other. We can encode that relationship by including an interaction feature that is the product of the individual features.<a data-primary="" data-startref="ix_interact_feat" data-type="indexterm" id="id1175"/><a data-primary="" data-startref="ix_num_data_poly_inter" data-type="indexterm" id="id1176"/><a data-primary="" data-startref="ix_poly_nom_feat" data-type="indexterm" id="id1177"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="4.5 Transforming Features" data-type="sect1"><div class="sect1" id="transforming-features">
<h1>4.5 Transforming Features</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id111">
<h2>Problem</h2>
<p>You want to make a custom <a data-primary="transforming features, numerical" data-type="indexterm" id="ix_trans_num_data"/><a data-primary="numerical data" data-secondary="transforming features" data-type="indexterm" id="ix_num_data_trans_feat"/>transformation to one or more features.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id112">
<h2>Solution</h2>
<p>In scikit-learn, use <code>FunctionTransformer</code> to <a data-primary="FunctionTransformer" data-type="indexterm" id="id1178"/>apply a function to a set
of features:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">FunctionTransformer</code>

<code class="c1"># Create feature matrix</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code><code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">],</code>
                     <code class="p">[</code><code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">],</code>
                     <code class="p">[</code><code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">]])</code>

<code class="c1"># Define a simple function</code>
<code class="k">def</code> <code class="nf">add_ten</code><code class="p">(</code><code class="n">x</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">int</code><code class="p">:</code>
    <code class="k">return</code> <code class="n">x</code> <code class="o">+</code> <code class="mi">10</code>

<code class="c1"># Create transformer</code>
<code class="n">ten_transformer</code> <code class="o">=</code> <code class="n">FunctionTransformer</code><code class="p">(</code><code class="n">add_ten</code><code class="p">)</code>

<code class="c1"># Transform feature matrix</code>
<code class="n">ten_transformer</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([[12, 13],
       [12, 13],
       [12, 13]])</pre>
<p>We can create the same transformation in <a data-primary="apply method" data-type="indexterm" id="id1179"/>pandas using <code>apply</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>

<code class="c1"># Create DataFrame</code>
<code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s2">"feature_1"</code><code class="p">,</code> <code class="s2">"feature_2"</code><code class="p">])</code>

<code class="c1"># Apply function</code>
<code class="n">df</code><code class="o">.</code><code class="n">apply</code><code class="p">(</code><code class="n">add_ten</code><code class="p">)</code></pre>
<table>
<thead>
<tr>
<th/>
<th>feature_1</th>
<th>feature_2</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>12</td>
<td>13</td>
</tr>
<tr>
<th>1</th>
<td>12</td>
<td>13</td>
</tr>
<tr>
<th>2</th>
<td>12</td>
<td>13</td>
</tr>
</tbody>
</table>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id113">
<h2>Discussion</h2>
<p>It is common to want to make some custom transformations to one or more
features. For example, we might want to create a feature that is the
natural log of the values of a different feature. We can do this by
creating a function and then mapping it to <a data-primary="EllipticEnvelope" data-type="indexterm" id="id1180"/>features using either scikit-learn’s <code>FunctionTransformer</code> or pandas’ <code>apply</code>. In the solution we created a very simple function, <code>add_ten</code>, which added 10
to each input, but there is no reason we could not define a much more complex function.<a data-primary="" data-startref="ix_num_data_trans_feat" data-type="indexterm" id="id1181"/><a data-primary="" data-startref="ix_trans_num_data" data-type="indexterm" id="id1182"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="4.6 Detecting Outliers" data-type="sect1"><div class="sect1" id="detecting-outliers">
<h1>4.6 Detecting Outliers</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id114">
<h2>Problem</h2>
<p>You want to <a data-primary="outliers" data-type="indexterm" id="ix_outlier_handle"/><a data-primary="numerical data" data-secondary="detecting outliers" data-type="indexterm" id="ix_num_data_det_out"/>identify extreme observations.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id115">
<h2>Solution</h2>
<p>Detecting outliers is unfortunately more of an art than a science.
However, a common method is to assume the data is normally distributed
and, based on that assumption, “draw” an ellipse around the data,
classifying any observation inside the ellipse as an inlier (labeled as
<code>1</code>) and any observation outside the ellipse as an outlier (labeled 
<span class="keep-together">as
<code>-1</code>):</span></p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn.covariance</code> <code class="kn">import</code> <code class="n">EllipticEnvelope</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_blobs</code>

<code class="c1"># Create simulated data</code>
<code class="n">features</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="n">make_blobs</code><code class="p">(</code><code class="n">n_samples</code> <code class="o">=</code> <code class="mi">10</code><code class="p">,</code>
                         <code class="n">n_features</code> <code class="o">=</code> <code class="mi">2</code><code class="p">,</code>
                         <code class="n">centers</code> <code class="o">=</code> <code class="mi">1</code><code class="p">,</code>
                         <code class="n">random_state</code> <code class="o">=</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Replace the first observation's values with extreme values</code>
<code class="n">features</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code><code class="mi">0</code><code class="p">]</code> <code class="o">=</code> <code class="mi">10000</code>
<code class="n">features</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code><code class="mi">1</code><code class="p">]</code> <code class="o">=</code> <code class="mi">10000</code>

<code class="c1"># Create detector</code>
<code class="n">outlier_detector</code> <code class="o">=</code> <code class="n">EllipticEnvelope</code><code class="p">(</code><code class="n">contamination</code><code class="o">=</code><code class="mf">.1</code><code class="p">)</code>

<code class="c1"># Fit detector</code>
<code class="n">outlier_detector</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Predict outliers</code>
<code class="n">outlier_detector</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">features</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([-1,  1,  1,  1,  1,  1,  1,  1,  1,  1])</pre>
<p>In these arrays, values of -1 refer to outliers whereas values of 1 refer to inliers. A major limitation of this approach is the need to specify a
<code>contamination</code> parameter, which is the proportion of observations that
are outliers—​a value that we don’t know. Think of <code>contamination</code> as
our estimate of the cleanliness of our data. If we expect our data to
have few outliers, we can set <code>contamination</code> to something small.
However, if we believe that the data is likely to have outliers, we
can set it to a higher value.</p>
<p>Instead of looking at observations as a whole, we can instead look at
individual features and identify extreme values in those features using
interquartile <a data-primary="interquartile range (IQR)" data-type="indexterm" id="id1183"/><a data-primary="IQR (interquartile range)" data-type="indexterm" id="id1184"/>range (IQR):</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create one feature</code>
<code class="n">feature</code> <code class="o">=</code> <code class="n">features</code><code class="p">[:,</code><code class="mi">0</code><code class="p">]</code>

<code class="c1"># Create a function to return index of outliers</code>
<code class="k">def</code> <code class="nf">indicies_of_outliers</code><code class="p">(</code><code class="n">x</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="nb">int</code><code class="p">):</code>
    <code class="n">q1</code><code class="p">,</code> <code class="n">q3</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">percentile</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="p">[</code><code class="mi">25</code><code class="p">,</code> <code class="mi">75</code><code class="p">])</code>
    <code class="n">iqr</code> <code class="o">=</code> <code class="n">q3</code> <code class="o">-</code> <code class="n">q1</code>
    <code class="n">lower_bound</code> <code class="o">=</code> <code class="n">q1</code> <code class="o">-</code> <code class="p">(</code><code class="n">iqr</code> <code class="o">*</code> <code class="mf">1.5</code><code class="p">)</code>
    <code class="n">upper_bound</code> <code class="o">=</code> <code class="n">q3</code> <code class="o">+</code> <code class="p">(</code><code class="n">iqr</code> <code class="o">*</code> <code class="mf">1.5</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">np</code><code class="o">.</code><code class="n">where</code><code class="p">((</code><code class="n">x</code> <code class="o">&gt;</code> <code class="n">upper_bound</code><code class="p">)</code> <code class="o">|</code> <code class="p">(</code><code class="n">x</code> <code class="o">&lt;</code> <code class="n">lower_bound</code><code class="p">))</code>

<code class="c1"># Run function</code>
<code class="n">indicies_of_outliers</code><code class="p">(</code><code class="n">feature</code><code class="p">)</code></pre>
<pre data-type="programlisting">(array([0]),)</pre>
<p>IQR is the difference between the first and third quartile of a set of data. You can think of IQR as the spread of the bulk of the data, with outliers being observations far from the main concentration of data. Outliers are commonly defined as any value 1.5 IQRs less than the first quartile, or 1.5 IQRs greater than the third quartile.</p>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id116">
<h2>Discussion</h2>
<p>There is no single best technique for detecting outliers. Instead, we
have a collection of techniques all with their own advantages and
disadvantages. Our best strategy is often trying multiple techniques
(e.g., both <code>EllipticEnvelope</code> and IQR-based detection) and looking at
the results as a whole.</p>
<p>If at all possible, we should look at observations we detect as
outliers and try to understand them. For example, if we have a dataset
of houses and one feature is number of rooms, is an outlier with 100
rooms really a house or is it actually a hotel that has been
misclassified?<a data-primary="" data-startref="ix_num_data_det_out" data-type="indexterm" id="id1185"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1186">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/wlwmH">Three Ways to Detect Outliers (and the source of the IQR function used in this recipe)</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="4.7 Handling Outliers" data-type="sect1"><div class="sect1" id="handling-outliers">
<h1>4.7 Handling Outliers</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id484">
<h2>Problem</h2>
<p>You have <a data-primary="numerical data" data-secondary="handling outliers" data-type="indexterm" id="ix_num_data_out_handle2"/>outliers in your data that you want to identify and then reduce their impact on the data distribution.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id1187">
<h2>Solution</h2>
<p>Typically we can use three strategies to handle outliers. First,
we can drop them:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>

<code class="c1"># Create DataFrame</code>
<code class="n">houses</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">()</code>
<code class="n">houses</code><code class="p">[</code><code class="s1">'Price'</code><code class="p">]</code> <code class="o">=</code> <code class="p">[</code><code class="mi">534433</code><code class="p">,</code> <code class="mi">392333</code><code class="p">,</code> <code class="mi">293222</code><code class="p">,</code> <code class="mi">4322032</code><code class="p">]</code>
<code class="n">houses</code><code class="p">[</code><code class="s1">'Bathrooms'</code><code class="p">]</code> <code class="o">=</code> <code class="p">[</code><code class="mi">2</code><code class="p">,</code> <code class="mf">3.5</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">116</code><code class="p">]</code>
<code class="n">houses</code><code class="p">[</code><code class="s1">'Square_Feet'</code><code class="p">]</code> <code class="o">=</code> <code class="p">[</code><code class="mi">1500</code><code class="p">,</code> <code class="mi">2500</code><code class="p">,</code> <code class="mi">1500</code><code class="p">,</code> <code class="mi">48000</code><code class="p">]</code>

<code class="c1"># Filter observations</code>
<code class="n">houses</code><code class="p">[</code><code class="n">houses</code><code class="p">[</code><code class="s1">'Bathrooms'</code><code class="p">]</code> <code class="o">&lt;</code> <code class="mi">20</code><code class="p">]</code></pre>
<table>
<thead>
<tr>
<th/>
<th>Price</th>
<th>Bathrooms</th>
<th>Square_Feet</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>534433</td>
<td>2.0</td>
<td>1500</td>
</tr>
<tr>
<th>1</th>
<td>392333</td>
<td>3.5</td>
<td>2500</td>
</tr>
<tr>
<th>2</th>
<td>293222</td>
<td>2.0</td>
<td>1500</td>
</tr>
</tbody>
</table>
<p class="less_space pagebreak-before">Second, we can mark them as outliers and include “Outlier” as a feature:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>

<code class="c1"># Create feature based on boolean condition</code>
<code class="n">houses</code><code class="p">[</code><code class="s2">"Outlier"</code><code class="p">]</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">where</code><code class="p">(</code><code class="n">houses</code><code class="p">[</code><code class="s2">"Bathrooms"</code><code class="p">]</code> <code class="o">&lt;</code> <code class="mi">20</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Show data</code>
<code class="n">houses</code></pre>
<table>
<thead>
<tr>
<th/>
<th>Price</th>
<th>Bathrooms</th>
<th>Square_Feet</th>
<th>Outlier</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>534433</td>
<td>2.0</td>
<td>1500</td>
<td>0</td>
</tr>
<tr>
<th>1</th>
<td>392333</td>
<td>3.5</td>
<td>2500</td>
<td>0</td>
</tr>
<tr>
<th>2</th>
<td>293222</td>
<td>2.0</td>
<td>1500</td>
<td>0</td>
</tr>
<tr>
<th>3</th>
<td>4322032</td>
<td>116.0</td>
<td>48000</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>Finally, we can transform the feature to dampen the effect of the
outlier:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Log feature</code>
<code class="n">houses</code><code class="p">[</code><code class="s2">"Log_Of_Square_Feet"</code><code class="p">]</code> <code class="o">=</code> <code class="p">[</code><code class="n">np</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="n">x</code><code class="p">)</code> <code class="k">for</code> <code class="n">x</code> <code class="ow">in</code> <code class="n">houses</code><code class="p">[</code><code class="s2">"Square_Feet"</code><code class="p">]]</code>

<code class="c1"># Show data</code>
<code class="n">houses</code></pre>
<table>
<thead>
<tr>
<th/>
<th>Price</th>
<th>Bathrooms</th>
<th>Square_Feet</th>
<th>Outlier</th>
<th>Log_Of_Square_Feet</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>534433</td>
<td>2.0</td>
<td>1500</td>
<td>0</td>
<td>7.313220</td>
</tr>
<tr>
<th>1</th>
<td>392333</td>
<td>3.5</td>
<td>2500</td>
<td>0</td>
<td>7.824046</td>
</tr>
<tr>
<th>2</th>
<td>293222</td>
<td>2.0</td>
<td>1500</td>
<td>0</td>
<td>7.313220</td>
</tr>
<tr>
<th>3</th>
<td>4322032</td>
<td>116.0</td>
<td>48000</td>
<td>1</td>
<td>10.778956</td>
</tr>
</tbody>
</table>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id117">
<h2>Discussion</h2>
<p>Similar to detecting outliers, there is no hard-and-fast rule for
handling them. How we handle them should be based on two aspects. First,
we should consider what makes them outliers. If we believe they are
errors<a data-primary="error handling" data-secondary="outliers" data-type="indexterm" id="id1188"/> in the data, such as from a broken sensor or a miscoded value,
then we might drop the observation or replace outlier values with <code>NaN</code> since we can’t trust those values. However, if we believe the outliers are genuine extreme values (e.g., a house [mansion] with 200 bathrooms), then marking them as outliers or transforming their values is more appropriate.</p>
<p>Second, how we handle outliers should be based on our goal for machine
learning. For example, if we want to predict house prices based on features of the house, we might reasonably assume the price for mansions with over 100 bathrooms is driven by a different dynamic than regular family homes. Furthermore, if we are training a model to use as part of an online home loan web application, we might assume that our potential users will not include billionaires looking to buy a mansion.</p>
<p>So what should we do if we have outliers? Think about why they are
outliers, have an end goal in mind for the data, and, most importantly,
remember that not making a decision to address outliers is itself a
decision with implications.</p>
<p>One additional point: if you do have outliers, standardization might not
be appropriate because the mean and variance might be highly influenced
by the outliers. In this <a data-primary="RobustScaler" data-type="indexterm" id="id1189"/>case, use a rescaling method more robust
against outliers, like <code>RobustScaler</code>.<a data-primary="" data-startref="ix_outlier_handle" data-type="indexterm" id="id1190"/><a data-primary="" data-startref="ix_num_data_out_handle2" data-type="indexterm" id="id1191"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1192">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/zgm-1"><code>RobustScaler</code> documentation</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="4.8 Discretizating Features" data-type="sect1"><div class="sect1" id="discretizating-features">
<h1>4.8 Discretizating Features</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id118">
<h2>Problem</h2>
<p>You have a numerical feature and want to break it up into <a data-primary="discretizating features, numerical data" data-type="indexterm" id="id1193"/><a data-primary="numerical data" data-secondary="discretizating features" data-type="indexterm" id="id1194"/>discrete bins.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id119">
<h2>Solution</h2>
<p>Depending on how we want to break up the data, there are two
techniques we can use. First, we can <a data-primary="Binarizer" data-type="indexterm" id="id1195"/>binarize the feature according to
some threshold:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">Binarizer</code>

<code class="c1"># Create feature</code>
<code class="n">age</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code><code class="mi">6</code><code class="p">],</code>
                <code class="p">[</code><code class="mi">12</code><code class="p">],</code>
                <code class="p">[</code><code class="mi">20</code><code class="p">],</code>
                <code class="p">[</code><code class="mi">36</code><code class="p">],</code>
                <code class="p">[</code><code class="mi">65</code><code class="p">]])</code>

<code class="c1"># Create binarizer</code>
<code class="n">binarizer</code> <code class="o">=</code> <code class="n">Binarizer</code><code class="p">(</code><code class="n">threshold</code><code class="o">=</code><code class="mi">18</code><code class="p">)</code>

<code class="c1"># Transform feature</code>
<code class="n">binarizer</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">age</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([[0],
       [0],
       [1],
       [1],
       [1]])</pre>
<p>Second, we can <a data-primary="digitize method" data-type="indexterm" id="id1196"/>break up numerical features according to multiple
thresholds:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Bin feature</code>
<code class="n">np</code><code class="o">.</code><code class="n">digitize</code><code class="p">(</code><code class="n">age</code><code class="p">,</code> <code class="n">bins</code><code class="o">=</code><code class="p">[</code><code class="mi">20</code><code class="p">,</code><code class="mi">30</code><code class="p">,</code><code class="mi">64</code><code class="p">])</code></pre>
<pre data-type="programlisting">array([[0],
       [0],
       [1],
       [2],
       [3]])</pre>
<p>Note that the arguments for the <code>bins</code> parameter denote the left edge of
each bin. For example, the <code>20</code> argument does not include the element
with the value of 20, only the two values smaller than 20. We can switch
this behavior by setting the parameter <code>right</code> to <code>True</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Bin feature</code>
<code class="n">np</code><code class="o">.</code><code class="n">digitize</code><code class="p">(</code><code class="n">age</code><code class="p">,</code> <code class="n">bins</code><code class="o">=</code><code class="p">[</code><code class="mi">20</code><code class="p">,</code><code class="mi">30</code><code class="p">,</code><code class="mi">64</code><code class="p">],</code> <code class="n">right</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([[0],
       [0],
       [0],
       [2],
       [3]])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id120">
<h2>Discussion</h2>
<p>Discretization can be a fruitful strategy when we have reason to believe that a numerical feature should behave more like a categorical feature. For example, we might believe there is very little difference in the spending habits of 19- and 20-year-olds, but a significant difference between 20- and 21-year-olds (the age in the United States when young adults can consume alcohol). In that example, it could be useful to break up individuals in our data into those who can drink alcohol and those who cannot. Similarly, in other cases it might be useful to discretize our data into three or more bins.</p>
<p>In the <a data-primary="Binarizer" data-type="indexterm" id="id1197"/>solution, we saw two methods of discretization—​scikit-learn’s <code>Binarizer</code> for two bins and NumPy’s <code>digitize</code> for three or more bins—​however, we can also use <code>digitize</code> to binarize features like <code>Binarizer</code> by specifying only a single threshold:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Bin feature</code>
<code class="n">np</code><code class="o">.</code><code class="n">digitize</code><code class="p">(</code><code class="n">age</code><code class="p">,</code> <code class="n">bins</code><code class="o">=</code><code class="p">[</code><code class="mi">18</code><code class="p">])</code></pre>
<pre data-type="programlisting">array([[0],
       [0],
       [1],
       [1],
       [1]])</pre>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1198">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/KipXX"><code>digitize</code> documentation</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="4.9 Grouping Observations Using Clustering" data-type="sect1"><div class="sect1" id="grouping-observations-using-clustering">
<h1>4.9 Grouping Observations Using Clustering</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id121">
<h2>Problem</h2>
<p>You want to <a data-primary="numerical data" data-secondary="grouping observations using clustering" data-type="indexterm" id="ix_num_data_group_cluster"/><a data-primary="observations" data-secondary="grouping using clustering" data-type="indexterm" id="ix_observ_group_cluster"/><a data-primary="grouping observations with clustering" data-type="indexterm" id="ix_group_observ_cluster"/><a data-primary="clustering" data-secondary="grouping observations using" data-type="indexterm" id="ix_cluster_group_observ"/>cluster observations so that similar observations are
grouped together.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id122">
<h2>Solution</h2>
<p>If you <a data-primary="k-means clustering" data-type="indexterm" id="ix_kmeans_cluster2"/><a data-primary="clustering" data-secondary="k-means" data-type="indexterm" id="ix_cluster_kmeans"/>know that you have <em>k</em> groups, you can use k-means
clustering to group similar observations and output a new feature
containing each observation’s <a data-primary="KMeans" data-type="indexterm" id="id1199"/>group membership:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_blobs</code>
<code class="kn">from</code> <code class="nn">sklearn.cluster</code> <code class="kn">import</code> <code class="n">KMeans</code>

<code class="c1"># Make simulated feature matrix</code>
<code class="n">features</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="n">make_blobs</code><code class="p">(</code><code class="n">n_samples</code> <code class="o">=</code> <code class="mi">50</code><code class="p">,</code>
                         <code class="n">n_features</code> <code class="o">=</code> <code class="mi">2</code><code class="p">,</code>
                         <code class="n">centers</code> <code class="o">=</code> <code class="mi">3</code><code class="p">,</code>
                         <code class="n">random_state</code> <code class="o">=</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Create DataFrame</code>
<code class="n">dataframe</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s2">"feature_1"</code><code class="p">,</code> <code class="s2">"feature_2"</code><code class="p">])</code>

<code class="c1"># Make k-means clusterer</code>
<code class="n">clusterer</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="mi">3</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Fit clusterer</code>
<code class="n">clusterer</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Predict values</code>
<code class="n">dataframe</code><code class="p">[</code><code class="s2">"group"</code><code class="p">]</code> <code class="o">=</code> <code class="n">clusterer</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># View first few observations</code>
<code class="n">dataframe</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code></pre>
<table>
<thead>
<tr>
<th/>
<th>feature_1</th>
<th>feature_2</th>
<th>group</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>–9.877554</td>
<td>–3.336145</td>
<td>0</td>
</tr>
<tr>
<th>1</th>
<td>–7.287210</td>
<td>–8.353986</td>
<td>2</td>
</tr>
<tr>
<th>2</th>
<td>–6.943061</td>
<td>–7.023744</td>
<td>2</td>
</tr>
<tr>
<th>3</th>
<td>–7.440167</td>
<td>–8.791959</td>
<td>2</td>
</tr>
<tr>
<th>4</th>
<td>–6.641388</td>
<td>–8.075888</td>
<td>2</td>
</tr>
</tbody>
</table>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id123">
<h2>Discussion</h2>
<p>We are jumping ahead of ourselves a bit and will go into much more depth
about clustering algorithms later in the book. However, I wanted to
point out that we can use clustering as a preprocessing step.
Specifically, we use unsupervised learning algorithms like k-means to
cluster observations into groups. The result is a categorical
feature with similar observations being members of the same group.</p>
<p>Don’t worry if you did not understand all of that: just file
away the idea that clustering can be used in preprocessing. And
if you really can’t wait, feel free to flip to <a data-type="xref" href="ch19.xhtml#clustering">Chapter 19</a> now.<a data-primary="" data-startref="ix_cluster_group_observ" data-type="indexterm" id="id1200"/><a data-primary="" data-startref="ix_cluster_kmeans" data-type="indexterm" id="id1201"/><a data-primary="" data-startref="ix_group_observ_cluster" data-type="indexterm" id="id1202"/><a data-primary="" data-startref="ix_kmeans_cluster2" data-type="indexterm" id="id1203"/><a data-primary="" data-startref="ix_num_data_group_cluster" data-type="indexterm" id="id1204"/><a data-primary="" data-startref="ix_observ_group_cluster" data-type="indexterm" id="id1205"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="4.10 Deleting Observations with Missing Values" data-type="sect1"><div class="sect1" id="deleting-observations-with-missing-values">
<h1>4.10 Deleting Observations with Missing Values</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id124">
<h2>Problem</h2>
<p>You need to <a data-primary="observations" data-secondary="deleting observations with missing values" data-type="indexterm" id="ix_obs_del_miss_val"/><a data-primary="missing data" data-secondary="deleting observations with missing values" data-type="indexterm" id="ix_miss_data_delete_observ_val"/><a data-primary="numerical data" data-secondary="deleting observations with missing values" data-type="indexterm" id="ix_num_data_del_obs_val"/><a data-primary="deleting observations with missing values" data-type="indexterm" id="ix_delete_observ_val"/>delete observations containing missing values.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id485">
<h2>Solution</h2>
<p>Deleting <a data-primary="NumPy arrays" data-secondary="dropping observations" data-type="indexterm" id="id1206"/>observations with missing values is easy with a clever line of
NumPy:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>

<code class="c1"># Create feature matrix</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code><code class="mf">1.1</code><code class="p">,</code> <code class="mf">11.1</code><code class="p">],</code>
                     <code class="p">[</code><code class="mf">2.2</code><code class="p">,</code> <code class="mf">22.2</code><code class="p">],</code>
                     <code class="p">[</code><code class="mf">3.3</code><code class="p">,</code> <code class="mf">33.3</code><code class="p">],</code>
                     <code class="p">[</code><code class="mf">4.4</code><code class="p">,</code> <code class="mf">44.4</code><code class="p">],</code>
                     <code class="p">[</code><code class="n">np</code><code class="o">.</code><code class="n">nan</code><code class="p">,</code> <code class="mi">55</code><code class="p">]])</code>

<code class="c1"># Keep only observations that are not (denoted by ~) missing</code>
<code class="n">features</code><code class="p">[</code><code class="o">~</code><code class="n">np</code><code class="o">.</code><code class="n">isnan</code><code class="p">(</code><code class="n">features</code><code class="p">)</code><code class="o">.</code><code class="n">any</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)]</code></pre>
<pre data-type="programlisting">array([[  1.1,  11.1],
       [  2.2,  22.2],
       [  3.3,  33.3],
       [  4.4,  44.4]])</pre>
<p>Alternatively, we can drop missing observations using pandas:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>

<code class="c1"># Load data</code>
<code class="n">dataframe</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s2">"feature_1"</code><code class="p">,</code> <code class="s2">"feature_2"</code><code class="p">])</code>

<code class="c1"># Remove observations with missing values</code>
<code class="n">dataframe</code><code class="o">.</code><code class="n">dropna</code><code class="p">()</code></pre>
<table>
<thead>
<tr>
<th/>
<th>feature_1</th>
<th>feature_2</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>1.1</td>
<td>11.1</td>
</tr>
<tr>
<th>1</th>
<td>2.2</td>
<td>22.2</td>
</tr>
<tr>
<th>2</th>
<td>3.3</td>
<td>33.3</td>
</tr>
<tr>
<th>3</th>
<td>4.4</td>
<td>44.4</td>
</tr>
</tbody>
</table>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id125">
<h2>Discussion</h2>
<p>Most machine learning algorithms cannot handle any missing values in the
target and feature arrays. For this reason, we cannot ignore missing
values in our data and must address the issue during preprocessing.</p>
<p>The simplest solution is to delete every observation that contains one
or more missing values, a task quickly and easily accomplished using
NumPy or pandas.</p>
<p>That said, we should be very reluctant to delete observations with
missing values. Deleting them is the nuclear option, since our algorithm
loses access to the information contained in the observation’s
nonmissing values.</p>
<p>Just as important, depending on the cause of the missing values,
deleting observations can introduce bias into our <a data-primary="missing data" data-secondary="types of" data-type="indexterm" id="id1207"/>data. There are three
types of missing data:</p>
<dl>
<dt>Missing completely at random (MCAR)</dt>
<dd>
<p>The <a data-primary="missing completely at random (MCAR) data" data-type="indexterm" id="id1208"/><a data-primary="MCAR (missing completely at random) data" data-type="indexterm" id="id1209"/>probability that a value is missing is independent of everything. For example, a survey respondent rolls a die before answering a question: if she rolls a six, she skips that question.</p>
</dd>
<dt>Missing at random (MAR)</dt>
<dd>
<p>The <a data-primary="missing at random (MAR) data" data-type="indexterm" id="id1210"/><a data-primary="MAR (missing at random) data" data-type="indexterm" id="id1211"/>probability that a value is missing is not completely random but depends on the information captured in other features. For example, a survey asks about gender identity and annual salary, and women are more likely to skip the salary question; however, their nonresponse depends only on information we have captured in our gender identity feature.</p>
</dd>
<dt>Missing not at random (MNAR)</dt>
<dd>
<p>The <a data-primary="missing not at random (MNAR) data" data-type="indexterm" id="id1212"/><a data-primary="MNAR (missing not at random) data" data-type="indexterm" id="id1213"/>probability that a value is missing is not random and depends on information not captured in our features. For example, a survey asks about annual salary, and women are more likely to skip the salary question, and we do not have a gender identity feature in our data.</p>
</dd>
</dl>
<p>It is sometimes acceptable to delete observations if they are MCAR or
MAR. However, if the value is MNAR, the fact that a value is missing is
itself information. Deleting MNAR observations can inject bias into our
data because we are removing observations produced by some unobserved
systematic effect.<a data-primary="" data-startref="ix_delete_observ_val" data-type="indexterm" id="id1214"/><a data-primary="" data-startref="ix_miss_data_delete_observ_val" data-type="indexterm" id="id1215"/><a data-primary="" data-startref="ix_num_data_del_obs_val" data-type="indexterm" id="id1216"/><a data-primary="" data-startref="ix_obs_del_miss_val" data-type="indexterm" id="id1217"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1218">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/sz9Fx">Identifying the 3 Types of Missing Data</a></p>
</li>
<li>
<p><a href="https://oreil.ly/swU2j">Missing-Data Imputation</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="4.11 Imputing Missing Values" data-type="sect1"><div class="sect1" id="imputing-missing-values">
<h1>4.11 Imputing Missing Values</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id126">
<h2>Problem</h2>
<p>You have missing values in your data and want to <a data-primary="numerical data" data-secondary="imputing missing values" data-type="indexterm" id="ix_num_data_imput_val"/><a data-primary="missing data" data-secondary="imputing missing values" data-type="indexterm" id="ix_miss_data_imput_val"/><a data-primary="imputing missing values" data-type="indexterm" id="ix_imput_miss_val"/>impute them via a generic method or prediction.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id127">
<h2>Solution</h2>
<p>You can impute missing values using <a data-primary="k-nearest neighbors (KNN)" data-secondary="imputing missing values with" data-type="indexterm" id="ix_knn_imput_miss_val"/><a data-primary="SimpleImputer" data-type="indexterm" id="id1219"/>k-nearest neighbors (KNN) or the scikit-learn <code>SimpleImputer</code> class. If you have a small amount of data, predict and impute the missing values using
k-nearest neighbors:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn.impute</code> <code class="kn">import</code> <code class="n">KNNImputer</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_blobs</code>

<code class="c1"># Make a simulated feature matrix</code>
<code class="n">features</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="n">make_blobs</code><code class="p">(</code><code class="n">n_samples</code> <code class="o">=</code> <code class="mi">1000</code><code class="p">,</code>
                         <code class="n">n_features</code> <code class="o">=</code> <code class="mi">2</code><code class="p">,</code>
                         <code class="n">random_state</code> <code class="o">=</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Standardize the features</code>
<code class="n">scaler</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>
<code class="n">standardized_features</code> <code class="o">=</code> <code class="n">scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Replace the first feature's first value with a missing value</code>
<code class="n">true_value</code> <code class="o">=</code> <code class="n">standardized_features</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code><code class="mi">0</code><code class="p">]</code>
<code class="n">standardized_features</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code><code class="mi">0</code><code class="p">]</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">nan</code>

<code class="c1"># Predict the missing values in the feature matrix</code>
<code class="n">knn_imputer</code> <code class="o">=</code> <code class="n">KNNImputer</code><code class="p">(</code><code class="n">n_neighbors</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code>
<code class="n">features_knn_imputed</code> <code class="o">=</code> <code class="n">knn_imputer</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">standardized_features</code><code class="p">)</code>

<code class="c1"># Compare true and imputed values</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"True Value:"</code><code class="p">,</code> <code class="n">true_value</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Imputed Value:"</code><code class="p">,</code> <code class="n">features_knn_imputed</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code><code class="mi">0</code><code class="p">])</code></pre>
<pre data-type="programlisting">True Value: 0.8730186114
Imputed Value: 1.09553327131</pre>
<p>Alternatively, we can use scikit-learn’s <code>SimpleImputer</code> class from the <code>imputer</code> module to fill in
missing values with the feature’s mean, median, or most frequent value.
However, we will typically get worse results than with KNN:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn.impute</code> <code class="kn">import</code> <code class="n">SimpleImputer</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_blobs</code>

<code class="c1"># Make a simulated feature matrix</code>
<code class="n">features</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="n">make_blobs</code><code class="p">(</code><code class="n">n_samples</code> <code class="o">=</code> <code class="mi">1000</code><code class="p">,</code>
                         <code class="n">n_features</code> <code class="o">=</code> <code class="mi">2</code><code class="p">,</code>
                         <code class="n">random_state</code> <code class="o">=</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Standardize the features</code>
<code class="n">scaler</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>
<code class="n">standardized_features</code> <code class="o">=</code> <code class="n">scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Replace the first feature's first value with a missing value</code>
<code class="n">true_value</code> <code class="o">=</code> <code class="n">standardized_features</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code><code class="mi">0</code><code class="p">]</code>
<code class="n">standardized_features</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code><code class="mi">0</code><code class="p">]</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">nan</code>

<code class="c1"># Create imputer using the "mean" strategy</code>
<code class="n">mean_imputer</code> <code class="o">=</code> <code class="n">SimpleImputer</code><code class="p">(</code><code class="n">strategy</code><code class="o">=</code><code class="s2">"mean"</code><code class="p">)</code>

<code class="c1"># Impute values</code>
<code class="n">features_mean_imputed</code> <code class="o">=</code> <code class="n">mean_imputer</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Compare true and imputed values</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"True Value:"</code><code class="p">,</code> <code class="n">true_value</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Imputed Value:"</code><code class="p">,</code> <code class="n">features_mean_imputed</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code><code class="mi">0</code><code class="p">])</code></pre>
<pre data-type="programlisting">True Value: 0.8730186114
Imputed Value: -3.05837272461</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id128">
<h2>Discussion</h2>
<p>There are two main strategies for replacing missing data with substitute
values, each of which has strengths and weaknesses. First, we can use
machine learning to predict the values of the missing data. To do this
we treat the feature with missing values as a target vector and use the
remaining subset of features to predict missing values. While we can use
a wide range of machine learning algorithms to impute values, a popular
choice is KNN. KNN is addressed in depth in <a data-type="xref" href="ch15.xhtml#k-nearest-neighbors">Chapter 15</a>, but the short explanation is that the algorithm uses the <em>k</em> nearest observations (according to some distance metric) to predict the missing value. In our solution we predicted the missing value using the five closest observations.</p>
<p>The downside to KNN is that in order to know which observations are the
closest to the missing value, it needs to calculate the distance between the missing value and every single observation. This is reasonable in smaller datasets but quickly becomes problematic if a dataset has millions of observations. In such cases, approximate nearest neighbors (ANN) is a more feasible approach. We will discuss ANN in 
<span class="keep-together"><a data-type="xref" href="ch15.xhtml#finding-approximate-nearest-neighbors">Recipe 15.5</a>.</span></p>
<p>An alternative and more scalable strategy than KNN is to fill in the missing values of numerical data with the mean, median, or mode. For example, in our solution we used scikit-learn to fill in missing values with a feature’s mean value. The imputed value is often not as close to the true value as when we used KNN, but we can scale mean-filling to data containing millions of observations more easily.</p>
<p>If we use imputation, it is a good idea to create a binary feature
indicating whether the observation contains an imputed value.<a data-primary="" data-startref="ix_num_data_ch4" data-type="indexterm" id="id1220"/><a data-primary="" data-startref="ix_imput_miss_val" data-type="indexterm" id="id1221"/><a data-primary="" data-startref="ix_knn_imput_miss_val" data-type="indexterm" id="id1222"/><a data-primary="" data-startref="ix_miss_data_imput_val" data-type="indexterm" id="id1223"/><a data-primary="" data-startref="ix_num_data_imput_val" data-type="indexterm" id="id1224"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1225">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/1M4bn">scikit-learn documentation: Imputation of Missing Values</a></p>
</li>
<li>
<p><a href="https://oreil.ly/012--">A Study of K-Nearest Neighbour as an Imputation Method</a></p>
</li>
</ul>
</div></section>
</div></section>
</div></section></div></body></html>