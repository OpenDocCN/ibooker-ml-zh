<html><head></head><body><section data-pdf-bookmark="Chapter 10. Recommender Systems Using Restricted Boltzmann Machines" data-type="chapter" epub:type="chapter"><div class="chapter" id="Chapter_10">&#13;
<h1><span class="label">Chapter 10. </span>Recommender Systems Using Restricted Boltzmann Machines</h1>&#13;
&#13;
&#13;
<p>Earlier<a data-primary="unsupervised deep learning" data-secondary="recommender systems using RBMs" data-type="indexterm" id="UDLrec10"/><a data-primary="unsupervised learning" data-secondary="deep neural networks" data-type="indexterm" id="ULdeep10"/> in this book, we used unsupervised learning to learn the underlying (hidden) structure in unlabeled data. Specifically, we performed dimensionality reduction, reducing a high-dimensional dataset to one with much fewer dimensions, and built an anomaly detection system. We also performed clustering, grouping objects together based on how similar or dissimilar they were to each other.</p>&#13;
&#13;
<p>Now, we<a data-primary="generative unsupervised models" data-type="indexterm" id="idm140637537235184"/> will move into <em>generative unsupervised models</em>, which involve learning a probability distribution from an original dataset and using it to make inferences about never-before-seen data. In later chapters, we will use such models to generate seemingly real data, which at times is virtually indistinguishable from the original data.</p>&#13;
&#13;
<p>Until<a data-primary="discriminative models" data-type="indexterm" id="idm140637537233024"/> now, we have looked at mostly <em>discriminative models</em> that learn to separate observations based on what the algorithms learn from the data; these discriminative models do not learn the probability distribution from the data. Discriminative models include supervised ones such as the logistic regression and decision trees from <a data-type="xref" href="ch02.html#Chapter_2">Chapter 2</a> as well as clustering methods such as <em>k</em>-means and hierarchical clustering from <a data-type="xref" href="ch05.html#Chapter_5">Chapter 5</a>.</p>&#13;
&#13;
<p>Let’s<a data-primary="restricted Boltzmann machines (RBMs)" data-secondary="Boltzmann machines" data-type="indexterm" id="RBMboltz10"/><a data-primary="Boltzmann machines" data-type="indexterm" id="boltz10"/> start with the simplest of the generative unsupervised models known as the <em>restricted Boltzmann machine</em>.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Boltzmann Machines" data-type="sect1"><div class="sect1" id="idm140637537226272">&#13;
<h1>Boltzmann Machines</h1>&#13;
&#13;
<p><em>Boltzmann machines</em> were first invented in 1985 by <a data-primary="Hinton, Geoffrey" data-type="indexterm" id="idm140637537224368"/>Geoffrey Hinton (then a professor at Carnegie Mellon University and now one of the fathers of the deep learning movement, a professor at the University of Toronto, and a machine learning researcher at Google) and <a data-primary="Sejnowski, Terry" data-type="indexterm" id="idm140637537223312"/>Terry Sejnowski (who was a professor at John Hopkins University at the time).</p>&#13;
&#13;
<p>Boltzmann machines—of the unrestricted type—consist of a neural network with an input layer and one or several hidden layers. The neurons or units in the neural network make stochastic decisions about whether to turn on or not based on the data fed in during training and the cost function the Boltzmann machine is trying to minimize. With this training, the Boltzmann machine discovers interesting features about the data, which helps model the complex underlying relationships and patterns present in the data.</p>&#13;
&#13;
<p>However, these unrestricted Boltzmann machines use neural networks with neurons that are connected not only to other neurons in other layers but also to neurons within the same layer. That, coupled with the presence of many hidden layers, makes training an unrestricted Boltzmann machine very inefficient. Unrestricted Boltzmann machines had little commercial success during the 1980s and 1990s as a result.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Restricted Boltzmann Machines" data-type="sect2"><div class="sect2" id="idm140637537220640">&#13;
<h2>Restricted Boltzmann Machines</h2>&#13;
&#13;
<p>In the 2000s, Geoffrey Hinton and others began to have commercial success by using a modified version of the original unrestricted Boltzmann machines. These <em>restricted Boltzmann machines (RBMs)</em> have an<a data-primary="visible layer" data-type="indexterm" id="idm140637537218464"/> input layer (also referred to as the <em>visible layer</em>) and just a single hidden layer, and the connections among neurons are restricted such that neurons are connected only to the neurons in other layers but not to neurons within the same layer. In other words, there are no visible-visible connections and no hidden-hidden connections.<sup><a data-type="noteref" href="ch10.html#idm140637537216896" id="idm140637537216896-marker">1</a></sup></p>&#13;
&#13;
<p>Geoffrey Hinton also demonstrated that such simple RBMs could be stacked on top of each other so that the output of the hidden layer of one RBM can be fed into the input layer of another RBM. This sort of RBM stacking can be repeated many times to learn progressively more nuanced hidden representations of the original data. This network of many RBMs can be viewed as one deep, multilayered neural network model—and thus the field of deep learning took off, starting in 2006.</p>&#13;
&#13;
<p>Note<a data-primary="stochastic approach" data-type="indexterm" id="idm140637537214896"/><a data-primary="deterministic approach" data-type="indexterm" id="idm140637537214160"/> that RBMs use a <em>stochastic</em> approach to learning the underlying structure of data, whereas autoencoders, for example, use a <em>deterministic</em> approach.<a data-primary="" data-startref="RBMboltz10" data-type="indexterm" id="idm140637537212416"/><a data-primary="" data-startref="boltz10" data-type="indexterm" id="idm140637537211408"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before" data-pdf-bookmark="Recommender Systems" data-type="sect1"><div class="sect1" id="idm140637537210208">&#13;
<h1>Recommender Systems</h1>&#13;
&#13;
<p>In<a data-primary="restricted Boltzmann machines (RBMs)" data-secondary="recommender systems" data-type="indexterm" id="idm140637537208560"/><a data-primary="recommender systems" data-type="indexterm" id="idm140637537207488"/> this chapter, we will use RBMs to build a <em>recommender system</em>, one of the most successful applications of machine learning to date and widely used in industry to help predict user preferences for movies, music, books, news, search, shopping, digital advertising, and online dating.</p>&#13;
&#13;
<p>There<a data-primary="collaborative filtering" data-type="indexterm" id="idm140637537205584"/><a data-primary="content-based filtering" data-type="indexterm" id="idm140637537204848"/> are two major categories of recommender systems—<em>collaborative filtering</em> recommender systems and <em>content-based filtering</em> recommender systems. Collaborative filtering involves building a recommender system from a user’s past behavior and those of other users to which the user is similar to. This recommender system can then predict items that the user may have an interest in even though the user has never expressed explicit interest. Movie recommendations on Netflix rely on collaborative filtering.</p>&#13;
&#13;
<p>Content-based filtering involves learning the distinct properties of an item to recommend additional items with similar properties. Music recommendations on Pandora rely on content-based filtering.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Collaborative Filtering" data-type="sect2"><div class="sect2" id="idm140637537202032">&#13;
<h2>Collaborative Filtering</h2>&#13;
&#13;
<p>Content-based filtering is not commonly used because it is a rather difficult task to learn the distinct properties of items—this level of understanding is very challenging for artificial machines to achieve currently. It is much easier to collect and analyze a large amount of information on users’ behaviors and preferences and make predictions based on this. Therefore, collaborative filtering is much more widely used and is the type of recommender system we will focus on here.</p>&#13;
&#13;
<p>Collaborative filtering requires no knowledge of the underlying items themselves. Rather, collaborative filtering assumes that users that agreed in the past will agree in the future and that user preferences remain stable over time. By modeling how <span class="keep-together">similar</span> users are to other users, collaborative filtering can make pretty powerful recommendations. Moreover, collaborative filtering<a data-primary="explicit data" data-type="indexterm" id="idm140637537198688"/><a data-primary="implicit data" data-type="indexterm" id="idm140637537197984"/> does not have to rely on <em>explicit data</em> (i.e., ratings that users provide). Rather, it can work with <em>implicit data</em> such as how long or how often a user views or clicks on a particular item. For example, in the past Netflix asked users to rate movies but now uses implicit user behavior to make inferences about user likes and dislikes.</p>&#13;
&#13;
<p>However, collaborative filtering has its challenges. First, it requires a lot of user data to make good recommendations. Second, it is a very computationally demanding task. Third, the datasets are generally very sparse since users will have exhibited preferences for only a small fraction of all the items in the universe of possible items. Assuming we have enough data, there are techniques we can use to handle the sparsity of the data and efficiently solve the problem, which we will cover in this chapter.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Netflix Prize" data-type="sect2"><div class="sect2" id="idm140637537194896">&#13;
<h2>The Netflix Prize</h2>&#13;
&#13;
<p>In 2006, Netflix sponsored<a data-primary="Netflix-sponsored competition" data-type="indexterm" id="idm140637537193520"/> a three-year-long competition to improve its movie recommender system. The company offered a grand prize of one million dollars to the team that could improve the accuracy of its existing recommender system by at least 10%. It also released a dataset of over 100 million movie ratings. In September 2009, BellKor’s Pramatic Chaos team won the prize, using an ensemble of many different algorithmic approaches.</p>&#13;
&#13;
<p>Such a high-profile competition with a rich dataset and meaningful prize energized the machine learning community and led to substantial progress in recommender system research, which paved the way for better recommender systems in industry over the past several years.</p>&#13;
&#13;
<p>In this chapter, we will use a similar movie rating dataset to build our own recommender system using RBMs.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="MovieLens Dataset" data-type="sect1"><div class="sect1" id="idm140637537190720">&#13;
<h1>MovieLens Dataset</h1>&#13;
&#13;
<p>Instead<a data-primary="MovieLens 20M Dataset" data-type="indexterm" id="movielens1"/><a data-primary="restricted Boltzmann machines (RBMs)" data-secondary="MovieLens 20M Dataset" data-type="indexterm" id="RBMmovie10"/> of the 100 million ratings Netflix dataset, we will use a smaller movie ratings dataset known as the <em>MovieLens 20M Dataset</em>, provided by GroupLens, a research lab in the Department of Computer Science and Engineering at the University of Minnesota, Twin Cities. The data contains 20,000,263 ratings across 27,278 movies created by 138,493 users from January 9, 1995 to March 31, 2015. Of users who rated at least 20 movies each, we will select a subset at random.</p>&#13;
&#13;
<p>This dataset is more manageable to work with than the 100 million ratings dataset from Netflix. Because the file size exceeds one hundred megabytes, the file is not accessible on GitHub. You will need to download the file directly from the <a href="http://bit.ly/2G0ZHCn">MovieLens website</a>.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Data Preparation" data-type="sect2"><div class="sect2" id="idm140637537184496">&#13;
<h2>Data Preparation</h2>&#13;
&#13;
<p>As before, let’s load in the necessary libraries:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="sd">'''Main'''</code>&#13;
<code class="kn">import</code> <code class="nn">numpy</code> <code class="kn">as</code> <code class="nn">np</code>&#13;
<code class="kn">import</code> <code class="nn">pandas</code> <code class="kn">as</code> <code class="nn">pd</code>&#13;
<code class="kn">import</code> <code class="nn">os</code><code class="o">,</code> <code class="nn">time</code><code class="o">,</code> <code class="nn">re</code>&#13;
<code class="kn">import</code> <code class="nn">pickle</code><code class="o">,</code> <code class="nn">gzip</code><code class="o">,</code> <code class="nn">datetime</code>&#13;
&#13;
<code class="sd">'''Data Viz'''</code>&#13;
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="kn">as</code> <code class="nn">plt</code>&#13;
<code class="kn">import</code> <code class="nn">seaborn</code> <code class="kn">as</code> <code class="nn">sns</code>&#13;
<code class="n">color</code> <code class="o">=</code> <code class="n">sns</code><code class="o">.</code><code class="n">color_palette</code><code class="p">()</code>&#13;
<code class="kn">import</code> <code class="nn">matplotlib</code> <code class="kn">as</code> <code class="nn">mpl</code>&#13;
&#13;
<code class="o">%</code><code class="n">matplotlib</code> <code class="n">inline</code>&#13;
&#13;
<code class="sd">'''Data Prep and Model Evaluation'''</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">preprocessing</code> <code class="k">as</code> <code class="n">pp</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">StratifiedKFold</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">log_loss</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">precision_recall_curve</code><code class="p">,</code> <code class="n">average_precision_score</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">roc_curve</code><code class="p">,</code> <code class="n">auc</code><code class="p">,</code> <code class="n">roc_auc_score</code><code class="p">,</code> <code class="n">mean_squared_error</code>&#13;
&#13;
<code class="sd">'''Algos'''</code>&#13;
<code class="kn">import</code> <code class="nn">lightgbm</code> <code class="kn">as</code> <code class="nn">lgb</code>&#13;
&#13;
<code class="sd">'''TensorFlow and Keras'''</code>&#13;
<code class="kn">import</code> <code class="nn">tensorflow</code> <code class="kn">as</code> <code class="nn">tf</code>&#13;
<code class="kn">import</code> <code class="nn">keras</code>&#13;
<code class="kn">from</code> <code class="nn">keras</code> <code class="kn">import</code> <code class="n">backend</code> <code class="k">as</code> <code class="n">K</code>&#13;
<code class="kn">from</code> <code class="nn">keras.models</code> <code class="kn">import</code> <code class="n">Sequential</code><code class="p">,</code> <code class="n">Model</code>&#13;
<code class="kn">from</code> <code class="nn">keras.layers</code> <code class="kn">import</code> <code class="n">Activation</code><code class="p">,</code> <code class="n">Dense</code><code class="p">,</code> <code class="n">Dropout</code>&#13;
<code class="kn">from</code> <code class="nn">keras.layers</code> <code class="kn">import</code> <code class="n">BatchNormalization</code><code class="p">,</code> <code class="n">Input</code><code class="p">,</code> <code class="n">Lambda</code>&#13;
<code class="kn">from</code> <code class="nn">keras</code> <code class="kn">import</code> <code class="n">regularizers</code>&#13;
<code class="kn">from</code> <code class="nn">keras.losses</code> <code class="kn">import</code> <code class="n">mse</code><code class="p">,</code> <code class="n">binary_crossentropy</code></pre>&#13;
&#13;
<p>Next, we will load in the ratings dataset and convert the fields into the appropriate data types. We have just a few fields. The user ID, the movie ID, the rating provided by the user for the movie, and the timestamp of the rating provided:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load the data</code>&#13;
<code class="n">current_path</code> <code class="o">=</code> <code class="n">os</code><code class="o">.</code><code class="n">getcwd</code><code class="p">()</code>&#13;
<code class="nb">file</code> <code class="o">=</code> <code class="s1">'</code><code class="se">\\</code><code class="s1">datasets</code><code class="se">\\</code><code class="s1">movielens_data</code><code class="se">\\</code><code class="s1">ratings.csv'</code>&#13;
<code class="n">ratingDF</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="n">current_path</code> <code class="o">+</code> <code class="nb">file</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Convert fields into appropriate data types</code>&#13;
<code class="n">ratingDF</code><code class="o">.</code><code class="n">userId</code> <code class="o">=</code> <code class="n">ratingDF</code><code class="o">.</code><code class="n">userId</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="nb">str</code><code class="p">)</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="nb">int</code><code class="p">)</code>&#13;
<code class="n">ratingDF</code><code class="o">.</code><code class="n">movieId</code> <code class="o">=</code> <code class="n">ratingDF</code><code class="o">.</code><code class="n">movieId</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="nb">str</code><code class="p">)</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="nb">int</code><code class="p">)</code>&#13;
<code class="n">ratingDF</code><code class="o">.</code><code class="n">rating</code> <code class="o">=</code> <code class="n">ratingDF</code><code class="o">.</code><code class="n">rating</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="nb">str</code><code class="p">)</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="nb">float</code><code class="p">)</code>&#13;
<code class="n">ratingDF</code><code class="o">.</code><code class="n">timestamp</code> <code class="o">=</code> <code class="n">ratingDF</code><code class="o">.</code><code class="n">timestamp</code><code class="o">.</code><code class="n">apply</code><code class="p">(</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> \&#13;
                <code class="n">datetime</code><code class="o">.</code><code class="n">utcfromtimestamp</code><code class="p">(</code><code class="n">x</code><code class="p">)</code><code class="o">.</code><code class="n">strftime</code><code class="p">(</code><code class="s1">'</code><code class="si">%Y</code><code class="s1">-</code><code class="si">%m</code><code class="s1">-</code><code class="si">%d</code><code class="s1"> </code><code class="si">%H</code><code class="s1">:</code><code class="si">%M</code><code class="s1">:</code><code class="si">%S</code><code class="s1">'</code><code class="p">))</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#movielens_ratings_data">Table 10-1</a> shows a partial view of the data.</p>&#13;
<table class="pagebreak-before" id="movielens_ratings_data">&#13;
<caption><span class="label">Table 10-1. </span>MovieLens ratings data</caption>&#13;
<thead>&#13;
<tr>&#13;
<th/>&#13;
<th>userId</th>&#13;
<th>movieId</th>&#13;
<th>rating</th>&#13;
<th>timestamp</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>0</p></td>&#13;
<td><p>1</p></td>&#13;
<td><p>2</p></td>&#13;
<td><p>3.5</p></td>&#13;
<td><p>2005-04-02 23:53:47</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>1</p></td>&#13;
<td><p>1</p></td>&#13;
<td><p>29</p></td>&#13;
<td><p>3.5</p></td>&#13;
<td><p>2005-04-02 23:31:16</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>2</p></td>&#13;
<td><p>1</p></td>&#13;
<td><p>32</p></td>&#13;
<td><p>3.5</p></td>&#13;
<td><p>2005-04-02 23:33:39</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>3</p></td>&#13;
<td><p>1</p></td>&#13;
<td><p>47</p></td>&#13;
<td><p>3.5</p></td>&#13;
<td><p>2005-04-02 23:32:07</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>4</p></td>&#13;
<td><p>1</p></td>&#13;
<td><p>50</p></td>&#13;
<td><p>3.5</p></td>&#13;
<td><p>2005-04-02 23:29:40</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>5</p></td>&#13;
<td><p>1</p></td>&#13;
<td><p>112</p></td>&#13;
<td><p>3.5</p></td>&#13;
<td><p>2004-09-10 03:09:00</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>6</p></td>&#13;
<td><p>1</p></td>&#13;
<td><p>151</p></td>&#13;
<td><p>4.0</p></td>&#13;
<td><p>2004-09-10 03:08:54</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>7</p></td>&#13;
<td><p>1</p></td>&#13;
<td><p>223</p></td>&#13;
<td><p>4.0</p></td>&#13;
<td><p>2005-04-02 23:46:13</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>8</p></td>&#13;
<td><p>1</p></td>&#13;
<td><p>253</p></td>&#13;
<td><p>4.0</p></td>&#13;
<td><p>2005-04-02 23:35:40</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>9</p></td>&#13;
<td><p>1</p></td>&#13;
<td><p>260</p></td>&#13;
<td><p>4.0</p></td>&#13;
<td><p>2005-04-02 23:33:46</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>10</p></td>&#13;
<td><p>1</p></td>&#13;
<td><p>293</p></td>&#13;
<td><p>4.0</p></td>&#13;
<td><p>2005-04-02 23:31:43</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>11</p></td>&#13;
<td><p>1</p></td>&#13;
<td><p>296</p></td>&#13;
<td><p>4.0</p></td>&#13;
<td><p>2005-04-02 23:32:47</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>12</p></td>&#13;
<td><p>1</p></td>&#13;
<td><p>318</p></td>&#13;
<td><p>4.0</p></td>&#13;
<td><p>2005-04-02 23:33:18</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>13</p></td>&#13;
<td><p>1</p></td>&#13;
<td><p>337</p></td>&#13;
<td><p>3.5</p></td>&#13;
<td><p>2004-09-10 03:08:29</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>Let’s confirm the number of unique users, unique movies, and total ratings, and we will also calculate the average number of ratings provided by users:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">n_users</code> <code class="o">=</code> <code class="n">ratingDF</code><code class="o">.</code><code class="n">userId</code><code class="o">.</code><code class="n">unique</code><code class="p">()</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>&#13;
<code class="n">n_movies</code> <code class="o">=</code> <code class="n">ratingDF</code><code class="o">.</code><code class="n">movieId</code><code class="o">.</code><code class="n">unique</code><code class="p">()</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>&#13;
<code class="n">n_ratings</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">ratingDF</code><code class="p">)</code>&#13;
<code class="n">avg_ratings_per_user</code> <code class="o">=</code> <code class="n">n_ratings</code><code class="o">/</code><code class="n">n_users</code>&#13;
&#13;
<code class="k">print</code><code class="p">(</code><code class="s1">'Number of unique users: '</code><code class="p">,</code> <code class="n">n_users</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s1">'Number of unique movies: '</code><code class="p">,</code> <code class="n">n_movies</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s1">'Number of total ratings: '</code><code class="p">,</code> <code class="n">n_ratings</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s1">'Average number of ratings per user: '</code><code class="p">,</code> <code class="n">avg_ratings_per_user</code><code class="p">)</code></pre>&#13;
&#13;
<p>The data is as we expected:</p>&#13;
&#13;
<pre data-type="programlisting">Number of unique users: 138493&#13;
Number of unique movies: 26744&#13;
Number of total ratings: 20000263&#13;
Average number of ratings per user: 144.4135299257002</pre>&#13;
&#13;
<p>To reduce the complexity and size of this dataset, let’s focus on the top one thousand most rated movies. This will reduce the number of ratings from about ~20 million to about ~12.8 million.</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">movieIndex</code> <code class="o">=</code> <code class="n">ratingDF</code><code class="o">.</code><code class="n">groupby</code><code class="p">(</code><code class="s2">"movieId"</code><code class="p">)</code><code class="o">.</code><code class="n">count</code><code class="p">()</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="n">by</code><code class="o">=</code> \&#13;
                <code class="s2">"rating"</code><code class="p">,</code><code class="n">ascending</code><code class="o">=</code><code class="bp">False</code><code class="p">)[</code><code class="mi">0</code><code class="p">:</code><code class="mi">1000</code><code class="p">]</code><code class="o">.</code><code class="n">index</code>&#13;
<code class="n">ratingDFX2</code> <code class="o">=</code> <code class="n">ratingDF</code><code class="p">[</code><code class="n">ratingDF</code><code class="o">.</code><code class="n">movieId</code><code class="o">.</code><code class="n">isin</code><code class="p">(</code><code class="n">movieIndex</code><code class="p">)]</code>&#13;
<code class="n">ratingDFX2</code><code class="o">.</code><code class="n">count</code><code class="p">()</code></pre>&#13;
&#13;
<p>We will also take a sample of one thousand users at random and filter the dataset for just these users. This will reduce the number of ratings from ~12.8 million to just 90,213. This number is sufficient to demonstrate collaborative filtering:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">userIndex</code> <code class="o">=</code> <code class="n">ratingDFX2</code><code class="o">.</code><code class="n">groupby</code><code class="p">(</code><code class="s2">"userId"</code><code class="p">)</code><code class="o">.</code><code class="n">count</code><code class="p">()</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="n">by</code><code class="o">=</code> \&#13;
    <code class="s2">"rating"</code><code class="p">,</code><code class="n">ascending</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="n">n</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">2018</code><code class="p">)</code><code class="o">.</code><code class="n">index</code>&#13;
<code class="n">ratingDFX3</code> <code class="o">=</code> <code class="n">ratingDFX2</code><code class="p">[</code><code class="n">ratingDFX2</code><code class="o">.</code><code class="n">userId</code><code class="o">.</code><code class="n">isin</code><code class="p">(</code><code class="n">userIndex</code><code class="p">)]</code>&#13;
<code class="n">ratingDFX3</code><code class="o">.</code><code class="n">count</code><code class="p">()</code></pre>&#13;
&#13;
<p>Let’s also reindex <code>movieID</code> and <code>userID</code> to a range of 1 to 1,000 for our reduced dataset:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">movies</code> <code class="o">=</code> <code class="n">ratingDFX3</code><code class="o">.</code><code class="n">movieId</code><code class="o">.</code><code class="n">unique</code><code class="p">()</code>&#13;
<code class="n">moviesDF</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">movies</code><code class="p">,</code><code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'originalMovieId'</code><code class="p">])</code>&#13;
<code class="n">moviesDF</code><code class="p">[</code><code class="s1">'newMovieId'</code><code class="p">]</code> <code class="o">=</code> <code class="n">moviesDF</code><code class="o">.</code><code class="n">index</code><code class="o">+</code><code class="mi">1</code>&#13;
&#13;
<code class="n">users</code> <code class="o">=</code> <code class="n">ratingDFX3</code><code class="o">.</code><code class="n">userId</code><code class="o">.</code><code class="n">unique</code><code class="p">()</code>&#13;
<code class="n">usersDF</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">users</code><code class="p">,</code><code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'originalUserId'</code><code class="p">])</code>&#13;
<code class="n">usersDF</code><code class="p">[</code><code class="s1">'newUserId'</code><code class="p">]</code> <code class="o">=</code> <code class="n">usersDF</code><code class="o">.</code><code class="n">index</code><code class="o">+</code><code class="mi">1</code>&#13;
&#13;
<code class="n">ratingDFX3</code> <code class="o">=</code> <code class="n">ratingDFX3</code><code class="o">.</code><code class="n">merge</code><code class="p">(</code><code class="n">moviesDF</code><code class="p">,</code><code class="n">left_on</code><code class="o">=</code><code class="s1">'movieId'</code><code class="p">,</code> \&#13;
                              <code class="n">right_on</code><code class="o">=</code><code class="s1">'originalMovieId'</code><code class="p">)</code>&#13;
<code class="n">ratingDFX3</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="n">labels</code><code class="o">=</code><code class="s1">'originalMovieId'</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>&#13;
&#13;
<code class="n">ratingDFX3</code> <code class="o">=</code> <code class="n">ratingDFX3</code><code class="o">.</code><code class="n">merge</code><code class="p">(</code><code class="n">usersDF</code><code class="p">,</code><code class="n">left_on</code><code class="o">=</code><code class="s1">'userId'</code><code class="p">,</code> \&#13;
                              <code class="n">right_on</code><code class="o">=</code><code class="s1">'originalUserId'</code><code class="p">)</code>&#13;
<code class="n">ratingDFX3</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="n">labels</code><code class="o">=</code><code class="s1">'originalUserId'</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code></pre>&#13;
&#13;
<p>Let’s calculate the number of unique users, unique movies, total ratings, and average number of ratings per user for our reduced dataset:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">n_users</code> <code class="o">=</code> <code class="n">ratingDFX3</code><code class="o">.</code><code class="n">userId</code><code class="o">.</code><code class="n">unique</code><code class="p">()</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>&#13;
<code class="n">n_movies</code> <code class="o">=</code> <code class="n">ratingDFX3</code><code class="o">.</code><code class="n">movieId</code><code class="o">.</code><code class="n">unique</code><code class="p">()</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>&#13;
<code class="n">n_ratings</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">ratingDFX3</code><code class="p">)</code>&#13;
<code class="n">avg_ratings_per_user</code> <code class="o">=</code> <code class="n">n_ratings</code><code class="o">/</code><code class="n">n_users</code>&#13;
&#13;
<code class="k">print</code><code class="p">(</code><code class="s1">'Number of unique users: '</code><code class="p">,</code> <code class="n">n_users</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s1">'Number of unique movies: '</code><code class="p">,</code> <code class="n">n_movies</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s1">'Number of total ratings: '</code><code class="p">,</code> <code class="n">n_ratings</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s1">'Average number of ratings per user: '</code><code class="p">,</code> <code class="n">avg_ratings_per_user</code><code class="p">)</code></pre>&#13;
&#13;
<p>The results are as expected:</p>&#13;
&#13;
<pre data-type="programlisting">Number of unique users: 1000&#13;
Number of unique movies: 1000&#13;
Number of total ratings: 90213&#13;
Average number of ratings per user: 90.213</pre>&#13;
&#13;
<p>Let’s generate a test set and a validation set from this reduced dataset so that each holdout set is 5% of the reduced dataset:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">ratingDFX3</code><code class="p">,</code>&#13;
 <code class="n">test_size</code><code class="o">=</code><code class="mf">0.10</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">2018</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_validation</code><code class="p">,</code> <code class="n">X_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code>&#13;
 <code class="n">test_size</code><code class="o">=</code><code class="mf">0.50</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">2018</code><code class="p">)</code></pre>&#13;
&#13;
<p>The following shows the sizes of the train, validation, and test sets:</p>&#13;
&#13;
<pre data-type="programlisting">Size of train set: 81191&#13;
Size of validation set: 4511&#13;
Size of test set: 4511</pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Define the Cost Function: Mean Squared Error" data-type="sect2"><div class="sect2" id="idm140637537183520">&#13;
<h2>Define the Cost Function: Mean Squared Error</h2>&#13;
&#13;
<p>Now we are ready to work with the data.</p>&#13;
&#13;
<p>First, let’s create a matrix <em>m</em> x <em>n</em>, where <em>m</em> are the users and <em>n</em> are the movies. This will be a sparsely populated matrix because users rate only a fraction of the movies. For example, a matrix with one thousand users and one thousand movies will have only 81,191 ratings in the training set. If each of the one thousand users rated each of the one thousand movies, we would have a matrix with one million ratings, but users rate only a small subset of movies  on average, so we have only 81,191 ratings on the training set. The rest (nearly 92% of the values in the matrix) will be zeros:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Generate ratings matrix for train</code>&#13;
<code class="n">ratings_train</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">((</code><code class="n">n_users</code><code class="p">,</code> <code class="n">n_movies</code><code class="p">))</code>&#13;
<code class="k">for</code> <code class="n">row</code> <code class="ow">in</code> <code class="n">X_train</code><code class="o">.</code><code class="n">itertuples</code><code class="p">():</code>&#13;
    <code class="n">ratings_train</code><code class="p">[</code><code class="n">row</code><code class="p">[</code><code class="mi">6</code><code class="p">]</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="n">row</code><code class="p">[</code><code class="mi">5</code><code class="p">]</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code> <code class="o">=</code> <code class="n">row</code><code class="p">[</code><code class="mi">3</code><code class="p">]</code>&#13;
&#13;
<code class="c1"># Calculate sparsity of the train ratings matrix</code>&#13;
<code class="n">sparsity</code> <code class="o">=</code> <code class="nb">float</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">ratings_train</code><code class="o">.</code><code class="n">nonzero</code><code class="p">()[</code><code class="mi">0</code><code class="p">]))</code>&#13;
<code class="n">sparsity</code> <code class="o">/=</code> <code class="p">(</code><code class="n">ratings_train</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">*</code> <code class="n">ratings_train</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code>&#13;
<code class="n">sparsity</code> <code class="o">*=</code> <code class="mi">100</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s1">'Sparsity: {:4.2f}%'</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">sparsity</code><code class="p">))</code></pre>&#13;
&#13;
<p>We will generate similar matrices for the validation set and the test set, which will be even sparser, of course:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Generate ratings matrix for validation</code>&#13;
<code class="n">ratings_validation</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">((</code><code class="n">n_users</code><code class="p">,</code> <code class="n">n_movies</code><code class="p">))</code>&#13;
<code class="k">for</code> <code class="n">row</code> <code class="ow">in</code> <code class="n">X_validation</code><code class="o">.</code><code class="n">itertuples</code><code class="p">():</code>&#13;
    <code class="n">ratings_validation</code><code class="p">[</code><code class="n">row</code><code class="p">[</code><code class="mi">6</code><code class="p">]</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="n">row</code><code class="p">[</code><code class="mi">5</code><code class="p">]</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code> <code class="o">=</code> <code class="n">row</code><code class="p">[</code><code class="mi">3</code><code class="p">]</code>&#13;
&#13;
<code class="c1"># Generate ratings matrix for test</code>&#13;
<code class="n">ratings_test</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">((</code><code class="n">n_users</code><code class="p">,</code> <code class="n">n_movies</code><code class="p">))</code>&#13;
<code class="k">for</code> <code class="n">row</code> <code class="ow">in</code> <code class="n">X_test</code><code class="o">.</code><code class="n">itertuples</code><code class="p">():</code>&#13;
    <code class="n">ratings_test</code><code class="p">[</code><code class="n">row</code><code class="p">[</code><code class="mi">6</code><code class="p">]</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="n">row</code><code class="p">[</code><code class="mi">5</code><code class="p">]</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code> <code class="o">=</code> <code class="n">row</code><code class="p">[</code><code class="mi">3</code><code class="p">]</code></pre>&#13;
&#13;
<p>Before we build our recommender systems, let’s define the cost function that we will use to judge the goodness of our model. We<a data-primary="mean squared error (MSE)" data-type="indexterm" id="idm140637535860816"/> will use <em>mean squared error (MSE)</em>, one of the simplest cost functions in machine learning. MSE measures the averaged squared error between the predicted values and the actual values. To calculate the MSE, we need two vectors of size <em>[n,1]</em>, where <em>n</em> is the number of ratings we are predicting—4,511 for the validation set. One vector has the actual ratings, and the other vector has the predictions.</p>&#13;
&#13;
<p>Let’s first flatten the sparse matrix with the ratings for the validation set. This will be the vector of actual ratings:</p>&#13;
&#13;
<pre data-type="programlisting">actual_validation = ratings_validation[ratings_validation.nonzero()].flatten()</pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Perform Baseline Experiments" data-type="sect2"><div class="sect2" id="idm140637535784672">&#13;
<h2>Perform Baseline Experiments</h2>&#13;
&#13;
<p>As a baseline, let’s predict an average rating of 3.5 for the validation set and calculate the MSE:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">pred_validation</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">((</code><code class="nb">len</code><code class="p">(</code><code class="n">X_validation</code><code class="p">),</code><code class="mi">1</code><code class="p">))</code>&#13;
<code class="n">pred_validation</code><code class="p">[</code><code class="n">pred_validation</code><code class="o">==</code><code class="mi">0</code><code class="p">]</code> <code class="o">=</code> <code class="mf">3.5</code>&#13;
<code class="n">pred_validation</code>&#13;
&#13;
<code class="n">mean_squared_error</code><code class="p">(</code><code class="n">pred_validation</code><code class="p">,</code> <code class="n">actual_validation</code><code class="p">)</code></pre>&#13;
&#13;
<p>The MSE of this very naive prediction is 1.05. This is our baseline:</p>&#13;
&#13;
<pre data-type="programlisting">Mean squared error using naive prediction: 1.055420084238528</pre>&#13;
&#13;
<p>Let’s see if we can improve our results by predicting a user’s rating for a given movie based on that user’s average rating for all other movies:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">ratings_validation_prediction</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">((</code><code class="n">n_users</code><code class="p">,</code> <code class="n">n_movies</code><code class="p">))</code>&#13;
<code class="n">i</code> <code class="o">=</code> <code class="mi">0</code>&#13;
<code class="k">for</code> <code class="n">row</code> <code class="ow">in</code> <code class="n">ratings_train</code><code class="p">:</code>&#13;
    <code class="n">ratings_validation_prediction</code><code class="p">[</code><code class="n">i</code><code class="p">][</code><code class="n">ratings_validation_prediction</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code class="o">==</code><code class="mi">0</code><code class="p">]</code> \&#13;
        <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">row</code><code class="p">[</code><code class="n">row</code><code class="o">&gt;</code><code class="mi">0</code><code class="p">])</code>&#13;
    <code class="n">i</code> <code class="o">+=</code> <code class="mi">1</code>&#13;
&#13;
<code class="n">pred_validation</code> <code class="o">=</code> <code class="n">ratings_validation_prediction</code> \&#13;
    <code class="p">[</code><code class="n">ratings_validation</code><code class="o">.</code><code class="n">nonzero</code><code class="p">()]</code><code class="o">.</code><code class="n">flatten</code><code class="p">()</code>&#13;
<code class="n">user_average</code> <code class="o">=</code> <code class="n">mean_squared_error</code><code class="p">(</code><code class="n">pred_validation</code><code class="p">,</code> <code class="n">actual_validation</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s1">'Mean squared error using user average:'</code><code class="p">,</code> <code class="n">user_average</code><code class="p">)</code></pre>&#13;
&#13;
<p>The MSE improves to 0.909:</p>&#13;
&#13;
<pre data-type="programlisting">Mean squared error using user average: 0.9090717929472647</pre>&#13;
&#13;
<p>Now, let’s predict a user’s rating for a given movie based on the average rating all other users have given that movie:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">ratings_validation_prediction</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">((</code><code class="n">n_users</code><code class="p">,</code> <code class="n">n_movies</code><code class="p">))</code><code class="o">.</code><code class="n">T</code>&#13;
<code class="n">i</code> <code class="o">=</code> <code class="mi">0</code>&#13;
<code class="k">for</code> <code class="n">row</code> <code class="ow">in</code> <code class="n">ratings_train</code><code class="o">.</code><code class="n">T</code><code class="p">:</code>&#13;
    <code class="n">ratings_validation_prediction</code><code class="p">[</code><code class="n">i</code><code class="p">][</code><code class="n">ratings_validation_prediction</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code class="o">==</code><code class="mi">0</code><code class="p">]</code> \&#13;
        <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">row</code><code class="p">[</code><code class="n">row</code><code class="o">&gt;</code><code class="mi">0</code><code class="p">])</code>&#13;
    <code class="n">i</code> <code class="o">+=</code> <code class="mi">1</code>&#13;
&#13;
<code class="n">ratings_validation_prediction</code> <code class="o">=</code> <code class="n">ratings_validation_prediction</code><code class="o">.</code><code class="n">T</code>&#13;
<code class="n">pred_validation</code> <code class="o">=</code> <code class="n">ratings_validation_prediction</code> \&#13;
    <code class="p">[</code><code class="n">ratings_validation</code><code class="o">.</code><code class="n">nonzero</code><code class="p">()]</code><code class="o">.</code><code class="n">flatten</code><code class="p">()</code>&#13;
<code class="n">movie_average</code> <code class="o">=</code> <code class="n">mean_squared_error</code><code class="p">(</code><code class="n">pred_validation</code><code class="p">,</code> <code class="n">actual_validation</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s1">'Mean squared error using movie average:'</code><code class="p">,</code> <code class="n">movie_average</code><code class="p">)</code></pre>&#13;
&#13;
<p>The<a data-primary="" data-startref="RBMmovie10" data-type="indexterm" id="idm140637535575456"/><a data-primary="" data-startref="movielens1" data-type="indexterm" id="idm140637535574608"/> MSE of this approach is 0.914, similar to that found using user average:</p>&#13;
&#13;
<pre data-type="programlisting">Mean squared error using movie average: 0.9136057106858655</pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Matrix Factorization" data-type="sect1"><div class="sect1" id="idm140637535784048">&#13;
<h1>Matrix Factorization</h1>&#13;
&#13;
<p>Before<a data-primary="matrix factorization" data-type="indexterm" id="matfact10"/><a data-primary="restricted Boltzmann machines (RBMs)" data-secondary="matrix factorization" data-type="indexterm" id="RBMmatrix10"/> we build a recommender system using RBMs, let’s first build one using <em>matrix factorization</em>, one of the most successful and popular collaborative filtering algorithms today. Matrix factorization decomposes the user-item matrix into a product of two lower dimensionality matrices. Users are represented in lower dimensional latent space, and so are the items.</p>&#13;
&#13;
<p>Assume our user-item matrix is <em>R</em>, with <em>m</em> users and <em>n</em> items. Matrix factorization will create two lower dimensionality matrices, <em>H</em> and <em>W</em>. <em>H</em> is an "<em>m</em> users” x "<em>k</em> latent factors” matrix, and <em>W</em> is a "<em>k</em> latent factors” x "<em>n</em> items” matrix.</p>&#13;
&#13;
<p>The ratings are computed by matrix multiplication: <em>R</em> = <em>H__W</em>.</p>&#13;
&#13;
<p>The number of <em>k</em> latent factors determines the capacity of the model. The higher the <em>k</em>, the greater the capacity of the model. By increasing <em>k</em>, we can improve the personalization of rating predictions for users, but, if <em>k</em> is too high, the model will overfit the data.</p>&#13;
&#13;
<p>All of this should be familiar to you. Matrix factorization learns representations for the users and items in a lower dimensional space and makes predictions based on the newly learned representations.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="One Latent Factor" data-type="sect2"><div class="sect2" id="idm140637535483536">&#13;
<h2>One Latent Factor</h2>&#13;
&#13;
<p>Let’s start with the simplest form of matrix factorization—with just one latent factor. We will use Keras to perform our matrix factorization.</p>&#13;
&#13;
<p>First, we need to define the graph. The input is the one-dimensional vector of users for the user embedding and the one-dimensional vector of movies for the movie embedding. We will embed these input vectors into a latent space of one and then flatten them. To generate the output vector <em>product</em>, we will take the dot product of the movie vector and user vector. We<a data-primary="algorithms" data-secondary="Adam optimization algorithm" data-type="indexterm" id="idm140637535480512"/><a data-primary="Adam optimization algorithm" data-type="indexterm" id="idm140637535479520"/> will use the <em>Adam optimizer</em> to minimize our cost fuction, which is defined as the <code>mean_squared_error</code>:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">n_latent_factors</code> <code class="o">=</code> <code class="mi">1</code>&#13;
&#13;
<code class="n">user_input</code> <code class="o">=</code> <code class="n">Input</code><code class="p">(</code><code class="n">shape</code><code class="o">=</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="n">name</code><code class="o">=</code><code class="s1">'user'</code><code class="p">)</code>&#13;
<code class="n">user_embedding</code> <code class="o">=</code> <code class="n">Embedding</code><code class="p">(</code><code class="n">input_dim</code><code class="o">=</code><code class="n">n_users</code> <code class="o">+</code> <code class="mi">1</code><code class="p">,</code> <code class="n">output_dim</code><code class="o">=</code><code class="n">n_latent_factors</code><code class="p">,</code>&#13;
 <code class="n">name</code><code class="o">=</code><code class="s1">'user_embedding'</code><code class="p">)(</code><code class="n">user_input</code><code class="p">)</code>&#13;
<code class="n">user_vec</code> <code class="o">=</code> <code class="n">Flatten</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s1">'flatten_users'</code><code class="p">)(</code><code class="n">user_embedding</code><code class="p">)</code>&#13;
&#13;
<code class="n">movie_input</code> <code class="o">=</code> <code class="n">Input</code><code class="p">(</code><code class="n">shape</code><code class="o">=</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="n">name</code><code class="o">=</code><code class="s1">'movie'</code><code class="p">)</code>&#13;
<code class="n">movie_embedding</code> <code class="o">=</code> <code class="n">Embedding</code><code class="p">(</code><code class="n">input_dim</code><code class="o">=</code><code class="n">n_movies</code> <code class="o">+</code> <code class="mi">1</code><code class="p">,</code> <code class="n">output_dim</code><code class="o">=</code><code class="n">n_latent_factors</code><code class="p">,</code>&#13;
 <code class="n">name</code><code class="o">=</code><code class="s1">'movie_embedding'</code><code class="p">)(</code><code class="n">movie_input</code><code class="p">)</code>&#13;
<code class="n">movie_vec</code> <code class="o">=</code> <code class="n">Flatten</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s1">'flatten_movies'</code><code class="p">)(</code><code class="n">movie_embedding</code><code class="p">)</code>&#13;
&#13;
<code class="n">product</code> <code class="o">=</code> <code class="n">dot</code><code class="p">([</code><code class="n">movie_vec</code><code class="p">,</code> <code class="n">user_vec</code><code class="p">],</code> <code class="n">axes</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">Model</code><code class="p">(</code><code class="n">inputs</code><code class="o">=</code><code class="p">[</code><code class="n">user_input</code><code class="p">,</code> <code class="n">movie_input</code><code class="p">],</code> <code class="n">outputs</code><code class="o">=</code><code class="n">product</code><code class="p">)</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="s1">'adam'</code><code class="p">,</code> <code class="s1">'mean_squared_error'</code><code class="p">)</code></pre>&#13;
&#13;
<p>Let’s train the model by feeding in the user and movie vectors from the training dataset. We will also evaluate the model on the validation set while we train. The MSE will be calculated against the actual ratings we have.</p>&#13;
&#13;
<p>We will train for one hundred epochs and store the history of the training and validation results. Let’s also plot the results:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">history</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="p">[</code><code class="n">X_train</code><code class="o">.</code><code class="n">newUserId</code><code class="p">,</code> <code class="n">X_train</code><code class="o">.</code><code class="n">newMovieId</code><code class="p">],</code> \&#13;
                    <code class="n">y</code><code class="o">=</code><code class="n">X_train</code><code class="o">.</code><code class="n">rating</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> \&#13;
                    <code class="n">validation_data</code><code class="o">=</code><code class="p">([</code><code class="n">X_validation</code><code class="o">.</code><code class="n">newUserId</code><code class="p">,</code> \&#13;
                    <code class="n">X_validation</code><code class="o">.</code><code class="n">newMovieId</code><code class="p">],</code> <code class="n">X_validation</code><code class="o">.</code><code class="n">rating</code><code class="p">),</code> \&#13;
                    <code class="n">verbose</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>&#13;
&#13;
<code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">history</code><code class="o">.</code><code class="n">history</code><code class="p">[</code><code class="s1">'val_loss'</code><code class="p">][</code><code class="mi">10</code><code class="p">:])</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">logy</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Epoch"</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Validation Error"</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s1">'Minimum MSE: '</code><code class="p">,</code> <code class="nb">min</code><code class="p">(</code><code class="n">history</code><code class="o">.</code><code class="n">history</code><code class="p">[</code><code class="s1">'val_loss'</code><code class="p">]))</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#plot_of_validation_mse_using_mf_and_one_latent_factor">Figure 10-1</a> shows the results.</p>&#13;
&#13;
<figure><div class="figure" id="plot_of_validation_mse_using_mf_and_one_latent_factor">&#13;
<img alt="Plot of Validation MSE using MF and One Latent Factor" src="assets/hulp_1001.png"/>&#13;
<h6><span class="label">Figure 10-1. </span>Plot of validation MSE using matrix factorization and one latent factor</h6>&#13;
</div></figure>&#13;
&#13;
<p>The minimum MSE using matrix factorization and one latent factor is 0.796. This is a better MSE than our user average and movie average approaches from earlier.</p>&#13;
&#13;
<p>Let’s see if we can do even better by increasing the number of latent factors (i.e., the capacity of the model).</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Three Latent Factors" data-type="sect2"><div class="sect2" id="idm140637535482912">&#13;
<h2>Three Latent Factors</h2>&#13;
&#13;
<p><a data-type="xref" href="#plot_of_validation_mse_using_mf_and_three_latent_factors">Figure 10-2</a> displays the results of using three latent factors.</p>&#13;
&#13;
<figure><div class="figure" id="plot_of_validation_mse_using_mf_and_three_latent_factors">&#13;
<img alt="Plot of Validation MSE using MF and Three Latent Factors" src="assets/hulp_1002.png"/>&#13;
<h6><span class="label">Figure 10-2. </span>Plot of validation MSE using matrix factorization and three latent factors</h6>&#13;
</div></figure>&#13;
&#13;
<p>The minimum MSE is 0.765, which is better than the one using one latent factor and the best yet.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Five Latent Factors" data-type="sect2"><div class="sect2" id="idm140637535065120">&#13;
<h2>Five Latent Factors</h2>&#13;
&#13;
<p>Let’s now build a matrix factorization model using five latent factors (see <a data-type="xref" href="#plot_of_validation_mse_using_mf_and_five_latent_factors">Figure 10-3</a> for the results).</p>&#13;
&#13;
<figure><div class="figure" id="plot_of_validation_mse_using_mf_and_five_latent_factors">&#13;
<img alt="Plot of Validation MSE using MF and Five Latent Factors" src="assets/hulp_1003.png"/>&#13;
<h6><span class="label">Figure 10-3. </span>Plot of validation MSE using matrix factorization and five latent factors</h6>&#13;
</div></figure>&#13;
&#13;
<p>The minimum MSE fails to improve, and there are clear signs of overfitting after the first 25 epochs or so. The validation error troughs and then begins to increase. Adding more capacity to the matrix factorization model will not help much more.<a data-primary="" data-startref="matfact10" data-type="indexterm" id="idm140637535060224"/><a data-primary="" data-startref="RBMmatrix10" data-type="indexterm" id="idm140637535059248"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Collaborative Filtering Using RBMs" data-type="sect1"><div class="sect1" id="idm140637535058048">&#13;
<h1>Collaborative Filtering Using RBMs</h1>&#13;
&#13;
<p>Let’s turn<a data-primary="restricted Boltzmann machines (RBMs)" data-secondary="collaborative filtering using" data-type="indexterm" id="RBMcollab10"/><a data-primary="collaborative filtering" data-type="indexterm" id="collab10"/> back to RBMs again. Recall that RBMs have two layers—the input/visible layer and the hidden layer. The neurons in each layer communicate with neurons in the other layer but not with neurons in the same layer. In other words, there is no intralayer communication among the neurons—this is the <em>restricted</em> bit of RBMs.</p>&#13;
&#13;
<p>Another important feature of RBMs is that the communication between layers happens in both directions—not just in one direction. For example, with autoencoders, the neurons communicate with the next layer, passing information only in a feedforward way.</p>&#13;
&#13;
<p>With RBMs, the neurons in the visible layer communicate with the hidden layer, and then the hidden layer passes back information to the visibile layer, going back and forth several times. RBMs perform this communication—the passes back and forth between the visible and hidden layer—to develop a generative model such that the reconstructions from the outputs of the hidden layer are similar to the original inputs.</p>&#13;
&#13;
<p>In other words, the RBMs are trying to create a generative model that will help predict whether a user will like a movie that the user has never seen based on how similar the movie is to other movies the user has rated and based on how similar the user is to the other users that have rated that movie.</p>&#13;
&#13;
<p>The visible layer will have X neurons, where X is the number of movies in the dataset. Each neuron will have a normalized rating value from zero to one, where zero means the user has not seen the movie. The closer the normalized rating value is to one, the more the user likes the movie represented by the neuron.</p>&#13;
&#13;
<p>The neurons in the visible layer will communicate with the neurons in the hidden layer, which will try to learn the underlying, latent features that characterize the user-movie preferences.</p>&#13;
&#13;
<p>Note that RBMs are also referred to as <em>symmetrical bipartite, bidirectional graphs</em>—symmetrical because each visible node is connected to each hidden node, bipartite because there are two layers of nodes, and bidirectional because the communication happens in both directions.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="RBM Neural Network Architecture" data-type="sect2"><div class="sect2" id="idm140637535048544">&#13;
<h2>RBM Neural Network Architecture</h2>&#13;
&#13;
<p>For our movie-recommender system, we have an <em>m</em> x <em>n</em> matrix with <em>m</em> users and <em>n</em> movies. To train the RBM, we pass along a batch of <em>k</em> users with their <em>n</em> movie ratings into the neural network and train for a certain number of <em>epochs</em>.</p>&#13;
&#13;
<p>Each input <em>x</em> that is passed into the neural network represents a single user’s rating preferences for all <em>n</em> movies, where <em>n</em> is one thousand in our example. Therefore, the visible layer has <em>n</em> nodes, one for each movie.</p>&#13;
&#13;
<p>We can specify the number of nodes in the hidden layer, which will generally be fewer than the nodes in the visible layer to force the hidden layer to learn the most salient aspects of the original input as efficiently as possible.</p>&#13;
&#13;
<p>Each input <em>v0</em> is multiplied by its respective weight <em>W</em>. The weights are learned by the connections from the visible layer to the hidden layer. Then we add a bias vector at the hidden layer called <em>hb</em>. The bias ensures that at least some of the neurons fire. This <em>W*v0+hb</em> result is passed through an activation function.</p>&#13;
&#13;
<p>After<a data-primary="Gibbs sampling" data-type="indexterm" id="idm140637535037904"/> this, we will take a sample of the outputs generated via a process known as <em>Gibbs sampling</em>. In other words, the activation of the hidden layer results in final outputs that are generated stochastically. This level of randomness helps build a better-performing and more robust generative model.</p>&#13;
&#13;
<p>Next, the output after Gibbs sampling—known as <em>h0</em>—is passed back through the neural network in the opposite direction in what is called<a data-primary="backward pass" data-type="indexterm" id="idm140637535253232"/> a <em>backward pass</em>. In the backward pass, the activations in the<a data-primary="forward pass" data-type="indexterm" id="idm140637535251920"/> <em>forward pass</em> after Gibbs sampling are fed into the hidden layer and multiplied by the same weights <em>W</em> as before. We then add a new bias vector at the visible layer called <em>vb</em>.</p>&#13;
&#13;
<p>This <em>W_h0+vb</em> is passed through an activation function, and then we perform Gibbs sampling. The output of this is <em>v1</em>, which is then passed as the new input into the visible layer and through the neural network as another forward pass.</p>&#13;
&#13;
<p>The RBM goes through a series of forward and backward passes like this to learn the optimal weights as it attempts to build a robust generative model. RBMs<a data-primary="generative learning" data-type="indexterm" id="idm140637535247520"/><a data-primary="probability distribution" data-type="indexterm" id="idm140637535246816"/> are the first type of <em>generative learning</em> model that we have explored. By performing Gibbs sampling and retraining weights via forward and backward passes, RBMs are trying to learn the <em>probability distribution</em> of the original input. Specifically, RBMs<a data-primary="Kullback–Leibler divergence" data-type="indexterm" id="idm140637535244992"/> minimize the <em>Kullback–Leibler divergence</em>, which measures how one probability distribution is different from another; in this case, RBMs are minimizing the probability distribution of the original input from the probability distribution of the reconstructed data.</p>&#13;
&#13;
<p>By iteratively readjusting the weights in the neural net, the RBM learns to approximate the original data as best as possible.</p>&#13;
&#13;
<p>With this newly learned probability distribution, RBMs are able to make predictions about never-before-seen data. In this case, the RBM we design will attempt to predict ratings for movies that the user has never seen based on the user’s similarity to other users and the ratings those movies have received by the other users.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Build the Components of the RBM Class" data-type="sect2"><div class="sect2" id="idm140637535047952">&#13;
<h2>Build the Components of the RBM Class</h2>&#13;
&#13;
<p>First, we will initialize the class with a few parameters; these are the input size of the RBM, the output size, the learning rate, the number of epochs to train for, and the batch size during the training process.</p>&#13;
&#13;
<p>We will also create zero matrices for the weight matrix, the hidden bias vector, and the visible bias vector:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Define RBM class</code>&#13;
<code class="k">class</code> <code class="nc">RBM</code><code class="p">(</code><code class="nb">object</code><code class="p">):</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">input_size</code><code class="p">,</code> <code class="n">output_size</code><code class="p">,</code>&#13;
                 <code class="n">learning_rate</code><code class="p">,</code> <code class="n">epochs</code><code class="p">,</code> <code class="n">batchsize</code><code class="p">):</code>&#13;
        <code class="c1"># Define hyperparameters</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">_input_size</code> <code class="o">=</code> <code class="n">input_size</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">_output_size</code> <code class="o">=</code> <code class="n">output_size</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">learning_rate</code> <code class="o">=</code> <code class="n">learning_rate</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">epochs</code> <code class="o">=</code> <code class="n">epochs</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">batchsize</code> <code class="o">=</code> <code class="n">batchsize</code>&#13;
&#13;
        <code class="c1"># Initialize weights and biases using zero matrices</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">w</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">([</code><code class="n">input_size</code><code class="p">,</code> <code class="n">output_size</code><code class="p">],</code> <code class="s2">"float"</code><code class="p">)</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">hb</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">([</code><code class="n">output_size</code><code class="p">],</code> <code class="s2">"float"</code><code class="p">)</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">vb</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">([</code><code class="n">input_size</code><code class="p">],</code> <code class="s2">"float"</code><code class="p">)</code></pre>&#13;
&#13;
<p>Next, let’s define functions for the forward pass, the backward pass, and the sampling of data during each of these passes back and forth.</p>&#13;
&#13;
<p>Here is the forward pass, where <em>h</em> is the hidden layer and <em>v</em> is the visible layer:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">prob_h_given_v</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">visible</code><code class="p">,</code> <code class="n">w</code><code class="p">,</code> <code class="n">hb</code><code class="p">):</code>&#13;
    <code class="k">return</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">visible</code><code class="p">,</code> <code class="n">w</code><code class="p">)</code> <code class="o">+</code> <code class="n">hb</code><code class="p">)</code></pre>&#13;
&#13;
<p>Here is the backward pass:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">prob_v_given_h</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">hidden</code><code class="p">,</code> <code class="n">w</code><code class="p">,</code> <code class="n">vb</code><code class="p">):</code>&#13;
    <code class="k">return</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">hidden</code><code class="p">,</code> <code class="n">tf</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="n">w</code><code class="p">))</code> <code class="o">+</code> <code class="n">vb</code><code class="p">)</code></pre>&#13;
&#13;
<p>Here is the sampling function:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">sample_prob</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">probs</code><code class="p">):</code>&#13;
    <code class="k">return</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">sign</code><code class="p">(</code><code class="n">probs</code> <code class="o">-</code> <code class="n">tf</code><code class="o">.</code><code class="n">random_uniform</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">shape</code><code class="p">(</code><code class="n">probs</code><code class="p">))))</code></pre>&#13;
&#13;
<p>Now we need a function that performs that training. Since we are using TensorFlow, we first need to create placeholders for the TensorFlow graph, which we will use when we feed data into the TensorFlow session.</p>&#13;
&#13;
<p>We will have placeholders for the weights matrix, the hidden bias vector, and the visible bias vector. We will also need to initialize the values for these three using zeros. And, we will need one set to hold the current values and one set to hold the previous values:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">_w</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">placeholder</code><code class="p">(</code><code class="s2">"float"</code><code class="p">,</code> <code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">_input_size</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">_output_size</code><code class="p">])</code>&#13;
<code class="n">_hb</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">placeholder</code><code class="p">(</code><code class="s2">"float"</code><code class="p">,</code> <code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">_output_size</code><code class="p">])</code>&#13;
<code class="n">_vb</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">placeholder</code><code class="p">(</code><code class="s2">"float"</code><code class="p">,</code> <code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">_input_size</code><code class="p">])</code>&#13;
&#13;
<code class="n">prv_w</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">([</code><code class="bp">self</code><code class="o">.</code><code class="n">_input_size</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">_output_size</code><code class="p">],</code> <code class="s2">"float"</code><code class="p">)</code>&#13;
<code class="n">prv_hb</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">([</code><code class="bp">self</code><code class="o">.</code><code class="n">_output_size</code><code class="p">],</code> <code class="s2">"float"</code><code class="p">)</code>&#13;
<code class="n">prv_vb</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">([</code><code class="bp">self</code><code class="o">.</code><code class="n">_input_size</code><code class="p">],</code> <code class="s2">"float"</code><code class="p">)</code>&#13;
&#13;
<code class="n">cur_w</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">([</code><code class="bp">self</code><code class="o">.</code><code class="n">_input_size</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">_output_size</code><code class="p">],</code> <code class="s2">"float"</code><code class="p">)</code>&#13;
<code class="n">cur_hb</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">([</code><code class="bp">self</code><code class="o">.</code><code class="n">_output_size</code><code class="p">],</code> <code class="s2">"float"</code><code class="p">)</code>&#13;
<code class="n">cur_vb</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">([</code><code class="bp">self</code><code class="o">.</code><code class="n">_input_size</code><code class="p">],</code> <code class="s2">"float"</code><code class="p">)</code></pre>&#13;
&#13;
<p>Likewise, we need a placeholder for the visible layer. The hidden layer is derived from matrix multiplication of the visible layer and the weights matrix and the matrix addition of the hidden bias vector:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">v0</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">placeholder</code><code class="p">(</code><code class="s2">"float"</code><code class="p">,</code> <code class="p">[</code><code class="bp">None</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">_input_size</code><code class="p">])</code>&#13;
<code class="n">h0</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">sample_prob</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">prob_h_given_v</code><code class="p">(</code><code class="n">v0</code><code class="p">,</code> <code class="n">_w</code><code class="p">,</code> <code class="n">_hb</code><code class="p">))</code></pre>&#13;
&#13;
<p>During the backward pass, we take the hidden layer output, multiply it with the transpose of the weights matrix used during the forward pass, and add the visible bias vector. Note that the weights matrix is the same during both the forward and the backward pass. Then, we perform the forward pass again:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">v1</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">sample_prob</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">prob_v_given_h</code><code class="p">(</code><code class="n">h0</code><code class="p">,</code> <code class="n">_w</code><code class="p">,</code> <code class="n">_vb</code><code class="p">))</code>&#13;
<code class="n">h1</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">prob_h_given_v</code><code class="p">(</code><code class="n">v1</code><code class="p">,</code> <code class="n">_w</code><code class="p">,</code> <code class="n">_hb</code><code class="p">)</code></pre>&#13;
&#13;
<p>To update the weights, we perform constrastive divergence.<sup><a data-type="noteref" href="ch10.html#idm140637534592544" id="idm140637534592544-marker">2</a></sup></p>&#13;
&#13;
<p>We also define the error as MSE.</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">positive_grad</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="n">v0</code><code class="p">),</code> <code class="n">h0</code><code class="p">)</code>&#13;
<code class="n">negative_grad</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="n">v1</code><code class="p">),</code> <code class="n">h1</code><code class="p">)</code>&#13;
&#13;
<code class="n">update_w</code> <code class="o">=</code> <code class="n">_w</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">learning_rate</code> <code class="o">*</code> \&#13;
    <code class="p">(</code><code class="n">positive_grad</code> <code class="o">-</code> <code class="n">negative_grad</code><code class="p">)</code> <code class="o">/</code> <code class="n">tf</code><code class="o">.</code><code class="n">to_float</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">shape</code><code class="p">(</code><code class="n">v0</code><code class="p">)[</code><code class="mi">0</code><code class="p">])</code>&#13;
<code class="n">update_vb</code> <code class="o">=</code> <code class="n">_vb</code> <code class="o">+</code>  <code class="bp">self</code><code class="o">.</code><code class="n">learning_rate</code> <code class="o">*</code> <code class="n">tf</code><code class="o">.</code><code class="n">reduce_mean</code><code class="p">(</code><code class="n">v0</code> <code class="o">-</code> <code class="n">v1</code><code class="p">,</code> <code class="mi">0</code><code class="p">)</code>&#13;
<code class="n">update_hb</code> <code class="o">=</code> <code class="n">_hb</code> <code class="o">+</code>  <code class="bp">self</code><code class="o">.</code><code class="n">learning_rate</code> <code class="o">*</code> <code class="n">tf</code><code class="o">.</code><code class="n">reduce_mean</code><code class="p">(</code><code class="n">h0</code> <code class="o">-</code> <code class="n">h1</code><code class="p">,</code> <code class="mi">0</code><code class="p">)</code>&#13;
&#13;
<code class="n">err</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">reduce_mean</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">square</code><code class="p">(</code><code class="n">v0</code> <code class="o">-</code> <code class="n">v1</code><code class="p">))</code></pre>&#13;
&#13;
<p>With this, we are ready to initialize the TensorFlow session with the variables we have just defined.</p>&#13;
&#13;
<p>Once we call <em>sess.run</em>, we can feed in batches of data to begin the training. During the training, forward and backward passes will be made, and the RBM will update weights based on how the generated data compares to the original input. We will print the reconstruction error from each epoch.</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">with</code> <code class="n">tf</code><code class="o">.</code><code class="n">Session</code><code class="p">()</code> <code class="k">as</code> <code class="n">sess</code><code class="p">:</code>&#13;
 <code class="n">sess</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">global_variables_initializer</code><code class="p">())</code>&#13;
&#13;
 <code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">epochs</code><code class="p">):</code>&#13;
     <code class="k">for</code> <code class="n">start</code><code class="p">,</code> <code class="n">end</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">X</code><code class="p">),</code>&#13;
      <code class="bp">self</code><code class="o">.</code><code class="n">batchsize</code><code class="p">),</code><code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">batchsize</code><code class="p">,</code><code class="nb">len</code><code class="p">(</code><code class="n">X</code><code class="p">),</code> <code class="bp">self</code><code class="o">.</code><code class="n">batchsize</code><code class="p">)):</code>&#13;
         <code class="n">batch</code> <code class="o">=</code> <code class="n">X</code><code class="p">[</code><code class="n">start</code><code class="p">:</code><code class="n">end</code><code class="p">]</code>&#13;
         <code class="n">cur_w</code> <code class="o">=</code> <code class="n">sess</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">update_w</code><code class="p">,</code> <code class="n">feed_dict</code><code class="o">=</code><code class="p">{</code><code class="n">v0</code><code class="p">:</code> <code class="n">batch</code><code class="p">,</code>&#13;
          <code class="n">_w</code><code class="p">:</code> <code class="n">prv_w</code><code class="p">,</code> <code class="n">_hb</code><code class="p">:</code> <code class="n">prv_hb</code><code class="p">,</code> <code class="n">_vb</code><code class="p">:</code> <code class="n">prv_vb</code><code class="p">})</code>&#13;
         <code class="n">cur_hb</code> <code class="o">=</code> <code class="n">sess</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">update_hb</code><code class="p">,</code> <code class="n">feed_dict</code><code class="o">=</code><code class="p">{</code><code class="n">v0</code><code class="p">:</code> <code class="n">batch</code><code class="p">,</code>&#13;
          <code class="n">_w</code><code class="p">:</code> <code class="n">prv_w</code><code class="p">,</code> <code class="n">_hb</code><code class="p">:</code> <code class="n">prv_hb</code><code class="p">,</code> <code class="n">_vb</code><code class="p">:</code> <code class="n">prv_vb</code><code class="p">})</code>&#13;
         <code class="n">cur_vb</code> <code class="o">=</code> <code class="n">sess</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">update_vb</code><code class="p">,</code> <code class="n">feed_dict</code><code class="o">=</code><code class="p">{</code><code class="n">v0</code><code class="p">:</code> <code class="n">batch</code><code class="p">,</code>&#13;
          <code class="n">_w</code><code class="p">:</code> <code class="n">prv_w</code><code class="p">,</code> <code class="n">_hb</code><code class="p">:</code> <code class="n">prv_hb</code><code class="p">,</code> <code class="n">_vb</code><code class="p">:</code> <code class="n">prv_vb</code><code class="p">})</code>&#13;
         <code class="n">prv_w</code> <code class="o">=</code> <code class="n">cur_w</code>&#13;
         <code class="n">prv_hb</code> <code class="o">=</code> <code class="n">cur_hb</code>&#13;
         <code class="n">prv_vb</code> <code class="o">=</code> <code class="n">cur_vb</code>&#13;
     <code class="n">error</code> <code class="o">=</code> <code class="n">sess</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">err</code><code class="p">,</code> <code class="n">feed_dict</code><code class="o">=</code><code class="p">{</code><code class="n">v0</code><code class="p">:</code> <code class="n">X</code><code class="p">,</code>&#13;
      <code class="n">_w</code><code class="p">:</code> <code class="n">cur_w</code><code class="p">,</code> <code class="n">_vb</code><code class="p">:</code> <code class="n">cur_vb</code><code class="p">,</code> <code class="n">_hb</code><code class="p">:</code> <code class="n">cur_hb</code><code class="p">})</code>&#13;
     <code class="k">print</code> <code class="p">(</code><code class="s1">'Epoch: </code><code class="si">%d</code><code class="s1">'</code> <code class="o">%</code> <code class="n">epoch</code><code class="p">,</code><code class="s1">'reconstruction error: </code><code class="si">%f</code><code class="s1">'</code> <code class="o">%</code> <code class="n">error</code><code class="p">)</code>&#13;
 <code class="bp">self</code><code class="o">.</code><code class="n">w</code> <code class="o">=</code> <code class="n">prv_w</code>&#13;
 <code class="bp">self</code><code class="o">.</code><code class="n">hb</code> <code class="o">=</code> <code class="n">prv_hb</code>&#13;
 <code class="bp">self</code><code class="o">.</code><code class="n">vb</code> <code class="o">=</code> <code class="n">prv_vb</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Train RBM Recommender System" data-type="sect2"><div class="sect2" id="idm140637535241392">&#13;
<h2>Train RBM Recommender System</h2>&#13;
&#13;
<p>To train the RBM, let’s create a NumPy array called <code>inputX</code> from <code>ratings_train</code> and convert these values to float32. We will also define the RBM to take in a one thousand-dimensional input, output a one thousand-dimensional output, use a learning rate of 0.3, train for five hundred epochs, and use a batch size of two hundred. These parameters are just preliminary parameter choices; you should be able to find more optimal parameters with experimentation, which is encouraged:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Begin the training cycle</code>&#13;
&#13;
<code class="c1"># Convert inputX into float32</code>&#13;
<code class="n">inputX</code> <code class="o">=</code> <code class="n">ratings_train</code>&#13;
<code class="n">inputX</code> <code class="o">=</code> <code class="n">inputX</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">float32</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Define the parameters of the RBMs we will train</code>&#13;
<code class="n">rbm</code><code class="o">=</code><code class="n">RBM</code><code class="p">(</code><code class="mi">1000</code><code class="p">,</code><code class="mi">1000</code><code class="p">,</code><code class="mf">0.3</code><code class="p">,</code><code class="mi">500</code><code class="p">,</code><code class="mi">200</code><code class="p">)</code></pre>&#13;
&#13;
<p>Let’s begin training:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">rbm</code><code class="o">.</code><code class="n">train</code><code class="p">(</code><code class="n">inputX</code><code class="p">)</code>&#13;
<code class="n">outputX</code><code class="p">,</code> <code class="n">reconstructedX</code><code class="p">,</code> <code class="n">hiddenX</code> <code class="o">=</code> <code class="n">rbm</code><code class="o">.</code><code class="n">rbm_output</code><code class="p">(</code><code class="n">inputX</code><code class="p">)</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#plot_of_rvm_errors">Figure 10-4</a> displays the plot of the reconstruction errors.</p>&#13;
&#13;
<figure><div class="figure" id="plot_of_rvm_errors">&#13;
<img alt="Plot of RBM Errors" src="assets/hulp_1004.png"/>&#13;
<h6><span class="label">Figure 10-4. </span>Plot of RBM errors</h6>&#13;
</div></figure>&#13;
&#13;
<p>The error terms generally decrease the longer we train.</p>&#13;
&#13;
<p>Now let’s take the RBM model we developed to predict the ratings for users in the validation set (which has the same users as the training set):</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Predict ratings for validation set</code>&#13;
<code class="n">inputValidation</code> <code class="o">=</code> <code class="n">ratings_validation</code>&#13;
<code class="n">inputValidation</code> <code class="o">=</code> <code class="n">inputValidation</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">float32</code><code class="p">)</code>&#13;
&#13;
<code class="n">finalOutput_validation</code><code class="p">,</code> <code class="n">reconstructedOutput_validation</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> \&#13;
    <code class="n">rbm</code><code class="o">.</code><code class="n">rbm_output</code><code class="p">(</code><code class="n">inputValidation</code><code class="p">)</code></pre>&#13;
&#13;
<p>Next, let’s convert the predictions into an array and calculate the MSE against the true validation ratings:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">predictionsArray</code> <code class="o">=</code> <code class="n">reconstructedOutput_validation</code>&#13;
<code class="n">pred_validation</code> <code class="o">=</code> \&#13;
    <code class="n">predictionsArray</code><code class="p">[</code><code class="n">ratings_validation</code><code class="o">.</code><code class="n">nonzero</code><code class="p">()]</code><code class="o">.</code><code class="n">flatten</code><code class="p">()</code>&#13;
<code class="n">actual_validation</code> <code class="o">=</code> \&#13;
    <code class="n">ratings_validation</code><code class="p">[</code><code class="n">ratings_validation</code><code class="o">.</code><code class="n">nonzero</code><code class="p">()]</code><code class="o">.</code><code class="n">flatten</code><code class="p">()</code>&#13;
&#13;
<code class="n">rbm_prediction</code> <code class="o">=</code> <code class="n">mean_squared_error</code><code class="p">(</code><code class="n">pred_validation</code><code class="p">,</code> <code class="n">actual_validation</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s1">'Mean squared error using RBM prediction:'</code><code class="p">,</code> <code class="n">rbm_prediction</code><code class="p">)</code></pre>&#13;
&#13;
<p>The following code displays the MSE on the validation set:</p>&#13;
&#13;
<pre data-type="programlisting">Mean squared error using RBM prediction: 9.331135003325205</pre>&#13;
&#13;
<p>This MSE is a starting point and will likely improve with greater experimentation.<a data-primary="" data-startref="collab10" data-type="indexterm" id="idm140637533957616"/><a data-primary="" data-startref="RBMcollab10" data-type="indexterm" id="idm140637533956736"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before" data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="idm140637535057584">&#13;
<h1>Conclusion</h1>&#13;
&#13;
<p>In this chapter, we explored restricted Boltzmann machines and used them to build a recommender system for movie ratings. The RBM recommender we built learned the probability distribution of ratings of movies for users given their previous ratings and the ratings of users to which they were most similar to. We then used the learned probability distribution to predict ratings on never-before-seen movies.</p>&#13;
&#13;
<p>In <a data-type="xref" href="ch11.html#Chapter_11">Chapter 11</a>, we will stack RBMs together to build deep belief networks and use them to perform even more powerful unsupervised learning tasks.<a data-primary="" data-startref="UDLrec10" data-type="indexterm" id="idm140637533952496"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm140637537216896"><sup><a href="ch10.html#idm140637537216896-marker">1</a></sup> The most common training algorithm for this class of RBMs is known as the gradient-based contrastive divergence algorithm.</p><p data-type="footnote" id="idm140637534592544"><sup><a href="ch10.html#idm140637534592544-marker">2</a></sup> For more on this topic, see the paper <a href="http://bit.ly/2RukFuX">“On Contrastive Divergence Learning”</a>.</p></div></div></section></body></html>