- en: Chapter 7\. Autoencoders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。自编码器
- en: The first six chapters of this book explored how to use unsupervised learning
    to perform dimensionality reduction and clustering, and the concepts we covered
    helped us build applications to detect anomalies and segment groups based on similarity.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的前六章探讨了如何利用无监督学习进行降维和聚类，我们讨论的概念帮助我们构建了检测异常和基于相似性分割群组的应用程序。
- en: However, unsupervised learning is capable of a lot more. One area that unsupervised
    learning excels in is *feature extraction*, which is a method used to generate
    a new feature representation from an original set of features; the new feature
    representation is called a *learned representation* and is used to improve performance
    on supervised learning problems. In other words, feature extraction is an unsupervised
    means to a supervised end.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，无监督学习能够做的远不止这些。无监督学习在特征提取方面表现出色，特征提取是一种从原始特征集生成新特征表示的方法；新的特征表示称为*学习表示*，并用于提高监督学习问题的性能。换句话说，特征提取是无监督学习到监督学习的手段。
- en: Autoencoders are one such form of feature extraction. They use a *feedforward,
    nonrecurrent neural network* to perform *representation learning*. Representation
    learning is a core part of an entire branch of machine learning involving neural
    networks.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是特征提取的一种形式。它们使用*前馈、非递归神经网络*执行*表示学习*。表示学习是涉及神经网络的整个机器学习分支的核心部分。
- en: In autoencoders—which are a form of representation learning—each layer of the
    neural network learns a representation of the original features, and subsequent
    layers build on the representation learned by the preceding layers. Layer by layer,
    the autoencoder learns increasingly complicated representations from simpler ones,
    building what is known as a hierarchy of concepts that become more and more abstract.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在自编码器中——它们是一种表示学习的形式——神经网络的每一层学习原始特征的表示，后续层基于前面层学到的表示进行构建。逐层递进，自编码器从简单的表示学习逐步建立更为复杂的表示，形成所谓的层次概念，并且这些概念变得越来越抽象。
- en: The output layer is the final newly learned representation of the original features.
    This learned representation can then be used as input into a supervised learning
    model with the objective of improving the generalization error.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层是原始特征的最终新学习表示。然后，可以将这种学习表示用作监督学习模型的输入，以改进泛化误差。
- en: But before we get too far ahead of ourselves, let’s begin by introducing neural
    networks and the Python frameworks TensorFlow and Keras.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 但在我们过多深入之前，让我们先介绍神经网络以及Python框架TensorFlow和Keras。
- en: Neural Networks
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络
- en: At their very essence, neural networks perform representation learning, where
    each layer of the neural network learns a representation from the previous layer.
    By building more nuanced and detailed representations layer by layer, neural networks
    can accomplish pretty amazing tasks such as computer vision, speech recognition,
    and machine translation.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在其根本上，神经网络执行表示学习，即神经网络的每一层从前一层学习到一个表示。通过逐层构建更加细致和详细的表示，神经网络可以完成非常惊人的任务，如计算机视觉、语音识别和机器翻译。
- en: Neural networks come in two forms—shallow and deep. Shallow networks have few
    layers, and deep networks have many layers. Deep learning gets its name from the
    deep (many-layered) neural networks it deploys. Shallow neural networks are not
    particularly powerful since the degree of representation learning is limited by
    the low number of layers. Deep learning, on the other hand, is incredibly powerful
    and is currently one of the hottest areas in machine learning.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络有两种形式——浅层和深层。浅层网络有少量层，而深层网络有许多层。深度学习因其使用深度（多层）神经网络而得名。浅层神经网络并不特别强大，因为表示学习的程度受到层次较少的限制。另一方面，深度学习非常强大，目前是机器学习中最热门的领域之一。
- en: To be clear, shallow and deep learning using neural networks are just a part
    of the entire machine learning ecosystem. The major difference between machine
    learning using neural networks and classical machine learning is that a lot of
    the feature representation is automatically performed in the neural networks case
    and is hand-designed in classical machine learning.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 明确一点，使用神经网络进行浅层和深层学习只是整个机器学习生态系统的一部分。使用神经网络和传统机器学习之间的主要区别在于，神经网络自动执行了大部分特征表示，而在传统机器学习中则是手动设计的。
- en: Neural networks have an *input layer*, one or many *hidden layers*, and an *output
    layer*. The number of hidden layers defines just how *deep* the neural network
    is. You can view these hidden layers as intermediate computations; these hidden
    layers together allow the entire neural network to perform complex function approximation.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络具有*输入层*、一个或多个*隐藏层*和一个*输出层*。隐藏层的数量定义了神经网络的*深度*。您可以将这些隐藏层视为中间计算；这些隐藏层共同允许整个神经网络执行复杂的函数逼近。
- en: Each layer has a certain number of *nodes* (also known as *neurons* or *units*)
    that comprise the layer. The nodes of each layer are then connected to the nodes
    of the next layer. During the training process, the neural network determines
    the optimal weights to assign to each node.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 每个层次有一定数量的*节点*（也称为*神经元*或*单元*）组成该层。然后，每层的节点连接到下一层的节点。在训练过程中，神经网络确定分配给每个节点的最佳权重。
- en: In addition to adding more layers, we can add more nodes to a neural network
    to increase the capacity of the neural network to model complex relationships.
    These nodes are fed into an *activation function*, which determines what value
    of the current layer is fed into the next layer of the neural network. Common
    activation functions include *linear*, *sigmoid*, *hyperbolic tangent*, and *rectified
    linear unit (ReLU)* activation functions. The final activation function is usually
    the *softmax function*, which outputs a class probability that the input observation
    falls in. This is pretty typical for classification type problems.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 除了增加更多的层次外，我们还可以向神经网络添加更多节点，以增加神经网络模拟复杂关系的能力。这些节点被输入到一个*激活函数*中，该函数决定了当前层的值被馈送到神经网络的下一层。常见的激活函数包括*线性*、*sigmoid*、*双曲正切*和*修正线性单元（ReLU）*激活函数。最终的激活函数通常是*softmax函数*，它输出输入观察值属于某个类的概率。这对于分类问题非常典型。
- en: Neural networks may also have *bias nodes*; these nodes are always constant
    values and, unlike the normal nodes, are not connected to the previous layer.
    Rather, they allow the output of an activation function to be shifted lower or
    higher. With the hidden layers—including the nodes, bias nodes, and activation
    functions—the neural network is trying to learn the right function approximation
    to use to map the input layer to the output layer.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可能还包括*偏置节点*；这些节点始终是常量值，并且与前一层的节点不连接。相反，它们允许激活函数的输出向上或向下偏移。通过隐藏层（包括节点、偏置节点和激活函数），神经网络试图学习正确的函数逼近，以便将输入层映射到输出层。
- en: In the case of supervised learning problems, this is pretty straightforward.
    The input layer represents the features that are fed into the neural network,
    and the output layer represents the label assigned to each observation. During
    the training process, the neural network determines which *weights* across the
    neural network help minimize the error between its predicted label for each observation
    and the true label. In unsupervised learning problems, the neural network learns
    representations of the input layer via the various hidden layers but is not guided
    by labels.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习问题中，这相当直观。输入层表示馈送到神经网络的特征，输出层表示分配给每个观察的标签。在训练过程中，神经网络确定了在整个神经网络中哪些*权重*有助于最小化每个观察的预测标签与真实标签之间的误差。在无监督学习问题中，神经网络通过各个隐藏层学习输入层的表示，但不受标签的指导。
- en: Neural networks are incredibly powerful and are capable of modeling complex
    nonlinear relationships to a degree that classicial machine learning algorithms
    struggle with. In general, this is a great characteristic of neural networks,
    but there is a potential risk. Because neural networks can model such complex
    nonlinear relationships, they are also much more prone to overfitting, which we
    should be aware of and address when designing machine learning applications using
    neural networks.^([1](ch07.html#idm140637542452288))
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络非常强大，能够模拟复杂的非线性关系，这是传统机器学习算法难以处理的。总体来说，这是神经网络的一个伟大特性，但也存在潜在风险。因为神经网络能够建模如此复杂的非线性关系，它们也更容易过拟合，这是在设计使用神经网络的机器学习应用时需要注意和解决的问题。^([1](ch07.html#idm140637542452288))
- en: 'Although there are multiple types of neural networks such as *recurrent neural
    networks* in which data can flow in any direction (used for speech recognition
    and machine translation) and *convolutional neural networks* (used for computer
    vision), we will focus on the more straightforward feedforward neural network
    in which data moves in just one direction: forward.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有多种类型的神经网络，比如*递归神经网络*（数据可以在任何方向上流动，用于语音识别和机器翻译）和*卷积神经网络*（用于计算机视觉），我们将专注于更为直接的前馈神经网络，其中数据仅向一个方向移动：向前。
- en: We also must perform a lot more hyperparameter optimization to get neural networks
    to perform well—including the choice of the cost function, the algorithm to minimize
    the loss, the type of initialization for the starting weights, the number of iterations
    to use to train the neural network (i.e., number of epochs), the number of observations
    to feed in before each weight update (i.e., batch size), and the step size to
    move the weights in (i.e., learning rate) during the training process.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须进行更多的超参数优化，以使神经网络表现良好——包括选择成本函数、用于最小化损失的算法、起始权重的初始化类型、用于训练神经网络的迭代次数（即周期数）、每次权重更新前要喂入的观察次数（即批量大小）以及在训练过程中移动权重的步长（即学习率）。
- en: TensorFlow
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow
- en: Before we introduce autoencoders, let’s explore *TensorFlow*, the primary library
    we will use to build neural networks. TensorFlow is an open source software library
    for high-performance numerical computation and was initially developed by the
    Google Brain team for internal Google use. In November 2015, it was released as
    open source software.^([2](ch07.html#idm140637542440736))
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍自动编码器之前，让我们先探索一下*TensorFlow*，这是我们用来构建神经网络的主要库。TensorFlow 是一个开源软件库，用于高性能数值计算，最初由
    Google Brain 团队为内部使用开发。在2015年11月，它作为开源软件发布。^([2](ch07.html#idm140637542440736))
- en: TensorFlow is available across many operating systems (including Linux, macOS,
    Windows, Android, and iOS) and can run on multiple CPUs and GPUs, making the software
    very scalable for fast performance and deployable to most users across desktop,
    mobile, web, and cloud.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 可在许多操作系统上使用（包括 Linux、macOS、Windows、Android 和 iOS），并且可以在多个 CPU 和 GPU
    上运行，使得软件在快速性能方面非常具有可扩展性，并且可以部署到桌面、移动、网络和云端用户。
- en: The beauty of TensorFlow is that users can define a neural network—or, more
    generally, a graph of computations—in Python, and can take the neural network
    and run it using C++ code, which is much faster than Python.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 的美妙之处在于用户可以在 Python 中定义神经网络——或者更普遍地说，定义计算图——然后使用 C++ 代码运行这个神经网络，这比
    Python 快得多。
- en: TensorFlow is also able to *parallelize* the computations, breaking down the
    entire series of operations into separate chunks and running them in parallel
    across multiple CPUs and GPUs. Performance like this is a very important consideration
    for large-scale machine learning applications like those that Google runs for
    its core operations such as search.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 还能够*并行化*计算，将整个操作序列分解为多个部分，并在多个 CPU 和 GPU 上并行运行。对于像 Google 为其核心操作（如搜索）运行的大规模机器学习应用程序来说，这样的性能非常重要。
- en: While there are other open source libraries capable of similar feats, TensorFlow
    has become the most popular, partly due to Google’s brand.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有其他能够实现类似功能的开源库，TensorFlow 已经成为最受欢迎的一个，部分原因是 Google 的品牌。
- en: TensorFlow example
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow 示例
- en: 'Before we move ahead, let’s build a TensorFlow graph and run a computation.
    We will import TensorFlow, define a few variables using the TensorFlow API (which
    resembles the Scikit-Learn API we’ve used in previous chapters), and then compute
    the values for those variables:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们建立一个 TensorFlow 计算图并运行一个计算。我们将导入 TensorFlow，使用 TensorFlow API 定义几个变量（类似于我们在之前章节中使用的
    Scikit-Learn API），然后计算这些变量的值：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: It is important to realize that there are two phases here. First, we construct
    the computation graph, defining b, x, and y. Then, we execute the graph by calling
    `tf.Session()`. Until we call this, no computations are being executed by the
    CPU and/or GPU. Rather, only the instructions for the computations are being stored.
    Once you execute this block of code, you will see the result of “550” as expected.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 很重要的一点是，这里有两个阶段。首先，我们构建计算图，定义了 b、x 和 y。然后，通过调用 `tf.Session()` 执行计算图。在调用之前，CPU
    和/或 GPU 不会执行任何计算。而是仅仅存储计算的指令。执行此代码块后，您将如预期看到结果为“550”。
- en: Later on, we will build actual neural networks using TensorFlow.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 后面，我们将使用 TensorFlow 构建实际的神经网络。
- en: Keras
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Keras
- en: Keras is an open source software library and provides a high-level API that
    runs on top of TensorFlow. It provides a much more user-friendly interface for
    TensorFlow, allowing data scientists and researchers to experiment faster and
    more easily than if they had to work directly with the TensorFlow commands. Keras
    was also primarily authored by a Google engineer, Francois Chollet.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Keras是一个开源软件库，提供在TensorFlow之上运行的高级API。它为TensorFlow提供了一个更加用户友好的接口，使数据科学家和研究人员能够比直接使用TensorFlow命令更快速、更轻松地进行实验。Keras的主要作者也是一位Google工程师，弗朗索瓦·朱勒。
- en: When we start building models using TensorFlow, we will work hands-on with Keras
    and explore its advantages.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始使用TensorFlow构建模型时，我们将亲自动手使用Keras并探索其优势。
- en: 'Autoencoder: The Encoder and the Decoder'
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器：编码器和解码器
- en: Now that we’ve introduced neural networks and the popular libraries to work
    with them in Python—TensorFlow and Keras—let’s build an autoencoder, one of the
    simplest unsupervised learning neural networks.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了神经网络及其在Python中的流行库——TensorFlow和Keras，让我们来构建一个自编码器，这是最简单的无监督学习神经网络之一。
- en: An autoencoder comprises two parts, an *encoder* and a *decoder*. The encoder
    converts the input set of features into a different representation—via representation
    learning—and the decoder converts this newly learned representation to the original
    format.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器包括两部分，一个*编码器*和一个*解码器*。编码器将输入的特征集通过表示学习转换为不同的表示，解码器将这个新学到的表示转换回原始格式。
- en: The core concept of an autoencoder is similar to the concept of dimensionality
    reduction we studied in [Chapter 3](ch03.html#Chapter_3). Similar to dimensionality
    reduction, an autoencoder does not memorize the original observations and features,
    which would be what is known as the *identity function*. If it learned the exact
    identity function, the autoencoder would not be useful. Rather, autoencoders must
    approximate the original observations as closely as possible—but not exactly—using
    a newly learned representation; in other words, the autoencoder learns an approximation
    of the identity function.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器的核心概念与我们在[第三章](ch03.html#Chapter_3)中学习的降维概念类似。类似于降维，自编码器不会记忆原始观察和特征，这将是所谓的*恒等函数*。如果它学到了确切的恒等函数，那么自编码器就没有用处。相反，自编码器必须尽可能接近但不完全复制原始观察，使用新学到的表示；换句话说，自编码器学习了恒等函数的近似。
- en: Since the autoencoder is constrained, it is forced to learn the most salient
    properties of the original data, capturing the underlying structure of the data;
    this is similar to what happens in dimensionality reduction. The constraint is
    a very important attribute of autoencoders—the constraint forces the autoencoder
    to intelligently choose which important information to capture and which irrelevant
    or less important information to discard.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于自编码器受到约束，它被迫学习原始数据的最显著特性，捕获数据的基础结构；这与降维中发生的情况类似。约束是自编码器的一个非常重要的属性——约束迫使自编码器智能地选择要捕获的重要信息和要丢弃的不相关或较不重要的信息。
- en: Autoencoders have been around for decades, and, as you may suspect already,
    they have been used widely for dimensionality reduction and automatic feature
    engineering/learning. Today, they are often used to build *generative models*
    such as *generative adversarial networks*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器已经存在几十年了，你可能已经怀疑它们已广泛用于降维和自动特征工程/学习。如今，它们经常用于构建*生成模型*，例如*生成对抗网络*。
- en: Undercomplete Autoencoders
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不完全自编码器
- en: In the autoencoder, we care most about the encoder because this component is
    the one that learns a new representation of the original data. This new representation
    is the new set of features derived from the original set of features and observations.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在自编码器中，我们最关心的是编码器，因为这个组件是学习原始数据新表示的组件。这个新表示是从原始特征和观察得到的新特征集。
- en: We will refer to the encoder function of the autoencoder as *h = f(x)*, which
    takes in the original observations *x* and uses the newly learned representation
    captured in function *f* to output *h*. The decoder function that reconstructs
    the original observations using the encoder function is *r = g(h)*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将自编码器的编码器函数称为*h = f(x)*，它接收原始观察*x*并使用函数*f*中捕获的新学到的表示输出*h*。解码器函数使用编码器函数重建原始观察，其形式为*r
    = g(h)*。
- en: As you can see, the decoder function feeds in the encoder’s output *h* and reconstructs
    the observations, known as *r*, using its reconstruction function *g*. If done
    correctly, *g(f(x))* will not be exactly equal to *x* everywhere but will be close
    enough.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: How do we restrict the encoder function to approximate *x* so that it is forced
    to learn only the most salient properties of *x* without copying it exactly?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: We can constrain the encoder function’s output, *h*, to have fewer dimensions
    than *x*. This is known as an *undercomplete* autoencoder since the encoder’s
    dimensions are fewer than the original input dimensions. This is again similar
    to what happens in dimensionality reduction, where we take in the original input
    dimensions and reduce them to a much smaller set.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Constrained in this manner, the autoencoder attempts to minimize a *loss function*
    we define such that the reconstruction error—after the decoder reconstructs the
    observations approximately using the encoder’s output—is as small as possible.
    It is important to realize that the hidden layers are where the dimensions are
    constrained. In other words, the output of the encoder has fewer dimensions than
    the original input. But the output of the decoder is the reconstructed original
    data and, therefore, has the same number of dimensions as the original input.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: When the decoder is linear and the loss function is the mean squared error,
    an undercomplete autoencoder learns the same sort of new representation as PCA,
    a form of dimensionality reduction we introduced in [Chapter 3](ch03.html#Chapter_3).
    However, if the encoder and decoder functions are nonlinear, the autoencoder can
    learn much more complex nonlinear representations. This is what we care about
    most. But be warned—if the autoencoder is given too much capacity and latitude
    to model complex, nonlinear representations, it will simply memorize/copy the
    original observations instead of extracting the most salient information from
    them. Therefore, we must restrict the autoencoder meaningfully enough to prevent
    this from happening.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Overcomplete Autoencoders
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the encoder learns a representation in a greater number of dimensions than
    the original input dimensions, the autoencoder is considered *overcomplete*. Such
    autoencoders simply copy the original observations and are not forced to efficiently
    and compactly capture information about the original distribution in a way that
    undercomplete autoencoders are. That being said, if we employ some form of *regularization*,
    which penalizes the neural network for learning unnecessarily complex functions,
    overcomplete autoencoders can be used successfully for dimensionality reduction
    and automatic feature engineering.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Compared to undercomplete autoeconders, *regularized overcomplete autoencoders*
    are harder to design successfully but are potentially more powerful because they
    can learn more complex—but not overly complex—representations that better approximate
    the original observations without copying them precisely.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 与欠完备自编码器相比，*正则化超完备自编码器*更难成功设计，但可能更强大，因为它们可以学习到更复杂但不过度复杂的表示，从而更好地近似原始观察结果而不是精确复制它们。
- en: In a nutshell, autoencoders that perform well are those that learn new representations
    that approximate the original obsevations close enough but not exactly. To do
    this, the autoencoder essentially learns a new probability distribution.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，表现良好的自编码器是那些学习到新表示，这些表示足够接近原始观察结果但并非完全相同的自编码器。为了做到这一点，自编码器本质上学习了一个新的概率分布。
- en: Dense vs. Sparse Autoencoders
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 密集自编码器 vs. 稀疏自编码器
- en: If you recall, in [Chapter 3](ch03.html#Chapter_3) we had both dense (the normal)
    and sparse versions of dimensionality reduction algorithms. Autoencoders work
    similarly. So far, we’ve discussed just the normal autoencoder that outputs a
    dense final matrix such that a handful of features have the most salient information
    that has been captured about the original data. However, we may instead want to
    output a sparse final matrix such that the information captured is more well-distributed
    across the features that the autoencoder learns.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得，在[第三章](ch03.html#Chapter_3)中，我们有密集（正常）和稀疏版本的降维算法。自编码器的工作原理类似。到目前为止，我们只讨论了输出密集最终矩阵的普通自编码器，以便少数特征具有有关原始数据的最显著信息。然而，我们可能希望输出一个稀疏的最终矩阵，以便捕获的信息更好地分布在自编码器学习到的特征之间。
- en: To do this, we need to include not just a *reconstruction error* as part of
    the autoencoder but also a *sparsity penalty* so that the autoencoder must take
    the sparsity of the final matrix into consideration. Sparse autoencoders are generally
    overcomplete—the hidden layers have more units than the number of input features
    with the caveat that only a small fraction of the hidden units are allowed to
    be active at the same time. When defined in this way, a *sparse autoencoder* will
    output a final matrix that has many more zeros embedded throughout and the information
    captured will be better distributed across the features learned.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们需要在自编码器中包括不仅作为一部分的*重构误差*，还要包括*稀疏惩罚*，以便自编码器必须考虑最终矩阵的稀疏性。稀疏自编码器通常是超完备的——隐藏层的单元数比输入特征的数量多，但只有很小一部分隐藏单元被允许同时处于活动状态。这样定义的*稀疏自编码器*将输出一个具有更多零值的最终矩阵，所捕获的信息将更好地分布在学习到的特征中。
- en: For certain machine learning applications, sparse autoencoders have better performance
    and also learn somewhat different representations than the normal (dense) autoencoders
    would. Later, we will work with real examples to see the difference between these
    two types of autoencoders.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些机器学习应用，稀疏自编码器具有更好的性能，并且学习到的表示也与正常（密集）自编码器略有不同。稍后，我们将使用真实示例来看看这两种类型的自编码器之间的区别。
- en: Denoising Autoencoder
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 去噪自编码器
- en: As you know by now, autoencoders are capable of learning new (and improved)
    representations from the original input data, capturing the most salient elements
    but disregarding the noise in the original data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你现在所知，自编码器能够从原始输入数据中学习新的（并且改进的）表示，捕获最显著的元素，但忽略原始数据中的噪音。
- en: In some cases, we may want the autoencoder we design to more aggressively ignore
    the noise in the data, especially if we suspect the original data is corrupted
    to some degree. Imagine recording a conversation between two people at a noisy
    coffee shop in the middle of the day. We would want to isolate the conversation
    (the signal) from the background chatter (the noise). Or, imagine a dataset of
    images that are grainy or distorted due to low resolution or some blurring effect.
    We want to isolate the core image (the signal) from the distortion (the noise).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们可能希望设计的自编码器更积极地忽略数据中的噪声，特别是如果我们怀疑原始数据在某种程度上被损坏。想象一下在白天嘈杂的咖啡店里记录两个人之间的对话。我们希望将对话（信号）与背景嘈杂声（噪音）隔离开来。又或者，想象一下由于低分辨率或某种模糊效果而导致图像有颗粒感或失真的数据集。我们希望将核心图像（信号）与失真（噪音）隔离开来。
- en: For these problems, we can design a *denoising autoencoder* that receives the
    corrupted data as input and is trained to output the original, uncorrupted data
    as best as possible. Of course, while this is not easy to do, this is clearly
    a very powerful application of autoencoders to solve real-world problems.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这些问题，我们可以设计一个*去噪自编码器*，它接收损坏的数据作为输入，并训练以尽可能地输出原始未损坏的数据。当然，尽管这并不容易，但这显然是自编码器应用于解决现实问题的一个非常强大的应用。
- en: Variational Autoencoder
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变分自编码器
- en: So far, we have discussed the use of autoencoders to learn new representations
    of the original input data (via the encoder) to minimize the reconstruction error
    between the newly reconstructed data (via the decoder) and the original input
    data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了使用自编码器来学习原始输入数据的新表示（通过编码器），以最小化新重构数据（通过解码器）与原始输入数据之间的重构误差。
- en: In these examples, the encoder is of a fixed size, *n*, where *n* is typically
    smaller than the number of original dimensions—in other words, we train an undercomplete
    autoencoder. Or *n* may be larger than the number of original dimensions—an overcomplete
    autoencoder—but constrained using a regularization penalty, a sparsity penalty,
    etc. But in all these cases, the encoder outputs a single vector of a fixed size
    *n*.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些示例中，编码器的大小是固定的，为*n*，其中*n*通常比原始维度的数量小——换句话说，我们训练了一个欠完备自编码器。或者*n*可能大于原始维度的数量——一个过完备自编码器——但通过使用正则化惩罚、稀疏性惩罚等进行约束。但在所有这些情况下，编码器输出一个固定大小为*n*的单个向量。
- en: 'An alternative autoencoder known as the *variational autoencoder* has an encoder
    that outputs two vectors instead of one: a vector of means, *mu*, and a vector
    of standard deviations, *sigma*. These two vectors form random variables such
    that the *ith* element of *mu* and *sigma* corresponds to the *mean* and *standard
    deviation* of the *ith* random variable. By forming this stochastic output via
    its encoder, the variational autoencoder is able to sample across a continuous
    space based on what it has learned from the input data.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一种替代的自编码器被称为*变分自编码器*，其编码器输出两个向量而不是一个：一个均值向量*mu*和一个标准差向量*sigma*。这两个向量形成随机变量，使得*mu*和*sigma*的第*i*个元素对应于第*i*个随机变量的*均值*和*标准差*。通过编码器形成这种随机输出，变分自编码器能够基于其从输入数据中学到的知识在连续空间中进行采样。
- en: The variational autoencoder is not confined to just the examples it has trained
    on but can generalize and output new examples even if it may have never seen precisely
    similar ones before. This is incredibly powerful because now the variational autoencoders
    can generate new synthetic data that appears to belong in the distribution the
    variational autoencoder has learned from the original input data. Advances like
    this have led to an entirely new and trending field in unsupervised learning known
    as generative modeling, which includes *generative adversarial networks*. With
    these models, it is possible to generate synthetic images, speech, music, art,
    etc., opening up a world of possibilities for AI-generated data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自编码器不仅局限于它训练过的示例，还可以进行泛化并输出新的示例，即使它可能以前从未见过完全相似的示例。这非常强大，因为现在变分自编码器可以生成看起来属于从原始输入数据学习的分布中的新合成数据。像这样的进展导致了一个完全新的和趋势的无监督学习领域，被称为生成建模，其中包括*生成对抗网络*。使用这些模型，可以生成合成图像、语音、音乐、艺术等，为AI生成数据开辟了无限可能。
- en: Conclusion
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, we introduced neural networks and the popular open source libraries,
    TensorFlow and Keras. We also explored autoencoders and their ability to learn
    new representations from original input data. Variations include sparse autoencoders,
    denoising autoencoders, and variational autoencoders, among others.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了神经网络及其流行的开源库TensorFlow和Keras。我们还探讨了自编码器及其从原始输入数据学习新表示的能力。变种包括稀疏自编码器、去噪自编码器和变分自编码器，等等。
- en: In [Chapter 8](ch08.html#Chapter_8), we will build hands-on applications using
    the techniques we have discussed in this chapter.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第8章](ch08.html#Chapter_8)中，我们将使用本章讨论的技术构建实际应用程序。
- en: Before we proceed, let’s revisit why automatic feature extraction is so important.
    Without the ability to automatically extract features, data scientists and machine
    learning engineers would have to design by hand features that might be important
    in solving real-world problems. This is very time-consuming and would dramatically
    limit progress in the field of AI.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们重新思考一下为什么自动特征提取如此重要。如果没有自动提取特征的能力，数据科学家和机器学习工程师将不得不手工设计可能在解决现实世界问题中重要的特征。这是非常耗时的，并且会极大地限制人工智能领域的进展。
- en: In fact, until Geoffrey Hinton and other researchers developed methods to automatically
    learn new features using neural networks—launching the deep learning revolution
    starting in 2006—problems involving computer vision, speech recognition, machine
    translation, etc., remained largely intractable.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，直到Geoffrey Hinton和其他研究人员开发出使用神经网络自动学习新特征的方法——从2006年开始引发了深度学习革命——涉及计算机视觉、语音识别、机器翻译等问题一直大多数难以解决。
- en: Once autoencoders and other variations of neural networks were used to automatically
    extract features from input data, a lot of these problems became solvable, leading
    to some major breakthroughs in machine learning over the past decade.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦自动编码器和其他神经网络变种被用来自动从输入数据中提取特征，许多这些问题就变得可以解决，导致过去十年间机器学习领域的一些重大突破。
- en: You will see the power of automatic feature extraction in the hands-on application
    of autoencoders in [Chapter 8](ch08.html#Chapter_8).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在[第8章](ch08.html#Chapter_8)中的自动编码器的实际应用中，你将看到自动特征提取的力量。
- en: ^([1](ch07.html#idm140637542452288-marker)) This process is known as regularization.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch07.html#idm140637542452288-marker)) 这个过程被称为正则化。
- en: ^([2](ch07.html#idm140637542440736-marker)) For more on TensorFlow, consult
    the [website](https://www.tensorflow.org/).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch07.html#idm140637542440736-marker)) 欲了解更多有关TensorFlow的信息，请查阅[网站](https://www.tensorflow.org/)。
