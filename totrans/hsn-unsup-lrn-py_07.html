<html><head></head><body><section data-pdf-bookmark="Chapter 5. Clustering" data-type="chapter" epub:type="chapter"><div class="chapter" id="Chapter_5">&#13;
<h1><span class="label">Chapter 5. </span>Clustering</h1>&#13;
&#13;
&#13;
<p>In <a data-type="xref" href="ch03.html#Chapter_3">Chapter 3</a>, we introduced the most important dimensionality reduction algorithms in unsupervised learning and highlighted their ability to densely capture information. In <a data-type="xref" href="ch04.html#Chapter_4">Chapter 4</a>, we used the dimensionality reduction algorithms to build an anomaly detection system. Specifically, we applied these algorithms to detect credit card fraud without using any labels. These algorithms learned the underlying structure in the credit card transactions. Then, we separated the normal transactions from the rare, potentially fraudulent ones based on the reconstruction error.</p>&#13;
&#13;
<p>In<a data-primary="clustering" data-secondary="goal of" data-type="indexterm" id="idm140637547510352"/> this chapter, we will build on these unsupervised learning concepts by introducing <em>clustering</em>, which attempts to group objects together based on similarity. Clustering<a data-primary="clustering" data-secondary="process of" data-type="indexterm" id="idm140637547508736"/> achieves this without using any labels, comparing how similar the data for one observation is to data for other observations and groups.</p>&#13;
&#13;
<p>Clustering<a data-primary="clustering" data-secondary="applications for" data-type="indexterm" id="idm140637547629472"/> has many applications. For example, in credit card fraud detection, clustering can group fraudulent transactions together, separating them from normal transactions. Or, if we had only a few labels for the observations in our dataset, we could use clustering to group the observations first (without using labels). Then, we could transfer the labels of the few labeled observations to the rest of the observations within the same group. This<a data-primary="transfer learning" data-type="indexterm" id="idm140637547627872"/> is a form of <em>transfer learning</em>, a rapidly growing field in machine learning.</p>&#13;
&#13;
<p>In<a data-primary="recommender systems" data-type="indexterm" id="idm140637547626240"/> areas such as online and retail shopping, marketing, social media, recommender systems for movies, music, books, dating, etc., clustering can group similar people together based on their behavior. Once these groups are established, business users will have better insight into their user base and can craft targeted business strategies for each of the distinct groups.</p>&#13;
&#13;
<p>As we did with dimensionality reduction, let’s introduce the concepts first in this chapter, and then we will build an applied unsupervised learning solution in the next chapter.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="MNIST Digits Dataset" data-type="sect1"><div class="sect1" id="idm140637547624288">&#13;
<h1>MNIST Digits Dataset</h1>&#13;
&#13;
<p>To<a data-primary="MNIST digits database" data-type="indexterm" id="idm140637547622960"/><a data-primary="clustering" data-secondary="MNIST digits database" data-type="indexterm" id="idm140637547622224"/> keep things simple, we will continue to work with the MNIST image dataset of digits that we introduced in <a data-type="xref" href="ch03.html#Chapter_3">Chapter 3</a>.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Data Preparation" data-type="sect2"><div class="sect2" id="idm140637547620096">&#13;
<h2>Data Preparation</h2>&#13;
&#13;
<p>Let’s first load the necessary libraries:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import libraries</code>&#13;
<code class="sd">'''Main'''</code>&#13;
<code class="kn">import</code> <code class="nn">numpy</code> <code class="kn">as</code> <code class="nn">np</code>&#13;
<code class="kn">import</code> <code class="nn">pandas</code> <code class="kn">as</code> <code class="nn">pd</code>&#13;
<code class="kn">import</code> <code class="nn">os</code><code class="o">,</code> <code class="nn">time</code>&#13;
<code class="kn">import</code> <code class="nn">pickle</code><code class="o">,</code> <code class="nn">gzip</code>&#13;
&#13;
<code class="sd">'''Data Viz'''</code>&#13;
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="kn">as</code> <code class="nn">plt</code>&#13;
<code class="kn">import</code> <code class="nn">seaborn</code> <code class="kn">as</code> <code class="nn">sns</code>&#13;
<code class="n">color</code> <code class="o">=</code> <code class="n">sns</code><code class="o">.</code><code class="n">color_palette</code><code class="p">()</code>&#13;
<code class="kn">import</code> <code class="nn">matplotlib</code> <code class="kn">as</code> <code class="nn">mpl</code>&#13;
&#13;
<code class="o">%</code><code class="n">matplotlib</code> <code class="n">inline</code>&#13;
&#13;
<code class="sd">'''Data Prep and Model Evaluation'''</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">preprocessing</code> <code class="k">as</code> <code class="n">pp</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">precision_recall_curve</code><code class="p">,</code> <code class="n">average_precision_score</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">roc_curve</code><code class="p">,</code> <code class="n">auc</code><code class="p">,</code> <code class="n">roc_auc_score</code></pre>&#13;
&#13;
<p>Next, let’s load the dataset and create Pandas DataFrames:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load the datasets</code>&#13;
<code class="n">current_path</code> <code class="o">=</code> <code class="n">os</code><code class="o">.</code><code class="n">getcwd</code><code class="p">()</code>&#13;
<code class="nb">file</code> <code class="o">=</code> <code class="s1">'</code><code class="se">\\</code><code class="s1">datasets</code><code class="se">\\</code><code class="s1">mnist_data</code><code class="se">\\</code><code class="s1">mnist.pkl.gz'</code>&#13;
&#13;
<code class="n">f</code> <code class="o">=</code> <code class="n">gzip</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">current_path</code><code class="o">+</code><code class="nb">file</code><code class="p">,</code> <code class="s1">'rb'</code><code class="p">)</code>&#13;
<code class="n">train_set</code><code class="p">,</code> <code class="n">validation_set</code><code class="p">,</code> <code class="n">test_set</code> <code class="o">=</code> <code class="n">pickle</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="n">f</code><code class="p">,</code> <code class="n">encoding</code><code class="o">=</code><code class="s1">'latin1'</code><code class="p">)</code>&#13;
<code class="n">f</code><code class="o">.</code><code class="n">close</code><code class="p">()</code>&#13;
&#13;
<code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code> <code class="o">=</code> <code class="n">train_set</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">train_set</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>&#13;
<code class="n">X_validation</code><code class="p">,</code> <code class="n">y_validation</code> <code class="o">=</code> <code class="n">validation_set</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">validation_set</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>&#13;
<code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">test_set</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">test_set</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>&#13;
&#13;
<code class="c1"># Create Pandas DataFrames from the datasets</code>&#13;
<code class="n">train_index</code> <code class="o">=</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code><code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">))</code>&#13;
<code class="n">validation_index</code> <code class="o">=</code> <code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">),</code> \&#13;
                         <code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code><code class="o">+</code><code class="nb">len</code><code class="p">(</code><code class="n">X_validation</code><code class="p">))</code>&#13;
<code class="n">test_index</code> <code class="o">=</code> <code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code><code class="o">+</code><code class="nb">len</code><code class="p">(</code><code class="n">X_validation</code><code class="p">),</code> \&#13;
                   <code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code><code class="o">+</code><code class="nb">len</code><code class="p">(</code><code class="n">X_validation</code><code class="p">)</code><code class="o">+</code><code class="nb">len</code><code class="p">(</code><code class="n">X_test</code><code class="p">))</code>&#13;
&#13;
<code class="n">X_train</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">train_index</code><code class="p">)</code>&#13;
<code class="n">y_train</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">y_train</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">train_index</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_validation</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_validation</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">validation_index</code><code class="p">)</code>&#13;
<code class="n">y_validation</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">y_validation</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">validation_index</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_test</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_test</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">test_index</code><code class="p">)</code>&#13;
<code class="n">y_test</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">y_test</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">test_index</code><code class="p">)</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Clustering Algorithms" data-type="sect1"><div class="sect1" id="idm140637547463888">&#13;
<h1>Clustering Algorithms</h1>&#13;
&#13;
<p>Before<a data-primary="clustering" data-secondary="algorithms for" data-type="indexterm" id="idm140637547462512"/><a data-primary="algorithms" data-secondary="for clustering" data-type="indexterm" id="idm140637547280512"/> we perform clustering, we will reduce the dimensionality of the data using PCA. As shown in <a data-type="xref" href="ch03.html#Chapter_3">Chapter 3</a>, dimensionality reduction algorithms capture the salient information in the original data while reducing the size of the dataset.</p>&#13;
&#13;
<p>As we move from a high number of dimensions to a lower number, the noise in the dataset is minimized because the dimensionality reduction algorithm (PCA, in this case) needs to capture the most important aspects of the original data and cannot devote attention to infrequently occurring elements (such as the noise in the dataset).</p>&#13;
&#13;
<p>Recall that dimensionality reduction algorithms are very powerful in learning the underlying structure in data. In <a data-type="xref" href="ch03.html#Chapter_3">Chapter 3</a>, we showed that it was possible to meaningfully separate the MNIST images based on the digits they displayed using just two dimensions after dimensionality reduction.</p>&#13;
&#13;
<p>Let’s apply PCA to the MNIST dataset again:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Principal Component Analysis</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">PCA</code>&#13;
&#13;
<code class="n">n_components</code> <code class="o">=</code> <code class="mi">784</code>&#13;
<code class="n">whiten</code> <code class="o">=</code> <code class="bp">False</code>&#13;
<code class="n">random_state</code> <code class="o">=</code> <code class="mi">2018</code>&#13;
&#13;
<code class="n">pca</code> <code class="o">=</code> <code class="n">PCA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="n">n_components</code><code class="p">,</code> <code class="n">whiten</code><code class="o">=</code><code class="n">whiten</code><code class="p">,</code> \&#13;
          <code class="n">random_state</code><code class="o">=</code><code class="n">random_state</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_train_PCA</code> <code class="o">=</code> <code class="n">pca</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>&#13;
<code class="n">X_train_PCA</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train_PCA</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">train_index</code><code class="p">)</code></pre>&#13;
&#13;
<p>Although we did not reduce the dimensionality, we will designate the number of principal components we will use during the clustering stage, effectively reducing the dimensionality.</p>&#13;
&#13;
<p>Now let’s move to clustering. The three major clustering algorithms are <em>k-means</em>, <em>hierarchical clustering</em>, and <em>DBSCAN</em>. We will introduce and explore each now.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="k-Means" data-type="sect1"><div class="sect1" id="idm140637547065616">&#13;
<h1>k-Means</h1>&#13;
&#13;
<p>The<a data-primary="clustering" data-secondary="k-means" data-type="indexterm" id="Ckmean05"/><a data-primary="Scikit-learn" data-secondary="clustering using" data-type="indexterm" id="SCLcluster05"/><a data-primary="k-means clustering" data-secondary="goal of" data-type="indexterm" id="idm140637547061488"/> objective of clustering is to identify distinct groups in a dataset such that the observations within a group are similar to each other but different from observations in other groups. In <em>k</em>-means clustering, we specify the number of desired clusters <em>k</em>, and the algorithm will assign each observation to exactly one of these <em>k</em> clusters. The<a data-primary="within-cluster variation" data-type="indexterm" id="idm140637547058992"/><a data-primary="inertia" data-type="indexterm" id="idm140637547058288"/> algorithm optimizes the groups by minimizing the <em>within-cluster variation</em> (also known as <em>inertia</em>) such that the sum of the within-cluster variations across all <em>k</em> clusters is as small as possible.</p>&#13;
&#13;
<p>Different runs of <em>k</em>-means will result in slightly different cluster assignments because <em>k</em>-means randomly assigns each observation to one of the <em>k</em> clusters to kick off the clustering process. <em>k</em>-means does this random initialization to speed up the clustering process. After<a data-primary="centroids" data-type="indexterm" id="idm140637547053936"/> this random initialization, <em>k</em>-means reassigns the observations to <span class="keep-together">different</span> clusters as it attempts to minimize the Euclidean distance between each observation and its cluster’s center point, or <em>centroid</em>. This random initialization is a source of randomness, resulting in slightly different clustering assignments, from one <em>k</em>-means run to another.</p>&#13;
&#13;
<p>Typically, the <em>k</em>-means algorithm does several runs and chooses the run that has the best separation, defined as the lowest total sum of within-cluster variations across all <em>k</em> clusters.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="k-Means Inertia" data-type="sect2"><div class="sect2" id="idm140637547049424">&#13;
<h2>k-Means Inertia</h2>&#13;
&#13;
<p>Let’s introduce<a data-primary="k-means clustering" data-secondary="k-means inertia" data-type="indexterm" id="idm140637547047600"/> the algorithm. We need to set the number of clusters we would like (<code>n_clusters</code>), the number of initializations we would like to perform (<code>n_init</code>), the maximum number of iterations the algorithm will run to reassign observations to minimize inertia (<code>max_iter</code>), and the tolerance to declare convergence (<code>tol</code>).</p>&#13;
&#13;
<p>We will keep the default values for number of initializations (10), maximum number of iterations (300), and tolerance (0.0001). Also, for now, we will use the first 100 principal components from PCA (<code>cutoff</code>). To test how the number of clusters we designate affects the inertia measure, let’s run <em>k</em>-means for cluster sizes 2 through 20 and record the inertia for each.</p>&#13;
&#13;
<p>Here is the code:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># k-means - Inertia as the number of clusters varies</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.cluster</code> <code class="kn">import</code> <code class="n">KMeans</code>&#13;
&#13;
<code class="n">n_clusters</code> <code class="o">=</code> <code class="mi">10</code>&#13;
<code class="n">n_init</code> <code class="o">=</code> <code class="mi">10</code>&#13;
<code class="n">max_iter</code> <code class="o">=</code> <code class="mi">300</code>&#13;
<code class="n">tol</code> <code class="o">=</code> <code class="mf">0.0001</code>&#13;
<code class="n">random_state</code> <code class="o">=</code> <code class="mi">2018</code>&#13;
<code class="n">n_jobs</code> <code class="o">=</code> <code class="mi">2</code>&#13;
&#13;
<code class="n">kMeans_inertia</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="p">[],</code><code class="n">index</code><code class="o">=</code><code class="nb">range</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code><code class="mi">21</code><code class="p">),</code> \&#13;
                              <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'inertia'</code><code class="p">])</code>&#13;
<code class="k">for</code> <code class="n">n_clusters</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code><code class="mi">21</code><code class="p">):</code>&#13;
    <code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="n">n_clusters</code><code class="p">,</code> <code class="n">n_init</code><code class="o">=</code><code class="n">n_init</code><code class="p">,</code> \&#13;
                <code class="n">max_iter</code><code class="o">=</code><code class="n">max_iter</code><code class="p">,</code> <code class="n">tol</code><code class="o">=</code><code class="n">tol</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="n">random_state</code><code class="p">,</code> \&#13;
                <code class="n">n_jobs</code><code class="o">=</code><code class="n">n_jobs</code><code class="p">)</code>&#13;
&#13;
    <code class="n">cutoff</code> <code class="o">=</code> <code class="mi">99</code>&#13;
    <code class="n">kmeans</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_PCA</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code><code class="mi">0</code><code class="p">:</code><code class="n">cutoff</code><code class="p">])</code>&#13;
    <code class="n">kMeans_inertia</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">n_clusters</code><code class="p">]</code> <code class="o">=</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">inertia_</code></pre>&#13;
&#13;
<p>As <a data-type="xref" href="#k_means_inertia_for_cluster_sizes_2_through_20">Figure 5-1</a> shows, the inertia decreases as the number of clusters increases. This makes sense. The more clusters we have, the greater the homogeneity among observations within each cluster. However, fewer clusters are easier to work with than more, so finding the right number of clusters to generate is an important consideration when running <em>k</em>-means.</p>&#13;
&#13;
<figure><div class="figure" id="k_means_inertia_for_cluster_sizes_2_through_20">&#13;
<img alt="k-Means Inertia for Cluster Sizes 2 through 20" src="assets/hulp_0501.png"/>&#13;
<h6><span class="label">Figure 5-1. </span>k-means inertia for cluster sizes 2 through 20</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Evaluating the Clustering Results" data-type="sect2"><div class="sect2" id="idm140637547231440">&#13;
<h2>Evaluating the Clustering Results</h2>&#13;
&#13;
<p>To<a data-primary="k-means clustering" data-secondary="evaluating clustering results" data-type="indexterm" id="idm140637547154560"/> demonstrate how <em>k</em>-means works and how increasing the number of clusters results in more homogeneous clusters, let’s define a function to analyze the results of each experiment we do. The cluster assignments—generated by the clustering algorithm—will be stored in a Pandas DataFrame called <code>clusterDF</code>.</p>&#13;
&#13;
<p>Let’s count the number of observations in each cluster and store these in a Pandas DataFrame called <code>countByCluster</code>:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">analyzeCluster</code><code class="p">(</code><code class="n">clusterDF</code><code class="p">,</code> <code class="n">labelsDF</code><code class="p">):</code>&#13;
    <code class="n">countByCluster</code> <code class="o">=</code> \&#13;
        <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">clusterDF</code><code class="p">[</code><code class="s1">'cluster'</code><code class="p">]</code><code class="o">.</code><code class="n">value_counts</code><code class="p">())</code>&#13;
    <code class="n">countByCluster</code><code class="o">.</code><code class="n">reset_index</code><code class="p">(</code><code class="n">inplace</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code><code class="n">drop</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code>&#13;
    <code class="n">countByCluster</code><code class="o">.</code><code class="n">columns</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'cluster'</code><code class="p">,</code><code class="s1">'clusterCount'</code><code class="p">]</code></pre>&#13;
&#13;
<p>Next, let’s join the <code>clusterDF</code> with the true labels array, which we will call <code>labelsDF</code>:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">    <code class="n">preds</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">([</code><code class="n">labelsDF</code><code class="p">,</code><code class="n">clusterDF</code><code class="p">],</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>&#13;
    <code class="n">preds</code><code class="o">.</code><code class="n">columns</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'trueLabel'</code><code class="p">,</code><code class="s1">'cluster'</code><code class="p">]</code></pre>&#13;
&#13;
<p>Let’s also count the number of observations for each true label in the training set (this won’t change but is good for us to know):</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">    <code class="n">countByLabel</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">preds</code><code class="o">.</code><code class="n">groupby</code><code class="p">(</code><code class="s1">'trueLabel'</code><code class="p">)</code><code class="o">.</code><code class="n">count</code><code class="p">())</code></pre>&#13;
&#13;
<p>Now, for each cluster, we will count the number of observations for each distinct label within a cluster. For example, if a given cluster has three thousand observations, two thousand may represent the number two, five hundred may represent the number one, three hundred may represent the number zero, and the remaining two hundred may represent the number nine.</p>&#13;
&#13;
<p>Once we calculate these, we will store the count for the most frequently occurring number for each cluster. In the example above, we would store a count of two thousand for this cluster:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">    <code class="n">countMostFreq</code> <code class="o">=</code> \&#13;
        <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">preds</code><code class="o">.</code><code class="n">groupby</code><code class="p">(</code><code class="s1">'cluster'</code><code class="p">)</code><code class="o">.</code><code class="n">agg</code><code class="p">(</code> \&#13;
                        <code class="k">lambda</code> <code class="n">x</code><code class="p">:</code><code class="n">x</code><code class="o">.</code><code class="n">value_counts</code><code class="p">()</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="mi">0</code><code class="p">]))</code>&#13;
    <code class="n">countMostFreq</code><code class="o">.</code><code class="n">reset_index</code><code class="p">(</code><code class="n">inplace</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code><code class="n">drop</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code>&#13;
    <code class="n">countMostFreq</code><code class="o">.</code><code class="n">columns</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'cluster'</code><code class="p">,</code><code class="s1">'countMostFrequent'</code><code class="p">]</code></pre>&#13;
&#13;
<p>Finally, we will judge the success of each clustering run based on how tightly grouped the observations are within each cluster. For example, in the example above, the cluster has two thousand observations that have the same label out of a total of three thousand observations in the cluster.</p>&#13;
&#13;
<p>This cluster is not great since we ideally want to group similar observations together in the same cluster and exclude dissimilar ones.</p>&#13;
&#13;
<p>Let’s define the overall accuracy of the clustering as the sum of the counts of the most frequently occuring observations across all the clusters divided by the total number of observations in the training set (i.e., 50,000):</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">    <code class="n">accuracyDF</code> <code class="o">=</code> <code class="n">countMostFreq</code><code class="o">.</code><code class="n">merge</code><code class="p">(</code><code class="n">countByCluster</code><code class="p">,</code> \&#13;
                        <code class="n">left_on</code><code class="o">=</code><code class="s2">"cluster"</code><code class="p">,</code><code class="n">right_on</code><code class="o">=</code><code class="s2">"cluster"</code><code class="p">)</code>&#13;
    <code class="n">overallAccuracy</code> <code class="o">=</code> <code class="n">accuracyDF</code><code class="o">.</code><code class="n">countMostFrequent</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code><code class="o">/</code> \&#13;
                        <code class="n">accuracyDF</code><code class="o">.</code><code class="n">clusterCount</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code></pre>&#13;
&#13;
<p>We can also assess the accuracy by cluster:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">    <code class="n">accuracyByLabel</code> <code class="o">=</code> <code class="n">accuracyDF</code><code class="o">.</code><code class="n">countMostFrequent</code><code class="o">/</code> \&#13;
                        <code class="n">accuracyDF</code><code class="o">.</code><code class="n">clusterCount</code></pre>&#13;
&#13;
<p>For the sake of conciseness, we have all this code in a single function, available on <a href="http://bit.ly/2Gd4v7e">GitHub</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="k-Means Accuracy" data-type="sect2"><div class="sect2" id="idm140637547155632">&#13;
<h2>k-Means Accuracy</h2>&#13;
&#13;
<p>Let’s<a data-primary="k-means clustering" data-secondary="accuracy of" data-type="indexterm" id="idm140637546478480"/> now perform the experiments we did earlier, but instead of calculating inertia, we will calculate the overall homogeneity of the clusters based on the accuracy measure we’ve defined for this MNIST digits dataset:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># k-means - Accuracy as the number of clusters varies</code>&#13;
&#13;
<code class="n">n_clusters</code> <code class="o">=</code> <code class="mi">5</code>&#13;
<code class="n">n_init</code> <code class="o">=</code> <code class="mi">10</code>&#13;
<code class="n">max_iter</code> <code class="o">=</code> <code class="mi">300</code>&#13;
<code class="n">tol</code> <code class="o">=</code> <code class="mf">0.0001</code>&#13;
<code class="n">random_state</code> <code class="o">=</code> <code class="mi">2018</code>&#13;
<code class="n">n_jobs</code> <code class="o">=</code> <code class="mi">2</code>&#13;
&#13;
<code class="n">kMeans_inertia</code> <code class="o">=</code> \&#13;
    <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="p">[],</code><code class="n">index</code><code class="o">=</code><code class="nb">range</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code><code class="mi">21</code><code class="p">),</code><code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'inertia'</code><code class="p">])</code>&#13;
<code class="n">overallAccuracy_kMeansDF</code> <code class="o">=</code> \&#13;
    <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="p">[],</code><code class="n">index</code><code class="o">=</code><code class="nb">range</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code><code class="mi">21</code><code class="p">),</code><code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'overallAccuracy'</code><code class="p">])</code>&#13;
&#13;
<code class="k">for</code> <code class="n">n_clusters</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code><code class="mi">21</code><code class="p">):</code>&#13;
    <code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="n">n_clusters</code><code class="p">,</code> <code class="n">n_init</code><code class="o">=</code><code class="n">n_init</code><code class="p">,</code> \&#13;
                <code class="n">max_iter</code><code class="o">=</code><code class="n">max_iter</code><code class="p">,</code> <code class="n">tol</code><code class="o">=</code><code class="n">tol</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="n">random_state</code><code class="p">,</code> \&#13;
                <code class="n">n_jobs</code><code class="o">=</code><code class="n">n_jobs</code><code class="p">)</code>&#13;
&#13;
    <code class="n">cutoff</code> <code class="o">=</code> <code class="mi">99</code>&#13;
    <code class="n">kmeans</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_PCA</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code><code class="mi">0</code><code class="p">:</code><code class="n">cutoff</code><code class="p">])</code>&#13;
    <code class="n">kMeans_inertia</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">n_clusters</code><code class="p">]</code> <code class="o">=</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">inertia_</code>&#13;
    <code class="n">X_train_kmeansClustered</code> <code class="o">=</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_train_PCA</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code><code class="mi">0</code><code class="p">:</code><code class="n">cutoff</code><code class="p">])</code>&#13;
    <code class="n">X_train_kmeansClustered</code> <code class="o">=</code> \&#13;
        <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train_kmeansClustered</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">X_train</code><code class="o">.</code><code class="n">index</code><code class="p">,</code> \&#13;
                     <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'cluster'</code><code class="p">])</code>&#13;
&#13;
    <code class="n">countByCluster_kMeans</code><code class="p">,</code> <code class="n">countByLabel_kMeans</code><code class="p">,</code> <code class="n">countMostFreq_kMeans</code><code class="p">,</code> \&#13;
        <code class="n">accuracyDF_kMeans</code><code class="p">,</code> <code class="n">overallAccuracy_kMeans</code><code class="p">,</code> <code class="n">accuracyByLabel_kMeans</code> \&#13;
        <code class="o">=</code> <code class="n">analyzeCluster</code><code class="p">(</code><code class="n">X_train_kmeansClustered</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
&#13;
    <code class="n">overallAccuracy_kMeansDF</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">n_clusters</code><code class="p">]</code> <code class="o">=</code> <code class="n">overallAccuracy_kMeans</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#k_means_accuracy_for_cluster_sizes_2_through_20">Figure 5-2</a> shows the plot of the overall accuracy for different cluster sizes.</p>&#13;
&#13;
<figure><div class="figure" id="k_means_accuracy_for_cluster_sizes_2_through_20">&#13;
<img alt="k-Means Accuracy for Cluster Sizes 2 through 20" src="assets/hulp_0502.png"/>&#13;
<h6><span class="label">Figure 5-2. </span>k-means accuracy for cluster sizes 2 through 20</h6>&#13;
</div></figure>&#13;
&#13;
<p>As <a data-type="xref" href="#k_means_accuracy_for_cluster_sizes_2_through_20">Figure 5-2</a> shows, the accuracy improves as the number of clusters increases. In other words, clusters become more homogeneous as we increase the number of clusters because each cluster becomes smaller and more tightly formed.</p>&#13;
&#13;
<p>Accuracy by cluster varies quite a bit, with some clusters exhibiting a high degree of homogeneity and others exhibiting less. For example, in some clusters, over 90% of the images have the same digit; in other clusters, less than 50% of the images have the same digit:</p>&#13;
&#13;
<pre data-type="programlisting">0    0.636506&#13;
1    0.928505&#13;
2    0.848714&#13;
3    0.521805&#13;
4    0.714337&#13;
5    0.950980&#13;
6    0.893103&#13;
7    0.919040&#13;
8    0.404707&#13;
9    0.500522&#13;
10   0.381526&#13;
11   0.587680&#13;
12   0.463382&#13;
13   0.958046&#13;
14   0.870888&#13;
15   0.942325&#13;
16   0.791192&#13;
17   0.843972&#13;
18   0.455679&#13;
19   0.926480&#13;
dtype:  float64</pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="k-Means and the Number of Principal Components" data-type="sect2"><div class="sect2" id="idm140637546399680">&#13;
<h2>k-Means and the Number of Principal Components</h2>&#13;
&#13;
<p>Let’s<a data-primary="k-means clustering" data-secondary="number of principal components and" data-type="indexterm" id="idm140637546398144"/> perform yet another experiment—this time, let’s assess how varying the number of principal components we use in the clustering algorithm impacts the homogeneity of the clusters (defined as <em>accuracy</em>).</p>&#13;
&#13;
<p>In the experiments earlier, we used one hundred principal components, derived from normal PCA. Recall that the original number of dimensions for the MNIST digits dataset is 784. If PCA does a good job of capturing the underlying structure in the data as compactly as possible, the clustering algorithm will have an easy time grouping similar images together, regardless of whether the clustering happens on just a fraction of the principal components or many more. In other words, clustering should perform just as well using 10 or 50 principal components as it does using one hundred or several hundred principal components.</p>&#13;
&#13;
<p>Let’s test this hypothesis. We will pass along 10, 50, 100, 200, 300, 400, 500, 600, 700, and 784 principal components and gauge the accuracy of each clustering experiment. We will then plot these results to see how varying the number of principal components affects the clustering accuracy:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># k-means - Accuracy as the number of components varies</code>&#13;
&#13;
<code class="n">n_clusters</code> <code class="o">=</code> <code class="mi">20</code>&#13;
<code class="n">n_init</code> <code class="o">=</code> <code class="mi">10</code>&#13;
<code class="n">max_iter</code> <code class="o">=</code> <code class="mi">300</code>&#13;
<code class="n">tol</code> <code class="o">=</code> <code class="mf">0.0001</code>&#13;
<code class="n">random_state</code> <code class="o">=</code> <code class="mi">2018</code>&#13;
<code class="n">n_jobs</code> <code class="o">=</code> <code class="mi">2</code>&#13;
&#13;
<code class="n">kMeans_inertia</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="p">[],</code><code class="n">index</code><code class="o">=</code><code class="p">[</code><code class="mi">9</code><code class="p">,</code> <code class="mi">49</code><code class="p">,</code> <code class="mi">99</code><code class="p">,</code> <code class="mi">199</code><code class="p">,</code> \&#13;
                    <code class="mi">299</code><code class="p">,</code> <code class="mi">399</code><code class="p">,</code> <code class="mi">499</code><code class="p">,</code> <code class="mi">599</code><code class="p">,</code> <code class="mi">699</code><code class="p">,</code> <code class="mi">784</code><code class="p">],</code><code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'inertia'</code><code class="p">])</code>&#13;
&#13;
<code class="n">overallAccuracy_kMeansDF</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="p">[],</code><code class="n">index</code><code class="o">=</code><code class="p">[</code><code class="mi">9</code><code class="p">,</code> <code class="mi">49</code><code class="p">,</code> \&#13;
                    <code class="mi">99</code><code class="p">,</code> <code class="mi">199</code><code class="p">,</code> <code class="mi">299</code><code class="p">,</code> <code class="mi">399</code><code class="p">,</code> <code class="mi">499</code><code class="p">,</code> <code class="mi">599</code><code class="p">,</code> <code class="mi">699</code><code class="p">,</code> <code class="mi">784</code><code class="p">],</code> \&#13;
                    <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'overallAccuracy'</code><code class="p">])</code>&#13;
&#13;
<code class="k">for</code> <code class="n">cutoffNumber</code> <code class="ow">in</code> <code class="p">[</code><code class="mi">9</code><code class="p">,</code> <code class="mi">49</code><code class="p">,</code> <code class="mi">99</code><code class="p">,</code> <code class="mi">199</code><code class="p">,</code> <code class="mi">299</code><code class="p">,</code> <code class="mi">399</code><code class="p">,</code> <code class="mi">499</code><code class="p">,</code> <code class="mi">599</code><code class="p">,</code> <code class="mi">699</code><code class="p">,</code> <code class="mi">784</code><code class="p">]:</code>&#13;
    <code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="n">n_clusters</code><code class="p">,</code> <code class="n">n_init</code><code class="o">=</code><code class="n">n_init</code><code class="p">,</code> \&#13;
                <code class="n">max_iter</code><code class="o">=</code><code class="n">max_iter</code><code class="p">,</code> <code class="n">tol</code><code class="o">=</code><code class="n">tol</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="n">random_state</code><code class="p">,</code> \&#13;
                <code class="n">n_jobs</code><code class="o">=</code><code class="n">n_jobs</code><code class="p">)</code>&#13;
&#13;
    <code class="n">cutoff</code> <code class="o">=</code> <code class="n">cutoffNumber</code>&#13;
    <code class="n">kmeans</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_PCA</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code><code class="mi">0</code><code class="p">:</code><code class="n">cutoff</code><code class="p">])</code>&#13;
    <code class="n">kMeans_inertia</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">cutoff</code><code class="p">]</code> <code class="o">=</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">inertia_</code>&#13;
    <code class="n">X_train_kmeansClustered</code> <code class="o">=</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_train_PCA</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code><code class="mi">0</code><code class="p">:</code><code class="n">cutoff</code><code class="p">])</code>&#13;
    <code class="n">X_train_kmeansClustered</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train_kmeansClustered</code><code class="p">,</code> \&#13;
                                <code class="n">index</code><code class="o">=</code><code class="n">X_train</code><code class="o">.</code><code class="n">index</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'cluster'</code><code class="p">])</code>&#13;
&#13;
    <code class="n">countByCluster_kMeans</code><code class="p">,</code> <code class="n">countByLabel_kMeans</code><code class="p">,</code> <code class="n">countMostFreq_kMeans</code><code class="p">,</code> \&#13;
        <code class="n">accuracyDF_kMeans</code><code class="p">,</code> <code class="n">overallAccuracy_kMeans</code><code class="p">,</code> <code class="n">accuracyByLabel_kMeans</code> \&#13;
        <code class="o">=</code> <code class="n">analyzeCluster</code><code class="p">(</code><code class="n">X_train_kmeansClustered</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
&#13;
    <code class="n">overallAccuracy_kMeansDF</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">cutoff</code><code class="p">]</code> <code class="o">=</code> <code class="n">overallAccuracy_kMeans</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#k_means_clustering_accuracy_as_number_of_principal_components_varies">Figure 5-3</a> shows the plot of the clustering accuracy for the different number of principal components.</p>&#13;
&#13;
<figure><div class="figure" id="k_means_clustering_accuracy_as_number_of_principal_components_varies">&#13;
<img alt="k-means Clustering Accuracy As Number of Principal Components Varies" src="assets/hulp_0503.png"/>&#13;
<h6><span class="label">Figure 5-3. </span>k-means clustering accuracy with varying number of principal components</h6>&#13;
</div></figure>&#13;
&#13;
<p>This plot supports our hypothesis. As the number of principal components varies from 10 to 784, the clustering accuracy remains stable and consistent around 70%. This is one reason why clustering should be performed on dimensionality-reduced datasets—the clustering algorithms generally perform better, both in terms of time and clustering accuracy, on dimensionality-reduced datasets.</p>&#13;
&#13;
<p>In our case, for the MNIST dataset, the original 784 dimensions are manageable for a clustering algorithm, but imagine if the original dataset were thousands or millions of dimensions large. The case for reducing the dimensionality before performing clustering is even stronger in such a scenario.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="k-Means on the Original Dataset" data-type="sect2"><div class="sect2" id="idm140637546383056">&#13;
<h2>k-Means on the Original Dataset</h2>&#13;
&#13;
<p>To<a data-primary="k-means clustering" data-secondary="on original dataset" data-secondary-sortas="original dataset" data-type="indexterm" id="idm140637546233408"/> make this point clearer, let’s perform clustering on the original dataset and measure how varying the number of dimensions we pass into the clustering algorithm affects clustering accuracy.</p>&#13;
&#13;
<p>For the PCA-reduced dataset in the previous section, varying the number of principal components that we passed into the clustering algorithm did not affect the clustering accuracy, which remained stable and consistent at approximately 70%. Is this true for the original dataset, too?</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># k-means - Accuracy as the number of components varies</code>&#13;
<code class="c1"># On the original MNIST data (not PCA-reduced)</code>&#13;
&#13;
<code class="n">n_clusters</code> <code class="o">=</code> <code class="mi">20</code>&#13;
<code class="n">n_init</code> <code class="o">=</code> <code class="mi">10</code>&#13;
<code class="n">max_iter</code> <code class="o">=</code> <code class="mi">300</code>&#13;
<code class="n">tol</code> <code class="o">=</code> <code class="mf">0.0001</code>&#13;
<code class="n">random_state</code> <code class="o">=</code> <code class="mi">2018</code>&#13;
<code class="n">n_jobs</code> <code class="o">=</code> <code class="mi">2</code>&#13;
&#13;
<code class="n">kMeans_inertia</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="p">[],</code><code class="n">index</code><code class="o">=</code><code class="p">[</code><code class="mi">9</code><code class="p">,</code> <code class="mi">49</code><code class="p">,</code> <code class="mi">99</code><code class="p">,</code> <code class="mi">199</code><code class="p">,</code> \&#13;
                    <code class="mi">299</code><code class="p">,</code> <code class="mi">399</code><code class="p">,</code> <code class="mi">499</code><code class="p">,</code> <code class="mi">599</code><code class="p">,</code> <code class="mi">699</code><code class="p">,</code> <code class="mi">784</code><code class="p">],</code><code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'inertia'</code><code class="p">])</code>&#13;
&#13;
<code class="n">overallAccuracy_kMeansDF</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="p">[],</code><code class="n">index</code><code class="o">=</code><code class="p">[</code><code class="mi">9</code><code class="p">,</code> <code class="mi">49</code><code class="p">,</code> \&#13;
                    <code class="mi">99</code><code class="p">,</code> <code class="mi">199</code><code class="p">,</code> <code class="mi">299</code><code class="p">,</code> <code class="mi">399</code><code class="p">,</code> <code class="mi">499</code><code class="p">,</code> <code class="mi">599</code><code class="p">,</code> <code class="mi">699</code><code class="p">,</code> <code class="mi">784</code><code class="p">],</code> \&#13;
                    <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'overallAccuracy'</code><code class="p">])</code>&#13;
&#13;
<code class="k">for</code> <code class="n">cutoffNumber</code> <code class="ow">in</code> <code class="p">[</code><code class="mi">9</code><code class="p">,</code> <code class="mi">49</code><code class="p">,</code> <code class="mi">99</code><code class="p">,</code> <code class="mi">199</code><code class="p">,</code> <code class="mi">299</code><code class="p">,</code> <code class="mi">399</code><code class="p">,</code> <code class="mi">499</code><code class="p">,</code> <code class="mi">599</code><code class="p">,</code> <code class="mi">699</code><code class="p">,</code> <code class="mi">784</code><code class="p">]:</code>&#13;
    <code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="n">n_clusters</code><code class="p">,</code> <code class="n">n_init</code><code class="o">=</code><code class="n">n_init</code><code class="p">,</code> \&#13;
                <code class="n">max_iter</code><code class="o">=</code><code class="n">max_iter</code><code class="p">,</code> <code class="n">tol</code><code class="o">=</code><code class="n">tol</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="n">random_state</code><code class="p">,</code> \&#13;
                <code class="n">n_jobs</code><code class="o">=</code><code class="n">n_jobs</code><code class="p">)</code>&#13;
&#13;
    <code class="n">cutoff</code> <code class="o">=</code> <code class="n">cutoffNumber</code>&#13;
    <code class="n">kmeans</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code><code class="mi">0</code><code class="p">:</code><code class="n">cutoff</code><code class="p">])</code>&#13;
    <code class="n">kMeans_inertia</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">cutoff</code><code class="p">]</code> <code class="o">=</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">inertia_</code>&#13;
    <code class="n">X_train_kmeansClustered</code> <code class="o">=</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_train</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code><code class="mi">0</code><code class="p">:</code><code class="n">cutoff</code><code class="p">])</code>&#13;
    <code class="n">X_train_kmeansClustered</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train_kmeansClustered</code><code class="p">,</code> \&#13;
                                <code class="n">index</code><code class="o">=</code><code class="n">X_train</code><code class="o">.</code><code class="n">index</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'cluster'</code><code class="p">])</code>&#13;
&#13;
    <code class="n">countByCluster_kMeans</code><code class="p">,</code> <code class="n">countByLabel_kMeans</code><code class="p">,</code> <code class="n">countMostFreq_kMeans</code><code class="p">,</code> \&#13;
        <code class="n">accuracyDF_kMeans</code><code class="p">,</code> <code class="n">overallAccuracy_kMeans</code><code class="p">,</code> <code class="n">accuracyByLabel_kMeans</code> \&#13;
        <code class="o">=</code> <code class="n">analyzeCluster</code><code class="p">(</code><code class="n">X_train_kmeansClustered</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
&#13;
    <code class="n">overallAccuracy_kMeansDF</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">cutoff</code><code class="p">]</code> <code class="o">=</code> <code class="n">overallAccuracy_kMeans</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#k_means_clustering_accuracy_as_number_of_original_dimension_varies">Figure 5-4</a> plots the clustering accuracy at the different original dimensions.</p>&#13;
&#13;
<figure><div class="figure" id="k_means_clustering_accuracy_as_number_of_original_dimension_varies">&#13;
<img alt="k-means Clustering Accuracy As Number of Original Dimensions Varies" src="assets/hulp_0504.png"/>&#13;
<h6><span class="label">Figure 5-4. </span>k-means clustering accuracy with varying number of original dimensions</h6>&#13;
</div></figure>&#13;
&#13;
<p>As the plot shows, clustering accuracy is very poor at lower dimensions but improves to nearly 70% only as the number of dimensions climbs to six hundred dimensions.</p>&#13;
&#13;
<p>In the PCA case, clustering accuracy was approximately 70% even at 10 dimensions, demonstrating the power of dimensionality reduction to densely capture salient information in the original dataset.<a data-primary="" data-startref="Ckmean05" data-type="indexterm" id="idm140637546219728"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before" data-pdf-bookmark="Hierarchical Clustering" data-type="sect1"><div class="sect1" id="idm140637545904496">&#13;
<h1>Hierarchical Clustering</h1>&#13;
&#13;
<p>Let’s<a data-primary="clustering" data-secondary="hierarchical" data-type="indexterm" id="CLhierar05"/><a data-primary="hierarchical clustering" data-secondary="overview of" data-type="indexterm" id="idm140637545901248"/> move to a second clustering approach called <em>hierarchical clustering</em>. This approach does not require us to precommit to a particular number of clusters. Instead, we can choose how many clusters we would like after hierarchical clustering has finished running.</p>&#13;
&#13;
<p>Using<a data-primary="dendrograms" data-type="indexterm" id="idm140637545899168"/> the observations in our dataset, the hierarchical clustering algorithm will build a <em>dendrogram</em>, which can be depicted as an upside-down tree where the leaves are at the bottom and the tree trunk is at the top.</p>&#13;
&#13;
<p>The leaves at the very bottom are individual instances in the dataset. Hierarchical clustering then joins the leaves together—as we move vertically up the upside-down tree—based on how similar they are to each other. The instances (or groups of instances) that are most similar to each other are joined sooner, while the instances that are not as similar are joined later.</p>&#13;
&#13;
<p>With this iterative process, all the instances are eventually linked together forming the single trunk of the tree.</p>&#13;
&#13;
<p>This vertical depiction is very helpful. Once the hierarchical clustering algorithm has finished running, we can view the dendrogram and determine where we want to cut the tree—the lower we cut, the more individual branches we are left with (i.e., more clusters). If we want fewer clusters, we can cut higher on the dendrogram, closer to the single trunk at the very top of this upside-down tree.</p>&#13;
&#13;
<p>The placement of this vertical cut is similar to choosing the number of <em>k</em> clusters in the <em>k</em>-means clustering algorithm.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Agglomerative Hierarchical Clustering" data-type="sect2"><div class="sect2" id="idm140637545894192">&#13;
<h2>Agglomerative Hierarchical Clustering</h2>&#13;
&#13;
<p>The<a data-primary="hierarchical clustering" data-secondary="agglomerative" data-type="indexterm" id="idm140637545892656"/><a data-primary="agglomerative hierarchical clustering" data-type="indexterm" id="idm140637545891648"/><a data-primary="fastcluster package" data-type="indexterm" id="idm140637545890960"/> version of hierarchical clustering we will explore is called <em>agglomerative clustering</em>. Although Scikit-Learn has a library for this, it performs very slowly. Instead, we will choose to use another version of hierarchical clustering called <em>fastcluster</em>. This package is a C++ library with an interface in Python/SciPy.<sup><a data-type="noteref" href="ch05.html#idm140637545889136" id="idm140637545889136-marker">1</a></sup></p>&#13;
&#13;
<p>The main function that we will use in this package is <code>fastcluster.linkage_vector</code>. This requires several arguments, including the training matrix <em>X</em>, the <em>method</em>, and the <em>metric</em>. The method—which can be set to <code>single</code>, <code>centroid</code>, <code>median</code>, or <code>ward</code>—specifies which clustering scheme to use to determine the distance from a new node in the dendrogram to the other nodes. The metric should be set to <code>euclidean</code> in most cases, and it is required to be <code>euclidean</code> if the method is <code>centroid</code>, <code>median</code>, or <code>ward</code>. For more on these arguments, refer to the fastcluster documentation.</p>&#13;
&#13;
<p>Let’s set up the hierarchical clustering algorithm for our data. As before, we will train the algorithm on the first one hundred principal components from the PCA-reduced MNIST image dataset. We will set the method to <code>ward</code> (which performed the best, by far, in the experimentation), and the metric to <code>euclidean</code>.</p>&#13;
&#13;
<p>Ward<a data-primary="Ward’s minimum variance method" data-type="indexterm" id="idm140637545879280"/> stands for <em>Ward’s minimum variance method</em>. You can learn more about this method <a href="http://bit.ly/2WwOJK5">online</a>. Ward is a good default choice to use in hierarchical clustering, but, as always, it is best to experiment on your specific datasets in practice.</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">fastcluster</code>&#13;
<code class="kn">from</code> <code class="nn">scipy.cluster.hierarchy</code> <code class="kn">import</code> <code class="n">dendrogram</code><code class="p">,</code> <code class="n">cophenet</code>&#13;
<code class="kn">from</code> <code class="nn">scipy.spatial.distance</code> <code class="kn">import</code> <code class="n">pdist</code>&#13;
&#13;
<code class="n">cutoff</code> <code class="o">=</code> <code class="mi">100</code>&#13;
<code class="n">Z</code> <code class="o">=</code> <code class="n">fastcluster</code><code class="o">.</code><code class="n">linkage_vector</code><code class="p">(</code><code class="n">X_train_PCA</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code><code class="mi">0</code><code class="p">:</code><code class="n">cutoff</code><code class="p">],</code> \&#13;
                               <code class="n">method</code><code class="o">=</code><code class="s1">'ward'</code><code class="p">,</code> <code class="n">metric</code><code class="o">=</code><code class="s1">'euclidean'</code><code class="p">)</code>&#13;
<code class="n">Z_dataFrame</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">Z</code><code class="p">,</code> \&#13;
    <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'clusterOne'</code><code class="p">,</code><code class="s1">'clusterTwo'</code><code class="p">,</code><code class="s1">'distance'</code><code class="p">,</code><code class="s1">'newClusterSize'</code><code class="p">])</code></pre>&#13;
&#13;
<p>The hierarchical clustering algorithm will return a matrix <em>Z</em>. The algorithm treats each observation in our 50,000 MNIST digits dataset as a single-point cluster, and, in each iteration of training, the algorithm will merge the two clusters that have the smallest distance between them.</p>&#13;
&#13;
<p>Initially, the algorithm is just merging single-point clusters together, but as it proceeds, it will merge multipoint clusters with either single-point or multipoint clusters. Eventually, through this iterative process, all the clusters are merged together, forming the trunk in the upside-down tree (dendrogram).</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Dendrogram" data-type="sect2"><div class="sect2" id="idm140637545646432">&#13;
<h2>The Dendrogram</h2>&#13;
&#13;
<p><a data-type="xref" href="#first_few_rows_of_z_matrix_of_hierarchical_clustering">Table 5-1</a> shows<a data-primary="dendrograms" data-type="indexterm" id="idm140637545644272"/><a data-primary="hierarchical clustering" data-secondary="Z matrix (dendrogram)" data-type="indexterm" id="idm140637545643568"/> the Z matrix that was generated by the clustering algorithm, showing what the algorithm can accomplish.</p>&#13;
<table class="pagebreak-before" id="first_few_rows_of_z_matrix_of_hierarchical_clustering">&#13;
<caption><span class="label">Table 5-1. </span>First few rows of Z matrix of hierarchical clustering</caption>&#13;
<thead>&#13;
<tr>&#13;
<th/>&#13;
<th>clusterOne</th>&#13;
<th>clusterTwo</th>&#13;
<th>distance</th>&#13;
<th>newClusterSize</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>0</p></td>&#13;
<td><p>42194.0</p></td>&#13;
<td><p>43025.0</p></td>&#13;
<td><p>0.562682</p></td>&#13;
<td><p>2.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>1</p></td>&#13;
<td><p>28350.0</p></td>&#13;
<td><p>37674.0</p></td>&#13;
<td><p>0.590866</p></td>&#13;
<td><p>2.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>2</p></td>&#13;
<td><p>26696.0</p></td>&#13;
<td><p>44705.0</p></td>&#13;
<td><p>0.621506</p></td>&#13;
<td><p>2.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>3</p></td>&#13;
<td><p>12634.0</p></td>&#13;
<td><p>32823.0</p></td>&#13;
<td><p>0.627762</p></td>&#13;
<td><p>2.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>4</p></td>&#13;
<td><p>24707.0</p></td>&#13;
<td><p>43151.0</p></td>&#13;
<td><p>0.637668</p></td>&#13;
<td><p>2.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>5</p></td>&#13;
<td><p>20465.0</p></td>&#13;
<td><p>24483.0</p></td>&#13;
<td><p>0.662557</p></td>&#13;
<td><p>2.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>6</p></td>&#13;
<td><p>466.0</p></td>&#13;
<td><p>42098.0</p></td>&#13;
<td><p>0.664189</p></td>&#13;
<td><p>2.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>7</p></td>&#13;
<td><p>46542.0</p></td>&#13;
<td><p>49961.0</p></td>&#13;
<td><p>0.665520</p></td>&#13;
<td><p>2.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>8</p></td>&#13;
<td><p>2301.0</p></td>&#13;
<td><p>5732.0</p></td>&#13;
<td><p>0.671215</p></td>&#13;
<td><p>2.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>9</p></td>&#13;
<td><p>37564.0</p></td>&#13;
<td><p>47668.0</p></td>&#13;
<td><p>0.675121</p></td>&#13;
<td><p>2.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>10</p></td>&#13;
<td><p>3375.0</p></td>&#13;
<td><p>26243.0</p></td>&#13;
<td><p>0.685797</p></td>&#13;
<td><p>2.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>11</p></td>&#13;
<td><p>15722.0</p></td>&#13;
<td><p>30368.0</p></td>&#13;
<td><p>0.686356</p></td>&#13;
<td><p>2.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>12</p></td>&#13;
<td><p>21247.0</p></td>&#13;
<td><p>21575.0</p></td>&#13;
<td><p>0.694412</p></td>&#13;
<td><p>2.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>13</p></td>&#13;
<td><p>14900.0</p></td>&#13;
<td><p>42486.0</p></td>&#13;
<td><p>0.696769</p></td>&#13;
<td><p>2.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>14</p></td>&#13;
<td><p>30100.0</p></td>&#13;
<td><p>41908.0</p></td>&#13;
<td><p>0.699261</p></td>&#13;
<td><p>2.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>15</p></td>&#13;
<td><p>12040.0</p></td>&#13;
<td><p>13254.0</p></td>&#13;
<td><p>0.701134</p></td>&#13;
<td><p>2.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>16</p></td>&#13;
<td><p>10508.0</p></td>&#13;
<td><p>25434.0</p></td>&#13;
<td><p>0.708872</p></td>&#13;
<td><p>2.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>17</p></td>&#13;
<td><p>30695.0</p></td>&#13;
<td><p>30757.0</p></td>&#13;
<td><p>0.710023</p></td>&#13;
<td><p>2.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>18</p></td>&#13;
<td><p>31019.0</p></td>&#13;
<td><p>31033.0</p></td>&#13;
<td><p>0.712052</p></td>&#13;
<td><p>2.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>19</p></td>&#13;
<td><p>36264.0</p></td>&#13;
<td><p>37285.0</p></td>&#13;
<td><p>0.713130</p></td>&#13;
<td><p>2.0</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>The first two columns in this table, <code>clusterOne</code> and <code>clusterTwo</code>, list which two clusters—could be single-point clusters (i.e., the original observations) or multipoint clusters—are being merged given their distance relative to each other. The third column, <code>distance</code>, displays this distance, which was determined by the Ward method and <code>euclidean</code> metric that we passed into the clustering algorithm.</p>&#13;
&#13;
<p>As you can see, the distance is monotonically increasing. In other words, the shortest-distance clusters are merged first, and the algorithm iteratively merges the next shortest-distance clusters until all the points have been joined into a single cluster at the top of the dendrogram.</p>&#13;
&#13;
<p>Initially, the algorithm merges single-point clusters together, forming new clusters with a size of two, as shown in the fourth column, <code>newClusterSize</code>. However, as we get much further along, the algorithm joins large multipoint clusters with other large multipoint clusters, as shown in <a data-type="xref" href="#last_few_rows_of_z_matrix_of_hierarchical_clustering">Table 5-2</a>. At the very last iteration (49,998), two large clusters are joined together, forming a single cluster—the top tree trunk—with all 50,000 original observations.</p>&#13;
<table id="last_few_rows_of_z_matrix_of_hierarchical_clustering">&#13;
<caption><span class="label">Table 5-2. </span>Last few rows of Z matrix of hierarchical clustering</caption>&#13;
<thead>&#13;
<tr>&#13;
<th/>&#13;
<th>clusterOne</th>&#13;
<th>clusterTwo</th>&#13;
<th>distance</th>&#13;
<th>newClusterSize</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>49980</p></td>&#13;
<td><p>99965.0</p></td>&#13;
<td><p>99972.0</p></td>&#13;
<td><p>161.106998</p></td>&#13;
<td><p>5197.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>49981</p></td>&#13;
<td><p>99932.0</p></td>&#13;
<td><p>99980.0</p></td>&#13;
<td><p>172.070003</p></td>&#13;
<td><p>6505.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>49982</p></td>&#13;
<td><p>99945.0</p></td>&#13;
<td><p>99960.0</p></td>&#13;
<td><p>182.840860</p></td>&#13;
<td><p>3245.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>49983</p></td>&#13;
<td><p>99964.0</p></td>&#13;
<td><p>99976.0</p></td>&#13;
<td><p>184.475761</p></td>&#13;
<td><p>3683.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>49984</p></td>&#13;
<td><p>99974.0</p></td>&#13;
<td><p>99979.0</p></td>&#13;
<td><p>185.027847</p></td>&#13;
<td><p>7744.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>49985</p></td>&#13;
<td><p>99940.0</p></td>&#13;
<td><p>99975.0</p></td>&#13;
<td><p>185.345207</p></td>&#13;
<td><p>5596.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>49986</p></td>&#13;
<td><p>99957.0</p></td>&#13;
<td><p>99967.0</p></td>&#13;
<td><p>211.854714</p></td>&#13;
<td><p>5957.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>49987</p></td>&#13;
<td><p>99938.0</p></td>&#13;
<td><p>99983.0</p></td>&#13;
<td><p>215.494857</p></td>&#13;
<td><p>4846.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>49988</p></td>&#13;
<td><p>99978.0</p></td>&#13;
<td><p>99984.0</p></td>&#13;
<td><p>216.760365</p></td>&#13;
<td><p>11072.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>49989</p></td>&#13;
<td><p>99970.0</p></td>&#13;
<td><p>99973.0</p></td>&#13;
<td><p>217.355871</p></td>&#13;
<td><p>4899.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>49990</p></td>&#13;
<td><p>99969.0</p></td>&#13;
<td><p>99986.0</p></td>&#13;
<td><p>225.468298</p></td>&#13;
<td><p>8270.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>49991</p></td>&#13;
<td><p>99981.0</p></td>&#13;
<td><p>99982.0</p></td>&#13;
<td><p>238.845135</p></td>&#13;
<td><p>9750.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>49992</p></td>&#13;
<td><p>99968.0</p></td>&#13;
<td><p>99977.0</p></td>&#13;
<td><p>266.146782</p></td>&#13;
<td><p>5567.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>49993</p></td>&#13;
<td><p>99985.0</p></td>&#13;
<td><p>99989.0</p></td>&#13;
<td><p>270.929453</p></td>&#13;
<td><p>10495.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>49994</p></td>&#13;
<td><p>99990.0</p></td>&#13;
<td><p>99991.0</p></td>&#13;
<td><p>346.840948</p></td>&#13;
<td><p>18020.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>49995</p></td>&#13;
<td><p>99988.0</p></td>&#13;
<td><p>99993.0</p></td>&#13;
<td><p>394.365194</p></td>&#13;
<td><p>21567.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>49996</p></td>&#13;
<td><p>99987.0</p></td>&#13;
<td><p>99995.0</p></td>&#13;
<td><p>425.142387</p></td>&#13;
<td><p>26413.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>49997</p></td>&#13;
<td><p>99992.0</p></td>&#13;
<td><p>99994.0</p></td>&#13;
<td><p>440.148301</p></td>&#13;
<td><p>23587.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>49998</p></td>&#13;
<td><p>99996.0</p></td>&#13;
<td><p>99997.0</p></td>&#13;
<td><p>494.383855</p></td>&#13;
<td><p>50000.0</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>You may be a bit confused by the <code>clusterOne</code> and <code>clusterTwo</code> entries in this table. For example, in the last row—49,998—cluster 99,996 is joined with cluster 99,997. But as you know, there are only 50,000 observations in the MNIST digits dataset.</p>&#13;
&#13;
<p><code>clusterOne</code> and <code>clusterTwo</code> refer to the original observations for numbers 0 through 49,999. For numbers above 49,999, the cluster numbers refer to previously clustered points. For example, 50,000 refers to the newly formed cluster in row 0, 50,001 refers to the newly formed cluster in row 1, etc.</p>&#13;
&#13;
<p>In row 49,998, <code>clusterOne</code>, 99,996 refers to the cluster formed in row 49,996, and <code>clusterTwo</code>, 99,997, refers to the cluster formed in row 49,997. You can continue to work your way through this table using this formula to see how the clusters are being joined.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Evaluating the Clustering Results" data-type="sect2"><div class="sect2" id="idm140637545645840">&#13;
<h2>Evaluating the Clustering Results</h2>&#13;
&#13;
<p>Now<a data-primary="hierarchical clustering" data-secondary="evaluating cluster results" data-type="indexterm" id="idm140637545338944"/> that we have the dendrogram in place, let’s determine where to cut off the <span class="keep-together">dendrogram</span> to make the number of clusters we desire. To more easily compare hierarchical clustering results with those of <em>k</em>-means, let’s cut the dendrogram to have exactly 20 clusters. We will then use the clustering accuracy metric—defined in the <code>k-means section</code>—to judge how homogenous the hierarchical clustering clusters are.</p>&#13;
&#13;
<p>To<a data-primary="fcluster library" data-type="indexterm" id="idm140637545335584"/> create the clusters we desire from the dendrogram, let’s pull in the <em>fcluster</em> library from SciPy. We need to specify the <em>distance threshold</em> of the dendrogram to determine how many distinct clusters we are left with. The larger the distance threshold, the fewer clusters we will have. Data points within the distance threshold we set will belong to the same cluster. A large distance threshold is akin to cutting the upside-down tree at a very high vertical point. Since more and more of the points are grouped together the higher up the tree we go, the fewer clusters we will have.</p>&#13;
&#13;
<p>To get exactly 20 clusters, we need to experiment with the distance threshold, as done here. The <em>fcluster</em> library will take our dendrogram and cut it with the distance threshold we specify. Each observation in the 50,000 observations MNIST digits dataset will get a cluster label, and we will store these in a Pandas DataFrame:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">scipy.cluster.hierarchy</code> <code class="kn">import</code> <code class="n">fcluster</code>&#13;
&#13;
<code class="n">distance_threshold</code> <code class="o">=</code> <code class="mi">160</code>&#13;
<code class="n">clusters</code> <code class="o">=</code> <code class="n">fcluster</code><code class="p">(</code><code class="n">Z</code><code class="p">,</code> <code class="n">distance_threshold</code><code class="p">,</code> <code class="n">criterion</code><code class="o">=</code><code class="s1">'distance'</code><code class="p">)</code>&#13;
<code class="n">X_train_hierClustered</code> <code class="o">=</code> \&#13;
    <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">clusters</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">X_train_PCA</code><code class="o">.</code><code class="n">index</code><code class="p">,</code><code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'cluster'</code><code class="p">])</code></pre>&#13;
&#13;
<p>Let’s verify that there are exactly 20 distinct clusters, given our choice of distance threshold:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">print</code><code class="p">(</code><code class="s2">"Number of distinct clusters: "</code><code class="p">,</code> \&#13;
      <code class="nb">len</code><code class="p">(</code><code class="n">X_train_hierClustered</code><code class="p">[</code><code class="s1">'cluster'</code><code class="p">]</code><code class="o">.</code><code class="n">unique</code><code class="p">()))</code></pre>&#13;
&#13;
<p>As expected, this confirms the 20 clusters:</p>&#13;
&#13;
<pre data-type="programlisting">Number of distinct clusters: 20</pre>&#13;
&#13;
<p>Now, let’s evaluate the results:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">countByCluster_hierClust</code><code class="p">,</code> <code class="n">countByLabel_hierClust</code><code class="p">,</code> \&#13;
    <code class="n">countMostFreq_hierClust</code><code class="p">,</code> <code class="n">accuracyDF_hierClust</code><code class="p">,</code> \&#13;
    <code class="n">overallAccuracy_hierClust</code><code class="p">,</code> <code class="n">accuracyByLabel_hierClust</code> \&#13;
    <code class="o">=</code> <code class="n">analyzeCluster</code><code class="p">(</code><code class="n">X_train_hierClustered</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
&#13;
<code class="k">print</code><code class="p">(</code><code class="s2">"Overall accuracy from hierarchical clustering: "</code><code class="p">,</code> \&#13;
      <code class="n">overallAccuracy_hierClust</code><code class="p">)</code></pre>&#13;
&#13;
<p>We find that the overall accuracy is approximately 77%, even better than the approximately 70% accuracy from <em>k</em>-means:</p>&#13;
&#13;
<pre data-type="programlisting">Overall accuracy from hierarchical clustering: 0.76882</pre>&#13;
&#13;
<p>Let’s also assess the accuracy by cluster.</p>&#13;
&#13;
<p>As shown here, the accuracy varies quite a bit. For some clusters, the accuracy is remarkably high, nearly 100%. For some, the accuracy is shy of 50%:</p>&#13;
&#13;
<pre data-type="programlisting" id="accuracy_by_cluster_for_hierarchical_clustering">0       0.987962&#13;
1       0.983727&#13;
2       0.988998&#13;
3       0.597356&#13;
4       0.678642&#13;
5       0.442478&#13;
6       0.950033&#13;
7       0.829060&#13;
8       0.976062&#13;
9       0.986141&#13;
10      0.990183&#13;
11      0.992183&#13;
12      0.971033&#13;
13      0.554273&#13;
14      0.553617&#13;
15      0.720183&#13;
16      0.538891&#13;
17      0.484590&#13;
18      0.957732&#13;
19      0.977310&#13;
dtype:  float64</pre>&#13;
&#13;
<p>Overall, hierarchical clustering performs well on the MNIST digits dataset. Remember that we accomplished this without using any labels.</p>&#13;
&#13;
<p>This is how it would work on real-world examples: we would apply dimensionality reduction first (such as PCA), then we would perform clustering (such as hierarchical clustering), and finally we would hand-label a few points per cluster. For example, for this MNIST digits dataset, if we did not have any labels, we would look at a few images per cluster and label those images based on the digits they displayed. So long as the clusters were homogeneous enough, the few hand labels we generated could be applied automatically to all the other images in the cluster.</p>&#13;
&#13;
<p>All of a sudden, without much effort, we could have labeled all the images in our 50,000 dataset with a near 77% accuracy. This is impressive and highlights the power of unsupervised learning.<a data-primary="" data-startref="CLhierar05" data-type="indexterm" id="idm140637545134432"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="DBSCAN" data-type="sect1"><div class="sect1" id="idm140637545904032">&#13;
<h1>DBSCAN</h1>&#13;
&#13;
<p>Now let’s<a data-primary="clustering" data-secondary="DBSCAN (density-based spatial clustering of applications with noise)" data-type="indexterm" id="Cdbscan05"/><a data-primary="DBSCAN (density-based spatial clustering of applications with noise)" data-type="indexterm" id="dbscan05"/> turn to the third and final major clustering algorithm, <em>DBSCAN</em>, which stands for <em>density-based spatial clustering of applications with noise</em>. As the name implies, this clustering algorithm groups based on the density of points.</p>&#13;
&#13;
<p>DBSCAN will group together closely packed points, where close together is defined as a minimum number of points that must exist within a certian distance. If the point is within a certain distance of multiple clusters, it will be grouped with the cluster to which it is most densely located. Any instance that is not within this certain distance of another cluster is labeled an outlier.</p>&#13;
&#13;
<p>In <em>k</em>-means and hierarchical clustering, all points had to be clustered, and outliers were poorly dealt with. In DBSCAN, we can explicitly label points as outliers and avoid having to cluster them. This is powerful. Compared to the other clustering algorithms, DBSCAN is much less prone to the distortion typically caused by outliers in the data. Also, like hierarchical clustering—and unlike <em>k</em>-means—we do not need to prespecify the number of clusters.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="DBSCAN Algorithm" data-type="sect2"><div class="sect2" id="idm140637545126384">&#13;
<h2>DBSCAN Algorithm</h2>&#13;
&#13;
<p>Let’s first use the DBSCAN library from Scikit-Learn. We need to specify the <em>maximum distance</em> (called <code>eps</code>) between two points for them to be considered in the same neighborhood and the <em>minimum samples</em> (called <code>min_samples</code>) for a group to be called a cluster. The default value for <code>eps</code> is 0.5, and the default value for <code>min_samples</code> is 5. If <code>eps</code> is set too low, no points may be close enough to other points for them to be considered in the same neighborhood. Hence, all the points would remain unclustered. If <code>eps</code> is set too high, many points may be clustered and only a handful of points would remain unclustered, effectively being labeled as outliers in the dataset.</p>&#13;
&#13;
<p>We need to search for the optimal <code>eps</code> for our MNIST digits dataset. <code>min_samples</code> designates how many points need to be within the <code>eps</code> distance in order for the points to be called a cluster. Once there are <code>min_samples</code> number of closely located points, any other point that is within the <code>eps</code> distance of any of these so-called <em>core points</em> is part of that cluster, even if those other points do not have the <code>min_samples</code> number of points within <code>eps</code> distance around them. These other points—if they do not have the <em>min_samples</em> number of points within <code>eps</code> distance around them—are called the <em>border points</em> of the cluster.</p>&#13;
&#13;
<p>Generally, as the <code>min_samples</code> increases, the number of clusters decreases. As with <code>eps</code>, we need to search for the optimal <code>min_samples</code> for our MNIST digits dataset. As you can see, the clusters have core points and border points, but for all intents and purposes, they belong to the same group. All points that do not get grouped—either as the core or border points of a cluster—are labeled as outliers.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Applying DBSCAN to Our Dataset" data-type="sect2"><div class="sect2" id="idm140637545113696">&#13;
<h2>Applying DBSCAN to Our Dataset</h2>&#13;
&#13;
<p>Let’s now move to our specific problem. As before, we will apply DBSCAN to the first one hundred principal components of the PCA-reduced MNIST digits dataset:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">sklearn.cluster</code> <code class="kn">import</code> <code class="n">DBSCAN</code>&#13;
&#13;
<code class="n">eps</code> <code class="o">=</code> <code class="mi">3</code>&#13;
<code class="n">min_samples</code> <code class="o">=</code> <code class="mi">5</code>&#13;
<code class="n">leaf_size</code> <code class="o">=</code> <code class="mi">30</code>&#13;
<code class="n">n_jobs</code> <code class="o">=</code> <code class="mi">4</code>&#13;
&#13;
<code class="n">db</code> <code class="o">=</code> <code class="n">DBSCAN</code><code class="p">(</code><code class="n">eps</code><code class="o">=</code><code class="n">eps</code><code class="p">,</code> <code class="n">min_samples</code><code class="o">=</code><code class="n">min_samples</code><code class="p">,</code> <code class="n">leaf_size</code><code class="o">=</code><code class="n">leaf_size</code><code class="p">,</code>&#13;
            <code class="n">n_jobs</code><code class="o">=</code><code class="n">n_jobs</code><code class="p">)</code>&#13;
&#13;
<code class="n">cutoff</code> <code class="o">=</code> <code class="mi">99</code>&#13;
<code class="n">X_train_PCA_dbscanClustered</code> <code class="o">=</code> <code class="n">db</code><code class="o">.</code><code class="n">fit_predict</code><code class="p">(</code><code class="n">X_train_PCA</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code><code class="mi">0</code><code class="p">:</code><code class="n">cutoff</code><code class="p">])</code>&#13;
<code class="n">X_train_PCA_dbscanClustered</code> <code class="o">=</code> \&#13;
    <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train_PCA_dbscanClustered</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">X_train</code><code class="o">.</code><code class="n">index</code><code class="p">,</code> \&#13;
                 <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'cluster'</code><code class="p">])</code>&#13;
&#13;
<code class="n">countByCluster_dbscan</code><code class="p">,</code> <code class="n">countByLabel_dbscan</code><code class="p">,</code> <code class="n">countMostFreq_dbscan</code><code class="p">,</code> \&#13;
    <code class="n">accuracyDF_dbscan</code><code class="p">,</code> <code class="n">overallAccuracy_dbscan</code><code class="p">,</code> <code class="n">accuracyByLabel_dbscan</code> \&#13;
    <code class="o">=</code> <code class="n">analyzeCluster</code><code class="p">(</code><code class="n">X_train_PCA_dbscanClustered</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
&#13;
<code class="n">overallAccuracy_dbscan</code></pre>&#13;
&#13;
<p>We will keep the <code>min_samples</code> at the default value of five, but we will adjust the <code>eps</code> to three to avoid having too few points clustered.</p>&#13;
&#13;
<p>Here is the overall accuracy:</p>&#13;
&#13;
<pre data-type="programlisting">Overall accuracy from DBSCAN: 0.242</pre>&#13;
&#13;
<p>As you can see, the accuracy is very poor compared to <em>k</em>-means and hierarchical clustering. We can fidget with the parameters <code>eps</code> and <code>min_samples</code> to improve the results, but it appears that DBSCAN is poorly suited to cluster the observations for this particular dataset.</p>&#13;
&#13;
<p>To explore why, let’s look at the clusters (<a data-type="xref" href="#cluster_results_for_dbscan">Table 5-3</a>).</p>&#13;
<table id="cluster_results_for_dbscan">&#13;
<caption><span class="label">Table 5-3. </span>Cluster results for DBSCAN</caption>&#13;
<thead>&#13;
<tr>&#13;
<th/>&#13;
<th>cluster</th>&#13;
<th>clusterCount</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>0</p></td>&#13;
<td><p>–1</p></td>&#13;
<td><p>39575</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>1</p></td>&#13;
<td><p>0</p></td>&#13;
<td><p>8885</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>2</p></td>&#13;
<td><p>8</p></td>&#13;
<td><p>720</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>3</p></td>&#13;
<td><p>5</p></td>&#13;
<td><p>92</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>4</p></td>&#13;
<td><p>18</p></td>&#13;
<td><p>51</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>5</p></td>&#13;
<td><p>38</p></td>&#13;
<td><p>38</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>6</p></td>&#13;
<td><p>41</p></td>&#13;
<td><p>22</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>7</p></td>&#13;
<td><p>39</p></td>&#13;
<td><p>22</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>8</p></td>&#13;
<td><p>4</p></td>&#13;
<td><p>16</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>9</p></td>&#13;
<td><p>20</p></td>&#13;
<td><p>16</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>Most of the points are unclustered. You can see this in the plot. 39,651 points—out of the 50,000 observations in the training set—are in cluster -1, which means that they do not belong to any cluster. They are labeled as outliers—noise, in other words.</p>&#13;
&#13;
<p>8,885 points belong in cluster 0. Then, there is a long tail of smaller-sized clusters. It appears that DBSCAN has a hard time finding distinct dense groups of points, and, therefore, does a poor job of clustering the MNIST images based on the digits they display.<a data-primary="" data-startref="dbscan05" data-type="indexterm" id="idm140637545026192"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="HDBSCAN" data-type="sect2"><div class="sect2" id="idm140637545113072">&#13;
<h2>HDBSCAN</h2>&#13;
&#13;
<p>Let’s<a data-primary="HDBSCAN (hierarchical DBSCAN)" data-type="indexterm" id="idm140637545023680"/> try another version of DBSCAN and see if the results improve. This one is known as <em>HDBSCAN</em>, or <em>hierarchical DBSCAN</em>. The takes the DBSCAN algorithm we introduced and converts it into a hierarchical clustering algorithm. In other words, it groups based on density and then links the density-based clusters based on distance iteratively, like in the hierarchical clustering algorithm we introduced in an earlier section.</p>&#13;
&#13;
<p>The two main parameters for this algorithm are <code>min_cluster_size</code> and <code>min_samples</code>, which defaults to <code>min_cluster_size</code> when set to <code>None</code>. Let’s use the out-of-the-box parameter selections and gauge if HDBSCAN performs better than DBSCAN did for our MNIST digits dataset:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">hdbscan</code>&#13;
&#13;
<code class="n">min_cluster_size</code> <code class="o">=</code> <code class="mi">30</code>&#13;
<code class="n">min_samples</code> <code class="o">=</code> <code class="bp">None</code>&#13;
<code class="n">alpha</code> <code class="o">=</code> <code class="mf">1.0</code>&#13;
<code class="n">cluster_selection_method</code> <code class="o">=</code> <code class="s1">'eom'</code>&#13;
&#13;
<code class="n">hdb</code> <code class="o">=</code> <code class="n">hdbscan</code><code class="o">.</code><code class="n">HDBSCAN</code><code class="p">(</code><code class="n">min_cluster_size</code><code class="o">=</code><code class="n">min_cluster_size</code><code class="p">,</code> \&#13;
        <code class="n">min_samples</code><code class="o">=</code><code class="n">min_samples</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="n">alpha</code><code class="p">,</code> \&#13;
        <code class="n">cluster_selection_method</code><code class="o">=</code><code class="n">cluster_selection_method</code><code class="p">)</code>&#13;
&#13;
<code class="n">cutoff</code> <code class="o">=</code> <code class="mi">10</code>&#13;
<code class="n">X_train_PCA_hdbscanClustered</code> <code class="o">=</code> \&#13;
    <code class="n">hdb</code><code class="o">.</code><code class="n">fit_predict</code><code class="p">(</code><code class="n">X_train_PCA</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code><code class="mi">0</code><code class="p">:</code><code class="n">cutoff</code><code class="p">])</code>&#13;
&#13;
<code class="n">X_train_PCA_hdbscanClustered</code> <code class="o">=</code> \&#13;
    <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train_PCA_hdbscanClustered</code><code class="p">,</code> \&#13;
    <code class="n">index</code><code class="o">=</code><code class="n">X_train</code><code class="o">.</code><code class="n">index</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'cluster'</code><code class="p">])</code>&#13;
&#13;
<code class="n">countByCluster_hdbscan</code><code class="p">,</code> <code class="n">countByLabel_hdbscan</code><code class="p">,</code> \&#13;
    <code class="n">countMostFreq_hdbscan</code><code class="p">,</code> <code class="n">accuracyDF_hdbscan</code><code class="p">,</code> \&#13;
    <code class="n">overallAccuracy_hdbscan</code><code class="p">,</code> <code class="n">accuracyByLabel_hdbscan</code> \&#13;
    <code class="o">=</code> <code class="n">analyzeCluster</code><code class="p">(</code><code class="n">X_train_PCA_hdbscanClustered</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code></pre>&#13;
&#13;
<p>Here is the overall accuracy:</p>&#13;
&#13;
<pre data-type="programlisting">Overall accuracy from HDBSCAN: 0.24696</pre>&#13;
&#13;
<p>At 25%, this is only marginally better than that of DBSCAN and well short of the 70%-plus achieved by <em>k</em>-means and hierarchical clustering. <a data-type="xref" href="#cluster_results_for_hdbscan">Table 5-4</a> displays the accuracy of the various clusters.</p>&#13;
<table class="pagebreak-before" id="cluster_results_for_hdbscan">&#13;
<caption><span class="label">Table 5-4. </span>Cluster results for HDBSCAN</caption>&#13;
<thead>&#13;
<tr>&#13;
<th/>&#13;
<th>cluster</th>&#13;
<th>clusterCount</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>0</p></td>&#13;
<td><p>–1</p></td>&#13;
<td><p>42570</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>1</p></td>&#13;
<td><p>4</p></td>&#13;
<td><p>5140</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>2</p></td>&#13;
<td><p>7</p></td>&#13;
<td><p>942</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>3</p></td>&#13;
<td><p>0</p></td>&#13;
<td><p>605</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>4</p></td>&#13;
<td><p>6</p></td>&#13;
<td><p>295</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>5</p></td>&#13;
<td><p>3</p></td>&#13;
<td><p>252</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>6</p></td>&#13;
<td><p>1</p></td>&#13;
<td><p>119</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>7</p></td>&#13;
<td><p>5</p></td>&#13;
<td><p>45</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>8</p></td>&#13;
<td><p>2</p></td>&#13;
<td><p>32</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>We see a similar phenomenon as we did for DBSCAN. Most points are unclustered, and then there is a long tail of small-sized clusters. The results do not improve much.<a data-primary="" data-startref="SCLcluster05" data-type="indexterm" id="idm140637544827664"/><a data-primary="" data-startref="Cdbscan05" data-type="indexterm" id="idm140637544826688"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="idm140637544825616">&#13;
<h1>Conclusion</h1>&#13;
&#13;
<p>In this chapter, we introduced three major types of clustering algorithms—<em>k</em>-means, hierarchical clustering, and DBSCAN—and applied them to a dimensionality-reduced version of the MNIST digits dataset. The first two clustering algorithms <span class="keep-together">performed</span> very well on the dataset, grouping the images well enough to have a 70%-plus consistency in labels across the clusters.</p>&#13;
&#13;
<p>DBSCAN did not perform quite so well for this dataset but remains a viable clustering algorithm. Now that we’ve introduced the clustering algorithms, let’s build an applied unsupervised learning solution using these algorithms in <a data-type="xref" href="ch06.html#Chapter_6">Chapter 6</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm140637545889136"><sup><a href="ch05.html#idm140637545889136-marker">1</a></sup> For more on <a href="https://pypi.org/project/fastcluster/">fastcluster</a>, check out the project’s web page.</p></div></div></section></body></html>