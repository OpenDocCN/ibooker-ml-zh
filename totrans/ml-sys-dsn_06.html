<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">5</span> </span> <span class="chapter-title-text">Loss functions and metrics</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">Selecting proper metrics and losses for your machine learning system</li> 
    <li class="readable-text" id="p3">Defining and utilizing proxy metrics</li> 
    <li class="readable-text" id="p4">Applying the hierarchy of metrics</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p5"> 
   <p>In the previous chapter, we first touched on the topic of creating a design document for your machine learning (ML) system. We figured out why a design document is subject to constant edits and why all the changes you implement in it are not only inevitable but also necessary.</p> 
  </div> 
  <div class="readable-text intended-text" id="p6"> 
   <p>Unfortunately, an ML system can’t directly solve a problem, but it can try to approximate it by optimizing a specific task. To do that efficiently, it must be adjusted, appropriately guided, and monitored.</p> 
  </div> 
  <div class="readable-text intended-text" id="p7"> 
   <p>To direct an ML system’s effort, we use its algorithm’s loss function to reward or punish if for reducing or increasing specific errors. However, the loss function is used to train the model and usually must be differentiable, meaning that there is a narrowed choice of available loss functions. Thus, to assess the model’s performance, we use metrics; and while every loss function can be used as a metric (a good example would be root mean squared error [RMSE], which is quite often used as a metric, although we are not sure that is the best decision), not every metric can be used as a loss function.</p> 
  </div> 
  <div class="readable-text intended-text" id="p8"> 
   <p>In this chapter, we will discuss how to pick the best-fitting metrics and loss functions, focusing on how to do proper research and provide motivation for choice during the design process.</p> 
  </div> 
  <div class="readable-text" id="p9"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_36"><span class="num-string">5.1</span> Losses</h2> 
  </div> 
  <div class="readable-text" id="p10"> 
   <p>The <em>loss function</em>, also known as the <em>objective</em> or <em>cost function</em>, effectively defines how a model learns about the world and the connections between dependent and independent variables, what it pays the most attention to, what it tries to avoid, and what it considers acceptable. Thus, the choice of a loss function can drastically affect your model’s overall performance, even if everything else—features, target, model architecture, dataset size—remains unchanged. Switching to a different loss function can completely reshape your whole system. </p> 
  </div> 
  <div class="readable-text intended-text" id="p11"> 
   <p>Picking the right loss function (i.e., choosing the way a model learns from its mistakes) is one of the most crucial decisions in designing an ML system. Recalling an evergreen anecdote, we can be pretty confident in optimizing for the mean while counting the average salary of bar visitors until Bill Gates walks in (<a href="https://mng.bz/M1w8">https://mng.bz/M1w8</a>). </p> 
  </div> 
  <div class="readable-text intended-text" id="p12"> 
   <p>Unfortunately, not every function can be used as a loss function. In general, a loss function feature two properties: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p13"> It is globally continuous (changes in predictions lead to changes in losses). </li> 
   <li class="readable-text" id="p14"> It is differentiable (its gradient can be calculated for optimization algorithms based on the gradient descent). There is one exclusion: in exotic cases, gradient-free optimization methods are applicable, although practitioners prefer to avoid them as gradient-based methods typically converge much better. </li> 
  </ul> 
  <div class="readable-text" id="p15"> 
   <p>While these two points are relevant for any loss, it is important to select a loss function that will best match your particular case and will be closest to the final goal of your system. </p> 
  </div> 
  <div class="readable-text intended-text" id="p16"> 
   <p>This is where advanced loss functions come into play, providing tempting ways of improving your model. Unlike manipulations with features or the model itself, they don’t usually affect the runtime aspect, meaning that all the code changes are only related to training pipelines, and isolating changes to a small part of a system is always a good property of design. But more often than not, we have witnessed ML engineers (especially recent graduates) sticking to a particular loss function just because they got used to applying it to similar problems. A notorious example is the regression problem with the mean squared error (MSE) or mean absolute error (MAE) loss function as the default choice and, many times, <em>the only choice</em> by many practitioners.</p> 
  </div> 
  <div class="readable-text intended-text" id="p17"> 
   <p>At the same time, while choosing a proper loss function (or a set of them) is a decision that may greatly improve your model’s performance, it is still not a silver bullet. We have worked with a few ML engineers (often with respectable academic backgrounds and PhDs) who tried to solve all the problems they had with just one elegant loss function. This approach is on the opposite end of the spectrum from paying no attention to the loss function at all, but it is still far from ideal. A good ML system designer keeps many tools in mind, not overfitting for one. Overall, the heuristic is the following: the more research-heavy your system is, the more likely it is that you need to invest time in finding or designing a nontrivial loss function.</p> 
  </div> 
  <div class="readable-text intended-text" id="p18"> 
   <p>A couple of years ago, Valerii worked with an intern on building a model to predict the exchange volume of cryptocurrencies. As always, he asked the intern to prepare a design document before doing anything, and this was an insightful exercise. The intern thoughtlessly skipped the loss function chapter, listing some metrics he would use to assess the system performance without any reasoning behind them.</p> 
  </div> 
  <div class="readable-text intended-text" id="p19"> 
   <p>Why is this not acceptable? By using an example, we can review a simplified situation with a knowledge of loss functions for regression problems being narrowed down to the two most widely used loss functions: MSE and MAE.</p> 
  </div> 
  <div class="readable-text intended-text" id="p20"> 
   <p>Imagine that we have a vector of target values Y = [100, 100, 100, 100, 100, 100, 100, 100, 100, 1000] and a vector of independent variables X being equal for all samples.</p> 
  </div> 
  <div class="readable-text intended-text" id="p21"> 
   <p>If we train a model using MSE as a loss function, it will output a vector of predictions:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p22"> 
   <div class="code-area-container"> 
    <pre class="code-area">Y_hat = [190, 190, 190, 190, 190, 190, 190, 190, 190, 190]</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p23"> 
   <p>If we train a model using MAE as a loss function, it will output a vector of predictions:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p24"> 
   <div class="code-area-container"> 
    <pre class="code-area">Y_hat = [100, 100, 100, 100, 100, 100, 100, 100, 100, 100]</pre>  
   </div> 
  </div> 
  <div class="readable-text print-book-callout" id="p25"> 
   <p><span class="print-book-callout-head">NOTE</span>  Please note that this is a thought experiment to highlight the idea and make it easier to comprehend. If we needed to, we could create synthetic data to reproduce the whole process—features, targets, and models—but for the sake of simplicity, we will use only the preceding numbers.</p> 
  </div> 
  <div class="readable-text" id="p26"> 
   <p>When we calculate MSE and MAE for a model with the RMSE loss function, it will result in the following numbers: MSE = 72,900, MAE = 162, with the mean of residuals equal to 0 and the median of residuals equal to –90 (figure 5.1).<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p27">  
   <img alt="figure" src="../Images/CH05_F01_Babushkin.png" width="576" height="388"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.1</span> Residuals after optimizing the mean </h5>
  </div> 
  <div class="readable-text" id="p28"> 
   <p>When we calculate MSE and MAE for a model with the MAE loss function, the result will be MSE = 81,000, MAE = 90, with the mean of residuals equal to 90 and the median of residuals equal to 0 (figure 5.2).<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p29">  
   <img alt="figure" src="../Images/CH05_F02_Babushkin.png" width="579" height="388"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.2</span> Residuals after optimizing the median</h5>
  </div> 
  <div class="readable-text" id="p30"> 
   <p>No wonder the model optimized for MSE yields better MSE, and as MSE tries to minimize the mean, the mean residuals are better. On the other hand, the model optimized for MAE delivers better MAE, and as MAE tries to optimize the median, the median residuals are better. But what does it mean for us? Which loss function is better? That depends on our application.</p> 
  </div> 
  <div class="readable-text intended-text" id="p31"> 
   <p>Let’s say we are optimizing a navigation system for aircraft, and an error larger than 850 means that a plane will go off a landing field and crash. In this case, optimizing for MAE is not an ideal decision. Of course, we can say 9 out of 10 times that we have a perfect result, and only 1 out of 10 times a vehicle is destroyed, but this is not acceptable by any means. We have to avoid outliers at all costs or penalize them, thus using MSE or even some higher-degree modifications.</p> 
  </div> 
  <div class="readable-text intended-text" id="p32"> 
   <p>But suppose we are optimizing the amount of liquidity for a cryptocurrency exchange we need for every trading day. <em>Liquidity</em> refers to a cryptocurrency’s capacity to be converted into cash or other cryptocurrencies without losing value, and it is essential for all cryptocurrency exchanges. High liquidity signifies a dynamic and stable market, allowing participants to trade quickly at reasonable prices. Excessive liquidity, however, means that allocated resources are not used. In this case, reserving more cash than required 9 times out of 10 is far from desired. We can review it from a different angle: the model optimized for MSE overallocated 810 units and underallocated 810 units, while the model optimized for MAE was on the spot 9 times out of 10 and underallocated 900 units, which seems like a better decision (if underallocation is less than 9 times worse than over allocation) to convey to the model what we need.</p> 
  </div> 
  <div class="readable-text intended-text" id="p33"> 
   <p>It’s easy to see that even though we used MSE and MAE to train the models, we applied different criteria to assess them. For the aircraft navigation system, we counted the number of times when the difference between the actual and predicted value was greater than 850. For liquidity optimization, it was the number of times we were on the spot or under an overallocation weighted sum. This illustrates that training the model to optimize specific loss functions and assess this model’s performance can represent two different tasks, which we will cover in section 5.2 on metrics. Before we proceed, we’d like to share some insights on the nuances and aspects of determining losses for deep learning models.</p> 
  </div> 
  <div class="readable-text" id="p34"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_37"><span class="num-string">5.1.1</span> Loss tricks for deep learning models</h3> 
  </div> 
  <div class="readable-text" id="p35"> 
   <p>In deep-learning-based systems, especially those processing text, image, or audio data, loss selection is even more crucial.</p> 
  </div> 
  <div class="readable-text intended-text" id="p36"> 
   <p>A properly chosen loss function can help with many problems related to model training, especially a sophisticated model and/or data domain. For example, a cross-entropy loss is a classical solution for the classification problem. One of the problems with it is related to class imbalance. If one class is heavily overrepresented, the model optimized by the entropy loss may face something called <em>mode collapse</em>—a situation when it outputs a constant (popular class) for any input. These problems have been solved in many ways (e.g., data undersampling/oversampling, custom weights for classes, etc.), but all of them required significant manual tuning and were not reliable. The problem was approached by researchers who tried to design a loss addressing it; the most notable result is probably by Lin et al. (“Focal Loss for Dense Object Detection,” <a href="https://arxiv.org/abs/1708.02002">https://arxiv.org/abs/1708.02002</a>), and this loss is now taking its honorable place among tools helping to solve the data imbalance problem.</p> 
  </div> 
  <div class="readable-text intended-text" id="p37"> 
   <p>Focal loss (see figure 5.3) is a dynamically scaled cross-entropy loss where the scaling factor decays to zero as confidence in the correct class increases. Intuitively, this scaling factor can automatically down-weight the contribution of easy examples during training and rapidly focus the model on hard examples (more information can be found at <a href="https://paperswithcode.com/method/focal-loss">https://paperswithcode.com/method/focal-loss</a>). <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p38">  
   <img alt="figure" src="../Images/CH05_F03_Babushkin.png" width="990" height="572"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.3</span> The suggested focal loss function focuses more on misclassified examples while reducing the relative loss for well-classified examples (source: Lin et al.). </h5>
  </div> 
  <div class="readable-text intended-text" id="p39"> 
   <p>Originally, this loss was introduced for the object detection problem specific to computer vision, and later, the approach expanded to many other domains, including those unrelated to images, like audio or natural language processing. The most distant application of the focal loss we have found has been introduced in the paper “Can Natural Language Processing Help Differentiate Inflammatory Intestinal Diseases in China?” (Tong et al.; <a href="https://mng.bz/aV9X">https://mng.bz/aV9X</a>), which confirms how ideas spread across domains.</p> 
  </div> 
  <div class="readable-text intended-text" id="p40"> 
   <p>In some cases, a reasonable solution will be to combine multiple losses for a single model. The need for such an approach may arise with complex problems, often multimodal and often associated with multiple concurrent datasets. We will not provide many details on using combined loss functions here as it is research-heavy, but we would like to give some examples: </p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p41"> “Authentic Volumetric Avatars from a Phone Scan” (Cao et al.; <a href="https://dl.acm.org/doi/abs/10.1145/3528223.3530143">https://dl.acm.org/doi/abs/10.1145/3528223.3530143</a>). The authors combined three families of losses (segmentation, reconstruction, perceptual). Generative computer vision models are often subject to considering combined losses. </li> 
   <li class="readable-text" id="p42"> “Highly Accurate Protein Structure Prediction with AlphaFold” (Jumper et al.; <a href="https://www.nature.com/articles/s41586-021-03819-2">https://www.nature.com/articles/s41586-021-03819-2</a>). The famous AlphaFold 2 model predicts 3D shapes of proteins from their genetic sequence with impressive accuracy. That’s a huge thing for the biotech world, and it uses multiple auxiliary losses under the hood. For example, a masked language modeling objective, the one that is likely to be inspired by a loss function used in BERT-like architectures, is a popular family of natural language processing models. </li> 
   <li class="readable-text" id="p43"> “GrokNet: Unified Computer Vision Model Trunk and Embeddings for Commerce” (Bell et al.; <a href="https://mng.bz/Xxr6">https://mng.bz/Xxr6</a>). This is a jewel among combined loss examples we can recall. The authors aim to build a single model to rule multiple problems, so they used 7 goods datasets and 83 (80 categorical and 3 embedding) losses! </li> 
  </ol> 
  <div class="readable-text" id="p44"> 
   <p>In general, multiple losses are usually used either to help models’ convergence or to solve multiple adjustment problems with a single model.</p> 
  </div> 
  <div class="readable-text intended-text" id="p45"> 
   <p>While loss functions help set up and fine-tune accuracy and efficiency and minimize errors for your system while training, metrics are used to evaluate its performance within a certain set of parameters.</p> 
  </div> 
  <div class="readable-text" id="p46"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_38"><span class="num-string">5.2</span> Metrics </h2> 
  </div> 
  <div class="readable-text" id="p47"> 
   <p>The loss function we optimize and the metric we use to assess our model’s performance can be very different from each other. Recall that the end goal of the demand forecast system for Supermegaretail in chapter 4 was to reduce the gap between delivered and sold items, making it as narrow as possible while avoiding an out-of-stock situation. If we try to visualize the pipeline, it might look like figure 5.4.</p> 
  </div> 
  <div class="readable-text intended-text" id="p48"> 
   <p>We know that a proper loss function is essential, but what about metrics? Can’t we pick some standard metrics, assess a variety of models, choose the best, deploy it, and estimate potential success through A/B tests?</p> 
  </div> 
  <div class="browsable-container figure-container" id="p49">  
   <img alt="figure" src="../Images/CH05_F04_Babushkin.png" width="1013" height="200"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.4</span> A general-purpose pipeline for a demand forecast system that perfectly fits the Supermegaretail case <span class="aframe-location"/></h5>
  </div> 
  <div class="readable-text" id="p50"> 
   <p>Unfortunately, no. Choosing the right set of metrics has to follow just as carefully an elaboration as selecting loss functions. Even more, while the set of popular losses is finite, there is always an opportunity to tailor a custom metric for a specific business domain. Choosing the wrong metric, in its turn, can cause misguided optimization when we set our model to train for irrelevant values, which eventually leads to poor performance in real-world scenarios. As a result, we have to roll back several steps in model development, resulting in a significant waste of time and resources. But even choosing the right metric for your ML system will not guarantee the project’s success.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p51"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Campfire story from Valerii</h5> 
   </div> 
   <div class="readable-text" id="p52"> 
    <p>Some time ago, I was developing an ML system for a bank that regularly encountered the problem of nonpaying debtors. The system we were preparing had two main goals:</p> 
   </div> 
   <ul> 
    <li class="readable-text" id="p53"> Reduce the number of delinquent payments </li> 
    <li class="readable-text" id="p54"> Make customers more responsive </li> 
   </ul> 
   <div class="readable-text" id="p55"> 
    <p>As a metric, we chose the conversion rate of clients from nonpayers to payers. </p> 
   </div> 
   <div class="readable-text" id="p56"> 
    <p>The first thing we did was to implement a system of promised payments that worked as follows. Let’s say Mr. Smith gets a call from the bank: “Mr. Smith, you haven’t paid your loan on time. Can we expect you to pay the required amount within three days?” “Oh, of course, I will, I will,” says Mr. Smith. The people at the bank hang up and check the “promised to pay” box. But then Mr. Smith would break the promise and not pay anything. </p> 
   </div> 
   <div class="readable-text" id="p57"> 
    <p>The conversion rate by the time we started our work was 0.5, which means cases like that were occurring half the time. It’s not that bad but definitely not brilliant.</p> 
   </div> 
   <div class="readable-text" id="p58"> 
    <p>Given the attitude of people to such calls from banks and their desire to hang up as soon as possible, broken promises are a very common case. But the fact is, it’s a stick with two ends. On the one hand, the client won’t find it pleasant to talk to the bank, especially if they did not initiate the conversation. But the bank also isn’t interested in futile communication, having to overspend on call centers and employees. </p> 
   </div> 
   <div class="readable-text" id="p59"> 
    <p>As a solution, we built a system to predict the probability of clients agreeing to make their payment and fulfilling it. And we replaced human calls with text messages. This spared us from having to call our customers and talk them into making promises. The system was also supposed to predict customer behavior. </p> 
   </div> 
   <div class="readable-text" id="p60"> 
    <p>At the validation stage, the system showed a conversion rate of 0.9—almost twice as high as manual work! Two weeks later and in combat conditions, however, the conversion plummeted to 0.35, and we had only a week until making a report to our vice president.</p> 
   </div> 
   <div class="readable-text" id="p61"> 
    <p>Something had obviously gone wrong, and we needed to figure out what it was. We examined how this metric worked before, and it was pretty simple: if the client had promised to pay the debt on a certain day of the month but did not do it within 3 days, they were marked as debtors. Why was it 3 days? The answer is that the gap between an actual operation and getting information about this operation in the bank’s database was 3 days. </p> 
   </div> 
   <div class="readable-text" id="p62"> 
    <p>Let’s say you are supposed to make your next loan payment by EOD March 1. At the end of the day, March 1, you go to the bank after work and pay the required amount. On March 2, a system checks the database and sees that the payment has not been made (no wonder, as the information will not reach it until March 4). “Looks like we have a delinquent,” the system thinks and initiates a text message because, according to the data collected by the system, you have a high probability (90%!) of paying the required amount after receiving the message. Later on March 2, you get a text message from the bank asking you to pay the loan. “They must have got something wrong. I’ll let them know I’ve already paid,” you think and start filling out the form in the reply message. The problem is that the form does not allow you to enter a payment due date earlier than the current date. You can only specify that you will pay on March 2 or later. But you already paid on March 1. What do you do? You indicate that you paid on March 2 and submit the form. Three days later, the system checks the list of nonpayers, opens your profile, and sees that you promised to pay on March 2 but haven’t done that within 3 days from this date.</p> 
   </div> 
   <div class="readable-text" id="p63"> 
    <p>When we reconfigured the system, the conversion rate almost reached the initial value, getting as high as 0.8, but the interim problems we faced show how reaching your metrics can be hindered by flaws in the overall system behavior.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p64"> 
   <p>On the surface, a framework for picking the right metric is very straightforward: choose the one that is closest to the final goal. However, as the next campfire story will show, it might be very tricky to do. You can try either finding that metric yourself or using some outside help. The following are some options we recommend considering: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p65"> If you’re lucky enough to have a hierarchy of metrics, which we will cover later on in this chapter, use it to navigate to the metric you need. </li> 
   <li class="readable-text" id="p66"> Some companies have a dedicated department working on metrics; if that is the case, use their help. </li> 
   <li class="readable-text" id="p67"> If neither of these two options is the case, you might use product managers and data scientists to develop the best metric. </li> 
   <li class="readable-text" id="p68"> If the problem you’re set to solve is similar to a problem solved before and the solution proved to be solid and efficient, it is natural to transfer metrics from one project to another with certain modifications, if necessary. </li> 
   <li class="readable-text" id="p69"> If you have an A/B testing team, they also usually have enough knowledge to select or create a metric. </li> 
  </ul> 
  <div class="readable-text" id="p70"> 
   <p>If you don’t have the luxury of having the things mentioned here, you can do the following:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p71"> Refer to the goals section in the design and align with it (it is essential to refresh what the end goals are, not how you remembered them). Knowing your goals will help you understand which metrics will help you achieve those goals or at least help you discard obviously inappropriate metrics. </li> 
   <li class="readable-text" id="p72"> Try to decompose the end goal by writing a map similar to the hierarchy of metrics (see section 5.2.2). It will probably take more than one stage to achieve, but this kind of exercise will help you break down your big goal into several smaller components, each with its own metric. Having many small parts on hand will help assemble the greater whole. </li> 
   <li class="readable-text" id="p73"> Find the best metrics describing the success of each stage. </li> 
   <li class="readable-text" id="p74"> If something is hard to measure directly, replace it with proxy metrics (see section 5.2.2). Proxy metrics will allow you to gather necessary and very important information before your system goes into release. </li> 
   <li class="readable-text" id="p75"> With this map, pick the metric that either represents the most critical stage or summarizes the map in the best possible way. </li> 
  </ul> 
  <div class="readable-text" id="p76"> 
   <p>In the next campfire story, we will review the canonical binary classification problem.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p77"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Campfire story from Valerii</h5> 
   </div> 
   <div class="readable-text" id="p78"> 
    <p>Recently, I had a conversation with a friend of mine regarding the evaluation of fraud models. Fraud models usually try to solve binary classification tasks where 0 is nonfraud and 1 is fraud.</p> 
   </div> 
   <div class="readable-text" id="p79"> 
    <p>No metric is ideal, and it always depends on the final goal. However, when we speak about fraud models, we usually want to maintain a ratio of fraud to legit transactions of some level. If we had 10 times more transactions, it would be okay to have 10 times more fraud, but not 20 or 30 times more. In other words, we want to have a probabilistic model.</p> 
   </div> 
   <div class="readable-text" id="p80"> 
    <p>Also, fraud usually belongs to the class imbalance problem, and that balance is not stable through time. One day the ratio can be 1:100 (outburst of fraudulent transactions), the next day, 1:1000 (an ordinary day), and the day after, 1:10,000 (fraudsters took a vacation). </p> 
   </div> 
   <div class="readable-text" id="p81"> 
    <p>The most popular set of metrics for this family of models is precision and recall, which may not be the best choice.</p> 
   </div> 
   <div class="readable-text" id="p82"> 
    <p>The problem with precision is that its calculations take both classes into account: <span class="aframe-location"/></p> 
   </div> 
   <div class="browsable-container figure-container" id="p83"> 
    <img alt="sidebar figure" src="../Images/babushkin-ch5-eqs-0x.png" width="238" height="55"/> 
   </div> 
   <div class="readable-text" id="p84"> 
    <p>Imagine that we have a model that has a probability of 95% to predict that fraud is fraud (true positive [TP]) and 5% to predict that nonfraud is fraud (false positive [FP]).</p> 
   </div> 
   <div class="readable-text" id="p85"> 
    <p>Let’s review three scenarios where P is the number of positive samples and N is the number of negative samples:</p> 
   </div> 
   <ul> 
    <li class="readable-text" id="p86"> P = 10,000, N = 10,000, <span class="aframe-location"/> </li> 
   </ul> 
   <div class="browsable-container figure-container" id="p87"> 
    <img alt="sidebar figure" src="../Images/babushkin-ch5-eqs-1x.png" width="484" height="42"/> 
   </div> 
   <ul> 
    <li class="readable-text" id="p88"> P = 100,000, N = 10,000, <span class="aframe-location"/> </li> 
   </ul> 
   <div class="browsable-container figure-container" id="p89"> 
    <img alt="sidebar figure" src="../Images/babushkin-ch5-eqs-2x.png" width="526" height="42"/> 
   </div> 
   <ul> 
    <li class="readable-text" id="p90"> P = 1000, N = 10,000, <span class="aframe-location"/> </li> 
   </ul> 
   <div class="browsable-container figure-container" id="p91"> 
    <img alt="sidebar figure" src="../Images/babushkin-ch5-eqs-3x.png" width="456" height="41"/> 
   </div> 
   <div class="readable-text" id="p92"> 
    <p>As you can see, the class balance affected the metric significantly even when nothing else changed.</p> 
   </div> 
   <div class="readable-text" id="p93"> 
    <p>Now let’s take a look at recall (Recall = TP/(TP+FN) = TP/P = True Positive Rate [TPR]) and examine the same three scenarios:</p> 
   </div> 
   <ul> 
    <li class="readable-text" id="p94"> P = 10,000, N = 10,000, <span class="aframe-location"/> </li> 
   </ul> 
   <div class="browsable-container figure-container" id="p95"> 
    <img alt="sidebar figure" src="../Images/babushkin-ch5-eqs-4bx.png" width="250" height="41"/> 
   </div> 
   <ul> 
    <li class="readable-text" id="p96"> P = 100,000, N = 10,000, <span class="aframe-location"/> </li> 
   </ul> 
   <div class="browsable-container figure-container" id="p97"> 
    <img alt="sidebar figure" src="../Images/babushkin-ch5-eqs-4x.png" width="265" height="41"/> 
   </div> 
   <ul> 
    <li class="readable-text" id="p98"> P = 1000, N = 10,000, <span class="aframe-location"/> </li> 
   </ul> 
   <div class="browsable-container figure-container" id="p99"> 
    <img alt="sidebar figure" src="../Images/babushkin-ch5-eqs-5x.png" width="235" height="38"/> 
   </div> 
   <div class="readable-text" id="p100"> 
    <p>In this case, the class balance didn’t affect the metric at all.</p> 
   </div> 
   <div class="readable-text" id="p101"> 
    <p>There is also a metric called specificity that can replace precision:<img alt="inline figure" src="../Images/babushkin-ch5-eqs-6x.png" width="1100" height="67"/><span class="aframe-location"/></p> 
   </div> 
   <div class="browsable-container figure-container" id="p102"> 
    <img alt="sidebar figure" src="../Images/babushkin-ch5-eqs-7x.png" width="211" height="45"/> 
   </div> 
   <div class="readable-text" id="p103"> 
    <p>The same three examples show the following:</p> 
   </div> 
   <ul> 
    <li class="readable-text" id="p104"> P = 10,000, N = 10,000, <span class="aframe-location"/> </li> 
   </ul> 
   <div class="browsable-container figure-container" id="p105"> 
    <img alt="sidebar figure" src="../Images/babushkin-ch5-eqs-8x.png" width="288" height="41"/> 
   </div> 
   <ul> 
    <li class="readable-text" id="p106"> P = 100,000, N = 10,000, <span class="aframe-location"/> </li> 
   </ul> 
   <div class="browsable-container figure-container" id="p107"> 
    <img alt="sidebar figure" src="../Images/babushkin-ch5-eqs-8x.png" width="288" height="41"/> 
   </div> 
   <ul> 
    <li class="readable-text" id="p108"> P = 1000, N = 10,000, <span class="aframe-location"/> </li> 
   </ul> 
   <div class="browsable-container figure-container" id="p109"> 
    <img alt="sidebar figure" src="../Images/babushkin-ch5-eqs-8x.png" width="288" height="41"/> 
   </div> 
   <div class="readable-text" id="p110"> 
    <p>Recall and specificity do not change because of class imbalance, as these metrics are class-balance insensitive. </p> 
   </div> 
   <div class="readable-text" id="p111"> 
    <p>Initially, my friend created a notebook (<a href="https://mng.bz/5Ov8">https://mng.bz/5Ov8</a>) to prove me wrong. The following code demonstrates his train of thought: </p> 
   </div> 
   <div class="browsable-container listing-container" id="p112"> 
    <div class="code-area-container"> 
     <pre class="code-area">import numpy as np

def gen_labels_preds(fraud, genuine, fraud_predicted, correct_fraud_predicted):
  labels = np.concatenate([np.repeat(True, fraud), np.repeat(False, genuine)])
  preds = np.concatenate([
    np.repeat(True, correct_fraud_predicted), # TP
    np.repeat(False, fraud - correct_fraud_predicted), # FP
    np.repeat(True, fraud_predicted - correct_fraud_predicted), # FN
    np.repeat(False, genuine - (fraud_predicted - correct_fraud_predicted)) # TN
    ])

  return labels, preds

def calculate_metrics(labels, preds):
  TP = (preds &amp; labels).sum()
  FP = (preds &amp; ~labels).sum()
  TN = (~preds &amp; ~labels).sum()
  FN = (~preds &amp; labels).sum()

  recall = TP / (TP + FN)
  precision = TP / (TP + FP)
  FPR = FP / (FP + TN)

  return recall, precision, FPR</pre>  
    </div> 
   </div> 
   <div class="readable-text" id="p113"> 
    <p>He devised two models with the following metrics:</p> 
   </div> 
   <ul> 
    <li class="readable-text" id="p114"> A has 20 false positives, and 80% of the fraud is caught. </li> 
    <li class="readable-text" id="p115"> B has 920 false positives, and 80% of the fraud is caught. </li> 
   </ul> 
   <div class="readable-text" id="p116"> 
    <p>Then he tried his two models in three scenarios with different numbers of transactions and fraud cases. In scenario 1, the number of transactions was 100,000. Overall, there were 100 fraud cases, so the class balance was 1:1,000:</p> 
   </div> 
   <div class="browsable-container listing-container" id="p117"> 
    <div class="code-area-container"> 
     <pre class="code-area">fraud = 100 # high imbalance
genuine = 100000
model_A_FP = 20
model_B_FP = 920


# Model A
a_total_fraud_predicted = model_A_FP + fraud*0.8
a_correct_fraud_predicted = fraud*0.8

a_labels, a_preds = gen_labels_preds(fraud, genuine,
a_total_fraud_predicted, a_correct_fraud_predicted)
a_recall, a_precision, a_FPR = calculate_metrics(a_labels, a_preds)

# Model B

b_total_fraud_predicted = model_B_FP + fraud*0.8 
# Flags many more transactions
b_correct_fraud_predicted = fraud*0.8
b_labels, b_preds = gen_labels_preds(fraud, genuine,b_total_fraud_predicted, b_correct_fraud_predicted)
b_recall, b_precision, b_FPR = calculate_metrics(b_labels, b_preds)

print("Model A Performance Metrics:")
print('TPR:', a_recall)
print("Precision:", a_precision)
print("FPR:", a_FPR)

print("\nModel B Performance Metrics:")
print('TPR:', b_recall)
print("Precision:", b_precision)
print("FPR:", b_FPR)

Model A Performance Metrics:
TPR: 0.8
Precision: 0.8
FPR: 0.0002

Model B Performance Metrics:
TPR: 0.8
Precision: 0.08
FPR: 0.0092</pre>  
    </div> 
   </div> 
   <div class="readable-text" id="p118"> 
    <p>In scenario 2, he used the same metrics as in scenario 1. The number of transactions was 100,000. Overall, there were 10 fraud cases, so the class balance was 1:10,000:</p> 
   </div> 
   <div class="browsable-container listing-container" id="p119"> 
    <div class="code-area-container"> 
     <pre class="code-area">fraud = 10 # high imbalance
genuine = 100000

# Model A
a_total_fraud_predicted = model_A_FP + fraud*0.8
a_correct_fraud_predicted = fraud*0.8

a_labels, a_preds = gen_labels_preds(fraud, genuine, a_total_fraud_predicted, a_correct_fraud_predicted)
a_recall, a_precision, a_FPR = calculate_metrics(a_labels, a_preds)

# Model B
b_total_fraud_predicted = model_B_FP + fraud*0.8 # Flags many more transactions
b_correct_fraud_predicted = fraud*0.8

b_labels, b_preds = gen_labels_preds(fraud, genuine, b_total_fraud_predicted, b_correct_fraud_predicted)
b_recall, b_precision, b_FPR = calculate_metrics(b_labels, b_preds)
print("Model A Performance Metrics:")
print('TPR:', a_recall)
print("Precision:", a_precision)
print("FPR:", a_FPR)

print("\nModel B Performance Metrics:")
print('TPR:', b_recall)
print("Precision:", b_precision)
print("FPR:", b_FPR)

Model A Performance Metrics:
TPR: 0.8
Precision: 0.2857142857142857
FPR: 0.0002

Model B Performance Metrics:
TPR: 0.8
Precision: 0.008620689655172414
FPR: 0.0092</pre>  
    </div> 
   </div> 
   <div class="readable-text" id="p120"> 
    <p>In scenario 3, he again used the same metrics and 100,000 transactions. Overall, there were 1,000 fraud cases, so the class balance was 1:100:</p> 
   </div> 
   <div class="browsable-container listing-container" id="p121"> 
    <div class="code-area-container"> 
     <pre class="code-area">fraud = 1000 # high imbalance
genuine = 100000

# Model A
a_total_fraud_predicted = model_A_FP + fraud*0.8
a_correct_fraud_predicted = fraud*0.8

a_labels, a_preds = gen_labels_preds(fraud, genuine, a_total_fraud_predicted, a_correct_fraud_predicted)
a_recall, a_precision, a_FPR = calculate_metrics(a_labels, a_preds)

# Model B
b_total_fraud_predicted = model_B_FP + fraud*0.8 # Flags many more transactions
b_correct_fraud_predicted = fraud*0.8

b_labels, b_preds = gen_labels_preds(fraud, genuine, b_total_fraud_predicted, b_correct_fraud_predicted)
b_recall, b_precision, b_FPR = calculate_metrics(b_labels, b_preds)

print("Model A Performance Metrics:")
print('TPR:', a_recall)
print("Precision:", a_precision)
print("FPR:", a_FPR)
print("\nModel B Performance Metrics:")
print('TPR:', b_recall)
print("Precision:", b_precision)
print("FPR:", b_FPR)
Model A Performance Metrics:
TPR: 0.8
Precision: 0.975609756097561
FPR: 0.0002

Model B Performance Metrics:
TPR: 0.8
Precision: 0.46511627906976744
FPR: 0.0092</pre>  
    </div> 
   </div> 
   <div class="readable-text" id="p122"> 
    <p>Model A is better according to both the receiver operating characteristic area under the curve (ROC AUC) and precision-recall AUC (PR AUC) metrics. Model B is a bad model but still gets a very good FPR (0.0092), even though, if it were put into production, the predictions would be rubbish (920 out of 1,000 fraud predictions would be incorrect). Precision allows us to see this. It’s just 0.08 for model B, so we’d never even think about putting it close to production.</p> 
   </div> 
   <div class="readable-text" id="p123"> 
    <p>What is the fallacy here?</p> 
   </div> 
   <div class="readable-text" id="p124"> 
    <p>First, model B has an FPR of 0.0092, which is 46 times higher than model A, with its FPR of 0.0002. There is no good or bad FPR. It depends on your volume, and even a slight difference might turn out to be huge. For example, 0.99 has a 10 times higher case ratio than 0.999 (1:100 vs. 1:1000).</p> 
   </div> 
   <div class="readable-text" id="p125"> 
    <p>But even in the notebook example, while precision is only 10 times worse, the FPR of model B is 46 times worse; it’s hard to call this a very good FPR.</p> 
   </div> 
   <div class="readable-text" id="p126"> 
    <p>As you can see from the previous calculations and the notebook, precision shows a very different number when there is a shift in class balance, even when the model’s performance stays the same. In contrast, both TPR and FPR remain unchanged.</p> 
   </div> 
   <div class="readable-text" id="p127"> 
    <p>How do we combine this information and apply it to pick proper metrics?</p> 
   </div> 
   <div class="readable-text" id="p128"> 
    <p>In one of the companies we worked for, we had a goal to reduce spam and fraudulent behavior with more than 100,000,000,000 events per day. We set specificity to be at least 0.999999 (Specificity = TNR = 1 – FPR [in other words, we were okay to have one false positive per 1 million events]) and maximized recall (TPR) at that specificity rate. This proved to be more beneficial than using a standard recall-precision pair, given the volatile nature of underlying data.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p129"> 
   <p>Some cases, however, force you to improvise in order to find the metric that will be able to obtain a required behavior pattern from your system.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p130"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Campfire story from Arseny</h5> 
   </div> 
   <div class="readable-text" id="p131"> 
    <p>I worked for a manufacturing optimization company and needed to improve a defect in its detection system, but in the midst of the process, another problem emerged: the metrics were not sensitive enough. The datasets required for running the planned scenario were too small—only 10 to 20 defective samples per customer product. And we couldn’t get any more data because there were simply no more existing defective units. The defect ratio was just too low, thanks to the high engineering quality.</p> 
   </div> 
   <div class="readable-text" id="p132"> 
    <p>Besides the dataset size, our customers weren’t interested in intermediate results (e.g., how calibrated the defect probability of our model was). Their judgment was very straightforward. For the sake of simplicity, let me frame it like this:</p> 
   </div> 
   <ul> 
    <li class="readable-text" id="p133"> There are 10 defective units and N regular units. </li> 
    <li class="readable-text" id="p134"> An ideal scenario is to have 0 errors. </li> 
    <li class="readable-text" id="p135"> 1 false positive or 1 false negative is good enough. </li> 
    <li class="readable-text" id="p136"> Otherwise, the system is unusable. </li> 
   </ul> 
   <div class="readable-text" id="p137"> 
    <p>Most of my attempts to improve the system as is were fruitless, until at some point I decided to design a custom continuous metric that utilized the internal metrics and had reasonable thresholds. The metric appeared very discrete: </p> 
   </div> 
   <ul> 
    <li class="readable-text" id="p138"> “0” would mean “perfect system.” </li> 
    <li class="readable-text" id="p139"> “1” would be “good enough.” </li> 
    <li class="readable-text" id="p140"> “2” would stand for “garbage.” </li> 
   </ul> 
   <div class="readable-text" id="p141"> 
    <p>With this metric in place, I was able to start improving the system gradually, step by step, while being confident that I was moving in the right direction. </p> 
   </div> 
   <div class="readable-text" id="p142"> 
    <p>After a series of minor improvements, the cumulative effect transformed the system from “garbage” to “good enough” and from “good enough” to “perfect” for multiple customers.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p143"> 
   <p>One important factor in the success of your ML system will always be its consistency. To achieve this, there is a separate category of metrics, which we cover in the following section.</p> 
  </div> 
  <div class="readable-text" id="p144"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_39"><span class="num-string">5.2.1</span> Consistency metrics</h3> 
  </div> 
  <div class="readable-text" id="p145"> 
   <p>In applied ML, a model that has a consistent output when presented with slightly perturbed inputs is often desired. This property, known in different subfields as consistency, robustness, stability, or smoothness, can be formally defined as the requirement that the model be invariant under certain transformations, such that the difference between the model’s output on the original input and the model’s output on the perturbed input tends toward zero. In other words, we can express this property mathematically as<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p146"> 
   <img alt="figure" src="../Images/babushkin-ch5-eqs-9x.png" width="228" height="23"/> 
  </div> 
  <div class="readable-text" id="p147"> 
   <p>where <em>f</em> represents the model, <em>x</em> represents the original input, and <em>eps</em> represents the perturbation applied to the input. Consistency metrics are not commonly discussed in academic ML but are an important consideration in practical applications where small changes to the input can have significant effects on the model’s output from the product perspective.</p> 
  </div> 
  <div class="readable-text intended-text" id="p148"> 
   <p>Perturbations can be different. For example, for a solid computer vision model, a minor change of lighting usually should not change model outputs, or a sentiment analysis model should not be sensitive to changing words with synonyms. We will talk about such perturbations and invariants in more detail later, when discussing ML system testing in chapter 10.</p> 
  </div> 
  <div class="readable-text intended-text" id="p149"> 
   <p>There’s another similar property: when the model is retrained (e.g., with the addition of new data or even with other seeds), we expect it to produce the same or close outputs, given that inputs remain unchanged. For an antifraud system, it is not acceptable if the same user is considered fraudulent today, legitimate tomorrow, and a fraudster again next week:<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p150"> 
   <img alt="figure" src="../Images/babushkin-ch5-eqs-10x.png" width="334" height="96"/> 
  </div> 
  <div class="readable-text" id="p151"> 
   <p>When the model outputs are different over time, the release of a new model (which should be a routine procedure for most ML systems) may affect the downstream system or end users of the system, disturbing their common usage scenarios. People rarely like unexpected changes in their tools and environment.</p> 
  </div> 
  <div class="readable-text intended-text" id="p152"> 
   <p>Such properties can be as important as default features we expect from a model (such as accurate predictions) because they shape expectations. As we discussed in earlier chapters, if a model can’t be trusted, its utility is reduced. Thus, we need specific metrics to measure this kind of behavior. </p> 
  </div> 
  <div class="readable-text intended-text" id="p153"> 
   <p>Luckily, we formulated these properties strictly enough, so the biggest open question left is to estimate a proper type of noise or perturbation for the preceding formulas: what are the invariants, and how are the conditions expected to change over time?</p> 
  </div> 
  <div class="readable-text intended-text" id="p154"> 
   <p>With these estimations in place, you can attach your regular metrics to estimate consistency. For example, for the search engine example (Photostock Inc.), we don’t want a document to change its rank for some query between releases of your system, and so the consistency metric could be a variance of ranks for the pair (query, document) over some time over corpora of documents and queries. Obviously, the less the variance is, the better it is for the system. Still, you can’t forget about ill-posed situations—say, a dummy constant model tends to provide the lowest variance, but that’s not the consistency ML engineers usually hunt for.</p> 
  </div> 
  <div class="readable-text intended-text" id="p155"> 
   <p>Consistency is often an important property of an ML system (see figure 5.5). If it’s the case for your system, consider adding a metric reflecting how your system responds to the changes to input data, training data, or training procedure tweaks.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p156">  
   <img alt="figure" src="../Images/CH05_F05_Babushkin.png" width="640" height="416"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.5</span> New model releases are fairly consistent when estimating the probability (P) of the user (U) being fraudulent (F).</h5>
  </div> 
  <div class="readable-text" id="p157"> 
   <p>Eventually, you will be able to form a single metrics system based on a clear hierarchy of offline and online metrics.<span class="aframe-location"/></p> 
  </div> 
  <div class="readable-text" id="p158"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_40"><span class="num-string">5.2.2</span> Offline and online metrics, proxy metrics, and hierarchy of metrics</h3> 
  </div> 
  <div class="readable-text" id="p159"> 
   <p>Setting and improving appropriate metrics is an important step in building an efficient ML system. But even that is not our end goal, as we have to go one level deeper into the rabbit hole. When we had a plan to reduce spam and fraudulent behavior, the goal was not to have the highest recall at a given specificity. It was to improve the user experience by lowering the number of spam messages and making it safer by reducing the risk of fraudulent behavior. </p> 
  </div> 
  <div class="readable-text intended-text" id="p160"> 
   <p>In the Supermegaretail case, the goal was to reduce losses due to out-of-stock and overstock situations, which can be expressed in cash equivalent, but not mean absolute error (MAE), mean square error (MSE), weighted mean absolute percentage error (wMAPE), weighted absolute percentage error (WAPE), or any other metric.</p> 
  </div> 
  <div class="readable-text intended-text" id="p161"> 
   <p>In other words, the metric we used to assess the model during the training/testing/validation stages and the final metrics are rarely the same (see table 5.1). </p> 
  </div> 
  <div class="readable-text intended-text" id="p162"> 
   <p>The previously discussed set is also called <em>offline metrics</em> because we can apply and calculate them without deploying the model into production. In contrast, some metrics, usually our goal metrics, can be calculated only after implementing the system and using its output in the business. And although sometimes offline and online metrics might coincide, we still have to assess them differently. The most common way to evaluate online metrics (change/improvement) is through A/B testing.</p> 
  </div> 
  <div class="readable-text intended-text" id="p163"> 
   <p>We use offline metrics for a simple reason: we can use them before deploying the system. This method is quick and reproducible, and it doesn’t require an expensive model deployment process. Offline metrics must have one quality: they must be a good predictor of online metrics. In other words, an increase or decrease in offline metrics has to be either strongly correlated or proportional to an increase/decrease in online metrics. Offline metrics play the role of proxy metrics for online metrics and can be used as efficient predictors of online metrics.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p164"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 5.1</span> Examples of offline and online metrics</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Offline metrics 
       </div></th> 
      <th> 
       <div>
         Online metrics 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Recall at given specificity for spam message classification <br/></td> 
      <td>  Number of user complaints about spam messages <br/></td> 
     </tr> 
     <tr> 
      <td>  Quantiles of 1.5, 25, 50, 75, 95, and 99 <br/></td> 
      <td>  Value of expired items, total sales <br/></td> 
     </tr> 
     <tr> 
      <td>  Mean reciprocal rank, normalized discounted cumulative gain <br/></td> 
      <td>  Click-through rate on search engine result page <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p165"> 
   <p>But if we can find offline metrics that are strongly correlated with our online metrics with the improvement being transitive, we can do the same for offline metrics. Let’s use an example to review this.</p> 
  </div> 
  <div class="readable-text intended-text" id="p166"> 
   <p>Imagine that we are building a recommender system for an eCommerce website. Our final goal is to increase gross merchandise value (GMV; this is a metric that measures the total value of sales over a given period). Unfortunately, as mentioned already, this is not something we can measure until we deploy our system into production and run A/B tests. We believe that increasing the number of items purchased will increase GMV. To achieve that, we want to increase the conversion rate by providing users with an offer that has a higher chance of being purchased (assuming this will increase the overall number of purchased items). </p> 
  </div> 
  <div class="readable-text intended-text" id="p167"> 
   <p>On average, 3% of offers end up being clicked, and 3% of those lead to a purchase: 3% times 3% means that if we show 10,000 offers, only 9 will lead to a purchase. This has two adverse, interconnected consequences:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p168"> Low amount of class 1 data (purchase), huge class imbalance </li> 
   <li class="readable-text" id="p169"> Increased A/B test duration </li> 
  </ul> 
  <div class="readable-text" id="p170"> 
   <p>For example, for A/B tests with a 9/10,000 ratio of success to attempts, we would need 100 times more data than for the 90/10,000 ratio (quadratic dependency between a minimum detectable effect and a number of samples; please see the following for an example).</p> 
  </div> 
  <div class="readable-text intended-text" id="p171"> 
   <p>To mitigate that, we can use a proxy metric, click-through rate (CTR), with the following context in mind:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p172"> No purchase can be made without a click. We can expect a positive correlation between the CTRs and conversion rates (CRs) and even calculate it. </li> 
   <li class="readable-text" id="p173"> There are 33.3 times more clicks than purchases, meaning that we will have 33.3 times more training data for class 1 of the system, and A/B tests will become 1,111 (33.(3)^2) times faster. (To be precise, we can expect that variance will change as well, as <span><img alt="equation image" src="../Images/eq-chapter-5-173-1.png" width="119" height="30"/></span>, so with <span><img alt="equation image" src="../Images/eq-chapter-5-173-2.png" width="65" height="30"/></span>, var = 0.000899 and with <span><img alt="equation image" src="../Images/eq-chapter-5-173-3.png" width="53" height="30"/></span>, var = 0.0291, meaning that overall we will increase the speed of convergence by <span><img alt="equation image" src="../Images/eq-chapter-5-173-4.png" width="108" height="30"/></span> times.) </li> 
  </ul> 
  <div class="readable-text" id="p174"> 
   <p>Using CTR instead of CR helps us iterate faster and with higher sensitivity, both offline (estimating metrics and loss is easier with more data for the class of interest) and online (at least partly through A/B testing).</p> 
  </div> 
  <div class="readable-text intended-text" id="p175"> 
   <p>We can represent this in the following relation:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p176"> CTR <span class="regular-symbol">→</span> CR <span class="regular-symbol">→</span> (overall number of purchased items) <span class="regular-symbol">→</span> GMV </li> 
  </ul> 
  <div class="readable-text" id="p177"> 
   <p>We can further generalize this by building a hierarchy of metrics:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p178"> The global, company-wide metric is revenue. </li> 
   <li class="readable-text" id="p179"> Global revenue (GMV) is composed of the revenue from different products, including the product we are responsible for. </li> 
   <li class="readable-text buletless-item" id="p180"> Our product revenue is affected by 
    <ul> 
     <li> Average purchase price </li> 
     <li> Purchase frequency </li> 
     <li> Number of users (they are interconnected and have mutual influence, thus dotted lines) </li> 
    </ul></li> 
   <li class="readable-text" id="p181"> Purchase frequency is affected by CR. </li> 
   <li class="readable-text" id="p182"> The conversion rate is affected by CTR. </li> 
  </ol> 
  <div class="readable-text" id="p183"> 
   <p>A hierarchy of metrics (see figure 5.6) facilitates finding proper proxy metrics. Even though creating it lies outside the scope of designing an ML system, it will be handy to have one in place and refer to it during the design process. Using a common ground helps prove the choice and reduces the risk of failure.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p184">  
   <img alt="figure" src="../Images/CH05_F06_Babushkin.png" width="1015" height="549"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 5.6</span> Hierarchy of metrics</h5>
  </div> 
  <div class="readable-text" id="p185"> 
   <p>A hierarchy of metrics is especially important when the system gets mature enough so that some metrics can be contradictory. A friend of ours once told us a short anecdote about building a recommendation system: a variant that demonstrated higher engagement by internal users (they preferred new recommendations over previous versions) appeared to be way less profitable on a wider audience.</p> 
  </div> 
  <div class="readable-text intended-text" id="p186"> 
   <p>A hierarchy of metrics and proxy metrics concepts are connected to the multicomponent losses we discussed earlier. For example, when building this recommender engine for Supermegaretail, we can tailor a specific loss function that will consider multiple levels of user activity (clicks, purchases, total amount of purchased items) and balance our interest between metrics. </p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p187"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Campfire story from Arseny</h5> 
   </div> 
   <div class="readable-text" id="p188"> 
    <p>Once I worked on a brand-new product feature based on computer vision. The proposed solution was broken down into components, with each component and subcomponent carefully annotated with metrics. Due to the innovative nature of the feature, the metrics were custom—mostly ratios between various possible outcomes. We designed the metrics hierarchy in collaboration with a product executive. After several experiments aimed at moving the needle for one of the metrics, I developed a gut feeling that it was imbalanced. To test this, I ran an adversarial experiment by replacing the model predictions with random noise generated with specific parameters. Surprisingly, the random model scored perfectly! The metric was originally designed to favor recall over precision, but such an extreme imbalance was clearly not desirable, so we had to redesign it as soon as possible.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p189"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_41"><span class="num-string">5.3</span> Design document: Adding losses and metrics</h2> 
  </div> 
  <div class="readable-text" id="p190"> 
   <p>Starting in chapter 4, we began to introduce design documents for two fictional cases: Supermegaretail and PhotoStock Inc. Here we continue to elaborate on the development of ML solutions for each case and cover the selection of loss functions and losses. We start with Supermegaretail followed by PhotoStock Inc.</p> 
  </div> 
  <div class="readable-text" id="p191"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_42"><span class="num-string">5.3.1</span> Metrics and loss functions for Supermegaretail</h3> 
  </div> 
  <div class="readable-text" id="p192"> 
   <p>Let’s refresh our memory on the Supermegaretail case. There, we were to reduce the gap between delivered and sold items, making it as narrow as possible while avoiding an out-of-stock situation with a specific service-level agreement (SLA) to be specified further. </p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p193"> 
    <h4 class="readable-text-h4 sigil_not_in_toc">Design document: Supermegaretail</h4> 
   </div> 
   <div class="readable-text" id="p194"> 
    <h4 class="readable-text-h4 sigil_not_in_toc">II. Metrics and losses</h4> 
   </div> 
   <div class="readable-text" id="p195"> 
    <h4 class="readable-text-h4 sigil_not_in_toc">i. Metrics</h4> 
   </div> 
   <div class="readable-text" id="p196"> 
    <p>Before picking up a metric on our own, it makes sense to do some preliminary research. Fortunately, there are many papers related to this problem, but the one that stands out is “Evaluating Predictive Count Data Distributions in Retail Sales Forecasting” by Stephen Kolassa (<a href="https://mng.bz/eVl9">https://mng.bz/eVl9</a>). </p> 
   </div> 
   <div class="readable-text intended-text" id="p197"> 
    <p>Let’s recall the project goal, which is to reduce the gap between delivered and sold items, making it as narrow as possible while avoiding an out-of-stock situation with a specific SLA to be specified further. To do that, we plan to forecast the demand for a specific item in a specific store during a particular period using an ML system.</p> 
   </div> 
   <div class="readable-text intended-text" id="p198"> 
    <p>In this case, this paper’s abstract looks like an almost perfect fit:</p> 
   </div> 
   <div class="readable-text" id="p199"> 
    <blockquote>
     <div>
      Massive increases in computing power and new database architectures allow data to be stored and processed at increasingly finer granularities, yielding count data time series with lower and lower counts. These series can no longer be dealt with using approximative methods appropriate for continuous probability distributions. In addition, it is not sufficient to calculate point forecasts alone: we need to forecast the entire (discrete) predictive distributions, particularly for supply chain forecasting and inventory control, but also for other planning processes.
     </div>
    </blockquote> 
   </div> 
   <div class="readable-text" id="p200"> 
    <p>(Count data is an integer-valued time series. It is essential for the supply chain forecasting we are facing, where most products are sold in units.) With that in mind, we can briefly review this paper (within the following lettered list) and pick the metrics that are most appropriate for our end goal.</p> 
   </div> 
   <div class="readable-text" id="p201"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">A. Measures based on absolute errors </h5> 
   </div> 
   <div class="readable-text" id="p202"> 
    <p>MAE optimizes the median; the weighted mean absolute percentage error (wMAPE) is MAE divided by the mean of the out-of-sample realizations, and the mean absolute scaled error is obtained by dividing the MAE by the in-sample MAE of the random walk forecast. </p> 
   </div> 
   <div class="readable-text intended-text" id="p203"> 
    <p>Optimizing for the median does not differ much from optimizing for the mean in a symmetric predictive distribution. However, the predictive distributions appropriate for low-volume count data are usually far from symmetric, and this distinction makes a difference in such cases and yields biased forecasts. </p> 
   </div> 
   <div class="readable-text" id="p204"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">B. Percentage errors </h5> 
   </div> 
   <div class="readable-text" id="p205"> 
    <p>The mean absolute percentage error (MAPE) is undefined if any future realization is zero, so it is singularly unsuitable for count data. </p> 
   </div> 
   <div class="readable-text intended-text" id="p206"> 
    <p>The symmetric MAPE is a “symmetrized” version of the MAPE, which is defined if the point forecasts and actuals are not both zero at all future time points. However, in any period with a zero actual, its contribution is 2, regardless of the point forecast, making it unsuitable for count data. </p> 
   </div> 
   <div class="readable-text" id="p207"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">C. Measures based on squared errors </h5> 
   </div> 
   <div class="readable-text" id="p208"> 
    <p>Minimizing the squared error leads naturally to an unbiased point forecast. However, the MSE is unsuitable for intermittent-demand items because it is sensitive to very high forecast errors. The same argument stands for nonintermittent count data. </p> 
   </div> 
   <div class="readable-text" id="p209"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">D. Relative errors </h5> 
   </div> 
   <div class="readable-text" id="p210"> 
    <p>Prominent variations are the median relative absolute error and the geometric mean relative absolute error. </p> 
   </div> 
   <div class="readable-text intended-text" id="p211"> 
    <p>In the specific context of forecasting count data, these suffer from two main weaknesses: </p> 
   </div> 
   <ul> 
    <li class="readable-text" id="p212"> Relative errors commonly compare absolute errors. As such, they are subject to the same criticism as MAE-based errors, as detailed earlier. </li> 
    <li class="readable-text" id="p213"> On a period-by-period basis, simple benchmarks such as the naive random walk may forecast without errors, and thus, this period’s relative error would be undefined because of a division by zero. </li> 
   </ul> 
   <div class="readable-text" id="p214"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">E. Rate-based errors </h5> 
   </div> 
   <div class="readable-text" id="p215"> 
    <p>Kourentzes (2014) recently suggested two new error measures for the intermittent demand: MSR and MAR, which aim to assess whether an intermittent demand point forecast captures the average demand correctly over an increasing period of time. This is an interesting suggestion, but one property of these measures is that they implicitly weigh the short-term future more heavily than the mid- to long-term future. One could argue that this is exactly what we want to do while forecasting, but even then, a case could be made that such weighting should be explicit—by using an appropriate weighting scheme when averaging over future time periods. </p> 
   </div> 
   <div class="readable-text" id="p216"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">F. Scaled errors </h5> 
   </div> 
   <div class="readable-text" id="p217"> 
    <p>Petropoulos and Kourentzes (2015) suggest a scaled version of the MSE, the sMSE, which is the mean over squared errors that have been scaled by the squared average actuals over the forecast horizon. The sMSE is well-defined unless all actuals are zero, is minimized by the expectation of <em>f,</em> and, due to the scaling, can be compared between different time series. In addition (again because of the scaling), it is not quite as sensitive to high-forecast errors as the MSE. Specifically, it is more robust to dramatic underforecasts, although it is still sensitive to large overforecasts. </p> 
   </div> 
   <div class="readable-text" id="p218"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">G. Functionals and loss functions </h5> 
   </div> 
   <div class="readable-text" id="p219"> 
    <p>An alternative way of looking at forecasts concentrates on point forecasts that are functionals of the predictive distribution. One could argue that a retailer aims at a certain level of service (say 95%) and that therefore they are only interested in the corresponding quantile of the predictive distribution. This would then be elicited with appropriate loss functions or scoring rules. This approach is closely related to the idea of considering forecasts as part of a stock control system. From this perspective, quantile forecasts are used as inputs to standard stock control strategies, and the quality of the forecasts is assessed by valuing the total stock position over time and weighing it against out-of-stocks. </p> 
   </div> 
   <div class="readable-text intended-text" id="p220"> 
    <p>Though the authors did not see this as the best solution and proposed an alternative, the last paragraph of the paper is quite promising. Not only does it make sense from a business perspective to predict different quantiles to uphold SLA, but it is desirable from the point of view of having the loss function equal to the metric. Thus, quantile metrics for quantiles of 1.5, 25, 50, 75, 95, and 99 look like a proper choice. Moreover, suppose we need to pay more attention to a specific SKU, item group, or cluster. In that case, quantile metrics support the calculation of object/group weights (for example, item price).</p> 
   </div> 
   <div class="readable-text" id="p221"> 
    <h4 class="readable-text-h4 sigil_not_in_toc">i.ii. Metrics to pick</h4> 
   </div> 
   <div class="readable-text" id="p222"> 
    <p>Quantile metrics for quantiles of 1.5, 25, 50, 75, 95, and 99 both as is and with weights equal to SKU price and an additional penalty for underforecasting or overforecasting if deemed necessary are calculated as point estimates with 95% confidence intervals (using bootstrap or cross-validation). In addition, we can further transform this metric, representing it not as an absolute value but as an absolute percentage error at a given quantile. All consideration from the Petropoulos and Kourentzes article regarding percentage errors have to be taken into account. Ultimately, a set of experiments will help to decide a final form. We will probably have both, as it makes sense to check both absolute values in money/pcs and percentage error. </p> 
   </div> 
   <div class="readable-text intended-text" id="p223"> 
    <p>Online metrics of interest during A/B test are</p> 
   </div> 
   <ul> 
    <li class="readable-text" id="p224"> Revenue—expected to increase </li> 
    <li class="readable-text" id="p225"> Level of stock—expected to decrease or maintain the same </li> 
    <li class="readable-text" id="p226"> Margin—expected to increase<span class="aframe-location"/> </li> 
   </ul> 
   <div class="browsable-container figure-container" id="p227"> 
    <img alt="figure" src="../Images/babushkin-ch5-eqs-15x.png" width="312" height="71"/> 
   </div> 
   <ul> 
    <li class="readable-text" id="p228"> Alpha—coefficient used in quantile-based losses </li> 
    <li class="readable-text" id="p229"> W—Weights </li> 
    <li class="readable-text" id="p230"> I—Indicator function </li> 
    <li class="readable-text" id="p231"> A—Model output </li> 
    <li class="readable-text" id="p232"> T—Label </li> 
   </ul> 
   <div class="readable-text" id="p233"> 
    <h4 class="readable-text-h4 sigil_not_in_toc">ii. Loss functions</h4> 
   </div> 
   <div class="readable-text" id="p234"> 
    <p>With metrics equal to our loss functions, it is straightforward to pick the latter. We will train six models using a quantile loss of 1.5, 25, 50, 75, 95, and 99, resulting in six different models, providing us with various guarantees for the corresponding quantile of the predictive distribution. </p> 
   </div> 
   <div class="readable-text intended-text" id="p235"> 
    <p>As a second line of experimentation, we will additionally review the Tweedie loss function. Tweedie distributions are a family of probability distributions, including the purely continuous normal, gamma, and inverse Gaussian distributions; the purely discrete scaled Poisson distribution; and the class of compound Poisson–gamma distributions that have positive mass at zero but are otherwise continuous. These qualities make it an attractive candidate for our Count data.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p236"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_43"><span class="num-string">5.3.2</span> Metrics and loss functions for PhotoStock Inc. </h3> 
  </div> 
  <div class="readable-text" id="p237"> 
   <p>Next up is the PhotoStock Inc. design document, where a whole different set of losses and metrics should be applied based on the nature of the business case and the problem to be solved. In the case of PhotoStock Inc., we were hired to build a modern search tool that can find the most relevant shots based on customers’ text queries while providing excellent performance and displaying the most relevant images in stock.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p238"> 
    <h4 class="readable-text-h4 sigil_not_in_toc">Design document: PhotoStock Inc.</h4> 
   </div> 
   <div class="readable-text" id="p239"> 
    <h4 class="readable-text-h4 sigil_not_in_toc">II. Metrics and loss functions</h4> 
   </div> 
   <div class="readable-text" id="p240"> 
    <h4 class="readable-text-h4 sigil_not_in_toc">i. Metrics</h4> 
   </div> 
   <div class="readable-text" id="p241"> 
    <p>When choosing metrics for a new PhotoStock search engine, we should keep in mind the expected behavior of the system, which includes the following:</p> 
   </div> 
   <ul> 
    <li class="readable-text" id="p242"> Users click on links in search results, with higher results getting more clicks. This behavior can be reflected in the CTR metric, which evaluates how many users click on search results. </li> 
    <li class="readable-text" id="p243"> Users purchase images they find via search. This behavior can be reflected in the CR metric, which evaluates how many clicks lead to purchase. </li> 
    <li class="readable-text" id="p244"> Users see diverse suggestions on the search engine result page (SERP). There are no ready-to-go solutions here because we don’t have a solid definition of diversity. Let’s discuss it later with the UX team. As a baseline, we can use the number of different categories of images represented on SERP as a measure of diversity. In the future, we should research other companies’ experiences—Airbnb’s paper “Learning to Rank Diversely at Airbnb” (<a href="https://arxiv.org/abs/2210.07774">https://arxiv.org/abs/2210.07774</a>).  </li> 
    <li class="readable-text" id="p245"> Search results look reasonable from the human perspective. This behavior can be reflected in the metric of human evaluation, which displays how many users think that search results are reasonable. </li> 
   </ul> 
   <div class="readable-text" id="p246"> 
    <p>CTR and CR are online metrics, which means that they can only be measured when the system is live. Diversity is an unsupervised offline metric, which means that it doesn’t require any additional data and can be measured on a regular basis at no cost. Human evaluation, on the other hand, is a supervised offline metric, which means that it requires additional data (human evaluation) and thus takes time and effort to collect.</p> 
   </div> 
   <div class="readable-text intended-text" id="p247"> 
    <p>To introduce offline proxy metrics for CTR and CR, we can use classic metrics for ranking problems, such as mean reciprocal rank (MRR) and normalized discounted cumulative gain (NDCG). MRR is a metric that calculates the average of the reciprocal ranks for a given set of results, which is a measure of the mean of the inverse of the rank for the first relevant result. NDCG is a metric that calculates the average of discounted cumulative gains (DCGs) for a given set of results, which is a measure of the sum of relevance scores taken from the first N results divided by the ideal DCG. In its turn, DCG is the sum of relevance scores among the first N results in the order of decreasing relevance.</p> 
   </div> 
   <div class="readable-text intended-text" id="p248"> 
    <p>Both MRR and NDCG require a list of relevant results for each query to calculate the metrics. We can use the same list of relevant results for both MRR and NDCG, but we need to create this list using crowdsourcing to ensure that it is representative of the results that users are likely to see. While MRR may be appropriate as an offline metric for CTR, it may not be a good proxy for CR because a crowdsourced list of relevant results is not representative of the real purchase data. Therefore, to accurately measure CR, we should consider using real purchase data. However, for the first version of the system, we may only be able to monitor CR online using A/B tests and a gradual rollout.</p> 
   </div> 
   <div class="readable-text intended-text" id="p249"> 
    <p>To summarize, here’s how we can divide metrics:</p> 
   </div> 
   <ul> 
    <li class="readable-text" id="p250"> Fast offline metrics: MRR, NDCG, diversity </li> 
    <li class="readable-text" id="p251"> Slow offline metrics: human evaluation </li> 
    <li class="readable-text" id="p252"> Online metrics: CTR, CR </li> 
   </ul> 
   <div class="readable-text" id="p253"> 
    <h4 class="readable-text-h4 sigil_not_in_toc">ii. Losses</h4> 
   </div> 
   <div class="readable-text" id="p254"> 
    <p>To use loss functions for training a search engine, it is important to consider available data and desired outcomes. In this case, the three main aspects we would like to optimize for are clicks, purchases, and diversity.</p> 
   </div> 
   <div class="readable-text intended-text" id="p255"> 
    <p>For the clicks and purchases aspect, we can use binary cross-entropy loss as a measure of success. However, it’s important to note that the data for clicks and purchases may be imbalanced, meaning that there may be more examples of one class than the other. In such cases, it may be beneficial to use a loss function that is more robust to class imbalance, such as focal loss or other loss functions designed for this purpose.</p> 
   </div> 
   <div class="readable-text intended-text" id="p256"> 
    <p>Focal loss is a loss function that was introduced in the paper “Focal Loss for Dense Object Detection” (<a href="https://arxiv.org/abs/1708.02002v2">https://arxiv.org/abs/1708.02002v2</a>). It is a generalization of a binary cross-entropy loss commonly used in classification tasks. The key difference between a focal loss and a binary cross-entropy loss is that focal loss down-weights easy examples, which are those examples that are classified correctly with high confidence. This is useful in cases where the data is imbalanced, as it helps the model to focus on the hard examples, which are typically more important for improving the overall performance of the model, so it seems relevant for the PhotoStock search engine.</p> 
   </div> 
   <div class="readable-text intended-text" id="p257"> 
    <p>As for the diversity aspect, we can add a term to the loss function that penalizes the similarity in results. One potential way to do this is to use the entropy of the category distribution of the results as a measure of diversity. However, this approach may not always be feasible, so the diversity loss should be considered optional.</p> 
   </div> 
   <div class="readable-text intended-text" id="p258"> 
    <p>Overall, the final loss function can be written as<span class="aframe-location"/></p> 
   </div> 
   <div class="browsable-container figure-container" id="p259"> 
    <img alt="figure" src="../Images/babushkin-ch5-eqs-16x.png" width="455" height="26"/> 
   </div> 
   <div class="readable-text" id="p260"> 
    <p>Here, alpha, beta, and gamma are represented as hyperparameters that control the relative importance of the three components. These hyperparameters can be tuned to find the optimal balance between the three aspects.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p261"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_44"><span class="num-string">5.3.3</span> Wrap up </h3> 
  </div> 
  <div class="readable-text" id="p262"> 
   <p>The examples from these two design documents show how important it is to choose the right metrics and loss functions. Just like any other key element in building an ML system, metrics and loss functions should coincide with the goals of your project. And if you feel there’s more time needed to define the appropriate parameters, please find a few days in your schedule to do it so you don’t have to roll back a few miles in a month or more.</p> 
  </div> 
  <div class="readable-text intended-text" id="p263"> 
   <p>The next chapter covers data gathering, datasets, the difference between data and metadata, and how to achieve a healthy data pipeline. </p> 
  </div> 
  <div class="readable-text" id="p264"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_45">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p265"> Don’t fall into the temptation of using time-tested loss functions just because they worked on your previous project(s). </li> 
   <li class="readable-text" id="p266"> A loss function must be globally continuous and differentiable. </li> 
   <li class="readable-text" id="p267"> Loss selection is an important step, but it is even more crucial with deep learning-based systems. </li> 
   <li class="readable-text" id="p268"> Consider applying consistency metrics when small changes to the inputs can have significant effects on the output of your model from the product perspective. </li> 
   <li class="readable-text" id="p269"> Offline metrics can be applied before putting your project into production and play the role of proxy metrics for online metrics. </li> 
   <li class="readable-text" id="p270"> Make sure to have the hierarchy of metrics at hand, as it will be useful while working on the design of your system.  </li> 
  </ul>
 </div></div></body></html>