<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 19. Clustering" data-type="chapter" epub:type="chapter"><div class="chapter" id="clustering">
<h1><span class="label">Chapter 19. </span>Clustering</h1>
<section data-pdf-bookmark="19.0 Introduction" data-type="sect1"><div class="sect1" id="id378">
<h1>19.0 Introduction</h1>
<p>In much of this <a data-primary="clustering" data-type="indexterm" id="ix_cluster_ch19"/>book we have looked at supervised machine learning—​where we have access to both the features and the target. This is, unfortunately, not always the case. Frequently, we run into situations where we only know the features. For example, imagine we have records of sales from a grocery store and we want to break up sales by whether the shopper is a member of a discount club. This would be impossible using supervised learning because we don’t have a target to train and evaluate our models. However, there is another option: unsupervised <a data-primary="unsupervised learning models" data-seealso="clustering" data-type="indexterm" id="id1882"/>learning. If the behavior of discount club members and nonmembers in the grocery store is actually disparate, then the average difference in behavior between two members will be smaller than the average difference in behavior between a member and nonmember shopper. Put another way, there will be two clusters of observations.</p>
<p>The goal of clustering algorithms is to identify those latent groupings
of observations, which, if done well, allows us to predict the class of
observations even without a target vector. There are many clustering
algorithms, and they have a wide variety of approaches to identifying the
clusters in data. In this chapter, we will cover a selection of
clustering algorithms using scikit-learn and how to use them in practice.</p>
</div></section>
<section data-pdf-bookmark="19.1 Clustering Using K-Means" data-type="sect1"><div class="sect1" id="clustering-using-k-means">
<h1>19.1 Clustering Using K-Means</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id379">
<h2>Problem</h2>
<p>You want to <a data-primary="k-means clustering" data-type="indexterm" id="ix_kmeans_cluster3"/><a data-primary="clustering" data-secondary="k-means" data-type="indexterm" id="ix_cluster_kmeans2"/>group observations into <em>k</em> groups.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id1883">
<h2>Solution</h2>
<p>Use <em>k-means clustering</em>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>
<code class="kn">from</code> <code class="nn">sklearn.cluster</code> <code class="kn">import</code> <code class="n">KMeans</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>

<code class="c1"># Standardize features</code>
<code class="n">scaler</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>
<code class="n">features_std</code> <code class="o">=</code> <code class="n">scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Create k-means object</code>
<code class="n">cluster</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">n_init</code><code class="o">=</code><code class="s2">"auto"</code><code class="p">)</code>

<code class="c1"># Train model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">cluster</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_std</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id380">
<h2>Discussion</h2>
<p>K-means clustering is one of the most common clustering techniques. In
k-means clustering, the algorithm attempts to group observations into
<em>k</em> groups, with each group having roughly equal variance. The number of groups, <em>k</em>, is specified by the user as a hyperparameter. Specifically, in k-means:</p>
<ol>
<li>
<p><em>k</em> cluster “center” points are created at random locations.</p>
</li>
<li>
<p>For each observation:</p>
<ol>
<li>
<p>The distance between each observation and the <em>k</em> center
points is calculated.</p>
</li>
<li>
<p>The observation is assigned to the cluster of the nearest center
point.</p>
</li>
</ol>
</li>
<li>
<p>The center points are moved to the means (i.e., centers) of their
respective <span class="keep-together">clusters.</span></p>
</li>
<li>
<p>Steps 2 and 3 are repeated until no observation changes in cluster
membership.</p>
</li>
</ol>
<p>At this point the algorithm is considered converged and stops.</p>
<p>It is important to note three things about k-means. First, k-means
clustering assumes the clusters are convex shaped (e.g., a circle, a
sphere). Second, all features are equally scaled. In our solution,
we standardized the features to meet this assumption. Third, the groups
are balanced (i.e., have roughly the same number of observations). If we
suspect that we cannot meet these assumptions, we might try other
clustering approaches.</p>
<p>In scikit-learn, <a data-primary="KMeans" data-type="indexterm" id="id1884"/>k-means clustering is implemented in the <code>KMeans</code>
class. The most important parameter is <code>n_clusters</code>, which sets the
number of clusters <em>k</em>. In some situations, the nature of
the data will determine the value for <em>k</em> (e.g., data on a
school’s students will have one cluster per grade), but often we don’t
know the number of clusters. In these cases, we will want to select
<em>k</em> based on using some criteria. For example, silhouette
coefficients (see <a data-type="xref" href="ch11.xhtml#evaluating-clustering-models">Recipe 11.9</a>) measure the similarity within clusters
compared with the similarity between clusters. Furthermore, because
k-means clustering is computationally expensive, we might want to take
advantage of all the cores on our computer. We can do this by setting
<code>n_jobs=-1</code>.</p>
<p>In our solution, we cheated a little and used the iris flower data,
which we know contains three classes. Therefore, we set
<em>k = 3</em>. We <a data-primary="labels_ method" data-type="indexterm" id="id1885"/>can use <code>labels_</code> to see the predicted classes of each observation:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View predicted class</code>
<code class="n">model</code><code class="o">.</code><code class="n">labels_</code></pre>
<pre data-type="programlisting">array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2,
       1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2,
       2, 1, 2, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2,
       2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1], dtype=int32)</pre>
<p>If we compare this to the observation’s true class, we can see that,
despite the difference in class labels (i.e., <code>0</code>, <code>1</code>, and <code>2</code>), k-means
did reasonably well:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View true class</code>
<code class="n">iris</code><code class="o">.</code><code class="n">target</code></pre>
<pre data-type="programlisting">array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])</pre>
<p>However, as you might imagine, the performance of k-means drops
considerably, even critically, if we select the wrong number of clusters.</p>
<p>Finally, as with other scikit-learn models, we can use the trained cluster to
predict the value of new observations:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create new observation</code>
<code class="n">new_observation</code> <code class="o">=</code> <code class="p">[[</code><code class="mf">0.8</code><code class="p">,</code> <code class="mf">0.8</code><code class="p">,</code> <code class="mf">0.8</code><code class="p">,</code> <code class="mf">0.8</code><code class="p">]]</code>

<code class="c1"># Predict observation's cluster</code>
<code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">new_observation</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([2], dtype=int32)</pre>
<p>The observation is predicted to belong to the cluster whose center point
is closest. We <a data-primary="cluster_centers_ method" data-type="indexterm" id="id1886"/>can even use <code>cluster_centers_</code> to see those center
points:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View cluster centers</code>
<code class="n">model</code><code class="o">.</code><code class="n">cluster_centers_</code></pre>
<pre data-type="programlisting">array([[-1.01457897,  0.85326268, -1.30498732, -1.25489349],
       [-0.01139555, -0.87600831,  0.37707573,  0.31115341],
       [ 1.16743407,  0.14530299,  1.00302557,  1.0300019 ]])</pre>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1887">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/HDfUz">Introduction to K-means Clustering</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="19.2 Speeding Up K-Means Clustering" data-type="sect1"><div class="sect1" id="speeding-up-k-means-clustering">
<h1>19.2 Speeding Up K-Means Clustering</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id548">
<h2>Problem</h2>
<p>You want to <a data-primary="performance" data-secondary="k-means clustering" data-type="indexterm" id="id1888"/>group observations into <em>k</em> groups, but k-means
takes too long.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id381">
<h2>Solution</h2>
<p>Use <a data-primary="mini-batch k-means" data-type="indexterm" id="id1889"/>mini-batch k-means:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>
<code class="kn">from</code> <code class="nn">sklearn.cluster</code> <code class="kn">import</code> <code class="n">MiniBatchKMeans</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>

<code class="c1"># Standardize features</code>
<code class="n">scaler</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>
<code class="n">features_std</code> <code class="o">=</code> <code class="n">scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Create k-mean object</code>
<code class="n">cluster</code> <code class="o">=</code> <code class="n">MiniBatchKMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code>
       <code class="n">n_init</code><code class="o">=</code><code class="s2">"auto"</code><code class="p">)</code>

<code class="c1"># Train model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">cluster</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_std</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id382">
<h2>Discussion</h2>
<p><em>Mini-batch k-means</em> works similarly to the k-means algorithm discussed in
<a data-type="xref" href="#clustering-using-k-means">Recipe 19.1</a>. Without going into too much detail, the difference is that in mini-batch <span class="keep-together">k-means</span> the most computationally costly step is conducted on only a random sample of observations as opposed to all observations. This approach can significantly reduce the time required for the algorithm to find convergence (i.e., fit the data) with only a small cost in quality.</p>
<p><code>MiniBatchKMeans</code> works <a data-primary="MiniBatchKMeans" data-type="indexterm" id="id1890"/><a data-primary="KMeans" data-type="indexterm" id="id1891"/>similarly to <code>KMeans</code>, with one significant
difference: the <code>batch_size</code> parameter. <code>batch_size</code> controls the number of randomly selected observations in each batch. The larger the size of the batch, the more computationally costly the training process.<a data-primary="" data-startref="ix_cluster_kmeans2" data-type="indexterm" id="id1892"/><a data-primary="" data-startref="ix_kmeans_cluster3" data-type="indexterm" id="id1893"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="19.3 Clustering Using Mean Shift" data-type="sect1"><div class="sect1" id="clustering-using-meanshift">
<h1>19.3 Clustering Using Mean Shift</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id549">
<h2>Problem</h2>
<p>You want to <a data-primary="clustering" data-secondary="mean shift" data-type="indexterm" id="ix_cluster_mean"/>group observations without assuming the number of clusters
or their shape.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id383">
<h2>Solution</h2>
<p>Use mean <a data-primary="mean shift clustering" data-type="indexterm" id="ix_mean_shift_cluster"/>shift clustering:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>
<code class="kn">from</code> <code class="nn">sklearn.cluster</code> <code class="kn">import</code> <code class="n">MeanShift</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>

<code class="c1"># Standardize features</code>
<code class="n">scaler</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>
<code class="n">features_std</code> <code class="o">=</code> <code class="n">scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Create mean shift object</code>
<code class="n">cluster</code> <code class="o">=</code> <code class="n">MeanShift</code><code class="p">(</code><code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Train model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">cluster</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_std</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id384">
<h2>Discussion</h2>
<p>One of the disadvantages of k-means clustering we discussed previously is that we needed to set the number of clusters, <em>k</em>, prior to training, and the method made assumptions about the shape of the
clusters. One clustering algorithm without these limitations is
mean shift.</p>
<p class="less_space pagebreak-before"><em>Mean shift</em> is a simple concept, but it’s somewhat difficult to explain.
Therefore, an analogy might be the best approach. Imagine a very foggy
football field (i.e., a two-dimensional feature space) with 100 people
standing on it (i.e., our observations). Because it is foggy, a person
can see only a short distance. Every minute each person looks around and
takes a step in the direction of the most people they can see. As time
goes on, people start to group together as they repeatedly take steps toward
larger and larger crowds. The end result is clusters of people around
the field. People are assigned to the clusters in which they end up.</p>
<p>scikit-learn’s actual implementation of mean shift, <code>MeanShift</code>, is more
complex but follows the same basic logic. <code>MeanShift</code> has two important
parameters we should be aware of. First, <code>bandwidth</code> sets the  radius of the area (i.e., kernel) an observation uses to determine the direction to shift. In our analogy, bandwidth is how far a person can see through the fog. We can set this parameter manually, but by default a reasonable bandwidth is estimated automatically (with a significant increase in computational cost). Second, sometimes in mean shift there are no other observations within an observation’s kernel. That is, a person on our football field cannot see a single other person. By default, <code>MeanShift</code> assigns all these “orphan” observations to the kernel of the nearest observation. However, if we want to leave out these orphans, we can set <code>cluster_all=False</code>, wherein orphan observations are given the label of <code>-1</code>.<a data-primary="" data-startref="ix_cluster_mean" data-type="indexterm" id="id1894"/><a data-primary="" data-startref="ix_mean_shift_cluster" data-type="indexterm" id="id1895"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1896">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/Gb3VG">The mean shift clustering algorithm,
EFAVDB</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="19.4 Clustering Using DBSCAN" data-type="sect1"><div class="sect1" id="clustering-using-dbscan">
<h1>19.4 Clustering Using DBSCAN</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id1897">
<h2>Problem</h2>
<p>You want to group observations into clusters of high density.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id385">
<h2>Solution</h2>
<p>Use <a data-primary="DBSCAN clustering" data-type="indexterm" id="ix_dbscan_cluster"/><a data-primary="clustering" data-secondary="DBSCAN" data-type="indexterm" id="ix_cluster_dbscan"/>DBSCAN clustering:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>
<code class="kn">from</code> <code class="nn">sklearn.cluster</code> <code class="kn">import</code> <code class="n">DBSCAN</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>

<code class="c1"># Standardize features</code>
<code class="n">scaler</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>
<code class="n">features_std</code> <code class="o">=</code> <code class="n">scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Create DBSCAN object</code>
<code class="n">cluster</code> <code class="o">=</code> <code class="n">DBSCAN</code><code class="p">(</code><code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Train model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">cluster</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_std</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id386">
<h2>Discussion</h2>
<p><em>DBSCAN</em> is motivated by the idea that clusters will be areas where many
observations are densely packed together and makes no assumptions of
cluster shape. Specifically, in DBSCAN:</p>
<ol>
<li>
<p>A random observation, <em>x<sub>i</sub></em>, is chosen.</p>
</li>
<li>
<p>If <em>x<sub>i</sub></em> has a minimum number of close neighbors, we consider it to be part of a cluster.</p>
</li>
<li>
<p>Step 2 is repeated recursively for all of <em>x<sub>i</sub></em>’s neighbors, then neighbor’s neighbor, and so on. These are the cluster’s core observations.</p>
</li>
<li>
<p>Once step 3 runs out of nearby observations, a new random point is
chosen (i.e., restart at step 1).</p>
</li>
</ol>
<p>Once this is complete, we have a set of core observations for a number
of clusters. Finally, any observation close to a cluster but not a core
sample is considered part of a cluster, while any observation not close
to the cluster is labeled an outlier.</p>
<p><code>DBSCAN</code> has three main parameters to set:</p>
<dl>
<dt><code>eps</code></dt>
<dd>
<p>  The maximum distance from an observation for another
observation to be considered its neighbor.</p>
</dd>
<dt><code>min_samples</code></dt>
<dd>
<p>  The minimum number of observations less than <code>eps</code>
distance from an observation for it to be considered a core observation.</p>
</dd>
<dt><code>metric</code></dt>
<dd>
<p>  The distance metric used by <code>eps</code>—for example, <code>minkowski</code> or <code>euclidean</code> (note that if Minkowski distance is used, the
parameter <code>p</code> can be used to set the power of the Minkowski metric).</p>
</dd>
</dl>
<p>If we look at the clusters in our training data we can see two clusters
have been identified, <code>0</code> and <code>1</code>, while outlier observations are
labeled <code>-1</code>:<a data-primary="" data-startref="ix_cluster_dbscan" data-type="indexterm" id="id1898"/><a data-primary="" data-startref="ix_dbscan_cluster" data-type="indexterm" id="id1899"/></p>
<pre class="less_space pagebreak-before" data-code-language="python" data-type="programlisting"><code class="c1"># Show cluster membership</code>
<code class="n">model</code><code class="o">.</code><code class="n">labels_</code></pre>
<pre data-type="programlisting">array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1,  0,
        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1,
        0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  1,
        1,  1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1,  1,  1,  1,  1,  1,
       -1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
       -1,  1, -1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1, -1,  1, -1,  1,
        1,  1,  1, -1, -1, -1, -1, -1,  1,  1,  1,  1, -1,  1,  1, -1, -1,
       -1,  1,  1, -1,  1,  1, -1,  1,  1,  1, -1, -1, -1,  1,  1,  1, -1,
       -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1])</pre>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1900">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/QBx3a">DBSCAN, Wikipedia</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="19.5 Clustering Using Hierarchical Merging" data-type="sect1"><div class="sect1" id="clustering-using-hierarchical-merging">
<h1>19.5 Clustering Using Hierarchical Merging</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id387">
<h2>Problem</h2>
<p>You want to <a data-primary="clustering" data-secondary="hierarchical merging" data-type="indexterm" id="id1901"/><a data-primary="hierarchical merging, clustering" data-type="indexterm" id="id1902"/>group observations using a hierarchy of clusters.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id388">
<h2>Solution</h2>
<p>Use <a data-primary="agglomerative clustering" data-type="indexterm" id="id1903"/>agglomerative clustering:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>
<code class="kn">from</code> <code class="nn">sklearn.cluster</code> <code class="kn">import</code> <code class="n">AgglomerativeClustering</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>

<code class="c1"># Standardize features</code>
<code class="n">scaler</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>
<code class="n">features_std</code> <code class="o">=</code> <code class="n">scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Create agglomerative clustering object</code>
<code class="n">cluster</code> <code class="o">=</code> <code class="n">AgglomerativeClustering</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code>

<code class="c1"># Train model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">cluster</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_std</code><code class="p">)</code></pre>
</div></section>
<section class="less_space pagebreak-before" data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id389">
<h2>Discussion</h2>
<p><em>Agglomerative clustering</em> is a powerful, flexible hierarchical
clustering algorithm. In agglomerative clustering, all observations
start as their own clusters. Next, clusters meeting some criteria are
merged. This process is repeated, growing clusters until some
end point is reached. In scikit-learn, <code>AgglomerativeClustering</code> uses
the <code>linkage</code> parameter to determine the merging strategy to minimize:</p>
<ul>
<li>
<p>Variance of merged clusters (<code>ward</code>)</p>
</li>
<li>
<p>Average distance between observations from pairs of clusters (<code>average</code>)</p>
</li>
<li>
<p>Maximum distance between observations from pairs of clusters (<code>complete</code>)</p>
</li>
</ul>
<p>Two other parameters are useful to know. First, the <code>affinity</code> parameter
determines the distance metric used for <code>linkage</code> (<code>minkowski</code>, <code>euclidean</code>, etc.). Second, 
<span class="keep-together"><code>n_clusters</code></span> sets the number of clusters the clustering algorithm will attempt to find. That is, clusters are successively merged until only <code>n_clusters</code> remain.</p>
<p>As with other clustering algorithms we have covered, we can use
<code>labels_</code> to see the cluster in which every observation is assigned:<a data-primary="" data-startref="ix_cluster_ch19" data-type="indexterm" id="id1904"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Show cluster membership</code>
<code class="n">model</code><code class="o">.</code><code class="n">labels_</code></pre>
<pre data-type="programlisting">array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,
       1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0,
       2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 2, 0, 0, 2,
       2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])</pre>
</div></section>
</div></section>
</div></section></div></body></html>