- en: Chapter 1\. Unsupervised Learning in the Machine Learning Ecosystem
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章 机器学习生态系统中的无监督学习
- en: Most of human and animal learning is unsupervised learning. If intelligence
    was a cake, unsupervised learning would be the cake, supervised learning would
    be the icing on the cake, and reinforcement learning would be the cherry on the
    cake. We know how to make the icing and the cherry, but we don’t know how to make
    the cake. We need to solve the unsupervised learning problem before we can even
    think of getting to true AI.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 大多数人类和动物的学习都是无监督学习。如果智能是一个蛋糕，无监督学习将是蛋糕，监督学习将是蛋糕上的糖衣，而强化学习将是蛋糕上的樱桃。我们知道如何制作糖衣和樱桃，但我们不知道如何制作蛋糕。在我们甚至考虑真正的AI之前，我们需要解决无监督学习问题。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yann LeCun
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 伊恩·拉坤
- en: In this chapter, we will explore the difference between a rules-based system
    and machine learning, the difference between supervised learning and unsupervised
    learning, and the relative strengths and weaknesses of each.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨基于规则的系统与机器学习、监督学习与无监督学习之间的区别，以及每种方法的相对优势和劣势。
- en: We will also cover many popular supervised learning algorithms and unsupervised
    learning algorithms and briefly examine how semisupervised learning and reinforcement
    learning fit into the mix.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将介绍许多流行的监督学习算法和无监督学习算法，并简要探讨半监督学习和强化学习如何融入其中。
- en: Basic Machine Learning Terminology
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基础机器学习术语
- en: 'Before we delve into the different types of machine learning, let’s take a
    look at a simple and commonly used machine learning example to help make the concepts
    we introduce tangible: the email spam filter. We need to build a simple program
    that takes in emails and correctly classifies them as either “spam” or “not spam.”
    This is a straightforward classification problem.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨不同类型的机器学习之前，让我们先看一个简单且常用的机器学习示例，以帮助我们更具体地理解我们介绍的概念：电子邮件垃圾过滤器。我们需要构建一个简单的程序，输入电子邮件并正确地将它们分类为“垃圾邮件”或“非垃圾邮件”。这是一个直接的分类问题。
- en: 'Here’s a bit of machine learning terminology as a refresher: the *input variables*
    into this problem are the text of the emails. These input variables are also known
    as *features* or *predictors* or *independent variables*. The *output variable*—what
    we are trying to predict—is the *label* “spam” or “not spam.” This is also known
    as the *target variable*, *dependent variable*, or *response variable* (or *class*
    since this is a classification problem).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些机器学习术语的复习：这个问题的*输入变量*是电子邮件的文本。这些输入变量也被称为*特征*或*预测变量*或*独立变量*。我们试图预测的*输出变量*是标签“垃圾邮件”或“非垃圾邮件”。这也被称为*目标变量*、*依赖变量*或*响应变量*（或*类*，因为这是一个分类问题）。
- en: The set of examples the AI trains on is known as the *training set*, and each
    individual example is called a training *instance* or *sample*. During the training,
    the AI is attempting to minimize its *cost function* or *error rate*, or framed
    more positively, to maximize its *value function*—in this case, the ratio of correctly
    classified emails. The AI actively optimizes for a minimal error rate during training.
    Its error rate is calculated by comparing the AI’s predicted label with the true
    label.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: AI训练的示例集被称为*训练集*，每个单独的示例称为训练*实例*或*样本*。在训练过程中，AI试图最小化其*成本函数*或*错误率*，或者更积极地说，最大化其*价值函数*—在本例中，是正确分类的电子邮件比例。AI在训练期间积极优化以达到最小的错误率。它的错误率是通过将AI预测的标签与真实标签进行比较来计算的。
- en: 'However, what we care about most is how well the AI generalizes its training
    to never-before-seen emails. This will be the true test for the AI: can it correctly
    classify emails that it has never seen before using what it has learned by training
    on the examples in the training set? This *generalization error* or *out-of-sample
    error* is the main thing we use to evaluate machine learning solutions.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们最关心的是AI如何将其训练推广到以前从未见过的电子邮件上。这将是AI的真正测试：它能否使用在训练集示例中学到的知识正确分类它以前从未见过的电子邮件？这种*泛化误差*或*样外误差*是我们用来评估机器学习解决方案的主要指标。
- en: This set of never-before-seen examples is known as the *test set* or *holdout
    set* (because the data is held out from the training). If we choose to have multiple
    holdout sets (perhaps to gauge our generalization error as we train, which is
    advisable), we may have intermediate holdout sets that we use to evaluate our
    progress before the final test set; these intermediate holdout sets are called
    *validation sets*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: To put all of this together, the AI trains on the training data (*experience*)
    to improve its error rate (*performance*) in flagging spam (*task*), and the ultimate
    success criterion is how well its experience generalizes to new, never-before-seen
    data (*generalization error*).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Rules-Based vs. Machine Learning
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using a rules-based approach, we can design a spam filter with explicit rules
    to catch spam such as flag emails with “u” instead of “you,” “4” instead of “for,”
    “BUY NOW,” etc. But this system would be difficult to maintain over time as bad
    guys change their spam behavior to evade the rules. If we used a rules-based system,
    we would have to frequently adjust the rules manually just to stay up-to-date.
    Also, it would be very expensive to set up—think of all the rules we would need
    to create to make this a well-functioning system.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Instead of a rules-based approach, we can use machine learning to train on the
    email data and automatically engineer rules to correctly flag malicious email
    as spam. This machine learning-based system could be automatically adjusted over
    time as well. This system would be much cheaper to train and maintain.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: In this simple email problem, it may be possible for us to handcraft rules,
    but, for many problems, handcrafting rules is not feasible at all. For example,
    consider designing a self-driving car—imagine drafting rules for how the car should
    behave in each and every single instance it ever encounters. This is an intractable
    problem unless the car can learn and adapt on its own based on its experience.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: We could also use machine learning systems as an exploration or data discovery
    tool to gain deeper insight into the problem we are trying to solve. For example,
    in the email spam filter example, we can learn which words or phrases are most
    predictive of spam and recognize newly emerging malicious spam patterns.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Supervised vs. Unsupervised
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The field of machine learning has two major branches—*supervised learning* and
    *unsupervised learning*—and plenty of sub-branches that bridge the two.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: In supervised learning, the AI agent has access to labels, which it can use
    to improve its performance on some task. In the email spam filter problem, we
    have a dataset of emails with all the text within each and every email. We also
    know which of these emails are spam or not (the so-called *labels*). These labels
    are very valuable in helping the supervised learning AI separate the spam emails
    from the rest.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: In unsupervised learning, labels are not available. Therefore, the task of the
    AI agent is not well-defined, and performance cannot be so clearly measured. Consider
    the email spam filter problem—this time without labels. Now, the AI agent will
    attempt to understand the underlying structure of emails, separating the database
    of emails into different groups such that emails within a group are similar to
    each other but different from emails in other groups.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，没有标签可用。因此，AI代理的任务并不是明确定义的，性能也不能如此清晰地衡量。考虑电子邮件垃圾邮件过滤器问题——这次没有标签。现在，AI代理将尝试理解电子邮件的基本结构，将电子邮件数据库分成不同的组，使得组内的电子邮件彼此相似但与其他组的电子邮件不同。
- en: This unsupervised learning problem is less clearly defined than the supervised
    learning problem and harder for the AI agent to solve. But, if handled well, the
    solution is more powerful.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个无监督学习问题比监督学习问题的定义不太明确，对AI代理来说更难解决。但是，如果处理得当，解决方案将更为强大。
- en: 'Here’s why: the unsupervised learning AI may find several groups that it later
    tags as being “spam”—but the AI may also find groups that it later tags as being
    “important” or categorize as “family,” “professional,” “news,” “shopping,” etc.
    In other words, because the problem does not have a strictly defined task, the
    AI agent may find interesting patterns above and beyond what we initially were
    looking for.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 原因在于：无监督学习AI可能会发现几个后来标记为“垃圾邮件”的组，但AI也可能会发现后来标记为“重要”的组，或者归类为“家庭”、“专业”、“新闻”、“购物”等。换句话说，由于问题没有严格定义的任务，AI代理可能会发现我们最初未曾寻找的有趣模式。
- en: Moreover, this unsupervised system is better than the supervised system at finding
    new patterns in future data, making the unsupervised solution more nimble on a
    go-forward basis. This is the power of unsupervised learning.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这种无监督系统在未来数据中发现新模式的能力优于监督系统，使得无监督解决方案在前进时更加灵活。这就是无监督学习的力量。
- en: The Strengths and Weaknesses of Supervised Learning
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习的优势和劣势
- en: Supervised learning excels at optimizing performance in well-defined tasks with
    plenty of labels. For example, consider a very large dataset of images of objects,
    where each image is labeled. If the dataset is sufficiently large enough and we
    train using the right machine learning algorithms (i.e., convolutional neural
    networks) and with powerful enough computers, we can build a very good supervised
    learning-based image classification system.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习在定义良好的任务和充足标签的情况下优化性能。例如，考虑一个非常大的对象图像数据集，其中每个图像都有标签。如果数据集足够大，并且我们使用正确的机器学习算法（即卷积神经网络）并且使用足够强大的计算机进行训练，我们可以构建一个非常好的基于监督学习的图像分类系统。
- en: As the supervised learning AI trains on the data, it will be able to measure
    its performance (via a cost function) by comparing its predicted image label with
    the true image label that we have on file. The AI will explicitly try to minimize
    this cost function such that its error on never-before-seen images (from a holdout
    set) is as low as possible.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当监督学习AI在数据上进行训练时，它将能够通过比较其预测的图像标签与我们文件中的真实图像标签来测量其性能（通过成本函数）。AI将明确尝试将这个成本函数最小化，使其在以前未见过的图像（从留存集）上的错误尽可能低。
- en: This is why labels are so powerful—they help guide the AI agent by providing
    it with an error measure. The AI uses the error measure to improve its performance
    over time. Without such labels, the AI does not know how successful it is (or
    isn’t) in correctly classifying images.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么标签如此强大——它们通过提供错误度量来指导AI代理。AI使用这个错误度量随着时间的推移来提高其性能。没有这样的标签，AI不知道它在正确分类图像方面有多成功（或不成功）。
- en: However, the costs of manually labeling an image dataset are high. And, even
    the best curated image datasets have only thousands of labels. This is a problem
    because supervised learning systems will be very good at classifying images of
    objects for which it has labels but poor at classifying images of objects for
    which it has no labels.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，手动标记图像数据集的成本很高。即使是最好的策划图像数据集也只有数千个标签。这是一个问题，因为监督学习系统在对具有标签的对象图像分类方面表现非常出色，但在对没有标签的对象图像分类方面表现不佳。
- en: As powerful as supervised learning systems are, they are also limited at generalizing
    knowledge beyond the labeled items they have trained on. Since the majority of
    the world’s data is unlabeled, with supervised learning, the ability of AI to
    expand its performance to never-before-seen instances is quite limited.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管监督学习系统非常强大，但它们在将知识推广到以前未见过的实例上的能力也受到限制。由于世界上大多数数据都没有标签，因此使用监督学习时，AI将其性能扩展到以前未见过的实例的能力是相当有限的。
- en: In other words, supervised learning is great at solving narrow AI problems but
    not so good at solving more ambitious, less clearly defined problems of the strong
    AI type.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，监督学习擅长解决狭义AI问题，但在解决更有雄心、定义不太明确的强AI类型问题时表现不佳。
- en: The Strengths and Weaknesses of Unsupervised Learning
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习的优势和劣势
- en: Supervised learning will trounce unsupervised learning at narrowly defined tasks
    for which we have well-defined patterns that do not change much over time and
    sufficiently large, readily available labeled datasets.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在狭义定义的任务中，有着明确定义的模式并且随时间变化不大以及具有充足可用的标记数据集时，监督学习将在效果上胜过无监督学习。
- en: However, for problems where patterns are unknown or constantly changing or for
    which we do not have sufficiently large labeled datasets, unsupervised learning
    truly shines.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于那些模式未知或不断变化，或者我们没有足够大的标记数据集的问题，无监督学习确实表现出色。
- en: Instead of being guided by labels, unsupervised learning works by learning the
    underlying structure of the data it has trained on. It does this by trying to
    represent the data it trains on with a set of parameters that is significantly
    smaller than the number of examples available in the dataset. By performing this
    representation learning, unsupervised learning is able to identify distinct patterns
    in the dataset.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习不依赖标签，而是通过学习其训练的数据的基本结构来工作。它通过试图用比数据集中可用示例数量显著较小的一组参数来表示其训练的数据来实现这一点。通过执行这种表示学习，无监督学习能够识别数据集中的不同模式。
- en: In the image dataset example (this time without labels), the unsupervised learning
    AI may be able to identify and group images based on how similar they are to each
    other and how different they are from the rest. For example, all the images that
    look like chairs will be grouped together, all the images that look like dogs
    will be grouped together, etc.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像数据集示例中（这次没有标签），无监督学习的AI可能能够根据它们彼此的相似性以及与其余图像的不同性将图像识别并分组。例如，所有看起来像椅子的图像将被分组在一起，所有看起来像狗的图像将被分组在一起，依此类推。
- en: Of course, the unsupervised learning AI itself cannot label these groups as
    “chairs” or “dogs” but now that similar images are grouped together, humans have
    a much simpler labeling task. Instead of labeling millions of images by hand,
    humans can manually label all the distinct groups, and the labels will apply to
    all the members within each group.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，无监督学习的AI本身无法将这些组标记为“椅子”或“狗”，但现在相似的图像被分组在一起后，人类的标记任务变得简单得多。人类可以手动标记所有不同的组，标签将应用于每个组内的所有成员。
- en: After the initial training, if the unsupervised learning AI finds images that
    do not belong to any of the labeled groups, the AI will create separate groups
    for the unclassified images, triggering a human to label the new, yet-to-be-labeled
    groups of images.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 经过初步训练后，如果无监督学习的AI发现了不属于任何已标记组的图像，AI将为未分类的图像创建单独的组，触发人类标记新的、尚未标记的图像组。
- en: Unsupervised learning makes previously intractable problems more solvable and
    is much more nimble at finding hidden patterns both in the historical data that
    is available for training and in future data. Moreover, we now have an AI approach
    for the huge troves of unlabeled data that exist in the world.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习使以前棘手的问题更易解决，并且在找到历史数据和未来数据中隐藏模式方面更为灵活。此外，我们现在有了一种处理世界上存在的大量未标记数据的AI方法。
- en: Even though unsupervised learning is less adept than supervised learning at
    solving specific, narrowly defined problems, it is better at tackling more open-ended
    problems of the strong AI type and at generalizing this knowledge.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管无监督学习在解决特定、狭义定义的问题方面不如监督学习熟练，但在解决更为开放的强AI类型问题和推广这种知识方面表现更佳。
- en: Just as importantly, unsupervised learning can address many of the common problems
    data scientists encounter when building machine learning solutions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 同样重要的是，无监督学习可以解决数据科学家在构建机器学习解决方案时遇到的许多常见问题。
- en: Using Unsupervised Learning to Improve Machine Learning Solutions
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用无监督学习来改善机器学习解决方案
- en: Recent successes in machine learning have been driven by the availability of
    lots of data, advances in computer hardware and cloud-based resources, and breakthroughs
    in machine learning algorithms. But these successes have been in mostly narrow
    AI problems such as image classification, computer vision, speech recognition,
    natural language processing, and machine translation.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的最近成功是由大量数据的可用性、计算硬件和基于云的资源的进步以及机器学习算法的突破推动的。但这些成功主要出现在狭义AI问题，如图像分类、计算机视觉、语音识别、自然语言处理和机器翻译领域。
- en: To solve more ambitious AI problems, we need to unlock the value of unsupervised
    learning. Let’s explore the most common challenges data scientists face when building
    solutions and how unsupervised learning can help.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决更雄心勃勃的AI问题，我们需要发挥无监督学习的价值。让我们探讨数据科学家在构建解决方案时面临的最常见挑战，以及无监督学习如何帮助解决这些挑战。
- en: Insufficient labeled data
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标记不足的数据
- en: I think AI is akin to building a rocket ship. You need a huge engine and a lot
    of fuel. If you have a large engine and a tiny amount of fuel, you won’t make
    it to orbit. If you have a tiny engine and a ton of fuel, you can’t even lift
    off. To build a rocket you need a huge engine and a lot of fuel.
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我认为AI就像建造一艘火箭。你需要一个巨大的引擎和大量的燃料。如果你有一个巨大的引擎和少量的燃料，你无法进入轨道。如果你有一个微小的引擎和大量的燃料，你甚至无法起飞。要建造一艘火箭，你需要一个巨大的引擎和大量的燃料。
- en: ''
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Andrew Ng
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Andrew Ng
- en: If machine learning were a rocket ship, data would be the fuel—without lots
    and lots of data, the rocket ship cannot fly. But not all data is created equal.
    To use supervised algorithms, we need lots of labeled data, which is hard and
    costly to generate.^([1](ch01.html#idm140637564299232))
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果机器学习是一艘火箭，数据就是燃料——没有大量数据，火箭是无法飞行的。但并非所有数据都是平等的。要使用监督算法，我们需要大量标记数据，这在生成过程中是困难且昂贵的。^([1](ch01.html#idm140637564299232))
- en: 'With unsupervised learning, we can automatically label unlabeled examples.
    Here is how it would work: we would cluster all the examples and then apply the
    labels from labeled examples to the unlabeled ones within the same cluster. Unlabeled
    examples would receive the label of the labeled ones they are most similar to.
    We will explore clustering in [Chapter 5](ch05.html#Chapter_5).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用无监督学习，我们可以自动标记未标记的示例。这里是它的工作原理：我们会对所有示例进行聚类，然后将标记示例的标签应用于同一聚类中的未标记示例。未标记的示例将获得它们与之最相似的已标记示例的标签。我们将在[第5章](ch05.html#Chapter_5)中探讨聚类。
- en: Overfitting
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过拟合
- en: If the machine learning algorithm learns an overly complex function based on
    the training data, it may perform very poorly on never-before-seen instances from
    holdout sets such as the validation set or test set. In this case, the algorithm
    has overfit the training data—by extracting too much from the noise in the data—and
    has very poor generalization error. In other words, the algorithm is memorizing
    the training data rather than learning how to generalize knowledge based off of
    it.^([2](ch01.html#idm140637564293600))
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果机器学习算法根据训练数据学习了一个过于复杂的函数，它在从保留集（例如验证集或测试集）中获得的以前未见实例上可能表现非常糟糕。在这种情况下，算法过度拟合了训练数据——从数据中提取了太多的噪声，并且具有非常差的泛化误差。换句话说，该算法是在记忆训练数据，而不是学习如何基于其泛化知识。^([2](ch01.html#idm140637564293600))
- en: To address this, we can introduce unsupervised learning as a *regularizer*.
    *Regularization* is a process used to reduce the complexity of a machine learning
    algorithm, helping it capture the signal in the data without adjusting too much
    to the noise. Unsupervised pretraining is one such form of regularization. Instead
    of feeding the original input data directly into a supervised learning algorithm,
    we can feed a new representation of the original input data that we generate.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以将无监督学习引入作为*正则化器*。*正则化*是一种用来降低机器学习算法复杂度的过程，帮助其捕捉数据中的信号而不是过多地调整到噪声。无监督预训练就是这种正则化的形式之一。我们可以不直接将原始输入数据馈送到监督学习算法中，而是馈送我们生成的原始输入数据的新表示。
- en: This new representation captures the essence of the original data—the true underlying
    structure—while losing some of the less representative noise along the way. When
    we feed this new representation into the supervised learning algorithm, it has
    less noise to wade through and captures more of the signal, improving its generalization
    error. We will explore feature extraction in [Chapter 7](ch07.html#Chapter_7).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Curse of dimensionality
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even with the advances in computational power, big data is hard for machine
    learning algorithms to manage. In general, adding more instances is not too problematic
    because we can parallelize operations using modern map-reduce solutions such as
    Spark. However, the more features we have, the more difficult training becomes.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: In a very high-dimensional space, supervised algorithms need to learn how to
    separate points and build a function approximation to make good decisions. When
    the features are very numerous, this search becomes very expensive, both from
    a time and compute perspective. In some cases, it may be impossible to find a
    good solution fast enough.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: This problem is known as the *curse of dimensionality*, and unsupervised learning
    is well suited to help manage this. With dimensionality reduction, we can find
    the most salient features in the original feature set, reduce the number of dimensions
    to a more manageable number while losing very little important information in
    the process, and then apply supervised algorithms to more efficiently perform
    the search for a good function approximation. We will cover dimensionality reduction
    in [Chapter 3](ch03.html#Chapter_3).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Feature engineering is one of the most vital tasks data scientists perform.
    Without the right features, the machine learning algorithm will not be able to
    separate points in space well enough to make good decisions on never-before-seen
    examples. However, feature engineering is typically very labor-intensive; it requires
    humans to creatively hand-engineer the right types of features. Instead, we can
    use representation learning from unsupervised learning algorithms to automatically
    learn the right types of feature representations to help solve the task at hand.
    We will explore automatic feature extraction in [Chapter 7](ch07.html#Chapter_7).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Outliers
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The quality of data is also very important. If machine learning algorithms train
    on rare, distortive outliers, their generalization error will be lower than if
    they ignored or addressed the outliers separately. With unsupervised learning,
    we can perform outlier detection using dimensionality reduction and create a solution
    specifically for the outliers and, separately, a solution for the normal data.
    We will build an anomaly detection system in [Chapter 4](ch04.html#Chapter_4).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Data drift
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Machine learning models also need to be aware of drift in the data. If the data
    the model is making predictions on differs statistically from the data the model
    trained on, the model may need to retrain on data that is more representative
    of the current data. If the model does not retrain or does not recognize the drift,
    the model’s prediction quality on current data will suffer.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: By building probability distributions using unsupervised learning, we can assess
    how different the current data is from the training set data—if the two are different
    enough, we can automatically trigger a retraining. We will explore how to build
    these types of data discriminators in [Chapter 12](ch12.html#Chapter_12).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: A Closer Look at Supervised Algorithms
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we delve into unsupervised learning systems, let’s take a look at supervised
    learning algorithms and how they work. This will help frame where unsupervised
    learning fits within the machine learning ecosystem.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'In supervised learning, there are two major types of problems: *classification*
    and *regression*. In classification, the AI must correctly classify items into
    one of two or more classes. If there are just two classes, the problem is called
    *binary classification*. If there are three or more classes, the problem is classed
    *multiclass classification*.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Classification problems are also known as *discrete* prediction problems because
    each class is a discrete group. Classification problems also may be referred to
    as *qualitative* or *categorical* problems.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: In regression, the AI must predict a *continuous* variable rather than a discrete
    one. Regression problems also may be referred to as *quantitative* problems.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Supervised machine learning algorithms span the gamut, from very simple to very
    complex, but they are all aimed at minimizing some cost function or error rate
    (or maximizing a value function) that is associated with the labels we have for
    the dataset.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned before, what we care about most is how well the machine learning
    solution generalizes to never-before-seen cases. The choice of the supervised
    learning algorithm is very important at minimizing this generalization error.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: To achieve the lowest possible generalization error, the complexity of the algorithmic
    model should match the complexity of the true function underlying the data. We
    do not know what this true function really is. If we did, we would not need to
    use machine learning to create a model—we would just solve the function to find
    the right answer. But since we do not know what this true function is, we choose
    a machine learning algorithm to test hypotheses and find the model that best approximates
    this true function (i.e., has the lowest possible generalization error).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: If what the algorithm models is less complex than the true function, we have
    *underfit* the data. In this case, we could improve the generalization error by
    choosing an algorithm that can model a more complex function. However, if the
    algorithm designs an overly complex model, we have *overfit* the training data
    and will have poor performance on never-before-seen cases, increasing our generalization
    error.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果算法模拟的内容比真实函数复杂度低，我们就*欠拟合*了数据。在这种情况下，我们可以通过选择能够模拟更复杂函数的算法来改善泛化误差。然而，如果算法设计了一个过于复杂的模型，我们就*过拟合*了训练数据，并且在以前从未见过的情况下表现不佳，增加了我们的泛化误差。
- en: In other words, choosing more complex algorithms over simpler ones is not always
    the right choice—sometimes simpler is better. Each algorithm comes with its set
    of strengths, weaknesses, and assumptions, and knowing what to use when given
    the data you have and the problem you are trying to solve is very important to
    mastering machine learning.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，选择复杂算法而不是简单算法并不总是正确的选择——有时简单才是更好的。每种算法都有其一系列的优点、弱点和假设，知道在给定你拥有的数据和你试图解决的问题时何时使用何种方法对于掌握机器学习非常重要。
- en: In the rest of this chapter, we will describe some of the most common supervised
    algorithms (including some real-world applications) before doing the same for
    unsupervised algorithms.^([3](ch01.html#idm140637564244608))
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分中，我们将描述一些最常见的监督学习算法（包括一些实际应用），然后再介绍无监督算法。^([3](ch01.html#idm140637564244608))
- en: Linear Methods
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性方法
- en: The most basic supervised learning algorithms model a simple linear relationship
    between the input features and the output variable that we wish to predict.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的监督学习算法模拟了输入特征与我们希望预测的输出变量之间的简单线性关系。
- en: Linear regression
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性回归
- en: The simplest of all the algorithms is *linear regression*, which uses a model
    that assumes a linear relationship between the input variables (x) and the single
    output variable (y). If the true relationship between the inputs and the output
    is linear and the input variables are not highly correlated (a situation known
    as *collinearity*), linear regression may be an appropriate choice. If the true
    relationship is more complex or nonlinear, linear regression will underfit the
    data.^([4](ch01.html#idm140637564235360))
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 所有算法中最简单的是*线性回归*，它使用一个模型假设输入变量（x）与单个输出变量（y）之间存在线性关系。如果输入与输出之间的真实关系是线性的，并且输入变量之间不高度相关（称为*共线性*），线性回归可能是一个合适的选择。如果真实关系更为复杂或非线性，线性回归将会欠拟合数据。^([4](ch01.html#idm140637564235360))
- en: Because it is so simple, interpreting the relationship modeled by the algorithm
    is also very straightforward. *Interpretability* is a very important consideration
    for applied machine learning because solutions need to be understood and enacted
    by both technical and nontechnical people in industry. Without interpretability,
    the solutions become inscrutable black boxes.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 因为它非常简单，解释算法模型的关系也非常直接。*可解释性* 对于应用机器学习非常重要，因为解决方案需要被技术和非技术人员在工业中理解和实施。如果没有可解释性，解决方案就会变成不可理解的黑匣子。
- en: Strengths
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 优点
- en: Linear regression is simple, intrepretable, and hard to overfit because it cannot
    model overly complex relationships. It is an excellent choice when the underlying
    relationship between the input and output variables is linear.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归简单、可解释，并且难以过拟合，因为它无法模拟过于复杂的关系。当输入和输出变量之间的基础关系是线性的时，它是一个极好的选择。
- en: Weaknesses
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 弱点
- en: Linear regression will underfit the data when the relationship between the input
    and output variables is nonlinear.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入和输出变量之间的关系是非线性的时，线性回归将欠拟合数据。
- en: Applications
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 应用
- en: Since the true underlying relationship between human weight and human height
    is linear, linear regression is great for predicting weight using height as the
    input variable or, vice versa, for predicting height using weight as the input
    variable.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 由于人类体重与身高之间的真实基础关系是线性的，因此线性回归非常适合使用身高作为输入变量来预测体重，或者反过来，使用体重作为输入变量来预测身高。
- en: Logistic regression
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: The simplest classification algorithm is *logistic regression*, which is also
    a linear method but the predictions are transformed using the logistic function.
    The outputs of this transformation are *class probabilities*—in other words, the
    probabilities that the instance belongs to the various classes, where the sum
    of the probabilities for each instance adds up to one. Each instance is then assigned
    to the class for which it has the highest probability of belonging in.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Strengths
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Like linear regression, logistic regression is simple and interpretable. When
    the classes we are trying to predict are nonoverlapping and linearly separable,
    logistic regression is an excellent choice.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Weaknesses
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: When classes are not linearly separable, logistic regression will fail.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: When classes are mostly nonoverlapping—for example, the heights of young children
    versus the heights of adults—logistic regression will work well.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Neighborhood-Based Methods
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another group of very simple algorithms are neighborhood-based methods. Neighborhood-based
    methods are *lazy learners* since they learn how to label new points based on
    the proximity of the new points to existing labeled points. Unlike linear regression
    or logistic regression, neighborhood-based models do not learn a set model to
    predict labels for new points; rather, these models predict labels for new points
    based purely on distance of new points to preexisting labeled points. Lazy learning
    is also referred to as *instance-based learning* or *nonparametric methods*.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: k-nearest neighbors
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most common neighborhood-based method is *k-nearest neighbors (KNN)*. To
    label each new point, KNN looks at a *k* number (where *k* is an integer value)
    of nearest labeled points and has these already labeled neighbors vote on how
    to label the new point. By default, KNN uses Euclidean distance to measure what
    is closest.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: The choice of *k* is very important. If *k* is set to a very low value, KNN
    becomes very flexible, drawing highly nuanced boundaries and potentially overfitting
    the data. If *k* is set to a very high value, KNN becomes inflexible, drawing
    a too rigid boundary and potentially underfitting the data.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Strengths
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Unlike linear methods, KNN is highly flexible and adept at learning more complex,
    nonlinear relationships. Yet, KNN remains simple and interpretable.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Weaknesses
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: KNN does poorly when the number of observations and features grow. KNN becomes
    computationally inefficient in this highly populated, high-dimensional space since
    it needs to calculate distances from the new point to many nearby labeled points
    in order to predict labels. It cannot rely on an efficient model with a reduced
    number of parameters to make the necessary prediction. Also, KNN is very sensitive
    to the choice of *k*. When *k* is set too low, KNN can overfit, and when *k* is
    set too high, KNN can underfit.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: KNN is regularly used in recommender systems, such as those used to predict
    taste in movies (Netflix), music (Spotify), friends (Facebook), photos (Instagram),
    search (Google), and shopping (Amazon). For example, KNN can help predict what
    a user will like given what similar users like (known as *collaborative filtering*)
    or what the user has liked in the past (known as *content-based filtering*).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Tree-Based Methods
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instead of using a linear method, we can have the AI build a *decision tree*
    where all the instances are *segmented* or *stratified* into many regions, guided
    by the labels we have. Once this segmentation is complete, each region corresponds
    to a particular class of label (for classification problems) or a range of predicted
    values (for regression problems). This process is similar to having the AI build
    rules automatically with the explicit goal of making better decisions or predictions.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Single decision tree
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The simplest tree-based method is a *single decision tree*, in which the AI
    goes once through the training data, creates rules for segmenting the data guided
    by the labels, and uses this tree to make predictions on the never-before-seen
    validation or test set. However, a single decision tree is usually poor at generalizing
    what it has learned during training to never-before-seen cases because it usually
    overfits the training data during its one and only training iteration.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Bagging
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To improve the single decision tree, we can introduce *bootstrap aggregation*
    (more commonly known as *bagging*), in which we take *multiple random samples
    of instances* from the training data, create a decision tree for each sample,
    and then predict the output for each instance by averaging the predictions of
    each of these trees. By using *randomization* of samples and averaging results
    from multiple trees—an approach that is also known as the *ensemble method*—bagging
    will address some of the overfitting that results from a single decision tree.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Random forests
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can improve overfitting further by sampling not only the instances but also
    the predictors. With *random forests*, we take multiple random samples of instances
    from the training data like we do in bagging, but, for each split in each decision
    tree, we make the split based not on all the predictors but rather a *random sample
    of the predictors*. The number of predictors we consider for each split is usually
    the square root of the total number of predictors.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: By sampling the predictors in this way, the random forests algorithm creates
    trees that are even less correlated with each other (compared to the trees in
    bagging), reducing overfitting and improving the generalization error.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Boosting
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another approach, known as *boosting*, is used to create multiple trees like
    in bagging but to *build the trees sequentially*, using what the AI learned from
    the previous tree to improve results on the subsequent tree. Each tree is kept
    pretty shallow, with only a few decision splits, and the learning occurs slowly,
    tree by tree. Of all the tree-based methods, *gradient boosting machines* are
    among the best-performing and are commonly used to win machine learning competitions.^([5](ch01.html#idm140637564424384))
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Strengths
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Tree-based methods are among the best-performing supervised-learning algorithms
    for prediction problems. These methods are able to capture complex relationships
    in the data by learning many simple rules, one rule at a time. They are also capable
    of handling missing data and categorical features.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Weaknesses
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Tree-based methods are difficult to interpret, especially if many rules are
    needed to make a good prediction. Performance also becomes an issue as the number
    of features increase.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosting and random forests are excellent for prediction problems.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Support Vector Machines
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instead of building trees to separate data, we can use algorithms to create
    hyperplanes in space that separate the data, guided by the labels that we have.
    The approach is known as *support vector machines (SVMs)*. SVMs allow some violations
    to this separation—not all the points within an area in hyperspace need to have
    the same label—but the distance between boundary-defining points of a certain
    label and the boundary-defining points of another label should be maximized as
    much as possible. Also, the boundaries do not have to be linear—we can use nonlinear
    kernels to more flexibly separate the data.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Neural Networks
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can learn representations of the data using neural networks, which are composed
    of an input layer, several hidden layers, and an output layer.^([6](ch01.html#idm140637564409376))
    The input layer uses the features, and the output layer tries to match the response
    variable. The hidden layers are a nested hierarchy of concepts—each layer (or
    concept) is trying to understand how the previous layer relates to the output
    layer.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Using this hierarchy of concepts, the neural network is able to learn complicated
    concepts by building them out of simpler ones. Neural networks are one of the
    most powerful approaches to function approximation but are prone to overfitting
    and are hard to interpret, shortcomings that we will explore in greater detail
    later in the book.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: A Closer Look at Unsupervised Algorithms
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now turn our attention to problems where we do not have labels. Instead
    of trying to make predictions, unsupervised learning algorithms will try to learn
    the underlying structure of the data.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality Reduction
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One family of algorithms—known as *dimensionality reduction algorithms*—projects
    the original high-dimensional input data to a low-dimensional space, filtering
    out the not-so-relevant features and keeping as much of the interesting ones as
    possible. Dimensionality reduction allows unsupervised learning AI to more effectively
    identify patterns and more efficiently solve large-scale, computationally expensive
    problems (often involving images, video, speech, and text).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Linear projection
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are two major branches of dimensionality—linear projection and nonlinear
    dimensionality reduction. We will start with linear projection first.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Principal component analysis (PCA)
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One approach to learning the underlying structure of data is to identify which
    features out of the full set of features are most important in explaining the
    variability among the instances in the data. Not all features are equal—for some
    features, the values in the dataset do not vary much, and these features are less
    useful in explaining the dataset. For other features, the values might vary considerably—these
    features are worth exploring in greater detail since they will be better at helping
    the model we design separate the data.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: In *PCA*, the algorithm finds a low-dimensional representation of the data while
    retaining as much of the variation as possible. The number of dimensions we are
    left with is considerably smaller than the number of dimensions of the full dataset
    (i.e., the number of total features). We lose some of the variance by moving to
    this low-dimensional space, but the underlying structure of the data is easier
    to identify, allowing us to perform tasks like clustering more efficiently.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: There are several variants of PCA, which we will explore later in the book.
    These include mini-batch variants such as *incremental PCA*, nonlinear variants
    such as *kernel PCA*, and sparse variants such as *sparse PCA*.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Singular value decomposition (SVD)
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another approach to learning the underlying structure of the data is to reduce
    the rank of the original matrix of features to a smaller rank such that the original
    matrix can be recreated using a linear combination of some of the vectors in the
    smaller rank matrix. This is known as *SVD*. To generate the smaller rank matrix,
    SVD keeps the vectors of the original matrix that have the most information (i.e.,
    the highest singular value). The smaller rank matrix captures the most important
    elements of the original feature space.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Random projection
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A similar dimensionality reduction algorithm involves projecting points from
    a high-dimensional space to a space of much lower dimensions in such a way that
    the scale of distances between the points is preserved. We can use either a *random
    Gaussian matrix* or a *random sparse matrix* to accomplish this.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Manifold learning
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Both PCA and random projection rely on projecting the data linearly from a high-dimensional
    space to a low-dimensional space. Instead of a linear projection, it may be better
    to perform a nonlinear transformation of the data—this is known as *manifold learning*
    or *nonlinear dimensionality reduction*.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Isomap
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Isomap* is one type of manifold learning approach. This algorithm learns the
    intrinsic geometry of the data manifold by estimating the *geodesic* or *curved
    distance* between each point and its neighbors rather than the Euclidean distance.
    Isomap uses this to then embed the original high-dimensional space to a low-dimensional
    one.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: t-distributed stochastic neighbor embedding (t-SNE)
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another nonlinear dimensionality reduction—known as *t-SNE*—embeds high-dimensional
    data into a space of just two or three dimensions, allowing the transformed data
    to be visualized. In this two- or three-dimensional space, similar instances are
    modeled closer together and dissimilar instances are modeled further away.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Dictionary learning
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An approach known as *dictionary learning* involves learning the sparse representation
    of the underlying data. These representative elements are simple, binary vectors
    (zeros and ones), and each instance in the dataset can be reconstructed as a weighted
    sum of the representative elements. The matrix (known as the *dictionary*) that
    this unsupervised learning generates is mostly populated by zeros with only a
    few nonzero weights.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: By creating such a dictionary, this algorithm is able to efficiently identify
    the most salient representative elements of the original feature space—these are
    the ones that have the most nonzero weights. The representative elements that
    are less important will have few nonzero weights. As with PCA, dictionary learning
    is excellent for learning the underlying structure of the data, which will be
    helpful in separating the data and in identifying interesting patterns.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Independent component analysis
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One common problem with unlabeled data is that there are many independent signals
    embedded together into the features we are given. Using *independent component
    analysis (ICA)*, we can separate these blended signals into their individual components.
    After the separation is complete, we can reconstruct any of the original features
    by adding together some combination of the individual components we generate.
    ICA is commonly used in signal processing tasks (for example, to identify the
    individual voices in an audio clip of a busy coffeehouse).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Latent Dirichlet allocation
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unsupervised learning can also explain a dataset by learning why some parts
    of the dataset are similar to each other. This requires learning unobserved elements
    within the dataset—an approach known as *latent Dirichlet allocation (LDA)*. For
    example, consider a document of text with many, many words. These words within
    a document are not purely random; rather, they exhibit some structure.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: This structure can be modeled as unobserved elements known as topics. After
    training, LDA is able to explain a given document with a small set of topics,
    where for each topic there is a small set of frequently used words. This is the
    hidden structure the LDA is able to capture, helping us better explain a previously
    unstructured corpus of text.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-157
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Dimensionality reduction reduces the original set of features to a smaller set
    of just the most important features. From here, we can run other unsupervised
    learning algorithms on this smaller set of features to find interesting patterns
    in the data (see the next section on clustering), or, if we have labels, we can
    speed up the training cycle of supervised learning algorithms by feeding in this
    smaller matrix of features instead of using the original feature matrix.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we have reduced the set of original features to a smaller, more manageable
    set, we can find interesting patterns by grouping similar instances of data together.
    This is known as clustering and can be accomplished with a variety of unsupervised
    learning algorithms and be used for real-world applications such as market segmentation.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: k-means
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To cluster well, we need to identify distinct groups such that the instances
    within a group are similar to each other but different from instances in other
    groups. One such algorithm is *k-means clustering*. With this algorithm, we specify
    the number of desired clusters *k*, and the algorithm will assign each instance
    to exactly one of these *k* clusters. It optimizes the grouping by minimizing
    the *within-cluster variation* (also known as *inertia*) such that the sum of
    the within-cluster variations across all *k* clusters is as small as possible.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: To speed up this clustering process, *k*-means randomly assigns each observation
    to one of the *k* clusters and then begins to reassign these observations to minimize
    the Euclidean distance between each observation and its cluster’s center point,
    or *centroid*. As a result, different runs of *k*-means—each with a randomized
    start—will result in slightly different clustering assignments of the observations.
    From these different runs, we can choose the one that has the best separation,
    defined as the lowest total sum of within-cluster variations across all *k* clusters.^([7](ch01.html#idm140637564325008))
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An alternative clustering approach—one that does not require us to precommit
    to a particular number of clusters—is known as *hierarchical clustering*. One
    version of hierarchical clustering called *agglomerative clustering* uses a tree-based
    clustering method, and builds what is called a *dendrogram*. A dendrogram can
    be depicted graphically as an upside-down tree, where the leaves are at the bottom
    and the tree trunk is at the top.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: The leaves at the very bottom are individual instances in the dataset. Hierarchical
    clustering then joins the leaves together—as we move vertically up the upside-down
    tree—based on how similar they are to each other. The instances (or groups of
    instances) that are most similar to each other are joined sooner, while the instances
    that are not as similar are joined later. With this iterative process, all the
    instances are eventually linked together forming the single trunk of the tree.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: This vertical depiction is very helpful. Once the hierarchical clustering algorithm
    has finished running, we can view the dendrogram and determine where we want to
    cut the tree—the lower we cut, the more individual branches we are left with (i.e.,
    more clusters). If we want fewer clusters, we can cut higher on the dendrogram,
    closer to the single trunk at the very top of this upside-down tree. The placement
    of this vertical cut is similar to choosing the number of *k* clusters in the
    *k*-means clustering algorithm.^([8](ch01.html#idm140637564993824))
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An even more powerful clustering algorithm (based on the density of points)
    is known as *DBSCAN* (density-based spatial clustering of applications with noise).
    Given all the instances we have in space, DBSCAN will group together those that
    are packed closely together, where close together is defined as a minimum number
    of instances that must exist within a certain distance. We specify both the minimum
    number of instances required and the distance.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: If an instance is within this specified distance of multiple clusters, it will
    be grouped with the cluster to which it is most densely located. Any instance
    that is not within this specified distance of another cluster is labeled an outlier.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Unlike *k*-means, we do not need to prespecify the number of clusters. We can
    also have arbitrarily shaped clusters. DBSCAN is much less prone to the distortion
    typically caused by outliers in the data.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Feature Extraction
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With unsupervised learning, we can learn new representations of the original
    features of data—a field known as *feature extraction*. Feature extraction can
    be used to reduce the number of original features to a smaller subset, effectively
    performing dimensionality reduction. But feature extraction can also generate
    new feature representations to help improve performance on supervised learning
    problems.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To generate new feature representations, we can use a feedforward, nonrecurrent
    neural network to perform representation learning, where the number of nodes in
    the output layer matches the number of nodes in the input layer. This neural network
    is known as an *autoencoder* and effectively reconstructs the original features,
    learning a new representation using the hidden layers in between.^([9](ch01.html#idm140637564978592))
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Each hidden layer of the autoencoder learns a representation of the original
    features, and subsequent layers build on the representation learned by the preceding
    layers. Layer by layer, the autoencoder learns increasingly complicated representations
    from simpler ones.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: The output layer is the final newly learned representation of the original features.
    This learned representation can then be used as an input into a supervised learning
    model with the objective of improving the generalization error.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction using supervised training of feedforward networks
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we have labels, an alternate feature extraction approach is to use a feedforward,
    nonrecurrent neural network where the output layer attempts to predict the correct
    label. Just like with autoencoders, each hidden layer learns a representation
    of the original features.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: However, when generating the new representations, this network is explicitly
    *guided by the labels*. To extract the final newly learned representation of the
    original features in this network, we extract the penultimate layer—the hidden
    layer just before the output layer. This penultimate layer can then be used as
    an input into any supervised learning model.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised Deep Learning
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unsupervised learning performs many important functions in the field of deep
    learning, some of which we will explore in this book. This field is known as *unsupervised
    deep learning*.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Until very recently, the training of deep neural networks was computationally
    intractable. In these neural networks, the hidden layers learn internal representations
    to help solve the problem at hand. The representations improve over time based
    on how the neural network uses the *gradient of the error function* in each training
    iteration to update the weights of the various nodes.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: These updates are computationally expensive, and two major types of problems
    may occur in the process. First, the gradient of the error function may become
    very small, and, since *backpropagation* relies on multiplying these small weights
    together, the weights of the network may update very slowly or not at all, preventing
    proper training of the network.^([10](ch01.html#idm140637564962448)) This is known
    as the *vanishing gradient problem*.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, the other issue is that the gradient of the error function might
    become very large; with backprop, the weights throughout the network may update
    in huge increments, making the training of the network very unstable. This is
    known as the *exploding gradient problem*.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised pretraining
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To address these difficulties in training very deep, multilayered neural networks,
    machine learning researchers train neural networks in multiple, successive stages,
    where each stage involves a shallow neural network. The output of one shallow
    network is then used as the input of the next neural network. Typically, the first
    shallow neural network in this pipeline involves an unsupervised neural network,
    but the later networks are supervised.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: This unsupervised portion is known as *greedy layer-wise unsupervised pretraining*.
    In 2006, Geoffrey Hinton demonstrated the successful application of unsupervised
    pretraining to initialize the training of deeper neural network pipelines, kicking
    off the current deep learning revolution. Unsupervised pretaining allows the AI
    to capture an improved representation of the original input data, which the supervised
    portion then takes advantage of to solve the specific task at hand.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: This approach is called “greedy” because each portion of the neural network
    is trained independently, not jointly. “Layer-wise” refers to the layers of the
    network. In most modern neural networks, pretraining is usually not necessary.
    Instead, all the layers are trained jointly using backpropagation. Major computer
    advances have made the vanishing gradient problem and the exploding gradient problem
    much more manageable.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised pretraining not only makes supervised problems easier to solve
    but also facilitates *transfer learning*. Transfer learning involves using machine
    learning algorithms to store knowledge gained from solving one task to solve another
    related task much more quickly and with considerably less data.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Restricted Boltzmann machines
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One applied example of unsupervised pretraining is the *restricted Boltzmann
    machine (RBM)*, a shallow, two-layer neural network. The first layer is the input
    layer, and the second layer is the hidden layer. Each node is connected to every
    node in the other layer, but nodes are not connected to nodes of the same layer—this
    is where the restriction occurs.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: RBMs can perform unsupervised tasks such as dimensionality reduction and feature
    extraction and provide helpful unsupervised pretraining as part of supervised
    learning solutions. RBMs are similar to autoencoders but differ in some important
    ways. For example, autoencoders have an output layer, while RBMs do not. We will
    explore these and other differences in detail later in the book.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Deep belief networks
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RBMs can be linked together to form a multistage neural network pipeline known
    as a *deep belief network (DBN)*. The hidden layer of each RBM is used as the
    input for the next RBM. In other words, each RBM generates a representation of
    the data that the next RBM then builds upon. By successively linking this type
    of representation learning, the deep belief network is able to learn more complicated
    representations that are often used as *feature detectors*.^([11](ch01.html#idm140637564940720))
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Generative adversarial networks
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One major advance in unsupervised deep learning has been the advent of *generative
    adversarial networks (GANs)*, introduced by Ian Goodfellow and his fellow researchers
    at the University of Montreal in 2014\. GANs have many applications; for example,
    we can use GANs to create near-realistic synthetic data, such as images and speech,
    or perform anomaly detection.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: In GANs, we have two neural networks. One network—known as the generator—generates
    data based on a model data distribution it has created using samples of real data
    it has received. The other network—known as the discriminator—discriminates between
    the data created by the generator and data from the true data distribution.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: As a simple analogy, the generator is the counterfeiter, and the discriminator
    is the police trying to identify the forgery. The two networks are locked in a
    zero-sum game. The generator is trying to fool the discriminator into thinking
    the synthetic data comes from the true data distribution, and the discriminator
    is trying to call out the synthetic data as fake.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: GANs are unsupervised learning algorithms because the generator can learn the
    underlying structure of the true data distribution even when there are no labels.
    GANs learn the underlying structure in the data through the training process and
    efficiently capture the structure using a small, manageable number of parameters.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: This process is similar to the representation learning that occurs in deep learning.
    Each hidden layer in the neutral network of a generator captures a representation
    of the underlying data—starting very simply—and subsequent layers pick up more
    complicated representations by building on the simpler preceding layers.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Using all these layers together, the generator learns the underlying structure
    of the data and, using what it has learned, the generator attempts to create synthetic
    data that is nearly identical to the true data distribution. If the generator
    has captured the essence of the true data distribution, the synthetic data will
    appear real.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Sequential Data Problems Using Unsupervised Learning
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unsupervised learning can also handle sequential data such as time series data.
    One such approach involves learning the hidden states of a *Markov model*. In
    the *simple Markov model*, states are fully observed and change stochastically
    (in other words, randomly). Future states depend only on the current state and
    are not dependent on previous states.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: In a *hidden Markov model*, the states are only partially observable, but, like
    with simple Markov models, the outputs of these partially observable states are
    fully observable. Since the observations that we have are insufficient to determine
    the state completely, we need unsupervised learning to help discover these hidden
    states more fully.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Hidden Markov model algorithms involve learning the probable next state given
    what we know about the sequence of previously occurring, partially observable
    states and fully observable outputs. These algorithms have had major commercial
    applications in sequential data problems involving speech, text, and time series.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning Using Unsupervised Learning
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning is the third major branch of machine learning, in which
    an *agent* determines its optimal behavior (*actions*) in an *environment* based
    on feedback (*reward*) that it receives. This feedback is known as the *reinforcement
    signal*. The agent’s goal is to maximize its cumulative reward over time.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: While reinforcement learning has been around since the 1950s, it has made mainstream
    headline news only in recent years. In 2013, DeepMind—now owned by Google—applied
    reinforcement learning to achieve superhuman-level performance at playing many
    different Atari games. DeepMind’s system achieved this with just raw sensory data
    as input and no prior knowledge of the rules of the games.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: In 2016, DeepMind again captured the imagination of the machine learning community—this
    time the DeepMind reinforcement learning-based AI agent AlphaGo beat Lee Sedol,
    one of the world’s best Go players. These successes have cemented reinforcement
    learning as a mainstream AI topic.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'Today, machine learning researchers are applying reinforcement learning to
    solve many different types of problems including:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Stock market trading, in which the agent buys and sells (actions) and receives
    profits or losses (rewards) in return
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Video games and board games, in which the agent makes game decisions (actions)
    and wins or loses (rewards)
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-driving cars, in which the agent directs the vehicle (actions) and either
    stays on course or crashes (rewards)
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine control, in which the agent moves about its environment (actions) and
    either completes the course or fails (rewards)
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the simplest reinforcement learning problems, we have a finite problem—with
    a finite number of states of the environment, a finite number of actions that
    are possible at any given state of the environment, and a finite number of rewards.
    The action taken by the agent given the current state of the environment determines
    the next state, and the agent’s goal is to maximize its long-term reward. This
    family of problems is known as finite *Markov decision processes*.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: However, in the real world, things are not so simple—the reward is unknown and
    dynamic rather than known and static. To help discover this unknown reward function
    and approximate it as best as possible, we can apply unsupervised learning. Using
    this approximated reward function, we can apply reinforcement learning solutions
    to increase the cumulative reward over time.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Semisupervised Learning
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though supervised learning and unsupervised learning are two distinct major
    branches of machine learning, the algorithms from each branch can be mixed together
    as part of a machine learning pipeline.^([12](ch01.html#idm140637564904048)) Typically,
    this mix of supervised and unsupervised is used when we want to take full advantage
    of the few labels that we have or when we want to find new, yet unknown patterns
    from unlabeled data in addition to the known patterns from the labeled data. These
    types of problems are solved using a hybrid of supervised and unsupervised learning
    known as semisupervised learning. We will explore this area in greater detail
    later in the book.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Successful Applications of Unsupervised Learning
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the last ten years, most successful commercial applications of machine learning
    have come from the supervised learning space, but this is changing. Unsupervised
    learning applications have become more commonplace. Sometimes, unsupervised learning
    is just a means to make supervised applications better. Other times, unsupervised
    learning achieves the commercial application itself. Here is a closer look at
    two of the biggest applications of unsupervised learning to date: anomaly detection
    and group segmentation.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly Detection
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Performing dimensionality reduction can reduce the original high-dimensional
    feature space into a transformed lower-dimensional space. In this lower-dimensional
    space, we find where the majority of points densely lie. This portion is the *normal
    space*. Points that lie much farther away are called *outliers*—or *anomalies*—and
    are worth investigating in greater detail.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection systems are commonly used for fraud detection such as credit
    card fraud, wire fraud, cyber fraud, and insurance fraud. Anomaly detection is
    also used to identify rare, malicious events such as hacking of internet-connected
    devices, maintenance failures in mission-critical equipment such as airplanes
    and trains, and cybersecurity breaches due to malware and other pernicious agents.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: We can use these systems for spam detection, such as the email spam filter example
    we used earlier in the chapter. Other applications include finding bad actors
    to stop activity such as terrorist financing, money laundering, human and narcotics
    trafficking, and arms dealing, identifying high risk events in financial trading,
    and discovering diseases such as cancer.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: To make the analysis of anomalies more manageable, we can use a clustering algorithm
    to group similar anomalies together and then hand-label these clusters based on
    the types of behavior they represent. With such a system, we can have an unsupervised
    learning AI that is able to identify anomalies, cluster them into appropriate
    groups, and, using the cluster labels provided by humans, recommend to business
    analysts the appropriate course of action.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: With anomaly detection systems, we can take an unsupervised problem and eventually
    create a semisupervised one with this cluster-and-label approach. Over time, we
    can run supervised algorithms on the labeled data alongside the unsupervised algorithms.
    For successful machine learning applications, unsupervised systems and supervised
    systems should be used in conjunction, complementing one another.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: The supervised system finds the known patterns with a high level of accuracy,
    while the unsupervised system discovers new patterns that may be of interest.
    Once these patterns are uncovered by the unsupervised AI, the patterns are labeled
    by humans, transitioning more of the data from unlabeled to labeled.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Group segmentation
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With clustering, we can segment groups based on similarity in behavior in areas
    such as marketing, customer retention, disease diagnosis, online shopping, music
    listening, video watching, online dating, social media activity, and document
    classification. The amount of data that is generated in each of these areas is
    massive, and the data is only partially labeled.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: For patterns that we already know and want to reinforce, we can use supervised
    learning algorithms. But often we want to discover new patterns and groups of
    interest—for this discovery process, unsupervised learning is a natural fit. Again,
    it is all about synergy. We should use supervised and unsupervised learning systems
    in conjunction to build a stronger machine learning solution.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we explored the following:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: The difference between a rules-based system and machine learning
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The difference between supervised and unsupervised learning
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How unsupervised learning can help address common problems in training machine
    learning models
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common algorithms for supervised, unsupervised, reinforcement, and semisupervised
    learning
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two major applications of unsupervised learning—anomaly detection and group
    segmentation
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In [Chapter 2](ch02.html#Chapter_2), we’ll explore how to build machine learning
    applications. Then, we will cover dimensionality reduction and clustering in detail,
    building an anomaly detection system and a group segmentation system in the process.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch01.html#idm140637564299232-marker)) There are startups such as Figure
    Eight that explicitly provide this *human in the loop* service.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch01.html#idm140637564293600-marker)) Underfitting is another problem
    that may occur in building machine learning applications, but this is easier to
    solve. Underfitting occurs because the model is too simple—the algorithm cannot
    build a complex enough function approximation to make good decisions for the task
    at hand. To solve this, we can allow the algorithm to grow in size (have more
    parameters, perform more training iterations, etc.) or apply a more complicated
    machine learning algorithm.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch01.html#idm140637564244608-marker)) This list is by no means exhaustive
    but does include the most commonly used machine learning algorithms.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch01.html#idm140637564235360-marker)) There may be other potential issues
    that might make linear regression a poor choice, including outliers, correlation
    of error terms, and nonconstant variance of error terms.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch01.html#idm140637564424384-marker)) For more on gradient boosting in
    machine learning competitions, consult Ben Gorman’s [blog post](http://bit.ly/2S1C8Qy).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch01.html#idm140637564409376-marker)) For more on neutral networks, check
    out [*Deep Learning*](http://www.deeplearningbook.org/) by Ian Goodfellow, Yoshua
    Bengio, and Aaron Courville (MIT Press).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch01.html#idm140637564325008-marker)) There are faster variants of *k*-means
    clustering such as mini-batch *k*-means, which we cover later in the book.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch01.html#idm140637564993824-marker)) Hierarchical clustering uses Euclidean
    distance by default, but it can also use other similarity metrics such as correlation-based
    distance, which we will explore in greater detail later in the book.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch01.html#idm140637564978592-marker)) There are several types of autoencoders,
    and each learns a different set of representations. These include denoising autoencoders,
    sparse autoencoders, and variational autoencoders, all of which we will explore
    later in the book.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch01.html#idm140637564962448-marker)) Backpropagation (also known as
    *backward propagation of errors*) is a gradient descent-based algorithm used by
    neural networks to update weights. In backprop, the weights of the final layer
    are calculated first and then used to update the weights of the preceding layers.
    This process continues until the weights of the very first layer are updated.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch01.html#idm140637564940720-marker)) Feature detectors learn good representations
    of the original data, helping separate distinct elements. For example, in images,
    feature detectors help separate elements such as noses, eyes, mouths, etc.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch01.html#idm140637564904048-marker)) Pipeline refers to a system of
    machine learning solutions that are applied in succession to achieve a larger
    objective.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
