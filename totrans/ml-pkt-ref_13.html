<html><head></head><body><section data-pdf-bookmark="Chapter 13. Explaining Models" data-type="chapter" epub:type="chapter"><div class="chapter" id="idm46066892453976">&#13;
<h1><span class="label">Chapter 13. </span>Explaining Models</h1>&#13;
&#13;
&#13;
<p><a data-primary="model explanation/interpretation" data-type="indexterm" id="ix_ch13-asciidoc0"/>Predictive models have different properties. Some are designed to handle linear data. Others can mold to more complex input. Some models can be interpreted very easily, others are like black boxes and don’t offer much insight into how the prediction is made.</p>&#13;
&#13;
<p>In this chapter we will look at interpreting different&#13;
models. We will look at some examples using the Titanic&#13;
data.</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">dt</code> <code class="o">=</code> <code class="n">DecisionTreeClassifier</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">,</code> <code class="n">max_depth</code><code class="o">=</code><code class="mi">3</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">dt</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code></pre>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Regression Coefficients" data-type="sect1"><div class="sect1" id="idm46066890684472">&#13;
<h1>Regression Coefficients</h1>&#13;
&#13;
<p><a data-primary="model explanation/interpretation" data-secondary="regression coefficients" data-type="indexterm" id="idm46066890667448"/><a data-primary="regression coefficients" data-type="indexterm" id="idm46066890666504"/>The intercepts and regression coefficients explain the&#13;
expected value, and how features impact the prediction.&#13;
A positive coefficient indicates that as a feature’s&#13;
value increases, the prediction increases as well.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Feature Importance" data-type="sect1"><div class="sect1" id="idm46066890664200">&#13;
<h1>Feature Importance</h1>&#13;
&#13;
<p>Tree-based models in the scikit-learn library<a data-primary="feature importance" data-secondary="model interpretation" data-type="indexterm" id="idm46066890662840"/><a data-primary="model explanation/interpretation" data-secondary="feature importance" data-type="indexterm" id="idm46066890661864"/><a data-primary="scikit-learn" data-secondary="feature_importances_ attribute" data-type="indexterm" id="idm46066890660952"/> include a <code>.fea⁠ture_</code><span class="keep-together"><code>importances_</code></span>&#13;
attribute for inspecting how the features of a dataset affect the model. We can inspect or plot them.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="LIME" data-type="sect1"><div class="sect1" id="idm46066890658408">&#13;
<h1>LIME</h1>&#13;
&#13;
<p><a data-primary="black box models" data-secondary="LIME and" data-type="indexterm" id="ix_ch13-asciidoc1"/><a data-primary="Local Interpretable Model-Agnostic Explanations (LIME)" data-type="indexterm" id="ix_ch13-asciidoc2"/><a data-primary="model explanation/interpretation" data-secondary="LIME" data-type="indexterm" id="ix_ch13-asciidoc3"/><a href="https://oreil.ly/shCR_">LIME</a> works to help&#13;
explain black-box models. It performs a <em>local</em> interpretation rather than an overall&#13;
interpretation. It will help explain a single sample.</p>&#13;
&#13;
<p>For a given data point or sample, LIME indicates which&#13;
features were important in determining the result. It does this by&#13;
perturbing the sample in question and fitting a linear model to it.&#13;
The linear model approximates the model close to the sample (see <a data-type="xref" href="#id38">Figure 13-1</a>).</p>&#13;
&#13;
<p>Here is an example explaining the last sample (which our decision tree predicts will survive) from the training data:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">lime</code> <code class="kn">import</code> <code class="n">lime_tabular</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">explainer</code> <code class="o">=</code> <code class="n">lime_tabular</code><code class="o">.</code><code class="n">LimeTabularExplainer</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">X_train</code><code class="o">.</code><code class="n">values</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">feature_names</code><code class="o">=</code><code class="n">X</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">class_names</code><code class="o">=</code><code class="p">[</code><code class="s">"died"</code><code class="p">,</code> <code class="s">"survived"</code><code class="p">],</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">exp</code> <code class="o">=</code> <code class="n">explainer</code><code class="o">.</code><code class="n">explain_instance</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">X_train</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">values</code><code class="p">,</code> <code class="n">dt</code><code class="o">.</code><code class="n">predict_proba</code>&#13;
<code class="gp">... </code><code class="p">)</code></pre>&#13;
&#13;
<p>LIME doesn’t like using DataFrames as input. Note that we converted the data&#13;
to numpy arrays using <code>.values</code>.</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>If you are doing this in Jupyter, follow up with this code:</p>&#13;
&#13;
<pre data-type="programlisting">exp.show_in_notebook()</pre>&#13;
&#13;
<p>This will render an HTML version of the explanation.</p>&#13;
</div>&#13;
&#13;
<p>We can create a matplotlib figure if we want to export the explanation (or aren’t using Jupyter):</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code> <code class="o">=</code> <code class="n">exp</code><code class="o">.</code><code class="n">as_pyplot_figure</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">tight_layout</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1301.png"</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id38">&#13;
<img alt="LIME explanation for the Titanic dataset. Features for the sample push the prediction toward the right (survival) or left (deceased)." src="assets/mlpr_1301.png"/>&#13;
<h6><span class="label">Figure 13-1. </span>LIME explanation for the Titanic dataset. Features for the sample push the prediction toward the right (survival) or left (deceased).</h6>&#13;
</div></figure>&#13;
&#13;
<p>Play around with this and notice that if you switch genders, the results are affected. Below we take the second to last&#13;
row in the training data. The prediction for that row is 48% deceased and 52% survived. If we switch the gender, we find that the prediction shifts toward 88% deceased:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">data</code> <code class="o">=</code> <code class="n">X_train</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="o">-</code><code class="mi">2</code><code class="p">]</code><code class="o">.</code><code class="n">values</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">dt</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="p">[</code><code class="n">data</code><code class="p">]</code>&#13;
<code class="gp">... </code><code class="p">)</code>  <code class="c"># predicting that a woman lives</code>&#13;
<code class="go">[[0.48062016 0.51937984]]</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">data</code><code class="p">[</code><code class="mi">5</code><code class="p">]</code> <code class="o">=</code> <code class="mi">1</code>  <code class="c"># change to male</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">dt</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">([</code><code class="n">data</code><code class="p">])</code>&#13;
<code class="go">array([[0.87954545, 0.12045455]])</code></pre>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>The <code>.predict_proba</code> method returns a probability for each label.<a data-startref="ix_ch13-asciidoc3" data-type="indexterm" id="idm46066890390024"/><a data-startref="ix_ch13-asciidoc2" data-type="indexterm" id="idm46066890389352"/><a data-startref="ix_ch13-asciidoc1" data-type="indexterm" id="idm46066890388680"/></p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Tree Interpretation" data-type="sect1"><div class="sect1" id="idm46066890657816">&#13;
<h1>Tree Interpretation</h1>&#13;
&#13;
<p><a data-primary="bias" data-type="indexterm" id="idm46066890386472"/><a data-primary="binary classifiers" data-secondary="tree interpretation" data-type="indexterm" id="idm46066890385768"/><a data-primary="decision tree" data-secondary="tree interpretation" data-type="indexterm" id="idm46066890384824"/><a data-primary="model explanation/interpretation" data-secondary="tree interpretation" data-type="indexterm" id="idm46066890383880"/><a data-primary="random forest" data-secondary="tree interpretation" data-type="indexterm" id="idm46066890382920"/><a data-primary="sklearn" data-secondary="tree interpretation" data-type="indexterm" id="idm46066890381976"/><a data-primary="tree interpretation" data-type="indexterm" id="idm46066890381032"/>For sklearn tree-based models (decision tree, random forest, and extra tree models) you can use the <a href="https://oreil.ly/vN1Bl">treeinterpreter package</a>. This&#13;
will calculate the bias and the contribution from each feature.&#13;
The bias is the mean of the training set.</p>&#13;
&#13;
<p>Each&#13;
contribution lists how it contributes to each of the labels. (The&#13;
bias plus the contributions should sum to the prediction.) Since this&#13;
is a binary classification, there are only two. We see that sex_male is&#13;
the most important, followed by age and fare:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">treeinterpreter</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">treeinterpreter</code> <code class="k">as</code> <code class="n">ti</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">instances</code> <code class="o">=</code> <code class="n">X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[:</code><code class="mi">2</code><code class="p">]</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">prediction</code><code class="p">,</code> <code class="n">bias</code><code class="p">,</code> <code class="n">contribs</code> <code class="o">=</code> <code class="n">ti</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">rf5</code><code class="p">,</code> <code class="n">instances</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">i</code> <code class="o">=</code> <code class="mi">0</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="k">print</code><code class="p">(</code><code class="s">"Instance"</code><code class="p">,</code> <code class="n">i</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="k">print</code><code class="p">(</code><code class="s">"Prediction"</code><code class="p">,</code> <code class="n">prediction</code><code class="p">[</code><code class="n">i</code><code class="p">])</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="k">print</code><code class="p">(</code><code class="s">"Bias (trainset mean)"</code><code class="p">,</code> <code class="n">bias</code><code class="p">[</code><code class="n">i</code><code class="p">])</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="k">print</code><code class="p">(</code><code class="s">"Feature contributions:"</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">c</code><code class="p">,</code> <code class="n">feature</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">contribs</code><code class="p">[</code><code class="n">i</code><code class="p">],</code> <code class="n">instances</code><code class="o">.</code><code class="n">columns</code>&#13;
<code class="gp">... </code><code class="p">):</code>&#13;
<code class="gp">... </code>    <code class="k">print</code><code class="p">(</code><code class="s">"  {} {}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">feature</code><code class="p">,</code> <code class="n">c</code><code class="p">))</code>&#13;
<code class="go">Instance 0</code>&#13;
<code class="go">Prediction [0.98571429 0.01428571]</code>&#13;
<code class="go">Bias (trainset mean) [0.63984716 0.36015284]</code>&#13;
<code class="go">Feature contributions:</code>&#13;
<code class="go">  pclass [ 0.03588478 -0.03588478]</code>&#13;
<code class="go">  age [ 0.08569306 -0.08569306]</code>&#13;
<code class="go">  sibsp [ 0.01024538 -0.01024538]</code>&#13;
<code class="go">  parch [ 0.0100742 -0.0100742]</code>&#13;
<code class="go">  fare [ 0.06850243 -0.06850243]</code>&#13;
<code class="go">  sex_male [ 0.12000073 -0.12000073]</code>&#13;
<code class="go">  embarked_Q [ 0.0026364 -0.0026364]</code>&#13;
<code class="go">  embarked_S [ 0.01283015 -0.01283015]</code></pre>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>This example is for classification, but there is support for regression&#13;
as well.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Partial Dependence Plots" data-type="sect1"><div class="sect1" id="idm46066890376216">&#13;
<h1>Partial Dependence Plots</h1>&#13;
&#13;
<p><a data-primary="feature importance" data-secondary="partial dependence plots" data-type="indexterm" id="ix_ch13-asciidoc4"/><a data-primary="model explanation/interpretation" data-secondary="partial dependence plots" data-type="indexterm" id="ix_ch13-asciidoc5"/><a data-primary="partial dependence plots" data-type="indexterm" id="ix_ch13-asciidoc6"/>With feature importance in trees we know that a feature is impacting the&#13;
outcome, but we don’t know how the impact varies as the feature’s value changes. Partial dependence plots allow us to visualize the relation between changes in just one feature and the outcome. We will use <a href="https://oreil.ly/O9zY2">pdpbox</a> to visualize how age affects survival (see <a data-type="xref" href="#id39">Figure 13-2</a>).</p>&#13;
&#13;
<p class="pagebreak-before">This example uses a random forest model:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">rf5</code> <code class="o">=</code> <code class="n">ensemble</code><code class="o">.</code><code class="n">RandomForestClassifier</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="o">**</code><code class="p">{</code>&#13;
<code class="gp">... </code>        <code class="s">"max_features"</code><code class="p">:</code> <code class="s">"auto"</code><code class="p">,</code>&#13;
<code class="gp">... </code>        <code class="s">"min_samples_leaf"</code><code class="p">:</code> <code class="mf">0.1</code><code class="p">,</code>&#13;
<code class="gp">... </code>        <code class="s">"n_estimators"</code><code class="p">:</code> <code class="mi">200</code><code class="p">,</code>&#13;
<code class="gp">... </code>        <code class="s">"random_state"</code><code class="p">:</code> <code class="mi">42</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="p">}</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rf5</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code></pre>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">pdpbox</code> <code class="kn">import</code> <code class="n">pdp</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">feat_name</code> <code class="o">=</code> <code class="s">"age"</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">p</code> <code class="o">=</code> <code class="n">pdp</code><code class="o">.</code><code class="n">pdp_isolate</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">rf5</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">X</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code> <code class="n">feat_name</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="n">pdp</code><code class="o">.</code><code class="n">pdp_plot</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">p</code><code class="p">,</code> <code class="n">feat_name</code><code class="p">,</code> <code class="n">plot_lines</code><code class="o">=</code><code class="bp">True</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1302.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
<div class="landscape">&#13;
&#13;
<figure><div class="figure" id="id39">&#13;
<img alt="Partial dependence plot showing what happens to the target as age changes." src="assets/mlpr_1302.png"/>&#13;
<h6><span class="label">Figure 13-2. </span>Partial dependence plot showing what happens to the target as age changes.</h6>&#13;
</div></figure>&#13;
</div>&#13;
&#13;
<p>We can also visualize the interactions between two features (see <a data-type="xref" href="#id40">Figure 13-3</a>):</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">features</code> <code class="o">=</code> <code class="p">[</code><code class="s">"fare"</code><code class="p">,</code> <code class="s">"sex_male"</code><code class="p">]</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">p</code> <code class="o">=</code> <code class="n">pdp</code><code class="o">.</code><code class="n">pdp_interact</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">rf5</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">X</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code> <code class="n">features</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="n">pdp</code><code class="o">.</code><code class="n">pdp_interact_plot</code><code class="p">(</code><code class="n">p</code><code class="p">,</code> <code class="n">features</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1303.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id40">&#13;
<img alt="Partial dependence plot with two features. As fare goes up and sex goes from male to female, survival goes up." src="assets/mlpr_1303.png"/>&#13;
<h6><span class="label">Figure 13-3. </span>Partial dependence plot with two features. As fare goes up and sex goes from male to female, survival goes up.</h6>&#13;
</div></figure>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>The partial dependence plot pins down a feature value across the&#13;
samples and then averages the result. (Be careful about outliers and&#13;
means.) Also, this plot assumes features are independent. (Not always the&#13;
case; for example, holding width of a sepal steady would probably have an effect on&#13;
the height.) The pdpbox library also prints out the individual&#13;
conditional expectations to better visualize these relationships.<a data-startref="ix_ch13-asciidoc6" data-type="indexterm" id="idm46066889961656"/><a data-startref="ix_ch13-asciidoc5" data-type="indexterm" id="idm46066889960952"/><a data-startref="ix_ch13-asciidoc4" data-type="indexterm" id="idm46066889960280"/></p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Surrogate Models" data-type="sect1"><div class="sect1" id="idm46066890375624">&#13;
<h1>Surrogate Models</h1>&#13;
&#13;
<p><a data-primary="decision tree" data-secondary="surrogate models and" data-type="indexterm" id="idm46066889958312"/><a data-primary="model explanation/interpretation" data-secondary="surrogate models" data-type="indexterm" id="idm46066889957336"/><a data-primary="surrogate models" data-type="indexterm" id="idm46066889956424"/>If you have a model that is not interpretable (SVM or neural network), you&#13;
can fit an interpretable model (decision tree) to that model. Using the&#13;
surrogate you can examine the feature importances.</p>&#13;
&#13;
<p>Here we create a Support Vector Classifier (SVC), but train a decision tree (without a depth limit to overfit and capture what is happening in this model) to explain it:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">svm</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">sv</code> <code class="o">=</code> <code class="n">svm</code><code class="o">.</code><code class="n">SVC</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">sv</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">sur_dt</code> <code class="o">=</code> <code class="n">tree</code><code class="o">.</code><code class="n">DecisionTreeClassifier</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">sur_dt</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">sv</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">col</code><code class="p">,</code> <code class="n">val</code> <code class="ow">in</code> <code class="nb">sorted</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="nb">zip</code><code class="p">(</code>&#13;
<code class="gp">... </code>        <code class="n">X_test</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code>&#13;
<code class="gp">... </code>        <code class="n">sur_dt</code><code class="o">.</code><code class="n">feature_importances_</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="p">),</code>&#13;
<code class="gp">... </code>    <code class="n">key</code><code class="o">=</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code>&#13;
<code class="gp">... </code>    <code class="n">reverse</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)[:</code><code class="mi">7</code><code class="p">]:</code>&#13;
<code class="gp">... </code>    <code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s">"{col:10}{val:10.3f}"</code><code class="p">)</code>&#13;
<code class="go">sex_male       0.723</code>&#13;
<code class="go">pclass         0.076</code>&#13;
<code class="go">sibsp          0.061</code>&#13;
<code class="go">age            0.056</code>&#13;
<code class="go">embarked_S     0.050</code>&#13;
<code class="go">fare           0.028</code>&#13;
<code class="go">parch          0.005</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Shapley" data-type="sect1"><div class="sect1" id="idm46066889953288">&#13;
<h1>Shapley</h1>&#13;
&#13;
<p><a data-primary="model explanation/interpretation" data-secondary="Shapley" data-type="indexterm" id="ix_ch13-asciidoc7"/><a data-primary="SHapley Additive exPlanations (SHAP)" data-type="indexterm" id="ix_ch13-asciidoc8"/>The SHapley Additive exPlanations, (<a href="https://oreil.ly/QYj-q">SHAP</a>) package can visualize feature contributions of any model. This is a really nice package because not only does it work with most models, it also can explain individual predictions and the global feature contributions.</p>&#13;
&#13;
<p><a data-primary="classification" data-secondary="SHAP and" data-type="indexterm" id="idm46066889796456"/><a data-primary="regression" data-secondary="SHAP and" data-type="indexterm" id="idm46066889795480"/>SHAP works for both classification and regression. It generates “SHAP” values. For classification models, the SHAP value sums to log odds for binary classification. For regression, the SHAP values sum to the target prediction.</p>&#13;
&#13;
<p>This library&#13;
requires Jupyter (JavaScript) for interactivity on some of its plots. (Some can render static images with matplotlib.) Here is an example for sample 20, predicted to die:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">rf5</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X_test</code><code class="o">.</code><code class="n">iloc</code><code class="p">[[</code><code class="mi">20</code><code class="p">]])</code>&#13;
<code class="go">array([[0.59223553, 0.40776447]])</code></pre>&#13;
&#13;
<p>In the force plot for sample 20, you can see the “base value.” This is a female who is predicted to die (see <a data-type="xref" href="#id41">Figure 13-4</a>). We will use the survival index (1) because we want the right-hand side of the plot to be survival. The features push this to the right or left. The larger the feature, the more impact it has. In this case, the low fare and third class push toward death (the output value is below .5):</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">shap</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">s</code> <code class="o">=</code> <code class="n">shap</code><code class="o">.</code><code class="n">TreeExplainer</code><code class="p">(</code><code class="n">rf5</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">shap_vals</code> <code class="o">=</code> <code class="n">s</code><code class="o">.</code><code class="n">shap_values</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">target_idx</code> <code class="o">=</code> <code class="mi">1</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">shap</code><code class="o">.</code><code class="n">force_plot</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">s</code><code class="o">.</code><code class="n">expected_value</code><code class="p">[</code><code class="n">target_idx</code><code class="p">],</code>&#13;
<code class="gp">... </code>    <code class="n">shap_vals</code><code class="p">[</code><code class="n">target_idx</code><code class="p">][</code><code class="mi">20</code><code class="p">,</code> <code class="p">:],</code>&#13;
<code class="gp">... </code>    <code class="n">feature_names</code><code class="o">=</code><code class="n">X_test</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id41">&#13;
<img alt="Shapley feature contributions for sample 20. This plot shows the base value and the features that push toward death." src="assets/mlpr_1304.png"/>&#13;
<h6><span class="label">Figure 13-4. </span>Shapley feature contributions for sample 20. This plot shows the base value and the features that push toward death.</h6>&#13;
</div></figure>&#13;
&#13;
<p>You can also visualize the explanations for the entire dataset (rotating&#13;
them by 90 and plotting them along the x axis) (see <a data-type="xref" href="#id13_5">Figure 13-5</a>):</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">shap</code><code class="o">.</code><code class="n">force_plot</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">s</code><code class="o">.</code><code class="n">expected_value</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code>&#13;
<code class="gp">... </code>    <code class="n">shap_vals</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code>&#13;
<code class="gp">... </code>    <code class="n">feature_names</code><code class="o">=</code><code class="n">X_test</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id13_5">&#13;
<img alt="Shapley feature contributions for dataset." src="assets/mlpr_1305.png"/>&#13;
<h6><span class="label">Figure 13-5. </span>Shapley feature contributions for dataset.</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-primary="dependence plots" data-type="indexterm" id="idm46066889678184"/>The SHAP library can also generate dependence plots. The following plot (see <a data-type="xref" href="#id43">Figure 13-6</a>)&#13;
visualizes the relationship between age and SHAP value (it is colored by&#13;
pclass, which SHAP chooses automatically; specify a column name as an <code>interaction_index</code> parameter to choose your own):</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">res</code> <code class="o">=</code> <code class="n">shap</code><code class="o">.</code><code class="n">dependence_plot</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="s">"age"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">shap_vals</code><code class="p">[</code><code class="n">target_idx</code><code class="p">],</code>&#13;
<code class="gp">... </code>    <code class="n">X_test</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">feature_names</code><code class="o">=</code><code class="n">X_test</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">alpha</code><code class="o">=</code><code class="mf">0.7</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="s">"images/mlpr_1306.png"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">bbox_inches</code><code class="o">=</code><code class="s">"tight"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id43">&#13;
<img alt="Shapley dependency plot for age. Young and old have a higher rate of survival. As age goes up, a lower pclass has more chance of survival." src="assets/mlpr_1306.png"/>&#13;
<h6><span class="label">Figure 13-6. </span>Shapley dependency plot for age. Young and old have a higher rate of survival. As age goes up, a lower pclass has more chance of survival.</h6>&#13;
</div></figure>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>You might get a dependence plot that has vertical lines. Setting the <code>x_jitter</code> parameter to 1 is useful if you are viewing ordinal categorical features.</p>&#13;
</div>&#13;
&#13;
<p>In addition, we can summarize all of the features. This is a very powerful chart to understand. It shows global impact, but also individual impacts. The features are ranked by importance. The most important features are at the top.</p>&#13;
&#13;
<p>Also the features are colored according to their value. We can see that a low sex_male score (female) has a strong push toward survival, while a high score has a less strong push toward death. The age feature is a little harder to interpret. That is because young and old values push toward survival, while middle values push toward death.</p>&#13;
&#13;
<p class="pagebreak-before">When you combine the summary plot with the dependence plot, you can get good&#13;
insight into model behavior (see <a data-type="xref" href="#id44">Figure 13-7</a>)<a data-startref="ix_ch13-asciidoc8" data-type="indexterm" id="idm46066889499960"/><a data-startref="ix_ch13-asciidoc7" data-type="indexterm" id="idm46066889499256"/>:<a data-startref="ix_ch13-asciidoc0" data-type="indexterm" id="idm46066889498456"/></p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">shap</code><code class="o">.</code><code class="n">summary_plot</code><code class="p">(</code><code class="n">shap_vals</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">X_test</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1307.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id44">&#13;
<img alt="Shapley summary plot showing most important features at the top. The coloring shows how the values of the feature affect the target." src="assets/mlpr_1307.png"/>&#13;
<h6><span class="label">Figure 13-7. </span>Shapley summary plot showing most important features at the top. The coloring shows how the values of the feature affect the <span class="keep-together">target.</span></h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>