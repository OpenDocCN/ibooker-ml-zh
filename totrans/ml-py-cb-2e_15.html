<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 15. K-Nearest Neighbors" data-type="chapter" epub:type="chapter"><div class="chapter" id="k-nearest-neighbors">
<h1><span class="label">Chapter 15. </span>K-Nearest Neighbors</h1>
<section data-pdf-bookmark="15.0 Introduction" data-type="sect1"><div class="sect1" id="id335">
<h1>15.0 Introduction</h1>
<p>The k-nearest <a data-primary="k-nearest neighbors (KNN)" data-type="indexterm" id="ix_knearest_neigh_ch15"/><a data-primary="classification and classifiers" data-secondary="KNN classifier" data-type="indexterm" id="ix_classif_knn_class"/>neighbors (KNN) classifier is one of the simplest yet most commonly used classifiers in supervised machine learning. KNN is often considered a lazy learner; it doesn’t technically train a model to make predictions. Instead an observation is predicted to be the same class as that of the largest proportion of the <em>k</em> nearest observations.</p>
<p>For example, if an observation with an unknown class is surrounded by an observation of class 1, then the observation is classified as class 1. In this chapter we will explore how to use scikit-learn to create and use a KNN classifier.</p>
</div></section>
<section data-pdf-bookmark="15.1 Finding an Observation’s Nearest Neighbors" data-type="sect1"><div class="sect1" id="finding-an-observations-nearest-neighbors">
<h1>15.1 Finding an Observation’s Nearest Neighbors</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id534">
<h2>Problem</h2>
<p>You need to find an <a data-primary="observations" data-secondary="finding nearest neighbors" data-type="indexterm" id="ix_obs_find_near"/><a data-primary="k-nearest neighbors (KNN)" data-secondary="finding observation’s nearest neighbors" data-type="indexterm" id="ix_knearest_neigh_find"/>observation’s <em>k</em> nearest observations (neighbors).</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id336">
<h2>Solution</h2>
<p>Use <a data-primary="NearestNeighbors" data-type="indexterm" id="ix_near_neigh"/>scikit-learn’s <code>NearestNeighbors</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.neighbors</code> <code class="kn">import</code> <code class="n">NearestNeighbors</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>

<code class="c1"># Create standardizer</code>
<code class="n">standardizer</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>

<code class="c1"># Standardize features</code>
<code class="n">features_standardized</code> <code class="o">=</code> <code class="n">standardizer</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Two nearest neighbors</code>
<code class="n">nearest_neighbors</code> <code class="o">=</code> <code class="n">NearestNeighbors</code><code class="p">(</code><code class="n">n_neighbors</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_standardized</code><code class="p">)</code>

<code class="c1"># Create an observation</code>
<code class="n">new_observation</code> <code class="o">=</code> <code class="p">[</code> <code class="mi">1</code><code class="p">,</code>  <code class="mi">1</code><code class="p">,</code>  <code class="mi">1</code><code class="p">,</code>  <code class="mi">1</code><code class="p">]</code>

<code class="c1"># Find distances and indices of the observation's nearest neighbors</code>
<code class="n">distances</code><code class="p">,</code> <code class="n">indices</code> <code class="o">=</code> <code class="n">nearest_neighbors</code><code class="o">.</code><code class="n">kneighbors</code><code class="p">([</code><code class="n">new_observation</code><code class="p">])</code>

<code class="c1"># View the nearest neighbors</code>
<code class="n">features_standardized</code><code class="p">[</code><code class="n">indices</code><code class="p">]</code></pre>
<pre data-type="programlisting">array([[[1.03800476, 0.55861082, 1.10378283, 1.18556721],
        [0.79566902, 0.32841405, 0.76275827, 1.05393502]]])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id337">
<h2>Discussion</h2>
<p>In our solution we used the dataset of iris flowers. We created an
observation, <code>new_observation</code>, with some values and then found the two
observations that are closest to our observation. <code>indices</code> contains the
locations of the observations in our dataset that are closest, so
<code>X[indices]</code> displays the values of those observations. Intuitively,
distance can be thought of as a measure of similarity, so the two
closest observations are the two flowers most similar to the flower we
created.</p>
<p>How do we <a data-primary="distance metrics" data-type="indexterm" id="ix_distance_metric"/>measure distance? scikit-learn offers a wide variety of
distance metrics, <math display="inline"><mi>d</mi></math>, including Euclidean:</p>
<div data-type="equation">
<math display="block">
<mrow>
<msub><mi>d</mi> <mrow><mi>e</mi><mi>u</mi><mi>c</mi><mi>l</mi><mi>i</mi><mi>d</mi><mi>e</mi><mi>a</mi><mi>n</mi></mrow> </msub>
<mo>=</mo>
<msqrt>
<mrow>
<msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </msubsup>
<msup><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi> </msub><mo>-</mo><msub><mi>y</mi> <mi>i</mi> </msub><mo>)</mo></mrow> <mn>2</mn> </msup>
</mrow>
</msqrt>
</mrow>
</math>
</div>
<p>and Manhattan distance:</p>
<div data-type="equation">
<math display="block">
<mrow>
<msub><mi>d</mi> <mrow><mi>m</mi><mi>a</mi><mi>n</mi><mi>h</mi><mi>a</mi><mi>t</mi><mi>t</mi><mi>a</mi><mi>n</mi></mrow> </msub>
<mo>=</mo>
<munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </munderover>
<mfenced close="|" open="|" separators="">
<msub><mi>x</mi> <mi>i</mi> </msub>
<mo>-</mo>
<msub><mi>y</mi> <mi>i</mi> </msub>
</mfenced>
</mrow>
</math>
</div>
<p>By default, <code>NearestNeighbors</code> uses Minkowski distance:</p>
<div data-type="equation">
<math display="block">
<mrow>
<msub><mi>d</mi> <mrow><mi>m</mi><mi>i</mi><mi>n</mi><mi>k</mi><mi>o</mi><mi>w</mi><mi>s</mi><mi>k</mi><mi>i</mi></mrow> </msub>
<mo>=</mo>
<msup><mfenced close=")" open="(" separators=""><munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </munderover><msup><mfenced close="|" open="|" separators=""><msub><mi>x</mi> <mi>i</mi> </msub><mo>-</mo><msub><mi>y</mi> <mi>i</mi> </msub></mfenced> <mi>p</mi> </msup></mfenced> <mrow><mn>1</mn><mo>/</mo><mi>p</mi></mrow> </msup>
</mrow>
</math>
</div>
<p>where <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math> and <math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math> are the two observations we are calculating the distance between. Minkowski includes a hyperparameter, <math display="inline"><mi>p</mi></math>, where <math display="inline"><mi>p</mi></math> = 1 is Manhattan distance and 
<span class="keep-together"><math display="inline"><mi>p</mi></math> = 2</span> is Euclidean distance, and so on. By default in scikit-learn <math display="inline"><mi>p</mi></math> = 2.</p>
<p>We can set the distance metric using the <code>metric</code> parameter:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Find two nearest neighbors based on Euclidean distance</code>
<code class="n">nearestneighbors_euclidean</code> <code class="o">=</code> <code class="n">NearestNeighbors</code><code class="p">(</code>
    <code class="n">n_neighbors</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">metric</code><code class="o">=</code><code class="s1">'euclidean'</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_standardized</code><code class="p">)</code></pre>
<p>The <code>distance</code> variable we created contains the actual distance
measurement to each of the two nearest neighbors:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View distances</code>
<code class="n">distances</code></pre>
<pre data-type="programlisting">array([[0.49140089, 0.74294782]])</pre>
<p>In <a data-primary="matrices" data-secondary="k-nearest neighbors" data-type="indexterm" id="id1769"/>addition, we can use <code>kneighbors_graph</code> to create a matrix indicating
each observation’s nearest neighbors:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Find each observation's three nearest neighbors</code>
<code class="c1"># based on Euclidean distance (including itself)</code>
<code class="n">nearestneighbors_euclidean</code> <code class="o">=</code> <code class="n">NearestNeighbors</code><code class="p">(</code>
    <code class="n">n_neighbors</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">metric</code><code class="o">=</code><code class="s2">"euclidean"</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_standardized</code><code class="p">)</code>

<code class="c1"># List of lists indicating each observation's three nearest neighbors</code>
<code class="c1"># (including itself)</code>
<code class="n">nearest_neighbors_with_self</code> <code class="o">=</code> <code class="n">nearestneighbors_euclidean</code><code class="o">.</code><code class="n">kneighbors_graph</code><code class="p">(</code>
    <code class="n">features_standardized</code><code class="p">)</code><code class="o">.</code><code class="n">toarray</code><code class="p">()</code>

<code class="c1"># Remove 1s marking an observation is a nearest neighbor to itself</code>
<code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">x</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">nearest_neighbors_with_self</code><code class="p">):</code>
    <code class="n">x</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="o">=</code> <code class="mi">0</code>

<code class="c1"># View first observation's two nearest neighbors</code>
<code class="n">nearest_neighbors_with_self</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code></pre>
<pre data-type="programlisting">array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])</pre>
<p>When we are finding nearest neighbors or using any learning algorithm based on distance, it is important to transform features so that they are on the same scale. This is because the distance metrics treat all features as if they were on the same scale, but if one feature is in millions of dollars and a second feature is in percentages, the distance
calculated will be biased toward the former. In our solution we addressed this potential issue by standardizing the features using
<code>StandardScaler</code>.<a data-primary="" data-startref="ix_near_neigh" data-type="indexterm" id="id1770"/><a data-primary="" data-startref="ix_knearest_neigh_find" data-type="indexterm" id="id1771"/><a data-primary="" data-startref="ix_obs_find_near" data-type="indexterm" id="id1772"/><a data-primary="" data-startref="ix_distance_metric" data-type="indexterm" id="id1773"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="15.2 Creating a K-Nearest Neighbors Classifier" data-type="sect1"><div class="sect1" id="creating-a-k-nearest-neighbor-classifier">
<h1>15.2 Creating a K-Nearest Neighbors Classifier</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id535">
<h2>Problem</h2>
<p>Given an <a data-primary="k-nearest neighbors (KNN)" data-secondary="creating classifier" data-type="indexterm" id="ix_knearest_neigh_create"/>observation of unknown class, you need to predict its class
based on the class of its neighbors.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id338">
<h2>Solution</h2>
<p>If the <a data-primary="KNeighborsClassifier" data-type="indexterm" id="ix_knearest_neigh_class"/>dataset is not very large, use <code>KNeighborsClassifier</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.neighbors</code> <code class="kn">import</code> <code class="n">KNeighborsClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">X</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">y</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create standardizer</code>
<code class="n">standardizer</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>

<code class="c1"># Standardize features</code>
<code class="n">X_std</code> <code class="o">=</code> <code class="n">standardizer</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>

<code class="c1"># Train a KNN classifier with 5 neighbors</code>
<code class="n">knn</code> <code class="o">=</code> <code class="n">KNeighborsClassifier</code><code class="p">(</code><code class="n">n_neighbors</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_std</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>

<code class="c1"># Create two observations</code>
<code class="n">new_observations</code> <code class="o">=</code> <code class="p">[[</code> <code class="mf">0.75</code><code class="p">,</code>  <code class="mf">0.75</code><code class="p">,</code>  <code class="mf">0.75</code><code class="p">,</code>  <code class="mf">0.75</code><code class="p">],</code>
                    <code class="p">[</code> <code class="mi">1</code><code class="p">,</code>  <code class="mi">1</code><code class="p">,</code>  <code class="mi">1</code><code class="p">,</code>  <code class="mi">1</code><code class="p">]]</code>

<code class="c1"># Predict the class of two observations</code>
<code class="n">knn</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">new_observations</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([1, 2])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id339">
<h2>Discussion</h2>
<p>In KNN, given an observation, <math display="inline"><msub><mi>x</mi><mi>u</mi></msub></math>, with an unknown target class, the algorithm first identifies the <math display="inline"><mi>k</mi></math> closest observations (sometimes called <math display="inline"><msub><mi>x</mi><mi>u</mi></msub></math>’s <em>neighborhood</em>) based on some distance metric (e.g., Euclidean distance), then these <math display="inline"><mi>k</mi></math> observations “vote” based on their class, and the class that wins the vote is <math display="inline"><msub><mi>x</mi><mi>u</mi></msub></math>’s predicted class. More formally, the probability <math display="inline"><msub><mi>x</mi><mi>u</mi></msub></math> of some class <math display="inline"><mi>j</mi></math> is:</p>
<div data-type="equation">
<math display="block">
<mrow>
<mfrac><mn>1</mn> <mi>k</mi></mfrac>
<munderover><mo>∑</mo> <mrow><mi>i</mi><mo>∈</mo><mi>ν</mi></mrow> <mrow/> </munderover>
<mi>I</mi>
<mrow>
<mo>(</mo>
<msub><mi>y</mi> <mi>i</mi> </msub>
<mo>=</mo>
<mi>j</mi>
<mo>)</mo>
</mrow>
</mrow>
</math>
</div>
<p>where ν is the <math display="inline"><mi>k</mi></math> observation in <math display="inline"><msub><mi>x</mi><mi>u</mi></msub></math>’s neighborhood, <math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math> is the class of the <em>i</em>th observation, and <math display="inline"><mi>I</mi></math> is an indicator function (i.e., 1 is true, 0 otherwise). In <a data-primary="predict_proba method" data-type="indexterm" id="id1774"/>scikit-learn we can see these probabilities using <code>predict_proba</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View probability that each observation is one of three classes</code>
<code class="n">knn</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">new_observations</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([[0. , 0.6, 0.4],
       [0. , 0. , 1. ]])</pre>
<p>The class with the highest probability becomes the predicted class. For
example, in the preceding output, the first observation should be class 1 (<em>Pr</em> = 0.6) while the second observation should be class 2
(<em>Pr</em> = 1), and this is just what we see:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">knn</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">new_observations</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([1, 2])</pre>
<p><code>KNeighborsClassifier</code> contains a number of important parameters to
consider. First, <code>metric</code> sets the distance metric used. Second, <code>n_jobs</code> determines how many of the computer’s cores to use. Because making a prediction requires calculating the distance from a point to every single point in the data, using multiple cores is highly recommended. Third, <code>algorithm</code> sets the method used to calculate the nearest neighbors. While there are real differences in the algorithms, by default <code>KNeighborsClassifier</code> attempts to auto-select the best algorithm so you often don’t need to
worry about this parameter. Fourth, by default <code>KNeighborsClassifier</code>
works how we described previously, with each observation in the neighborhood getting one vote; however, if we set the <code>weights</code> parameter to <code>distance</code>, the closer observations’ votes are weighted more than observations farther away. Intuitively this make sense, since more similar neighbors might tell us more about an observation’s class than others.</p>
<p>Finally, because distance calculations treat all features as if they are on the same scale, it is important to standardize the features prior to using a KNN classifier.<a data-primary="" data-startref="ix_knearest_neigh_create" data-type="indexterm" id="id1775"/><a data-primary="" data-startref="ix_knearest_neigh_class" data-type="indexterm" id="id1776"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="15.3 Identifying the Best Neighborhood Size" data-type="sect1"><div class="sect1" id="identifying-the-best-neighborhood-size">
<h1>15.3 Identifying the Best Neighborhood Size</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id536">
<h2>Problem</h2>
<p>You want to <a data-primary="k-nearest neighbors (KNN)" data-secondary="identifying best neighborhood size" data-type="indexterm" id="ix_knearest_neigh_best"/>select the best value for <em>k</em> in a k-nearest neighbors classifier.</p>
</div></section>
<section class="less_space pagebreak-before" data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id537">
<h2>Solution</h2>
<p>Use <a data-primary="cross-validation (CV) of ML models" data-secondary="GridSearchCV" data-type="indexterm" id="id1777"/>model selection techniques like <code>GridSearchCV</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.neighbors</code> <code class="kn">import</code> <code class="n">KNeighborsClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>
<code class="kn">from</code> <code class="nn">sklearn.pipeline</code> <code class="kn">import</code> <code class="n">Pipeline</code><code class="p">,</code> <code class="n">FeatureUnion</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">GridSearchCV</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create standardizer</code>
<code class="n">standardizer</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>

<code class="c1"># Create a KNN classifier</code>
<code class="n">knn</code> <code class="o">=</code> <code class="n">KNeighborsClassifier</code><code class="p">(</code><code class="n">n_neighbors</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Create a pipeline</code>
<code class="n">pipe</code> <code class="o">=</code> <code class="n">Pipeline</code><code class="p">([(</code><code class="s2">"standardizer"</code><code class="p">,</code> <code class="n">standardizer</code><code class="p">),</code> <code class="p">(</code><code class="s2">"knn"</code><code class="p">,</code> <code class="n">knn</code><code class="p">)])</code>

<code class="c1"># Create space of candidate values</code>
<code class="n">search_space</code> <code class="o">=</code> <code class="p">[{</code><code class="s2">"knn__n_neighbors"</code><code class="p">:</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="mi">6</code><code class="p">,</code> <code class="mi">7</code><code class="p">,</code> <code class="mi">8</code><code class="p">,</code> <code class="mi">9</code><code class="p">,</code> <code class="mi">10</code><code class="p">]}]</code>

<code class="c1"># Create grid search</code>
<code class="n">classifier</code> <code class="o">=</code> <code class="n">GridSearchCV</code><code class="p">(</code>
    <code class="n">pipe</code><code class="p">,</code> <code class="n">search_space</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">verbose</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_standardized</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id340">
<h2>Discussion</h2>
<p>The size of <em>k</em> has real implications in KNN classifiers. In
machine learning we are trying to find a balance between bias and
variance, and in few places is that as explicit as the value of
<em>k</em>. If <em>k</em> = <em>n</em>, where <em>n</em> is the number of observations, then we have high bias but low variance. If <em>k</em> = 1, we will have low bias but high variance. The best model will come from finding the value of <em>k</em> that balances this bias-variance trade-off. In our solution, we used <code>GridSearchCV</code> to conduct five-fold cross-validation on KNN classifiers with different values of <em>k</em>. When that is completed, we can see the <em>k</em> that produces the best model:<a data-primary="" data-startref="ix_knearest_neigh_best" data-type="indexterm" id="id1778"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Best neighborhood size (k)</code>
<code class="n">classifier</code><code class="o">.</code><code class="n">best_estimator_</code><code class="o">.</code><code class="n">get_params</code><code class="p">()[</code><code class="s2">"knn__n_neighbors"</code><code class="p">]</code></pre>
<pre data-type="programlisting">6</pre>
</div></section>
</div></section>
<section data-pdf-bookmark="15.4 Creating a Radius-Based Nearest Neighbors Classifier" data-type="sect1"><div class="sect1" id="creating-a-radius-based-nearest-neighbor-classifier">
<h1>15.4 Creating a Radius-Based Nearest Neighbors Classifier</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id341">
<h2>Problem</h2>
<p>Given an <a data-primary="k-nearest neighbors (KNN)" data-secondary="radius-based nearest neighbors" data-type="indexterm" id="id1779"/><a data-primary="radius-based KNN classifier" data-type="indexterm" id="id1780"/><a data-primary="radius-based nearest neighbors classifier (RNN)" data-type="indexterm" id="id1781"/><a data-primary="RNN (radius-based nearest neighbors classifier)" data-type="indexterm" id="id1782"/>observation of unknown class, you need to predict its class based on the class of all <a data-primary="" data-startref="RadiusNeighborsClassifier" data-type="indexterm" id="id1783"/>observations within a certain distance.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id1784">
<h2>Solution</h2>
<p>Use <code>RadiusNeighborsClassifier</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.neighbors</code> <code class="kn">import</code> <code class="n">RadiusNeighborsClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create standardizer</code>
<code class="n">standardizer</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>

<code class="c1"># Standardize features</code>
<code class="n">features_standardized</code> <code class="o">=</code> <code class="n">standardizer</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Train a radius neighbors classifier</code>
<code class="n">rnn</code> <code class="o">=</code> <code class="n">RadiusNeighborsClassifier</code><code class="p">(</code>
    <code class="n">radius</code><code class="o">=</code><code class="mf">.5</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_standardized</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>

<code class="c1"># Create two observations</code>
<code class="n">new_observations</code> <code class="o">=</code> <code class="p">[[</code> <code class="mi">1</code><code class="p">,</code>  <code class="mi">1</code><code class="p">,</code>  <code class="mi">1</code><code class="p">,</code>  <code class="mi">1</code><code class="p">]]</code>

<code class="c1"># Predict the class of two observations</code>
<code class="n">rnn</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">new_observations</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([2])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id1785">
<h2>Discussion</h2>
<p>In KNN classification, an observation’s class is predicted from the
classes of its <em>k</em> neighbors. A less common technique is classification in a <em>radius-based nearest neighbor</em> (RNN) classifier, where an observation’s class is predicted from the classes of all observations within a given radius <em>r</em>.</p>
<p class="less_space pagebreak-before">In scikit-learn, <code>RadiusNeighborsClassifier</code> is very similar to <code>KNeighbors​Classi⁠fier</code>, with the exception of two parameters. First, in <code>RadiusNeighbors​Clas⁠sifier</code> we need to specify the radius of the fixed area used to determine if an observation is a neighbor using <code>radius</code>. Unless there is some substantive reason for setting <code>radius</code> to some value, it’s best to treat it like any other hyperparameter and tune it during model selection. The second useful parameter is <code>outlier_label</code>, which indicates what label to give an observation that has no observations within the 
<span class="keep-together">radius—​which</span> itself
can be a useful tool for identifying outliers.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="15.5 Finding Approximate Nearest Neighbors" data-type="sect1"><div class="sect1" id="finding-approximate-nearest-neighbors">
<h1>15.5 Finding Approximate Nearest Neighbors</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id538">
<h2>Problem</h2>
<p>You want to <a data-primary="k-nearest neighbors (KNN)" data-secondary="finding ANNs" data-type="indexterm" id="ix_knear_neigh_ann"/>fetch nearest neighbors for big data at low latency:</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id342">
<h2>Solution</h2>
<p>Use <a data-primary="faiss library" data-type="indexterm" id="id1786"/><a data-primary="ANN (approximate nearest neighbors)" data-type="indexterm" id="ix_ann_approx_near_neigh"/><a data-primary="approximate nearest neighbors (ANN)" data-type="indexterm" id="ix_approx_near_neigh_ann"/>an <em>approximate nearest neighbors</em> (ANN) based search with Facebook’s <code>faiss</code> library:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">faiss</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.neighbors</code> <code class="kn">import</code> <code class="n">NearestNeighbors</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>

<code class="c1"># Create standardizer</code>
<code class="n">standardizer</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>

<code class="c1"># Standardize features</code>
<code class="n">features_standardized</code> <code class="o">=</code> <code class="n">standardizer</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Set faiss parameters</code>
<code class="n">n_features</code> <code class="o">=</code> <code class="n">features_standardized</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>
<code class="n">nlist</code> <code class="o">=</code> <code class="mi">3</code>
<code class="n">k</code> <code class="o">=</code> <code class="mi">2</code>

<code class="c1"># Create an IVF index</code>
<code class="n">quantizer</code> <code class="o">=</code> <code class="n">faiss</code><code class="o">.</code><code class="n">IndexFlatIP</code><code class="p">(</code><code class="n">n_features</code><code class="p">)</code>
<code class="n">index</code> <code class="o">=</code> <code class="n">faiss</code><code class="o">.</code><code class="n">IndexIVFFlat</code><code class="p">(</code><code class="n">quantizer</code><code class="p">,</code> <code class="n">n_features</code><code class="p">,</code> <code class="n">nlist</code><code class="p">)</code>

<code class="c1"># Train the index and add feature vectors</code>
<code class="n">index</code><code class="o">.</code><code class="n">train</code><code class="p">(</code><code class="n">features_standardized</code><code class="p">)</code>
<code class="n">index</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">features_standardized</code><code class="p">)</code>

<code class="c1"># Create an observation</code>
<code class="n">new_observation</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code> <code class="mi">1</code><code class="p">,</code>  <code class="mi">1</code><code class="p">,</code>  <code class="mi">1</code><code class="p">,</code>  <code class="mi">1</code><code class="p">]])</code>

<code class="c1"># Search the index for the 2 nearest neighbors</code>
<code class="n">distances</code><code class="p">,</code> <code class="n">indices</code> <code class="o">=</code> <code class="n">index</code><code class="o">.</code><code class="n">search</code><code class="p">(</code><code class="n">new_observation</code><code class="p">,</code> <code class="n">k</code><code class="p">)</code>

<code class="c1"># Show the feature vectors for the two nearest neighbors</code>
<code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="nb">list</code><code class="p">(</code><code class="n">features_standardized</code><code class="p">[</code><code class="n">i</code><code class="p">])</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="n">indices</code><code class="p">[</code><code class="mi">0</code><code class="p">]])</code></pre>
<pre data-type="programlisting">array([[1.03800476, 0.55861082, 1.10378283, 1.18556721],
       [0.79566902, 0.32841405, 0.76275827, 1.05393502]])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id343">
<h2>Discussion</h2>
<p>KNN is a <a data-primary="performance" data-secondary="KNN versus ANN" data-type="indexterm" id="ix_perf_knn_ann"/><a data-primary="k-nearest neighbors (KNN)" data-secondary="versus ANN" data-secondary-sortas="ANN" data-type="indexterm" id="ix_knear_neigh_ann_knn"/>great approach to finding the most similar observations in a set of small data. However, as the size of our data increases, so does the time it takes to compute the distance between any one observation and all other points in our dataset. Large scale ML systems such as search or recommendation engines often use some form of vector similarity measure to retrieve similar observations. But at scale in real time, where we need results in less than 100 ms, KNN becomes infeasible to run.</p>
<p>ANN helps us overcome this problem by sacrificing some of the quality of the exact nearest neighbors search in favor of speed. This is to say that although the order and items in the first 10 nearest neighbors of an ANN search may not match the first 10 results from an exact KNN search, we get those first 10 nearest neighbors much faster.</p>
<p>In this example, we use an ANN approach called <a data-primary="inverted file index (IVF)" data-type="indexterm" id="id1787"/><a data-primary="Voronoi tessellations" data-type="indexterm" id="ix_voronoi_tess"/>inverted file index (IVF). This approach works by using clustering to limit the scope of the search space for our nearest neighbors search. IVF uses Voronoi tessellations to partition our search space into a number of distinct areas (or clusters). And when we go to find nearest neighbors, we visit a limited number of clusters to find similar observations, as opposed to conducting a comparison across every point in our dataset.</p>
<p>How Voronoi tessellations are created from data is best visualized using simple data. Take a scatter plot of random data visualized in two dimensions, as shown in <a data-type="xref" href="#fig1501">Figure 15-1</a>.</p>
<figure><div class="figure" id="fig1501">
<img alt="mpc2 1501" height="391" src="assets/mpc2_1501.png" width="529"/>
<h6><span class="label">Figure 15-1. </span>A scatter plot of randomly generated two-dimensional data</h6>
</div></figure>
<p>Using Voronoi tessellations, we can create a number of subspaces, each of which contains only a small subset of the total observations we want to search, as shown in <a data-type="xref" href="#fig1502">Figure 15-2</a>.</p>
<figure><div class="figure" id="fig1502">
<img alt="mpc2 1502" height="391" src="assets/mpc2_1502.png" width="529"/>
<h6><span class="label">Figure 15-2. </span>Randomly generated two-dimensional data separated into a number of different subspaces</h6>
</div></figure>
<p class="less_space pagebreak-before">The <code>nlist</code> parameter in the <code>Faiss</code> library lets us define the number of clusters we want to create. An additional parameter, <code>nprobe</code>, can be used at query time to define the number of clusters we want to search to retrieve nearest neighbors for a given observation. Increasing both <code>nlist</code> and <code>nprobe</code> can result in higher quality neighbors at the cost of larger computational effort and thus a longer runtime for IVF indices. Decreasing each of these parameters will have the inverse effect, and your code will run faster but at the risk of returning lower quality results.</p>
<p>Notice this example returns the exact same output as the first recipe in this chapter. This is because we are working with very small data and using only three clusters, which makes it unlikely our ANN results will differ significantly from our KNN results.<a data-primary="" data-startref="ix_knear_neigh_ann" data-type="indexterm" id="id1788"/><a data-primary="" data-startref="ix_voronoi_tess" data-type="indexterm" id="id1789"/><a data-primary="" data-startref="ix_knear_neigh_ann_knn" data-type="indexterm" id="id1790"/><a data-primary="" data-startref="ix_perf_knn_ann" data-type="indexterm" id="id1791"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1792">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/DVqgn">Nearest Neighbor Indexes for Similarity Search (different ANN index types)</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="15.6 Evaluating Approximate Nearest Neighbors" data-type="sect1"><div class="sect1" id="evaluating-approximate-nearest-neighbors">
<h1>15.6 Evaluating Approximate Nearest Neighbors</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id539">
<h2>Problem</h2>
<p>You want to see how your <a data-primary="k-nearest neighbors (KNN)" data-secondary="evaluating ANNs" data-type="indexterm" id="ix_knear_neigh_ann_eval"/>ANN compares to exact nearest neighbors (KNN):</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id1793">
<h2>Solution</h2>
<p>Compute the recall @k nearest neighbors of the ANN as compared to the KNN:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">faiss</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.neighbors</code> <code class="kn">import</code> <code class="n">NearestNeighbors</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>

<code class="c1"># Number of nearest neighbors</code>
<code class="n">k</code> <code class="o">=</code> <code class="mi">10</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>

<code class="c1"># Create standardizer</code>
<code class="n">standardizer</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>

<code class="c1"># Standardize features</code>
<code class="n">features_standardized</code> <code class="o">=</code> <code class="n">standardizer</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Create KNN with 10 NN</code>
<code class="n">nearest_neighbors</code> <code class="o">=</code> <code class="n">NearestNeighbors</code><code class="p">(</code><code class="n">n_neighbors</code><code class="o">=</code><code class="n">k</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_standardized</code><code class="p">)</code>

<code class="c1"># Set faiss parameters</code>
<code class="n">n_features</code> <code class="o">=</code> <code class="n">features_standardized</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>
<code class="n">nlist</code> <code class="o">=</code> <code class="mi">3</code>

<code class="c1"># Create an IVF index</code>
<code class="n">quantizer</code> <code class="o">=</code> <code class="n">faiss</code><code class="o">.</code><code class="n">IndexFlatIP</code><code class="p">(</code><code class="n">n_features</code><code class="p">)</code>
<code class="n">index</code> <code class="o">=</code> <code class="n">faiss</code><code class="o">.</code><code class="n">IndexIVFFlat</code><code class="p">(</code><code class="n">quantizer</code><code class="p">,</code> <code class="n">n_features</code><code class="p">,</code> <code class="n">nlist</code><code class="p">)</code>

<code class="c1"># Train the index and add feature vectors</code>
<code class="n">index</code><code class="o">.</code><code class="n">train</code><code class="p">(</code><code class="n">features_standardized</code><code class="p">)</code>
<code class="n">index</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">features_standardized</code><code class="p">)</code>
<code class="n">index</code><code class="o">.</code><code class="n">nprobe</code> <code class="o">=</code> <code class="mi">1</code>

<code class="c1"># Create an observation</code>
<code class="n">new_observation</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code> <code class="mi">1</code><code class="p">,</code>  <code class="mi">1</code><code class="p">,</code>  <code class="mi">1</code><code class="p">,</code>  <code class="mi">1</code><code class="p">]])</code>

<code class="c1"># Find distances and indices of the observation's exact nearest neighbors</code>
<code class="n">knn_distances</code><code class="p">,</code> <code class="n">knn_indices</code> <code class="o">=</code> <code class="n">nearest_neighbors</code><code class="o">.</code><code class="n">kneighbors</code><code class="p">(</code><code class="n">new_observation</code><code class="p">)</code>

<code class="c1"># Search the index for the two nearest neighbors</code>
<code class="n">ivf_distances</code><code class="p">,</code> <code class="n">ivf_indices</code> <code class="o">=</code> <code class="n">index</code><code class="o">.</code><code class="n">search</code><code class="p">(</code><code class="n">new_observation</code><code class="p">,</code> <code class="n">k</code><code class="p">)</code>

<code class="c1"># Get the set overlap</code>
<code class="n">recalled_items</code> <code class="o">=</code> <code class="nb">set</code><code class="p">(</code><code class="nb">list</code><code class="p">(</code><code class="n">knn_indices</code><code class="p">[</code><code class="mi">0</code><code class="p">]))</code> <code class="o">&amp;</code> <code class="nb">set</code><code class="p">(</code><code class="nb">list</code><code class="p">(</code><code class="n">ivf_indices</code><code class="p">[</code><code class="mi">0</code><code class="p">]))</code>

<code class="c1"># Print the recall</code>
<code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Recall @k=</code><code class="si">{</code><code class="n">k</code><code class="si">}</code><code class="s2">: </code><code class="si">{</code><code class="nb">len</code><code class="p">(</code><code class="n">recalled_items</code><code class="p">)</code><code class="o">/</code><code class="n">k</code> <code class="o">*</code> <code class="mi">100</code><code class="si">}</code><code class="s2">%"</code><code class="p">)</code></pre>
<pre data-type="programlisting">Recall @k=10: 100.0%</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id344">
<h2>Discussion</h2>
<p><em>Recall @k</em> is most simply defined as the number of items returned by the ANN at some <em>k</em> nearest neighbors that also appear in the exact nearest neighbors at the same <em>k</em>, divided by <em>k</em>. In this example, at 10 nearest neighbors we have 100% recall, which means that our ANN is returning the same indices as our KNN at k=10 (though not necessarily in the same order).</p>
<p>Recall is a common metric to use when evaluating ANNs against exact nearest neighbors.<a data-primary="" data-startref="ix_classif_knn_class" data-type="indexterm" id="id1794"/><a data-primary="" data-startref="ix_knearest_neigh_ch15" data-type="indexterm" id="id1795"/><a data-primary="" data-startref="ix_approx_near_neigh_ann" data-type="indexterm" id="id1796"/><a data-primary="" data-startref="ix_ann_approx_near_neigh" data-type="indexterm" id="id1797"/><a data-primary="" data-startref="ix_knear_neigh_ann_eval" data-type="indexterm" id="id1798"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1799">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/-COc9">Google’s note on ANN for its Vertex Matching Engine Service</a></p>
</li>
</ul>
</div></section>
</div></section>
</div></section></div></body></html>