<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 16. Logistic Regression" data-type="chapter" epub:type="chapter"><div class="chapter" id="logistic-regression">
<h1><span class="label">Chapter 16. </span>Logistic Regression</h1>
<section data-pdf-bookmark="16.0 Introduction" data-type="sect1"><div class="sect1" id="id345">
<h1>16.0 Introduction</h1>
<p>Despite being called a <a data-primary="logistic regression" data-type="indexterm" id="ix_log_reg_ch16"/>regression, <em>logistic regression</em> is actually a widely used <a data-primary="supervised learning models" data-secondary="logistic regression" data-type="indexterm" id="id1800"/>supervised classification technique. Logistic regression (and its extensions, like multinomial logistic regression) is a straightforward, well-understood approach to predicting the probability that an observation is of a certain class. In this chapter, we will cover training a variety of classifiers using logistic regression in scikit-learn.</p>
</div></section>
<section data-pdf-bookmark="16.1 Training a Binary Classifier" data-type="sect1"><div class="sect1" id="training_binary_classifier">
<h1>16.1 Training a Binary Classifier</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id540">
<h2>Problem</h2>
<p>You need to <a data-primary="classification and classifiers" data-secondary="logistic regression" data-type="indexterm" id="ix_classif_log_reg"/><a data-primary="classification and classifiers" data-secondary="binary classifiers" data-type="indexterm" id="ix_classif_bin_class2"/><a data-primary="logistic regression" data-secondary="binary classifier training" data-type="indexterm" id="ix_log_reg_bin_classif"/><a data-primary="binary classifiers" data-secondary="logistic regression" data-type="indexterm" id="ix_bin_class_log"/>train a simple classifier model.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id346">
<h2>Solution</h2>
<p>Train a logistic regression in <a data-primary="LogisticRegression" data-type="indexterm" id="id1801"/>scikit-learn using <code>LogisticRegression</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LogisticRegression</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>

<code class="c1"># Load data with only two classes</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code><code class="p">[:</code><code class="mi">100</code><code class="p">,:]</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code><code class="p">[:</code><code class="mi">100</code><code class="p">]</code>

<code class="c1"># Standardize features</code>
<code class="n">scaler</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>
<code class="n">features_standardized</code> <code class="o">=</code> <code class="n">scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Create logistic regression object</code>
<code class="n">logistic_regression</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Train model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">logistic_regression</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_standardized</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id347">
<h2>Discussion</h2>
<p>Despite having “regression” in its name, a logistic regression is
actually a widely used binary classifier (i.e., the target vector can take only two values). In a logistic regression, a linear model (e.g.,
<em>β<sub>0</sub> +	β<sub>1</sub>x</em>) is included in a logistic (also called sigmoid) function, <math display="inline"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mi>z</mi></mrow> </msup></mrow></mfrac></math>, such that:</p>
<div data-type="equation">
<math display="block">
<mrow>
<mi>P</mi>
<mrow>
<mo>(</mo>
<msub><mi>y</mi> <mi>i</mi> </msub>
<mo>=</mo>
<mn>1</mn>
<mo>∣</mo>
<mi>X</mi>
<mo>)</mo>
</mrow>
<mo>=</mo>
<mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi> <mn>0</mn> </msub><mo>+</mo><msub><mi>β</mi> <mn>1</mn> </msub><mi>x</mi><mo>)</mo></mrow> </msup></mrow></mfrac>
</mrow>
</math>
</div>
<p>where <math display="inline"><mi>P</mi><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi> </msub><mo>=</mo><mn>1</mn><mo>∣</mo><mi>X</mi><mo>)</mo></mrow></math> is the probability of the <math display="inline"><mi>i</mi></math>th observation’s target value, <math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math>, being class 1; <math display="inline"><mi>X</mi></math> is the training data; <math display="inline"><msub><mi>β</mi><mn>0</mn></msub></math> and <math display="inline"><msub><mi>β</mi><mn>1</mn></msub></math> are the parameters to be learned; and <math display="inline"><mi>e</mi></math> is Euler’s number. The effect of the logistic function is to constrain the value of the function’s output to between 0 and 1, so that it can be interpreted as a probability. If
<math display="inline"><mi>P</mi><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi> </msub><mo>=</mo><mn>1</mn><mo>∣</mo><mi>X</mi><mo>)</mo></mrow></math> is greater than 0.5, class 1 is predicted; otherwise, class 0 is predicted.</p>
<p>In scikit-learn, we can train a logistic regression model using
<code>LogisticRegression</code>. Once it is trained, we can use the model to predict the class of new observations:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create new observation</code>
<code class="n">new_observation</code> <code class="o">=</code> <code class="p">[[</code><code class="mf">.5</code><code class="p">,</code> <code class="mf">.5</code><code class="p">,</code> <code class="mf">.5</code><code class="p">,</code> <code class="mf">.5</code><code class="p">]]</code>

<code class="c1"># Predict class</code>
<code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">new_observation</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([1])</pre>
<p>In this example, our observation was predicted to be class 1.
Additionally, we can see the probability that an observation is a member of each class:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View predicted probabilities</code>
<code class="n">model</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">new_observation</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([[0.17738424, 0.82261576]])</pre>
<p>Our observation had a 17.7% chance of being class 0 and an 82.2% chance of
being class 1.<a data-primary="" data-startref="ix_bin_class_log" data-type="indexterm" id="id1802"/><a data-primary="" data-startref="ix_classif_bin_class2" data-type="indexterm" id="id1803"/><a data-primary="" data-startref="ix_log_reg_bin_classif" data-type="indexterm" id="id1804"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="16.2 Training a Multiclass Classifier" data-type="sect1"><div class="sect1" id="training-a-multiclass-classifier-ch16">
<h1>16.2 Training a Multiclass Classifier</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id348">
<h2>Problem</h2>
<p>Given <a data-primary="classification and classifiers" data-secondary="multiclass predictions" data-type="indexterm" id="id1805"/><a data-primary="logistic regression" data-secondary="multiclass classifier training" data-type="indexterm" id="id1806"/><a data-primary="multiclass classifiers" data-type="indexterm" id="id1807"/>more than two classes, you need to train a classifier model.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id349">
<h2>Solution</h2>
<p>Train a <a data-primary="LogisticRegression" data-type="indexterm" id="id1808"/>logistic regression in scikit-learn with <code>LogisticRegression</code>
using one-vs-rest or multinomial methods:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LogisticRegression</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Standardize features</code>
<code class="n">scaler</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>
<code class="n">features_standardized</code> <code class="o">=</code> <code class="n">scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Create one-vs-rest logistic regression object</code>
<code class="n">logistic_regression</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">multi_class</code><code class="o">=</code><code class="s2">"ovr"</code><code class="p">)</code>

<code class="c1"># Train model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">logistic_regression</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_standardized</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id350">
<h2>Discussion</h2>
<p>On their own, <a data-primary="one-vs-rest (OvR) logistic regression" data-type="indexterm" id="id1809"/><a data-primary="OvR (one-vs-rest) logistic regression" data-type="indexterm" id="id1810"/>logistic regressions are only binary classifiers, meaning
they cannot handle target vectors with more than two classes. However,
two clever extensions to logistic regression do just that. First, in
<em>one-vs-rest</em> logistic regression (OvR) a separate model is trained for
each class predicted, whether an observation is that class or not (thus
making it a binary classification problem). It assumes that each
classification problem (e.g., class 0 or not) is independent.</p>
<p>Alternatively, in <em>multinomial logistic regression</em> (MLR), the <a data-primary="MLR (multinomial logistic regression)" data-type="indexterm" id="id1811"/><a data-primary="multinomial logistic regression (MLR)" data-type="indexterm" id="id1812"/>logistic
function we saw in <a data-type="xref" href="#training_binary_classifier">Recipe 16.1</a> is replaced with a softmax function:</p>
<div data-type="equation">
<math display="block">
<mrow>
<mi>P</mi>
<mrow>
<mo>(</mo>
<msub><mi>y</mi> <mi>i</mi> </msub>
<mo>=</mo>
<mi>k</mi>
<mo>∣</mo>
<mi>X</mi>
<mo>)</mo>
</mrow>
<mo>=</mo>
<mfrac><msup><mi>e</mi> <mrow><msub><mi>β</mi> <mi>k</mi> </msub><msub><mi>x</mi> <mi>i</mi> </msub></mrow> </msup> <mrow><msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>K</mi> </msubsup><msup><mi>e</mi> <mrow><msub><mi>β</mi> <mi>j</mi> </msub><msub><mi>x</mi> <mi>i</mi> </msub></mrow> </msup></mrow></mfrac>
</mrow>
</math>
</div>
<p>where <math display="inline"><mi>P</mi><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi> </msub><mo>=</mo><mi>k</mi><mo>∣</mo><mi>X</mi><mo>)</mo></mrow></math> is the probability of the
<math display="inline"><mi>i</mi> </math>th observation’s target value, <math display="inline"><msub><mi>y</mi> <mi>i</mi></msub> </math>, being in
class <math display="inline"><mi>k</mi> </math>, and <math display="inline"><mi>K</mi> </math> is the total number of classes. <a data-primary="predict_proba method" data-type="indexterm" id="id1813"/>One practical advantage of MLR is that its predicted probabilities using the <code>predict_proba</code> method are more reliable (i.e., better calibrated).</p>
<p>When using <code>LogisticRegression</code> we can select which of the two
techniques we want, with OvR (<code>ovr</code>) being the default argument. We can
switch to MLR by setting the argument to <code>multinomial</code>.<a data-primary="" data-startref="ix_classif_log_reg" data-type="indexterm" id="id1814"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="16.3 Reducing Variance Through Regularization" data-type="sect1"><div class="sect1" id="reducing-variance-through-regularization">
<h1>16.3 Reducing Variance Through Regularization</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id541">
<h2>Problem</h2>
<p>You need to reduce the <a data-primary="regularization" data-secondary="reducing variance with" data-type="indexterm" id="id1815"/><a data-primary="logistic regression" data-secondary="regularization, to reduce variance" data-type="indexterm" id="id1816"/>variance of your logistic regression model.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id351">
<h2>Solution</h2>
<p>Tune the <a data-primary="hyperparameters" data-secondary="regularization penalty" data-type="indexterm" id="id1817"/><a data-primary="regularization penalty hyperparameter" data-type="indexterm" id="id1818"/>regularization strength hyperparameter, <code>C</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LogisticRegressionCV</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Standardize features</code>
<code class="n">scaler</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>
<code class="n">features_standardized</code> <code class="o">=</code> <code class="n">scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Create decision tree regression object</code>
<code class="n">logistic_regression</code> <code class="o">=</code> <code class="n">LogisticRegressionCV</code><code class="p">(</code>
    <code class="n">penalty</code><code class="o">=</code><code class="s1">'l2'</code><code class="p">,</code> <code class="n">Cs</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Train model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">logistic_regression</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_standardized</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id352">
<h2>Discussion</h2>
<p><em>Regularization</em> is a method of penalizing complex models to reduce their
variance. Specifically, a penalty term is added to the loss function we
are trying to minimize, typically the L1 and L2 penalties. In the L1
penalty:</p>
<div data-type="equation">
<math display="block">
<mrow>
<mi>α</mi>
<munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>p</mi> </munderover>
<mfenced close="|" open="|" separators="">
<msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mi>j</mi> </msub>
</mfenced>
</mrow>
</math>
</div>
<p>where <math display="inline"><msub><mover accent="true"><mi>β</mi><mo>^</mo></mover> <mi>j</mi> </msub></math> is the parameters of the <math display="inline"><mi>j</mi></math>th of <math display="inline"><mi>p</mi></math> features being learned, and <math display="inline"><mi>α</mi></math> is a hyperparameter denoting the regularization strength. With the L2 penalty:</p>
<div data-type="equation">
<math display="block">
<mrow>
<mi>α</mi>
<munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>p</mi> </munderover>
<msup><msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mi>j</mi></msub> <mn>2</mn> </msup>
</mrow>
</math>
</div>
<p>Higher values of <math display="inline"><mi>α</mi></math> increase the penalty for larger
parameter values (i.e., more complex models). scikit-learn follows the
common method of using <math display="inline"><mi>C</mi></math> instead of <math display="inline"><mi>α</mi></math>
where <math display="inline"><mi>C</mi></math> is the inverse of the regularization strength:
<math display="inline"> <mrow><mi>C</mi><mo>=</mo><mfrac><mn>1</mn> <mi>α</mi></mfrac></mrow></math>. To reduce variance while using
logistic regression, we can treat <math display="inline"><mi>C</mi></math> as a hyperparameter to
be tuned to find the value of <math display="inline"><mi>C</mi></math> that creates the best
model. In <a data-primary="LogisticRegressionCV" data-type="indexterm" id="id1819"/><a data-primary="C hyperparameter" data-secondary="LogisticRegressionCV" data-type="indexterm" id="id1820"/><a data-primary="cross-validation (CV) of ML models" data-secondary="LogisticRegressionCV" data-type="indexterm" id="id1821"/>scikit-learn we can use the 
<span class="keep-together"><code>LogisticRegressionCV</code></span> class to efficiently tune <math display="inline"><mi>C</mi></math>. <code>LogisticRegressionCV</code>’s parameter <code>Cs</code> can either accept a range of values for <math display="inline"><mi>C</mi></math> to search over (if a list of floats is supplied as an argument) or, if supplied an integer, will generate a list of that many candidate values drawn from a logarithmic scale between –10,000 and 10,000.</p>
<p>Unfortunately, <code>LogisticRegressionCV</code> does not allow us to search over
different penalty terms. To do this we have to use the less efficient
model selection techniques discussed in <a data-type="xref" href="ch12.xhtml#model-selection">Chapter 12</a>.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="16.4 Training a Classifier on Very Large Data" data-type="sect1"><div class="sect1" id="training-a-classifier-on-very-large-data">
<h1>16.4 Training a Classifier on Very Large Data</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id353">
<h2>Problem</h2>
<p>You need to <a data-primary="logistic regression" data-secondary="very large dataset, training classifier on" data-type="indexterm" id="id1822"/><a data-primary="very large dataset, training classifier on" data-type="indexterm" id="id1823"/>train a simple classifier model on a very large set of data.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id354">
<h2>Solution</h2>
<p>Train a logistic regression in <a data-primary="LogisticRegression" data-type="indexterm" id="id1824"/>scikit-learn with <code>LogisticRegression</code>
using the<a data-primary="stochastic average gradient (SAG) solver" data-type="indexterm" id="id1825"/><a data-primary="SAG (stochastic average gradient) solver" data-type="indexterm" id="id1826"/> <em>stochastic average gradient</em> (SAG) solver:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LogisticRegression</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Standardize features</code>
<code class="n">scaler</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>
<code class="n">features_standardized</code> <code class="o">=</code> <code class="n">scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Create logistic regression object</code>
<code class="n">logistic_regression</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">solver</code><code class="o">=</code><code class="s2">"sag"</code><code class="p">)</code>

<code class="c1"># Train model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">logistic_regression</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_standardized</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id355">
<h2>Discussion</h2>
<p>scikit-learn’s <code>LogisticRegression</code> offers a<a data-primary="solvers" data-type="indexterm" id="id1827"/> number of techniques for
training a logistic regression, called <em>solvers</em>. Most of the time
scikit-learn will select the best solver automatically for us or warn us
that we cannot do something with that solver. However, there is one
particular case we should be aware of.</p>
<p>While an exact explanation is beyond the bounds of this book (for more
information see Mark Schmidt’s slides in the <a data-xrefstyle="select:labelnumber" href="#see-also-ch16a">“See Also”</a> section of this recipe), stochastic average gradient descent allows us to train a model much faster than other solvers when our data is very large. However, it is also very sensitive to feature scaling, so standardizing our features is particularly important. We can set our learning algorithm to use this
solver by setting <code>solver="sag"</code>.</p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="see-also-ch16a">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/K5rEG">Minimizing Finite Sums with the Stochastic Average Gradient Algorithm, Mark Schmidt</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="16.5 Handling Imbalanced Classes" data-type="sect1"><div class="sect1" id="id818">
<h1>16.5 Handling Imbalanced Classes</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id542">
<h2>Problem</h2>
<p>You need to <a data-primary="logistic regression" data-secondary="and imbalanced classes" data-secondary-sortas="imbalanced classes" data-type="indexterm" id="id1828"/><a data-primary="imbalanced classes, handling" data-secondary="logistic regression" data-type="indexterm" id="id1829"/>train a simple classifier model.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id356">
<h2>Solution</h2>
<p>Train a <a data-primary="LogisticRegression" data-type="indexterm" id="id1830"/>logistic regression in scikit-learn using <code>LogisticRegression</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LogisticRegression</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Make class highly imbalanced by removing first 40 observations</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">features</code><code class="p">[</code><code class="mi">40</code><code class="p">:,:]</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">target</code><code class="p">[</code><code class="mi">40</code><code class="p">:]</code>

<code class="c1"># Create target vector indicating if class 0, otherwise 1</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">where</code><code class="p">((</code><code class="n">target</code> <code class="o">==</code> <code class="mi">0</code><code class="p">),</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Standardize features</code>
<code class="n">scaler</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>
<code class="n">features_standardized</code> <code class="o">=</code> <code class="n">scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Create decision tree regression object</code>
<code class="n">logistic_regression</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">class_weight</code><code class="o">=</code><code class="s2">"balanced"</code><code class="p">)</code>

<code class="c1"># Train model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">logistic_regression</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_standardized</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id357">
<h2>Discussion</h2>
<p>Like many other learning algorithms in scikit-learn, <code>LogisticRegression</code> comes with a built-in method of handling imbalanced
classes. If we have highly imbalanced classes and have not addressed it
during preprocessing, we have the option of using the <code>class_weight</code> parameter to weight the classes to make certain we have a balanced mix of each class. Specifically, the <code>balanced</code> argument will automatically weigh classes inversely proportional to their frequency:<a data-primary="" data-startref="ix_log_reg_ch16" data-type="indexterm" id="id1831"/></p>
<div data-type="equation">
<math display="block">
<mrow>
<msub><mi>w</mi> <mi>j</mi> </msub>
<mo>=</mo>
<mfrac><mi>n</mi> <mrow><mi>k</mi><msub><mi>n</mi> <mi>j</mi> </msub></mrow></mfrac>
</mrow>
</math>
</div>
<p>where <math display="inline"><msub><mi>w</mi><mi>j</mi></msub></math> is the weight to class <math display="inline"><mi>j</mi></math>, <math display="inline"><mi>n</mi></math> is the number of observations, <math display="inline"><msub><mi>n</mi><mi>j</mi></msub></math> is the number of observations in class <math display="inline"><mi>j</mi></math>, and <math display="inline"><mi>k</mi></math> is the total number of classes.</p>
</div></section>
</div></section>
</div></section></div></body></html>