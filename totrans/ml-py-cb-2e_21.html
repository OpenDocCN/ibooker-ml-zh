<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 21. Neural Networks" data-type="chapter" epub:type="chapter"><div class="chapter" id="neural-networks">
<h1><span class="label">Chapter 21. </span>Neural Networks</h1>
<section data-pdf-bookmark="21.0 Introduction" data-type="sect1"><div class="sect1" id="id404">
<h1>21.0 Introduction</h1>
<p>At the heart of <a data-primary="neural networks" data-type="indexterm" id="ix_neural_net_ch21"/><a data-primary="unit, in neural network" data-type="indexterm" id="id1957"/><a data-primary="neuron, in neural network" data-type="indexterm" id="id1958"/><a data-primary="parameters" data-secondary="neural network" data-type="indexterm" id="id1959"/><a data-primary="node, in neural network" data-type="indexterm" id="id1960"/>basic neural networks is the <em>unit</em> (also called a <em>node</em> or <em>neuron</em>). A unit takes in one or more inputs, multiplies each input by a parameter (also called a <em>weight</em>), sums the weighted input’s values along with some bias value (typically 0), and then feeds the value into an activation function. This output is then sent forward to the other neurons deeper in the neural network (if they exist).</p>
<p>Neural <a data-primary="multilayer perceptron" data-type="indexterm" id="id1961"/><a data-primary="feedforward neural networks" data-type="indexterm" id="id1962"/>networks can be visualized as a series of connected layers that form a network connecting an observation’s feature values at one end and the target value (e.g., observation’s class) at the other end. <em>Feedforward</em> neural networks—​also called <em>multilayer perceptron</em>—are the simplest artificial neural networks used in any real-world setting. The name “feedforward” comes from the fact that an observation’s feature values are fed “forward” through the network, with each layer successively transforming the feature values with the goal that the output is the same as (or close to) the target’s value.</p>
<p>Specifically, <a data-primary="output layer, neural network" data-type="indexterm" id="id1963"/><a data-primary="input layer, neural network" data-type="indexterm" id="id1964"/><a data-primary="layers, in neural networks" data-type="indexterm" id="id1965"/><a data-primary="hidden layers, neural network" data-type="indexterm" id="id1966"/>feedforward neural networks contain three types of layers. At the start of the neural network is an input layer, where
each unit contains an observation’s value for a single feature. For
example, if an observation has 100 features, the input layer has 100
units. At the end of the neural network is the output layer, which
transforms the output of intermediate layers (called <em>hidden layers</em>) into values useful for the
task at hand. For example, if our goal is binary classification, we can use an output layer with a single unit that uses a sigmoid function to scale its own output to between 0 and 1, representing a predicted class probability.</p>
<p>Between the <a data-primary="deep learning" data-type="indexterm" id="id1967"/>input and output layers are the so-called hidden layers. These hidden layers successively transform the feature values from the input layer to something that, once processed by the output layer, resembles the target class. Neural networks with many hidden layers (e.g., 10, 100, 1,000) are considered “deep” networks. Training deep neural networks is a process known as <em>deep learning</em>.</p>
<p>Neural <a data-primary="observations" data-secondary="batch of neural network" data-type="indexterm" id="id1968"/><a data-primary="back propagation" data-type="indexterm" id="id1969"/><a data-primary="forward propagation" data-type="indexterm" id="id1970"/><a data-primary="batch of observations, neural network" data-type="indexterm" id="id1971"/><a data-primary="parameters" data-secondary="neural network" data-type="indexterm" id="id1972"/>networks are typically created with all parameters initialized as small random values from a Gaussian or normal uniform distribution. Once an
observation (or more often a set number of observations called a <em>batch</em>) is fed through the network, the outputted value is compared with the observation’s true value using a loss function. This is called <em>forward propagation</em>. Next an algorithm goes “backward” through the network identifying how much each parameter contributed to the error between the predicted and true values, a process called <em>back propagation</em>. At each parameter, the optimization algorithm determines how much each weight should be adjusted to improve the output.</p>
<p>Neural <a data-primary="epoch, in neural network" data-type="indexterm" id="id1973"/><a data-primary="gradient descent, in neural network" data-type="indexterm" id="id1974"/>networks learn by repeating this process of forward propagation
and back propagation for every observation in the training data multiple
times (each time all observations have been sent through the network is
called an <em>epoch</em> and training typically consists of multiple epochs),
iteratively updating the values of the parameters utilizing a process called <em>gradient descent</em> to slowly optimize the values of the parameters for the given output.</p>
<p>In this chapter, we will use the same Python library used in the last chapter, PyTorch, to build,
train, and evaluate a variety of neural networks. PyTorch is a popular tool within the deep learning space due to its well-written APIs and intuitive representation of the low-level tensor operations that power neural networks. One key feature of PyTorch is called <em>autograd</em>, which automatically computes and stores the gradients used to optimize the parameters of the network after undergoing forward propagation and back propagation.</p>
<p>Neural networks created using PyTorch code can be trained using both CPUs
(i.e., on your laptop) and GPUs (i.e., on a specialized deep learning
computer). In the real world with real data, it is often necessary to train neural networks using GPUs, as the training process on large data for complex networks runs orders of magnitude faster on GPUs than CPUs. However, all the neural networks in this book are small and simple enough to be trained on a CPU-only laptop in only a few minutes. Just be aware that when we have larger networks and more training data, training using CPUs is <em>significantly</em> slower than training using GPUs.</p>
</div></section>
<section data-pdf-bookmark="21.1 Using Autograd with PyTorch" data-type="sect1"><div class="sect1" id="using-autograd-with-pytorch">
<h1>21.1 Using Autograd with PyTorch</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id405">
<h2>Problem</h2>
<p>You want to use <a data-primary="neural networks" data-secondary="autograd" data-type="indexterm" id="ix_nn_autograd_ch21"/><a data-primary="PyTorch" data-secondary="autograd feature" data-type="indexterm" id="ix_pyt_autograd"/><a data-primary="autograd" data-type="indexterm" id="ix_autograd_ch21"/>PyTorch’s autograd features to compute and store the gradients after undergoing forward propagation and back propagation.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id1975">
<h2>Solution</h2>
<p>Create tensors with the <code>requires_grad</code> option set to <code>True</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import libraries</code>
<code class="kn">import</code> <code class="nn">torch</code>

<code class="c1"># Create a torch tensor that requires gradients</code>
<code class="n">t</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([</code><code class="mf">1.0</code><code class="p">,</code> <code class="mf">2.0</code><code class="p">,</code> <code class="mf">3.0</code><code class="p">],</code> <code class="n">requires_grad</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>

<code class="c1"># Perform a tensor operation simulating "forward propagation"</code>
<code class="n">tensor_sum</code> <code class="o">=</code> <code class="n">t</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code>

<code class="c1"># Perform back propagation</code>
<code class="n">tensor_sum</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>

<code class="c1"># View the gradients</code>
<code class="n">t</code><code class="o">.</code><code class="n">grad</code></pre>
<pre data-type="programlisting">tensor([1., 1., 1.])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id406">
<h2>Discussion</h2>
<p>Autograd is one of the core features of PyTorch and a big factor in its popularity as a deep learning library. The ability to easily compute, store, and visualize gradients makes PyTorch very intuitive for researchers and enthusiasts building neural networks from scratch.</p>
<p>PyTorch uses a <a data-primary="DAG (directed acyclic graph)" data-type="indexterm" id="id1976"/><a data-primary="directed acyclic graph (DAG)" data-type="indexterm" id="id1977"/>directed acyclic graph (DAG) to keep a record of all data and computational operations being performed on that data. This is incredibly useful, but it also means we need to be careful with what operations we try to apply on our PyTorch data that requires gradients. When working with autograd, we can’t easily convert our tensors to NumPy arrays and back without “breaking the graph,” a phrase used to describe operations that don’t support autograd:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">torch</code>

<code class="n">tensor</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([</code><code class="mf">1.0</code><code class="p">,</code><code class="mf">2.0</code><code class="p">,</code><code class="mf">3.0</code><code class="p">],</code> <code class="n">requires_grad</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">tensor</code><code class="o">.</code><code class="n">numpy</code><code class="p">()</code></pre>
<pre data-type="programlisting">RuntimeError: Can't call numpy() on Tensor that requires grad. Use
    tensor.detach().numpy() instead.</pre>
<p>To convert this tensor into a NumPy array, we need to call the <code>detach()</code> method<a data-primary="detach() method" data-type="indexterm" id="id1978"/> on it, which will break the graph and thus our ability to automatically compute gradients. While this can definitely be useful, it’s worth knowing that detaching the tensor will prevent PyTorch from automatically computing the gradient. <a data-primary="" data-startref="ix_autograd_ch21" data-type="indexterm" id="id1979"/><a data-primary="" data-startref="ix_nn_autograd_ch21" data-type="indexterm" id="id1980"/><a data-primary="" data-startref="ix_pyt_autograd" data-type="indexterm" id="id1981"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1982">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/mOWSw">PyTorch Autograd Tutorial</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="21.2 Preprocessing Data for Neural Networks" data-type="sect1"><div class="sect1" id="preprocessing-data-for-neural-networks">
<h1>21.2 Preprocessing Data for Neural Networks</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id560">
<h2>Problem</h2>
<p>You want to <a data-primary="preprocessing data" data-secondary="neural networks" data-type="indexterm" id="ix_preproc_data_nn"/><a data-primary="neural networks" data-secondary="preprocessing data for" data-type="indexterm" id="ix_nn_preproc_data"/>preprocess data for use in a neural network.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id407">
<h2>Solution</h2>
<p>Standardize each <a data-primary="StandardScaler" data-type="indexterm" id="id1983"/>feature using scikit-learn’s <code>StandardScaler</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">preprocessing</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>

<code class="c1"># Create feature</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code><code class="o">-</code><code class="mf">100.1</code><code class="p">,</code> <code class="mf">3240.1</code><code class="p">],</code>
                     <code class="p">[</code><code class="o">-</code><code class="mf">200.2</code><code class="p">,</code> <code class="o">-</code><code class="mf">234.1</code><code class="p">],</code>
                     <code class="p">[</code><code class="mf">5000.5</code><code class="p">,</code> <code class="mf">150.1</code><code class="p">],</code>
                     <code class="p">[</code><code class="mf">6000.6</code><code class="p">,</code> <code class="o">-</code><code class="mf">125.1</code><code class="p">],</code>
                     <code class="p">[</code><code class="mf">9000.9</code><code class="p">,</code> <code class="o">-</code><code class="mf">673.1</code><code class="p">]])</code>

<code class="c1"># Create scaler</code>
<code class="n">scaler</code> <code class="o">=</code> <code class="n">preprocessing</code><code class="o">.</code><code class="n">StandardScaler</code><code class="p">()</code>

<code class="c1"># Convert to a tensor</code>
<code class="n">features_standardized_tensor</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Show features</code>
<code class="n">features_standardized_tensor</code></pre>
<pre data-type="programlisting">tensor([[-100.1000, 3240.1000],
        [-200.2000, -234.1000],
        [5000.5000,  150.1000],
        [6000.6000, -125.1000],
        [9000.9000, -673.1000]], dtype=torch.float64)</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id408">
<h2>Discussion</h2>
<p>While this recipe is very similar to <a data-type="xref" href="ch04.xhtml#standardizing-a-feature">Recipe 4.2</a>, it is worth repeating
because of how important it is for neural networks. Typically, a neural
network’s parameters are initialized (i.e., created) as small random
numbers. Neural networks often behave poorly when the feature values
are much larger than the parameter values. Furthermore, since an observation’s
feature values are combined as they pass through individual units,
it is important that all features have the same scale.</p>
<p>For these reasons, it is best practice (although not always necessary;
for example, when we have all binary features) to standardize each
feature such that the feature’s values have the mean of 0 and the
standard deviation of 1. This can be accomplished easily with
scikit-learn’s <code>StandardScaler</code>.</p>
<p>However, if you need to perform this operation after having created tensors with <code>requires_grad=True</code>, you’ll need to do this natively in PyTorch, so as not to break the graph. While you’ll typically standardize features prior to starting to train the network, it’s worth knowing how to accomplish the same thing in PyTorch:<a data-primary="" data-startref="ix_nn_preproc_data" data-type="indexterm" id="id1984"/><a data-primary="" data-startref="ix_preproc_data_nn" data-type="indexterm" id="id1985"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">import</code> <code class="nn">torch</code>

<code class="c1"># Create features</code>
<code class="n">torch_features</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([[</code><code class="o">-</code><code class="mf">100.1</code><code class="p">,</code> <code class="mf">3240.1</code><code class="p">],</code>
                               <code class="p">[</code><code class="o">-</code><code class="mf">200.2</code><code class="p">,</code> <code class="o">-</code><code class="mf">234.1</code><code class="p">],</code>
                               <code class="p">[</code><code class="mf">5000.5</code><code class="p">,</code> <code class="mf">150.1</code><code class="p">],</code>
                               <code class="p">[</code><code class="mf">6000.6</code><code class="p">,</code> <code class="o">-</code><code class="mf">125.1</code><code class="p">],</code>
                               <code class="p">[</code><code class="mf">9000.9</code><code class="p">,</code> <code class="o">-</code><code class="mf">673.1</code><code class="p">]],</code> <code class="n">requires_grad</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>

<code class="c1"># Compute the mean and standard deviation</code>
<code class="n">mean</code> <code class="o">=</code> <code class="n">torch_features</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="n">keepdim</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">standard_deviation</code> <code class="o">=</code> <code class="n">torch_features</code><code class="o">.</code><code class="n">std</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="n">unbiased</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code> <code class="n">keepdim</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>

<code class="c1"># Standardize the features using the mean and standard deviation</code>
<code class="n">torch_features_standardized</code> <code class="o">=</code> <code class="n">torch_features</code> <code class="o">-</code> <code class="n">mean</code>
<code class="n">torch_features_standardized</code> <code class="o">/=</code> <code class="n">standard_deviation</code>

<code class="c1"># Show standardized features</code>
<code class="n">torch_features_standardized</code></pre>
<pre data-type="programlisting">tensor([[-1.1254,  1.9643],
        [-1.1533, -0.5007],
        [ 0.2953, -0.2281],
        [ 0.5739, -0.4234],
        [ 1.4096, -0.8122]], grad_fn=&lt;DivBackward0&gt;)</pre>
</div></section>
</div></section>
<section data-pdf-bookmark="21.3 Designing a Neural Network" data-type="sect1"><div class="sect1" id="designing-a-neural-network">
<h1>21.3 Designing a Neural Network</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id561">
<h2>Problem</h2>
<p>You want to <a data-primary="neural networks" data-secondary="designing" data-type="indexterm" id="ix_nn_designing"/>design a neural network.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id409">
<h2>Solution</h2>
<p>Use the <a data-primary="nn.Module" data-type="indexterm" id="ix_nn_module"/>PyTorch <code>nn.Module</code> class to define a simple neural network architecture:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import libraries</code>
<code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>

<code class="c1"># Define a neural network</code>
<code class="k">class</code> <code class="nc">SimpleNeuralNet</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">SimpleNeuralNet</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">fc1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">16</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">fc2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code> <code class="mi">16</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">fc3</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
        <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">functional</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">fc1</code><code class="p">(</code><code class="n">x</code><code class="p">))</code>
        <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">functional</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">fc2</code><code class="p">(</code><code class="n">x</code><code class="p">))</code>
        <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">functional</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">fc3</code><code class="p">(</code><code class="n">x</code><code class="p">))</code>
        <code class="k">return</code> <code class="n">x</code>

<code class="c1"># Initialize the neural network</code>
<code class="n">network</code> <code class="o">=</code> <code class="n">SimpleNeuralNet</code><code class="p">()</code>

<code class="c1"># Define loss function, optimizer</code>
<code class="n">loss_criterion</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">BCELoss</code><code class="p">()</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">RMSprop</code><code class="p">(</code><code class="n">network</code><code class="o">.</code><code class="n">parameters</code><code class="p">())</code>

<code class="c1"># Show the network</code>
<code class="n">network</code></pre>
<pre data-type="programlisting">SimpleNeuralNet(
  (fc1): Linear(in_features=10, out_features=16, bias=True)
  (fc2): Linear(in_features=16, out_features=16, bias=True)
  (fc3): Linear(in_features=16, out_features=1, bias=True)
)</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id410">
<h2>Discussion</h2>
<p class="fix_tracking2">Neural networks consist of layers of units. However, there’s incredible variety in the types of layers and how they are combined to form the
network’s architecture. While there are commonly used
architecture patterns (which we’ll cover in this chapter), the truth is
that selecting the right architecture is mostly an art and the topic of
much research.</p>
<p>To construct a feedforward neural network in PyTorch, we need to make a
number of choices about both the network architecture and training
process. Remember that each unit in the hidden layers:</p>
<ol>
<li>
<p>Receives a number of inputs.</p>
</li>
<li>
<p>Weights each input by a parameter value.</p>
</li>
<li>
<p>Sums together all weighted inputs along with some bias (typically
0).</p>
</li>
<li>
<p>Most often then applies some function (called an <em>activation
function</em>).</p>
</li>
<li>
<p>Sends the output on to units in the next layer.</p>
</li>
</ol>
<p>First, for each <a data-primary="activation functions, neural network" data-type="indexterm" id="id1986"/>layer in the hidden and output layers we must define the number of units to include in the layer and the activation function. Overall, the more units we have in a layer, the more complex patterns our network is able to learn. However, more units might make our network overfit the training data in a way detrimental to the performance on the test data.</p>
<p>For hidden layers, a <a data-primary="rectified linear unit (ReLU)" data-type="indexterm" id="id1987"/><a data-primary="ReLU (rectified linear unit)" data-type="indexterm" id="id1988"/>popular activation function is the <em>rectified linear unit</em> (ReLU):</p>
<div data-type="equation">
<math display="block">
<mrow>
<mi>f</mi>
<mo>(</mo>
<mi>z</mi>
<mo>)</mo>
<mo>=</mo>
<mo form="prefix" movablelimits="true">max</mo>
<mo>(</mo>
<mn>0</mn>
<mo>,</mo>
<mi>z</mi>
<mo>)</mo>
</mrow>
</math>
</div>
<p>where <math display="inline"><mi>z</mi></math> is the sum of the weighted inputs and bias. As we can see, if <math display="inline"><mi>z</mi></math> is greater than 0, the activation function returns <math display="inline"><mi>z</mi></math>; otherwise, the function returns 0. This simple activation function has a number of desirable properties (a discussion of which is beyond the scope of this book), and this has made it a popular choice in neural networks. We should be aware, however, that many dozens of activation functions exist.</p>
<p>Second, we need to define the number of hidden layers to use in the
network. More layers allow the network to learn more complex relationships, but with a computational cost.</p>
<p>Third, we have to define the structure of the activation function (if any) of the output layer. The nature of the output function is often determined by the goal of the network. Here are some common output layer patterns:</p>
<dl>
<dt>Binary classification</dt>
<dd>
<p>One unit with a sigmoid activation function</p>
</dd>
<dt>Multiclass classification</dt>
<dd>
<p><em>k</em> units (where <em>k</em> is the number of target classes) and a softmax activation 
<span class="keep-together">function</span></p>
</dd>
<dt>Regression</dt>
<dd>
<p>One unit with no activation function</p>
</dd>
</dl>
<p>Fourth, we need to define a <a data-primary="loss function, neural network" data-type="indexterm" id="id1989"/>loss function (the function that measures
how well a predicted value matches the true value); again, this is often
determined by the problem type:</p>
<dl>
<dt>Binary classification</dt>
<dd>
<p>Binary cross-entropy</p>
</dd>
<dt>Multiclass classification</dt>
<dd>
<p>Categorical cross-entropy</p>
</dd>
<dt>Regression</dt>
<dd>
<p>Mean square error</p>
</dd>
</dl>
<p>Fifth, we have to define an <a data-primary="optimizer, neural network" data-type="indexterm" id="id1990"/>optimizer, which intuitively can be thought
of as our strategy “walking around” the loss function to find the parameter values that produce the lowest error. Common choices for optimizers are stochastic gradient descent, stochastic gradient descent with momentum, root mean square propagation, and adaptive moment estimation (for more information on these optimizers, see <a data-type="xref" href="#see-also-ch20">“See Also”</a>).</p>
<p>Sixth, we can select one or more <a data-primary="metrics" data-secondary="neural network" data-type="indexterm" id="id1991"/>metrics to use to evaluate the
performance, such as accuracy.</p>
<p>In our example, we use the <code>torch.nn.Module</code> namespace to compose a simple, sequential neural network that can make binary classifications. The standard PyTorch approach for this is to create a child class that inherits from the <code>torch.nn.Module</code> class, instantiating a network architecture in the <code>__init__</code> method, and defining the mathematical operations we want to perform upon each forward pass in the <code>forward</code> method of the class. There are many ways to define networks in PyTorch, and although in this case we use functional methods for our activation functions (such as <code>nn.functional.relu</code>) we can also define these activation functions as layers. If we <a data-primary="Sequential" data-type="indexterm" id="id1992"/>wanted to compose everything in the network as a layer, we could use the <code>Sequential</code> class:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import libraries</code>
<code class="kn">import</code> <code class="nn">torch</code>

<code class="c1"># Define a neural network using `Sequential`</code>
<code class="k">class</code> <code class="nc">SimpleNeuralNet</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">SimpleNeuralNet</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">sequential</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">16</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code><code class="mi">16</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Sigmoid</code><code class="p">()</code>
        <code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">sequential</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">x</code>

<code class="c1"># Instantiate and view the network</code>
<code class="n">SimpleNeuralNet</code><code class="p">()</code></pre>
<pre data-type="programlisting">SimpleNeuralNet(
  (sequential): Sequential(
    (0): Linear(in_features=10, out_features=16, bias=True)
    (1): ReLU()
    (2): Linear(in_features=16, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=1, bias=True)
    (5): Sigmoid()
  )
)</pre>
<p>In both cases, the network itself is a two-layer neural network (when counting
layers we don’t include the input layer because it does not have any
parameters to learn) defined using PyTorch’s sequential model. Each layer is
“dense” (also called “fully connected”), meaning that all the units in the previous layer are connected to all the units in the next layer.</p>
<p>In the first hidden layer we set <code>out_features=16</code>, meaning that layer contains 16 units. These units have ReLU activation functions as defined in the <code>forward</code> method of our class: <code>x = nn.functional.relu(self.fc1(x))</code>. The first layer of our network has the size <code>(10, 16)</code>, which tells the first layer to expect each observation from our input data to have 10 feature values. This network is designed for binary classification so the output layer contains only one unit with a sigmoid activation function, which constrains the output to between 0 and 1 (representing the probability an observation is class 1).<a data-primary="" data-startref="ix_nn_module" data-type="indexterm" id="id1993"/><a data-primary="" data-startref="ix_nn_designing" data-type="indexterm" id="id1994"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="see-also-ch20">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/iT8iv">PyTorch tutorial: Build the Neural Network</a></p>
</li>
<li>
<p><a href="https://oreil.ly/4bPXv">Loss functions for classification, Wikipedia</a></p>
</li>
<li>
<p><a href="https://oreil.ly/pplP-">On Loss Functions for Deep Neural Networks in Classification, Katarzyna Janocha and Wojciech Marian Czarnecki</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="21.4 Training a Binary Classifier" data-type="sect1"><div class="sect1" id="training-a-binary-classifier">
<h1>21.4 Training a Binary Classifier</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id562">
<h2>Problem</h2>
<p>You want to train a <a data-primary="neural networks" data-secondary="binary classifier training" data-type="indexterm" id="ix_nn_bin_class_train"/><a data-primary="binary classifiers" data-secondary="neural networks" data-type="indexterm" id="ix_binary_class_nn"/><a data-primary="classification and classifiers" data-secondary="binary classifiers" data-type="indexterm" id="ix_classif_bin_class3"/>binary classifier neural network.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id411">
<h2>Solution</h2>
<p>Use PyTorch to construct a <a data-primary="feedforward neural networks" data-type="indexterm" id="ix_feed_for_nn"/>feedforward neural network and train it:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import libraries</code>
<code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">torch.utils.data</code> <code class="kn">import</code> <code class="n">DataLoader</code><code class="p">,</code> <code class="n">TensorDataset</code>
<code class="kn">from</code> <code class="nn">torch.optim</code> <code class="kn">import</code> <code class="n">RMSprop</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_classification</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>

<code class="c1"># Create training and test sets</code>
<code class="n">features</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="n">make_classification</code><code class="p">(</code><code class="n">n_classes</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">n_features</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code>
    <code class="n">n_samples</code><code class="o">=</code><code class="mi">1000</code><code class="p">)</code>
<code class="n">features_train</code><code class="p">,</code> <code class="n">features_test</code><code class="p">,</code> <code class="n">target_train</code><code class="p">,</code> <code class="n">target_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Set random seed</code>
<code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
<code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Convert data to PyTorch tensors</code>
<code class="n">x_train</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">features_train</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>
<code class="n">y_train</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">target_train</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
<code class="n">x_test</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">features_test</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>
<code class="n">y_test</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">target_test</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Define a neural network using `Sequential`</code>
<code class="k">class</code> <code class="nc">SimpleNeuralNet</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">SimpleNeuralNet</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">sequential</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">16</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code><code class="mi">16</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Sigmoid</code><code class="p">()</code>
        <code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">sequential</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">x</code>

<code class="c1"># Initialize neural network</code>
<code class="n">network</code> <code class="o">=</code> <code class="n">SimpleNeuralNet</code><code class="p">()</code>

<code class="c1"># Define loss function, optimizer</code>
<code class="n">criterion</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">BCELoss</code><code class="p">()</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">RMSprop</code><code class="p">(</code><code class="n">network</code><code class="o">.</code><code class="n">parameters</code><code class="p">())</code>

<code class="c1"># Define data loader</code>
<code class="n">train_data</code> <code class="o">=</code> <code class="n">TensorDataset</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="n">train_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">train_data</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>

<code class="c1"># Compile the model using torch 2.0's optimizer</code>
<code class="n">network</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">network</code><code class="p">)</code>

<code class="c1"># Train neural network</code>
<code class="n">epochs</code> <code class="o">=</code> <code class="mi">3</code>
<code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">epochs</code><code class="p">):</code>
    <code class="k">for</code> <code class="n">batch_idx</code><code class="p">,</code> <code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">train_loader</code><code class="p">):</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code>
        <code class="n">output</code> <code class="o">=</code> <code class="n">network</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>
        <code class="n">loss</code> <code class="o">=</code> <code class="n">criterion</code><code class="p">(</code><code class="n">output</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>
        <code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">()</code>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"Epoch:"</code><code class="p">,</code> <code class="n">epoch</code><code class="o">+</code><code class="mi">1</code><code class="p">,</code> <code class="s2">"</code><code class="se">\t</code><code class="s2">Loss:"</code><code class="p">,</code> <code class="n">loss</code><code class="o">.</code><code class="n">item</code><code class="p">())</code>

<code class="c1"># Evaluate neural network</code>
<code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
    <code class="n">output</code> <code class="o">=</code> <code class="n">network</code><code class="p">(</code><code class="n">x_test</code><code class="p">)</code>
    <code class="n">test_loss</code> <code class="o">=</code> <code class="n">criterion</code><code class="p">(</code><code class="n">output</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>
    <code class="n">test_accuracy</code> <code class="o">=</code> <code class="p">(</code><code class="n">output</code><code class="o">.</code><code class="n">round</code><code class="p">()</code> <code class="o">==</code> <code class="n">y_test</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"Test Loss:"</code><code class="p">,</code> <code class="n">test_loss</code><code class="o">.</code><code class="n">item</code><code class="p">(),</code> <code class="s2">"</code><code class="se">\t</code><code class="s2">Test Accuracy:"</code><code class="p">,</code>
        <code class="n">test_accuracy</code><code class="o">.</code><code class="n">item</code><code class="p">())</code></pre>
<pre data-type="programlisting">Epoch: 1 	Loss: 0.19006995856761932
Epoch: 2 	Loss: 0.14092367887496948
Epoch: 3 	Loss: 0.03935524448752403
Test Loss: 0.06877756118774414 	Test Accuracy: 0.9700000286102295</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id412">
<h2>Discussion</h2>
<p>In <a data-type="xref" href="#designing-a-neural-network">Recipe 21.3</a>, we <a data-primary="make_classification method" data-type="indexterm" id="id1995"/>discussed how to construct a neural network using PyTorch’s sequential model. In this recipe we train that neural network using 10 features and 1,000 observations of fake classification generated from scikit-learn’s 
<span class="keep-together"><code>make_classification</code></span> function.</p>
<p>The neural network we are using is the same as the one in <a data-type="xref" href="#designing-a-neural-network">Recipe 21.3</a> (see that recipe for a detailed explanation). The difference there is that we only created the neural network; we didn’t train it.</p>
<p>
At the end, we use <code>with torch.no_grad()</code> to evaluate the network. This says that we should not compute gradients for any tensor operations conducted in this section of code. Since we use gradients only during the model training process, we don’t want to store new gradients for operations that occur outside of it (such as prediction or evaluation).</p>
<p>
The <code>epochs</code> variable defines how many epochs to use when training the
data. <code>batch_size</code> sets the number of
observations to propagate through the network before updating the
parameters.
</p>
<p>We then iterate over the number of epochs, making forward passes through the network using the <code>forward</code> method, and then backward passes to update the gradients. The result is a trained model.<a data-primary="" data-startref="ix_binary_class_nn" data-type="indexterm" id="id1996"/><a data-primary="" data-startref="ix_classif_bin_class3" data-type="indexterm" id="id1997"/><a data-primary="" data-startref="ix_nn_bin_class_train" data-type="indexterm" id="id1998"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="21.5 Training a Multiclass Classifier" data-type="sect1"><div class="sect1" id="training-a-multiclass-classifier-ch20">
<h1>21.5 Training a Multiclass Classifier</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id413">
<h2>Problem</h2>
<p>You want to <a data-primary="neural networks" data-secondary="multiclass classifier training" data-type="indexterm" id="ix_nn_multi_class_classif"/><a data-primary="classification and classifiers" data-secondary="multiclass predictions" data-type="indexterm" id="ix_classif_multicl_pred"/><a data-primary="multiclass classifiers" data-type="indexterm" id="ix_multi_class_classif2"/>train a multiclass classifier neural network.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id414">
<h2>Solution</h2>
<p>Use PyTorch to <a data-primary="softmax activation functions" data-type="indexterm" id="ix_softmax_act_func"/><a data-primary="activation functions, neural network" data-type="indexterm" id="ix_act_func_nn"/>construct a feedforward neural network with an output layer
with softmax activation functions:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import libraries</code>
<code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">torch.utils.data</code> <code class="kn">import</code> <code class="n">DataLoader</code><code class="p">,</code> <code class="n">TensorDataset</code>
<code class="kn">from</code> <code class="nn">torch.optim</code> <code class="kn">import</code> <code class="n">RMSprop</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_classification</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>

<code class="n">N_CLASSES</code><code class="o">=</code><code class="mi">3</code>
<code class="n">EPOCHS</code><code class="o">=</code><code class="mi">3</code>

<code class="c1"># Create training and test sets</code>
<code class="n">features</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="n">make_classification</code><code class="p">(</code><code class="n">n_classes</code><code class="o">=</code><code class="n">N_CLASSES</code><code class="p">,</code> <code class="n">n_informative</code><code class="o">=</code><code class="mi">9</code><code class="p">,</code>
    <code class="n">n_redundant</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">n_features</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">n_samples</code><code class="o">=</code><code class="mi">1000</code><code class="p">)</code>
<code class="n">features_train</code><code class="p">,</code> <code class="n">features_test</code><code class="p">,</code> <code class="n">target_train</code><code class="p">,</code> <code class="n">target_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Set random seed</code>
<code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
<code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Convert data to PyTorch tensors</code>
<code class="n">x_train</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">features_train</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>
<code class="n">y_train</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">functional</code><code class="o">.</code><code class="n">one_hot</code><code class="p">(</code><code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">target_train</code><code class="p">)</code><code class="o">.</code><code class="n">long</code><code class="p">(),</code>
    <code class="n">num_classes</code><code class="o">=</code><code class="n">N_CLASSES</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>
<code class="n">x_test</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">features_test</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>
<code class="n">y_test</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">functional</code><code class="o">.</code><code class="n">one_hot</code><code class="p">(</code><code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">target_test</code><code class="p">)</code><code class="o">.</code><code class="n">long</code><code class="p">(),</code>
    <code class="n">num_classes</code><code class="o">=</code><code class="n">N_CLASSES</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>

<code class="c1"># Define a neural network using `Sequential`</code>
<code class="k">class</code> <code class="nc">SimpleNeuralNet</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">SimpleNeuralNet</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">sequential</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">16</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code><code class="mi">16</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code><code class="mi">3</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Softmax</code><code class="p">()</code>
        <code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">sequential</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">x</code>

<code class="c1"># Initialize neural network</code>
<code class="n">network</code> <code class="o">=</code> <code class="n">SimpleNeuralNet</code><code class="p">()</code>

<code class="c1"># Define loss function, optimizer</code>
<code class="n">criterion</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">CrossEntropyLoss</code><code class="p">()</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">RMSprop</code><code class="p">(</code><code class="n">network</code><code class="o">.</code><code class="n">parameters</code><code class="p">())</code>

<code class="c1"># Define data loader</code>
<code class="n">train_data</code> <code class="o">=</code> <code class="n">TensorDataset</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="n">train_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">train_data</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>

<code class="c1"># Compile the model using torch 2.0's optimizer</code>
<code class="n">network</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">network</code><code class="p">)</code>

<code class="c1"># Train neural network</code>
<code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">EPOCHS</code><code class="p">):</code>
    <code class="k">for</code> <code class="n">batch_idx</code><code class="p">,</code> <code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">train_loader</code><code class="p">):</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code>
        <code class="n">output</code> <code class="o">=</code> <code class="n">network</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>
        <code class="n">loss</code> <code class="o">=</code> <code class="n">criterion</code><code class="p">(</code><code class="n">output</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>
        <code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">()</code>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"Epoch:"</code><code class="p">,</code> <code class="n">epoch</code><code class="o">+</code><code class="mi">1</code><code class="p">,</code> <code class="s2">"</code><code class="se">\t</code><code class="s2">Loss:"</code><code class="p">,</code> <code class="n">loss</code><code class="o">.</code><code class="n">item</code><code class="p">())</code>

<code class="c1"># Evaluate neural network</code>
<code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
    <code class="n">output</code> <code class="o">=</code> <code class="n">network</code><code class="p">(</code><code class="n">x_test</code><code class="p">)</code>
    <code class="n">test_loss</code> <code class="o">=</code> <code class="n">criterion</code><code class="p">(</code><code class="n">output</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>
    <code class="n">test_accuracy</code> <code class="o">=</code> <code class="p">(</code><code class="n">output</code><code class="o">.</code><code class="n">round</code><code class="p">()</code> <code class="o">==</code> <code class="n">y_test</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"Test Loss:"</code><code class="p">,</code> <code class="n">test_loss</code><code class="o">.</code><code class="n">item</code><code class="p">(),</code> <code class="s2">"</code><code class="se">\t</code><code class="s2">Test Accuracy:"</code><code class="p">,</code>
        <code class="n">test_accuracy</code><code class="o">.</code><code class="n">item</code><code class="p">())</code></pre>
<pre data-type="programlisting">Epoch: 1 	Loss: 0.8022041916847229
Epoch: 2 	Loss: 0.775616466999054
Epoch: 3 	Loss: 0.7751263380050659
Test Loss: 0.8105319142341614 	Test Accuracy: 0.8199999928474426</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id415">
<h2>Discussion</h2>
<p>In this <a data-primary="one-hot encoding" data-type="indexterm" id="id1999"/>solution we created a similar neural network to the binary
classifier from the last recipe, but with some notable changes. In the classification data we generated, we set <code>N_CLASSES=3</code>. To handle multiclass classification, we also use 
<span class="keep-together"><code>nn.CrossEntropyLoss()</code>,</span> which expects the target to be one-hot encoded. To accomplish this, we use the <code>torch.nn.functional.one_hot</code> function and end up with a one-hot encoded array where the position of <code>1.</code> indicates the class for a given observation:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View target matrix</code>
<code class="n">y_train</code></pre>
<pre data-type="programlisting">tensor([[1., 0., 0.],
        [0., 1., 0.],
        [1., 0., 0.],
        ...,
        [0., 1., 0.],
        [1., 0., 0.],
        [0., 0., 1.]])</pre>
<p>Since this is a multiclass classification problem, we used an
output layer of size 3 (one per class) containing a softmax
activation function. The softmax activation function will return an
array of 3 values summing to 1. These 3 values represent an
observation’s probability of being a member of each of the 3 classes.</p>
<p>As <a data-primary="CrossEntropyLoss() function" data-type="indexterm" id="id2000"/><a data-primary="loss function, neural network" data-type="indexterm" id="id2001"/>mentioned in this recipe, we used a loss function suited to multiclass classification, the
categorical cross-entropy loss function: <code>nn.CrossEntropyLoss()</code>.<a data-primary="" data-startref="ix_act_func_nn" data-type="indexterm" id="id2002"/><a data-primary="" data-startref="ix_classif_multicl_pred" data-type="indexterm" id="id2003"/><a data-primary="" data-startref="ix_multi_class_classif2" data-type="indexterm" id="id2004"/><a data-primary="" data-startref="ix_nn_multi_class_classif" data-type="indexterm" id="id2005"/><a data-primary="" data-startref="ix_softmax_act_func" data-type="indexterm" id="id2006"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="21.6 Training a Regressor" data-type="sect1"><div class="sect1" id="training-a-regressor">
<h1>21.6 Training a Regressor</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id563">
<h2>Problem</h2>
<p>You want to <a data-primary="neural networks" data-secondary="regressor training" data-type="indexterm" id="ix_nn_regress_train"/><a data-primary="regression and regressors" data-secondary="neural network training" data-type="indexterm" id="ix_regress_train_nn"/>train a neural network for regression.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id2007">
<h2>Solution</h2>
<p>Use PyTorch to construct a feedforward neural network with a single
output unit that has no activation function:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import libraries</code>
<code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">torch.utils.data</code> <code class="kn">import</code> <code class="n">DataLoader</code><code class="p">,</code> <code class="n">TensorDataset</code>
<code class="kn">from</code> <code class="nn">torch.optim</code> <code class="kn">import</code> <code class="n">RMSprop</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_regression</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>

<code class="n">EPOCHS</code><code class="o">=</code><code class="mi">5</code>

<code class="c1"># Create training and test sets</code>
<code class="n">features</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="n">make_regression</code><code class="p">(</code><code class="n">n_features</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">n_samples</code><code class="o">=</code><code class="mi">1000</code><code class="p">)</code>
<code class="n">features_train</code><code class="p">,</code> <code class="n">features_test</code><code class="p">,</code> <code class="n">target_train</code><code class="p">,</code> <code class="n">target_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Set random seed</code>
<code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
<code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Convert data to PyTorch tensors</code>
<code class="n">x_train</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">features_train</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>
<code class="n">y_train</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">target_train</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code><code class="mi">1</code><code class="p">)</code>
<code class="n">x_test</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">features_test</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>
<code class="n">y_test</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">target_test</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Define a neural network using `Sequential`</code>
<code class="k">class</code> <code class="nc">SimpleNeuralNet</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">SimpleNeuralNet</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">sequential</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">16</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code><code class="mi">16</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code><code class="mi">1</code><code class="p">),</code>
        <code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">sequential</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">x</code>

<code class="c1"># Initialize neural network</code>
<code class="n">network</code> <code class="o">=</code> <code class="n">SimpleNeuralNet</code><code class="p">()</code>

<code class="c1"># Define loss function, optimizer</code>
<code class="n">criterion</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">MSELoss</code><code class="p">()</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">RMSprop</code><code class="p">(</code><code class="n">network</code><code class="o">.</code><code class="n">parameters</code><code class="p">())</code>

<code class="c1"># Define data loader</code>
<code class="n">train_data</code> <code class="o">=</code> <code class="n">TensorDataset</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="n">train_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">train_data</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>

<code class="c1"># Compile the model using torch 2.0's optimizer</code>
<code class="n">network</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">network</code><code class="p">)</code>

<code class="c1"># Train neural network</code>
<code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">EPOCHS</code><code class="p">):</code>
    <code class="k">for</code> <code class="n">batch_idx</code><code class="p">,</code> <code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">train_loader</code><code class="p">):</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code>
        <code class="n">output</code> <code class="o">=</code> <code class="n">network</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>
        <code class="n">loss</code> <code class="o">=</code> <code class="n">criterion</code><code class="p">(</code><code class="n">output</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>
        <code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">()</code>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"Epoch:"</code><code class="p">,</code> <code class="n">epoch</code><code class="o">+</code><code class="mi">1</code><code class="p">,</code> <code class="s2">"</code><code class="se">\t</code><code class="s2">Loss:"</code><code class="p">,</code> <code class="n">loss</code><code class="o">.</code><code class="n">item</code><code class="p">())</code>

<code class="c1"># Evaluate neural network</code>
<code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
    <code class="n">output</code> <code class="o">=</code> <code class="n">network</code><code class="p">(</code><code class="n">x_test</code><code class="p">)</code>
    <code class="n">test_loss</code> <code class="o">=</code> <code class="nb">float</code><code class="p">(</code><code class="n">criterion</code><code class="p">(</code><code class="n">output</code><code class="p">,</code> <code class="n">y_test</code><code class="p">))</code>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"Test MSE:"</code><code class="p">,</code> <code class="n">test_loss</code><code class="p">)</code></pre>
<pre data-type="programlisting">Epoch: 1 	Loss: 10764.02734375
Epoch: 2 	Loss: 1356.510009765625
Epoch: 3 	Loss: 504.9664306640625
Epoch: 4 	Loss: 199.11314392089844
Epoch: 5 	Loss: 191.20834350585938
Test MSE: 162.24497985839844</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id416">
<h2>Discussion</h2>
<p class="fix_tracking">
It’s completely possible to create a neural network to predict
continuous values instead of class probabilities. In the case of our
binary classifier (<a data-type="xref" href="#training-a-binary-classifier">Recipe 21.4</a>) we used an output layer with a single unit and a sigmoid activation function to produce a probability that an observation was class 1. Importantly, the sigmoid activation function constrained the outputted value to between 0 and 1. If we remove that constraint by having no activation function, we allow the output to be a continuous value.
</p>
<p>Furthermore, <a data-primary="mean squared error (MSE)" data-type="indexterm" id="id2008"/><a data-primary="MSE (mean squared error)" data-type="indexterm" id="id2009"/><a data-primary="sigmoid activation function" data-type="indexterm" id="id2010"/><a data-primary="activation functions, neural network" data-type="indexterm" id="id2011"/>because we are training a regression, we should use an
appropriate loss function and evaluation metric, in our case the mean
square error:</p>
<div data-type="equation">
<math display="block">
<mrow>
<mo form="prefix">MSE</mo>
<mo>=</mo>
<mfrac><mn>1</mn> <mi>n</mi></mfrac>
<munderover><mo>∑</mo> <mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mrow> <mi>n</mi> </munderover>
<msup><mrow><mo>(</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>i</mi> </msub><mo>-</mo><msub><mi>y</mi> <mi>i</mi> </msub><mo>)</mo></mrow> <mn>2</mn> </msup>
</mrow>
</math>
</div>
<p>where <math display="inline"><mi>n</mi></math> is the number of observations; <math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math>
is the true value of the target we are trying to predict, <math display="inline"><mi>y</mi></math>, for observation <math display="inline"><mi>i</mi></math>; and <math display="inline"><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover><mi>i</mi> </msub> </math> is the model’s predicted value for <math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math>.</p>
<p>Finally, <a data-primary="make_regression method" data-type="indexterm" id="id2012"/>because we are using simulated data using scikit-learn
<code>make_regression</code>, we didn’t have to standardize the features. It should be noted, however, that in almost all real-world cases, standardization would be necessary.<a data-primary="" data-startref="ix_feed_for_nn" data-type="indexterm" id="id2013"/><a data-primary="" data-startref="ix_nn_regress_train" data-type="indexterm" id="id2014"/><a data-primary="" data-startref="ix_regress_train_nn" data-type="indexterm" id="id2015"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="21.7 Making Predictions" data-type="sect1"><div class="sect1" id="making-predictions">
<h1>21.7 Making Predictions</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id564">
<h2>Problem</h2>
<p>You want to use a <a data-primary="predictions and predicting" data-secondary="neural networks" data-type="indexterm" id="ix_predict_nn"/><a data-primary="neural networks" data-secondary="predicting" data-type="indexterm" id="ix_nn_predict"/>neural network to make predictions.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id417">
<h2>Solution</h2>
<p>Use PyTorch to <a data-primary="forward method" data-type="indexterm" id="id2016"/>construct a feedforward neural network, then make
predictions using <code>forward</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import libraries</code>
<code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">torch.utils.data</code> <code class="kn">import</code> <code class="n">DataLoader</code><code class="p">,</code> <code class="n">TensorDataset</code>
<code class="kn">from</code> <code class="nn">torch.optim</code> <code class="kn">import</code> <code class="n">RMSprop</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_classification</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>

<code class="c1"># Create training and test sets</code>
<code class="n">features</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="n">make_classification</code><code class="p">(</code><code class="n">n_classes</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">n_features</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code>
    <code class="n">n_samples</code><code class="o">=</code><code class="mi">1000</code><code class="p">)</code>
<code class="n">features_train</code><code class="p">,</code> <code class="n">features_test</code><code class="p">,</code> <code class="n">target_train</code><code class="p">,</code> <code class="n">target_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Set random seed</code>
<code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
<code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Convert data to PyTorch tensors</code>
<code class="n">x_train</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">features_train</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>
<code class="n">y_train</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">target_train</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
<code class="n">x_test</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">features_test</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>
<code class="n">y_test</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">target_test</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Define a neural network using `Sequential`</code>
<code class="k">class</code> <code class="nc">SimpleNeuralNet</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">SimpleNeuralNet</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">sequential</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">16</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code><code class="mi">16</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Sigmoid</code><code class="p">()</code>
        <code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">sequential</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">x</code>

<code class="c1"># Initialize neural network</code>
<code class="n">network</code> <code class="o">=</code> <code class="n">SimpleNeuralNet</code><code class="p">()</code>

<code class="c1"># Define loss function, optimizer</code>
<code class="n">criterion</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">BCELoss</code><code class="p">()</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">RMSprop</code><code class="p">(</code><code class="n">network</code><code class="o">.</code><code class="n">parameters</code><code class="p">())</code>

<code class="c1"># Define data loader</code>
<code class="n">train_data</code> <code class="o">=</code> <code class="n">TensorDataset</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="n">train_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">train_data</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>

<code class="c1"># Compile the model using torch 2.0's optimizer</code>
<code class="n">network</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">network</code><code class="p">)</code>

<code class="c1"># Train neural network</code>
<code class="n">epochs</code> <code class="o">=</code> <code class="mi">3</code>
<code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">epochs</code><code class="p">):</code>
    <code class="k">for</code> <code class="n">batch_idx</code><code class="p">,</code> <code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">train_loader</code><code class="p">):</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code>
        <code class="n">output</code> <code class="o">=</code> <code class="n">network</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>
        <code class="n">loss</code> <code class="o">=</code> <code class="n">criterion</code><code class="p">(</code><code class="n">output</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>
        <code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">()</code>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"Epoch:"</code><code class="p">,</code> <code class="n">epoch</code><code class="o">+</code><code class="mi">1</code><code class="p">,</code> <code class="s2">"</code><code class="se">\t</code><code class="s2">Loss:"</code><code class="p">,</code> <code class="n">loss</code><code class="o">.</code><code class="n">item</code><code class="p">())</code>

<code class="c1"># Evaluate neural network</code>
<code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
    <code class="n">predicted_class</code> <code class="o">=</code> <code class="n">network</code><code class="o">.</code><code class="n">forward</code><code class="p">(</code><code class="n">x_train</code><code class="p">)</code><code class="o">.</code><code class="n">round</code><code class="p">()</code>

<code class="n">predicted_class</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code></pre>
<pre data-type="programlisting">Epoch: 1 	Loss: 0.19006995856761932
Epoch: 2 	Loss: 0.14092367887496948
Epoch: 3 	Loss: 0.03935524448752403
tensor([1.])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id418">
<h2>Discussion</h2>
<p>Making <a data-primary="round method" data-type="indexterm" id="id2017"/>predictions is easy in PyTorch. Once we have trained our neural
network we can use the <code>forward</code> method (already used as part of the training process), which takes as input a set of features and does a forward pass through the network. In
our solution the neural network is set up for binary classification, so
the predicted output is the probability of being class 1. Observations
with predicted values very close to 1 are highly likely to be class 1,
while observations with predicted values very close to 0 are highly
likely to be class 0. Hence, we use the <code>round</code> method to convert these values to 1s and 0s for our binary classifier.<a data-primary="" data-startref="ix_nn_predict" data-type="indexterm" id="id2018"/><a data-primary="" data-startref="ix_predict_nn" data-type="indexterm" id="id2019"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="21.8 Visualize Training History" data-type="sect1"><div class="sect1" id="visualize-training-history">
<h1>21.8 Visualize Training History</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id565">
<h2>Problem</h2>
<p>You want to <a data-primary="visualization" data-secondary="neural networks" data-type="indexterm" id="ix_vis_nn_hist"/><a data-primary="neural networks" data-secondary="training history visualization" data-type="indexterm" id="ix_nn_hist_vis"/>find the “sweet spot” in a neural network’s loss and/or
accuracy score.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id2020">
<h2>Solution</h2>
<p>Use Matplotlib to visualize the loss of the test and training set over
each epoch:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>
<code class="kn">from</code> <code class="nn">torch.utils.data</code> <code class="kn">import</code> <code class="n">DataLoader</code><code class="p">,</code> <code class="n">TensorDataset</code>
<code class="kn">from</code> <code class="nn">torch.optim</code> <code class="kn">import</code> <code class="n">RMSprop</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_classification</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>

<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>

<code class="c1"># Create training and test sets</code>
<code class="n">features</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="n">make_classification</code><code class="p">(</code><code class="n">n_classes</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">n_features</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code>
    <code class="n">n_samples</code><code class="o">=</code><code class="mi">1000</code><code class="p">)</code>
<code class="n">features_train</code><code class="p">,</code> <code class="n">features_test</code><code class="p">,</code> <code class="n">target_train</code><code class="p">,</code> <code class="n">target_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Set random seed</code>
<code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
<code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Convert data to PyTorch tensors</code>
<code class="n">x_train</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">features_train</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>
<code class="n">y_train</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">target_train</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
<code class="n">x_test</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">features_test</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>
<code class="n">y_test</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">target_test</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Define a neural network using `Sequential`</code>
<code class="k">class</code> <code class="nc">SimpleNeuralNet</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">SimpleNeuralNet</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">sequential</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">16</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code><code class="mi">16</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Sigmoid</code><code class="p">()</code>
        <code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">sequential</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">x</code>

<code class="c1"># Initialize neural network</code>
<code class="n">network</code> <code class="o">=</code> <code class="n">SimpleNeuralNet</code><code class="p">()</code>

<code class="c1"># Define loss function, optimizer</code>
<code class="n">criterion</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">BCELoss</code><code class="p">()</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">RMSprop</code><code class="p">(</code><code class="n">network</code><code class="o">.</code><code class="n">parameters</code><code class="p">())</code>

<code class="c1"># Define data loader</code>
<code class="n">train_data</code> <code class="o">=</code> <code class="n">TensorDataset</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="n">train_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">train_data</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>

<code class="c1"># Compile the model using torch 2.0's optimizer</code>
<code class="n">network</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">network</code><code class="p">)</code>

<code class="c1"># Train neural network</code>
<code class="n">epochs</code> <code class="o">=</code> <code class="mi">8</code>
<code class="n">train_losses</code> <code class="o">=</code> <code class="p">[]</code>
<code class="n">test_losses</code> <code class="o">=</code> <code class="p">[]</code>
<code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">epochs</code><code class="p">):</code>
    <code class="k">for</code> <code class="n">batch_idx</code><code class="p">,</code> <code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">train_loader</code><code class="p">):</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code>
        <code class="n">output</code> <code class="o">=</code> <code class="n">network</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>
        <code class="n">loss</code> <code class="o">=</code> <code class="n">criterion</code><code class="p">(</code><code class="n">output</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>
        <code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">()</code>

    <code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
        <code class="n">train_output</code> <code class="o">=</code> <code class="n">network</code><code class="p">(</code><code class="n">x_train</code><code class="p">)</code>
        <code class="n">train_loss</code> <code class="o">=</code> <code class="n">criterion</code><code class="p">(</code><code class="n">output</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>
        <code class="n">train_losses</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">train_loss</code><code class="o">.</code><code class="n">item</code><code class="p">())</code>

        <code class="n">test_output</code> <code class="o">=</code> <code class="n">network</code><code class="p">(</code><code class="n">x_test</code><code class="p">)</code>
        <code class="n">test_loss</code> <code class="o">=</code> <code class="n">criterion</code><code class="p">(</code><code class="n">test_output</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>
        <code class="n">test_losses</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">test_loss</code><code class="o">.</code><code class="n">item</code><code class="p">())</code>

<code class="c1"># Visualize loss history</code>
<code class="n">epochs</code> <code class="o">=</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="n">epochs</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">epochs</code><code class="p">,</code> <code class="n">train_losses</code><code class="p">,</code> <code class="s2">"r--"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">epochs</code><code class="p">,</code> <code class="n">test_losses</code><code class="p">,</code> <code class="s2">"b-"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">([</code><code class="s2">"Training Loss"</code><code class="p">,</code> <code class="s2">"Test Loss"</code><code class="p">])</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Epoch"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Loss"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">();</code></pre>
<figure><div class="figure">
<img alt="mpc2 21in01" height="415" src="assets/mpc2_21in01.png" width="557"/>
<h6/>
</div></figure>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id419">
<h2>Discussion</h2>
<p>When our neural network is new, it will have poor performance. As the
neural network learns on the training data, the model’s error on both the training and test set will tend to decrease. However, at a certain point, a neural network can start “memorizing” the training data and overfit. When this starts happening, the training error may decrease while the test error starts increasing. Therefore, in many cases, there is a “sweet spot” where the test error (which is the error we mainly care about) is at its lowest point. This effect can be <a data-primary="overfitting of data" data-secondary="visualizing" data-type="indexterm" id="id2021"/>seen in the solution, where we visualize the training and test loss at each epoch. Note that the test error is lowest around epoch 6, after which the training loss plateaus while the test loss starts increasing. From this point onward, the model is overfitting.<a data-primary="" data-startref="ix_nn_hist_vis" data-type="indexterm" id="id2022"/><a data-primary="" data-startref="ix_vis_nn_hist" data-type="indexterm" id="id2023"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="21.9 Reducing Overfitting with Weight Regularization" data-type="sect1"><div class="sect1" id="reducing-overfitting-with-weight-regularization">
<h1>21.9 Reducing Overfitting with Weight Regularization</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id566">
<h2>Problem</h2>
<p>You want to <a data-primary="fitting of data to a line" data-secondary="reducing overfitting in neural networks" data-type="indexterm" id="ix_fit_data_lin_nn"/><a data-primary="overfitting of data" data-secondary="weight regularization to reduce" data-type="indexterm" id="ix_overfit_weight_reg"/><a data-primary="neural networks" data-secondary="weight regularization to reduce overfitting" data-type="indexterm" id="ix_nn_weight_reg"/>reduce overfitting by regularizing the weights of your network.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id420">
<h2>Solution</h2>
<p>Try <a data-primary="weight regularization (weight decay)" data-type="indexterm" id="ix_weight_reg"/><a data-primary="regularization" data-secondary="reducing variance with" data-type="indexterm" id="ix_reg_red_var_nn"/>penalizing the parameters of the network, also called <em>weight regularization</em>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import libraries</code>
<code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">torch.utils.data</code> <code class="kn">import</code> <code class="n">DataLoader</code><code class="p">,</code> <code class="n">TensorDataset</code>
<code class="kn">from</code> <code class="nn">torch.optim</code> <code class="kn">import</code> <code class="n">RMSprop</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_classification</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>

<code class="c1"># Create training and test sets</code>
<code class="n">features</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="n">make_classification</code><code class="p">(</code><code class="n">n_classes</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">n_features</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code>
    <code class="n">n_samples</code><code class="o">=</code><code class="mi">1000</code><code class="p">)</code>
<code class="n">features_train</code><code class="p">,</code> <code class="n">features_test</code><code class="p">,</code> <code class="n">target_train</code><code class="p">,</code> <code class="n">target_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Set random seed</code>
<code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
<code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Convert data to PyTorch tensors</code>
<code class="n">x_train</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">features_train</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>
<code class="n">y_train</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">target_train</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
<code class="n">x_test</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">features_test</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>
<code class="n">y_test</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">target_test</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Define a neural network using `Sequential`</code>
<code class="k">class</code> <code class="nc">SimpleNeuralNet</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">SimpleNeuralNet</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">sequential</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">16</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code><code class="mi">16</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Sigmoid</code><code class="p">()</code>
        <code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">sequential</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">x</code>

<code class="c1"># Initialize neural network</code>
<code class="n">network</code> <code class="o">=</code> <code class="n">SimpleNeuralNet</code><code class="p">()</code>

<code class="c1"># Define loss function, optimizer</code>
<code class="n">criterion</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">BCELoss</code><code class="p">()</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">Adam</code><code class="p">(</code><code class="n">network</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">lr</code><code class="o">=</code><code class="mf">1e-4</code><code class="p">,</code> <code class="n">weight_decay</code><code class="o">=</code><code class="mf">1e-5</code><code class="p">)</code>

<code class="c1"># Define data loader</code>
<code class="n">train_data</code> <code class="o">=</code> <code class="n">TensorDataset</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="n">train_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">train_data</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>

<code class="c1"># Compile the model using torch 2.0's optimizer</code>
<code class="n">network</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">network</code><code class="p">)</code>

<code class="c1"># Train neural network</code>
<code class="n">epochs</code> <code class="o">=</code> <code class="mi">100</code>
<code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">epochs</code><code class="p">):</code>
    <code class="k">for</code> <code class="n">batch_idx</code><code class="p">,</code> <code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">train_loader</code><code class="p">):</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code>
        <code class="n">output</code> <code class="o">=</code> <code class="n">network</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>
        <code class="n">loss</code> <code class="o">=</code> <code class="n">criterion</code><code class="p">(</code><code class="n">output</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>
        <code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">()</code>

<code class="c1"># Evaluate neural network</code>
<code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
    <code class="n">output</code> <code class="o">=</code> <code class="n">network</code><code class="p">(</code><code class="n">x_test</code><code class="p">)</code>
    <code class="n">test_loss</code> <code class="o">=</code> <code class="n">criterion</code><code class="p">(</code><code class="n">output</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>
    <code class="n">test_accuracy</code> <code class="o">=</code> <code class="p">(</code><code class="n">output</code><code class="o">.</code><code class="n">round</code><code class="p">()</code> <code class="o">==</code> <code class="n">y_test</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"Test Loss:"</code><code class="p">,</code> <code class="n">test_loss</code><code class="o">.</code><code class="n">item</code><code class="p">(),</code> <code class="s2">"</code><code class="se">\t</code><code class="s2">Test Accuracy:"</code><code class="p">,</code>
        <code class="n">test_accuracy</code><code class="o">.</code><code class="n">item</code><code class="p">())</code></pre>
<pre data-type="programlisting">Test Loss: 0.4030887186527252 	Test Accuracy: 0.9599999785423279</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id421">
<h2>Discussion</h2>
<p>One strategy to combat overfitting neural networks is by penalizing the
parameters (i.e., weights) of the neural network such that they are
driven to be small values, creating a simpler model less prone to
overfit. This method is called weight regularization or weight decay.
More specifically, in weight regularization a penalty is added to the
loss function, such as the L2 norm.</p>
<p>In PyTorch, we can add weight regularization by including <code>weight_decay=1e-5</code> in the optimizer where regularization happens. In this example, <code>1e-5</code> determines how much we penalize
higher parameter values. Values greater than 0 indicate L2 regularization in PyTorch.<a data-primary="" data-startref="ix_nn_weight_reg" data-type="indexterm" id="id2024"/><a data-primary="" data-startref="ix_overfit_weight_reg" data-type="indexterm" id="id2025"/><a data-primary="" data-startref="ix_reg_red_var_nn" data-type="indexterm" id="id2026"/><a data-primary="" data-startref="ix_weight_reg" data-type="indexterm" id="id2027"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="21.10 Reducing Overfitting with Early Stopping" data-type="sect1"><div class="sect1" id="reducing-overfitting-with-early-stopping">
<h1>21.10 Reducing Overfitting with Early Stopping</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id422">
<h2>Problem</h2>
<p>You want to <a data-primary="neural networks" data-secondary="early stopping for reducing overfitting" data-type="indexterm" id="ix_nn_early_stop_reg"/><a data-primary="overfitting of data" data-secondary="early stopping to reduce" data-type="indexterm" id="ix_overfit_data_early_stop"/><a data-primary="early stopping, reducing overfitting with" data-type="indexterm" id="ix_early_stop_reg"/>reduce overfitting by stopping training when your train and test scores diverge.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id423">
<h2>Solution</h2>
<p>Use <a data-primary="PyTorch Lightning" data-type="indexterm" id="ix_pyt_lightning"/>PyTorch Lightning to implement a strategy called <em>early stopping</em>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import libraries</code>
<code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">torch.utils.data</code> <code class="kn">import</code> <code class="n">DataLoader</code><code class="p">,</code> <code class="n">TensorDataset</code>
<code class="kn">from</code> <code class="nn">torch.optim</code> <code class="kn">import</code> <code class="n">RMSprop</code>
<code class="kn">import</code> <code class="nn">lightning</code> <code class="k">as</code> <code class="nn">pl</code>
<code class="kn">from</code> <code class="nn">lightning.pytorch.callbacks.early_stopping</code> <code class="kn">import</code> <code class="n">EarlyStopping</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_classification</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>

<code class="c1"># Create training and test sets</code>
<code class="n">features</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="n">make_classification</code><code class="p">(</code><code class="n">n_classes</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">n_features</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code>
    <code class="n">n_samples</code><code class="o">=</code><code class="mi">1000</code><code class="p">)</code>
<code class="n">features_train</code><code class="p">,</code> <code class="n">features_test</code><code class="p">,</code> <code class="n">target_train</code><code class="p">,</code> <code class="n">target_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Set random seed</code>
<code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
<code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Convert data to PyTorch tensors</code>
<code class="n">x_train</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">features_train</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>
<code class="n">y_train</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">target_train</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
<code class="n">x_test</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">features_test</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>
<code class="n">y_test</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">target_test</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Define a neural network using `Sequential`</code>
<code class="k">class</code> <code class="nc">SimpleNeuralNet</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">SimpleNeuralNet</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">sequential</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">16</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code><code class="mi">16</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Sigmoid</code><code class="p">()</code>
        <code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">sequential</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">x</code>

<code class="k">class</code> <code class="nc">LightningNetwork</code><code class="p">(</code><code class="n">pl</code><code class="o">.</code><code class="n">LightningModule</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">network</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">network</code> <code class="o">=</code> <code class="n">network</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">criterion</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">BCELoss</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">metric</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">functional</code><code class="o">.</code><code class="n">binary_cross_entropy</code>

    <code class="k">def</code> <code class="nf">training_step</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">batch</code><code class="p">,</code> <code class="n">batch_idx</code><code class="p">):</code>
        <code class="c1"># training_step defines the train loop.</code>
        <code class="n">data</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="n">batch</code>
        <code class="n">output</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">network</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>
        <code class="n">loss</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">criterion</code><code class="p">(</code><code class="n">output</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="s2">"val_loss"</code><code class="p">,</code> <code class="n">loss</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">loss</code>

    <code class="k">def</code> <code class="nf">configure_optimizers</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="k">return</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">Adam</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">lr</code><code class="o">=</code><code class="mf">1e-3</code><code class="p">)</code>

<code class="c1"># Define data loader</code>
<code class="n">train_data</code> <code class="o">=</code> <code class="n">TensorDataset</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="n">train_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">train_data</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>

<code class="c1"># Initialize neural network</code>
<code class="n">network</code> <code class="o">=</code> <code class="n">LightningNetwork</code><code class="p">(</code><code class="n">SimpleNeuralNet</code><code class="p">())</code>

<code class="c1"># Train network</code>
<code class="n">trainer</code> <code class="o">=</code> <code class="n">pl</code><code class="o">.</code><code class="n">Trainer</code><code class="p">(</code><code class="n">callbacks</code><code class="o">=</code><code class="p">[</code><code class="n">EarlyStopping</code><code class="p">(</code><code class="n">monitor</code><code class="o">=</code><code class="s2">"val_loss"</code><code class="p">,</code> <code class="n">mode</code><code class="o">=</code><code class="s2">"min"</code><code class="p">,</code>
    <code class="n">patience</code><code class="o">=</code><code class="mi">3</code><code class="p">)],</code> <code class="n">max_epochs</code><code class="o">=</code><code class="mi">1000</code><code class="p">)</code>
<code class="n">trainer</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">model</code><code class="o">=</code><code class="n">network</code><code class="p">,</code> <code class="n">train_dataloaders</code><code class="o">=</code><code class="n">train_loader</code><code class="p">)</code></pre>
<pre data-type="programlisting">GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs

  | Name      | Type            | Params
----------------------------------------------
0 | network   | SimpleNeuralNet | 465
1 | criterion | BCELoss         | 0
----------------------------------------------
465       Trainable params
0         Non-trainable params
465       Total params
0.002     Total estimated model params size (MB)
/usr/local/lib/python3.10/site-packages/lightning/pytorch/trainer/
    connectors/data_connector.py:224: PossibleUserWarning:
    The dataloader, train_dataloader, does not have many workers which
    may be a bottleneck. Consider increasing the value of the `num_workers`
    argument (try 7 which is the number of cpus on this machine)
    in the `DataLoader` init to improve performance.
  rank_zero_warn(
/usr/local/lib/python3.10/site-packages/lightning/pytorch/trainer/
    trainer.py:1609: PossibleUserWarning: The number of training batches (9)
    is smaller than the logging interval Trainer(log_every_n_steps=50).
    Set a lower value for log_every_n_steps if you want to see logs
    for the training epoch.
  rank_zero_warn(
Epoch 23: 100%|███████████████| 9/9 [00:00&lt;00:00, 59.29it/s, loss=0.147, v_num=5]</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id424">
<h2>Discussion</h2>
<p>As we discussed in <a data-type="xref" href="#visualize-training-history">Recipe 21.8</a>, typically in the first several training epochs, both the training and test errors will decrease, but at some point the network will start “memorizing” the training data, causing the training error to continue to decrease even while the test error starts increasing. Because of this phenomenon, one of the most common and very effective methods to counter overfitting is to monitor the training process and stop training when the test error starts to increase. This strategy is called <em>early stopping</em>.</p>
<p>In PyTorch, we can <a data-primary="callback functions, for early stopping" data-type="indexterm" id="id2028"/>implement early stopping as a callback function. Callbacks are functions that can be applied at certain stages of the training process, such as at the end of each epoch. However, PyTorch itself does not define an early stopping class for you, so here we use the popular library <code>lightning</code> (known as PyTorch Lightning) to use an out-of-the-box one. PyTorch Lightning is a high-level library for PyTorch that provides a lot of useful features. In our solution, we included PyTorch Lightning’s <code>EarlyStopping(monitor="val_loss", mode="min", patience=3)</code> to define that we wanted to monitor the test (validation) loss at each epoch, and if the test loss has not improved after three epochs (the default), training is interrupted.</p>
<p>If we did not include the <code>EarlyStopping</code> callback, the model would train for the full 1,000 max epochs without stopping on its own:<a data-primary="" data-startref="ix_early_stop_reg" data-type="indexterm" id="id2029"/><a data-primary="" data-startref="ix_nn_early_stop_reg" data-type="indexterm" id="id2030"/><a data-primary="" data-startref="ix_overfit_data_early_stop" data-type="indexterm" id="id2031"/><a data-primary="" data-startref="ix_pyt_lightning" data-type="indexterm" id="id2032"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Train network</code>
<code class="n">trainer</code> <code class="o">=</code> <code class="n">pl</code><code class="o">.</code><code class="n">Trainer</code><code class="p">(</code><code class="n">max_epochs</code><code class="o">=</code><code class="mi">1000</code><code class="p">)</code>
<code class="n">trainer</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">model</code><code class="o">=</code><code class="n">network</code><code class="p">,</code> <code class="n">train_dataloaders</code><code class="o">=</code><code class="n">train_loader</code><code class="p">)</code></pre>
<pre data-type="programlisting">GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs

  | Name      | Type            | Params
----------------------------------------------
0 | network   | SimpleNeuralNet | 465
1 | criterion | BCELoss         | 0
----------------------------------------------
465       Trainable params
0         Non-trainable params
465       Total params
0.002     Total estimated model params size (MB)
Epoch 999: 100%|████████████| 9/9 [00:01&lt;00:00,  7.95it/s, loss=0.00188, v_num=6]
`Trainer.fit` stopped: `max_epochs=1000` reached.
Epoch 999: 100%|████████████| 9/9 [00:01&lt;00:00,  7.80it/s, loss=0.00188, v_num=6]</pre>
</div></section>
</div></section>
<section data-pdf-bookmark="21.11 Reducing Overfitting with Dropout" data-type="sect1"><div class="sect1" id="reducing-overfitting-with-dropout">
<h1>21.11 Reducing Overfitting with Dropout</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id567">
<h2>Problem</h2>
<p>You want to <a data-primary="neural networks" data-secondary="dropout for reducing overfitting" data-type="indexterm" id="ix_nn_dropout_red_over"/><a data-primary="overfitting of data" data-secondary="dropout to reduce" data-type="indexterm" id="ix_overfit_data_dropout"/>reduce overfitting.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id425">
<h2>Solution</h2>
<p>Introduce <a data-primary="dropout, reducing overfitting with" data-type="indexterm" id="ix_dropout_red_over"/>noise into your network’s architecture using dropout:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">torch.utils.data</code> <code class="kn">import</code> <code class="n">DataLoader</code><code class="p">,</code> <code class="n">TensorDataset</code>
<code class="kn">from</code> <code class="nn">torch.optim</code> <code class="kn">import</code> <code class="n">RMSprop</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_classification</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>

<code class="c1"># Create training and test sets</code>
<code class="n">features</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="n">make_classification</code><code class="p">(</code><code class="n">n_classes</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">n_features</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code>
    <code class="n">n_samples</code><code class="o">=</code><code class="mi">1000</code><code class="p">)</code>
<code class="n">features_train</code><code class="p">,</code> <code class="n">features_test</code><code class="p">,</code> <code class="n">target_train</code><code class="p">,</code> <code class="n">target_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Set random seed</code>
<code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
<code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Convert data to PyTorch tensors</code>
<code class="n">x_train</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">features_train</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>
<code class="n">y_train</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">target_train</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
<code class="n">x_test</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">features_test</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>
<code class="n">y_test</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">target_test</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Define a neural network using `Sequential`</code>
<code class="k">class</code> <code class="nc">SimpleNeuralNet</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">SimpleNeuralNet</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">sequential</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">16</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code><code class="mi">16</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="mf">0.1</code><code class="p">),</code> <code class="c1"># Drop 10% of neurons</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Sigmoid</code><code class="p">(),</code>
        <code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">sequential</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">x</code>

<code class="c1"># Initialize neural network</code>
<code class="n">network</code> <code class="o">=</code> <code class="n">SimpleNeuralNet</code><code class="p">()</code>

<code class="c1"># Define loss function, optimizer</code>
<code class="n">criterion</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">BCELoss</code><code class="p">()</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">RMSprop</code><code class="p">(</code><code class="n">network</code><code class="o">.</code><code class="n">parameters</code><code class="p">())</code>

<code class="c1"># Define data loader</code>
<code class="n">train_data</code> <code class="o">=</code> <code class="n">TensorDataset</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="n">train_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">train_data</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>

<code class="c1"># Compile the model using torch 2.0's optimizer</code>
<code class="n">network</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">network</code><code class="p">)</code>

<code class="c1"># Train neural network</code>
<code class="n">epochs</code> <code class="o">=</code> <code class="mi">3</code>
<code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">epochs</code><code class="p">):</code>
    <code class="k">for</code> <code class="n">batch_idx</code><code class="p">,</code> <code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">train_loader</code><code class="p">):</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code>
        <code class="n">output</code> <code class="o">=</code> <code class="n">network</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>
        <code class="n">loss</code> <code class="o">=</code> <code class="n">criterion</code><code class="p">(</code><code class="n">output</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>
        <code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">()</code>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"Epoch:"</code><code class="p">,</code> <code class="n">epoch</code><code class="o">+</code><code class="mi">1</code><code class="p">,</code> <code class="s2">"</code><code class="se">\t</code><code class="s2">Loss:"</code><code class="p">,</code> <code class="n">loss</code><code class="o">.</code><code class="n">item</code><code class="p">())</code>

<code class="c1"># Evaluate neural network</code>
<code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
    <code class="n">output</code> <code class="o">=</code> <code class="n">network</code><code class="p">(</code><code class="n">x_test</code><code class="p">)</code>
    <code class="n">test_loss</code> <code class="o">=</code> <code class="n">criterion</code><code class="p">(</code><code class="n">output</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>
    <code class="n">test_accuracy</code> <code class="o">=</code> <code class="p">(</code><code class="n">output</code><code class="o">.</code><code class="n">round</code><code class="p">()</code> <code class="o">==</code> <code class="n">y_test</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"Test Loss:"</code><code class="p">,</code> <code class="n">test_loss</code><code class="o">.</code><code class="n">item</code><code class="p">(),</code> <code class="s2">"</code><code class="se">\t</code><code class="s2">Test Accuracy:"</code><code class="p">,</code>
        <code class="n">test_accuracy</code><code class="o">.</code><code class="n">item</code><code class="p">())</code></pre>
<pre data-type="programlisting">Epoch: 1 	Loss: 0.18791493773460388
Epoch: 2 	Loss: 0.17331615090370178
Epoch: 3 	Loss: 0.1384529024362564
Test Loss: 0.12702330946922302 	Test Accuracy: 0.9100000262260437</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id426">
<h2>Discussion</h2>
<p><em>Dropout</em> is a fairly common method for regularizing smaller neural networks. In dropout, every time a batch of observations is created for training, a proportion of the units in one or more layers is multiplied by zero (i.e., dropped). In this setting, every batch is trained on the same network (e.g., the same parameters), but each batch is confronted by a slightly different version of that network’s <em>architecture</em>.</p>
<p>Dropout is thought to be effective because by constantly and randomly dropping units
in each batch, it forces units to learn parameter values able to perform under a wide variety of network architectures. That is, they learn to be robust to disruptions (i.e., noise) in the other hidden units, and this prevents the network from simply memorizing the training data.</p>
<p>It is possible to add dropout to both the hidden and input layers. When
an input layer is dropped, its feature value is not introduced into the
network for that batch.</p>
<p>In PyTorch, we can <a data-primary="Dropout layer" data-type="indexterm" id="id2033"/>implement dropout by adding an <code>nn.Dropout</code> layer into our
network architecture. Each <code>nn.Dropout</code> layer will drop a user-defined
hyperparameter of units in the previous layer every batch.<a data-primary="" data-startref="ix_fit_data_lin_nn" data-type="indexterm" id="id2034"/><a data-primary="" data-startref="ix_dropout_red_over" data-type="indexterm" id="id2035"/><a data-primary="" data-startref="ix_nn_dropout_red_over" data-type="indexterm" id="id2036"/><a data-primary="" data-startref="ix_overfit_data_dropout" data-type="indexterm" id="id2037"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="21.12 Saving Model Training Progress" data-type="sect1"><div class="sect1" id="saving-model-training-progress">
<h1>21.12 Saving Model Training Progress</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id427">
<h2>Problem</h2>
<p>Given a <a data-primary="neural networks" data-secondary="saving model training progress" data-type="indexterm" id="ix_nn_load_save_mod"/><a data-primary="saving and loading models" data-type="indexterm" id="ix_saved_model_nn"/><a data-primary="loading and saving models" data-type="indexterm" id="ix_load_save_mod"/>neural network that will take a long time to train, you want to
save your progress in case the training process is interrupted.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id428">
<h2>Solution</h2>
<p>Use the <code>torch.save</code> <a data-primary="torch.save function" data-type="indexterm" id="id2038"/>function to save the model after
every epoch:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">torch.utils.data</code> <code class="kn">import</code> <code class="n">DataLoader</code><code class="p">,</code> <code class="n">TensorDataset</code>
<code class="kn">from</code> <code class="nn">torch.optim</code> <code class="kn">import</code> <code class="n">RMSprop</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_classification</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>

<code class="c1"># Create training and test sets</code>
<code class="n">features</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="n">make_classification</code><code class="p">(</code><code class="n">n_classes</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">n_features</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code>
    <code class="n">n_samples</code><code class="o">=</code><code class="mi">1000</code><code class="p">)</code>
<code class="n">features_train</code><code class="p">,</code> <code class="n">features_test</code><code class="p">,</code> <code class="n">target_train</code><code class="p">,</code> <code class="n">target_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Set random seed</code>
<code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
<code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Convert data to PyTorch tensors</code>
<code class="n">x_train</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">features_train</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>
<code class="n">y_train</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">target_train</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
<code class="n">x_test</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">features_test</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>
<code class="n">y_test</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">target_test</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Define a neural network using `Sequential`</code>
<code class="k">class</code> <code class="nc">SimpleNeuralNet</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">SimpleNeuralNet</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">sequential</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">16</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code><code class="mi">16</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="mf">0.1</code><code class="p">),</code> <code class="c1"># Drop 10% of neurons</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Sigmoid</code><code class="p">(),</code>
        <code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">sequential</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">x</code>

<code class="c1"># Initialize neural network</code>
<code class="n">network</code> <code class="o">=</code> <code class="n">SimpleNeuralNet</code><code class="p">()</code>

<code class="c1"># Define loss function, optimizer</code>
<code class="n">criterion</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">BCELoss</code><code class="p">()</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">RMSprop</code><code class="p">(</code><code class="n">network</code><code class="o">.</code><code class="n">parameters</code><code class="p">())</code>

<code class="c1"># Define data loader</code>
<code class="n">train_data</code> <code class="o">=</code> <code class="n">TensorDataset</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="n">train_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">train_data</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>

<code class="c1"># Compile the model using torch 2.0's optimizer</code>
<code class="n">network</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">network</code><code class="p">)</code>

<code class="c1"># Train neural network</code>
<code class="n">epochs</code> <code class="o">=</code> <code class="mi">5</code>
<code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">epochs</code><code class="p">):</code>
    <code class="k">for</code> <code class="n">batch_idx</code><code class="p">,</code> <code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">train_loader</code><code class="p">):</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code>
        <code class="n">output</code> <code class="o">=</code> <code class="n">network</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>
        <code class="n">loss</code> <code class="o">=</code> <code class="n">criterion</code><code class="p">(</code><code class="n">output</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>
        <code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">()</code>
        <code class="c1"># Save the model at the end of every epoch</code>
        <code class="n">torch</code><code class="o">.</code><code class="n">save</code><code class="p">(</code>
            <code class="p">{</code>
                <code class="s1">'epoch'</code><code class="p">:</code> <code class="n">epoch</code><code class="p">,</code>
                <code class="s1">'model_state_dict'</code><code class="p">:</code> <code class="n">network</code><code class="o">.</code><code class="n">state_dict</code><code class="p">(),</code>
                <code class="s1">'optimizer_state_dict'</code><code class="p">:</code> <code class="n">optimizer</code><code class="o">.</code><code class="n">state_dict</code><code class="p">(),</code>
                <code class="s1">'loss'</code><code class="p">:</code> <code class="n">loss</code><code class="p">,</code>
            <code class="p">},</code>
            <code class="s2">"model.pt"</code>
        <code class="p">)</code>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"Epoch:"</code><code class="p">,</code> <code class="n">epoch</code><code class="o">+</code><code class="mi">1</code><code class="p">,</code> <code class="s2">"</code><code class="se">\t</code><code class="s2">Loss:"</code><code class="p">,</code> <code class="n">loss</code><code class="o">.</code><code class="n">item</code><code class="p">())</code></pre>
<pre data-type="programlisting">Epoch: 1 	Loss: 0.18791493773460388
Epoch: 2 	Loss: 0.17331615090370178
Epoch: 3 	Loss: 0.1384529024362564
Epoch: 4 	Loss: 0.1435958743095398
Epoch: 5 	Loss: 0.17967987060546875</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id429">
<h2>Discussion</h2>
<p>In the real world, it is common for neural networks to train for hours or even days. During that
time a lot can go wrong: computers can lose power, servers can crash, or inconsiderate graduate students can close your laptop.</p>
<p>We can use <code>torch.save</code> to alleviate this problem by saving the model after
every epoch. Specifically, after every epoch, we save a
model to the location <code>model.pt</code>, the second argument to the <code>torch.save</code> function. If we
include only a filename (e.g., <em>model.pt</em>) that file will be
overridden with the latest model every epoch.</p>
<p>As you can imagine, we can introduce additional logic to save the model every few epochs, only save a model if the loss goes down, etc. We could even combine this approach with the early stopping approach in PyTorch Lightning to ensure we save a model no matter at what epoch the training ends.<a data-primary="" data-startref="ix_load_save_mod" data-type="indexterm" id="id2039"/><a data-primary="" data-startref="ix_nn_load_save_mod" data-type="indexterm" id="id2040"/><a data-primary="" data-startref="ix_saved_model_nn" data-type="indexterm" id="id2041"/></p>
</div></section>
</div></section>
<section class="less_space pagebreak-before" data-pdf-bookmark="21.13 Tuning Neural Networks" data-type="sect1"><div class="sect1" id="tuning-neural-networks">
<h1>21.13 Tuning Neural Networks</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id568">
<h2>Problem</h2>
<p>You want to <a data-primary="neural networks" data-secondary="tuning hyperparameters for" data-type="indexterm" id="ix_nn_hyperp"/><a data-primary="hyperparameters" data-secondary="for neural networks" data-secondary-sortas="neural networks" data-type="indexterm" id="ix_hyperp_nn"/>automatically select the best hyperparameters for your
neural network.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id430">
<h2>Solution</h2>
<p>Use the <code>ray</code> <a data-primary="ray tuning library" data-type="indexterm" id="ix_ray_tune_lib"/>tuning library with PyTorch:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">functools</code> <code class="kn">import</code> <code class="n">partial</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">import</code> <code class="nn">os</code>
<code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>
<code class="kn">import</code> <code class="nn">torch.nn.functional</code> <code class="k">as</code> <code class="nn">F</code>
<code class="kn">import</code> <code class="nn">torch.optim</code> <code class="k">as</code> <code class="nn">optim</code>
<code class="kn">from</code> <code class="nn">torch.optim</code> <code class="kn">import</code> <code class="n">RMSprop</code>
<code class="kn">from</code> <code class="nn">torch.utils.data</code> <code class="kn">import</code> <code class="n">random_split</code><code class="p">,</code> <code class="n">DataLoader</code><code class="p">,</code> <code class="n">TensorDataset</code>
<code class="kn">from</code> <code class="nn">ray</code> <code class="kn">import</code> <code class="n">tune</code>
<code class="kn">from</code> <code class="nn">ray.tune</code> <code class="kn">import</code> <code class="n">CLIReporter</code>
<code class="kn">from</code> <code class="nn">ray.tune.schedulers</code> <code class="kn">import</code> <code class="n">ASHAScheduler</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_classification</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>

<code class="c1"># Create training and test sets</code>
<code class="n">features</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="n">make_classification</code><code class="p">(</code><code class="n">n_classes</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">n_features</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code>
    <code class="n">n_samples</code><code class="o">=</code><code class="mi">1000</code><code class="p">)</code>
<code class="n">features_train</code><code class="p">,</code> <code class="n">features_test</code><code class="p">,</code> <code class="n">target_train</code><code class="p">,</code> <code class="n">target_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Set random seed</code>
<code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
<code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Convert data to PyTorch tensors</code>
<code class="n">x_train</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">features_train</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>
<code class="n">y_train</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">target_train</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
<code class="n">x_test</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">features_test</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>
<code class="n">y_test</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">target_test</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Define a neural network using `Sequential`</code>
<code class="k">class</code> <code class="nc">SimpleNeuralNet</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">layer_size_1</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">layer_size_2</code><code class="o">=</code><code class="mi">10</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">SimpleNeuralNet</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">sequential</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="n">layer_size_1</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">layer_size_1</code><code class="p">,</code> <code class="n">layer_size_2</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">layer_size_2</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Sigmoid</code><code class="p">()</code>
        <code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">sequential</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">x</code>

<code class="n">config</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s2">"layer_size_1"</code><code class="p">:</code> <code class="n">tune</code><code class="o">.</code><code class="n">sample_from</code><code class="p">(</code><code class="k">lambda</code> <code class="n">_</code><code class="p">:</code> <code class="mi">2</code> <code class="o">**</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">randint</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">9</code><code class="p">)),</code>
    <code class="s2">"layer_size_2"</code><code class="p">:</code> <code class="n">tune</code><code class="o">.</code><code class="n">sample_from</code><code class="p">(</code><code class="k">lambda</code> <code class="n">_</code><code class="p">:</code> <code class="mi">2</code> <code class="o">**</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">randint</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">9</code><code class="p">)),</code>
    <code class="s2">"lr"</code><code class="p">:</code> <code class="n">tune</code><code class="o">.</code><code class="n">loguniform</code><code class="p">(</code><code class="mf">1e-4</code><code class="p">,</code> <code class="mf">1e-1</code><code class="p">),</code>
<code class="p">}</code>

<code class="n">scheduler</code> <code class="o">=</code> <code class="n">ASHAScheduler</code><code class="p">(</code>
    <code class="n">metric</code><code class="o">=</code><code class="s2">"loss"</code><code class="p">,</code>
    <code class="n">mode</code><code class="o">=</code><code class="s2">"min"</code><code class="p">,</code>
    <code class="n">max_t</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code>
    <code class="n">grace_period</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>
    <code class="n">reduction_factor</code><code class="o">=</code><code class="mi">2</code>
<code class="p">)</code>

<code class="n">reporter</code> <code class="o">=</code> <code class="n">CLIReporter</code><code class="p">(</code>
    <code class="n">parameter_columns</code><code class="o">=</code><code class="p">[</code><code class="s2">"layer_size_1"</code><code class="p">,</code> <code class="s2">"layer_size_2"</code><code class="p">,</code> <code class="s2">"lr"</code><code class="p">],</code>
    <code class="n">metric_columns</code><code class="o">=</code><code class="p">[</code><code class="s2">"loss"</code><code class="p">]</code>
<code class="p">)</code>

<code class="c1"># # Train neural network</code>
<code class="k">def</code> <code class="nf">train_model</code><code class="p">(</code><code class="n">config</code><code class="p">,</code> <code class="n">epochs</code><code class="o">=</code><code class="mi">3</code><code class="p">):</code>
    <code class="n">network</code> <code class="o">=</code> <code class="n">SimpleNeuralNet</code><code class="p">(</code><code class="n">config</code><code class="p">[</code><code class="s2">"layer_size_1"</code><code class="p">],</code> <code class="n">config</code><code class="p">[</code><code class="s2">"layer_size_2"</code><code class="p">])</code>

    <code class="n">criterion</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">BCELoss</code><code class="p">()</code>
    <code class="n">optimizer</code> <code class="o">=</code> <code class="n">optim</code><code class="o">.</code><code class="n">SGD</code><code class="p">(</code><code class="n">network</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">lr</code><code class="o">=</code><code class="n">config</code><code class="p">[</code><code class="s2">"lr"</code><code class="p">],</code> <code class="n">momentum</code><code class="o">=</code><code class="mf">0.9</code><code class="p">)</code>

    <code class="n">train_data</code> <code class="o">=</code> <code class="n">TensorDataset</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
    <code class="n">train_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">train_data</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>

    <code class="c1"># Compile the model using torch 2.0's optimizer</code>
    <code class="n">network</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">network</code><code class="p">)</code>

    <code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">epochs</code><code class="p">):</code>
        <code class="k">for</code> <code class="n">batch_idx</code><code class="p">,</code> <code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">train_loader</code><code class="p">):</code>
            <code class="n">optimizer</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code>
            <code class="n">output</code> <code class="o">=</code> <code class="n">network</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>
            <code class="n">loss</code> <code class="o">=</code> <code class="n">criterion</code><code class="p">(</code><code class="n">output</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>
            <code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
            <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">()</code>
            <code class="n">tune</code><code class="o">.</code><code class="n">report</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="p">(</code><code class="n">loss</code><code class="o">.</code><code class="n">item</code><code class="p">()))</code>

<code class="n">result</code> <code class="o">=</code> <code class="n">tune</code><code class="o">.</code><code class="n">run</code><code class="p">(</code>
    <code class="n">train_model</code><code class="p">,</code>
    <code class="n">resources_per_trial</code><code class="o">=</code><code class="p">{</code><code class="s2">"cpu"</code><code class="p">:</code> <code class="mi">2</code><code class="p">},</code>
    <code class="n">config</code><code class="o">=</code><code class="n">config</code><code class="p">,</code>
    <code class="n">num_samples</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>
    <code class="n">scheduler</code><code class="o">=</code><code class="n">scheduler</code><code class="p">,</code>
    <code class="n">progress_reporter</code><code class="o">=</code><code class="n">reporter</code>
<code class="p">)</code>

<code class="n">best_trial</code> <code class="o">=</code> <code class="n">result</code><code class="o">.</code><code class="n">get_best_trial</code><code class="p">(</code><code class="s2">"loss"</code><code class="p">,</code> <code class="s2">"min"</code><code class="p">,</code> <code class="s2">"last"</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Best trial config: </code><code class="si">{}</code><code class="s2">"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">best_trial</code><code class="o">.</code><code class="n">config</code><code class="p">))</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Best trial final validation loss: </code><code class="si">{}</code><code class="s2">"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code>
    <code class="n">best_trial</code><code class="o">.</code><code class="n">last_result</code><code class="p">[</code><code class="s2">"loss"</code><code class="p">]))</code>

<code class="n">best_trained_model</code> <code class="o">=</code> <code class="n">SimpleNeuralNet</code><code class="p">(</code><code class="n">best_trial</code><code class="o">.</code><code class="n">config</code><code class="p">[</code><code class="s2">"layer_size_1"</code><code class="p">],</code>
    <code class="n">best_trial</code><code class="o">.</code><code class="n">config</code><code class="p">[</code><code class="s2">"layer_size_2"</code><code class="p">])</code></pre>
<pre data-type="programlisting">== Status ==
Current time: 2023-03-05 23:31:33 (running for 00:00:00.07)
Memory usage on this node: 1.7/15.6 GiB
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 512.000: None | Iter 256.000: None | Iter 128.000: None |
    Iter 64.000: None | Iter 32.000: None | Iter 16.000: None |
    Iter 8.000: None | Iter 4.000: None | Iter 2.000: None |
    Iter 1.000: None
Resources requested: 2.0/7 CPUs, 0/0 GPUs, 0.0/8.95 GiB heap,
    0.0/4.48 GiB objects
Result logdir: /root/ray_results/train_model_2023-03-05_23-31-33
Number of trials: 1/1 (1 RUNNING)
...</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id431">
<h2>Discussion</h2>
<p>In Recipes <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch12.xhtml#selecting-best-models-using-exhaustive-search">12.1</a> and
<a data-type="xref" data-xrefstyle="select:labelnumber" href="ch12.xhtml#selecting-best-models-using-randomized-search">12.2</a>, we covered using scikit-learn’s model selection
techniques to identify the best hyperparameters of a scikit-learn model. While in general the scikit-learn approach can also be applied to neural networks, the <code>ray</code> tuning library provides a sophisticated API that allows you to schedule experiments on both CPUs and GPUs.</p>
<p>The hyperparameters of a model <em>are</em> important and should be selected
with care. However, running experiments to select hyperparameters can be both cost and time prohibitive. Therefore, automatic hyperparameter tuning of neural networks is not the silver bullet, but it is a useful tool to have in certain circumstances.</p>
<p>In our solution we conducted a search of different parameters for layer sizes and the learning rate of our optimizer. The <code>best_trial.config</code> shows the parameters in our <code>ray</code> tuning configuration that led to the lowest loss and best experiment outcome.<a data-primary="" data-startref="ix_neural_net_ch21" data-type="indexterm" id="id2042"/><a data-primary="" data-startref="ix_hyperp_nn" data-type="indexterm" id="id2043"/><a data-primary="" data-startref="ix_nn_hyperp" data-type="indexterm" id="id2044"/><a data-primary="" data-startref="ix_ray_tune_lib" data-type="indexterm" id="id2045"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="21.14 Visualizing Neural Networks" data-type="sect1"><div class="sect1" id="visualizing-neural-networks">
<h1>21.14 Visualizing Neural Networks</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id569">
<h2>Problem</h2>
<p>You want to <a data-primary="visualization" data-secondary="neural networks" data-type="indexterm" id="id2046"/><a data-primary="neural networks" data-secondary="visualizing" data-type="indexterm" id="id2047"/>quickly visualize a neural network’s architecture.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id432">
<h2>Solution</h2>
<p>Use the <code>make_dot</code> <a data-primary="torch_viz library" data-type="indexterm" id="id2048"/><a data-primary="make_dot function, torch_viz" data-type="indexterm" id="id2049"/>function from <code>torch_viz</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">torch.utils.data</code> <code class="kn">import</code> <code class="n">DataLoader</code><code class="p">,</code> <code class="n">TensorDataset</code>
<code class="kn">from</code> <code class="nn">torch.optim</code> <code class="kn">import</code> <code class="n">RMSprop</code>
<code class="kn">from</code> <code class="nn">torchviz</code> <code class="kn">import</code> <code class="n">make_dot</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_classification</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>

<code class="c1"># Create training and test sets</code>
<code class="n">features</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="n">make_classification</code><code class="p">(</code><code class="n">n_classes</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">n_features</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code>
    <code class="n">n_samples</code><code class="o">=</code><code class="mi">1000</code><code class="p">)</code>
<code class="n">features_train</code><code class="p">,</code> <code class="n">features_test</code><code class="p">,</code> <code class="n">target_train</code><code class="p">,</code> <code class="n">target_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Set random seed</code>
<code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
<code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Convert data to PyTorch tensors</code>
<code class="n">x_train</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">features_train</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>
<code class="n">y_train</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">target_train</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
<code class="n">x_test</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">features_test</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code>
<code class="n">y_test</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">target_test</code><code class="p">)</code><code class="o">.</code><code class="n">float</code><code class="p">()</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Define a neural network using Sequential</code>
<code class="k">class</code> <code class="nc">SimpleNeuralNet</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">SimpleNeuralNet</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">sequential</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">16</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code><code class="mi">16</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code>
            <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Sigmoid</code><code class="p">()</code>
        <code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">sequential</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">x</code>

<code class="c1"># Initialize neural network</code>
<code class="n">network</code> <code class="o">=</code> <code class="n">SimpleNeuralNet</code><code class="p">()</code>

<code class="c1"># Define loss function, optimizer</code>
<code class="n">criterion</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">BCELoss</code><code class="p">()</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">RMSprop</code><code class="p">(</code><code class="n">network</code><code class="o">.</code><code class="n">parameters</code><code class="p">())</code>

<code class="c1"># Define data loader</code>
<code class="n">train_data</code> <code class="o">=</code> <code class="n">TensorDataset</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="n">train_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">train_data</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>

<code class="c1"># Compile the model using torch 2.0's optimizer</code>
<code class="n">network</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">network</code><code class="p">)</code>

<code class="c1"># Train neural network</code>
<code class="n">epochs</code> <code class="o">=</code> <code class="mi">3</code>
<code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">epochs</code><code class="p">):</code>
    <code class="k">for</code> <code class="n">batch_idx</code><code class="p">,</code> <code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">train_loader</code><code class="p">):</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code>
        <code class="n">output</code> <code class="o">=</code> <code class="n">network</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>
        <code class="n">loss</code> <code class="o">=</code> <code class="n">criterion</code><code class="p">(</code><code class="n">output</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>
        <code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">()</code>

<code class="n">make_dot</code><code class="p">(</code><code class="n">output</code><code class="o">.</code><code class="n">detach</code><code class="p">(),</code> <code class="n">params</code><code class="o">=</code><code class="nb">dict</code><code class="p">(</code>
    <code class="nb">list</code><code class="p">(</code>
        <code class="n">network</code><code class="o">.</code><code class="n">named_parameters</code><code class="p">()</code>
        <code class="p">)</code>
      <code class="p">)</code>
    <code class="p">)</code><code class="o">.</code><code class="n">render</code><code class="p">(</code>
        <code class="s2">"simple_neural_network"</code><code class="p">,</code>
        <code class="nb">format</code><code class="o">=</code><code class="s2">"png"</code>
<code class="p">)</code></pre>
<pre data-type="programlisting">'simple_neural_network.png'</pre>
<p>If we open the image that was saved to our machine, we can see the following:</p>
<figure><div class="figure">
<img alt="mpc2 21in02" height="741" src="assets/mpc2_21in02.png" width="600"/>
<h6/>
</div></figure>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id2050">
<h2>Discussion</h2>
<p>The <code>torchviz</code> library provides easy utility functions to quickly visualize our neural networks and write them out as images.</p>
</div></section>
</div></section>
</div></section></div></body></html>