["```py\n# Create text\ntext_data = [\"   Interrobang. By Aishwarya Henriette     \",\n             \"Parking And Going. By Karl Gautier\",\n             \"    Today Is The night. By Jarek Prakash   \"]\n\n# Strip whitespaces\nstrip_whitespace = [string.strip() for string in text_data]\n\n# Show text\nstrip_whitespace\n```", "```py\n['Interrobang. By Aishwarya Henriette',\n 'Parking And Going. By Karl Gautier',\n 'Today Is The night. By Jarek Prakash']\n```", "```py\n# Remove periods\nremove_periods = [string.replace(\".\", \"\") for string in strip_whitespace]\n\n# Show text\nremove_periods\n```", "```py\n['Interrobang By Aishwarya Henriette',\n 'Parking And Going By Karl Gautier',\n 'Today Is The night By Jarek Prakash']\n```", "```py\n# Create function\ndef capitalizer(string: str) -> str:\n    return string.upper()\n\n# Apply function\n[capitalizer(string) for string in remove_periods]\n```", "```py\n['INTERROBANG BY AISHWARYA HENRIETTE',\n 'PARKING AND GOING BY KARL GAUTIER',\n 'TODAY IS THE NIGHT BY JAREK PRAKASH']\n```", "```py\n# Import library\nimport re\n\n# Create function\ndef replace_letters_with_X(string: str) -> str:\n    return re.sub(r\"[a-zA-Z]\", \"X\", string)\n\n# Apply function\n[replace_letters_with_X(string) for string in remove_periods]\n```", "```py\n['XXXXXXXXXXX XX XXXXXXXXX XXXXXXXXX',\n 'XXXXXXX XXX XXXXX XX XXXX XXXXXXX',\n 'XXXXX XX XXX XXXXX XX XXXXX XXXXXXX']\n```", "```py\n# Define a string\ns = \"machine learning in python cookbook\"\n\n# Find the first index of the letter \"n\"\nfind_n = s.find(\"n\")\n\n# Whether or not the string starts with \"m\"\nstarts_with_m = s.startswith(\"m\")\n\n# Whether or not the string ends with \"python\"\nends_with_python = s.endswith(\"python\")\n\n# Is the string alphanumeric\nis_alnum = s.isalnum()\n\n# Is it composed of only alphabetical characters (not including spaces)\nis_alpha = s.isalpha()\n\n# Encode as utf-8\nencode_as_utf8 = s.encode(\"utf-8\")\n\n# Decode the same utf-8\ndecode = encode_as_utf8.decode(\"utf-8\")\n\nprint(\n  find_n,\n  starts_with_m,\n  ends_with_python,\n  is_alnum,\n  is_alpha,\n  encode_as_utf8,\n  decode,\n  sep = \"|\"\n)\n```", "```py\n5|True|False|False|False|b'machine learning in python cookbook'|machine learning\n  in python cookbook\n```", "```py\n# Load library\nfrom bs4 import BeautifulSoup\n\n# Create some HTML code\nhtml = \"<div class='full_name'>\"\\\n       \"<span style='font-weight:bold'>Masego\"\\\n       \"</span> Azra</div>\"\n\n# Parse html\nsoup = BeautifulSoup(html, \"lxml\")\n\n# Find the div with the class \"full_name\", show text\nsoup.find(\"div\", { \"class\" : \"full_name\" }).text\n```", "```py\n'Masego Azra'\n```", "```py\n# Load libraries\nimport unicodedata\nimport sys\n\n# Create text\ntext_data = ['Hi!!!! I. Love. This. Song....',\n             '10000% Agree!!!! #LoveIT',\n             'Right?!?!']\n\n# Create a dictionary of punctuation characters\npunctuation = dict.fromkeys(\n  (i for i in range(sys.maxunicode)\n  if unicodedata.category(chr(i)).startswith('P')\n  ),\n  None\n)\n\n# For each string, remove any punctuation characters\n[string.translate(punctuation) for string in text_data]\n```", "```py\n['Hi I Love This Song', '10000 Agree LoveIT', 'Right']\n```", "```py\n# Load library\nfrom nltk.tokenize import word_tokenize\n\n# Create text\nstring = \"The science of today is the technology of tomorrow\"\n\n# Tokenize words\nword_tokenize(string)\n```", "```py\n['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrow']\n```", "```py\n# Load library\nfrom nltk.tokenize import sent_tokenize\n\n# Create text\nstring = \"The science of today is the technology of tomorrow. Tomorrow is today.\"\n\n# Tokenize sentences\nsent_tokenize(string)\n```", "```py\n['The science of today is the technology of tomorrow.', 'Tomorrow is today.']\n```", "```py\n# Load library\nfrom nltk.corpus import stopwords\n\n# You will have to download the set of stop words the first time\n# import nltk\n# nltk.download('stopwords')\n\n# Create word tokens\ntokenized_words = ['i',\n                   'am',\n                   'going',\n                   'to',\n                   'go',\n                   'to',\n                   'the',\n                   'store',\n                   'and',\n                   'park']\n\n# Load stop words\nstop_words = stopwords.words('english')\n\n# Remove stop words\n[word for word in tokenized_words if word not in stop_words]\n```", "```py\n['going', 'go', 'store', 'park']\n```", "```py\n# Show stop words\nstop_words[:5]\n```", "```py\n['i', 'me', 'my', 'myself', 'we']\n```", "```py\n# Load library\nfrom nltk.stem.porter import PorterStemmer\n\n# Create word tokens\ntokenized_words = ['i', 'am', 'humbled', 'by', 'this', 'traditional', 'meeting']\n\n# Create stemmer\nporter = PorterStemmer()\n\n# Apply stemmer\n[porter.stem(word) for word in tokenized_words]\n```", "```py\n['i', 'am', 'humbl', 'by', 'thi', 'tradit', 'meet']\n```", "```py\n# Load libraries\nfrom nltk import pos_tag\nfrom nltk import word_tokenize\n\n# Create text\ntext_data = \"Chris loved outdoor running\"\n\n# Use pretrained part of speech tagger\ntext_tagged = pos_tag(word_tokenize(text_data))\n\n# Show parts of speech\ntext_tagged\n```", "```py\n[('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]\n```", "```py\n# Filter words\n[word for word, tag in text_tagged if tag in ['NN','NNS','NNP','NNPS'] ]\n```", "```py\n['Chris']\n```", "```py\n# Import libraries\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\n# Create text\ntweets = [\"I am eating a burrito for breakfast\",\n          \"Political science is an amazing field\",\n          \"San Francisco is an awesome city\"]\n\n# Create list\ntagged_tweets = []\n\n# Tag each word and each tweet\nfor tweet in tweets:\n    tweet_tag = nltk.pos_tag(word_tokenize(tweet))\n    tagged_tweets.append([tag for word, tag in tweet_tag])\n\n# Use one-hot encoding to convert the tags into features\none_hot_multi = MultiLabelBinarizer()\none_hot_multi.fit_transform(tagged_tweets)\n```", "```py\narray([[1, 1, 0, 1, 0, 1, 1, 1, 0],\n       [1, 0, 1, 1, 0, 0, 0, 0, 1],\n       [1, 0, 1, 1, 1, 0, 0, 0, 1]])\n```", "```py\n# Show feature names\none_hot_multi.classes_\n```", "```py\narray(['DT', 'IN', 'JJ', 'NN', 'NNP', 'PRP', 'VBG', 'VBP', 'VBZ'], dtype=object)\n```", "```py\n# Import libraries\nimport spacy\n\n# Load the spaCy package and use it to parse the text\n# make sure you have run \"python -m spacy download en\"\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"Elon Musk offered to buy Twitter using $21B of his own money.\")\n\n# Print each entity\nprint(doc.ents)\n\n# For each entity print the text and the entity label\nfor entity in doc.ents:\n    print(entity.text, entity.label_, sep=\",\")\n```", "```py\n(Elon Musk, Twitter, 21B)\nElon Musk, PERSON\nTwitter, ORG\n21B, MONEY\n```", "```py\n# Load library\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Create text\ntext_data = np.array(['I love Brazil. Brazil!',\n                      'Sweden is best',\n                      'Germany beats both'])\n\n# Create the bag of words feature matrix\ncount = CountVectorizer()\nbag_of_words = count.fit_transform(text_data)\n\n# Show feature matrix\nbag_of_words\n```", "```py\n<3x8 sparse matrix of type '<class 'numpy.int64'>'\n    with 8 stored elements in Compressed Sparse Row format>\n```", "```py\nbag_of_words.toarray()\n```", "```py\narray([[0, 0, 0, 2, 0, 0, 1, 0],\n       [0, 1, 0, 0, 0, 1, 0, 1],\n       [1, 0, 1, 0, 1, 0, 0, 0]], dtype=int64)\n```", "```py\n# Show feature names\ncount.get_feature_names_out()\n```", "```py\narray(['beats', 'best', 'both', 'brazil', 'germany', 'is', 'love',\n       'sweden'], dtype=object)\n```", "```py\n# Create feature matrix with arguments\ncount_2gram = CountVectorizer(ngram_range=(1,2),\n                              stop_words=\"english\",\n                              vocabulary=['brazil'])\nbag = count_2gram.fit_transform(text_data)\n\n# View feature matrix\nbag.toarray()\n```", "```py\narray([[2],\n       [0],\n       [0]])\n```", "```py\n# View the 1-grams and 2-grams\ncount_2gram.vocabulary_\n```", "```py\n{'brazil': 0}\n```", "```py\n# Load libraries\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Create text\ntext_data = np.array(['I love Brazil. Brazil!',\n                      'Sweden is best',\n                      'Germany beats both'])\n\n# Create the tf-idf feature matrix\ntfidf = TfidfVectorizer()\nfeature_matrix = tfidf.fit_transform(text_data)\n\n# Show tf-idf feature matrix\nfeature_matrix\n```", "```py\n<3x8 sparse matrix of type '<class 'numpy.float64'>'\n    with 8 stored elements in Compressed Sparse Row format>\n```", "```py\n# Show tf-idf feature matrix as dense matrix\nfeature_matrix.toarray()\n```", "```py\narray([[ 0\\.        ,  0\\.        ,  0\\.        ,  0.89442719,  0\\.        ,\n         0\\.        ,  0.4472136 ,  0\\.        ],\n       [ 0\\.        ,  0.57735027,  0\\.        ,  0\\.        ,  0\\.        ,\n         0.57735027,  0\\.        ,  0.57735027],\n       [ 0.57735027,  0\\.        ,  0.57735027,  0\\.        ,  0.57735027,\n         0\\.        ,  0\\.        ,  0\\.        ]])\n```", "```py\n# Show feature names\ntfidf.vocabulary_\n```", "```py\n{'love': 6,\n 'brazil': 3,\n 'sweden': 7,\n 'is': 5,\n 'best': 1,\n 'germany': 4,\n 'beats': 0,\n 'both': 2}\n```", "```py\n# Load libraries\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\n\n# Create searchable text data\ntext_data = np.array(['I love Brazil. Brazil!',\n                      'Sweden is best',\n                      'Germany beats both'])\n\n# Create the tf-idf feature matrix\ntfidf = TfidfVectorizer()\nfeature_matrix = tfidf.fit_transform(text_data)\n\n# Create a search query and transform it into a tf-idf vector\ntext = \"Brazil is the best\"\nvector = tfidf.transform([text])\n\n# Calculate the cosine similarities between the input vector and all other\n  vectors\ncosine_similarities = linear_kernel(vector, feature_matrix).flatten()\n\n# Get the index of the most relevent items in order\nrelated_doc_indicies = cosine_similarities.argsort()[:-10:-1]\n\n# Print the most similar texts to the search query along with the cosine\n  similarity\nprint([(text_data[i], cosine_similarities[i]) for i in related_doc_indicies])\n```", "```py\n[\n  (\n    'Sweden is best', 0.6666666666666666),\n    ('I love Brazil. Brazil!', 0.5163977794943222),\n    ('Germany beats both', 0.0\n    )\n]\n```", "```py\n# Import libraries\nfrom transformers import pipeline\n\n# Create an NLP pipeline that runs sentiment analysis\nclassifier = pipeline(\"sentiment-analysis\")\n\n# Classify some text\n# (this may download some data and models the first time you run it)\nsentiment_1 = classifier(\"I hate machine learning! It's the absolute worst.\")\nsentiment_2 = classifier(\n    \"Machine learning is the absolute\"\n    \"bees knees I love it so much!\"\n)\n\n# Print sentiment output\nprint(sentiment_1, sentiment_2)\n```", "```py\n[\n  {\n    'label': 'NEGATIVE',\n    'score': 0.9998020529747009\n  }\n]\n[\n  {\n    'label': 'POSITIVE',\n    'score': 0.9990628957748413\n  }\n]\n```"]