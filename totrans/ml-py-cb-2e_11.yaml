- en: Chapter 11\. Model Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 11.0 Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we will examine strategies for evaluating the quality of models
    created through our learning algorithms. It might appear strange to discuss model
    evaluation before discussing how to create them, but there is a method to our
    madness. Models are only as useful as the quality of their predictions, and thus,
    fundamentally, our goal is not to create models (which is easy) but to create
    high-quality models (which is hard). Therefore, before we explore the myriad learning
    algorithms, let’s first learn how we can evaluate the models they produce.
  prefs: []
  type: TYPE_NORMAL
- en: 11.1 Cross-Validating Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to evaluate how well your classification model generalizes to unforeseen
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Create a pipeline that preprocesses the data, trains the model, and then evaluates
    it using cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At first consideration, evaluating supervised-learning models might appear
    straightforward: train a model and then calculate how well it did using some performance
    metric (accuracy, squared errors, etc.). However, this approach is fundamentally
    flawed. If we train a model using our data, and then evaluate how well it did
    on that data, we are not achieving our desired goal. Our goal is not to evaluate
    how well the model does on our training data, but how well it does on data it
    has never seen before (e.g., a new customer, a new crime, a new image). For this
    reason, our method of evaluation should help us understand how well models are
    able to make predictions from data they have never seen before.'
  prefs: []
  type: TYPE_NORMAL
- en: One strategy might be to hold off a slice of data for testing. This is called
    *validation* (or *hold-out*). In validation, our observations (features and targets)
    are split into two sets, traditionally called the *training set* and the *test
    set*. We take the test set and put it off to the side, pretending that we have
    never seen it before. Next we train our model using our training set, using the
    features and target vector to teach the model how to make the best prediction.
    Finally, we simulate having never-before-seen external data by evaluating how
    our model performs on our test set. However, the validation approach has two major
    weaknesses. First, the performance of the model can be highly dependent on which
    few observations were selected for the test set. Second, the model is not being
    trained using all the available data, and it’s not being evaluated on all the
    available data.
  prefs: []
  type: TYPE_NORMAL
- en: A better strategy, which overcomes these weaknesses, is called *k-fold cross-validation*
    (KFCV). In KFCV, we split the data into *k* parts called *folds*. The model is
    then trained using *k – 1* folds—​combined into one training set—​and then the
    last fold is used as a test set. We repeat this *k* times, each time using a different
    fold as the test set. The performance on the model for each of the *k* iterations
    is then averaged to produce an overall measurement.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our solution, we conducted k-fold cross-validation using five folds and
    outputted the evaluation scores to `cv_results`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: There are three important points to consider when we are using KFCV. First,
    KFCV assumes that each observation was created independently from the other (i.e.,
    the data is independent and identically distributed [IID]). If the data is IID,
    it is a good idea to shuffle observations when assigning to folds. In scikit-learn
    we can set `shuffle=True` to perform shuffling.
  prefs: []
  type: TYPE_NORMAL
- en: Second, when we are using KFCV to evaluate a classifier, it is often beneficial
    to have folds containing roughly the same percentage of observations from each
    of the different target classes (called *stratified k-fold*). For example, if
    our target vector contained gender and 80% of the observations were male, then
    each fold would contain 80% male and 20% female observations. In scikit-learn,
    we can conduct stratified k-fold cross-validation by replacing the `KFold` class
    with `StratifiedKFold`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, when we are using validation sets or cross-validation, it is important
    to preprocess data based on the training set and then apply those transformations
    to both the training and test set. For example, when we `fit` our standardization
    object, `standardizer`, we calculate the mean and variance of only the training
    set. Then we apply that transformation (using `transform`) to both the training
    and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The reason for this is because we are pretending that the test set is unknown
    data. If we fit both our preprocessors using observations from both training and
    test sets, some of the information from the test set leaks into our training set.
    This rule applies for any preprocessing step such as feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: 'scikit-learn’s `pipeline` package makes this easy to do while using cross-validation
    techniques. We first create a pipeline that preprocesses the data (e.g., `standardizer`)
    and then trains a model (logistic regression, `logit`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we run KFCV using that pipeline and scikit does all the work for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`cross_val_score` comes with three parameters we have not discussed, but that
    are worth noting:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cv`'
  prefs: []
  type: TYPE_NORMAL
- en: '`cv` determines our cross-validation technique. K-fold is the most common by
    far, but there are others, such as leave-one-out cross-validation where the number
    of folds *k* equals the number of data points in the set.'
  prefs: []
  type: TYPE_NORMAL
- en: '`scoring`'
  prefs: []
  type: TYPE_NORMAL
- en: '`scoring` defines the metric for success, a number of which are discussed in
    other recipes in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_jobs=-1`'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_jobs=-1` tells scikit-learn to use every core available. For example, if
    your computer has four cores (a common number for laptops), then scikit-learn
    will use all four cores at once to speed up the operation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One small note: when running some of these examples, you may see a warning
    that says “ConvergenceWarning: lbfgs failed to converge.” The configuration used
    in these examples is designed to prevent this, but should it still occur, you
    can ignore it for now. We will troubleshoot issues like this later in the book
    as we dive into specific types of models.'
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Why Every Statistician Should Know About Cross-Validation](https://oreil.ly/vrGXy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Cross-Validation Gone Wrong](https://oreil.ly/NE-B8)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 11.2 Creating a Baseline Regression Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want a simple baseline regression model to use as a comparison against other
    models that you train.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use scikit-learn’s `DummyRegressor` to create a simple model to use as a baseline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To compare, we train our model and evaluate the performance score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`DummyRegressor` allows us to create a very simple model that we can use as
    a baseline to compare against any other models that we train. This can often be
    useful to simulate a “naive” existing prediction process in a product or system.
    For example, a product might have been originally hardcoded to assume that all
    new users will spend $100 in the first month, regardless of their features. If
    we encode that assumption into a baseline model, we are able to concretely state
    the benefits of using a machine learning approach by comparing the dummy model’s
    `score` with that of a trained model.'
  prefs: []
  type: TYPE_NORMAL
- en: '`DummyRegressor` uses the `strategy` parameter to set the method of making
    predictions, including the mean or median value in the training set. Furthermore,
    if we set `strategy` to `constant` and use the `constant` parameter, we can set
    the dummy regressor to predict some constant value for every observation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'One small note regarding `score`. By default, `score` returns the coefficient
    of determination (R-squared, <math display="inline"><msup><mi>R</mi> <mn>2</mn></msup></math>
    ) score:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msup><mi>R</mi> <mn>2</mn></msup> <mo>=</mo> <mn>1</mn>
    <mo>-</mo> <mfrac><mrow><msub><mo>∑</mo> <mi>i</mi></msub> <msup><mrow><mo>(</mo><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mi>i</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow> <mrow><msub><mo>∑</mo>
    <mi>i</mi></msub> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub> <mo>-</mo><mover
    accent="true"><mi>y</mi> <mo>¯</mo></mover><mo>)</mo></mrow> <mn>2</mn></msup></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math display="inline"><msub><mi>y</mi> <mi>i</mi></msub></math> is the
    true value of the target observation, <math display="inline"><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover>
    <mi>i</mi></msub></math> is the predicted value, and <math display="inline"><mover
    accent="true"><mi>y</mi><mo>¯</mo></mover></math> is the mean value for the target
    vector.
  prefs: []
  type: TYPE_NORMAL
- en: The closer <math display="inline"><msup><mi>R</mi> <mn>2</mn></msup></math>
    is to 1, the more of the variance in the target vector that is explained by the
    features.
  prefs: []
  type: TYPE_NORMAL
- en: 11.3 Creating a Baseline Classification Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want a simple baseline classifier to compare against your model.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use scikit-learn’s `DummyClassifier`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'By comparing the baseline classifier to our trained classifier, we can see
    the improvement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A common measure of a classifier’s performance is how much better it is than
    random guessing. scikit-learn’s `DummyClassifier` makes this comparison easy.
    The `strategy` parameter gives us a number of options for generating values. There
    are two particularly useful strategies. First, `stratified` makes predictions
    proportional to the class proportions of the training set’s target vector (e.g.,
    if 20% of the observations in the training data are women, then `DummyClassifier`
    will predict women 20% of the time). Second, `uniform` will generate predictions
    uniformly at random between the different classes. For example, if 20% of observations
    are women and 80% are men, `uniform` will produce predictions that are 50% women
    and 50% men.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[scikit-learn documentation: DummyClassifier](https://oreil.ly/bwqQU)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 11.4 Evaluating Binary Classifier Predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given a trained classification model, you want to evaluate its quality.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use scikit-learn’s `cross_val_score` to conduct cross-validation while using
    the `scoring` parameter to define one of a number of performance metrics, including
    accuracy, precision, recall, and *F[1]*. *Accuracy* is a common performance metric.
    It is simply the proportion of observations predicted correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mstyle displaystyle="true" scriptlevel="0"><mrow><mi>A</mi>
    <mi>c</mi> <mi>c</mi> <mi>u</mi> <mi>r</mi> <mi>a</mi> <mi>c</mi> <mi>y</mi> <mo>=</mo>
    <mfrac><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi></mrow> <mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></mrow></mstyle></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><mi>T</mi><mi>P</mi></math>
  prefs: []
  type: TYPE_NORMAL
- en: The number of true positives. These are observations that are part of the *positive*
    class (has the disease, purchased the product, etc.) and that we predicted correctly.
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><mi>T</mi><mi>N</mi></math>
  prefs: []
  type: TYPE_NORMAL
- en: The number of true negatives. These are observations that are part of the *negative*
    class (does not have the disease, did not purchase the product, etc.) and that
    we predicted correctly.
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><mi>F</mi><mi>P</mi></math>
  prefs: []
  type: TYPE_NORMAL
- en: The number of false positives, also called a *Type I error*. These are observations
    that are predicted to be part of the *positive* class but are actually part of
    the *negative* class.
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><mi>F</mi><mi>N</mi></math>
  prefs: []
  type: TYPE_NORMAL
- en: The number of false negatives, also called a *Type II error*. These are observations
    that are predicted to be part of the *negative* class but are actually part of
    the *positive* class.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can measure accuracy in three-fold (the default number of folds) cross-validation
    by setting `scoring="accuracy"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The appeal of accuracy is that it has an intuitive and plain English explanation:
    the proportion of observations predicted correctly. However, in the real world,
    often our data has imbalanced classes (e.g., the 99.9% of observations are of
    class 1 and only 0.1% are class 2). When in the presence of imbalanced classes,
    accuracy suffers from a paradox where a model is highly accurate but lacks predictive
    power. For example, imagine we are trying to predict the presence of a very rare
    cancer that occurs in 0.1% of the population. After training our model, we find
    the accuracy is at 95%. However, 99.9% of people do not have the cancer: if we
    simply created a model that “predicted” that nobody had that form of cancer, our
    naive model would be 4.9% more accurate, but it clearly is not able to *predict*
    anything. For this reason, we are often motivated to use other metrics such as
    precision, recall, and the *F[1]* score.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Precision* is the proportion of every observation predicted to be positive
    that is actually positive. We can think about it as a measurement noise in our
    predictions—that is, how likely we are to be right when we predict something is
    positive. Models with high precision are pessimistic in that they predict an observation
    is of the positive class only when they are very certain about it. Formally, precision
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mstyle displaystyle="true" scriptlevel="0"><mrow><mi
    fontstyle="italic">Precision</mi> <mo>=</mo> <mfrac><mrow><mi>T</mi><mi>P</mi></mrow>
    <mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></mrow></mstyle></math>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '*Recall* is the proportion of every positive observation that is truly positive.
    Recall measures the model’s ability to identify an observation of the positive
    class. Models with high recall are optimistic in that they have a low bar for
    predicting that an observation is in the positive class:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mstyle displaystyle="true" scriptlevel="0"><mrow><mi
    fontstyle="italic">Recall</mi> <mo>=</mo> <mfrac><mrow><mi>T</mi><mi>P</mi></mrow>
    <mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></mrow></mstyle></math>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'If this is the first time you have encountered precision and recall, it is
    understandable if it takes a little while to fully understand them. This is one
    of the downsides to accuracy; precision and recall are less intuitive. Almost
    always we want some kind of balance between precision and recall, and this role
    is filled by the *F[1]* score. The *F[1]* score is the *harmonic mean* (a kind
    of average used for ratios):'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mstyle displaystyle="true" scriptlevel="0"><mrow><msub><mi>F</mi>
    <mn fontstyle="italic">1</mn></msub> <mo>=</mo> <mn>2</mn> <mo>×</mo> <mfrac><mrow><mi
    fontstyle="italic">Precision</mi> <mo>×</mo> <mi fontstyle="italic">Recall</mi></mrow>
    <mrow><mi fontstyle="italic">Precision</mi> <mo>+</mo> <mi fontstyle="italic">Recall</mi></mrow></mfrac></mrow></mstyle></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'This score is a measure of correctness achieved in positive prediction—that
    is, of observations labeled as positive, how many are actually positive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As an evaluation metric, accuracy has some valuable properties, especially its
    intuitiveness. However, better metrics often involve using some balance of precision
    and recall—that is, a trade-off between the optimism and pessimism of our model.
    *F[1]* represents a balance between the recall and precision, where the relative
    contributions of both are equal.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an alternative to using `cross_val_score`, if we already have the true y
    values and the predicted y values, we can calculate the metrics accuracy and recall
    directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Accuracy paradox, Wikipedia](https://oreil.ly/vjgZ-)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 11.5 Evaluating Binary Classifier Thresholds
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to evaluate a binary classifier and various probability thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the *receiver operating characteristic* (ROC) curve to evaluate the quality
    of the binary classifier. In scikit-learn, we can use `roc_curve` to calculate
    the true and false positives at each threshold, and then plot them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 11in01](assets/mpc2_11in01.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The receiver operating characteristic curve is a common method for evaluating
    the quality of a binary classifier. ROC compares the presence of true positives
    and false positives at every probability threshold (i.e., the probability at which
    an observation is predicted to be a class). By plotting the ROC curve, we can
    see how the model performs. A classifier that predicts every observation correctly
    would look like the solid light gray line in the ROC output in the previous figure,
    going straight up to the top immediately. A classifier that predicts at random
    will appear as the diagonal line. The better the model, the closer it is to the
    solid line.
  prefs: []
  type: TYPE_NORMAL
- en: 'Until now we have only examined models based on the values they predict. However,
    in many learning algorithms, those predicted values are based on probability estimates.
    That is, each observation is given an explicit probability of belonging in each
    class. In our solution, we can use `predict_proba` to see the predicted probabilities
    for the first observation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the classes using `classes_`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, the first observation has an ~87% chance of being in the negative
    class (`0`) and a 13% chance of being in the positive class (`1`). By default,
    scikit-learn predicts an observation is part of the positive class if the probability
    is greater than 0.5 (called the *threshold*). However, instead of a middle ground,
    we will often want to explicitly bias our model to use a different threshold for
    substantive reasons. For example, if a false positive is very costly to our company,
    we might prefer a model that has a high probability threshold. We fail to predict
    some positives, but when an observation is predicted to be positive, we can be
    very confident that the prediction is correct. This trade-off is represented in
    the *true positive rate* (TPR) and the *false positive rate* (FPR). The TPR is
    the number of observations correctly predicted true divided by all true positive
    observations:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>TPR</mtext> <mo>=</mo> <mfrac><mrow><mtext>TP</mtext></mrow>
    <mrow><mtext>TP</mtext><mo>+</mo><mtext>FN</mtext></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The FPR is the number of incorrectly predicted positives divided by all true
    negative observations:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>FPR</mtext> <mo>=</mo> <mfrac><mrow><mtext>FP</mtext></mrow>
    <mrow><mtext>FP</mtext><mo>+</mo><mtext>TN</mtext></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The ROC curve represents the respective TPR and FPR for every probability threshold.
    For example, in our solution a threshold of roughly 0.50 has a TPR of ~0.83 and
    an FPR of ~0.16:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'However, if we increase the threshold to ~80% (i.e., increase how certain the
    model has to be before it predicts an observation as positive) the TPR drops significantly
    but so does the FPR:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This is because our higher requirement for being predicted to be in the positive
    class has caused the model to not identify a number of positive observations (the
    lower TPR) but has also reduced the noise from negative observations being predicted
    as positive (the lower FPR).
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to being able to visualize the trade-off between TPR and FPR, the
    ROC curve can also be used as a general metric for a model. The better a model
    is, the higher the curve and thus the greater the area under the curve. For this
    reason, it is common to calculate the area under the ROC curve (AUC ROC) to judge
    the overall quality of a model at all possible thresholds. The closer the AUC
    ROC is to 1, the better the model. In scikit-learn we can calculate the AUC ROC
    using `roc_auc_score`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[ROC Curves in Python and R](https://oreil.ly/0qcpZ)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Area Under an ROC Curve](https://oreil.ly/re7sT)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 11.6 Evaluating Multiclass Classifier Predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have a model that predicts three or more classes and want to evaluate the
    model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use cross-validation with an evaluation metric capable of handling more than
    two classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we have balanced classes (i.e., a roughly equal number of observations
    in each class of the target vector), accuracy is—​just like in the binary class
    setting—​a simple and interpretable choice for an evaluation metric. Accuracy
    is the number of correct predictions divided by the number of observations and
    works just as well in the multiclass as in the binary setting. However, when we
    have imbalanced classes (a common scenario), we should be inclined to use other
    evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many of scikit-learn’s built-in metrics are for evaluating binary classifiers.
    However, many of these metrics can be extended for use when we have more than
    two classes. Precision, recall, and *F[1]* scores are useful metrics that we have
    already covered in detail in previous recipes. While all of them were originally
    designed for binary classifiers, we can apply them to multiclass settings by treating
    our data as a set of binary classes. Doing so enables us to apply the metrics
    to each class as if it were the only class in the data, and then aggregate the
    evaluation scores for all the classes by averaging them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code, `macro` refers to the method used to average the evaluation scores
    from the classes. The options are `macro`, `weighted`, and `micro`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`macro`'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the mean of metric scores for each class, weighting each class equally.
  prefs: []
  type: TYPE_NORMAL
- en: '`weighted`'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the mean of metric scores for each class, weighting each class proportional
    to its size in the data.
  prefs: []
  type: TYPE_NORMAL
- en: '`micro`'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the mean of metric scores for each observation-class combination.
  prefs: []
  type: TYPE_NORMAL
- en: 11.7 Visualizing a Classifier’s Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given predicted classes and true classes of the test data, you want to visually
    compare the model’s quality.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use a *confusion matrix*, which compares predicted classes and true classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 11in02](assets/mpc2_11in02.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Confusion matrices are an easy, effective visualization of a classifier’s performance.
    One of the major benefits of confusion matrices is their interpretability. Each
    column of the matrix (often visualized as a heatmap) represents predicted classes,
    while every row shows true classes. The result is that every cell is one possible
    combination of predicted and true classes. This is probably best explained using
    an example. In the solution, the top-left cell is the number of observations predicted
    to be *Iris setosa* (indicated by the column) that are actually *Iris setosa*
    (indicated by the row). This means the model accurately predicted all *Iris setosa*
    flowers. However, the model does not do as well at predicting *Iris virginica*.
    The bottom-right cell indicates that the model successfully predicted eleven observations
    were *Iris virginica*, but (looking one cell up) predicted one flower to be *virginica*
    that was actually *Iris versicolor*.
  prefs: []
  type: TYPE_NORMAL
- en: There are three things worth noting about confusion matrices. First, a perfect
    model will have values along the diagonal and zeros everywhere else. A bad model
    will have the observation counts spread evenly around cells. Second, a confusion
    matrix lets us see not only where the model was wrong but also how it was wrong.
    That is, we can look at patterns of misclassification. For example, our model
    had an easy time differentiating *Iris virginica* and *Iris setosa*, but a slightly
    more difficult time classifying *Iris virginica* and *Iris versicolor*. Finally,
    confusion matrices work with any number of classes (although if we had one million
    classes in our target vector, the confusion matrix visualization might be difficult
    to read).
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Confusion matrix, Wikipedia](https://oreil.ly/tDWPB)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[scikit-learn documentation: Confusion Matrix](https://oreil.ly/fdsTg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 11.8 Evaluating Regression Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to evaluate the performance of a regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use *mean squared error* (MSE):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Another common regression metric is the coefficient of determination, *R²*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'MSE is one of the most common evaluation metrics for regression models. Formally,
    MSE is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mo form="prefix">MSE</mo> <mo>=</mo> <mfrac><mn>1</mn>
    <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mrow>
    <mi>n</mi></munderover> <msup><mrow><mo>(</mo><msub><mover accent="true"><mi>y</mi>
    <mo>^</mo></mover><mi>i</mi></msub><mo>-</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'where <math display="inline"><mi>n</mi></math> is the number of observations,
    <math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math> is the true value
    of the target we are trying to predict for observation <math display="inline"><mi>i</mi></math>,
    and <math display="inline"><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover><mi>i</mi></msub></math>
    is the model’s predicted value for <math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math>.
    MSE is a measurement of the squared sum of all distances between predicted and
    true values. The higher the value of MSE, the greater the total squared error
    and thus the worse the model. There are a number of mathematical benefits to squaring
    the error term, including that it forces all error values to be positive, but
    one often unrealized implication is that squaring penalizes a few large errors
    more than many small errors, even if the absolute value of the errors is the same.
    For example, imagine two models, A and B, each with two observations:'
  prefs: []
  type: TYPE_NORMAL
- en: Model A has errors of 0 and 10, and thus its MSE is *0² + 10² = 100*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model B has two errors of 5 each, and thus its MSE is *5² + 5² = 50*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both models have the same total errors, 10; however, MSE would consider model
    A (MSE = 100) worse than model B (MSE = 50). In practice this implication is rarely
    an issue (and indeed can be theoretically beneficial), and MSE works perfectly
    fine as an evaluation metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'One important note: by default, in scikit-learn, arguments of the `scoring`
    parameter assume that higher values are better than lower values. However, this
    is not the case for MSE, where higher values mean a worse model. For this reason,
    scikit-learn looks at the *negative* MSE using the `neg_mean_squared_error` argument.'
  prefs: []
  type: TYPE_NORMAL
- en: A common alternative regression evaluation metric is the default metric we used
    in [Recipe 11.2](#creating-a-baseline-regression-model), <math display="inline"><msup><mi>R</mi><mn>2</mn></msup></math>,
    which measures the amount of variance in the target vector that is explained by
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msup><mi>R</mi> <mn>2</mn></msup> <mo>=</mo> <mn>1</mn>
    <mo>-</mo> <mfrac><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow> <mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>-</mo><mover accent="true"><mi>y</mi> <mo>¯</mo></mover><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math display="inline"><msub><mi>y</mi> <mi>i</mi></msub></math> is the
    true target value of the *i*th observation, <math display="inline"><msub><mover
    accent="true"><mi>y</mi><mo>^</mo></mover> <mi>i</mi></msub></math> is the predicted
    value for the *i*th observation, and <math display="inline"><mover accent="true"><mi>y</mi>
    <mo>¯</mo></mover></math> is the mean value of the target vector. The closer that
    <math display="inline"><msup><mi>R</mi><mn>2</mn></msup></math> is to 1.0, the
    better the model.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Mean squared error, Wikipedia](https://oreil.ly/MWDlR)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Coefficient of determination, Wikipedia](https://oreil.ly/lKKWk)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 11.9 Evaluating Clustering Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have used an unsupervised learning algorithm to cluster your data. Now you
    want to know how well it did.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use *silhouette coefficients* to measure the quality of the clusters (note
    that this does not measure predictive performance):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Supervised model evaluation* compares predictions (e.g., classes or quantitative
    values) with the corresponding true values in the target vector. However, the
    most common motivation for using clustering methods is that your data doesn’t
    have a target vector. A number of clustering evaluation metrics require a target
    vector, but again, using unsupervised learning approaches like clustering when
    you have a target vector available to you is probably handicapping yourself unnecessarily.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While we cannot evaluate predictions versus true values if we don’t have a
    target vector, we can evaluate the nature of the clusters themselves. Intuitively,
    we can imagine “good” clusters having very small distances between observations
    in the same cluster (i.e., dense clusters) and large distances between the different
    clusters (i.e., well-separated clusters). Silhouette coefficients provide a single
    value measuring both traits. Formally, the *i*th observation’s silhouette coefficient
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>s</mi> <mi>i</mi></msub> <mo>=</mo> <mfrac><mrow><msub><mi>b</mi>
    <mi>i</mi></msub> <mo>-</mo><msub><mi>a</mi> <mi>i</mi></msub></mrow> <mrow><mtext>max</mtext><mo>(</mo><msub><mi>a</mi>
    <mi>i</mi></msub> <mo>,</mo><msub><mi>b</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math display="inline"><msub><mi>s</mi><mi>i</mi></msub></math> is the
    silhouette coefficient for observation <math display="inline"><mi>i</mi></math>,
    <math display="inline"><msub><mi>a</mi><mi>i</mi></msub></math> is the mean distance
    between <math display="inline"><mi>i</mi></math> and all observations of the same
    class, and <math display="inline"><msub><mi>b</mi><mi>i</mi></msub></math> is
    the mean distance between <math display="inline"><mi>i</mi></math> and all observations
    from the closest cluster of a different class. The value returned by `silhouette_score`
    is the mean silhouette coefficient for all observations. Silhouette coefficients
    range between –1 and 1, with 1 indicating dense, well-separated clusters.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[scikit-learn documentation: silhouette_score](https://oreil.ly/gGjQj)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 11.10 Creating a Custom Evaluation Metric
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to evaluate a model using a metric you created.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Create the metric as a function and convert it into a scorer function using
    scikit-learn’s `make_scorer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While scikit-learn has a number of built-in metrics for evaluating model performance,
    it is often useful to define our own metrics. scikit-learn makes this easy using
    `make_scorer`. First, we define a function that takes in two arguments—​the ground
    truth target vector and our predicted values—​and outputs some score. Second,
    we use `make_scorer` to create a scorer object, making sure to specify whether
    higher or lower scores are desirable (using the `greater_is_better` parameter).
  prefs: []
  type: TYPE_NORMAL
- en: 'The custom metric in the solution (`custom_metric`) is a toy example since
    it simply wraps a built-in metric for calculating the *R²* score. In a real-world
    situation, we would replace the `custom_metric` function with whatever custom
    metric we wanted. However, we can see that the custom metric that calculates *R²*
    does work by comparing the results to scikit-learn’s `r2_score` built-in method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[scikit-learn documentation: make_scorer](https://oreil.ly/-RqFY)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 11.11 Visualizing the Effect of Training Set Size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to evaluate the effect of the number of observations in your training
    set on some metric (accuracy, *F[1]*, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Plot the accuracy against the training set size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 11in03](assets/mpc2_11in03.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Learning curves* visualize the performance (e.g., accuracy, recall) of a model
    on the training set and during cross-validation as the number of observations
    in the training set increases. They are commonly used to determine if our learning
    algorithms would benefit from gathering additional training data.'
  prefs: []
  type: TYPE_NORMAL
- en: In our solution, we plot the accuracy of a random forest classifier at 50 different
    training set sizes, ranging from 1% of observations to 100%. The increasing accuracy
    score of the cross-validated models tell us that we would likely benefit from
    additional observations (although in practice this might not be feasible).
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[scikit-learn documentation: Learning Curve](https://oreil.ly/jAKwy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 11.12 Creating a Text Report of Evaluation Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want a quick description of a classifier’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use scikit-learn’s `classification_report`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`classification_report` provides a quick means for us to see some common evaluation
    metrics, including precision, recall, and *F[1]* score (described in [Recipe 11.4](#evaluating-binary-classifier-predictions)).
    Support refers to the number of observations in each class.'
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Precision and recall, Wikipedia](https://oreil.ly/9mBSF)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 11.13 Visualizing the Effect of Hyperparameter Values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to understand how the performance of a model changes as the value of
    some hyperparameter changes.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Plot the hyperparameter against the model accuracy (validation curve):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 11in04](assets/mpc2_11in04.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most training algorithms (including many covered in this book) contain hyperparameters
    that must be chosen before the training process begins. For example, a *random
    forest classifier* creates a “forest” of decision trees, each of which votes on
    the predicted class of an observation. One hyperparameter in random forest classifiers
    is the number of trees in the forest. Most often hyperparameter values are selected
    during model selection (see [Chapter 12](ch12.xhtml#model-selection)). However,
    it is occasionally useful to visualize how model performance changes as the hyperparameter
    value changes. In our solution, we plot the changes in accuracy for a random forest
    classifier for the training set and during cross-validation as the number of trees
    increases. When we have a small number of trees, both the training and cross-validation
    score are low, suggesting the model is underfitted. As the number of trees increases
    to 250, the accuracy of both levels off, suggesting there is probably not much
    value in the computational cost of training a massive forest.
  prefs: []
  type: TYPE_NORMAL
- en: 'In scikit-learn, we can calculate the validation curve using `validation_curve`,
    which contains three important parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`param_name`'
  prefs: []
  type: TYPE_NORMAL
- en: Name of the hyperparameter to vary
  prefs: []
  type: TYPE_NORMAL
- en: '`param_range`'
  prefs: []
  type: TYPE_NORMAL
- en: Value of the hyperparameter to use
  prefs: []
  type: TYPE_NORMAL
- en: '`scoring`'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation metric used to judge to model
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[scikit-learn documentation: Validation Curve](https://oreil.ly/FH_kH)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
