["```py\n'''Main'''\nimport numpy as np\nimport pandas as pd\nimport os, time, re\nimport pickle, gzip, datetime\n\n'''Data Viz'''\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nimport matplotlib as mpl\n\n%matplotlib inline\n\n'''Data Prep and Model Evaluation'''\nfrom sklearn import preprocessing as pp\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss, accuracy_score\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score, mean_squared_error\n\n'''Algos'''\nimport lightgbm as lgb\n\n'''TensorFlow and Keras'''\nimport tensorflow as tf\nimport keras\nfrom keras import backend as K\nfrom keras.models import Sequential, Model\nfrom keras.layers import Activation, Dense, Dropout\nfrom keras.layers import BatchNormalization, Input, Lambda\nfrom keras.layers import Embedding, Flatten, dot\nfrom keras import regularizers\nfrom keras.losses import mse, binary_crossentropy\n```", "```py\n# Load the datasets\ncurrent_path = os.getcwd()\nfile = '\\\\datasets\\\\mnist_data\\\\mnist.pkl.gz'\nf = gzip.open(current_path+file, 'rb')\ntrain_set, validation_set, test_set = pickle.load(f, encoding='latin1')\nf.close()\n\nX_train, y_train = train_set[0], train_set[1]\nX_validation, y_validation = validation_set[0], validation_set[1]\nX_test, y_test = test_set[0], test_set[1]\n\n# Create Pandas DataFrames from the datasets\ntrain_index = range(0,len(X_train))\nvalidation_index = range(len(X_train),len(X_train)+len(X_validation))\ntest_index = range(len(X_train)+len(X_validation), \\\n                   len(X_train)+len(X_validation)+len(X_test))\n\nX_train = pd.DataFrame(data=X_train,index=train_index)\ny_train = pd.Series(data=y_train,index=train_index)\n\nX_validation = pd.DataFrame(data=X_validation,index=validation_index)\ny_validation = pd.Series(data=y_validation,index=validation_index)\n\nX_test = pd.DataFrame(data=X_test,index=test_index)\ny_test = pd.Series(data=y_test,index=test_index)\n\ndef view_digit(X, y, example):\n    label = y.loc[example]\n    image = X.loc[example,:].values.reshape([28,28])\n    plt.title('Example: %d Label: %d' % (example, label))\n    plt.imshow(image, cmap=plt.get_cmap('gray'))\n    plt.show()\n\ndef one_hot(series):\n    label_binarizer = pp.LabelBinarizer()\n    label_binarizer.fit(range(max(series)+1))\n    return label_binarizer.transform(series)\n\n# Create one-hot vectors for the labels\ny_train_oneHot = one_hot(y_train)\ny_validation_oneHot = one_hot(y_validation)\ny_test_oneHot = one_hot(y_test)\n```", "```py\n# Define RBM class\nclass RBM(object):\n\n    def __init__(self, input_size, output_size,\n                 learning_rate, epochs, batchsize):\n        # Define hyperparameters\n        self._input_size = input_size\n        self._output_size = output_size\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.batchsize = batchsize\n\n        # Initialize weights and biases using zero matrices\n        self.w = np.zeros([input_size, output_size], \"float\")\n        self.hb = np.zeros([output_size], \"float\")\n        self.vb = np.zeros([input_size], \"float\")\n```", "```py\ndef prob_h_given_v(self, visible, w, hb):\n    return tf.nn.sigmoid(tf.matmul(visible, w) + hb)\n```", "```py\ndef prob_v_given_h(self, hidden, w, vb):\n    return tf.nn.sigmoid(tf.matmul(hidden, tf.transpose(w)) + vb)\n```", "```py\ndef sample_prob(self, probs):\n    return tf.nn.relu(tf.sign(probs - tf.random_uniform(tf.shape(probs))))\n```", "```py\n_w = tf.placeholder(\"float\", [self._input_size, self._output_size])\n_hb = tf.placeholder(\"float\", [self._output_size])\n_vb = tf.placeholder(\"float\", [self._input_size])\n\nprv_w = np.zeros([self._input_size, self._output_size], \"float\")\nprv_hb = np.zeros([self._output_size], \"float\")\nprv_vb = np.zeros([self._input_size], \"float\")\n\ncur_w = np.zeros([self._input_size, self._output_size], \"float\")\ncur_hb = np.zeros([self._output_size], \"float\")\ncur_vb = np.zeros([self._input_size], \"float\")\n```", "```py\nv0 = tf.placeholder(\"float\", [None, self._input_size])\nh0 = self.sample_prob(self.prob_h_given_v(v0, _w, _hb))\n```", "```py\nv1 = self.sample_prob(self.prob_v_given_h(h0, _w, _vb))\nh1 = self.prob_h_given_v(v1, _w, _hb)\n```", "```py\npositive_grad = tf.matmul(tf.transpose(v0), h0)\nnegative_grad = tf.matmul(tf.transpose(v1), h1)\n\nupdate_w = _w + self.learning_rate * \\\n    (positive_grad - negative_grad) / tf.to_float(tf.shape(v0)[0])\nupdate_vb = _vb +  self.learning_rate * tf.reduce_mean(v0 - v1, 0)\nupdate_hb = _hb +  self.learning_rate * tf.reduce_mean(h0 - h1, 0)\n\nerr = tf.reduce_mean(tf.square(v0 - v1))\n```", "```py\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    for epoch in range(self.epochs):\n        for start, end in zip(range(0, len(X), self.batchsize), \\\n                range(self.batchsize,len(X), self.batchsize)):\n            batch = X[start:end]\n            cur_w = sess.run(update_w, \\\n                feed_dict={v0: batch, _w: prv_w, \\\n                           _hb: prv_hb, _vb: prv_vb})\n            cur_hb = sess.run(update_hb, \\\n                feed_dict={v0: batch, _w: prv_w, \\\n                           _hb: prv_hb, _vb: prv_vb})\n            cur_vb = sess.run(update_vb, \\\n                feed_dict={v0: batch, _w: prv_w, \\\n                           _hb: prv_hb, _vb: prv_vb})\n            prv_w = cur_w\n            prv_hb = cur_hb\n            prv_vb = cur_vb\n        error = sess.run(err, feed_dict={v0: X, _w: cur_w, \\\n                                        _vb: cur_vb, _hb: cur_hb})\n        print ('Epoch: %d' % epoch,'reconstruction error: %f' % error)\n    self.w = prv_w\n    self.hb = prv_hb\n    self.vb = prv_vb\n```", "```py\ndef rbm_output(self, X):\n\n    input_X = tf.constant(X)\n    _w = tf.constant(self.w)\n    _hb = tf.constant(self.hb)\n    _vb = tf.constant(self.vb)\n    out = tf.nn.sigmoid(tf.matmul(input_X, _w) + _hb)\n    hiddenGen = self.sample_prob(self.prob_h_given_v(input_X, _w, _hb))\n    visibleGen = self.sample_prob(self.prob_v_given_h(hiddenGen, _w, _vb))\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        return sess.run(out), sess.run(visibleGen), sess.run(hiddenGen)\n```", "```py\ndef show_features(self, shape, suptitle, count=-1):\n    maxw = np.amax(self.w.T)\n    minw = np.amin(self.w.T)\n    count = self._output_size if count == -1 or count > \\\n            self._output_size else count\n    ncols = count if count < 14 else 14\n    nrows = count//ncols\n    nrows = nrows if nrows > 2 else 3\n    fig = plt.figure(figsize=(ncols, nrows), dpi=100)\n    grid = Grid(fig, rect=111, nrows_ncols=(nrows, ncols), axes_pad=0.01)\n\n    for i, ax in enumerate(grid):\n        x = self.w.T[i] if i<self._input_size else np.zeros(shape)\n        x = (x.reshape(1, -1) - minw)/maxw\n        ax.imshow(x.reshape(*shape), cmap=mpl.cm.Greys)\n        ax.set_axis_off()\n\n    fig.text(0.5,1, suptitle, fontsize=20, horizontalalignment='center')\n    fig.tight_layout()\n    plt.show()\n    return\n```", "```py\n# Since we are training, set input as training data\ninputX = np.array(X_train)\n\n# Create list to hold our RBMs\nrbm_list = []\n\n# Define the parameters of the RBMs we will train\nrbm_list.append(RBM(784,700,1.0,100,200))\nrbm_list.append(RBM(700,600,1.0,100,200))\nrbm_list.append(RBM(600,500,1.0,100,200))\n```", "```py\noutputList = []\nerror_list = []\n#For each RBM in our list\nfor i in range(0,len(rbm_list)):\n    print('RBM', i+1)\n    #Train a new one\n    rbm = rbm_list[i]\n    err = rbm.train(inputX)\n    error_list.append(err)\n    #Return the output layer\n    outputX, reconstructedX, hiddenX = rbm.rbm_output(inputX)\n    outputList.append(outputX)\n    inputX = hiddenX\n```", "```py\nrbm_shapes = [(28,28),(25,24),(25,20)]\nfor i in range(0,len(rbm_list)):\n    rbm = rbm_list[i]\n    print(rbm.show_features(rbm_shapes[i],\n     \"RBM learned features from MNIST\", 56))\n```", "```py\ninputX = np.array(X_train)\nrbmOne = rbm_list[0]\n\nprint('RBM 1')\noutputX_rbmOne, reconstructedX_rbmOne, hiddenX_rbmOne =\n rbmOne.rbm_output(inputX)\nreconstructedX_rbmOne = pd.DataFrame(data=reconstructedX_rbmOne,\n index=X_train.index)\nfor j in range(0,10):\n    example = j\n    view_digit(reconstructedX, y_train, example)\n    view_digit(X_train, y_train, example)\n```", "```py\nclass DBN(object):\n    def __init__(self, original_input_size, input_size, output_size,\n                 learning_rate, epochs, batchsize, rbmOne, rbmTwo, rbmThree):\n        # Define hyperparameters\n        self._original_input_size = original_input_size\n        self._input_size = input_size\n        self._output_size = output_size\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.batchsize = batchsize\n        self.rbmOne = rbmOne\n        self.rbmTwo = rbmTwo\n        self.rbmThree = rbmThree\n\n        self.w = np.zeros([input_size, output_size], \"float\")\n        self.hb = np.zeros([output_size], \"float\")\n        self.vb = np.zeros([input_size], \"float\")\n```", "```py\ndef prob_h_given_v(self, visible, w, hb):\n    return tf.nn.sigmoid(tf.matmul(visible, w) + hb)\n\ndef prob_v_given_h(self, hidden, w, vb):\n    return tf.nn.sigmoid(tf.matmul(hidden, tf.transpose(w)) + vb)\n\ndef sample_prob(self, probs):\n    return tf.nn.relu(tf.sign(probs - tf.random_uniform(tf.shape(probs))))\n```", "```py\ndef train(self, X):\n    _w = tf.placeholder(\"float\", [self._input_size, self._output_size])\n    _hb = tf.placeholder(\"float\", [self._output_size])\n    _vb = tf.placeholder(\"float\", [self._input_size])\n\n    prv_w = np.zeros([self._input_size, self._output_size], \"float\")\n    prv_hb = np.zeros([self._output_size], \"float\")\n    prv_vb = np.zeros([self._input_size], \"float\")\n\n    cur_w = np.zeros([self._input_size, self._output_size], \"float\")\n    cur_hb = np.zeros([self._output_size], \"float\")\n    cur_vb = np.zeros([self._input_size], \"float\")\n```", "```py\nv0 = tf.placeholder(\"float\", [None, self._original_input_size])\nforwardOne = tf.nn.relu(tf.sign(tf.nn.sigmoid(tf.matmul(v0, \\\n                self.rbmOne.w) + self.rbmOne.hb) - tf.random_uniform( \\\n                tf.shape(tf.nn.sigmoid(tf.matmul(v0, self.rbmOne.w) + \\\n                self.rbmOne.hb)))))\nforwardTwo = tf.nn.relu(tf.sign(tf.nn.sigmoid(tf.matmul(forwardOne, \\\n                self.rbmTwo.w) + self.rbmTwo.hb) - tf.random_uniform( \\\n                tf.shape(tf.nn.sigmoid(tf.matmul(forwardOne, \\\n                self.rbmTwo.w) + self.rbmTwo.hb)))))\nforward = tf.nn.relu(tf.sign(tf.nn.sigmoid(tf.matmul(forwardTwo, \\\n                self.rbmThree.w) + self.rbmThree.hb) - \\\n                tf.random_uniform(tf.shape(tf.nn.sigmoid(tf.matmul( \\\n                forwardTwo, self.rbmThree.w) + self.rbmThree.hb)))))\nh0 = self.sample_prob(self.prob_h_given_v(forward, _w, _hb))\nv1 = self.sample_prob(self.prob_v_given_h(h0, _w, _vb))\nh1 = self.prob_h_given_v(v1, _w, _hb)\n```", "```py\npositive_grad = tf.matmul(tf.transpose(forward), h0)\nnegative_grad = tf.matmul(tf.transpose(v1), h1)\n\nupdate_w = _w + self.learning_rate * (positive_grad - negative_grad) / \\\n                tf.to_float(tf.shape(forward)[0])\nupdate_vb = _vb +  self.learning_rate * tf.reduce_mean(forward - v1, 0)\nupdate_hb = _hb +  self.learning_rate * tf.reduce_mean(h0 - h1, 0)\n```", "```py\nbackwardOne = tf.nn.relu(tf.sign(tf.nn.sigmoid(tf.matmul(v1, \\\n                    self.rbmThree.w.T) + self.rbmThree.vb) - \\\n                    tf.random_uniform(tf.shape(tf.nn.sigmoid( \\\n                    tf.matmul(v1, self.rbmThree.w.T) + \\\n                    self.rbmThree.vb)))))\nbackwardTwo = tf.nn.relu(tf.sign(tf.nn.sigmoid(tf.matmul(backwardOne, \\\n                    self.rbmTwo.w.T) + self.rbmTwo.vb) - \\\n                    tf.random_uniform(tf.shape(tf.nn.sigmoid( \\\n                    tf.matmul(backwardOne, self.rbmTwo.w.T) + \\\n                    self.rbmTwo.vb)))))\nbackward = tf.nn.relu(tf.sign(tf.nn.sigmoid(tf.matmul(backwardTwo, \\\n                    self.rbmOne.w.T) + self.rbmOne.vb) - \\\n                    tf.random_uniform(tf.shape(tf.nn.sigmoid( \\\n                    tf.matmul(backwardTwo, self.rbmOne.w.T) + \\\n                    self.rbmOne.vb)))))\n\nerr = tf.reduce_mean(tf.square(v0 - backward))\n```", "```py\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    for epoch in range(self.epochs):\n        for start, end in zip(range(0, len(X), self.batchsize), \\\n                range(self.batchsize,len(X), self.batchsize)):\n            batch = X[start:end]\n            cur_w = sess.run(update_w, feed_dict={v0: batch, _w: \\\n                                prv_w, _hb: prv_hb, _vb: prv_vb})\n            cur_hb = sess.run(update_hb, feed_dict={v0: batch, _w: \\\n                                prv_w, _hb: prv_hb, _vb: prv_vb})\n            cur_vb = sess.run(update_vb, feed_dict={v0: batch, _w: \\\n                                prv_w, _hb: prv_hb, _vb: prv_vb})\n            prv_w = cur_w\n            prv_hb = cur_hb\n            prv_vb = cur_vb\n        error = sess.run(err, feed_dict={v0: X, _w: cur_w, _vb: \\\n                            cur_vb, _hb: cur_hb})\n        print ('Epoch: %d' % epoch,'reconstruction error: %f' % error)\n    self.w = prv_w\n    self.hb = prv_hb\n    self.vb = prv_vb\n```", "```py\ndef dbn_output(self, X):\n\n    input_X = tf.constant(X)\n    forwardOne = tf.nn.sigmoid(tf.matmul(input_X, self.rbmOne.w) + \\\n                               self.rbmOne.hb)\n    forwardTwo = tf.nn.sigmoid(tf.matmul(forwardOne, self.rbmTwo.w) + \\\n                               self.rbmTwo.hb)\n    forward = tf.nn.sigmoid(tf.matmul(forwardTwo, self.rbmThree.w) + \\\n                            self.rbmThree.hb)\n\n    _w = tf.constant(self.w)\n    _hb = tf.constant(self.hb)\n    _vb = tf.constant(self.vb)\n\n    out = tf.nn.sigmoid(tf.matmul(forward, _w) + _hb)\n    hiddenGen = self.sample_prob(self.prob_h_given_v(forward, _w, _hb))\n    visibleGen = self.sample_prob(self.prob_v_given_h(hiddenGen, _w, _vb))\n\n    backwardTwo = tf.nn.sigmoid(tf.matmul(visibleGen, self.rbmThree.w.T) + \\\n                                self.rbmThree.vb)\n    backwardOne = tf.nn.sigmoid(tf.matmul(backwardTwo, self.rbmTwo.w.T) + \\\n                                self.rbmTwo.vb)\n    backward = tf.nn.sigmoid(tf.matmul(backwardOne, self.rbmOne.w.T) + \\\n                             self.rbmOne.vb)\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        return sess.run(out), sess.run(backward)\n```", "```py\ndef show_features(self, shape, suptitle, count=-1):\n    maxw = np.amax(self.w.T)\n    minw = np.amin(self.w.T)\n    count = self._output_size if count == -1 or count > \\\n            self._output_size else count\n    ncols = count if count < 14 else 14\n    nrows = count//ncols\n    nrows = nrows if nrows > 2 else 3\n    fig = plt.figure(figsize=(ncols, nrows), dpi=100)\n    grid = Grid(fig, rect=111, nrows_ncols=(nrows, ncols), axes_pad=0.01)\n\n    for i, ax in enumerate(grid):\n        x = self.w.T[i] if i<self._input_size else np.zeros(shape)\n        x = (x.reshape(1, -1) - minw)/maxw\n        ax.imshow(x.reshape(*shape), cmap=mpl.cm.Greys)\n        ax.set_axis_off()\n\n    fig.text(0.5,1, suptitle, fontsize=20, horizontalalignment='center')\n    fig.tight_layout()\n    plt.show()\n    return\n```", "```py\n# Instantiate DBN Class\ndbn = DBN(784, 500, 500, 1.0, 50, 200, rbm_list[0], rbm_list[1], rbm_list[2])\n```", "```py\ninputX = np.array(X_train)\nerror_list = []\nerror_list = dbn.train(inputX)\n```", "```py\n# Generate images and store them\ninputXReduced = X_train.loc[:4999]\nfor i in range(0,20):\n    print(\"Run \",i)\n    finalOutput_DBN, reconstructedOutput_DBN = dbn.dbn_output(inputXReduced)\n    if i==0:\n        generatedImages = finalOutput_DBN\n    else:\n        generatedImages = np.append(generatedImages, finalOutput_DBN, axis=0)\n```", "```py\n# Generate a vector of labels for the generated images\nfor i in range(0,20):\n    if i==0:\n        labels = y_train.loc[:4999]\n    else:\n        labels = np.append(labels,y_train.loc[:4999])\n```", "```py\n# Generate images based on the validation set\ninputValidation = np.array(X_validation)\nfinalOutput_DBN_validation, reconstructedOutput_DBN_validation = \\\n    dbn.dbn_output(inputValidation)\n```", "```py\n# View reconstructed images\nfor i in range(0,10):\n    example = i\n    reconstructedX = pd.DataFrame(data=reconstructedOutput_DBN, \\\n                                  index=X_train[0:5000].index)\n    view_digit(reconstructedX, y_train, example)\n    view_digit(X_train, y_train, example)\n```", "```py\n# Generate the first example 10 times\ninputXReduced = X_train.loc[:0]\nfor i in range(0,10):\n    example = 0\n    print(\"Run \",i)\n    finalOutput_DBN_fives, reconstructedOutput_DBN_fives = \\\n        dbn.dbn_output(inputXReduced)\n    reconstructedX_fives = pd.DataFrame(data=reconstructedOutput_DBN_fives, \\\n                                        index=[0])\n    print(\"Generated\")\n    view_digit(reconstructedX_fives, y_train.loc[:0], example)\n```", "```py\npredictionColumns = ['0','1','2','3','4','5','6','7','8','9']\n\nparams_lightGB = {\n    'task': 'train',\n    'application':'binary',\n    'num_class':10,\n    'boosting': 'gbdt',\n    'objective': 'multiclass',\n    'metric': 'multi_logloss',\n    'metric_freq':50,\n    'is_training_metric':False,\n    'max_depth':4,\n    'num_leaves': 31,\n    'learning_rate': 0.1,\n    'feature_fraction': 1.0,\n    'bagging_fraction': 1.0,\n    'bagging_freq': 0,\n    'bagging_seed': 2018,\n    'verbose': 0,\n    'num_threads':16\n}\n```", "```py\ntrainingScore = []\nvalidationScore = []\npredictionsLightGBM = pd.DataFrame(data=[], \\\n                        index=y_validation.index, \\\n                        columns=predictionColumns)\n\nlgb_train = lgb.Dataset(X_train.loc[:4999], y_train.loc[:4999])\nlgb_eval = lgb.Dataset(X_validation, y_validation, reference=lgb_train)\ngbm = lgb.train(params_lightGB, lgb_train, num_boost_round=2000,\n                   valid_sets=lgb_eval, early_stopping_rounds=200)\n\nloglossTraining = log_loss(y_train.loc[:4999], \\\n    gbm.predict(X_train.loc[:4999], num_iteration=gbm.best_iteration))\ntrainingScore.append(loglossTraining)\n\npredictionsLightGBM.loc[X_validation.index,predictionColumns] = \\\n    gbm.predict(X_validation, num_iteration=gbm.best_iteration)\nloglossValidation = log_loss(y_validation,\n    predictionsLightGBM.loc[X_validation.index,predictionColumns])\nvalidationScore.append(loglossValidation)\n\nprint('Training Log Loss: ', loglossTraining)\nprint('Validation Log Loss: ', loglossValidation)\n\nloglossLightGBM = log_loss(y_validation, predictionsLightGBM)\nprint('LightGBM Gradient Boosting Log Loss: ', loglossLightGBM)\n```", "```py\nTraining Log Loss: 0.0018646953029132292\nValidation Log Loss: 0.19124276982588717\n```", "```py\npredictionsLightGBM_firm = np.argmax(np.array(predictionsLightGBM), axis=1)\naccuracyValidation_lightGBM = accuracy_score(np.array(y_validation), \\\n                                            predictionsLightGBM_firm)\nprint(\"Supervised-Only Accuracy: \", accuracyValidation_lightGBM)\n```", "```py\nSupervised-Only Accuracy: 0.9439\n```", "```py\n# Prepare DBN-based DataFrames for LightGBM use\ngeneratedImagesDF = pd.DataFrame(data=generatedImages,index=range(0,100000))\nlabelsDF = pd.DataFrame(data=labels,index=range(0,100000))\n\nX_train_lgb = pd.DataFrame(data=generatedImagesDF,\n                           index=generatedImagesDF.index)\nX_validation_lgb = pd.DataFrame(data=finalOutput_DBN_validation,\n                                index=X_validation.index)\n```", "```py\n# Train LightGBM\ntrainingScore = []\nvalidationScore = []\npredictionsDBN = pd.DataFrame(data=[],index=y_validation.index,\n                              columns=predictionColumns)\n\nlgb_train = lgb.Dataset(X_train_lgb, labels)\nlgb_eval = lgb.Dataset(X_validation_lgb, y_validation, reference=lgb_train)\ngbm = lgb.train(params_lightGB, lgb_train, num_boost_round=2000,\n                   valid_sets=lgb_eval, early_stopping_rounds=200)\n\nloglossTraining = log_loss(labelsDF, gbm.predict(X_train_lgb, \\\n                            num_iteration=gbm.best_iteration))\ntrainingScore.append(loglossTraining)\n\npredictionsDBN.loc[X_validation.index,predictionColumns] = \\\n    gbm.predict(X_validation_lgb, num_iteration=gbm.best_iteration)\nloglossValidation = log_loss(y_validation,\n    predictionsDBN.loc[X_validation.index,predictionColumns])\nvalidationScore.append(loglossValidation)\n\nprint('Training Log Loss: ', loglossTraining)\nprint('Validation Log Loss: ', loglossValidation)\n\nloglossDBN = log_loss(y_validation, predictionsDBN)\nprint('LightGBM Gradient Boosting Log Loss: ', loglossDBN)\n```", "```py\nTraining Log Loss: 0.004145635328203315\nValidation Log Loss: 0.16377638170016542\n```", "```py\nDBN-Based Solution Accuracy: 0.9525\n```"]