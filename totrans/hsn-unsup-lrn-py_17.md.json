["```py\n'''Main'''\nimport numpy as np\nimport pandas as pd\nimport os, time, re\nimport pickle, gzip, datetime\nfrom os import listdir, walk\nfrom os.path import isfile, join\n\n'''Data Viz'''\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nimport matplotlib as mpl\nfrom mpl_toolkits.axes_grid1 import Grid\n\n%matplotlib inline\n\n'''Data Prep and Model Evaluation'''\nfrom sklearn import preprocessing as pp\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss, accuracy_score\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score, mean_squared_error\nfrom keras.utils import to_categorical\nfrom sklearn.metrics import adjusted_rand_score\nimport random\n\n'''Algos'''\nfrom kshape.core import kshape, zscore\nimport tslearn\nfrom tslearn.utils import to_time_series_dataset\nfrom tslearn.clustering import KShape, TimeSeriesScalerMeanVariance\nfrom tslearn.clustering import TimeSeriesKMeans\nimport hdbscan\n\n'''TensorFlow and Keras'''\nimport tensorflow as tf\nimport keras\nfrom keras import backend as K\nfrom keras.models import Sequential, Model\nfrom keras.layers import Activation, Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.layers import LeakyReLU, Reshape, UpSampling2D, Conv2DTranspose\nfrom keras.layers import BatchNormalization, Input, Lambda\nfrom keras.layers import Embedding, Flatten, dot\nfrom keras import regularizers\nfrom keras.losses import mse, binary_crossentropy\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.optimizers import Adam, RMSprop\nfrom tensorflow.examples.tutorials.mnist import input_data\n```", "```py\n# Load the datasets\ncurrent_path = os.getcwd()\nfile = '\\\\datasets\\\\ucr_time_series_data\\\\'\ndata_train = np.loadtxt(current_path+file+\n                        \"ECGFiveDays/ECGFiveDays_TRAIN\",\n                        delimiter=\",\")\nX_train = to_time_series_dataset(data_train[:, 1:])\ny_train = data_train[:, 0].astype(np.int)\n\ndata_test = np.loadtxt(current_path+file+\n                       \"ECGFiveDays/ECGFiveDays_TEST\",\n                       delimiter=\",\")\nX_test = to_time_series_dataset(data_test[:, 1:])\ny_test = data_test[:, 0].astype(np.int)\n```", "```py\n# Basic summary statistics\nprint(\"Number of time series:\", len(data_train))\nprint(\"Number of unique classes:\", len(np.unique(data_train[:,0])))\nprint(\"Time series length:\", len(data_train[0,1:]))\n```", "```py\nNumber of time series: 23\nNumber of unique classes: 2\nTime series length: 136\n```", "```py\n# Examples of Class 1.0\nfor i in range(0,10):\n    if data_train[i,0]==1.0:\n        print(\"Plot \",i,\" Class \",data_train[i,0])\n        plt.plot(data_train[i])\n        plt.show()\n```", "```py\n# Examples of Class 2.0\nfor i in range(0,10):\n    if data_train[i,0]==2.0:\n        print(\"Plot \",i,\" Class \",data_train[i,0])\n        plt.plot(data_train[i])\n        plt.show()\n```", "```py\n# Prepare the data - Scale\nX_train = TimeSeriesScalerMeanVariance(mu=0., std=1.).fit_transform(X_train)\nX_test = TimeSeriesScalerMeanVariance(mu=0., std=1.).fit_transform(X_test)\n```", "```py\n# Train using k-Shape\nks = KShape(n_clusters=2, max_iter=100, n_init=100,verbose=0)\nks.fit(X_train)\n```", "```py\n# Make predictions and calculate adjusted Rand index\npreds = ks.predict(X_train)\nars = adjusted_rand_score(data_train[:,0],preds)\nprint(\"Adjusted Rand Index:\", ars)\n```", "```py\nAdjusted Rand Index: 0.668041237113402\n```", "```py\n# Make predictions on test set and calculate adjusted Rand index\npreds_test = ks.predict(X_test)\nars = adjusted_rand_score(data_test[:,0],preds_test)\nprint(\"Adjusted Rand Index on Test Set:\", ars)\n```", "```py\nAdjusted Rand Index on Test Set: 0.0006332050676187496\n```", "```py\n# Load the datasets\ncurrent_path = os.getcwd()\nfile = '\\\\datasets\\\\ucr_time_series_data\\\\'\ndata_train = np.loadtxt(current_path+file+\n                        \"ECG5000/ECG5000_TRAIN\",\n                        delimiter=\",\")\n\ndata_test = np.loadtxt(current_path+file+\n                       \"ECG5000/ECG5000_TEST\",\n                       delimiter=\",\")\n\ndata_joined = np.concatenate((data_train,data_test),axis=0)\ndata_train, data_test = train_test_split(data_joined,\n                                    test_size=0.20, random_state=2019)\n\nX_train = to_time_series_dataset(data_train[:, 1:])\ny_train = data_train[:, 0].astype(np.int)\nX_test = to_time_series_dataset(data_test[:, 1:])\ny_test = data_test[:, 0].astype(np.int)\n```", "```py\n# Summary statistics\nprint(\"Number of time series:\", len(data_train))\nprint(\"Number of unique classes:\", len(np.unique(data_train[:,0])))\nprint(\"Time series length:\", len(data_train[0,1:]))\n```", "```py\nNumber of time series: 4000\nNumber of unique classes: 5\nTime series length: 140\n```", "```py\n# Calculate number of readings per class\nprint(\"Number of time series in class 1.0:\",\n      len(data_train[data_train[:,0]==1.0]))\nprint(\"Number of time series in class 2.0:\",\n      len(data_train[data_train[:,0]==2.0]))\nprint(\"Number of time series in class 3.0:\",\n      len(data_train[data_train[:,0]==3.0]))\nprint(\"Number of time series in class 4.0:\",\n      len(data_train[data_train[:,0]==4.0]))\nprint(\"Number of time series in class 5.0:\",\n      len(data_train[data_train[:,0]==5.0]))\n```", "```py\n# Display readings from each class\nfor j in np.unique(data_train[:,0]):\n    dataPlot = data_train[data_train[:,0]==j]\n    cnt = len(dataPlot)\n    dataPlot = dataPlot[:,1:].mean(axis=0)\n    print(\" Class \",j,\" Count \",cnt)\n    plt.plot(dataPlot)\n    plt.show()\n```", "```py\n# Prepare data - Scale\nX_train = TimeSeriesScalerMeanVariance(mu=0., std=1.).fit_transform(X_train)\nX_test = TimeSeriesScalerMeanVariance(mu=0., std=1.).fit_transform(X_test)\n\n# Train using k-Shape\nks = KShape(n_clusters=5, max_iter=100, n_init=10,verbose=1,random_state=2019)\nks.fit(X_train)\n```", "```py\n# Predict on train set and calculate adjusted Rand index\npreds = ks.predict(X_train)\nars = adjusted_rand_score(data_train[:,0],preds)\nprint(\"Adjusted Rand Index on Training Set:\", ars)\n```", "```py\nAdjusted Rand Index on Training Set: 0.7499312374127193\n```", "```py\n# Predict on test set and calculate adjusted Rand index\npreds_test = ks.predict(X_test)\nars = adjusted_rand_score(data_test[:,0],preds_test)\nprint(\"Adjusted Rand Index on Test Set:\", ars)\n```", "```py\nAdjusted Rand Index on Test Set: 0.7172302400677499\n```", "```py\n# Evaluate goodness of the clusters\npreds_test = preds_test.reshape(1000,1)\npreds_test = np.hstack((preds_test,data_test[:,0].reshape(1000,1)))\npreds_test = pd.DataFrame(data=preds_test)\npreds_test = preds_test.rename(columns={0: 'prediction', 1: 'actual'})\n\ncounter = 0\nfor i in np.sort(preds_test.prediction.unique()):\n    print(\"Predicted Cluster \", i)\n    print(preds_test.actual[preds_test.prediction==i].value_counts())\n    print()\n    cnt = preds_test.actual[preds_test.prediction==i] \\\n                        .value_counts().iloc[1:].sum()\n    counter = counter + cnt\nprint(\"Count of Non-Primary Points: \", counter)\n```", "```py\nECG 5000 k-shape predicted cluster analysis\n\nPredicted Cluster 0.0\n    2.0   29\n    4.0   2\n    1.0   2\n    3.0   2\n    5.0   1\n    Name: actual, dtype: int64\n\nPredicted Cluster 1.0\n    2.0   270\n    4.0   14\n    3.0   8\n    1.0   2\n    5.0   1\n    Name: actual, dtype: int64\n\nPredicted Cluster 2.0\n    1.0   553\n    4.0   16\n    2.0   9\n    3.0   7\n    Name: actual, dtype: int64\n\nPredicted Cluster 3.0\n    2.0   35\n    1.0   5\n    4.0   5\n    5.0   3\n    3.0   3\n    Name: actual, dtype: int64\n\nPredicted Cluster 4.0\n    1.0   30\n    4.0   1\n    3.0   1\n    2.0   1\n    Name: actual, dtype: int64\n\nCount of Non-Primary Points: 83\n```", "```py\n# Train using Time Series k-Means\nkm = TimeSeriesKMeans(n_clusters=5, max_iter=100, n_init=100, \\\n                      metric=\"euclidean\", verbose=1, random_state=2019)\nkm.fit(X_train)\n\n# Predict on training set and evaluate using adjusted Rand index\npreds = km.predict(X_train)\nars = adjusted_rand_score(data_train[:,0],preds)\nprint(\"Adjusted Rand Index on Training Set:\", ars)\n\n# Predict on test set and evaluate using adjusted Rand index\npreds_test = km.predict(X_test)\nars = adjusted_rand_score(data_test[:,0],preds_test)\nprint(\"Adjusted Rand Index on Test Set:\", ars)\n```", "```py\nAdjusted Rand Index of Time Series k-Means on Training Set: 0.5063464656715959\n```", "```py\nAdjusted Rand Index of Time Series k-Means on Test Set: 0.4864981997585834\n```", "```py\n# Train model and evaluate on training set\nmin_cluster_size = 5\nmin_samples = None\nalpha = 1.0\ncluster_selection_method = 'eom'\nprediction_data = True\n\nhdb = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, \\\n                      min_samples=min_samples, alpha=alpha, \\\n                      cluster_selection_method=cluster_selection_method, \\\n                      prediction_data=prediction_data)\n\npreds = hdb.fit_predict(X_train.reshape(4000,140))\nars = adjusted_rand_score(data_train[:,0],preds)\nprint(\"Adjusted Rand Index on Training Set:\", ars)\n```", "```py\nAdjusted Rand Index on Training Set using HDBSCAN: 0.7689563655060421\n```", "```py\n# Predict on test set and evaluate\npreds_test = hdbscan.prediction.approximate_predict( \\\n                hdb, X_test.reshape(1000,140))\nars = adjusted_rand_score(data_test[:,0],preds_test[0])\nprint(\"Adjusted Rand Index on Test Set:\", ars)\n```", "```py\nAdjusted Rand Index on Test Set using HDBSCAN: 0.7200816245545564\n```", "```py\n# Load the datasets\ncurrent_path = os.getcwd()\nfile = '\\\\datasets\\\\ucr_time_series_data\\\\'\n\nmypath = current_path + file\nd = []\nf = []\nfor (dirpath, dirnames, filenames) in walk(mypath):\n    for i in dirnames:\n        newpath = mypath+\"\\\\\"+i+\"\\\\\"\n        onlyfiles = [f for f in listdir(newpath) if isfile(join(newpath, f))]\n        f.extend(onlyfiles)\n    d.extend(dirnames)\n    break\n```", "```py\n# k-Shape Experiment\nkShapeDF = pd.DataFrame(data=[],index=[v for v in d],\n                        columns=[\"Train ARS\",\"Test ARS\"])\n\n# Train and Evaluate k-Shape\nclass ElapsedTimer(object):\n    def __init__(self):\n        self.start_time = time.time()\n    def elapsed(self,sec):\n        if sec < 60:\n            return str(sec) + \" sec\"\n        elif sec < (60 * 60):\n            return str(sec / 60) + \" min\"\n        else:\n            return str(sec / (60 * 60)) + \" hr\"\n    def elapsed_time(self):\n        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time))\n        return (time.time() - self.start_time)\n\ntimer = ElapsedTimer()\ncnt = 0\nfor i in d:\n    cnt += 1\n    print(\"Dataset \", cnt)\n    newpath = mypath+\"\\\\\"+i+\"\\\\\"\n    onlyfiles = [f for f in listdir(newpath) if isfile(join(newpath, f))]\n    j = onlyfiles[0]\n    k = onlyfiles[1]\n    data_train = np.loadtxt(newpath+j, delimiter=\",\")\n    data_test = np.loadtxt(newpath+k, delimiter=\",\")\n\n    data_joined = np.concatenate((data_train,data_test),axis=0)\n    data_train, data_test = train_test_split(data_joined,\n                                        test_size=0.20, random_state=2019)\n\n    X_train = to_time_series_dataset(data_train[:, 1:])\n    y_train = data_train[:, 0].astype(np.int)\n    X_test = to_time_series_dataset(data_test[:, 1:])\n    y_test = data_test[:, 0].astype(np.int)\n\n    X_train = TimeSeriesScalerMeanVariance(mu=0., std=1.) \\\n                                .fit_transform(X_train)\n    X_test = TimeSeriesScalerMeanVariance(mu=0., std=1.) \\\n                                .fit_transform(X_test)\n\n    classes = len(np.unique(data_train[:,0]))\n    ks = KShape(n_clusters=classes, max_iter=10, n_init=3,verbose=0)\n    ks.fit(X_train)\n\n    print(i)\n    preds = ks.predict(X_train)\n    ars = adjusted_rand_score(data_train[:,0],preds)\n    print(\"Adjusted Rand Index on Training Set:\", ars)\n    kShapeDF.loc[i,\"Train ARS\"] = ars\n\n    preds_test = ks.predict(X_test)\n    ars = adjusted_rand_score(data_test[:,0],preds_test)\n    print(\"Adjusted Rand Index on Test Set:\", ars)\n    kShapeDF.loc[i,\"Test ARS\"] = ars\n\nkShapeTime = timer.elapsed_time()\n```", "```py\n# k-Means Experiment - FULL RUN\n# Create dataframe\nkMeansDF = pd.DataFrame(data=[],index=[v for v in d], \\\n                        columns=[\"Train ARS\",\"Test ARS\"])\n\n# Train and Evaluate k-Means\ntimer = ElapsedTimer()\ncnt = 0\nfor i in d:\n    cnt += 1\n    print(\"Dataset \", cnt)\n    newpath = mypath+\"\\\\\"+i+\"\\\\\"\n    onlyfiles = [f for f in listdir(newpath) if isfile(join(newpath, f))]\n    j = onlyfiles[0]\n    k = onlyfiles[1]\n    data_train = np.loadtxt(newpath+j, delimiter=\",\")\n    data_test = np.loadtxt(newpath+k, delimiter=\",\")\n\n    data_joined = np.concatenate((data_train,data_test),axis=0)\n    data_train, data_test = train_test_split(data_joined, \\\n                                        test_size=0.20, random_state=2019)\n\n    X_train = to_time_series_dataset(data_train[:, 1:])\n    y_train = data_train[:, 0].astype(np.int)\n    X_test = to_time_series_dataset(data_test[:, 1:])\n    y_test = data_test[:, 0].astype(np.int)\n\n    X_train = TimeSeriesScalerMeanVariance(mu=0., std=1.) \\\n                                    .fit_transform(X_train)\n    X_test = TimeSeriesScalerMeanVariance(mu=0., std=1.) \\\n                                    .fit_transform(X_test)\n\n    classes = len(np.unique(data_train[:,0]))\n    km = TimeSeriesKMeans(n_clusters=5, max_iter=10, n_init=10, \\\n                          metric=\"euclidean\", verbose=0, random_state=2019)\n    km.fit(X_train)\n\n    print(i)\n    preds = km.predict(X_train)\n    ars = adjusted_rand_score(data_train[:,0],preds)\n    print(\"Adjusted Rand Index on Training Set:\", ars)\n    kMeansDF.loc[i,\"Train ARS\"] = ars\n\n    preds_test = km.predict(X_test)\n    ars = adjusted_rand_score(data_test[:,0],preds_test)\n    print(\"Adjusted Rand Index on Test Set:\", ars)\n    kMeansDF.loc[i,\"Test ARS\"] = ars\n\nkMeansTime = timer.elapsed_time()\n```", "```py\n# HDBSCAN Experiment - FULL RUN\n# Create dataframe\nhdbscanDF = pd.DataFrame(data=[],index=[v for v in d], \\\n                         columns=[\"Train ARS\",\"Test ARS\"])\n\n# Train and Evaluate HDBSCAN\ntimer = ElapsedTimer()\ncnt = 0\nfor i in d:\n    cnt += 1\n    print(\"Dataset \", cnt)\n    newpath = mypath+\"\\\\\"+i+\"\\\\\"\n    onlyfiles = [f for f in listdir(newpath) if isfile(join(newpath, f))]\n    j = onlyfiles[0]\n    k = onlyfiles[1]\n    data_train = np.loadtxt(newpath+j, delimiter=\",\")\n    data_test = np.loadtxt(newpath+k, delimiter=\",\")\n\n    data_joined = np.concatenate((data_train,data_test),axis=0)\n    data_train, data_test = train_test_split(data_joined, \\\n                                    test_size=0.20, random_state=2019)\n\n    X_train = data_train[:, 1:]\n    y_train = data_train[:, 0].astype(np.int)\n    X_test = data_test[:, 1:]\n    y_test = data_test[:, 0].astype(np.int)\n\n    X_train = TimeSeriesScalerMeanVariance(mu=0., std=1.) \\\n                                    .fit_transform(X_train)\n    X_test = TimeSeriesScalerMeanVariance(mu=0., std=1.)  \\\n                                    .fit_transform(X_test)\n\n    classes = len(np.unique(data_train[:,0]))\n    min_cluster_size = 5\n    min_samples = None\n    alpha = 1.0\n    cluster_selection_method = 'eom'\n    prediction_data = True\n\n    hdb = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, \\\n                          min_samples=min_samples, alpha=alpha, \\\n                          cluster_selection_method= \\\n                              cluster_selection_method, \\\n                          prediction_data=prediction_data)\n\n    print(i)\n    preds = hdb.fit_predict(X_train.reshape(X_train.shape[0], \\\n                                            X_train.shape[1]))\n    ars = adjusted_rand_score(data_train[:,0],preds)\n    print(\"Adjusted Rand Index on Training Set:\", ars)\n    hdbscanDF.loc[i,\"Train ARS\"] = ars\n\n    preds_test = hdbscan.prediction.approximate_predict(hdb,\n                            X_test.reshape(X_test.shape[0], \\\n                                           X_test.shape[1]))\n    ars = adjusted_rand_score(data_test[:,0],preds_test[0])\n    print(\"Adjusted Rand Index on Test Set:\", ars)\n    hdbscanDF.loc[i,\"Test ARS\"] = ars\n\nhdbscanTime = timer.elapsed_time()\n```", "```py\nk-Shape Results\n\nTrain ARS     0.165139\nTest ARS      0.151103\n```", "```py\nk-Means Results\n\nTrain ARS     0.184789\nTest ARS      0.178960\n```", "```py\nHDBSCAN Results\n\nTrain ARS     0.178754\nTest ARS 0.158238\n```", "```py\n# Count top place finishes\ntimeSeriesClusteringDF = pd.DataFrame(data=[],index=kShapeDF.index, \\\n                            columns=[\"kShapeTest\", \\\n                                    \"kMeansTest\", \\\n                                    \"hdbscanTest\"])\n\ntimeSeriesClusteringDF.kShapeTest = kShapeDF[\"Test ARS\"]\ntimeSeriesClusteringDF.kMeansTest = kMeansDF[\"Test ARS\"]\ntimeSeriesClusteringDF.hdbscanTest = hdbscanDF[\"Test ARS\"]\n\ntscResults = timeSeriesClusteringDF.copy()\n\nfor i in range(0,len(tscResults)):\n    maxValue = tscResults.iloc[i].max()\n    tscResults.iloc[i][tscResults.iloc[i]==maxValue]=1\n    minValue = tscResults .iloc[i].min()\n    tscResults.iloc[i][tscResults.iloc[i]==minValue]=-1\n    medianValue = tscResults.iloc[i].median()\n    tscResults.iloc[i][tscResults.iloc[i]==medianValue]=0\n```", "```py\n# Show results\ntscResultsDF = pd.DataFrame(data=np.zeros((3,3)), \\\n                index=[\"firstPlace\",\"secondPlace\",\"thirdPlace\"], \\\n                columns=[\"kShape\", \"kMeans\",\"hdbscan\"])\ntscResultsDF.loc[\"firstPlace\",:] = tscResults[tscResults==1].count().values\ntscResultsDF.loc[\"secondPlace\",:] = tscResults[tscResults==0].count().values\ntscResultsDF.loc[\"thirdPlace\",:] = tscResults[tscResults==-1].count().values\ntscResultsDF\n```"]