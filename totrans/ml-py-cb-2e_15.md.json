["```py\n# Load libraries\nfrom sklearn import datasets\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.preprocessing import StandardScaler\n\n# Load data\niris = datasets.load_iris()\nfeatures = iris.data\n\n# Create standardizer\nstandardizer = StandardScaler()\n\n# Standardize features\nfeatures_standardized = standardizer.fit_transform(features)\n\n# Two nearest neighbors\nnearest_neighbors = NearestNeighbors(n_neighbors=2).fit(features_standardized)\n\n# Create an observation\nnew_observation = [ 1,  1,  1,  1]\n\n# Find distances and indices of the observation's nearest neighbors\ndistances, indices = nearest_neighbors.kneighbors([new_observation])\n\n# View the nearest neighbors\nfeatures_standardized[indices]\n```", "```py\narray([[[1.03800476, 0.55861082, 1.10378283, 1.18556721],\n        [0.79566902, 0.32841405, 0.76275827, 1.05393502]]])\n```", "```py\n# Find two nearest neighbors based on Euclidean distance\nnearestneighbors_euclidean = NearestNeighbors(\n    n_neighbors=2, metric='euclidean').fit(features_standardized)\n```", "```py\n# View distances\ndistances\n```", "```py\narray([[0.49140089, 0.74294782]])\n```", "```py\n# Find each observation's three nearest neighbors\n# based on Euclidean distance (including itself)\nnearestneighbors_euclidean = NearestNeighbors(\n    n_neighbors=3, metric=\"euclidean\").fit(features_standardized)\n\n# List of lists indicating each observation's three nearest neighbors\n# (including itself)\nnearest_neighbors_with_self = nearestneighbors_euclidean.kneighbors_graph(\n    features_standardized).toarray()\n\n# Remove 1s marking an observation is a nearest neighbor to itself\nfor i, x in enumerate(nearest_neighbors_with_self):\n    x[i] = 0\n\n# View first observation's two nearest neighbors\nnearest_neighbors_with_self[0]\n```", "```py\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n```", "```py\n# Load libraries\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import datasets\n\n# Load data\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Create standardizer\nstandardizer = StandardScaler()\n\n# Standardize features\nX_std = standardizer.fit_transform(X)\n\n# Train a KNN classifier with 5 neighbors\nknn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1).fit(X_std, y)\n\n# Create two observations\nnew_observations = [[ 0.75,  0.75,  0.75,  0.75],\n                    [ 1,  1,  1,  1]]\n\n# Predict the class of two observations\nknn.predict(new_observations)\n```", "```py\narray([1, 2])\n```", "```py\n# View probability that each observation is one of three classes\nknn.predict_proba(new_observations)\n```", "```py\narray([[0\\. , 0.6, 0.4],\n       [0\\. , 0\\. , 1\\. ]])\n```", "```py\nknn.predict(new_observations)\n```", "```py\narray([1, 2])\n```", "```py\n# Load libraries\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.model_selection import GridSearchCV\n\n# Load data\niris = datasets.load_iris()\nfeatures = iris.data\ntarget = iris.target\n\n# Create standardizer\nstandardizer = StandardScaler()\n\n# Create a KNN classifier\nknn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n\n# Create a pipeline\npipe = Pipeline([(\"standardizer\", standardizer), (\"knn\", knn)])\n\n# Create space of candidate values\nsearch_space = [{\"knn__n_neighbors\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}]\n\n# Create grid search\nclassifier = GridSearchCV(\n    pipe, search_space, cv=5, verbose=0).fit(features_standardized, target)\n```", "```py\n# Best neighborhood size (k)\nclassifier.best_estimator_.get_params()[\"knn__n_neighbors\"]\n```", "```py\n6\n```", "```py\n# Load libraries\nfrom sklearn.neighbors import RadiusNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import datasets\n\n# Load data\niris = datasets.load_iris()\nfeatures = iris.data\ntarget = iris.target\n\n# Create standardizer\nstandardizer = StandardScaler()\n\n# Standardize features\nfeatures_standardized = standardizer.fit_transform(features)\n\n# Train a radius neighbors classifier\nrnn = RadiusNeighborsClassifier(\n    radius=.5, n_jobs=-1).fit(features_standardized, target)\n\n# Create two observations\nnew_observations = [[ 1,  1,  1,  1]]\n\n# Predict the class of two observations\nrnn.predict(new_observations)\n```", "```py\narray([2])\n```", "```py\n# Load libraries\nimport faiss\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.preprocessing import StandardScaler\n\n# Load data\niris = datasets.load_iris()\nfeatures = iris.data\n\n# Create standardizer\nstandardizer = StandardScaler()\n\n# Standardize features\nfeatures_standardized = standardizer.fit_transform(features)\n\n# Set faiss parameters\nn_features = features_standardized.shape[1]\nnlist = 3\nk = 2\n\n# Create an IVF index\nquantizer = faiss.IndexFlatIP(n_features)\nindex = faiss.IndexIVFFlat(quantizer, n_features, nlist)\n\n# Train the index and add feature vectors\nindex.train(features_standardized)\nindex.add(features_standardized)\n\n# Create an observation\nnew_observation = np.array([[ 1,  1,  1,  1]])\n\n# Search the index for the 2 nearest neighbors\ndistances, indices = index.search(new_observation, k)\n\n# Show the feature vectors for the two nearest neighbors\nnp.array([list(features_standardized[i]) for i in indices[0]])\n```", "```py\narray([[1.03800476, 0.55861082, 1.10378283, 1.18556721],\n       [0.79566902, 0.32841405, 0.76275827, 1.05393502]])\n```", "```py\n# Load libraries\nimport faiss\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.preprocessing import StandardScaler\n\n# Number of nearest neighbors\nk = 10\n\n# Load data\niris = datasets.load_iris()\nfeatures = iris.data\n\n# Create standardizer\nstandardizer = StandardScaler()\n\n# Standardize features\nfeatures_standardized = standardizer.fit_transform(features)\n\n# Create KNN with 10 NN\nnearest_neighbors = NearestNeighbors(n_neighbors=k).fit(features_standardized)\n\n# Set faiss parameters\nn_features = features_standardized.shape[1]\nnlist = 3\n\n# Create an IVF index\nquantizer = faiss.IndexFlatIP(n_features)\nindex = faiss.IndexIVFFlat(quantizer, n_features, nlist)\n\n# Train the index and add feature vectors\nindex.train(features_standardized)\nindex.add(features_standardized)\nindex.nprobe = 1\n\n# Create an observation\nnew_observation = np.array([[ 1,  1,  1,  1]])\n\n# Find distances and indices of the observation's exact nearest neighbors\nknn_distances, knn_indices = nearest_neighbors.kneighbors(new_observation)\n\n# Search the index for the two nearest neighbors\nivf_distances, ivf_indices = index.search(new_observation, k)\n\n# Get the set overlap\nrecalled_items = set(list(knn_indices[0])) & set(list(ivf_indices[0]))\n\n# Print the recall\nprint(f\"Recall @k={k}: {len(recalled_items)/k * 100}%\")\n```", "```py\nRecall @k=10: 100.0%\n```"]