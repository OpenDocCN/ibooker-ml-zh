<html><head></head><body><section data-pdf-bookmark="Chapter 11. Feature Detection Using Deep Belief Networks" data-type="chapter" epub:type="chapter"><div class="chapter" id="Chapter_11">&#13;
<h1><span class="label">Chapter 11. </span>Feature Detection Using <span class="keep-together">Deep Belief Networks</span></h1>&#13;
&#13;
&#13;
<p>In <a data-type="xref" href="ch10.html#Chapter_10">Chapter 10</a>, we<a data-primary="unsupervised deep learning" data-secondary="feature detection using DBNs" data-type="indexterm" id="UDLfeature11"/><a data-primary="feature detection" data-see="also deep belief networks (DBNs)" data-type="indexterm" id="idm140637533946832"/> explored restricted Boltzmann machines and used them to build a recommender system for movie ratings. In this chapter, we will stack RBMs together to build <em>deep belief networks (DBNs)</em>. DBNs were first introduced by <a data-primary="Hinton, Geoffrey" data-type="indexterm" id="idm140637534116512"/>Geoff Hinton at the University of Toronto in 2006.</p>&#13;
&#13;
<p>RBMs<a data-primary="deep belief networks (DBNs)" data-secondary="versus RBMs" data-type="indexterm" id="idm140637534115328"/><a data-primary="restricted Boltzmann machines (RBMs)" data-secondary="versus DBNs" data-type="indexterm" id="idm140637534114256"/> have just two layers, a visible layer and a hidden layer; in other words, RBMs are just shallow neural networks. DBNs are made up of multiple RBMs—the hidden layer of one RBM serves as the visible layer of the next RBM. Because they involve many layers, DBNs are deep neural networks. In fact, they are the first type of deep unsupervised neural network we’ve introduced so far.</p>&#13;
&#13;
<p>Shallow unsupervised neural networks, such as RBMs, cannot capture structure in complex data such as images, sound, and text, but DBNs can. DBNs have been used to recognize and cluster images, video capture, sound, and text, although other deep learning methods have surpassed DBNs in performance over the past decade.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Deep Belief Networks in Detail" data-type="sect1"><div class="sect1" id="idm140637534111920">&#13;
<h1>Deep Belief Networks in Detail</h1>&#13;
&#13;
<p>Like<a data-primary="deep belief networks (DBNs)" data-secondary="structure and function of" data-type="indexterm" id="idm140637534110384"/> RBMs, DBNs can learn the underlying structure of input and probabilistically reconstruct it. In other words, DBNs—like RBMs—are generative models. And, as with RBMs, the layers in DBNs have connections only between layers but not between units within each layer.</p>&#13;
&#13;
<p>In the DBN, one layer is trained at a time, starting with the very first hidden layer, which, along with the input layer, makes up the first RBM. Once this first RBM is trained, the hidden layer of the first RBM serves as the visible layer of the next RBM and is used to train the second hidden layer of the DBN.</p>&#13;
&#13;
<p>This process continues until all the layers of the DBN are trained. Except for the first and final layers of the DBN, each layer in the DBN serves as both a hidden layer and a visible layer of an RBM.</p>&#13;
&#13;
<p>The DBN is a hierarchy of representations and, like all neural networks, is a form of representation learning. Note that the DBN does not use any labels. Instead, the DBN is learning the underlying structure in the input data one layer at a time.</p>&#13;
&#13;
<p>Labels<a data-primary="pretraining" data-type="indexterm" id="idm140637534106432"/><a data-primary="fine-tuning" data-type="indexterm" id="idm140637534105696"/> can be used to fine-tune the last few layers of the DBN but only after the initial unsupervised learning has been completed. For example, if we want the DBN to be a classifier, we would perform unsupervised learning first (a process known as <em>pre-training</em>) and then use labels to fine-tune the DBN (a process called <em>fine-tuning</em>).</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="MNIST Image Classification" data-type="sect1"><div class="sect1" id="idm140637534103712">&#13;
<h1>MNIST Image Classification</h1>&#13;
&#13;
<p>Let’s<a data-primary="deep belief networks (DBNs)" data-secondary="MNIST image classification" data-type="indexterm" id="idm140637534102304"/><a data-primary="MNIST digits database" data-type="indexterm" id="idm140637534101216"/> build an image classifier using DBNs. We will turn to the MNIST dataset once again.</p>&#13;
&#13;
<p>First, let’s load the necessary libraries:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="sd">'''Main'''</code>&#13;
<code class="kn">import</code> <code class="nn">numpy</code> <code class="kn">as</code> <code class="nn">np</code>&#13;
<code class="kn">import</code> <code class="nn">pandas</code> <code class="kn">as</code> <code class="nn">pd</code>&#13;
<code class="kn">import</code> <code class="nn">os</code><code class="o">,</code> <code class="nn">time</code><code class="o">,</code> <code class="nn">re</code>&#13;
<code class="kn">import</code> <code class="nn">pickle</code><code class="o">,</code> <code class="nn">gzip</code><code class="o">,</code> <code class="nn">datetime</code>&#13;
&#13;
<code class="sd">'''Data Viz'''</code>&#13;
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="kn">as</code> <code class="nn">plt</code>&#13;
<code class="kn">import</code> <code class="nn">seaborn</code> <code class="kn">as</code> <code class="nn">sns</code>&#13;
<code class="n">color</code> <code class="o">=</code> <code class="n">sns</code><code class="o">.</code><code class="n">color_palette</code><code class="p">()</code>&#13;
<code class="kn">import</code> <code class="nn">matplotlib</code> <code class="kn">as</code> <code class="nn">mpl</code>&#13;
&#13;
<code class="o">%</code><code class="n">matplotlib</code> <code class="n">inline</code>&#13;
&#13;
<code class="sd">'''Data Prep and Model Evaluation'''</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">preprocessing</code> <code class="k">as</code> <code class="n">pp</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">StratifiedKFold</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">log_loss</code><code class="p">,</code> <code class="n">accuracy_score</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">precision_recall_curve</code><code class="p">,</code> <code class="n">average_precision_score</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">roc_curve</code><code class="p">,</code> <code class="n">auc</code><code class="p">,</code> <code class="n">roc_auc_score</code><code class="p">,</code> <code class="n">mean_squared_error</code>&#13;
&#13;
<code class="sd">'''Algos'''</code>&#13;
<code class="kn">import</code> <code class="nn">lightgbm</code> <code class="kn">as</code> <code class="nn">lgb</code>&#13;
&#13;
<code class="sd">'''TensorFlow and Keras'''</code>&#13;
<code class="kn">import</code> <code class="nn">tensorflow</code> <code class="kn">as</code> <code class="nn">tf</code>&#13;
<code class="kn">import</code> <code class="nn">keras</code>&#13;
<code class="kn">from</code> <code class="nn">keras</code> <code class="kn">import</code> <code class="n">backend</code> <code class="k">as</code> <code class="n">K</code>&#13;
<code class="kn">from</code> <code class="nn">keras.models</code> <code class="kn">import</code> <code class="n">Sequential</code><code class="p">,</code> <code class="n">Model</code>&#13;
<code class="kn">from</code> <code class="nn">keras.layers</code> <code class="kn">import</code> <code class="n">Activation</code><code class="p">,</code> <code class="n">Dense</code><code class="p">,</code> <code class="n">Dropout</code>&#13;
<code class="kn">from</code> <code class="nn">keras.layers</code> <code class="kn">import</code> <code class="n">BatchNormalization</code><code class="p">,</code> <code class="n">Input</code><code class="p">,</code> <code class="n">Lambda</code>&#13;
<code class="kn">from</code> <code class="nn">keras.layers</code> <code class="kn">import</code> <code class="n">Embedding</code><code class="p">,</code> <code class="n">Flatten</code><code class="p">,</code> <code class="n">dot</code>&#13;
<code class="kn">from</code> <code class="nn">keras</code> <code class="kn">import</code> <code class="n">regularizers</code>&#13;
<code class="kn">from</code> <code class="nn">keras.losses</code> <code class="kn">import</code> <code class="n">mse</code><code class="p">,</code> <code class="n">binary_crossentropy</code></pre>&#13;
&#13;
<p>We will then load the data and store it in Pandas DataFrames. We will also encode the labels as one-hot vectors. This is all similar to what we did when we first introduced the MNIST dataset earlier in the book:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load the datasets</code>&#13;
<code class="n">current_path</code> <code class="o">=</code> <code class="n">os</code><code class="o">.</code><code class="n">getcwd</code><code class="p">()</code>&#13;
<code class="nb">file</code> <code class="o">=</code> <code class="s1">'</code><code class="se">\\</code><code class="s1">datasets</code><code class="se">\\</code><code class="s1">mnist_data</code><code class="se">\\</code><code class="s1">mnist.pkl.gz'</code>&#13;
<code class="n">f</code> <code class="o">=</code> <code class="n">gzip</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">current_path</code><code class="o">+</code><code class="nb">file</code><code class="p">,</code> <code class="s1">'rb'</code><code class="p">)</code>&#13;
<code class="n">train_set</code><code class="p">,</code> <code class="n">validation_set</code><code class="p">,</code> <code class="n">test_set</code> <code class="o">=</code> <code class="n">pickle</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="n">f</code><code class="p">,</code> <code class="n">encoding</code><code class="o">=</code><code class="s1">'latin1'</code><code class="p">)</code>&#13;
<code class="n">f</code><code class="o">.</code><code class="n">close</code><code class="p">()</code>&#13;
&#13;
<code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code> <code class="o">=</code> <code class="n">train_set</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">train_set</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>&#13;
<code class="n">X_validation</code><code class="p">,</code> <code class="n">y_validation</code> <code class="o">=</code> <code class="n">validation_set</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">validation_set</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>&#13;
<code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">test_set</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">test_set</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>&#13;
&#13;
<code class="c1"># Create Pandas DataFrames from the datasets</code>&#13;
<code class="n">train_index</code> <code class="o">=</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code><code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">))</code>&#13;
<code class="n">validation_index</code> <code class="o">=</code> <code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">),</code><code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code><code class="o">+</code><code class="nb">len</code><code class="p">(</code><code class="n">X_validation</code><code class="p">))</code>&#13;
<code class="n">test_index</code> <code class="o">=</code> <code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code><code class="o">+</code><code class="nb">len</code><code class="p">(</code><code class="n">X_validation</code><code class="p">),</code> \&#13;
                   <code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code><code class="o">+</code><code class="nb">len</code><code class="p">(</code><code class="n">X_validation</code><code class="p">)</code><code class="o">+</code><code class="nb">len</code><code class="p">(</code><code class="n">X_test</code><code class="p">))</code>&#13;
&#13;
<code class="n">X_train</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">train_index</code><code class="p">)</code>&#13;
<code class="n">y_train</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">y_train</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">train_index</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_validation</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_validation</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">validation_index</code><code class="p">)</code>&#13;
<code class="n">y_validation</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">y_validation</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">validation_index</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_test</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_test</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">test_index</code><code class="p">)</code>&#13;
<code class="n">y_test</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">y_test</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">test_index</code><code class="p">)</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">view_digit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">example</code><code class="p">):</code>&#13;
    <code class="n">label</code> <code class="o">=</code> <code class="n">y</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">example</code><code class="p">]</code>&#13;
    <code class="n">image</code> <code class="o">=</code> <code class="n">X</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">example</code><code class="p">,:]</code><code class="o">.</code><code class="n">values</code><code class="o">.</code><code class="n">reshape</code><code class="p">([</code><code class="mi">28</code><code class="p">,</code><code class="mi">28</code><code class="p">])</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s1">'Example: </code><code class="si">%d</code><code class="s1">  Label: </code><code class="si">%d</code><code class="s1">'</code> <code class="o">%</code> <code class="p">(</code><code class="n">example</code><code class="p">,</code> <code class="n">label</code><code class="p">))</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="n">plt</code><code class="o">.</code><code class="n">get_cmap</code><code class="p">(</code><code class="s1">'gray'</code><code class="p">))</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">one_hot</code><code class="p">(</code><code class="n">series</code><code class="p">):</code>&#13;
    <code class="n">label_binarizer</code> <code class="o">=</code> <code class="n">pp</code><code class="o">.</code><code class="n">LabelBinarizer</code><code class="p">()</code>&#13;
    <code class="n">label_binarizer</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="nb">max</code><code class="p">(</code><code class="n">series</code><code class="p">)</code><code class="o">+</code><code class="mi">1</code><code class="p">))</code>&#13;
    <code class="k">return</code> <code class="n">label_binarizer</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">series</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Create one-hot vectors for the labels</code>&#13;
<code class="n">y_train_oneHot</code> <code class="o">=</code> <code class="n">one_hot</code><code class="p">(</code><code class="n">y_train</code><code class="p">)</code>&#13;
<code class="n">y_validation_oneHot</code> <code class="o">=</code> <code class="n">one_hot</code><code class="p">(</code><code class="n">y_validation</code><code class="p">)</code>&#13;
<code class="n">y_test_oneHot</code> <code class="o">=</code> <code class="n">one_hot</code><code class="p">(</code><code class="n">y_test</code><code class="p">)</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Restricted Boltzmann Machines" data-type="sect1"><div class="sect1" id="idm140637533666832">&#13;
<h1>Restricted Boltzmann Machines</h1>&#13;
&#13;
<p>Next, let’s<a data-primary="deep belief networks (DBNs)" data-secondary="restricted Boltzmann machines (RBMs)" data-type="indexterm" id="DBNrest11"/> define an RBM class so we can train several RBMs (which are the building blocks for DBNs) in quick succession.</p>&#13;
&#13;
<p>Remember that RBMs have an input layer (also referred to as the visible layer) and a single hidden layer, and the connections among neurons are restricted such that neurons are connected only to the neurons in other layers but not to neurons within the same layer. Also, recall that communication between layers happens in both directions—not just in one direction or a feedforward way, as in the case of autoencoders.</p>&#13;
&#13;
<p>In an RBM, the neurons in the visible layer communicate with the hidden layer, the hidden layer generates data from the probabilistic model the RBM has learned, and then the hidden layer passes this generated information back to the visible layer. The visible layer takes the generated data from the hidden layer, samples it, compares it to the original data, and, based on the reconstruction error between the generated data sample and the original data, sends new information to the hidden layer to repeat the process once again.</p>&#13;
&#13;
<p>By communicating in this bidirectional way, the RBM develops a generative model such that the reconstructions from the output of the hidden layer are similar to the original input.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Build the Components of the RBM Class" data-type="sect2"><div class="sect2" id="idm140637533326800">&#13;
<h2>Build the Components of the RBM Class</h2>&#13;
&#13;
<p>Like we did in <a data-type="xref" href="ch10.html#Chapter_10">Chapter 10</a>, let’s walk through the various components of the <code>RBM</code> class.</p>&#13;
&#13;
<p>First, we will initialize the class with a few parameters; these are the input size of the RBM, the output size, the learning rate, the number of epochs to train for, and the batch size during the training process. We will also create zero matrices for the weight matrix, the hidden bias vector, and the visible bias vector:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Define RBM class</code>&#13;
<code class="k">class</code> <code class="nc">RBM</code><code class="p">(</code><code class="nb">object</code><code class="p">):</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">input_size</code><code class="p">,</code> <code class="n">output_size</code><code class="p">,</code>&#13;
                 <code class="n">learning_rate</code><code class="p">,</code> <code class="n">epochs</code><code class="p">,</code> <code class="n">batchsize</code><code class="p">):</code>&#13;
        <code class="c1"># Define hyperparameters</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">_input_size</code> <code class="o">=</code> <code class="n">input_size</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">_output_size</code> <code class="o">=</code> <code class="n">output_size</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">learning_rate</code> <code class="o">=</code> <code class="n">learning_rate</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">epochs</code> <code class="o">=</code> <code class="n">epochs</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">batchsize</code> <code class="o">=</code> <code class="n">batchsize</code>&#13;
&#13;
        <code class="c1"># Initialize weights and biases using zero matrices</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">w</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">([</code><code class="n">input_size</code><code class="p">,</code> <code class="n">output_size</code><code class="p">],</code> <code class="s2">"float"</code><code class="p">)</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">hb</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">([</code><code class="n">output_size</code><code class="p">],</code> <code class="s2">"float"</code><code class="p">)</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">vb</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">([</code><code class="n">input_size</code><code class="p">],</code> <code class="s2">"float"</code><code class="p">)</code></pre>&#13;
&#13;
<p>Next, let’s define functions for the forward pass, the backward pass, and the sampling of data during each of these passes back and forth.</p>&#13;
&#13;
<p>Here is the forward pass, where <em>h</em> is the hidden layer and <em>v</em> is the visible layer:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">prob_h_given_v</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">visible</code><code class="p">,</code> <code class="n">w</code><code class="p">,</code> <code class="n">hb</code><code class="p">):</code>&#13;
    <code class="k">return</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">visible</code><code class="p">,</code> <code class="n">w</code><code class="p">)</code> <code class="o">+</code> <code class="n">hb</code><code class="p">)</code></pre>&#13;
&#13;
<p>Here is the backward pass:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">prob_v_given_h</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">hidden</code><code class="p">,</code> <code class="n">w</code><code class="p">,</code> <code class="n">vb</code><code class="p">):</code>&#13;
    <code class="k">return</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">hidden</code><code class="p">,</code> <code class="n">tf</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="n">w</code><code class="p">))</code> <code class="o">+</code> <code class="n">vb</code><code class="p">)</code></pre>&#13;
&#13;
<p>Here is the sampling function:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">sample_prob</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">probs</code><code class="p">):</code>&#13;
    <code class="k">return</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">sign</code><code class="p">(</code><code class="n">probs</code> <code class="o">-</code> <code class="n">tf</code><code class="o">.</code><code class="n">random_uniform</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">shape</code><code class="p">(</code><code class="n">probs</code><code class="p">))))</code></pre>&#13;
&#13;
<p>Now we need a function that performs that training. Since we are using TensorFlow, we first need to create placeholders for the TensorFlow graph, which we will use when we feed data into the TensorFlow session.</p>&#13;
&#13;
<p>We will have placeholders for the weights matrix, the hidden bias vector, and the visible bias vector. We will also need to initialize the values for these three using zeros. And, we will need one set to hold the current values and one set to hold the previous values:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">_w</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">placeholder</code><code class="p">(</code><code class="s2">"float"</code><code class="p">,</code> <code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">_input_size</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">_output_size</code><code class="p">])</code>&#13;
<code class="n">_hb</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">placeholder</code><code class="p">(</code><code class="s2">"float"</code><code class="p">,</code> <code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">_output_size</code><code class="p">])</code>&#13;
<code class="n">_vb</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">placeholder</code><code class="p">(</code><code class="s2">"float"</code><code class="p">,</code> <code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">_input_size</code><code class="p">])</code>&#13;
&#13;
<code class="n">prv_w</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">([</code><code class="bp">self</code><code class="o">.</code><code class="n">_input_size</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">_output_size</code><code class="p">],</code> <code class="s2">"float"</code><code class="p">)</code>&#13;
<code class="n">prv_hb</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">([</code><code class="bp">self</code><code class="o">.</code><code class="n">_output_size</code><code class="p">],</code> <code class="s2">"float"</code><code class="p">)</code>&#13;
<code class="n">prv_vb</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">([</code><code class="bp">self</code><code class="o">.</code><code class="n">_input_size</code><code class="p">],</code> <code class="s2">"float"</code><code class="p">)</code>&#13;
&#13;
<code class="n">cur_w</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">([</code><code class="bp">self</code><code class="o">.</code><code class="n">_input_size</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">_output_size</code><code class="p">],</code> <code class="s2">"float"</code><code class="p">)</code>&#13;
<code class="n">cur_hb</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">([</code><code class="bp">self</code><code class="o">.</code><code class="n">_output_size</code><code class="p">],</code> <code class="s2">"float"</code><code class="p">)</code>&#13;
<code class="n">cur_vb</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">([</code><code class="bp">self</code><code class="o">.</code><code class="n">_input_size</code><code class="p">],</code> <code class="s2">"float"</code><code class="p">)</code></pre>&#13;
&#13;
<p>Likewise, we need a placeholder for the visible layer. The hidden layer is derived from matrix multiplication of the visible layer and the weights matrix and the matrix addition of the hidden bias vector:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">v0</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">placeholder</code><code class="p">(</code><code class="s2">"float"</code><code class="p">,</code> <code class="p">[</code><code class="bp">None</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">_input_size</code><code class="p">])</code>&#13;
<code class="n">h0</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">sample_prob</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">prob_h_given_v</code><code class="p">(</code><code class="n">v0</code><code class="p">,</code> <code class="n">_w</code><code class="p">,</code> <code class="n">_hb</code><code class="p">))</code></pre>&#13;
&#13;
<p>During the backward pass, we take the hidden layer output, multiply it with the transpose of the weights matrix used during the forward pass, and add the visible bias vector. Note that the weights matrix is the same weights matrix during both the forward and the backward pass.</p>&#13;
&#13;
<p>Then we perform the forward pass again:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">v1</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">sample_prob</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">prob_v_given_h</code><code class="p">(</code><code class="n">h0</code><code class="p">,</code> <code class="n">_w</code><code class="p">,</code> <code class="n">_vb</code><code class="p">))</code>&#13;
<code class="n">h1</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">prob_h_given_v</code><code class="p">(</code><code class="n">v1</code><code class="p">,</code> <code class="n">_w</code><code class="p">,</code> <code class="n">_hb</code><code class="p">)</code></pre>&#13;
&#13;
<p>To update the weights, we perform constrastive divergence, which we introduced in <a data-type="xref" href="ch10.html#Chapter_10">Chapter 10</a>. We also define the error as the MSE:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">positive_grad</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="n">v0</code><code class="p">),</code> <code class="n">h0</code><code class="p">)</code>&#13;
<code class="n">negative_grad</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="n">v1</code><code class="p">),</code> <code class="n">h1</code><code class="p">)</code>&#13;
&#13;
<code class="n">update_w</code> <code class="o">=</code> <code class="n">_w</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">learning_rate</code> <code class="o">*</code> \&#13;
    <code class="p">(</code><code class="n">positive_grad</code> <code class="o">-</code> <code class="n">negative_grad</code><code class="p">)</code> <code class="o">/</code> <code class="n">tf</code><code class="o">.</code><code class="n">to_float</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">shape</code><code class="p">(</code><code class="n">v0</code><code class="p">)[</code><code class="mi">0</code><code class="p">])</code>&#13;
<code class="n">update_vb</code> <code class="o">=</code> <code class="n">_vb</code> <code class="o">+</code>  <code class="bp">self</code><code class="o">.</code><code class="n">learning_rate</code> <code class="o">*</code> <code class="n">tf</code><code class="o">.</code><code class="n">reduce_mean</code><code class="p">(</code><code class="n">v0</code> <code class="o">-</code> <code class="n">v1</code><code class="p">,</code> <code class="mi">0</code><code class="p">)</code>&#13;
<code class="n">update_hb</code> <code class="o">=</code> <code class="n">_hb</code> <code class="o">+</code>  <code class="bp">self</code><code class="o">.</code><code class="n">learning_rate</code> <code class="o">*</code> <code class="n">tf</code><code class="o">.</code><code class="n">reduce_mean</code><code class="p">(</code><code class="n">h0</code> <code class="o">-</code> <code class="n">h1</code><code class="p">,</code> <code class="mi">0</code><code class="p">)</code>&#13;
&#13;
<code class="n">err</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">reduce_mean</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">square</code><code class="p">(</code><code class="n">v0</code> <code class="o">-</code> <code class="n">v1</code><code class="p">))</code></pre>&#13;
&#13;
<p>With this, we are ready to initialize the TensorFlow session with the variables we have just defined.</p>&#13;
&#13;
<p>Once we call <code>sess.run</code>, we can feed in batches of data to begin the training. During the training, forward and backward passes will be made, and the RBM will update weights based on how the generated data compares to the original input. We will print the reconstruction error from each epoch:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">with</code> <code class="n">tf</code><code class="o">.</code><code class="n">Session</code><code class="p">()</code> <code class="k">as</code> <code class="n">sess</code><code class="p">:</code>&#13;
    <code class="n">sess</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">global_variables_initializer</code><code class="p">())</code>&#13;
&#13;
    <code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">epochs</code><code class="p">):</code>&#13;
        <code class="k">for</code> <code class="n">start</code><code class="p">,</code> <code class="n">end</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">X</code><code class="p">),</code> <code class="bp">self</code><code class="o">.</code><code class="n">batchsize</code><code class="p">),</code> \&#13;
                <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">batchsize</code><code class="p">,</code><code class="nb">len</code><code class="p">(</code><code class="n">X</code><code class="p">),</code> <code class="bp">self</code><code class="o">.</code><code class="n">batchsize</code><code class="p">)):</code>&#13;
            <code class="n">batch</code> <code class="o">=</code> <code class="n">X</code><code class="p">[</code><code class="n">start</code><code class="p">:</code><code class="n">end</code><code class="p">]</code>&#13;
            <code class="n">cur_w</code> <code class="o">=</code> <code class="n">sess</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">update_w</code><code class="p">,</code> \&#13;
                <code class="n">feed_dict</code><code class="o">=</code><code class="p">{</code><code class="n">v0</code><code class="p">:</code> <code class="n">batch</code><code class="p">,</code> <code class="n">_w</code><code class="p">:</code> <code class="n">prv_w</code><code class="p">,</code> \&#13;
                           <code class="n">_hb</code><code class="p">:</code> <code class="n">prv_hb</code><code class="p">,</code> <code class="n">_vb</code><code class="p">:</code> <code class="n">prv_vb</code><code class="p">})</code>&#13;
            <code class="n">cur_hb</code> <code class="o">=</code> <code class="n">sess</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">update_hb</code><code class="p">,</code> \&#13;
                <code class="n">feed_dict</code><code class="o">=</code><code class="p">{</code><code class="n">v0</code><code class="p">:</code> <code class="n">batch</code><code class="p">,</code> <code class="n">_w</code><code class="p">:</code> <code class="n">prv_w</code><code class="p">,</code> \&#13;
                           <code class="n">_hb</code><code class="p">:</code> <code class="n">prv_hb</code><code class="p">,</code> <code class="n">_vb</code><code class="p">:</code> <code class="n">prv_vb</code><code class="p">})</code>&#13;
            <code class="n">cur_vb</code> <code class="o">=</code> <code class="n">sess</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">update_vb</code><code class="p">,</code> \&#13;
                <code class="n">feed_dict</code><code class="o">=</code><code class="p">{</code><code class="n">v0</code><code class="p">:</code> <code class="n">batch</code><code class="p">,</code> <code class="n">_w</code><code class="p">:</code> <code class="n">prv_w</code><code class="p">,</code> \&#13;
                           <code class="n">_hb</code><code class="p">:</code> <code class="n">prv_hb</code><code class="p">,</code> <code class="n">_vb</code><code class="p">:</code> <code class="n">prv_vb</code><code class="p">})</code>&#13;
            <code class="n">prv_w</code> <code class="o">=</code> <code class="n">cur_w</code>&#13;
            <code class="n">prv_hb</code> <code class="o">=</code> <code class="n">cur_hb</code>&#13;
            <code class="n">prv_vb</code> <code class="o">=</code> <code class="n">cur_vb</code>&#13;
        <code class="n">error</code> <code class="o">=</code> <code class="n">sess</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">err</code><code class="p">,</code> <code class="n">feed_dict</code><code class="o">=</code><code class="p">{</code><code class="n">v0</code><code class="p">:</code> <code class="n">X</code><code class="p">,</code> <code class="n">_w</code><code class="p">:</code> <code class="n">cur_w</code><code class="p">,</code> \&#13;
                                        <code class="n">_vb</code><code class="p">:</code> <code class="n">cur_vb</code><code class="p">,</code> <code class="n">_hb</code><code class="p">:</code> <code class="n">cur_hb</code><code class="p">})</code>&#13;
        <code class="k">print</code> <code class="p">(</code><code class="s1">'Epoch: </code><code class="si">%d</code><code class="s1">'</code> <code class="o">%</code> <code class="n">epoch</code><code class="p">,</code><code class="s1">'reconstruction error: </code><code class="si">%f</code><code class="s1">'</code> <code class="o">%</code> <code class="n">error</code><code class="p">)</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">w</code> <code class="o">=</code> <code class="n">prv_w</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">hb</code> <code class="o">=</code> <code class="n">prv_hb</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">vb</code> <code class="o">=</code> <code class="n">prv_vb</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Generate Images Using the RBM Model" data-type="sect2"><div class="sect2" id="idm140637533325856">&#13;
<h2>Generate Images Using the RBM Model</h2>&#13;
&#13;
<p>Let’s also define a function to generate new images from the generative model that the RBM has learned:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">rbm_output</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>&#13;
&#13;
    <code class="n">input_X</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">constant</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>&#13;
    <code class="n">_w</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">constant</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">w</code><code class="p">)</code>&#13;
    <code class="n">_hb</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">constant</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">hb</code><code class="p">)</code>&#13;
    <code class="n">_vb</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">constant</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">vb</code><code class="p">)</code>&#13;
    <code class="n">out</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">input_X</code><code class="p">,</code> <code class="n">_w</code><code class="p">)</code> <code class="o">+</code> <code class="n">_hb</code><code class="p">)</code>&#13;
    <code class="n">hiddenGen</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">sample_prob</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">prob_h_given_v</code><code class="p">(</code><code class="n">input_X</code><code class="p">,</code> <code class="n">_w</code><code class="p">,</code> <code class="n">_hb</code><code class="p">))</code>&#13;
    <code class="n">visibleGen</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">sample_prob</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">prob_v_given_h</code><code class="p">(</code><code class="n">hiddenGen</code><code class="p">,</code> <code class="n">_w</code><code class="p">,</code> <code class="n">_vb</code><code class="p">))</code>&#13;
    <code class="k">with</code> <code class="n">tf</code><code class="o">.</code><code class="n">Session</code><code class="p">()</code> <code class="k">as</code> <code class="n">sess</code><code class="p">:</code>&#13;
        <code class="n">sess</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">global_variables_initializer</code><code class="p">())</code>&#13;
        <code class="k">return</code> <code class="n">sess</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">out</code><code class="p">),</code> <code class="n">sess</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">visibleGen</code><code class="p">),</code> <code class="n">sess</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">hiddenGen</code><code class="p">)</code></pre>&#13;
&#13;
<p>We feed the original matrix of images, called <em>X</em>, into the function. We create TensorFlow placeholders for the original matrix of images, the weights matrix, the hidden bias vector, and the visible bias vector. Then, we push the input matrix to produce the output of a forward pass (<code>out</code>), a sample of the hidden layer (<code>hiddenGen</code>), and a sample of the reconstructed images generated by the model (<code>visibleGen</code>).</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="View the Intermediate Feature Detectors" data-type="sect2"><div class="sect2" id="idm140637532187584">&#13;
<h2>View the Intermediate Feature Detectors</h2>&#13;
&#13;
<p>Finally, let’s<a data-primary="feature detection" data-type="indexterm" id="idm140637531801440"/> define a function to show the feature detectors of the hidden layer:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">show_features</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">shape</code><code class="p">,</code> <code class="n">suptitle</code><code class="p">,</code> <code class="n">count</code><code class="o">=-</code><code class="mi">1</code><code class="p">):</code>&#13;
    <code class="n">maxw</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">amax</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">w</code><code class="o">.</code><code class="n">T</code><code class="p">)</code>&#13;
    <code class="n">minw</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">amin</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">w</code><code class="o">.</code><code class="n">T</code><code class="p">)</code>&#13;
    <code class="n">count</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">_output_size</code> <code class="k">if</code> <code class="n">count</code> <code class="o">==</code> <code class="o">-</code><code class="mi">1</code> <code class="ow">or</code> <code class="n">count</code> <code class="o">&gt;</code> \&#13;
            <code class="bp">self</code><code class="o">.</code><code class="n">_output_size</code> <code class="k">else</code> <code class="n">count</code>&#13;
    <code class="n">ncols</code> <code class="o">=</code> <code class="n">count</code> <code class="k">if</code> <code class="n">count</code> <code class="o">&lt;</code> <code class="mi">14</code> <code class="k">else</code> <code class="mi">14</code>&#13;
    <code class="n">nrows</code> <code class="o">=</code> <code class="n">count</code><code class="o">//</code><code class="n">ncols</code>&#13;
    <code class="n">nrows</code> <code class="o">=</code> <code class="n">nrows</code> <code class="k">if</code> <code class="n">nrows</code> <code class="o">&gt;</code> <code class="mi">2</code> <code class="k">else</code> <code class="mi">3</code>&#13;
    <code class="n">fig</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="n">ncols</code><code class="p">,</code> <code class="n">nrows</code><code class="p">),</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">100</code><code class="p">)</code>&#13;
    <code class="n">grid</code> <code class="o">=</code> <code class="n">Grid</code><code class="p">(</code><code class="n">fig</code><code class="p">,</code> <code class="n">rect</code><code class="o">=</code><code class="mi">111</code><code class="p">,</code> <code class="n">nrows_ncols</code><code class="o">=</code><code class="p">(</code><code class="n">nrows</code><code class="p">,</code> <code class="n">ncols</code><code class="p">),</code> <code class="n">axes_pad</code><code class="o">=</code><code class="mf">0.01</code><code class="p">)</code>&#13;
&#13;
    <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">ax</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">grid</code><code class="p">):</code>&#13;
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">w</code><code class="o">.</code><code class="n">T</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="k">if</code> <code class="n">i</code><code class="o">&lt;</code><code class="bp">self</code><code class="o">.</code><code class="n">_input_size</code> <code class="k">else</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">(</code><code class="n">shape</code><code class="p">)</code>&#13;
        <code class="n">x</code> <code class="o">=</code> <code class="p">(</code><code class="n">x</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">)</code> <code class="o">-</code> <code class="n">minw</code><code class="p">)</code><code class="o">/</code><code class="n">maxw</code>&#13;
        <code class="n">ax</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">x</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="o">*</code><code class="n">shape</code><code class="p">),</code> <code class="n">cmap</code><code class="o">=</code><code class="n">mpl</code><code class="o">.</code><code class="n">cm</code><code class="o">.</code><code class="n">Greys</code><code class="p">)</code>&#13;
        <code class="n">ax</code><code class="o">.</code><code class="n">set_axis_off</code><code class="p">()</code>&#13;
&#13;
    <code class="n">fig</code><code class="o">.</code><code class="n">text</code><code class="p">(</code><code class="mf">0.5</code><code class="p">,</code><code class="mi">1</code><code class="p">,</code> <code class="n">suptitle</code><code class="p">,</code> <code class="n">fontsize</code><code class="o">=</code><code class="mi">20</code><code class="p">,</code> <code class="n">horizontalalignment</code><code class="o">=</code><code class="s1">'center'</code><code class="p">)</code>&#13;
    <code class="n">fig</code><code class="o">.</code><code class="n">tight_layout</code><code class="p">()</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>&#13;
    <code class="k">return</code></pre>&#13;
&#13;
<p>We will use this and the other functions on the MNIST dataset now.<a data-primary="" data-startref="DBNrest11" data-type="indexterm" id="idm140637531799584"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Train the Three RBMs for the DBN" data-type="sect1"><div class="sect1" id="idm140637531798480">&#13;
<h1>Train the Three RBMs for the DBN</h1>&#13;
&#13;
<p>We<a data-primary="deep belief networks (DBNs)" data-secondary="RBM training" data-type="indexterm" id="DBNrbmtrain11"/> will now use the MNIST data to train three RBMs, one at a time, such that the hidden layer of one RBM is used as the visible layer of the next RBM. These three RBMs will make up the DBN that we are building to perform image classification.</p>&#13;
&#13;
<p>First, let’s take the training data and store it as a NumPy array. Next, we will create a list to hold the RBMs we train called <code>rbm_list</code>. Then, we will define the hyperparameters for the three RBMs, including the input size, the output size, the learning rate, the number of epochs to train for, and the batch size for training.</p>&#13;
&#13;
<p>All of these can be built using the RBM class we defined earlier.</p>&#13;
&#13;
<p>For our DBN, we will use the following RBMs: the first will take the original 784-dimension input and output a 700-dimension matrix. The next RBM will use the 700-dimension matrix output of the first RBM and output a 600-dimension matrix. Finally, the last RBM we train will take the 600-dimension matrix and output a 500-dimension matrix.</p>&#13;
&#13;
<p>We will train all three RBMs using a learning rate of 1.0, train for 100 epochs each, and use a batch size of two hundred:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Since we are training, set input as training data</code>&#13;
<code class="n">inputX</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Create list to hold our RBMs</code>&#13;
<code class="n">rbm_list</code> <code class="o">=</code> <code class="p">[]</code>&#13;
&#13;
<code class="c1"># Define the parameters of the RBMs we will train</code>&#13;
<code class="n">rbm_list</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">RBM</code><code class="p">(</code><code class="mi">784</code><code class="p">,</code><code class="mi">700</code><code class="p">,</code><code class="mf">1.0</code><code class="p">,</code><code class="mi">100</code><code class="p">,</code><code class="mi">200</code><code class="p">))</code>&#13;
<code class="n">rbm_list</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">RBM</code><code class="p">(</code><code class="mi">700</code><code class="p">,</code><code class="mi">600</code><code class="p">,</code><code class="mf">1.0</code><code class="p">,</code><code class="mi">100</code><code class="p">,</code><code class="mi">200</code><code class="p">))</code>&#13;
<code class="n">rbm_list</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">RBM</code><code class="p">(</code><code class="mi">600</code><code class="p">,</code><code class="mi">500</code><code class="p">,</code><code class="mf">1.0</code><code class="p">,</code><code class="mi">100</code><code class="p">,</code><code class="mi">200</code><code class="p">))</code></pre>&#13;
&#13;
<p>Now let’s train the RBMs. We will store the trained RBMs in a list called <code>outputList</code>.</p>&#13;
&#13;
<p>Note that we use the <code>rbm_output</code> function we defined earlier to produce the output matrix—in other words, the hidden layer—for use as the input/visible layer of the subsequent RBM we train:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">outputList</code> <code class="o">=</code> <code class="p">[]</code>&#13;
<code class="n">error_list</code> <code class="o">=</code> <code class="p">[]</code>&#13;
<code class="c1">#For each RBM in our list</code>&#13;
<code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code><code class="nb">len</code><code class="p">(</code><code class="n">rbm_list</code><code class="p">)):</code>&#13;
    <code class="k">print</code><code class="p">(</code><code class="s1">'RBM'</code><code class="p">,</code> <code class="n">i</code><code class="o">+</code><code class="mi">1</code><code class="p">)</code>&#13;
    <code class="c1">#Train a new one</code>&#13;
    <code class="n">rbm</code> <code class="o">=</code> <code class="n">rbm_list</code><code class="p">[</code><code class="n">i</code><code class="p">]</code>&#13;
    <code class="n">err</code> <code class="o">=</code> <code class="n">rbm</code><code class="o">.</code><code class="n">train</code><code class="p">(</code><code class="n">inputX</code><code class="p">)</code>&#13;
    <code class="n">error_list</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">err</code><code class="p">)</code>&#13;
    <code class="c1">#Return the output layer</code>&#13;
    <code class="n">outputX</code><code class="p">,</code> <code class="n">reconstructedX</code><code class="p">,</code> <code class="n">hiddenX</code> <code class="o">=</code> <code class="n">rbm</code><code class="o">.</code><code class="n">rbm_output</code><code class="p">(</code><code class="n">inputX</code><code class="p">)</code>&#13;
    <code class="n">outputList</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">outputX</code><code class="p">)</code>&#13;
    <code class="n">inputX</code> <code class="o">=</code> <code class="n">hiddenX</code></pre>&#13;
&#13;
<p>The errors of each RBM decline the longer we train (see Figures <a data-type="xref" data-xrefstyle="select:labelnumber" href="#reconstruction_Errors_of_first_rbm">11-1</a>, <a data-type="xref" data-xrefstyle="select:labelnumber" href="#reconstruction_errors_of_second_rbm">11-2</a>, and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#reconstruction_errors_of_third_rbm">11-3</a>). Note that the RBM error reflects how similar the reconstructed data of a given RBM is to the data fed into the visible layer of that very RBM.</p>&#13;
&#13;
<figure><div class="figure" id="reconstruction_Errors_of_first_rbm">&#13;
<img alt="Reconstruction Errors of First RBM" src="assets/hulp_1101.png"/>&#13;
<h6><span class="label">Figure 11-1. </span>Reconstruction errors of first RBM</h6>&#13;
</div></figure>&#13;
&#13;
<figure><div class="figure" id="reconstruction_errors_of_second_rbm">&#13;
<img alt="Reconstruction Errors of Second RBM" src="assets/hulp_1102.png"/>&#13;
<h6><span class="label">Figure 11-2. </span>Reconstruction errors of second RBM</h6>&#13;
</div></figure>&#13;
&#13;
<figure><div class="figure" id="reconstruction_errors_of_third_rbm">&#13;
<img alt="Reconstruction Errors of Third RBM" src="assets/hulp_1103.png"/>&#13;
<h6><span class="label">Figure 11-3. </span>Reconstruction errors of third RBM</h6>&#13;
</div></figure>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Examine Feature Detectors" data-type="sect2"><div class="sect2" id="idm140637531553776">&#13;
<h2>Examine Feature Detectors</h2>&#13;
&#13;
<p>Now let’s view the learned features from each of the RBMs using the <code>rbm.show_features</code> function we defined earlier:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">rbm_shapes</code> <code class="o">=</code> <code class="p">[(</code><code class="mi">28</code><code class="p">,</code><code class="mi">28</code><code class="p">),(</code><code class="mi">25</code><code class="p">,</code><code class="mi">24</code><code class="p">),(</code><code class="mi">25</code><code class="p">,</code><code class="mi">20</code><code class="p">)]</code>&#13;
<code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code><code class="nb">len</code><code class="p">(</code><code class="n">rbm_list</code><code class="p">)):</code>&#13;
    <code class="n">rbm</code> <code class="o">=</code> <code class="n">rbm_list</code><code class="p">[</code><code class="n">i</code><code class="p">]</code>&#13;
    <code class="k">print</code><code class="p">(</code><code class="n">rbm</code><code class="o">.</code><code class="n">show_features</code><code class="p">(</code><code class="n">rbm_shapes</code><code class="p">[</code><code class="n">i</code><code class="p">],</code>&#13;
     <code class="s2">"RBM learned features from MNIST"</code><code class="p">,</code> <code class="mi">56</code><code class="p">))</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#learned_features_of_the_rbms">Figure 11-4</a> displays the learned features for the various RBMs.</p>&#13;
&#13;
<p>As you can see, each RBM learns increasingly abstract features from the MNIST data. The features of the first RBM vaguely resemble digits, and the features of the second and the third RBMs are increasingly nuanced and less discernible. This is pretty typical of how feature detectors work on image data; the deeper layers of the neural <span class="keep-together">network</span> recognize increasingly abstract elements from the original images.</p>&#13;
&#13;
<figure><div class="figure" id="learned_features_of_the_rbms">&#13;
<img alt="Learned Features of the RBMs" src="assets/hulp_1104.png"/>&#13;
<h6><span class="label">Figure 11-4. </span>Learned features of the RBMs</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="View Generated Images" data-type="sect2"><div class="sect2" id="idm140637531231264">&#13;
<h2>View Generated Images</h2>&#13;
&#13;
<p>Before we build the full DBN, let’s view some of the generated images from one of the RBMs we just trained.</p>&#13;
&#13;
<p>To keep things simple, we will feed the original MNIST training matrix into the first RBM we trained, which performs a forward pass and a backward pass, then will produce the generated images we need. We will compare the first ten images of the MNIST dataset with the newly generated images:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">inputX</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>&#13;
<code class="n">rbmOne</code> <code class="o">=</code> <code class="n">rbm_list</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>&#13;
&#13;
<code class="k">print</code><code class="p">(</code><code class="s1">'RBM 1'</code><code class="p">)</code>&#13;
<code class="n">outputX_rbmOne</code><code class="p">,</code> <code class="n">reconstructedX_rbmOne</code><code class="p">,</code> <code class="n">hiddenX_rbmOne</code> <code class="o">=</code>&#13;
 <code class="n">rbmOne</code><code class="o">.</code><code class="n">rbm_output</code><code class="p">(</code><code class="n">inputX</code><code class="p">)</code>&#13;
<code class="n">reconstructedX_rbmOne</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">reconstructedX_rbmOne</code><code class="p">,</code>&#13;
 <code class="n">index</code><code class="o">=</code><code class="n">X_train</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>&#13;
<code class="k">for</code> <code class="n">j</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code><code class="mi">10</code><code class="p">):</code>&#13;
    <code class="n">example</code> <code class="o">=</code> <code class="n">j</code>&#13;
    <code class="n">view_digit</code><code class="p">(</code><code class="n">reconstructedX</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">example</code><code class="p">)</code>&#13;
    <code class="n">view_digit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">example</code><code class="p">)</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#first_generated_image_of_the_first_rbm">Figure 11-5</a> shows the first image produced by the RBM compared to the first original image.</p>&#13;
&#13;
<figure><div class="figure" id="first_generated_image_of_the_first_rbm">&#13;
<img alt="First Generated Image of the First RBM" src="assets/hulp_1105.png"/>&#13;
<h6><span class="label">Figure 11-5. </span>First generated image of the first RBM</h6>&#13;
</div></figure>&#13;
&#13;
<p>As you can see, the generated image is similar to the original image — both display the digit five.</p>&#13;
&#13;
<p>Let’s view a few more images like this to compare the RBM-generated images with the original ones (see Figures <a data-type="xref" data-xrefstyle="select:labelnumber" href="#second_generated_image_of_the_first_rbm">11-6</a> through <a data-type="xref" data-xrefstyle="select:labelnumber" href="#fifth_generated_image_of_the_first_rbm">11-9</a>).</p>&#13;
&#13;
<figure><div class="figure" id="second_generated_image_of_the_first_rbm">&#13;
<img alt="Second Generated Image of the First RBM" src="assets/hulp_1106.png"/>&#13;
<h6><span class="label">Figure 11-6. </span>Second generated image of the first RBM</h6>&#13;
</div></figure>&#13;
&#13;
<figure class="width-25"><div class="figure">&#13;
<img alt="Third Generated Image of the First RBM" src="assets/hulp_1107.png"/>&#13;
<h6><span class="label">Figure 11-7. </span>Third generated image of the first RBM</h6>&#13;
</div></figure>&#13;
&#13;
<figure class="width-25"><div class="figure">&#13;
<img alt="Fourth Generated Image of the First RBM" src="assets/hulp_1108.png"/>&#13;
<h6><span class="label">Figure 11-8. </span>Fourth generated image of the first RBM</h6>&#13;
</div></figure>&#13;
&#13;
<figure><div class="figure" id="fifth_generated_image_of_the_first_rbm">&#13;
<img alt="Fifth Generated Image of the First RBM" src="assets/hulp_1109.png"/>&#13;
<h6><span class="label">Figure 11-9. </span>Fifth generated image of the first RBM</h6>&#13;
</div></figure>&#13;
&#13;
<p>These digits are zero, four, one, and nine, respectively, and the generated images look reasonably similar to the original images.<a data-primary="" data-startref="DBNrbmtrain11" data-type="indexterm" id="idm140637531159920"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Full DBN" data-type="sect1"><div class="sect1" id="idm140637531798016">&#13;
<h1>The Full DBN</h1>&#13;
&#13;
<p>Now, let’s define<a data-primary="deep belief networks (DBNs)" data-secondary="DBN training" data-type="indexterm" id="DBNtrain11"/> the DBN class, which will take in the three RBMs we just trained and add a fourth RBM that performs forward and backward passes to refine the overall DBN-based generative model.</p>&#13;
&#13;
<p>First, let’s define the hyperparameters of the class. These include the original input size, the input size of the third RBM we just trained, the final output size we would like to have from the DBN, the learning rate, the number of epochs we wish to train for, the batch size for training, and the three RBMs we just trained. Like before, we will need to generate zero matrices for the weights, hidden bias, and visible bias:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">class</code> <code class="nc">DBN</code><code class="p">(</code><code class="nb">object</code><code class="p">):</code>&#13;
    <code class="k">def</code> <code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">original_input_size</code><code class="p">,</code> <code class="n">input_size</code><code class="p">,</code> <code class="n">output_size</code><code class="p">,</code>&#13;
                 <code class="n">learning_rate</code><code class="p">,</code> <code class="n">epochs</code><code class="p">,</code> <code class="n">batchsize</code><code class="p">,</code> <code class="n">rbmOne</code><code class="p">,</code> <code class="n">rbmTwo</code><code class="p">,</code> <code class="n">rbmThree</code><code class="p">):</code>&#13;
        <code class="c1"># Define hyperparameters</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">_original_input_size</code> <code class="o">=</code> <code class="n">original_input_size</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">_input_size</code> <code class="o">=</code> <code class="n">input_size</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">_output_size</code> <code class="o">=</code> <code class="n">output_size</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">learning_rate</code> <code class="o">=</code> <code class="n">learning_rate</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">epochs</code> <code class="o">=</code> <code class="n">epochs</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">batchsize</code> <code class="o">=</code> <code class="n">batchsize</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">rbmOne</code> <code class="o">=</code> <code class="n">rbmOne</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">rbmTwo</code> <code class="o">=</code> <code class="n">rbmTwo</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">rbmThree</code> <code class="o">=</code> <code class="n">rbmThree</code>&#13;
&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">w</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">([</code><code class="n">input_size</code><code class="p">,</code> <code class="n">output_size</code><code class="p">],</code> <code class="s2">"float"</code><code class="p">)</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">hb</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">([</code><code class="n">output_size</code><code class="p">],</code> <code class="s2">"float"</code><code class="p">)</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">vb</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">([</code><code class="n">input_size</code><code class="p">],</code> <code class="s2">"float"</code><code class="p">)</code></pre>&#13;
&#13;
<p>Similar to before, we will define functions to perform the forward pass and the backward pass and take samples from each:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">prob_h_given_v</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">visible</code><code class="p">,</code> <code class="n">w</code><code class="p">,</code> <code class="n">hb</code><code class="p">):</code>&#13;
    <code class="k">return</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">visible</code><code class="p">,</code> <code class="n">w</code><code class="p">)</code> <code class="o">+</code> <code class="n">hb</code><code class="p">)</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">prob_v_given_h</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">hidden</code><code class="p">,</code> <code class="n">w</code><code class="p">,</code> <code class="n">vb</code><code class="p">):</code>&#13;
    <code class="k">return</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">hidden</code><code class="p">,</code> <code class="n">tf</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="n">w</code><code class="p">))</code> <code class="o">+</code> <code class="n">vb</code><code class="p">)</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">sample_prob</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">probs</code><code class="p">):</code>&#13;
    <code class="k">return</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">sign</code><code class="p">(</code><code class="n">probs</code> <code class="o">-</code> <code class="n">tf</code><code class="o">.</code><code class="n">random_uniform</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">shape</code><code class="p">(</code><code class="n">probs</code><code class="p">))))</code></pre>&#13;
&#13;
<p>For the training, we need placeholders for the weights, hidden bias, and visible bias. We also need matrices for the previous and current weights, hidden biases, and visible biases:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">train</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>&#13;
    <code class="n">_w</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">placeholder</code><code class="p">(</code><code class="s2">"float"</code><code class="p">,</code> <code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">_input_size</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">_output_size</code><code class="p">])</code>&#13;
    <code class="n">_hb</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">placeholder</code><code class="p">(</code><code class="s2">"float"</code><code class="p">,</code> <code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">_output_size</code><code class="p">])</code>&#13;
    <code class="n">_vb</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">placeholder</code><code class="p">(</code><code class="s2">"float"</code><code class="p">,</code> <code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">_input_size</code><code class="p">])</code>&#13;
&#13;
    <code class="n">prv_w</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">([</code><code class="bp">self</code><code class="o">.</code><code class="n">_input_size</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">_output_size</code><code class="p">],</code> <code class="s2">"float"</code><code class="p">)</code>&#13;
    <code class="n">prv_hb</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">([</code><code class="bp">self</code><code class="o">.</code><code class="n">_output_size</code><code class="p">],</code> <code class="s2">"float"</code><code class="p">)</code>&#13;
    <code class="n">prv_vb</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">([</code><code class="bp">self</code><code class="o">.</code><code class="n">_input_size</code><code class="p">],</code> <code class="s2">"float"</code><code class="p">)</code>&#13;
&#13;
    <code class="n">cur_w</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">([</code><code class="bp">self</code><code class="o">.</code><code class="n">_input_size</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">_output_size</code><code class="p">],</code> <code class="s2">"float"</code><code class="p">)</code>&#13;
    <code class="n">cur_hb</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">([</code><code class="bp">self</code><code class="o">.</code><code class="n">_output_size</code><code class="p">],</code> <code class="s2">"float"</code><code class="p">)</code>&#13;
    <code class="n">cur_vb</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">([</code><code class="bp">self</code><code class="o">.</code><code class="n">_input_size</code><code class="p">],</code> <code class="s2">"float"</code><code class="p">)</code></pre>&#13;
&#13;
<p>We will set a placeholder for the visible layer.</p>&#13;
&#13;
<p>Next, we will take the initial input—the visible layer—and pass it through the three RBMs we trained earlier. This results in the output <em>forward</em>, which we will pass into the fourth RBM we train as part of this DBN class:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">v0</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">placeholder</code><code class="p">(</code><code class="s2">"float"</code><code class="p">,</code> <code class="p">[</code><code class="bp">None</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">_original_input_size</code><code class="p">])</code>&#13;
<code class="n">forwardOne</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">sign</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">v0</code><code class="p">,</code> \&#13;
                <code class="bp">self</code><code class="o">.</code><code class="n">rbmOne</code><code class="o">.</code><code class="n">w</code><code class="p">)</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">rbmOne</code><code class="o">.</code><code class="n">hb</code><code class="p">)</code> <code class="o">-</code> <code class="n">tf</code><code class="o">.</code><code class="n">random_uniform</code><code class="p">(</code> \&#13;
                <code class="n">tf</code><code class="o">.</code><code class="n">shape</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">v0</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">rbmOne</code><code class="o">.</code><code class="n">w</code><code class="p">)</code> <code class="o">+</code> \&#13;
                <code class="bp">self</code><code class="o">.</code><code class="n">rbmOne</code><code class="o">.</code><code class="n">hb</code><code class="p">)))))</code>&#13;
<code class="n">forwardTwo</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">sign</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">forwardOne</code><code class="p">,</code> \&#13;
                <code class="bp">self</code><code class="o">.</code><code class="n">rbmTwo</code><code class="o">.</code><code class="n">w</code><code class="p">)</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">rbmTwo</code><code class="o">.</code><code class="n">hb</code><code class="p">)</code> <code class="o">-</code> <code class="n">tf</code><code class="o">.</code><code class="n">random_uniform</code><code class="p">(</code> \&#13;
                <code class="n">tf</code><code class="o">.</code><code class="n">shape</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">forwardOne</code><code class="p">,</code> \&#13;
                <code class="bp">self</code><code class="o">.</code><code class="n">rbmTwo</code><code class="o">.</code><code class="n">w</code><code class="p">)</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">rbmTwo</code><code class="o">.</code><code class="n">hb</code><code class="p">)))))</code>&#13;
<code class="n">forward</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">sign</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">forwardTwo</code><code class="p">,</code> \&#13;
                <code class="bp">self</code><code class="o">.</code><code class="n">rbmThree</code><code class="o">.</code><code class="n">w</code><code class="p">)</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">rbmThree</code><code class="o">.</code><code class="n">hb</code><code class="p">)</code> <code class="o">-</code> \&#13;
                <code class="n">tf</code><code class="o">.</code><code class="n">random_uniform</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">shape</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code> \&#13;
                <code class="n">forwardTwo</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">rbmThree</code><code class="o">.</code><code class="n">w</code><code class="p">)</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">rbmThree</code><code class="o">.</code><code class="n">hb</code><code class="p">)))))</code>&#13;
<code class="n">h0</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">sample_prob</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">prob_h_given_v</code><code class="p">(</code><code class="n">forward</code><code class="p">,</code> <code class="n">_w</code><code class="p">,</code> <code class="n">_hb</code><code class="p">))</code>&#13;
<code class="n">v1</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">sample_prob</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">prob_v_given_h</code><code class="p">(</code><code class="n">h0</code><code class="p">,</code> <code class="n">_w</code><code class="p">,</code> <code class="n">_vb</code><code class="p">))</code>&#13;
<code class="n">h1</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">prob_h_given_v</code><code class="p">(</code><code class="n">v1</code><code class="p">,</code> <code class="n">_w</code><code class="p">,</code> <code class="n">_hb</code><code class="p">)</code></pre>&#13;
&#13;
<p>We will define the contrastive divergence like we did before:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">positive_grad</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="n">forward</code><code class="p">),</code> <code class="n">h0</code><code class="p">)</code>&#13;
<code class="n">negative_grad</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="n">v1</code><code class="p">),</code> <code class="n">h1</code><code class="p">)</code>&#13;
&#13;
<code class="n">update_w</code> <code class="o">=</code> <code class="n">_w</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">learning_rate</code> <code class="o">*</code> <code class="p">(</code><code class="n">positive_grad</code> <code class="o">-</code> <code class="n">negative_grad</code><code class="p">)</code> <code class="o">/</code> \&#13;
                <code class="n">tf</code><code class="o">.</code><code class="n">to_float</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">shape</code><code class="p">(</code><code class="n">forward</code><code class="p">)[</code><code class="mi">0</code><code class="p">])</code>&#13;
<code class="n">update_vb</code> <code class="o">=</code> <code class="n">_vb</code> <code class="o">+</code>  <code class="bp">self</code><code class="o">.</code><code class="n">learning_rate</code> <code class="o">*</code> <code class="n">tf</code><code class="o">.</code><code class="n">reduce_mean</code><code class="p">(</code><code class="n">forward</code> <code class="o">-</code> <code class="n">v1</code><code class="p">,</code> <code class="mi">0</code><code class="p">)</code>&#13;
<code class="n">update_hb</code> <code class="o">=</code> <code class="n">_hb</code> <code class="o">+</code>  <code class="bp">self</code><code class="o">.</code><code class="n">learning_rate</code> <code class="o">*</code> <code class="n">tf</code><code class="o">.</code><code class="n">reduce_mean</code><code class="p">(</code><code class="n">h0</code> <code class="o">-</code> <code class="n">h1</code><code class="p">,</code> <code class="mi">0</code><code class="p">)</code></pre>&#13;
&#13;
<p>Once we generate a full forward pass through this DBN—which includes the three RBMs we trained earlier plus the latest fourth RBM—we need to send the output of the fourth RBM’s hidden layer back through the entire DBN.</p>&#13;
&#13;
<p>This requires a backward pass through the fourth RBM as well as a backward pass through the first three. We will also use MSE as before. Here is how the backward pass occurs:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">backwardOne</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">sign</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">v1</code><code class="p">,</code> \&#13;
                    <code class="bp">self</code><code class="o">.</code><code class="n">rbmThree</code><code class="o">.</code><code class="n">w</code><code class="o">.</code><code class="n">T</code><code class="p">)</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">rbmThree</code><code class="o">.</code><code class="n">vb</code><code class="p">)</code> <code class="o">-</code> \&#13;
                    <code class="n">tf</code><code class="o">.</code><code class="n">random_uniform</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">shape</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code> \&#13;
                    <code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">v1</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">rbmThree</code><code class="o">.</code><code class="n">w</code><code class="o">.</code><code class="n">T</code><code class="p">)</code> <code class="o">+</code> \&#13;
                    <code class="bp">self</code><code class="o">.</code><code class="n">rbmThree</code><code class="o">.</code><code class="n">vb</code><code class="p">)))))</code>&#13;
<code class="n">backwardTwo</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">sign</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">backwardOne</code><code class="p">,</code> \&#13;
                    <code class="bp">self</code><code class="o">.</code><code class="n">rbmTwo</code><code class="o">.</code><code class="n">w</code><code class="o">.</code><code class="n">T</code><code class="p">)</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">rbmTwo</code><code class="o">.</code><code class="n">vb</code><code class="p">)</code> <code class="o">-</code> \&#13;
                    <code class="n">tf</code><code class="o">.</code><code class="n">random_uniform</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">shape</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code> \&#13;
                    <code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">backwardOne</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">rbmTwo</code><code class="o">.</code><code class="n">w</code><code class="o">.</code><code class="n">T</code><code class="p">)</code> <code class="o">+</code> \&#13;
                    <code class="bp">self</code><code class="o">.</code><code class="n">rbmTwo</code><code class="o">.</code><code class="n">vb</code><code class="p">)))))</code>&#13;
<code class="n">backward</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">sign</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">backwardTwo</code><code class="p">,</code> \&#13;
                    <code class="bp">self</code><code class="o">.</code><code class="n">rbmOne</code><code class="o">.</code><code class="n">w</code><code class="o">.</code><code class="n">T</code><code class="p">)</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">rbmOne</code><code class="o">.</code><code class="n">vb</code><code class="p">)</code> <code class="o">-</code> \&#13;
                    <code class="n">tf</code><code class="o">.</code><code class="n">random_uniform</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">shape</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code> \&#13;
                    <code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">backwardTwo</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">rbmOne</code><code class="o">.</code><code class="n">w</code><code class="o">.</code><code class="n">T</code><code class="p">)</code> <code class="o">+</code> \&#13;
                    <code class="bp">self</code><code class="o">.</code><code class="n">rbmOne</code><code class="o">.</code><code class="n">vb</code><code class="p">)))))</code>&#13;
&#13;
<code class="n">err</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">reduce_mean</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">square</code><code class="p">(</code><code class="n">v0</code> <code class="o">-</code> <code class="n">backward</code><code class="p">))</code></pre>&#13;
&#13;
<p>Here is the actual training portion of the DBN class, again very similar to the RBM one earlier:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">with</code> <code class="n">tf</code><code class="o">.</code><code class="n">Session</code><code class="p">()</code> <code class="k">as</code> <code class="n">sess</code><code class="p">:</code>&#13;
    <code class="n">sess</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">global_variables_initializer</code><code class="p">())</code>&#13;
&#13;
    <code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">epochs</code><code class="p">):</code>&#13;
        <code class="k">for</code> <code class="n">start</code><code class="p">,</code> <code class="n">end</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">X</code><code class="p">),</code> <code class="bp">self</code><code class="o">.</code><code class="n">batchsize</code><code class="p">),</code> \&#13;
                <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">batchsize</code><code class="p">,</code><code class="nb">len</code><code class="p">(</code><code class="n">X</code><code class="p">),</code> <code class="bp">self</code><code class="o">.</code><code class="n">batchsize</code><code class="p">)):</code>&#13;
            <code class="n">batch</code> <code class="o">=</code> <code class="n">X</code><code class="p">[</code><code class="n">start</code><code class="p">:</code><code class="n">end</code><code class="p">]</code>&#13;
            <code class="n">cur_w</code> <code class="o">=</code> <code class="n">sess</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">update_w</code><code class="p">,</code> <code class="n">feed_dict</code><code class="o">=</code><code class="p">{</code><code class="n">v0</code><code class="p">:</code> <code class="n">batch</code><code class="p">,</code> <code class="n">_w</code><code class="p">:</code> \&#13;
                                <code class="n">prv_w</code><code class="p">,</code> <code class="n">_hb</code><code class="p">:</code> <code class="n">prv_hb</code><code class="p">,</code> <code class="n">_vb</code><code class="p">:</code> <code class="n">prv_vb</code><code class="p">})</code>&#13;
            <code class="n">cur_hb</code> <code class="o">=</code> <code class="n">sess</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">update_hb</code><code class="p">,</code> <code class="n">feed_dict</code><code class="o">=</code><code class="p">{</code><code class="n">v0</code><code class="p">:</code> <code class="n">batch</code><code class="p">,</code> <code class="n">_w</code><code class="p">:</code> \&#13;
                                <code class="n">prv_w</code><code class="p">,</code> <code class="n">_hb</code><code class="p">:</code> <code class="n">prv_hb</code><code class="p">,</code> <code class="n">_vb</code><code class="p">:</code> <code class="n">prv_vb</code><code class="p">})</code>&#13;
            <code class="n">cur_vb</code> <code class="o">=</code> <code class="n">sess</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">update_vb</code><code class="p">,</code> <code class="n">feed_dict</code><code class="o">=</code><code class="p">{</code><code class="n">v0</code><code class="p">:</code> <code class="n">batch</code><code class="p">,</code> <code class="n">_w</code><code class="p">:</code> \&#13;
                                <code class="n">prv_w</code><code class="p">,</code> <code class="n">_hb</code><code class="p">:</code> <code class="n">prv_hb</code><code class="p">,</code> <code class="n">_vb</code><code class="p">:</code> <code class="n">prv_vb</code><code class="p">})</code>&#13;
            <code class="n">prv_w</code> <code class="o">=</code> <code class="n">cur_w</code>&#13;
            <code class="n">prv_hb</code> <code class="o">=</code> <code class="n">cur_hb</code>&#13;
            <code class="n">prv_vb</code> <code class="o">=</code> <code class="n">cur_vb</code>&#13;
        <code class="n">error</code> <code class="o">=</code> <code class="n">sess</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">err</code><code class="p">,</code> <code class="n">feed_dict</code><code class="o">=</code><code class="p">{</code><code class="n">v0</code><code class="p">:</code> <code class="n">X</code><code class="p">,</code> <code class="n">_w</code><code class="p">:</code> <code class="n">cur_w</code><code class="p">,</code> <code class="n">_vb</code><code class="p">:</code> \&#13;
                            <code class="n">cur_vb</code><code class="p">,</code> <code class="n">_hb</code><code class="p">:</code> <code class="n">cur_hb</code><code class="p">})</code>&#13;
        <code class="k">print</code> <code class="p">(</code><code class="s1">'Epoch: </code><code class="si">%d</code><code class="s1">'</code> <code class="o">%</code> <code class="n">epoch</code><code class="p">,</code><code class="s1">'reconstruction error: </code><code class="si">%f</code><code class="s1">'</code> <code class="o">%</code> <code class="n">error</code><code class="p">)</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">w</code> <code class="o">=</code> <code class="n">prv_w</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">hb</code> <code class="o">=</code> <code class="n">prv_hb</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">vb</code> <code class="o">=</code> <code class="n">prv_vb</code></pre>&#13;
&#13;
<p>Let’s define functions to produce generated images from the DBN and show features. These are similar to the RBM versions earlier, but we send the data through all four RBMs in the DBN class instead of just through a single RBM:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">dbn_output</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>&#13;
&#13;
    <code class="n">input_X</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">constant</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>&#13;
    <code class="n">forwardOne</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">input_X</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">rbmOne</code><code class="o">.</code><code class="n">w</code><code class="p">)</code> <code class="o">+</code> \&#13;
                               <code class="bp">self</code><code class="o">.</code><code class="n">rbmOne</code><code class="o">.</code><code class="n">hb</code><code class="p">)</code>&#13;
    <code class="n">forwardTwo</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">forwardOne</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">rbmTwo</code><code class="o">.</code><code class="n">w</code><code class="p">)</code> <code class="o">+</code> \&#13;
                               <code class="bp">self</code><code class="o">.</code><code class="n">rbmTwo</code><code class="o">.</code><code class="n">hb</code><code class="p">)</code>&#13;
    <code class="n">forward</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">forwardTwo</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">rbmThree</code><code class="o">.</code><code class="n">w</code><code class="p">)</code> <code class="o">+</code> \&#13;
                            <code class="bp">self</code><code class="o">.</code><code class="n">rbmThree</code><code class="o">.</code><code class="n">hb</code><code class="p">)</code>&#13;
&#13;
    <code class="n">_w</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">constant</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">w</code><code class="p">)</code>&#13;
    <code class="n">_hb</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">constant</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">hb</code><code class="p">)</code>&#13;
    <code class="n">_vb</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">constant</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">vb</code><code class="p">)</code>&#13;
&#13;
    <code class="n">out</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">forward</code><code class="p">,</code> <code class="n">_w</code><code class="p">)</code> <code class="o">+</code> <code class="n">_hb</code><code class="p">)</code>&#13;
    <code class="n">hiddenGen</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">sample_prob</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">prob_h_given_v</code><code class="p">(</code><code class="n">forward</code><code class="p">,</code> <code class="n">_w</code><code class="p">,</code> <code class="n">_hb</code><code class="p">))</code>&#13;
    <code class="n">visibleGen</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">sample_prob</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">prob_v_given_h</code><code class="p">(</code><code class="n">hiddenGen</code><code class="p">,</code> <code class="n">_w</code><code class="p">,</code> <code class="n">_vb</code><code class="p">))</code>&#13;
&#13;
    <code class="n">backwardTwo</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">visibleGen</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">rbmThree</code><code class="o">.</code><code class="n">w</code><code class="o">.</code><code class="n">T</code><code class="p">)</code> <code class="o">+</code> \&#13;
                                <code class="bp">self</code><code class="o">.</code><code class="n">rbmThree</code><code class="o">.</code><code class="n">vb</code><code class="p">)</code>&#13;
    <code class="n">backwardOne</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">backwardTwo</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">rbmTwo</code><code class="o">.</code><code class="n">w</code><code class="o">.</code><code class="n">T</code><code class="p">)</code> <code class="o">+</code> \&#13;
                                <code class="bp">self</code><code class="o">.</code><code class="n">rbmTwo</code><code class="o">.</code><code class="n">vb</code><code class="p">)</code>&#13;
    <code class="n">backward</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">backwardOne</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">rbmOne</code><code class="o">.</code><code class="n">w</code><code class="o">.</code><code class="n">T</code><code class="p">)</code> <code class="o">+</code> \&#13;
                             <code class="bp">self</code><code class="o">.</code><code class="n">rbmOne</code><code class="o">.</code><code class="n">vb</code><code class="p">)</code>&#13;
&#13;
    <code class="k">with</code> <code class="n">tf</code><code class="o">.</code><code class="n">Session</code><code class="p">()</code> <code class="k">as</code> <code class="n">sess</code><code class="p">:</code>&#13;
        <code class="n">sess</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">global_variables_initializer</code><code class="p">())</code>&#13;
        <code class="k">return</code> <code class="n">sess</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">out</code><code class="p">),</code> <code class="n">sess</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">backward</code><code class="p">)</code></pre>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">show_features</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">shape</code><code class="p">,</code> <code class="n">suptitle</code><code class="p">,</code> <code class="n">count</code><code class="o">=-</code><code class="mi">1</code><code class="p">):</code>&#13;
    <code class="n">maxw</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">amax</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">w</code><code class="o">.</code><code class="n">T</code><code class="p">)</code>&#13;
    <code class="n">minw</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">amin</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">w</code><code class="o">.</code><code class="n">T</code><code class="p">)</code>&#13;
    <code class="n">count</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">_output_size</code> <code class="k">if</code> <code class="n">count</code> <code class="o">==</code> <code class="o">-</code><code class="mi">1</code> <code class="ow">or</code> <code class="n">count</code> <code class="o">&gt;</code> \&#13;
            <code class="bp">self</code><code class="o">.</code><code class="n">_output_size</code> <code class="k">else</code> <code class="n">count</code>&#13;
    <code class="n">ncols</code> <code class="o">=</code> <code class="n">count</code> <code class="k">if</code> <code class="n">count</code> <code class="o">&lt;</code> <code class="mi">14</code> <code class="k">else</code> <code class="mi">14</code>&#13;
    <code class="n">nrows</code> <code class="o">=</code> <code class="n">count</code><code class="o">//</code><code class="n">ncols</code>&#13;
    <code class="n">nrows</code> <code class="o">=</code> <code class="n">nrows</code> <code class="k">if</code> <code class="n">nrows</code> <code class="o">&gt;</code> <code class="mi">2</code> <code class="k">else</code> <code class="mi">3</code>&#13;
    <code class="n">fig</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="n">ncols</code><code class="p">,</code> <code class="n">nrows</code><code class="p">),</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">100</code><code class="p">)</code>&#13;
    <code class="n">grid</code> <code class="o">=</code> <code class="n">Grid</code><code class="p">(</code><code class="n">fig</code><code class="p">,</code> <code class="n">rect</code><code class="o">=</code><code class="mi">111</code><code class="p">,</code> <code class="n">nrows_ncols</code><code class="o">=</code><code class="p">(</code><code class="n">nrows</code><code class="p">,</code> <code class="n">ncols</code><code class="p">),</code> <code class="n">axes_pad</code><code class="o">=</code><code class="mf">0.01</code><code class="p">)</code>&#13;
&#13;
    <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">ax</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">grid</code><code class="p">):</code>&#13;
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">w</code><code class="o">.</code><code class="n">T</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="k">if</code> <code class="n">i</code><code class="o">&lt;</code><code class="bp">self</code><code class="o">.</code><code class="n">_input_size</code> <code class="k">else</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">(</code><code class="n">shape</code><code class="p">)</code>&#13;
        <code class="n">x</code> <code class="o">=</code> <code class="p">(</code><code class="n">x</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">)</code> <code class="o">-</code> <code class="n">minw</code><code class="p">)</code><code class="o">/</code><code class="n">maxw</code>&#13;
        <code class="n">ax</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">x</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="o">*</code><code class="n">shape</code><code class="p">),</code> <code class="n">cmap</code><code class="o">=</code><code class="n">mpl</code><code class="o">.</code><code class="n">cm</code><code class="o">.</code><code class="n">Greys</code><code class="p">)</code>&#13;
        <code class="n">ax</code><code class="o">.</code><code class="n">set_axis_off</code><code class="p">()</code>&#13;
&#13;
    <code class="n">fig</code><code class="o">.</code><code class="n">text</code><code class="p">(</code><code class="mf">0.5</code><code class="p">,</code><code class="mi">1</code><code class="p">,</code> <code class="n">suptitle</code><code class="p">,</code> <code class="n">fontsize</code><code class="o">=</code><code class="mi">20</code><code class="p">,</code> <code class="n">horizontalalignment</code><code class="o">=</code><code class="s1">'center'</code><code class="p">)</code>&#13;
    <code class="n">fig</code><code class="o">.</code><code class="n">tight_layout</code><code class="p">()</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>&#13;
    <code class="k">return</code></pre>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="How Training of a DBN Works" data-type="sect2"><div class="sect2" id="idm140637529602000">&#13;
<h2>How Training of a DBN Works</h2>&#13;
&#13;
<p>Each of the three RBMs we have trained already has its own weights matrix, hidden bias vector, and visible bias vector. During the training of the fourth RBM as part of the DBN, we will not adjust the weights matrix, hidden bias vector, and visible bias vector of those first three RBMs. Rather, we will use the first three RBMs as fixed components of the DBN. We will call upon the first three RBMs just to do the forward and backward passes (and use samples of the data these three generate).</p>&#13;
&#13;
<p>During the training of the fourth RBM in the DBN, we will only adjust weights and biases of the fourth RBM. In other words, the fourth RBM in the DBN takes the output of the first three RBMs as given and performs forward and backward passes to learn a generative model that minimizes the reconstruction error between its generated images and the original images.</p>&#13;
&#13;
<p>Another way to train the DBNs would be to allow the DBN to learn and adjust weights for all four RBMs as it performs forward and backward passes through the entire network. However, training of the DBN would be very computationally expensive (perhaps not so with computers of today but certainly by the standards of 2006, when DBNs were first introduced).</p>&#13;
&#13;
<p>That being said, if we wish to perform more nuanced pretraining, we could allow the weights of the individual RBMs to be adjusted—one RBM at a time—as we perform batches of forward and backward passes through the network. We will not delve into this, but I encourage you to experiment on your own time.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Train the DBN" data-type="sect2"><div class="sect2" id="idm140637528799632">&#13;
<h2>Train the DBN</h2>&#13;
&#13;
<p>We will now train the DBN. We set the original image dimensions as 784, the dimensions of the third RBM output as 500, and the desired dimensions of the DBN as 500. We will use a learning rate of 1.0, train for 50 epochs, and use a batch size of 200. Finally, we will call the first three trained RBMs as part of the DBN:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Instantiate DBN Class</code>&#13;
<code class="n">dbn</code> <code class="o">=</code> <code class="n">DBN</code><code class="p">(</code><code class="mi">784</code><code class="p">,</code> <code class="mi">500</code><code class="p">,</code> <code class="mi">500</code><code class="p">,</code> <code class="mf">1.0</code><code class="p">,</code> <code class="mi">50</code><code class="p">,</code> <code class="mi">200</code><code class="p">,</code> <code class="n">rbm_list</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">rbm_list</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="n">rbm_list</code><code class="p">[</code><code class="mi">2</code><code class="p">])</code></pre>&#13;
&#13;
<p>Now, let’s train:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">inputX</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>&#13;
<code class="n">error_list</code> <code class="o">=</code> <code class="p">[]</code>&#13;
<code class="n">error_list</code> <code class="o">=</code> <code class="n">dbn</code><code class="o">.</code><code class="n">train</code><code class="p">(</code><code class="n">inputX</code><code class="p">)</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#reconstruction_errors_of_the_dbn">Figure 11-10</a> displays the reconstruction errors of the DBN over the course of the training.</p>&#13;
&#13;
<figure><div class="figure" id="reconstruction_errors_of_the_dbn">&#13;
<img alt="Reconstruction Errors of the DBN" src="assets/hulp_1110.png"/>&#13;
<h6><span class="label">Figure 11-10. </span>Reconstruction errors of the DBN</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-type="xref" href="#learned_features_of_the_fourth_rbm_in_the_dbn">Figure 11-11</a> displays the learned features from the last layer of the DBN — the hidden layer of the fourth RBM.</p>&#13;
&#13;
<figure><div class="figure" id="learned_features_of_the_fourth_rbm_in_the_dbn">&#13;
<img alt="Learned Features of the Fourth RBM in the DBN" src="assets/hulp_1111.png"/>&#13;
<h6><span class="label">Figure 11-11. </span>Learned features of the fourth RBM in the DBN</h6>&#13;
</div></figure>&#13;
&#13;
<p>Both the reconstruction errors and the learned features look reasonable and similar to the ones from the individual RBMs we analyzed earlier.<a data-primary="" data-startref="DBNtrain11" data-type="indexterm" id="idm140637529108320"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="How Unsupervised Learning Helps Supervised Learning" data-type="sect1"><div class="sect1" id="idm140637531158160">&#13;
<h1>How Unsupervised Learning Helps Supervised Learning</h1>&#13;
&#13;
<p>So far, all<a data-primary="deep belief networks (DBNs)" data-secondary="benefits of generative models" data-type="indexterm" id="DBNbene11"/> the work we have done training the RBMs and the DBN involve unsupervised learning. We have not used any labels for the images at all. Instead, we have built generative models by learning relevant latent features from the original MNIST images provided in the 50,000 example training set. These generative models generate images that look reasonably similar to the original images (minimizing the reconstruction error).</p>&#13;
&#13;
<p>Let’s take a step back to understand the usefulness of such a generative model.</p>&#13;
&#13;
<p>Recall that most of the data in the world is unlabeled. Therefore, as powerful and effective as supervised learning is, we need unsupervised learning to help make sense of all the unlabeled data that exists. Supervised learning is not enough.</p>&#13;
&#13;
<p>To demonstrate the usefulness of unsupervised learning, imagine if instead of 50,000 labeled MNIST images in the training set, we had just a fraction—let’s say we had only 5,000 labeled MNIST images. A supervised learning-based image classifer that had only 5,000 labeled images would not be nearly as effective as a supervised learning-based image classifier that had 50,000 images. The more labeled data we have, the better the machine learning solution.</p>&#13;
&#13;
<p>How does unsupervised learning help in such a situation? One way unsupervised learning could help is by generating new labeled examples to help supplement the originally labeled dataset. Then, the supervised learning could occur on a much larger labeled dataset, resulting in a better overall solution.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Generate Images to Build a Better Image Classifier" data-type="sect2"><div class="sect2" id="idm140637529101712">&#13;
<h2>Generate Images to Build a Better Image Classifier</h2>&#13;
&#13;
<p>To simulate this benefit that unsupervised learning is able to provide, let’s reduce our MNIST training dataset to just five thousand labeled examples. We will store the first five thousand images in a dataframe called <code>inputXReduced</code>.</p>&#13;
&#13;
<p>Then, from these five thousand labeled images, we will generate new images from the generative model we just built using a DBN. And, we will do this 20 times over. In other words, we will generate five thousand new images 20 times to create a dataset that is 100,000 large, all of which will be labeled. Technically, we are storing the final hidden layer outputs not the reconstructed images directly, although we will store the reconstructed images, too, so we can evaluate them soon.</p>&#13;
&#13;
<p>We will store these 100,000 outputs in a NumPy array called <code>generatedImages</code>:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Generate images and store them</code>&#13;
<code class="n">inputXReduced</code> <code class="o">=</code> <code class="n">X_train</code><code class="o">.</code><code class="n">loc</code><code class="p">[:</code><code class="mi">4999</code><code class="p">]</code>&#13;
<code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code><code class="mi">20</code><code class="p">):</code>&#13;
    <code class="k">print</code><code class="p">(</code><code class="s2">"Run "</code><code class="p">,</code><code class="n">i</code><code class="p">)</code>&#13;
    <code class="n">finalOutput_DBN</code><code class="p">,</code> <code class="n">reconstructedOutput_DBN</code> <code class="o">=</code> <code class="n">dbn</code><code class="o">.</code><code class="n">dbn_output</code><code class="p">(</code><code class="n">inputXReduced</code><code class="p">)</code>&#13;
    <code class="k">if</code> <code class="n">i</code><code class="o">==</code><code class="mi">0</code><code class="p">:</code>&#13;
        <code class="n">generatedImages</code> <code class="o">=</code> <code class="n">finalOutput_DBN</code>&#13;
    <code class="k">else</code><code class="p">:</code>&#13;
        <code class="n">generatedImages</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">generatedImages</code><code class="p">,</code> <code class="n">finalOutput_DBN</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code></pre>&#13;
&#13;
<p>We will loop through the first five thousand labels from the training labels, called <code>y_train</code>, 20 times to generate an array of labels called <code>labels</code>:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Generate a vector of labels for the generated images</code>&#13;
<code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code><code class="mi">20</code><code class="p">):</code>&#13;
    <code class="k">if</code> <code class="n">i</code><code class="o">==</code><code class="mi">0</code><code class="p">:</code>&#13;
        <code class="n">labels</code> <code class="o">=</code> <code class="n">y_train</code><code class="o">.</code><code class="n">loc</code><code class="p">[:</code><code class="mi">4999</code><code class="p">]</code>&#13;
    <code class="k">else</code><code class="p">:</code>&#13;
        <code class="n">labels</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">labels</code><code class="p">,</code><code class="n">y_train</code><code class="o">.</code><code class="n">loc</code><code class="p">[:</code><code class="mi">4999</code><code class="p">])</code></pre>&#13;
&#13;
<p>Finally, we will generate the output on the validation set, which we will need to evaluate the image classifier we will build soon:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Generate images based on the validation set</code>&#13;
<code class="n">inputValidation</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">X_validation</code><code class="p">)</code>&#13;
<code class="n">finalOutput_DBN_validation</code><code class="p">,</code> <code class="n">reconstructedOutput_DBN_validation</code> <code class="o">=</code> \&#13;
    <code class="n">dbn</code><code class="o">.</code><code class="n">dbn_output</code><code class="p">(</code><code class="n">inputValidation</code><code class="p">)</code></pre>&#13;
&#13;
<p>Before we use the data we just generated, let’s view a few of the reconstructed images:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View reconstructed images</code>&#13;
<code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code><code class="mi">10</code><code class="p">):</code>&#13;
    <code class="n">example</code> <code class="o">=</code> <code class="n">i</code>&#13;
    <code class="n">reconstructedX</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">reconstructedOutput_DBN</code><code class="p">,</code> \&#13;
                                  <code class="n">index</code><code class="o">=</code><code class="n">X_train</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="mi">5000</code><code class="p">]</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>&#13;
    <code class="n">view_digit</code><code class="p">(</code><code class="n">reconstructedX</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">example</code><code class="p">)</code>&#13;
    <code class="n">view_digit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">example</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="first_generated_image_of_the_dbn">&#13;
<img alt="First Generated Image of the DBN" src="assets/hulp_1112.png"/>&#13;
<h6><span class="label">Figure 11-12. </span>First generated image of the DBN</h6>&#13;
</div></figure>&#13;
&#13;
<p>As you can see in <a data-type="xref" href="#first_generated_image_of_the_dbn">Figure 11-12</a>, the generated image is very similar to the original image—both display the digit five. Unlike the RBM-generated images we saw earlier, these are more similar to the original MNIST images, including the pixelated bits.</p>&#13;
&#13;
<p>Let’s view a few more images like this to compare the DBN-generated images with the original MNIST ones (see Figures <a data-type="xref" data-xrefstyle="select:labelnumber" href="#second_generated_image_of_the_dbn">11-13</a> through <a data-type="xref" data-xrefstyle="select:labelnumber" href="#fifth_generated_image_of_the_dbn">11-16</a>).</p>&#13;
&#13;
<figure class="width-25"><div class="figure" id="second_generated_image_of_the_dbn">&#13;
<img alt="Second Generated Image of the DBN" src="assets/hulp_1113.png"/>&#13;
<h6><span class="label">Figure 11-13. </span>Second generated image of the DBN</h6>&#13;
</div></figure>&#13;
&#13;
<figure class="width-25"><div class="figure">&#13;
<img alt="Third Generated Image of the DBN" src="assets/hulp_1114.png"/>&#13;
<h6><span class="label">Figure 11-14. </span>Third generated image of the DBN</h6>&#13;
</div></figure>&#13;
&#13;
<figure class="width-25"><div class="figure">&#13;
<img alt="Fourth Generated Image of the DBN" src="assets/hulp_1115.png"/>&#13;
<h6><span class="label">Figure 11-15. </span>Fourth generated image of the DBN</h6>&#13;
</div></figure>&#13;
&#13;
<figure class="width-25"><div class="figure" id="fifth_generated_image_of_the_dbn">&#13;
<img alt="Fifth Generated Image of the DBN" src="assets/hulp_1116.png"/>&#13;
<h6><span class="label">Figure 11-16. </span>Fifth generated image of the DBN</h6>&#13;
</div></figure>&#13;
&#13;
<p>Also note that the DBN model (as well as the RBM models) is generative and therefore the images are produced using a stochastic process. The images are not produced using a deterministic process, and, therefore, the images of a single example vary from one DBN run to another.</p>&#13;
&#13;
<p>To simulate this, we will take the first MNIST image and use the DBN to generate a new one and do this 10 times over:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Generate the first example 10 times</code>&#13;
<code class="n">inputXReduced</code> <code class="o">=</code> <code class="n">X_train</code><code class="o">.</code><code class="n">loc</code><code class="p">[:</code><code class="mi">0</code><code class="p">]</code>&#13;
<code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code><code class="mi">10</code><code class="p">):</code>&#13;
    <code class="n">example</code> <code class="o">=</code> <code class="mi">0</code>&#13;
    <code class="k">print</code><code class="p">(</code><code class="s2">"Run "</code><code class="p">,</code><code class="n">i</code><code class="p">)</code>&#13;
    <code class="n">finalOutput_DBN_fives</code><code class="p">,</code> <code class="n">reconstructedOutput_DBN_fives</code> <code class="o">=</code> \&#13;
        <code class="n">dbn</code><code class="o">.</code><code class="n">dbn_output</code><code class="p">(</code><code class="n">inputXReduced</code><code class="p">)</code>&#13;
    <code class="n">reconstructedX_fives</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">reconstructedOutput_DBN_fives</code><code class="p">,</code> \&#13;
                                        <code class="n">index</code><code class="o">=</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code>&#13;
    <code class="k">print</code><code class="p">(</code><code class="s2">"Generated"</code><code class="p">)</code>&#13;
    <code class="n">view_digit</code><code class="p">(</code><code class="n">reconstructedX_fives</code><code class="p">,</code> <code class="n">y_train</code><code class="o">.</code><code class="n">loc</code><code class="p">[:</code><code class="mi">0</code><code class="p">],</code> <code class="n">example</code><code class="p">)</code></pre>&#13;
&#13;
<p>As you see from  Figures <a data-type="xref" data-xrefstyle="select:labelnumber" href="#first_two_generated_images_of_the_digit_five">11-17</a> through <a data-type="xref" data-xrefstyle="select:labelnumber" href="#fifth_two_generated_images_of_the_digit_five">11-21</a>, all the generated images display the number five, but they vary from image to image even though they all were generated using the same original MNIST image.</p>&#13;
&#13;
<figure class="width-25"><div class="figure" id="first_two_generated_images_of_the_digit_five">&#13;
<img alt="First and second Generated Images of the Digit Five" src="assets/hulp_1117.png"/>&#13;
<h6><span class="label">Figure 11-17. </span>First and second generated images of the digit five</h6>&#13;
</div></figure>&#13;
&#13;
<figure class="width-25"><div class="figure">&#13;
<img alt="Third and fourth Generated Images of the Digit Five" src="assets/hulp_1118.png"/>&#13;
<h6><span class="label">Figure 11-18. </span>Third and fourth generated images of the digit five</h6>&#13;
</div></figure>&#13;
&#13;
<figure class="width-25"><div class="figure">&#13;
<img alt="Fifth and sixth Generated Images of the Digit Five" src="assets/hulp_1119.png"/>&#13;
<h6><span class="label">Figure 11-19. </span>Fifth and sixth generated images of the digit five</h6>&#13;
</div></figure>&#13;
&#13;
<figure class="width-25"><div class="figure">&#13;
<img alt="Seventh and eighth Generated Images of the Digit Five" src="assets/hulp_1120.png"/>&#13;
<h6><span class="label">Figure 11-20. </span>Seventh and eighth generated images of the digit five</h6>&#13;
</div></figure>&#13;
&#13;
<figure class="width-25"><div class="figure" id="fifth_two_generated_images_of_the_digit_five">&#13;
<img alt="Ninth and tenth Generated Images of the Digit Five" src="assets/hulp_1121.png"/>&#13;
<h6><span class="label">Figure 11-21. </span>Ninth and tenth generated images of the digit five</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Image Classifier Using LightGBM" data-type="sect1"><div class="sect1" id="idm140637529100768">&#13;
<h1>Image Classifier Using LightGBM</h1>&#13;
&#13;
<p>Now let’s<a data-primary="" data-startref="DBNbene11" data-type="indexterm" id="idm140637528366208"/><a data-primary="deep belief networks (DBNs)" data-secondary="image classifier using LightGBM" data-type="indexterm" id="DBNlight11"/><a data-primary="LightGBM" data-secondary="image classifier using" data-type="indexterm" id="LGBMimage11"/> build an image classifier using a supervised learning algorithm we introduced earlier in the book: the gradient boosting algorithm <em>LightGBM</em>.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Supervised Only" data-type="sect2"><div class="sect2" id="idm140637528362000">&#13;
<h2>Supervised Only</h2>&#13;
&#13;
<p>The first image classifier will rely on just the first five thousand labeled MNIST images. This is the reduced set from the original 50,000 labeled MNIST training set; we designed this to simulate real-world problems where we have relatively few labeled examples. Since we covered gradient boosting and the LightGBM algorithm in depth earlier in the book, we will not go into a lot of detail here.</p>&#13;
&#13;
<p>Let’s set the parameters for the algorithm:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">predictionColumns</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'0'</code><code class="p">,</code><code class="s1">'1'</code><code class="p">,</code><code class="s1">'2'</code><code class="p">,</code><code class="s1">'3'</code><code class="p">,</code><code class="s1">'4'</code><code class="p">,</code><code class="s1">'5'</code><code class="p">,</code><code class="s1">'6'</code><code class="p">,</code><code class="s1">'7'</code><code class="p">,</code><code class="s1">'8'</code><code class="p">,</code><code class="s1">'9'</code><code class="p">]</code>&#13;
&#13;
<code class="n">params_lightGB</code> <code class="o">=</code> <code class="p">{</code>&#13;
    <code class="s1">'task'</code><code class="p">:</code> <code class="s1">'train'</code><code class="p">,</code>&#13;
    <code class="s1">'application'</code><code class="p">:</code><code class="s1">'binary'</code><code class="p">,</code>&#13;
    <code class="s1">'num_class'</code><code class="p">:</code><code class="mi">10</code><code class="p">,</code>&#13;
    <code class="s1">'boosting'</code><code class="p">:</code> <code class="s1">'gbdt'</code><code class="p">,</code>&#13;
    <code class="s1">'objective'</code><code class="p">:</code> <code class="s1">'multiclass'</code><code class="p">,</code>&#13;
    <code class="s1">'metric'</code><code class="p">:</code> <code class="s1">'multi_logloss'</code><code class="p">,</code>&#13;
    <code class="s1">'metric_freq'</code><code class="p">:</code><code class="mi">50</code><code class="p">,</code>&#13;
    <code class="s1">'is_training_metric'</code><code class="p">:</code><code class="bp">False</code><code class="p">,</code>&#13;
    <code class="s1">'max_depth'</code><code class="p">:</code><code class="mi">4</code><code class="p">,</code>&#13;
    <code class="s1">'num_leaves'</code><code class="p">:</code> <code class="mi">31</code><code class="p">,</code>&#13;
    <code class="s1">'learning_rate'</code><code class="p">:</code> <code class="mf">0.1</code><code class="p">,</code>&#13;
    <code class="s1">'feature_fraction'</code><code class="p">:</code> <code class="mf">1.0</code><code class="p">,</code>&#13;
    <code class="s1">'bagging_fraction'</code><code class="p">:</code> <code class="mf">1.0</code><code class="p">,</code>&#13;
    <code class="s1">'bagging_freq'</code><code class="p">:</code> <code class="mi">0</code><code class="p">,</code>&#13;
    <code class="s1">'bagging_seed'</code><code class="p">:</code> <code class="mi">2018</code><code class="p">,</code>&#13;
    <code class="s1">'verbose'</code><code class="p">:</code> <code class="mi">0</code><code class="p">,</code>&#13;
    <code class="s1">'num_threads'</code><code class="p">:</code><code class="mi">16</code>&#13;
<code class="p">}</code></pre>&#13;
&#13;
<p>Next, we will train on the 5,000 labeled MNIST training set (the reduced set) and validate on the 10,000 labeled MNIST validation set:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">trainingScore</code> <code class="o">=</code> <code class="p">[]</code>&#13;
<code class="n">validationScore</code> <code class="o">=</code> <code class="p">[]</code>&#13;
<code class="n">predictionsLightGBM</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="p">[],</code> \&#13;
                        <code class="n">index</code><code class="o">=</code><code class="n">y_validation</code><code class="o">.</code><code class="n">index</code><code class="p">,</code> \&#13;
                        <code class="n">columns</code><code class="o">=</code><code class="n">predictionColumns</code><code class="p">)</code>&#13;
&#13;
<code class="n">lgb_train</code> <code class="o">=</code> <code class="n">lgb</code><code class="o">.</code><code class="n">Dataset</code><code class="p">(</code><code class="n">X_train</code><code class="o">.</code><code class="n">loc</code><code class="p">[:</code><code class="mi">4999</code><code class="p">],</code> <code class="n">y_train</code><code class="o">.</code><code class="n">loc</code><code class="p">[:</code><code class="mi">4999</code><code class="p">])</code>&#13;
<code class="n">lgb_eval</code> <code class="o">=</code> <code class="n">lgb</code><code class="o">.</code><code class="n">Dataset</code><code class="p">(</code><code class="n">X_validation</code><code class="p">,</code> <code class="n">y_validation</code><code class="p">,</code> <code class="n">reference</code><code class="o">=</code><code class="n">lgb_train</code><code class="p">)</code>&#13;
<code class="n">gbm</code> <code class="o">=</code> <code class="n">lgb</code><code class="o">.</code><code class="n">train</code><code class="p">(</code><code class="n">params_lightGB</code><code class="p">,</code> <code class="n">lgb_train</code><code class="p">,</code> <code class="n">num_boost_round</code><code class="o">=</code><code class="mi">2000</code><code class="p">,</code>&#13;
                   <code class="n">valid_sets</code><code class="o">=</code><code class="n">lgb_eval</code><code class="p">,</code> <code class="n">early_stopping_rounds</code><code class="o">=</code><code class="mi">200</code><code class="p">)</code>&#13;
&#13;
<code class="n">loglossTraining</code> <code class="o">=</code> <code class="n">log_loss</code><code class="p">(</code><code class="n">y_train</code><code class="o">.</code><code class="n">loc</code><code class="p">[:</code><code class="mi">4999</code><code class="p">],</code> \&#13;
    <code class="n">gbm</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_train</code><code class="o">.</code><code class="n">loc</code><code class="p">[:</code><code class="mi">4999</code><code class="p">],</code> <code class="n">num_iteration</code><code class="o">=</code><code class="n">gbm</code><code class="o">.</code><code class="n">best_iteration</code><code class="p">))</code>&#13;
<code class="n">trainingScore</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">loglossTraining</code><code class="p">)</code>&#13;
&#13;
<code class="n">predictionsLightGBM</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">X_validation</code><code class="o">.</code><code class="n">index</code><code class="p">,</code><code class="n">predictionColumns</code><code class="p">]</code> <code class="o">=</code> \&#13;
    <code class="n">gbm</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_validation</code><code class="p">,</code> <code class="n">num_iteration</code><code class="o">=</code><code class="n">gbm</code><code class="o">.</code><code class="n">best_iteration</code><code class="p">)</code>&#13;
<code class="n">loglossValidation</code> <code class="o">=</code> <code class="n">log_loss</code><code class="p">(</code><code class="n">y_validation</code><code class="p">,</code>&#13;
    <code class="n">predictionsLightGBM</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">X_validation</code><code class="o">.</code><code class="n">index</code><code class="p">,</code><code class="n">predictionColumns</code><code class="p">])</code>&#13;
<code class="n">validationScore</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">loglossValidation</code><code class="p">)</code>&#13;
&#13;
<code class="k">print</code><code class="p">(</code><code class="s1">'Training Log Loss: '</code><code class="p">,</code> <code class="n">loglossTraining</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s1">'Validation Log Loss: '</code><code class="p">,</code> <code class="n">loglossValidation</code><code class="p">)</code>&#13;
&#13;
<code class="n">loglossLightGBM</code> <code class="o">=</code> <code class="n">log_loss</code><code class="p">(</code><code class="n">y_validation</code><code class="p">,</code> <code class="n">predictionsLightGBM</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s1">'LightGBM Gradient Boosting Log Loss: '</code><code class="p">,</code> <code class="n">loglossLightGBM</code><code class="p">)</code></pre>&#13;
&#13;
<p>The following code shows the training and the validation log loss from this supervised-only solution:</p>&#13;
&#13;
<pre data-type="programlisting">Training Log Loss: 0.0018646953029132292&#13;
Validation Log Loss: 0.19124276982588717</pre>&#13;
&#13;
<p>The following code shows the overall accuracy of this supervised-only image classification solution:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">predictionsLightGBM_firm</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">predictionsLightGBM</code><code class="p">),</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>&#13;
<code class="n">accuracyValidation_lightGBM</code> <code class="o">=</code> <code class="n">accuracy_score</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">y_validation</code><code class="p">),</code> \&#13;
                                            <code class="n">predictionsLightGBM_firm</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s2">"Supervised-Only Accuracy: "</code><code class="p">,</code> <code class="n">accuracyValidation_lightGBM</code><code class="p">)</code></pre>&#13;
&#13;
<pre data-type="programlisting">Supervised-Only Accuracy: 0.9439</pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Unsupervised and Supervised Solution" data-type="sect2"><div class="sect2" id="idm140637528361344">&#13;
<h2>Unsupervised and Supervised Solution</h2>&#13;
&#13;
<p>Now, instead of training on the five thousand labeled MNIST images, let’s train on the 100,000 generated images from the DBN:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Prepare DBN-based DataFrames for LightGBM use</code>&#13;
<code class="n">generatedImagesDF</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">generatedImages</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code><code class="mi">100000</code><code class="p">))</code>&#13;
<code class="n">labelsDF</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">labels</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code><code class="mi">100000</code><code class="p">))</code>&#13;
&#13;
<code class="n">X_train_lgb</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">generatedImagesDF</code><code class="p">,</code>&#13;
                           <code class="n">index</code><code class="o">=</code><code class="n">generatedImagesDF</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>&#13;
<code class="n">X_validation_lgb</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">finalOutput_DBN_validation</code><code class="p">,</code>&#13;
                                <code class="n">index</code><code class="o">=</code><code class="n">X_validation</code><code class="o">.</code><code class="n">index</code><code class="p">)</code></pre>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Train LightGBM</code>&#13;
<code class="n">trainingScore</code> <code class="o">=</code> <code class="p">[]</code>&#13;
<code class="n">validationScore</code> <code class="o">=</code> <code class="p">[]</code>&#13;
<code class="n">predictionsDBN</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="p">[],</code><code class="n">index</code><code class="o">=</code><code class="n">y_validation</code><code class="o">.</code><code class="n">index</code><code class="p">,</code>&#13;
                              <code class="n">columns</code><code class="o">=</code><code class="n">predictionColumns</code><code class="p">)</code>&#13;
&#13;
<code class="n">lgb_train</code> <code class="o">=</code> <code class="n">lgb</code><code class="o">.</code><code class="n">Dataset</code><code class="p">(</code><code class="n">X_train_lgb</code><code class="p">,</code> <code class="n">labels</code><code class="p">)</code>&#13;
<code class="n">lgb_eval</code> <code class="o">=</code> <code class="n">lgb</code><code class="o">.</code><code class="n">Dataset</code><code class="p">(</code><code class="n">X_validation_lgb</code><code class="p">,</code> <code class="n">y_validation</code><code class="p">,</code> <code class="n">reference</code><code class="o">=</code><code class="n">lgb_train</code><code class="p">)</code>&#13;
<code class="n">gbm</code> <code class="o">=</code> <code class="n">lgb</code><code class="o">.</code><code class="n">train</code><code class="p">(</code><code class="n">params_lightGB</code><code class="p">,</code> <code class="n">lgb_train</code><code class="p">,</code> <code class="n">num_boost_round</code><code class="o">=</code><code class="mi">2000</code><code class="p">,</code>&#13;
                   <code class="n">valid_sets</code><code class="o">=</code><code class="n">lgb_eval</code><code class="p">,</code> <code class="n">early_stopping_rounds</code><code class="o">=</code><code class="mi">200</code><code class="p">)</code>&#13;
&#13;
<code class="n">loglossTraining</code> <code class="o">=</code> <code class="n">log_loss</code><code class="p">(</code><code class="n">labelsDF</code><code class="p">,</code> <code class="n">gbm</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_train_lgb</code><code class="p">,</code> \&#13;
                            <code class="n">num_iteration</code><code class="o">=</code><code class="n">gbm</code><code class="o">.</code><code class="n">best_iteration</code><code class="p">))</code>&#13;
<code class="n">trainingScore</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">loglossTraining</code><code class="p">)</code>&#13;
&#13;
<code class="n">predictionsDBN</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">X_validation</code><code class="o">.</code><code class="n">index</code><code class="p">,</code><code class="n">predictionColumns</code><code class="p">]</code> <code class="o">=</code> \&#13;
    <code class="n">gbm</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_validation_lgb</code><code class="p">,</code> <code class="n">num_iteration</code><code class="o">=</code><code class="n">gbm</code><code class="o">.</code><code class="n">best_iteration</code><code class="p">)</code>&#13;
<code class="n">loglossValidation</code> <code class="o">=</code> <code class="n">log_loss</code><code class="p">(</code><code class="n">y_validation</code><code class="p">,</code>&#13;
    <code class="n">predictionsDBN</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">X_validation</code><code class="o">.</code><code class="n">index</code><code class="p">,</code><code class="n">predictionColumns</code><code class="p">])</code>&#13;
<code class="n">validationScore</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">loglossValidation</code><code class="p">)</code>&#13;
&#13;
<code class="k">print</code><code class="p">(</code><code class="s1">'Training Log Loss: '</code><code class="p">,</code> <code class="n">loglossTraining</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s1">'Validation Log Loss: '</code><code class="p">,</code> <code class="n">loglossValidation</code><code class="p">)</code>&#13;
&#13;
<code class="n">loglossDBN</code> <code class="o">=</code> <code class="n">log_loss</code><code class="p">(</code><code class="n">y_validation</code><code class="p">,</code> <code class="n">predictionsDBN</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s1">'LightGBM Gradient Boosting Log Loss: '</code><code class="p">,</code> <code class="n">loglossDBN</code><code class="p">)</code></pre>&#13;
&#13;
<p>The following code displays the log loss of this unsupervised-enchanced image classification solution:</p>&#13;
&#13;
<pre data-type="programlisting">Training Log Loss: 0.004145635328203315&#13;
Validation Log Loss: 0.16377638170016542</pre>&#13;
&#13;
<p>The following code shows the overall accuracy of this unsupervised-enchanced image classification solution:</p>&#13;
&#13;
<pre data-type="programlisting">DBN-Based Solution Accuracy: 0.9525</pre>&#13;
&#13;
<p>As you see, the solution improves by nearly one percentage point, which is <span class="keep-together">considerable</span>.<a data-primary="" data-startref="LGBMimage11" data-type="indexterm" id="idm140637527591280"/><a data-primary="" data-startref="DBNlight11" data-type="indexterm" id="idm140637527590272"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="idm140637527589072">&#13;
<h1>Conclusion</h1>&#13;
&#13;
<p>In <a data-type="xref" href="ch10.html#Chapter_10">Chapter 10</a>, we introduced the first class of generative models called restricted Boltzmann machines. In this chapter, we built upon this concept by introducing more advanced generative models known as deep belief networks, which are comprised of multiple RBMs stacked on top of each other.</p>&#13;
&#13;
<p>We demonstrated how DBNs work—in a purely unsupervised manner, the DBN learns the underlying structure of data and uses its learning to generate new synthetic data. Based on how the new synthetic data compares to the original data, the DBN improves its generative ability so much so that the synthetic data increasingly resembles the original data. We also showed how synthetic data generated by DBNs could supplement existing labeled datasets, improving the performance of supervised learning models by increasing the size of the overall training set.</p>&#13;
&#13;
<p>The semisupervised solution we developed using DBNs (unsupervised learning) and gradient boosting (supervised learning) outperformed the purely supervised solution in the MNIST image classifaction problem we had.</p>&#13;
&#13;
<p>In <a data-type="xref" href="ch12.html#Chapter_12">Chapter 12</a>, we introduce one of the latest advances in unsupervised learning (and generative modeling, more specifically) known as generative adversarial networks.<a data-primary="" data-startref="UDLfeature11" data-type="indexterm" id="idm140637527583328"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>