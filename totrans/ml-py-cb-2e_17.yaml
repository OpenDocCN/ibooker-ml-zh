- en: Chapter 17\. Support Vector Machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 17.0 Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand support vector machines, we must understand hyperplanes. Formally,
    a *hyperplane* is an *n – 1* subspace in an *n*-dimensional space. While that
    sounds complex, it actually is pretty simple. For example, if we wanted to divide
    a two-dimensional space, we’d use a one-dimensional hyperplane (i.e., a line).
    If we wanted to divide a three-dimensional space, we’d use a two-dimensional hyperplane
    (i.e., a flat piece of paper or a bed sheet). A hyperplane is simply a generalization
    of that concept into *n* dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: '*Support vector machines* classify data by finding the hyperplane that maximizes
    the margin between the classes in the training data. In a two-dimensional example
    with two classes, we can think of a hyperplane as the widest straight “band” (i.e.,
    line with margins) that separates the two classes.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we cover training support vector machines in a variety of situations
    and dive under the hood to look at how we can extend the approach to tackle common
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: 17.1 Training a Linear Classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to train a model to classify observations.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use a *support vector classifier* (SVC) to find the hyperplane that maximizes
    the margins between the classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'scikit-learn’s `LinearSVC` implements a simple SVC. To get an intuition behind
    what an SVC is doing, let’s plot out the data and hyperplane. While SVCs work
    well in high dimensions, in our solution we loaded only two features and took
    a subset of observations so that the data contains only two classes. This will
    let us visualize the model. Recall that SVC attempts to find the hyperplane—​a
    line when we have only two dimensions—​with the maximum margin between the classes.
    In the following code we plot the two classes on a two-dimensional space, then
    draw the hyperplane:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 17in01](assets/mpc2_17in01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this visualization, all observations of class 0 are black and observations
    of class 1 are light gray. The hyperplane is the decision boundary deciding how
    new observations are classified. Specifically, any observation above the line
    will by classified as class 0, while any observation below the line will be classified
    as class 1\. We can prove this by creating a new observation in the top-left corner
    of our visualization, meaning it should be predicted to be class 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: There are a few things to note about SVCs. First, for the sake of visualization,
    we limited our example to a binary example (i.e., only two classes); however,
    SVCs can work well with multiple classes. Second, as our visualization shows,
    the hyperplane is by definition linear (i.e., not curved). This was okay in this
    example because the data was linearly separable, meaning there was a hyperplane
    that could perfectly separate the two classes. Unfortunately, in the real world
    this is rarely the case.
  prefs: []
  type: TYPE_NORMAL
- en: More typically, we will not be able to perfectly separate classes. In these
    situations there is a balance between SVC maximizing the margin of the hyperplane
    and minimizing the misclassification. In SVC, the latter is controlled with the
    hyperparameter *C*. *C* is a parameter of the SVC learner and is the penalty for
    misclassifying a data point. When *C* is small, the classifier is okay with misclassified
    data points (high bias but low variance). When *C* is large, the classifier is
    heavily penalized for misclassified data and therefore bends over backward to
    avoid any misclassified data points (low bias but high variance).
  prefs: []
  type: TYPE_NORMAL
- en: In scikit-learn, *C* is determined by the parameter `C` and defaults to `C=1.0`.
    We should treat *C* has a hyperparameter of our learning algorithm, which we tune
    using model selection techniques in [Chapter 12](ch12.xhtml#model-selection).
  prefs: []
  type: TYPE_NORMAL
- en: 17.2 Handling Linearly Inseparable Classes Using Kernels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to train a support vector classifier, but your classes are linearly
    inseparable.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Train an extension of a support vector machine using kernel functions to create
    nonlinear decision boundaries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A full explanation of support vector machines is outside the scope of this
    book. However, a short explanation is likely beneficial for understanding support
    vector machines and kernels. For reasons best learned elsewhere, a support vector
    classifier can be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo> <munder><mo>∑</mo> <mrow><mi>i</mi><mi>ϵ</mi><mi>S</mi></mrow></munder>
    <msub><mi>α</mi> <mi>i</mi></msub> <mi>K</mi> <mrow><mo>(</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>,</mo> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo></mrow></msub>
    <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'where <math display="inline"><msub><mi>β</mi><mn>0</mn></msub></math> is the
    bias, <math display="inline"><mi>S</mi></math> is the set of all support vector
    observations, <math display="inline"><mi>α</mi></math> is the model parameters
    to be learned, and <math display="inline"><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>,</mo> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>''</mo></mrow></msub> <mo>)</mo></math>
    are pairs of two support vector observations, <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math>
    and <math display="inline"><msub><mi>x</mi> <mrow><mi>i</mi> <mo>''</mo></mrow></msub></math>
    . Most importantly, <math display="inline"><mi>K</mi></math> is a kernel function
    that compares the similarity between <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math>
    and <math display="inline"><msub><mi>x</mi> <mrow><mi>i</mi> <mo>''</mo></mrow></msub></math>
    . Don’t worry if you don’t understand kernel functions. For our purposes, just
    realize that (1) <math display="inline"><mi>K</mi></math> determines the type
    of hyperplane used to separate our classes, and (2) we create different hyperplanes
    by using different kernels. For example, if we want a basic linear hyperplane
    like the one we created in [Recipe 17.1](#training-a-linear-classifier), we can
    use the linear kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>K</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>,</mo> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo></mrow></msub> <mo>)</mo></mrow>
    <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>p</mi></munderover> <msub><mi>x</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo> <mi>j</mi></mrow></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'where <math display="inline"><mi>p</mi></math> is the number of features. However,
    if we want a nonlinear decision boundary, we swap the linear kernel with a polynomial
    kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>K</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>,</mo> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo></mrow></msub> <mo>)</mo></mrow>
    <mo>=</mo> <msup><mrow><mo>(</mo><mi>r</mi><mo>+</mo><mi>γ</mi><msubsup><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>p</mi></msubsup> <msub><mi>x</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo>
    <mi>j</mi></mrow></msub> <mo>)</mo></mrow> <mi>d</mi></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'where <math display="inline"><mi>d</mi></math> is the degree of the polynomial
    kernel function. Alternatively, we can use one of the most common kernels in support
    vectors machines, the *radial basis function kernel*:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>K</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>,</mo> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo></mrow></msub> <mo>)</mo></mrow>
    <mo>=</mo> <msup><mi>e</mi> <mrow><mo>(</mo><mo>-</mo><mi>γ</mi><msubsup><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>p</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>x</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo>
    <mi>j</mi></mrow></msub> <mo>)</mo></mrow> <mn>2</mn></msup> <mo>)</mo></mrow></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math display="inline"><mi>γ</mi></math> is a hyperparameter and must
    be greater than zero. The main point of the preceding explanation is that if we
    have linearly inseparable data, we can swap out a linear kernel with an alternative
    kernel to create a nonlinear hyperplane decision boundary.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can understand the intuition behind kernels by visualizing a simple example.
    This function, based on one by Sebastian Raschka, plots the observations and decision
    boundary hyperplane of a two-dimensional space. You do not need to understand
    how this function works; I have included it here so you can experiment on your
    own:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In our solution, we have data containing two features (i.e., two dimensions)
    and a target vector with the class of each observation. Importantly, the classes
    are assigned such that they are *linearly inseparable*. That is, there is no straight
    line we can draw that will divide the two classes. First, let’s create a support
    vector machine classifier with a linear kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, since we have only two features, we are working in a two-dimensional
    space and can visualize the observations, their classes, and our model’s linear
    hyperplane:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 17in02](assets/mpc2_17in02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can see, our linear hyperplane did very poorly at dividing the two classes!
    Now, let’s swap out the linear kernel with a radial basis function kernel and
    use it to train a new model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'And then visualize the observations and hyperplane:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 17in03](assets/mpc2_17in03.png)'
  prefs: []
  type: TYPE_IMG
- en: By using the radial basis function kernel we can create a decision boundary
    that is able to do a much better job of separating the two classes than the linear
    kernel. This is the motivation behind using kernels in support vector machines.
  prefs: []
  type: TYPE_NORMAL
- en: In scikit-learn, we can select the kernel we want to use by using the `kernel`
    parameter. Once we select a kernel, we need to specify the appropriate kernel
    options, such as the value of *d* (using the `degree` parameter) in polynomial
    kernels, and the value of γ (using the `gamma` parameter) in radial basis function
    kernels. We will also need to set the penalty parameter, `C`. When training the
    model, in most cases we should treat all of these as hyperparameters and use model
    selection techniques to identify the combination of their values that produces
    the model with the best performance.
  prefs: []
  type: TYPE_NORMAL
- en: 17.3 Creating Predicted Probabilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to know the predicted class probabilities for an observation.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When using scikit-learn’s `SVC`, set `probability=True`, train the model, then
    use `predict_proba` to see the calibrated probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many of the supervised learning algorithms we have covered use probability
    estimates to predict classes. For example, in k-nearest neighbors, an observation’s
    *k* neighbor’s classes were treated as votes to create a probability that an observation
    was of that class. Then the class with the highest probability was predicted.
    SVC’s use of a hyperplane to create decision regions does not naturally output
    a probability estimate that an observation is a member of a certain class. However,
    we can in fact output calibrated class probabilities with a few caveats. In an
    SVC with two classes, *Platt scaling* can be used, wherein first the SVC is trained,
    and then a separate cross-validated logistic regression is trained to map the
    SVC outputs into probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <mi>y</mi> <mo>=</mo>
    <mn>1</mn> <mo>∣</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn>
    <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>(</mo><mi>A</mi><mo>×</mo><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>+</mo><mi>B</mi><mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math display="inline"><mi>A</mi></math> and <math display="inline"><mi>B</mi></math>
    are parameter vectors, and <math display="inline"><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo></math>
    is the <math display="inline"><mi>i</mi></math>th observation’s signed distance
    from the hyperplane. When we have more than two classes, an extension of Platt
    scaling is used.
  prefs: []
  type: TYPE_NORMAL
- en: In more practical terms, creating predicted probabilities has two major issues.
    First, because we are training a second model with cross-validation, generating
    predicted probabilities can significantly increase the time it takes to train
    our model. Second, because the predicted probabilities are created using cross-validation,
    they might not always match the predicted classes. That is, an observation might
    be predicted to be class 1 but have a predicted probability of being class 1 of
    less than 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: In scikit-learn, the predicted probabilities must be generated when the model
    is being trained. We can do this by setting `SVC`’s `probability` to `True`. After
    the model is trained, we can output the estimated probabilities for each class
    using `predict_proba`.
  prefs: []
  type: TYPE_NORMAL
- en: 17.4 Identifying Support Vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to identify which observations are the support vectors of the decision
    hyperplane.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Train the model, then use `support_vectors_`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Support vector machines get their name from the fact that the hyperplane is
    being determined by a relatively small number of observations, called the *support
    vectors*. Intuitively, think of the hyperplane as being “carried” by these support
    vectors. These support vectors are therefore very important to our model. For
    example, if we remove an observation that is not a support vector from the data,
    the model does not change; however, if we remove a support vector, the hyperplane
    will not have the maximum margin.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we have trained an SVC, scikit-learn offers a number of options for identifying
    the support vector. In our solution, we used `support_vectors_` to output the
    actual observations’ features of the four support vectors in our model. Alternatively,
    we can view the indices of the support vectors using `support_`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can use `n_support_` to find the number of support vectors belonging
    to each class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 17.5 Handling Imbalanced Classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to train a support vector machine classifier in the presence of imbalanced
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Increase the penalty for misclassifying the smaller class using `class_weight`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In support vector machines, <math display="inline"><mi>C</mi></math> is a hyperparameter
    that determines the penalty for misclassifying an observation. One method for
    handling imbalanced classes in support vector machines is to weight <math display="inline"><mi>C</mi></math>
    by classes, so that:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>C</mi> <mi>k</mi></msub> <mo>=</mo> <mi>C</mi>
    <mo>×</mo> <msub><mi>w</mi> <mi>j</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math display="inline"><mi>C</mi></math> is the penalty for misclassification,
    <math display="inline"><msub><mi>w</mi><mi>j</mi></msub></math> is a weight inversely
    proportional to class <math display="inline"><mi>j</mi></math>’s frequency, and
    <math display="inline"><msub><mi>C</mi><mi>k</mi></msub></math> is the <math display="inline"><mi>C</mi></math>
    value for class <math display="inline"><mi>k</mi></math>. The general idea is
    to increase the penalty for misclassifying minority classes to prevent them from
    being “overwhelmed” by the majority class.
  prefs: []
  type: TYPE_NORMAL
- en: 'In scikit-learn, when using `SVC` we can set the values for <math display="inline"><msub><mi>C</mi><mi>k</mi></msub></math>
    automatically by setting `class_weight="balanced"`. The `balanced` argument automatically
    weighs classes such that:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>w</mi> <mi>j</mi></msub> <mo>=</mo> <mfrac><mi>n</mi>
    <mrow><mi>k</mi><msub><mi>n</mi> <mi>j</mi></msub></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math display="inline"><msub><mi>w</mi><mi>j</mi></msub></math> is the
    weight to class <math display="inline"><mi>j</mi></math>, <math display="inline"><mi>n</mi></math>
    is the number of observations, <math display="inline"><msub><mi>n</mi><mi>j</mi></msub></math>
    is the number of observations in class <math display="inline"><mi>j</mi></math>,
    and <math display="inline"><mi>k</mi></math> is the total number of classes.
  prefs: []
  type: TYPE_NORMAL
