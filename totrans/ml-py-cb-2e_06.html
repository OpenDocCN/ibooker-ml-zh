<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 6. Handling Text" data-type="chapter" epub:type="chapter"><div class="chapter" id="handling-text">
<h1><span class="label">Chapter 6. </span>Handling Text</h1>
<section data-pdf-bookmark="6.0 Introduction" data-type="sect1"><div class="sect1" id="id144">
<h1>6.0 Introduction</h1>
<p>Unstructured <a data-primary="text" data-type="indexterm" id="ix_text_data_ch6"/>text data, like the contents of a book or a tweet, is both one of the most interesting sources of features and one of the most complex to handle. In this chapter, we will cover strategies for transforming text into information-rich features and use some out-of-the-box features (termed <em>embeddings</em>) that have <a data-primary="embeddings" data-type="indexterm" id="id1261"/>become increasingly ubiquitous in tasks that involve <a data-primary="natural language processing (NLP)" data-type="indexterm" id="id1262"/><a data-primary="NLP (natural language processing)" data-type="indexterm" id="id1263"/>natural language processing (NLP).</p>
<p>This is not to say that the recipes covered here are comprehensive. Entire academic disciplines focus on handling unstructured data such as text. In this chapter, we will cover some commonly used techniques; knowledge of these will add valuable tools to our preprocessing toolbox. In addition to many generic text processing recipes, we’ll also demonstrate how you can import and leverage some pretrained machine learning models to generate richer text features.</p>
</div></section>
<section data-pdf-bookmark="6.1 Cleaning Text" data-type="sect1"><div class="sect1" id="cleaning-text">
<h1>6.1 Cleaning Text</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id145">
<h2>Problem</h2>
<p>You have some unstructured <a data-primary="strings" data-secondary="cleaning text" data-type="indexterm" id="ix_string_clean_text"/><a data-primary="text" data-secondary="cleaning" data-type="indexterm" id="ix_text_clean"/><a data-primary="cleaning text" data-type="indexterm" id="ix_clean_text"/>text data and want to complete some basic
cleaning.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id146">
<h2>Solution</h2>
<p>In the <a data-primary="replace operation, cleaning text" data-type="indexterm" id="id1264"/><a data-primary="split operation, cleaning text" data-type="indexterm" id="id1265"/><a data-primary="strip operation, cleaning text" data-type="indexterm" id="id1266"/>following example, we look at the text for three books and clean it by using Python’s core
string operations, in particular <code>strip</code>, <code>replace</code>, and <code>split</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create text</code>
<code class="n">text_data</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"   Interrobang. By Aishwarya Henriette     "</code><code class="p">,</code>
             <code class="s2">"Parking And Going. By Karl Gautier"</code><code class="p">,</code>
             <code class="s2">"    Today Is The night. By Jarek Prakash   "</code><code class="p">]</code>

<code class="c1"># Strip whitespaces</code>
<code class="n">strip_whitespace</code> <code class="o">=</code> <code class="p">[</code><code class="n">string</code><code class="o">.</code><code class="n">strip</code><code class="p">()</code> <code class="k">for</code> <code class="n">string</code> <code class="ow">in</code> <code class="n">text_data</code><code class="p">]</code>

<code class="c1"># Show text</code>
<code class="n">strip_whitespace</code></pre>
<pre data-type="programlisting">['Interrobang. By Aishwarya Henriette',
 'Parking And Going. By Karl Gautier',
 'Today Is The night. By Jarek Prakash']</pre>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Remove periods</code>
<code class="n">remove_periods</code> <code class="o">=</code> <code class="p">[</code><code class="n">string</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s2">"."</code><code class="p">,</code> <code class="s2">""</code><code class="p">)</code> <code class="k">for</code> <code class="n">string</code> <code class="ow">in</code> <code class="n">strip_whitespace</code><code class="p">]</code>

<code class="c1"># Show text</code>
<code class="n">remove_periods</code></pre>
<pre data-type="programlisting">['Interrobang By Aishwarya Henriette',
 'Parking And Going By Karl Gautier',
 'Today Is The night By Jarek Prakash']</pre>
<p>We also create and apply a custom transformation function:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create function</code>
<code class="k">def</code> <code class="nf">capitalizer</code><code class="p">(</code><code class="n">string</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">str</code><code class="p">:</code>
    <code class="k">return</code> <code class="n">string</code><code class="o">.</code><code class="n">upper</code><code class="p">()</code>

<code class="c1"># Apply function</code>
<code class="p">[</code><code class="n">capitalizer</code><code class="p">(</code><code class="n">string</code><code class="p">)</code> <code class="k">for</code> <code class="n">string</code> <code class="ow">in</code> <code class="n">remove_periods</code><code class="p">]</code></pre>
<pre data-type="programlisting">['INTERROBANG BY AISHWARYA HENRIETTE',
 'PARKING AND GOING BY KARL GAUTIER',
 'TODAY IS THE NIGHT BY JAREK PRAKASH']</pre>
<p>Finally, we can use regular expressions to make powerful string
operations:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import library</code>
<code class="kn">import</code> <code class="nn">re</code>

<code class="c1"># Create function</code>
<code class="k">def</code> <code class="nf">replace_letters_with_X</code><code class="p">(</code><code class="n">string</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">str</code><code class="p">:</code>
    <code class="k">return</code> <code class="n">re</code><code class="o">.</code><code class="n">sub</code><code class="p">(</code><code class="sa">r</code><code class="s2">"[a-zA-Z]"</code><code class="p">,</code> <code class="s2">"X"</code><code class="p">,</code> <code class="n">string</code><code class="p">)</code>

<code class="c1"># Apply function</code>
<code class="p">[</code><code class="n">replace_letters_with_X</code><code class="p">(</code><code class="n">string</code><code class="p">)</code> <code class="k">for</code> <code class="n">string</code> <code class="ow">in</code> <code class="n">remove_periods</code><code class="p">]</code></pre>
<pre data-type="programlisting">['XXXXXXXXXXX XX XXXXXXXXX XXXXXXXXX',
 'XXXXXXX XXX XXXXX XX XXXX XXXXXXX',
 'XXXXX XX XXX XXXXX XX XXXXX XXXXXXX']</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id147">
<h2>Discussion</h2>
<p>Some text data will need to be cleaned before we can use it to build
features, or be preprocessed in some way prior to being fed into an algorithm. Most basic text cleaning can be completed using Python’s
standard string operations. In the real world, we will most likely define
a custom cleaning function (e.g., <code>capitalizer</code>) combining some
cleaning tasks and apply that to the text data. Although cleaning strings can remove some information, it makes the data much easier to work with. Strings have many inherent methods that are useful for cleaning and processing; some additional examples can be found here:<a data-primary="" data-startref="ix_clean_text" data-type="indexterm" id="id1267"/><a data-primary="" data-startref="ix_string_clean_text" data-type="indexterm" id="id1268"/><a data-primary="" data-startref="ix_text_clean" data-type="indexterm" id="id1269"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Define a string</code>
<code class="n">s</code> <code class="o">=</code> <code class="s2">"machine learning in python cookbook"</code>

<code class="c1"># Find the first index of the letter "n"</code>
<code class="n">find_n</code> <code class="o">=</code> <code class="n">s</code><code class="o">.</code><code class="n">find</code><code class="p">(</code><code class="s2">"n"</code><code class="p">)</code>

<code class="c1"># Whether or not the string starts with "m"</code>
<code class="n">starts_with_m</code> <code class="o">=</code> <code class="n">s</code><code class="o">.</code><code class="n">startswith</code><code class="p">(</code><code class="s2">"m"</code><code class="p">)</code>

<code class="c1"># Whether or not the string ends with "python"</code>
<code class="n">ends_with_python</code> <code class="o">=</code> <code class="n">s</code><code class="o">.</code><code class="n">endswith</code><code class="p">(</code><code class="s2">"python"</code><code class="p">)</code>

<code class="c1"># Is the string alphanumeric</code>
<code class="n">is_alnum</code> <code class="o">=</code> <code class="n">s</code><code class="o">.</code><code class="n">isalnum</code><code class="p">()</code>

<code class="c1"># Is it composed of only alphabetical characters (not including spaces)</code>
<code class="n">is_alpha</code> <code class="o">=</code> <code class="n">s</code><code class="o">.</code><code class="n">isalpha</code><code class="p">()</code>

<code class="c1"># Encode as utf-8</code>
<code class="n">encode_as_utf8</code> <code class="o">=</code> <code class="n">s</code><code class="o">.</code><code class="n">encode</code><code class="p">(</code><code class="s2">"utf-8"</code><code class="p">)</code>

<code class="c1"># Decode the same utf-8</code>
<code class="n">decode</code> <code class="o">=</code> <code class="n">encode_as_utf8</code><code class="o">.</code><code class="n">decode</code><code class="p">(</code><code class="s2">"utf-8"</code><code class="p">)</code>

<code class="nb">print</code><code class="p">(</code>
  <code class="n">find_n</code><code class="p">,</code>
  <code class="n">starts_with_m</code><code class="p">,</code>
  <code class="n">ends_with_python</code><code class="p">,</code>
  <code class="n">is_alnum</code><code class="p">,</code>
  <code class="n">is_alpha</code><code class="p">,</code>
  <code class="n">encode_as_utf8</code><code class="p">,</code>
  <code class="n">decode</code><code class="p">,</code>
  <code class="n">sep</code> <code class="o">=</code> <code class="s2">"|"</code>
<code class="p">)</code></pre>
<pre data-type="programlisting">5|True|False|False|False|b'machine learning in python cookbook'|machine learning
  in python cookbook</pre>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1270">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/hSqsa">Beginners Tutorial for Regular Expressions in Python</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="6.2 Parsing and Cleaning HTML" data-type="sect1"><div class="sect1" id="parsing-and-cleaning-html">
<h1>6.2 Parsing and Cleaning HTML</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id148">
<h2>Problem</h2>
<p>You have text data with <a data-primary="text" data-secondary="parsing and cleaning HTML" data-type="indexterm" id="id1271"/><a data-primary="parsing and cleaning HTML" data-type="indexterm" id="id1272"/><a data-primary="HTML, parsing and cleaning" data-type="indexterm" id="id1273"/>HTML elements and want to extract just the text.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id149">
<h2>Solution</h2>
<p>Use Beautiful Soup’s <a data-primary="Beautiful Soup library" data-type="indexterm" id="id1274"/>extensive set of options to parse and extract from HTML:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">from</code> <code class="nn">bs4</code> <code class="kn">import</code> <code class="n">BeautifulSoup</code>

<code class="c1"># Create some HTML code</code>
<code class="n">html</code> <code class="o">=</code> <code class="s2">"&lt;div class='full_name'&gt;"</code>\
       <code class="s2">"&lt;span style='font-weight:bold'&gt;Masego"</code>\
       <code class="s2">"&lt;/span&gt; Azra&lt;/div&gt;"</code>

<code class="c1"># Parse html</code>
<code class="n">soup</code> <code class="o">=</code> <code class="n">BeautifulSoup</code><code class="p">(</code><code class="n">html</code><code class="p">,</code> <code class="s2">"lxml"</code><code class="p">)</code>

<code class="c1"># Find the div with the class "full_name", show text</code>
<code class="n">soup</code><code class="o">.</code><code class="n">find</code><code class="p">(</code><code class="s2">"div"</code><code class="p">,</code> <code class="p">{</code> <code class="s2">"class"</code> <code class="p">:</code> <code class="s2">"full_name"</code> <code class="p">})</code><code class="o">.</code><code class="n">text</code></pre>
<pre data-type="programlisting">'Masego Azra'</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id1275">
<h2>Discussion</h2>
<p>Despite the strange name, Beautiful Soup is a powerful Python library
designed for scraping HTML. Typically Beautiful Soup is used to process HTML during live web scraping, but we can just as easily use it to extract text data
embedded in static HTML. The full range of Beautiful Soup operations is beyond
the scope of this book, but even the method we use in our
solution shows how easy it can be to parse HTML and extract information from specific tags using <code>find()</code>.</p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1276">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/vh8h3">Beautiful Soup</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="6.3 Removing Punctuation" data-type="sect1"><div class="sect1" id="removing-punctuation">
<h1>6.3 Removing Punctuation</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id150">
<h2>Problem</h2>
<p>You have a feature of text data and want to remove <a data-primary="text" data-secondary="removing punctuation" data-type="indexterm" id="id1277"/><a data-primary="punctuation, removing from text" data-type="indexterm" id="id1278"/>punctuation.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id151">
<h2>Solution</h2>
<p>Define a <a data-primary="translate method" data-type="indexterm" id="id1279"/>function that uses <code>translate</code> with a dictionary of punctuation
characters:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">unicodedata</code>
<code class="kn">import</code> <code class="nn">sys</code>

<code class="c1"># Create text</code>
<code class="n">text_data</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'Hi!!!! I. Love. This. Song....'</code><code class="p">,</code>
             <code class="s1">'10000% Agree!!!! #LoveIT'</code><code class="p">,</code>
             <code class="s1">'Right?!?!'</code><code class="p">]</code>

<code class="c1"># Create a dictionary of punctuation characters</code>
<code class="n">punctuation</code> <code class="o">=</code> <code class="nb">dict</code><code class="o">.</code><code class="n">fromkeys</code><code class="p">(</code>
  <code class="p">(</code><code class="n">i</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">sys</code><code class="o">.</code><code class="n">maxunicode</code><code class="p">)</code>
  <code class="k">if</code> <code class="n">unicodedata</code><code class="o">.</code><code class="n">category</code><code class="p">(</code><code class="nb">chr</code><code class="p">(</code><code class="n">i</code><code class="p">))</code><code class="o">.</code><code class="n">startswith</code><code class="p">(</code><code class="s1">'P'</code><code class="p">)</code>
  <code class="p">),</code>
  <code class="kc">None</code>
<code class="p">)</code>

<code class="c1"># For each string, remove any punctuation characters</code>
<code class="p">[</code><code class="n">string</code><code class="o">.</code><code class="n">translate</code><code class="p">(</code><code class="n">punctuation</code><code class="p">)</code> <code class="k">for</code> <code class="n">string</code> <code class="ow">in</code> <code class="n">text_data</code><code class="p">]</code></pre>
<pre data-type="programlisting">['Hi I Love This Song', '10000 Agree LoveIT', 'Right']</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id1280">
<h2>Discussion</h2>
<p>The Python <code>translate</code> method is popular due to its speed. In our
solution, first we created a dictionary, <code>punctuation</code>, with all
punctuation characters according to Unicode as its keys and <code>None</code> as
its values. Next we translated all characters in the string that are in
<code>punctuation</code> into <code>None</code>, effectively removing them. There are more
readable ways to remove punctuation, but this somewhat hacky
solution has the advantage of being far faster than alternatives.</p>
<p>It is important to be conscious of the fact that punctuation contains
information (e.g., “Right?” versus “Right!”). Removing punctuation can be a
necessary evil when we need to manually create features; however, if the punctuation is
important we should make sure to take that into account. Depending on the downstream task we’re trying to accomplish, punctuation might contain important information we want to keep (e.g., using a “?” to classify if some text contains a question).</p>
</div></section>
</div></section>
<section data-pdf-bookmark="6.4 Tokenizing Text" data-type="sect1"><div class="sect1" id="tokenizing-text">
<h1>6.4 Tokenizing Text</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id152">
<h2>Problem</h2>
<p>You have <a data-primary="text" data-secondary="tokenizing" data-type="indexterm" id="ix_text_token"/><a data-primary="tokenizing text" data-type="indexterm" id="ix_token_text"/>text and want to break it up into individual words.</p>
</div></section>
<section class="less_space pagebreak-before" data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id153">
<h2>Solution</h2>
<p>Natural <a data-primary="Natural Language Toolkit (NLTK)" data-type="indexterm" id="ix_nat_lang_toolkit_nltk"/><a data-primary="NLTK (Natural Language Toolkit)" data-type="indexterm" id="ix_nat_lang_toolkit_nltk_rev"/>Language Toolkit for Python (NLTK) has a powerful set of text
manipulation operations, including word tokenizing:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">from</code> <code class="nn">nltk.tokenize</code> <code class="kn">import</code> <code class="n">word_tokenize</code>

<code class="c1"># Create text</code>
<code class="n">string</code> <code class="o">=</code> <code class="s2">"The science of today is the technology of tomorrow"</code>

<code class="c1"># Tokenize words</code>
<code class="n">word_tokenize</code><code class="p">(</code><code class="n">string</code><code class="p">)</code></pre>
<pre data-type="programlisting">['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrow']</pre>
<p>We can also tokenize into sentences:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">from</code> <code class="nn">nltk.tokenize</code> <code class="kn">import</code> <code class="n">sent_tokenize</code>

<code class="c1"># Create text</code>
<code class="n">string</code> <code class="o">=</code> <code class="s2">"The science of today is the technology of tomorrow. Tomorrow is today."</code>

<code class="c1"># Tokenize sentences</code>
<code class="n">sent_tokenize</code><code class="p">(</code><code class="n">string</code><code class="p">)</code></pre>
<pre data-type="programlisting">['The science of today is the technology of tomorrow.', 'Tomorrow is today.']</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id1281">
<h2>Discussion</h2>
<p><em>Tokenization</em>, especially word tokenization, is a common task after
cleaning text data because it is the first step in the process of
turning the text into data we will use to construct useful features. Some pretrained NLP models (such as Google’s BERT) utilize model-specific tokenization techniques; however, word-level tokenization is still a fairly common tokenization approach before getting features from individual words.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="6.5 Removing Stop Words" data-type="sect1"><div class="sect1" id="removing-stop-words">
<h1>6.5 Removing Stop Words</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id154">
<h2>Problem</h2>
<p>Given tokenized text data, you want to remove extremely common <a data-primary="text" data-secondary="removing stop words" data-type="indexterm" id="ix_text_remove_stop"/><a data-primary="stop words, removing from text" data-type="indexterm" id="ix_stop_word_remove"/>words
(e.g., <em>a</em>, <em>is</em>, <em>of</em>, <em>on</em>) that contain little informational value.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id1282">
<h2>Solution</h2>
<p>Use NLTK’s <code>stopwords</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">from</code> <code class="nn">nltk.corpus</code> <code class="kn">import</code> <code class="n">stopwords</code>

<code class="c1"># You will have to download the set of stop words the first time</code>
<code class="c1"># import nltk</code>
<code class="c1"># nltk.download('stopwords')</code>

<code class="c1"># Create word tokens</code>
<code class="n">tokenized_words</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'i'</code><code class="p">,</code>
                   <code class="s1">'am'</code><code class="p">,</code>
                   <code class="s1">'going'</code><code class="p">,</code>
                   <code class="s1">'to'</code><code class="p">,</code>
                   <code class="s1">'go'</code><code class="p">,</code>
                   <code class="s1">'to'</code><code class="p">,</code>
                   <code class="s1">'the'</code><code class="p">,</code>
                   <code class="s1">'store'</code><code class="p">,</code>
                   <code class="s1">'and'</code><code class="p">,</code>
                   <code class="s1">'park'</code><code class="p">]</code>

<code class="c1"># Load stop words</code>
<code class="n">stop_words</code> <code class="o">=</code> <code class="n">stopwords</code><code class="o">.</code><code class="n">words</code><code class="p">(</code><code class="s1">'english'</code><code class="p">)</code>

<code class="c1"># Remove stop words</code>
<code class="p">[</code><code class="n">word</code> <code class="k">for</code> <code class="n">word</code> <code class="ow">in</code> <code class="n">tokenized_words</code> <code class="k">if</code> <code class="n">word</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">stop_words</code><code class="p">]</code></pre>
<pre data-type="programlisting">['going', 'go', 'store', 'park']</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id155">
<h2>Discussion</h2>
<p>While “stop words” can refer to any set of words we want to remove
before processing, frequently the term refers to extremely common words
that themselves contain little information value. Whether or not you choose to remove stop words will depend on your individual use case. NLTK has a list of
common stop words that we can use to find and remove stop words in our
tokenized words:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Show stop words</code>
<code class="n">stop_words</code><code class="p">[:</code><code class="mi">5</code><code class="p">]</code></pre>
<pre data-type="programlisting">['i', 'me', 'my', 'myself', 'we']</pre>
<p>Note that NLTK’s <code>stopwords</code> assumes the tokenized words are all
lowercased.<a data-primary="" data-startref="ix_stop_word_remove" data-type="indexterm" id="id1283"/><a data-primary="" data-startref="ix_text_remove_stop" data-type="indexterm" id="id1284"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="6.6 Stemming Words" data-type="sect1"><div class="sect1" id="stemming-words">
<h1>6.6 Stemming Words</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id156">
<h2>Problem</h2>
<p>You have <a data-primary="stemming of words" data-type="indexterm" id="id1285"/><a data-primary="text" data-secondary="stemming words" data-type="indexterm" id="id1286"/>tokenized words and want to convert them into their root forms.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id1287">
<h2>Solution</h2>
<p>Use NLTK’s <code>PorterStemmer</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">from</code> <code class="nn">nltk.stem.porter</code> <code class="kn">import</code> <code class="n">PorterStemmer</code>

<code class="c1"># Create word tokens</code>
<code class="n">tokenized_words</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'i'</code><code class="p">,</code> <code class="s1">'am'</code><code class="p">,</code> <code class="s1">'humbled'</code><code class="p">,</code> <code class="s1">'by'</code><code class="p">,</code> <code class="s1">'this'</code><code class="p">,</code> <code class="s1">'traditional'</code><code class="p">,</code> <code class="s1">'meeting'</code><code class="p">]</code>

<code class="c1"># Create stemmer</code>
<code class="n">porter</code> <code class="o">=</code> <code class="n">PorterStemmer</code><code class="p">()</code>

<code class="c1"># Apply stemmer</code>
<code class="p">[</code><code class="n">porter</code><code class="o">.</code><code class="n">stem</code><code class="p">(</code><code class="n">word</code><code class="p">)</code> <code class="k">for</code> <code class="n">word</code> <code class="ow">in</code> <code class="n">tokenized_words</code><code class="p">]</code></pre>
<pre data-type="programlisting">['i', 'am', 'humbl', 'by', 'thi', 'tradit', 'meet']</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id157">
<h2>Discussion</h2>
<p><em>Stemming</em> reduces a word to its stem by identifying and removing affixes
(e.g., gerunds) while keeping the root meaning of the word. For example,
both “tradition” and “traditional” have “tradit” as their stem, indicating that while they are different words, they represent the same
general concept. By stemming our text data, we transform it to something
less readable but closer to its base meaning and thus more suitable for
comparison across observations. NLTK’s <code>PorterStemmer</code> implements the
widely used Porter stemming algorithm to remove or replace common
suffixes to produce the word stem.<a data-primary="" data-startref="ix_nat_lang_toolkit_nltk" data-type="indexterm" id="id1288"/><a data-primary="" data-startref="ix_nat_lang_toolkit_nltk_rev" data-type="indexterm" id="id1289"/><a data-primary="" data-startref="ix_token_text" data-type="indexterm" id="id1290"/><a data-primary="" data-startref="ix_text_token" data-type="indexterm" id="id1291"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1292">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/Z4NTp">The Porter Stemming Algorithm</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="6.7 Tagging Parts of Speech" data-type="sect1"><div class="sect1" id="tagging-parts-of-speech">
<h1>6.7 Tagging Parts of Speech</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id158">
<h2>Problem</h2>
<p>You have text data and want to <a data-primary="text" data-secondary="tagging parts of speech" data-type="indexterm" id="ix_text_tag_part_speech"/><a data-primary="tagging parts of speech" data-type="indexterm" id="ix_tag_part_speech"/>tag each word or character with its part of speech.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id1293">
<h2>Solution</h2>
<p>Use NLTK’s pretrained parts-of-speech tagger:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">nltk</code> <code class="kn">import</code> <code class="n">pos_tag</code>
<code class="kn">from</code> <code class="nn">nltk</code> <code class="kn">import</code> <code class="n">word_tokenize</code>

<code class="c1"># Create text</code>
<code class="n">text_data</code> <code class="o">=</code> <code class="s2">"Chris loved outdoor running"</code>

<code class="c1"># Use pretrained part of speech tagger</code>
<code class="n">text_tagged</code> <code class="o">=</code> <code class="n">pos_tag</code><code class="p">(</code><code class="n">word_tokenize</code><code class="p">(</code><code class="n">text_data</code><code class="p">))</code>

<code class="c1"># Show parts of speech</code>
<code class="n">text_tagged</code></pre>
<pre data-type="programlisting">[('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]</pre>
<p>The output is a list of tuples with the word and the tag of the part of
speech. NLTK uses the Penn Treebank parts for speech tags. Some examples
of the Penn Treebank tags are:</p>
<table>
<thead><tr>
<th>Tag</th>
<th>Part of speech</th>
</tr></thead>
<tbody>
<tr>
<td>NNP</td>
<td>Proper noun, singular</td>
</tr>
<tr>
<td>NN</td>
<td>Noun, singular or mass</td>
</tr>
<tr>
<td>RB</td>
<td>Adverb</td>
</tr>
<tr>
<td>VBD</td>
<td>Verb, past tense</td>
</tr>
<tr>
<td>VBG</td>
<td>Verb, gerund or present participle</td>
</tr>
<tr>
<td>JJ</td>
<td>Adjective</td>
</tr>
<tr>
<td>PRP</td>
<td>Personal pronoun</td>
</tr></tbody>
</table>
<p>Once the text has been tagged, we can use the tags to find certain parts
of speech. For example, here are all nouns:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Filter words</code>
<code class="p">[</code><code class="n">word</code> <code class="k">for</code> <code class="n">word</code><code class="p">,</code> <code class="n">tag</code> <code class="ow">in</code> <code class="n">text_tagged</code> <code class="k">if</code> <code class="n">tag</code> <code class="ow">in</code> <code class="p">[</code><code class="s1">'NN'</code><code class="p">,</code><code class="s1">'NNS'</code><code class="p">,</code><code class="s1">'NNP'</code><code class="p">,</code><code class="s1">'NNPS'</code><code class="p">]</code> <code class="p">]</code></pre>
<pre data-type="programlisting">['Chris']</pre>
<p>A more realistic situation would be to have data where every
observation contains a tweet, and we want to convert those sentences into
features for individual parts of speech (e.g., a feature with <code>1</code> if a proper noun is present, and <code>0</code> otherwise):</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">MultiLabelBinarizer</code>

<code class="c1"># Create text</code>
<code class="n">tweets</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"I am eating a burrito for breakfast"</code><code class="p">,</code>
          <code class="s2">"Political science is an amazing field"</code><code class="p">,</code>
          <code class="s2">"San Francisco is an awesome city"</code><code class="p">]</code>

<code class="c1"># Create list</code>
<code class="n">tagged_tweets</code> <code class="o">=</code> <code class="p">[]</code>

<code class="c1"># Tag each word and each tweet</code>
<code class="k">for</code> <code class="n">tweet</code> <code class="ow">in</code> <code class="n">tweets</code><code class="p">:</code>
    <code class="n">tweet_tag</code> <code class="o">=</code> <code class="n">nltk</code><code class="o">.</code><code class="n">pos_tag</code><code class="p">(</code><code class="n">word_tokenize</code><code class="p">(</code><code class="n">tweet</code><code class="p">))</code>
    <code class="n">tagged_tweets</code><code class="o">.</code><code class="n">append</code><code class="p">([</code><code class="n">tag</code> <code class="k">for</code> <code class="n">word</code><code class="p">,</code> <code class="n">tag</code> <code class="ow">in</code> <code class="n">tweet_tag</code><code class="p">])</code>

<code class="c1"># Use one-hot encoding to convert the tags into features</code>
<code class="n">one_hot_multi</code> <code class="o">=</code> <code class="n">MultiLabelBinarizer</code><code class="p">()</code>
<code class="n">one_hot_multi</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">tagged_tweets</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([[1, 1, 0, 1, 0, 1, 1, 1, 0],
       [1, 0, 1, 1, 0, 0, 0, 0, 1],
       [1, 0, 1, 1, 1, 0, 0, 0, 1]])</pre>
<p>Using <code>classes_</code> we can see that each feature is a part-of-speech tag:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Show feature names</code>
<code class="n">one_hot_multi</code><code class="o">.</code><code class="n">classes_</code></pre>
<pre data-type="programlisting">array(['DT', 'IN', 'JJ', 'NN', 'NNP', 'PRP', 'VBG', 'VBP', 'VBZ'], dtype=object)</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id159">
<h2>Discussion</h2>
<p>If our text is English and not on a specialized topic (e.g., medicine) the simplest solution is to use NLTK’s pretrained parts-of-speech
tagger. However, if <code>pos_tag</code> is not very accurate, NLTK also gives us the ability to train our own tagger. The major downside of training a tagger is that we need a large corpus of text where the tag of each word is known. Constructing this tagged corpus is obviously labor intensive and is probably going to be a last resort.<a data-primary="" data-startref="ix_tag_part_speech" data-type="indexterm" id="id1294"/><a data-primary="" data-startref="ix_text_tag_part_speech" data-type="indexterm" id="id1295"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1296">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/31xKf">Alphabetical list of part-of-speech tags used in the Penn Treebank Project</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="6.8 Performing Named-Entity Recognition" data-type="sect1"><div class="sect1" id="performing-named-entity-recognition">
<h1>6.8 Performing Named-Entity Recognition</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id160">
<h2>Problem</h2>
<p>You want to perform <a data-primary="named-entity recognition" data-type="indexterm" id="id1297"/><a data-primary="text" data-secondary="named-entity recognition" data-type="indexterm" id="id1298"/>named-entity recognition in freeform text (such as “Person,” “State,” etc.).</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id161">
<h2>Solution</h2>
<p>Use spaCy’s <a data-primary="spaCy library, named-entity recognition" data-type="indexterm" id="id1299"/>default named-entity recognition pipeline and models to extract entites from text:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import libraries</code>
<code class="kn">import</code> <code class="nn">spacy</code>

<code class="c1"># Load the spaCy package and use it to parse the text</code>
<code class="c1"># make sure you have run "python -m spacy download en"</code>
<code class="n">nlp</code> <code class="o">=</code> <code class="n">spacy</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="s2">"en_core_web_sm"</code><code class="p">)</code>
<code class="n">doc</code> <code class="o">=</code> <code class="n">nlp</code><code class="p">(</code><code class="s2">"Elon Musk offered to buy Twitter using $21B of his own money."</code><code class="p">)</code>

<code class="c1"># Print each entity</code>
<code class="nb">print</code><code class="p">(</code><code class="n">doc</code><code class="o">.</code><code class="n">ents</code><code class="p">)</code>

<code class="c1"># For each entity print the text and the entity label</code>
<code class="k">for</code> <code class="n">entity</code> <code class="ow">in</code> <code class="n">doc</code><code class="o">.</code><code class="n">ents</code><code class="p">:</code>
    <code class="nb">print</code><code class="p">(</code><code class="n">entity</code><code class="o">.</code><code class="n">text</code><code class="p">,</code> <code class="n">entity</code><code class="o">.</code><code class="n">label_</code><code class="p">,</code> <code class="n">sep</code><code class="o">=</code><code class="s2">","</code><code class="p">)</code></pre>
<pre data-type="programlisting">(Elon Musk, Twitter, 21B)
Elon Musk, PERSON
Twitter, ORG
21B, MONEY</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id1300">
<h2>Discussion</h2>
<p>Named-entity recognition is the process of recognizing specific entities from text. Tools like spaCy offer preconfigured pipelines, and even pretrained or fine-tuned machine learning models that can easily identify these entities. In this case, we use spaCy to identify a person (“Elon Musk”), organization (“Twitter”), and money value (“21B”) from the raw text. Using this information, we can extract structured information from the unstructured textual data. This information can then be used in downstream machine learning models or data analysis.</p>
<p>Training a custom named-entity recognition model is outside the scope of this example; however, it is often done using deep learning and other NLP techniques.</p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1301">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/cN8KM">spaCy Named Entity Recognition documentation</a></p>
</li>
<li>
<p><a href="https://oreil.ly/G8WDF">Named-entity recognition, Wikipedia</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="6.9 Encoding Text as a Bag of Words" data-type="sect1"><div class="sect1" id="encoding-text-as-a-bag-of-words">
<h1>6.9 Encoding Text as a Bag of Words</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id162">
<h2>Problem</h2>
<p>You have <a data-primary="text" data-secondary="encoding as bag of words" data-type="indexterm" id="ix_text_bag_words"/><a data-primary="bag-of-words model" data-type="indexterm" id="ix_bag_words_mod"/>text data and want to create a set of features indicating the number of times an observation’s text contains a particular word.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id163">
<h2>Solution</h2>
<p>Use <a data-primary="CountVectorizer" data-type="indexterm" id="ix_count_vector"/>scikit-learn’s <code>CountVectorizer</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn.feature_extraction.text</code> <code class="kn">import</code> <code class="n">CountVectorizer</code>

<code class="c1"># Create text</code>
<code class="n">text_data</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="s1">'I love Brazil. Brazil!'</code><code class="p">,</code>
                      <code class="s1">'Sweden is best'</code><code class="p">,</code>
                      <code class="s1">'Germany beats both'</code><code class="p">])</code>

<code class="c1"># Create the bag of words feature matrix</code>
<code class="n">count</code> <code class="o">=</code> <code class="n">CountVectorizer</code><code class="p">()</code>
<code class="n">bag_of_words</code> <code class="o">=</code> <code class="n">count</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">text_data</code><code class="p">)</code>

<code class="c1"># Show feature matrix</code>
<code class="n">bag_of_words</code></pre>
<pre data-type="programlisting">&lt;3x8 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
    with 8 stored elements in Compressed Sparse Row format&gt;</pre>
<p>This output is a sparse array, which is often necessary when we have a
large amount of text. However, in our toy example we can use <code>toarray</code>
to view a matrix of word counts for each observation:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">bag_of_words</code><code class="o">.</code><code class="n">toarray</code><code class="p">()</code></pre>
<pre data-type="programlisting">array([[0, 0, 0, 2, 0, 0, 1, 0],
       [0, 1, 0, 0, 0, 1, 0, 1],
       [1, 0, 1, 0, 1, 0, 0, 0]], dtype=int64)</pre>
<p>We can use the <code>get_feature_names</code> method to view the word associated with
each feature:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Show feature names</code>
<code class="n">count</code><code class="o">.</code><code class="n">get_feature_names_out</code><code class="p">()</code></pre>
<pre data-type="programlisting">array(['beats', 'best', 'both', 'brazil', 'germany', 'is', 'love',
       'sweden'], dtype=object)</pre>
<p>Note that the <code>I</code> from <code>I love Brazil</code> is not considered a token because the default <code>token_pattern</code> only considers tokens of two or more alphanumeric characters.</p>
<p>Still, this might be confusing so, for the sake of clarity, here is what the
feature matrix looks like with the words as column names (each row is
one observation):</p>
<table> <thead> <tr> <th>beats</th> <th>best</th> <th>both</th> <th>brazil</th> <th>germany</th> <th>is</th> <th>love</th> <th>sweden</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>0</td> <td>0</td> <td>2</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> </tr> <tr> <td>0</td> <td>1</td> <td>0</td> <td>0</td> <td>0</td> <td>1</td> <td>0</td> <td>1</td> </tr> <tr> <td>1</td> <td>0</td> <td>1</td> <td>0</td> <td>1</td> <td>0</td> <td>0</td> <td>0</td> </tr> </tbody></table>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id164">
<h2>Discussion</h2>
<p>One of the most common methods of transforming text into features is
using a bag-of-words model. Bag-of-words models output a feature for
every unique word in text data, with each feature containing a count of
occurrences in observations. For example, in our solution, the sentence
“I love Brazil. Brazil!” has a value of <code>2</code> in the “brazil” feature
because the word <em>brazil</em> appears two times.</p>
<p>The text data in our solution was purposely small. In the real world,
a single observation of text data could be the contents of an entire book! Since our bag-of-words model creates a feature for every unique word in the data, the resulting matrix can contain thousands of features. This means the size of the matrix can sometimes become very large in memory. Luckily, we can exploit a common characteristic of bag-of-words feature matrices to reduce the amount of data we need to store.</p>
<p>Most <a data-primary="sparse matrix" data-secondary="encoding text as bag of words" data-type="indexterm" id="id1302"/>words likely do not occur in most observations, and therefore bag-of-words feature matrices will contain mostly 0s as values. We call these types of matrices <em>sparse</em>. Instead of storing all values of the matrix, we can store only nonzero values and then assume all other values are 0. This will save memory when we have large feature matrices. One of the nice features of <code>CountVectorizer</code> is that the output is a sparse matrix by default.</p>
<p><code>CountVectorizer</code> comes with a number of useful parameters to make it easy to
create bag-of-words feature matrices. First, while by default
every feature is a word, that does not have to be the case. Instead we
can set every feature to be the combination of two words (called a
2-gram) or even three words (3-gram). <code>ngram_range</code> sets the minimum and
maximum size of our <em>n</em>-grams. For example, <code>(2,3)</code> will return all 2-grams and 3-grams. Second, we can easily remove low-information filler words by using <code>stop_words</code>, either with a built-in list or a custom list. Finally, we can restrict the words or phrases we want to consider to a certain list of words using <code>vocabulary</code>. For example, we could create a bag-of-words feature matrix only for occurrences of country names:<a data-primary="" data-startref="ix_bag_words_mod" data-type="indexterm" id="id1303"/><a data-primary="" data-startref="ix_count_vector" data-type="indexterm" id="id1304"/><a data-primary="" data-startref="ix_text_bag_words" data-type="indexterm" id="id1305"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create feature matrix with arguments</code>
<code class="n">count_2gram</code> <code class="o">=</code> <code class="n">CountVectorizer</code><code class="p">(</code><code class="n">ngram_range</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code class="mi">2</code><code class="p">),</code>
                              <code class="n">stop_words</code><code class="o">=</code><code class="s2">"english"</code><code class="p">,</code>
                              <code class="n">vocabulary</code><code class="o">=</code><code class="p">[</code><code class="s1">'brazil'</code><code class="p">])</code>
<code class="n">bag</code> <code class="o">=</code> <code class="n">count_2gram</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">text_data</code><code class="p">)</code>

<code class="c1"># View feature matrix</code>
<code class="n">bag</code><code class="o">.</code><code class="n">toarray</code><code class="p">()</code></pre>
<pre data-type="programlisting">array([[2],
       [0],
       [0]])</pre>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View the 1-grams and 2-grams</code>
<code class="n">count_2gram</code><code class="o">.</code><code class="n">vocabulary_</code></pre>
<pre data-type="programlisting">{'brazil': 0}</pre>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1306">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/XWIrM"><em>n</em>-gram, Wikipedia</a></p>
</li>
<li>
<p><a href="https://oreil.ly/IiyRV">Bag of Words Meets Bags of Popcorn</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="6.10 Weighting Word Importance" data-type="sect1"><div class="sect1" id="weighting-word-importance">
<h1>6.10 Weighting Word Importance</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id165">
<h2>Problem</h2>
<p>You want a bag of words with words <a data-primary="weighting word importance" data-type="indexterm" id="ix_weight_words"/><a data-primary="text" data-secondary="weighting word importance" data-type="indexterm" id="ix_text_weight_words"/>weighted by their importance to
an observation.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id166">
<h2>Solution</h2>
<p>Compare the frequency of the word in a document (a tweet, movie
review, speech transcript, etc.) with the frequency of the word in all
other documents using <a data-primary="TfidfVectorizer" data-type="indexterm" id="ix_tfid_vector_class"/><a data-primary="term frequency-inverse document frequency (tf-idf)" data-type="indexterm" id="ix_term_freq_inv_doc_freq"/>term frequency-inverse document frequency
(<math display="inline"><mtext class="left_paren" fontstyle="italic">tf-idf</mtext></math>). scikit-learn makes this easy with <code>TfidfVectorizer</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn.feature_extraction.text</code> <code class="kn">import</code> <code class="n">TfidfVectorizer</code>

<code class="c1"># Create text</code>
<code class="n">text_data</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="s1">'I love Brazil. Brazil!'</code><code class="p">,</code>
                      <code class="s1">'Sweden is best'</code><code class="p">,</code>
                      <code class="s1">'Germany beats both'</code><code class="p">])</code>

<code class="c1"># Create the tf-idf feature matrix</code>
<code class="n">tfidf</code> <code class="o">=</code> <code class="n">TfidfVectorizer</code><code class="p">()</code>
<code class="n">feature_matrix</code> <code class="o">=</code> <code class="n">tfidf</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">text_data</code><code class="p">)</code>

<code class="c1"># Show tf-idf feature matrix</code>
<code class="n">feature_matrix</code></pre>
<pre data-type="programlisting">&lt;3x8 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
    with 8 stored elements in Compressed Sparse Row format&gt;</pre>
<p>Just as in <a data-type="xref" href="#encoding-text-as-a-bag-of-words">Recipe 6.9</a>, the output is a <a data-primary="sparse matrix" data-secondary="weighting word importance" data-type="indexterm" id="id1307"/>sparse matrix. However, if we
want to view the output as a <a data-primary="dense matrix, weighting word importance" data-type="indexterm" id="id1308"/>dense matrix, we can use <code>toarray</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Show tf-idf feature matrix as dense matrix</code>
<code class="n">feature_matrix</code><code class="o">.</code><code class="n">toarray</code><code class="p">()</code></pre>
<pre data-type="programlisting">array([[ 0.        ,  0.        ,  0.        ,  0.89442719,  0.        ,
         0.        ,  0.4472136 ,  0.        ],
       [ 0.        ,  0.57735027,  0.        ,  0.        ,  0.        ,
         0.57735027,  0.        ,  0.57735027],
       [ 0.57735027,  0.        ,  0.57735027,  0.        ,  0.57735027,
         0.        ,  0.        ,  0.        ]])</pre>
<p><code>vocabulary_</code> shows us the word of each feature:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Show feature names</code>
<code class="n">tfidf</code><code class="o">.</code><code class="n">vocabulary_</code></pre>
<pre data-type="programlisting">{'love': 6,
 'brazil': 3,
 'sweden': 7,
 'is': 5,
 'best': 1,
 'germany': 4,
 'beats': 0,
 'both': 2}</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id167">
<h2>Discussion</h2>
<p>The more a word appears in a document, the more likely it is that the word is important
to that document. For example, if the word <em>economy</em> appears frequently,
it is evidence that the document might be about economics. We <a data-primary="term frequency (tf)" data-type="indexterm" id="id1309"/>call this
<em>term frequency</em> (<math display="inline"><mtext class="left_paren" fontstyle="italic">tf</mtext></math>).</p>
<p>In contrast, if a word appears in many documents, it is likely less
important to any individual document. For example, if every document in
some text data contains the word <em>after</em> then it is probably an
unimportant word. We <a data-primary="df (document frequency)" data-type="indexterm" id="id1310"/><a data-primary="document frequency (df)" data-type="indexterm" id="id1311"/>call this <em>document frequency</em> (<math display="inline"><mtext class="left_paren" fontstyle="italic">df</mtext></math>).</p>
<p>By combining these two statistics, we can assign a score to every word
representing how important that word is in a document. <a data-primary="idf (inverse of document frequency)" data-type="indexterm" id="id1312"/><a data-primary="inverse of document frequency (idf)" data-type="indexterm" id="id1313"/>Specifically, we multiply <math display="inline"><mtext fontstyle="italic">tf</mtext></math> to the inverse of document frequency (<math display="inline"><mtext class="left_paren" fontstyle="italic">idf</mtext></math>):</p>
<div data-type="equation">
<math display="block">
<mrow>
<mtext class="left_paren" fontstyle="italic">tf-idf</mtext>
<mo>(</mo>
<mi>t</mi>
<mo>,</mo>
<mi>d</mi>
<mo>)</mo>
<mo>=</mo>
<mi>t</mi>
<mi>f</mi>
<mo class="left_paren">(</mo>
<mi>t</mi>
<mo>,</mo>
<mi>d</mi>
<mo>)</mo>
<mo>×</mo>
<mtext class="left_paren" fontstyle="italic">idf</mtext>
<mo>(</mo>
<mi class="left_paren">t</mi>
<mo>)</mo>
</mrow>
</math>
</div>
<p>where <math display="inline"><mi>t</mi></math> is a word (term) and <math display="inline"><mi>d</mi></math> is a document. There are a number of variations in how <math display="inline"><mtext fontstyle="italic">tf</mtext></math> and <math display="inline"><mtext fontstyle="italic">idf</mtext></math> are calculated. In scikit-learn, <math display="inline"><mtext fontstyle="italic">tf</mtext></math> is simply the number of times a word appears in the document, and <math display="inline"><mtext fontstyle="italic">idf</mtext></math> is calculated as:</p>
<div data-type="equation">
<math display="block">
<mrow>
<mtext class="left_paren" fontstyle="italic">idf</mtext>
<mrow>
<mo>(</mo>
<mi>t</mi>
<mo>)</mo>
</mrow>
<mo>=</mo>
<mi>l</mi>
<mi>o</mi>
<mi>g</mi>
<mfrac><mrow><mn>1</mn><mo>+</mo><msub><mi>n</mi> <mi>d</mi> </msub></mrow> <mrow><mn>1</mn><mo>+</mo><mtext class="left_paren" fontstyle="italic">df</mtext><mo>(</mo><mi>d</mi><mo>,</mo><mi>t</mi><mo>)</mo></mrow></mfrac>
<mo>+</mo>
<mn>1</mn>
</mrow>
</math>
</div>
<p>where <math display="inline"><msub><mi>n</mi><mi>d</mi></msub></math> is the number of documents, and
<math display="inline"><mtext class="left_paren" fontstyle="italic">df</mtext><mo>(</mo><mi>d</mi><mo>,</mo><mi>t</mi><mo>)</mo></math> is term <math display="inline"><mi>t</mi></math>’s document frequency (i.e., the number of documents where the term appears).</p>
<p>By default, scikit-learn then normalizes the <math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math> vectors using the Euclidean norm (L2 norm). The higher the resulting value, the more important the word is to a document.<a data-primary="" data-startref="ix_text_weight_words" data-type="indexterm" id="id1314"/><a data-primary="" data-startref="ix_weight_words" data-type="indexterm" id="id1315"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1316">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/40WeT">scikit-learn documentation: <em>tf–idf</em> term weighting</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="6.11 Using Text Vectors to Calculate Text Similarity in a Search Query" data-type="sect1"><div class="sect1" id="using-text-vectors-to-calculate-text-similarity-in-a-search-query">
<h1>6.11 Using Text Vectors to Calculate Text Similarity in a Search Query</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id168">
<h2>Problem</h2>
<p>You want to use<a data-primary="tf-idf vectors to implement text" data-type="indexterm" id="ix_tf_idf_text"/><a data-primary="text" data-secondary="tf-idf vectors to implement" data-type="indexterm" id="ix_text_tf_idf"/> <math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math> vectors to implement a text search function in Python.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id1317">
<h2>Solution</h2>
<p>Calculate the cosine similarity between <math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math> vectors using scikit-learn:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn.feature_extraction.text</code> <code class="kn">import</code> <code class="n">TfidfVectorizer</code>
<code class="kn">from</code> <code class="nn">sklearn.metrics.pairwise</code> <code class="kn">import</code> <code class="n">linear_kernel</code>

<code class="c1"># Create searchable text data</code>
<code class="n">text_data</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="s1">'I love Brazil. Brazil!'</code><code class="p">,</code>
                      <code class="s1">'Sweden is best'</code><code class="p">,</code>
                      <code class="s1">'Germany beats both'</code><code class="p">])</code>

<code class="c1"># Create the tf-idf feature matrix</code>
<code class="n">tfidf</code> <code class="o">=</code> <code class="n">TfidfVectorizer</code><code class="p">()</code>
<code class="n">feature_matrix</code> <code class="o">=</code> <code class="n">tfidf</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">text_data</code><code class="p">)</code>

<code class="c1"># Create a search query and transform it into a tf-idf vector</code>
<code class="n">text</code> <code class="o">=</code> <code class="s2">"Brazil is the best"</code>
<code class="n">vector</code> <code class="o">=</code> <code class="n">tfidf</code><code class="o">.</code><code class="n">transform</code><code class="p">([</code><code class="n">text</code><code class="p">])</code>

<code class="c1"># Calculate the cosine similarities between the input vector and all other</code>
  <code class="n">vectors</code>
<code class="n">cosine_similarities</code> <code class="o">=</code> <code class="n">linear_kernel</code><code class="p">(</code><code class="n">vector</code><code class="p">,</code> <code class="n">feature_matrix</code><code class="p">)</code><code class="o">.</code><code class="n">flatten</code><code class="p">()</code>

<code class="c1"># Get the index of the most relevent items in order</code>
<code class="n">related_doc_indicies</code> <code class="o">=</code> <code class="n">cosine_similarities</code><code class="o">.</code><code class="n">argsort</code><code class="p">()[:</code><code class="o">-</code><code class="mi">10</code><code class="p">:</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code>

<code class="c1"># Print the most similar texts to the search query along with the cosine</code>
  <code class="n">similarity</code>
<code class="nb">print</code><code class="p">([(</code><code class="n">text_data</code><code class="p">[</code><code class="n">i</code><code class="p">],</code> <code class="n">cosine_similarities</code><code class="p">[</code><code class="n">i</code><code class="p">])</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="n">related_doc_indicies</code><code class="p">])</code></pre>
<pre data-type="programlisting">[
  (
    'Sweden is best', 0.6666666666666666),
    ('I love Brazil. Brazil!', 0.5163977794943222),
    ('Germany beats both', 0.0
    )
]</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id169">
<h2>Discussion</h2>
<p>Text vectors are incredibly useful for <a data-primary="natural language processing (NLP)" data-type="indexterm" id="id1318"/><a data-primary="NLP (natural language processing)" data-type="indexterm" id="id1319"/>NLP use cases such as search engines. After calculating the <math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math> vectors of a set of sentences or documents, we can use the same <code>tfidf</code> object to vectorize future sets of text. Then, we can compute cosine similarity between our input vector and the matrix of other vectors and sort by the most relevant documents.<a data-primary="" data-startref="ix_tfid_vector_class" data-type="indexterm" id="id1320"/><a data-primary="" data-startref="ix_term_freq_inv_doc_freq" data-type="indexterm" id="id1321"/><a data-primary="" data-startref="ix_text_tf_idf" data-type="indexterm" id="id1322"/><a data-primary="" data-startref="ix_tf_idf_text" data-type="indexterm" id="id1323"/></p>
<p class="fix_tracking">
Cosine similarities take on the range of [0, 1.0], with 0 being least similar and 1 being most similar. Since we’re using <math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math> vectors to compute the similarity between vectors, the frequency of a word’s occurrence is also taken into account. However, with a small corpus (set of documents) even “frequent” words may not appear frequently. In this example, “Sweden is best” is the most relevant text to our search query “Brazil is the best”. Since the query mentions Brazil, we might expect “I love Brazil. Brazil!” to be the most relevant; however, “Sweden is best” is the most similar due to the words “is” and “best”. As the number of documents we add to our corpus increases, less important words will be weighted less and have less effect on our cosine similarity calculation.
</p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1324">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/-5Odv">Cosine Similarity, Geeks for Geeks</a></p>
</li>
<li>
<p><a href="https://oreil.ly/pAxbR">Nvidia Gave Me a $15K Data Science Workstation—Here’s What I Did with It (building a Pubmed search engine in Python)</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="6.12 Using a Sentiment Analysis Classifier" data-type="sect1"><div class="sect1" id="using-a-sentiment-analysis-classifier">
<h1>6.12 Using a Sentiment Analysis Classifier</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id170">
<h2>Problem</h2>
<p>You want to <a data-primary="classification and classifiers" data-secondary="sentiment analysis classifier" data-type="indexterm" id="id1325"/><a data-primary="sentiment analysis classifier" data-type="indexterm" id="id1326"/><a data-primary="text" data-secondary="sentiment analysis classifier" data-type="indexterm" id="id1327"/>classify the sentiment of some text to use as a feature or in downstream data analysis.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id171">
<h2>Solution</h2>
<p>Use the <code>transformers</code> library’s <a data-primary="transformers library" data-type="indexterm" id="id1328"/>sentiment classifier.</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import libraries</code>
<code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">pipeline</code>

<code class="c1"># Create an NLP pipeline that runs sentiment analysis</code>
<code class="n">classifier</code> <code class="o">=</code> <code class="n">pipeline</code><code class="p">(</code><code class="s2">"sentiment-analysis"</code><code class="p">)</code>

<code class="c1"># Classify some text</code>
<code class="c1"># (this may download some data and models the first time you run it)</code>
<code class="n">sentiment_1</code> <code class="o">=</code> <code class="n">classifier</code><code class="p">(</code><code class="s2">"I hate machine learning! It's the absolute worst."</code><code class="p">)</code>
<code class="n">sentiment_2</code> <code class="o">=</code> <code class="n">classifier</code><code class="p">(</code>
    <code class="s2">"Machine learning is the absolute"</code>
    <code class="s2">"bees knees I love it so much!"</code>
<code class="p">)</code>

<code class="c1"># Print sentiment output</code>
<code class="nb">print</code><code class="p">(</code><code class="n">sentiment_1</code><code class="p">,</code> <code class="n">sentiment_2</code><code class="p">)</code></pre>
<pre data-type="programlisting">[
  {
    'label': 'NEGATIVE',
    'score': 0.9998020529747009
  }
]
[
  {
    'label': 'POSITIVE',
    'score': 0.9990628957748413
  }
]</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id172">
<h2>Discussion</h2>
<p>The <code>transformers</code> library is an extremely popular library for <a data-primary="natural language processing (NLP)" data-type="indexterm" id="id1329"/><a data-primary="NLP (natural language processing)" data-type="indexterm" id="id1330"/>NLP tasks and contains a number of easy-to-use APIs for training models or using pretrained ones. We’ll talk more about NLP and this library in <a data-type="xref" href="ch22.xhtml#neural-networks-for-unstructured-data">Chapter 22</a>, but this example serves as a high-level introduction to the power of using pretrained classifiers in your machine learning pipelines to generate features, classify text, or analyze unstructured data.<a data-primary="" data-startref="ix_text_data_ch6" data-type="indexterm" id="id1331"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1332">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/7hT6W">Hugging Face Transformers Quick Tour</a></p>
</li>
</ul>
</div></section>
</div></section>
</div></section></div></body></html>