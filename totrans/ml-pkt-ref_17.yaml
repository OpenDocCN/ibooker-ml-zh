- en: Chapter 17\. Dimensionality Reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many techniques to decompose features into a smaller subset. This
    can be useful for exploratory data analysis, visualization, making predictive
    models, or clustering.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we will explore the Titanic dataset using various techniques.
    We will look at PCA, UMAP, t-SNE, and PHATE.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Principal Component Analysis (PCA) takes a matrix (X) of rows (samples) and
    columns (features). PCA returns a new matrix that has columns that are linear
    combinations of the original columns. These linear combinations maximize the variance.
  prefs: []
  type: TYPE_NORMAL
- en: Each column is orthogonal (a right angle) to the other columns. The columns
    are sorted in order of decreasing variance.
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn has an implementation of this model. It is best to standardize
    the data prior to running the algorithm. After calling the `.fit` method, you
    will have access to an `.explained_variance_ratio_` attribute that lists the percentage
    of variance in each column.
  prefs: []
  type: TYPE_NORMAL
- en: PCA is useful to visualize data in two (or three) dimensions. It is also used
    as a preprocessing step to filter out random noise in data. It is good for finding
    global structures, but not local ones, and works well with linear data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we are going to run PCA on the Titanic features. The PCA class
    is a *transformer* in scikit-learn; you call the `.fit` method to teach it how
    to get the principal components, then you call `.transform` to convert a matrix
    into a matrix of principal components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Instance parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_components=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of components to generate. If `None`, return same number as number of
    columns. Can be a float (0, 1), then will create as many components as needed
    to get that ratio of variance.
  prefs: []
  type: TYPE_NORMAL
- en: '`copy=True`'
  prefs: []
  type: TYPE_NORMAL
- en: Will mutate data on `.fit` if `True`.
  prefs: []
  type: TYPE_NORMAL
- en: '`whiten=False`'
  prefs: []
  type: TYPE_NORMAL
- en: Whiten data after transform to ensure uncorrelated components.
  prefs: []
  type: TYPE_NORMAL
- en: '`svd_solver=''auto''`'
  prefs: []
  type: TYPE_NORMAL
- en: '`''auto''` runs `''randomized''` SVD if `n_components` is less than 80% of
    the smallest dimension (faster, but an approximation). Otherwise runs `''full''`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`tol=0.0`'
  prefs: []
  type: TYPE_NORMAL
- en: Tolerance for singular values.
  prefs: []
  type: TYPE_NORMAL
- en: '`iterated_power=''auto''`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of iterations for `'randomized'` `svd_solver`.
  prefs: []
  type: TYPE_NORMAL
- en: '`random_state=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Random state for `'randomized'` `svd_solver`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`components_`'
  prefs: []
  type: TYPE_NORMAL
- en: Principal components (columns of linear combination weights for original features).
  prefs: []
  type: TYPE_NORMAL
- en: '`explained_variance_`'
  prefs: []
  type: TYPE_NORMAL
- en: Amount of variance for each component.
  prefs: []
  type: TYPE_NORMAL
- en: '`explained_variance_ratio_`'
  prefs: []
  type: TYPE_NORMAL
- en: Amount of variance for each component normalized (sums to 1).
  prefs: []
  type: TYPE_NORMAL
- en: '`singular_values_`'
  prefs: []
  type: TYPE_NORMAL
- en: Singular values for each component.
  prefs: []
  type: TYPE_NORMAL
- en: '`mean_`'
  prefs: []
  type: TYPE_NORMAL
- en: Mean of each feature.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_components_`'
  prefs: []
  type: TYPE_NORMAL
- en: When `n_components` is a float, this is the size of the components.
  prefs: []
  type: TYPE_NORMAL
- en: '`noise_variance_`'
  prefs: []
  type: TYPE_NORMAL
- en: Estimated noise covariance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plotting the cumulative sum of the explained variance ratio is called a *scree
    plot* (see [Figure 17-1](#idpca-comp1)). It will show how much information is
    stored in the components. You can use the *elbow method* to see if it bends to
    determine how many components to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![PCA scree plot.](assets/mlpr_1701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-1\. PCA scree plot.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Another way to view this data is using a cumulative plot (see [Figure 17-2](#id49)).
    Our original data had 8 columns, but from the plot it appears that we keep around
    90% of the variance if we use just 4 of the PCA components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![PCA cumulative explained variance.](assets/mlpr_1702.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-2\. PCA cumulative explained variance.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: How much do features impact components? Use the matplotlib `imshow` function
    to plot the components along the x axis and the original features along the y
    axis (see [Figure 17-3](#id50)). The darker the color, the more the original column
    contributes to the component.
  prefs: []
  type: TYPE_NORMAL
- en: It looks like the first component is heavily influenced by the pclass, age,
    and fare columns. (Using the spectral colormap (`cmap`) emphasizes nonzero values,
    and providing `vmin` and `vmax` adds limits to the colorbar legend.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![PCA features in components.](assets/mlpr_1703.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-3\. PCA features in components.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'An alternative view is to look at a bar plot (see [Figure 17-4](#id51)). Each
    component is shown with the contributions from the original data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![PCA features in components.](assets/mlpr_1704.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-4\. PCA features in components.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If we have many features, we may want to limit the plots above by showing only
    features that meet a minimum weight. Here is code to find all the features in
    the first two components that have absolute values of at least .5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: PCA is commonly used to visualize high dimension datasets in two components.
    Here we visualize the Titanic features in 2D. They are colored by survival status.
    Sometimes clusters may appear in the visualization. In this case, there doesn’t
    appear to be clustering of survivors (see [Figure 17-5](#id52)).
  prefs: []
  type: TYPE_NORMAL
- en: 'We generate this visualization using Yellowbrick:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![Yellowbrick PCA plot.](assets/mlpr_1705.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-5\. Yellowbrick PCA plot.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you want to color the scatter plot by a column and add a legend (not a colorbar),
    you need to loop over each color and plot that group individually in pandas or
    matplotlib (or use seaborn). Below we also set the aspect ratio to the ratio of
    the explained variances for the components we are looking at (see [Figure 17-6](#id53)).
    Because the second component only has 90% of the first component, it is a little
    shorter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the seaborn version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![Seaborn PCA with legend and relative aspect.](assets/mlpr_1706.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-6\. Seaborn PCA with legend and relative aspect.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Below, we augment the scatter plot by showing a *loading plot* on top of it.
    This plot is called a biplot because it has the scatter plot and the loadings
    (see [Figure 17-7](#idpca52)). The loadings indicate how strong features are and
    how they correlate. If their angles are close, they likely correlate. If the angles
    are at 90 degrees, they likely don’t correlate. Finally, if the angle between
    them is close to 180 degrees, they have a negative correlation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![Seaborn biplot with scatter plot and loading plot.](assets/mlpr_1707.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-7\. Seaborn biplot with scatter plot and loading plot.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From previous tree models, we know that age, fare, and sex are important for
    determining whether a passenger survived. The first principal component is influenced
    by pclass, age, and fare, while the fourth is influenced by sex. Let’s plot those
    components against each other.
  prefs: []
  type: TYPE_NORMAL
- en: Again, this plot is scaling the aspect ratio of the plot based on the ratios
    of variance of the components (see [Figure 17-8](#idpca6)).
  prefs: []
  type: TYPE_NORMAL
- en: 'This plot appears to more accurately separate the survivors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![PCA plot showing components 1 against 4.](assets/mlpr_1708.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-8\. PCA plot showing components 1 against 4.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Matplotlib can create pretty plots, but it is less useful for interactive plots.
    When performing PCA, it is often useful to view the data for scatter plots. I
    have included a function that uses the [Bokeh library](https://bokeh.pydata.org)
    for interacting with scatter plots (see [Figure 17-9](#bok1)). It works well in
    Jupyter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![Bokeh scatter plot with tooltips.](assets/mlpr_1709.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-9\. Bokeh scatter plot with tooltips.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Yellowbrick can also plot in three dimensions (see [Figure 17-10](#id532)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![Yellowbrick 3D PCA.](assets/mlpr_1710.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-10\. Yellowbrick 3D PCA.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The [scprep library](https://oreil.ly/Jdq1s) (which is a dependency for the
    PHATE library, which we discuss shortly) has a useful plotting function. The `rotate_scatter3d`
    function can generate a plot that will animate in Jupyter (see [Figure 17-11](#idpca_3d)).
    This makes it easier to understand 3D plots.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use this library to visualize any 3D data, not just PHATE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![scprep 3D PCA animation.](assets/mlpr_1711.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-11\. scprep 3D PCA animation.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you change the matplotlib cell magic mode in Jupyter to `notebook`, you can
    get an interactive 3D plot from matplotlib (see [Figure 17-12](#idpca_3d2)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![Matplotlib interactive 3D PCA with notebook mode.](assets/mlpr_1712.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-12\. Matplotlib interactive 3D PCA with notebook mode.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Note that switching the cell magic for matplotlib in Jupyter from:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: can sometimes cause Jupyter to stop responding. Tread with caution.
  prefs: []
  type: TYPE_NORMAL
- en: UMAP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Uniform Manifold Approximation and Projection [(UMAP)](https://oreil.ly/qF8RJ)
    is a dimensionality reduction technique that uses manifold learning. As such it
    tends to keeps similar items together topologically. It tries to preserve both
    the global and the local structure, as opposed to t-SNE (explained in [“t-SNE”](#tsne1)),
    which favors local structure.
  prefs: []
  type: TYPE_NORMAL
- en: The Python implementation doesn’t have multicore support.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization of features is a good idea to get values on the same scale.
  prefs: []
  type: TYPE_NORMAL
- en: 'UMAP is very sensitive to hyperparameters (`n_neighbors`, `min_dist`, `n_components`,
    or `metric`). Here are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Instance parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_neighbors=15`'
  prefs: []
  type: TYPE_NORMAL
- en: Local neighborhood size. Larger means use a global view, smaller means more
    local.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_components=2`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of dimensions for embedding.
  prefs: []
  type: TYPE_NORMAL
- en: '`metric=''euclidean''`'
  prefs: []
  type: TYPE_NORMAL
- en: Metric to use for distance. Can be a function that accepts two 1D arrays and
    returns a float.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_epochs=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of training epochs. Default will be 200 or 500 (depending on size of
    data).
  prefs: []
  type: TYPE_NORMAL
- en: '`learning_rate=1.0`'
  prefs: []
  type: TYPE_NORMAL
- en: Learning rate for embedding optimization.
  prefs: []
  type: TYPE_NORMAL
- en: '`init=''spectral''`'
  prefs: []
  type: TYPE_NORMAL
- en: Initialization type. Spectral embedding is the default. Can be `'random'` or
    a numpy array of locations.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_dist=0.1`'
  prefs: []
  type: TYPE_NORMAL
- en: Between 0 and 1\. Minimum distance between embedded points. Smaller means more
    clumps, larger means spread out.
  prefs: []
  type: TYPE_NORMAL
- en: '`spread=1.0`'
  prefs: []
  type: TYPE_NORMAL
- en: Determines distance of embedded points.
  prefs: []
  type: TYPE_NORMAL
- en: '`set_op_mix_ratio=1.0`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Between 0 and 1: fuzzy union (1) or fuzzy intersection (0).'
  prefs: []
  type: TYPE_NORMAL
- en: '`local_connectivity=1.0`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of neighbors for local connectivity. As this goes up, more local connections
    are created.
  prefs: []
  type: TYPE_NORMAL
- en: '`repulsion_strength=1.0`'
  prefs: []
  type: TYPE_NORMAL
- en: Repulsion strength. Higher values give more weight to negative samples.
  prefs: []
  type: TYPE_NORMAL
- en: '`negative_sample_rate=5`'
  prefs: []
  type: TYPE_NORMAL
- en: Negative samples per positive sample. Higher value has more repulsion, more
    optimization costs, and better accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '`transform_queue_size=4.0`'
  prefs: []
  type: TYPE_NORMAL
- en: Aggressiveness for nearest neighbors search. Higher value is lower performance
    but better accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '`a=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameter to control embedding. If equal to `None`, UMAP determines these from
    `min_dist` and `spread`.
  prefs: []
  type: TYPE_NORMAL
- en: '`b=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameter to control embedding. If equal to `None`, UMAP determines these from
    `min_dist` and `spread`.
  prefs: []
  type: TYPE_NORMAL
- en: '`random_state=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Random seed.
  prefs: []
  type: TYPE_NORMAL
- en: '`metric_kwds=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Metrics dictionary for additional parameters if function is used for `metric`.
    Also `minkowsi` (and other metrics) can be parameterized with this.
  prefs: []
  type: TYPE_NORMAL
- en: '`angular_rp_forest=False`'
  prefs: []
  type: TYPE_NORMAL
- en: Use angular random projection.
  prefs: []
  type: TYPE_NORMAL
- en: '`target_n_neighbors=-1`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of neighbors for simplicity set.
  prefs: []
  type: TYPE_NORMAL
- en: '`target_metric=''categorical''`'
  prefs: []
  type: TYPE_NORMAL
- en: For using supervised reduction. Can also be `'L1'` or `'L2'`. Also supports
    a function that takes two arrays from `X` as input and returns the distance value
    between them.
  prefs: []
  type: TYPE_NORMAL
- en: '`target_metric_kwds=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Metrics dictionary to use if function is used for `target_metric`.
  prefs: []
  type: TYPE_NORMAL
- en: '`target_weight=0.5`'
  prefs: []
  type: TYPE_NORMAL
- en: Weighting factor. Between 0.0 and 1.0, where 0 means base on data only, and
    1 means base on target only.
  prefs: []
  type: TYPE_NORMAL
- en: '`transform_seed=42`'
  prefs: []
  type: TYPE_NORMAL
- en: Random seed for transform operations.
  prefs: []
  type: TYPE_NORMAL
- en: '`verbose=False`'
  prefs: []
  type: TYPE_NORMAL
- en: Verbosity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`embedding_`'
  prefs: []
  type: TYPE_NORMAL
- en: The embedding results
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s visualize the default results of UMAP on the Titanic dataset (see [Figure 17-13](#id54)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![UMAP results.](assets/mlpr_1713.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-13\. UMAP results.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To adjust the results of UMAP, focus on the `n_neighbors` and `min_dist` hyperparameters
    first. Here are illustrations of changing those values (see Figures [17-14](#idumap5)
    and [17-15](#idumap6)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![UMAP results adjusting +n_neighbors+.](assets/mlpr_1714.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-14\. UMAP results adjusting `n_neighbors`.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![UMAP results adjusting +min_dist+.](assets/mlpr_1715.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-15\. UMAP results adjusting `min_dist`.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Sometimes PCA is performed before UMAP to reduce the dimensions and speed up
    the computations.
  prefs: []
  type: TYPE_NORMAL
- en: t-SNE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The t-Distributed Stochastic Neighboring Embedding (t-SNE) technique is a visualization
    and dimensionality reduction technique. It uses distributions of the input and
    low dimension embedding, and minimizes the joint probabilities between them. Because
    this is computationally intensive, you might not be able to use this technique
    with a large dataset.
  prefs: []
  type: TYPE_NORMAL
- en: One characteristic of t-SNE is that it is quite sensitive to hyperparameters.
    Also, while it preserves local clusters quite well, global information is not
    preserved. As such, the distance between clusters is meaningless. Finally, this
    is not a deterministic algorithm and may not converge.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is a good idea to standardize the data before using this technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Instance parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_components=2`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of dimensions for embedding.
  prefs: []
  type: TYPE_NORMAL
- en: '`perplexity=30.0`'
  prefs: []
  type: TYPE_NORMAL
- en: Suggested values are between 5 and 50\. Smaller numbers tend to make tighter
    clumps.
  prefs: []
  type: TYPE_NORMAL
- en: '`early_exaggeration=12.0`'
  prefs: []
  type: TYPE_NORMAL
- en: Controls cluster tightness and spacing between them. Larger values mean larger
    spacing.
  prefs: []
  type: TYPE_NORMAL
- en: '`learning_rate=200.0`'
  prefs: []
  type: TYPE_NORMAL
- en: Usually between 10 and 1000\. If data looks like a ball, lower it. If data looks
    compressed, raise it.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_iter=1000`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_iter_without_progress=300`'
  prefs: []
  type: TYPE_NORMAL
- en: Abort if no progress after this number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_grad_norm=1e-07`'
  prefs: []
  type: TYPE_NORMAL
- en: Optimization stops if the gradient norm is below this value.
  prefs: []
  type: TYPE_NORMAL
- en: '`metric=''euclidean''`'
  prefs: []
  type: TYPE_NORMAL
- en: Distance metric from `scipy.spatial.distance.pdist`, `pairwise.PAIRWISE_DISTANCE_METRIC`,
    or a function.
  prefs: []
  type: TYPE_NORMAL
- en: '`init=''random''`'
  prefs: []
  type: TYPE_NORMAL
- en: Embedding initialization.
  prefs: []
  type: TYPE_NORMAL
- en: '`verbose=0`'
  prefs: []
  type: TYPE_NORMAL
- en: Verbosity.
  prefs: []
  type: TYPE_NORMAL
- en: '`random_state=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Random seed.
  prefs: []
  type: TYPE_NORMAL
- en: '`method=''barnes_hut''`'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient calculation algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '`angle=0.5`'
  prefs: []
  type: TYPE_NORMAL
- en: For gradient calculation. Less than .2 increases runtime. Greater than .8 increases
    error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`embedding_`'
  prefs: []
  type: TYPE_NORMAL
- en: Embedding vectors
  prefs: []
  type: TYPE_NORMAL
- en: '`kl_divergence_`'
  prefs: []
  type: TYPE_NORMAL
- en: Kullback-Leibler divergence
  prefs: []
  type: TYPE_NORMAL
- en: '`n_iter_`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of iterations
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a visualization of the results of t-SNE using matplotlib (see [Figure 17-16](#idtsne1)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![t-SNE result with matplotlib.](assets/mlpr_1716.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-16\. t-SNE result with matplotlib.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Changing the value of `perplexity` can have big effects on the plot (see [Figure 17-17](#idtsne2)).
    Here are a few different values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![Changing +perplexity+ for t-SNE.](assets/mlpr_1717.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-17\. Changing `perplexity` for t-SNE.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: PHATE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Potential of Heat-diffusion for Affinity-based Trajectory Embedding ([PHATE](https://phate.readthedocs.io))
    is a tool for visualization of high dimensional data. It tends to keep both global
    structure (like PCA) and local structure (like t-SNE).
  prefs: []
  type: TYPE_NORMAL
- en: 'PHATE first encodes local information (points close to each other should remain
    close). It uses “diffusion” to discover global data, then reduce dimensionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Instance parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_components=2`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: '`knn=5`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of neighbors for the kernel. Increase if the embedding is disconnected
    or dataset is larger than 100,000 samples.
  prefs: []
  type: TYPE_NORMAL
- en: '`decay=40`'
  prefs: []
  type: TYPE_NORMAL
- en: Decay rate of kernel. Lowering this value increases graph connectivity.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_landmark=2000`'
  prefs: []
  type: TYPE_NORMAL
- en: Landmarks to use.
  prefs: []
  type: TYPE_NORMAL
- en: '`t=''auto''`'
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion power. Smoothing is performed on the data. Increase if embedding lacks
    structure. Decrease if structure is tight and compact.
  prefs: []
  type: TYPE_NORMAL
- en: '`gamma=1`'
  prefs: []
  type: TYPE_NORMAL
- en: Log potential (between -1 and 1). If embeddings are concentrated around a single
    point, try setting this to 0.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_pca=100`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of principle components for neighborhood calculation.
  prefs: []
  type: TYPE_NORMAL
- en: '`knn_dist=''euclidean''`'
  prefs: []
  type: TYPE_NORMAL
- en: KNN metric.
  prefs: []
  type: TYPE_NORMAL
- en: '`mds_dist=''euclidean''`'
  prefs: []
  type: TYPE_NORMAL
- en: Multidimensional scaling (MDS) metric.
  prefs: []
  type: TYPE_NORMAL
- en: '`mds=''metric''`'
  prefs: []
  type: TYPE_NORMAL
- en: MDS algorithm for dimension reduction.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_jobs=1`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of CPUs to use.
  prefs: []
  type: TYPE_NORMAL
- en: '`random_state=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Random seed.
  prefs: []
  type: TYPE_NORMAL
- en: '`verbose=1`'
  prefs: []
  type: TYPE_NORMAL
- en: Verbosity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attributes (note that these aren’t followed by `_`):'
  prefs: []
  type: TYPE_NORMAL
- en: '`X`'
  prefs: []
  type: TYPE_NORMAL
- en: Input data
  prefs: []
  type: TYPE_NORMAL
- en: '`embedding`'
  prefs: []
  type: TYPE_NORMAL
- en: Embedding space
  prefs: []
  type: TYPE_NORMAL
- en: '`diff_op`'
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion operator
  prefs: []
  type: TYPE_NORMAL
- en: '`graph`'
  prefs: []
  type: TYPE_NORMAL
- en: KNN graph built from input
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of using PHATE (see [Figure 17-18](#idphate1)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![PHATE results.](assets/mlpr_1718.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-18\. PHATE results.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As noted in the instance parameters above, there are a few parameters that
    we can adjust to change the behavior of the model. Below is an example of adjusting
    the `knn` parameter (see [Figure 17-19](#idphate-nn)). Note that if we use the
    `.set_params` method, it will speed up the calculation as it uses the precomputed
    graph and diffusion operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![Changing the +knn+ parameter for PHATE.](assets/mlpr_1719.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-19\. Changing the `knn` parameter for PHATE.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
