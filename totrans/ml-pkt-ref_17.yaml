- en: Chapter 17\. Dimensionality Reduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many techniques to decompose features into a smaller subset. This
    can be useful for exploratory data analysis, visualization, making predictive
    models, or clustering.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we will explore the Titanic dataset using various techniques.
    We will look at PCA, UMAP, t-SNE, and PHATE.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the data:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: PCA
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Principal Component Analysis (PCA) takes a matrix (X) of rows (samples) and
    columns (features). PCA returns a new matrix that has columns that are linear
    combinations of the original columns. These linear combinations maximize the variance.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Each column is orthogonal (a right angle) to the other columns. The columns
    are sorted in order of decreasing variance.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn has an implementation of this model. It is best to standardize
    the data prior to running the algorithm. After calling the `.fit` method, you
    will have access to an `.explained_variance_ratio_` attribute that lists the percentage
    of variance in each column.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: PCA is useful to visualize data in two (or three) dimensions. It is also used
    as a preprocessing step to filter out random noise in data. It is good for finding
    global structures, but not local ones, and works well with linear data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we are going to run PCA on the Titanic features. The PCA class
    is a *transformer* in scikit-learn; you call the `.fit` method to teach it how
    to get the principal components, then you call `.transform` to convert a matrix
    into a matrix of principal components:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Instance parameters:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '`n_components=None`'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Number of components to generate. If `None`, return same number as number of
    columns. Can be a float (0, 1), then will create as many components as needed
    to get that ratio of variance.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '`copy=True`'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Will mutate data on `.fit` if `True`.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '`whiten=False`'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Whiten data after transform to ensure uncorrelated components.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '`svd_solver=''auto''`'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '`''auto''` runs `''randomized''` SVD if `n_components` is less than 80% of
    the smallest dimension (faster, but an approximation). Otherwise runs `''full''`.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '`tol=0.0`'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Tolerance for singular values.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '`iterated_power=''auto''`'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Number of iterations for `'randomized'` `svd_solver`.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '`random_state=None`'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Random state for `'randomized'` `svd_solver`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'Attributes:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '`components_`'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Principal components (columns of linear combination weights for original features).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '`explained_variance_`'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Amount of variance for each component.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '`explained_variance_ratio_`'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Amount of variance for each component normalized (sums to 1).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '`singular_values_`'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Singular values for each component.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '`mean_`'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Mean of each feature.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '`n_components_`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: When `n_components` is a float, this is the size of the components.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '`noise_variance_`'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Estimated noise covariance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'Plotting the cumulative sum of the explained variance ratio is called a *scree
    plot* (see [Figure 17-1](#idpca-comp1)). It will show how much information is
    stored in the components. You can use the *elbow method* to see if it bends to
    determine how many components to use:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![PCA scree plot.](assets/mlpr_1701.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![PCA 屏幕图。](assets/mlpr_1701.png)'
- en: Figure 17-1\. PCA scree plot.
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 17-1\. PCA 屏幕图。
- en: 'Another way to view this data is using a cumulative plot (see [Figure 17-2](#id49)).
    Our original data had 8 columns, but from the plot it appears that we keep around
    90% of the variance if we use just 4 of the PCA components:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 查看数据的另一种方法是使用累计图（见[图 17-2](#id49)）。我们的原始数据有 8 列，但从图中看来，如果仅使用 4 个 PCA 成分，我们可以保留大约
    90% 的方差：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![PCA cumulative explained variance.](assets/mlpr_1702.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![PCA 累计解释方差。](assets/mlpr_1702.png)'
- en: Figure 17-2\. PCA cumulative explained variance.
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 17-2\. PCA 累计解释方差。
- en: How much do features impact components? Use the matplotlib `imshow` function
    to plot the components along the x axis and the original features along the y
    axis (see [Figure 17-3](#id50)). The darker the color, the more the original column
    contributes to the component.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 特征对成分的影响有多大？使用 matplotlib 的 `imshow` 函数将成分沿 x 轴和原始特征沿 y 轴绘制出来（见[图 17-3](#id50)）。颜色越深，原始列对成分的贡献越大。
- en: It looks like the first component is heavily influenced by the pclass, age,
    and fare columns. (Using the spectral colormap (`cmap`) emphasizes nonzero values,
    and providing `vmin` and `vmax` adds limits to the colorbar legend.)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来第一个成分受到 pclass、age 和 fare 列的影响很大。（使用光谱色图（`cmap`）强调非零值，并提供 `vmin` 和 `vmax`
    为色条图例添加限制。）
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![PCA features in components.](assets/mlpr_1703.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![PCA 特征在成分中。](assets/mlpr_1703.png)'
- en: Figure 17-3\. PCA features in components.
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 17-3\. 主成分分析中的 PCA 特征。
- en: 'An alternative view is to look at a bar plot (see [Figure 17-4](#id51)). Each
    component is shown with the contributions from the original data:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种查看数据的方式是使用条形图（见[图 17-4](#id51)）。每个成分显示了来自原始数据的贡献：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![PCA features in components.](assets/mlpr_1704.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![PCA 特征在成分中。](assets/mlpr_1704.png)'
- en: Figure 17-4\. PCA features in components.
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 17-4\. 主成分分析中的 PCA 特征。
- en: 'If we have many features, we may want to limit the plots above by showing only
    features that meet a minimum weight. Here is code to find all the features in
    the first two components that have absolute values of at least .5:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有很多特征，可能希望通过仅显示满足最小权重要求的特征来限制上述图形。以下是找出前两个成分中具有至少 0.5 绝对值的所有特征的代码：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: PCA is commonly used to visualize high dimension datasets in two components.
    Here we visualize the Titanic features in 2D. They are colored by survival status.
    Sometimes clusters may appear in the visualization. In this case, there doesn’t
    appear to be clustering of survivors (see [Figure 17-5](#id52)).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 常用于以两个成分可视化高维数据集。在这里，我们在 2D 中可视化了 Titanic 的特征。它们根据生存状态进行了着色。有时可视化中可能会出现聚类。在这种情况下，似乎没有幸存者的聚类现象（见[图
    17-5](#id52)）。
- en: 'We generate this visualization using Yellowbrick:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Yellowbrick 生成此可视化：
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Yellowbrick PCA plot.](assets/mlpr_1705.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![Yellowbrick PCA 绘图。](assets/mlpr_1705.png)'
- en: Figure 17-5\. Yellowbrick PCA plot.
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 17-5\. Yellowbrick PCA 绘图。
- en: If you want to color the scatter plot by a column and add a legend (not a colorbar),
    you need to loop over each color and plot that group individually in pandas or
    matplotlib (or use seaborn). Below we also set the aspect ratio to the ratio of
    the explained variances for the components we are looking at (see [Figure 17-6](#id53)).
    Because the second component only has 90% of the first component, it is a little
    shorter.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要根据一列着色散点图并添加图例（而不是色条图），则需要在 pandas 或 matplotlib 中循环每个颜色并单独绘制该组（或使用 seaborn）。下面我们还将纵横比设置为我们查看的成分解释方差的比率（见[图
    17-6](#id53)）。因为第二个成分仅具有第一个成分的 90%，所以它会稍微短一些。
- en: 'Here is the seaborn version:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 seaborn 版本：
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Seaborn PCA with legend and relative aspect.](assets/mlpr_1706.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![带有图例和相对纵横比的 Seaborn PCA。](assets/mlpr_1706.png)'
- en: Figure 17-6\. Seaborn PCA with legend and relative aspect.
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 17-6\. 带有图例和相对纵横比的 Seaborn PCA。
- en: 'Below, we augment the scatter plot by showing a *loading plot* on top of it.
    This plot is called a biplot because it has the scatter plot and the loadings
    (see [Figure 17-7](#idpca52)). The loadings indicate how strong features are and
    how they correlate. If their angles are close, they likely correlate. If the angles
    are at 90 degrees, they likely don’t correlate. Finally, if the angle between
    them is close to 180 degrees, they have a negative correlation:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 下面，我们通过在散点图上显示一个*载荷图*来增强可视化效果。这种图被称为双标图，因为它包含散点图和载荷（见[图 17-7](#idpca52)）。载荷显示特征的强度和它们的相关性。如果它们的角度接近，则它们可能相关。如果角度为
    90 度，则它们可能不相关。最后，如果它们之间的角度接近 180 度，则它们具有负相关性：
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![Seaborn biplot with scatter plot and loading plot.](assets/mlpr_1707.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![带有散点图和加载图的 Seaborn 双图绘图。](assets/mlpr_1707.png)'
- en: Figure 17-7\. Seaborn biplot with scatter plot and loading plot.
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-7\. Seaborn 双图绘图和加载图。
- en: From previous tree models, we know that age, fare, and sex are important for
    determining whether a passenger survived. The first principal component is influenced
    by pclass, age, and fare, while the fourth is influenced by sex. Let’s plot those
    components against each other.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 根据先前的树模型，我们知道年龄、票价和性别对乘客是否生存很重要。第一个主成分受 pclass、年龄和票价的影响，而第四个主成分受性别的影响。让我们将这些组件相互绘制。
- en: Again, this plot is scaling the aspect ratio of the plot based on the ratios
    of variance of the components (see [Figure 17-8](#idpca6)).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，这个图是根据组件方差比例调整了绘图的纵横比（见图17-8）。
- en: 'This plot appears to more accurately separate the survivors:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此图似乎更准确地区分了幸存者：
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![PCA plot showing components 1 against 4.](assets/mlpr_1708.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![显示组件1与4之间的 PCA 图。](assets/mlpr_1708.png)'
- en: Figure 17-8\. PCA plot showing components 1 against 4.
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-8\. PCA 组件1与4的图示。
- en: 'Matplotlib can create pretty plots, but it is less useful for interactive plots.
    When performing PCA, it is often useful to view the data for scatter plots. I
    have included a function that uses the [Bokeh library](https://bokeh.pydata.org)
    for interacting with scatter plots (see [Figure 17-9](#bok1)). It works well in
    Jupyter:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Matplotlib 可以创建漂亮的图表，但在交互图上不太实用。在执行PCA时，查看散点图通常很有用。我已经包含了一个使用 [Bokeh 库](https://bokeh.pydata.org)
    的函数，用于与散点图进行交互（见图17-9）。它在 Jupyter 中运行良好：
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![Bokeh scatter plot with tooltips.](assets/mlpr_1709.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![带有工具提示的 Bokeh 散点图。](assets/mlpr_1709.png)'
- en: Figure 17-9\. Bokeh scatter plot with tooltips.
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-9\. 带有工具提示的 Bokeh 散点图。
- en: 'Yellowbrick can also plot in three dimensions (see [Figure 17-10](#id532)):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Yellowbrick 也可以在三维中绘图（见图17-10）：
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![Yellowbrick 3D PCA.](assets/mlpr_1710.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![Yellowbrick 3D PCA。](assets/mlpr_1710.png)'
- en: Figure 17-10\. Yellowbrick 3D PCA.
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-10\. Yellowbrick 3D PCA。
- en: The [scprep library](https://oreil.ly/Jdq1s) (which is a dependency for the
    PHATE library, which we discuss shortly) has a useful plotting function. The `rotate_scatter3d`
    function can generate a plot that will animate in Jupyter (see [Figure 17-11](#idpca_3d)).
    This makes it easier to understand 3D plots.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[scprep 库](https://oreil.ly/Jdq1s)（这是 PHATE 库的依赖项，我们稍后会讨论）具有一个有用的绘图函数。`rotate_scatter3d`
    函数可以生成一个在 Jupyter 中动画显示的图形（见图17-11）。这使得理解3D图形变得更容易。'
- en: 'You can use this library to visualize any 3D data, not just PHATE:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用此库可视化任何3D数据，而不仅限于 PHATE：
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![scprep 3D PCA animation.](assets/mlpr_1711.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![scprep 3D PCA 动画。](assets/mlpr_1711.png)'
- en: Figure 17-11\. scprep 3D PCA animation.
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-11\. scprep 3D PCA 动画。
- en: If you change the matplotlib cell magic mode in Jupyter to `notebook`, you can
    get an interactive 3D plot from matplotlib (see [Figure 17-12](#idpca_3d2)).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在 Jupyter 中将 matplotlib 的单元格魔术模式更改为 `notebook`，您可以从 matplotlib 获取交互式3D绘图（见图17-12）。
- en: '[PRE14]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![Matplotlib interactive 3D PCA with notebook mode.](assets/mlpr_1712.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![在笔记本模式下与 Matplotlib 交互的3D PCA。](assets/mlpr_1712.png)'
- en: Figure 17-12\. Matplotlib interactive 3D PCA with notebook mode.
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-12\. Matplotlib 在笔记本模式下的交互式3D PCA。
- en: Warning
  id: totrans-98
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Note that switching the cell magic for matplotlib in Jupyter from:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，从：
- en: '[PRE15]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'to:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 转到：
- en: '[PRE16]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: can sometimes cause Jupyter to stop responding. Tread with caution.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 有时可能会导致 Jupyter 停止响应。请谨慎使用。
- en: UMAP
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: UMAP
- en: Uniform Manifold Approximation and Projection [(UMAP)](https://oreil.ly/qF8RJ)
    is a dimensionality reduction technique that uses manifold learning. As such it
    tends to keeps similar items together topologically. It tries to preserve both
    the global and the local structure, as opposed to t-SNE (explained in [“t-SNE”](#tsne1)),
    which favors local structure.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 统一流形逼近和投影 [(UMAP)](https://oreil.ly/qF8RJ) 是一种使用流形学习的降维技术。因此，它倾向于将相似的项目在拓扑上保持在一起。它试图同时保留全局和局部结构，与偏好局部结构的
    t-SNE 相反（详见[“t-SNE”](#tsne1)）。
- en: The Python implementation doesn’t have multicore support.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Python 实现不支持多核。
- en: Normalization of features is a good idea to get values on the same scale.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 特征归一化是将值放在相同尺度上的一个好主意。
- en: 'UMAP is very sensitive to hyperparameters (`n_neighbors`, `min_dist`, `n_components`,
    or `metric`). Here are some examples:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: UMAP 对超参数（`n_neighbors`、`min_dist`、`n_components` 或 `metric`）非常敏感。以下是一些示例：
- en: '[PRE17]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Instance parameters:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 实例参数：
- en: '`n_neighbors=15`'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_neighbors=15`'
- en: Local neighborhood size. Larger means use a global view, smaller means more
    local.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 本地邻域大小。较大意味着使用全局视图，较小意味着更局部。
- en: '`n_components=2`'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_components=2`'
- en: Number of dimensions for embedding.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '`metric=''euclidean''`'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Metric to use for distance. Can be a function that accepts two 1D arrays and
    returns a float.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '`n_epochs=None`'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Number of training epochs. Default will be 200 or 500 (depending on size of
    data).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '`learning_rate=1.0`'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Learning rate for embedding optimization.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '`init=''spectral''`'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Initialization type. Spectral embedding is the default. Can be `'random'` or
    a numpy array of locations.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '`min_dist=0.1`'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Between 0 and 1\. Minimum distance between embedded points. Smaller means more
    clumps, larger means spread out.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '`spread=1.0`'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Determines distance of embedded points.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '`set_op_mix_ratio=1.0`'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'Between 0 and 1: fuzzy union (1) or fuzzy intersection (0).'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '`local_connectivity=1.0`'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Number of neighbors for local connectivity. As this goes up, more local connections
    are created.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '`repulsion_strength=1.0`'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Repulsion strength. Higher values give more weight to negative samples.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '`negative_sample_rate=5`'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Negative samples per positive sample. Higher value has more repulsion, more
    optimization costs, and better accuracy.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '`transform_queue_size=4.0`'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Aggressiveness for nearest neighbors search. Higher value is lower performance
    but better accuracy.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '`a=None`'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Parameter to control embedding. If equal to `None`, UMAP determines these from
    `min_dist` and `spread`.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '`b=None`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Parameter to control embedding. If equal to `None`, UMAP determines these from
    `min_dist` and `spread`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '`random_state=None`'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Random seed.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '`metric_kwds=None`'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Metrics dictionary for additional parameters if function is used for `metric`.
    Also `minkowsi` (and other metrics) can be parameterized with this.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '`angular_rp_forest=False`'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Use angular random projection.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '`target_n_neighbors=-1`'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Number of neighbors for simplicity set.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '`target_metric=''categorical''`'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: For using supervised reduction. Can also be `'L1'` or `'L2'`. Also supports
    a function that takes two arrays from `X` as input and returns the distance value
    between them.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '`target_metric_kwds=None`'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Metrics dictionary to use if function is used for `target_metric`.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '`target_weight=0.5`'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Weighting factor. Between 0.0 and 1.0, where 0 means base on data only, and
    1 means base on target only.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '`transform_seed=42`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Random seed for transform operations.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '`verbose=False`'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Verbosity.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'Attributes:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '`embedding_`'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: The embedding results
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s visualize the default results of UMAP on the Titanic dataset (see [Figure 17-13](#id54)):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![UMAP results.](assets/mlpr_1713.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: Figure 17-13\. UMAP results.
  id: totrans-165
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To adjust the results of UMAP, focus on the `n_neighbors` and `min_dist` hyperparameters
    first. Here are illustrations of changing those values (see Figures [17-14](#idumap5)
    and [17-15](#idumap6)):'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![UMAP results adjusting +n_neighbors+.](assets/mlpr_1714.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
- en: Figure 17-14\. UMAP results adjusting `n_neighbors`.
  id: totrans-169
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![UMAP results adjusting +min_dist+.](assets/mlpr_1715.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: Figure 17-15\. UMAP results adjusting `min_dist`.
  id: totrans-172
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Sometimes PCA is performed before UMAP to reduce the dimensions and speed up
    the computations.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: t-SNE
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The t-Distributed Stochastic Neighboring Embedding (t-SNE) technique is a visualization
    and dimensionality reduction technique. It uses distributions of the input and
    low dimension embedding, and minimizes the joint probabilities between them. Because
    this is computationally intensive, you might not be able to use this technique
    with a large dataset.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: t-分布随机邻域嵌入（t-SNE）技术是一种可视化和降维技术。它使用输入的分布和低维嵌入，并最小化它们之间的联合概率。由于计算量大，可能无法在大数据集上使用这种技术。
- en: One characteristic of t-SNE is that it is quite sensitive to hyperparameters.
    Also, while it preserves local clusters quite well, global information is not
    preserved. As such, the distance between clusters is meaningless. Finally, this
    is not a deterministic algorithm and may not converge.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE的一个特征是对超参数非常敏感。此外，虽然它能够很好地保留局部聚类，但全局信息并未保留。最后，这不是一个确定性算法，可能不会收敛。
- en: 'It is a good idea to standardize the data before using this technique:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用此技术之前标准化数据是一个好主意：
- en: '[PRE21]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Instance parameters:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 实例参数：
- en: '`n_components=2`'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_components=2`'
- en: Number of dimensions for embedding.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入的维度数。
- en: '`perplexity=30.0`'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`perplexity=30.0`'
- en: Suggested values are between 5 and 50\. Smaller numbers tend to make tighter
    clumps.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 建议的取值范围为5到50。较小的数值倾向于形成更紧密的聚类。
- en: '`early_exaggeration=12.0`'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`early_exaggeration=12.0`'
- en: Controls cluster tightness and spacing between them. Larger values mean larger
    spacing.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 控制簇紧密度和它们之间的间距。较大的值意味着较大的间距。
- en: '`learning_rate=200.0`'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`learning_rate=200.0`'
- en: Usually between 10 and 1000\. If data looks like a ball, lower it. If data looks
    compressed, raise it.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 通常在10到1000之间。如果数据看起来像一个球，则降低它。如果数据看起来压缩，则增加它。
- en: '`n_iter=1000`'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_iter=1000`'
- en: Number of iterations.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代次数。
- en: '`n_iter_without_progress=300`'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_iter_without_progress=300`'
- en: Abort if no progress after this number of iterations.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在这些迭代次数之后没有进展，则中止。
- en: '`min_grad_norm=1e-07`'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`min_grad_norm=1e-07`'
- en: Optimization stops if the gradient norm is below this value.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如果梯度范数低于此值，则优化停止。
- en: '`metric=''euclidean''`'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`metric=''euclidean''`'
- en: Distance metric from `scipy.spatial.distance.pdist`, `pairwise.PAIRWISE_DISTANCE_METRIC`,
    or a function.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 来自`scipy.spatial.distance.pdist`、`pairwise.PAIRWISE_DISTANCE_METRIC`或函数的距离度量。
- en: '`init=''random''`'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`init=''random''`'
- en: Embedding initialization.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入初始化。
- en: '`verbose=0`'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`verbose=0`'
- en: Verbosity.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 冗长性。
- en: '`random_state=None`'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`random_state=None`'
- en: Random seed.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 随机种子。
- en: '`method=''barnes_hut''`'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`method=''barnes_hut''`'
- en: Gradient calculation algorithm.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度计算算法。
- en: '`angle=0.5`'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`angle=0.5`'
- en: For gradient calculation. Less than .2 increases runtime. Greater than .8 increases
    error.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 用于梯度计算。小于0.2会增加运行时间。大于0.8会增加错误。
- en: 'Attributes:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 属性：
- en: '`embedding_`'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`embedding_`'
- en: Embedding vectors
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入向量。
- en: '`kl_divergence_`'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '`kl_divergence_`'
- en: Kullback-Leibler divergence
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Kullback-Leibler散度。
- en: '`n_iter_`'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_iter_`'
- en: Number of iterations
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代次数。
- en: 'Here’s a visualization of the results of t-SNE using matplotlib (see [Figure 17-16](#idtsne1)):'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了使用matplotlib进行t-SNE结果的可视化（见[图17-16](#idtsne1)）：
- en: '[PRE22]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![t-SNE result with matplotlib.](assets/mlpr_1716.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![使用matplotlib进行t-SNE结果。](assets/mlpr_1716.png)'
- en: Figure 17-16\. t-SNE result with matplotlib.
  id: totrans-216
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-16。使用matplotlib进行t-SNE结果。
- en: 'Changing the value of `perplexity` can have big effects on the plot (see [Figure 17-17](#idtsne2)).
    Here are a few different values:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 改变`perplexity`的值可能会对绘图产生重大影响（见[图17-17](#idtsne2)）。
- en: '[PRE23]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![Changing +perplexity+ for t-SNE.](assets/mlpr_1717.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![改变`perplexity`用于t-SNE。](assets/mlpr_1717.png)'
- en: Figure 17-17\. Changing `perplexity` for t-SNE.
  id: totrans-220
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-17。改变`t-SNE`的`perplexity`。
- en: PHATE
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PHATE
- en: Potential of Heat-diffusion for Affinity-based Trajectory Embedding ([PHATE](https://phate.readthedocs.io))
    is a tool for visualization of high dimensional data. It tends to keep both global
    structure (like PCA) and local structure (like t-SNE).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 通过Affinity-based Trajectory Embedding（PHATE）进行热扩散的潜力是高维数据可视化的工具。它倾向于同时保留全局结构（如PCA）和局部结构（如t-SNE）。
- en: 'PHATE first encodes local information (points close to each other should remain
    close). It uses “diffusion” to discover global data, then reduce dimensionality:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: PHATE首先编码局部信息（接近的点应该保持接近）。它使用“扩散”来发现全局数据，然后减少维度：
- en: '[PRE24]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Instance parameters:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 实例参数：
- en: '`n_components=2`'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_components=2`'
- en: Number of dimensions.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 维度数。
- en: '`knn=5`'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '`knn=5`'
- en: Number of neighbors for the kernel. Increase if the embedding is disconnected
    or dataset is larger than 100,000 samples.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 核的邻居数。如果嵌入是断开的或数据集大于10万个样本，则增加。
- en: '`decay=40`'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`decay=40`'
- en: Decay rate of kernel. Lowering this value increases graph connectivity.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 核的衰减率。降低此值会增加图的连通性。
- en: '`n_landmark=2000`'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_landmark=2000`'
- en: Landmarks to use.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 用于标记的地标点。
- en: '`t=''auto''`'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '`t=''auto''`'
- en: Diffusion power. Smoothing is performed on the data. Increase if embedding lacks
    structure. Decrease if structure is tight and compact.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散力度。对数据进行平滑处理。如果嵌入缺乏结构，请增加；如果结构紧密而紧凑，请减少。
- en: '`gamma=1`'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '`gamma=1`'
- en: Log potential (between -1 and 1). If embeddings are concentrated around a single
    point, try setting this to 0.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 对数潜力（在 -1 到 1 之间）。如果嵌入集中在单个点周围，请尝试将其设置为 0。
- en: '`n_pca=100`'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_pca=100`'
- en: Number of principle components for neighborhood calculation.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 用于邻域计算的主成分数。
- en: '`knn_dist=''euclidean''`'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '`knn_dist=''euclidean''`'
- en: KNN metric.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 距离度量。
- en: '`mds_dist=''euclidean''`'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '`mds_dist=''euclidean''`'
- en: Multidimensional scaling (MDS) metric.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 多维缩放（MDS）度量。
- en: '`mds=''metric''`'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '`mds=''metric''`'
- en: MDS algorithm for dimension reduction.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: MDS 算法用于降维。
- en: '`n_jobs=1`'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_jobs=1`'
- en: Number of CPUs to use.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用的 CPU 数量。
- en: '`random_state=None`'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '`random_state=None`'
- en: Random seed.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 随机种子。
- en: '`verbose=1`'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '`verbose=1`'
- en: Verbosity.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 冗余性。
- en: 'Attributes (note that these aren’t followed by `_`):'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 属性（请注意这些后面没有 `_`）：
- en: '`X`'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '`X`'
- en: Input data
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据
- en: '`embedding`'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '`embedding`'
- en: Embedding space
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入空间
- en: '`diff_op`'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '`diff_op`'
- en: Diffusion operator
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散算子
- en: '`graph`'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '`graph`'
- en: KNN graph built from input
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 基于输入构建的 KNN 图
- en: 'Here is an example of using PHATE (see [Figure 17-18](#idphate1)):'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用 PHATE 的一个示例（见 [Figure 17-18](#idphate1)）：
- en: '[PRE25]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![PHATE results.](assets/mlpr_1718.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![PHATE 结果。](assets/mlpr_1718.png)'
- en: Figure 17-18\. PHATE results.
  id: totrans-264
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 17-18\. PHATE 结果。
- en: 'As noted in the instance parameters above, there are a few parameters that
    we can adjust to change the behavior of the model. Below is an example of adjusting
    the `knn` parameter (see [Figure 17-19](#idphate-nn)). Note that if we use the
    `.set_params` method, it will speed up the calculation as it uses the precomputed
    graph and diffusion operator:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述的实例参数中，有一些参数可以调整以改变模型的行为。以下是调整 `knn` 参数的示例（见 [Figure 17-19](#idphate-nn)）。请注意，如果使用
    `.set_params` 方法，它将加快计算速度，因为它使用预计算的图和扩散算子：
- en: '[PRE26]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![Changing the +knn+ parameter for PHATE.](assets/mlpr_1719.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![更改 PHATE 的 `knn` 参数。](assets/mlpr_1719.png)'
- en: Figure 17-19\. Changing the `knn` parameter for PHATE.
  id: totrans-268
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 17-19\. 更改 PHATE 的 `knn` 参数。
