<html><head></head><body><section data-pdf-bookmark="Chapter 9. Semisupervised Learning" data-type="chapter" epub:type="chapter"><div class="chapter" id="Chapter_9">&#13;
<h1><span class="label">Chapter 9. </span>Semisupervised Learning</h1>&#13;
&#13;
&#13;
<p>Until now, we have viewed supervised learning and unsupervised learning as two separate and distinct branches of machine learning. Supervised learning is appropriate when our dataset is labeled, and unsupervised learning is necessary when our dataset is unlabeled.</p>&#13;
&#13;
<p>In the real world, the distinction is not quite so clear. Datasets are usually partially labeled, and we want to efficiently label the unlabeled observations while leveraging the information in the labeled set. With supervised learning, we would have to toss away the majority of the dataset because it is unlabeled. With unsupervised learning, we would have the majority of the data to work with but would not know how to take advantage of the few labels we have.</p>&#13;
&#13;
<p>The<a data-primary="semisupervised learning" data-secondary="advantages of" data-type="indexterm" id="idm140637539421984"/> field of <em>semisupervised learning</em> blends the benefits of both supervised and unsupervised learning, taking advantage of the few labels that are available to uncover structure in a dataset and help label the rest.</p>&#13;
&#13;
<p>We will continue to use the credit card transactions dataset in this chapter to showcase semisupervised learning.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Data Preparation" data-type="sect1"><div class="sect1" id="idm140637539419472">&#13;
<h1>Data Preparation</h1>&#13;
&#13;
<p>As<a data-primary="semisupervised learning" data-secondary="data preparation" data-type="indexterm" id="SSLdata09"/> before, let’s load in the necessary libraries and prepare the data. This should be pretty familiar by now:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="sd">'''Main'''</code>&#13;
<code class="kn">import</code> <code class="nn">numpy</code> <code class="kn">as</code> <code class="nn">np</code>&#13;
<code class="kn">import</code> <code class="nn">pandas</code> <code class="kn">as</code> <code class="nn">pd</code>&#13;
<code class="kn">import</code> <code class="nn">os</code><code class="o">,</code> <code class="nn">time</code><code class="o">,</code> <code class="nn">re</code>&#13;
<code class="kn">import</code> <code class="nn">pickle</code><code class="o">,</code> <code class="nn">gzip</code>&#13;
&#13;
<code class="sd">'''Data Viz'''</code>&#13;
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="kn">as</code> <code class="nn">plt</code>&#13;
<code class="kn">import</code> <code class="nn">seaborn</code> <code class="kn">as</code> <code class="nn">sns</code>&#13;
<code class="n">color</code> <code class="o">=</code> <code class="n">sns</code><code class="o">.</code><code class="n">color_palette</code><code class="p">()</code>&#13;
<code class="kn">import</code> <code class="nn">matplotlib</code> <code class="kn">as</code> <code class="nn">mpl</code>&#13;
&#13;
<code class="o">%</code><code class="n">matplotlib</code> <code class="n">inline</code>&#13;
&#13;
<code class="sd">'''Data Prep and Model Evaluation'''</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">preprocessing</code> <code class="k">as</code> <code class="n">pp</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">StratifiedKFold</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">log_loss</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">precision_recall_curve</code><code class="p">,</code> <code class="n">average_precision_score</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">roc_curve</code><code class="p">,</code> <code class="n">auc</code><code class="p">,</code> <code class="n">roc_auc_score</code>&#13;
&#13;
<code class="sd">'''Algos'''</code>&#13;
<code class="kn">import</code> <code class="nn">lightgbm</code> <code class="kn">as</code> <code class="nn">lgb</code>&#13;
&#13;
<code class="sd">'''TensorFlow and Keras'''</code>&#13;
<code class="kn">import</code> <code class="nn">tensorflow</code> <code class="kn">as</code> <code class="nn">tf</code>&#13;
<code class="kn">import</code> <code class="nn">keras</code>&#13;
<code class="kn">from</code> <code class="nn">keras</code> <code class="kn">import</code> <code class="n">backend</code> <code class="k">as</code> <code class="n">K</code>&#13;
<code class="kn">from</code> <code class="nn">keras.models</code> <code class="kn">import</code> <code class="n">Sequential</code><code class="p">,</code> <code class="n">Model</code>&#13;
<code class="kn">from</code> <code class="nn">keras.layers</code> <code class="kn">import</code> <code class="n">Activation</code><code class="p">,</code> <code class="n">Dense</code><code class="p">,</code> <code class="n">Dropout</code>&#13;
<code class="kn">from</code> <code class="nn">keras.layers</code> <code class="kn">import</code> <code class="n">BatchNormalization</code><code class="p">,</code> <code class="n">Input</code><code class="p">,</code> <code class="n">Lambda</code>&#13;
<code class="kn">from</code> <code class="nn">keras</code> <code class="kn">import</code> <code class="n">regularizers</code>&#13;
<code class="kn">from</code> <code class="nn">keras.losses</code> <code class="kn">import</code> <code class="n">mse</code><code class="p">,</code> <code class="n">binary_crossentropy</code></pre>&#13;
&#13;
<p>As<a data-primary="partially labeled datasets" data-type="indexterm" id="idm140637539413712"/> before, we will generate a training and test set. But we will drop 90% of the fraudulent credit card transactions from the training set to simulate how to work with <span class="keep-together"><em>partially</em></span> <em>labeled</em> datasets.</p>&#13;
&#13;
<p>While this may seem like a very aggressive move, real-world problems involving payment fraud have similarly low incidences of fraud (as little as 1 fraud per 10,000 cases). By removing 90% of the labels from the training set, we are simulating this type of phenomenon:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load the data</code>&#13;
<code class="n">current_path</code> <code class="o">=</code> <code class="n">os</code><code class="o">.</code><code class="n">getcwd</code><code class="p">()</code>&#13;
<code class="nb">file</code> <code class="o">=</code> <code class="s1">'</code><code class="se">\\</code><code class="s1">datasets</code><code class="se">\\</code><code class="s1">credit_card_data</code><code class="se">\\</code><code class="s1">credit_card.csv'</code>&#13;
<code class="n">data</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="n">current_path</code> <code class="o">+</code> <code class="nb">file</code><code class="p">)</code>&#13;
&#13;
<code class="n">dataX</code> <code class="o">=</code> <code class="n">data</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code><code class="o">.</code><code class="n">drop</code><code class="p">([</code><code class="s1">'Class'</code><code class="p">,</code><code class="s1">'Time'</code><code class="p">],</code><code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>&#13;
<code class="n">dataY</code> <code class="o">=</code> <code class="n">data</code><code class="p">[</code><code class="s1">'Class'</code><code class="p">]</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code>&#13;
&#13;
<code class="c1"># Scale data</code>&#13;
<code class="n">featuresToScale</code> <code class="o">=</code> <code class="n">dataX</code><code class="o">.</code><code class="n">columns</code>&#13;
<code class="n">sX</code> <code class="o">=</code> <code class="n">pp</code><code class="o">.</code><code class="n">StandardScaler</code><code class="p">(</code><code class="n">copy</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">with_mean</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">with_std</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>&#13;
<code class="n">dataX</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code><code class="n">featuresToScale</code><code class="p">]</code> <code class="o">=</code> <code class="n">sX</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">dataX</code><code class="p">[</code><code class="n">featuresToScale</code><code class="p">])</code>&#13;
&#13;
<code class="c1"># Split into train and test</code>&#13;
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> \&#13;
    <code class="n">train_test_split</code><code class="p">(</code><code class="n">dataX</code><code class="p">,</code> <code class="n">dataY</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="mf">0.33</code><code class="p">,</code> \&#13;
                     <code class="n">random_state</code><code class="o">=</code><code class="mi">2018</code><code class="p">,</code> <code class="n">stratify</code><code class="o">=</code><code class="n">dataY</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Drop 95% of the labels from the training set</code>&#13;
<code class="n">toDrop</code> <code class="o">=</code> <code class="n">y_train</code><code class="p">[</code><code class="n">y_train</code><code class="o">==</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="n">frac</code><code class="o">=</code><code class="mf">0.90</code><code class="p">,</code><code class="n">random_state</code><code class="o">=</code><code class="mi">2018</code><code class="p">)</code>&#13;
<code class="n">X_train</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="n">labels</code><code class="o">=</code><code class="n">toDrop</code><code class="o">.</code><code class="n">index</code><code class="p">,</code><code class="n">inplace</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>&#13;
<code class="n">y_train</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="n">labels</code><code class="o">=</code><code class="n">toDrop</code><code class="o">.</code><code class="n">index</code><code class="p">,</code><code class="n">inplace</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code></pre>&#13;
&#13;
<p>We will also reuse the <code>anomalyScores</code> and <code>plotResults</code> functions:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">anomalyScores</code><code class="p">(</code><code class="n">originalDF</code><code class="p">,</code> <code class="n">reducedDF</code><code class="p">):</code>&#13;
    <code class="n">loss</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">sum</code><code class="p">((</code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">originalDF</code><code class="p">)</code> <code class="o">-</code> \&#13;
                   <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">reducedDF</code><code class="p">))</code><code class="o">**</code><code class="mi">2</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>&#13;
    <code class="n">loss</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">loss</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">originalDF</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>&#13;
    <code class="n">loss</code> <code class="o">=</code> <code class="p">(</code><code class="n">loss</code><code class="o">-</code><code class="n">np</code><code class="o">.</code><code class="n">min</code><code class="p">(</code><code class="n">loss</code><code class="p">))</code><code class="o">/</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="n">loss</code><code class="p">)</code><code class="o">-</code><code class="n">np</code><code class="o">.</code><code class="n">min</code><code class="p">(</code><code class="n">loss</code><code class="p">))</code>&#13;
    <code class="k">return</code> <code class="n">loss</code></pre>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">plotResults</code><code class="p">(</code><code class="n">trueLabels</code><code class="p">,</code> <code class="n">anomalyScores</code><code class="p">,</code> <code class="n">returnPreds</code> <code class="o">=</code> <code class="bp">False</code><code class="p">):</code>&#13;
    <code class="n">preds</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">([</code><code class="n">trueLabels</code><code class="p">,</code> <code class="n">anomalyScores</code><code class="p">],</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>&#13;
    <code class="n">preds</code><code class="o">.</code><code class="n">columns</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'trueLabel'</code><code class="p">,</code> <code class="s1">'anomalyScore'</code><code class="p">]</code>&#13;
    <code class="n">precision</code><code class="p">,</code> <code class="n">recall</code><code class="p">,</code> <code class="n">thresholds</code> <code class="o">=</code> \&#13;
        <code class="n">precision_recall_curve</code><code class="p">(</code><code class="n">preds</code><code class="p">[</code><code class="s1">'trueLabel'</code><code class="p">],</code> \&#13;
                               <code class="n">preds</code><code class="p">[</code><code class="s1">'anomalyScore'</code><code class="p">])</code>&#13;
    <code class="n">average_precision</code> <code class="o">=</code> <code class="n">average_precision_score</code><code class="p">(</code> \&#13;
                        <code class="n">preds</code><code class="p">[</code><code class="s1">'trueLabel'</code><code class="p">],</code> <code class="n">preds</code><code class="p">[</code><code class="s1">'anomalyScore'</code><code class="p">])</code>&#13;
&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">recall</code><code class="p">,</code> <code class="n">precision</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s1">'k'</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.7</code><code class="p">,</code> <code class="n">where</code><code class="o">=</code><code class="s1">'post'</code><code class="p">)</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">fill_between</code><code class="p">(</code><code class="n">recall</code><code class="p">,</code> <code class="n">precision</code><code class="p">,</code> <code class="n">step</code><code class="o">=</code><code class="s1">'post'</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.3</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s1">'k'</code><code class="p">)</code>&#13;
&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'Recall'</code><code class="p">)</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'Precision'</code><code class="p">)</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">ylim</code><code class="p">([</code><code class="mf">0.0</code><code class="p">,</code> <code class="mf">1.05</code><code class="p">])</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">xlim</code><code class="p">([</code><code class="mf">0.0</code><code class="p">,</code> <code class="mf">1.0</code><code class="p">])</code>&#13;
&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s1">'Precision-Recall curve: Average Precision = </code><code class="se">\</code>&#13;
<code class="s1">        {0:0.2f}'</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">average_precision</code><code class="p">))</code>&#13;
&#13;
    <code class="n">fpr</code><code class="p">,</code> <code class="n">tpr</code><code class="p">,</code> <code class="n">thresholds</code> <code class="o">=</code> <code class="n">roc_curve</code><code class="p">(</code><code class="n">preds</code><code class="p">[</code><code class="s1">'trueLabel'</code><code class="p">],</code> \&#13;
                                     <code class="n">preds</code><code class="p">[</code><code class="s1">'anomalyScore'</code><code class="p">])</code>&#13;
    <code class="n">areaUnderROC</code> <code class="o">=</code> <code class="n">auc</code><code class="p">(</code><code class="n">fpr</code><code class="p">,</code> <code class="n">tpr</code><code class="p">)</code>&#13;
&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">()</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">fpr</code><code class="p">,</code> <code class="n">tpr</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s1">'r'</code><code class="p">,</code> <code class="n">lw</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'ROC curve'</code><code class="p">)</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">([</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">color</code><code class="o">=</code><code class="s1">'k'</code><code class="p">,</code> <code class="n">lw</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">linestyle</code><code class="o">=</code><code class="s1">'--'</code><code class="p">)</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">xlim</code><code class="p">([</code><code class="mf">0.0</code><code class="p">,</code> <code class="mf">1.0</code><code class="p">])</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">ylim</code><code class="p">([</code><code class="mf">0.0</code><code class="p">,</code> <code class="mf">1.05</code><code class="p">])</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'False Positive Rate'</code><code class="p">)</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'True Positive Rate'</code><code class="p">)</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s1">'Receiver operating characteristic: Area under the </code><code class="se">\</code>&#13;
<code class="s1">        curve = {0:0.2f}'</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">areaUnderROC</code><code class="p">))</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">(</code><code class="n">loc</code><code class="o">=</code><code class="s2">"lower right"</code><code class="p">)</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>&#13;
&#13;
    <code class="k">if</code> <code class="n">returnPreds</code><code class="o">==</code><code class="bp">True</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="n">preds</code><code class="p">,</code> <code class="n">average_precision</code></pre>&#13;
&#13;
<p>Finally, here’s<a data-primary="precisionAnalysis function" data-type="indexterm" id="idm140637538808096"/> a new function called <code>precisionAnalysis</code> to help us assess the precision of our models at a certain level of recall. Specifically, we will determine what the model’s precision is to catch 75% of the fraudulent credit card transactions in the test set. The higher the precision, the better the model.</p>&#13;
&#13;
<p>This is a reasonable benchmark. In other words, we want to catch 75% of the fraud with as high of a precision as possible. If we do not achieve a high enough precision, we will unnecessarily reject good credit card transactions, potentially angering our customer<a data-primary="" data-startref="SSLdata09" data-type="indexterm" id="idm140637538806112"/> base:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">precisionAnalysis</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="n">column</code><code class="p">,</code> <code class="n">threshold</code><code class="p">):</code>&#13;
    <code class="n">df</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="n">by</code><code class="o">=</code><code class="n">column</code><code class="p">,</code> <code class="n">ascending</code><code class="o">=</code><code class="bp">False</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>&#13;
    <code class="n">threshold_value</code> <code class="o">=</code> <code class="n">threshold</code><code class="o">*</code><code class="n">df</code><code class="o">.</code><code class="n">trueLabel</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code>&#13;
    <code class="n">i</code> <code class="o">=</code> <code class="mi">0</code>&#13;
    <code class="n">j</code> <code class="o">=</code> <code class="mi">0</code>&#13;
    <code class="k">while</code> <code class="n">i</code> <code class="o">&lt;</code> <code class="n">threshold_value</code><code class="o">+</code><code class="mi">1</code><code class="p">:</code>&#13;
        <code class="k">if</code> <code class="n">df</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">j</code><code class="p">][</code><code class="s2">"trueLabel"</code><code class="p">]</code><code class="o">==</code><code class="mi">1</code><code class="p">:</code>&#13;
            <code class="n">i</code> <code class="o">+=</code> <code class="mi">1</code>&#13;
        <code class="n">j</code> <code class="o">+=</code> <code class="mi">1</code>&#13;
    <code class="k">return</code> <code class="n">df</code><code class="p">,</code> <code class="n">i</code><code class="o">/</code><code class="n">j</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Supervised Model" data-type="sect1"><div class="sect1" id="idm140637539418848">&#13;
<h1>Supervised Model</h1>&#13;
&#13;
<p>To<a data-primary="semisupervised learning" data-secondary="supervised model" data-type="indexterm" id="SSLsuper09"/> benchmark our semisupervised model, let’s first see how well a supervised model and a unsupervised model do in isolation.</p>&#13;
&#13;
<p>We will start with a supervised learning solution based on light gradient boosting like the one that performed best in <a data-type="xref" href="ch02.html#Chapter_2">Chapter 2</a>. We<a data-primary="k-fold cross-validation" data-type="indexterm" id="idm140637538408224"/> will use <em>k</em>-fold cross-validation to create five folds:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">k_fold</code> <code class="o">=</code> <code class="n">StratifiedKFold</code><code class="p">(</code><code class="n">n_splits</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code><code class="n">shuffle</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code><code class="n">random_state</code><code class="o">=</code><code class="mi">2018</code><code class="p">)</code></pre>&#13;
&#13;
<p>Let’s next set the parameters for gradient boosting:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">params_lightGB</code> <code class="o">=</code> <code class="p">{</code>&#13;
    <code class="s1">'task'</code><code class="p">:</code> <code class="s1">'train'</code><code class="p">,</code>&#13;
    <code class="s1">'application'</code><code class="p">:</code><code class="s1">'binary'</code><code class="p">,</code>&#13;
    <code class="s1">'num_class'</code><code class="p">:</code><code class="mi">1</code><code class="p">,</code>&#13;
    <code class="s1">'boosting'</code><code class="p">:</code> <code class="s1">'gbdt'</code><code class="p">,</code>&#13;
    <code class="s1">'objective'</code><code class="p">:</code> <code class="s1">'binary'</code><code class="p">,</code>&#13;
    <code class="s1">'metric'</code><code class="p">:</code> <code class="s1">'binary_logloss'</code><code class="p">,</code>&#13;
    <code class="s1">'metric_freq'</code><code class="p">:</code><code class="mi">50</code><code class="p">,</code>&#13;
    <code class="s1">'is_training_metric'</code><code class="p">:</code><code class="bp">False</code><code class="p">,</code>&#13;
    <code class="s1">'max_depth'</code><code class="p">:</code><code class="mi">4</code><code class="p">,</code>&#13;
    <code class="s1">'num_leaves'</code><code class="p">:</code> <code class="mi">31</code><code class="p">,</code>&#13;
    <code class="s1">'learning_rate'</code><code class="p">:</code> <code class="mf">0.01</code><code class="p">,</code>&#13;
    <code class="s1">'feature_fraction'</code><code class="p">:</code> <code class="mf">1.0</code><code class="p">,</code>&#13;
    <code class="s1">'bagging_fraction'</code><code class="p">:</code> <code class="mf">1.0</code><code class="p">,</code>&#13;
    <code class="s1">'bagging_freq'</code><code class="p">:</code> <code class="mi">0</code><code class="p">,</code>&#13;
    <code class="s1">'bagging_seed'</code><code class="p">:</code> <code class="mi">2018</code><code class="p">,</code>&#13;
    <code class="s1">'verbose'</code><code class="p">:</code> <code class="mi">0</code><code class="p">,</code>&#13;
    <code class="s1">'num_threads'</code><code class="p">:</code><code class="mi">16</code>&#13;
<code class="p">}</code></pre>&#13;
&#13;
<p>Now, let’s train the algorithm:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">trainingScores</code> <code class="o">=</code> <code class="p">[]</code>&#13;
<code class="n">cvScores</code> <code class="o">=</code> <code class="p">[]</code>&#13;
<code class="n">predictionsBasedOnKFolds</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="p">[],</code> <code class="n">index</code><code class="o">=</code><code class="n">y_train</code><code class="o">.</code><code class="n">index</code><code class="p">,</code> \&#13;
                                        <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'prediction'</code><code class="p">])</code>&#13;
&#13;
<code class="k">for</code> <code class="n">train_index</code><code class="p">,</code> <code class="n">cv_index</code> <code class="ow">in</code> <code class="n">k_fold</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">)),</code> \&#13;
                                          <code class="n">y_train</code><code class="o">.</code><code class="n">ravel</code><code class="p">()):</code>&#13;
    <code class="n">X_train_fold</code><code class="p">,</code> <code class="n">X_cv_fold</code> <code class="o">=</code> <code class="n">X_train</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">train_index</code><code class="p">,:],</code> \&#13;
        <code class="n">X_train</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">cv_index</code><code class="p">,:]</code>&#13;
    <code class="n">y_train_fold</code><code class="p">,</code> <code class="n">y_cv_fold</code> <code class="o">=</code> <code class="n">y_train</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">train_index</code><code class="p">],</code> \&#13;
        <code class="n">y_train</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">cv_index</code><code class="p">]</code>&#13;
&#13;
    <code class="n">lgb_train</code> <code class="o">=</code> <code class="n">lgb</code><code class="o">.</code><code class="n">Dataset</code><code class="p">(</code><code class="n">X_train_fold</code><code class="p">,</code> <code class="n">y_train_fold</code><code class="p">)</code>&#13;
    <code class="n">lgb_eval</code> <code class="o">=</code> <code class="n">lgb</code><code class="o">.</code><code class="n">Dataset</code><code class="p">(</code><code class="n">X_cv_fold</code><code class="p">,</code> <code class="n">y_cv_fold</code><code class="p">,</code> <code class="n">reference</code><code class="o">=</code><code class="n">lgb_train</code><code class="p">)</code>&#13;
    <code class="n">gbm</code> <code class="o">=</code> <code class="n">lgb</code><code class="o">.</code><code class="n">train</code><code class="p">(</code><code class="n">params_lightGB</code><code class="p">,</code> <code class="n">lgb_train</code><code class="p">,</code> <code class="n">num_boost_round</code><code class="o">=</code><code class="mi">2000</code><code class="p">,</code>&#13;
                   <code class="n">valid_sets</code><code class="o">=</code><code class="n">lgb_eval</code><code class="p">,</code> <code class="n">early_stopping_rounds</code><code class="o">=</code><code class="mi">200</code><code class="p">)</code>&#13;
&#13;
    <code class="n">loglossTraining</code> <code class="o">=</code> <code class="n">log_loss</code><code class="p">(</code><code class="n">y_train_fold</code><code class="p">,</code> <code class="n">gbm</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_train_fold</code><code class="p">,</code> \&#13;
                                <code class="n">num_iteration</code><code class="o">=</code><code class="n">gbm</code><code class="o">.</code><code class="n">best_iteration</code><code class="p">))</code>&#13;
    <code class="n">trainingScores</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">loglossTraining</code><code class="p">)</code>&#13;
&#13;
    <code class="n">predictionsBasedOnKFolds</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">X_cv_fold</code><code class="o">.</code><code class="n">index</code><code class="p">,</code><code class="s1">'prediction'</code><code class="p">]</code> <code class="o">=</code> \&#13;
        <code class="n">gbm</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_cv_fold</code><code class="p">,</code> <code class="n">num_iteration</code><code class="o">=</code><code class="n">gbm</code><code class="o">.</code><code class="n">best_iteration</code><code class="p">)</code>&#13;
    <code class="n">loglossCV</code> <code class="o">=</code> <code class="n">log_loss</code><code class="p">(</code><code class="n">y_cv_fold</code><code class="p">,</code> \&#13;
        <code class="n">predictionsBasedOnKFolds</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">X_cv_fold</code><code class="o">.</code><code class="n">index</code><code class="p">,</code><code class="s1">'prediction'</code><code class="p">])</code>&#13;
    <code class="n">cvScores</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">loglossCV</code><code class="p">)</code>&#13;
&#13;
    <code class="k">print</code><code class="p">(</code><code class="s1">'Training Log Loss: '</code><code class="p">,</code> <code class="n">loglossTraining</code><code class="p">)</code>&#13;
    <code class="k">print</code><code class="p">(</code><code class="s1">'CV Log Loss: '</code><code class="p">,</code> <code class="n">loglossCV</code><code class="p">)</code>&#13;
&#13;
<code class="n">loglossLightGBMGradientBoosting</code> <code class="o">=</code> <code class="n">log_loss</code><code class="p">(</code><code class="n">y_train</code><code class="p">,</code> \&#13;
        <code class="n">predictionsBasedOnKFolds</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code><code class="s1">'prediction'</code><code class="p">])</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s1">'LightGBM Gradient Boosting Log Loss: '</code><code class="p">,</code> \&#13;
        <code class="n">loglossLightGBMGradientBoosting</code><code class="p">)</code></pre>&#13;
&#13;
<p>We will now use this model to predict the fraud on the test set of credit card transactions.</p>&#13;
&#13;
<p><a data-type="xref" href="#results_of_supervised_model">Figure 9-1</a> displays the results.</p>&#13;
&#13;
<figure><div class="figure" id="results_of_supervised_model">&#13;
<img alt="Results of Supervised Model" src="assets/hulp_0901.png"/>&#13;
<h6><span class="label">Figure 9-1. </span>Results of supervised model</h6>&#13;
</div></figure>&#13;
&#13;
<p>The average precision on the test based on the precision-recall curve is 0.62. To catch 75% of the fraud, we have a precision of just 0.5%.<a data-primary="" data-startref="SSLsuper09" data-type="indexterm" id="idm140637538124560"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Unsupervised Model" data-type="sect1"><div class="sect1" id="idm140637538693968">&#13;
<h1>Unsupervised Model</h1>&#13;
&#13;
<p>Now let’s<a data-primary="semisupervised learning" data-secondary="unsupervised model" data-type="indexterm" id="SSLunsuper09"/> build a fraud detection solution using unsupervised learning. Specifically, we will build a sparse two-layer overcomplete autoencoder with a linear activation function. We will have 40 nodes in the hidden layer and a dropout of 2%.</p>&#13;
&#13;
<p>However, we<a data-primary="oversampling" data-type="indexterm" id="idm140637538119968"/> will adjust our training set by <em>oversampling</em> the number of fraudulent cases we have. Oversampling is a technique used to adjust the class distribution in a given dataset. We want to add more fraudulent cases to our dataset so that the autoencoder we train has an easier time separating the normal/nonfraudulent transactions from the abnormal/fraudulent ones.</p>&#13;
&#13;
<p>Recall that after having dropped 90% of the fraudulent cases from the training set, we have just 33 fraudulent cases left. We will take the 33 fraudulent cases, duplicate these 100 times, and then append them to the training set. We will also keep copies of the nonoversampled training set so we can use them for the rest of our machine learning pipeline.</p>&#13;
&#13;
<p>Remember we do not touch the test set—there is no oversampling with the test set, just the training set:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">oversample_multiplier</code> <code class="o">=</code> <code class="mi">100</code>&#13;
&#13;
<code class="n">X_train_original</code> <code class="o">=</code> <code class="n">X_train</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code>&#13;
<code class="n">y_train_original</code> <code class="o">=</code> <code class="n">y_train</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code>&#13;
<code class="n">X_test_original</code> <code class="o">=</code> <code class="n">X_test</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code>&#13;
<code class="n">y_test_original</code> <code class="o">=</code> <code class="n">y_test</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code>&#13;
&#13;
<code class="n">X_train_oversampled</code> <code class="o">=</code> <code class="n">X_train</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code>&#13;
<code class="n">y_train_oversampled</code> <code class="o">=</code> <code class="n">y_train</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code>&#13;
<code class="n">X_train_oversampled</code> <code class="o">=</code> <code class="n">X_train_oversampled</code><code class="o">.</code><code class="n">append</code><code class="p">(</code> \&#13;
        <code class="p">[</code><code class="n">X_train_oversampled</code><code class="p">[</code><code class="n">y_train</code><code class="o">==</code><code class="mi">1</code><code class="p">]]</code><code class="o">*</code><code class="n">oversample_multiplier</code><code class="p">,</code> \&#13;
        <code class="n">ignore_index</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code>&#13;
<code class="n">y_train_oversampled</code> <code class="o">=</code> <code class="n">y_train_oversampled</code><code class="o">.</code><code class="n">append</code><code class="p">(</code> \&#13;
        <code class="p">[</code><code class="n">y_train_oversampled</code><code class="p">[</code><code class="n">y_train</code><code class="o">==</code><code class="mi">1</code><code class="p">]]</code><code class="o">*</code><code class="n">oversample_multiplier</code><code class="p">,</code> \&#13;
        <code class="n">ignore_index</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_train</code> <code class="o">=</code> <code class="n">X_train_oversampled</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code>&#13;
<code class="n">y_train</code> <code class="o">=</code> <code class="n">y_train_oversampled</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code></pre>&#13;
&#13;
<p>Let’s now train our autoencoder:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">model</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">()</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="n">units</code><code class="o">=</code><code class="mi">40</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s1">'linear'</code><code class="p">,</code> \&#13;
                <code class="n">activity_regularizer</code><code class="o">=</code><code class="n">regularizers</code><code class="o">.</code><code class="n">l1</code><code class="p">(</code><code class="mf">10e-5</code><code class="p">),</code> \&#13;
                <code class="n">input_dim</code><code class="o">=</code><code class="mi">29</code><code class="p">,</code><code class="n">name</code><code class="o">=</code><code class="s1">'hidden_layer'</code><code class="p">))</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dropout</code><code class="p">(</code><code class="mf">0.02</code><code class="p">))</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="n">units</code><code class="o">=</code><code class="mi">29</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s1">'linear'</code><code class="p">))</code>&#13;
&#13;
<code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">optimizer</code><code class="o">=</code><code class="s1">'adam'</code><code class="p">,</code>&#13;
              <code class="n">loss</code><code class="o">=</code><code class="s1">'mean_squared_error'</code><code class="p">,</code>&#13;
              <code class="n">metrics</code><code class="o">=</code><code class="p">[</code><code class="s1">'accuracy'</code><code class="p">])</code>&#13;
&#13;
<code class="n">num_epochs</code> <code class="o">=</code> <code class="mi">5</code>&#13;
<code class="n">batch_size</code> <code class="o">=</code> <code class="mi">32</code>&#13;
&#13;
<code class="n">history</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="n">X_train</code><code class="p">,</code>&#13;
                    <code class="n">epochs</code><code class="o">=</code><code class="n">num_epochs</code><code class="p">,</code>&#13;
                    <code class="n">batch_size</code><code class="o">=</code><code class="n">batch_size</code><code class="p">,</code>&#13;
                    <code class="n">shuffle</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>&#13;
                    <code class="n">validation_split</code><code class="o">=</code><code class="mf">0.20</code><code class="p">,</code>&#13;
                    <code class="n">verbose</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>&#13;
&#13;
<code class="n">predictions</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">verbose</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>&#13;
<code class="n">anomalyScoresAE</code> <code class="o">=</code> <code class="n">anomalyScores</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">predictions</code><code class="p">)</code>&#13;
<code class="n">preds</code><code class="p">,</code> <code class="n">average_precision</code> <code class="o">=</code> <code class="n">plotResults</code><code class="p">(</code><code class="n">y_test</code><code class="p">,</code> <code class="n">anomalyScoresAE</code><code class="p">,</code> <code class="bp">True</code><code class="p">)</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#results_of_unsupervised_model">Figure 9-2</a> displays the results.</p>&#13;
&#13;
<figure><div class="figure" id="results_of_unsupervised_model">&#13;
<img alt="Results of Unsupervised Model" src="assets/hulp_0902.png"/>&#13;
<h6><span class="label">Figure 9-2. </span>Results of unsupervised model</h6>&#13;
</div></figure>&#13;
&#13;
<p>The average precision on the test based on the precision-recall curve is 0.57. To catch 75% of the fraud, we have a precision of just 45%. While the average precision of the unsupervised solution is similar to the average precision of the supervised solution, the precision of 45% at 75% recall is better.</p>&#13;
&#13;
<p>However, the unsupervised solution by itself is still not great.<a data-primary="" data-startref="SSLunsuper09" data-type="indexterm" id="idm140637537966528"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Semisupervised Model" data-type="sect1"><div class="sect1" id="idm140637538122992">&#13;
<h1>Semisupervised Model</h1>&#13;
&#13;
<p>Now, let’s<a data-primary="semisupervised learning" data-secondary="semisupervised model" data-type="indexterm" id="SSLmodel09"/> take the representation learned by the autoencoder (the hidden layer), combine it with the original training set, and feed this into the gradient boosting algorithm. This a semisupervised approach, taking advantage of supervised and unsupervised learning.</p>&#13;
&#13;
<p>To get the hidden layer, we call the <code>Model()</code> class from the Keras API and use the <code>get_layer</code> function:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">layer_name</code> <code class="o">=</code> <code class="s1">'hidden_layer'</code>&#13;
&#13;
<code class="n">intermediate_layer_model</code> <code class="o">=</code> <code class="n">Model</code><code class="p">(</code><code class="n">inputs</code><code class="o">=</code><code class="n">model</code><code class="o">.</code><code class="n">input</code><code class="p">,</code> \&#13;
                                 <code class="n">outputs</code><code class="o">=</code><code class="n">model</code><code class="o">.</code><code class="n">get_layer</code><code class="p">(</code><code class="n">layer_name</code><code class="p">)</code><code class="o">.</code><code class="n">output</code><code class="p">)</code>&#13;
<code class="n">intermediate_output_train</code> <code class="o">=</code> <code class="n">intermediate_layer_model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_train_original</code><code class="p">)</code>&#13;
<code class="n">intermediate_output_test</code> <code class="o">=</code> <code class="n">intermediate_layer_model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test_original</code><code class="p">)</code></pre>&#13;
&#13;
<p>Let’s store these autoencoder representations into DataFrames and then combine them with the original training set:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">intermediate_output_trainDF</code> <code class="o">=</code> \&#13;
    <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">intermediate_output_train</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">X_train_original</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>&#13;
<code class="n">intermediate_output_testDF</code> <code class="o">=</code> \&#13;
    <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">intermediate_output_test</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">X_test_original</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_train</code> <code class="o">=</code> <code class="n">X_train_original</code><code class="o">.</code><code class="n">merge</code><code class="p">(</code><code class="n">intermediate_output_trainDF</code><code class="p">,</code> \&#13;
                                 <code class="n">left_index</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code><code class="n">right_index</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>&#13;
<code class="n">X_test</code> <code class="o">=</code> <code class="n">X_test_original</code><code class="o">.</code><code class="n">merge</code><code class="p">(</code><code class="n">intermediate_output_testDF</code><code class="p">,</code> \&#13;
                               <code class="n">left_index</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code><code class="n">right_index</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>&#13;
<code class="n">y_train</code> <code class="o">=</code> <code class="n">y_train_original</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code></pre>&#13;
&#13;
<p>We will now train the gradient boosting model on this new training set of 69 features (29 from the original dataset and 40 from the autoencoder’s representation):</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">trainingScores</code> <code class="o">=</code> <code class="p">[]</code>&#13;
<code class="n">cvScores</code> <code class="o">=</code> <code class="p">[]</code>&#13;
<code class="n">predictionsBasedOnKFolds</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="p">[],</code><code class="n">index</code><code class="o">=</code><code class="n">y_train</code><code class="o">.</code><code class="n">index</code><code class="p">,</code> \&#13;
                                        <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'prediction'</code><code class="p">])</code>&#13;
&#13;
<code class="k">for</code> <code class="n">train_index</code><code class="p">,</code> <code class="n">cv_index</code> <code class="ow">in</code> <code class="n">k_fold</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">)),</code> \&#13;
                                          <code class="n">y_train</code><code class="o">.</code><code class="n">ravel</code><code class="p">()):</code>&#13;
    <code class="n">X_train_fold</code><code class="p">,</code> <code class="n">X_cv_fold</code> <code class="o">=</code> <code class="n">X_train</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">train_index</code><code class="p">,:],</code> \&#13;
        <code class="n">X_train</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">cv_index</code><code class="p">,:]</code>&#13;
    <code class="n">y_train_fold</code><code class="p">,</code> <code class="n">y_cv_fold</code> <code class="o">=</code> <code class="n">y_train</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">train_index</code><code class="p">],</code> \&#13;
        <code class="n">y_train</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">cv_index</code><code class="p">]</code>&#13;
&#13;
    <code class="n">lgb_train</code> <code class="o">=</code> <code class="n">lgb</code><code class="o">.</code><code class="n">Dataset</code><code class="p">(</code><code class="n">X_train_fold</code><code class="p">,</code> <code class="n">y_train_fold</code><code class="p">)</code>&#13;
    <code class="n">lgb_eval</code> <code class="o">=</code> <code class="n">lgb</code><code class="o">.</code><code class="n">Dataset</code><code class="p">(</code><code class="n">X_cv_fold</code><code class="p">,</code> <code class="n">y_cv_fold</code><code class="p">,</code> <code class="n">reference</code><code class="o">=</code><code class="n">lgb_train</code><code class="p">)</code>&#13;
    <code class="n">gbm</code> <code class="o">=</code> <code class="n">lgb</code><code class="o">.</code><code class="n">train</code><code class="p">(</code><code class="n">params_lightGB</code><code class="p">,</code> <code class="n">lgb_train</code><code class="p">,</code> <code class="n">num_boost_round</code><code class="o">=</code><code class="mi">5000</code><code class="p">,</code>&#13;
                   <code class="n">valid_sets</code><code class="o">=</code><code class="n">lgb_eval</code><code class="p">,</code> <code class="n">early_stopping_rounds</code><code class="o">=</code><code class="mi">200</code><code class="p">)</code>&#13;
&#13;
    <code class="n">loglossTraining</code> <code class="o">=</code> <code class="n">log_loss</code><code class="p">(</code><code class="n">y_train_fold</code><code class="p">,</code>&#13;
                                <code class="n">gbm</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_train_fold</code><code class="p">,</code> \&#13;
                                <code class="n">num_iteration</code><code class="o">=</code><code class="n">gbm</code><code class="o">.</code><code class="n">best_iteration</code><code class="p">))</code>&#13;
    <code class="n">trainingScores</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">loglossTraining</code><code class="p">)</code>&#13;
&#13;
    <code class="n">predictionsBasedOnKFolds</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">X_cv_fold</code><code class="o">.</code><code class="n">index</code><code class="p">,</code><code class="s1">'prediction'</code><code class="p">]</code> <code class="o">=</code> \&#13;
        <code class="n">gbm</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_cv_fold</code><code class="p">,</code> <code class="n">num_iteration</code><code class="o">=</code><code class="n">gbm</code><code class="o">.</code><code class="n">best_iteration</code><code class="p">)</code>&#13;
    <code class="n">loglossCV</code> <code class="o">=</code> <code class="n">log_loss</code><code class="p">(</code><code class="n">y_cv_fold</code><code class="p">,</code> \&#13;
            <code class="n">predictionsBasedOnKFolds</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">X_cv_fold</code><code class="o">.</code><code class="n">index</code><code class="p">,</code><code class="s1">'prediction'</code><code class="p">])</code>&#13;
    <code class="n">cvScores</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">loglossCV</code><code class="p">)</code>&#13;
&#13;
    <code class="k">print</code><code class="p">(</code><code class="s1">'Training Log Loss: '</code><code class="p">,</code> <code class="n">loglossTraining</code><code class="p">)</code>&#13;
    <code class="k">print</code><code class="p">(</code><code class="s1">'CV Log Loss: '</code><code class="p">,</code> <code class="n">loglossCV</code><code class="p">)</code>&#13;
&#13;
<code class="n">loglossLightGBMGradientBoosting</code> <code class="o">=</code> <code class="n">log_loss</code><code class="p">(</code><code class="n">y_train</code><code class="p">,</code> \&#13;
                        <code class="n">predictionsBasedOnKFolds</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code><code class="s1">'prediction'</code><code class="p">])</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s1">'LightGBM Gradient Boosting Log Loss: '</code><code class="p">,</code> \&#13;
                        <code class="n">loglossLightGBMGradientBoosting</code><code class="p">)</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#results_of_semi_supervised_model">Figure 9-3</a> displays the results.</p>&#13;
&#13;
<figure><div class="figure" id="results_of_semi_supervised_model">&#13;
<img alt="Results of Semisupervised Model" src="assets/hulp_0903.png"/>&#13;
<h6><span class="label">Figure 9-3. </span>Results of semisupervised model</h6>&#13;
</div></figure>&#13;
&#13;
<p>The average precision on the test set based on the precision-recall curve is 0.78. This is a good bit higher than both the supervised and the unsupervised models.</p>&#13;
&#13;
<p>To catch 75% of the fraud, we have a precision of 92%. This is a considerable improvement. With this level of precision, the payment processor should feel comfortable rejecting transactions that the model flags as potentially fraudulent. Less than one in ten will be wrong, and we will catch approximately 75% of the fraud.<a data-primary="" data-startref="SSLmodel09" data-type="indexterm" id="idm140637537291968"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Power of Supervised and Unsupervised" data-type="sect1"><div class="sect1" id="idm140637537964960">&#13;
<h1>The Power of Supervised and Unsupervised</h1>&#13;
&#13;
<p>In<a data-primary="semisupervised learning" data-secondary="power of combined learning" data-type="indexterm" id="idm140637537289488"/> this semisupervised credit card fraud detection solution, both supervised learning and unsupervised learning have important roles to play. One way to explore this is by analyzing which features the final gradient boosting model found most important.</p>&#13;
&#13;
<p class="pagebreak-before">Let’s find and store those feature importance values from the model we just trained:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">featuresImportance</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="nb">list</code><code class="p">(</code><code class="n">gbm</code><code class="o">.</code><code class="n">feature_importance</code><code class="p">()),</code> \&#13;
                        <code class="n">index</code><code class="o">=</code><code class="n">X_train</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code><code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'featImportance'</code><code class="p">])</code>&#13;
<code class="n">featuresImportance</code> <code class="o">=</code> <code class="n">featuresImportance</code><code class="o">/</code><code class="n">featuresImportance</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code>&#13;
<code class="n">featuresImportance</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="n">by</code><code class="o">=</code><code class="s1">'featImportance'</code><code class="p">,</code> \&#13;
                               <code class="n">ascending</code><code class="o">=</code><code class="bp">False</code><code class="p">,</code><code class="n">inplace</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>&#13;
<code class="n">featuresImportance</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#feature_importantce_from_semi_supervised_model">Table 9-1</a> shows some of the most important features, sorted in descending order.</p>&#13;
<table id="feature_importantce_from_semi_supervised_model">&#13;
<caption><span class="label">Table 9-1. </span>Feature importance from semisupervised model</caption>&#13;
<thead>&#13;
<tr>&#13;
<th/>&#13;
<th>featImportance</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>V28</p></td>&#13;
<td><p>0.047843</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Amount</p></td>&#13;
<td><p>0.037263</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>21</p></td>&#13;
<td><p>0.030244</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>V21</p></td>&#13;
<td><p>0.029624</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>V26</p></td>&#13;
<td><p>0.029469</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>V12</p></td>&#13;
<td><p>0.028334</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>V27</p></td>&#13;
<td><p>0.028024</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>6</p></td>&#13;
<td><p>0.027405</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>28</p></td>&#13;
<td><p>0.026941</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>36</p></td>&#13;
<td><p>0.024050</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>5</p></td>&#13;
<td><p>0.022347</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>As you can see here, some of the top features are features the hidden layer learned by the autoencoder (the non “V” features) while others are the principal components from the original dataset (the “V” features) as well as the amount of the transaction.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="idm140637537048672">&#13;
<h1>Conclusion</h1>&#13;
&#13;
<p>The semisupervised model trounces the performance of both the standalone supervised model and the standalone unsupervised model.</p>&#13;
&#13;
<p>We just scratched the surface of what’s possible with semisupervised learning, but this should help reframe the conversation from debating between supervised and unsupervised learning to combining supervised and unsupervised learning in the search for an optimal applied solution.<a data-primary="" data-startref="ULshallow07" data-type="indexterm" id="idm140637537252336"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>