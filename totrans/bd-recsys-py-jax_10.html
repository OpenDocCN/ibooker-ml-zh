<html><head></head><body><section data-pdf-bookmark="Chapter 8. Putting It All Together: Data Processing and Counting Recommender" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch:wikipedia-e2e">&#13;
<h1><span class="label">Chapter 8. </span>Putting It All Together: Data Processing and Counting Recommender</h1>&#13;
&#13;
&#13;
<p>Now that we have discussed the broad outline of recommender systems, this chapter will put it into a concrete implementation so that we can talk about the choices of technologies and specifics of how the implementation works in real life.</p>&#13;
&#13;
<p>This chapter covers the following topics:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Data representation with protocol buffers</p>&#13;
</li>&#13;
<li>&#13;
<p>Data processing frameworks</p>&#13;
</li>&#13;
<li>&#13;
<p>A PySpark sample program</p>&#13;
</li>&#13;
<li>&#13;
<p>GloVE embedding model</p>&#13;
</li>&#13;
<li>&#13;
<p>Additional foundational techniques in JAX, Flax, and Optax</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>We will show step-by-step how to go from a downloaded Wikipedia dataset to a recommender system that can recommend words from Wikipedia based on the co-occurrence with words in a Wikipedia article. We use a natural language example because words are easily understood, and their relationships are readily grasped because we can see that related words occur near one another in a sentence. Furthermore, the Wikipedia corpus is easily downloadable and browsable by anyone with an internet connection. This idea of co-occurrence can be generalized to any co-occurring collection of items, such as watching a video in the same session or purchasing cheeses in the same shopping bag.</p>&#13;
&#13;
<p>This chapter will demonstrate concrete implementations of an item-item and a feature-item recommender. Items in this case are the words in an article, and the features are word-count similarity—a MinHash or a kind of locality sensitive hash for words. <a data-type="xref" href="ch16.html#acceleration_structures">Chapter 16</a> covers locality sensitive hash in more detail, but for now, we’ll consider these simple hashing functions to be encoding functions over content, such that content with similar properties maps to similar co-domains. This general idea can be used as a warm-start mechanism on a new corpus in the absence of logging data, and if we have user-item features such as likes, these can be used as features for a feature-item recommender. The principles of co-occurrence are the same, but by using Wikipedia as an example, you can download the data and play with it by using the tools provided.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id735">&#13;
<h1>Warm and Cold Starts</h1>&#13;
<p>A <em>cold start</em> occurs<a data-primary="cold starting" data-type="indexterm" id="id736"/> when we do not have any information about a corpus or people’s preferences and resort to a best-guess approach such as recommending popular items. On the other hand, if items naturally occur in typical groupings, like the selection and arrangement in the cheese aisle of a grocery store, then we call this a warm start: using information like co-occurrence of cheeses with each other or with other items like salami as a means of starting out the recommender engine more intelligently.</p>&#13;
&#13;
<p>In the Wikipedia example, even before we have users click articles, we’ll be able to<a data-primary="warm starts" data-type="indexterm" id="id737"/> warm-start the word-to-word recommender simply based on how close words are to each other in a sentence. Similarly, if you had a bunch of items that naturally fall into some kind of hierarchical taxonomy, you might be able to warm-start your recommender by having items that are in the same branch of the taxonomy count as co-occurring with one another.</p>&#13;
</div></aside>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Tech Stack" data-type="sect1"><div class="sect1" id="id90">&#13;
<h1>Tech Stack</h1>&#13;
&#13;
<p>A set of technologies used together is commonly called a<a data-primary="data processing" data-secondary="technology stacks" data-type="indexterm" id="id738"/><a data-primary="technology stacks (tech stacks)" data-type="indexterm" id="id739"/> <em>technology stack</em>, or <em>tech stack</em>. Each component of a tech stack can usually be replaced by other similar technologies. We will list a few alternatives for each component but not go into detail about their pros and cons, as there can be many, and the situation of the deployment will affect the choice of components. For example, your company might already use a particular component, so for familiarity and support, you might wish to use that one.</p>&#13;
&#13;
<p>This chapter covers some of the technology choices for processing the data that goes into building a concrete implementation of a collector.</p>&#13;
&#13;
<p>The<a data-primary="code" data-secondary="obtaining and using code examples" data-type="indexterm" id="id740"/><a data-primary="GitHub/Git" data-type="indexterm" id="id741"/> sample code is available on <a href="https://github.com/BBischof/ESRecsys">GitHub</a>. You might want to clone the code into a local directory.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Data Representation" data-type="sect1"><div class="sect1" id="id91">&#13;
<h1>Data Representation</h1>&#13;
&#13;
<p>The<a data-primary="data processing" data-secondary="data representation" data-type="indexterm" id="id742"/><a data-primary="protocol buffers" data-type="indexterm" id="id743"/> first choice of technology we need to make will determine how we represent the data. Some of the choices are as follows:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><a href="https://oreil.ly/Oc0cE">Protocol buffers</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/BUHkW">Apache Thrift</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/_QwWR">JSON</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/JigfM">XML</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/it5TA">CSV</a></p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>In this implementation, we’re mostly using protocol buffers because of the ease of specifying a schema and then subsequently serializing and deserializing it.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id744">&#13;
<h1>Protocol Buffers</h1>&#13;
<p>Before protocol buffers were invented, people used to store their binary data in all sorts of custom formats that involved various syntax and specifications (like starting a file with a magic number, followed by rules on how to parse and store various data types like integers, strings, bytes, and floating-point numbers). Protocol buffers unified the storage of custom binary data by allowing users to specify a<a data-primary="schemas" data-type="indexterm" id="id745"/> <em>schema</em>, or a named representation of each field and the type of each field (like <code>first_name</code> being a string and <code>age</code> being an integer). This enables us to easily read and write structured data in a binary format, and the parsing of the data is handled automatically by the protocol buffer library.</p>&#13;
</div></aside>&#13;
&#13;
<p>For the file format, we’re using<a data-primary="serialized protocol buffers" data-type="indexterm" id="id746"/> serialized protocol buffers that are uuencoded and written as a single line per record and then bzipped up for compression. This is just for convenience so that we can parse the files easily without having dependencies on too many libraries. Your company might instead store data in a data warehouse that is accessible by SQL, for example.</p>&#13;
&#13;
<p>Protocol buffers are generally easier to parse and handle than raw data. In our implementation, we will parse the Wikipedia XML into protocol buffers for easier handling using <em>xml2proto.py</em>. You can see from the code that<a data-primary="XML parsing" data-type="indexterm" id="id747"/> XML parsing is a complicated affair, whereas protocol buffer parsing is as simple as calling the <code>ParseFromString</code> method, and all the data is then subsequently available as a convenient Python object.</p>&#13;
&#13;
<p>As of June 2022, the<a data-primary="Wikipedia corpus" data-type="indexterm" id="id748"/> Wikipedia dump is about 20 GB in size, and converting to protocol buffer format takes about 10 minutes. Please follow the steps described in the README in the GitHub repo for the most up-to-date steps to run the programs.</p>&#13;
&#13;
<p>In the <em>proto</em> directory, take a look at some of the protocol messages defined.&#13;
This, for example, is how we might store the text from a Wikipedia page:</p>&#13;
&#13;
<pre data-type="programlisting">// Generic text document.&#13;
message TextDocument {&#13;
  // Primary entity, in wikipedia it is the title.&#13;
  string primary = 1;&#13;
  // Secondary entity, in wikipedia it is other titles.&#13;
  repeated string secondary = 2;&#13;
  // Raw body tokens.&#13;
  repeated string tokens = 3;&#13;
  // URL. Only visible documents have urls, some e.g. redirect shouldn't.&#13;
  string url = 4;&#13;
}</pre>&#13;
&#13;
<p>The<a data-primary="protocol buffer compiler" data-type="indexterm" id="id749"/> types supported and the schema definitions can be found on the protocol buffer documentation page. This schema is converted into code by using the protocol buffer compiler. This compiler’s job is to convert the schema into code that you can call in different languages, which in our case is Python. The installation of the protocol buffer compiler depends on the platform, and installation instructions can be found in the <a href="https://oreil.ly/k2QEv">protocol buffer documentation</a>.</p>&#13;
&#13;
<p>Each time you change the schema, you will have to use the protocol buffer compiler to get a new version of the protocol buffer code. This step can easily be automated by using a build system like Bazel, but this is out of scope for this book. For the purposes of this book, we will simply generate the protocol buffer code once and check it into the repository for simplicity.</p>&#13;
&#13;
<p>Following the directions on the GitHub README, download a copy of the Wikipedia dataset and then run <em>xml2proto.py</em> to convert the data to a protocol buffer format. Optionally, use <em>codex.py</em> to see what the protocol buffer format looks like. These steps took 10 minutes on a Windows workstation using Windows Subsystem for Linux. The XML parser used doesn’t parallelize very well, so this step is fundamentally serial. We’ll next discuss how we would distribute the work in parallel either among multiple cores locally or on a cluster.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Big Data Frameworks" data-type="sect1"><div class="sect1" id="id92">&#13;
<h1>Big Data Frameworks</h1>&#13;
&#13;
<p>The<a data-primary="data processing" data-secondary="big data frameworks" data-type="indexterm" id="DPbigdata08"/><a data-primary="big data frameworks" data-secondary="options for" data-type="indexterm" id="id750"/> next technology we choose will process data at scale on multiple machines. Some options are listed here:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><a href="https://spark.apache.org">Apache Spark</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://beam.apache.org">Apache Beam</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://flink.apache.org">Apache Flink</a></p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>In this implementation, we’re using Apache Spark in Python, or PySpark. The README in the repository shows how to<a data-primary="PySpark" data-secondary="installing" data-type="indexterm" id="id751"/> install a copy of PySpark locally using <code>pip install</code>.</p>&#13;
&#13;
<p>The<a data-primary="PySpark" data-secondary="tokenization and URL normalization" data-type="indexterm" id="id752"/><a data-primary="tokenization" data-type="indexterm" id="id753"/><a data-primary="URL normalization" data-type="indexterm" id="id754"/><a data-primary="normalization" data-type="indexterm" id="id755"/> first step implemented in PySpark is tokenization and URL normalization. The code is in <a href="https://oreil.ly/TF_vU"><em>tokenize_wiki_pyspark.py</em></a>, but we won’t go over it here because a lot of the processing is simply distributed natural language parsing and writing out the data into protocol buffer format. We will instead talk in detail about the second step, which is to make a dictionary of tokens (the <em>words</em> in the article) and some statistics about the word counts. However, we will run the code just to see what the Spark usage experience looks like. Spark programs are run using the program <code>spark-submit</code> as follows:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">bin/spark-submit<code class="w"/>&#13;
--master<code class="o">=</code>local<code class="o">[</code><code class="m">4</code><code class="o">]</code><code class="w"/>&#13;
--conf<code class="o">=</code><code class="s2">"spark.files.ignoreCorruptFiles=true"</code><code class="w"/>&#13;
tokenize_wiki_pyspark.py<code class="w"/>&#13;
--input_file<code class="o">=</code>data/enwiki-latest-parsed<code class="w"> </code>--output_file<code class="o">=</code>data/enwiki-latest-tokenized<code class="w"/></pre>&#13;
&#13;
<p>Running the Spark submit script allows you to execute the controller program, in this case, <a href="https://oreil.ly/pQp7r"><em>tokenize_wiki_pyspark.py</em></a>, on a local machine as we have in the command line—note that the line <code>local[4]</code> means use up to four cores. The same command can be used to submit the job to a YARN cluster for running on hundreds of machines, but for the purposes of trying out PySpark, a decent enough workstation should be able to process all the data in minutes.</p>&#13;
&#13;
<p>This tokenization program converts from a source-specific format (in this case, a Wikipedia protocol buffer) into a more generic text document used for NLP. In general, it’s a good idea to use a generic format that all your sources of data can be converted into because that simplifies the data processing downstream. The data conversion can be done from each corpus into a standard format that is handled uniformly by all the later programs in the pipeline.</p>&#13;
&#13;
<p>After submitting the job, you can navigate to the<a data-primary="PySpark" data-secondary="Spark UI" data-type="indexterm" id="id756"/> Spark UI (shown in <a data-type="xref" href="#spark_ui">Figure 8-1</a>) on your local machine at <em>localhost:4040/stages/</em>. You should see the job executing in parallel, using up all the cores in your machine. You might want to play with the <code>local[4]</code> parameter; using <code>local[*]</code> will use up all the free cores on your machine. If you have access to a cluster, you can also point to the appropriate cluster URL.</p>&#13;
&#13;
<figure><div class="figure" id="spark_ui">&#13;
<img alt="Spark UI showing the stages of computation" src="assets/brpj_0801.png"/>&#13;
<h6><span class="label">Figure 8-1. </span>Spark UI</h6>&#13;
</div></figure>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Cluster Frameworks" data-type="sect2"><div class="sect2" id="id93">&#13;
<h2>Cluster Frameworks</h2>&#13;
&#13;
<p>The<a data-primary="big data frameworks" data-secondary="cluster frameworks" data-type="indexterm" id="id757"/><a data-primary="cluster frameworks" data-type="indexterm" id="id758"/> nice thing about writing a Spark program is that it can scale from a single machine with multiple cores to a cluster of many machines with thousands of cores. The full list of<a data-primary="PySpark" data-secondary="cluster types" data-type="indexterm" id="id759"/> cluster types can be found in the <a href="https://oreil.ly/0apFm">Spark “Submitting Applications” documentation</a>.</p>&#13;
&#13;
<p>Spark can run on the following cluster types:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><a href="https://oreil.ly/NIiwB">Spark Standalone cluster</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/lHzRG">Mesos cluster</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/nuEQh">YARN cluster</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/sXIfK">Kubernetes cluster</a></p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Depending on the kind of cluster your company or institution has set up, most of the time submitting the job is just a matter of pointing to the correct URL. Many companies such as Databricks and Google also have fully managed Spark solutions that allow you to set up a Spark cluster with little effort.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="PySpark Example" data-type="sect2"><div class="sect2" id="id94">&#13;
<h2>PySpark Example</h2>&#13;
&#13;
<p>Counting words<a data-primary="PySpark" data-secondary="example program" data-type="indexterm" id="PSexample08"/><a data-primary="term frequency, inverse document frequency (TF-IDF)" data-type="indexterm" id="id760"/> turns out to be a powerful tool in information retrieval, as we can use handy tricks like term frequency, inverse document frequency (TF-IDF), which is simply the count of words in the documents divided by the number of documents the word has occurred in. This is represented as follows:</p>&#13;
<div data-type="equation">&#13;
<math alttext="t f i d f Subscript word Baseline left-parenthesis i right-parenthesis equals StartFraction log Subscript 10 Baseline left-parenthesis number of times w o r d Subscript i Baseline has occurred in corpus right-parenthesis Over number of documents in corpus containing w o r d Subscript i Baseline EndFraction" display="block">&#13;
  <mrow>&#13;
    <mi>t</mi>&#13;
    <mi>f</mi>&#13;
    <mi>i</mi>&#13;
    <mi>d</mi>&#13;
    <msub><mi>f</mi> <mtext>word</mtext> </msub>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>i</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mstyle displaystyle="true" scriptlevel="0">&#13;
      <mfrac><mrow><msub><mo form="prefix">log</mo> <mn>10</mn> </msub><mfenced close=")" open="(" separators=""><mtext>number</mtext><mspace width="4.pt"/><mtext>of</mtext><mspace width="4.pt"/><mtext>times</mtext><mspace width="4.pt"/><mi>w</mi><mi>o</mi><mi>r</mi><msub><mi>d</mi> <mi>i</mi> </msub><mspace width="4.pt"/><mtext>has</mtext><mspace width="4.pt"/><mtext>occurred</mtext><mspace width="4.pt"/><mtext>in</mtext><mspace width="4.pt"/><mtext>corpus</mtext></mfenced></mrow> <mrow><mtext>number</mtext><mspace width="4.pt"/><mtext>of</mtext><mspace width="4.pt"/><mtext>documents</mtext><mspace width="4.pt"/><mtext>in</mtext><mspace width="4.pt"/><mtext>corpus</mtext><mspace width="4.pt"/><mtext>containing</mtext><mspace width="4.pt"/><mi>w</mi><mi>o</mi><mi>r</mi><msub><mi>d</mi> <mi>i</mi> </msub></mrow></mfrac>&#13;
    </mstyle>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>For example, because the word <em>the</em> appears frequently, we might think it is an important word. But by dividing by the document frequency, <em>the</em> becomes less special and drops in importance. This trick is quite handy in simple NLP to get a better-than-random weighting of word importance.</p>&#13;
&#13;
<p>Therefore, our next step is to run <a href="https://oreil.ly/lESlx"><em>make_dictionary.py</em></a>. As the name indicates, this program simply counts the words and documents and makes a dictionary with the number of times a word has occurred.</p>&#13;
&#13;
<p>We have some concepts to cover in order for you to properly grok how Spark helps process data in a distributed manner. The entry point of most Spark programs is <code>SparkContext</code>. This Python object is created on the controller. The<a data-primary="controller program" data-type="indexterm" id="id761"/> <em>controller</em> is the central program that launches workers that actually process the data. The workers can be run locally on a single machine as a process or on many machines on the cloud as separate workers.</p>&#13;
&#13;
<p><code>SparkContext</code> can be used to create<a data-primary="RDDs (resilient distributed datasets)" data-type="indexterm" id="id762"/><a data-primary="resilient distributed datasets (RDDs)" data-type="indexterm" id="id763"/> resilient distributed datasets, or RDDs. These are references to data streams that can be manipulated on the controller, and processing on the RDD can be farmed out to all the workers. <code>SparkContext</code> allows you to load up data files stored on a distributed filesystem like Hadoop Distributed File System (HDFS) or cloud buckets. By calling the <code>SparkContext</code>’s <code>textFile</code> method, we are returned a handle to an RDD. A stateless function can then be applied or mapped on the RDD to transform it from one RDD to another by repeatedly applying the function to the contents of the RDD.</p>&#13;
&#13;
<p>For example, this program fragment loads a text file and converts all lines to lowercase by running an anonymous lambda function that converts single lines to lowercase:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">lower_rdd</code><code class="p">(</code><code class="n">input_file</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code>&#13;
              <code class="n">output_file</code><code class="p">:</code> <code class="nb">str</code><code class="p">):</code>&#13;
  <code class="sd">"""Takes a text file and converts it to lowercase.."""</code>&#13;
  <code class="n">sc</code> <code class="o">=</code> <code class="n">SparkContext</code><code class="p">()</code>&#13;
  <code class="n">input_rdd</code> <code class="o">=</code> <code class="n">sc</code><code class="o">.</code><code class="n">textFile</code><code class="p">(</code><code class="n">input_file</code><code class="p">)</code>&#13;
  <code class="n">input_rdd</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="k">lambda</code> <code class="n">line</code><code class="p">:</code> <code class="n">line</code><code class="o">.</code><code class="n">lower</code><code class="p">())</code><code class="o">.</code><code class="n">saveAsTextFile</code><code class="p">(</code><code class="n">output_file</code><code class="p">)</code></pre>&#13;
&#13;
<p>In a single-machine implementation, we would simply load up each Wikipedia article, keep a running dictionary in RAM, and count each token and then add 1 to the token count in the dictionary. A<a data-primary="tokens" data-type="indexterm" id="id764"/> <em>token</em> is an atomic element of a document that is divided into pieces. In regular English, it would be a word, but Wikipedia documents have other entities such as the document references themselves that need to be kept track of separately, so we call the division into pieces<a data-primary="tokenization" data-type="indexterm" id="id765"/> <em>tokenization</em> and the atomic elements <em>tokens</em>. The single-machine implementation would take a while to go through the thousands of documents on Wikipedia, which is why we use a distributed processing framework like Spark. In the Spark paradigm, computation is broken into maps, where a function is applied statelessly on each document in parallel. Spark also has a reduce function, where the outputs of separate maps are joined together.</p>&#13;
&#13;
<p>For example, suppose we have a list of word counts and want to sum up the values of words that occur in different documents. The input to the reducer will be something like this:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>(apple, 10)</p>&#13;
</li>&#13;
<li>&#13;
<p>(orange, 20)</p>&#13;
</li>&#13;
<li>&#13;
<p>(apple, 7)</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p class="less_space pagebreak-before">Then we call the Spark function <code>reduceByKey(lambda a, b: a+ b)</code>,&#13;
which adds all the values with the same key together and returns the following:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>(orange, 20)</p>&#13;
</li>&#13;
<li>&#13;
<p>(apple, 17)</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>If you look at the code in <a href="https://oreil.ly/lESlx"><em>make_dictionary.py</em></a>, the<a data-primary="map phase" data-type="indexterm" id="id766"/><a data-primary="reduce phase" data-type="indexterm" id="id767"/> <em>map phase</em> is where we take a document as input and then break it into tuples of (token, 1). In the <em>reduce phase</em>, the map outputs are joined by the key, which in this case is the token itself, and the reduce function is simply to sum up all the counts of tokens.</p>&#13;
&#13;
<p>Note that the reduce function assumes that the reduction is associative—that is, <math alttext="left-parenthesis a plus b plus c right-parenthesis equals left-parenthesis a plus b right-parenthesis plus c equals a plus left-parenthesis b plus c right-parenthesis">&#13;
  <mrow>&#13;
    <mo>(</mo>&#13;
    <mi>a</mi>&#13;
    <mo>+</mo>&#13;
    <mi>b</mi>&#13;
    <mo>+</mo>&#13;
    <mi>c</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mo>(</mo>&#13;
    <mi>a</mi>&#13;
    <mo>+</mo>&#13;
    <mi>b</mi>&#13;
    <mo>)</mo>&#13;
    <mo>+</mo>&#13;
    <mi>c</mi>&#13;
    <mo>=</mo>&#13;
    <mi>a</mi>&#13;
    <mo>+</mo>&#13;
    <mo>(</mo>&#13;
    <mi>b</mi>&#13;
    <mo>+</mo>&#13;
    <mi>c</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>. This allows the Spark framework to sum up some parts of the token dictionary in memory on the map phase (in some frameworks, this is called the<a data-primary="combine step" data-type="indexterm" id="id768"/> <em>combine step</em>, where you run part of the reduction on the output of the map phase on the mapper machine) and then sum them up over several passes on the reduce phase.</p>&#13;
&#13;
<p>As an optimization, we use the Spark function <code>mapPartitions</code>. Map runs the provided function once per line (for which we have encoded an entire Wikipedia document as a protocol buffer, uuencoded as a single text line), whereas <code>mapPartitions</code> runs it over an entire partition, which is many documents, usually 64 MB of them. This optimization lets us construct a small Python dictionary over the entire partition so that we have many fewer token-count pairs to reduce. This saves on network bandwidth so the mapper has less data to send to the reducer, and is a good tip in general for these data processing pipelines to reduce network bandwidth (which is generally the most time-consuming part of data processing compared to computation).</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id769">&#13;
<h1>Group Theory</h1>&#13;
<p>Because<a data-primary="group theory" data-type="indexterm" id="id770"/> we are math nerds, and also because group theory shows up a lot in reduction operations, we will briefly introduce an algebraic structure known as <em>groups</em> so that you clearly understand all the terms used in the reduction phase.</p>&#13;
&#13;
<p>The concept of sets was mentioned in the introductory chapters; a <em>set</em> is a collection of items. The other concept you need to know is an operator. A binary <em>operator</em> takes two items and returns another item that is in the set.</p>&#13;
&#13;
<p>Examples of sets that are commonly used are integers, real numbers, and matrices. Examples of binary operators are addition, multiplication, and composition.</p>&#13;
&#13;
<p>An operator and a set denoted by the tuple (binary operator, a set of integers) form a group only if the group axioms are satisfied, namely:</p>&#13;
<dl>&#13;
<dt>An identity element exists.</dt>&#13;
<dd>&#13;
<p>For every element <math alttext="x">&#13;
  <mi>x</mi>&#13;
</math> in the group, there exists an element e such that <math alttext="x plus e equals e plus x equals e">&#13;
  <mrow>&#13;
    <mi>x</mi>&#13;
    <mo>+</mo>&#13;
    <mi>e</mi>&#13;
    <mo>=</mo>&#13;
    <mi>e</mi>&#13;
    <mo>+</mo>&#13;
    <mi>x</mi>&#13;
    <mo>=</mo>&#13;
    <mi>e</mi>&#13;
  </mrow>&#13;
</math>. For the addition operation, the identity is 0, and for multiplication, the identity is called 1. This concept is important in the reduction step because in some frameworks the reduction step is initialized with the identity element. For example, sums are usually initialized with 0, and products are usually initialized with 1.</p>&#13;
</dd>&#13;
<dt>The operator is associative.</dt>&#13;
<dd>&#13;
<p>For elements <math alttext="x comma y comma z">&#13;
  <mrow>&#13;
    <mi>x</mi>&#13;
    <mo>,</mo>&#13;
    <mi>y</mi>&#13;
    <mo>,</mo>&#13;
    <mi>z</mi>&#13;
  </mrow>&#13;
</math> in the set, <math alttext="left-parenthesis x plus y right-parenthesis plus z equals x plus left-parenthesis y plus z right-parenthesis">&#13;
  <mrow>&#13;
    <mo>(</mo>&#13;
    <mi>x</mi>&#13;
    <mo>+</mo>&#13;
    <mi>y</mi>&#13;
    <mo>)</mo>&#13;
    <mo>+</mo>&#13;
    <mi>z</mi>&#13;
    <mo>=</mo>&#13;
    <mi>x</mi>&#13;
    <mo>+</mo>&#13;
    <mo>(</mo>&#13;
    <mi>y</mi>&#13;
    <mo>+</mo>&#13;
    <mi>z</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>.</p>&#13;
</dd>&#13;
<dt>An inverse exists.</dt>&#13;
<dd>&#13;
<p>For every element <math alttext="x">&#13;
  <mi>x</mi>&#13;
</math> in the group, there exists a <math alttext="y">&#13;
  <mi>y</mi>&#13;
</math> in the group, such that <math alttext="x plus y equals e">&#13;
  <mrow>&#13;
    <mi>x</mi>&#13;
    <mo>+</mo>&#13;
    <mi>y</mi>&#13;
    <mo>=</mo>&#13;
    <mi>e</mi>&#13;
  </mrow>&#13;
</math>.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>An operator can also be commutative. This isn’t a requirement to be a group, but groups that have this property are called<a data-primary="commutative groups" data-type="indexterm" id="id771"/> <em>commutative groups</em>. With commutativity, for elements <math alttext="x comma y">&#13;
  <mrow>&#13;
    <mi>x</mi>&#13;
    <mo>,</mo>&#13;
    <mi>y</mi>&#13;
  </mrow>&#13;
</math> in the group, <math alttext="x plus y equals y plus x">&#13;
  <mrow>&#13;
    <mi>x</mi>&#13;
    <mo>+</mo>&#13;
    <mi>y</mi>&#13;
    <mo>=</mo>&#13;
    <mi>y</mi>&#13;
    <mo>+</mo>&#13;
    <mi>x</mi>&#13;
  </mrow>&#13;
</math>. This property is helpful in the reduction step because it allows the reducer to perform the operations in parallel and then reduce them together without worrying which operations occur in what order.</p>&#13;
&#13;
<p>It is important to note that while addition over real numbers is associative and commutative, addition of floating-point numbers isn’t. The reason is that floating-point approximately represents real numbers. So when you add a large number with a small number in floating-point, the small number isn’t represented accurately and might simply be discarded. A more accurate and consistent way to add floating-point numbers is to sort the list of numbers to be added first and add all the small numbers up before adding them to the large numbers. Adding two small numbers together first to make a larger number ensures that they do not get lost when being absorbed into the accumulator (the sum). Thus, while addition of numbers is in theory associative and commutative with real numbers, you might get different results in practice with floating-point numbers, depending on the order of operations.</p>&#13;
</div></aside>&#13;
&#13;
<p>Next we show a complete Spark program that reads in documents in the protocol buffer format of <code>TextDocument</code> shown in the preceding code block and then counts how often the words, or tokens, occur in the entire corpus. The file in the GitHub repo is <a href="https://oreil.ly/lESlx"><em>make_dictionary.py</em></a>. The following code is presented slightly differently from the repo file in that it is broken into three chunks for readability and the order of the main and subroutines have been swapped for clarity. Here, we present first the dependencies and flags, then the main body, and then the functions being called by the main body so that the purposes of the functions are clearer.</p>&#13;
&#13;
<p class="less_space pagebreak-before">First, let’s look at the dependencies. The main ones are the protocol buffer representing the text document of the Wikipedia article, as discussed earlier. This is the input we are expecting. For the output, we have the <code>TokenDictionary</code> protocol buffer, which mainly counts the occurrences of words in the article. We will use the co-occurrences of words to form a similarity graph of articles that we can then use as the basis of a warm-start recommender system. We also have dependencies on PySpark, the data processing framework we are using to process the data, as well as a flag library that handles the options of our program. The absl flags library is pretty handy for parsing and explaining the purposes of command-line flags and also retrieving the set values of flags easily. Here are the dependencies and flags:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="ch">#!/usr/bin/env python</code>&#13;
<code class="c1"># -*- coding: utf-8 -*-</code>&#13;
<code class="c1">#</code>&#13;
<code class="c1">#</code>&#13;
&#13;
<code class="sd">"""</code>&#13;
<code class="sd">  This reads a doc.pb.b64.bz2 file and generates a dictionary.</code>&#13;
<code class="sd">"""</code>&#13;
<code class="kn">import</code> <code class="nn">base64</code>&#13;
<code class="kn">import</code> <code class="nn">bz2</code>&#13;
<code class="kn">import</code> <code class="nn">nlp_pb2</code> <code class="k">as</code> <code class="nn">nlp_pb</code>&#13;
<code class="kn">import</code> <code class="nn">re</code>&#13;
<code class="kn">from</code> <code class="nn">absl</code> <code class="kn">import</code> <code class="n">app</code>&#13;
<code class="kn">from</code> <code class="nn">absl</code> <code class="kn">import</code> <code class="n">flags</code>&#13;
<code class="kn">from</code> <code class="nn">pyspark</code> <code class="kn">import</code> <code class="n">SparkContext</code>&#13;
<code class="kn">from</code> <code class="nn">token_dictionary</code> <code class="kn">import</code> <code class="n">TokenDictionary</code>&#13;
&#13;
<code class="n">FLAGS</code> <code class="o">=</code> <code class="n">flags</code><code class="o">.</code><code class="n">FLAGS</code>&#13;
<code class="n">flags</code><code class="o">.</code><code class="n">DEFINE_string</code><code class="p">(</code><code class="s2">"input_file"</code><code class="p">,</code> <code class="kc">None</code><code class="p">,</code> <code class="s2">"Input doc.pb.b64.bz2 file."</code><code class="p">)</code>&#13;
<code class="n">flags</code><code class="o">.</code><code class="n">DEFINE_string</code><code class="p">(</code><code class="s2">"title_output"</code><code class="p">,</code> <code class="kc">None</code><code class="p">,</code>&#13;
                    <code class="s2">"The title dictionary output file."</code><code class="p">)</code>&#13;
<code class="n">flags</code><code class="o">.</code><code class="n">DEFINE_string</code><code class="p">(</code><code class="s2">"token_output"</code><code class="p">,</code> <code class="kc">None</code><code class="p">,</code>&#13;
                    <code class="s2">"The token dictionary output file."</code><code class="p">)</code>&#13;
<code class="n">flags</code><code class="o">.</code><code class="n">DEFINE_integer</code><code class="p">(</code><code class="s2">"min_token_frequency"</code><code class="p">,</code> <code class="mi">20</code><code class="p">,</code>&#13;
                     <code class="s2">"Minimum token frequency"</code><code class="p">)</code>&#13;
<code class="n">flags</code><code class="o">.</code><code class="n">DEFINE_integer</code><code class="p">(</code><code class="s2">"max_token_dictionary_size"</code><code class="p">,</code> <code class="mi">500000</code><code class="p">,</code>&#13;
                     <code class="s2">"Maximum size of the token dictionary."</code><code class="p">)</code>&#13;
<code class="n">flags</code><code class="o">.</code><code class="n">DEFINE_integer</code><code class="p">(</code><code class="s2">"max_title_dictionary_size"</code><code class="p">,</code> <code class="mi">500000</code><code class="p">,</code>&#13;
                     <code class="s2">"Maximum size of the title dictionary."</code><code class="p">)</code>&#13;
<code class="n">flags</code><code class="o">.</code><code class="n">DEFINE_integer</code><code class="p">(</code><code class="s2">"min_title_frequency"</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code>&#13;
                     <code class="s2">"Titles must occur this often."</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Required flag.</code>&#13;
<code class="n">flags</code><code class="o">.</code><code class="n">mark_flag_as_required</code><code class="p">(</code><code class="s2">"input_file"</code><code class="p">)</code>&#13;
<code class="n">flags</code><code class="o">.</code><code class="n">mark_flag_as_required</code><code class="p">(</code><code class="s2">"token_output"</code><code class="p">)</code>&#13;
<code class="n">flags</code><code class="o">.</code><code class="n">mark_flag_as_required</code><code class="p">(</code><code class="s2">"title_output"</code><code class="p">)</code></pre>&#13;
&#13;
<p class="less_space pagebreak-before">Next, we have the main body of the program, which is where all the subroutines are called. We first create <code>SparkContext</code>, which is the entry point into the Spark data processing system, and then call its <code>textFile</code> method to read in the bzipped Wikipedia articles. Please read the README on the repo to understand how it was generated. Next, we parse the text document and send the RDD to two processing pipelines, one to make a dictionary for the body of the article and another to make a dictionary of the titles. We could choose to make a single unified dictionary for both, but having them separate allows us to create a content-based recommender using the token dictionary and an article-to-article recommender using the title dictionary, as titles are identifiers for the Wikipedia article. Here’s the main body:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">main</code><code class="p">(</code><code class="n">argv</code><code class="p">):</code>&#13;
  <code class="sd">"""Main function."""</code>&#13;
  <code class="k">del</code> <code class="n">argv</code>  <code class="c1"># Unused.</code>&#13;
  <code class="n">sc</code> <code class="o">=</code> <code class="n">SparkContext</code><code class="p">()</code>&#13;
  <code class="n">input_rdd</code> <code class="o">=</code> <code class="n">sc</code><code class="o">.</code><code class="n">textFile</code><code class="p">(</code><code class="n">FLAGS</code><code class="o">.</code><code class="n">input_file</code><code class="p">)</code>&#13;
  <code class="n">text_doc</code> <code class="o">=</code> <code class="n">parse_document</code><code class="p">(</code><code class="n">input_rdd</code><code class="p">)</code>&#13;
  <code class="n">make_token_dictionary</code><code class="p">(</code>&#13;
    <code class="n">text_doc</code><code class="p">,</code>&#13;
    <code class="n">FLAGS</code><code class="o">.</code><code class="n">token_output</code><code class="p">,</code>&#13;
    <code class="n">FLAGS</code><code class="o">.</code><code class="n">min_token_frequency</code><code class="p">,</code>&#13;
    <code class="n">FLAGS</code><code class="o">.</code><code class="n">max_token_dictionary_size</code>&#13;
  <code class="p">)</code>&#13;
  <code class="n">make_title_dictionary</code><code class="p">(</code>&#13;
    <code class="n">text_doc</code><code class="p">,</code>&#13;
    <code class="n">FLAGS</code><code class="o">.</code><code class="n">title_output</code><code class="p">,</code>&#13;
    <code class="n">FLAGS</code><code class="o">.</code><code class="n">min_title_frequency</code><code class="p">,</code>&#13;
    <code class="n">FLAGS</code><code class="o">.</code><code class="n">max_title_dictionary_size</code>&#13;
  <code class="p">)</code>&#13;
&#13;
&#13;
<code class="k">if</code> <code class="vm">__name__</code> <code class="o">==</code> <code class="s2">"__main__"</code><code class="p">:</code>&#13;
    <code class="n">app</code><code class="o">.</code><code class="n">run</code><code class="p">(</code><code class="n">main</code><code class="p">)</code></pre>&#13;
&#13;
<p>Finally, we have the subroutines called by the main function, all decomposed into smaller subroutines for counting the tokens in the article body and the titles:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">update_dict_term</code><code class="p">(</code><code class="n">term</code><code class="p">,</code> <code class="n">dictionary</code><code class="p">):</code>&#13;
    <code class="sd">"""Updates a dictionary with a term."""</code>&#13;
    <code class="k">if</code> <code class="n">term</code> <code class="ow">in</code> <code class="n">dictionary</code><code class="p">:</code>&#13;
        <code class="n">x</code> <code class="o">=</code> <code class="n">dictionary</code><code class="p">[</code><code class="n">term</code><code class="p">]</code>&#13;
    <code class="k">else</code><code class="p">:</code>&#13;
        <code class="n">x</code> <code class="o">=</code> <code class="n">nlp_pb</code><code class="o">.</code><code class="n">TokenStat</code><code class="p">()</code>&#13;
        <code class="n">x</code><code class="o">.</code><code class="n">token</code> <code class="o">=</code> <code class="n">term</code>&#13;
        <code class="n">dictionary</code><code class="p">[</code><code class="n">term</code><code class="p">]</code> <code class="o">=</code> <code class="n">x</code>&#13;
    <code class="n">x</code><code class="o">.</code><code class="n">frequency</code> <code class="o">+=</code> <code class="mi">1</code>&#13;
&#13;
&#13;
<code class="k">def</code> <code class="nf">update_dict_doc</code><code class="p">(</code><code class="n">term</code><code class="p">,</code> <code class="n">dictionary</code><code class="p">):</code>&#13;
    <code class="sd">"""Updates a dictionary with the doc frequency."""</code>&#13;
    <code class="n">dictionary</code><code class="p">[</code><code class="n">term</code><code class="p">]</code><code class="o">.</code><code class="n">doc_frequency</code> <code class="o">+=</code> <code class="mi">1</code>&#13;
&#13;
&#13;
<code class="k">def</code> <code class="nf">count_titles</code><code class="p">(</code><code class="n">doc</code><code class="p">,</code> <code class="n">title_dict</code><code class="p">):</code>&#13;
    <code class="sd">"""Counts the titles."""</code>&#13;
    <code class="c1"># Handle the titles.</code>&#13;
    <code class="n">all_titles</code> <code class="o">=</code> <code class="p">[</code><code class="n">doc</code><code class="o">.</code><code class="n">primary</code><code class="p">]</code>&#13;
    <code class="n">all_titles</code><code class="o">.</code><code class="n">extend</code><code class="p">(</code><code class="n">doc</code><code class="o">.</code><code class="n">secondary</code><code class="p">)</code>&#13;
    <code class="k">for</code> <code class="n">title</code> <code class="ow">in</code> <code class="n">all_titles</code><code class="p">:</code>&#13;
        <code class="n">update_dict_term</code><code class="p">(</code><code class="n">title</code><code class="p">,</code> <code class="n">title_dict</code><code class="p">)</code>&#13;
    <code class="n">title_set</code> <code class="o">=</code> <code class="nb">set</code><code class="p">(</code><code class="n">all_titles</code><code class="p">)</code>&#13;
    <code class="k">for</code> <code class="n">title</code> <code class="ow">in</code> <code class="n">title_set</code><code class="p">:</code>&#13;
        <code class="n">update_dict_doc</code><code class="p">(</code><code class="n">title</code><code class="p">,</code> <code class="n">title_dict</code><code class="p">)</code>&#13;
&#13;
&#13;
<code class="k">def</code> <code class="nf">count_tokens</code><code class="p">(</code><code class="n">doc</code><code class="p">,</code> <code class="n">token_dict</code><code class="p">):</code>&#13;
    <code class="sd">"""Counts the tokens."""</code>&#13;
    <code class="c1"># Handle the tokens.</code>&#13;
    <code class="k">for</code> <code class="n">term</code> <code class="ow">in</code> <code class="n">doc</code><code class="o">.</code><code class="n">tokens</code><code class="p">:</code>&#13;
        <code class="n">update_dict_term</code><code class="p">(</code><code class="n">term</code><code class="p">,</code> <code class="n">token_dict</code><code class="p">)</code>&#13;
    <code class="n">term_set</code> <code class="o">=</code> <code class="nb">set</code><code class="p">(</code><code class="n">doc</code><code class="o">.</code><code class="n">tokens</code><code class="p">)</code>&#13;
    <code class="k">for</code> <code class="n">term</code> <code class="ow">in</code> <code class="n">term_set</code><code class="p">:</code>&#13;
        <code class="n">update_dict_doc</code><code class="p">(</code><code class="n">term</code><code class="p">,</code> <code class="n">token_dict</code><code class="p">)</code>&#13;
&#13;
&#13;
<code class="k">def</code> <code class="nf">parse_document</code><code class="p">(</code><code class="n">rdd</code><code class="p">):</code>&#13;
    <code class="sd">"""Parses documents."""</code>&#13;
    <code class="k">def</code> <code class="nf">parser</code><code class="p">(</code><code class="n">x</code><code class="p">):</code>&#13;
        <code class="n">result</code> <code class="o">=</code> <code class="n">nlp_pb</code><code class="o">.</code><code class="n">TextDocument</code><code class="p">()</code>&#13;
        <code class="k">try</code><code class="p">:</code>&#13;
            <code class="n">result</code><code class="o">.</code><code class="n">ParseFromString</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>&#13;
        <code class="k">except</code> <code class="n">google</code><code class="o">.</code><code class="n">protobuf</code><code class="o">.</code><code class="n">message</code><code class="o">.</code><code class="n">DecodeError</code><code class="p">:</code>&#13;
            <code class="n">result</code> <code class="o">=</code> <code class="kc">None</code>&#13;
        <code class="k">return</code> <code class="n">result</code>&#13;
    <code class="n">output</code> <code class="o">=</code> <code class="n">rdd</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="n">base64</code><code class="o">.</code><code class="n">b64decode</code><code class="p">)</code>\&#13;
        <code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="n">parser</code><code class="p">)</code>\&#13;
        <code class="o">.</code><code class="n">filter</code><code class="p">(</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code> <code class="ow">is</code> <code class="ow">not</code> <code class="kc">None</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="n">output</code>&#13;
&#13;
&#13;
<code class="k">def</code> <code class="nf">process_partition_for_tokens</code><code class="p">(</code><code class="n">doc_iterator</code><code class="p">):</code>&#13;
    <code class="sd">"""Processes a document partition for tokens."""</code>&#13;
    <code class="n">token_dict</code> <code class="o">=</code> <code class="p">{}</code>&#13;
    <code class="k">for</code> <code class="n">doc</code> <code class="ow">in</code> <code class="n">doc_iterator</code><code class="p">:</code>&#13;
        <code class="n">count_tokens</code><code class="p">(</code><code class="n">doc</code><code class="p">,</code> <code class="n">token_dict</code><code class="p">)</code>&#13;
    <code class="k">for</code> <code class="n">token_stat</code> <code class="ow">in</code> <code class="n">token_dict</code><code class="o">.</code><code class="n">values</code><code class="p">():</code>&#13;
        <code class="k">yield</code> <code class="p">(</code><code class="n">token_stat</code><code class="o">.</code><code class="n">token</code><code class="p">,</code> <code class="n">token_stat</code><code class="p">)</code>&#13;
&#13;
&#13;
<code class="k">def</code> <code class="nf">tokenstat_reducer</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">):</code>&#13;
    <code class="sd">"""Combines two token stats together."""</code>&#13;
    <code class="n">x</code><code class="o">.</code><code class="n">frequency</code> <code class="o">+=</code> <code class="n">y</code><code class="o">.</code><code class="n">frequency</code>&#13;
    <code class="n">x</code><code class="o">.</code><code class="n">doc_frequency</code> <code class="o">+=</code> <code class="n">y</code><code class="o">.</code><code class="n">doc_frequency</code>&#13;
    <code class="k">return</code> <code class="n">x</code>&#13;
&#13;
&#13;
<code class="k">def</code> <code class="nf">make_token_dictionary</code><code class="p">(</code>&#13;
    <code class="n">text_doc</code><code class="p">,</code>&#13;
    <code class="n">token_output</code><code class="p">,</code>&#13;
    <code class="n">min_term_frequency</code><code class="p">,</code>&#13;
    <code class="n">max_token_dictionary_size</code>&#13;
<code class="p">):</code>&#13;
    <code class="sd">"""Makes the token dictionary."""</code>&#13;
    <code class="n">tokens</code> <code class="o">=</code> <code class="n">text_doc</code><code class="o">.</code><code class="n">mapPartitions</code><code class="p">(</code><code class="n">process_partition_for_tokens</code><code class="p">)</code>&#13;
        <code class="o">.</code><code class="n">reduceByKey</code><code class="p">(</code><code class="n">tokenstat_reducer</code><code class="p">)</code><code class="o">.</code><code class="n">values</code><code class="p">()</code>&#13;
    <code class="n">filtered_tokens</code> <code class="o">=</code> <code class="n">tokens</code><code class="o">.</code><code class="n">filter</code><code class="p">(</code>&#13;
        <code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code><code class="o">.</code><code class="n">frequency</code> <code class="o">&gt;=</code> <code class="n">min_term_frequency</code><code class="p">)</code>&#13;
    <code class="n">all_tokens</code> <code class="o">=</code> <code class="n">filtered_tokens</code><code class="o">.</code><code class="n">collect</code><code class="p">()</code>&#13;
    <code class="n">sorted_token_dict</code> <code class="o">=</code> <code class="nb">sorted</code><code class="p">(</code>&#13;
        <code class="n">all_tokens</code><code class="p">,</code> <code class="n">key</code><code class="o">=</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code><code class="o">.</code><code class="n">frequency</code><code class="p">,</code> <code class="n">reverse</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>&#13;
    <code class="n">count</code> <code class="o">=</code> <code class="nb">min</code><code class="p">(</code><code class="n">max_token_dictionary_size</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">sorted_token_dict</code><code class="p">))</code>&#13;
    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">count</code><code class="p">):</code>&#13;
        <code class="n">sorted_token_dict</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code class="o">.</code><code class="n">index</code> <code class="o">=</code> <code class="n">i</code>&#13;
    <code class="n">TokenDictionary</code><code class="o">.</code><code class="n">save</code><code class="p">(</code><code class="n">sorted_token_dict</code><code class="p">[:</code><code class="n">count</code><code class="p">],</code> <code class="n">token_output</code><code class="p">)</code>&#13;
&#13;
&#13;
<code class="k">def</code> <code class="nf">process_partition_for_titles</code><code class="p">(</code><code class="n">doc_iterator</code><code class="p">):</code>&#13;
    <code class="sd">"""Processes a document partition for titles."""</code>&#13;
    <code class="n">title_dict</code> <code class="o">=</code> <code class="p">{}</code>&#13;
    <code class="k">for</code> <code class="n">doc</code> <code class="ow">in</code> <code class="n">doc_iterator</code><code class="p">:</code>&#13;
        <code class="n">count_titles</code><code class="p">(</code><code class="n">doc</code><code class="p">,</code> <code class="n">title_dict</code><code class="p">)</code>&#13;
    <code class="k">for</code> <code class="n">token_stat</code> <code class="ow">in</code> <code class="n">title_dict</code><code class="o">.</code><code class="n">values</code><code class="p">():</code>&#13;
        <code class="k">yield</code> <code class="p">(</code><code class="n">token_stat</code><code class="o">.</code><code class="n">token</code><code class="p">,</code> <code class="n">token_stat</code><code class="p">)</code>&#13;
&#13;
&#13;
<code class="k">def</code> <code class="nf">make_title_dictionary</code><code class="p">(</code>&#13;
    <code class="n">text_doc</code><code class="p">,</code>&#13;
    <code class="n">title_output</code><code class="p">,</code>&#13;
    <code class="n">min_title_frequency</code><code class="p">,</code>&#13;
    <code class="n">max_title_dictionary_size</code>&#13;
<code class="p">):</code>&#13;
    <code class="sd">"""Makes the title dictionary."""</code>&#13;
    <code class="n">titles</code> <code class="o">=</code> <code class="n">text_doc</code>&#13;
      <code class="o">.</code><code class="n">mapPartitions</code><code class="p">(</code><code class="n">process_partition_for_titles</code><code class="p">)</code>&#13;
      <code class="o">.</code><code class="n">reduceByKey</code><code class="p">(</code><code class="n">tokenstat_reducer</code><code class="p">)</code><code class="o">.</code><code class="n">values</code><code class="p">()</code>&#13;
    <code class="n">filtered_titles</code> <code class="o">=</code> <code class="n">titles</code><code class="o">.</code><code class="n">filter</code><code class="p">(</code>&#13;
      <code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code><code class="o">.</code><code class="n">frequency</code> <code class="o">&gt;=</code> <code class="n">min_title_frequency</code><code class="p">)</code>&#13;
    <code class="n">all_titles</code> <code class="o">=</code> <code class="n">filtered_titles</code><code class="o">.</code><code class="n">collect</code><code class="p">()</code>&#13;
    <code class="n">sorted_title_dict</code> <code class="o">=</code> <code class="nb">sorted</code><code class="p">(</code>&#13;
      <code class="n">all_titles</code><code class="p">,</code> <code class="n">key</code><code class="o">=</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code><code class="o">.</code><code class="n">frequency</code><code class="p">,</code> <code class="n">reverse</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>&#13;
    <code class="n">count</code> <code class="o">=</code> <code class="nb">min</code><code class="p">(</code><code class="n">max_title_dictionary_size</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">sorted_title_dict</code><code class="p">))</code>&#13;
    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">count</code><code class="p">):</code>&#13;
        <code class="n">sorted_title_dict</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code class="o">.</code><code class="n">index</code> <code class="o">=</code> <code class="n">i</code>&#13;
    <code class="n">TokenDictionary</code><code class="o">.</code><code class="n">save</code><code class="p">(</code><code class="n">sorted_title_dict</code><code class="p">[:</code><code class="n">count</code><code class="p">],</code> <code class="n">title_output</code><code class="p">)</code></pre>&#13;
&#13;
<p>As you can see, Spark makes it easy to scale a program from a single machine to run on a cluster of many machines! Starting from the main function, we create <code>SparkContext</code>, read in the input file as a text file, parse it, and then make the token and title dictionaries. The RDD is passed around as arguments of the processing function and can be used multiple times and fed to various map functions (such as the token and title dictionary methods).</p>&#13;
&#13;
<p>The heavy lifting in the make-dictionary methods is done by the process-partitions functions, which are map functions that are applied to entire partitions at once. <em>Partitions</em> are<a data-primary="partitions" data-type="indexterm" id="id772"/> large chunks of the input, typically about 64 MB in size and processed as one chunk so that we save on network bandwidth by doing map-side combines. This is a technique to apply the reducer repeatedly on mapped partitions as well as after joining by the key (which in this case is the token) and summing up the counts. The reason we do this is to save on network bandwidth, which is typically the slowest part of data processing pipelines after disk access.</p>&#13;
&#13;
<p>You can view the output of the <code>make_dictionary</code> phase by using the utility <em>codex.py</em>, which dumps protocol buffers of different kinds registered in the program. Since all our data is serialized as bzipped and uuencoded text files, the only difference is which protocol buffer schema is used to decode the serialized data, so we can use just one program to print out the first few elements of the data for debugging. Although it might be much simpler to store data as JSON, XML, or CSV files, having a schema will save you from future grief because protocol buffers are extensible and support optional fields. They are also typed, which can save you from accidental mistakes in JSON, such as not knowing whether a value is a string or float or int, or having a field as a string in some files and as an int in others. Having an explicit typed schema saves us from a lot of these mistakes.</p>&#13;
&#13;
<p>The next step in the pipeline is <em>make_cooccurrence.py</em>. As the name implies, this program simply counts the number of times each token occurs with another token. This is essentially a sparse way of representing a graph. In <em>nlp.proto</em>, each row of the sparse co-occurrence matrix is as follows:</p>&#13;
&#13;
<pre data-type="programlisting">// Co-occurrence matrix row.&#13;
message CooccurrenceRow {&#13;
    uint64 index = 1;&#13;
    repeated uint64 other_index = 2;&#13;
    repeated float count = 3;&#13;
}</pre>&#13;
&#13;
<p>In<a data-primary="co-occurrence" data-secondary="co-occurrence matrix" data-type="indexterm" id="id773"/> a <em>co-occurrence matrix</em>, each row <em>i</em> has an entry at column <em>j</em> that represents the number of times token <em>j</em> has co-occurred with token <em>i</em>. This is a handy way of associating the similarity between tokens <em>i</em> and <em>j</em> because if they co-occur a lot, they must be more related to each other than tokens that do not co-occur. In the protocol buffer format, these are stored as two parallel arrays of <code>other_index</code> and <code>count</code>. We use indices because they are smaller than storing raw words, especially with the varying encoding that protocol buffers use (i.e., the matrix of rows and columns indexed by tokens, and elements that are the co-occurrences of the indices). In this encoding, small integers take fewer bits to represent than large integers; since we reverse-sorted the dictionary by frequency, the most commonly occurring tokens have the smallest indices.</p>&#13;
&#13;
<p>At this stage, if you wanted to make a very simple recommender based on frequent item similarity co-occurrence, you would look up the row for token <em>i</em> and return by count order the tokens <math alttext="j">&#13;
  <mi>j</mi>&#13;
</math>. The simple recommender would make a good variant on the popular item recommender as described in the earlier chapters.</p>&#13;
<div data-type="tip" id="CaB"><h1>Customers Also Bought</h1>&#13;
<p>This<a data-primary="most-popular-item recommender (MPIR)" data-type="indexterm" id="id774"/><a data-primary="recommendation systems" data-secondary="most-popular-item recommender (MPIR)" data-type="indexterm" id="id775"/> concept of co-occurrences will be developed further in <a data-type="xref" href="ch09.html#feature-counting">Chapter 9</a>, but let’s take a moment to reflect on this concept of the MPIR and co-occurrences. When we look at the co-occurrence matrix for items, we can take row sums or column sums to determine the number of times each item has been seen (or purchased). That was how we built the MPIR in <a data-type="xref" href="ch02.html#ch:user-item">Chapter 2</a>. If instead we look at the MPIR for a particular row corresponding to an item the user has seen, that’s simply<a data-primary="conditional MPIR" data-type="indexterm" id="id776"/> the <em>conditional MPIR</em>—i.e., the most popular item, given that the user has seen item <math alttext="i">&#13;
  <mi>i</mi>&#13;
</math>.</p>&#13;
</div>&#13;
&#13;
<p>However, here we can choose to do an embedding or low-rank representation of the co-occurrence matrix. An embedding representation of a matrix is handy because it allows us to represent each item as a vector. One way to factor the matrix is via singular value decomposition, or SVD (see <a data-type="xref" href="ch10.html#latent-spaces">“Latent Spaces”</a>), but we won’t be doing that here. Instead we will be learning GloVE embeddings, which were developed for NLP.</p>&#13;
&#13;
<p>The objective function of<a data-primary="GloVE embedding" data-type="indexterm" id="gloveem08"/><a data-primary="embedding models" data-secondary="GloVE embedding" data-type="indexterm" id="EMglove08"/> GloVE embedding is to learn two vectors such that their dot product is proportional to the log count of co-occurrence between the two vectors. The reason this loss function works is that the dot product will then be proportional to the log count of co-occurrence; thus, words that frequently occur together will have a larger dot product than words that do not. To compute the embeddings, we need to have the co-occurrence matrix handy, and luckily the previous step in the pipeline has generated such a matrix for us to process.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id777">&#13;
<h1>Feature-Item Versus Item-Item</h1>&#13;
<p>We introduce<a data-primary="feature-item recommenders" data-type="indexterm" id="id778"/><a data-primary="recommendation systems" data-secondary="feature-item recommenders" data-type="indexterm" id="id779"/> feature-item recommenders in this section via the conversion step from words to token IDs. The way we look up the embedding ID for the model is based on the index—either features or items. For the top <em>N</em> popular words, we have a one-to-one mapping from the dictionary index to the embedding ID. However, for long-tailed words, we want them to map to the same value of <code>embedding_id</code> if we can help it.</p>&#13;
&#13;
<p>One cheap way of computing a feature from a word is called<a data-primary="hashing" data-secondary="min-hashing" data-type="indexterm" id="id780"/><a data-primary="min-hashing" data-type="indexterm" id="id781"/> <em>min-hashing</em>: we find 4 consecutive bytes of a word, compute the hash of these bytes, and find the minimum hash of the overlapping 4 bytes. This process makes it more likely to relate <span>*z*e*b*r*a* h*a*s*h*e*s*</span> to zebras. This feature is then used to represent these sets of words as an equivalence class. All words that hash to the same MinHash value are in the same equivalence class. This allows us to handle any new long-tailed word naturally for the time being until a new dictionary is built. It might result in undesirable mistakes in certain applications, but for other applications where it might be safe to do so, a feature-based representation of an item might be mixed into an embedding system as we have done.</p>&#13;
&#13;
<p>Another alternate way to get feature embeddings is to train an autoencoder or some kind of embedding representation that is learned off the features of the item so that the recommender might generalize to new, unseen items. However, for the sake of simplicity and in this word embedding case, we simply use the MinHash for ease of understanding. The MinHash implementation can be seen at <a href="https://oreil.ly/CSaOY"><em>wikipedia/token_dictionary.py</em></a>.<a data-primary="" data-startref="DPbigdata08" data-type="indexterm" id="id782"/><a data-primary="" data-startref="PSexample08" data-type="indexterm" id="id783"/></p>&#13;
</div></aside>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="GloVE Model Definition" data-type="sect1"><div class="sect1" id="Glove">&#13;
<h1>GloVE Model Definition</h1>&#13;
&#13;
<p>For this section, please refer to the code at <a href="https://oreil.ly/exOH2"><em>train_coccurence.py</em></a>.</p>&#13;
&#13;
<p>Suppose we have tokens <em>i</em> and <em>j</em> from the token dictionary. We know that they have co-occurred with each other <em>N</em> times. We want to somehow generate an embedding space such that the vectors <math alttext="x left-parenthesis i right-parenthesis asterisk x left-parenthesis j right-parenthesis">&#13;
  <mrow>&#13;
    <mi>x</mi>&#13;
    <mo>(</mo>&#13;
    <mi>i</mi>&#13;
    <mo>)</mo>&#13;
    <mo>*</mo>&#13;
    <mi>x</mi>&#13;
    <mo>(</mo>&#13;
    <mi>j</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math> are proportional to log(<em>N</em>). The arguments for log count and the exact equation are derived in the <a href="https://oreil.ly/cMHB3">“GloVe: Global Vectors for Word Representation”</a> by Jeffrey Pennington et al. We will show just the derived result:</p>&#13;
<div data-type="equation">&#13;
<math alttext="y Subscript predicted Baseline equals x left-parenthesis i right-parenthesis ModifyingAbove x With dot left-parenthesis j right-parenthesis plus bias left-parenthesis i right-parenthesis plus bias left-parenthesis j right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <msub><mi>y</mi> <mtext>predicted</mtext> </msub>&#13;
    <mo>=</mo>&#13;
    <mi>x</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>i</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mover accent="true"><mi>x</mi> <mo>˙</mo></mover>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>j</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>+</mo>&#13;
    <mtext>bias</mtext>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>i</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>+</mo>&#13;
    <mtext>bias</mtext>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>j</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Here, <math alttext="x">&#13;
  <mi>x</mi>&#13;
</math> is the embedding lookup. In the code, we use 64-dimensional vectors, which are not too small as to have insufficient capacity to represent the embedding space but are not too large that it would take up too much memory when we have an embedding for the entire dictionary. The bias terms are there to soak up the large counts from very popular items such as <em>the</em>, <em>a</em>, and <em>and</em> that co-occur with many other terms.</p>&#13;
&#13;
<p class="less_space pagebreak-before">The loss we want to minimize is the squared difference between the prediction and the actual value:</p>&#13;
<div data-type="equation">&#13;
<math alttext="StartLayout 1st Row 1st Column Blank 2nd Column y Subscript target Baseline equals 1 plus log Subscript 10 Baseline left-parenthesis upper N right-parenthesis 2nd Row 1st Column Blank 2nd Column weight equals min left-parenthesis 1 comma upper N slash 100 right-parenthesis Superscript 0.75 Baseline 3rd Row 1st Column Blank 2nd Column loss equals weight asterisk left-parenthesis y Subscript predicted Baseline minus y Subscript target Baseline right-parenthesis squared EndLayout" display="block">&#13;
  <mtable displaystyle="true">&#13;
    <mtr>&#13;
      <mtd/>&#13;
      <mtd columnalign="left">&#13;
        <mrow>&#13;
          <msub><mi>y</mi> <mtext>target</mtext> </msub>&#13;
          <mo>=</mo>&#13;
          <mn>1</mn>&#13;
          <mo>+</mo>&#13;
          <msub><mtext>log</mtext> <mn>10</mn> </msub>&#13;
          <mrow>&#13;
            <mo>(</mo>&#13;
            <mi>N</mi>&#13;
            <mo>)</mo>&#13;
          </mrow>&#13;
        </mrow>&#13;
      </mtd>&#13;
    </mtr>&#13;
    <mtr>&#13;
      <mtd/>&#13;
      <mtd columnalign="left">&#13;
        <mrow>&#13;
          <mtext>weight</mtext>&#13;
          <mo>=</mo>&#13;
          <mtext>min</mtext>&#13;
          <msup><mfenced close=")" open="(" separators=""><mn>1</mn><mo>,</mo><mi>N</mi><mo>/</mo><mn>100</mn></mfenced> <mrow><mn>0</mn><mo>.</mo><mn>75</mn></mrow> </msup>&#13;
        </mrow>&#13;
      </mtd>&#13;
    </mtr>&#13;
    <mtr>&#13;
      <mtd/>&#13;
      <mtd columnalign="left">&#13;
        <mrow>&#13;
          <mtext>loss</mtext>&#13;
          <mo>=</mo>&#13;
          <mtext>weight</mtext>&#13;
          <mo>*</mo>&#13;
          <msup><mrow><mo>(</mo><msub><mi>y</mi> <mtext>predicted</mtext> </msub><mo>-</mo><msub><mi>y</mi> <mtext>target</mtext> </msub><mo>)</mo></mrow> <mn>2</mn> </msup>&#13;
        </mrow>&#13;
      </mtd>&#13;
    </mtr>&#13;
  </mtable>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>The weighting term in the loss function is to prevent domination by very popular co-occurrences as well as to downweight rarer co-occurrences.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="GloVE Model Specification in JAX and Flax" data-type="sect2"><div class="sect2" id="id95">&#13;
<h2>GloVE Model Specification in JAX and Flax</h2>&#13;
&#13;
<p>Let’s<a data-primary="JAX framework" data-secondary="GloVE model specification" data-type="indexterm" id="JAXglove08"/><a data-primary="Flax framework" data-secondary="GloVE model specification" data-type="indexterm" id="FFglove08"/> look at the implementation of the GloVE model based on JAX and Flax. This is in the file <em>wikipedia/models.py</em> on the GitHub repository:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">flax</code>&#13;
<code class="kn">from</code> <code class="nn">flax</code> <code class="kn">import</code> <code class="n">linen</code> <code class="k">as</code> <code class="n">nn</code>&#13;
<code class="kn">from</code> <code class="nn">flax.training</code> <code class="kn">import</code> <code class="n">train_state</code>&#13;
<code class="kn">import</code> <code class="nn">jax</code>&#13;
<code class="kn">import</code> <code class="nn">jax.numpy</code> <code class="k">as</code> <code class="nn">jnp</code>&#13;
&#13;
&#13;
<code class="k">class</code> <code class="nc">Glove</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>&#13;
    <code class="sd">"""A simple embedding model based on gloVe.</code>&#13;
<code class="sd">       https://nlp.stanford.edu/projects/glove/</code>&#13;
<code class="sd">    """</code>&#13;
    <code class="n">num_embeddings</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">1024</code>&#13;
    <code class="n">features</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">64</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">setup</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">_token_embedding</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Embed</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">num_embeddings</code><code class="p">,</code>&#13;
                                         <code class="bp">self</code><code class="o">.</code><code class="n">features</code><code class="p">)</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">_bias</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Embed</code><code class="p">(</code>&#13;
            <code class="bp">self</code><code class="o">.</code><code class="n">num_embeddings</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="n">embedding_init</code><code class="o">=</code><code class="n">flax</code><code class="o">.</code><code class="n">linen</code><code class="o">.</code><code class="n">initializers</code><code class="o">.</code><code class="n">zeros</code><code class="p">)</code>&#13;
&#13;
    <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">inputs</code><code class="p">):</code>&#13;
        <code class="sd">"""Calculates the approximate log count between tokens 1 and 2.</code>&#13;
<code class="sd">        Args:</code>&#13;
<code class="sd">          A batch of (token1, token2) integers representing co-occurence.</code>&#13;
<code class="sd">        Returns:</code>&#13;
<code class="sd">          Approximate log count between x and y.</code>&#13;
<code class="sd">        """</code>&#13;
        <code class="n">token1</code><code class="p">,</code> <code class="n">token2</code> <code class="o">=</code> <code class="n">inputs</code>&#13;
        <code class="n">embed1</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">_token_embedding</code><code class="p">(</code><code class="n">token1</code><code class="p">)</code>&#13;
        <code class="n">bias1</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">_bias</code><code class="p">(</code><code class="n">token1</code><code class="p">)</code>&#13;
        <code class="n">embed2</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">_token_embedding</code><code class="p">(</code><code class="n">token2</code><code class="p">)</code>&#13;
        <code class="n">bias2</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">_bias</code><code class="p">(</code><code class="n">token2</code><code class="p">)</code>&#13;
        <code class="n">dot_vmap</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">vmap</code><code class="p">(</code><code class="n">jnp</code><code class="o">.</code><code class="n">dot</code><code class="p">,</code> <code class="n">in_axes</code><code class="o">=</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">out_axes</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>&#13;
        <code class="n">dot</code> <code class="o">=</code> <code class="n">dot_vmap</code><code class="p">(</code><code class="n">embed1</code><code class="p">,</code> <code class="n">embed2</code><code class="p">)</code>&#13;
        <code class="n">output</code> <code class="o">=</code> <code class="n">dot</code> <code class="o">+</code> <code class="n">bias1</code> <code class="o">+</code> <code class="n">bias2</code>&#13;
        <code class="k">return</code> <code class="n">output</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">score_all</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">token</code><code class="p">):</code>&#13;
        <code class="sd">"""Finds the score of token vs all tokens.</code>&#13;
<code class="sd">        Args:</code>&#13;
<code class="sd">          max_count: The maximum count of tokens to return.</code>&#13;
<code class="sd">          token: Integer index of token to find neighbors of.</code>&#13;
<code class="sd">        Returns:</code>&#13;
<code class="sd">          Scores of nearest tokens.</code>&#13;
<code class="sd">        """</code>&#13;
        <code class="n">embed1</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">_token_embedding</code><code class="p">(</code><code class="n">token</code><code class="p">)</code>&#13;
        <code class="n">all_tokens</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">num_embeddings</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="n">jnp</code><code class="o">.</code><code class="n">int32</code><code class="p">)</code>&#13;
        <code class="n">all_embeds</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">_token_embedding</code><code class="p">(</code><code class="n">all_tokens</code><code class="p">)</code>&#13;
        <code class="n">dot_vmap</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">vmap</code><code class="p">(</code><code class="n">jnp</code><code class="o">.</code><code class="n">dot</code><code class="p">,</code> <code class="n">in_axes</code><code class="o">=</code><code class="p">[</code><code class="kc">None</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">out_axes</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>&#13;
        <code class="n">scores</code> <code class="o">=</code> <code class="n">dot_vmap</code><code class="p">(</code><code class="n">embed1</code><code class="p">,</code> <code class="n">all_embeds</code><code class="p">)</code>&#13;
        <code class="k">return</code> <code class="n">scores</code></pre>&#13;
&#13;
<p>Flax is rather simple to use; all networks inherit from Flax’s linen neural network library and are modules. Flax modules are also Python dataclasses, so any hyper-parameters for the module are defined at the start of the module as variables. We have only two for this simple model: the number of embeddings we want, which corresponds to the number of tokens in the dictionary, and the dimension of the embedding vectors. Next, in the setup of the module, we actually create the layers we want, which is just the bias term and embedding for each token.</p>&#13;
&#13;
<p>The next part of the definition is the default method that is called when we use this module. In this case, we want to pass in a pair of tokens, <em>i</em>, <em>j</em>; convert them to embeddings, <math alttext="x left-parenthesis i right-parenthesis comma x left-parenthesis j right-parenthesis">&#13;
  <mrow>&#13;
    <mi>x</mi>&#13;
    <mo>(</mo>&#13;
    <mi>i</mi>&#13;
    <mo>)</mo>&#13;
    <mo>,</mo>&#13;
    <mi>x</mi>&#13;
    <mo>(</mo>&#13;
    <mi>j</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>; and then compute the predicted log(<em>count</em>(<em>y</em><sub>predicted</sub>)).</p>&#13;
&#13;
<p>In this section of code, we encounter the first difference between JAX and <span class="keep-together">NumPy—namely,</span> a<a data-primary="vectorized map (vmap)" data-type="indexterm" id="id784"/> vectorized map, or <code>vmap</code>. A <code>vmap</code> takes in a function and applies it in the same way across axes of tensors; this makes coding easier because you just have to think about how the original function operates on lower-rank tensors such as vectors. In this example, since we are passing in batches of pairs of tokens and then embedding them, we actually have a batch of vectors, and so we want to run the dot product over the batch dimension. We pass in JAX’s dot function, which takes vectors, run it over the batch dimension (which is axis 0), and tell <code>vmap</code> to return the outputs as another batch dimension as axis 0. This allows us to efficiently and simply write code for lower-dimensional tensors and obtain a function that can operate on higher-dimensional tensors by <code>vmap</code>ping over the extra axes. Conceptually, it would be as if we looped over the first dimension and returned an array of the dot products. However, by converting this process to a function, we allow JAX to push this loop into JITable code that can be compiled to run fast on a GPU.</p>&#13;
&#13;
<p>Finally, we also declare the helper function <a href="https://oreil.ly/-zYon"><code>score_all</code></a>, which takes one token and scores it against all the other tokens. Again, we use <code>vmap</code> to take the dot product with the particular token <math alttext="x left-parenthesis i right-parenthesis">&#13;
  <mrow>&#13;
    <mi>x</mi>&#13;
    <mo>(</mo>&#13;
    <mi>i</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math> but run it against all the other token embeddings. The difference here is that since <math alttext="x left-parenthesis i right-parenthesis">&#13;
  <mrow>&#13;
    <mi>x</mi>&#13;
    <mo>(</mo>&#13;
    <mi>i</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math> is already a vector, we don’t need to <code>vmap</code> over it. Therefore, in <code>in_axes</code>, we supply <code>[None, 0]</code>, which means don’t <code>vmap</code> over the axes of the first argument but instead <code>vmap</code> over axis 0 of the second argument, which is the batch of all the embeddings of all the tokens. Then we return the result, which is an array that is the dot product of <math alttext="x left-parenthesis i right-parenthesis">&#13;
  <mrow>&#13;
    <mi>x</mi>&#13;
    <mo>(</mo>&#13;
    <mi>i</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math> against all other embeddings but without the bias terms. We don’t use the bias term in scoring because it was used in part to soak up the popularity of very common tokens, and our scoring function would be more interesting if we just used the dot product part of it for scoring.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="GloVE Model Training with Optax" data-type="sect2"><div class="sect2" id="id96">&#13;
<h2>GloVE Model Training with Optax</h2>&#13;
&#13;
<p>Next, let’s<a data-primary="Optax library" data-type="indexterm" id="id785"/> take a look at <a href="https://oreil.ly/A1o24"><em>wikipedia/train_coocurrence.py</em></a>. Let’s look specifically at the part where the model is called to dig into some JAX specifics:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="nd">@jax</code><code class="o">.</code><code class="n">jit</code>&#13;
<code class="k">def</code> <code class="nf">apply_model</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">inputs</code><code class="p">,</code> <code class="n">target</code><code class="p">):</code>&#13;
    <code class="sd">"""Computes the gradients and loss for a single batch."""</code>&#13;
&#13;
    <code class="c1"># Define glove loss.</code>&#13;
    <code class="k">def</code> <code class="nf">glove_loss</code><code class="p">(</code><code class="n">params</code><code class="p">):</code>&#13;
        <code class="sd">"""The GloVe weighted loss."""</code>&#13;
        <code class="n">predicted</code> <code class="o">=</code> <code class="n">state</code><code class="o">.</code><code class="n">apply_fn</code><code class="p">({</code><code class="s1">'params'</code><code class="p">:</code> <code class="n">params</code><code class="p">},</code> <code class="n">inputs</code><code class="p">)</code>&#13;
        <code class="n">ones</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">ones_like</code><code class="p">(</code><code class="n">target</code><code class="p">)</code>&#13;
        <code class="n">weight</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">minimum</code><code class="p">(</code><code class="n">ones</code><code class="p">,</code> <code class="n">target</code> <code class="o">/</code> <code class="mf">100.0</code><code class="p">)</code>&#13;
        <code class="n">weight</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">power</code><code class="p">(</code><code class="n">weight</code><code class="p">,</code> <code class="mf">0.75</code><code class="p">)</code>&#13;
        <code class="n">log_target</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">log10</code><code class="p">(</code><code class="mf">1.0</code> <code class="o">+</code> <code class="n">target</code><code class="p">)</code>&#13;
        <code class="n">loss</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">jnp</code><code class="o">.</code><code class="n">square</code><code class="p">(</code><code class="n">log_target</code> <code class="o">-</code> <code class="n">predicted</code><code class="p">)</code> <code class="o">*</code> <code class="n">weight</code><code class="p">)</code>&#13;
        <code class="k">return</code> <code class="n">loss</code>&#13;
&#13;
    <code class="n">grad_fn</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">value_and_grad</code><code class="p">(</code><code class="n">glove_loss</code><code class="p">)</code>&#13;
    <code class="n">loss</code><code class="p">,</code> <code class="n">grads</code> <code class="o">=</code> <code class="n">grad_fn</code><code class="p">(</code><code class="n">state</code><code class="o">.</code><code class="n">params</code><code class="p">)</code>&#13;
&#13;
    <code class="k">return</code> <code class="n">grads</code><code class="p">,</code> <code class="n">loss</code></pre>&#13;
&#13;
<p>The first point you will notice is the function decorator, <code>@jax.jit</code>. This tells JAX that everything in the function is JITable. There are some requirements for a function to be JITable—mostly that it is pure, which is a computer science term indicating that if you call a function with the same arguments, you would expect the same result. That function should not have any side effects and shouldn’t rely on a cached state such as a private counter or random-number generator with implicit state. The tensors that are passed in as arguments should probably also have fixed shape, because every new shape would trigger a new JIT compilation. You can give hints to the compiler that certain parameters are constants with <code>static_argnums</code>, but these arguments shouldn’t change too frequently, or else a lot of time will be spent compiling a program for each of these constants.</p>&#13;
&#13;
<p>One consequence of this pure function philosophy is that the model structure and model parameters are separated. This way, the model functions are pure and the parameters are passed in to the model functions, allowing the model functions to be jitted. This is why we apply the model’s <code>apply_fn</code> to the parameters rather than simply having the parameters as part of the model.</p>&#13;
&#13;
<p>This <code>apply_model</code> function can then be compiled to implement the GloVE loss that we described earlier. The other new functionality that JAX provides above NumPy is automatically computing gradients of functions. The JAX function <code>value_and_grad</code> computes the gradient of the loss with respect to the parameters. Since the gradient always points in the direction in which the loss increases, we can use gradient descent to go the other way and minimize the loss. The Optax library has a few optimizers to pick from, including SGD (stochastic gradient descent with momentum) and ADAM.</p>&#13;
&#13;
<p>When you run the training program, it will loop over the co-occurence matrix and try to generate a succinct form of it by using the GloVE loss function. After about an hour, you should be able to see the highest-scoring term.</p>&#13;
&#13;
<p>The nearest neighbors for “democracy,” for example, are as follows: democracy:1.064498, liberal:1.024733, reform:1.000746, affairs:0.961664, socialist:0.952792, organizations:0.935910, political:0.919937, policy:0.917884, policies:0.907138, and <span class="keep-together">--date:0.889342.</span></p>&#13;
&#13;
<p>As you can see, the query token itself is usually the highest-scoring neighbor, but this is not necessarily true, as a very popular token might actually be higher scoring to the token than the query token itself.<a data-primary="" data-startref="gloveem08" data-type="indexterm" id="id786"/><a data-primary="" data-startref="EMglove08" data-type="indexterm" id="id787"/><a data-primary="" data-startref="JAXglove08" data-type="indexterm" id="id788"/><a data-primary="" data-startref="FFglove08" data-type="indexterm" id="id789"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect2"><div class="sect2" id="id311">&#13;
<h2>Summary</h2>&#13;
&#13;
<p>After reading this chapter, you should have a good overview of the basic ingredients for assembling a recommender system. You have seen how to set up a basic Python development environment; manage packages; specify inputs and outputs with flags; encode data in various ways, including using protocol buffers; and process the data with a distributed framework with PySpark. You also learned how to compress gigabytes of data into a few megabytes of a model that is able to generalize and quickly score items, given a query item.</p>&#13;
&#13;
<p>Take some time to play with the code and read the documentation of the various packages referenced to get a good sense of the basics. These foundational examples have widespread applications, and having a firm grasp on them will make your production environments more accurate.</p>&#13;
</div></section>&#13;
</div></section>&#13;
</div></section></body></html>