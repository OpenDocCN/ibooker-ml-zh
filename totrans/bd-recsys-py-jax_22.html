<html><head></head><body><section data-pdf-bookmark="Chapter 17. Sequential Recommenders" data-type="chapter" epub:type="chapter"><div class="chapter" id="Attention">&#13;
<h1><span class="label">Chapter 17. </span>Sequential Recommenders</h1>&#13;
&#13;
&#13;
<p>In<a data-primary="recommendation systems" data-secondary="sequential recommenders" data-type="indexterm" id="RSsequential17"/> our journey so far, you’ve learned about a variety of features that appear as explicit or as latent components in the recommendation problem. One kind of feature, which has appeared implicitly, is the history of previous recommendations and interactions. You may wish to protest here: “All of the work we’ve done so far considers the previous recommendations and interactions! We’ve even learned about prequential training data.”</p>&#13;
&#13;
<p>That<a data-primary="sequential recommenders" data-secondary="challenges of" data-type="indexterm" id="id1187"/> is true, but it fails to account for more explicit relationships between the <em>sequence of recommendations leading up to the inference request</em>. Let’s look at an example to distinguish the two. Your video-streaming website knows that you’ve previously seen all of Darren Aronofsky’s films, so when <em>The Whale</em> is released, the website is very likely to recommend it. But this type of recommendation is different from one you might receive after finishing episode 10 of <em>Succession</em>. You may have been watching Aronofsky films over a long time period—<em>Pi</em> many years ago and <em>Black Swan</em> earlier this year. But you have been watching an episode of <em>Succession</em> each night this week, and your entire recent history is made up of Logan Roy. This latter example is a sequential recommendation problem: using the most recent ordered list of interactions to predict what you’ll enjoy next.</p>&#13;
&#13;
<p>In terms of the modeling objective, the recommenders we’ve seen use pairwise relationships between potential recommendations and historical interactions. Sequential recommendation aims to predict users’ next actions based on the sequential interactions in the past that may be of much higher <em>order</em>—i.e., combinations of interactions among three or more items. Most sequential recommendation models involve sequential data-mining techniques such as Markov chains, recurrent neural networks (RNNs), and self-attention. These models usually take into consideration short-term user behavior and are less sensitive, even oblivious, to the global user preferences that have stabilized over time.</p>&#13;
&#13;
<p>Initial work in sequential recommendations focused on modeling the transitions between successive items. These used Markov chains and translation-based methods. As deep learning methods showed more and more promise in modeling sequential data—such as their biggest success in NLP—there have been many attempts to use neural network architectures to model sequential dynamics of a user’s interaction history. Early successes in this direction include GRU4Rec using an RNN to model users’ sequential interactions. Recently, transformer architectures have demonstrated superior performance for sequential data modeling. The transformer architecture lends itself to efficient parallelization and is effective at modeling long-range <span class="keep-together">sequences.</span></p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Markov Chains" data-type="sect1"><div class="sect1" id="id173">&#13;
<h1>Markov Chains</h1>&#13;
&#13;
<p>Despite<a data-primary="sequential recommenders" data-secondary="Markov chains" data-type="indexterm" id="SRmarkov17"/><a data-primary="Markov chains" data-type="indexterm" id="markovchains17"/> mining for relationships to historical recommendations, the models we’ve been considering often fail to capture sequential patterns in user behavior, thereby disregarding the chronological order of user interactions. To address this shortcoming, sequential recommender systems were developed, incorporating techniques like Markov chains to model the temporal dependencies between items.</p>&#13;
&#13;
<p>A <em>Markov chain</em> is<a data-primary="memorylessness principle" data-type="indexterm" id="id1188"/> a stochastic model that operates on the principle of <em>memorylessness</em>. It models the probability of transitioning from one state to another—given the current state—without considering the sequence of preceding events. Markov chains model the sequential behavior of users by considering each state as an item, and the transition probabilities as the likelihood of a user interacting with a certain item after the current one.</p>&#13;
&#13;
<p>The first-order Markov chain, in which the future state depends solely on the current state, was a common strategy in early sequential recommenders. Despite its simplicity, the first-order Markov chain is effective in capturing short-term, item-to-item transition patterns, improving the quality of recommendations over nonsequential methods.</p>&#13;
&#13;
<p>Take, for example, our preceding <em>Succession</em> example. If you’re using only a first-order Markov chain, a really great heuristic would be “What is the next episode in the series, if it’s a series; otherwise, fall back on a collaborative filtering (CF) model.” You can see that for a huge percentage of watch hours, this naive first-order chain would simply tell the user to watch the next episode of a show. Not particularly enlightening, but a good sign. When you abstract this out further, you start to get more powerful methods.</p>&#13;
&#13;
<p>The first-order assumption does not always hold in real-world applications, as user behavior is often influenced by a longer history of interactions. To overcome this limitation, higher-order Markov chains look further back: the next state is determined by a set of previous states, providing a richer model of user behavior. Nevertheless, it’s crucial to select the appropriate order, as too high an order may lead to overfitting and sparsity of the transition matrix.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Order-Two Markov Chain" data-type="sect2"><div class="sect2" id="id174">&#13;
<h2>Order-Two Markov Chain</h2>&#13;
&#13;
<p>Let’s consider an example of an<a data-primary="order-two Markov chain" data-type="indexterm" id="id1189"/> <em>order-two Markov chain</em> model using the weather. Assume we have three states: sunny (<math alttext="upper S">&#13;
  <mi>S</mi>&#13;
</math>), cloudy (<math alttext="upper C">&#13;
  <mi>C</mi>&#13;
</math>), and rainy (<math alttext="upper R">&#13;
  <mi>R</mi>&#13;
</math>).</p>&#13;
&#13;
<p>In an order-two Markov chain, the weather of today (<math alttext="t">&#13;
  <mi>t</mi>&#13;
</math>) would depend on the weather of yesterday (<math alttext="t minus 1">&#13;
  <mrow>&#13;
    <mi>t</mi>&#13;
    <mo>-</mo>&#13;
    <mn>1</mn>&#13;
  </mrow>&#13;
</math>) and the day before yesterday (<math alttext="t minus 2">&#13;
  <mrow>&#13;
    <mi>t</mi>&#13;
    <mo>-</mo>&#13;
    <mn>2</mn>&#13;
  </mrow>&#13;
</math>). The transition probability can be denoted as <math alttext="upper P left-parenthesis upper S Subscript t Baseline vertical-bar upper S Subscript t minus 1 Baseline comma upper S Subscript t minus 2 Baseline right-parenthesis">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <msub><mi>S</mi> <mi>t</mi> </msub>&#13;
    <mo>|</mo>&#13;
    <msub><mi>S</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow> </msub>&#13;
    <mo>,</mo>&#13;
    <msub><mi>S</mi> <mrow><mi>t</mi><mo>-</mo><mn>2</mn></mrow> </msub>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>.</p>&#13;
&#13;
<p>The Markov chain can be defined by a transition matrix that provides the probabilities of transitioning from one state to another. However, because we’re dealing with an order-two Markov chain, we would have a transition tensor instead. For simplicity, let’s say we have the following transition probabilities:</p>&#13;
<div data-type="equation">&#13;
<math alttext="StartLayout 1st Row 1st Column upper P left-parenthesis upper S vertical-bar upper S comma upper S right-parenthesis 2nd Column equals 0.7 comma upper P left-parenthesis upper C vertical-bar upper S comma upper S right-parenthesis equals 0.2 comma upper P left-parenthesis upper R vertical-bar upper S comma upper S right-parenthesis equals 0.1 comma 2nd Row 1st Column upper P left-parenthesis upper S vertical-bar upper S comma upper C right-parenthesis 2nd Column equals 0.3 comma upper P left-parenthesis upper C vertical-bar upper S comma upper C right-parenthesis equals 0.4 comma upper P left-parenthesis upper R vertical-bar upper S comma upper C right-parenthesis equals 0.3 comma 3rd Row 1st Column Blank 2nd Column ellipsis EndLayout" display="block">&#13;
  <mtable displaystyle="true">&#13;
    <mtr>&#13;
      <mtd columnalign="right">&#13;
        <mrow>&#13;
          <mi>P</mi>&#13;
          <mo>(</mo>&#13;
          <mi>S</mi>&#13;
          <mo>|</mo>&#13;
          <mi>S</mi>&#13;
          <mo>,</mo>&#13;
          <mi>S</mi>&#13;
          <mo>)</mo>&#13;
        </mrow>&#13;
      </mtd>&#13;
      <mtd>&#13;
        <mrow>&#13;
          <mo>=</mo>&#13;
          <mn>0</mn>&#13;
          <mo>.</mo>&#13;
          <mn>7</mn>&#13;
          <mo>,</mo>&#13;
          <mi>P</mi>&#13;
          <mo>(</mo>&#13;
          <mi>C</mi>&#13;
          <mo>|</mo>&#13;
          <mi>S</mi>&#13;
          <mo>,</mo>&#13;
          <mi>S</mi>&#13;
          <mo>)</mo>&#13;
          <mo>=</mo>&#13;
          <mn>0</mn>&#13;
          <mo>.</mo>&#13;
          <mn>2</mn>&#13;
          <mo>,</mo>&#13;
          <mi>P</mi>&#13;
          <mo>(</mo>&#13;
          <mi>R</mi>&#13;
          <mo>|</mo>&#13;
          <mi>S</mi>&#13;
          <mo>,</mo>&#13;
          <mi>S</mi>&#13;
          <mo>)</mo>&#13;
          <mo>=</mo>&#13;
          <mn>0</mn>&#13;
          <mo>.</mo>&#13;
          <mn>1</mn>&#13;
          <mo>,</mo>&#13;
        </mrow>&#13;
      </mtd>&#13;
    </mtr>&#13;
    <mtr>&#13;
      <mtd columnalign="right">&#13;
        <mrow>&#13;
          <mi>P</mi>&#13;
          <mo>(</mo>&#13;
          <mi>S</mi>&#13;
          <mo>|</mo>&#13;
          <mi>S</mi>&#13;
          <mo>,</mo>&#13;
          <mi>C</mi>&#13;
          <mo>)</mo>&#13;
        </mrow>&#13;
      </mtd>&#13;
      <mtd>&#13;
        <mrow>&#13;
          <mo>=</mo>&#13;
          <mn>0</mn>&#13;
          <mo>.</mo>&#13;
          <mn>3</mn>&#13;
          <mo>,</mo>&#13;
          <mi>P</mi>&#13;
          <mo>(</mo>&#13;
          <mi>C</mi>&#13;
          <mo>|</mo>&#13;
          <mi>S</mi>&#13;
          <mo>,</mo>&#13;
          <mi>C</mi>&#13;
          <mo>)</mo>&#13;
          <mo>=</mo>&#13;
          <mn>0</mn>&#13;
          <mo>.</mo>&#13;
          <mn>4</mn>&#13;
          <mo>,</mo>&#13;
          <mi>P</mi>&#13;
          <mo>(</mo>&#13;
          <mi>R</mi>&#13;
          <mo>|</mo>&#13;
          <mi>S</mi>&#13;
          <mo>,</mo>&#13;
          <mi>C</mi>&#13;
          <mo>)</mo>&#13;
          <mo>=</mo>&#13;
          <mn>0</mn>&#13;
          <mo>.</mo>&#13;
          <mn>3</mn>&#13;
          <mo>,</mo>&#13;
        </mrow>&#13;
      </mtd>&#13;
    </mtr>&#13;
    <mtr>&#13;
      <mtd/>&#13;
      <mtd>&#13;
        <mo>...</mo>&#13;
      </mtd>&#13;
    </mtr>&#13;
  </mtable>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>You can visualize these probabilities in a three-dimensional cube. The first two dimensions represent the state of today and yesterday, and the third dimension represents the possible states of tomorrow.</p>&#13;
&#13;
<p>If the weather was sunny for the last two days and we want to predict the weather for tomorrow, we would look at the transition probabilities starting with <math alttext="left-parenthesis upper S comma upper S right-parenthesis">&#13;
  <mrow>&#13;
    <mo>(</mo>&#13;
    <mi>S</mi>&#13;
    <mo>,</mo>&#13;
    <mi>S</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>, which are <math alttext="upper P left-parenthesis upper S vertical-bar upper S comma upper S right-parenthesis equals 0.7">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>S</mi>&#13;
    <mo>|</mo>&#13;
    <mi>S</mi>&#13;
    <mo>,</mo>&#13;
    <mi>S</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mn>0</mn>&#13;
    <mo>.</mo>&#13;
    <mn>7</mn>&#13;
  </mrow>&#13;
</math>, <math alttext="upper P left-parenthesis upper C vertical-bar upper S comma upper S right-parenthesis equals 0.2">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>C</mi>&#13;
    <mo>|</mo>&#13;
    <mi>S</mi>&#13;
    <mo>,</mo>&#13;
    <mi>S</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mn>0</mn>&#13;
    <mo>.</mo>&#13;
    <mn>2</mn>&#13;
  </mrow>&#13;
</math>, and <math alttext="upper P left-parenthesis upper R vertical-bar upper S comma upper S right-parenthesis equals 0.1">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>R</mi>&#13;
    <mo>|</mo>&#13;
    <mi>S</mi>&#13;
    <mo>,</mo>&#13;
    <mi>S</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mn>0</mn>&#13;
    <mo>.</mo>&#13;
    <mn>1</mn>&#13;
  </mrow>&#13;
</math>. Therefore, according to our model, there’s a 70% chance that it will be sunny, a 20% chance that it will be cloudy, and a 10% chance that it will be rainy.</p>&#13;
&#13;
<p>The probabilities in the transition matrix (or tensor) are typically estimated from data. If you have a historical record of the weather for several years, you can count the number of times each transition occurs and divide by the total number of transitions to estimate the probability.</p>&#13;
&#13;
<p>This is only a basic demonstration of an order-two Markov chain. In real applications, the states might be much more numerous and the transition matrix much larger, but the principles remain the same.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Other Markov Models" data-type="sect2"><div class="sect2" id="id175">&#13;
<h2>Other Markov Models</h2>&#13;
&#13;
<p>A more advanced Markovian approach is the<a data-primary="MDP (Markov decision process)" data-type="indexterm" id="id1190"/><a data-primary="Markov decision process (MDP)" data-type="indexterm" id="id1191"/> <em>Markov decision process</em> (<em>MDP</em>), which extends the Markov chain by introducing actions and rewards. In the context of recommender systems, each action could represent a recommendation, and the reward could be the user’s response to the recommendation. By incorporating user feedback, the MDP can learn more personalized recommendation strategies.</p>&#13;
&#13;
<p>MDPs are defined by a tuple <math alttext="left-parenthesis upper S comma upper A comma upper P comma upper R right-parenthesis">&#13;
  <mrow>&#13;
    <mo>(</mo>&#13;
    <mi>S</mi>&#13;
    <mo>,</mo>&#13;
    <mi>A</mi>&#13;
    <mo>,</mo>&#13;
    <mi>P</mi>&#13;
    <mo>,</mo>&#13;
    <mi>R</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>, where <math alttext="upper S">&#13;
  <mi>S</mi>&#13;
</math> is the set of states, <math alttext="upper A">&#13;
  <mi>A</mi>&#13;
</math> is the set of actions, <math alttext="upper P">&#13;
  <mi>P</mi>&#13;
</math> is the state transition probability matrix, and <math alttext="upper R">&#13;
  <mi>R</mi>&#13;
</math> is the reward function.</p>&#13;
&#13;
<p>Let’s use a simplified MDP for a movie recommender system as an example:</p>&#13;
<dl>&#13;
<dt>States (<math alttext="upper S">&#13;
  <mi>S</mi>&#13;
</math>)</dt>&#13;
<dd>&#13;
<p>These could represent the genres of movies a user has watched in the past. For simplicity, let’s say we have three states: Comedy (<math alttext="upper C">&#13;
  <mi>C</mi>&#13;
</math>), Drama (<math alttext="upper D">&#13;
  <mi>D</mi>&#13;
</math>), and Action (<math alttext="upper A">&#13;
  <mi>A</mi>&#13;
</math>).</p>&#13;
</dd>&#13;
<dt>Actions (<math alttext="upper A">&#13;
  <mi>A</mi>&#13;
</math>)</dt>&#13;
<dd>&#13;
<p>These could represent the movies that can be recommended. For this example, let’s say we have five actions (movies): Movies 1, 2, 3, 4, and 5.</p>&#13;
</dd>&#13;
<dt>Transition probabilities (<math alttext="upper P">&#13;
  <mi>P</mi>&#13;
</math>)</dt>&#13;
<dd>&#13;
<p>This represents the likelihood of transitioning from one state to another, given a specific action. For instance, if the user just watched a Drama (<math alttext="upper D">&#13;
  <mi>D</mi>&#13;
</math>) and we recommend Movie 3 (which is an Action movie), the transition probability <math alttext="upper P left-parenthesis upper A vertical-bar upper D comma upper M o v i e Baseline 3 right-parenthesis">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>A</mi>&#13;
    <mo>|</mo>&#13;
    <mi>D</mi>&#13;
    <mo>,</mo>&#13;
    <mi>M</mi>&#13;
    <mi>o</mi>&#13;
    <mi>v</mi>&#13;
    <mi>i</mi>&#13;
    <mi>e</mi>&#13;
    <mn>3</mn>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math> might be 0.6, indicating a 60% chance the user will watch another Action movie.</p>&#13;
</dd>&#13;
<dt>Rewards (<math alttext="upper R">&#13;
  <mi>R</mi>&#13;
</math>)</dt>&#13;
<dd>&#13;
<p>This is the feedback from the user after taking an action (recommendation). Let’s assume for simplicity that a user’s click on a recommended movie gives a reward of +1 and no click is a reward of 0.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>The aim of the recommender system in this context is to learn a policy <math alttext="pi colon upper S right-arrow upper A">&#13;
  <mrow>&#13;
    <mi>π</mi>&#13;
    <mo>:</mo>&#13;
    <mi>S</mi>&#13;
    <mo>→</mo>&#13;
    <mi>A</mi>&#13;
  </mrow>&#13;
</math> that maximizes the expected cumulative reward. A policy dictates which action the agent (the recommender system) should take in each state.</p>&#13;
&#13;
<p>This policy can be learned via reinforcement learning algorithms, such as Q-learning or policy iteration, which essentially learn the value of taking an action in a state (i.e., recommending a movie after the user has watched a certain genre), considering the immediate reward and the potential future rewards.</p>&#13;
&#13;
<p>The main challenge in a real-world recommender system scenario is that both the state and action spaces are extremely large, and the transition dynamics and reward function can be complex and difficult to estimate accurately. But, the principles demonstrated in this simple example remain the same.</p>&#13;
&#13;
<p>Despite the promising performance of Markov chain-based recommender systems, several challenges remain. The <em>memorylessness</em> assumption of the Markov chain may not hold in certain scenarios where long-term dependencies exist. Furthermore, most Markov chain models treat user-item interactions as binary events (either interaction or no interaction), which oversimplifies the variety of interactions users may have with items, such as browsing, clicking, and purchasing.<a data-primary="" data-startref="SRmarkov17" data-type="indexterm" id="id1192"/><a data-primary="" data-startref="markovchains17" data-type="indexterm" id="id1193"/></p>&#13;
&#13;
<p>Next, we’ll cover neural networks. We’ll see how some architectures you’re likely familiar with can be relevant to learning a sequential recommender task.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="RNN and CNN Architectures" data-type="sect1"><div class="sect1" id="id176">&#13;
<h1>RNN and CNN Architectures</h1>&#13;
&#13;
<p><em>Recurrent neural networks</em> (RNNs) are<a data-primary="sequential recommenders" data-secondary="RNN and CNN architectures" data-type="indexterm" id="id1194"/><a data-primary="recurrent neural networks (RNNs)" data-type="indexterm" id="id1195"/><a data-primary="RNNs (recurrent neural networks)" data-type="indexterm" id="id1196"/> a type of neural network architecture designed to recognize patterns in sequences of data, such as text, speech, or time series data. These networks are <em>recurrent</em> in that the outputs from one step in the sequence are fed back into the network as inputs while processing the next step. This gives RNNs a form of memory, which is helpful for tasks like language modeling, where each word depends on the previous words.</p>&#13;
&#13;
<p>At each time step, an RNN takes in an input (like a word in a sentence) and produces an output (like a prediction of the next word). It also updates its internal state, which is a representation of what it has “seen” last in the sequence. This internal state is passed back into the network when processing the next input. As a result, the network can use information from previous steps to influence its predictions for the current step. This is what allows RNNs to effectively process sequential data.</p>&#13;
&#13;
<p><a href="https://oreil.ly/OwEFj">GRU4Rec</a> used<a data-primary="GRU4Rec" data-type="indexterm" id="id1197"/> recurrent neural networks to model session-based recommendations in one of the first applications of neural network architectures to the recommendation problem. A<a data-primary="sessions" data-type="indexterm" id="id1198"/> <em>session</em> refers to a single contiguous period of user interaction, like time spent on a page without the user navigating away or turning off their computer.</p>&#13;
&#13;
<p>Here we will see a dramatic advantage of sequential recommendation systems: most traditional recommendation methods rely on an explicit user ID to build a user-interest profile. However, session-based recommendations operate over anonymous user sessions that are often quite short to allow for a profile modeling. Moreover, a lot of variance can occur in user motivations in different sessions. A solution via user-agnostic recommendation that works for such recommendation situations is an item-based model in which an item-item similarity matrix is calculated based on items co-occurring within a single session. This precomputed similarity matrix is employed at runtime to recommend the most similar item to the one last clicked. This approach has obvious limitations such as relying only on the last clicked item. To this end, GRU4Rec uses all the items in the session and models the session as a sequence of items. The task of recommending items to be added translates to the prediction of the next item in the sequence.</p>&#13;
&#13;
<p class="less_space pagebreak-before">Unlike the small fixed-size vocabulary of languages, recommendation systems are required to reason over a large number of items that grows over time as more items are added. To handle this concern, pairwise ranking losses (e.g., BPR) are considered. GRU4Rec is further extended in <a href="https://oreil.ly/Y17DB">GRU4Rec+</a>, which utilizes a new loss function specifically designed for gains in top-<em>k</em> recommendation. These loss functions blend deep learning and LTR to address neural recommendation settings.</p>&#13;
&#13;
<p>A<a data-primary="convolutional neural networks (CNNs)" data-secondary="for sequential recommendation" data-secondary-sortas="sequential recommendation" data-type="indexterm" id="id1199"/> different approach to neural networks for recommendations adopted CNNs for sequential recommendation. We won’t cover the basics of CNNs here, but you can consult  <a href="https://oreil.ly/-jEiE">“How Do Convolutional Neural Networks Work?”</a> by Brandon Rohrer for the essentials.</p>&#13;
&#13;
<p>Let’s<a data-primary="CosRec" data-type="indexterm" id="id1200"/> discuss one method that has shown a lot of success, <a href="https://oreil.ly/wlCdN">CosRec</a>, as visualized in <a data-type="xref" href="#CosRecImage">Figure 17-1</a>. This method (and others) starts with a structure similar to that of our MF used throughout most of the book: a user-item matrix. We assume that there are two latent factor matrices, <math alttext="upper E Subscript script upper I">&#13;
  <msub><mi>E</mi> <mi>ℐ</mi> </msub>&#13;
</math> and <math alttext="upper E Subscript script upper U">&#13;
  <msub><mi>E</mi> <mi>𝒰</mi> </msub>&#13;
</math>, but let’s first focus on the item matrix.</p>&#13;
&#13;
<p>Each vector in the item matrix is an embedding vector for a single item, but we wish to encode sequences: take sequences of length <math alttext="upper L">&#13;
  <mi>L</mi>&#13;
</math> and collect those embedding vectors. We now have an <math alttext="upper L times upper D">&#13;
  <mrow>&#13;
    <mi>L</mi>&#13;
    <mo>×</mo>&#13;
    <mi>D</mi>&#13;
  </mrow>&#13;
</math> matrix with a row per each item in the sequence. Take adjacent rows as pairs and concatenate them for each vector in a three-tensor; this effectively captures the sequence as a series of pairwise transitions. This three-tensor can be passed through a vectorized 2D CNN to yield a vector (of length <math alttext="upper L">&#13;
  <mi>L</mi>&#13;
</math>) that is concatenated with the original user vector and fed through a fully connected layer. Finally, binary cross-entropy is our loss function to attempt to predict the best recommendation.</p>&#13;
&#13;
<figure><div class="figure" id="CosRecImage">&#13;
<img alt="CosRec CNN Architecture from Yan et. al. 2019" src="assets/brpj_1701.png"/>&#13;
<h6><span class="label">Figure 17-1. </span>CosRec CNN</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Attention Architectures" data-type="sect1"><div class="sect1" id="id177">&#13;
<h1>Attention Architectures</h1>&#13;
&#13;
<p>A<a data-primary="attention architectures" data-secondary="introduction to" data-type="indexterm" id="id1201"/><a data-primary="sequential recommenders" data-secondary="attention architectures" data-type="indexterm" id="SRattention17"/> term that is commonly associated with neural networks and that may ring a bell for you by now is <em>attention</em>. This is because transformers, in particular the kind that appear in large language models (LLMs) like the generalized pretrained transformer, have become a central focus among AI users.</p>&#13;
&#13;
<p>We will give an extremely brief, and less technical, introduction to self-attention and the transformer here. For a more complete guide on transformers, consult the excellent overview in <a href="https://oreil.ly/4PSx-">“Transformers from Scratch”</a> by Brandon Rohrer.</p>&#13;
&#13;
<p>First, let’s state the key differentiating assumption about a transformer model: the embeddings are positional. We’re hoping to learn not only one embedding for every item but also an embedding for every item-position pair. Therefore, when an article is the first in a session and the last in a session, those two instances are treated as <em>two separate items</em>.</p>&#13;
&#13;
<p>Another important notion is stacking. When building transformers, we often think of the architecture as a layer cake, with sections stacked on top of one another. The key components are the embeddings, the self-attention layer, the skip-addition, and the feed-forward layer. The most complicated operations happen in self-attention, so let’s focus on that first. We just discussed the positional embeddings, which are sent as a sequence of these embedding vectors; recall that a transformer is a sequence-to-sequence model! The skip-addition means that we push the embedding forward <em>around</em> the self-attention layer (and the feed-forward layer above) and add it to the positional output of the attention layer. The feed-forward layer is an unexciting multilayer perceptron that stays in the positional columns and uses a ReLU or GeLU activation.</p>&#13;
<div data-type="tip"><h1>ReLU Versus GeLU</h1>&#13;
<p>ReLU (Rectified Linear Unit)<a data-primary="Rectified Linear Unit (ReLU)" data-type="indexterm" id="id1202"/><a data-primary="ReLU (Rectified Linear Unit)" data-type="indexterm" id="id1203"/> is an activation function defined as <math alttext="f left-parenthesis x right-parenthesis equals max left-parenthesis 0 comma x right-parenthesis">&#13;
  <mrow>&#13;
    <mi>f</mi>&#13;
    <mo>(</mo>&#13;
    <mi>x</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mo form="prefix" movablelimits="true">max</mo>&#13;
    <mo>(</mo>&#13;
    <mn>0</mn>&#13;
    <mo>,</mo>&#13;
    <mi>x</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>. GeLU (Gaussian Error Linear Unit) is<a data-primary="Gaussian Error Linear Unit (GeLU)" data-type="indexterm" id="id1204"/> another activation function approximated as <math alttext="f left-parenthesis x right-parenthesis equals 0.5 x left-parenthesis 1 plus hyperbolic tangent left-parenthesis StartRoot StartFraction 2 Over pi EndFraction EndRoot left-parenthesis x plus 0.044715 x cubed right-parenthesis right-parenthesis right-parenthesis">&#13;
  <mrow>&#13;
    <mi>f</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>x</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mn>0</mn>&#13;
    <mo>.</mo>&#13;
    <mn>5</mn>&#13;
    <mi>x</mi>&#13;
    <mfenced close=")" open="(" separators="">&#13;
      <mn>1</mn>&#13;
      <mo>+</mo>&#13;
      <mo form="prefix">tanh</mo>&#13;
      <mfenced close=")" open="(" separators="">&#13;
        <msqrt>&#13;
          <mfrac><mn>2</mn> <mi>π</mi></mfrac>&#13;
        </msqrt>&#13;
        <mfenced close=")" open="(" separators="">&#13;
          <mi>x</mi>&#13;
          <mo>+</mo>&#13;
          <mn>0</mn>&#13;
          <mo>.</mo>&#13;
          <mn>044715</mn>&#13;
          <msup><mi>x</mi> <mn>3</mn> </msup>&#13;
        </mfenced>&#13;
      </mfenced>&#13;
    </mfenced>&#13;
  </mrow>&#13;
</math>, inspired by the Gaussian cumulative distribution function. The intuition behind GeLU is that it tends to allow small values of <math alttext="x">&#13;
  <mi>x</mi>&#13;
</math> to pass through while smoothly saturating extreme values, potentially enabling better gradient flow for deep models. Both functions introduce nonlinearity in neural networks, with GeLU often demonstrating improved learning dynamics over ReLU in certain contexts.</p>&#13;
</div>&#13;
&#13;
<p class="less_space pagebreak-before">Here are some quick tips on<a data-primary="self-attention" data-type="indexterm" id="id1205"/> self-attention:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The idea behind self-attention is that everything in the sequence affects everything else, in some manner.</p>&#13;
</li>&#13;
<li>&#13;
<p>The self-attention layer is learning four weight matrices per head.</p>&#13;
</li>&#13;
<li>&#13;
<p>The heads are in 1-1 correspondence with the sequence length.</p>&#13;
</li>&#13;
<li>&#13;
<p>We often call the weight matrices <math alttext="upper Q comma upper K comma upper O comma upper V">&#13;
  <mrow>&#13;
    <mi>Q</mi>&#13;
    <mo>,</mo>&#13;
    <mi>K</mi>&#13;
    <mo>,</mo>&#13;
    <mi>O</mi>&#13;
    <mo>,</mo>&#13;
    <mi>V</mi>&#13;
  </mrow>&#13;
</math>. Both <math alttext="upper Q">&#13;
  <mi>Q</mi>&#13;
</math> and <math alttext="upper K">&#13;
  <mi>K</mi>&#13;
</math> get crossed with the positional embedding, but <math alttext="upper O">&#13;
  <mi>O</mi>&#13;
</math> and <math alttext="upper V">&#13;
  <mi>V</mi>&#13;
</math> are first crossed into an embedding-dimension-sized square matrix before dotting with the embedding. <math alttext="upper Q ModifyingAbove upper E With dot">&#13;
  <mrow>&#13;
    <mi>Q</mi>&#13;
    <mover accent="true"><mi>E</mi> <mo>˙</mo></mover>&#13;
  </mrow>&#13;
</math> and <math alttext="upper K ModifyingAbove upper E With dot">&#13;
  <mrow>&#13;
    <mi>K</mi>&#13;
    <mover accent="true"><mi>E</mi> <mo>˙</mo></mover>&#13;
  </mrow>&#13;
</math> multiply to create the eponymous<a data-primary="attention matrix" data-type="indexterm" id="id1206"/> <em>attention</em> matrix, over which we take a row-wise softmax to get the attention vector.</p>&#13;
</li>&#13;
<li>&#13;
<p>Some normalizations exist, but we’ll disregard them as inessential for understanding.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>When we want to speak accurately but briefly about attention, we usually say, “It takes a sequence of positional embeddings and mushes them all together to learn how they’re related.”</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Self-Attentive Sequential Recommendation" data-type="sect2"><div class="sect2" id="id178">&#13;
<h2>Self-Attentive Sequential Recommendation</h2>&#13;
&#13;
<p><a href="https://oreil.ly/aKKzg">SASRec</a> is<a data-primary="attention architectures" data-secondary="self-attentive sequential recommendation (SASRec)" data-type="indexterm" id="id1207"/><a data-primary="SASRec (self-attentive sequential recommendation)" data-type="indexterm" id="id1208"/><a data-primary="self-attentive sequential recommendation (SASRec)" data-type="indexterm" id="id1209"/><a data-primary="models" data-secondary="SASRec model" data-type="indexterm" id="id1210"/> the first transformer model we’ll consider. This autoregressive sequential model (similar to a causal language model) predicts the next user interaction from past user interactions. Inspired by the success of the transformer models in sequential mining tasks, the self-attention-based architecture is used for sequential recommendation.</p>&#13;
&#13;
<p>When we say that the SASRec model is trained in an autoregressive manner, we mean that the self-attention is allowed to attend to only the earlier positions in the sequence; looking into the future is not permitted. In terms of the mushing we referenced earlier, think of this as only mushing forward the influence. Some people call this “causal” because it respects the causal arrow of time. The model also allows for a learnable positional encoding, which means that the updates carry down to the embedding layer. This model uses two transformer blocks.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="BERT4Rec" data-type="sect2"><div class="sect2" id="id179">&#13;
<h2>BERT4Rec</h2>&#13;
&#13;
<p>Inspired<a data-primary="attention architectures" data-secondary="BERT4Rec" data-type="indexterm" id="id1211"/><a data-primary="BERT4Rec model" data-type="indexterm" id="id1212"/><a data-primary="models" data-secondary="BERT4Rec model" data-type="indexterm" id="id1213"/> by the BERT model in NLP, <a href="https://oreil.ly/SH9ON">BERT4Rec</a> improves upon SASRec by training a bidirectional masked sequential (language) model.</p>&#13;
&#13;
<p>While BERT uses a masked language model for pretraining word embeddings, BERT4Rec uses this architecture to train end-to-end recommendation systems. It tries to predict the masked items in the user-interaction sequence. Similar to the original BERT model, the self-attention is bidirectional: it can look at both past and future interactions in the action sequence. To prevent leakage of future information and to emulate the realistic settings, only the last item in the sequence is masked during inference. Using item masking, BERT4Rec outperforms SASRec. However, a drawback of the BERT4Rec model is that it is quite compute intensive and requires much more training time.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Recency Sampling" data-type="sect2"><div class="sect2" id="id180">&#13;
<h2>Recency Sampling</h2>&#13;
&#13;
<p>Sequential recommendation<a data-primary="attention architectures" data-secondary="recency sampling" data-type="indexterm" id="id1214"/><a data-primary="recency sampling" data-type="indexterm" id="id1215"/> and the adoption of transformer architecture in these tasks has seen a lot of interest recently. These deep neural network models like BERT4Rec and SASRec have shown improved performance over traditional approaches. However, these models suffer from slow training problems. A recently published paper—ha ha, get it—addresses the question of improving training efficiency while achieving state-of-the-art performance.  See <a href="https://oreil.ly/yV4ro">“Effective and Efficient Training for Sequential Recommendation Using Recency Sampling”</a> by Aleksandr Petrov and Craig Macdonald for details.</p>&#13;
&#13;
<p>The two training paradigms we’ve just described for sequential models are autoregressive, which tries to predict the next item in the user-interaction sequence, and masked, which tries to predict masked items in the interaction sequence. The autoregressive approach doesn’t use the beginning of the sequence as labels in the training process, and thus valuable information is lost. The masked approach, on the other hand, is only weakly related to the end goal of the sequential recommendation.</p>&#13;
&#13;
<p>The paper by Petrov and Macdonald proposes a recency-based sampling of positive examples from the sequences to build the training data. The sampling is designed to give more recent interactions higher chances of being sampled. However, because of the probabilistic nature of the sampling mechanism, even the oldest of the interactions have nonzero chances of being chosen. An exponential function is employed as a sampling routine that interpolates between the masking-based sampling, where each interaction has equal probability of being sampled, and autoregressive sampling, where items from the end of the sequence are sampled. This showed superior performance in sequential recommendation tasks while requiring much less training time. Compare this approach to some of the other examples where we saw sampling provide significant improvements in training recommender systems!</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Merging Static and Sequential" data-type="sect2"><div class="sect2" id="id181">&#13;
<h2>Merging Static and Sequential</h2>&#13;
&#13;
<p>Pinterest<a data-primary="attention architectures" data-secondary="merging static and sequential" data-type="indexterm" id="id1216"/> recently released  <a href="https://oreil.ly/r_kPN">“Rethinking Personalized Ranking at Pinterest: An End-to-End Approach”</a> by Jiajing Xu et al. describing its personalized recommendation system, which is built to leverage raw user actions. The recommendation task is decomposed into modeling users’ long-term and short-term intentions.</p>&#13;
&#13;
<p class="less_space pagebreak-before">The process of comprehending long-term user interests is accomplished by training an end-to-end embedding model, referred to as<a data-primary="PinnerFormer model" data-type="indexterm" id="id1217"/><a data-primary="models" data-secondary="PinnerFormer" data-type="indexterm" id="id1218"/> PinnerFormer, to learn from a user’s historical actions on the platform. These actions are subsequently transformed into user embeddings, which are designed for optimization based on anticipated long-term future user activities.</p>&#13;
&#13;
<p>This procedure employs an adapted transformer model to operate on users’ sequential actions with the intent to forecast their long-term future activities. Each user’s activity is compiled into a sequence, encompassing their actions over a specific time window, such as one year. The graph neural network–based (GNN-based) PinnerSage embeddings, in conjunction with relevant metadata (for example, the type of action, the timestamp, and so forth), are used to add features to each action in the sequence.</p>&#13;
&#13;
<p>Distinct from traditional sequential modeling tasks and sequential recommendation systems, PinnerFormer is designed to predict extended future user activities rather than the immediately subsequent action. This objective is achieved by training the model to foresee a user’s positive future interactions over a window of 14 days following the embedding’s generation. In comparison, traditional sequential models would anticipate only the subsequent action.</p>&#13;
&#13;
<p>This alternate approach allows for the embedding generation to occur offline in a batch-processing mode, resulting in significant reductions in infrastructure needs. In contrast to most traditional sequential modeling systems, which operate in real time and incur substantial computational and infrastructure costs, these embeddings can be produced in batches (for instance, on a daily basis) rather than every time a user performs an action.</p>&#13;
&#13;
<p>A<a data-primary="dense all-action loss" data-type="indexterm" id="id1219"/><a data-primary="loss functions" data-secondary="dense all-action loss" data-type="indexterm" id="id1220"/> dense all-action loss is introduced in this methodology to facilitate batch training of the model. The objective here is not to predict the immediate next action but rather all the actions the user will undertake over the subsequent <math alttext="k">&#13;
  <mi>k</mi>&#13;
</math> days. The aim is to predict all occurrences of a user’s positive interactions at intervals such as <math alttext="upper T plus 3">&#13;
  <mrow>&#13;
    <mi>T</mi>&#13;
    <mo>+</mo>&#13;
    <mn>3</mn>&#13;
  </mrow>&#13;
</math>, <math alttext="upper T plus 8">&#13;
  <mrow>&#13;
    <mi>T</mi>&#13;
    <mo>+</mo>&#13;
    <mn>8</mn>&#13;
  </mrow>&#13;
</math>, and <math alttext="upper T plus 12">&#13;
  <mrow>&#13;
    <mi>T</mi>&#13;
    <mo>+</mo>&#13;
    <mn>12</mn>&#13;
  </mrow>&#13;
</math>, thereby compelling the system to learn long-term intentions. While traditionally the last action’s embedding is used to make the prediction, the dense all-action loss employs randomly selected positions in the action sequence, and the corresponding embedding is used to predict all actions for each of those positions.</p>&#13;
&#13;
<p>Based on offline and online experimental results, the use of dense all-action loss to train for long-term user actions has significantly bridged the gap between batch generation and real-time generation of user embeddings. Moreover, to accommodate users’ short-term interests, the transformer model retrieves the most recent actions for each user in real time, processing them along with the long-term user <span class="keep-together">embeddings.</span> <a data-primary="" data-startref="SRattention17" data-type="indexterm" id="id1221"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="less_space pagebreak-before" data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id182">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>Transformers and sequential recommendation systems are really at the cutting edge of modern recommenders. These days, most research in recommendation systems is in the area of sequential datasets, and the hottest recommenders are using longer and longer sequences for prediction. Two important projects are worthy of attention:</p>&#13;
<dl>&#13;
<dt>Transformers4Rec</dt>&#13;
<dd>&#13;
<p>This<a data-primary="sequential recommenders" data-secondary="Transformers4Rec" data-type="indexterm" id="id1222"/><a data-primary="Transformers4Rec" data-type="indexterm" id="id1223"/> open source project is geared toward scalable transformer models by the NVIDIA Merlin team. For more details, see <a href="https://oreil.ly/jwWBq">“Transformers4Rec: Bridging the Gap Between NLP and Sequential/Session-Based Recommendation”</a> by Gabriel de Souza Pereira Moreira et al.</p>&#13;
</dd>&#13;
<dt>Monolith</dt>&#13;
<dd>&#13;
<p>Also<a data-primary="sequential recommenders" data-secondary="Monolith" data-type="indexterm" id="id1224"/><a data-primary="Monolith" data-type="indexterm" id="id1225"/> known as the TikTok For You page recommender, this is one of the most popular and exciting recommendation systems at this time. It is a fundamentally sequential recommender, with some elegant hybrid approaches. <a href="https://oreil.ly/EADgK">“Monolith: Real-Time Recommendation System with Collisionless Embedding Table”</a> by Zhuoran Liu et al. covers the architectural considerations.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Our final step before this book concludes is to consider a few approaches to recommendations. These don’t build exactly on top of what we’ve done but will use some of what we’ve done and introduce a few new ideas. Let’s sprint to the finish!<a data-primary="" data-startref="RSsequential17" data-type="indexterm" id="id1226"/></p>&#13;
</div></section>&#13;
</div></section></body></html>