<html><head></head><body><section data-pdf-bookmark="Chapter 8. Feature Selection" data-type="chapter" epub:type="chapter"><div class="chapter" id="idm46066899446008">&#13;
<h1><span class="label">Chapter 8. </span>Feature Selection</h1>&#13;
&#13;
&#13;
<p><a data-primary="feature selection" data-type="indexterm" id="ix_ch08-asciidoc0"/>We use feature selection to select features that are useful to the&#13;
model. Irrelevant features may have a negative effect on a model.&#13;
Correlated features can make coefficients in regression (or feature&#13;
importance in tree models) unstable&#13;
or difficult to interpret.</p>&#13;
&#13;
<p><a data-primary="curse of dimensionality" data-type="indexterm" id="idm46066897677032"/>The <em>curse of dimensionality</em> is another issue to consider. As you increase&#13;
the number of dimensions of your data, it becomes more sparse. This&#13;
can make it difficult to pull out a signal unless you have more data.&#13;
Neighbor calculations tend to lose their usefulness as more dimensions are&#13;
added.</p>&#13;
&#13;
<p>Also, training time is usually a function of the number of columns&#13;
(and sometimes it is worse than linear).&#13;
If you can be concise and precise with your columns, you can have&#13;
a better model in less time.&#13;
We will walk through some examples using the <code>agg_df</code> dataset from the last chapter.&#13;
Remember that this is the Titanic dataset with some extra columns for&#13;
cabin information.&#13;
Because this dataset is aggregating numeric values for each cabin,&#13;
it will show many correlations.&#13;
Other options include PCA and looking at the <code>.feature_importances_</code> of&#13;
a tree classifier.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Collinear Columns" data-type="sect1"><div class="sect1" id="idm46066897673560">&#13;
<h1>Collinear Columns</h1>&#13;
&#13;
<p><a data-primary="collinear columns" data-type="indexterm" id="idm46066897585256"/><a data-primary="columns" data-secondary="collinear" data-type="indexterm" id="idm46066897584552"/><a data-primary="feature selection" data-secondary="collinear columns" data-type="indexterm" id="idm46066897583608"/>We can use the previously defined <code>correlated_columns</code> function or run the following&#13;
code to find columns that have a correlation coefficient of .95 or above:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">limit</code> <code class="o">=</code> <code class="mf">0.95</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">corr</code> <code class="o">=</code> <code class="n">agg_df</code><code class="o">.</code><code class="n">corr</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">mask</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">triu</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">np</code><code class="o">.</code><code class="n">ones</code><code class="p">(</code><code class="n">corr</code><code class="o">.</code><code class="n">shape</code><code class="p">),</code> <code class="n">k</code><code class="o">=</code><code class="mi">1</code>&#13;
<code class="gp">... </code><code class="p">)</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="nb">bool</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">corr_no_diag</code> <code class="o">=</code> <code class="n">corr</code><code class="o">.</code><code class="n">where</code><code class="p">(</code><code class="n">mask</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">coll</code> <code class="o">=</code> <code class="p">[</code>&#13;
<code class="gp">... </code>    <code class="n">c</code>&#13;
<code class="gp">... </code>    <code class="k">for</code> <code class="n">c</code> <code class="ow">in</code> <code class="n">corr_no_diag</code><code class="o">.</code><code class="n">columns</code>&#13;
<code class="gp">... </code>    <code class="k">if</code> <code class="nb">any</code><code class="p">(</code><code class="nb">abs</code><code class="p">(</code><code class="n">corr_no_diag</code><code class="p">[</code><code class="n">c</code><code class="p">])</code> <code class="o">&gt;</code> <code class="n">threshold</code><code class="p">)</code>&#13;
<code class="gp">... </code><code class="p">]</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">coll</code>&#13;
<code class="go">['pclass_min', 'pclass_max', 'pclass_mean',</code>&#13;
<code class="go"> 'sibsp_mean', 'parch_mean', 'fare_mean',</code>&#13;
<code class="go"> 'body_max', 'body_mean', 'sex_male', 'embarked_S']</code></pre>&#13;
&#13;
<p><a data-primary="Yellowbrick" data-secondary="correlation heat map" data-type="indexterm" id="idm46066897580488"/>The Yellowbrick <code>Rank2</code> visualizer, shown previously, will plot a heat map of correlations.</p>&#13;
&#13;
<p><a data-primary="multicollinearity" data-type="indexterm" id="idm46066897436104"/><a data-primary="rfpimp" data-type="indexterm" id="idm46066897435176"/>The <a href="https://oreil.ly/MsnXc">rfpimp package</a> has a visualization of <em>multicollinearity</em>. The <code>plot_dependence_heatmap</code> function trains a random forest for each numeric column from the other columns in a training dataset. The dependence value is the R2 score from the out-of-bag (OOB) estimates for predicting that column (see <a data-type="xref" href="#iddephm">FigureÂ 8-1</a>).</p>&#13;
&#13;
<p>The suggested way to use this plot is to find values close to 1. The label on the X axis is the feature that predicts the Y axis label. If a feature predicts another, you can remove the predicted feature (the feature on the Y axis). In our example, <code>fare</code> predicts <code>pclass</code>, <code>sibsp</code>, <code>parch</code>, and <code>embarked_Q</code>. We should be able to keep <code>fare</code> and remove the others and get similar <span class="keep-together">performance:</span></p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">rfpimp</code><code class="o">.</code><code class="n">plot_dependence_heatmap</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">rfpimp</code><code class="o">.</code><code class="n">feature_dependence_matrix</code><code class="p">(</code><code class="n">X_train</code><code class="p">),</code>&#13;
<code class="gp">... </code>    <code class="n">value_fontsize</code><code class="o">=</code><code class="mi">12</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">label_fontsize</code><code class="o">=</code><code class="mi">14</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">8</code><code class="p">,</code> <code class="mi">8</code><code class="p">),</code><code class="n">sn</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">gcf</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="s">"images/mlpr_0801.png"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">bbox_inches</code><code class="o">=</code><code class="s">"tight"</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="iddephm">&#13;
<img alt="Dependence heat map. Pclass, sibsp, parch, and embarked_Q can be predicted from fare, so we can remove them." src="assets/mlpr_0801.png"/>&#13;
<h6><span class="label">Figure 8-1. </span>Dependence heat map. Pclass, sibsp, parch, and embarked_Q can be predicted from fare, so we can remove them.</h6>&#13;
</div></figure>&#13;
&#13;
<p>Here is code showing that we get a similar score if we remove these columns:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">cols_to_remove</code> <code class="o">=</code> <code class="p">[</code>&#13;
<code class="gp">... </code>    <code class="s">"pclass"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="s">"sibsp"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="s">"parch"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="s">"embarked_Q"</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">]</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rf3</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rf3</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">X_train</code><code class="p">[</code>&#13;
<code class="gp">... </code>        <code class="p">[</code>&#13;
<code class="gp">... </code>            <code class="n">c</code>&#13;
<code class="gp">... </code>            <code class="k">for</code> <code class="n">c</code> <code class="ow">in</code> <code class="n">X_train</code><code class="o">.</code><code class="n">columns</code>&#13;
<code class="gp">... </code>            <code class="k">if</code> <code class="n">c</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">cols_to_remove</code>&#13;
<code class="gp">... </code>        <code class="p">]</code>&#13;
<code class="gp">... </code>    <code class="p">],</code>&#13;
<code class="gp">... </code>    <code class="n">y_train</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rf3</code><code class="o">.</code><code class="n">score</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">X_test</code><code class="p">[</code>&#13;
<code class="gp">... </code>        <code class="p">[</code>&#13;
<code class="gp">... </code>            <code class="n">c</code>&#13;
<code class="gp">... </code>            <code class="k">for</code> <code class="n">c</code> <code class="ow">in</code> <code class="n">X_train</code><code class="o">.</code><code class="n">columns</code>&#13;
<code class="gp">... </code>            <code class="k">if</code> <code class="n">c</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">cols_to_remove</code>&#13;
<code class="gp">... </code>        <code class="p">]</code>&#13;
<code class="gp">... </code>    <code class="p">],</code>&#13;
<code class="gp">... </code>    <code class="n">y_test</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="go">0.7684478371501272</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rf4</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rf4</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rf4</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>&#13;
<code class="go">0.7659033078880407</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Lasso Regression" data-type="sect1"><div class="sect1" id="idm46066897586232">&#13;
<h1>Lasso Regression</h1>&#13;
&#13;
<p><a data-primary="feature selection" data-secondary="lasso regression" data-type="indexterm" id="idm46066897316232"/><a data-primary="lasso regression" data-type="indexterm" id="idm46066897315256"/>If you use lasso regression, you can set an <code>alpha</code> parameter that acts as a regularization parameter. As you increase the value, it gives less weight to features that are less important. Here we use the <code>LassoLarsCV</code> model to iterate over various values of alpha and track the feature coefficients (see <a data-type="xref" href="#idlr1">FigureÂ 8-2</a>):</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">linear_model</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">model</code> <code class="o">=</code> <code class="n">linear_model</code><code class="o">.</code><code class="n">LassoLarsCV</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">cv</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">max_n_alphas</code><code class="o">=</code><code class="mi">10</code>&#13;
<code class="gp">... </code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">12</code><code class="p">,</code> <code class="mi">8</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">cm</code> <code class="o">=</code> <code class="nb">iter</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">plt</code><code class="o">.</code><code class="n">get_cmap</code><code class="p">(</code><code class="s">"tab20"</code><code class="p">)(</code>&#13;
<code class="gp">... </code>        <code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="n">X</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code>&#13;
<code class="gp">... </code>    <code class="p">)</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">]):</code>&#13;
<code class="gp">... </code>    <code class="n">c</code> <code class="o">=</code> <code class="nb">next</code><code class="p">(</code><code class="n">cm</code><code class="p">)</code>&#13;
<code class="gp">... </code>    <code class="n">ax</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code>&#13;
<code class="gp">... </code>        <code class="n">model</code><code class="o">.</code><code class="n">alphas_</code><code class="p">,</code>&#13;
<code class="gp">... </code>        <code class="n">model</code><code class="o">.</code><code class="n">coef_path_</code><code class="o">.</code><code class="n">T</code><code class="p">[:,</code> <code class="n">i</code><code class="p">],</code>&#13;
<code class="gp">... </code>        <code class="n">c</code><code class="o">=</code><code class="n">c</code><code class="p">,</code>&#13;
<code class="gp">... </code>        <code class="n">alpha</code><code class="o">=</code><code class="mf">0.8</code><code class="p">,</code>&#13;
<code class="gp">... </code>        <code class="n">label</code><code class="o">=</code><code class="n">X</code><code class="o">.</code><code class="n">columns</code><code class="p">[</code><code class="n">i</code><code class="p">],</code>&#13;
<code class="gp">... </code>    <code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">ax</code><code class="o">.</code><code class="n">axvline</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">model</code><code class="o">.</code><code class="n">alpha_</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">linestyle</code><code class="o">=</code><code class="s">"-"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">c</code><code class="o">=</code><code class="s">"k"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">label</code><code class="o">=</code><code class="s">"alphaCV"</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s">"Regression Coefficients"</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">ax</code><code class="o">.</code><code class="n">legend</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code> <code class="n">bbox_to_anchor</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s">"alpha"</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="s">"Regression Coefficients Progression for Lasso Paths"</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="s">"images/mlpr_0802.png"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">bbox_inches</code><code class="o">=</code><code class="s">"tight"</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="idlr1">&#13;
<img alt="Coefficients of features as alpha varies during lasso regression." src="assets/mlpr_0802.png"/>&#13;
<h6><span class="label">Figure 8-2. </span>Coefficients of features as alpha varies during lasso <span class="keep-together">regression.</span></h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Recursive Feature Elimination" data-type="sect1"><div class="sect1" id="idm46066896912648">&#13;
<h1>Recursive Feature Elimination</h1>&#13;
&#13;
<p><a data-primary="feature selection" data-secondary="recursive feature elimination" data-type="indexterm" id="idm46066896911240"/><a data-primary="recursive feature elimination" data-type="indexterm" id="idm46066896910296"/><a data-primary="scikit-learn" data-secondary="recursive feature elimination" data-type="indexterm" id="idm46066896909656"/>Recursive feature elimination will remove the weakest features, then fit&#13;
a model (see <a data-type="xref" href="#id20">FigureÂ 8-3</a>). It does this by passing in a scikit-learn model with a <code>.coef_</code>&#13;
or <code>.feature_importances_</code> attribute:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">yellowbrick.features</code> <code class="kn">import</code> <code class="n">RFECV</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rfe</code> <code class="o">=</code> <code class="n">RFECV</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">ensemble</code><code class="o">.</code><code class="n">RandomForestClassifier</code><code class="p">(</code>&#13;
<code class="gp">... </code>        <code class="n">n_estimators</code><code class="o">=</code><code class="mi">100</code>&#13;
<code class="gp">... </code>    <code class="p">),</code>&#13;
<code class="gp">... </code>    <code class="n">cv</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rfe</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rfe</code><code class="o">.</code><code class="n">rfe_estimator_</code><code class="o">.</code><code class="n">ranking_</code>&#13;
<code class="go">array([1, 1, 2, 3, 1, 1, 5, 4])</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rfe</code><code class="o">.</code><code class="n">rfe_estimator_</code><code class="o">.</code><code class="n">n_features_</code>&#13;
<code class="go">4</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rfe</code><code class="o">.</code><code class="n">rfe_estimator_</code><code class="o">.</code><code class="n">support_</code>&#13;
<code class="go">array([ True,  True, False, False,  True,</code>&#13;
<code class="go">        True, False, False])</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rfe</code><code class="o">.</code><code class="n">poof</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_0803.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id20">&#13;
<img alt="Recursive feature elimination." src="assets/mlpr_0803.png"/>&#13;
<h6><span class="label">Figure 8-3. </span>Recursive feature elimination.</h6>&#13;
</div></figure>&#13;
&#13;
<p>We will use recursive feature elimination to find the 10 most important&#13;
features. (In this aggregated dataset we find that we have leaked the&#13;
survival column!)</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.feature_selection</code> <code class="kn">import</code> <code class="n">RFE</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">model</code> <code class="o">=</code> <code class="n">ensemble</code><code class="o">.</code><code class="n">RandomForestClassifier</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">n_estimators</code><code class="o">=</code><code class="mi">100</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rfe</code> <code class="o">=</code> <code class="n">RFE</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="mi">4</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rfe</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">agg_X</code><code class="o">.</code><code class="n">columns</code><code class="p">[</code><code class="n">rfe</code><code class="o">.</code><code class="n">support_</code><code class="p">]</code>&#13;
<code class="go">Index(['pclass', 'age', 'fare', 'sex_male'], dtype='object')</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Mutual Information" data-type="sect1"><div class="sect1" id="idm46066896570264">&#13;
<h1>Mutual Information</h1>&#13;
&#13;
<p><a data-primary="k-nearest neighbor (KNN)" data-secondary="for mutual information determination" data-type="indexterm" id="idm46066896551064"/><a data-primary="mutual information" data-type="indexterm" id="idm46066896550152"/><a data-primary="sklearn" data-secondary="mutual information determination" data-type="indexterm" id="idm46066896549480"/>Sklearn provides nonparametric tests that will use k-nearest neighbor&#13;
to determine the <em>mutual information</em> between features and the target.&#13;
Mutual information quantifies the amount of information gained by&#13;
observing another variable. The value is zero or more. If the value&#13;
is zero, then there is no relation between them (see <a data-type="xref" href="#mi1">FigureÂ 8-4</a>). This number is not&#13;
bounded and represents the number of <em>bits</em> shared between the feature and&#13;
the target:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">feature_selection</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">mic</code> <code class="o">=</code> <code class="n">feature_selection</code><code class="o">.</code><code class="n">mutual_info_classif</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">X</code><code class="p">,</code> <code class="n">y</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">8</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code>&#13;
<code class="gp">... </code>        <code class="p">{</code><code class="s">"feature"</code><code class="p">:</code> <code class="n">X</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code> <code class="s">"vimp"</code><code class="p">:</code> <code class="n">mic</code><code class="p">}</code>&#13;
<code class="gp">... </code>    <code class="p">)</code>&#13;
<code class="gp">... </code>    <code class="o">.</code><code class="n">set_index</code><code class="p">(</code><code class="s">"feature"</code><code class="p">)</code>&#13;
<code class="gp">... </code>    <code class="o">.</code><code class="n">plot</code><code class="o">.</code><code class="n">barh</code><code class="p">(</code><code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_0804.png"</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="mi1">&#13;
<img alt="Mutual information plot." src="assets/mlpr_0804.png"/>&#13;
<h6><span class="label">Figure 8-4. </span>Mutual information plot.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Principal Component Analysis" data-type="sect1"><div class="sect1" id="idm46066896485368">&#13;
<h1>Principal Component Analysis</h1>&#13;
&#13;
<p><a data-primary="feature selection" data-secondary="principal component analysis (PCA)" data-type="indexterm" id="idm46066896484200"/><a data-primary="principal component analysis (PCA)" data-type="indexterm" id="idm46066896483208"/>Another option for feature selection is to run principal component analysis.&#13;
Once you have the main principal components, examine the features that&#13;
contribute to them the most. These are features that have more variance.&#13;
Note that this is an unsupervised algorithm and doesnât take <code>y</code> into account.</p>&#13;
&#13;
<p>See <a data-type="xref" href="ch17.html#pca1">âPCAâ</a> for more details.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Feature Importance" data-type="sect1"><div class="sect1" id="idm46066896480136">&#13;
<h1>Feature Importance</h1>&#13;
&#13;
<p><a data-primary="feature importance" data-secondary="feature selection" data-type="indexterm" id="idm46066896478936"/><a data-primary="feature selection" data-secondary="feature importance and" data-type="indexterm" id="idm46066896477960"/>Most tree models provide access to a <code>.feature_importances_</code> attribute&#13;
following training. A higher importance typically means that there is&#13;
higher error when the feature is removed from the model. See the chapters&#13;
for the various tree models for more details.<a data-startref="ix_ch08-asciidoc0" data-type="indexterm" id="idm46066896476280"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>