- en: Chapter 8\. Handling Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 8.0 Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image classification is one of the most exciting areas of machine learning.
    The ability of computers to recognize patterns and objects from images is an incredibly
    powerful tool in our toolkit. However, before we can apply machine learning to
    images, we often first need to transform the raw images to features usable by
    our learning algorithms. As with textual data, there are also many pretrained
    classifiers available for images that we can use to extract features or objects
    of interest to use as inputs to our own models.
  prefs: []
  type: TYPE_NORMAL
- en: To work with images, we will primarily use the Open Source Computer Vision Library
    (OpenCV). While there are a number of good libraries out there, OpenCV is the
    most popular and well-documented library for handling images. It can occasionally
    be challenging to install, but if you run into issues, there are many guides online.
    This book in particular was written with `opencv-python-headless==4.7.0.68`. You
    can also run these chapters with [the ML in Python Cookbook Runner](https://oreil.ly/MLwPython)
    to ensure all commands are reproducible.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, we will use as examples a set of images, which is available
    to download from [GitHub](https://oreil.ly/gV5Zc).
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Loading Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to load an image for preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use OpenCV’s `imread`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to view the image, we can use the Python plotting library Matplotlib:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 08in01](assets/mpc2_08in01.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Fundamentally, images are data, and when we use `imread`, we convert that data
    into a data type we are very familiar with—​a NumPy array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We have transformed the image into a matrix whose elements correspond to individual
    pixels. We can even take a look at the actual values of the matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The resolution of our image is 3600 × 2270, the exact dimensions of our matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'What does each element in the matrix actually represent? In grayscale images,
    the value of an individual element is the pixel intensity. Intensity values range
    from black (0) to white (255). For example, the intensity of the top leftmost
    pixel in our image has a value of 140:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In a matrix representing a color image, each element actually contains three
    values corresponding to blue, green, and red values, respectively (BGR):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'One small caveat: by default OpenCV uses BGR, but many image applications—​including
    Matplotlib—​use red, green, blue (RGB), meaning the red and the blue values are
    swapped. To properly display OpenCV color images in Matplotlib, we first need
    to convert the color to RGB (apologies to hardcopy readers for whom there are
    no color images):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 08in02](assets/mpc2_08in02.png)'
  prefs: []
  type: TYPE_IMG
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Difference Between RGB and BGR](https://oreil.ly/N1Ub6)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[RGB color model, Wikipedia](https://oreil.ly/OEesQ)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 8.2 Saving Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to save an image for preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use OpenCV’s `imwrite`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'OpenCV’s `imwrite` saves images to the filepath specified. The format of the
    image is defined by the filename’s extension (*.jpg*, *.png*, etc.). One behavior
    to be careful about: `imwrite` will overwrite existing files without outputting
    an error or asking for confirmation.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 Resizing Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to resize an image for further preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use `resize` to change the size of an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 08in03](assets/mpc2_08in03.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Resizing images is a common task in image preprocessing for two reasons. First,
    images come in all shapes and sizes, and to be usable as features, images must
    have the same dimensions. Standardizing (resizing) images does come at the cost
    of losing some information present in the larger image, as can be seen in the
    picture of the airplane. Images are matrices of information, and when we reduce
    the size of the image, we are reducing the size of that matrix and the information
    it contains. Second, machine learning can require thousands or hundreds of thousands
    of images. When those images are very large they can take up a lot of memory,
    and by resizing them we can dramatically reduce memory usage. Some common image
    sizes for machine learning are 32 × 32, 64 × 64, 96 × 96, and 256 × 256\. In essence,
    the method we choose for image resizing will often be a tradeoff between the statistical
    performance of our model and computational cost to train it. The [Pillow library
    offers many options for resizing images](https://oreil.ly/NiJn_) for this reason.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4 Cropping Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to remove the outer portion of the image to change its dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The image is encoded as a two-dimensional NumPy array, so we can crop the image
    easily by slicing the array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 08in04](assets/mpc2_08in04.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since OpenCV represents images as a matrix of elements, by selecting the rows
    and columns we want to keep we can easily crop the image. Cropping can be particularly
    useful if we know that we want to keep only a certain part of every image. For
    example, if our images come from a stationary security camera we can crop all
    the images so they contain only the area of interest.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Slicing NumPy Arrays](https://oreil.ly/8JN5p)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 8.5 Blurring Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to smooth out an image.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To blur an image, each pixel is transformed to be the average value of its
    neighbors. This neighbor and the operation performed are mathematically represented
    as a kernel (don’t worry if you don’t know what a kernel is). The size of this
    kernel determines the amount of blurring, with larger kernels producing smoother
    images. Here we blur an image by averaging the values of a 5 × 5 kernel around
    each pixel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 08in05](assets/mpc2_08in05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To highlight the effect of kernel size, here is the same blurring with a 100
    × 100 kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 08in06](assets/mpc2_08in06.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kernels are widely used in image processing to do everything from sharpening
    to edge detection and will come up repeatedly in this chapter. The blurring kernel
    we used looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The center element in the kernel is the pixel being examined, while the remaining
    elements are its neighbors. Since all elements have the same value (normalized
    to add up to 1), each has an equal say in the resulting value of the pixel of
    interest. We can manually apply a kernel to an image using `filter2D` to produce
    a similar blurring effect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 08in07](assets/mpc2_08in07.png)'
  prefs: []
  type: TYPE_IMG
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Image Kernels Explained Visually](https://oreil.ly/9yvdg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Kernel (image processing), Wikipedia](https://oreil.ly/ByREC)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 8.6 Sharpening Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to sharpen an image.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Create a kernel that highlights the target pixel. Then apply it to the image
    using `filter2D`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 08in08](assets/mpc2_08in08.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sharpening works similarly to blurring, except instead of using a kernel to
    average the neighboring values, we constructed a kernel to highlight the pixel
    itself. The resulting effect makes contrasts in edges stand out more.
  prefs: []
  type: TYPE_NORMAL
- en: 8.7 Enhancing Contrast
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We want to increase the contrast between pixels in an image.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Histogram equalization* is a tool for image processing that can make objects
    and shapes stand out. When we have a grayscale image, we can apply OpenCV’s `equalizeHist`
    directly on the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 08in09](assets/mpc2_08in09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, when we have a color image, we first need to convert the image to
    the YUV color format. The Y is the luma, or brightness, and U and V denote the
    color. After the conversion, we can apply `equalizeHist` to the image and then
    convert it back to BGR or RGB (apologies to hardcopy readers for whom there are
    no color images):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 08in10](assets/mpc2_08in10.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While a detailed explanation of how histogram equalization works is beyond the
    scope of this book, the short explanation is that it transforms the image so that
    it uses a wider range of pixel intensities.
  prefs: []
  type: TYPE_NORMAL
- en: While the resulting image often does not look “realistic,” we need to remember
    that the image is just a visual representation of the underlying data. If histogram
    equalization is able to make objects of interest more distinguishable from other
    objects or backgrounds (which is not always the case), then it can be a valuable
    addition to our image preprocessing pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 8.8 Isolating Colors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to isolate a color in an image.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Define a range of colors and then apply a mask to the image (apologies to hardcopy
    readers for whom there are no color images):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 08in11](assets/mpc2_08in11.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Isolating colors in OpenCV is straightforward. First we convert an image into
    HSV (hue, saturation, and value). Second, we define a range of values we want
    to isolate, which is probably the most difficult and time-consuming part. Third,
    we create a mask for the image. Image masking is a common technique meant to extract
    regions of interest. In this case, our mask keeps only the white areas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 08in12](assets/mpc2_08in12.png)'
  prefs: []
  type: TYPE_IMG
- en: Finally, we apply the mask to the image using `bitwise_and` and convert to our
    desired output format.
  prefs: []
  type: TYPE_NORMAL
- en: 8.9 Binarizing Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given an image, you want to output a simplified version.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Thresholding* is the process of setting pixels with intensity greater than
    some value to be white and less than the value to be black. A more advanced technique
    is *adaptive thresholding*, where the threshold value for a pixel is determined
    by the pixel intensities of its neighbors. This can be helpful when lighting conditions
    change over different regions in an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 08in13](assets/mpc2_08in13.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The process of binarizing an image involves converting a greyscale image to
    its black and white form. Our solution has four important arguments in `adaptiveThreshold`.
    `max_output_value` simply determines the maximum intensity of the output pixel
    intensities. `cv2.ADAPTIVE_THRESH_GAUSSIAN_C` sets a pixel’s threshold to be a
    weighted sum of the neighboring pixel intensities. The weights are determined
    by a Gaussian window. Alternatively, we could set the threshold to simply the
    mean of the neighboring pixels with `cv2.ADAPTIVE_THRESH_MEAN_C`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 08in14](assets/mpc2_08in14.png)'
  prefs: []
  type: TYPE_IMG
- en: The last two parameters are the block size (the size of the neighborhood used
    to determine a pixel’s threshold) and a constant subtracted from the calculated
    threshold (used to manually fine-tune the threshold).
  prefs: []
  type: TYPE_NORMAL
- en: A major benefit of thresholding is *denoising* an image—​keeping only the most
    important elements. For example, thresholding is often applied to photos of printed
    text to isolate the letters from the page.
  prefs: []
  type: TYPE_NORMAL
- en: 8.10 Removing Backgrounds
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to isolate the foreground of an image.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Mark a rectangle around the desired foreground, then run the GrabCut algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 08in15](assets/mpc2_08in15.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first thing we notice is that even though GrabCut did a pretty good job,
    there are still areas of background left in the image. We could go back and manually
    mark those areas as background, but in the real world we have thousands of images
    and manually fixing them individually is not feasible. Therefore, we would do
    well by simply accepting that the image data will still contain some background
    noise.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our solution, we start by marking a rectangle around the area that contains
    the foreground. GrabCut assumes everything outside this rectangle to be background
    and uses that information to figure out what is likely background inside the square.
    (To learn how the algorithm does this, see this explanation from [Itay Blumenthal](https://oreil.ly/DTGwb).)
    Then a mask is created that denotes the different definitely/likely background/foreground
    regions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 08in16](assets/mpc2_08in16.png)'
  prefs: []
  type: TYPE_IMG
- en: The black region is the area outside our rectangle that is assumed to be definitely
    background. The gray area is what GrabCut considered likely background, while
    the white area is likely foreground.
  prefs: []
  type: TYPE_NORMAL
- en: 'This mask is then used to create a second mask that merges the black and gray
    regions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 08in17](assets/mpc2_08in17.png)'
  prefs: []
  type: TYPE_IMG
- en: The second mask is then applied to the image so that only the foreground remains.
  prefs: []
  type: TYPE_NORMAL
- en: 8.11 Detecting Edges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to find the edges in an image.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use an edge detection technique like the Canny edge detector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 08in18](assets/mpc2_08in18.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Edge detection is a major topic of interest in computer vision. Edges are important
    because they are areas of high information. For example, in our image one patch
    of sky looks very much like another and is unlikely to contain unique or interesting
    information. However, patches where the background sky meets the airplane contain
    a lot of information (e.g., an object’s shape). Edge detection allows us to remove
    low-information areas and isolate the areas of images containing the most information.
  prefs: []
  type: TYPE_NORMAL
- en: There are many edge detection techniques (Sobel filters, Laplacian edge detector,
    etc.). However, our solution uses the commonly used Canny edge detector. How the
    Canny detector works is too detailed for this book, but there is one point that
    we need to address. The Canny detector requires two parameters denoting low and
    high gradient threshold values. Potential edge pixels between the low and high
    thresholds are considered weak edge pixels, while those above the high threshold
    are considered strong edge pixels. OpenCV’s `Canny` method includes the low and
    high thresholds as required parameters. In our solution, we set the lower and
    upper thresholds to be one standard deviation below and above the image’s median
    pixel intensity. However, we often get better results if we determine a good pair
    of low and high threshold values through manual trial and error on a few images
    before running `Canny` on our entire collection of images.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Canny Edge Detector, Wikipedia](https://oreil.ly/gG9xo)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Canny Edge Detection Auto Thresholding](https://oreil.ly/YvjM5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 8.12 Detecting Corners
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to detect the corners in an image.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use OpenCV’s implementation of the Harris corner detector, `cornerHarris`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 08in19](assets/mpc2_08in19.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The *Harris corner detector* is a commonly used method of detecting the intersection
    of two edges. Our interest in detecting corners is motivated by the same reason
    as for detecting edges: corners are points of high information. A complete explanation
    of the Harris corner detector is available in the external resources at the end
    of this recipe, but a simplified explanation is that it looks for windows (also
    called *neighborhoods* or *patches*) where small movements of the window (imagine
    shaking the window) create big changes in the contents of the pixels inside the
    window. `cornerHarris` contains three important parameters that we can use to
    control the edges detected. First, `block_size` is the size of the neighbor around
    each pixel used for corner detection. Second, `aperture` is the size of the Sobel
    kernel used (don’t worry if you don’t know what that is), and finally there is
    a free parameter where larger values correspond to identifying softer corners.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is a grayscale image depicting potential corners:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 08in20](assets/mpc2_08in20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We then apply thresholding to keep only the most likely corners. Alternatively,
    we can use a similar detector, the Shi-Tomasi corner detector, which works in
    a similar way to the Harris detector (`goodFeaturesToTrack`) to identify a fixed
    number of strong corners. `goodFeaturesToTrack` takes three major parameters—​the
    number of corners to detect, the minimum quality of the corner (0 to 1), and the
    minimum Euclidean distance between corners:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 08in21](assets/mpc2_08in21.png)'
  prefs: []
  type: TYPE_IMG
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[OpenCV’s cornerHarris](https://oreil.ly/vLMBj)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OpenCV’s goodFeaturesToTrack](https://oreil.ly/Ra-x6)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 8.13 Creating Features for Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to convert an image into an observation for machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use NumPy’s `flatten` to convert the multidimensional array containing image
    data into a vector containing the observation’s values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Images are presented as a grid of pixels. If an image is in grayscale, each
    pixel is presented by one value (i.e., pixel intensity is `1` if white, `0` if
    black). For example, imagine we have a 10 × 10–pixel image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 08in22](assets/mpc2_08in22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, the dimensions of the image’s data will be 10 × 10:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'And if we flatten the array, we get a vector of length 100 (10 multiplied by
    10):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: This is the feature data for our image that can be joined with the vectors from
    other images to create the data we will feed to our machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the image is in color, instead of each pixel being represented by one value,
    it is represented by multiple values (most often three) representing the channels
    (red, green, blue, etc.) that blend to make the final color of that pixel. For
    this reason, if our 10 × 10 image is in color, we will have 300 feature values
    for each observation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'One of the major challenges of image processing and computer vision is that
    since every pixel location in a collection of images is a feature, as the images
    get larger, the number of features explodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'And the number of features grows even larger when the image is in color:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: As the output shows, even a small color image has almost 200,000 features, which
    can cause problems when we are training our models because the number of features
    might far exceed the number of observations.
  prefs: []
  type: TYPE_NORMAL
- en: This problem will motivate dimensionality strategies discussed in a later chapter,
    which attempt to reduce the number of features while not losing an excessive amount
    of information contained in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 8.14 Encoding Color Histograms as Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to create a set of features representing the colors appearing in an
    image.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Compute the histograms for each color channel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the RGB color model, each color is the combination of three color channels
    (i.e., red, green, blue). In turn, each channel can take on one of 256 values
    (represented by an integer between 0 and 255). For example, the top leftmost pixel
    in our image has the following channel values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'A histogram is a representation of the distribution of values in data. Here’s
    a simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 08in23](assets/mpc2_08in23.png)'
  prefs: []
  type: TYPE_IMG
- en: In this example, we have some data with two `1`s, two `2`s, three `3`s, one
    `4`, and one `5`. In the histogram, each bar represents the number of times each
    value (`1`, `2`, etc.) appears in our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can apply this same technique to each of the color channels, but instead
    of five possible values, we have 256 (the number of possible values for a channel
    value). The x-axis represents the 256 possible channel values, and the y-axis
    represents the number of times a particular channel value appears across all pixels
    in an image (apologies to hardcopy readers for whom there are no color images):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 08in24](assets/mpc2_08in24.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see in the histogram, barely any pixels contain the blue channel values
    between 0 and ~180, while many pixels contain blue channel values between ~190
    and ~210\. This distribution of channel values is shown for all three channels.
    The histogram, however, is not simply a visualization; it has 256 features for
    each color channel, making for 768 total features representing the distribution
    of colors in an image.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Histogram, Wikipedia](https://oreil.ly/nPbJT)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[pandas documentation: Histogram](https://oreil.ly/h60M5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OpenCV tutorial: Histogram](https://oreil.ly/BuX1C)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 8.15 Using Pretrained Embeddings as Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to load pretrained embeddings from an existing model in PyTorch and
    use them as input to one of your own models.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use `torchvision.models` to select a model and then retrieve an embedding from
    it for a given image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the ML space, *transfer learning* is often defined as taking information
    learned from one task and using it as input to another task. Instead of starting
    from zero, we can use representations already learned from large pretrained image
    models (such as ResNet) to get a head start on our own machine learning models.
    More intuitively, you can understand how we could use the weights of a model trained
    to recognize cats as a good start for a model we want to train to recognize dogs.
    By sharing information form one model to another, we can leverage the information
    learned from other datasets and model architectures without the overhead of training
    a model from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'The entire application of transfer learning in computer vision is outside the
    scope of this book; however, there are many different ways we can extract embeddings-based
    representations of images outside of PyTorch. In TensorFlow, another common library
    for deep learning, we can use `tensorflow_hub`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PyTorch tutorial: Transfer Learning for Computer Vision](https://oreil.ly/R8RTk)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TensorFlow Hub](https://oreil.ly/iwHI6)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 8.16 Detecting Objects with OpenCV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to detect objects in images using pretrained cascade classifiers with
    OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Download and run one of OpenCV’s [Haar cascade classifiers](https://oreil.ly/XlXbm).
    In this case, we use a pretrained face detection model to detect and draw a rectangle
    around a face in an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 08in25](assets/mpc2_08in25.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Haar cascade classifiers* are machine learning models used to learn a set
    of image features (specifically Haar features) that can be used to detect objects
    in images. The features themselves are simple rectangular features that are determined
    by calculating the difference in sums between rectangular regions. Subsequently,
    a gradient boosting algorithm is applied to learn the most important features
    and, finally, create a relatively strong model using cascading classifiers.'
  prefs: []
  type: TYPE_NORMAL
- en: While the details of this process are outside the scope of this book, it’s noteworthy
    that these pretrained models can be easily downloaded from places such as the
    [OpenCV GitHub](https://oreil.ly/273DA) as XML files and applied to images without
    training a model yourself. This is useful in cases where you want to add simple
    binary image features such as `contains_face` (or any other object) to your data.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[OpenCV tutorial: Cascade Classifier](https://oreil.ly/dFhu6)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 8.17 Classifying Images with Pytorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to classify images using pretrained deep learning models in Pytorch.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use `torchvision.models` to select a pretrained image classification model
    and feed the image through it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many pretrained deep learning models for image classification are easily available
    via both PyTorch and TensorFlow. In this example, we used ResNet18, a deep neural
    network architecture that was trained on the ImageNet dataset that is 18 layers
    deep. Deeper ResNet models, such as ResNet101 and ResNet152, are also available
    in Pytorch—and beyond that there are many other image models to choose from. Models
    trained on the ImageNet dataset are able to output predicted probabilities for
    all classes defined in the `imagenet_class_index` variable in the previous code
    snippet, which we downloaded from GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: Like the facial recognition example in OpenCV (see [Recipe 8.16](#detecting-objects-with-open-cv)),
    we can use the predicted image classes as downstream features for future ML models
    or handy metadata tags that add more information to our images.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PyTorch documentation: Models and Pre-trained Weights](https://oreil.ly/MhlxR)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
