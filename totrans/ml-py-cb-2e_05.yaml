- en: Chapter 5\. Handling Categorical Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 5.0 Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is often useful to measure objects not in terms of their quantity but in
    terms of some quality. We frequently represent qualitative information in categories
    such as gender, colors, or brand of car. However, not all categorical data is
    the same. Sets of categories with no intrinsic ordering are called *nominal*.
    Examples of nominal categories include:'
  prefs: []
  type: TYPE_NORMAL
- en: Blue, Red, Green
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Man, Woman
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Banana, Strawberry, Apple
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In contrast, when a set of categories has some natural ordering we refer to
    it as *ordinal*. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: Low, Medium, High
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Young, Old
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agree, Neutral, Disagree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furthermore, categorical information is often represented in data as a vector
    or column of strings (e.g., `"Maine"`, `"Texas"`, `"Delaware"`). The problem is
    that most machine learning algorithms require inputs to be numerical values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The k-nearest neighbors algorithm is an example of an algorithm that requires
    numerical data. One step in the algorithm is calculating the distances between
    observations—​often using Euclidean distance:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><msqrt><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math display="inline"><mi>x</mi></math> and <math display="inline"><mi>y</mi></math>
    are two observations and subscript <math display="inline"><mi>i</mi></math> denotes
    the value for the observations’ <math display="inline"><mi>i</mi></math>th feature.
    However, the distance calculation obviously is impossible if the value of <math
    display="inline"><msub><mi>x</mi><mi>i</mi></msub></math> is a string (e.g., `"Texas"`).
    Instead, we need to convert the string into some numerical format so that it can
    be input into the Euclidean distance equation. Our goal is to transform the data
    in a way that properly captures the information in the categories (ordinality,
    relative intervals between categories, etc.). In this chapter we will cover techniques
    for making this transformation as well as overcoming other challenges often encountered
    when handling categorical data.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Encoding Nominal Categorical Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have a feature with nominal classes that has no intrinsic ordering (e.g.,
    apple, pear, banana), and you want to encode the feature into numerical values.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One-hot encode the feature using scikit-learn’s `LabelBinarizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the `classes_` attribute to output the classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to reverse the one-hot encoding, we can use `inverse_transform`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can even use pandas to one-hot encode the feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '|  | California | Delaware | Texas |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'One helpful feature of scikit-learn is the ability to handle a situation where
    each observation lists multiple classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Once again, we can see the classes with the `classes_` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We might think the proper strategy is to assign each class a numerical value
    (e.g., Texas = 1, California = 2). However, when our classes have no intrinsic
    ordering (e.g., Texas isn’t “less” than California), our numerical values erroneously
    create an ordering that is not present.
  prefs: []
  type: TYPE_NORMAL
- en: The proper strategy is to create a binary feature for each class in the original
    feature. This is often called *one-hot encoding* (in machine learning literature)
    or *dummying* (in statistical and research literature). Our solution’s feature
    was a vector containing three classes (i.e., Texas, California, and Delaware).
    In one-hot encoding, each class becomes its own feature with 1s when the class
    appears and 0s otherwise. Because our feature had three classes, one-hot encoding
    returned three binary features (one for each class). By using one-hot encoding
    we can capture the membership of an observation in a class while preserving the
    notion that the class lacks any sort of hierarchy.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it is often recommended that after one-hot encoding a feature, we drop
    one of the one-hot encoded features in the resulting matrix to avoid linear dependence.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Dummy Variable Trap in Regression Models, Algosome](https://oreil.ly/xjBhG)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Dropping one of the columns when using one-hot encoding, Cross Validated](https://oreil.ly/CTdpG)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5.2 Encoding Ordinal Categorical Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have an ordinal categorical feature (e.g., high, medium, low), and you want
    to transform it into numerical values.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the pandas DataFrame `replace` method to transform string labels to numerical
    equivalents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Often we have a feature with classes that have some kind of natural ordering.
    A famous example is the Likert scale:'
  prefs: []
  type: TYPE_NORMAL
- en: Strongly Agree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neutral
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disagree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strongly Disagree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When encoding the feature for use in machine learning, we need to transform
    the ordinal classes into numerical values that maintain the notion of ordering.
    The most common approach is to create a dictionary that maps the string label
    of the class to a number and then apply that map to the feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important that our choice of numeric values is based on our prior information
    on the ordinal classes. In our solution, `high` is literally three times larger
    than `low`. This is fine in many instances but can break down if the assumed intervals
    between the classes are not equal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, the distance between `Low` and `Medium` is the same as the
    distance between `Medium` and `Barely More Than Medium`, which is almost certainly
    not accurate. The best approach is to be conscious about the numerical values
    mapped to classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 5.3 Encoding Dictionaries of Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have a dictionary and want to convert it into a feature matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use `DictVectorizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: By default `DictVectorizer` outputs a sparse matrix that only stores elements
    with a value other than 0\. This can be very helpful when we have massive matrices
    (often encountered in natural language processing) and want to minimize the memory
    requirements. We can force `DictVectorizer` to output a dense matrix using `sparse=False`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get the names of each generated feature using the `get_feature_names`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'While not necessary, for the sake of illustration we can create a pandas DataFrame
    to view the output better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Blue | Red | Yellow |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 4.0 | 2.0 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 3.0 | 4.0 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.0 | 1.0 | 2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.0 | 2.0 | 2.0 |'
  prefs: []
  type: TYPE_TB
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A dictionary is a popular data structure used by many programming languages;
    however, machine learning algorithms expect the data to be in the form of a matrix.
    We can accomplish this using scikit-learn’s `DictVectorizer`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a common situation when working with natural language processing. For
    example, we might have a collection of documents and for each document we have
    a dictionary containing the number of times every word appears in the document.
    Using `DictVectorizer`, we can easily create a feature matrix where every feature
    is the number of times a word appears in each document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In our toy example there are only three unique words (`Red`, `Yellow`, `Blue`)
    so there are only three features in our matrix; however, you can imagine that
    if each document was actually a book in a university library our feature matrix
    would be very large (and then we would want to set `sparse` to `True`).
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[How to Create Dictionaries in Python](https://oreil.ly/zu5hU)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SciPy Sparse Matrices](https://oreil.ly/5nAsU)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5.4 Imputing Missing Class Values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have a categorical feature containing missing values that you want to replace
    with predicted values.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The ideal solution is to train a machine learning classifier algorithm to predict
    the missing values, commonly a k-nearest neighbors (KNN) classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'An alternative solution is to fill in missing values with the feature’s most
    frequent value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we have missing values in a categorical feature, our best solution is to
    open our toolbox of machine learning algorithms to predict the values of the missing
    observations. We can accomplish this by treating the feature with the missing
    values as the target vector and the other features as the feature matrix. A commonly
    used algorithm is KNN (discussed in depth in [Chapter 15](ch15.xhtml#k-nearest-neighbors)),
    which assigns to the missing value the most frequent class of the *k* nearest
    observations.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we can fill in missing values with the most frequent class of
    the feature or even discard the observations with missing values. While less sophisticated
    than KNN, these options are much more scalable to larger data. In any case, it
    is advisable to include a binary feature indicating which observations contain
    imputed values.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[scikit-learn documentation: Imputation of Missing Values](https://oreil.ly/joZ6J)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Overcoming Missing Values in a Random Forest Classifier](https://oreil.ly/TcvOf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Study of K-Nearest Neighbour as an Imputation Method](https://oreil.ly/kDFEC)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5.5 Handling Imbalanced Classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have a target vector with highly imbalanced classes, and you want to make
    adjustments so that you can handle the class imbalance.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Collect more data. If that isn’t possible, change the metrics used to evaluate
    your model. If that doesn’t work, consider using a model’s built-in class weight
    parameters (if available), downsampling, or upsampling. We cover evaluation metrics
    in a later chapter, so for now let’s focus on class weight parameters, downsampling,
    and upsampling.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate our solutions, we need to create some data with imbalanced classes.
    Fisher’s Iris dataset contains three balanced classes of 50 observations, each
    indicating the species of flower (*Iris setosa*, *Iris virginica*, and *Iris versicolor*).
    To unbalance the dataset, we remove 40 of the 50 *Iris setosa* observations and
    then merge the *Iris virginica* and *Iris versicolor* classes. The end result
    is a binary target vector indicating if an observation is an *Iris setosa* flower
    or not. The result is 10 observations of *Iris setosa* (class 0) and 100 observations
    of not *Iris setosa* (class 1):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Many algorithms in scikit-learn offer a parameter to weight classes during
    training to counteract the effect of their imbalance. While we have not covered
    it yet, `RandomForestClassifier` is a popular classification algorithm and includes
    a `class_weight` parameter; learn more about the `RandomForestClassifier` in [Recipe
    14.4](ch14.xhtml#training-a-random-forest-classifier). You can pass an argument
    explicitly specifying the desired class weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Or you can pass `balanced`, which automatically creates weights inversely proportional
    to class frequencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can downsample the majority class or upsample the minority
    class. In *downsampling*, we randomly sample without replacement from the majority
    class (i.e., the class with more observations) to create a new subset of observations
    equal in size to the minority class. For example, if the minority class has 10
    observations, we will randomly select 10 observations from the majority class
    and use those 20 observations as our data. Here we do exactly that using our unbalanced
    iris data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Our other option is to upsample the minority class. In *upsampling*, for every
    observation in the majority class, we randomly select an observation from the
    minority class with replacement. The result is the same number of observations
    from the minority and majority classes. Upsampling is implemented very similarly
    to downsampling, just in reverse:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the real world, imbalanced classes are everywhere—​most visitors don’t click
    the buy button, and many types of cancer are thankfully rare. For this reason,
    handling imbalanced classes is a common activity in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Our best strategy is simply to collect more observations—​especially observations
    from the minority class. However, often this is just not possible, so we have
    to resort to other options.
  prefs: []
  type: TYPE_NORMAL
- en: A second strategy is to use a model evaluation metric better suited to imbalanced
    classes. Accuracy is often used as a metric for evaluating the performance of
    a model, but when imbalanced classes are present, accuracy can be ill suited.
    For example, if only 0.5% of observations have some rare cancer, then even a naive
    model that predicts nobody has cancer will be 99.5% accurate. Clearly this is
    not ideal. Some better metrics we discuss in later chapters are confusion matrices,
    precision, recall, *F[1]* scores, and ROC curves.
  prefs: []
  type: TYPE_NORMAL
- en: A third strategy is to use the class weighing parameters included in implementations
    of some models. This allows the algorithm to adjust for imbalanced classes. Fortunately,
    many scikit-learn classifiers have a `class_weight` parameter, making it a good
    option.
  prefs: []
  type: TYPE_NORMAL
- en: 'The fourth and fifth strategies are related: downsampling and upsampling. In
    downsampling we create a random subset of the majority class of equal size to
    the minority class. In upsampling we repeatedly sample with replacement from the
    minority class to make it of equal size as the majority class. The decision between
    using downsampling and upsampling is context-specific, and in general we should
    try both to see which produces better results.'
  prefs: []
  type: TYPE_NORMAL
