["```py\n# Import libraries\nimport numpy as np\nfrom sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer\n\n# Create feature\nfeature = np.array([[\"Texas\"],\n                    [\"California\"],\n                    [\"Texas\"],\n                    [\"Delaware\"],\n                    [\"Texas\"]])\n\n# Create one-hot encoder\none_hot = LabelBinarizer()\n\n# One-hot encode feature\none_hot.fit_transform(feature)\n```", "```py\narray([[0, 0, 1],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1]])\n```", "```py\n# View feature classes\none_hot.classes_\n```", "```py\narray(['California', 'Delaware', 'Texas'],\n      dtype='<U10')\n```", "```py\n# Reverse one-hot encoding\none_hot.inverse_transform(one_hot.transform(feature))\n```", "```py\narray(['Texas', 'California', 'Texas', 'Delaware', 'Texas'],\n      dtype='<U10')\n```", "```py\n# Import library\nimport pandas as pd\n\n# Create dummy variables from feature\npd.get_dummies(feature[:,0])\n```", "```py\n# Create multiclass feature\nmulticlass_feature = [(\"Texas\", \"Florida\"),\n                      (\"California\", \"Alabama\"),\n                      (\"Texas\", \"Florida\"),\n                      (\"Delaware\", \"Florida\"),\n                      (\"Texas\", \"Alabama\")]\n\n# Create multiclass one-hot encoder\none_hot_multiclass = MultiLabelBinarizer()\n\n# One-hot encode multiclass feature\none_hot_multiclass.fit_transform(multiclass_feature)\n```", "```py\narray([[0, 0, 0, 1, 1],\n       [1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 1],\n       [0, 0, 1, 1, 0],\n       [1, 0, 0, 0, 1]])\n```", "```py\n# View classes\none_hot_multiclass.classes_\n```", "```py\narray(['Alabama', 'California', 'Delaware', 'Florida', 'Texas'], dtype=object)\n```", "```py\n# Load library\nimport pandas as pd\n\n# Create features\ndataframe = pd.DataFrame({\"Score\": [\"Low\", \"Low\", \"Medium\", \"Medium\", \"High\"]})\n\n# Create mapper\nscale_mapper = {\"Low\":1,\n                \"Medium\":2,\n                \"High\":3}\n\n# Replace feature values with scale\ndataframe[\"Score\"].replace(scale_mapper)\n```", "```py\n0    1\n1    1\n2    2\n3    2\n4    3\nName: Score, dtype: int64\n```", "```py\ndataframe = pd.DataFrame({\"Score\": [\"Low\",\n                                    \"Low\",\n                                    \"Medium\",\n                                    \"Medium\",\n                                    \"High\",\n                                    \"Barely More Than Medium\"]})\n\nscale_mapper = {\"Low\":1,\n                \"Medium\":2,\n                \"Barely More Than Medium\":3,\n                \"High\":4}\n\ndataframe[\"Score\"].replace(scale_mapper)\n```", "```py\n0    1\n1    1\n2    2\n3    2\n4    4\n5    3\nName: Score, dtype: int64\n```", "```py\nscale_mapper = {\"Low\":1,\n                \"Medium\":2,\n                \"Barely More Than Medium\":2.1,\n                \"High\":3}\n\ndataframe[\"Score\"].replace(scale_mapper)\n```", "```py\n0    1.0\n1    1.0\n2    2.0\n3    2.0\n4    3.0\n5    2.1\nName: Score, dtype: float64\n```", "```py\n# Import library\nfrom sklearn.feature_extraction import DictVectorizer\n\n# Create dictionary\ndata_dict = [{\"Red\": 2, \"Blue\": 4},\n             {\"Red\": 4, \"Blue\": 3},\n             {\"Red\": 1, \"Yellow\": 2},\n             {\"Red\": 2, \"Yellow\": 2}]\n\n# Create dictionary vectorizer\ndictvectorizer = DictVectorizer(sparse=False)\n\n# Convert dictionary to feature matrix\nfeatures = dictvectorizer.fit_transform(data_dict)\n\n# View feature matrix\nfeatures\n```", "```py\narray([[ 4.,  2.,  0.],\n       [ 3.,  4.,  0.],\n       [ 0.,  1.,  2.],\n       [ 0.,  2.,  2.]])\n```", "```py\n# Get feature names\nfeature_names = dictvectorizer.get_feature_names()\n\n# View feature names\nfeature_names\n```", "```py\n['Blue', 'Red', 'Yellow']\n```", "```py\n# Import library\nimport pandas as pd\n\n# Create dataframe from features\npd.DataFrame(features, columns=feature_names)\n```", "```py\n# Create word count dictionaries for four documents\ndoc_1_word_count = {\"Red\": 2, \"Blue\": 4}\ndoc_2_word_count = {\"Red\": 4, \"Blue\": 3}\ndoc_3_word_count = {\"Red\": 1, \"Yellow\": 2}\ndoc_4_word_count = {\"Red\": 2, \"Yellow\": 2}\n\n# Create list\ndoc_word_counts = [doc_1_word_count,\n                   doc_2_word_count,\n                   doc_3_word_count,\n                   doc_4_word_count]\n\n# Convert list of word count dictionaries into feature matrix\ndictvectorizer.fit_transform(doc_word_counts)\n```", "```py\narray([[ 4.,  2.,  0.],\n       [ 3.,  4.,  0.],\n       [ 0.,  1.,  2.],\n       [ 0.,  2.,  2.]])\n```", "```py\n# Load libraries\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Create feature matrix with categorical feature\nX = np.array([[0, 2.10, 1.45],\n              [1, 1.18, 1.33],\n              [0, 1.22, 1.27],\n              [1, -0.21, -1.19]])\n\n# Create feature matrix with missing values in the categorical feature\nX_with_nan = np.array([[np.nan, 0.87, 1.31],\n                       [np.nan, -0.67, -0.22]])\n\n# Train KNN learner\nclf = KNeighborsClassifier(3, weights='distance')\ntrained_model = clf.fit(X[:,1:], X[:,0])\n\n# Predict class of missing values\nimputed_values = trained_model.predict(X_with_nan[:,1:])\n\n# Join column of predicted class with their other features\nX_with_imputed = np.hstack((imputed_values.reshape(-1,1), X_with_nan[:,1:]))\n\n# Join two feature matrices\nnp.vstack((X_with_imputed, X))\n```", "```py\narray([[ 0\\.  ,  0.87,  1.31],\n       [ 1\\.  , -0.67, -0.22],\n       [ 0\\.  ,  2.1 ,  1.45],\n       [ 1\\.  ,  1.18,  1.33],\n       [ 0\\.  ,  1.22,  1.27],\n       [ 1\\.  , -0.21, -1.19]])\n```", "```py\nfrom sklearn.impute import SimpleImputer\n\n# Join the two feature matrices\nX_complete = np.vstack((X_with_nan, X))\n\nimputer = SimpleImputer(strategy='most_frequent')\n\nimputer.fit_transform(X_complete)\n```", "```py\narray([[ 0\\.  ,  0.87,  1.31],\n       [ 0\\.  , -0.67, -0.22],\n       [ 0\\.  ,  2.1 ,  1.45],\n       [ 1\\.  ,  1.18,  1.33],\n       [ 0\\.  ,  1.22,  1.27],\n       [ 1\\.  , -0.21, -1.19]])\n```", "```py\n# Load libraries\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_iris\n\n# Load iris data\niris = load_iris()\n\n# Create feature matrix\nfeatures = iris.data\n\n# Create target vector\ntarget = iris.target\n\n# Remove first 40 observations\nfeatures = features[40:,:]\ntarget = target[40:]\n\n# Create binary target vector indicating if class 0\ntarget = np.where((target == 0), 0, 1)\n\n# Look at the imbalanced target vector\ntarget\n```", "```py\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n```", "```py\n# Create weights\nweights = {0: 0.9, 1: 0.1}\n\n# Create random forest classifier with weights\nRandomForestClassifier(class_weight=weights)\n```", "```py\nRandomForestClassifier(class_weight={0: 0.9, 1: 0.1})\n```", "```py\n# Train a random forest with balanced class weights\nRandomForestClassifier(class_weight=\"balanced\")\n```", "```py\nRandomForestClassifier(class_weight='balanced')\n```", "```py\n# Indicies of each class's observations\ni_class0 = np.where(target == 0)[0]\ni_class1 = np.where(target == 1)[0]\n\n# Number of observations in each class\nn_class0 = len(i_class0)\nn_class1 = len(i_class1)\n\n# For every observation of class 0, randomly sample\n# from class 1 without replacement\ni_class1_downsampled = np.random.choice(i_class1, size=n_class0, replace=False)\n\n# Join together class 0's target vector with the\n# downsampled class 1's target vector\nnp.hstack((target[i_class0], target[i_class1_downsampled]))\n```", "```py\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n```", "```py\n# Join together class 0's feature matrix with the\n# downsampled class 1's feature matrix\nnp.vstack((features[i_class0,:], features[i_class1_downsampled,:]))[0:5]\n```", "```py\narray([[ 5\\. ,  3.5,  1.3,  0.3],\n       [ 4.5,  2.3,  1.3,  0.3],\n       [ 4.4,  3.2,  1.3,  0.2],\n       [ 5\\. ,  3.5,  1.6,  0.6],\n       [ 5.1,  3.8,  1.9,  0.4]])\n```", "```py\n# For every observation in class 1, randomly sample from class 0 with\n# replacement\ni_class0_upsampled = np.random.choice(i_class0, size=n_class1, replace=True)\n\n# Join together class 0's upsampled target vector with class 1's target vector\nnp.concatenate((target[i_class0_upsampled], target[i_class1]))\n```", "```py\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n```", "```py\n# Join together class 0's upsampled feature matrix with class 1's feature matrix\nnp.vstack((features[i_class0_upsampled,:], features[i_class1,:]))[0:5]\n```", "```py\narray([[ 5\\. ,  3.5,  1.6,  0.6],\n       [ 5\\. ,  3.5,  1.6,  0.6],\n       [ 5\\. ,  3.3,  1.4,  0.2],\n       [ 4.5,  2.3,  1.3,  0.3],\n       [ 4.8,  3\\. ,  1.4,  0.3]])\n```"]