<html><head></head><body><section data-pdf-bookmark="Chapter 16. Explaining Regression Models" data-type="chapter" epub:type="chapter"><div class="chapter" id="idm46066886327944">&#13;
<h1><span class="label">Chapter 16. </span>Explaining Regression Models</h1>&#13;
&#13;
&#13;
<p><a data-primary="regression models" data-type="indexterm" id="ix_ch16-asciidoc0"/><a data-primary="regression models" data-secondary="Shapley" data-type="indexterm" id="ix_ch16-asciidoc1"/><a data-primary="SHapley Additive exPlanations (SHAP)" data-type="indexterm" id="ix_ch16-asciidoc2"/><a data-primary="XGBoost" data-secondary="regression models" data-type="indexterm" id="ix_ch16-asciidoc3"/>Most of the techniques used to explain classification&#13;
models apply to regression models. In this chapter, I will show&#13;
how to use the SHAP library to interpret regression models.</p>&#13;
&#13;
<p>We will interpret an XGBoost model for the Boston housing dataset:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">xgboost</code> <code class="kn">as</code> <code class="nn">xgb</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">xgr</code> <code class="o">=</code> <code class="n">xgb</code><code class="o">.</code><code class="n">XGBRegressor</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">,</code> <code class="n">base_score</code><code class="o">=</code><code class="mf">0.5</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">xgr</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">bos_X_train</code><code class="p">,</code> <code class="n">bos_y_train</code><code class="p">)</code></pre>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Shapley" data-type="sect1"><div class="sect1" id="idm46066886225576">&#13;
<h1>Shapley</h1>&#13;
&#13;
<p>I’m a big fan of Shapley because it is model agnostic. This library also&#13;
gives us global insight into our model and helps explain individual&#13;
predictions. <a data-primary="black box models" data-secondary="SHAP and" data-type="indexterm" id="idm46066886214488"/>If you have a black-box model, I find it very useful.</p>&#13;
&#13;
<p>We will first look at the prediction for index 5. Our model predicts&#13;
the value to be 27.26:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">sample_idx</code> <code class="o">=</code> <code class="mi">5</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">xgr</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">bos_X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[[</code><code class="n">sample_idx</code><code class="p">]])</code>&#13;
<code class="go">array([27.269186], dtype=float32)</code></pre>&#13;
&#13;
<p>To use the model, we have to create a <code>TreeExplainer</code> from our model&#13;
and estimate the SHAP values for our samples. <a data-primary="Jupyter" data-secondary="and regression models" data-type="indexterm" id="idm46066886191880"/>If we want to use&#13;
Jupyter and have an interactive interface, we also need to call the&#13;
<code>initjs</code> function:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">shap</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">shap</code><code class="o">.</code><code class="n">initjs</code><code class="p">()</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">exp</code> <code class="o">=</code> <code class="n">shap</code><code class="o">.</code><code class="n">TreeExplainer</code><code class="p">(</code><code class="n">xgr</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">vals</code> <code class="o">=</code> <code class="n">exp</code><code class="o">.</code><code class="n">shap_values</code><code class="p">(</code><code class="n">bos_X</code><code class="p">)</code></pre>&#13;
&#13;
<p>With the explainer and the SHAP values, we can create a force&#13;
plot to explain the prediction (see <a data-type="xref" href="#shapr1">Figure 16-1</a>). This informs us that the base&#13;
prediction is 23, and that the population status (LSTAT) and property&#13;
tax rate (TAX) push the price up, while the number of rooms (RM)&#13;
pushes the price down:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">shap</code><code class="o">.</code><code class="n">force_plot</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">exp</code><code class="o">.</code><code class="n">expected_value</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">vals</code><code class="p">[</code><code class="n">sample_idx</code><code class="p">],</code>&#13;
<code class="gp">... </code>    <code class="n">bos_X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">sample_idx</code><code class="p">],</code>&#13;
<code class="gp">... </code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="shapr1">&#13;
<img alt="Force plot for regression. The expected value is pushed up from 23 to 27 due to the population status and tax rate." src="assets/mlpr_1601.png"/>&#13;
<h6><span class="label">Figure 16-1. </span>Force plot for regression. The expected value is pushed up from 23 to 27 due to the population status and tax rate.</h6>&#13;
</div></figure>&#13;
&#13;
<p>We can view the force plot for all of the samples as well to get an overall feel of the behavior. If we are using the interactive JavaScript mode on Jupyter, we can mouse over the samples and see what features are impacting the result (see <a data-type="xref" href="#shapr2">Figure 16-2</a>):</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">shap</code><code class="o">.</code><code class="n">force_plot</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">exp</code><code class="o">.</code><code class="n">expected_value</code><code class="p">,</code> <code class="n">vals</code><code class="p">,</code> <code class="n">bos_X</code>&#13;
<code class="gp">... </code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="shapr2">&#13;
<img alt="Force plot for regression for all samples." src="assets/mlpr_1602.png"/>&#13;
<h6><span class="label">Figure 16-2. </span>Force plot for regression for all samples.</h6>&#13;
</div></figure>&#13;
&#13;
<p>From the force plot of the sample, we saw that the LSTAT feature had a big impact. To visualize how LSTAT affects the result, we can create a dependence plot. The library will automatically choose a feature to color it by (you can provide the <code>interaction_index</code> parameter to set your own).</p>&#13;
&#13;
<p><a data-primary="dependence plots" data-type="indexterm" id="idm46066886095544"/>From the dependence plot for LSTAT (see <a data-type="xref" href="#shapr3">Figure 16-3</a>), we can see that as LSTAT increases (the percent of lower status population), the SHAP value goes down (pushing down the target). A very low LSTAT value pushes SHAP up. From viewing the coloring of the TAX (property tax rate), it appears that as the rate goes down (more blue), the SHAP value goes up:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">shap</code><code class="o">.</code><code class="n">dependence_plot</code><code class="p">(</code><code class="s">"LSTAT"</code><code class="p">,</code> <code class="n">vals</code><code class="p">,</code> <code class="n">bos_X</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="s">"images/mlpr_1603.png"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">bbox_inches</code><code class="o">=</code><code class="s">"tight"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="shapr3">&#13;
<img alt="Dependence plot for LSTAT. As LSTAT goes up, the predicted value goes down." src="assets/mlpr_1603.png"/>&#13;
<h6><span class="label">Figure 16-3. </span>Dependence plot for LSTAT. As LSTAT goes up, the predicted value goes down.</h6>&#13;
</div></figure>&#13;
&#13;
<p>Here is another dependence plot, shown in <a data-type="xref" href="#shapr4">Figure 16-4</a>, to explore the DIS (distance to employment centers). It appears that this feature has little effect unless it is very small:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">shap</code><code class="o">.</code><code class="n">dependence_plot</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="s">"DIS"</code><code class="p">,</code> <code class="n">vals</code><code class="p">,</code> <code class="n">bos_X</code><code class="p">,</code> <code class="n">interaction_index</code><code class="o">=</code><code class="s">"RM"</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="s">"images/mlpr_1604.png"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">bbox_inches</code><code class="o">=</code><code class="s">"tight"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="shapr4">&#13;
<img alt="Dependence plot for DIS. Unless DIS is very small, SHAP stays relatively flat." src="assets/mlpr_1604.png"/>&#13;
<h6><span class="label">Figure 16-4. </span>Dependence plot for DIS. Unless DIS is very small, SHAP stays relatively flat.</h6>&#13;
</div></figure>&#13;
&#13;
<p>Finally, we will look at the global effect of the features using a summary plot (see <a data-type="xref" href="#shapr5">Figure 16-5</a>). The features at the top have the most impact to the model.&#13;
From this view you can see that large values of RM (number of rooms) push up the target a lot, while medium and smaller values push it down a little:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">shap</code><code class="o">.</code><code class="n">summary_plot</code><code class="p">(</code><code class="n">vals</code><code class="p">,</code> <code class="n">bos_X</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="s">"images/mlpr_1605.png"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">bbox_inches</code><code class="o">=</code><code class="s">"tight"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="shapr5">&#13;
<img alt="Summary plot. The most important features are at the top." src="assets/mlpr_1605.png"/>&#13;
<h6><span class="label">Figure 16-5. </span>Summary plot. The most important features are at the top.</h6>&#13;
</div></figure>&#13;
&#13;
<p>The SHAP library is a great tool to have in your toolbelt. It helps understand the global impact of features and also helps explain individual predictions.<a data-startref="ix_ch16-asciidoc3" data-type="indexterm" id="idm46066885815608"/><a data-startref="ix_ch16-asciidoc2" data-type="indexterm" id="idm46066885814904"/><a data-startref="ix_ch16-asciidoc1" data-type="indexterm" id="idm46066885814232"/><a data-startref="ix_ch16-asciidoc0" data-type="indexterm" id="idm46066885813560"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>