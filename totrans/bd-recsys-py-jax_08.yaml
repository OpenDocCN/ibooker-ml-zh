- en: Chapter 6\. Data Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the trivial recommender that we defined in [ChapterÂ 1](ch01.html#CH0), we
    used the method `get_availability`; and in the MPIR, we used the method `get_item_popularities`.
    We hoped the choice of naming would provide sufficient context about their function,
    but we did not focus on the implementation details. Now we will start unpacking
    the details of some of this complexity and present the toolsets for online and
    offline collectors.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Hydrating Your System
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Getting data into the pipeline is punnily referred to as *hydration*. The ML
    and data fields have a lot of water-themed naming conventions; [â€œ(Data âˆ© Water)
    Termsâ€](https://oreil.ly/XVlzd) by Pardis Noorzad covers this topic.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: PySpark
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spark is an extremely general computing library, with APIs for Java, Python,
    SQL, and Scala. PySparkâ€™s role in many ML pipelines is for data processing and
    transforming the large-scale datasets.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s return to the data structure we introduced for our recommendation problem;
    recall that the user-item matrix is the linear-algebraic representation of all
    the triples of users, items, and the userâ€™s rating of the item. These triples
    are not naturally occurring in the wild. Most commonly, you begin with log files
    from your system; for example, Bookshop.org may have something that looks like
    this:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is a made-up log file that may look similar to the backend data for Bookshop.orgâ€™s
    best sellers of the week. These are the kinds of events that you consume from
    engineering and are likely stored in your columnar database. For data like this,
    utilizing SQL syntax will be our entry point.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: PySpark provides a convenient SQL API. Based on your infrastructure, this API
    will allow you to write what looks like SQL queries against a potentially massive
    dataset.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Example Schemas
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These example database schemas are only guesses at what Bookshop.org may use,
    but they are modeled on the authorsâ€™ experience of looking at hundreds of database
    schemas at multiple companies over many years. Additionally, we attempt to distill
    these schemas to the components relevant to our topic. In real systems, youâ€™d
    expect much more complexity but the same essential parts. Each data warehouse
    and event stream will have its own quirks. Please consult a data engineer near
    you.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s use Spark to query the preceding logs:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This is a simple SQL query, assuming the preceding log schema, that would allow
    us to see, for each user-item pair, how many times that user has viewed that pair.
    The convenience of writing pure SQL here means that we can use our experience
    in columnar databases to quickly ramp up on Spark.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: The major advantage of Spark, however, is not yet on display. When executing
    the preceding code in a Spark session, this query will not be immediately run.
    It will be staged for execution, but Spark waits until you use this data downstream
    in a way that *requires immediate execution* before it begins doing so. This is
    called *lazy evaluation*, and it allows you to work on your data object without
    every change and interaction immediately being applied. For more details, itâ€™s
    worth consulting a more in-depth guide like [*Learning Spark*](https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/)
    by Jules Damji et al. (Oâ€™Reilly), but thereâ€™s one more important characteristic
    of the Spark paradigm that is essential to discuss.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼ŒSpark çš„ä¸»è¦ä¼˜åŠ¿å°šæœªå±•ç°å‡ºæ¥ã€‚åœ¨ Spark ä¼šè¯ä¸­æ‰§è¡Œå‰è¿°ä»£ç æ—¶ï¼Œæ­¤æŸ¥è¯¢ä¸ä¼šç«‹å³è¿è¡Œã€‚å®ƒå°†è¢«å‡†å¤‡å¥½ä»¥æ‰§è¡Œï¼Œä½† Spark å°†ç­‰å¾…ç›´åˆ°æ‚¨åœ¨ä¸‹æ¸¸ä½¿ç”¨æ­¤æ•°æ®ï¼Œå¹¶ä¸”éœ€è¦*ç«‹å³æ‰§è¡Œ*æ—¶æ‰å¼€å§‹æ‰§è¡Œã€‚è¿™è¢«ç§°ä¸º*æƒ°æ€§è¯„ä¼°*ï¼Œå®ƒå…è®¸æ‚¨åœ¨ä¸ç«‹å³åº”ç”¨æ¯ä¸ªæ›´æ”¹å’Œäº¤äº’çš„æƒ…å†µä¸‹æ“ä½œæ•°æ®å¯¹è±¡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…åƒ
    [*Learning Spark*](https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/)
    è¿™æ ·æ›´æ·±å…¥çš„æŒ‡å—ï¼Œç”± Jules Damji ç­‰äººï¼ˆO'Reillyï¼‰ç¼–å†™ï¼Œä½† Spark èŒƒå¼çš„å¦ä¸€ä¸ªé‡è¦ç‰¹å¾å€¼å¾—è®¨è®ºã€‚
- en: Spark is natively a distributed computing language. In particular, this means
    that the preceding queryâ€”even after we force it to executeâ€”will store its data
    on multiple computers. Spark works via a *driver program* in your program or notebook,
    which drives a *cluster manager*, which in turn coordinates *executors* on *worker
    nodes.* When we query data with Spark, instead of all that data being returned
    into a DataFrame in memory on the computer weâ€™re using, parts of that data are
    sent to memory on the executors. And when we do a transformation on the DataFrame,
    it is applied appropriately on the pieces of the DataFrame that are stored on
    each of the executors.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Spark æœ¬è´¨ä¸Šæ˜¯ä¸€ç§åˆ†å¸ƒå¼è®¡ç®—è¯­è¨€ã€‚å…·ä½“æ¥è¯´ï¼Œè¿™æ„å‘³ç€å³ä½¿åœ¨å¼ºåˆ¶æ‰§è¡Œä¹‹åï¼Œå‰è¿°æŸ¥è¯¢ä»å°†æ•°æ®å­˜å‚¨åœ¨å¤šå°è®¡ç®—æœºä¸Šã€‚Spark é€šè¿‡ç¨‹åºæˆ–ç¬”è®°æœ¬ä¸­çš„*é©±åŠ¨ç¨‹åº*å·¥ä½œï¼Œè¯¥é©±åŠ¨ç¨‹åºé©±åŠ¨*é›†ç¾¤ç®¡ç†å™¨*ï¼Œåè€…è¿›ä¸€æ­¥åè°ƒ*å·¥ä½œèŠ‚ç‚¹*ä¸Šçš„*æ‰§è¡Œç¨‹åº*ã€‚å½“æˆ‘ä»¬ä½¿ç”¨
    Spark æŸ¥è¯¢æ•°æ®æ—¶ï¼Œæ•°æ®å¹¶éå…¨éƒ¨è¿”å›åˆ°æˆ‘ä»¬ä½¿ç”¨çš„è®¡ç®—æœºä¸Šçš„å†…å­˜ä¸­çš„ DataFrame ä¸­ï¼Œè€Œæ˜¯å°†éƒ¨åˆ†æ•°æ®å‘é€åˆ°æ‰§è¡Œç¨‹åºçš„å†…å­˜ä¸­ã€‚å½“æˆ‘ä»¬å¯¹ DataFrame
    è¿›è¡Œè½¬æ¢æ—¶ï¼Œå®ƒå°†é€‚ç”¨äºå­˜å‚¨åœ¨æ¯ä¸ªæ‰§è¡Œç¨‹åºä¸Šçš„ DataFrame ç‰‡æ®µã€‚
- en: If this sounds a bit like magic, thatâ€™s because itâ€™s obscuring a lot of technical
    details behind several convenience layers. Spark is a layer of technology that
    allows the ML engineer to program as if theyâ€™re working on one machine, and have
    those changes take effect on an entire cluster of machines. Itâ€™s not important
    to understand the network structure when querying, but it is important to be aware
    of some of these details in case things go wrong; the ability to understand what
    the error output is referring to is crucial in troubleshooting. This is all summarized
    in [FigureÂ 6-1](#fig:sparkitecture), which is a diagram from the [Spark documentation](https://oreil.ly/89kAm).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœè¿™å¬èµ·æ¥æœ‰ç‚¹åƒé­”æœ¯ï¼Œé‚£æ˜¯å› ä¸ºå®ƒåœ¨å‡ ä¸ªä¾¿åˆ©å±‚åé¢éšè—äº†è®¸å¤šæŠ€æœ¯ç»†èŠ‚ã€‚Spark æ˜¯ä¸€ç§æŠ€æœ¯å±‚ï¼Œå…è®¸æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆç¼–ç¨‹æ—¶åƒåœ¨ä¸€å°æœºå™¨ä¸Šå·¥ä½œä¸€æ ·ï¼Œå¹¶ä½¿è¿™äº›æ›´æ”¹åœ¨æ•´ä¸ªæœºå™¨ç¾¤é›†ä¸Šç”Ÿæ•ˆã€‚åœ¨æŸ¥è¯¢æ—¶ç†è§£ç½‘ç»œç»“æ„å¹¶ä¸é‡è¦ï¼Œä½†å¦‚æœå‡ºäº†é—®é¢˜ï¼Œäº†è§£ä¸€äº›ç»†èŠ‚æ˜¯å¾ˆé‡è¦çš„ï¼›åœ¨æ•…éšœæ’é™¤æ—¶ç†è§£é”™è¯¯è¾“å‡ºæ‰€æŒ‡çš„å†…å®¹è‡³å…³é‡è¦ã€‚è¿™ä¸€åˆ‡éƒ½åœ¨
    [å›¾Â 6-1](#fig:sparkitecture) ä¸­æ€»ç»“ï¼Œè¿™æ˜¯æ¥è‡ª [Spark æ–‡æ¡£](https://oreil.ly/89kAm) çš„ä¸€å¼ å›¾ã€‚
- en: '![Sparkitecture](assets/brpj_0601.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![Sparkitecture](assets/brpj_0601.png)'
- en: Figure 6-1\. Component architecture of Spark 3.0
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 6-1\. Spark 3.0 çš„ç»„ä»¶æ¶æ„
- en: Itâ€™s important to note that all this does not come for free; both lazy evaluation
    and distributed DataFrames come at the cost of needing additional thought when
    writing programs. Even though Spark makes a lot of this work far easier, understanding
    how to write efficient code in this paradigm that works with the architecture
    but still achieves complicated goals can require a yearâ€™s worth of experience.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸€åˆ‡å¹¶éå…è´¹ï¼›æƒ°æ€§è¯„ä¼°å’Œåˆ†å¸ƒå¼ DataFrame éœ€è¦åœ¨ç¼–å†™ç¨‹åºæ—¶é¢å¤–æ€è€ƒã€‚å°½ç®¡ Spark è®©è®¸å¤šå·¥ä½œå˜å¾—æ›´å®¹æ˜“ï¼Œä½†ç†è§£å¦‚ä½•åœ¨è¿™ç§èŒƒå¼ä¸­ç¼–å†™ä¸ä½“ç³»ç»“æ„å…¼å®¹ä½†ä»èƒ½å®ç°å¤æ‚ç›®æ ‡çš„é«˜æ•ˆä»£ç ï¼Œå¯èƒ½éœ€è¦ä¸€å¹´çš„ç»éªŒç§¯ç´¯ã€‚
- en: Returning to recommendation systemsâ€”and in particular, the offline collectorâ€”we
    want to use PySpark to build the types of datasets needed to train our models.
    One simple thing to do with PySpark is to transform our logs data into the appropriate
    form for training a model. In our simple query, we applied a few filters to our
    data and grouped by user and item to get the number of views. A variety of other
    tasks may fit naturally into this paradigmâ€”perhaps adding user or item features
    stored in other databases, or high-level aggregations.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å›åˆ°æ¨èç³»ç»Ÿï¼Œç‰¹åˆ«æ˜¯ç¦»çº¿æ”¶é›†å™¨ï¼Œæˆ‘ä»¬å¸Œæœ›ä½¿ç”¨ PySpark æ„å»ºè®­ç»ƒæ¨¡å‹æ‰€éœ€çš„æ•°æ®é›†ã€‚ä½¿ç”¨ PySpark å¯ä»¥è½»æ¾å°†æ—¥å¿—æ•°æ®è½¬æ¢ä¸ºè®­ç»ƒæ¨¡å‹æ‰€éœ€çš„é€‚å½“å½¢å¼ã€‚åœ¨æˆ‘ä»¬çš„ç®€å•æŸ¥è¯¢ä¸­ï¼Œæˆ‘ä»¬å¯¹æ•°æ®åº”ç”¨äº†ä¸€äº›è¿‡æ»¤å™¨ï¼Œå¹¶æŒ‰ç”¨æˆ·å’Œé¡¹ç›®åˆ†ç»„ä»¥è·å–è§‚çœ‹æ¬¡æ•°ã€‚è®¸å¤šå…¶ä»–ä»»åŠ¡å¯èƒ½è‡ªç„¶è€Œç„¶åœ°é€‚åˆè¿™ç§èŒƒä¾‹ï¼Œä¾‹å¦‚æ·»åŠ å­˜å‚¨åœ¨å…¶ä»–æ•°æ®åº“ä¸­çš„ç”¨æˆ·æˆ–é¡¹ç›®ç‰¹å¾ï¼Œæˆ–è€…è¿›è¡Œé«˜çº§èšåˆã€‚
- en: 'In our MPIR, we asked for `get_item_popularities`; and we sort of assumed a
    few things:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„ MPIR ä¸­ï¼Œæˆ‘ä»¬è¦æ±‚ä½¿ç”¨ `get_item_popularities`ï¼›æˆ‘ä»¬æœ‰ç‚¹å‡è®¾äº†ä¸€äº›äº‹æƒ…ï¼š
- en: This would return the number of times each item was chosen.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™å°†è¿”å›æ¯ä¸ªé¡¹ç›®è¢«é€‰æ‹©çš„æ¬¡æ•°ã€‚
- en: This method would be fast.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•å°†ä¼šå¾ˆå¿«ã€‚
- en: The second point is important if the endpoint is going to be called in real
    time. So how might Spark come into play?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœè¦å®æ—¶è°ƒç”¨ç»ˆç«¯èŠ‚ç‚¹ï¼Œåˆ™ç¬¬äºŒç‚¹éå¸¸é‡è¦ã€‚é‚£ä¹ˆ Spark å¯èƒ½å¦‚ä½•å‘æŒ¥ä½œç”¨å‘¢ï¼Ÿ
- en: 'First, letâ€™s assume we have a lot of data, enough that we canâ€™t get it all
    to fit into our little MacBook Proâ€™s memory. Additionally, letâ€™s continue to use
    the preceding schema. We can write an even simpler query:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®©æˆ‘ä»¬å‡è®¾æˆ‘ä»¬æœ‰å¤§é‡æ•°æ®ï¼Œè¶³ä»¥ä½¿æˆ‘ä»¬æ— æ³•å°†å…¶å…¨éƒ¨é€‚åº”æˆ‘ä»¬çš„å° MacBook Pro çš„å†…å­˜ä¸­ã€‚æ­¤å¤–ï¼Œè®©æˆ‘ä»¬ç»§ç»­ä½¿ç”¨å‰é¢çš„æ¶æ„ã€‚æˆ‘ä»¬å¯ä»¥ç¼–å†™ä¸€ä¸ªæ›´ç®€å•çš„æŸ¥è¯¢ï¼š
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can now write this aggregated list of `(item, count)` pairs to an app database
    to serve `get_item_popularities` (something that doesnâ€™t require us to do any
    parsing when this is called), or potentially we can take a subset of the top-
    <math alttext="upper N"><mi>N</mi></math> of this list and store it in memory
    to get the best items with respect to a particular ranking. Either way, weâ€™ve
    separated concerns of parsing all our log data, and doing aggregation, from the
    `get_item_popularities` function call in real time.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªèšåˆçš„`(item, count)`å¯¹åˆ—è¡¨å†™å…¥åº”ç”¨ç¨‹åºæ•°æ®åº“ä»¥æä¾›`get_item_popularities`ï¼ˆå½“è°ƒç”¨æ—¶ä¸éœ€è¦æˆ‘ä»¬è¿›è¡Œä»»ä½•è§£æï¼‰ï¼Œæˆ–è€…æˆ‘ä»¬å¯ä»¥è·å–æ­¤åˆ—è¡¨çš„å‰<math
    alttext="upper N"><mi>N</mi></math>ä¸ªå­é›†ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼Œä»¥æ ¹æ®ç‰¹å®šæ’åè·å–æœ€ä½³é¡¹ç›®ã€‚æ— è®ºå“ªç§æ–¹å¼ï¼Œæˆ‘ä»¬éƒ½å·²ç»å°†è§£ææ‰€æœ‰æ—¥å¿—æ•°æ®å’Œè¿›è¡Œèšåˆçš„ä»»åŠ¡ä¸å®æ—¶è°ƒç”¨ä¸­çš„`get_item_popularities`å‡½æ•°è°ƒç”¨åˆ†ç¦»å¼€æ¥ã€‚
- en: This example used an overly simple data aggregation, one just as easy to do
    in something like PostgreSQL, so why bother? The first reason is scalability.
    Spark is really built to scale horizontally, which means that as the data we need
    to access grows, we merely add more worker nodes.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤ç¤ºä¾‹ä½¿ç”¨äº†ä¸€ä¸ªè¿‡äºç®€å•çš„æ•°æ®èšåˆï¼Œå¯ä»¥åœ¨è¯¸å¦‚ PostgreSQL ç­‰æ•°æ®åº“ä¸­è½»æ¾å®Œæˆï¼Œé‚£ä¹ˆä¸ºä»€ä¹ˆè¿˜è¦è´¹è¿™ä¸ªåŠ²å‘¢ï¼Ÿç¬¬ä¸€ä¸ªåŸå› æ˜¯å¯ä¼¸ç¼©æ€§ã€‚Spark çœŸçš„æ˜¯ä¸ºæ°´å¹³æ‰©å±•è€Œæ„å»ºçš„ï¼Œè¿™æ„å‘³ç€éšç€æˆ‘ä»¬éœ€è¦è®¿é—®çš„æ•°æ®å¢é•¿ï¼Œæˆ‘ä»¬åªéœ€æ·»åŠ æ›´å¤šçš„å·¥ä½œèŠ‚ç‚¹ã€‚
- en: The second reason is that PySpark is more than just SparkSQL; anyone whoâ€™s done
    complicated SQL queries can probably agree that the power and flexibility of SQL
    is enormous, but frequently certain tasks that you want to achieve require a lot
    of creativity to carry out in the fully SQL environment. PySpark gives you all
    the expressiveness of pandas DataFrames, Python functions and classes, and a simple
    interface to apply Python code to the PySpark data structureâ€™s user-defined functions
    (UDFs). UDFs are similar to lambda functions that youâ€™d use in pandas, but theyâ€™re
    built and optimized for PySpark DataFrames. As youâ€™ve probably experienced when
    writing ML programs in smaller data regimes, at some point you switch away from
    using only SQL to using pandas API functions to perform data transformationsâ€”so
    too will you appreciate this power at the Spark data scale.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªåŸå› æ˜¯ PySpark ä¸ä»…ä»…æ˜¯ SparkSQLï¼›ä»»ä½•å®Œæˆå¤æ‚ SQL æŸ¥è¯¢çš„äººéƒ½å¯èƒ½åŒæ„ SQL çš„å¼ºå¤§å’Œçµæ´»æ€§æ˜¯å·¨å¤§çš„ï¼Œä½†æ˜¯ç»å¸¸éœ€è¦åœ¨å®Œå…¨
    SQL ç¯å¢ƒä¸­æ‰§è¡Œä¸€äº›ä½ æƒ³è¦çš„ä»»åŠ¡éœ€è¦å¾ˆå¤šåˆ›é€ åŠ›ã€‚PySpark ä¸ºæ‚¨æä¾›äº† pandas DataFramesã€Python å‡½æ•°å’Œç±»çš„æ‰€æœ‰è¡¨ç°åŠ›ï¼Œä»¥åŠå°†
    Python ä»£ç åº”ç”¨äº PySpark æ•°æ®ç»“æ„çš„ç”¨æˆ·å®šä¹‰å‡½æ•°ï¼ˆUDFsï¼‰çš„ç®€å•æ¥å£ã€‚UDFs ç±»ä¼¼äºæ‚¨åœ¨ pandas ä¸­ä½¿ç”¨çš„ lambda å‡½æ•°ï¼Œä½†å®ƒä»¬æ˜¯ä¸º
    PySpark DataFrames æ„å»ºå’Œä¼˜åŒ–çš„ã€‚æ­£å¦‚æ‚¨åœ¨è¾ƒå°çš„æ•°æ®èŒƒå›´å†…ç¼–å†™ ML ç¨‹åºæ—¶å¯èƒ½é‡åˆ°çš„æƒ…å†µä¸€æ ·ï¼Œæœ‰ä¸€å¤©æ‚¨ä¼šä»ä»…ä½¿ç”¨ SQL åˆ‡æ¢åˆ°ä½¿ç”¨ pandas
    API å‡½æ•°æ‰§è¡Œæ•°æ®è½¬æ¢ï¼ŒåŒæ ·æ‚¨å°†æ¬£èµåˆ°åœ¨ Spark æ•°æ®è§„æ¨¡ä¸Šæ‹¥æœ‰çš„è¿™ç§åŠŸèƒ½ã€‚
- en: PySpark allows you to write what looks very much like Python and pandas code
    and have that code executed in a distributed fashion! You donâ€™t need to write
    code to specify which worker nodes operations should happen; thatâ€™s handled for
    you by PySpark. This framework isnâ€™t perfect; some things you expect to work may
    require a bit of care, and optimization of your code can require an additional
    level of abstraction, but generally, PySpark gives you a rapid way to move your
    code from one node to a cluster and utilize that power.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: PySparkå…è®¸æ‚¨ç¼–å†™çœ‹èµ·æ¥éå¸¸åƒPythonå’Œpandasä»£ç çš„ä»£ç ï¼Œå¹¶ä»¥åˆ†å¸ƒå¼æ–¹å¼æ‰§è¡Œè¯¥ä»£ç ï¼æ‚¨ä¸éœ€è¦ç¼–å†™ä»£ç æ¥æŒ‡å®šåº”åœ¨å“ªäº›å·¥ä½œèŠ‚ç‚¹æ‰§è¡Œæ“ä½œï¼›PySparkä¼šä¸ºæ‚¨å¤„ç†è¿™äº›å·¥ä½œã€‚è¿™ä¸ªæ¡†æ¶å¹¶ä¸å®Œç¾ï¼›ä¸€äº›æ‚¨æœŸæœ›èƒ½å¤Ÿæ­£å¸¸å·¥ä½œçš„äº‹æƒ…å¯èƒ½éœ€è¦ä¸€äº›å°å¿ƒï¼Œè€Œä¸”å¯¹ä»£ç çš„ä¼˜åŒ–å¯èƒ½éœ€è¦é¢å¤–çš„æŠ½è±¡çº§åˆ«ï¼Œä½†æ€»çš„æ¥è¯´ï¼ŒPySparkä¸ºæ‚¨æä¾›äº†ä¸€ç§å¿«é€Ÿå°†ä»£ç ä»ä¸€ä¸ªèŠ‚ç‚¹ç§»åŠ¨åˆ°ä¸€ä¸ªé›†ç¾¤å¹¶åˆ©ç”¨è¯¥èƒ½åŠ›çš„æ–¹æ³•ã€‚
- en: To illustrate something a bit more useful in PySpark, letâ€™s return to collaborative
    filtering (CF) and compute some features more relevant for ranking.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åœ¨ PySpark ä¸­æ›´å®ç”¨åœ°è¯´æ˜ä¸€äº›å†…å®¹ï¼Œè®©æˆ‘ä»¬å›åˆ°ååŒè¿‡æ»¤ï¼ˆCFï¼‰å¹¶è®¡ç®—ä¸€äº›æ›´é€‚åˆæ’åçš„ç‰¹å¾ã€‚
- en: 'Example: User Similarity in PySpark'
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼šPySpark ä¸­çš„ç”¨æˆ·ç›¸ä¼¼åº¦
- en: A user similarity table allows you to map a user to other users who are relevant
    to the recommender. This recalls the assumption that two similar users like similar
    things, and thus you can recommend to both users the items that one hasnâ€™t seen.
    Constructing this user similarity table is an example of a PySpark job that you
    might see as part of the offline collectorâ€™s responsibility. Even though in many
    cases ratings would continue to stream in all the time, for the purposes of large
    offline jobs, we often think of a daily batch to update the essential tables for
    our model. In practice, in many cases this daily batch job suffices to provide
    features that are good enough for most of the ML work downstream. Other important
    paradigms exist, but those frequently *marry* the more frequent updates with these
    daily batch jobs, instead of totally eliminating them.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨æˆ·ç›¸ä¼¼åº¦è¡¨å…è®¸æ‚¨å°†ç”¨æˆ·æ˜ å°„åˆ°ä¸æ¨èç³»ç»Ÿç›¸å…³çš„å…¶ä»–ç”¨æˆ·ã€‚è¿™æé†’äº†ä¸€ä¸ªå‡è®¾ï¼Œå³ä¸¤ä¸ªç›¸ä¼¼çš„ç”¨æˆ·å–œæ¬¢ç›¸ä¼¼çš„äº‹ç‰©ï¼Œå› æ­¤æ‚¨å¯ä»¥å‘è¿™ä¸¤ä¸ªç”¨æˆ·æ¨èå…¶ä¸­ä¸€ä¸ªå°šæœªçœ‹è¿‡çš„é¡¹ç›®ã€‚æ„å»ºè¿™ä¸ªç”¨æˆ·ç›¸ä¼¼åº¦è¡¨æ˜¯ä¸€ä¸ªPySparkä½œä¸šçš„ç¤ºä¾‹ï¼Œæ‚¨å¯èƒ½ä¼šåœ¨ç¦»çº¿æ”¶é›†å™¨çš„èŒè´£èŒƒå›´å†…çœ‹åˆ°ã€‚å°½ç®¡åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œè¯„åˆ†å°†ç»§ç»­ä¸æ–­æµå…¥ï¼Œä½†ä¸ºäº†å¤§å‹ç¦»çº¿ä½œä¸šçš„ç›®çš„ï¼Œæˆ‘ä»¬é€šå¸¸è€ƒè™‘æ¯æ—¥æ‰¹å¤„ç†ä»¥æ›´æ–°æˆ‘ä»¬æ¨¡å‹çš„åŸºæœ¬è¡¨ã€‚å®é™…ä¸Šï¼Œåœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œè¿™ç§æ¯æ—¥æ‰¹å¤„ç†ä½œä¸šè¶³ä»¥æä¾›è¶³å¤Ÿå¥½çš„ç‰¹æ€§ï¼Œä»¥æ»¡è¶³å¤§å¤šæ•°MLå·¥ä½œçš„éœ€æ±‚ã€‚å…¶ä»–é‡è¦çš„èŒƒä¾‹å­˜åœ¨ï¼Œä½†è¿™äº›èŒƒä¾‹é€šå¸¸å°†æ›´é¢‘ç¹çš„æ›´æ–°ä¸è¿™äº›æ¯æ—¥æ‰¹å¤„ç†ä½œä¸šç»“åˆèµ·æ¥ï¼Œè€Œä¸æ˜¯å®Œå…¨æ¶ˆé™¤å®ƒä»¬ã€‚
- en: This architecture of daily batch jobs with smaller, more frequent batch jobs
    is called the *lambda architecture*, and weâ€™ll get more into the details of how
    and why later. In brief, the two layersâ€”batch and speedâ€”which are distinguished
    (inversely) by the frequency of processing and the volume per run of data they
    process. Note that the speed layer may have varying frequencies associated with
    it, and itâ€™s possible to have different speed layers for hourly, and another speed
    layer for minute-frequency jobs that do different things. [FigureÂ 6-2](#fig:lamb-architecture)
    provides an overview of the architecture.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ¯æ—¥æ‰¹å¤„ç†ä½œä¸šä¸æ›´å°ã€æ›´é¢‘ç¹çš„æ‰¹å¤„ç†ä½œä¸šçš„æ¶æ„ç§°ä¸º*lambdaæ¶æ„*ï¼Œæˆ‘ä»¬å°†åœ¨ç¨åæ›´è¯¦ç»†åœ°è®¨è®ºå¦‚ä½•ä»¥åŠä¸ºä»€ä¹ˆè¿™æ ·åšã€‚ç®€è¨€ä¹‹ï¼Œè¿™ä¸¤ä¸ªå±‚æ¬¡â€”â€”æ‰¹å¤„ç†å’Œé€Ÿåº¦â€”â€”é€šè¿‡å®ƒä»¬å¤„ç†æ•°æ®çš„å¤„ç†é¢‘ç‡å’Œæ¯æ¬¡è¿è¡Œçš„æ•°æ®é‡ï¼ˆåå‘ï¼‰è¿›è¡ŒåŒºåˆ†ã€‚è¯·æ³¨æ„ï¼Œé€Ÿåº¦å±‚å¯èƒ½å…·æœ‰ä¸ä¹‹å…³è”çš„ä¸åŒé¢‘ç‡ï¼Œå¹¶ä¸”å¯èƒ½ä¼šæœ‰ä¸åŒçš„é€Ÿåº¦å±‚ï¼Œç”¨äºæ‰§è¡Œä¸åŒæ“ä½œçš„å°æ—¶å’Œåˆ†é’Ÿé¢‘ç‡ä½œä¸šã€‚[å›¾6-2](#fig:lamb-architecture)æ¦‚è¿°äº†æ¶æ„ã€‚
- en: '![LambdaArchitecture](assets/brpj_0602.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![LambdaArchitecture](assets/brpj_0602.png)'
- en: Figure 6-2\. Overview of a lambda architecture
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾6-2\. Lambda æ¶æ„æ¦‚è§ˆ
- en: 'In the case of user similarity, letâ€™s work on a batch job implementation of
    computing a daily table. First weâ€™ll need to get ratings from our schema before
    today. Weâ€™ll also include a few other filters that simulate how this query might
    look in real life:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç”¨æˆ·ç›¸ä¼¼åº¦çš„æƒ…å†µä¸‹ï¼Œè®©æˆ‘ä»¬ç€æ‰‹å®ç°ä¸€ä¸ªè®¡ç®—æ¯æ—¥è¡¨æ ¼çš„æ‰¹å¤„ç†ä½œä¸šã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä»æ˜¨å¤©ä¹‹å‰çš„æ¶æ„ä¸­è·å–è¯„åˆ†ã€‚æˆ‘ä»¬è¿˜å°†åŒ…æ‹¬ä¸€äº›å…¶ä»–æ¨¡æ‹Ÿè¿™ä¸ªæŸ¥è¯¢åœ¨ç°å®ç”Ÿæ´»ä¸­å¯èƒ½çœ‹èµ·æ¥å¦‚ä½•çš„è¿‡æ»¤å™¨ï¼š
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As before, utilizing the SQL syntax to get the dataset into a Spark DataFrame
    is the first step, but now we have additional work on the PySpark side. A common
    pattern is to get the dataset you want to work with via simple SQL syntax and
    logic, and then use the PySpark API to do more detailed data processing.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä»¥å‰ä¸€æ ·ï¼Œåˆ©ç”¨SQLè¯­æ³•å°†æ•°æ®é›†å¯¼å…¥Spark DataFrameæ˜¯ç¬¬ä¸€æ­¥ï¼Œä½†ç°åœ¨æˆ‘ä»¬åœ¨PySparkæ–¹é¢æœ‰æ›´å¤šçš„å·¥ä½œã€‚ä¸€ä¸ªå¸¸è§çš„æ¨¡å¼æ˜¯é€šè¿‡ç®€å•çš„SQLè¯­æ³•å’Œé€»è¾‘è·å–è¦å¤„ç†çš„æ•°æ®é›†ï¼Œç„¶åä½¿ç”¨PySpark
    APIè¿›è¡Œæ›´è¯¦ç»†çš„æ•°æ®å¤„ç†ã€‚
- en: 'Letâ€™s first observe that we have no assumptions about uniqueness of a user-item
    rating. For the sake of this table, letâ€™s decide that weâ€™ll use the most recent
    rating for a pair:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®©æˆ‘ä»¬è§‚å¯Ÿä¸€ä¸‹ï¼Œæˆ‘ä»¬å¯¹ç”¨æˆ·-é¡¹ç›®è¯„åˆ†çš„å”¯ä¸€æ€§æ²¡æœ‰ä»»ä½•å‡è®¾ã€‚ä¸ºäº†è¿™ä¸ªè¡¨æ ¼ï¼Œè®©æˆ‘ä»¬å†³å®šä½¿ç”¨æœ€è¿‘çš„è¯„åˆ†å¯¹ï¼š
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Weâ€™ll now use `current_rating` as our ratings column for the purpose of downstream
    calculation. Recall from before our ratings-based definition of user similarity:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`current_rating`ä½œä¸ºæˆ‘ä»¬çš„è¯„åˆ†åˆ—ï¼Œç”¨äºä¸‹æ¸¸è®¡ç®—ã€‚å›é¡¾ä¹‹å‰æˆ‘ä»¬åŸºäºè¯„åˆ†å®šä¹‰çš„ç”¨æˆ·ç›¸ä¼¼åº¦ï¼š
- en: <math alttext="normal upper U normal upper S normal i normal m Subscript upper
    A comma upper B Baseline equals StartFraction sigma-summation Underscript x element-of
    script upper R Subscript upper A comma upper B Baseline Endscripts left-parenthesis
    r Subscript upper A comma x Baseline minus r overbar Subscript upper A Baseline
    right-parenthesis left-parenthesis r Subscript upper B comma x Baseline minus
    r overbar Subscript upper B Baseline right-parenthesis Over StartRoot sigma-summation
    Underscript x element-of script upper R Subscript upper A comma upper B Baseline
    Endscripts left-parenthesis r Subscript upper A comma x Baseline minus r overbar
    Subscript upper A Baseline right-parenthesis squared EndRoot StartRoot sigma-summation
    Underscript x element-of script upper R Subscript upper A comma upper B Baseline
    Endscripts left-parenthesis r Subscript upper B comma x Baseline minus r overbar
    Subscript upper B Baseline right-parenthesis squared EndRoot EndFraction" display="block"><mrow><msub><mi>USim</mi>
    <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub> <mo>=</mo> <mfrac><mrow><msub><mo>âˆ‘</mo>
    <mrow><mi>x</mi><mo>âˆˆ</mo><msub><mi>â„›</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub></mrow></msub>
    <mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>A</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>Â¯</mo></mover> <mi>A</mi></msub>
    <mo>)</mo></mrow><mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>B</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>Â¯</mo></mover> <mi>B</mi></msub>
    <mo>)</mo></mrow></mrow> <mrow><msqrt><mrow><msub><mo>âˆ‘</mo> <mrow><mi>x</mi><mo>âˆˆ</mo><msub><mi>â„›</mi>
    <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub></mrow></msub> <msup><mrow><mo>(</mo><msub><mi>r</mi>
    <mrow><mi>A</mi><mo>,</mo><mi>x</mi></mrow></msub> <mo>-</mo><msub><mover accent="true"><mi>r</mi>
    <mo>Â¯</mo></mover> <mi>A</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt>
    <msqrt><mrow><msub><mo>âˆ‘</mo> <mrow><mi>x</mi><mo>âˆˆ</mo><msub><mi>â„›</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub></mrow></msub>
    <msup><mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>B</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>Â¯</mo></mover> <mi>B</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt></mrow></mfrac></mrow></math>
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal upper U normal upper S normal i normal m Subscript upper
    A comma upper B Baseline equals StartFraction sigma-summation Underscript x element-of
    script upper R Subscript upper A comma upper B Baseline Endscripts left-parenthesis
    r Subscript upper A comma x Baseline minus r overbar Subscript upper A Baseline
    right-parenthesis left-parenthesis r Subscript upper B comma x Baseline minus
    r overbar Subscript upper B Baseline right-parenthesis Over StartRoot sigma-summation
    Underscript x element-of script upper R Subscript upper A comma upper B Baseline
    Endscripts left-parenthesis r Subscript upper A comma x Baseline minus r overbar
    Subscript upper A Baseline right-parenthesis squared EndRoot StartRoot sigma-summation
    Underscript x element-of script upper R Subscript upper A comma upper B Baseline
    Endscripts left-parenthesis r Subscript upper B comma x Baseline minus r overbar
    Subscript upper B Baseline right-parenthesis squared EndRoot EndFraction" display="block"><mrow><msub><mi>USim</mi>
    <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub> <mo>=</mo> <mfrac><mrow><msub><mo>âˆ‘</mo>
    <mrow><mi>x</mi><mo>âˆˆ</mo><msub><mi>â„›</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub></mrow></msub>
    <mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>A</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>Â¯</mo></mover> <mi>A</mi></msub>
    <mo>)</mo></mrow><mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>B</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>Â¯</mo></mover> <mi>B</mi></msub>
    <mo>)</mo></mrow></mrow> <mrow><msqrt><mrow><msub><mo>âˆ‘</mo> <mrow><mi>x</mi><mo>âˆˆ</mo><msub><mi>â„›</mi>
    <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub></mrow></msub> <msup><mrow><mo>(</mo><msub><mi>r</mi>
    <mrow><mi>A</mi><mo>,</mo><mi>x</mi></mrow></msub> <mo>-</mo><msub><mover accent="true"><mi>r</mi>
    <mo>Â¯</mo></mover> <mi>A</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt>
    <msqrt><mrow><msub><mo>âˆ‘</mo> <mrow><mi>x</mi><mo>âˆˆ</mo><msub><mi>â„›</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub></mrow></msub>
    <msup><mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>B</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>Â¯</mo></mover> <mi>B</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt></mrow></mfrac></mrow></math>
- en: 'The important values weâ€™ll need are as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦çš„é‡è¦å€¼å¦‚ä¸‹ï¼š
- en: <math alttext="r Subscript left-parenthesis minus comma minus right-parenthesis"><msub><mi>r</mi>
    <mrow><mo>(</mo><mo>-</mo><mo>,</mo><mo>-</mo><mo>)</mo></mrow></msub></math>
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="r Subscript left-parenthesis minus comma minus right-parenthesis"><msub><mi>r</mi>
    <mrow><mo>(</mo><mo>-</mo><mo>,</mo><mo>-</mo><mo>)</mo></mrow></msub></math>
- en: The rating corresponding to a user-item pair
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨æˆ·-é¡¹ç›®å¯¹åº”çš„è¯„åˆ†
- en: <math alttext="r overbar Subscript left-parenthesis minus right-parenthesis"><msub><mover
    accent="true"><mi>r</mi> <mo>Â¯</mo></mover> <mrow><mo>(</mo><mo>-</mo><mo>)</mo></mrow></msub></math>
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="r overbar Subscript left-parenthesis minus right-parenthesis"><msub><mover
    accent="true"><mi>r</mi> <mo>Â¯</mo></mover> <mrow><mo>(</mo><mo>-</mo><mo>)</mo></mrow></msub></math>
- en: The average rating across all items for a user
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨æˆ·çš„æ‰€æœ‰é¡¹ç›®çš„å¹³å‡è¯„åˆ†
- en: 'The rows are already the <math alttext="r Subscript left-parenthesis minus
    comma minus right-parenthesis"><msub><mi>r</mi> <mrow><mo>(</mo><mo>-</mo><mo>,</mo><mo>-</mo><mo>)</mo></mrow></msub></math>
    values, so letâ€™s compute user average ratings, <math alttext="r overbar Subscript
    left-parenthesis minus right-parenthesis"><msub><mover accent="true"><mi>r</mi>
    <mo>Â¯</mo></mover> <mrow><mo>(</mo><mo>-</mo><mo>)</mo></mrow></msub></math> ,
    and the rating deviations:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: è¡Œå·²ç»æ˜¯<math alttext="r Subscript left-parenthesis minus comma minus right-parenthesis"><msub><mi>r</mi>
    <mrow><mo>(</mo><mo>-</mo><mo>,</mo><mo>-</mo><mo>)</mo></mrow></msub></math>å€¼ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬è®¡ç®—ç”¨æˆ·å¹³å‡è¯„åˆ†<math
    alttext="r overbar Subscript left-parenthesis minus right-parenthesis"><msub><mover
    accent="true"><mi>r</mi> <mo>Â¯</mo></mover> <mrow><mo>(</mo><mo>-</mo><mo>)</mo></mrow></msub></math>å’Œè¯„åˆ†åå·®ï¼š
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now our schema should look like this (weâ€™ve formatted it slightly nicer than
    the default Spark output):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çš„æ¨¡å¼åº”è¯¥å¦‚ä¸‹æ‰€ç¤ºï¼ˆæˆ‘ä»¬å¯¹å…¶è¿›è¡Œäº†æ¯”é»˜è®¤Sparkè¾“å‡ºç¨å¾®ä¼˜åŒ–çš„æ ¼å¼åŒ–ï¼‰ï¼š
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Letâ€™s finish creating a dataset that contains our User Similarity calculations:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å®Œæˆåˆ›å»ºä¸€ä¸ªåŒ…å«æˆ‘ä»¬ç”¨æˆ·ç›¸ä¼¼åº¦è®¡ç®—çš„æ•°æ®é›†ï¼š
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In constructing this dataset, we begin by taking a self-join, which avoids matching
    the same users with themselves but rather joins on books that match. As we do
    this join, we take the rating deviation from the userâ€™s mean ratings that we computed
    previously. We also use this opportunity to multiply them together for the numerator
    in our user similarity function. In the last step, weâ€™ll `groupBy` again so that
    we can sum over all matching book IDs (by `groupBy` on `user_id_1` and `user_id_2`);
    we sum the product and the powers of each set of deviations so that we can finally
    divide and generate a new column for our user similarity.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ„å»ºè¿™ä¸ªæ•°æ®é›†æ—¶ï¼Œæˆ‘ä»¬é¦–å…ˆè¿›è¡Œè‡ªè¿æ¥ï¼Œé¿å…å°†ç›¸åŒç”¨æˆ·ä¸è‡ªèº«åŒ¹é…ï¼Œè€Œæ˜¯æ ¹æ®åŒ¹é…çš„ä¹¦ç±è¿›è¡Œè¿æ¥ã€‚åœ¨è¿›è¡Œæ­¤è¿æ¥æ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¹‹å‰è®¡ç®—å¾—å‡ºçš„ç”¨æˆ·å¹³å‡è¯„åˆ†åå·®å€¼ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬åˆ©ç”¨è¿™ä¸ªæœºä¼šå°†å®ƒä»¬ç›¸ä¹˜ï¼Œä½œä¸ºç”¨æˆ·ç›¸ä¼¼åº¦å‡½æ•°ä¸­çš„åˆ†å­ã€‚åœ¨æœ€åä¸€æ­¥ï¼Œæˆ‘ä»¬å†æ¬¡ä½¿ç”¨`groupBy`ï¼Œä»¥ä¾¿å¯¹æ‰€æœ‰åŒ¹é…çš„ä¹¦ç±IDï¼ˆé€šè¿‡å¯¹`user_id_1`å’Œ`user_id_2`è¿›è¡Œ`groupBy`ï¼‰è¿›è¡Œæ±‚å’Œï¼›æˆ‘ä»¬å¯¹æ¯ç»„åå·®å€¼çš„äº§å“å’Œå¹‚è¿›è¡Œæ±‚å’Œï¼Œä»¥ä¾¿æœ€ç»ˆè¿›è¡Œé™¤æ³•ï¼Œå¹¶ç”Ÿæˆæ–°çš„ç”¨æˆ·ç›¸ä¼¼åº¦åˆ—ã€‚
- en: While this computation isnâ€™t particularly complex, letâ€™s take note of a few
    things that we might appreciate. First, we built our user similarity matrix in
    full from our records. This matrix may now be stored in a faster-access format
    so that if we wish to do operations in real time, itâ€™s ready to go. Second, we
    did all these data transformations in Spark, so we can run these operations on
    massive datasets and let Spark handle the parallelization onto the cluster. We
    even were able to do this while writing code that looks a lot like pandas and
    SQL. Finally, all our operations were columnar and required no iteration-based
    calculation. This means this code will scale much better than some approaches.
    This also ensures that Spark can parallelize our code well, and we can expect
    good performance.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡è¿™ä¸ªè®¡ç®—å¹¶ä¸ç‰¹åˆ«å¤æ‚ï¼Œä½†è®©æˆ‘ä»¬æ³¨æ„ä¸€äº›æˆ‘ä»¬å¯èƒ½ä¼šæ¬£èµçš„äº‹æƒ…ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»è®°å½•ä¸­å®Œæ•´æ„å»ºäº†ç”¨æˆ·ç›¸ä¼¼åº¦çŸ©é˜µã€‚ç°åœ¨å¯ä»¥å°†è¿™ä¸ªçŸ©é˜µå­˜å‚¨åœ¨æ›´å¿«çš„è®¿é—®æ ¼å¼ä¸­ï¼Œå› æ­¤å¦‚æœæˆ‘ä»¬å¸Œæœ›è¿›è¡Œå®æ—¶æ“ä½œï¼Œå®ƒå·²ç»å‡†å¤‡å¥½äº†ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åœ¨Sparkä¸­å®Œæˆäº†æ‰€æœ‰è¿™äº›æ•°æ®è½¬æ¢ï¼Œå› æ­¤å¯ä»¥åœ¨å¤§æ•°æ®é›†ä¸Šè¿è¡Œè¿™äº›æ“ä½œï¼Œå¹¶è®©Sparkå¤„ç†å¹¶è¡ŒåŒ–åˆ°é›†ç¾¤ä¸Šã€‚æˆ‘ä»¬ç”šè‡³èƒ½å¤Ÿç¼–å†™ç±»ä¼¼äºpandaså’ŒSQLçš„ä»£ç ã€‚æœ€åï¼Œæ‰€æœ‰æˆ‘ä»¬çš„æ“ä½œéƒ½æ˜¯åŸºäºåˆ—çš„ï¼Œä¸éœ€è¦è¿­ä»£è®¡ç®—ã€‚è¿™æ„å‘³ç€è¿™æ®µä»£ç å°†æ¯”æŸäº›å…¶ä»–æ–¹æ³•æ›´å¥½åœ°æ‰©å±•ã€‚è¿™è¿˜ç¡®ä¿äº†Sparkèƒ½å¤Ÿå¾ˆå¥½åœ°å¹¶è¡ŒåŒ–æˆ‘ä»¬çš„ä»£ç ï¼Œå¹¶ä¸”æˆ‘ä»¬å¯ä»¥æœŸæœ›è‰¯å¥½çš„æ€§èƒ½ã€‚
- en: Weâ€™ve seen how PySpark can be used to prepare our user similarity matrix. We
    have this definition of affinity estimating the appropriateness of an item for
    a user; we can collect each of those scores into a tabular formâ€”user rows and
    item columnsâ€”to yield a matrix. As an exercise, can you take this matrix and generate
    the affinity matrix?
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»çœ‹åˆ°äº†PySparkå¦‚ä½•ç”¨äºå‡†å¤‡æˆ‘ä»¬çš„ç”¨æˆ·ç›¸ä¼¼åº¦çŸ©é˜µã€‚æˆ‘ä»¬æœ‰è¿™æ ·ä¸€ä¸ªå®šä¹‰çš„äº²å’ŒåŠ›ï¼Œç”¨äºè¯„ä¼°ä¸€ä¸ªé¡¹ç›®å¯¹äºç”¨æˆ·çš„é€‚å®œæ€§ï¼›æˆ‘ä»¬å¯ä»¥å°†è¿™äº›åˆ†æ•°æ”¶é›†åˆ°ä¸€ä¸ªè¡¨æ ¼ä¸­â€”â€”ç”¨æˆ·è¡Œå’Œé¡¹ç›®åˆ—â€”â€”ä»¥ç”Ÿæˆä¸€ä¸ªçŸ©é˜µã€‚ä½œä¸ºä¸€é¡¹ç»ƒä¹ ï¼Œä½ èƒ½å¦æ‹¿è¿™ä¸ªçŸ©é˜µå¹¶ç”Ÿæˆäº²å’ŒåŠ›çŸ©é˜µï¼Ÿ
- en: <math alttext="normal upper A normal f normal f Subscript upper A comma i Baseline
    equals r overbar Subscript upper A Baseline plus StartFraction sigma-summation
    Underscript upper U element-of script upper N left-parenthesis upper A right-parenthesis
    Endscripts normal upper U normal upper S normal i normal m Subscript upper A comma
    upper U Baseline asterisk left-parenthesis r Subscript upper U comma i Baseline
    minus r overbar Subscript upper A Baseline right-parenthesis Over sigma-summation
    Underscript upper U element-of script upper N left-parenthesis upper A right-parenthesis
    Endscripts normal upper U normal upper S normal i normal m Subscript upper A comma
    upper U Baseline EndFraction" display="block"><mrow><msub><mi>Aff</mi> <mrow><mi>A</mi><mo>,</mo><mi>i</mi></mrow></msub>
    <mo>=</mo> <msub><mover accent="true"><mi>r</mi> <mo>Â¯</mo></mover> <mi>A</mi></msub>
    <mo>+</mo> <mfrac><mrow><msub><mo>âˆ‘</mo> <mrow><mi>U</mi><mo>âˆˆ</mo><mi>ğ’©</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow></msub>
    <msub><mi>USim</mi> <mrow><mi>A</mi><mo>,</mo><mi>U</mi></mrow></msub> <mo>*</mo><mrow><mo>(</mo><msub><mi>r</mi>
    <mrow><mi>U</mi><mo>,</mo><mi>i</mi></mrow></msub> <mo>-</mo><msub><mover accent="true"><mi>r</mi>
    <mo>Â¯</mo></mover> <mi>A</mi></msub> <mo>)</mo></mrow></mrow> <mrow><msub><mo>âˆ‘</mo>
    <mrow><mi>U</mi><mo>âˆˆ</mo><mi>ğ’©</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow></msub>
    <msub><mi>USim</mi> <mrow><mi>A</mi><mo>,</mo><mi>U</mi></mrow></msub></mrow></mfrac></mrow></math>
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal upper A normal f normal f Subscript upper A comma i Baseline
    equals r overbar Subscript upper A Baseline plus StartFraction sigma-summation
    Underscript upper U element-of script upper N left-parenthesis upper A right-parenthesis
    Endscripts normal upper U normal upper S normal i normal m Subscript upper A comma
    upper U Baseline asterisk left-parenthesis r Subscript upper U comma i Baseline
    minus r overbar Subscript upper A Baseline right-parenthesis Over sigma-summation
    Underscript upper U element-of script upper N left-parenthesis upper A right-parenthesis
    Endscripts normal upper U normal upper S normal i normal m Subscript upper A comma
    upper U Baseline EndFraction" display="block"><mrow><msub><mi>Aff</mi> <mrow><mi>A</mi><mo>,</mo><mi>i</mi></mrow></msub>
    <mo>=</mo> <msub><mover accent="true"><mi>r</mi> <mo>Â¯</mo></mover> <mi>A</mi></msub>
    <mo>+</mo> <mfrac><mrow><msub><mo>âˆ‘</mo> <mrow><mi>U</mi><mo>âˆˆ</mo><mi>ğ’©</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow></msub>
    <msub><mi>USim</mi> <mrow><mi>A</mi><mo>,</mo><mi>U</mi></mrow></msub> <mo>*</mo><mrow><mo>(</mo><msub><mi>r</mi>
    <mrow><mi>U</mi><mo>,</mo><mi>i</mi></mrow></msub> <mo>-</mo><msub><mover accent="true"><mi>r</mi>
    <mo>Â¯</mo></mover> <mi>A</mi></msub> <mo>)</mo></mrow></mrow> <mrow><msub><mo>âˆ‘</mo>
    <mrow><mi>U</mi><mo>âˆˆ</mo><mi>ğ’©</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow></msub>
    <msub><mi>USim</mi> <mrow><mi>A</mi><mo>,</mo><mi>U</mi></mrow></msub></mrow></mfrac></mrow></math>
- en: Feel free to assume that <math alttext="script upper N left-parenthesis upper
    A right-parenthesis"><mrow><mi>ğ’©</mi> <mo>(</mo> <mi>A</mi> <mo>)</mo></mrow></math>
    is just the five nearest neighbors to <math alttext="upper A"><mi>A</mi></math>
    with respect to user similarity.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·éšæ„å‡è®¾<math alttext="script upper N left-parenthesis upper A right-parenthesis"><mrow><mi>ğ’©</mi>
    <mo>(</mo> <mi>A</mi> <mo>)</mo></mrow></math>åªæ˜¯å…³äºç”¨æˆ·ç›¸ä¼¼æ€§çš„<math alttext="upper A"><mi>A</mi></math>çš„äº”ä¸ªæœ€è¿‘é‚»ã€‚
- en: DataLoaders
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®åŠ è½½å™¨
- en: DataLoaders is a programming paradigm originating from PyTorch, but it has been
    embraced in other gradient-optimized ML workflows. As we begin to integrate gradient-based
    learning into our recommendation system architectures, we will face challenges
    in our MLOps tooling. The first is related to training data size and available
    memory. DataLoaders are a way to prescribe how data is batched and sent to the
    training loop efficiently; as datasets get large, careful scheduling of these
    training sets can have major effects on learning. But why must we think about
    *batches* of data? Thatâ€™s because weâ€™ll use a variant of gradient descent appropriate
    for large amounts of data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®åŠ è½½å™¨æ˜¯æºè‡ªPyTorchçš„ç¼–ç¨‹èŒƒå¼ï¼Œä½†å·²ç»åœ¨å…¶ä»–æ¢¯åº¦ä¼˜åŒ–çš„MLå·¥ä½œæµä¸­å¾—åˆ°äº†åº”ç”¨ã€‚éšç€æˆ‘ä»¬å¼€å§‹å°†åŸºäºæ¢¯åº¦çš„å­¦ä¹ æ•´åˆåˆ°æˆ‘ä»¬çš„æ¨èç³»ç»Ÿæ¶æ„ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨MLOpså·¥å…·ä¸­é¢ä¸´æŒ‘æˆ˜ã€‚ç¬¬ä¸€ä¸ªä¸è®­ç»ƒæ•°æ®å¤§å°å’Œå¯ç”¨å†…å­˜æœ‰å…³ã€‚æ•°æ®åŠ è½½å™¨æ˜¯ä¸€ç§æŒ‡å®šæ•°æ®å¦‚ä½•è¢«æ‰¹å¤„ç†å¹¶æœ‰æ•ˆåœ°å‘é€åˆ°è®­ç»ƒå¾ªç¯ä¸­çš„æ–¹å¼ï¼›éšç€æ•°æ®é›†å˜å¤§ï¼Œè¿™äº›è®­ç»ƒé›†çš„è°¨æ…è°ƒåº¦å¯èƒ½ä¼šå¯¹å­¦ä¹ äº§ç”Ÿé‡å¤§å½±å“ã€‚ä½†ä¸ºä»€ä¹ˆæˆ‘ä»¬å¿…é¡»è€ƒè™‘æ•°æ®çš„*æ‰¹æ¬¡*ï¼Ÿé‚£æ˜¯å› ä¸ºæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ç§é€‚ç”¨äºå¤§é‡æ•°æ®çš„æ¢¯åº¦ä¸‹é™çš„å˜ä½“ã€‚
- en: 'First, letâ€™s review the basics of *mini-batched gradient descent*. During training
    via gradient descent, we make a forward pass of our training sample through our
    model to yield a prediction, and we then compute the error and the appropriate
    gradient backward through our model to update parameters. Batched gradient descent
    takes all our data in a single pass to compute the gradient for the training set
    and push it back through; this implies you have the entire training dataset in
    memory. As the dataset scales, this ranges from expensive to impossible; to avoid
    this, we can instead compute gradients of the loss function for only a subset
    of the dataset at a time. The simplest paradigm for this, called *stochastic gradient
    descent* (SGD), computes these gradients and parameter updates one sample at a
    time. The mini-batched version performs our batched gradient descent, but over
    a series of subsets to form a partition of our dataset. In mathematical notation,
    we write the update rule in terms of Jacobians on the smaller batches:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹*å°æ‰¹é‡æ¢¯åº¦ä¸‹é™*çš„åŸºç¡€çŸ¥è¯†ã€‚åœ¨é€šè¿‡æ¢¯åº¦ä¸‹é™è¿›è¡Œè®­ç»ƒæœŸé—´ï¼Œæˆ‘ä»¬å¯¹è®­ç»ƒæ ·æœ¬è¿›è¡Œå‰å‘ä¼ æ’­ï¼Œå¾—å‡ºé¢„æµ‹ç»“æœï¼Œç„¶åé€šè¿‡æˆ‘ä»¬çš„æ¨¡å‹è®¡ç®—é”™è¯¯å’Œé€‚å½“çš„æ¢¯åº¦å‘åä¼ æ’­ä»¥æ›´æ–°å‚æ•°ã€‚æ‰¹é‡æ¢¯åº¦ä¸‹é™åœ¨å•æ¬¡ä¼ é€’ä¸­è·å–æ‰€æœ‰æ•°æ®ä»¥è®¡ç®—è®­ç»ƒé›†çš„æ¢¯åº¦å¹¶å°†å…¶ä¼ å›ï¼›è¿™æ„å‘³ç€æ‚¨åœ¨å†…å­˜ä¸­æ‹¥æœ‰æ•´ä¸ªè®­ç»ƒæ•°æ®é›†ã€‚éšç€æ•°æ®é›†çš„æ‰©å¤§ï¼Œè¿™ä»æ˜‚è´µåˆ°ä¸å¯èƒ½ï¼›ä¸ºäº†é¿å…è¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬å¯ä»¥åªè®¡ç®—ä¸€æ¬¡æ•°æ®é›†çš„ä¸€éƒ¨åˆ†çš„æŸå¤±å‡½æ•°çš„æ¢¯åº¦ã€‚è¿™ç§æœ€ç®€å•çš„èŒƒå¼ç§°ä¸º*éšæœºæ¢¯åº¦ä¸‹é™*ï¼ˆSGDï¼‰ï¼Œå®ƒé€ä¸ªæ ·æœ¬è®¡ç®—è¿™äº›æ¢¯åº¦å’Œå‚æ•°æ›´æ–°ã€‚å°æ‰¹é‡ç‰ˆæœ¬æ‰§è¡Œæˆ‘ä»¬çš„æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼Œä½†æ˜¯åœ¨ä¸€ç³»åˆ—å­é›†ä¸Šè¿›è¡Œï¼Œä»¥å½¢æˆæ•°æ®é›†çš„åˆ†åŒºã€‚åœ¨æ•°å­¦è¡¨ç¤ºä¸­ï¼Œæˆ‘ä»¬æ ¹æ®è¾ƒå°çš„æ‰¹æ¬¡ç¼–å†™æ›´æ–°è§„åˆ™ï¼š
- en: <math alttext="theta equals theta minus eta asterisk nabla Subscript theta Baseline
    upper J left-parenthesis theta semicolon x Superscript left-parenthesis i colon
    i plus n right-parenthesis Baseline semicolon y Superscript left-parenthesis i
    colon i plus n right-parenthesis Baseline right-parenthesis" display="block"><mrow><mi>Î¸</mi>
    <mo>=</mo> <mi>Î¸</mi> <mo>-</mo> <mi>Î·</mi> <mo>*</mo> <msub><mi>âˆ‡</mi> <mi>Î¸</mi></msub>
    <mi>J</mi> <mfenced close=")" open="(" separators=""><mi>Î¸</mi> <mo>;</mo> <msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>:</mo><mi>i</mi><mo>+</mo><mi>n</mi><mo>)</mo></mrow></msup>
    <mo>;</mo> <msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>:</mo><mi>i</mi><mo>+</mo><mi>n</mi><mo>)</mo></mrow></msup></mfenced></mrow></math>
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="theta equals theta minus eta asterisk nabla Subscript theta Baseline
    upper J left-parenthesis theta semicolon x Superscript left-parenthesis i colon
    i plus n right-parenthesis Baseline semicolon y Superscript left-parenthesis i
    colon i plus n right-parenthesis Baseline right-parenthesis" display="block"><mrow><mi>Î¸</mi>
    <mo>=</mo> <mi>Î¸</mi> <mo>-</mo> <mi>Î·</mi> <mo>*</mo> <msub><mi>âˆ‡</mi> <mi>Î¸</mi></msub>
    <mi>J</mi> <mfenced close=")" open="(" separators=""><mi>Î¸</mi> <mo>;</mo> <msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>:</mo><mi>i</mi><mo>+</mo><mi>n</mi><mo>)</mo></mrow></msup>
    <mo>;</mo> <msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>:</mo><mi>i</mi><mo>+</mo><mi>n</mi><mo>)</mo></mrow></msup></mfenced></mrow></math>
- en: This optimization serves a few purposes. First, it requires only potentially
    small subsets of our data held in memory during the steps. Second, it requires
    far fewer passes than the purely iterative version in SGD. Third, the gradient
    operating on these mini-batches can be organized as a Jacobian, and thus we have
    linear-algebraic operations that may be highly optimized.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§ä¼˜åŒ–æœ‰å‡ ä¸ªç›®çš„ã€‚é¦–å…ˆï¼Œåœ¨è¿™äº›æ­¥éª¤æœŸé—´ï¼Œå®ƒä»…éœ€è¦å¯èƒ½å¾ˆå°çš„æ•°æ®å­é›†ä¿å­˜åœ¨å†…å­˜ä¸­ã€‚å…¶æ¬¡ï¼Œå®ƒæ¯”SGDä¸­çš„çº¯è¿­ä»£ç‰ˆæœ¬éœ€è¦çš„ä¼ é€’è¦å°‘å¾—å¤šã€‚ç¬¬ä¸‰ï¼Œå¯¹è¿™äº›å°æ‰¹é‡çš„æ¢¯åº¦æ“ä½œå¯ä»¥ç»„ç»‡ä¸ºé›…å¯æ¯”çŸ©é˜µï¼Œå› æ­¤æˆ‘ä»¬æœ‰å¯èƒ½é«˜åº¦ä¼˜åŒ–çš„çº¿æ€§ä»£æ•°è¿ç®—ã€‚
- en: Jacobians
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é›…å¯æ¯”çŸ©é˜µ
- en: The mathematical notion of a Jacobian in the simplest sense is an organizational
    tool for a set of vector derivatives with relevant indexes. You may recall that
    for functions of several variables, you can take the derivative *with respect
    to* each of those variables. For a single multivariable scalar function, the Jacobian
    is simply the row vector of first derivatives of the functionâ€”which happens to
    be the transpose of the gradient.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°å­¦æ¦‚å¿µä¸­çš„é›…å¯æ¯”çŸ©é˜µæœ€ç®€å•çš„æ„ä¹‰æ˜¯ä¸€ç§ç”¨äºä¸€ç»„å…·æœ‰ç›¸å…³ç´¢å¼•çš„å‘é‡å¯¼æ•°çš„ç»„ç»‡å·¥å…·ã€‚æ‚¨å¯èƒ½è¿˜è®°å¾—ï¼Œå¯¹äºå¤šä¸ªå˜é‡çš„å‡½æ•°ï¼Œæ‚¨å¯ä»¥ç›¸å¯¹äºæ¯ä¸ªå˜é‡è¿›è¡Œå¯¼æ•°è®¡ç®—ã€‚å¯¹äºå•ä¸ªå¤šå˜é‡æ ‡é‡å‡½æ•°ï¼Œé›…å¯æ¯”çŸ©é˜µç®€å•åœ°æ˜¯è¯¥å‡½æ•°çš„ä¸€é˜¶å¯¼æ•°çš„è¡Œå‘é‡â€”â€”æ°å¥½æ˜¯æ¢¯åº¦çš„è½¬ç½®ã€‚
- en: This is the simplest case; the gradient of a multivariable scalar function may
    be written as a Jacobian. However, once we have a vector of (vector) derivatives,
    we can write that as a matrix; the utility here is really only in the notation,
    though. When you collect a series of multivariable scalar functions into a vector
    of functions, the associated vector of gradients is a vector of vectors of derivatives.
    This is called a *Jacobian matrix*, and it generalizes the gradient to vector-valued
    functions. As youâ€™ve likely realized, layers of neural networks are a great source
    of vector-valued functions for which youâ€™d like to derivate.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æœ€ç®€å•çš„æƒ…å†µï¼›å¤šå˜é‡æ ‡é‡å‡½æ•°çš„æ¢¯åº¦å¯ä»¥å†™æˆé›…å¯æ¯”çŸ©é˜µã€‚ç„¶è€Œï¼Œä¸€æ—¦æˆ‘ä»¬æœ‰äº†ï¼ˆå‘é‡ï¼‰å¯¼æ•°çš„å‘é‡ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶å†™æˆçŸ©é˜µï¼›è¿™é‡Œçš„å®ç”¨æ€§å®é™…ä¸Šåªåœ¨äºç¬¦å·ï¼Œè™½ç„¶å¦‚æ­¤ã€‚å½“æ‚¨å°†ä¸€ç³»åˆ—å¤šå˜é‡æ ‡é‡å‡½æ•°æ”¶é›†æˆå‡½æ•°å‘é‡æ—¶ï¼Œç›¸å…³çš„æ¢¯åº¦å‘é‡æ˜¯å¯¼æ•°çš„å‘é‡ã€‚è¿™ç§°ä¸º
    *é›…å¯æ¯”çŸ©é˜µ*ï¼Œå®ƒå°†æ¢¯åº¦æ¨å¹¿åˆ°çŸ¢é‡å€¼å‡½æ•°ã€‚æ­£å¦‚æ‚¨å¯èƒ½å·²ç»æ„è¯†åˆ°çš„é‚£æ ·ï¼Œç¥ç»ç½‘ç»œå±‚æ˜¯å¸Œæœ›è¿›è¡Œå¯¼æ•°æ¨å¯¼çš„çŸ¢é‡å€¼å‡½æ•°çš„ç»ä½³æ¥æºã€‚
- en: 'If youâ€™re convinced mini-batches are useful, itâ€™s time to discuss *DataLoaders*â€”a
    simple PyTorch API for facilitating mini-batch access from a large dataset. The
    key parameters for a DataLoader are `batch_size`, `shuffle`, and `num_workers`.
    The batch size is easy to understand: itâ€™s the number of samples included in each
    batch (often an integer factor of the total size of the dataset). Often a shuffle
    operation is applied in serving up these batches; the shuffle allows batches in
    each epoch to be shown to the network in a randomized order; this is intended
    to improve robustness. Finally, `num_workers` is a parallelization parameter for
    the CPUâ€™s batch generation.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ç¡®ä¿¡å°æ‰¹é‡å¾ˆæœ‰ç”¨ï¼Œç°åœ¨æ˜¯è®¨è®º *DataLoaders* çš„æ—¶å€™äº†â€”â€”è¿™æ˜¯ä¸€ä¸ªç®€å•çš„ PyTorch APIï¼Œç”¨äºä»å¤§å‹æ•°æ®é›†ä¸­æå–å°æ‰¹é‡ã€‚DataLoader
    çš„å…³é”®å‚æ•°åŒ…æ‹¬ `batch_size`ã€`shuffle` å’Œ `num_workers`ã€‚æ‰¹æ¬¡å¤§å°å¾ˆå®¹æ˜“ç†è§£ï¼šæ¯ä¸ªæ‰¹æ¬¡ä¸­åŒ…å«çš„æ ·æœ¬æ•°é‡ï¼ˆé€šå¸¸æ˜¯æ•°æ®é›†æ€»å¤§å°çš„æ•´æ•°å› å­ï¼‰ã€‚é€šå¸¸ä¼šå¯¹è¿™äº›æ‰¹æ¬¡åº”ç”¨éšæœºé¡ºåºæ“ä½œï¼›è¿™æ—¨åœ¨æé«˜æ¨¡å‹çš„ç¨³å¥æ€§ã€‚æœ€åï¼Œ`num_workers`
    æ˜¯ç”¨äº CPU æ‰¹æ¬¡ç”Ÿæˆçš„å¹¶è¡ŒåŒ–å‚æ•°ã€‚
- en: 'The utility of a DataLoader is really best understood via demonstration:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: DataLoader çš„æ•ˆç”¨æœ€å¥½é€šè¿‡æ¼”ç¤ºæ¥ç†è§£ï¼š
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The first important detail in this code is that any of its generators will be
    reading in mini-batches from your total dataset and can be instructed to load
    those batches in parallel. Note also that any differential steps in the model
    computations will now be operating on these mini-batches.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£ç ä¸­çš„ç¬¬ä¸€ä¸ªé‡è¦ç»†èŠ‚æ˜¯ï¼Œå®ƒçš„ä»»ä½•ç”Ÿæˆå™¨éƒ½å°†ä»æ‚¨çš„æ€»æ•°æ®é›†ä¸­è¯»å–å°æ‰¹é‡ï¼Œå¹¶å¯ä»¥æŒ‡ç¤ºå¹¶è¡ŒåŠ è½½è¿™äº›æ‰¹æ¬¡ã€‚è¿˜è¦æ³¨æ„ï¼Œæ¨¡å‹è®¡ç®—ä¸­çš„ä»»ä½•å·®åˆ†æ­¥éª¤ç°åœ¨å°†åœ¨è¿™äº›å°æ‰¹é‡ä¸Šè¿è¡Œã€‚
- en: Itâ€™s easy to think of DataLoaders as merely a tool for code cleanliness (which,
    admittedly, it does improve), but itâ€™s important to not underestimate how the
    control of batch order, parallelization, and shape are significant features for
    training your model. Lastly, the structure of your code now looks like batch gradient
    descent, but it is taking advantage of mini-batching, further exposing what your
    code actually does instead of the steps necessary to do it.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å®¹æ˜“è®¤ä¸º DataLoaders åªæ˜¯ç”¨äºæé«˜ä»£ç æ¸…æ´åº¦çš„å·¥å…·ï¼ˆè¯šç„¶ï¼Œå®ƒç¡®å®æ”¹è¿›äº†ï¼‰ï¼Œä½†é‡è¦çš„æ˜¯ä¸è¦ä½ä¼°æ‰¹æ¬¡é¡ºåºã€å¹¶è¡ŒåŒ–å’Œå½¢çŠ¶æ§åˆ¶å¯¹æ¨¡å‹è®­ç»ƒçš„é‡è¦æ€§ã€‚æœ€åï¼Œæ‚¨çš„ä»£ç ç»“æ„ç°åœ¨çœ‹èµ·æ¥åƒæ˜¯æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼Œä½†å®ƒåˆ©ç”¨äº†å°æ‰¹é‡ï¼Œè¿›ä¸€æ­¥æš´éœ²äº†ä»£ç å®é™…æ‰§è¡Œçš„å†…å®¹è€Œéæ‰€éœ€æ­¥éª¤ã€‚
- en: Database Snapshots
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®åº“å¿«ç…§
- en: 'Letâ€™s round out this section by stepping back from these fancy technologies
    to discuss something important and classic: snapshotting a production database.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€šè¿‡è¿œç¦»è¿™äº›é«˜çº§æŠ€æœ¯æ¥ç»“æŸè¿™ä¸€éƒ¨åˆ†ï¼Œè®¨è®ºä¸€äº›é‡è¦è€Œç»å…¸çš„äº‹æƒ…ï¼šå¯¹ç”Ÿäº§æ•°æ®åº“è¿›è¡Œå¿«ç…§ã€‚
- en: An extremely likely scenario is that the engineers (potentially also you) who
    have built the recommendations server are writing their logs and other application
    data to an SQL database. More likely than not, this database architecture and
    deployment are optimized for fast querying by the application across its most
    common use cases. As weâ€™ve discussed, those logs may be in an event-style schema,
    and there are other tables that may require aggregation and roll-up to make any
    sense. For example, a *current inventory* table may require knowledge of start-of-day
    inventory and then aggregate a list of purchase events.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ææœ‰å¯èƒ½çš„æƒ…å†µæ˜¯ï¼Œå»ºç«‹æ¨èæœåŠ¡å™¨çš„å·¥ç¨‹å¸ˆä»¬ï¼ˆå¯èƒ½è¿˜åŒ…æ‹¬æ‚¨ï¼‰æ­£åœ¨å°†å…¶æ—¥å¿—å’Œå…¶ä»–åº”ç”¨ç¨‹åºæ•°æ®å†™å…¥ SQL æ•°æ®åº“ã€‚å¾ˆå¯èƒ½ï¼Œè¿™ç§æ•°æ®åº“æ¶æ„å’Œéƒ¨ç½²æ˜¯é’ˆå¯¹åº”ç”¨ç¨‹åºè·¨å…¶æœ€å¸¸è§ä½¿ç”¨æƒ…å†µè¿›è¡Œå¿«é€ŸæŸ¥è¯¢è¿›è¡Œäº†ä¼˜åŒ–ã€‚æ­£å¦‚æˆ‘ä»¬è®¨è®ºè¿‡çš„é‚£æ ·ï¼Œè¿™äº›æ—¥å¿—å¯èƒ½å¤„äºäº‹ä»¶æ ·å¼æ¶æ„ä¸­ï¼Œè¿˜æœ‰å…¶ä»–å¯èƒ½éœ€è¦èšåˆå’Œæ±‡æ€»ä»¥å¾—å‡ºä»»ä½•æ„ä¹‰çš„è¡¨æ ¼ã€‚ä¾‹å¦‚ï¼Œ*å½“å‰åº“å­˜*
    è¡¨å¯èƒ½éœ€è¦äº†è§£æ¯å¤©å¼€å§‹çš„åº“å­˜ï¼Œç„¶åèšåˆä¸€ç³»åˆ—è´­ä¹°äº‹ä»¶åˆ—è¡¨ã€‚
- en: 'All told, the production SQL database is usually a crucial component in the
    stack thatâ€™s geared to specific use. As the downstream consumer of this data,
    you may find yourself wanting different schemas, wanting lots of access to this
    database, and performing serious operations on this data. The most common paradigm
    is *database snapshotting*. Snapshotting is a functionality provided by various
    flavors of SQL to performantly make a clone of a database. While this snapshotting
    may take form in a variety of ways, letâ€™s focus on a few that serve to simplify
    our systems and ensure they have the necessary data on hand:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»çš„æ¥è¯´ï¼Œç”Ÿäº§SQLæ•°æ®åº“é€šå¸¸æ˜¯é’ˆå¯¹ç‰¹å®šç”¨é€”è®¾è®¡çš„å †æ ˆä¸­çš„å…³é”®ç»„ä»¶ã€‚ä½œä¸ºè¿™äº›æ•°æ®çš„ä¸‹æ¸¸æ¶ˆè´¹è€…ï¼Œæ‚¨å¯èƒ½å¸Œæœ›æ‹¥æœ‰ä¸åŒçš„æ¨¡å¼ï¼Œå¯¹è¿™äº›æ•°æ®åº“æœ‰å¤§é‡çš„è®¿é—®ï¼Œå¹¶å¯¹è¿™äº›æ•°æ®æ‰§è¡Œé‡è¦çš„æ“ä½œã€‚æœ€å¸¸è§çš„èŒƒå¼æ˜¯*æ•°æ®åº“å¿«ç…§*ã€‚å¿«ç…§æ˜¯ç”±å„ç§SQLç‰ˆæœ¬æä¾›çš„åŠŸèƒ½ï¼Œç”¨äºé«˜æ•ˆåœ°åˆ›å»ºæ•°æ®åº“çš„å…‹éš†ã€‚è™½ç„¶å¿«ç…§å¯ä»¥é‡‡ç”¨å„ç§å½¢å¼ï¼Œä½†è®©æˆ‘ä»¬ä¸“æ³¨äºä¸€äº›ç®€åŒ–ç³»ç»Ÿå¹¶ç¡®ä¿å…¶å…·å¤‡æ‰€éœ€æ•°æ®çš„æ–¹å¼ï¼š
- en: A daily table snapshot may be tied to an `as_of` field, or *the state of this
    table on this day*.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯æ—¥è¡¨å¿«ç…§å¯èƒ½ä¸`as_of`å­—æ®µç›¸å…³è”ï¼Œæˆ–è€…*è¿™ä¸ªè¡¨åœ¨è¿™ä¸€å¤©çš„çŠ¶æ€*ã€‚
- en: A daily table snapshot may be limited by time to see *what records have been
    added today*.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯æ—¥è¡¨å¿«ç…§å¯èƒ½å—æ—¶é—´é™åˆ¶ï¼Œåªèƒ½æŸ¥çœ‹*ä»Šå¤©æ–°å¢äº†å“ªäº›è®°å½•*ã€‚
- en: An event table snapshot may be used to feed a set of events into an event stream
    processor like Segment (note that you may also set up live event streams like
    Kafka).
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: äº‹ä»¶è¡¨å¿«ç…§å¯ç”¨äºå°†ä¸€ç»„äº‹ä»¶é¦ˆé€åˆ°åƒSegmentè¿™æ ·çš„äº‹ä»¶æµå¤„ç†å™¨ï¼ˆè¯·æ³¨æ„ï¼Œæ‚¨ä¹Ÿå¯ä»¥è®¾ç½®åƒKafkaè¿™æ ·çš„å®æ—¶äº‹ä»¶æµï¼‰ã€‚
- en: An hourly aggregated table can be used for status logging or monitoring.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯å°æ—¶æ±‡æ€»çš„è¡¨å¯ç”¨äºçŠ¶æ€è®°å½•æˆ–ç›‘è§†ã€‚
- en: In general, the paradigm is usually to operate on snapshots for downstream data
    processing. Many of the kinds of data processing we mentioned earlierâ€”like computing
    user similarityâ€”are operations that may require significant data reads. *Itâ€™s
    important to not build ML applications that require extensive querying on the
    production database*, because doing so would likely decrease performance of the
    app and result in a slower user experience. This decrease will undermine the improvement
    made possible by your recommendations.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€èˆ¬è€Œè¨€ï¼ŒèŒƒå¼é€šå¸¸æ˜¯æ“ä½œä¸‹æ¸¸æ•°æ®å¤„ç†çš„å¿«ç…§ã€‚æˆ‘ä»¬å‰é¢æåˆ°çš„è®¸å¤šæ•°æ®å¤„ç†ç§ç±»ï¼Œå¦‚è®¡ç®—ç”¨æˆ·ç›¸ä¼¼æ€§ï¼Œéƒ½æ˜¯å¯èƒ½éœ€è¦å¤§é‡æ•°æ®è¯»å–çš„æ“ä½œã€‚*é‡è¦çš„æ˜¯ä¸è¦æ„å»ºéœ€è¦åœ¨ç”Ÿäº§æ•°æ®åº“ä¸Šè¿›è¡Œå¤§é‡æŸ¥è¯¢çš„MLåº”ç”¨ç¨‹åº*ï¼Œå› ä¸ºè¿™æ ·åšå¯èƒ½ä¼šé™ä½åº”ç”¨ç¨‹åºçš„æ€§èƒ½ï¼Œå¹¶å¯¼è‡´ç”¨æˆ·ä½“éªŒå˜æ…¢ã€‚è¿™ç§é™ä½å°†ä¼šæŸå®³æ¨èç³»ç»Ÿå¯èƒ½å®ç°çš„æ”¹è¿›ã€‚
- en: Once youâ€™ve snapshotted the tables youâ€™re interested in, you can often find
    a collection of data pipelines useful to transform that data into even more specific
    tables in your *data warehouse* (where you should be doing most of your work anyway).
    Tools like Dagster, dbt, Apache Airflow, Argo, and Luigi are popular data-pipeline
    and workflow orchestration tools for extract, transform, load (ETL) operations.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ‚¨å¯¹æ„Ÿå…´è¶£çš„è¡¨è¿›è¡Œäº†å¿«ç…§ï¼Œé€šå¸¸å¯ä»¥æ‰¾åˆ°ä¸€ç³»åˆ—æ•°æ®ç®¡é“ï¼Œæœ‰åŠ©äºå°†æ•°æ®è½¬æ¢ä¸ºæ›´å…·ä½“çš„è¡¨æ ¼ï¼Œå­˜æ”¾åœ¨*æ•°æ®ä»“åº“*ä¸­ï¼ˆæ‚¨åº”è¯¥åœ¨è¿™é‡Œå®Œæˆå¤§éƒ¨åˆ†å·¥ä½œï¼‰ã€‚åƒDagsterã€dbtã€Apache
    Airflowã€Argoå’ŒLuigiè¿™æ ·çš„å·¥å…·æ˜¯æµè¡Œçš„æ•°æ®ç®¡é“å’Œå·¥ä½œæµç¼–æ’å·¥å…·ï¼Œç”¨äºæå–ã€è½¬æ¢å’ŒåŠ è½½ï¼ˆETLï¼‰æ“ä½œã€‚
- en: Data Structures for Learning and Inference
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç”¨äºå­¦ä¹ å’Œæ¨æ–­çš„æ•°æ®ç»“æ„
- en: This section introduces three important data structures that will enable our
    recommendation system to perform complex operations quickly. The goal of each
    structure is to sacrifice precision as little as possible, while speeding up access
    to the data in real time. As youâ€™ll see, these data structures form the backbone
    of the real-time inference pipeline and approximate what takes place in the batch
    pipeline as accurately as possible.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚ä»‹ç»äº†ä¸‰ç§é‡è¦çš„æ•°æ®ç»“æ„ï¼Œè¿™äº›ç»“æ„å°†ä½¿æˆ‘ä»¬çš„æ¨èç³»ç»Ÿèƒ½å¤Ÿå¿«é€Ÿæ‰§è¡Œå¤æ‚æ“ä½œã€‚æ¯ç§ç»“æ„çš„ç›®æ ‡æ˜¯å°½å¯èƒ½å°‘åœ°ç‰ºç‰²ç²¾åº¦ï¼ŒåŒæ—¶åŠ é€Ÿå¯¹å®æ—¶æ•°æ®çš„è®¿é—®ã€‚æ­£å¦‚æ‚¨å°†çœ‹åˆ°çš„ï¼Œè¿™äº›æ•°æ®ç»“æ„æ„æˆäº†å®æ—¶æ¨æ–­ç®¡é“çš„æ ¸å¿ƒï¼Œå¹¶å°½å¯èƒ½ç²¾ç¡®åœ°è¿‘ä¼¼äº†æ‰¹å¤„ç†ç®¡é“çš„è¿è¡Œè¿‡ç¨‹ã€‚
- en: 'The three data structures are as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸‰ç§æ•°æ®ç»“æ„å¦‚ä¸‹ï¼š
- en: Vector search/ANN index
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‘é‡æœç´¢/ANNç´¢å¼•
- en: Bloom filters for candidate filtering
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¸ƒéš†è¿‡æ»¤å™¨ç”¨äºå€™é€‰è¿‡æ»¤
- en: Feature stores
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç‰¹å¾å­˜å‚¨
- en: So far, weâ€™ve discussed the necessary components for getting data flowing in
    your system. These help organize data to make it more accessible during the learning
    and inference processes. Also, weâ€™ll find some shortcuts to speed up inference
    during retrieval. Vector search will allow us to identify similar items at scale.
    Bloom filters will allow us to rapidly evaluate many criteria for excluding results.
    Feature stores will provide us with necessary data about users for recommendation
    inference.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Vector Search
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have discussed user similarity and item similarity in terms of understanding
    the relationships between those entities, but we havenâ€™t talked about any *acceleration
    structures* for these processes.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: First letâ€™s discuss a bit of terminology;. If we think of a collection of vectors
    that represent entities with a similarity metric provided by a distance function,
    we refer to this as a *latent space.* The simple goal is to utilize our latent
    space and its associated similarity metric (or complementary distance metric)
    to be able to retrieve *similar* items quickly. In our previous examples with
    similarity, we talked about neighborhoods of users and how they can be utilized
    to build an affinity score between users and unseen items. But how do you find
    the neighborhood?
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: To understand this, recall that we defined neighborhoods of an element <math
    alttext="x"><mi>x</mi></math> , written <math alttext="script upper N left-parenthesis
    x right-parenthesis"><mrow><mi>ğ’©</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>
    , as the set of <math alttext="k"><mi>k</mi></math> elements in the latent space
    with the maximum similarity; or said differently, the set of <math alttext="j"><mi>j</mi></math>
    th order statistics for <math alttext="j less-than-or-equal-to k"><mrow><mi>j</mi>
    <mo>â‰¤</mo> <mi>k</mi></mrow></math> from the sample of item similarities to <math
    alttext="x"><mi>x</mi></math> . These *<math alttext="k"><mi>k</mi></math> -nearest
    neighbors*, as theyâ€™re often called, will be used as the set of elements considered
    similar to <math alttext="x"><mi>x</mi></math> .
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'These vectors from CF yield a few other useful side effects:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: A simple recommender that randomly samples unseen items from a user neighborhoodâ€™s
    liked items
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predictions about features of a user, from known features of users in the neighborhood
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User segmentation via taste similarity
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So how can we speed up these processes? One of the first significant improvements
    in this area came from inverted indices. Utilizing inverted indices is at its
    core carefully constructing a large hash between tokens of the query (for text-based
    search) and the candidates.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach is great for tokenizable entities like sentences or small-lexicon
    collections. Given the ability to look up items that share one or many tokens
    with the query, you can even use a general latent embedding to rank the candidate
    responses by similarity. This approach deserves extra consideration as you scale:
    it incurs a speed cost because it entails two steps, and because the similarity
    distribution may not be well correlated with the token similarity required to
    return many more candidates than we need.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Classic approaches to building a search system are based on large lookup tables
    and feel deterministic. As we move toward ANN lookup, we want to relax some of
    that strong deterministic behavior and introduce data structures that make assumptions
    to *prune* these large indices. Instead of building indices for only tokenizable
    components of your elements, you could precompute the *k*-d tree and use the indices
    as the index. The *k*-d tree would precompute the nearest neighbors in a batch
    process (which may be slow), to populate a top-*k* response for fast lookup. *k*-d
    trees are an efficient data structure for encoding the preceding neighborhoods
    but are notoriously slow to read from in higher dimensions. Using them instead
    to build inverted indices, though, can be a great improvement.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: More recently, explicitly using vector databases with vector search is becoming
    much more possible and feasible. Elasticsearch has added this capability; [Faiss](https://oreil.ly/AZ-Ai)
    is a Python library that helps you implement this functionality in your systems;
    [Pinecone](https://oreil.ly/LSaos) is a vector-database system explicitly targeting
    this goal; and [Weaviate](https://oreil.ly/Z6la_) is a native vector-database
    architecture that allows you to layer the previous token-based inverted indices
    and vector similarity search.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Approximate Nearest Neighbors
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What are this elementâ€™s *k*-nearest neighbors? Incredibly, approximate nearest
    neighbors (ANN) can get very high accuracy compared to the actual nearest neighbors,
    and you get there faster with head-spinning speedups. You often are satisfied
    with approximate solutions to these problems.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'One open source library that specializes in these approximations is [PyNNDescent](https://oreil.ly/i5LyM),
    which uses clever speedups via both optimized implementation and careful mathematical
    tricks. With ANN, you are opened up to two strategies as discussed:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: The pre-index can be dramatically improved.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On queries without a pre-indexing option, you can still expect good performance.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In practice, these similarity lookups are incredibly important for making your
    applications actually work. While weâ€™ve mostly talked about recommendations for
    full known catalogs of items, we cannot assume this in other recommendation contexts.
    These include the following:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Query-based recommendations (like search)
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contextual recommendations
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cold-starting new items
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As we go, you will see more and more references to similarity in spaces and
    nearest neighbors; at each of those moments, think: â€œI know how to make this fast!â€'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Bloom Filters
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Bloom filters* are probabilistic data structures that allow us to test for
    set inclusion very efficiently but with a downside: set exclusion is deterministic,
    but set inclusion is probabilistic. *In practice, this means that asking the question
    â€œIs <math alttext="x"><mi>x</mi></math> in this setâ€ never results in a false
    negative but may result in a false positive!* Note that this type-I error increases
    as the size of the bloom increases.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Via vector search, we have identified a large pool of potential recommendations
    for the user. From this pool, we need to do some immediate elimination. The most
    obvious type of high-level filtering thatâ€™s essential is to remove those items
    that the *user has previously not shown interest in or has already purchased.*
    Youâ€™ve probably had the experience of being recommended the same item, over and
    over, and thinking, â€œI donâ€™t want this; stop showing me this.â€ From the simple
    CF models weâ€™ve introduced, you may now see why this could happen.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'The system has identified a set of items via CF that youâ€™re more likely to
    pick. Without any outside influence, those computations will continue to return
    the same results, and youâ€™ll never escape those recommendations. As the system
    designer, you may start with a heuristic:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: If the user has seen this item recommended three times and never clicked, letâ€™s
    not show it to them anymore.
  id: totrans-117
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is a totally reasonable strategy to improve *freshness* (the idea of ensuring
    users see new item recommendations) in your recommendation system. While this
    is a simple strategy to improve your recommendations, how might you implement
    this at scale?
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'A bloom filter may be used by defining the sets in question with the following:
    â€œHas this user seen this item recommended three times and never clicked?â€ Bloom
    filters have a caveat that theyâ€™re additive only: once something is in the bloom,
    you canâ€™t remove it. This is not a problem when observing a binary state like
    this heuristic.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s construct a user-item ID to use as our hash in the bloom. Remember that
    the key feature of the bloom filter is to quickly determine whether the hashed
    item is in the bloom. When we observe a user-item pair that satisfies the preceding
    criteria, take that pair as an ID and hash it. Now, because that hashed pair can
    be easily reconstructed from a list of items for a user, we have a very fast way
    to filter.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s discuss a few technical details on this topic. First, you might want to
    do a variety of kinds of filteringâ€”maybe freshness is one, and another may be
    items the user has already bought, and a third could exclude items that have sold
    out.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Here it would be good to implement each of these filters independently; the
    first two can follow our user-item ID hashing as before, and the third one can
    be a hash only on item IDs.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Another consideration is populating the bloom filters. Itâ€™s best practice to
    build these blooms from a database during the offline batch jobs. On whatever
    schedule your batch training is run, rebuild your blooms from the records storage
    to ensure youâ€™re keeping your blooms accurate. Remember that blooms donâ€™t allow
    deletion, so in the previous example, if an item goes from sold out to restocked,
    your batch refresh of your blooms can pick up the availability again. In between
    batch retraining, adding to a bloom is also very performant, so you can continue
    to add to the bloom as you observe more data that needs to be considered for the
    filtering in real time. Be sure these transactions are logged to a table, though!
    That logging will be important when you want to refresh.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'Fun Aside: Bloom Filters as the Recommendation System'
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bloom filters not only provide an effective way to eliminate some recommendations
    based on conditions for inclusion, but can also be used to do the recommending
    itself! In particular, [â€œAn Item/User Representation for Recommender Systems Based
    on Bloom Filtersâ€](https://oreil.ly/VsvN2) by Manuel Pozo et al. shows that for
    high-dimensional feature sets with a lot of sparsity (as we discussed in [ChapterÂ 3](ch03.html#ch:math)),
    the type of hashing bloom filters do can help overcome some of the key challenges
    in defining good similarity functions!
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s observe that we can do two natural operations on sets via the bloom filter
    data structures. First, consider two sets <math alttext="upper A"><mi>A</mi></math>
    and <math alttext="upper B"><mi>B</mi></math> , and associate to them bloom filters
    <math alttext="script upper B script upper F Subscript upper A"><msub><mi>â„¬â„±</mi>
    <mi>A</mi></msub></math> and <math alttext="script upper B script upper F Subscript
    upper B"><msub><mi>â„¬â„±</mi> <mi>B</mi></msub></math> . Then whatâ€™s the definition
    of <math alttext="upper A intersection upper B"><mrow><mi>A</mi> <mo>âˆ©</mo> <mi>B</mi></mrow></math>
    ? Can we come up with a bloom filter for this intersection? Yep! Recall that our
    bloom filters are guaranteed to tell us when an element is not contained in the
    set, but if an element is in the set, the bloom filter can respond with only a
    certain probability. In this case, weâ€™d simply look for elements that are *in*
    according to <math alttext="script upper B script upper F Subscript upper A"><msub><mi>â„¬â„±</mi>
    <mi>A</mi></msub></math> *AND* *in* according to <math alttext="script upper B
    script upper F Subscript upper B"><msub><mi>â„¬â„±</mi> <mi>B</mi></msub></math> .
    Of course, the set of items returned as *in* each set is larger than the actual
    set (i.e., <math alttext="upper A subset-of script upper B script upper F Subscript
    upper A"><mrow><mi>A</mi> <mo>âŠ‚</mo> <msub><mi>â„¬â„±</mi> <mi>A</mi></msub></mrow></math>
    ), so the intersection will also be larger:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper A intersection upper B subset-of script upper B script
    upper F Subscript upper A intersection script upper B script upper F Subscript
    upper B" display="block"><mrow><mi>A</mi> <mo>âˆ©</mo> <mi>B</mi> <mo>âŠ‚</mo> <msub><mi>â„¬â„±</mi>
    <mi>A</mi></msub> <mo>âˆ©</mo> <msub><mi>â„¬â„±</mi> <mi>B</mi></msub></mrow></math>
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper A intersection upper B subset-of script upper B script
    upper F Subscript upper A intersection script upper B script upper F Subscript
    upper B" display="block"><mrow><mi>A</mi> <mo>âˆ©</mo> <mi>B</mi> <mo>âŠ‚</mo> <msub><mi>â„¬â„±</mi>
    <mi>A</mi></msub> <mo>âˆ©</mo> <msub><mi>â„¬â„±</mi> <mi>B</mi></msub></mrow></math>
- en: Note that you can compute the exact difference in cardinality via information
    about your choice of hashing functions. Also note that the equation is an abuse
    of notation by calling <math alttext="script upper B script upper F Subscript
    upper A"><msub><mi>â„¬â„±</mi> <mi>A</mi></msub></math> the set of things returned
    by the bloom filter corresponding to <math alttext="upper A"><mi>A</mi></math>
    .
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, we also need to construct the union. This is similarly easy by considering
    elements that are *in* according to <math alttext="script upper B script upper
    F Subscript upper A"><msub><mi>â„¬â„±</mi> <mi>A</mi></msub></math> *OR* *in* according
    to <math alttext="script upper B script upper F Subscript upper B"><msub><mi>â„¬â„±</mi>
    <mi>B</mi></msub></math> . And so, similarly:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper A union upper B subset-of script upper B script upper F
    Subscript upper A union script upper B script upper F Subscript upper B" display="block"><mrow><mi>A</mi>
    <mo>âˆª</mo> <mi>B</mi> <mo>âŠ‚</mo> <msub><mi>â„¬â„±</mi> <mi>A</mi></msub> <mo>âˆª</mo>
    <msub><mi>â„¬â„±</mi> <mi>B</mi></msub></mrow></math>
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper A union upper B subset-of script upper B script upper F
    Subscript upper A union script upper B script upper F Subscript upper B" display="block"><mrow><mi>A</mi>
    <mo>âˆª</mo> <mi>B</mi> <mo>âŠ‚</mo> <msub><mi>â„¬â„±</mi> <mi>A</mi></msub> <mo>âˆª</mo>
    <msub><mi>â„¬â„±</mi> <mi>B</mi></msub></mrow></math>
- en: 'Now, if we consider items <math alttext="upper X"><mi>X</mi></math> and <math
    alttext="upper Y"><mi>Y</mi></math> as concatenated vectors of potentially many
    features, and hash those concatenated features, we are representing each of them
    as the bitwise vectors of our bloom. From before, we saw that the intersection
    of two blooms makes sense, and in fact is equivalent to the bitwise *AND* of their
    bloom representations. This means two itemsâ€™ feature similarities can be expressed
    by the bitwise *and* similarity of their bloom hashes:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="normal s normal i normal m left-parenthesis upper X comma upper
    Y right-parenthesis equals StartAbsoluteValue script upper B script upper F left-parenthesis
    upper X right-parenthesis intersection script upper B script upper F left-parenthesis
    upper Y right-parenthesis EndAbsoluteValue equals script upper B script upper
    F left-parenthesis upper X right-parenthesis asterisk Subscript normal b normal
    i normal t normal w normal i normal s normal e Baseline script upper B script
    upper F left-parenthesis upper X right-parenthesis" display="block"><mrow><mi>sim</mi>
    <mrow><mo>(</mo> <mi>X</mi> <mo>,</mo> <mi>Y</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mrow><mo>|</mo> <mi>â„¬â„±</mi> <mrow><mo>(</mo> <mi>X</mi> <mo>)</mo></mrow> <mo>âˆ©</mo>
    <mi>â„¬â„±</mi> <mrow><mo>(</mo> <mi>Y</mi> <mo>)</mo></mrow> <mo>|</mo></mrow> <mo>=</mo>
    <mi>â„¬â„±</mi> <mrow><mo>(</mo> <mi>X</mi> <mo>)</mo></mrow> <msub><mo>*</mo> <mi>bitwise</mi></msub>
    <mi>â„¬â„±</mi> <mrow><mo>(</mo> <mi>X</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal s normal i normal m left-parenthesis upper X comma upper
    Y right-parenthesis equals StartAbsoluteValue script upper B script upper F left-parenthesis
    upper X right-parenthesis intersection script upper B script upper F left-parenthesis
    upper Y right-parenthesis EndAbsoluteValue equals script upper B script upper
    F left-parenthesis upper X right-parenthesis asterisk Subscript normal b normal
    i normal t normal w normal i normal s normal e Baseline script upper B script
    upper F left-parenthesis upper X right-parenthesis" display="block"><mrow><mi>sim</mi>
    <mrow><mo>(</mo> <mi>X</mi> <mo>,</mo> <mi>Y</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mrow><mo>|</mo> <mi>â„¬â„±</mi> <mrow><mo>(</mo> <mi>X</mi> <mo>)</mo></mrow> <mo>âˆ©</mo>
    <mi>â„¬â„±</mi> <mrow><mo>(</mo> <mi>Y</mi> <mo>)</mo></mrow> <mo>|</mo></mrow> <mo>=</mo>
    <mi>â„¬â„±</mi> <mrow><mo>(</mo> <mi>X</mi> <mo>)</mo></mrow> <msub><mo>*</mo> <mi>bitwise</mi></msub>
    <mi>â„¬â„±</mi> <mrow><mo>(</mo> <mi>X</mi> <mo>)</mo></mrow></mrow></math>
- en: For static datasets, this method has real advantages, including speed, scalability,
    and performance. Limitations are based on a variety of features and on the ability
    to change the set of possible items. Later we will discuss *locally sensitive
    hashing*, which further iterates on lookup speed with lower risks of collision
    in high-dimensional spaces, and some similar ideas will reemerge.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Feature Stores
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have focused on recommendation systems that we might call *pure collaborative
    filtering*. Weâ€™ve made use of the user- or item-similarity data only when attempting
    to make good recommendations. If youâ€™ve been wondering, â€œHey, what about information
    about the actual users and items?â€ your curiosity will now be sated.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a huge variety of reasons you could be interested in features in
    addition to your previous CF methods. Letâ€™s list a few high-level concerns:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: You may wish to show new users a specific set of items first.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may wish to consider geographic boundaries in your recommendations.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distinguishing between children and adults may be important for the types of
    recommendations theyâ€™re given.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Item features may be used to ensure high-level diversity in the recommendations
    (more to come in [ChapterÂ 15](ch15.html#Diversity)).
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User features can enable various kinds of experimental testing.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Item features could be used to group items into sets for contextual recommendations
    (more to come in [ChapterÂ 15](ch15.html#Diversity)).
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition to these issues, another kind of feature is often essential: real-time
    features. While the point of feature stores is to provide real-time access to
    all the necessary features, itâ€™s worthwhile to distinguish stable features that
    change infrequently from real-time features that we anticipate will change often.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Some important examples of a real-time feature store are dynamic prices, current
    item availability, *trending* status, wish-list status, and so on. These features
    may change throughout the day, and we want their values in the feature store to
    be mutable in real-time via other services and systems. Therefore, the real-time
    feature store will need to provide API access for feature mutation. This is something
    you may not want to provide for *stable* features.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: When we design our feature store, weâ€™re likely to want the stable features to
    be built from data warehouse tables via ETLs and transformations, and we likely
    want the real-time features to be built this way as well, but on a faster schedule
    or allowing API access for mutation. In either case, the key quality of a feature
    store is *very fast read access*. Itâ€™s often a good idea to separately build feature
    stores for offline training of models that can be built in test to ensure support
    for new models.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: So how might the architecture and implementation look? See [FigureÂ 6-3](#fig:feat-store).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature Store](assets/brpj_0603.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: Figure 6-3\. Demonstration of a feature store
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Designing a feature store involves designing pipelines that define and *transform
    the features into that store* (coordinated via things like Airflow, Luigi, Argo,
    etc.) and often look similar to the type of data pipelines used in building our
    collector. One additional complication that the feature store needs to concern
    itself with is a speed layer. During our discussion of the lambda architecture
    earlier in this chapter, we mentioned that we can think of batch data processing
    for the collector and a more rapid speed layer for intermediary updates, but this
    is even more important for the feature store. The feature store may also need
    a *streaming layer*. This layer operates on continuous streams of data and can
    perform data transformations on those; it then writes the appropriate output to
    the online feature store in real time. This adds complexity because data transformations
    on streaming data present a very different set of challenges and often require
    different algorithmic strategies. Some technologies that help here are Spark Streaming
    and Kinesis. Youâ€™ll also need to configure the system to properly handle the data
    stream, the most common of which is Kafka. Data streaming layers involve many
    components and architectural considerations that fall outside our scope; if youâ€™re
    considering getting started with Kafka, check out [*Kafka: The Definitive Guide*](https://www.oreilly.com/library/view/kafka-the-definitive/9781492043072/)
    by Gwen Shapira et al. (Oâ€™Reilly).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: A feature store also needs a *storage layer*; many approaches exist here, but
    using a NoSQL database is common, especially in the online feature store. The
    reason is faster retrieval and the nature of the data storage. Feature stores
    for recommendation systems tend to be very key based (i.e., *get the features
    for this user*, or *get the features for this item*), which lend themselves well
    to key-value stores. Some example technologies here are DynamoDB, Redis, and Cassandra.
    The storage layer for an offline feature store may simply be an SQL-style database
    to reduce complexity, but instead youâ€™ll pay a tax of a delta between offline
    and online. This delta and others like it are called [*training-serving skew*](https://oreil.ly/IcE1R).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: A unique but essential aspect of feature stores is the *registry*. A registry
    is incredibly useful for a feature store because it coordinates existing features
    and information on how theyâ€™re defined. A more sophisticated instance of a registry
    also includes input and output schemas with typing, and distributional expectations.
    These are contracts that the data pipelines must adhere to and satisfy to avoid
    populating your feature store with garbage data. Additionally, the registryâ€™s
    definitions allow parallel data scientists and ML engineers to develop new features,
    use one anotherâ€™s features, and generally understand the assumptions of features
    their models may utilize.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: One important advantage of these registries is that they incentivize alignment
    between teams and developers. In particular, if you decide you care about *country*
    for your user, and you see a feature *country* in the registry, youâ€™re more likely
    to use that (or ask the developer whoâ€™s assigned to this feature in the registry)
    than to make a new one from scratch. Practically, data scientists make hundreds
    of small decisions and assumptions when defining their models, and this removes
    some of that load thatâ€™s relying on the existing resources.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Model Registries
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A closely related concept to feature registries is model registries. The concepts
    have a lot in common, but we caution you to think of them differently. A great
    model registry can have type contracts for the input and output of your models,
    and can serve many of the same benefits around alignment and clarity. A feature
    registry should really be focused on definitions of the business logic and features.
    Because feature engineering can also be model driven, speaking clearly about the
    differences between these two things can be challenging, so to sum it up, weâ€™ll
    focus on what they serve: a model registry concerns itself with ML models and
    the relevant metadata, whereas a feature registry concerns itself with features
    that models will use.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we need to talk about *serving* these features. Backed by the appropriately
    performant storage layer, we need to serve via API request the necessary feature
    vectors. Those feature vectors are details about the user that the model will
    need when serving recommendationsâ€”for example, the userâ€™s location or content
    age restrictions. The API can serve back the entire set of features for the key
    or allow for more specification. Often the responses are JSON serialized for fast
    data transfer. Itâ€™s important that the features being served are the *most up-to-date
    set of features*, and latency here is expected to be < 100 ms for more serious
    industrial applications.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: One important caveat here is that for offline training, these feature stores
    need to accommodate *time travel*. Because our goal during training is to give
    the model the appropriate data to learn in the *most generalizable way*, when
    training our model, itâ€™s crucial to not give it access to features out of time.
    This is called *data leakage* and can cause massive divergence in performance
    between training and production. The feature store for offline training thus must
    have knowledge of the features through time, so that during training, a time index
    may be provided to get the features as they were then. These `as_of` keys can
    be tied to the historical training data as we *replay* the history of what the
    user-item interactions looked like.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: With these pieces in placeâ€”and the important monitoring this system needsâ€”youâ€™ll
    be able to serve offline and online features to your models. In [PartÂ III](part03.html#ranking),
    you will see model architectures that make use of them.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Weâ€™ve discussed not only the crucial components necessary to hydrate your system
    and serve recommendations, but also some of the engineering building blocks needed
    to make those components a reality. Equipped with data loaders, embeddings, feature
    stores, and retrieval mechanisms, we are ready to start constructing our pipeline
    and system topology.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, weâ€™ll focus our sights on MLOps and the rest of the engineering
    work required to build and iterate on these systems. Itâ€™s going to be important
    for us to think carefully about deployment and monitoring so our recommendation
    systems are constrained to life in IPython Notebooks.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Continue onward to see the architectural considerations to move to production.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
