- en: Chapter 8\. Feature Selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We use feature selection to select features that are useful to the model. Irrelevant
    features may have a negative effect on a model. Correlated features can make coefficients
    in regression (or feature importance in tree models) unstable or difficult to
    interpret.
  prefs: []
  type: TYPE_NORMAL
- en: The *curse of dimensionality* is another issue to consider. As you increase
    the number of dimensions of your data, it becomes more sparse. This can make it
    difficult to pull out a signal unless you have more data. Neighbor calculations
    tend to lose their usefulness as more dimensions are added.
  prefs: []
  type: TYPE_NORMAL
- en: Also, training time is usually a function of the number of columns (and sometimes
    it is worse than linear). If you can be concise and precise with your columns,
    you can have a better model in less time. We will walk through some examples using
    the `agg_df` dataset from the last chapter. Remember that this is the Titanic
    dataset with some extra columns for cabin information. Because this dataset is
    aggregating numeric values for each cabin, it will show many correlations. Other
    options include PCA and looking at the `.feature_importances_` of a tree classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Collinear Columns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can use the previously defined `correlated_columns` function or run the
    following code to find columns that have a correlation coefficient of .95 or above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The Yellowbrick `Rank2` visualizer, shown previously, will plot a heat map of
    correlations.
  prefs: []
  type: TYPE_NORMAL
- en: The [rfpimp package](https://oreil.ly/MsnXc) has a visualization of *multicollinearity*.
    The `plot_dependence_heatmap` function trains a random forest for each numeric
    column from the other columns in a training dataset. The dependence value is the
    R2 score from the out-of-bag (OOB) estimates for predicting that column (see [Figure 8-1](#iddephm)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The suggested way to use this plot is to find values close to 1\. The label
    on the X axis is the feature that predicts the Y axis label. If a feature predicts
    another, you can remove the predicted feature (the feature on the Y axis). In
    our example, `fare` predicts `pclass`, `sibsp`, `parch`, and `embarked_Q`. We
    should be able to keep `fare` and remove the others and get similar performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![Dependence heat map. Pclass, sibsp, parch, and embarked_Q can be predicted
    from fare, so we can remove them.](assets/mlpr_0801.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. Dependence heat map. Pclass, sibsp, parch, and embarked_Q can be
    predicted from fare, so we can remove them.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here is code showing that we get a similar score if we remove these columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Lasso Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you use lasso regression, you can set an `alpha` parameter that acts as
    a regularization parameter. As you increase the value, it gives less weight to
    features that are less important. Here we use the `LassoLarsCV` model to iterate
    over various values of alpha and track the feature coefficients (see [Figure 8-2](#idlr1)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![Coefficients of features as alpha varies during lasso regression.](assets/mlpr_0802.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. Coefficients of features as alpha varies during lasso regression.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recursive Feature Elimination
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recursive feature elimination will remove the weakest features, then fit a
    model (see [Figure 8-3](#id20)). It does this by passing in a scikit-learn model
    with a `.coef_` or `.feature_importances_` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![Recursive feature elimination.](assets/mlpr_0803.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-3\. Recursive feature elimination.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We will use recursive feature elimination to find the 10 most important features.
    (In this aggregated dataset we find that we have leaked the survival column!)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Mutual Information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sklearn provides nonparametric tests that will use k-nearest neighbor to determine
    the *mutual information* between features and the target. Mutual information quantifies
    the amount of information gained by observing another variable. The value is zero
    or more. If the value is zero, then there is no relation between them (see [Figure 8-4](#mi1)).
    This number is not bounded and represents the number of *bits* shared between
    the feature and the target:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![Mutual information plot.](assets/mlpr_0804.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-4\. Mutual information plot.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Principal Component Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another option for feature selection is to run principal component analysis.
    Once you have the main principal components, examine the features that contribute
    to them the most. These are features that have more variance. Note that this is
    an unsupervised algorithm and doesn’t take `y` into account.
  prefs: []
  type: TYPE_NORMAL
- en: See [“PCA”](ch17.html#pca1) for more details.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Importance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most tree models provide access to a `.feature_importances_` attribute following
    training. A higher importance typically means that there is higher error when
    the feature is removed from the model. See the chapters for the various tree models
    for more details.
  prefs: []
  type: TYPE_NORMAL
