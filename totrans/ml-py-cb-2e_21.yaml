- en: Chapter 21\. Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 21.0 Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the heart of basic neural networks is the *unit* (also called a *node* or
    *neuron*). A unit takes in one or more inputs, multiplies each input by a parameter
    (also called a *weight*), sums the weighted input’s values along with some bias
    value (typically 0), and then feeds the value into an activation function. This
    output is then sent forward to the other neurons deeper in the neural network
    (if they exist).
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks can be visualized as a series of connected layers that form
    a network connecting an observation’s feature values at one end and the target
    value (e.g., observation’s class) at the other end. *Feedforward* neural networks—​also
    called *multilayer perceptron*—are the simplest artificial neural networks used
    in any real-world setting. The name “feedforward” comes from the fact that an
    observation’s feature values are fed “forward” through the network, with each
    layer successively transforming the feature values with the goal that the output
    is the same as (or close to) the target’s value.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, feedforward neural networks contain three types of layers. At
    the start of the neural network is an input layer, where each unit contains an
    observation’s value for a single feature. For example, if an observation has 100
    features, the input layer has 100 units. At the end of the neural network is the
    output layer, which transforms the output of intermediate layers (called *hidden
    layers*) into values useful for the task at hand. For example, if our goal is
    binary classification, we can use an output layer with a single unit that uses
    a sigmoid function to scale its own output to between 0 and 1, representing a
    predicted class probability.
  prefs: []
  type: TYPE_NORMAL
- en: Between the input and output layers are the so-called hidden layers. These hidden
    layers successively transform the feature values from the input layer to something
    that, once processed by the output layer, resembles the target class. Neural networks
    with many hidden layers (e.g., 10, 100, 1,000) are considered “deep” networks.
    Training deep neural networks is a process known as *deep learning*.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks are typically created with all parameters initialized as small
    random values from a Gaussian or normal uniform distribution. Once an observation
    (or more often a set number of observations called a *batch*) is fed through the
    network, the outputted value is compared with the observation’s true value using
    a loss function. This is called *forward propagation*. Next an algorithm goes
    “backward” through the network identifying how much each parameter contributed
    to the error between the predicted and true values, a process called *back propagation*.
    At each parameter, the optimization algorithm determines how much each weight
    should be adjusted to improve the output.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks learn by repeating this process of forward propagation and back
    propagation for every observation in the training data multiple times (each time
    all observations have been sent through the network is called an *epoch* and training
    typically consists of multiple epochs), iteratively updating the values of the
    parameters utilizing a process called *gradient descent* to slowly optimize the
    values of the parameters for the given output.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will use the same Python library used in the last chapter,
    PyTorch, to build, train, and evaluate a variety of neural networks. PyTorch is
    a popular tool within the deep learning space due to its well-written APIs and
    intuitive representation of the low-level tensor operations that power neural
    networks. One key feature of PyTorch is called *autograd*, which automatically
    computes and stores the gradients used to optimize the parameters of the network
    after undergoing forward propagation and back propagation.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks created using PyTorch code can be trained using both CPUs (i.e.,
    on your laptop) and GPUs (i.e., on a specialized deep learning computer). In the
    real world with real data, it is often necessary to train neural networks using
    GPUs, as the training process on large data for complex networks runs orders of
    magnitude faster on GPUs than CPUs. However, all the neural networks in this book
    are small and simple enough to be trained on a CPU-only laptop in only a few minutes.
    Just be aware that when we have larger networks and more training data, training
    using CPUs is *significantly* slower than training using GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 21.1 Using Autograd with PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to use PyTorch’s autograd features to compute and store the gradients
    after undergoing forward propagation and back propagation.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Create tensors with the `requires_grad` option set to `True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Autograd is one of the core features of PyTorch and a big factor in its popularity
    as a deep learning library. The ability to easily compute, store, and visualize
    gradients makes PyTorch very intuitive for researchers and enthusiasts building
    neural networks from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch uses a directed acyclic graph (DAG) to keep a record of all data and
    computational operations being performed on that data. This is incredibly useful,
    but it also means we need to be careful with what operations we try to apply on
    our PyTorch data that requires gradients. When working with autograd, we can’t
    easily convert our tensors to NumPy arrays and back without “breaking the graph,”
    a phrase used to describe operations that don’t support autograd:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: To convert this tensor into a NumPy array, we need to call the `detach()` method
    on it, which will break the graph and thus our ability to automatically compute
    gradients. While this can definitely be useful, it’s worth knowing that detaching
    the tensor will prevent PyTorch from automatically computing the gradient.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PyTorch Autograd Tutorial](https://oreil.ly/mOWSw)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 21.2 Preprocessing Data for Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to preprocess data for use in a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Standardize each feature using scikit-learn’s `StandardScaler`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While this recipe is very similar to [Recipe 4.2](ch04.xhtml#standardizing-a-feature),
    it is worth repeating because of how important it is for neural networks. Typically,
    a neural network’s parameters are initialized (i.e., created) as small random
    numbers. Neural networks often behave poorly when the feature values are much
    larger than the parameter values. Furthermore, since an observation’s feature
    values are combined as they pass through individual units, it is important that
    all features have the same scale.
  prefs: []
  type: TYPE_NORMAL
- en: For these reasons, it is best practice (although not always necessary; for example,
    when we have all binary features) to standardize each feature such that the feature’s
    values have the mean of 0 and the standard deviation of 1\. This can be accomplished
    easily with scikit-learn’s `StandardScaler`.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if you need to perform this operation after having created tensors
    with `requires_grad=True`, you’ll need to do this natively in PyTorch, so as not
    to break the graph. While you’ll typically standardize features prior to starting
    to train the network, it’s worth knowing how to accomplish the same thing in PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 21.3 Designing a Neural Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to design a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the PyTorch `nn.Module` class to define a simple neural network architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Neural networks consist of layers of units. However, there’s incredible variety
    in the types of layers and how they are combined to form the network’s architecture.
    While there are commonly used architecture patterns (which we’ll cover in this
    chapter), the truth is that selecting the right architecture is mostly an art
    and the topic of much research.
  prefs: []
  type: TYPE_NORMAL
- en: 'To construct a feedforward neural network in PyTorch, we need to make a number
    of choices about both the network architecture and training process. Remember
    that each unit in the hidden layers:'
  prefs: []
  type: TYPE_NORMAL
- en: Receives a number of inputs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Weights each input by a parameter value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sums together all weighted inputs along with some bias (typically 0).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Most often then applies some function (called an *activation function*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sends the output on to units in the next layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First, for each layer in the hidden and output layers we must define the number
    of units to include in the layer and the activation function. Overall, the more
    units we have in a layer, the more complex patterns our network is able to learn.
    However, more units might make our network overfit the training data in a way
    detrimental to the performance on the test data.
  prefs: []
  type: TYPE_NORMAL
- en: 'For hidden layers, a popular activation function is the *rectified linear unit*
    (ReLU):'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>f</mi> <mo>(</mo> <mi>z</mi> <mo>)</mo> <mo>=</mo>
    <mo form="prefix" movablelimits="true">max</mo> <mo>(</mo> <mn>0</mn> <mo>,</mo>
    <mi>z</mi> <mo>)</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math display="inline"><mi>z</mi></math> is the sum of the weighted inputs
    and bias. As we can see, if <math display="inline"><mi>z</mi></math> is greater
    than 0, the activation function returns <math display="inline"><mi>z</mi></math>;
    otherwise, the function returns 0\. This simple activation function has a number
    of desirable properties (a discussion of which is beyond the scope of this book),
    and this has made it a popular choice in neural networks. We should be aware,
    however, that many dozens of activation functions exist.
  prefs: []
  type: TYPE_NORMAL
- en: Second, we need to define the number of hidden layers to use in the network.
    More layers allow the network to learn more complex relationships, but with a
    computational cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'Third, we have to define the structure of the activation function (if any)
    of the output layer. The nature of the output function is often determined by
    the goal of the network. Here are some common output layer patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: Binary classification
  prefs: []
  type: TYPE_NORMAL
- en: One unit with a sigmoid activation function
  prefs: []
  type: TYPE_NORMAL
- en: Multiclass classification
  prefs: []
  type: TYPE_NORMAL
- en: '*k* units (where *k* is the number of target classes) and a softmax activation
    function'
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs: []
  type: TYPE_NORMAL
- en: One unit with no activation function
  prefs: []
  type: TYPE_NORMAL
- en: 'Fourth, we need to define a loss function (the function that measures how well
    a predicted value matches the true value); again, this is often determined by
    the problem type:'
  prefs: []
  type: TYPE_NORMAL
- en: Binary classification
  prefs: []
  type: TYPE_NORMAL
- en: Binary cross-entropy
  prefs: []
  type: TYPE_NORMAL
- en: Multiclass classification
  prefs: []
  type: TYPE_NORMAL
- en: Categorical cross-entropy
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs: []
  type: TYPE_NORMAL
- en: Mean square error
  prefs: []
  type: TYPE_NORMAL
- en: Fifth, we have to define an optimizer, which intuitively can be thought of as
    our strategy “walking around” the loss function to find the parameter values that
    produce the lowest error. Common choices for optimizers are stochastic gradient
    descent, stochastic gradient descent with momentum, root mean square propagation,
    and adaptive moment estimation (for more information on these optimizers, see
    [“See Also”](#see-also-ch20)).
  prefs: []
  type: TYPE_NORMAL
- en: Sixth, we can select one or more metrics to use to evaluate the performance,
    such as accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, we use the `torch.nn.Module` namespace to compose a simple,
    sequential neural network that can make binary classifications. The standard PyTorch
    approach for this is to create a child class that inherits from the `torch.nn.Module`
    class, instantiating a network architecture in the `__init__` method, and defining
    the mathematical operations we want to perform upon each forward pass in the `forward`
    method of the class. There are many ways to define networks in PyTorch, and although
    in this case we use functional methods for our activation functions (such as `nn.functional.relu`)
    we can also define these activation functions as layers. If we wanted to compose
    everything in the network as a layer, we could use the `Sequential` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In both cases, the network itself is a two-layer neural network (when counting
    layers we don’t include the input layer because it does not have any parameters
    to learn) defined using PyTorch’s sequential model. Each layer is “dense” (also
    called “fully connected”), meaning that all the units in the previous layer are
    connected to all the units in the next layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first hidden layer we set `out_features=16`, meaning that layer contains
    16 units. These units have ReLU activation functions as defined in the `forward`
    method of our class: `x = nn.functional.relu(self.fc1(x))`. The first layer of
    our network has the size `(10, 16)`, which tells the first layer to expect each
    observation from our input data to have 10 feature values. This network is designed
    for binary classification so the output layer contains only one unit with a sigmoid
    activation function, which constrains the output to between 0 and 1 (representing
    the probability an observation is class 1).'
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PyTorch tutorial: Build the Neural Network](https://oreil.ly/iT8iv)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Loss functions for classification, Wikipedia](https://oreil.ly/4bPXv)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[On Loss Functions for Deep Neural Networks in Classification, Katarzyna Janocha
    and Wojciech Marian Czarnecki](https://oreil.ly/pplP-)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 21.4 Training a Binary Classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to train a binary classifier neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use PyTorch to construct a feedforward neural network and train it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Recipe 21.3](#designing-a-neural-network), we discussed how to construct
    a neural network using PyTorch’s sequential model. In this recipe we train that
    neural network using 10 features and 1,000 observations of fake classification
    generated from scikit-learn’s `make_classification` function.
  prefs: []
  type: TYPE_NORMAL
- en: The neural network we are using is the same as the one in [Recipe 21.3](#designing-a-neural-network)
    (see that recipe for a detailed explanation). The difference there is that we
    only created the neural network; we didn’t train it.
  prefs: []
  type: TYPE_NORMAL
- en: At the end, we use `with torch.no_grad()` to evaluate the network. This says
    that we should not compute gradients for any tensor operations conducted in this
    section of code. Since we use gradients only during the model training process,
    we don’t want to store new gradients for operations that occur outside of it (such
    as prediction or evaluation).
  prefs: []
  type: TYPE_NORMAL
- en: The `epochs` variable defines how many epochs to use when training the data.
    `batch_size` sets the number of observations to propagate through the network
    before updating the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: We then iterate over the number of epochs, making forward passes through the
    network using the `forward` method, and then backward passes to update the gradients.
    The result is a trained model.
  prefs: []
  type: TYPE_NORMAL
- en: 21.5 Training a Multiclass Classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to train a multiclass classifier neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use PyTorch to construct a feedforward neural network with an output layer
    with softmax activation functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this solution we created a similar neural network to the binary classifier
    from the last recipe, but with some notable changes. In the classification data
    we generated, we set `N_CLASSES=3`. To handle multiclass classification, we also
    use `nn.CrossEntropyLoss()`, which expects the target to be one-hot encoded. To
    accomplish this, we use the `torch.nn.functional.one_hot` function and end up
    with a one-hot encoded array where the position of `1.` indicates the class for
    a given observation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Since this is a multiclass classification problem, we used an output layer of
    size 3 (one per class) containing a softmax activation function. The softmax activation
    function will return an array of 3 values summing to 1\. These 3 values represent
    an observation’s probability of being a member of each of the 3 classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned in this recipe, we used a loss function suited to multiclass classification,
    the categorical cross-entropy loss function: `nn.CrossEntropyLoss()`.'
  prefs: []
  type: TYPE_NORMAL
- en: 21.6 Training a Regressor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to train a neural network for regression.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use PyTorch to construct a feedforward neural network with a single output
    unit that has no activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s completely possible to create a neural network to predict continuous values
    instead of class probabilities. In the case of our binary classifier ([Recipe
    21.4](#training-a-binary-classifier)) we used an output layer with a single unit
    and a sigmoid activation function to produce a probability that an observation
    was class 1\. Importantly, the sigmoid activation function constrained the outputted
    value to between 0 and 1\. If we remove that constraint by having no activation
    function, we allow the output to be a continuous value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, because we are training a regression, we should use an appropriate
    loss function and evaluation metric, in our case the mean square error:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mo form="prefix">MSE</mo> <mo>=</mo> <mfrac><mn>1</mn>
    <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mrow>
    <mi>n</mi></munderover> <msup><mrow><mo>(</mo><msub><mover accent="true"><mi>y</mi>
    <mo>^</mo></mover> <mi>i</mi></msub> <mo>-</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math display="inline"><mi>n</mi></math> is the number of observations;
    <math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math> is the true value
    of the target we are trying to predict, <math display="inline"><mi>y</mi></math>,
    for observation <math display="inline"><mi>i</mi></math>; and <math display="inline"><msub><mover
    accent="true"><mi>y</mi> <mo>^</mo></mover><mi>i</mi></msub></math> is the model’s
    predicted value for <math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math>.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, because we are using simulated data using scikit-learn `make_regression`,
    we didn’t have to standardize the features. It should be noted, however, that
    in almost all real-world cases, standardization would be necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 21.7 Making Predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to use a neural network to make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use PyTorch to construct a feedforward neural network, then make predictions
    using `forward`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Making predictions is easy in PyTorch. Once we have trained our neural network
    we can use the `forward` method (already used as part of the training process),
    which takes as input a set of features and does a forward pass through the network.
    In our solution the neural network is set up for binary classification, so the
    predicted output is the probability of being class 1\. Observations with predicted
    values very close to 1 are highly likely to be class 1, while observations with
    predicted values very close to 0 are highly likely to be class 0\. Hence, we use
    the `round` method to convert these values to 1s and 0s for our binary classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 21.8 Visualize Training History
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to find the “sweet spot” in a neural network’s loss and/or accuracy
    score.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use Matplotlib to visualize the loss of the test and training set over each
    epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 21in01](assets/mpc2_21in01.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When our neural network is new, it will have poor performance. As the neural
    network learns on the training data, the model’s error on both the training and
    test set will tend to decrease. However, at a certain point, a neural network
    can start “memorizing” the training data and overfit. When this starts happening,
    the training error may decrease while the test error starts increasing. Therefore,
    in many cases, there is a “sweet spot” where the test error (which is the error
    we mainly care about) is at its lowest point. This effect can be seen in the solution,
    where we visualize the training and test loss at each epoch. Note that the test
    error is lowest around epoch 6, after which the training loss plateaus while the
    test loss starts increasing. From this point onward, the model is overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 21.9 Reducing Overfitting with Weight Regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to reduce overfitting by regularizing the weights of your network.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Try penalizing the parameters of the network, also called *weight regularization*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One strategy to combat overfitting neural networks is by penalizing the parameters
    (i.e., weights) of the neural network such that they are driven to be small values,
    creating a simpler model less prone to overfit. This method is called weight regularization
    or weight decay. More specifically, in weight regularization a penalty is added
    to the loss function, such as the L2 norm.
  prefs: []
  type: TYPE_NORMAL
- en: In PyTorch, we can add weight regularization by including `weight_decay=1e-5`
    in the optimizer where regularization happens. In this example, `1e-5` determines
    how much we penalize higher parameter values. Values greater than 0 indicate L2
    regularization in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: 21.10 Reducing Overfitting with Early Stopping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to reduce overfitting by stopping training when your train and test
    scores diverge.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use PyTorch Lightning to implement a strategy called *early stopping*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we discussed in [Recipe 21.8](#visualize-training-history), typically in
    the first several training epochs, both the training and test errors will decrease,
    but at some point the network will start “memorizing” the training data, causing
    the training error to continue to decrease even while the test error starts increasing.
    Because of this phenomenon, one of the most common and very effective methods
    to counter overfitting is to monitor the training process and stop training when
    the test error starts to increase. This strategy is called *early stopping*.
  prefs: []
  type: TYPE_NORMAL
- en: In PyTorch, we can implement early stopping as a callback function. Callbacks
    are functions that can be applied at certain stages of the training process, such
    as at the end of each epoch. However, PyTorch itself does not define an early
    stopping class for you, so here we use the popular library `lightning` (known
    as PyTorch Lightning) to use an out-of-the-box one. PyTorch Lightning is a high-level
    library for PyTorch that provides a lot of useful features. In our solution, we
    included PyTorch Lightning’s `EarlyStopping(monitor="val_loss", mode="min", patience=3)`
    to define that we wanted to monitor the test (validation) loss at each epoch,
    and if the test loss has not improved after three epochs (the default), training
    is interrupted.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we did not include the `EarlyStopping` callback, the model would train for
    the full 1,000 max epochs without stopping on its own:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 21.11 Reducing Overfitting with Dropout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to reduce overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Introduce noise into your network’s architecture using dropout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Dropout* is a fairly common method for regularizing smaller neural networks.
    In dropout, every time a batch of observations is created for training, a proportion
    of the units in one or more layers is multiplied by zero (i.e., dropped). In this
    setting, every batch is trained on the same network (e.g., the same parameters),
    but each batch is confronted by a slightly different version of that network’s
    *architecture*.'
  prefs: []
  type: TYPE_NORMAL
- en: Dropout is thought to be effective because by constantly and randomly dropping
    units in each batch, it forces units to learn parameter values able to perform
    under a wide variety of network architectures. That is, they learn to be robust
    to disruptions (i.e., noise) in the other hidden units, and this prevents the
    network from simply memorizing the training data.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to add dropout to both the hidden and input layers. When an input
    layer is dropped, its feature value is not introduced into the network for that
    batch.
  prefs: []
  type: TYPE_NORMAL
- en: In PyTorch, we can implement dropout by adding an `nn.Dropout` layer into our
    network architecture. Each `nn.Dropout` layer will drop a user-defined hyperparameter
    of units in the previous layer every batch.
  prefs: []
  type: TYPE_NORMAL
- en: 21.12 Saving Model Training Progress
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given a neural network that will take a long time to train, you want to save
    your progress in case the training process is interrupted.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the `torch.save` function to save the model after every epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the real world, it is common for neural networks to train for hours or even
    days. During that time a lot can go wrong: computers can lose power, servers can
    crash, or inconsiderate graduate students can close your laptop.'
  prefs: []
  type: TYPE_NORMAL
- en: We can use `torch.save` to alleviate this problem by saving the model after
    every epoch. Specifically, after every epoch, we save a model to the location
    `model.pt`, the second argument to the `torch.save` function. If we include only
    a filename (e.g., *model.pt*) that file will be overridden with the latest model
    every epoch.
  prefs: []
  type: TYPE_NORMAL
- en: As you can imagine, we can introduce additional logic to save the model every
    few epochs, only save a model if the loss goes down, etc. We could even combine
    this approach with the early stopping approach in PyTorch Lightning to ensure
    we save a model no matter at what epoch the training ends.
  prefs: []
  type: TYPE_NORMAL
- en: 21.13 Tuning Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to automatically select the best hyperparameters for your neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the `ray` tuning library with PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Recipes [12.1](ch12.xhtml#selecting-best-models-using-exhaustive-search)
    and [12.2](ch12.xhtml#selecting-best-models-using-randomized-search), we covered
    using scikit-learn’s model selection techniques to identify the best hyperparameters
    of a scikit-learn model. While in general the scikit-learn approach can also be
    applied to neural networks, the `ray` tuning library provides a sophisticated
    API that allows you to schedule experiments on both CPUs and GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: The hyperparameters of a model *are* important and should be selected with care.
    However, running experiments to select hyperparameters can be both cost and time
    prohibitive. Therefore, automatic hyperparameter tuning of neural networks is
    not the silver bullet, but it is a useful tool to have in certain circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: In our solution we conducted a search of different parameters for layer sizes
    and the learning rate of our optimizer. The `best_trial.config` shows the parameters
    in our `ray` tuning configuration that led to the lowest loss and best experiment
    outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 21.14 Visualizing Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to quickly visualize a neural network’s architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the `make_dot` function from `torch_viz`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'If we open the image that was saved to our machine, we can see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![mpc2 21in02](assets/mpc2_21in02.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `torchviz` library provides easy utility functions to quickly visualize
    our neural networks and write them out as images.
  prefs: []
  type: TYPE_NORMAL
