<html><head></head><body><section data-pdf-bookmark="Chapter 15. Metrics and Regression Evaluation" data-type="chapter" epub:type="chapter"><div class="chapter" id="idm46066889456296">&#13;
<h1><span class="label">Chapter 15. </span>Metrics and Regression Evaluation</h1>&#13;
&#13;
&#13;
<p><a data-primary="metrics" data-secondary="random forest" data-type="indexterm" id="ix_ch15-asciidoc0"/><a data-primary="random forest" data-secondary="metrics and regression evaluation" data-type="indexterm" id="ix_ch15-asciidoc1"/><a data-primary="regression evaluation" data-type="indexterm" id="ix_ch15-asciidoc2"/>This chapter will evaluate the results of a random forest regressor trained on the Boston housing data:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">rfr</code> <code class="o">=</code> <code class="n">RandomForestRegressor</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">,</code> <code class="n">n_estimators</code><code class="o">=</code><code class="mi">100</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rfr</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">bos_X_train</code><code class="p">,</code> <code class="n">bos_y_train</code><code class="p">)</code></pre>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Metrics" data-type="sect1"><div class="sect1" id="idm46066887104024">&#13;
<h1>Metrics</h1>&#13;
&#13;
<p><a data-primary="metrics" data-secondary="regression evaluation" data-type="indexterm" id="ix_ch15-asciidoc3"/><a data-primary="metrics" data-secondary="for regression model evaluation" data-type="indexterm" id="ix_ch15-asciidoc4"/><a data-primary="regression" data-secondary="metrics" data-type="indexterm" id="ix_ch15-asciidoc5"/><a data-primary="regression evaluation" data-secondary="metrics" data-type="indexterm" id="ix_ch15-asciidoc6"/><a data-primary="sklearn" data-secondary="regression model evaluation" data-type="indexterm" id="ix_ch15-asciidoc7"/>The <code>sklearn.metrics</code> module includes metrics to evaluate regression models. Metric functions ending in <code>loss</code> or <code>error</code> should be minimized. Functions ending in <code>score</code> should be maximized.</p>&#13;
&#13;
<p><a data-primary="coefficient of determination" data-type="indexterm" id="idm46066887069064"/>The <em>coefficient of determination</em> (r²) is a common regression metric. This value is typically between 0 and 1. It represents the percent of the variance of the target that the features contribute. Higher values are better, but in general it is difficult to evaluate the model from this metric alone. Does a .7 mean it is a good score? It depends. For a given dataset, .5 might be a good score, while for another dataset, a .9 may be a bad score. Typically we use this number in combination with other metrics or visualizations to evaluate a model.</p>&#13;
&#13;
<p>For example, it is easy to make a model that predicts stock prices for the next day with an r² of .99. But I wouldn’t trade my own money with that model. It might be slightly low or high, which can wreak havoc on trades.</p>&#13;
&#13;
<p>The r² metric is the default metric used during grid search. You can&#13;
specify other metrics using the <code>scoring</code> parameter.</p>&#13;
&#13;
<p>The <code>.score</code> method calculates this for regression models:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">metrics</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rfr</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">bos_X_test</code><code class="p">,</code> <code class="n">bos_y_test</code><code class="p">)</code>&#13;
<code class="go">0.8721182042634867</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">metrics</code><code class="o">.</code><code class="n">r2_score</code><code class="p">(</code><code class="n">bos_y_test</code><code class="p">,</code> <code class="n">bos_y_test_pred</code><code class="p">)</code>&#13;
<code class="go">0.8721182042634867</code></pre>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p><a data-primary="explained variance" data-type="indexterm" id="idm46066887020040"/>There is also an <em>explained variance</em> metric (<code>'explained_variance'</code> in grid search). If the mean of the <em>residuals</em> (errors in predictions) is 0 (in ordinary least squares (OLS) models), then the variance explained is the same as the coefficient of determination:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">metrics</code><code class="o">.</code><code class="n">explained_variance_score</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">bos_y_test</code><code class="p">,</code> <code class="n">bos_y_test_pred</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="go">0.8724890451227875</code></pre>&#13;
</div>&#13;
&#13;
<p><a data-primary="mean absolute error" data-type="indexterm" id="idm46066887010568"/><em>Mean absolute error</em> (<code>'neg_mean_absolute_error'</code> when used in grid search) expresses the average absolute model prediction error. A perfect model would score 0, but this metric has no upper bounds, unlike the coefficient of determination. However, since it is in units of the target, it is more interpretable. If you want to ignore outliers, this is a good metric to use.</p>&#13;
&#13;
<p>This measure cannot indicate how bad a model is, but can be used to compare two models. If you have two models, the model with a lower score is better.</p>&#13;
&#13;
<p>This number tells us that the average error is about two above or below the real value:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">metrics</code><code class="o">.</code><code class="n">mean_absolute_error</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">bos_y_test</code><code class="p">,</code> <code class="n">bos_y_test_pred</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="go">2.0839802631578945</code></pre>&#13;
&#13;
<p><a data-primary="root mean squared error" data-type="indexterm" id="idm46066886959064"/><em>Root mean squared error</em> (<code>'neg_mean_squared_error'</code> in grid search) also measures model error in terms of the target. However, because it averages the square of errors before taking the square root, it penalizes large errors. If you want to penalize large errors, this is a good metric to use. For example, if being off by eight is more than two times worse than being off by four.</p>&#13;
&#13;
<p>As with mean absolute error, this measure cannot indicate how bad a model is, but can be used to compare two models. If you assume that errors are normally distributed, this is a good choice.</p>&#13;
&#13;
<p>The result tells us if we square the errors and average them, the result will be around 9.5:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">metrics</code><code class="o">.</code><code class="n">mean_squared_error</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">bos_y_test</code><code class="p">,</code> <code class="n">bos_y_test_pred</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="go">9.52886846710526</code></pre>&#13;
&#13;
<p><a data-primary="mean squared logarithmic error" data-type="indexterm" id="idm46066886919416"/>The <em>mean squared logarithmic error</em> (in grid search, <code>'neg_mean_squared_log_error'</code>) penalizes underprediction more than overprediction. If you have targets that experience exponential growth (population, stock, etc.), this is a good metric.</p>&#13;
&#13;
<p>If you take the log of the error and then square it, the average of these results will be 0.021:<a data-startref="ix_ch15-asciidoc7" data-type="indexterm" id="idm46066886917240"/><a data-startref="ix_ch15-asciidoc6" data-type="indexterm" id="idm46066886916376"/><a data-startref="ix_ch15-asciidoc5" data-type="indexterm" id="idm46066886915704"/><a data-startref="ix_ch15-asciidoc4" data-type="indexterm" id="idm46066886915032"/><a data-startref="ix_ch15-asciidoc3" data-type="indexterm" id="idm46066886914360"/></p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">metrics</code><code class="o">.</code><code class="n">mean_squared_log_error</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">bos_y_test</code><code class="p">,</code> <code class="n">bos_y_test_pred</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="go">0.02128263061776433</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Residuals Plot" data-type="sect1"><div class="sect1" id="idm46066887098568">&#13;
<h1>Residuals Plot</h1>&#13;
&#13;
<p><a data-primary="homoscedasticity" data-type="indexterm" id="idm46066886885880"/><a data-primary="regression evaluation" data-secondary="residuals plot" data-type="indexterm" id="idm46066886885176"/><a data-primary="residuals plot" data-type="indexterm" id="idm46066886884232"/>Good models (with appropriate R2 scores) will exhibit <em>homoscedasticity</em>. This means the variance is the same for all values of targets regardless of the input.&#13;
Plotted, this looks like randomly&#13;
distributed values in a residuals plot. If there are patterns, the model or the data are problematic.</p>&#13;
&#13;
<p>Residuals plots also show outliers, which can have a big impact on model&#13;
fitting (see <a data-type="xref" href="#id46">Figure 15-1</a>).</p>&#13;
&#13;
<p><a data-primary="Yellowbrick" data-secondary="residuals plot" data-type="indexterm" id="idm46066886849912"/>Yellowbrick can make residuals plots to visualize this:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">yellowbrick.regressor</code> <code class="kn">import</code> <code class="n">ResidualsPlot</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rpv</code> <code class="o">=</code> <code class="n">ResidualsPlot</code><code class="p">(</code><code class="n">rfr</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rpv</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">bos_X_train</code><code class="p">,</code> <code class="n">bos_y_train</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rpv</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">bos_X_test</code><code class="p">,</code> <code class="n">bos_y_test</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rpv</code><code class="o">.</code><code class="n">poof</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1501.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id46">&#13;
<img alt="Residuals plot. Further testing will show these to be heteroscedastic." src="assets/mlpr_1501.png"/>&#13;
<h6><span class="label">Figure 15-1. </span>Residuals plot. Further testing will show these to be heteroscedastic.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Heteroscedasticity" data-type="sect1"><div class="sect1" id="idm46066886761064">&#13;
<h1>Heteroscedasticity</h1>&#13;
&#13;
<p><a data-primary="heteroscedasticity" data-secondary="regression evaluation and" data-type="indexterm" id="idm46066886759896"/><a data-primary="regression evaluation" data-secondary="heteroscedasticity" data-type="indexterm" id="idm46066886758952"/>The <a href="https://oreil.ly/HtIi5">statsmodel library</a> includes the <em>Breusch-Pagan test</em> for heteroscedasticity. This means that variance of the residuals varies over the predicted values. In the Breusch-Pagan test, if the p-values are significant (<code>p-value</code> less than 0.05), the null hypothesis of homoscedasticity is rejected. This indicates that residuals are heteroscedastic, and the predictions are biased.</p>&#13;
&#13;
<p>The test confirms heteroscedasticity:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">statsmodels.stats.api</code> <code class="kn">as</code> <code class="nn">sms</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">hb</code> <code class="o">=</code> <code class="n">sms</code><code class="o">.</code><code class="n">het_breuschpagan</code><code class="p">(</code><code class="n">resids</code><code class="p">,</code> <code class="n">bos_X_test</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">labels</code> <code class="o">=</code> <code class="p">[</code>&#13;
<code class="gp">... </code>    <code class="s">"Lagrange multiplier statistic"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="s">"p-value"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="s">"f-value"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="s">"f p-value"</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">]</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">name</code><code class="p">,</code> <code class="n">num</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">name</code><code class="p">,</code> <code class="n">hb</code><code class="p">):</code>&#13;
<code class="gp">... </code>    <code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s">"{name}: {num:.2}"</code><code class="p">)</code>&#13;
<code class="go">Lagrange multiplier statistic: 3.6e+01</code>&#13;
<code class="go">p-value: 0.00036</code>&#13;
<code class="go">f-value: 3.3</code>&#13;
<code class="go">f p-value: 0.00022</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Normal Residuals" data-type="sect1"><div class="sect1" id="idm46066886754104">&#13;
<h1>Normal Residuals</h1>&#13;
&#13;
<p><a data-primary="normal residuals" data-type="indexterm" id="idm46066886706600"/><a data-primary="regression evaluation" data-secondary="normal residuals" data-type="indexterm" id="idm46066886706008"/><a data-primary="residuals plot" data-type="indexterm" id="idm46066886705064"/><a data-primary="scipy" data-type="indexterm" id="idm46066886704392"/>The scipy library includes a <em>probability plot</em> and the <em>Kolmogorov-Smirnov test</em>, both of which&#13;
measure whether the residuals are normal.</p>&#13;
&#13;
<p>We can plot a histogram (see <a data-type="xref" href="#idres1">Figure 15-2</a>) to visualize the residuals and check for normality:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">resids</code> <code class="o">=</code> <code class="n">bos_y_test</code> <code class="o">-</code> <code class="n">rfr</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">bos_X_test</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">resids</code><code class="p">,</code> <code class="n">name</code><code class="o">=</code><code class="s">"residuals"</code><code class="p">)</code><code class="o">.</code><code class="n">plot</code><code class="o">.</code><code class="n">hist</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">bins</code><code class="o">=</code><code class="mi">20</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">,</code> <code class="n">title</code><code class="o">=</code><code class="s">"Residual Histogram"</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1502.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="idres1">&#13;
<img alt="Histogram of residuals." src="assets/mlpr_1502.png"/>&#13;
<h6><span class="label">Figure 15-2. </span>Histogram of residuals.</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-type="xref" href="#idres2">Figure 15-3</a> shows a <a data-primary="probability plot" data-type="indexterm" id="idm46066886549848"/>probability plot. If the samples plotted against the quantiles line up, the residuals are normal. We can see that this fails in this case:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">scipy</code> <code class="kn">import</code> <code class="n">stats</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">_</code> <code class="o">=</code> <code class="n">stats</code><code class="o">.</code><code class="n">probplot</code><code class="p">(</code><code class="n">resids</code><code class="p">,</code> <code class="n">plot</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1503.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="idres2">&#13;
<img alt="Probability plot of residuals." src="assets/mlpr_1503.png"/>&#13;
<h6><span class="label">Figure 15-3. </span>Probability plot of residuals.</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-primary="Kolmogorov-Smirnov test" data-type="indexterm" id="idm46066886449512"/>The Kolmogorov-Smirnov test can evaluate whether a distribution is normal. If the p-value is significant (&lt; 0.05), then the values are not normal.</p>&#13;
&#13;
<p>This fails as well, which tells us the residuals are not normal:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">stats</code><code class="o">.</code><code class="n">kstest</code><code class="p">(</code><code class="n">resids</code><code class="p">,</code> <code class="n">cdf</code><code class="o">=</code><code class="s">"norm"</code><code class="p">)</code>&#13;
<code class="go">KstestResult(statistic=0.1962230021010155, pvalue=1.3283596864921421e-05)</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Prediction Error Plot" data-type="sect1"><div class="sect1" id="idm46066886753640">&#13;
<h1>Prediction Error Plot</h1>&#13;
&#13;
<p><a data-primary="prediction error plot" data-type="indexterm" id="idm46066886440712"/><a data-primary="regression evaluation" data-secondary="prediction error plot" data-type="indexterm" id="idm46066886440008"/>A prediction error plot shows the real targets against the predicted values. For a perfect model these points would line up in a 45-degree line.</p>&#13;
&#13;
<p>As our model seems to predict lower values for the high end of y, the model has some performance issues. This is also evident in the residuals plot (see <a data-type="xref" href="#id47">Figure 15-4</a>).</p>&#13;
&#13;
<p><a data-primary="Yellowbrick" data-secondary="prediction error plot" data-type="indexterm" id="idm46066886437096"/>Here is the Yellowbrick version:<a data-startref="ix_ch15-asciidoc2" data-type="indexterm" id="idm46066886400728"/><a data-startref="ix_ch15-asciidoc1" data-type="indexterm" id="idm46066886400056"/><a data-startref="ix_ch15-asciidoc0" data-type="indexterm" id="idm46066886399384"/></p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">yellowbrick.regressor</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">PredictionError</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">6</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">pev</code> <code class="o">=</code> <code class="n">PredictionError</code><code class="p">(</code><code class="n">rfr</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">pev</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">bos_X_train</code><code class="p">,</code> <code class="n">bos_y_train</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">pev</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">bos_X_test</code><code class="p">,</code> <code class="n">bos_y_test</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">pev</code><code class="o">.</code><code class="n">poof</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1504.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id47">&#13;
<img alt="Prediction error. Plots predicted y (y-hat) versus actual y." src="assets/mlpr_1504.png"/>&#13;
<h6><span class="label">Figure 15-4. </span>Prediction error. Plots predicted y (y-hat) versus actual y.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>