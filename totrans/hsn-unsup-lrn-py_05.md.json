["```py\n# Import libraries\n'''Main'''\nimport numpy as np\nimport pandas as pd\nimport os, time\nimport pickle, gzip\n\n'''Data Viz'''\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nimport matplotlib as mpl\n\n%matplotlib inline\n\n'''Data Prep and Model Evaluation'''\nfrom sklearn import preprocessing as pp\nfrom scipy.stats import pearsonr\nfrom numpy.testing import assert_array_almost_equal\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n'''Algos'''\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\n```", "```py\n# Load the datasets\ncurrent_path = os.getcwd()\nfile = '\\\\datasets\\\\mnist_data\\\\mnist.pkl.gz'\n\nf = gzip.open(current_path+file, 'rb')\ntrain_set, validation_set, test_set = pickle.load(f, encoding='latin1')\nf.close()\n\nX_train, y_train = train_set[0], train_set[1]\nX_validation, y_validation = validation_set[0], validation_set[1]\nX_test, y_test = test_set[0], test_set[1]\n```", "```py\n# Verify shape of datasets\nprint(\"Shape of X_train: \", X_train.shape)\nprint(\"Shape of y_train: \", y_train.shape)\nprint(\"Shape of X_validation: \", X_validation.shape)\nprint(\"Shape of y_validation: \", y_validation.shape)\nprint(\"Shape of X_test: \", X_test.shape)\nprint(\"Shape of y_test: \", y_test.shape)\n```", "```py\nShape of X_train:       (50000, 784)\nShape of y_train:       (50000,)\nShape of X_validation:  (10000, 784)\nShape of y_validation:  (10000,)\nShape of X_test:        (10000, 784)\nShape of y_test:        (10000,)\n```", "```py\n# Create Pandas DataFrames from the datasets\ntrain_index = range(0,len(X_train))\nvalidation_index = range(len(X_train), /\n                         len(X_train)+len(X_validation))\ntest_index = range(len(X_train)+len(X_validation), /\n                   len(X_train)+len(X_validation)+len(X_test))\n\nX_train = pd.DataFrame(data=X_train,index=train_index)\ny_train = pd.Series(data=y_train,index=train_index)\n\nX_validation = pd.DataFrame(data=X_validation,index=validation_index)\ny_validation = pd.Series(data=y_validation,index=validation_index)\n\nX_test = pd.DataFrame(data=X_test,index=test_index)\ny_test = pd.Series(data=y_test,index=test_index)\n```", "```py\n# Describe the training matrix\nX_train.describe()\n```", "```py\n# Show the labels\ny_train.head()\n```", "```py\n  0   5\n  1   0\n  2   4\n  3   1\n  4   9\n  dtype: int64\n```", "```py\ndef view_digit(example):\n    label = y_train.loc[0]\n    image = X_train.loc[example,:].values.reshape([28,28])\n    plt.title('Example: %d Label: %d' % (example, label))\n    plt.imshow(image, cmap=plt.get_cmap('gray'))\n    plt.show()\n```", "```py\nfrom sklearn.decomposition import PCA\n\nn_components = 784\nwhiten = False\nrandom_state = 2018\n\npca = PCA(n_components=n_components, whiten=whiten, \\\n          random_state=random_state)\n```", "```py\nX_train_PCA = pca.fit_transform(X_train)\nX_train_PCA = pd.DataFrame(data=X_train_PCA, index=train_index)\n```", "```py\n# Percentage of Variance Captured by 784 principal components\nprint(\"Variance Explained by all 784 principal components: \", \\\n      sum(pca.explained_variance_ratio_))\n```", "```py\nVariance Explained by all 784 principal components: 0.9999999999999997\n```", "```py\n# Percentage of Variance Captured by X principal components\nimportanceOfPrincipalComponents = \\\n    pd.DataFrame(data=pca.explained_variance_ratio_)\nimportanceOfPrincipalComponents = importanceOfPrincipalComponents.T\n\nprint('Variance Captured by First 10 Principal Components: ',\n      importanceOfPrincipalComponents.loc[:,0:9].sum(axis=1).values)\nprint('Variance Captured by First 20 Principal Components: ',\n      importanceOfPrincipalComponents.loc[:,0:19].sum(axis=1).values)\nprint('Variance Captured by First 50 Principal Components: ',\n      importanceOfPrincipalComponents.loc[:,0:49].sum(axis=1).values)\nprint('Variance Captured by First 100 Principal Components: ',\n      importanceOfPrincipalComponents.loc[:,0:99].sum(axis=1).values)\nprint('Variance Captured by First 200 Principal Components: ',\n      importanceOfPrincipalComponents.loc[:,0:199].sum(axis=1).values)\nprint('Variance Captured by First 300 Principal Components: ',\n      importanceOfPrincipalComponents.loc[:,0:299].sum(axis=1).values)\n```", "```py\nVariance Captured by First 10 Principal Components: [0.48876238]\nVariance Captured by First 20 Principal Components: [0.64398025]\nVariance Captured by First 50 Principal Components: [0.8248609]\nVariance Captured by First 100 Principal Components: [0.91465857]\nVariance Captured by First 200 Principal Components: [0.96650076]\nVariance Captured by First 300 Principal Components: [0.9862489]\n```", "```py\ndef scatterPlot(xDF, yDF, algoName):\n    tempDF = pd.DataFrame(data=xDF.loc[:,0:1], index=xDF.index)\n    tempDF = pd.concat((tempDF,yDF), axis=1, join=\"inner\")\n    tempDF.columns = [\"First Vector\", \"Second Vector\", \"Label\"]\n    sns.lmplot(x=\"First Vector\", y=\"Second Vector\", hue=\"Label\", \\\n               data=tempDF, fit_reg=False)\n    ax = plt.gca()\n    ax.set_title(\"Separation of Observations using \"+algoName)\n\nscatterPlot(X_train_PCA, y_train, \"PCA\")\n```", "```py\n# Incremental PCA\nfrom sklearn.decomposition import IncrementalPCA\n\nn_components = 784\nbatch_size = None\n\nincrementalPCA = IncrementalPCA(n_components=n_components, \\\n                                batch_size=batch_size)\n\nX_train_incrementalPCA = incrementalPCA.fit_transform(X_train)\nX_train_incrementalPCA = \\\n    pd.DataFrame(data=X_train_incrementalPCA, index=train_index)\n\nX_validation_incrementalPCA = incrementalPCA.transform(X_validation)\nX_validation_incrementalPCA = \\\n    pd.DataFrame(data=X_validation_incrementalPCA, index=validation_index)\n\nscatterPlot(X_train_incrementalPCA, y_train, \"Incremental PCA\")\n```", "```py\n# Sparse PCA\nfrom sklearn.decomposition import SparsePCA\n\nn_components = 100\nalpha = 0.0001\nrandom_state = 2018\nn_jobs = -1\n\nsparsePCA = SparsePCA(n_components=n_components, \\\n                alpha=alpha, random_state=random_state, n_jobs=n_jobs)\n\nsparsePCA.fit(X_train.loc[:10000,:])\nX_train_sparsePCA = sparsePCA.transform(X_train)\nX_train_sparsePCA = pd.DataFrame(data=X_train_sparsePCA, index=train_index)\n\nX_validation_sparsePCA = sparsePCA.transform(X_validation)\nX_validation_sparsePCA = \\\n    pd.DataFrame(data=X_validation_sparsePCA, index=validation_index)\n\nscatterPlot(X_train_sparsePCA, y_train, \"Sparse PCA\")\n```", "```py\n# Kernel PCA\nfrom sklearn.decomposition import KernelPCA\n\nn_components = 100\nkernel = 'rbf'\ngamma = None\nrandom_state = 2018\nn_jobs = 1\n\nkernelPCA = KernelPCA(n_components=n_components, kernel=kernel, \\\n                      gamma=gamma, n_jobs=n_jobs, random_state=random_state)\n\nkernelPCA.fit(X_train.loc[:10000,:])\nX_train_kernelPCA = kernelPCA.transform(X_train)\nX_train_kernelPCA = pd.DataFrame(data=X_train_kernelPCA,index=train_index)\n\nX_validation_kernelPCA = kernelPCA.transform(X_validation)\nX_validation_kernelPCA = \\\n    pd.DataFrame(data=X_validation_kernelPCA, index=validation_index)\n\nscatterPlot(X_train_kernelPCA, y_train, \"Kernel PCA\")\n```", "```py\n# Singular Value Decomposition\nfrom sklearn.decomposition import TruncatedSVD\n\nn_components = 200\nalgorithm = 'randomized'\nn_iter = 5\nrandom_state = 2018\n\nsvd = TruncatedSVD(n_components=n_components, algorithm=algorithm, \\\n                   n_iter=n_iter, random_state=random_state)\n\nX_train_svd = svd.fit_transform(X_train)\nX_train_svd = pd.DataFrame(data=X_train_svd, index=train_index)\n\nX_validation_svd = svd.transform(X_validation)\nX_validation_svd = pd.DataFrame(data=X_validation_svd, index=validation_index)\n\nscatterPlot(X_train_svd, y_train, \"Singular Value Decomposition\")\n```", "```py\n# Gaussian Random Projection\nfrom sklearn.random_projection import GaussianRandomProjection\n\nn_components = 'auto'\neps = 0.5\nrandom_state = 2018\n\nGRP = GaussianRandomProjection(n_components=n_components, eps=eps, \\\n                               random_state=random_state)\n\nX_train_GRP = GRP.fit_transform(X_train)\nX_train_GRP = pd.DataFrame(data=X_train_GRP, index=train_index)\n\nX_validation_GRP = GRP.transform(X_validation)\nX_validation_GRP = pd.DataFrame(data=X_validation_GRP, index=validation_index)\n\nscatterPlot(X_train_GRP, y_train, \"Gaussian Random Projection\")\n```", "```py\n# Sparse Random Projection\nfrom sklearn.random_projection import SparseRandomProjection\n\nn_components = 'auto'\ndensity = 'auto'\neps = 0.5\ndense_output = False\nrandom_state = 2018\n\nSRP = SparseRandomProjection(n_components=n_components, \\\n        density=density, eps=eps, dense_output=dense_output, \\\n        random_state=random_state)\n\nX_train_SRP = SRP.fit_transform(X_train)\nX_train_SRP = pd.DataFrame(data=X_train_SRP, index=train_index)\n\nX_validation_SRP = SRP.transform(X_validation)\nX_validation_SRP = pd.DataFrame(data=X_validation_SRP, index=validation_index)\n\nscatterPlot(X_train_SRP, y_train, \"Sparse Random Projection\")\n```", "```py\n# Isomap\n\nfrom sklearn.manifold import Isomap\n\nn_neighbors = 5\nn_components = 10\nn_jobs = 4\n\nisomap = Isomap(n_neighbors=n_neighbors, \\\n                n_components=n_components, n_jobs=n_jobs)\n\nisomap.fit(X_train.loc[0:5000,:])\nX_train_isomap = isomap.transform(X_train)\nX_train_isomap = pd.DataFrame(data=X_train_isomap, index=train_index)\n\nX_validation_isomap = isomap.transform(X_validation)\nX_validation_isomap = pd.DataFrame(data=X_validation_isomap, \\\n                                   index=validation_index)\n\nscatterPlot(X_train_isomap, y_train, \"Isomap\")\n```", "```py\n# Multidimensional Scaling\nfrom sklearn.manifold import MDS\n\nn_components = 2\nn_init = 12\nmax_iter = 1200\nmetric = True\nn_jobs = 4\nrandom_state = 2018\n\nmds = MDS(n_components=n_components, n_init=n_init, max_iter=max_iter, \\\n          metric=metric, n_jobs=n_jobs, random_state=random_state)\n\nX_train_mds = mds.fit_transform(X_train.loc[0:1000,:])\nX_train_mds = pd.DataFrame(data=X_train_mds, index=train_index[0:1001])\n\nscatterPlot(X_train_mds, y_train, \"Multidimensional Scaling\")\n```", "```py\n# Locally Linear Embedding (LLE)\nfrom sklearn.manifold import LocallyLinearEmbedding\n\nn_neighbors = 10\nn_components = 2\nmethod = 'modified'\nn_jobs = 4\nrandom_state = 2018\n\nlle = LocallyLinearEmbedding(n_neighbors=n_neighbors, \\\n        n_components=n_components, method=method, \\\n        random_state=random_state, n_jobs=n_jobs)\n\nlle.fit(X_train.loc[0:5000,:])\nX_train_lle = lle.transform(X_train)\nX_train_lle = pd.DataFrame(data=X_train_lle, index=train_index)\n\nX_validation_lle = lle.transform(X_validation)\nX_validation_lle = pd.DataFrame(data=X_validation_lle, index=validation_index)\n\nscatterPlot(X_train_lle, y_train, \"Locally Linear Embedding\")\n```", "```py\n# t-SNE\nfrom sklearn.manifold import TSNE\n\nn_components = 2\nlearning_rate = 300\nperplexity = 30\nearly_exaggeration = 12\ninit = 'random'\nrandom_state = 2018\n\ntSNE = TSNE(n_components=n_components, learning_rate=learning_rate, \\\n            perplexity=perplexity, early_exaggeration=early_exaggeration, \\\n            init=init, random_state=random_state)\n\nX_train_tSNE = tSNE.fit_transform(X_train_PCA.loc[:5000,:9])\nX_train_tSNE = pd.DataFrame(data=X_train_tSNE, index=train_index[:5001])\n\nscatterPlot(X_train_tSNE, y_train, \"t-SNE\")\n```", "```py\n# Mini-batch dictionary learning\n\nfrom sklearn.decomposition import MiniBatchDictionaryLearning\n\nn_components = 50\nalpha = 1\nbatch_size = 200\nn_iter = 25\nrandom_state = 2018\n\nminiBatchDictLearning = MiniBatchDictionaryLearning( \\\n                        n_components=n_components, alpha=alpha, \\\n                        batch_size=batch_size, n_iter=n_iter, \\\n                        random_state=random_state)\n\nminiBatchDictLearning.fit(X_train.loc[:,:10000])\nX_train_miniBatchDictLearning = miniBatchDictLearning.fit_transform(X_train)\nX_train_miniBatchDictLearning = pd.DataFrame( \\\n    data=X_train_miniBatchDictLearning, index=train_index)\n\nX_validation_miniBatchDictLearning = \\\n    miniBatchDictLearning.transform(X_validation)\nX_validation_miniBatchDictLearning = \\\n    pd.DataFrame(data=X_validation_miniBatchDictLearning, \\\n    index=validation_index)\n\nscatterPlot(X_train_miniBatchDictLearning, y_train, \\\n            \"Mini-batch Dictionary Learning\")\n```", "```py\n# Independent Component Analysis\nfrom sklearn.decomposition import FastICA\n\nn_components = 25\nalgorithm = 'parallel'\nwhiten = True\nmax_iter = 100\nrandom_state = 2018\n\nfastICA = FastICA(n_components=n_components, algorithm=algorithm, \\\n                  whiten=whiten, max_iter=max_iter, random_state=random_state)\n\nX_train_fastICA = fastICA.fit_transform(X_train)\nX_train_fastICA = pd.DataFrame(data=X_train_fastICA, index=train_index)\n\nX_validation_fastICA = fastICA.transform(X_validation)\nX_validation_fastICA = pd.DataFrame(data=X_validation_fastICA, \\\n                                    index=validation_index)\n\nscatterPlot(X_train_fastICA, y_train, \"Independent Component Analysis\")\n```"]