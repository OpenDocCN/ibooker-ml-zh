- en: 6 Gathering datasets
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 收集数据集
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Data sources
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据来源
- en: Turning raw data into datasets
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将原始数据转换为数据集
- en: Distinguishing data from metadata
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区分数据和元数据
- en: Defining how much is enough
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义多少才算足够
- en: Solving the cold start problem
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决冷启动问题
- en: Looking for properties of a healthy data pipeline
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找健康数据管道的特性
- en: In the preceding chapters, we’ve covered the inherent steps in the preparation
    for building a machine learning (ML) system, including the problem space and solution
    space, identifying risks, and finding the right loss functions and metrics. Now
    we will talk about an aspect your ML project simply won’t take off without—datasets.
    We will compare them with vital elements of our lives. Just like you’ll need fuel
    to start your car or a nutritious breakfast to get a charge before a busy day
    at work, an ML system needs a dataset to function properly.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们已经涵盖了构建机器学习（ML）系统准备中的固有步骤，包括问题空间和解决方案空间，识别风险，以及找到合适的损失函数和度量标准。现在我们将讨论你的机器学习项目没有它就无法起飞的一个方面——数据集。我们将把它们与我们生活中的重要元素进行比较。就像你需要燃料来启动你的汽车或一顿营养早餐来在忙碌的工作日之前充电一样，机器学习系统需要一个数据集才能正常工作。
- en: 'There is an old popular quote about real estate: the three most important things
    about it are location, location, and location. Similarly, if we were to choose
    only three things to focus on while building an ML system, those would be data,
    data, and data. Another classic quote from the computer science world says “garbage
    in, garbage out,” and we can’t doubt its correctness.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个关于房地产的古老流行谚语：最重要的是位置，位置，还是位置。同样，如果我们只能选择三个在构建机器学习系统时需要关注的事情，那么这些将是数据，数据，还是数据。计算机科学世界中的另一个经典谚语说“垃圾进，垃圾出”，我们无法怀疑其正确性。
- en: Here we’ll break down the essence of working with datasets, from finding and
    processing data sources to properly cooking your dataset and building data pipelines.
    As a culmination of the whole chapter, we will look at datasets as a part of design
    documents, using the examples of Supermegaretail and PhotoStock Inc.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将从寻找和处理数据源到正确处理你的数据集和构建数据管道的工作本质进行分解。作为本章的总结，我们将使用Supermegaretail和PhotoStock
    Inc.的例子，将数据集视为设计文档的一部分。
- en: 6.1 Data sources
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 数据来源
- en: 'You can use absolutely any source to find data for your dataset. The availability
    and quality of these sources will depend on your work environment, your company’s
    legacy, and many other factors. Which should be addressed in the first place will
    depend mostly on the goals of your ML system. Here we list the most popular data
    sources or their categories while accompanying them with real-world examples:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用任何来源来寻找数据集的数据。这些来源的可用性和质量将取决于你的工作环境，你公司的传统，以及许多其他因素。首先应该解决什么问题，主要取决于你的机器学习系统的目标。在这里，我们列出了最流行的数据来源或其类别，同时附上现实世界的例子：
- en: '*Global activities**—*This is a huge category of data sources that includes
    any activity in a single entity that is regularly recorded and stored on an ongoing
    basis. As an example, in the stock trading business, traders around the globe
    act on the stock market, and the result of their actions (trades and prices) later
    becomes available for other parties.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*全球活动**—*这是一个庞大的数据来源类别，包括任何单一实体中定期记录并持续存储的活动。例如，在股票交易业务中，全球的贸易商在股票市场上采取行动，他们的行动结果（交易和价格）后来对其他各方可用。'
- en: '*Physical processes**—*These are global changes happening on the planet as
    we speak. They can be environmental shifts monitored on various levels, from satellite
    images to microsensors on farming lands.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*物理过程**—*这些是我们正在谈论的全球变化，发生在我们星球上。这些变化可以在各种层面上进行监测，从卫星图像到农田上的微传感器。'
- en: '*External databases**—*Certain third-party companies thrive on collecting domain-specific
    data using their proprietary methods and know-how. You will want to address them,
    if their data meets your requirements and needs.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*外部数据库**—*某些第三方公司通过他们专有的方法和知识收集特定领域的数据而蓬勃发展。如果你需要的数据符合你的需求和标准，你将想要与他们打交道。'
- en: '*Local business processes**—*Here we are going from global to local. The business
    itself can generate a huge amount of data as it operates and grows. If you work
    in eCommerce, purchase history can be your primary source of data.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*本地业务流程**—*这里我们从全球转向本地。业务本身在运营和成长过程中可以产生大量数据。如果你从事电子商务，购买历史可以成为你的主要数据来源。'
- en: '*Labeling by a dedicated team**—*Your company can hire a team of experts to
    generate labels for a specific problem.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*由专业团队标注**——*您的公司可以雇佣一支专家团队为特定问题生成标签。'
- en: '*Labeling by end system users**—*This is a similar approach, where the company
    may provide a user interface for end users. There, they will specify the inputs
    for your ML system.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*由最终系统用户标注**——*这是一种类似的方法，其中公司可能为最终用户提供一个用户界面。在那里，他们将指定您机器学习系统的输入。'
- en: '*Artificially generated datasets*—This is where data is created by scientific
    simulators, rendered environments, or other synthetic sources. Items created with
    generative AI (e.g., image generators or large language models) can be attributed
    to this category as well.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*人工生成数据集*——这是数据由科学模拟器、渲染环境或其他合成来源创建的地方。使用生成式AI（例如，图像生成器或大型语言模型）创建的项目也可以归入这一类别。'
- en: Some data sources are unique, and having access to them may be a significant
    competitive advantage. Many big tech companies like Google and Meta are successful
    mainly because of their valuable user behavior data used for ad targeting. On
    the other hand, other datasets are easy to acquire; the information is either
    free to download or can be created for a nonrestrictive price (many datasets sold
    by data providers are relatively cheap). This doesn’t mean that cheap equals low
    quality, as it all depends on what kind of data you need. It might turn out that
    this particular free-of-charge source fits your ML system perfectly. There are
    also intermediate points on this spectrum, though, and not in terms of price.
    Access to data can be limited in some regions or be in a “gray zone” legality-wise.
    Mature companies tend to follow the law (and we recommend you do so as well!),
    while young startups with the YOLO mentality sometimes consider minor violations.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一些数据源是独特的，能够访问它们可能是一个重要的竞争优势。许多大型科技公司，如谷歌和Meta，之所以成功，主要是因为它们用于广告定位的有价值用户行为数据。另一方面，其他数据集很容易获取；信息要么可以免费下载，要么可以以非限制性价格（许多数据提供商销售的数据集相对便宜）创建。但这并不意味着便宜等于低质量，因为这完全取决于你需要什么类型的数据。可能结果是这个免费来源恰好完美地符合你的机器学习系统。然而，在这个光谱上也有中间点，但这并不是指价格。在某些地区，数据访问可能受到限制，或者从法律角度来看处于“灰色地带”。成熟的公司倾向于遵守法律（我们也建议您这样做！），而拥有YOLO心态的年轻初创公司有时会考虑轻微的违规行为。
- en: Some datasets become valuable when enriched or annotated/labeled. *Annotation*
    means combining a raw dataset with proper labels or, in other words, creating
    a closely tied dataset and connecting it with a new one. It is a popular pattern
    of mixing a unique proprietary dataset with a public data source and getting a
    much more valuable dataset as a result.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一些数据集在丰富或标注/标记后变得有价值。*标注*意味着将原始数据集与适当的标签相结合，或者换句话说，创建一个紧密相关的数据集并将其与一个新的数据集连接起来。这是一种将独特的专有数据集与公共数据源混合，从而得到更有价值的数据集的流行模式。
- en: 'Ad tech companies, mentioned earlier, may benefit from joining datasets as
    well. Let’s take a look at a classic example. A company operates a free-to-play
    game, which means it has a lot of players, and only some of them pay money. The
    number and list of paying customers is kept secret and available only to the game’s
    publisher. At the same time, its partnering ad network has millions of detailed
    user profiles derived from behavior. When these datasets are combined (see figure
    6.1), it opens a great marketing opportunity: the company can target its new ad
    to potential customers who are similar to its paying players. Data exchanges like
    this increase the efficiency of online marketing and thus are one of the powers
    driving the modern web.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 之前提到的广告技术公司也可以从加入数据集中受益。让我们看看一个经典的例子。一家公司运营一款免费游玩的游戏，这意味着它拥有大量玩家，但只有其中一部分玩家付费。付费客户的人数和名单是保密的，只有游戏的出版商可以访问。同时，它的合作伙伴广告网络拥有数百万个详细用户档案，这些档案来自行为数据。当这些数据集结合在一起（见图6.1）时，它开辟了一个巨大的营销机会：公司可以将新的广告定位到与其付费玩家相似的可能客户。这种类型的数据交换提高了在线营销的效率，因此是推动现代网络发展的力量之一。
- en: '![figure](../Images/CH06_F01_Babushkin.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F01_Babushkin.png)'
- en: Figure 6.1 Combining data sources of two completely different businesses can
    eventually benefit both.
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.1 将两个完全不同业务的数据源结合起来，最终可能对双方都有益。
- en: When we’re talking about joining datasets, it’s not always the case of a straightforward
    connection between similar datasets (like a SQL join between two tables). An important
    concept here is multimodality, which is an intersection of various modalities
    in a dataset. In simple words, modality is a kind of information we receive; it
    is common to describe the world as multimodal for humans (we hear sounds, see
    colors, feel motions, etc.). In ML-related literature, multimodal datasets are
    those that combine various kinds of data to describe one problem. Think of images
    of an item being sold and its text description. Combining datasets of different
    origins and modalities is a powerful technique.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论合并数据集时，并不总是像在两个表之间进行SQL连接那样简单地将相似数据集连接起来。这里的一个重要概念是多模态，它是一个数据集中各种模态的交集。简单来说，模态是我们接收到的信息的一种类型；对于人类来说，通常将世界描述为多模态（我们听到声音，看到颜色，感受到运动等）。在机器学习相关的文献中，多模态数据集是指结合各种类型的数据来描述一个问题的数据集。想想看，一个正在出售的商品的图片和它的文字描述。将不同来源和模态的数据集结合起来是一种强大的技术。
- en: 'Speaking about combining data sources, Arseny once helped kickstart a startup
    as an advisor. The company worked in the agri-tech area, helping farmers and related
    companies to increase their operating efficiency, and its secret sauce was based
    on datasets. The way it worked was as follows. One data source was public, using
    satellite images of the planet available thanks to several space initiatives like
    Landsat by NASA and Copernicus by ESA (where you can fetch countless images of
    agricultural lands). But having those images alone could not bolster the efficiency
    of the startup, as it lacked information describing these agricultural lands.
    The main problem is most agricultural companies are far from innovative, and there
    is no single solid source of data on what crops were grown, the yield results,
    and more. Such data is poorly digitized but is really valuable for multiple business
    needs: it can be used to reduce the amount of fertilizer used, to estimate future
    prices for food commodities, and so on. Eventually, the team implemented smart
    ways of gathering such data and merging it with the huge photo database. An ML
    system built on these joint datasets helped the company to grow rapidly.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 谈到合并数据源，阿森尼曾经作为顾问帮助启动了一家初创公司。该公司在农业技术领域工作，帮助农民和相关公司提高运营效率，其秘密配方基于数据集。其工作方式如下。一个数据源是公开的，使用NASA的Landsat和ESA的Copernicus等几个太空倡议提供的地球卫星图像（你可以获取无数农业用地的图像）。但仅仅拥有这些图像并不能提高初创公司的效率，因为它缺乏描述这些农业用地的信息。主要问题是大多数农业公司缺乏创新，没有关于种植了哪些作物、产量结果等单一可靠的数据来源。这类数据数字化程度较低，但对于多种商业需求来说确实非常有价值：它可以用来减少使用的肥料量，估计食品商品的未来价格，等等。最终，团队实施了智能的方法来收集此类数据并将其与庞大的照片数据库合并。建立在这些联合数据集之上的机器学习系统帮助公司迅速发展。
- en: Defining data sources and the way they interconnect is the first cornerstone
    of solving the data problem for the system. But raw data is often almost useless
    until we make it available for the system and ML model, filter it, and preprocess
    it in other ways.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 定义数据源及其相互连接的方式是解决系统数据问题的第一个基石。但原始数据通常几乎毫无用处，直到我们使其对系统和机器学习模型可用，对其进行过滤，并以其他方式进行预处理。
- en: 6.2 Cooking the dataset
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 数据集的加工
- en: Experienced engineers who work with data know that in the vast majority of cases,
    the data in its raw form is too raw to work effectively. Hence the name *raw data*,
    meaning a chaotically compiled, unorganized giant clump of information. Thus,
    initial raw datasets are rarely in good enough shape to use them as is. You need
    to cook the dataset to apply it to your ML system in the most efficient way possible.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有经验的工程师知道，在绝大多数情况下，原始数据过于原始，无法有效地进行处理。因此，我们称之为“原始数据”，意味着这是一个混乱编译、无组织的巨大信息块。因此，最初的原始数据集很少处于足够好的状态，可以直接使用。你需要对数据集进行加工，以便以最有效的方式将其应用于你的机器学习系统。
- en: We’ve gathered a list of techniques you can use to properly cook your dataset,
    with each technique presented in a separate subsection. This list is not strictly
    ordered, and the order of actions may vary depending on your domain, so there
    is no single universal answer. In some cases, you can’t filter data before labeling,
    while sometimes filtering happens multiple times throughout the cooking process.
    Let’s briefly touch on these techniques.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经收集了一份您可以用来正确处理数据集的技术列表，每个技术都在一个单独的小节中介绍。这个列表并不是严格有序的，而且根据您的领域，操作顺序可能会有所不同，因此没有唯一的通用答案。在某些情况下，您可能需要在标记之前过滤数据，而在某些情况下，过滤可能会在整个处理过程中多次发生。让我们简要地探讨这些技术。
- en: 6.2.1 ETL
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 ETL
- en: ETL, which stands for “extract, transform, load,” is a data preparation phase,
    such as fetching information from an external data source and tailoring its structure
    to your needs. As a simplified example, you can fetch JSON files from a third-party
    API and store them in local storage as CSV or Parquet files.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ETL，即“提取、转换、加载”，是一个数据准备阶段，例如从外部数据源获取信息并将其结构调整为满足您的需求。作为一个简化的例子，您可以从第三方 API 获取
    JSON 文件，并将它们存储为 CSV 或 Parquet 文件到本地存储。
- en: Note  The high-level goal of ETL is to solve the data availability problem.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：ETL 的高级目标是解决数据可用性问题。
- en: 'Data availability at this point implies two things:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，数据可用性意味着两件事：
- en: Data can be fetched easily and effectively for a training process (e.g., if
    the target dataset is a product of multiple interactions from multiple sources,
    it is useful to understand how to make it fetchable with a single click, command,
    or call).
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可以轻松有效地用于训练过程（例如，如果目标数据集是来自多个来源的多次交互的结果，了解如何通过单次点击、命令或调用使其可获取是有用的）。
- en: Data will be available for training and runtime phases. We don’t care if fetching
    data is effective for inference at this point (we will discuss it later, in chapter
    11), but we need to guarantee the same data sources are available for inference.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据将在训练阶段和运行时阶段可用。我们目前不关心获取数据是否对推理有效（我们将在第 11 章中讨论这个问题），但我们需要保证推理时可以使用相同的数据源。
- en: Note  Designing an effective ETL process is an art of its own, as it requires
    a good understanding of various data storages and data processing tools. We only
    scratch the surface of this topic in our book and recommend consulting other sources
    for a more in-depth understanding.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：设计一个有效的 ETL 流程是一门艺术，因为它需要很好地理解各种数据存储和数据处理工具。我们在本书中仅触及了这个话题的皮毛，并建议您查阅其他资料以获得更深入的了解。
- en: 'The important question here is: “Should I care about data storage and structure
    at this point?” The answer to this question lies in the following spectrum:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的重要问题是：“我是否应该在这个阶段关心数据存储和结构？” 这个问题的答案在于以下范围：
- en: Sometimes datasets are small enough, and the chances of a sudden growth by multiple
    orders of magnitude are minuscule. It means you can choose almost any storage
    (e.g., the one that is already actively used in your organization or the one you’re
    most familiar with). You would be surprised how often inexperienced ML engineers
    tend to overengineer things here, like designing a distributed multicluster storage
    for a static tabular dataset with thousands of rows and tens of columns.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时数据集足够小，突然增长多个数量级的可能性极小。这意味着您可以选择几乎任何存储方式（例如，已经在您的组织中积极使用的存储方式，或者您最熟悉的存储方式）。您可能会惊讶地发现，不经验证的
    ML 工程师往往倾向于过度设计，例如为具有数千行和数十列的静态表格数据集设计分布式多集群存储。
- en: Sometimes it’s clear from the very beginning that your dataset will be huge
    and will grow rapidly, so you should pay close attention as you design the data
    model. We don’t consider ourselves world-class experts in data engineering of
    this kind and recommend checking out other books if this is the case for your
    system. Our favorite work on the subject is *Designing Data-Intensive Applications*
    by Martin Kleppmann.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时从一开始就很明显，您的数据集将会非常大并且会迅速增长，因此在设计数据模型时您应该特别关注。我们不认为自己是这种类型的数据工程的世界级专家，如果您的情况是这样，我们建议您查阅其他书籍。我们最喜欢的关于这个主题的作品是马丁·克莱普曼的《设计数据密集型应用》。
- en: 6.2.2 Filtering
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 过滤
- en: No data source is absolutely perfect. By *perfect*, we mean clean, consistent,
    and relevant to your problem. In the short story at the beginning of this chapter,
    we mentioned APIs that provide satellite images, but the company only needed those
    that weren’t too cloudy and were related to agricultural regions; otherwise, storing
    irrelevant data would significantly increase the costs. It meant that a good chunk
    of preliminary work of selecting appropriate satellite photos had to be done as
    the very first step.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 没有数据源是绝对完美的。当我们说*完美*时，意味着数据是干净的、一致的，并且与你的问题相关。在本章开头的故事中，我们提到了提供卫星图像的API，但公司只需要那些不太多云且与农业区域相关的图像；否则，存储无关数据将显著增加成本。这意味着选择合适的卫星照片的大量初步工作必须作为第一步完成。
- en: 'Data filtering is a very domain-specific operation. In some cases, it can be
    done fully automatically based on a set of rules or statistics; in others, it
    requires human attention at scale. Experience shows that the end approach normally
    lies somewhere between those two extremes. A combination of the human eye and
    automated heuristics is what works best, with the following algorithm being a
    popular approach: look through a subset of data (either randomly or based on some
    initial insight or feedback), find patterns, reflect them in the code to extend
    coverage, and then look through a narrower subset.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 数据过滤是一个非常特定领域的操作。在某些情况下，它可以完全基于一组规则或统计自动完成；在其他情况下，它需要大规模的人类关注。经验表明，最终的方法通常位于这两个极端之间。人眼和自动化启发式方法的结合是最佳选择，以下算法是一个流行的方法：检查数据的一个子集（可以是随机选择，也可以基于一些初始洞察或反馈），寻找模式，在代码中反映这些模式以扩展覆盖范围，然后检查更窄的子集。
- en: 'While the absence of data filtering leads to extensive noise in the dataset
    and thus worse performance of the whole system, overly aggressive filtering may
    have a negative effect as well: in some cases, it may distort data distribution
    and lead to a worse performance on real data.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然缺乏数据过滤会导致数据集中出现大量噪声，从而降低整个系统的性能，但过于激进的过滤也可能产生负面影响：在某些情况下，它可能会扭曲数据分布，导致在真实数据上的性能更差。
- en: 6.2.3 Feature engineering
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.3 特征工程
- en: '*Feature engineering* means transforming the data view in a manner so it’s
    most valuable for an ML algorithm. We’ll cover the topic in more detail later
    in the book when it’s time to discuss intermediate steps, as it’s rarely detailed
    during the early stages of ML system design. At this stage, we tend to focus on
    the question of how to get initial data to make a baseline model, which requires
    some level of abstraction at the current stage.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*特征工程*意味着以某种方式转换数据视图，使其对机器学习算法最有价值。我们将在本书的后续章节中更详细地介绍这个主题，当讨论到中间步骤时，因为在机器学习系统设计的早期阶段，它很少被详细讨论。在这个阶段，我们倾向于关注如何获取初始数据以构建基线模型的问题，这需要当前阶段一定程度的抽象。'
- en: 'Sometimes features are not engineered manually but created by a more complicated
    model; unlike “regular” features, they can be non-human-readable vectors. In these
    cases, it’s more precise to use the term *representations*, although at some level
    of abstraction, they’re the same thing: inputs to the ML model itself.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 有时特征不是通过手动工程创建的，而是由一个更复杂的模型创建的；与“常规”特征不同，它们可以是不可读的向量。在这些情况下，使用术语*表示*更准确，尽管在某种抽象层面上，它们是同一件事：机器学习模型本身的输入。
- en: For example, in a big ML-heavy organization, there may be a core team building
    a model that generates the best possible representations of users, goods, or other
    items at scale. Their model doesn’t solve business problems directly, but applied
    teams can use it to generate representations for their specific needs. That is
    a popular pattern when we’re talking about such data as images, videos, texts,
    or audio.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个以机器学习为核心的大组织中，可能有一个核心团队构建一个模型，该模型可以大规模地生成用户、商品或其他项目的最佳表示。他们的模型并不直接解决业务问题，但应用团队可以使用它来生成满足他们特定需求的表示。当我们谈论图像、视频、文本或音频等数据时，这是一个流行的模式。
- en: 6.2.4 Labeling
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.4 标注
- en: In many scenarios, a dataset itself is not too valuable, but adding extra annotations,
    often known in the ML world as *labels*, is a game changer. Deciding what kind
    of labels should be used is extremely important, as it dictates many other choices
    down the road.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，数据集本身并不太有价值，但添加额外的标注，在机器学习世界中通常被称为*标签*，却是一个转折点。决定使用哪种类型的标签非常重要，因为它决定了后续许多其他选择。
- en: 'Let’s imagine you’re building a medical assistance product, a system that helps
    radiologists analyze patients’ images. It is one of the most popular ML applications
    in medicine that is still very complicated to design properly. A use case may
    seem simple: a doctor looks at an image and judges if there is a malignancy, so
    you want doctors to label the dataset.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设你正在构建一个医疗辅助产品，一个帮助放射科医生分析患者图像的系统。这是医学中非常复杂但仍然非常受欢迎的机器学习应用之一。一个用例可能看起来很简单：医生查看图像并判断是否存在恶性，因此你希望医生对数据集进行标注。
- en: There are numerous ways to label it (see figure 6.2), including
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以对其进行标注（见图6.2），包括
- en: '*Binary classification style*—Is there a malignancy in the image?'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*二分类风格*——图像中是否存在恶性？'
- en: '*Multiclass classification style*—What kind of malignancy is in the image,
    if any?'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多类分类风格*——图像中是否存在某种恶性，如果有？'
- en: '*Detection style*—What region of the image contains a malignancy, if any?'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*检测风格*——如果存在，图像中包含恶性的区域是什么？'
- en: '*Segmentation style*—Exactly which pixels of the image represent malignancy,
    if any?'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分割风格*——如果存在，图像中哪些像素代表恶性？'
- en: '![figure](../Images/CH06_F02_Babushkin.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH06_F02_Babushkin.png)'
- en: Figure 6.2 Various approaches to data labeling
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.2 数据标注的各种方法
- en: These ways of labeling data require different labeling tools and different expertise
    from the labeling team. They also vary in terms of time required. The method affects
    what kind of model you can use for the problem, and, most importantly, it limits
    the product. Making the right decision in this case is impossible unless you follow
    the steps from chapter 2 and gather enough information about product goals and
    needs.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据标注方法需要不同的标注工具和来自标注团队的不同专业知识。它们在所需时间上也有所不同。方法会影响你可以为问题使用的模型类型，最重要的是，它限制了产品。除非你遵循第2章的步骤并收集足够的产品目标和需求信息，否则在这种情况下做出正确的决定是不可能的。
- en: In some situations, labeling data with the proper level of detail is nearly
    impossible. For example, it could take too much time, there may not be enough
    experts to cover enough data, or the toolset is not ready. In these cases, it
    is worth considering using weakly supervised approaches that allow the use of
    inaccurate or partial labels. If you’re not familiar with this branch of ML tricks,
    we recommend “A Brief Introduction to Weakly Supervised Learning” ([https://mng.bz/75My](https://mng.bz/75My))
    for an overview; more links for further reading are at Awesome-Weak- Supervision
    ([https://mng.bz/mRp2](https://mng.bz/mRp2)).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，使用适当的详细程度标注数据几乎是不可能的。例如，可能需要太多时间，可能没有足够的专家来覆盖足够的数据，或者工具集尚未准备好。在这些情况下，考虑使用弱监督方法是有价值的，这些方法允许使用不准确或部分标签。如果你不熟悉这个机器学习技巧分支，我们推荐阅读“弱监督学习简介”（[https://mng.bz/75My](https://mng.bz/75My)）以获得概述；更多阅读链接可在Awesome-Weak-Supervision
    ([https://mng.bz/mRp2](https://mng.bz/mRp2))找到。
- en: Efficient data labeling requires a choice of a labeling platform, proper decomposition
    of tasks, task instructions that are easy to read and follow, easy-to-use task
    interfaces, quality control techniques, an overview of aggregation methods, and
    pricing. Additionally, best practices should be followed when designing instructions
    and interfaces, setting up different types of templates, training and examining
    performers, and constructing pipelines for evaluating the labeling process.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的数据标注需要选择一个标注平台，正确分解任务，编写易于阅读和遵循的任务说明，易于使用的任务界面，质量控制技术，汇总方法的概述，以及定价。此外，在设计说明和界面、设置不同类型的模板、培训和评估执行者以及构建评估标注流程的管道时，应遵循最佳实践。
- en: 'When datasets need to be labeled manually, there are two main ways to go: an
    in-house labeling team or third-party crowdsourcing services. In a nutshell, the
    former provides a more controlled and consistent labeling process, while the latter
    can scale up the annotation process quickly, possibly at a lower cost. The simplest
    heuristic of choosing one over another is based on the labeling complexity: once
    it requires specific knowledge or skills, it makes sense to develop an in-house
    team of curated experts; otherwise, using an external service is a good option.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要手动标注数据集时，主要有两种选择：内部标注团队或第三方众包服务。简而言之，前者提供了一种更加控制和一致的标注流程，而后者可以快速扩展标注过程，可能成本更低。选择其中一种方式而放弃另一种的最简单的方法是基于标注的复杂性：一旦需要特定的知识或技能，组建一个内部的专业团队是有意义的；否则，使用外部服务是一个不错的选择。
- en: 'There are dozens of crowdsourcing platforms available; the most popular is
    probably Amazon Mechanical Turk. As we opt for width over depth in this book,
    we will not focus on features of different platforms. Instead, we focus on generic
    properties of labeling with crowdsourcing:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的众包平台有数十个；最流行的大概是亚马逊机械师。由于我们在本书中更注重广度而非深度，我们将不会关注不同平台的功能。相反，我们关注众包标注的通用属性：
- en: Labelers tend to make mistakes even in the simplest tasks, such as binary classification
    of common objects. Most labelers on the platform work for multiple customers,
    so there is little chance they will memorize the nuances of the labeling instructions
    you’ve provided. As a result, labelers should be considered interchangeable, and
    the instruction should be as simple and nonambiguous as possible.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标注者往往即使在最简单的任务中也容易出错，例如对常见物体的二分类。平台上的大多数标注者为多个客户工作，因此他们很少有机会记住您提供的标注说明的细微差别。因此，应将标注者视为可互换的，并且说明应尽可能简单和非歧义。
- en: Some labelers are more attentive than others, and you may want to motivate them
    to dedicate more time to your tasks, not other gigs. Other labelers can be less
    attentive or adversarial; for example, they may try to use bots generating unverified
    labels instead of their own judgment. It is crucial to be able to distinguish
    the former from the latter. The most popular way of doing so is by including tests
    in the labeling process. Pick a share of a dataset with a label you’re confident
    in, and use it for labeling tasks so you can measure accuracy and other metrics
    of each particular labeler. You can also log some details of the labeling process
    for a clearer picture. If labels are generated very fast, it may be a strong signal
    of a bot or other illegal automation tool.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些标注者比其他标注者更细心，您可能希望激励他们更多地投入到您的任务中，而不是其他零工。有些标注者可能不够细心或具有对抗性；例如，他们可能会尝试使用生成未经验证标签的机器人，而不是依赖自己的判断。能够区分这两者至关重要。最流行的方法是在标注过程中包含测试。选择一个您对标签有信心的数据集的一部分，并用于标注任务，这样您就可以衡量每个特定标注者的准确性和其他指标。您还可以记录标注过程的某些细节，以获得更清晰的了解。如果标签生成得非常快，这可能是一个机器人或其他非法自动化工具的强烈信号。
- en: 'The labeling task should be designed in a way that reduces the variance caused
    by crowdsource workers. Consider the following scenario: a company is developing
    a chatbot and employs labelers to evaluate responses (questions and several possible
    answers). The most straightforward evaluation method here would be to ask labelers
    to rate each answer with a single number (e.g., 1 to 5). However, such labels
    might be inconsistent. A basic improvement in this case would be to introduce
    multiple evaluation criteria so that each answer is assessed separately based
    on factual correctness, tone of voice, and so on. These labels would be more consistent
    but still far from perfect. Another improvement would be asking for a pairwise
    comparison. Instead of assigning a number to each answer, labelers would perform
    a ternary comparison based on the criteria (version A is better / version B is
    better / A and B are equal). These labels are easier to collect, and the expected
    variance between labelers’ outputs is lower.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标注任务应设计成减少众包工作者引起的方差。考虑以下场景：一家公司正在开发聊天机器人，并雇佣标注者来评估响应（问题和几个可能的答案）。这里最直接的评价方法就是要求标注者用一个数字（例如，1到5）对每个答案进行评分。然而，这样的标签可能不一致。在这种情况下，一个基本的改进就是引入多个评价标准，以便根据事实正确性、语气等因素单独评估每个答案。这些标签将更加一致，但仍然远非完美。另一个改进是要求进行成对比较。而不是给每个答案分配一个数字，标注者会根据标准（版本A优于版本B/版本B优于版本A/版本A和版本B相等）进行三元比较。这些标签更容易收集，并且标注者输出之间的预期方差较低。
- en: 'In the 18th century, the French mathematician and philosopher Marquis de Condorcet
    stated a theorem applicable to political science. De Condorcet revealed the following
    idea: if a group needs to make a binary decision based on a majority vote and
    each group member is correct with a probability p > 0.5, the probability of a
    correct decision by the group grows with the number of group members asymptotically.
    This could be a formal reason for collective decision-making in the early days
    of the French Revolution, and now the very same flawless logic is applicable to
    ML systems!'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在18世纪，法国数学家和哲学家Marquis de Condorcet提出了一个适用于政治学的定理。De Condorcet揭示了以下观点：如果一个群体需要基于多数投票做出二元决策，并且每个群体成员正确决策的概率为p
    > 0.5，那么群体做出正确决策的概率将随着群体成员数量的增加而呈指数增长。这可能是法国大革命初期集体决策的正式原因，而现在，同样的完美逻辑也适用于机器学习系统！
- en: Let’s imagine we have three labelers annotating the same object for a binary
    classification. The problem is tricky, so each one is only right in 70% of cases.
    So we can expect a majority vote of three labelers to be correct in 0.7 * 0.7
    * 0.7 + 0.3 * 0.7 * 0.7 + 0.7 * 0.3 * 0.7 + 0.7 * 0.7 * 0.3 = 78.4%. That’s a
    nice boost!
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设我们有三个标注者对同一个二元分类的物体进行标注。这个问题很棘手，所以每个人在70%的情况下是正确的。因此，我们可以预期三个标注者的多数投票在78.4%的情况下是正确的（0.7
    * 0.7 * 0.7 + 0.3 * 0.7 * 0.7 + 0.7 * 0.3 * 0.7 + 0.7 * 0.7 * 0.3）。这是一个很好的提升！
- en: 'These numbers should be interpreted with a grain of salt. Condorcet’s jury
    theorem implies assumptions that are not exactly met here: labelers are not completely
    independent, and their origin of error is correlated (this may be the case when
    data samples are noisy and hard to read). But still, an ensemble of labelers is
    more accurate compared to a single labeler, and this technique can be used to
    improve labeling results.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字应该谨慎解读。Condorcet的陪审团定理隐含了一些在这里并不完全成立的假设：标注者并不完全独立，他们的错误来源是相关的（这可能是当数据样本嘈杂且难以阅读时的情况）。但即便如此，标注者群体的标注比单个标注者的标注更准确，并且这项技术可以用来提高标注结果。
- en: 'This idea can be modified to decrease costs. One algorithm modification we
    have faced is the following:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法可以被修改以降低成本。我们面临的一个算法修改如下：
- en: Label the object with two labelers.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用两个标注者对物体进行标注。
- en: If they agree, the label is accepted.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果他们意见一致，则接受该标签。
- en: If not, add three more votes and use the majority vote as a label.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果不一致，则增加三个额外的投票，并使用多数投票作为标签。
- en: With this modification, more votes are used for complicated samples that lack
    consistency, while fewer votes are required for simple cases.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这次修改，更复杂的样本需要更多的投票来保证一致性，而简单情况则需要的投票较少。
- en: The heuristic described here is a simple though powerful baseline. However,
    if data labeling is a key ingredient for your problem and you see the need to
    invest some effort, there is a lot of research dedicated to better design. Some
    of the materials we recommend are “A Survey on Task Assignment in Crowdsourcing”
    ([https://arxiv.org/abs/2111.08501](https://arxiv.org/abs/2111.08501)) and a tutorial
    by crowdsourcing vendor Toloka.ai ([https://web.archive.org/web/20240309071018/https:/toloka.ai/events/tutorial-wsdm//](https://web.archive.org/web/20240309071018/https:/toloka.ai/events/tutorial-wsdm/)).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这里描述的启发式方法简单但强大。然而，如果你的问题中数据标注是一个关键因素，并且你需要投入一些努力，那么有许多研究致力于更好的设计。我们推荐的一些材料包括“关于众包任务分配的调查”（[https://arxiv.org/abs/2111.08501](https://arxiv.org/abs/2111.08501)）和众包供应商Toloka.ai的教程（[https://web.archive.org/web/20240309071018/https:/toloka.ai/events/tutorial-wsdm//](https://web.archive.org/web/20240309071018/https:/toloka.ai/events/tutorial-wsdm/)）。
- en: Labelers’ consistency and mutual agreement may and should be measured. While
    ML practitioners often tend to use regular ML metrics to estimate it, those who
    have a statistical background may recall a concept called *interrater reliability*
    and its separate set of metrics (Cohen’s kappa, Scott’s pi, and Krippendorff’s
    alpha, to name a few). These metrics help keep the labeling team’s work reliable,
    filter out unscrupulous labels (thus greatly improving the overall system performance),
    and, in some scenarios, give a solid upper bound for the performance of your model
    (recall the human-level performance we mentioned earlier in chapter 3).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 标注者的一致性和相互同意应该被衡量。虽然机器学习从业者通常倾向于使用常规的机器学习指标来估计它，但那些有统计学背景的人可能会回忆起一个叫做*评分者间可靠性*的概念及其单独的指标集（例如Cohen的kappa、Scott的pi和Krippendorff的alpha等）。这些指标有助于保持标注团队工作的可靠性，过滤掉不诚实的标签（从而大大提高整体系统性能），并且在某些情况下，为你的模型性能提供一个坚实的上限（回想一下我们在第3章中提到的接近人类水平的表现）。
- en: 'Labeling teams require proper tooling, which may boost efficiency and improve
    the quality of created labels. If you go with third-party tooling, it’s very likely
    it will have a toolset for the most popular problems; for your own team, you need
    to set up or create your own. Aspects of choosing an existing solution or building
    a brand-new one are very domain specific, although the general approach is: the
    more popular the ML formulation of your problem, the more chances you have to
    find a modern quality toolset for the labeler team. As an example, there are multiple
    software solutions to create labels for image detection at scale, but once you
    need to annotate videos with 3D polygon meshes, you’re very likely to require
    something custom made.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 标注团队需要适当的工具，这可能会提高效率并改善创建标签的质量。如果你选择第三方工具，它很可能会有针对最常见问题的工具集；对于你自己的团队，你需要设置或创建自己的工具。选择现有解决方案或构建全新的解决方案的方面非常特定于领域，尽管一般的方法是：你的问题在机器学习公式中越受欢迎，你找到现代高质量工具集的机会就越大。以一个例子来说，有多个软件解决方案可以用于大规模图像检测的标签创建，但一旦你需要用3D多边形网格标注视频，你很可能需要定制化的解决方案。
- en: 'Recent advances in large foundation models have become a game changer in the
    labeling process as well: they often can be used to gather initial labels in a
    few-shot setup with the accuracy higher than nonexpert human labelers. One of
    many examples of a tool used for large language model labeling is Refuel Docs
    ([https://docs.refuel.ai/](https://docs.refuel.ai/)), notable for being focused
    on text data; however, using foundation models is also possible in other domains
    (e.g., addressing universal segmentation models like Segment Anything [[https://segment-anything.com/](https://segment-anything.com/)]
    or multimodal solutions like LLaVA [[https://llava-vl.github.io/](https://llava-vl.github.io/)]).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 大型基础模型在标注过程中的最新进展已经成为一个游戏规则的改变者：它们通常可以在几轮设置中收集初始标签，其准确性高于非专家人类标注员。用于大型语言模型标注的工具之一是Refuel
    Docs ([https://docs.refuel.ai/](https://docs.refuel.ai/))，它以专注于文本数据而著称；然而，在其他领域（例如，处理像Segment
    Anything [[https://segment-anything.com/](https://segment-anything.com/)]这样的通用分割模型或像LLaVA
    [[https://llava-vl.github.io/](https://llava-vl.github.io/)]这样的多模态解决方案）时，也可以使用基础模型。
- en: There are various algorithmic tricks that may help simplify/reduce the efforts
    required for the data labeling aspect of building a system, but we will not go
    into an in-depth review of those in this book, as they are too subject-specific.
    What we would like to additionally highlight is that very often efforts invested
    in the data labeling process and tools can be a driver for the success of your
    ML system.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种算法技巧可以帮助简化/减少构建系统所需的数据标注方面的努力，但在这本书中我们不会深入探讨这些技巧，因为它们太特定于领域了。我们还想强调的是，在数据标注过程和工具上投入的努力往往可以成为你机器学习系统成功的关键驱动因素。
- en: 6.3 Data and metadata
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 数据和元数据
- en: 'While datasets are used for the ML system we’re building, there is a layer
    of information on top of it—let’s use the word *metadata* for it. Metadata is
    information that describes certain properties of your datasets. Here are some
    examples:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们构建机器学习系统时，数据集被使用，但在这之上还有一个信息层——让我们用“元数据”这个词来指代它。元数据是描述你的数据集某些属性的信息。以下是一些例子：
- en: '*Time-related attributes**—*These include when the event happened and when
    it was processed and stored.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*时间相关属性**—*这些包括事件发生的时间和它被处理及存储的时间。'
- en: '*Source attributes**—*Source attributes tell whether or not data is gathered
    from multiple sources.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*来源属性**—*来源属性说明数据是否来自多个来源。'
- en: '*User attributes**—*If a user is somehow involved in dataset generation, which
    is not a rare situation, their metadata becomes important metadata of related
    data samples.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用户属性**—*如果一个用户以某种方式参与了数据集的生成，这种情况并不少见，他们的元数据就变成了相关数据样本的重要元数据。'
- en: '*Versions**—*If the data is processed, it can be useful to understand which
    version of software has been involved in processing.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*版本**—*如果数据被处理，了解参与处理软件的版本可能是有用的。'
- en: 'Metadata is crucial for data flows and guarantees their consistency. Let’s
    get back to the previous scenario with a medical application. Imagine that a company
    hired 10 medical professionals to label data for your system. After working for
    over a year, they have finally labeled a big dataset. Suddenly, you get an email:
    one of the labelers turns out to be a fraud, his diploma is fake, and his license
    has been suspended. This is an unacceptable situation because a tangible part
    of the data can be considered unreliable, which jeopardizes the entire dataset.
    So you enact the only adequate solution, which is to stop using his labels in
    the model ASAP. While this example is somewhat exaggerated, tens of less significant
    problems will occur in your system over time. Problems with data sources, regulatory
    interventions, and attempts to interpret model errors will make you refer to metadata
    on a regular basis.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据对于数据流至关重要，并保证了它们的连续性。让我们回到之前的医疗应用场景。想象一下，一家公司雇佣了10名医疗专业人员为你的系统标记数据。经过一年多的工作，他们终于标记了一个大数据集。突然，你收到一封邮件：其中一名标记者被发现是骗子，他的文凭是假的，他的执照已被吊销。这是一个不可接受的情况，因为数据的一个有形部分可能被认为不可靠，这会危及整个数据集。因此，你采取了唯一合适的解决方案，即立即停止在模型中使用他的标记。虽然这个例子有些夸张，但你的系统中随着时间的推移可能会出现许多不那么严重的问题。数据源问题、监管干预和尝试解释模型错误将使你定期参考元数据。
- en: As we discussed earlier, while recipes for datasets may vary, most likely, your
    dataset will be cooked properly. It’s okay to reveal new aspects of the problem,
    thus adjusting the dataset structure over time, and reflect them accordingly in
    how the dataset was processed. All these differences should be reflected in metadata.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，虽然数据集的配方可能不同，但你的数据集很可能会被正确处理。揭示问题的新的方面，从而随着时间的推移调整数据集结构，并在处理数据集的方式中相应地反映出来，这是可以的。所有这些差异都应该反映在元数据中。
- en: Another source of changes is testing and bug fixing. Imagine that you work for
    a ridesharing company and are to solve a problem of estimating how long it will
    take to reach point A from point B. You start with historical data your company
    generated, and there is a preprocessing job that extracts the information from
    logs and stores it in your favorite database, so you end up with a table like
    `latitude_from`, `longitude_ from`, `datetime_start`, `datetime_finish`, `distance`,
    `data_source`.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个变化来源是测试和错误修复。想象一下，你为一家共享出行公司工作，需要解决从B点到A点需要多长时间的问题。你从公司生成的历史数据开始，有一个预处理作业从日志中提取信息并将其存储在你最喜欢的数据库中，所以你最终得到一个像`latitude_from`、`longitude_from`、`datetime_start`、`datetime_finish`、`distance`、`data_source`这样的表格。
- en: At some point, the company makes a big acquisition of a competitor, and you
    decide to add its data to your ETL process. A new engineer on the team starts
    coding, new sources are attached, new data points are added to your dataset with
    the same schema, and suddenly your model performance drops. What happened?
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在某个时候，公司收购了一个竞争对手，你决定将其数据添加到你的ETL流程中。团队中的新工程师开始编码，新的来源被附加，新的数据点以相同的模式添加到你的数据集中，突然你的模型性能下降。发生了什么？
- en: 'Fast forward: your company stored distance in kilometers, while the acquired
    team stored distance in miles. An engineer who handled the integration knew it
    and implemented the support of miles for the new source but accidentally enabled
    it for the old one as well. So new data samples are correct for the new data source
    but not for the old one. If you had a metadata field like `preprocessing_function_version`,
    you could easily find affected samples. Without it, you need to gather the dataset
    from scratch after the code defect is discovered.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 快进：你的公司在存储距离时使用千米，而收购的团队使用英里。处理集成的工程师知道这一点，并为新的数据源实现了英里支持，但意外地也为旧的数据源启用了它。因此，新的数据样本对于新的数据源是正确的，但对于旧的数据源则不是。如果你有一个像`preprocessing_function_version`这样的元数据字段，你可以轻松地找到受影响的样本。如果没有，你需要在发现代码缺陷后从头开始收集数据集。
- en: 'Defects like this are not the only scenario where you may need to reveal the
    date of data sample origin. Some ML systems may affect data sources, creating
    a phenomenon called a *feedback loop*. A very common example of a feedback loop
    is recommender systems: let’s say a marketplace sells many items, and there is
    a block on the website titled “You may also be interested in these goods.” As
    a naive baseline, the company could put in the items that are most popular overall.
    But when this baseline is replaced with an ML system, new items appear, and old
    leaders lose their popularity in favor of new recommended items. With the poor
    design of such a system, there is a chance for an item that may not be that good
    to gain its place among popular ones and dominate for a long time. Storing the
    information when the sample was generated and what versions of related systems
    were live at the moment is crucial (not enough, though!) to avoid feedback loops.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这种缺陷并不是唯一可能需要揭示数据样本来源日期的场景。一些机器学习系统可能会影响数据来源，产生一种称为*反馈循环*的现象。反馈循环的一个非常常见的例子是推荐系统：假设一个市场销售许多商品，网站上有“你可能也感兴趣的商品”这样的板块。作为一个简单的基线，公司可能会放入最受欢迎的商品。但是当这个基线被机器学习系统取代时，新的商品会出现，而旧的商品领导者会失去其受欢迎程度，转而被新的推荐商品所取代。这种系统的设计不佳，可能会让一些并不那么好的商品在热门商品中占据一席之地，并长期占据主导地位。存储样本生成时的信息和当时活跃的相关系统版本至关重要（尽管还不够！）以避免反馈循环。
- en: Another important scenario related to metadata is *stratification*, a process
    of data sampling that shapes the distribution of various subgroups. For example,
    say a company started its operation in country X, gathering a lot of data related
    to customers from X. Later the company reached a new market, Y, and has a goal
    of providing the same level of service, including accuracy of ML models, for customers
    in both countries. It will require representing customers from X and Y in training
    and test datasets with proper balance.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 与元数据相关的一个重要场景是*分层*，这是一种数据抽样过程，它塑造了各种子组的分布。例如，假设一家公司在国家X开始运营，收集了大量与X国客户相关的数据。后来，公司进入了一个新的市场Y，并希望为两个国家的客户提供相同水平的服务，包括机器学习模型的准确性。这需要在训练和测试数据集中以适当的平衡来表示X和Y国的客户。
- en: Stratification is crucial for validation design and resisting the algorithmic
    bias, and both topics will be covered in future chapters.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 分层对于验证设计和抵抗算法偏差至关重要，这两个主题将在未来的章节中讨论。
- en: 6.4 How much data is enough?
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 需要多少数据才算足够？
- en: After everything we’ve mentioned on the importance of datasets, inexperienced
    ML practitioners may think that building a data pipeline that will stream numerous
    samples is enough to build a good system. Well, that’s where we can’t say confidently
    yes or no.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论了数据集的重要性之后，缺乏经验的机器学习从业者可能会认为，建立一个可以流式传输大量样本的数据管道就足以构建一个良好的系统。嗯，这就是我们无法自信地说是或否的地方。
- en: First, not all data samples are equally useful. Growing the dataset size is
    only useful if new objects help the model learn something new and related to the
    problem. Adding samples very similar to existing ones is pointless; the same goes
    for extremely noisy data. That’s why there is a popular research avenue dedicated
    to finding the most samples for dataset extension called *active learning*. The
    simplest intuition is to enrich the dataset with samples where the model was incorrect
    (this signal is often fetched using humans in the loop processed) or demonstrated
    the lowest confidence. The academic community has developed tens of methods related
    to active learning; see a recent survey (e.g., “A Comparative Survey of Deep Active
    Learning”by Xueying Zhan et al., [https://arxiv.org/abs/2203.13450](https://arxiv.org/abs/2203.13450))
    for more information.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，并非所有数据样本都具有同等的重要性。增加数据集的大小只有当新的对象有助于模型学习与问题相关的新知识时才有用。添加与现有样本非常相似的数据样本是没有意义的；对于极其嘈杂的数据也是如此。这就是为什么有一个流行的研究方向是寻找数据集扩展的最优样本，称为*主动学习*。最简单的直觉是，通过添加模型错误的样本（这种信号通常通过人工在循环处理中获取）或表现出最低置信度的样本来丰富数据集。学术界已经开发了与主动学习相关的数十种方法；有关更多信息，请参阅最近的调查（例如，Xueying
    Zhan等人撰写的“深度主动学习比较调查”，[https://arxiv.org/abs/2203.13450](https://arxiv.org/abs/2203.13450)）。
- en: One more aspect related to dataset size is *sampling*. While in most cases the
    question we ask ourselves is “How do we get more data?” sometimes it is “What
    part of data should I use to increase efficiency?” It usually happens when a dataset
    needs no manual labeling and is generated by a defined process—for example, the
    clickstream in popular B2C web services (search engines, marketplaces).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据集大小相关的一个更多方面是*采样*。虽然大多数情况下我们问自己的问题是“我们如何获取更多数据？”，但有时问题是“我应该使用数据集的哪一部分来提高效率？”这种情况通常发生在数据集不需要手动标记且由定义的过程生成时——例如，在流行的B2C网络服务（搜索引擎、市场）中的点击流。
- en: Sampling is effective when a dataset is not only huge but also tends to be imbalanced
    and/or may contain a lot of duplicates. Different strategies can be applicable
    here, and the most common of them are based on stratification, where you split
    data into groups (based on key features or algorithmically defined clusters) and
    limit the amount of data used for each group. The amounts don’t have to be equal—for
    example, if the temporal component is relevant to your problem, it’s very likely
    you will want to prioritize fresh data over old groups.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集不仅巨大，而且往往不平衡和/或可能包含大量重复项时，采样是有效的。这里可以适用不同的策略，其中最常见的是基于分层的方法，即根据关键特征或算法定义的簇将数据分成组，并限制每个组使用的数据量。这些数量不必相等——例如，如果时间成分与你的问题相关，你很可能希望优先考虑新鲜数据而不是旧组。
- en: Another point is about *data noisiness*. A while ago there was a strong consensus
    in the ML community that a few clean samples are better than multiple noisy samples.
    Things have changed recently, however; the progress with large models like GPT-3
    and CLIP demonstrated that at a certain scale, manual filtering is almost impossible
    (processing a dataset of 500 billion text tokens or tens of millions of images
    would cost a fortune), but using a huge amount of weakly or self-supervised (automatically,
    using heuristics) data works, so for some tasks massive imperfect datasets are
    more suitable than smaller hand-picked ones.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 另一点是关于*数据噪声*。不久前，机器学习社区中有一个强烈的共识，认为几个干净样本比多个噪声样本要好。然而，最近情况发生了变化；像GPT-3和CLIP这样的大型模型的进展表明，在某个规模上，手动过滤几乎是不可能的（处理5000亿个文本标记或数百万张图像将花费巨额资金），但使用大量弱监督或自监督（自动使用启发式方法）的数据是有效的，因此对于某些任务，大量不完美的数据集比小量精心挑选的数据集更适合。
- en: NOTE  While some amount of noise is safe to have in the training data, it’s
    way less acceptable for validation/test datasets. We’ll cover it more in chapter
    7.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：虽然训练数据中存在一些噪声是可以接受的，但在验证/测试数据集中，这要低得多。我们将在第7章中详细讨论。
- en: You could be expecting the model’s performance to be improved as a square root
    of dataset size asymptotically, no matter what metric is being used. This estimation
    is very rough and does not have to fit your problem exactly, but it may give you
    some intuition (see figure 6.3).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能期望模型的性能随着数据集大小的平方根而渐进地提高，无论使用什么指标。这种估计非常粗略，并不一定完全符合你的问题，但它可能给你一些直觉（见图6.3）。
- en: 'Not every model can be improved by adding new data because of the uncertainty
    of origin. Uncertainty primarily comes from two central origins:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 并非每个模型都可以通过添加新数据来改进，因为其来源的不确定性。不确定性主要来自两个核心来源：
- en: Lack of knowledge or information (known as *epistemic uncertainty*)
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 知识或信息缺乏（称为*认识不确定性*）
- en: Uncertainty found in the given data itself (also called *aleatoric uncertainly*)
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在给定数据本身中发现的*随机不确定性*（也称为*随机不确定性*）
- en: Aleatoric uncertainty emerges primarily due to the existing complexity in the
    data, like overlapping classes or additive noise. A critical characteristic of
    data uncertainty is that no matter how much additional training data gets collated,
    it does not reduce.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 随机不确定性主要源于数据中存在的复杂性，如重叠的类别或加性噪声。数据不确定性的一个关键特征是，无论收集多少额外的训练数据，它都不会减少。
- en: '![figure](../Images/CH06_F03_Babushkin.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F03_Babushkin.png)'
- en: Figure 6.3 A plot showcasing model metric improvements over dataset size for
    a real project. Each line demonstrates the dynamics of the target metric for a
    single task once the dataset size is growing.
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.3 展示了一个真实项目中模型指标随数据集大小变化的图表。每条线都展示了目标指标在数据集大小增长时的动态变化。
- en: On the other hand, epistemic uncertainty occurs when the model encounters an
    input located in a region that is either thinly covered by the training data or
    falls outside the scope of the training data (see figure 6.4).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，当模型遇到训练数据覆盖薄弱或超出训练数据范围的区域时，就会发生认知不确定性（参见图6.4）。
- en: '![figure](../Images/CH06_F04_Babushkin.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F04_Babushkin.png)'
- en: 'Figure 6.4 A schematic view of the main differences between aleatoric and epistemic
    uncertainties (source: “Explainable Uncertainty Quantifications for Deep Learning-Based
    Molecular Property Prediction” by ChuI Yang and YiPei Li, [https://mng.bz/n0ZV](https://mng.bz/n0ZV))'
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.4 随机不确定性和认知不确定性之间主要差异的示意图（来源：“基于深度学习的分子性质预测的可解释不确定性量化”由ChuI Yang和YiPei Li撰写，[https://mng.bz/n0ZV](https://mng.bz/n0ZV))
- en: 'Once you have gathered some data and formed a training pipeline (see chapter
    10 for details), it unlocks an option of making an informed decision on how much
    more data you need and estimating the economic efficiency of new data. It is especially
    reasonable when working with expensive data (e.g., labeled by highly skilled professionals).
    A high-level algorithm is the following:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你收集了一些数据并形成了一个训练管道（详见第10章），它就解锁了做出关于你需要多少更多数据的知情决策以及估计新数据的经济效率的选项。当与昂贵的数据（例如，由高度熟练的专业人士标注的数据）一起工作时，这尤其合理。一个高级算法如下：
- en: Split the dataset into buckets so that the bucket size is close to uniform and
    similarity between samples of different buckets is maximized. This way, each sample
    for a user goes to the same bucket determined by a user ID (this brings us back
    to the stratification aspect mentioned earlier).
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据集分成桶，使得桶的大小接近均匀，并且不同桶之间的样本相似性最大化。这样，每个用户的样本都流向由用户ID确定的相同桶（这使我们回到了之前提到的分层方面）。
- en: Fix the computational budget if applicable. No matter what the dataset size
    is, you train the model for N batches.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果适用，固定计算预算。无论数据集的大小如何，你都要为N个批次训练模型。
- en: Train the model on data subsets in a range from a tiny share of the dataset
    to the whole thing, ensuring the subset used is cumulative (e.g., if sample X
    was used when training on a 10% subset, it must be used on 20%, 30%, etc.).
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在从数据集的一小部分到全部的数据子集范围内训练模型，确保使用的子集是累积的（例如，如果在训练10%的子集时使用了样本X，那么它必须在20%、30%等情况下使用）。
- en: Calculate key metrics for each trained model and make a plot using a horizontal
    axis for dataset size and a vertical axis for the metric itself.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算每个训练模型的键指标，并使用数据集大小作为水平轴，指标本身作为垂直轴绘制图表。
- en: With some imagination enabled, extrapolate how much more data is required to
    squeeze another 1% of metric change.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过一些想象力的激发，推测还需要多少数据才能使指标变化再增加1%。
- en: The precision of this metric is low, and it doesn’t take systematic aspects
    into account (e.g., concept drifts that we’ll discuss in chapter 14), although
    even this precision is often crucial to making an important decision on how much
    we should invest in the data labeling pipeline.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这个指标的精度较低，并且没有考虑系统性的方面（例如，第14章中我们将讨论的概念漂移），尽管即使这个精度对于做出关于我们应该在数据标注管道中投入多少的重要决策也是至关重要的。
- en: 6.5 Chicken-or-egg problem
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 鸡生蛋还是蛋生鸡的问题
- en: 'One of the hardest problems you may face with regard to datasets is a problem
    of a cold start (aka chicken-or-egg problem): when you need data to build a system
    but the data is not available until the system is launched. What a terrible loop
    to get stuck in!'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 你在数据集方面可能遇到的最困难的问题之一是一个冷启动问题（也称为鸡生蛋还是蛋生鸡的问题）：当你需要数据来构建系统，但数据直到系统启动后才可用。陷入这样一个可怕的循环是多么可怕啊！
- en: It is often a problem for startups or companies trying to launch products in
    new markets or verticals. And, as often happens in the ML world, the go-to solution
    is approximation. Since we don’t have data perfectly matching our problem, we
    need to find something as close as possible. What data will be close depends on
    the problem, so let’s look at some examples.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常是初创公司或试图在新的市场或垂直领域推出产品的公司遇到的问题。在机器学习世界中，常见的解决方案是近似。由于我们没有与我们的问题完美匹配的数据，我们需要找到尽可能接近的数据。哪些数据会接近取决于问题，让我们看看一些例子。
- en: For this example, let’s imagine a company focused on employee safety that builds
    products monitoring how workers follow safety rules in various environments. The
    new product should check if hard hats were worn on factory grounds. When the product
    is available, there will be a lot of data coming from customers who agree to share
    it, but before that, customers’ cameras are not available for the company.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，让我们想象一家专注于员工安全的公司，该公司生产的产品用于监控工人在各种环境中的安全规则遵守情况。新产品应检查工厂地面上是否佩戴了安全帽。当产品可用时，将会有大量来自同意分享数据的客户的数据，但在那之前，公司的客户摄像头是不可用的。
- en: So how do we approach the problem?
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何解决这个问题呢？
- en: '*Approach 1*—Let’s use the computer graphics world. We take a stack of 3D human
    models and render them on the factory backgrounds, both wearing hats and not.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*方法 1*—让我们使用计算机图形世界。我们取一叠3D人体模型，并在工厂背景上渲染它们，有的戴帽子，有的不戴。'
- en: '*Approach 2*—Let’s use public sources. We look for relevant images in public
    sources (from Google images to photo banks) and scrape or buy them.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*方法 2*—让我们使用公共来源。我们在公共来源（从谷歌图片到图片库）中寻找相关的图片，并抓取或购买它们。'
- en: '*Approach 2.5* *(hybrid)*—Let’s do a crossover: we take some images of people
    from the factory and draw/render hats on some of them.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*方法 2.5* *(混合)*—让我们进行交叉：我们从工厂中取一些人的照片，并在其中一些照片上绘制/渲染帽子。'
- en: '*Approach 3*—Let’s try acting! Your team buys hard hats, goes to an abandoned
    factory, and runs a photo session with hats on.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*方法 3*—让我们尝试表演！你的团队购买安全帽，去一个废弃的工厂，并戴着帽子进行一次拍照。'
- en: '*Approach 4*—Let’s get lazy. We find a public dataset with people wearing very
    similar hats but outdoors, on a construction site—not too close but still something.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*方法 4*—让我们偷懒。我们找到一个公共数据集，其中人们戴着非常相似的帽子，但户外，在建筑工地上——不太近，但仍然有。'
- en: '*Approach 5*—Let’s build a very naive baseline with no actual ML under the
    hood and suggest it to the customer to break the loop. An example of such a baseline
    could be finding human heads using an existing face detector and then cropping
    those faces and adding a simple heuristic trying to localize a hat (e.g., a bright
    enclosed blob).'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*方法 5*—让我们构建一个非常简单的基础线，没有实际机器学习在幕后，并将其建议给客户以打破循环。这样一个基础线的例子可能是使用现有的面部检测器找到人头，然后裁剪这些面部，并添加一个简单的启发式方法尝试定位帽子（例如，一个明亮的封闭块）。'
- en: '*Approach 6*—Instead of using a naive baseline, we integrate a solution provided
    by a large vendor if it is available. This option works better if vendors’ labels
    can be reused from the legal perspective. However, proxying a vendor’s output
    can provide you with initial unlabeled data that has its own value.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*方法 6*—如果可用，我们将集成一个大型供应商提供的解决方案。如果从法律角度来看，可以重用供应商的标签，这个选项将工作得更好。然而，代理供应商的输出可以为你提供具有自身价值的初始未标记数据。'
- en: 'These examples may not always be reasonable and applicable, but they represent
    several ways of solving this problem:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子可能并不总是合理和适用的，但它们代表了解决这个问题的几种方法：
- en: Generating synthetic data
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成合成数据
- en: Using available data from similar situations
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用类似情况下的可用数据
- en: Creating data manually
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动创建数据
- en: Taking data from a similar problem and trying to adjust it
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从类似问题中获取数据并尝试调整
- en: Use a dummy baseline model or third party to bootstrap
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用虚拟基线模型或第三方进行启动
- en: 'We should mention that not every scenario is applicable to every problem. You
    obviously won’t build a medical system for lung cancer detection using images
    of brain scans, and a naive baseline as a medical advisor is no good at all. But
    training on scans from other hospitals using the same equipment may be a good
    idea to consider; while every scanner will be calibrated slightly differently,
    together they can provide some generalization (a model trained on data from hospitals
    A, B, and C is likely to be useful for hospital D). Scraping other websites is
    rarely greenlighted in a respectful public company, although it is a popular technique
    among small startups. Synthetic images obtained by a straightforward rendering
    pipeline are usually not the way to go because they’re not realistic enough (imperfect
    lighting, shadows, etc.). But with some secret sauce, you can make them realistic.
    In the paper “MobilePose: Real-Time Pose Estimation for Unseen Objects with Weak
    Shape Supervision” (Hou et al., 2020, [https://arxiv.org/abs/2003.03522](https://arxiv.org/abs/2003.03522)),
    researchers rendered objects on top of augmented reality recordings (with accurately
    estimated lights) to make images way more realistic and thus valuable for a model.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该指出，并非每个场景都适用于每个问题。显然，你不会用脑部扫描的图像来构建用于肺癌检测的医疗系统，而一个天真的基线作为医疗顾问是完全不合适的。但是，使用相同设备从其他医院获取扫描数据可能是一个值得考虑的好主意；虽然每个扫描仪的校准可能略有不同，但它们一起可以提供一些泛化（在A、B和C医院的数据库上训练的模型可能对D医院也很有用）。在尊重的公众公司中，很少会批准抓取其他网站，尽管这在小型初创公司中是一种流行的技术。通过简单的渲染管道获得的合成图像通常不是最佳选择，因为它们不够真实（不完美的光照、阴影等）。但是，通过一些秘密配方，你可以使它们变得逼真。在论文“MobilePose：具有弱形状监督的未见物体实时姿态估计”（Hou等人，2020年，[https://arxiv.org/abs/2003.03522](https://arxiv.org/abs/2003.03522)）中，研究人员在增强现实记录上渲染了物体（使用准确估计的光照），从而使图像更加逼真，因此对模型更有价值。
- en: In every scenario, we need to keep in mind that data is not a representative
    sample of the real distribution we’re going to work with when the system goes
    live. This means validation results should be taken with a grain of salt, and
    replacing such proxy datasets with more realistic ones is a top priority. Let’s
    talk about it when discussing the properties of a healthy data pipeline.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情况下，我们都需要记住，当系统上线时，数据并不是我们所要处理的实际分布的代表样本。这意味着验证结果应该持保留态度，用更现实的样本替换这样的代理数据集是首要任务。在讨论健康数据管道的特性时，我们再详细讨论这个问题。
- en: 6.6 Properties of a healthy data pipeline
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.6 健康数据管道的特性
- en: 'Data gathering and preprocessing need to have three important properties:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 数据收集和预处理需要具备三个重要特性：
- en: Reproducibility
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可重现性
- en: Consistency
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一致性
- en: Availability
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用性
- en: Let’s review the properties one by one.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一回顾这些特性。
- en: Reproducibility means you should be able to create a dataset from scratch if
    needed. There should be no golden data file in your storage that is crafted by
    a wise engineer with a bit of dark magic. But there must be a software solution
    (a simple script, more advanced pipeline, or big system) that allows you to create
    such files again using the same origin as used before. This solution needs to
    be documented and tested well enough so every project collaborator is able to
    run it when needed.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 可重现性意味着如果需要，你应该能够从头开始创建数据集。在你的存储中不应该有由一位聪明的工程师用一点黑暗魔法制作出来的“金数据文件”。但必须有一个软件解决方案（一个简单的脚本、更高级的管道或大型系统），允许你使用与之前相同的来源重新创建这样的文件。这个解决方案需要得到良好的文档和测试，以便每个项目合作者都能在需要时运行它。
- en: The reasoning behind reproducibility is similar to the infrastructure-as-code
    paradigm popular in the DevOps community. Yes, it may seem easier to create an
    initial dataset manually in the first place, but it’s not scalable in the long
    run and is very error-prone. Next time—when a new batch of data is going to be
    added—you may forget to run a preprocessing step, thus causing implicit data inconsistency,
    a hard-to-detect problem that will affect the whole system’s performance.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 可重现性的推理与DevOps社区中流行的基础设施即代码范式类似。是的，最初手动创建初始数据集可能看起来更容易，但从长远来看，它并不具有可扩展性，并且容易出错。下次——当准备添加新批次数据时——你可能会忘记运行预处理步骤，从而导致隐式数据不一致，这是一个难以检测的问题，会影响整个系统的性能。
- en: Consistency itself is the key. ML problems often have situations when labels
    are partially ill-defined, and defining a strict separating plane is impossible.
    In such cases, experts who do labeling tend to disagree with each other.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性本身是关键。机器学习问题通常存在标签部分不明确的情况，定义一个严格的分离平面是不可能的。在这种情况下，进行标签的专家往往意见不一致。
- en: 'A very demonstrative example from our background is related to the credit card
    transactions classification problem. Customers are expected to give each transaction
    a label describing the purpose of the expense. The initial label taxonomy contains
    “food and drink,” “bars and wineries,” and “coffee houses.” A rhetorical question
    arises: What label is more suitable for payment for a coffee and a sandwich ordered
    in a wine bar? A partial solution for this example would be to have some kind
    of protocol on how ties are to be broken and how blurry boundaries are resolved
    (e.g., “when a merchant type and purchased item type are conflicting, the first
    to be opened takes precedence”)—it greatly simplifies model training and especially
    validation.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 来自我们背景的一个非常典型的例子与信用卡交易分类问题相关。客户被期望为每一笔交易提供一个标签，描述支出的目的。初始的标签分类包括“食品和饮料”、“酒吧和酒庄”和“咖啡屋”。一个修辞问题出现了：在一家酒吧订购咖啡和三明治的支付，哪个标签更合适？这个例子的一个部分解决方案可能是制定某种协议来决定如何打破平局以及如何解决模糊的边界（例如，“当商家类型和购买物品类型冲突时，首先打开的优先”）——这极大地简化了模型训练，特别是验证。
- en: 'Consistency is not only about labels. All aspects of data should be consistent:
    what the data origin is, how data is preprocessed, what filtering is applied,
    etc. It is relatively easy when the system is being built within a small company
    and way more challenging for an international corporation, and some formal definitions
    can be helpful. Once you feel there is a chance of misreading a core term used
    in the data pipeline, it may be useful to add it to the design doc to make sure
    the whole team is on the same page.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性不仅关乎标签。数据的各个方面都应该是一致的：数据的来源是什么，数据是如何预处理的，应用了哪些过滤等。当系统在一个小型公司内部构建时相对容易，而对于跨国公司来说则更具挑战性，一些正式的定义可能会有所帮助。一旦你感觉有可能误解数据管道中使用的核心术语，将其添加到设计文档中可能是有用的，以确保整个团队在同一个页面上。
- en: One more aspect of consistency is how data is gathered at the system-building
    stage and what the system inputs are during system usage. It is a common problem,
    and in ML terms, it leads to distribution mismatch, a problem affecting model
    performance and fairness that is not easy to detect before the system goes live.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性的另一个方面是在系统构建阶段数据的收集方式和系统使用期间的系统输入。这是一个常见问题，在机器学习的术语中，它导致分布不匹配，这是一个影响模型性能和公平性的问题，在系统上线之前很难检测到。
- en: We recently mentioned the feedback loop phenomena—the model affects data origin
    and thus violates the consistency assumption. We also described situations when
    data is not available until the system launch, and this also may lead to a mismatch.
    Problems like this are still among the most challenging aspects of applied ML,
    and you can never ignore them while designing a system.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最近提到了反馈循环现象——模型影响数据来源，从而违反了一致性假设。我们也描述了数据在系统启动后才可用的情况，这也可能导致不匹配。这类问题仍然是应用机器学习中最具挑战性的方面之一，在设计系统时你永远不能忽视它们。
- en: Data consistency is always an open question, so you should care about it at
    every stage of ML system development, from initial drafts to long-term maintenance.
    We will get back to this aspect in chapter 14 when discussing monitoring and drift
    detection (there will even will be a short but rather cool campfire story from
    one of our friends).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 数据一致性始终是一个开放性问题，因此在机器学习系统开发的每个阶段，从最初的草稿到长期的维护，你都应该关注它。我们将在第14章讨论监控和漂移检测时回到这个方面（甚至会有一个来自我们朋友的一个简短但相当酷的篝火故事）。
- en: 'Last but not least is availability. This property is actually an umbrella for
    two ideas: availability for a system and availability for engineers. The first
    can also be named *reliability*; a system designer should be very critical of
    nonreliable data sources. Here is a good situation for a negative example—one
    where things can ultimately go wrong. Imagine a system that depends on a third-party
    API enriching your data stream, and everything was good until it wasn’t—a small
    company powering the API lost its key site reliability engineer and thus the ability
    to handle the infrastructure. If your system’s dependency on this data source
    is critical, their problems become your problems.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是可用性。这个属性实际上涵盖了两个想法：系统的可用性和工程师的可用性。第一个也可以称为*可靠性*；系统设计者应该对不可靠的数据源非常批判。这里是一个很好的负面例子——事情最终可能会出错。想象一下，一个依赖于第三方API丰富你的数据流的系统，一切都很顺利，直到它不再顺利——一个为API提供动力的公司失去了其关键站点可靠性工程师，因此无法处理基础设施。如果你的系统对这个数据源的依赖是关键的，他们的问题就变成了你的问题。
- en: Of course, it doesn’t mean using third-party APIs is not an option. As we mentioned
    earlier, using external solutions is often a good practice—and not only provided
    by giants like Amazon, Google, and Microsoft. But data availability is important,
    and these risks should be taken into account seriously. There are some services
    provided by vendors that can be less reliable (we can’t imagine a top-priority
    problem caused by the outage of an experimental visualization tool), but data
    sources are not one of them.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这并不意味着使用第三方API不是一个选择。正如我们之前提到的，使用外部解决方案通常是一种好做法——不仅由像亚马逊、谷歌和微软这样的巨头提供。但数据可用性很重要，这些风险应该被认真考虑。有些服务提供商提供的服务可能不太可靠（我们无法想象一个由实验性可视化工具故障引起的高优先级问题），但数据源不是其中之一。
- en: 'The same philosophy goes for internal systems (e.g., those controlled by your
    peers from other teams). There are different control gears for external and internal
    systems: for the former, service-level agreements are used to estimate risks,
    while internal systems exist only in mature organizations. At the same time, in
    smaller companies, it is easier to be aligned with a team maintaining the system
    and effectively reducing related risks.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的哲学也适用于内部系统（例如，由其他团队中的同事控制的系统）。外部和内部系统有不同的控制齿轮：对于前者，使用服务级别协议来评估风险，而内部系统仅存在于成熟组织中。同时，在较小的公司中，与维护系统的团队保持一致并有效降低相关风险更容易。
- en: It is also worth mentioning that problems related to data availability are not
    strictly software related. Even if all related systems are built and maintained
    properly, there may be problems of more sophisticated origins—for example, caused
    by information security and privacy rights (such as new legal regulations that
    affect using personal data or your key customer CEO’s decision that their data
    should not leave their infrastructure anymore).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，与数据可用性相关的问题并不严格属于软件相关。即使所有相关系统都得到适当的构建和维护，也可能存在更复杂起源的问题——例如，由信息安全和个人隐私权（例如影响使用个人数据的新法律法规或你的关键客户CEO的决定，即他们的数据不应再离开他们的基础设施）引起。
- en: 'Availability for engineers can be overestimated: you can hear something like
    “Come on, our engineers can fetch the data even if it’s not a simple operation—they
    are professionals and will handle the tech difficulties, if any.” Most likely
    it is true—we assume our readers and their colleagues are outstanding professionals,
    but you can’t ignore time constraints. Imagine that an engineer went for lunch
    with their buddy from another team; they chatted about work-related stuff, and
    after a cup of coffee, a new hypothesis related to the system of current interest
    popped up.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 工程师的可用性可能被高估：你可能会听到类似这样的话，“来吧，即使这不是一个简单的操作，我们的工程师也能获取数据——他们是专业人士，如果有的话，会处理技术难题。”很可能是真的——我们假设我们的读者和他们的同事都是杰出的专业人士，但你不能忽视时间限制。想象一下，一个工程师和他的另一个团队的伙伴一起去吃午餐；他们讨论了与工作相关的事情，喝了一杯咖啡后，一个与当前感兴趣的系统相关的新假设突然出现。
- en: If data is attainable easily, you can make an informative decision relatively
    quickly by pulling the dataset, aggregating some statistics, and maybe even running
    basic experiments. If the idea gets confirmed in the first naive approach, it
    can be prioritized higher, meaning more resources should be allocated. Who knows,
    maybe it is going to be a significant improvement in the future.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据容易获取，你可以通过拉取数据集、汇总一些统计数据，甚至运行基本实验，相对快速地做出信息丰富的决策。如果第一个简单的方法得到证实，它可以被优先考虑，这意味着应该分配更多资源。谁知道呢，这可能会在未来带来重大的改进。
- en: Otherwise, if pulling data and making data-driven decisions is time-consuming,
    the hypothesis is likely to be ignored (“Well, it might be interesting, but I
    have so many things to do and finding the data will take so long!”). To avoid
    these cases, we recommend dedicating a share of engineering efforts to building
    tools that will make datasets more available to people who should work with data.
    While it is somewhat applicable to any engineering productivity tools, return
    on investment is especially high when the tools improve data availability. As
    we said in the very beginning, it’s hard to overestimate the importance of quality
    data for ML systems, and thus smoothing interactions with the datasets is a good
    investment in the long run.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，如果拉取数据和做出数据驱动的决策耗时，那么假设很可能会被忽视（“嗯，这可能很有趣，但我有很多事情要做，找到数据会花费很长时间！”）。为了避免这些情况，我们建议将一部分工程努力投入到构建工具中，使数据集对应该使用数据的人更容易获取。虽然这适用于任何工程生产力工具，但当工具提高数据可用性时，投资回报率尤其高。正如我们一开始所说的，很难高估高质量数据对机器学习系统的重要性，因此平滑与数据集的交互是长期的良好投资。
- en: 'This is sufficient depth in the information on consistency and availability
    at this point in the book. We will return to these two properties of the pipeline
    in greater detail later in chapter 10\. Meanwhile, it’s time to move on to the
    practical part of the chapter: the design document.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的这个阶段，关于一致性和可用性的信息已经足够深入。我们将在第10章中更详细地讨论这两个管道属性。同时，现在是时候进入本章的实践部分：设计文档。
- en: '6.7 Design document: Dataset'
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.7 设计文档：数据集
- en: As we describe steps for the data problem, it organically leads us to additional
    questions to be answered in the design document. The following is a checklist
    of questions we suggest asking yourself at this point.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们描述数据问题的步骤时，它自然地引导我们回答设计文档中需要回答的额外问题。以下是我们建议在这个阶段问自己的问题清单。
- en: 'ETL:'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ETL：
- en: What are the data sources?
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据来源是什么？
- en: How should we represent and store the data for our system?
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该如何表示和存储我们系统的数据？
- en: 'Filtering:'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤：
- en: What are the criteria for good and bad data samples?
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 好的和差的数据样本的标准是什么？
- en: What corner cases can we expect? How do we handle them?
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以预期哪些边缘情况？我们如何处理它们？
- en: Do we filter data automatically or set up a process for manual verification?
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是自动过滤数据还是设置手动验证的过程？
- en: 'Feature engineering:'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程：
- en: How are the features computed?
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征是如何计算的？
- en: How are representations generated?
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表示是如何生成的？
- en: 'Labeling:'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签：
- en: What labels do we need?
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要哪些标签？
- en: What’s the label’s source?
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签的来源是什么？
- en: As often happens, answering these questions can spawn even more questions. Give
    yourself the freedom to think about those. Time dedicated to answering data-related
    questions always has an outstanding return on investment in ML systems design.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如同经常发生的那样，回答这些问题可能会产生更多问题。给自己自由去思考这些问题。在机器学习系统设计中，投入时间回答与数据相关的问题总是有出色的投资回报。
- en: 6.7.1 Dataset for Supermegaretail
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.7.1 超级大零售的数据集
- en: Now let’s go back to preparing the design document. This time we are preparing
    a section devoted to datasets for our imaginary companies. As usual, we’ll start
    with Supermegaretail.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到准备设计文档。这次我们正在准备一个专门针对我们虚构公司数据集的部分。像往常一样，我们将从超级大零售开始。
- en: 'Design document: Supermegaretail'
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设计文档：超级大零售
- en: III. Dataset
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III. 数据集
- en: The atomic object of the dataset is a bundle of (date, product, store), and
    the target variable we aim to predict is the number of units sold.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的原子对象是一组（日期、产品、商店），我们旨在预测的目标变量是销售的单位数。
- en: i. Data sources
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: i. 数据来源
- en: There are multiple sources of data we can utilize for our objective.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用多个数据来源来实现我们的目标。
- en: Inner sources
  id: totrans-183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内部来源
- en: '*Historical data on purchases (i.e., transaction history)* is collected from
    the chain of Supermegaretail stores and saved to a centralized database. It will
    be our *primary source* of truth: the number of sales, the amount of money spent,
    the discounts applied, transaction ID, and so on.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*购买历史数据（即，交易历史）*是从Supermegaretail连锁店收集的，并保存到集中式数据库中。它将成为我们的*主要真实来源*：销售数量、花费的金额、应用的折扣、交易ID等。'
- en: '*Stock history* is the second most important source of truth for our problem
    since it directly determines how many units of each product can be sold in each
    store. This source can help estimate how many products are available for sale
    at the beginning of each day and how many were expired and withdrawn from sale.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*库存历史*是我们问题的第二个重要真实来源，因为它直接决定了每个商店可以销售多少个产品单位。这个来源可以帮助估计每天开始时有多少产品可供销售，以及有多少已经过期并被撤出销售。'
- en: Metadata of *each product, store, and transaction*.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*每个产品、商店和交易的元数据*。'
- en: Calendar of planned *promo activities*. This is a significant factor affecting
    future sales that definitely needs to be taken into account.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计划的*促销活动*日历。这是一个影响未来销售的重要因素，肯定需要考虑。
- en: 'Outer sources: Manually gathered data'
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 外部来源：手动收集的数据
- en: '*Price monitoring*. Prices and other product info collected from our competitors.
    They are manually gathered daily from a subset of stores of different competitors.
    It could be done either by our in-house team or by a third party (outsourced).
    A hybrid approach also can take place. Each product should also contain a global
    product identifier (barcode) so we can easily match collected data with our product.
    Knowing aggregated competitors’ prices and their dynamics helps us understand
    what is happening in the market.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*价格监控*。从我们的竞争对手那里收集的价格和其他产品信息。他们每天从不同竞争对手的子集中手动收集。这可能是由我们内部团队或第三方（外包）完成的。也可以采用混合方法。每个产品也应包含一个全球产品标识符（条形码），这样我们就可以轻松地将收集到的数据与我们的产品匹配。了解汇总的竞争对手价格及其动态有助于我们了解市场正在发生什么。'
- en: 'Outer sources: Purchased data'
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 外部来源：购买数据
- en: '*Weather history and forecast* we buy from the meteorological service. Weather
    is an important factor directly affecting consumer behavior.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*气象服务购买的历史和预测天气*。天气是直接影响消费者行为的一个重要因素。'
- en: '*Customer traffic* estimation near our stores (from telecom providers).'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*顾客流量*估计（来自电信提供商）。'
- en: '*Global market indicators*.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*全球市场指标*。'
- en: Mobile app and website data (optional)
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 移动应用和网站数据（可选）
- en: Supermegaretail has a *delivery service* (even if it generates less than 5%
    of revenue). We will collect *additional data* about specific sales in a specific
    location. Sometimes this information can be a valuable predictor.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Supermegaretail有一个*配送服务*（即使它产生的收入不到5%）。我们将收集关于特定地点特定销售的*额外数据*。有时这些信息可以是一个有价值的预测因素。
- en: Also, mobile and web services collect implicit feedback about user activity,
    including views, clicks, or adding to the cart, which also can predict sales in
    physical stores.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，移动和网站服务收集关于用户活动的隐式反馈，包括查看、点击或添加到购物车，这些也可以预测实体店的销售额。
- en: ii. Data labeling
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ii. 数据标注
- en: Since we are dealing with a demand forecasting problem, we don’t need extra
    data labeling derived directly from the transaction history.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们处理的是一个需求预测问题，我们不需要直接从交易历史中派生的额外数据标注。
- en: iii. Available metadata
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: iii. 可用元数据
- en: 'We forecast demand based on the SKU per store level with three key elements:
    products, stores, and transactions.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据每个商店级别的SKU进行需求预测，有三个关键要素：产品、商店和交易。
- en: Products
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 产品
- en: Product ID and barcode.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品ID和条形码。
- en: Category codes of different levels (1, 2, 3). We can use a hierarchy of categories
    for a rule-based measurement of similarity between products. Also, other categorical
    info like brand, manufacturer, or pricing group.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同级别的类别代码（1，2，3）。我们可以使用类别层次结构来对产品之间的相似性进行基于规则的测量。此外，其他分类信息，如品牌、制造商或定价组。
- en: Shelf life, which determines how bad it is to overpredict sales for this product.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 储存期限，它决定了过度预测该产品销售的糟糕程度。
- en: Date when the product was added to the assortment matrix of the chain.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品被添加到连锁店组合矩阵中的日期。
- en: Dimensions and weight of the product.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品的尺寸和重量。
- en: Stores
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 商店
- en: Store ID.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 店铺ID。
- en: Location (coordinates), with the support of third-party sources—we can use it
    to add information about the weather, flow of people, and distance to critical
    points, as well as other related things like city, region, and associated logistics
    center.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置（坐标），在第三方来源的支持下——我们可以用它来添加有关天气、人流、到关键点的距离以及其他相关事项的信息，如城市、地区和相关的物流中心。
- en: The nearest competitors’ stores (with their IDs and distances).
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最接近的竞争对手的商店（包括它们的ID和距离）。
- en: The size of the store and its format. They determine which products and how
    many unique products will be in the assortment at this store.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 商店的大小及其格式。它们决定了在这个商店的品种中会有哪些产品和多少独特的商品。
- en: The dates when the store is open and closed.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 商店开闭的日期。
- en: Transactions
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 交易
- en: '*Timestamp*. This allows us to enrich the dataset with things like holidays.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*时间戳*。这使我们能够通过添加诸如假日等信息来丰富数据集。'
- en: '*Customer ID* (if a loyalty card is applied). Despite the fact that the final
    unit of the dataset is (product, store), a bundle of (customer, product) can be
    used in a separate data pipeline for calculating product embeddings via aggregating
    transactions to a user-item matrix and its factorization. The embeddings will
    contain patterns of purchasing behavior.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*客户ID*（如果使用了忠诚度卡）。尽管数据集的最终单元是（产品，商店），但可以捆绑（客户，产品）用于单独的数据管道，通过聚合交易到一个用户-项目矩阵及其分解来计算产品嵌入。嵌入将包含购买行为模式。'
- en: '*Store ID* and *product ID*.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*商店ID* 和 *产品ID*。'
- en: iv. Available history
  id: totrans-217
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: iv. 可用历史
- en: Demand forecast is nothing new for Supermegaretail. Critical ETL processes are
    already in place. Supermegaretail has been collecting data for more than 3 years.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Supermegaretail来说，需求预测并不新鲜。关键的ETL流程已经到位。Supermegaretail已经收集了超过3年的数据。
- en: This history is essential for our forecasting model to learn patterns, catching
    the seasonality of sales, estimating trends, etc. The same is applicable for products
    and stores metadata. Weather data (which we take from external sources) has been
    available for a period in the past as long as we need.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这个历史对于我们的预测模型学习模式、捕捉销售的季节性、估计趋势等至关重要。同样适用于产品和商店元数据。天气数据（我们从外部来源获取）在过去一段时间内一直可用，只要我们需要。
- en: Stocks history and promo activities have been gathered as well.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 股票历史和促销活动也已收集。
- en: Price monitoring data of competitors has been collected for the last 2 years.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 竞争对手的价格监控数据已经收集了两年。
- en: v. Data quality issues
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: v. 数据质量问题
- en: Transactions, stock, and promo data may contain missing or duplicated values,
    so additional filtering or preprocessing is required before aggregation.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 交易、库存和促销数据可能包含缺失或重复的值，因此在聚合之前需要额外的过滤或预处理。
- en: The external data we bought has already been cleaned and passed some quality
    control before coming to us. However, necessary checks need to be implemented.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们购买的外部数据在到达我们这里之前已经过清洗并通过了某些质量控制。然而，需要实施必要的检查。
- en: The competitors’ prices cover about 25% of SKU and have gaps.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 竞争对手的价格覆盖了大约25%的SKU，存在差距。
- en: vi. Final ETL pipeline
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: vi. 最终ETL管道
- en: 'The top-level scheme is as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 最高级方案如下：
- en: The transactions data is aggregated daily.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 交易数据是按日汇总的。
- en: Newly aggregated partition is added to the table of transaction aggregates.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 新汇总的分区被添加到交易汇总表的表中。
- en: (Optional) We rewrite not only the last day but the previous 2 to 3 days to
    fix possible corruptions in the data (duplicates, incomplete data, duplicated
    data, and so on).
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （可选）我们不仅重写了最后一天，还重写了前2到3天，以修复数据中可能存在的损坏（重复、不完整数据、重复数据等）。
- en: We join other sources of internal/external data based on data, product ID, or
    store ID.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们根据数据、产品ID或商店ID将其他内部/外部数据源合并。
- en: Finally, we calculate features based on the joined dataset.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们根据合并后的数据集计算特征。
- en: Optionally, we can add a data pipeline for product embeddings, as described
    in section III, if needed.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，我们可以添加一个产品嵌入的数据管道，如第III节所述。
- en: 6.7.2 Dataset for PhotoStock Inc.
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.7.2 PhotoStock Inc.的数据集
- en: Let’s now switch to the PhotoStock Inc. design document.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们切换到PhotoStock Inc.的设计文档。
- en: 'Design document: PhotoStock Inc.'
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设计文档：PhotoStock Inc.
- en: III. Dataset
  id: totrans-237
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III. 数据集
- en: i. Dataset and sources
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: i. 数据集和来源
- en: One potential data source that can be used to gather information for the PhotoStock
    Inc. search engine is the data associated with each photo in the stock library.
    This data may include information such as tags, labels, and descriptions associated
    with the photos, which can provide valuable context about the content of the photos.
    This dataset should also contain URLs of actual photos and URLs of thumbnails
    we used. We suggest naming this dataset “description dataset.”
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 可以用于为 PhotoStock Inc. 搜索引擎收集信息的潜在数据源之一是库存库中每张照片相关的数据。这些数据可能包括与照片相关的标签、标签和描述等信息，这些信息可以为照片内容提供有价值的背景。此数据集还应包含实际照片的
    URL 和我们使用的缩略图的 URL。我们建议将此数据集命名为“描述数据集”。
- en: 'Another potential data source is the search queries that users have submitted
    to the PhotoStock Inc. platform. These queries can provide insight into the types
    of photos that users are looking for and can be used to help guide the development
    of the ML models. When combined with user clicks, they provide a strong signal
    of relevancy and user interest. An extension of this dataset may contain further
    information about sessions related to these clicks: how much time did a user spend
    on the photo page, and did they purchase it afterward? We suggest naming this
    dataset “clicks dataset.”'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个潜在的数据源是用户提交给 PhotoStock Inc. 平台上的搜索查询。这些查询可以提供关于用户正在寻找的图片类型的洞察，并可用于帮助指导机器学习模型的发展。当与用户点击相结合时，它们提供了关于相关性和用户兴趣的强烈信号。此数据集的扩展可能包含有关这些点击相关的会话的进一步信息：用户在照片页面上花费了多长时间，以及他们之后是否购买了该照片？我们建议将此数据集命名为“点击数据集”。
- en: Additionally, we can hire labelers to manually assign relevancy scores to pairs
    of queries and images. The labelers will be given a set of search queries and
    a selection of images from the PhotoStock Inc. library and will be asked to assign
    a relevancy score to each query–image pair based on how closely the image matches
    the content of the query. Initially, query–image pairs should be selected randomly
    from the pool of past user queries and available photos in the stock library;
    however, it could be improved later to involve some active learning approaches
    and getting signals for the pairs where the model is less confident or tends to
    have more errors. We suggest naming this dataset “labels dataset.”
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以雇佣标签员手动为查询和图像对分配相关性评分。标签员将获得一组搜索查询和 PhotoStock Inc. 库中的一组图像选择，并要求根据图像与查询内容匹配的紧密程度为每个查询-图像对分配相关性评分。最初，查询-图像对应从过去用户查询和库存库中可用的照片池中随机选择；然而，以后可以改进，涉及一些主动学习方法，并为模型不太自信或容易出错的对提供信号。我们建议将此数据集命名为“标签数据集”。
- en: We could have considered some public text/image datasets (e.g., COCO—Microsoft
    Common Objects in Context) for model bootstrapping. However, given that there
    is a solid history of user interactions, it should be fine to use our own data,
    while the public datasets will be only used indirectly—we can start with models
    pretrained on such datasets earlier.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以考虑一些公共文本/图像数据集（例如，COCO—Microsoft Common Objects in Context）用于模型启动。然而，鉴于有用户交互的坚实历史，使用我们自己的数据应该没问题，而公共数据集将仅间接使用——我们可以从在这些数据集上预训练的模型开始。
- en: Description and clicks datasets are generated organically by the main flow of
    the PhotoStock Inc. business. So it doesn’t require too much attention from us
    right now, except for building proper ETL process and storage. The labels dataset
    needs to have its own budget given that it requires hiring a labelers team or
    using a third-party service.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 描述和点击数据集是通过 PhotoStock Inc. 业务的主体流程自然生成的。因此，目前我们不需要过多关注，除了构建适当的 ETL 流程和存储。由于标签数据集需要雇佣标签团队或使用第三方服务，因此需要为其分配专门的预算。
- en: ii. Metadata, filtering, and subsampling
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ii. 元数据、过滤和子采样
- en: The description dataset is expected to be clean enough given that photo descriptions
    are moderated by the content quality team, as their quality is already a core
    part of our product. The clicks dataset is expected to be noisy because of different
    patterns in user behavior and the existence of scraping bots; we need to suggest
    heuristics to handle these problems. The extended version, with data on purchases,
    can be less noisy but also way smaller considering the conversion rate from click
    to purchase. Given these aspects, we expect the click+purchase and labels datasets
    to be more valuable for validation, while description and clicks without purchases
    are potentially more noisy and thus more suitable for training.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 由于照片描述由内容质量团队审核，因此预期描述数据集将足够干净，因为他们的质量已经是我们的产品核心部分。点击数据集可能会因为用户行为的不同模式和爬虫的存在而嘈杂；我们需要提出启发式方法来处理这些问题。包含购买数据的扩展版本可能不太嘈杂，但考虑到点击到购买的转换率，它也可能小得多。考虑到这些方面，我们预计点击+购买和标签数据集对于验证更有价值，而描述和没有购买的点击数据可能更嘈杂，因此更适合用于训练。
- en: To address the quality problems with filtering, we should take care of metadata.
    The description dataset should be annotated with seller information and change
    dates; the clicks dataset should be annotated with user information, clicked item
    seller information, and additional search session information. At this point,
    there is no need to aggregate all the information about user and seller information.
    It may be part of further feature engineering steps, but at the very least we
    need to be sure to store `relevant user_id`, `seller_id`, `session_id`, and `purchase_id`
    to keep the ability to join this data for later use.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决过滤过程中出现的质量问题，我们应该注意元数据。描述数据集应该标注卖家信息和变更日期；点击数据集应该标注用户信息、点击项卖家信息和额外的搜索会话信息。在这个阶段，没有必要聚合所有关于用户和卖家信息。这可能是进一步的特征工程步骤的一部分，但至少我们需要确保存储`相关用户_id`、`卖家_id`、`会话_id`和`购买_id`，以保持将来使用这些数据的能力。
- en: 'Given the volume of searches and purchases at PhotoStock Inc., it’s not likely
    we need additional subsampling for purely engineering needs; full datasets can
    be processed. However, we may need to run subsampling to adjust class balances:
    there are way more items clicked than purchased, shown at the search engine result
    page than clicked, etc.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到PhotoStock Inc.的搜索和购买量，我们可能不需要为了纯粹的技术需求进行额外的子采样；可以处理完整的数据集。然而，我们可能需要运行子采样来调整类别平衡：点击的物品比购买的物品多，搜索结果页面显示的物品比点击的物品多等。
- en: iii. ETL and data preparation
  id: totrans-248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: iii. ETL和数据准备
- en: Training data should be fetched on a regular (daily for now) basis in a batch
    manner. We suggest using the Flyte framework to orchestrate the jobs because it’s
    already a framework of choice for other batch jobs in PhotoStock Inc.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据应该定期（目前为每日）以批量方式获取。我们建议使用Flyte框架来编排任务，因为它已经是PhotoStock Inc.中其他批量任务的首选框架。
- en: No fancy stuff is required here—just gathering the data from production databases
    and storing it separately in the form of Parquet files for a simpler read should
    be enough. We don’t expect any need for sophisticated preprocessing for now.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这里不需要任何花哨的东西——只需从生产数据库中收集数据，并以Parquet文件的形式单独存储，以便于读取即可。目前我们预计不需要进行复杂的预处理。
- en: iv. Labeling
  id: totrans-251
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: iv. 标注
- en: It’s not clear what platform we should prefer for human labeling; it’s also
    an open question whether we should use an in-house team (e.g., our customer support
    and moderators teams) or hire a third-party service. This question needs to be
    addressed together with these team managers. Also, we should get cost estimates
    from some third-party services to compare the costs of internal labeling with
    them.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不清楚应该优先考虑哪个平台进行人工标注；是否应该使用内部团队（例如，我们的客户支持和审核团队）或雇佣第三方服务也是一个悬而未决的问题。这个问题需要与这些团队经理一起解决。此外，我们还应该从一些第三方服务中获得成本估算，以比较内部标注与他们的成本。
- en: 'We need to label pairs (query, image) and divide them among three classes:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要标注（查询，图像）对，并将它们分为三个类别：
- en: Relevant
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关
- en: Irrelevant
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不相关
- en: Can’t answer
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法回答
- en: We should be ready to split “Relevant” into “Very relevant” and “Somewhat relevant”
    for higher granularity; the same goes for “Irrelevant.” As for “Can’t answer,”
    we need to require labelers to type a reason why they can’t label it—first iterations
    of labeling can provide us with newer insights on the dataset.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该准备好将“相关”细分为“非常相关”和“有些相关”，以获得更高的粒度；对于“不相关”也是如此。至于“无法回答”，我们需要要求标注者输入他们无法标注的原因——标注的早期迭代可以为我们提供关于数据集的新见解。
- en: Summary
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Don’t limit yourself to just one data source. Determine if you have enough internal
    sources or if you need to look outside the ecosystem of your business and expand
    your search range.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要仅限于使用单一的数据源。确定你是否拥有足够内部来源，或者是否需要超出你业务生态系统的范围来扩展你的搜索范围。
- en: Access to unique datasets can give you a significant competitive advantage.
    In their turn, datasets that are easier to acquire can bring great value if “cooked”
    properly.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问独特的数据集可以给你带来显著的竞争优势。反过来，如果处理得当，易于获取的数据集也能带来巨大的价值。
- en: When working on datasets, don’t neglect metadata, which is crucial for data
    flows and guarantees its consistency.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在处理数据集时，不要忽视元数据，这对于数据流至关重要，并保证了其一致性。
- en: Keep in mind that there is no strict sequence of techniques when preparing a
    dataset. The final order of operations will depend on the business goals of your
    system, the domain in which you operate, and other factors.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请记住，在准备数据集时没有严格的技巧顺序。操作的最后顺序将取决于你系统的业务目标、你所在的领域和其他因素。
- en: Avoid situations where a dataset is populated with samples similar to those
    already present. Instead, fill the dataset with samples where the model did not
    work correctly.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免将数据集填充与已存在的样本相似的样本。相反，应该填充那些模型未能正确工作的样本。
- en: Make sure you have data that you can feed to the system before running it.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在运行系统之前，确保你有可以喂给系统的数据。
- en: 'Remember that the data pipeline must meet three criteria: reproducibility,
    consistency, and availability.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记住，数据管道必须满足三个标准：可重复性、一致性和可用性。
