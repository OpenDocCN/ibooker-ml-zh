- en: Chapter 4\. Anomaly Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 3](ch03.html#Chapter_3), we introduced the core dimensionality reduction
    algorithms and explored their ability to capture the most salient information
    in the MNIST digits database in significantly fewer dimensions than the original
    784 dimensions. Even in just two dimensions, the algorithms meaningfully separated
    the digits, without using labels. This is the power of unsupervised learning algorithms—they
    can learn the underlying structure of data and help discover hidden patterns in
    the absence of labels.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s build an applied machine learning solution using these dimensionality
    reduction methods. We will turn to the problem we introduced in [Chapter 2](ch02.html#Chapter_2)
    and build a credit card fraud detection system without using labels.
  prefs: []
  type: TYPE_NORMAL
- en: In the real world, fraud often goes undiscovered, and only the fraud that is
    caught provides any labels for the datasets. Moreover, fraud patterns change over
    time, so supervised systems that are built using fraud labels—like the one we
    built in [Chapter 2](ch02.html#Chapter_2)—become stale, capturing historical patterns
    of fraud but failing to adapt to newly emerging patterns.
  prefs: []
  type: TYPE_NORMAL
- en: For these reasons (the lack of sufficient labels and the need to adapt to newly
    emerging patterns of fraud as quickly as possible), unsupervised learning fraud
    detection systems are in vogue.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will build such a solution using some of the dimensionality
    reduction algorithms we explored in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Credit Card Fraud Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s revisit the credit card transactions problem from [Chapter 2](ch02.html#Chapter_2).
  prefs: []
  type: TYPE_NORMAL
- en: Prepare the Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like we did in [Chapter 2](ch02.html#Chapter_2), let’s load the credit card
    transactions dataset, generate the features matrix and labels array, and split
    the data into training and test sets. We will not use the labels to perform anomaly
    detection, but we will use the labels to help evaluate the fraud detection systems
    we build.
  prefs: []
  type: TYPE_NORMAL
- en: As a reminder, we have 284,807 credit card transactions in total, of which 492
    are fraudulent, with a positive (fraud) label of one. The rest are normal transactions,
    with a negative (not fraud) label of zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have 30 features to use for anomaly detection—time, amount, and 28 principal
    components. And, we will split the dataset into a training set (with 190,820 transactions
    and 330 cases of fraud) and a test set (with the remaining 93,987 transactions
    and 162 cases of fraud):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Define Anomaly Score Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we need to define a function that calculates how anomalous each transaction
    is. The more anomalous the transaction is, the more likely it is to be fraudulent,
    assuming that fraud is rare and looks somewhat different than the majority of
    transactions, which are normal.
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed in the previous chapter, dimensionality reduction algorithms
    reduce the dimensionality of data while attempting to minimize the reconstruction
    error. In other words, these algorithms try to capture the most salient information
    of the original features in such a way that they can reconstruct the original
    feature set from the reduced feature set as well as possible. However, these dimensionality
    reduction algorithms cannot capture all the information of the original features
    as they move to a lower dimensional space; therefore, there will be some error
    as these algorithms reconstruct the reduced feature set back to the original number
    of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of our credit card transactions dataset, the algorithms will
    have the largest reconstruction error on those transactions that are hardest to
    model—in other words, those that occur the least often and are the most anomalous.
    Since fraud is rare and presumably different than normal transactions, the fraudulent
    transactions should exhibit the largest reconstruction error. So let’s define
    the anomaly score as the reconstruction error. The reconstruction error for each
    transaction is the sum of the squared differences between the original feature
    matrix and the reconstructed matrix using the dimensionality reduction algorithm.
    We will scale the sum of the squared differences by the max-min range of the sum
    of the squared differences for the entire dataset, so that all the reconstruction
    errors are within a zero to one range.
  prefs: []
  type: TYPE_NORMAL
- en: The transactions that have the largest sum of squared differences will have
    an error close to one, while those that have the smallest sum of squared differences
    will have an error close to zero.
  prefs: []
  type: TYPE_NORMAL
- en: This should be familiar. Like the supervised fraud detection solution we built
    in [Chapter 2](ch02.html#Chapter_2), the dimensionality reduction algorithm will
    effectively assign each transaction an anomaly score between zero and one. Zero
    is normal and one is anomalous (and most likely to be fraudulent).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Define Evaluation Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although we will not use the fraud labels to build the unsupervised fraud detection
    solutions, we will use the labels to evaluate the unsupervised solutions we develop.
    The labels will help us understand just how well these solutions are at catching
    known patterns of fraud.
  prefs: []
  type: TYPE_NORMAL
- en: As we did in [Chapter 2](ch02.html#Chapter_2), we will use the precision-recall
    curve, the average precision, and the auROC as our evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the function that will plot these results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The fraud labels and the evaluation metrics will help us assess just how good
    the unsupervised fraud detection systems are at catching known patterns of fraud—fraud
    that we have caught in the past and have labels for.
  prefs: []
  type: TYPE_NORMAL
- en: However, we will not be able to assess how good the unsupervised fraud detection
    systems are at catching unknown patterns of fraud. In other words, there may be
    fraud in the dataset that is incorrectly labeled as not fraud because the financial
    company never discovered it.
  prefs: []
  type: TYPE_NORMAL
- en: As you may see already, unsupervised learning systems are much harder to evaluate
    than supervised learning systems. Often, unsupervised learning systems are judged
    by their ability to catch known patterns of fraud. This is an incomplete assessment;
    a better evaluation metric would be to assess them on their ability to identify
    unknown patterns of fraud, both in the past and in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Since we cannot go back to the financial company and have them evaluate any
    unknown patterns of fraud we identify, we will have to evaluate these unsupervised
    systems solely based on how well they detect the known patterns of fraud. It’s
    important to be mindful of this limitation as we proceed in evaluating the results.
  prefs: []
  type: TYPE_NORMAL
- en: Define Plotting Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will reuse the scatterplot function from [Chapter 3](ch03.html#Chapter_3)
    to display the separation of points the dimensionality reduction algorithm achieves
    in just the first two dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Normal PCA Anomaly Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 3](ch03.html#Chapter_3), we demonstrated how PCA captured the majority
    of information in the MNIST digits dataset in just a few principal components,
    far fewer in number than the original dimensions. In fact, with just two dimensions,
    it was possible to visually separate the images into distinct groups based on
    the digits they displayed.
  prefs: []
  type: TYPE_NORMAL
- en: Building on this concept, we will now use PCA to learn the underlying structure
    of the credit card transactions dataset. Once we learn this structure, we will
    use the learned model to reconstruct the credit card transactions and then calculate
    how different the reconstructed transactions are from the original transactions.
    Those transactions that PCA does the poorest job of reconstructing are the most
    anomalous (and most likely to be fraudulent).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Remember that the features in the credit card transactions dataset we have are
    already the output of PCA—this is what we were given by the financial company.
    However, there is nothing unusual about performing PCA for anomaly detection on
    an already dimensionality-reduced dataset. We just treat the original principal
    components that we are given as the original features.
  prefs: []
  type: TYPE_NORMAL
- en: Going forward, we will refer to the original principal components that we were
    given as the original features. Any future mention of principal components will
    refer to the principal components from the PCA process rather than the original
    features we were given.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by developing a deeper understanding of how PCA—and dimensionality
    reduction in general—helps perform anomaly detection. As we’ve defined it, anomaly
    detection relies on reconstruction error. We want the reconstruction error for
    rare transactions—the ones that are most likely to be fraudulent—to be as high
    as possible and the reconstruction error for the rest to be as low as possible.
  prefs: []
  type: TYPE_NORMAL
- en: For PCA, the reconstruction error will depend largely on the number of principal
    components we keep and use to reconstruct the original transactions. The more
    principal components we keep, the better PCA will be at learning the underlying
    structure of the original transactions.
  prefs: []
  type: TYPE_NORMAL
- en: However, there is a balance. If we keep too many principal components, PCA may
    too easily reconstruct the original transactions, so much so that the reconstruction
    error will be minimal for all of the transactions. If we keep too few principal
    components, PCA may not be able to reconstruct any of the original transactions
    well enough—not even the normal, nonfraudulent transactions.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s search for the right number of principal components to keep to build a
    good fraud detection system.
  prefs: []
  type: TYPE_NORMAL
- en: PCA Components Equal Number of Original Dimensions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, let’s think about something. If we use PCA to generate the same number
    of principal components as the number of original features, will we be able to
    perform anomaly detection?
  prefs: []
  type: TYPE_NORMAL
- en: If you think through this, the answer should be obvious. Recall our PCA example
    from the previous chapter for the MNIST digits dataset.
  prefs: []
  type: TYPE_NORMAL
- en: When the number of principal components equals the number of original dimensions,
    PCA captures nearly 100% of the variance/information in the data as it generates
    the principal components. Therefore, when PCA reconstructs the transactions from
    the principal components, it will have too little reconstruction error for all
    the transactions, fraudulent or otherwise. We will not be able to differentiate
    between rare transactions and normal ones—in other words, anomaly detection will
    be poor.
  prefs: []
  type: TYPE_NORMAL
- en: To highlight this, let’s apply PCA to generate the same number of principal
    components as the number of original features (30 for our credit card transactions
    dataset). This is accomplished with the `fit_transform` function from Scikit-Learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'To reconstruct the original transactions from the principal components we generate,
    we will use the `inverse_transform` function from Scikit-Learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 4-1](#separation_of_observations_using_normal_pca_and_30_principal_components)
    shows the plot of the separation of transactions using the first two principal
    components of PCA.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Separation of Obversations Using Normal PCA and 30 Principal Components](assets/hulp_0401.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-1\. Separation of observations using normal PCA and 30 principal components
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s calculate the precision-recall curve and the ROC curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: With an average precision of 0.11, this is a poor fraud detection solution (see
    [Figure 4-2](#results_using_30_principal_components)). It catches very little
    of the fraud.
  prefs: []
  type: TYPE_NORMAL
- en: '![Results Using Normal PCA and 30 Principal Components](assets/hulp_0402.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-2\. Results using 30 principal components
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Search for the Optimal Number of Principal Components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let’s perform a few experiments by reducing the number of principal components
    PCA generates and evaluate the fraud detection results. We need the PCA-based
    fraud detection solution to have enough error on the rare cases that it can meaningfully
    separate fraud cases from the normal ones. But the error cannot be so low or so
    high for all the transactions that the rare and normal transactions are virtually
    indistinguishable.
  prefs: []
  type: TYPE_NORMAL
- en: After some experimentation, which you can perform using the [GitHub code](http://bit.ly/2Gd4v7e),
    we find that 27 principal components is the optimal number for this credit card
    transactions dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-3](#separation_of_observations_using_normal_pca_and_27_principal_components)
    shows the plot of the separation of transactions using the first two principal
    components of PCA.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Separation of Obversations Using Normal PCA and 27 Principal Components](assets/hulp_0403.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-3\. Separation of observations using normal PCA and 27 principal components
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 4-4](#results_using_normal_pca_and_27_principal_components) shows the
    precision-recall curve, average precision, and auROC curve.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Results Using Normal PCA and 27 Principal Components](assets/hulp_0404.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-4\. Results using normal PCA and 27 principal components
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, we are able to catch 80% of the fraud with 75% precision. This
    is very impressive considering that we did not use any labels. To make these results
    more tangible, consider that there are 190,820 transactions in the training set
    and only 330 are fraudulent.
  prefs: []
  type: TYPE_NORMAL
- en: Using PCA, we calculated the reconstruction error for each of these 190,820
    transactions. If we sort these transactions by highest reconstruction error (also
    referred to as anomaly score) in descending order and extract the top 350 transactions
    from the list, we can see that 264 of these transactions are fraudulent.
  prefs: []
  type: TYPE_NORMAL
- en: That is a precision of 75%. Moreover, the 264 transactions we caught from the
    350 we picked represent 80% of the total fraud in the training set (264 out of
    330 fraudulent cases). And, remember that we accomplished this without using labels.
    This is a truly unsupervised fraud detection solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to highlight this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code summarizes the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Although this is a pretty good solution already, let’s try to develop fraud
    detection systems using some of the other dimensionality reduction methods.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse PCA Anomaly Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s try to use sparse PCA to design a fraud detection solution. Recall that
    sparse PCA is similar to normal PCA but delivers a less dense version; in other
    words, sparse PCA provides a sparse representation of the principal components.
  prefs: []
  type: TYPE_NORMAL
- en: We still need to specify the number of principal components we desire, but we
    must also set the alpha parameter, which controls the degree of sparsity. We will
    experiment with different values for the principal components and the alpha parameter
    as we search for the optimal sparse PCA fraud detection solution.
  prefs: []
  type: TYPE_NORMAL
- en: Note that for normal PCA Scikit-Learn used a `fit_transform` function to generate
    the principal components and an `inverse_transform` function to reconstruct the
    original dimensions from the principal components. Using these two functions,
    we were able to calculate the reconstruction error between the original feature
    set and the reconstructed feature set derived from the PCA.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, Scikit-Learn does not provide an `inverse_transform` function
    for sparse PCA. Therefore, we must reconstruct the original dimensions after we
    perform sparse PCA ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin by generating the sparse PCA matrix with 27 principal components
    and the default alpha parameter of 0.0001:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 4-5](#separation_of_observations_using_sparse_pca_and_27_principal_components)
    shows the scatterplot for sparse PCA.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Separation of Obversations Using Sparse PCA and 27 Principal Components](assets/hulp_0405.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-5\. Separation of observations using sparse PCA and 27 principal components
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now let’s generate the original dimensions from the sparse PCA matrix by simple
    matrix multiplication of the sparse PCA matrix (with 190,820 samples and 27 dimensions)
    and the sparse PCA components (a 27 x 30 matrix), provided by Scikit-Learn library.
    This creates a matrix that is the original size (a 190,820 x 30 matrix). We also
    need to add the mean of each original feature to this new matrix, but then we
    are done.
  prefs: []
  type: TYPE_NORMAL
- en: 'From this newly derived inverse matrix, we can calculate the reconstruction
    errors (anomaly scores) as we did with normal PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s generate the precision-recall curve and ROC curve.
  prefs: []
  type: TYPE_NORMAL
- en: '![Results Using Sparse PCA and 27 Principal Components](assets/hulp_0406.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-6\. Results using sparse PCA and 27 principal components
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As [Figure 4-6](#results_using_sparse_pca_and_27_principal_components) shows,
    the results are identical to those of normal PCA. This is expected since normal
    and sparse PCA are very similar—the latter is just a sparse representaion of the
    former.
  prefs: []
  type: TYPE_NORMAL
- en: Using the [GitHub code](http://bit.ly/2Gd4v7e), you can experiment by changing
    the number of principal components generated and the alpha parameter, but, based
    on our experimentation, this is the best sparse PCA-based fraud detection solution.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel PCA Anomaly Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let’s design a fraud detection solution using kernel PCA, which is a nonlinear
    form of PCA and is useful if the fraud transactions are not linearly separable
    from the nonfraud transactions.
  prefs: []
  type: TYPE_NORMAL
- en: We need to specify the number of components we would like to generate, the kernel
    (we will use the RBF kernel as we did in the previous chapter), and the gamma
    (which is set to 1/n_features by default, so 1/30 in our case). We also need to
    set the `fit_inverse_transform` to `true` to apply the built-in `inverse_transform`
    function provided by Scikit-Learn.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, because kernel PCA is so expensive to train with, we will train on
    just the first two thousand samples in the transactions dataset. This is not ideal
    but it is necessary to perform experiments quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use this training to transform the entire training set and generate
    the principal components. Then, we will use the `inverse_transform` function to
    recreate the original dimension from the principal components derived by kernel
    PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 4-7](#separation_of_obversations_using_kernel_pca_and_27_principal_components)
    shows the scatterplot for kernel PCA.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Separation of Obversations Using Kernel PCA and 27 Principal Components](assets/hulp_0407.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-7\. Separation of observations using kernel PCA and 27 principal components
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now, let’s calculate the anomaly scores and print the results.
  prefs: []
  type: TYPE_NORMAL
- en: '![Results Using Kernel PCA and 27 Principal Components](assets/hulp_0408.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-8\. Results using kernel PCA and 27 principal components
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As [Figure 4-8](#results_using_kernel_pca_and_27_principal_components) shows,
    the results are far worse than those for normal PCA and sparse PCA. While it was
    worth experimenting with kernel PCA, we will not use this solution for fraud detection
    given that we have better performing solutions from earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We will not build an anomaly detection solution using SVD because the solution
    is very similar to that of normal PCA. This is expected—PCA and SVD are closely
    related.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, let’s move to random projection-based anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian Random Projection Anomaly Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let’s try to develop a fraud detection solution using Gaussian random projection.
    Remember that we can set either the number of components we want or the *eps*
    parameter, which controls the quality of the embedding derived based on the Johnson–Lindenstrauss
    lemma.
  prefs: []
  type: TYPE_NORMAL
- en: We will choose to explicitly set the number of components. Gaussian random projection
    trains very quickly, so we can train on the entire training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with sparse PCA, we will need to derive our own `inverse_transform` function
    because none is provided by Scikit-Learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 4-9](#separation_of_observations_using_gaussian_random_projection_and_27_components)
    shows the scatterplot for Gaussian random projection. [Figure 4-10](#results_using_gaussian_random_projection_and_27_components)
    displays the results for Gaussian random projection.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Separation of Obversations Using Gaussian Random Projection and 27 Components](assets/hulp_0409.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-9\. Separation of observations using Gaussian random projection and
    27 components
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Results Using Gaussian Random Projection and 27 Components](assets/hulp_0410.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-10\. Results using Gaussian random projection and 27 components
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: These results are poor, so we won’t use Gaussian random projection for fraud
    detection.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse Random Projection Anomaly Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s try to design a fraud detection solution using sparse random projection.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will designate the number of components we want (instead of setting the
    *eps* parameter). And, like with Gaussian random projection, we will use our own
    `inverse_transform` function to create the original dimensions from the sparse
    random projection-derived components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 4-11](#separation_of_obversations_using_sparse_random_projection_and_27_components)
    shows the scatterplot for sparse random projection. [Figure 4-12](#results_using_sparse_random_projection_and_27_components)
    displays the results for sparse random projection.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Separation of Obversations Using Sparse Random Projection and 27 Components](assets/hulp_0411.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-11\. Separation of observations using sparse random projection and
    27 components
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Results Using Sparse Random Projection and 27 Components](assets/hulp_0412.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-12\. Results using sparse random projection and 27 components
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As with Gaussian random projection, these results are poor. Let’s continue to
    build anomaly detection systems using other dimensionality reduction methods.
  prefs: []
  type: TYPE_NORMAL
- en: Nonlinear Anomaly Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have developed fraud detection solutions using linear dimensionality
    reduction methods such as normal PCA, sparse PCA, Gaussian random projection,
    and sparse random projection. We also developed a solution using the nonlinear
    version of PCA—kernel PCA.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, PCA is by far the best solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could turn to nonlinear dimensionality reduction algorithms, but the open
    source versions of these algorithms run very slowly and are not viable for fast
    fraud detection. Therefore, we will skip this and go directly to nondistance-based
    dimensionality reduction methods: dictionary learning and independent component
    analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: Dictionary Learning Anomaly Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s use dictionary learning to develop a fraud detection solution. Recall
    that, in dictionary learning, the algorithm learns the sparse representation of
    the original data. Using the vectors in the learned dictionary, each instance
    in the original data can be reconstructed as a weighted sum of these learned vectors.
  prefs: []
  type: TYPE_NORMAL
- en: For anomaly detection, we want to learn an undercomplete dictionary so that
    the vectors in the dictionary are fewer in number than the original dimensions.
    With this constraint, it will be easier to reconstruct the more frequently occurring
    normal transactions and much more difficult to construct the rarer fraud transactions.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, we will generate 28 vectors (or components). To learn the dictionary,
    we will feed in 10 batches, where each batch has 200 samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will need to use our own `inverse_transform` function, too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 4-13](#separation_of_obversations_using_dictionary_learning_and_28_components)
    shows the scatterplot for dictionary learning. [Figure 4-14](#results_using_dictionary_learning_and_28_components)
    shows the results for dictionary learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Separation of Obversations Using Dictionary Learning and 28 Components](assets/hulp_0413.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-13\. Separation of observations using dictionary learning and 28 components
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Results Using Dictionary Learning and 28 Components](assets/hulp_0414.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-14\. Results using dictionary learning and 28 components
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: These results are much better than those for kernal PCA, Gaussian random projection,
    and sparse random projection but are no match for those of normal PCA.
  prefs: []
  type: TYPE_NORMAL
- en: You can experiment with the code on GitHub to see if you could improve on this
    solution, but, for now, PCA remains the best fraud detection solution for this
    credit card transactions dataset.
  prefs: []
  type: TYPE_NORMAL
- en: ICA Anomaly Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s use ICA to design our last fraud detection solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to specify the number of components, which we will set to 27\. Scikit-Learn
    provides an `inverse_transform` function so we do not need to use our own:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 4-15](#separation_of_obversations_using_independent_component_analysis_and_27_components)
    shows the scatterplot for ICA. [Figure 4-16](#results_using_independent_component_analysis_and_27_components)
    shows the results for ICA.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Separation of Obversations Using Dictionary Learning and 27 Components](assets/hulp_0415.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-15\. Separation of observations using ICA and 27 components
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Results Using Independent Component Analsysis and 27 Components](assets/hulp_0416.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-16\. Results using ICA and 27 components
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: These results are identical to those of normal PCA. The fraud detection solution
    using ICA matches the best solution we’ve developed so far.
  prefs: []
  type: TYPE_NORMAL
- en: Fraud Detection on the Test Set
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, to evaluate our fraud detection solutions, let’s apply them to the never-before-seen
    test set. We will do this for the top three solutions we’ve developed: normal
    PCA, ICA, and dictionary learning. We will not use sparse PCA because it is very
    similar to the normal PCA solution.'
  prefs: []
  type: TYPE_NORMAL
- en: Normal PCA Anomaly Detection on the Test Set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start with normal PCA. We will use the PCA embedding that the PCA algorithm
    learned from the training set and use this to transform the test set. We will
    then use the Scikit-Learn `inverse_transform` function to recreate the original
    dimensions from the principal components matrix of the test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'By comparing the original test set matrix with the newly reconstructed one,
    we can calculate the anomaly scores (as we’ve done many times before in this chapter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 4-17](#separation_of_obversations_using_pca_and_27_components_on_the_test_set)
    shows the scatterplot for PCA on the test set. [Figure 4-18](#results_using_pca_and_27_components_on_the_test_set)
    displays the results for PCA on the test set.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Separation of Obversations Using PCA and 27 Components on the Test Set](assets/hulp_0417.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-17\. Separation of observations using PCA and 27 components on the
    test set
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Results Using PCA and 27 Components on the Test Set](assets/hulp_0418.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-18\. Results using PCA and 27 components on the test set
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: These are impressive results. We are able to catch 80% of the known fraud in
    the test set with an 80% precision—all without using any labels.
  prefs: []
  type: TYPE_NORMAL
- en: ICA Anomaly Detection on the Test Set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s now move to ICA and perform fraud detection on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 4-19](#separation_of_obversations_using_independent_component_analysis_and_27_components_on_the_test_set)
    shows the scatterplot for ICA on the test set. [Figure 4-20](#results_using_independent_component_analysis_and_27_components_on_the_test_set)
    shows the results for ICA on the test set.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Separation of Obversations Using Independent Component Analysis and 27 Components
    on the Test Set](assets/hulp_0419.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-19\. Separation of observations using ICA and 27 components on the
    test set
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Results Using Independent Component Analysis and 27 Components on the Test
    Set](assets/hulp_0420.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-20\. Results using ICA and 27 components on the test set
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The results are identical to normal PCA and thus quite impressive.
  prefs: []
  type: TYPE_NORMAL
- en: Dictionary Learning Anomaly Detection on the Test Set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s now turn to dictionary learning, which did not perform as well as normal
    PCA and ICA but is worth a final look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 4-21](#separation_of_obversations_using_dictionary_learning_and_28_components_on_the_test_set)
    shows the scatterplot for dictionary learning on the test set. [Figure 4-22](#results_using_the_dictionary_learning_and_28_components_on_the_test_set)
    displays the results for dictionary learning on the test set.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Separation of Obversations Using Dictionary Learning and 28 Components on
    the Test Set](assets/hulp_0421.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-21\. Separation of observations using dictionary learning and 28 components
    on the test set
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Results Using Dictionary Learning and 28 Components on the Test Set](assets/hulp_0422.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-22\. Results using dictionary learning and 28 components on the test
    set
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While the results are not terrible—we can catch 80% of the fraud with a 20%
    precision—they fall far short of the results from normal PCA and ICA.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we used the core dimensionality reduction algorithms from the
    previous chapter to develop fraud detection solutions for the credit card transactions
    dataset from [Chapter 2](ch02.html#Chapter_2).
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 2](ch02.html#Chapter_2) we used labels to build a fraud detection
    solution, but we did not use any labels during the training process in this chapter.
    In other words, we built an applied fraud detection system using unsupervised
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: While not all the dimensionality reduction algorithms performed well on this
    credit card transactions dataset, two performed remarkably well—normal PCA and
    ICA.
  prefs: []
  type: TYPE_NORMAL
- en: Normal PCA and ICA caught over 80% of the known fraud with an 80% precision.
    By comparison, the best-performing supervised learning-based fraud detection system
    from [Chapter 2](ch02.html#Chapter_2) caught nearly 90% of the known fraud with
    an 80% precision. The unsupervised fraud detection system is only marginally worse
    than the supervised system at catching known patterns of fraud.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that unsupervised fraud detection systems require no labels for training,
    adapt well to changing fraud patterns, and can catch fraud that had gone previously
    undiscovered. Given these additional advantages, the unsupervised learning-based
    solution will generally perform better than the supervised learning-based solution
    at catching known and unknown or newly emerging patterns of fraud in the future,
    although using both in tandem is best.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered dimensionality reduction and anomaly detection, let’s
    explore clustering, another major concept in the field of unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
