- en: Chapter 18\. What’s Next for Recs?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第18章。推荐系统的下一步是什么？
- en: We find ourselves in a transitionary time for recommendation systems. However,
    this is quite normal for this field, as it is in many segments of the tech industry.
    One of the realities of a field that is so closely aligned with business objectives
    and with such strong capabilities for business value is that the field tends to
    be constantly searching for any and all opportunities to advance.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正处于推荐系统的过渡时期。然而，这对于这个领域来说是非常正常的，因为它和技术行业的许多部分都是如此紧密相关的。一个与业务目标紧密对齐并具有强大业务价值能力的领域的现实是，该领域往往在不断寻找一切进步的机会。
- en: In this chapter, we’ll briefly introduce some of the modern views of where recommendation
    systems are going. An important point to consider is that recommendation systems
    as a science spread both depth first and breadth first simultaneously. Looking
    at the most cutting-edge research in the field means that you’re seeing deep optimization
    in areas that have been under study for decades or areas that seem like pure fantasy
    for now.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将简要介绍推荐系统未来的一些现代观点。一个重要的考虑点是，推荐系统作为一门科学同时深度优先和广度优先地传播。查看该领域最尖端的研究意味着你正在看到几十年来一直在研究的深度优化领域，或者看起来现在纯粹是幻想的领域。
- en: 'We’ve chosen three areas to focus on in this final chapter. The first you’ve
    seen a bit of throughout this text: multimodal recommendations. This area is increasingly
    important as users turn to platforms to do more things. Recall that multimodal
    recommendations occur when a user is represented by several latent vectors simultaneously.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这最后一章选择了三个重点领域。第一个你在本文中已经看到了一些：多模态推荐。随着用户转向平台进行更多活动，这个领域变得越来越重要。请记住，多模态推荐发生在用户同时由几个潜在向量表示的时候。
- en: Next up is graph-based recommenders. We’ve discussed co-occurrence models, which
    are the simplest such models for graph-based recommendation systems. They go much
    deeper! GNNs are becoming an incredibly powerful mechanism for encoding relations
    between entities and utilizing these representations, making them useful for recommendations.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个是基于图的推荐系统。我们已经讨论了共现模型，这是图推荐系统中最简单的模型。它们深入得多！GNN（图神经网络）正变成一种非常强大的机制，用于编码实体之间的关系并利用这些表示，使其对推荐变得有用。
- en: Finally, we’ll turn our attention to large language models and generative AI.
    During the writing of this book, LLMs have gone from something that a small subset
    of ML experts understood to something mentioned on HBO comedy broadcasts. While
    a rush is occurring to find relevant applications of LLMs to recommendation systems,
    the industry already has confidence in applying these tools in certain ways. Also
    exciting, however, is the application of recommendation systems to LLM apps.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将把注意力转向大型语言模型和生成AI。在撰写本书期间，LLM（大型语言模型）已经从少数ML专家理解的东西变成了HBO喜剧广播中提到的内容。虽然人们正忙于寻找LLM在推荐系统中的相关应用，但业界已经有信心以某些方式应用这些工具。然而，令人兴奋的是，推荐系统应用于LLM应用程序的应用。
- en: Let’s see what’s coming next!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看接下来会发生什么！
- en: Multimodal Recommendations
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多模态推荐
- en: '*Multimodal recommenders* allow for the concession that *users contain multitudes*:
    a single representation for a user’s preferences may not capture the entire story.
    Someone shopping on a large everything-ecommerce website, for example, may be
    all of the following:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*多模态推荐* 允许承认 *用户拥有多种特质*：一个用户偏好的单一表示可能无法完全捕捉整个故事。例如，在一个大型综合电子商务网站上购物的人，可能同时是以下所有内容之一：'
- en: A dog owner who frequently needs items for their dog
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个经常需要为他们的狗购买物品的狗主人
- en: A parent who is always updating the closet for the growing baby
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个父母，一直在更新宝宝成长所需的衣柜。
- en: A hobbyist race-car driver who buys the pieces necessary to drive their car
    on a track
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个业余赛车手，购买了驾驶赛车所需的零件以在赛道上驾驶他们的车。
- en: A LEGO investor who keeps hundreds of sealed boxes of Star Wars sets hidden
    away in the closet
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个乐高投资者，把数百个未拆封的星球大战套装放在壁橱里。
- en: 'The methods you’ve learned throughout this book should do well at providing
    recommendations for all of these users. However, you may notice in this list a
    few areas that are conflicting:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你在本书中学到的方法应该能够很好地为所有这些用户提供推荐。然而，你可能会注意到这个列表中有一些冲突的地方：
- en: If your child is very young, why do you buy LEGO sets already? Also, doesn’t
    your dog chew on them?
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的孩子还很小，为什么会买乐高积木套装呢？而且，你的狗不会咬它们吗？
- en: If your garage is full of LEGO sets, where do you keep all these car parts?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的车库里摆满了乐高积木，你把所有这些汽车零件放在哪里？
- en: Where do you put your dog in that two-seater Mazdaspeed MX-5 Miata?
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在两座位的 Mazdaspeed MX-5 Miata 中你把你的狗放在哪里？
- en: 'You can probably think of other cases where some aspects of what you buy just
    don’t match up well with others. This leads to a problem of multimodality: several
    places in the latent space of your interests coalesce into modes or medoids, but
    not only one.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想到其他一些情况，其中你购买的一些方面与其他方面不太匹配。这导致了多模态性问题：在你的兴趣的潜在空间中，几个地方聚合成模式或中点，但不只有一个。
- en: 'Let’s return to some of our geometric discussions from before: if you are using
    nearest neighbors to a user vector, then which of the medoids will take on the
    most importance?'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下之前的几何讨论：如果你正在使用用户向量的最近邻居，那么哪一个中点将承担最重要的角色？
- en: The way we approach this problem is by multimodality, or providing several vectors
    associated to a single user. While a naive approach to scaling to consider all
    the modes for a user would be to simply increase the dimensionality of the model
    on the item side (to create more areas in which different types of items can be
    embedded disjointly), this presents serious challenges at scale in terms of training
    and memory concerns.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们解决这个问题的方式是通过多模态性，或者为单个用户提供几个关联的向量。尽管一个简单的扩展以考虑用户的所有模式的方法是简单地增加模型在项目方面的维度（以创建更多区域，其中不同类型的项目可以不相交地嵌入），但在规模方面，这提出了严重的训练和内存问题。
- en: One of the first significant works in this area is coauthored by one of this
    book’s authors and introduces an extension to MF to deal with this; see [“Nonlinear
    Latent Factorization by Embedding Multiple User Interests”](https://oreil.ly/OkzmZ)
    by Jason Weston et al. The goal is to build multiple latent factors simultaneously
    as we did in our other matrix factorization methods, each factor hopefully taking
    on representation for one of the user’s interests.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这个领域的第一项重要工作之一是由本书的一位作者与合著者共同完成的，并引入了一个扩展 MF 来处理这个问题；参见 Jason Weston 等人的 [“通过嵌入多个用户兴趣进行非线性潜在因子分解”](https://oreil.ly/OkzmZ)。目标是同时构建多个潜在因子，正如我们在其他矩阵因子分解方法中所做的那样，希望每个因子都能代表用户的一个兴趣。
- en: This is achieved by constructing a tensor that has its third tensor dimension
    represent each of the latent factors for distinct interests rather than encoding
    a user item factorization matrix. The factorization is generalized to the tensor
    case, and the WSABIE loss you saw earlier is used to train.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过构造一个张量来实现的，该张量的第三个张量维度表示不同兴趣的每个潜在因子，而不是编码用户项目因子分解矩阵。因子分解推广到张量情况，并使用你之前看到的
    WSABIE 损失来进行训练。
- en: 'Building on this work, several years later Pinterest released PinnerSage, as
    we mentioned in [Chapter 15](ch15.html#Diversity). This modifies some of the assumptions
    of the Weston et al. paper, by not assuming a known number of representations
    for each user. Additionally, this approach uses graph-based feature representations,
    which we’ll talk more about in the next section. Finally, the last important modification
    that this method uses is clustering: it attempts to build the modes via clustering
    in item space.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作的基础上，几年后，Pinterest发布了 PinnerSage，正如我们在 [第15章](ch15.html#Diversity) 中提到的那样。这修改了
    Weston 等人的论文中的一些假设，不假设每个用户的表示数量已知。此外，这种方法使用基于图的特征表示，我们将在下一节中详细讨论。最后，这种方法使用的最后一个重要修改是聚类：它试图通过在项目空间中进行聚类来构建模式。
- en: 'The basic PinnerSage approach is to do the following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的 PinnerSage 方法是：
- en: Fix item embeddings (they call these *pins*).
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 固定项目嵌入（他们称之为 *pins*）。
- en: Cluster user interactions (unsupervised and unspecified in cardinality).
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 簇用户交互（无监督和基数未指定）。
- en: Build cluster representations as the medoid of the cluster embeddings.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建簇表示作为簇嵌入的中点。
- en: Retrieve using medoid-anchored ANN search.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用中点锚定的近邻搜索进行检索。
- en: PinnerSage is still considered to be near state of the art for large-scale multimodal
    recommenders. Some systems take another approach to allow users to more directly
    modify their “mode” by selecting the theme of what they’re looking for, while
    others hope to learn it from a sequence of interactions.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: PinnerSage 仍然被认为是大规模多模式推荐系统的最新技术。一些系统采取了另一种方法，允许用户通过选择他们正在寻找的主题来更直接地修改他们的“模式”，而另一些系统则希望从一系列交互中学习它。
- en: Next up, we’ll look at how higher-order relationships between items or users
    can be explicitly specified.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看如何明确指定项目或用户之间的高阶关系。
- en: Graph-Based Recommenders
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于图的推荐系统
- en: '*Graph neural networks* (GNNs) are a class of neural networks that use the
    structural information of data to build deeper representations of your data. They’ve
    proven especially useful when dealing with relational or networked data, both
    of which have utility.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*图神经网络*（GNNs）是一类利用数据的结构信息来构建数据更深层次表示的神经网络。在处理关系型或网络数据时，它们被证明尤为有效。'
- en: 'One moment of disambiguation before we continue: *graphs* in the sense that
    we will use them here refer to collections of *nodes* and *edges*. These are purely
    mathematical concepts, but generally we can think of nodes as the objects of interest
    and edges as the relationships between them. These mathematical objects are useful
    for distilling down the core of what is necessary for the kind of representation
    you wish to build. While the objects may seem very simple, we can add just the
    right amount of complexity in a variety of ways to capture more nuance.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，需要澄清一个概念：在这里我们将使用“图”的意义是指*节点*和*边*的集合。这些是纯数学概念，但通常我们可以认为节点是感兴趣的对象，边是它们之间的关系。这些数学对象有助于提炼出构建所需表示的核心内容。虽然这些对象可能看起来非常简单，但我们可以通过多种方式添加适量的复杂性来捕捉更多细微差别。
- en: In the simplest setups, each node on the graph represents an item or user, and
    each edge represents a relationship such as a user’s interaction with an item.
    However, user-to-user and item-to-item networks are extremely powerful extensions
    as well. Our co-occurrence models are simple graph networks; however, we did not
    learn a representation from these and instead directly took these as our models.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在最简单的设置中，图上的每个节点表示一个项目或用户，每条边表示诸如用户与项目之间的关系。然而，用户到用户和项目到项目的网络也是极为强大的扩展。我们的共现模型是简单的图网络；然而，我们并没有从中学习表示，而是直接将其作为我们的模型。
- en: 'Let’s consider a few examples of adding more structure to a graph to encode
    ideas:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一些例子，添加更多结构到图中以编码想法：
- en: Directionality
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 方向性
- en: This ordering on an edge’s vertices can be added to indicate a strict relationship
    of one node acting on the other; e.g., a user *reads* a book but not the other
    way around.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 可以添加到边的顶点上的排序，以指示一个节点对另一个节点的严格关系；例如，一个用户*阅读*一本书，但反过来不行。
- en: Edge decorations
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 边的装饰
- en: Descriptors such as edge labels can be added to communicate features about the
    relationships; e.g., two users share account credentials, *and one of the users
    is identified as a child*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 可以添加描述符如边标签以传达关系特征；例如，两个用户共享账户凭证，并且*一个用户被识别为儿童*。
- en: Multiedges
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 多重边
- en: These can allow for relationships to have higher multiplicity, or allow for
    the same two entities to have multiple relationships. In a graph of outfits with
    clothing items as nodes, each edge can be another clothing item that makes the
    other two go well together.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这些可以允许关系具有更高的多重性，或者允许相同的两个实体具有多个关系。在一个服装项为节点的图中，每条边可以是另一种能够使其他两个服装项搭配得当的服装项。
- en: Hyper-edges
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 超边
- en: A step further up the level of abstraction may add these edges, which connect
    multiple nodes simultaneously. For video scenes, you may detect objects of various
    classes, and your graph may have nodes for those classes, but understanding not
    only which pairs of object classes appear but which higher-order combinations
    appear can be identified with hyper-edges.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 更进一步提高抽象级别可能会添加这些边，这些边同时连接多个节点。对于视频场景，您可能会检测到各种类别的对象，并且您的图表可能会有这些类别的节点，但不仅要理解哪些对象类别成对出现，还要识别出哪些更高阶的组合可以用超边表示。
- en: Let’s explore the basics of GNNs and how their representations are a bit different.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨一下图神经网络（GNNs）的基础知识以及它们的表示稍有不同之处。
- en: Neural Message Passing
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经消息传递
- en: In GNNs our object of interest is assigned as the nodes in our graph. Usually,
    the main objective in GNNs is to build powerful representations of the nodes and
    edges, or both, via their relationships.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在GNNs中，我们感兴趣的对象被分配为图中的节点。通常，GNNs的主要目标是通过它们之间的关系来构建节点和边的强大表示。
- en: The fundamental difference between GNNs and traditional neural networks is that
    during the training, we’re explicitly using operators that transfer data between
    node representations “along the edges.” This is called *message passing*. Let’s
    start with an example to prime the basic idea.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: GNNs与传统神经网络的根本区别在于，在训练期间，我们明确地使用在节点表示之间“沿着边缘”传递数据的操作符。这被称为*消息传递*。让我们通过一个例子来介绍基本思想。
- en: 'Let nodes represent users, and their features are persona details such as demographic,
    onboarding survey question, etc. Let edges be the social network graph: are they
    friends? And let’s add decoration to the edges, such as the number of DMs exchanged
    between them on the platform. If we are the social media company that wants to
    introduce ad shopping to our platform, we may start with those persona features,
    but we’d ideally like to use something about this network of communication. In
    theory, people who communicate and share content with each other a lot may have
    similar tastes. Somewhat tellingly, we introduce a concept called a *message function*,
    which allows features to be sent from node to node. The message function uses
    features from each node and the edge between them, written mathematically as follows,
    for <math alttext="h Subscript i Superscript left-parenthesis k right-parenthesis"><msubsup><mi>h</mi>
    <mi>i</mi> <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msubsup></math> the features
    at node <math alttext="i"><mi>i</mi></math> and <math alttext="h Subscript j Superscript
    left-parenthesis k right-parenthesis"><msubsup><mi>h</mi> <mi>j</mi> <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msubsup></math>
    at node <math alttext="j"><mi>j</mi></math> , respectively:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让节点代表用户，它们的特征是人物细节，如人口统计信息、入职调查问题等。让边表示社交网络图：它们是朋友吗？让我们给边增加装饰，例如在平台上交换的私信数量。如果我们是希望在平台上引入广告购物的社交媒体公司，我们可能会从这些人物特征开始，但理想情况下，我们希望使用一些关于这种交流网络的信息。从理论上讲，经常沟通和分享内容的人可能有相似的兴趣。有点透露，我们引入了一个称为*消息函数*的概念，允许从一个节点发送到另一个节点的特征。数学上写为如下形式，对于节点
    <math alttext="i"><mi>i</mi></math> 处的特征 <math alttext="h Subscript i Superscript
    left-parenthesis k right-parenthesis"><msubsup><mi>h</mi> <mi>i</mi> <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msubsup></math>
    和节点 <math alttext="j"><mi>j</mi></math> 处的特征 <math alttext="h Subscript j Superscript
    left-parenthesis k right-parenthesis"><msubsup><mi>h</mi> <mi>j</mi> <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msubsup></math>：
- en: <math alttext="m Subscript i j Superscript left-parenthesis k right-parenthesis
    Baseline equals script upper M left-parenthesis h Subscript i Superscript left-parenthesis
    k right-parenthesis Baseline comma h Subscript j Superscript left-parenthesis
    k right-parenthesis Baseline comma e Subscript i j Baseline right-parenthesis"
    display="block"><mrow><msubsup><mi>m</mi> <mrow><mi>i</mi><mi>j</mi></mrow> <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msubsup>
    <mo>=</mo> <mi>ℳ</mi> <mrow><mo>(</mo> <msubsup><mi>h</mi> <mi>i</mi> <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msubsup>
    <mo>,</mo> <msubsup><mi>h</mi> <mi>j</mi> <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msubsup>
    <mo>,</mo> <msub><mi>e</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="m Subscript i j Superscript left-parenthesis k right-parenthesis
    Baseline equals script upper M left-parenthesis h Subscript i Superscript left-parenthesis
    k right-parenthesis Baseline comma h Subscript j Superscript left-parenthesis
    k right-parenthesis Baseline comma e Subscript i j Baseline right-parenthesis"
    display="block"><mrow><msubsup><mi>m</mi> <mrow><mi>i</mi><mi>j</mi></mrow> <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msubsup>
    <mo>=</mo> <mi>ℳ</mi> <mrow><mo>(</mo> <msubsup><mi>h</mi> <mi>i</mi> <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msubsup>
    <mo>,</mo> <msubsup><mi>h</mi> <mi>j</mi> <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msubsup>
    <mo>,</mo> <msub><mi>e</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub> <mo>)</mo></mrow></mrow></math>
- en: 'The features of the edge are <math alttext="e Subscript i j"><msub><mi>e</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub></math> and <math alttext="script upper
    M"><mi>ℳ</mi></math> is some differentiable function. Note that the superscript
    <math alttext="Superscript left-parenthesis k right-parenthesis"><msup><mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msup></math>
    refers to the layer as is standard in back-prop notation. Here are two simple
    examples:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 边的特征为 <math alttext="e Subscript i j"><msub><mi>e</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub></math>，<math
    alttext="script upper M"><mi>ℳ</mi></math> 是某个可微函数。注意，上标 <math alttext="Superscript
    left-parenthesis k right-parenthesis"><msup><mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msup></math>
    按照反向传播符号标准表示层。以下是两个简单的例子：
- en: <math alttext="m Subscript i j Superscript left-parenthesis k right-parenthesis
    Baseline equals h Subscript i Superscript left-parenthesis k right-parenthesis"><mrow><msubsup><mi>m</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow> <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msubsup>
    <mo>=</mo> <msubsup><mi>h</mi> <mi>i</mi> <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msubsup></mrow></math>
    means “take the features from a neighbor node”
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math alttext="m Subscript i j Superscript left-parenthesis k right-parenthesis
    Baseline equals h Subscript i Superscript left-parenthesis k right-parenthesis"><mrow><msubsup><mi>m</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow> <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msubsup>
    <mo>=</mo> <msubsup><mi>h</mi> <mi>i</mi> <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msubsup></mrow></math>
    意味着“从相邻节点获取特征”
- en: <math alttext="m Subscript i j Superscript left-parenthesis k right-parenthesis
    Baseline equals StartFraction h Subscript i Superscript left-parenthesis k right-parenthesis
    Baseline Over c Subscript i j Baseline EndFraction"><mrow><msubsup><mi>m</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow> <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msubsup>
    <mo>=</mo> <mstyle displaystyle="true" scriptlevel="0"><mfrac><msubsup><mi>h</mi>
    <mi>i</mi> <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msubsup> <msub><mi>c</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub></mfrac></mstyle></mrow></math> means
    “average by the number of edges between *i* and *j*"
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math alttext="m Subscript i j Superscript left-parenthesis k right-parenthesis
    Baseline equals StartFraction h Subscript i Superscript left-parenthesis k right-parenthesis
    Baseline Over c Subscript i j Baseline EndFraction"><mrow><msubsup><mi>m</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow> <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msubsup>
    <mo>=</mo> <mstyle displaystyle="true" scriptlevel="0"><mfrac><msubsup><mi>h</mi>
    <mi>i</mi> <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msubsup> <msub><mi>c</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub></mfrac></mstyle></mrow></math> 意味着“由
    *i* 和 *j* 之间的边的数量取平均”
- en: Many powerful message-passing schemes that use learning use approaches from
    other areas of ML—like adding an attention mechanism on node features—but this
    book doesn’t dive deep into this theory.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 许多强大的消息传递方案使用从ML其他领域借鉴的学习方法，比如在节点特征上添加注意力机制，但本书不深入探讨这个理论。
- en: 'The next function we’ll introduce is the *aggregation function*, which takes
    as input the collection of messages and aggregates them. The most common types
    of aggregation functions do the following:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将介绍的是*聚合函数*，它以消息集合作为输入并将它们聚合起来。最常见的聚合函数做以下几种事情：
- en: Concatenate all the messages
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接所有消息
- en: Sum all the messages
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总和所有消息
- en: Average all the messages
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均所有消息
- en: Take the max of the messages
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 取消息的最大值
- en: Finally, we will use the output of the aggregation as part of our update function,
    which takes node features and aggregated message functions and then applies additional
    transformations. If you’ve been wondering, “Where does this model learn anything?”
    the answer is in the update function. The update function usually has a weight
    matrix associated to it, so as you train this neural network, you are learning
    the weights in the update function. The simplest update functions multiply a weight
    matrix by the vectorized output of your aggregation and then apply an activation
    function per vector.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将聚合的输出作为我们的更新函数的一部分，该函数接受节点特征和聚合的消息函数，然后应用额外的转换。如果你一直在想，“这个模型到底是在哪里学习东西？”答案就在更新函数中。更新函数通常与一个权重矩阵相关联，因此当你训练这个神经网络时，你正在学习更新函数中的权重。最简单的更新函数会将权重矩阵乘以你的聚合的向量化输出，然后对每个向量应用激活函数。
- en: This chain of message passing, aggregating, and updating is the core of GNNs
    and encompasses a broad capability. They’ve been useful for ML tasks of every
    kind, including recommendations. Let’s see some direct applications to recommendation
    systems.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这种消息传递、聚合和更新的链条是GNN的核心，涵盖了广泛的功能。它们对于包括推荐在内的各种ML任务都非常有用。让我们看看推荐系统的一些直接应用。
- en: Applications
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用
- en: Let’s revisit some of the high-level ideas that GNNs may touch in the RecSys
    space.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视一些GNN在推荐系统领域可能触及的高层思想。
- en: Modeling user-item interactions
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 建模用户-项目交互
- en: In other methods we’ve presented, such as matrix factorization, the interactions
    between users and items are considered, but the complex network among users or
    items is not exploited. In contrast, GNNs can capture the complex connections
    in the user-item interaction graph and then use the structure of this graph to
    make more accurate recommendations.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前提到的其他方法中，比如矩阵分解，考虑了用户和项目之间的交互，但没有利用用户或项目之间复杂的网络。相反，GNN能够捕捉用户-项目交互图中的复杂连接，然后利用该图的结构来做出更精确的推荐。
- en: Thinking back to our message passing, it allowed us to “spread” the information
    of some nodes (in this case, user and items) to their neighbors. An analogy for
    this would be that as a user interacts more and more with items with specific
    features, some of those features are imbued onto the user. This may sound similar
    to latent features, because it is! These are ultimately helping the network build
    a latent representation from the messages that pass features from items to user.
    This can be even more powerful than other latent embedding methods, because you
    explicitly define the structural relationships and how they communicate these
    features.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们的消息传递，它使我们能够将一些节点（在本例中是用户和项目）的信息“传播”给它们的邻居。这个类比可以是，随着用户与具有特定特征的项目进行越来越多的交互，一些特征会被赋予用户。这听起来可能与潜在特征相似，因为它确实如此！这些最终有助于网络从传递特征的消息中建立起潜在表示。这甚至可能比其他潜在嵌入方法更强大，因为你明确定义了结构关系以及它们如何传递这些特征。
- en: Feature learning
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征学习
- en: GNNs can learn more expressive feature representations of nodes (users or items)
    in a graph by aggregating feature information from their neighbors, leveraging
    the connections between nodes. These learned features can provide rich information
    about users’ preferences or items’ characteristics, which can greatly enhance
    the performance of recommendation systems.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: GNN可以通过聚合来自邻居的特征信息，在图中学习节点（用户或项目）更具表达力的特征表示，利用节点之间的连接。这些学到的特征可以提供关于用户偏好或项目特性的丰富信息，极大地增强推荐系统的性能。
- en: Previously, we talked about how a user’s representations can learn from the
    items they interact with, but items can also learn from one another. Similar to
    the way item-item collaborative filtering (CF) allows items to pick up latent
    features from shared users, GNNs allow us to add potentially many other direct
    relationships between items.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们谈到用户的表示可以从他们互动的物品中学习，但物品也可以互相学习。类似于物品-物品的协同过滤（CF）允许物品从共享用户中获得潜在特征，GNN允许我们添加潜在的许多其他直接的物品之间的关系。
- en: Cold-start problem
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 冷启动问题
- en: 'Recall our cold-start problem: providing recommendations for new users or items
    is difficult because of the lack of historical interactions. By using the features
    of nodes and the structure of the graph, GNNs can learn the embeddings for new
    users or items, potentially alleviating the cold-start problem.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们的冷启动问题：为新用户或物品提供推荐是困难的，因为缺乏历史互动。通过使用节点的特征和图的结构，GNN可以学习新用户或物品的嵌入，可能缓解冷启动问题。
- en: In some of our graphical representations of our user graph, the edges need not
    only exist between users with lots of prior recommendations. It’s possible to
    use other user actions to *bootstrap* some early edges. Structural edges like
    “share a physical location” or “invited by the same user” or “answers onboarding
    questions similarly” can be enough to quickly bootstrap several user-user edges,
    which allow us to warm-start recommendations for them.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的用户图形表示中，有些边不仅需要存在于具有大量先前推荐的用户之间。可以使用其他用户行为来*引导*一些早期边。结构性边缘，如“分享物理位置”或“由同一用户邀请”或“类似地回答入职问题”，足以快速引导几个用户-用户边缘，从而为他们预热推荐。
- en: Context-aware recommendations
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 上下文感知推荐
- en: GNNs can incorporate contextual information into the recommendation process.
    For example, in a session-based recommendation, a GNN can model the sequence of
    items a user has interacted with in a session as a graph, where each item is a
    node and the sequential order forms edges. The GNN can then learn the dynamic
    and complex transitions among items to make context-aware recommendations.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: GNN可以将上下文信息整合到推荐过程中。例如，在基于会话的推荐中，GNN可以将用户在会话中互动的物品序列建模为一个图，其中每个物品是一个节点，顺序形成边缘。然后，GNN可以学习物品之间的动态复杂转换，以进行上下文感知推荐。
- en: 'These high-level ideas should point to the opportunity in graph encoding for
    recommender problems, but let’s look at two specific applications next: random
    walks and metapaths.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这些高层次的想法应该指向推荐问题中图编码的机会，但接下来让我们看看两个具体的应用：随机游走和元路径。
- en: Random Walks
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机游走
- en: Random walks in GNNs enable methods to use the user-item interaction graph to
    learn effective node (i.e., user or item) embeddings. The embeddings are then
    used to make recommendations. In the context of graphs, a random walk is an iterative
    process of starting on a particular node and then stochastically moving to another
    connected node via a randomized choice.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: GNN中的随机游走使得方法能够利用用户-物品交互图来学习有效的节点（即用户或物品）嵌入。然后利用这些嵌入来进行推荐。在图的背景下，随机游走是一个迭代过程，从特定节点开始，然后通过随机选择随机移动到另一个连接节点。
- en: One popular random-walk-based algorithm for network embedding is DeepWalk, which
    has been adapted and extended in many ways for various tasks, including recommendation
    systems.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行的基于随机游走的网络嵌入算法是DeepWalk，已经在各种任务中被改编和扩展，包括推荐系统。
- en: 'Here’s how a random-walk GNN approach might work in a recommendation context:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是随机游走GNN方法在推荐上下文中的工作原理：
- en: 'Random walks generation: start by performing random walks on the interaction
    graph. Starting from each node, make a series of random steps to other connected
    nodes. This results in a set of paths, or “walks,” that represent the relationships
    between different nodes.'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机游走生成：从交互图上的每个节点开始进行随机游走。从每个节点开始，向其他连接的节点进行一系列随机步骤。这将产生一组路径或“游走”，代表不同节点之间的关系。
- en: 'Node embeddings: the sequences of nodes generated by the random walks are treated
    similar to sentences in a corpus of text, and each node is treated like a word.
    Word2vec or similar language-modeling techniques are then used to learn embeddings
    for the nodes (vector representations), such that nodes appearing in similar contexts
    (in the same walks) have similar embeddings.'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 节点嵌入：通过随机游走生成的节点序列类似于文本语料库中的句子，每个节点类似于一个词。然后使用Word2vec或类似的语言建模技术来学习节点的嵌入（向量表示），使得在相似上下文（在相同的游走中）中出现的节点具有相似的嵌入。
- en: 'Recommendations: once you have learned node embeddings, you can use them to
    make recommendations. For a given user, you might recommend items that are “close”
    to that user in the embedding space, according to a distance metric. This can
    use all the techniques we’ve previously developed for recommendations from latent
    space representations.'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 推荐：一旦学习了节点嵌入，您可以使用它们来做推荐。对于给定的用户，您可以根据距离度量推荐在嵌入空间中与该用户“接近”的项目。这可以使用我们之前开发的所有推荐技术，从潜在空间表示中得到。
- en: 'This approach has some nice properties:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法具有一些良好的特性：
- en: It can capture the high-order connections in the graph. Each random walk can
    explore a part of the graph that’s not directly connected to the starting node.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以捕捉图中的高阶连接。每个随机游走可以探索与起始节点直接不连接的图的一部分。
- en: It can help with the sparsity problem in recommender systems because it uses
    the graph’s structure to learn representations, which requires less interaction
    data.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以帮助解决推荐系统中的稀疏问题，因为它利用图的结构来学习表示，这样就需要较少的交互数据。
- en: It naturally attempts to handle cold-start issues. For new users or items with
    few interactions, their embeddings can be learned from connected nodes.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它自然地尝试处理冷启动问题。对于新用户或与少数交互的项目，它们的嵌入可以从连接的节点中学习得到。
- en: Nevertheless, this approach has some challenges. Random walks can be computationally
    expensive on large graphs, and it might be difficult to choose appropriate hyperparameters,
    such as the length of the random walks. Also, this approach may not work as well
    for dynamic graphs, where interactions change over time, since it doesn’t inherently
    consider temporal information.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法也面临一些挑战。在大型图上，随机游走可能计算成本高昂，并且可能难以选择适当的超参数，例如随机游走的长度。此外，这种方法在动态图上可能效果不佳，因为它并未固有地考虑时间信息变化。
- en: This method implicitly assumes that the nodes are heterogeneous, and so co-embedding
    them via connections is natural. While it was not an explicit requirement, the
    type of sequence embeddings DeepWalk builds tends to structurally assume this.
    Let’s break this rule to accommodate learning between heterogeneous types in our
    next architecture example, metapaths.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法隐含地假设节点是异质的，因此通过连接它们来共嵌入是自然的。虽然这不是一个明确的要求，DeepWalk构建的序列嵌入类型往往假定了这一点。让我们打破这个规则，以适应学习下一个体系结构示例中异质类型之间的学习，元路径。
- en: Metapath and Heterogeneity
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 元路径和异质性
- en: '[Metapath](https://oreil.ly/pZIkC) was introduced to improve explainable recommendations
    and integrate the ideas of knowledge graphs with GNNs.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[Metapath](https://oreil.ly/pZIkC) 旨在改进可解释的推荐并将知识图谱的思想与GNN集成。'
- en: A *metapath* is a path in a heterogeneous network (or graph) that connects different
    types of nodes via different types of relationships. Heterogeneous networks contain
    various types of nodes and edges, representing multiple types of objects and interactions.
    Beyond simply users and items, the node types can be “carts of items” or “viewing
    sessions” or “channel used for purchase.”
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*元路径* 是在异质网络（或图）中连接不同类型节点的路径。异质网络包含多种类型的节点和边，代表多种对象和交互类型。除了简单的用户和项目外，节点类型还可以是“项目的购物车”或“查看会话”或“用于购买的渠道”。'
- en: Metapaths can be used in GNNs for handling heterogeneous information networks
    (HINs). These networks provide a more comprehensive representation of the real
    world. When used in a GNN, a metapath provides a scheme for the way information
    should be aggregated and propagated through the network. It defines the type of
    paths to be considered when pooling information from a node’s neighborhood.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 元路径可以用于处理异质信息网络（HINs）中的GNNs。这些网络提供了对真实世界更全面的表示。当用于GNN时，元路径提供了一种方案，用于在网络中聚合和传播信息。它定义了在汇集节点邻域时应考虑的路径类型。
- en: For example, in a recommender system, you might have a heterogeneous network
    with users, movies, and genres as node types, and “watches” and “belongs to” as
    edge types. A metapath could be defined as “User - watches → Movie - belongs to
    → Genre - belongs to → Movie - watches → User.” This metapath represents a way
    of connecting two users through the movies they watch and the genres of those
    movies.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在推荐系统中，您可能有一个包含用户、电影和流派作为节点类型，以及“观看”和“属于”作为边类型的异质网络。“User - watches → Movie
    - belongs to → Genre - belongs to → Movie - watches → User”可以定义为一个元路径。这个元路径表示通过他们观看的电影和这些电影的流派来连接两个用户的方式。
- en: A popular method that utilizes metapaths is the heterogeneous GNN (Hetero-GNN)
    and its variants. These models leverage the metapath concept to capture the rich
    semantics in HINs, enhancing the learning of node representations.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行的方法是利用元路径的异构图神经网络（Hetero-GNN）及其变体。这些模型利用元路径概念捕获HIN中丰富的语义信息，增强节点表示的学习能力。
- en: Metapath-based models have shown promising results in various applications,
    as they allow you to explicitly encode much more abstract relationships into the
    message-passing mechanisms we’ve mentioned.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 基于元路径的模型在各种应用中显示出有希望的结果，因为它们允许你明确地将更抽象的关系编码到我们提到的消息传递机制中。
- en: If higher-order modeling is your thing, buckle up for the last concept we’ll
    cover in this book. This topic is state of the art and full of high-level abstractions.
    Language-model-backed agents are at the absolute cutting edge of ML modeling.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果高阶建模是你的菜，那就准备好学习我们将在本书中涵盖的最后一个概念。这个主题是目前的技术水平，并充满高级抽象。语言模型支持的代理处于机器学习建模的绝对前沿。
- en: LLM Applications
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM 应用
- en: 'All of the superlatives for LLMs have been used up. For that reason, we’ll
    just say this: LLMs are powerful and have a surprisingly large number of applications.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 所有关于LLM的赞美之词都已经用完了。因此，我们只能说：LLM非常强大，并且具有令人惊讶的多种应用。
- en: LLMs are general models that allow users to interact with them via natural language.
    Fundamentally, these models are generative (they write text) and auto-regressive
    (what they write is determined by what came before). Because LLMs can speak conversationally,
    they’ve been branded as general artificial *agents*. It’s natural to then ask,
    “Can an agent recommend things for me?” Let’s start by examining how to use an
    LLM to make recommendations.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: LLM是通用模型，允许用户通过自然语言与它们交互。从根本上说，这些模型是生成型的（它们写文本）和自回归的（它们写的内容由前面的内容决定）。因为LLM可以进行对话，它们被定位为通用的人工*代理*。然后很自然地会问：“一个代理可以为我推荐东西吗？”让我们首先看看如何使用LLM来进行推荐。
- en: LLM Recommenders
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM 推荐系统
- en: Natural language is a wonderful interface to ask for recommendations. If you
    want a coworker’s recommendation for lunch, maybe you’ll show up at their desk
    and say nothing—hoping they’ll remember their latent knowledge of your preferences,
    identify the time-of-day context, recall the availability of restaurants based
    on day-of-week, and keep in mind that yesterday you had a pastrami sandwich.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言是一个很棒的界面，可以用来寻求推荐。如果你想让同事推荐午餐，也许你会出现在他们的桌前一言不发，希望他们记得你的偏好，识别出当前时间的语境，回忆起根据每周几天的餐馆开放情况，并记得昨天你吃了熏牛肉三明治的事实。
- en: More effectively, you could simply ask, “Any suggestions for lunch?”
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 更有效地，你可以简单地问一句：“午餐有什么建议吗？”
- en: Like your astute coworker, models may be more effective at providing recommendations
    if you simply ask them to. This approach also adds the capability of defining
    more precisely the kind of recommendation you want. A popular application of LLMs
    is to ask them for recipes that use a set of ingredients. Thinking through this
    in the context of the kind of recommenders we’ve built, building a recommender
    of this kind has some hurdles. It probably needs some user modeling, but it’s
    very dependent on the items specified. This means that there’s a very low signal
    for each combination of specified items.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 像你聪明的同事一样，如果你只是请求模型提供推荐，它们可能会更有效。这种方法还增加了定义更精确所需推荐种类的能力。LLM的一个流行应用是询问它们使用一组食材的菜谱。在我们构建的推荐系统的背景下思考这一点，构建这种推荐系统存在一些障碍。它可能需要一些用户建模，但它非常依赖于指定的项目。这意味着每个指定项目的信号非常低。
- en: 'An LLM, on the other hand, is quite effective at the autoregressive nature
    of this task: given a few ingredients, what’s most likely to be included next
    in the context of a recipe. By generating several items like this, a ranking model
    can augment this to provide a realistic recommender.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，LLM在这个任务的自回归性质方面非常有效：给定一些食材，下一个最有可能包含在食谱中的是什么。通过生成几个类似的项，排名模型可以增强这一点，以提供一个真实的推荐系统。
- en: LLM Training
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM 训练
- en: 'Large generative language models of the type that have exploded in popularity
    are trained in three stages:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 类似这种因其流行而广受欢迎的大型生成语言模型，通常是通过三个阶段进行训练：
- en: Pretraining for completion
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预训练以完成
- en: Supervised fine-tuning for dialogue
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 监督对话的精细调整
- en: Reinforcement learning from human feedback
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从人类反馈中进行强化学习
- en: Sometimes the latter two steps are combined into what is called *Instruct*.
    For an exceptionally deep dive into this topic, see the original InstructGPT paper
    [“Training Language Models to Follow Instructions with Human Feedback”](https://oreil.ly/e-T2J)
    by Long Ouyang et al.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，后两个步骤合并为所谓的 *Instruct*。对于这个主题的深度探讨，请参阅Long Ouyang等人的原始InstructGPT论文 [“Training
    Language Models to Follow Instructions with Human Feedback”](https://oreil.ly/e-T2J)
    。
- en: Let’s recall that text-completion tasks are equivalent to training the model
    to predict the correct word in a sequence after seeing *k* previous ones. This
    may remind you of GloVe from [Chapter 8](ch08.html#ch:wikipedia-e2e), or our discussion
    about sequential recommenders.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下文本完成任务相当于训练模型，在看到 *k* 个前文之后预测正确单词的能力。这可能会让你想起GloVe在 [第8章](ch08.html#ch:wikipedia-e2e)
    的讨论，或者我们关于顺序推荐系统的讨论。
- en: Next up is fine-tuning for dialogue; this step is necessary to teach the model
    that the “next word or phrase” should sometimes be a response instead of an extension
    of the original statement.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是对话微调阶段；这一步骤是教会模型，“下一个单词或短语”有时应该是回复，而不是原始语句的延续。
- en: 'During this stage, the data used for this training is in the form of *demonstration
    data*, i.e., pairs of statements and responses. Examples include the following:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，用于训练的数据以 *演示数据* 的形式存在，即语句和响应的配对。例如包括以下内容：
- en: A request and then a response to that request
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个请求，然后是对该请求的回应
- en: A statement and then a translation of that statement
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种陈述，然后是该陈述的翻译
- en: A long text and then a summarization of that text
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一段长文本，然后是该文本的摘要
- en: For recommendations, you can imagine that the first is highly relevant to the
    task we hope the model to demonstrate.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对于推荐系统，可以想象第一个与我们希望模型展示的任务高度相关。
- en: Finally, we move to the reinforcement learning from human feedback (RLHF) stage;
    the goal here is to learn a reward function that we can later use to further optimize
    our LLM. However, the reward model *itself* needs to be trained. Interestingly
    for recommendation systems enthusiasts like yourself, AI engineers do this via
    a ranking dataset.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们进入从人类反馈中进行强化学习（RLHF）阶段；这里的目标是学习一个奖励函数，稍后可以用来进一步优化我们的LLM。然而，奖励模型 *本身* 需要训练。有趣的是，像您这样的推荐系统爱好者，AI工程师通过排名数据集来完成这一过程。
- en: 'A large number of tuples—similar to the demonstration data we’ve seen—provide
    statements and responses, although instead of only one response, there are multiple
    responses. They are ranked (via a human labeler), and then for each pair of superior-inferior
    responses <math alttext="left-parenthesis x comma s u p comma i n f right-parenthesis"><mrow><mo>(</mo>
    <mi>x</mi> <mo>,</mo> <mi>s</mi> <mi>u</mi> <mi>p</mi> <mo>,</mo> <mi>i</mi> <mi>n</mi>
    <mi>f</mi> <mo>)</mo></mrow></math> , we evaluate the loss:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 大量的元组——类似于我们见过的演示数据——提供了语句和响应，尽管不仅仅是一个响应，还有多个响应。它们被排名（通过人工标注器），然后对于每对优秀-次优响应
    <math alttext="left-parenthesis x comma s u p comma i n f right-parenthesis"><mrow><mo>(</mo>
    <mi>x</mi> <mo>,</mo> <mi>s</mi> <mi>u</mi> <mi>p</mi> <mo>,</mo> <mi>i</mi> <mi>n</mi>
    <mi>f</mi> <mo>)</mo></mrow></math>，我们评估损失：
- en: <math alttext="r Subscript s u p Baseline equals normal upper Theta left-parenthesis
    x comma s u p right-parenthesis"><mrow><msub><mi>r</mi> <mrow><mi>s</mi><mi>u</mi><mi>p</mi></mrow></msub>
    <mo>=</mo> <mi>Θ</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>s</mi> <mi>u</mi>
    <mi>p</mi> <mo>)</mo></mrow></mrow></math> is the reward model’s score for the
    superior response.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math alttext="r Subscript s u p Baseline equals normal upper Theta left-parenthesis
    x comma s u p right-parenthesis"><mrow><msub><mi>r</mi> <mrow><mi>s</mi><mi>u</mi><mi>p</mi></mrow></msub>
    <mo>=</mo> <mi>Θ</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>s</mi> <mi>u</mi>
    <mi>p</mi> <mo>)</mo></mrow></mrow></math> 是优秀响应的奖励模型评分。
- en: <math alttext="r Subscript i n f Baseline equals normal upper Theta left-parenthesis
    x comma i n f right-parenthesis"><mrow><msub><mi>r</mi> <mrow><mi>i</mi><mi>n</mi><mi>f</mi></mrow></msub>
    <mo>=</mo> <mi>Θ</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>i</mi> <mi>n</mi>
    <mi>f</mi> <mo>)</mo></mrow></mrow></math> is the reward model’s score for the
    inferior response.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math alttext="r Subscript i n f Baseline equals normal upper Theta left-parenthesis
    x comma i n f right-parenthesis"><mrow><msub><mi>r</mi> <mrow><mi>i</mi><mi>n</mi><mi>f</mi></mrow></msub>
    <mo>=</mo> <mi>Θ</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>i</mi> <mi>n</mi>
    <mi>f</mi> <mo>)</mo></mrow></mrow></math> 是次优响应的奖励模型评分。
- en: 'The final loss is computed: <math alttext="minus l o g left-parenthesis sigma
    left-parenthesis s u p minus i n f right-parenthesis right-parenthesis"><mrow><mo>-</mo>
    <mi>l</mi> <mi>o</mi> <mi>g</mi> <mo>(</mo> <mi>σ</mi> <mo>(</mo> <mi>s</mi> <mi>u</mi>
    <mi>p</mi> <mo>-</mo> <mi>i</mi> <mi>n</mi> <mi>f</mi> <mo>)</mo> <mo>)</mo></mrow></math>
    .'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最终损失计算如下：<math alttext="minus l o g left-parenthesis sigma left-parenthesis
    s u p minus i n f right-parenthesis right-parenthesis"><mrow><mo>-</mo> <mi>l</mi>
    <mi>o</mi> <mi>g</mi> <mo>(</mo> <mi>σ</mi> <mo>(</mo> <mi>s</mi> <mi>u</mi> <mi>p</mi>
    <mo>-</mo> <mi>i</mi> <mi>n</mi> <mi>f</mi> <mo>)</mo> <mo>)</mo></mrow></math>。
- en: This reward function is then used to fine-tune the model.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用这个奖励函数对模型进行微调。
- en: OpenAI summarizes this approach via the diagram in [Figure 18-1](#fig-18-1).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 18-1](#fig-18-1) 概括了 OpenAI 通过图表的这种方法。'
- en: '![Instruct methodology for model fine-tuning](assets/brpj_1801.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![指导模型微调的方法论](assets/brpj_1801.png)'
- en: Figure 18-1\. Instruct methodology for model fine-tuning
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 18-1\. 指导模型微调的方法论
- en: From this brief overview, you can see that these LLMs are trained to respond
    to requests—something well suited for a recommender. Let’s see how to augment
    this training.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个简要概述中，你可以看到这些LLM是经过训练以响应请求的——这对于推荐器来说非常合适。让我们看看如何增强这种训练。
- en: Instruct Tuning for Recommendations
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于推荐的指导微调
- en: 'In the previous discussion of instruct pairs, we saw that ultimately the aim
    of the training was to learn a rank comparison between two responses. This kind
    of training should feel quite familiar. In [“TALLRec: An Effective and Efficient
    Tuning Framework to Align Large Language Model with Recommendation”](https://oreil.ly/ViZCT)
    by Keqin Bao et al., the authors use a similar setup to teach user preferences
    to the model.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '在先前关于指导对中的讨论中，我们看到训练的最终目标是学习两个响应之间的排名比较。这种训练应该感觉非常熟悉。在 Keqin Bao 等人的《“TALLRec:
    An Effective and Efficient Tuning Framework to Align Large Language Model with
    Recommendation”》中，作者使用类似的设置来教给模型用户偏好。'
- en: 'As the paper mentions, historical interaction items are collected into two
    groups based on their ratings: user likes and user dislikes. They collect this
    information into natural language prompts to format a final “Rec Input”:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如论文所述，根据其评分，历史交互项目被收集到两组中：用户喜欢和用户不喜欢。他们将这些信息收集到自然语言提示中，以格式化最终的“Rec Input”。
- en: 'User preference: <math alttext="left-bracket i t e m 1 comma period period
    period comma i t e m Subscript n Baseline"><mrow><mo>[</mo> <mi>i</mi> <mi>t</mi>
    <mi>e</mi> <msub><mi>m</mi> <mn>1</mn></msub> <mo>,</mo> <mo>.</mo> <mo>.</mo>
    <mo>.</mo> <mo>,</mo> <mi>i</mi> <mi>t</mi> <mi>e</mi> <msub><mi>m</mi> <mi>n</mi></msub></mrow></math>
    ]'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户偏好：<math alttext="left-bracket i t e m 1 comma period period period comma
    i t e m Subscript n Baseline"><mrow><mo>[</mo> <mi>i</mi> <mi>t</mi> <mi>e</mi>
    <msub><mi>m</mi> <mn>1</mn></msub> <mo>,</mo> <mo>.</mo> <mo>.</mo> <mo>.</mo>
    <mo>,</mo> <mi>i</mi> <mi>t</mi> <mi>e</mi> <msub><mi>m</mi> <mi>n</mi></msub></mrow></math>
    ]
- en: 'User preference: <math alttext="left-bracket i t e m 1 comma period period
    period comma i t e m Subscript n Baseline"><mrow><mo>[</mo> <mi>i</mi> <mi>t</mi>
    <mi>e</mi> <msub><mi>m</mi> <mn>1</mn></msub> <mo>,</mo> <mo>.</mo> <mo>.</mo>
    <mo>.</mo> <mo>,</mo> <mi>i</mi> <mi>t</mi> <mi>e</mi> <msub><mi>m</mi> <mi>n</mi></msub></mrow></math>
    ]'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户偏好：<math alttext="left-bracket i t e m 1 comma period period period comma
    i t e m Subscript n Baseline"><mrow><mo>[</mo> <mi>i</mi> <mi>t</mi> <mi>e</mi>
    <msub><mi>m</mi> <mn>1</mn></msub> <mo>,</mo> <mo>.</mo> <mo>.</mo> <mo>.</mo>
    <mo>,</mo> <mi>i</mi> <mi>t</mi> <mi>e</mi> <msub><mi>m</mi> <mi>n</mi></msub></mrow></math>
    ]
- en: Will the user enjoy the User preference, <math alttext="left-bracket i t e m
    Subscript n plus 1 Baseline"><mrow><mo>[</mo> <mi>i</mi> <mi>t</mi> <mi>e</mi>
    <msub><mi>m</mi> <mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></math>
    ]?
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户是否喜欢用户偏好 <math alttext="left-bracket i t e m Subscript n plus 1 Baseline"><mrow><mo>[</mo>
    <mi>i</mi> <mi>t</mi> <mi>e</mi> <msub><mi>m</mi> <mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></math>
    ]？
- en: These follow the same training pattern as InstructGPT noted previously. The
    authors achieve dramatically improved performance on recommender problems as compared
    to an untrained LLM for recommendations; however, those should be considered baselines
    as it’s not their target task.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这些遵循与先前提到的 InstructGPT 相同的训练模式。与未经训练的 LLM 相比，作者在推荐问题上取得了显著改善的性能；然而，这些应该被视为基线，因为这不是它们的目标任务。
- en: LLM Rankers
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM 排名器
- en: So far in this chapter, we’ve thought of the LLM as a recommender in totality,
    but instead, the LLM can be used as simply the ranker. The most trivial approach
    to this is to simply prompt the LLM with the relevant features of a user and a
    list of items and ask it to suggest the best options.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们一直将 LLM 视为整体的推荐器，但实际上，LLM 可以被简单地用作排名器。这样做的最简单方法是仅仅使用用户的相关特征和一组项目提示
    LLM，并要求它推荐最佳选项。
- en: 'While naive, variants on this approach have seen somewhat surprising results
    in very generic settings: “The user wants to watch a scary movie tonight and isn’t
    sure which will be the best if he doesn’t like gore: movie-1, movie-2, etc.” But
    we can do better.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然天真，但是这种方法的变体在非常普通的情况下取得了一些令人惊讶的结果：“用户今晚想看一部恐怖电影，但不确定如果他不喜欢血腥场面的话哪部会是最好的：电影1，电影2，等等。”但我们可以做得更好。
- en: 'Ultimately, as with LTR approaches, we can think of pointwise, pairwise, and
    listwise. If we wish to use an LLM for a pointwise ranking, we should constrain
    our prompting and responses to a setting in which these models may be useful.
    Take, for example, a recommender for scientific papers; a user may wish to write
    what they’re working on and have the LLM helpfully suggest papers of relevance.
    While a traditional search problem, this is a setting in which our modern tools
    can bring a lot of utility: LLMs are effective at summarizing and semantic matching,
    which means that semantically similar results may be found from a large corpus,
    and then the agent can synthesize the output of those results into a cogent response.
    The biggest challenge here is hallucination, or suggesting papers that may not
    exist.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，就像LTR方法一样，我们可以考虑点对、对对和列表。如果我们希望使用LLM进行点对排序，我们应该限制我们的提示和响应设置，这些模型可能会有用。举个例子，为科学论文的推荐系统；用户可能希望写出他们正在研究的内容，并且LLM可以帮助建议相关的论文。虽然是传统的搜索问题，但这是我们现代工具可以提供很多效用的一种设置：LLM擅长总结和语义匹配，这意味着可以从大量语料库中找到语义相似的结果，然后代理可以将这些结果的输出综合成一篇连贯的响应。这里最大的挑战是幻觉，或者建议可能不存在的论文。
- en: 'You can think of pairwise and listwise similarly: distilling the reference
    data into a shape that the unique capabilities of these LLMs can use to make significant
    assists.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以类比成对和列表：将参考数据浓缩为这些LLM独特能力可以使用的形状，从而做出重要的帮助。
- en: 'While we’re near the topic of search and retrieval, it’s important to mention
    one of the ways in which recommendation can help LLM applications: retrieval augmentation.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 谈到搜索和检索，有一种方式很重要，那就是推荐可以帮助LLM应用的一种方式：检索增强。
- en: Recommendations for AI
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AI推荐
- en: We’ve seen how LLMs can be used to generate recommendations, but how do recommenders
    improve LLM applications? LLM agents are extremely general in their capabilities
    but lack specificity on many tasks. If you ask an agent, “Which of the books I
    read this year were written by nonwestern authors?” the agent has no chance of
    success. Fundamentally, this is because the general pretrained models have no
    idea what books you’ve read this year.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到LLM可以用来生成推荐，但推荐系统如何改进LLM应用呢？LLM代理在其功能上非常普遍，但在许多任务上缺乏具体性。如果你问一个代理，“我今年读的书有哪些是由非西方作者写的？”那么代理就没有成功的机会。从根本上讲，这是因为通用预训练模型不知道你今年读过哪些书。
- en: To solve for this, you’ll want to leverage *retrieval augmentation*, i.e., providing
    relevant information to the model from an existing data store. The data store
    may be an SQL database, a lookup table, or a vector database, but ultimately the
    important component here is that somehow from your request, you’re able to find
    relevant information and then provide it to an agent.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决这个问题，你需要利用*检索增强*，即从现有数据存储中为模型提供相关信息。数据存储可以是SQL数据库、查找表或向量数据库，但最重要的组成部分是从你的请求中，你能够找到相关信息，然后将其提供给代理。
- en: 'One assumption we’ve made here is that your request is interpretable by your
    retrieval system. In the preceding example, you’d like the system to automatically
    understand the “which of the books I read this year” phrase as an information-retrieval
    task equivalent to something like this:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里做出的一个假设是，你的请求可以被你的检索系统解释。在前面的例子中，你希望系统自动理解“我今年读的书中哪些”这样的短语，作为类似于以下内容的信息检索任务：
- en: '[PRE0]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here we’ve just made up an SQL database, but you can imagine schema to satisfy
    this request. Converting from the request to this SQL is now yet another task
    you need to model—maybe it’s the job of another agent request.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们刚刚创建了一个SQL数据库，但你可以想象一个满足这个请求的模式。从请求转换到这个SQL现在是你需要建模的另一个任务——也许这是另一个代理请求的工作。
- en: 'In other contexts, you want a full-scale recommender to help with the retrieval:
    if you want users to ask an agent for a movie tonight, but also to continue to
    use your deep understanding of each user’s tastes, you could first filter the
    potential movies by the user’s preference and then send only movies your recommender
    model thinks are great for them. The agent can then service the text request from
    a subset of movies that are already determined to be great.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情境中，您希望全面的推荐系统帮助检索：如果您希望用户今晚询问代理要看电影，同时继续使用您对每位用户口味的深入理解，您可以首先根据用户的偏好过滤潜在的电影，然后只发送您的推荐模型认为非常适合他们的电影。然后，代理可以从已确定为优秀的电影子集中为文本请求提供服务。
- en: 'The intersection of LLMs and recommendation systems is going to dominate much
    of the conversation in recommendation systems for a while. There’s a lot of low-hanging
    fruit in bringing the knowledge of recommender systems to this new industry. As
    Eugene Yan recently said:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs与推荐系统的交汇将在未来一段时间内主导推荐系统的讨论。在将推荐系统的知识引入这个新行业中，有很多低 hanging 的果实。正如Eugene Yan最近所说：
- en: I think the key challenge, and solution, is getting them [LLMs] the right information
    at the right time. Having a well-organized document store can help. And by using
    a hybrid of keyword and semantic search, we can accurately retrieve the context
    that LLMs need.
  id: totrans-147
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我认为关键的挑战和解决方案是在正确的时间为它们[LLMs]提供正确的信息。拥有一个良好组织的文档存储可以帮助。通过使用关键词和语义搜索的混合，我们可以准确地检索LLMs所需的上下文。
- en: Summary
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: The future of recommendation systems is bright, but the technology will continue
    to get more complicated. One of the major changes over the last five years has
    been an incredible shift to GPU-based training and the architectures that can
    use these GPUs. This is the primary motivation for why this book favors JAX over
    TensorFlow or Torch.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统的未来一片光明，但技术将继续变得更加复杂。在过去五年中的主要变化之一是对基于GPU训练和能够利用这些GPU的架构的令人难以置信的转变。这是为什么这本书更青睐JAX而不是TensorFlow或Torch的主要动机。
- en: 'The methods in this chapter embrace bigger models, more interconnections, and
    potentially inference on a scale that’s hard to house in most organizations. Ultimately,
    recommendation problems will always be solved via the following:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的方法包括更大的模型、更多的互联和潜在的难以在大多数组织中承载的推理规模。最终，推荐问题总是通过以下方式解决：
- en: Careful problem framing
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谨慎的问题框架
- en: Deeply relevant representations of users and items
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户和物品的深入相关表达
- en: Thoughtful loss functions that encode the nuances of the task
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑周全的损失函数，编码任务的微妙之处
- en: Great data collection
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 极佳的数据收集
