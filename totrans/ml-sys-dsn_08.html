<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">7</span> </span> <span class="chapter-title-text">Validation schemas</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">Ensuring reliable evaluation</li> 
    <li class="readable-text" id="p3">Standard validation schemas</li> 
    <li class="readable-text" id="p4">Nontrivial validation schemas</li> 
    <li class="readable-text" id="p5">Split updating procedure</li> 
    <li class="readable-text" id="p6">Validation schemas as part of the design document</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>Building a robust evaluation process is essential for a machine learning (ML) system, and in this chapter, we will cover the process of building a proper validation schema to achieve confident estimates of system performance. We will touch upon typical validation schemas, as well as how to select the right validation based on the specifics of a given problem and what factors to consider when designing the evaluation process in the wild.</p> 
  </div> 
  <div class="readable-text intended-text" id="p8"> 
   <p>A proper validation procedure aims to imitate what knowledge we are supposed to have and what knowledge can be dropped while operating in a real-life environment. This is somewhat connected to the overfitting problem or generalization, which we’ll cover in detail in chapter 9. </p> 
  </div> 
  <div class="readable-text intended-text" id="p9"> 
   <p>It also provides a reliable and robust estimation of a system’s performance, ideally with some theoretical guarantees. As an example, we guarantee that a real value will be in the range between the lower confidence bound and upper confidence bound 95 times out of 100 (this case will be covered in a campfire story from Valerii later in the chapter). It also helps detect and prevent data leaks, overfitting, and divergence between offline and online performance.</p> 
  </div> 
  <div class="readable-text intended-text" id="p10"> 
   <p>Performance estimation is the primary goal of validation. We use validation to estimate the model’s predictive power on unseen data, and the preferred schema is usually the one with the highest reliability and robustness (i.e., low bias/low variance).</p> 
  </div> 
  <div class="readable-text intended-text" id="p11"> 
   <p>As long as we have a reliable and robust performance estimation, we can use it for various things, like hyperparameter optimization, architecture, algorithm, and feature selection. To some extent, there is a similarity to A/B testing where schema yielding lower variance provides higher sensitivity, which we will cover later in the chapter.</p> 
  </div> 
  <div class="readable-text" id="p12"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_60"><span class="num-string">7.1</span> Reliable evaluation</h2> 
  </div> 
  <div class="readable-text" id="p13"> 
   <p>When validating anything, it is almost always a good idea to build a stable and reliable pipeline that produces repeatable results (see figure 7.1). Standard advice that you most probably can find in the literature comes down to the following three classic conditions: all you need to do is to split the data into training, validation, and test datasets. A training set is used for model training, a validation set is designed to evaluate performance during training, and a test set is used to calculate final metrics. This three-set approach is well known to those familiar with competitive ML (e.g., challenges hosted by Kaggle) or academia. At the same time, there are subtle but important distinctions within applied ML that we will discuss further in this chapter.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p14">  
   <img alt="figure" src="../Images/CH07_F01_Babushkin.png" width="647" height="376"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.1</span> Basic high-level model development cycle</h5>
  </div> 
  <div class="readable-text" id="p15"> 
   <p>There are some points to pay attention to:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p16"> A simple train-validation-test split assumes that all three datasets come from the same distribution and that this will hold in the future. This is a strong assumption that has to be validated by itself. If this assumption doesn’t hold, there is no guarantee of future performance. </li> 
   <li class="readable-text" id="p17"> There must be a repeatable use of the validation set to estimate model performance. Overestimating the model’s performance based on the validation set leads to a bias and overfit toward this set. Stop and think: when we talk about things like hyperparameter optimization, feature selection, or model selection from a high-level perspective, it is basically a part of the learning process as well. By induction, the test set can be abused in the same manner. </li> 
  </ul> 
  <div class="readable-text" id="p18"> 
   <p>That is why using the same validation split over and over again for evaluation and searching for optimal hyperparameters or anything else will lead to biased/overfitted and nonrobust results. For this reason, instead of viewing validation as the thing done once at the very beginning, we view it as a continuous process to be done repeatedly once the context of the system changes (e.g., there are new sources of data, new features, potential feedback loops caused by model usage, etc.). </p> 
  </div> 
  <div class="readable-text intended-text" id="p19"> 
   <p>We are never 100% sure what the world will bring next; that’s why we must expect the unexpected.</p> 
  </div> 
  <div class="readable-text" id="p20"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_61"><span class="num-string">7.2</span> Standard schemas</h2> 
  </div> 
  <div class="readable-text" id="p21"> 
   <p>As practice shows, you won’t need to reinvent the wheel when picking a validation schema for your ML system. Most of the standard schemas are time-tested and well-performing solutions that mostly require you to pick one that fits the requirements of your project. We will briefly cover these schemas in several subsections.</p> 
  </div> 
  <div class="readable-text intended-text" id="p22"> 
   <p>Classic validation schemas are well implemented in the evergreen Python ML library scikit-learn, and all the relevant documentation is worth reading if you have doubts about your knowledge of the material. The information is available at <a href="https://mng.bz/aV6B">https://mng.bz/aV6B</a>.</p> 
  </div> 
  <div class="readable-text" id="p23"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_62"><span class="num-string">7.2.1</span> Holdout sets</h3> 
  </div> 
  <div class="readable-text" id="p24"> 
   <p>We’ll start by splitting the dataset into two or more chunks. Probably the golden classic mentioned in almost any book on ML is the training/validation/test split we discussed earlier.</p> 
  </div> 
  <div class="readable-text intended-text" id="p25"> 
   <p>With this approach, we partition data into three sets (it might be random or based on a specific criterion or strata) with different ratios—for example, 60/20/20 (see figure 7.2). The percentage may vary depending on the number of samples and metrics (the amount of data, metric variance, sensitivity, robustness, and reliability requirements). Empirically, the bigger the full dataset, the smaller the share that’s dedicated to validation and testing, so the training set is growing faster. The test set (i.e., outer validation) is used for the final model evaluation and should never be used for any other purpose. Meanwhile, we can use the validation set (i.e., inner validation) primarily for model comparison or tuning hyperparameters.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p26">  
   <img alt="figure" src="../Images/CH07_F02_Babushkin.png" width="711" height="256"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.2</span> Standard by-the-book data split</h5>
  </div> 
  <div class="readable-text" id="p27"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_63"><span class="num-string">7.2.2</span> Cross-validation<span class="aframe-location"/></h3> 
  </div> 
  <div class="readable-text" id="p28"> 
   <p>The holdout validation is a good choice for computationally expensive models, such as deep learning models. It is easy to implement and doesn’t add much time to the learning loop.</p> 
  </div> 
  <div class="readable-text intended-text" id="p29"> 
   <p>But let’s remember that we take a single random subsample from all the data. We are not reusing all available data that might lead to biased evaluation or underutilization of available data. What’s the worst part? We get a single number that does not allow us to understand the distribution of the estimates. </p> 
  </div> 
  <div class="readable-text intended-text" id="p30"> 
   <p>The silver bullet for resolving such a problem in statistics is a bootstrap procedure. In the validation case, it would look like randomly sampling train validation splits many times, training and evaluating the model each iteration. Training a model is time-consuming, and we want to iterate quickly for general parameter tweaking and experimentation. So how can we do it? </p> 
  </div> 
  <div class="readable-text intended-text" id="p31"> 
   <p>We can use a similar but simplified sampling procedure called <em>cross-validation</em>. We can split data into K folds (usually five), exclude each of them one by one, fit the model to the K – 1 folds of data, and measure performance on the excluded fold. Hence, we get K estimates and can calculate their mean and standard deviation. As a result, we get five numbers instead of one, which is more representative (see figure 7.3).<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p32">  
   <img alt="figure" src="../Images/CH07_F03_Babushkin.png" width="915" height="405"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.3</span> K-fold split: each sample is assigned to a fold, and each fold provides validation once and trains once in the rest of the training rounds. </h5>
  </div> 
  <div class="readable-text" id="p33"> 
   <p>There are several variations of cross-validation, including:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p34"> <em>Stratified cross-validation</em> (we need to maintain the balance of classes). </li> 
   <li class="readable-text" id="p35"> <em>Repeated cross-validation</em> (we split into K folds N times, so that each object participates in the evaluation N times). </li> 
   <li class="readable-text" id="p36"> <em>Grouped cross-validation</em> (when objects are similar within groups, we may want to avoid a leak; the entire group must be fully included either in the training sample or in the validation sample). </li> 
  </ul> 
  <div class="readable-text" id="p37"> 
   <p>Suppose we predict the flow rate of oil at hundreds of wells. Wells are grouped based on their location: neighboring wells extract oil from the same oil field, so their production affects each other. For this case, a grouped K-fold is a reasonable choice. Finding a proper criterion for grouping samples while assigning them to folds is one of the key decisions for validation overall, and mistakes here greatly affect the result.</p> 
  </div> 
  <div class="readable-text" id="p38"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_64"><span class="num-string">7.2.3</span> The choice of K</h3> 
  </div> 
  <div class="readable-text" id="p39"> 
   <p>The only question left is what number of folds to choose. The choice of K is dictated by three variables: bias, variance, and computation time. The rule of thumb is to use K = 5, which provides a good balance between bias and variance. </p> 
  </div> 
  <div class="readable-text intended-text" id="p40"> 
   <p>An extreme case for K is a leave-one-out cross-validation when each fold contains a single sample of data; thus K is equal to the overall number of samples in the dataset. This schema is the worst in terms of computation time and variance, but it’s the best in terms of bias.</p> 
  </div> 
  <div class="readable-text intended-text" id="p41"> 
   <p>There is a classic paper by Ron Kohavi from 1995 titled “A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection” (<a href="https://mng.bz/4pn5">https://mng.bz/4pn5</a>) that provides the following guidelines:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p42"> Increasing the number of folds reduces bias and improves performance estimation. </li> 
   <li class="readable-text" id="p43"> At the same time, variance increases along with the number of folds due to a lower number of samples in each validation fold (the estimates become too noisy). With an assumption of consistent bias, the sensitivity of the validation schema is determined by variance. </li> 
   <li class="readable-text" id="p44"> Using repeated cross-validation for model comparison goals with K = 2 or K = 3 repeated 10 to 20 times is a good idea. However, for the bias optimization, repeated K-fold isn’t helpful since estimates between different repeats already share consistent bias. </li> 
   <li class="readable-text" id="p45"> The number of required folds naturally decreases with the growth of the dataset size. The more data you have in each fold, the more representative it is. </li> 
   <li class="readable-text" id="p46"> For simpler models (which is the case when dealing with baseline solutions) and well-behaved datasets, you expect both bias and variance to decrease with the number of folds. </li> 
  </ul> 
  <div class="readable-text" id="p47"> 
   <p>It is important to remember that the validation schema’s high sensitivity (i.e., low variance) only matters when the changes we try to catch in the model’s performance are small. </p> 
  </div> 
  <div class="readable-text" id="p48"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_65"><span class="num-string">7.2.4</span> Time-series validation</h3> 
  </div> 
  <div class="readable-text" id="p49"> 
   <p>When dealing with time-sensitive data, we can’t sample data randomly. Sales of products on neighboring days share some information with each other. Similarly, recent user actions provide a hint on some aspects of their later actions. But we can’t predict the past based on data from the future. In time series, the distribution of patterns is not uniform along the dataset, and we must figure out other kinds of validation schemas. How do we evaluate the model in this case? </p> 
  </div> 
  <div class="readable-text intended-text" id="p50"> 
   <p>Validation schemas used in time-series data are similar to the holdout set and cross-validation but with nonrandom splitting by timestamp. The recommendations for choosing the number of folds and their size in rolling cross-validation are similar to the ordinal K-fold.</p> 
  </div> 
  <div class="readable-text intended-text" id="p51"> 
   <p>Time-series validation adds several extra degrees of freedom that need to be considered. A great paper, “Evaluating Time Series Forecasting Models” by Cerqueira et al. (<a href="https://arxiv.org/pdf/1905.11744.pdf">https://arxiv.org/pdf/1905.11744.pdf</a>), elaborates on the following points:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p52"> <em>Window size</em>—The size of the testing set should reflect how far we make the forecast and how long the model will stay in production before retraining. </li> 
   <li class="readable-text" id="p53"> <em>Training size</em>—There are two options in regard to the amount of data used for training: we either use all available history or limit the training size to one or two previous periods (those can be weeks, months, or years, depending on a given seasonality) and discard all previous history as irrelevant. </li> 
   <li class="readable-text" id="p54"> <em>Seasonality</em>—There are patterns in data that depend on cycles of days, weeks, months, quarters, or years. We should select sizes of testing and training sets accordingly to capture these patterns. For example, to capture yearly patterns, the training data should include at least 2 years of history. Another example is a weekly seasonality in a testing set: to minimize variance between folds, each should contain the same days of the week (so we take whole weeks in each fold). </li> 
   <li class="readable-text" id="p55"> <em>Gap</em>—There can be a gap between training and testing data, which pursues two goals. First, it prepares us for a lag in receiving new data (which leads to a lag in features), and second, it makes training and testing data less correlated, thus minimizing the risk of a leak. For instance, we may skip 2 to 3 days between training and testing sets in both cases. </li> 
  </ul> 
  <div class="readable-text" id="p56"> 
   <p>While time-series validation is one of the most defect-sensitive validation methods, relying solely on the simple “don’t look at future data while training” rule would be far too shortsighted. Following this rule can save you from 95% of typical mistakes; still, there are cases where you may need to break it. For example, ML applied to financial data (such as stock market time series) is known for its high bar in precise validation requirements. At the same time, some experts in the area highlight that trivial time-series validation, as shown in figure 7.4, can lead to overfitting caused by limited data subsets (for more details, see “Backtesting Through Cross-Validation,” chapter 12 of Marcos Lopez de Prado’s <em>Advances in Financial Machine Learning</em>; Wiley). A similar reason to violate this rule may be rooted in your need to estimate how the model performs in anomaly scenarios. To get this signal, you can train the model on data from 2017 to 2019 and 2021 to 2023 and later test it on data from the COVID period of 2020. Such a split barely works as the default validation schema but still can be useful as auxiliary information.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p57">  
   <img alt="figure" src="../Images/CH07_F04_Babushkin.png" width="690" height="377"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.4</span> Standard time-based split. The test dataset always follows the train one, so train samples are “past” and test is “future.”</h5>
  </div> 
  <div class="readable-text" id="p58"> 
   <p>Sometimes you need to use a combination of different schemas. In the earlier example of flow rate prediction, we might combine grouped K-fold validation with time-series validation:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p59"> 
   <div class="code-area-container"> 
    <pre class="code-area">import numpy as np
from sklearn.model_selection import GroupKFold

import numpy as np
from sklearn.model_selection import GroupKFold
from sklearn.exceptions import NotFittedError

def grouped_time_series_kfold(model, X, y, groups, n_folds=5, 
n_repeats=10, seed=0):
    scores = []
    np.random.seed(seed)
    unique_groups = np.unique(groups)

    for i in range(n_repeats):
        gkf = GroupKFold(n_splits=n_folds)
        shuffled_groups = np.random.permutation(unique_groups)

        for train_group_idx, test_group_idx in gkf.split(X, y,
        groups=shuffled_groups):
            train_groups = shuffled_groups[train_group_idx]
            test_groups = shuffled_groups[test_group_idx]

            # Find the earliest and latest indices for train and test groups
            train_indices = np.where(np.isin(groups, train_groups))[0]
            test_indices = np.where(np.isin(groups, test_groups))[0]
            train_end = np.min(test_indices)

            # Ensure temporal order
            train_mask = np.isin(groups, train_groups) &amp;
            (np.arange(len(groups)) &lt; train_end)
            test_mask = np.isin(groups, test_groups)

            model.fit(X[train_mask], y[train_mask])
            score = model.score(X[test_mask], y[test_mask])
            scores.append(score)

    return np.array(scores)</pre>  
   </div> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p60"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Campfire story from Valerii</h5> 
   </div> 
   <div class="readable-text" id="p61"> 
    <p>When I was working in the dynamic pricing service of a large online retailer, we were set to build a sales forecast model that would predict sales volumes 1 week ahead, along with postprocessing the predictions to determine optimal prices.</p> 
   </div> 
   <div class="readable-text" id="p62"> 
    <p>Initially, we took the previous week for validation. As new daily data became available, the validation week was shifted 1 day forward. However, it was observed that the performance metrics on the validation set showed significant fluctuations from day to day. This made it difficult to determine how the model’s quality was changing in the context of periodic feature additions and adjustments, as well as changes to prediction postprocessing.</p> 
   </div> 
   <div class="readable-text" id="p63"> 
    <p>We wanted to understand the fluctuations in the metrics, and after thoroughly investigating the issue, we discovered that the variety of products changed by 15% week to week and by 40% month to month. Additionally, the sales dynamics of individual products were found to be highly heterogeneous (e.g., 10 units sold today but 0 units sold in the next 2 days). As a result, we relied on changes in the metric, which were caused by the daily updates to the validation set, rather than on actual changes in the model’s quality.</p> 
   </div> 
   <div class="readable-text" id="p64"> 
    <p>To address this issue, we implemented a “delayed shift” validation approach. Instead of updating the validation set daily, we updated it once a month while still using a 1-week validation period. This ensured that the data used for calculating metrics remained relatively fresh (no older than 1 month) while keeping the validation set fixed for an entire month. Consequently, the comparison between the two models became more meaningful, and the performance metrics became far less noisy.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p65"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_66"><span class="num-string">7.3</span> Nontrivial schemas</h2> 
  </div> 
  <div class="readable-text" id="p66"> 
   <p>We’ve reviewed standard validation schemas that cover most ML applications. Sometimes they are not enough to reflect the actual difference between seen and unseen data, even if you use a combination of them (e.g., time-based validation with group K-fold). As you know, inadequate validation leads to data leakage and, consequently, too optimistic model performance estimation (if not random!). </p> 
  </div> 
  <div class="readable-text intended-text" id="p67"> 
   <p>Such situations require you to look for unorthodox processes. Let’s review some of them.</p> 
  </div> 
  <div class="readable-text" id="p68"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_67"><span class="num-string">7.3.1</span> Nested validation</h3> 
  </div> 
  <div class="readable-text" id="p69"> 
   <p>Nested validation is an approach used when we want to run hyperparameter optimization (or any other model selection procedure) as part of the learning process. We can’t just use an excluded fold or holdout set, which we will need for the final evaluation to estimate how good a given set of parameters is. Access to the score on the testing data while fitting any parameters is a direct way to overfitting. </p> 
  </div> 
  <div class="readable-text intended-text" id="p70"> 
   <p>Instead, we use a fold-in-fold schema. We add an “inner” split of training data in each “outer” split to tune the parameters first. Then we fit the model on all available training folds with selected hyperparameters and make a prediction for the data that was not seen during hyperparameter tuning. Thus, we get two layers of validation, each of which can have its specific properties (e.g., we may prefer the inner layer to have lower variance and the outer layer to have a lower bias). We can apply nesting not only to cross-validation but also to time-series validation and ordinal holdout split (or mixed schemas of different natures) (see figures 7.5 and 7.6).<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p71">  
   <img alt="figure" src="../Images/CH07_F05_Babushkin.png" width="721" height="503"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.5</span> Example of nested cross-validation</h5>
  </div> 
  <div class="browsable-container figure-container" id="p72">  
   <img alt="figure" src="../Images/CH07_F06_Babushkin.png" width="766" height="324"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.6</span> Example of nested validation with mixed schemas: holdout split for the outer loop and K-fold for the inner loop</h5>
  </div> 
  <div class="readable-text" id="p73"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_68"><span class="num-string">7.3.2</span> Adversarial validation</h3> 
  </div> 
  <div class="readable-text" id="p74"> 
   <p>Instead of using a random subsample of data like in a standard holdout set, you may prefer to choose a different path. There is a technique called adversarial validation, a popular approach on ML competition platforms such as Kaggle. It applies an ML model for better validation of another ML model.</p> 
  </div> 
  <div class="readable-text intended-text" id="p75"> 
   <p>Adversarial validation numerically estimates whether two given datasets differ (those two may be sets of labeled and unlabeled data). And, if so, it can even quantify it on the sample level, making it possible to construct an arbitrary number of datasets, representative of each other, providing a perfect tool for estimation. An additional bonus is that it does not require data to be labeled.</p> 
  </div> 
  <div class="readable-text intended-text" id="p76"> 
   <p>The algorithm is simple: </p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p77"> We combine datasets of interest (cutting off the target variable, if present), labeling the anchor dataset (the one we want to represent) as 1 and marking the rest as 0. </li> 
   <li class="readable-text" id="p78"> We fit an auxiliary model on this concatenated dataset to solve the binary classification task (thus 0 and 1 marks). </li> 
   <li class="readable-text" id="p79"> If datasets are representative of each other and come from the same distribution, we could expect receiver operating characteristic area under the curve (ROC AUC) to be near 0.5. If they are separable (e.g., ROC AUC is greater than 0.6), then we can use the output from the model as a measure of proximity. </li> 
  </ol> 
  <div class="readable-text" id="p80"> 
   <p>Note that while this trick was used in ML competitions for a long time (the first mention we found has been there since 2016, <a href="http://fastml.com/adversarial-validation-part-one/">http://fastml.com/adversarial-validation-part-one/</a>), it was not part of more formal research until 2020 when it appeared in the paper “Adversarial Validation Approach to Concept Drift Problem in User Targeting Automation Systems at Uber” by Pan et al. (<a href="https://arxiv.org/abs/2004.03045">https://arxiv.org/abs/2004.03045</a>). </p> 
  </div> 
  <div class="readable-text intended-text" id="p81"> 
   <p>We can use this kind of splitting in many cases. When we’re checking the similarity of labeled and unlabeled datasets, there are questions we should keep in mind. How different are their distributions? What features are the best predictors of this difference? Analyzing the model created by adversarial validation may answer these questions. We will also reuse this technique for a similar matter in chapter 9.</p> 
  </div> 
  <div class="readable-text" id="p82"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_69"><span class="num-string">7.3.3</span> Quantifying dataset leakage exploitation</h3> 
  </div> 
  <div class="readable-text" id="p83"> 
   <p>We find an interesting validation technique in a paper by DeepMind titled “Improving Language Models by Retrieving from Trillions of Tokens (2021; <a href="https://arxiv.org/abs/2112.04426">https://arxiv.org/abs/2112.04426</a>), which proposes a generative model trained on the next-word-prediction task. </p> 
  </div> 
  <div class="readable-text intended-text" id="p84"> 
   <p>The paper’s authors enhance the language model by conditioning it on a context retrieved from a large corpus based on local similarity with preceding tokens. This system memorizes the entire dataset and performs the nearest-neighbors search to find chunks of text in the history that are relevant to the recent sentences. But what if the sentences we try to continue are almost identical to those the model has seen in the training set? It looks like there is a high probability of encountering dataset leakage.</p> 
  </div> 
  <div class="readable-text intended-text" id="p85"> 
   <p>The authors discussed this problem in advance and proposed a noteworthy evaluation procedure. They developed a specific measure to quantify leakage exploitation. </p> 
  </div> 
  <div class="readable-text intended-text" id="p86"> 
   <p>The general idea is the following: </p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p87"> Partition the dataset into training and validation sets as in the usual holdout validation. </li> 
   <li class="readable-text" id="p88"> Split both into chunks of fixed length. </li> 
   <li class="readable-text" id="p89"> For each chunk in the validation set, retrieve N nearest neighbors from the training set based on chunk embeddings (here we will omit how chunks are transformed into embedding space, but you can find the details in the paper). </li> 
   <li class="readable-text" id="p90"> Calculate the ratio of tokens that are common in the two chunks (they use a score similar to the Jaccard Index); this gives us a score ranging from 0 (a chunk is totally different) to 1 (a chunk is a duplicate). </li> 
   <li class="readable-text" id="p91"> If this score exceeds a certain threshold, filter out this chunk from the training set. </li> 
  </ol> 
  <div class="readable-text" id="p92"> 
   <p>This approach forces the model to retrieve useful information from similar texts and paraphrase it instead of copy-pasting it. You can use this procedure with any modern language model. It is a good example of an exotic technique that allows for minimizing data leakage and increasing the representativity of your dataset. A clear understanding of how the model will be applied will help you develop your own nontrivial validation schema if standard approaches are unsuitable.</p> 
  </div> 
  <div class="readable-text" id="p93"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_70"><span class="num-string">7.4</span> Split updating procedure</h2> 
  </div> 
  <div class="readable-text" id="p94"> 
   <blockquote>
    <div>
     We spend as much time on the test data as on the training data.  
     <div class=" quote-cite">
       — Andrej Karpathy 
     </div>
    </div>
   </blockquote> 
  </div> 
  <div class="readable-text" id="p95"> 
   <p>Regardless of which schema we use, we will probably apply it to a dynamically changing dataset. Periodically we get new data that may differ in distribution and include new patterns. How often should we update the test set to make sure our evaluation is always relevant? </p> 
  </div> 
  <div class="readable-text intended-text" id="p96"> 
   <p>There are at least two goals we may want to reach while designing a split update procedure for new data. First, we want our test set to be representative of these new patterns. From this point of view, the evaluation process should be adaptive.</p> 
  </div> 
  <div class="readable-text intended-text" id="p97"> 
   <p>Second, we want to see an evaluation dynamic: how has the model been changing through time with all updates in the architecture or features? For that, the estimates must be robust. </p> 
  </div> 
  <div class="readable-text intended-text" id="p98"> 
   <p>Some of the most common options are as follows (see figure 7.7):</p> 
  </div> 
  <div class="browsable-container figure-container" id="p99">  
   <img alt="figure" src="../Images/CH07_F07_Babushkin.png" width="927" height="702"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.7</span> Common options for updating train/validation sets. The light data blocks are used for training, while the dark ones are used for validation.</h5>
  </div> 
  <ul> 
   <li class="readable-text" id="p100"> <em>Fixed shift</em>—When dealing with data that has a strong dependence on time and novelty, we are not interested in evaluating the model’s performance on data from a year ago or older due to the drastic change in target distribution. Instead, we would like to use only recent data for validation.  <br/>For instance, we take the last 2 weeks as a validation set (starting from the last finished day) and update this set daily while retraining the model used for evaluation each time. <span class="aframe-location"/> </li> 
   <li class="readable-text" id="p102"> <em>Fixed ratio</em>—When dealing with images or text, we don’t gather new labels for data regularly. In contrast to the first case, we may have no strong dependence on data recency, meaning that newly added data may not be more important than the old data. Typically we expand the set of available data after receiving an extra portion of labels.  <br/>If we include newly labeled data only in the training set, we increase metrics due to more data available for the model. If we include this data only in the validation set, the model may miss some unseen patterns. The optimal solution is to keep the ratio between training and validation dataset sizes unchanged so that newly added data will be split accordingly. </li> 
   <li class="readable-text" id="p104"> <em>Fixed set</em>—Sometimes, instead of a balanced subset of all currently available data, we would like to evaluate our model’s quality on an unchanged “golden set” used as a benchmark. This approach guarantees that the two models are still comparable in terms of any metric, even if there is a long modeling period between them. <br/>This fixed set can be sampled from the dataset before modeling or cherry-picked manually to contain a diverse range of hard cases and reference responses. It is not supposed to be updated by design to ensure consistent model comparison. If we extend this golden set in the future, we will treat it as a completely new benchmark. </li> 
  </ul> 
  <div class="readable-text" id="p106"> 
   <p>Remember: we should perform validation on the whole pipeline, including the dataset; inference on the test set should be the same as in production. If we want to compare models side by side accurately, we should somehow save previous versions of both datasets and models. Tools for data version control and model version control (such as DVC, Git LFS, or Deep Lake) may be of help.</p> 
  </div> 
  <div class="readable-text intended-text" id="p107"> 
   <p>Once there are clues that the options here do not cover your particular use case, you may want to dive deeper into the literature dedicated to dynamic (nonstationary) data streams and concept drifts to get a holistic overview of related theory (e.g., “Scarcity of Labels in Non-Stationary Data Streams: A Survey” [<a href="https://mng.bz/gAXE">https://mng.bz/gAXE</a>]). We will also touch on the surface of the concept drift problem in chapter 11 as one of the underlying reasons why setting up a reliable validation schema is not easy.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p108"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Campfire story from Valerii</h5> 
   </div> 
   <div class="readable-text" id="p109"> 
    <p>When I was working in a big tech company, we would train a number of ML models on a local ML platform to catch spammers, scammers, scrapers, and other malevolent agents. However, the platform only produced point estimates when assessing model performance on the validation set. This turned out to be a problem, as offline estimation often was significantly different from online performance, creating either a considerable number of falsely banned users or wrong expectations.</p> 
   </div> 
   <div class="readable-text" id="p110"> 
    <p>To illustrate the point estimate problem, let’s take a coin toss example.</p> 
   </div> 
   <div class="readable-text" id="p111"> 
    <p>If we flip a fair coin 100 times, we can calculate the number of times it lands heads. That is our point estimate. If we do it again, we will end up with another number. If, instead, we say that 95 out of 100 times, we expect this number to be within the range of 40 to 60, this is a confidence interval. The lower confidence bound will be 40, meaning that we expect this number to be at least 40 in 95% of cases.</p> 
   </div> 
   <div class="readable-text" id="p112"> 
    <p>The point estimate lacks robustness, as it does not consider an ever-present uncertainty, which is easy to illustrate graphically. The plots in the following figures demonstrate the variance of the two metrics, precision and recall, using the same threshold, ML classifier, and validation data generated by the same distribution on offline data.</p> 
   </div> 
   <div class="browsable-container figure-container" id="p114">  
    <img alt="sidebar figure" src="../Images/CH07_UN01_Babushkin.png" width="879" height="558"/> 
    <h5 class="figure-container-h5 sigil_not_in_toc">Distribution of precision and recall with sample size equal to 100,000; every point is an independent dataset.<span class="aframe-location"/></h5>
   </div> 
   <div class="browsable-container figure-container" id="p115">  
    <img alt="sidebar figure" src="../Images/CH07_UN02_Babushkin.png" width="879" height="559"/> 
    <h5 class="figure-container-h5 sigil_not_in_toc">Distribution of precision and recall with sample size equal to 200,000; every point is an independent dataset.</h5>
   </div> 
   <div class="browsable-container figure-container" id="p116">  
    <img alt="sidebar figure" src="../Images/CH07_UN03_Babushkin.png" width="879" height="558"/> 
    <h5 class="figure-container-h5 sigil_not_in_toc">Distribution of precision and recall with sample size equal to 500,000; every point is an independent dataset</h5>
   </div> 
   <div class="readable-text" id="p117"> 
    <p>It was no surprise that when we compared offline point estimates and online performance, they were almost always far apart. Even within offline evaluation, the variance was huge even when the validation data size was 500,000. This situation lacks robustness and creates fragility in the whole system.</p> 
   </div> 
   <div class="readable-text" id="p118"> 
    <p>With chunks of test data, it is easy to show uncertainty for precision, recall, or other metrics. Still, there are better ways to do this. The gold standard would be random sampling with replacement or, in other words, bootstrap. Unfortunately, bootstrap is very computationally expensive. For each bootstrap iteration (between 10,000 and 100,000), we have to sample the multinomial distribution of length N (with the sample size reaching thousands or millions) and do this N times.</p> 
   </div> 
   <div class="readable-text" id="p119"> 
    <p>This proved to be a problem. On the one hand, I couldn’t use the existing estimation solution provided by the platform, as it needed to be more reliable and robust. On the other hand, integrating bootstrap into every validation step was also impossible, as it would make even a single training loop run too long.</p> 
   </div> 
   <div class="readable-text" id="p120"> 
    <p>The solution came from math. Suppose we review each sample independently and run bootstrap in parallel. In that case, we can switch from multinomial sampling to binomial(n,1/n) and independently sample each observation for each bootstrap iteration. With N &gt;&gt; 100, sampling a Poisson with lambda parameter = 1 becomes a close approximation of binomial(n,1/n)—in other words, binomial(n,1/n) ~ Poisson(1) with N &gt;&gt;100. (You can find more details at <a href="https://mng.bz/OmyR">https://mng.bz/OmyR</a>.) </p> 
   </div> 
   <div class="readable-text" id="p122"> 
    <p>No N exists in Poisson(1), making it completely independent of the data size and easy to parallel. This significantly increased speed (circa 100–1,000 times in my case with some additional tricks).</p> 
   </div> 
   <div class="readable-text" id="p123"> 
    <p>We can pick a confidence bound to hold once we have distribution for the metric of interest. In the following figure, we can see a 99% lower confidence bound. On average, 99 times out of 100, recall will not be lower than 0.071. <span class="aframe-location"/></p> 
   </div> 
   <div class="browsable-container figure-container" id="p124">  
    <img alt="sidebar figure" src="../Images/CH07_UN04_Babushkin.png" width="877" height="511"/> 
    <h5 class="figure-container-h5 sigil_not_in_toc">Distribution recall with every point being a bootstrapped original dataset; the red line is a 99% lower confidence bound.</h5>
   </div> 
   <div class="readable-text" id="p125"> 
    <p>There is one more thing to take into consideration here. Some metrics, including precision and recall, depend on the threshold we pick to calculate them. The following figures demonstrate the distributions of precision and recall with and without some minor noise (normally distributed with mean = 0 and standard deviation –0.0125) applied to the samples.</p> 
   </div> 
   <div class="readable-text" id="p126"> 
    <p>It is easy to see that the results with and without applied noise differ significantly, with increased recall and decreased precision in the latter. In a sense, these plots prove that in this case, the decision boundary margin is narrow and not robust. Adding some noise as a hyperparameter helps to estimate the distribution confidence intervals with increased trust in decision boundary robustness.</p> 
   </div> 
   <div class="browsable-container figure-container" id="p128">  
    <img alt="sidebar figure" src="../Images/CH07_UN05_Babushkin.png" width="837" height="552"/> 
    <h5 class="figure-container-h5 sigil_not_in_toc">Distribution of precision and recall with sample size of 200;000; every point is a bootstrapped original dataset, no noise added.</h5>
   </div> 
   <div class="browsable-container figure-container" id="p130">  
    <img alt="sidebar figure" src="../Images/CH07_UN06_Babushkin.png" width="879" height="581"/> 
    <h5 class="figure-container-h5 sigil_not_in_toc">Distribution of precision and recall with a sample size of 200,000; every point is a bootstrapped original dataset, noise added.</h5>
   </div> 
   <div class="readable-text" id="p131"> 
    <p>Estimating recall at a given precision/specificity is nothing new, but combined with Poisson bootstrap and noise addition, it created new metrics: bootstrapped lower confidence bound of recall at a given precision and bootstrapped lower confidence bound of recall at a given specificity. These metrics provided guaranteed (within a specific confidence level), reliable, and robust estimation of ML model performance.<span class="aframe-location"/></p> 
   </div> 
   <div class="browsable-container figure-container" id="p132">  
    <img alt="sidebar figure" src="../Images/CH07_UN07_Babushkin.png" width="1100" height="344"/> 
    <h5 class="figure-container-h5 sigil_not_in_toc">Metrics embedded into a native ML platform</h5>
   </div> 
  </div> 
  <div class="readable-text" id="p133"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_71"><span class="num-string">7.5</span> Design document: Choosing validation schemas</h2> 
  </div> 
  <div class="readable-text" id="p134"> 
   <p>Time for another block of the design document, and this time we will fill in the information about preferred validation schemas for both Supermegaretail and PhotoStock Inc. </p> 
  </div> 
  <div class="readable-text" id="p135"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_72"><span class="num-string">7.5.1</span> Validation schemas for Supermegaretail</h3> 
  </div> 
  <div class="readable-text" id="p136"> 
   <p>We start with Supermegaretail.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p137"> 
    <h4 class="readable-text-h4 sigil_not_in_toc">Design document: Supermegaretail</h4> 
   </div> 
   <div class="readable-text" id="p138"> 
    <h4 class="readable-text-h4 sigil_not_in_toc">IV. Validation schema</h4> 
   </div> 
   <div class="readable-text" id="p139"> 
    <h4 class="readable-text-h4 sigil_not_in_toc">i. Requirements</h4> 
   </div> 
   <div class="readable-text" id="p140"> 
    <p>What are the assumptions that we need to pay attention to when figuring out the evaluation process?</p> 
   </div> 
   <ul> 
    <li class="readable-text" id="p141"> New data is coming daily. </li> 
    <li class="readable-text" id="p142"> Data can arrive with a delay of up to 48 hours. </li> 
    <li class="readable-text" id="p143"> New labels (number of units sold) come with the new data. </li> 
    <li class="readable-text" id="p144"> Recent data is most probably more relevant for the prediction task. </li> 
    <li class="readable-text" id="p145"> The assortment matrix changes by 15% every month. </li> 
    <li class="readable-text" id="p146"> There’s seasonality present in the data (weekly/annual cycles). </li> 
   </ul> 
   <div class="readable-text" id="p147"> 
    <p>Despite the fact that the data is naturally divided into categories, it is irrelevant to the choice of validation schema.</p> 
   </div> 
   <div class="readable-text" id="p148"> 
    <h4 class="readable-text-h4 sigil_not_in_toc">ii. Inference </h4> 
   </div> 
   <div class="readable-text" id="p149"> 
    <p>After fixing a model (within the hyperparameter optimization procedure), we train it on the last 2 years of data and predict future demand for the next 4 weeks. This process is fully reproduced in both inner and outer validation.</p> 
   </div> 
   <div class="readable-text intended-text" id="p150"> 
    <p>It is important to note that there should be a gap of 3 days between training and validation sets to be prepared for the fact that data may arrive with a delay. Subsequently, this will affect which features we can and cannot calculate when building a model.<span class="aframe-location"/></p> 
   </div> 
   <div class="browsable-container figure-container" id="p151"> 
    <img alt="figure" src="../Images/CH07_UN08_Babushkin.png" width="867" height="470"/> 
   </div> 
   <div class="readable-text" id="p152"> 
    <h4 class="readable-text-h4 sigil_not_in_toc">iii. Inner and outer loops</h4> 
   </div> 
   <div class="readable-text" id="p153"> 
    <p>We use two layers of validation. The outer loop is used for the final estimation of the model’s performance, while the inner loop is used for hyperparameter optimization. </p> 
   </div> 
   <div class="readable-text intended-text" id="p154"> 
    <p>First, for the outer loop, given that we are working with time series, rolling cross-validation is an obvious choice. We set K = 5 to train five models with optimal parameters. Since we are predicting 4 weeks ahead, the validation window size also consists of 28 days in all splits. There is a gap of 3 days between sets, and the step size is 7 days.</p> 
   </div> 
   <div class="readable-text intended-text" id="p155"> 
    <p>The following is an example of the outer loop:</p> 
   </div> 
   <ul> 
    <li class="readable-text buletless-item" id="p156"> First outer fold: 
     <ul> 
      <li> Data for the testing is from 2022-10-10 to 2022-11-06 (4 weeks). </li> 
      <li> Data for the training is from 2020-10-07 to 2022-10-06 (2 years). </li> 
     </ul></li> 
    <li class="readable-text buletless-item" id="p157"> Second outer fold: 
     <ul> 
      <li> Data for the testing is from 2022-10-03 to 2022-10-30.  </li> 
      <li> Data for the training is from 2020-09-29 to 2022-09-28. </li> 
     </ul></li> 
    <li class="readable-text buletless-item" id="p158"> Fifth outer fold: 
     <ul> 
      <li> Data for the testing is from 2022-09-12 to 2022-10-09.  </li> 
      <li> Data for the training is from 2020-09-09 to 2022-09-08.  </li> 
     </ul></li> 
   </ul> 
   <div class="readable-text" id="p159"> 
    <p>Second, for the inner loop, inside each “train set” of the outer validation, we perform additional rolling cross-validation with a three-fold split. Each inner loop training sample consists of a 2-year history as well to capture both annual and weekly seasonality. We use the inner loop to tune hyperparameters or for feature selection. </p> 
   </div> 
   <div class="readable-text intended-text" id="p160"> 
    <p>The following is an example of the inner loop:</p> 
   </div> 
   <ul> 
    <li class="readable-text buletless-item" id="p161"> Second fold of the outer loop: 
     <ul> 
      <li> Training data for the second outer fold is from 2020-10-03 to 2022-10-02. </li> 
     </ul></li> 
    <li class="readable-text buletless-item" id="p162"> First inner fold: 
     <ul> 
      <li> Data for the testing is from 2022-09-05 to 2022-10-02 (4 weeks). </li> 
      <li> Data for the training is from 2020-09-02 to 2022-09-01 (2 years). </li> 
     </ul></li> 
    <li class="readable-text buletless-item" id="p163"> Second inner fold: 
     <ul> 
      <li> Data for the testing is from 2022-08-29 to 2022-9-25.  </li> 
      <li> Data for the training is from 2020-08-26 to 2022-08-25. </li> 
     </ul></li> 
    <li class="readable-text buletless-item" id="p164"> Third inner fold: 
     <ul> 
      <li> Data for the testing is from 2022-08-22 to 2022-09-18.  </li> 
      <li> Data for the training is from 2020-08-19 to 2022-08-18.  </li> 
     </ul></li> 
   </ul> 
   <div class="readable-text" id="p165"> 
    <p>If the model does not require model tuning yet, we can skip the inner loop.<span class="aframe-location"/></p> 
   </div> 
   <div class="browsable-container figure-container" id="p166"> 
    <img alt="figure" src="../Images/CH07_UN09_Babushkin.png" width="867" height="390"/> 
   </div> 
   <div class="readable-text" id="p167"> 
    <h4 class="readable-text-h4 sigil_not_in_toc">iv. Update frequency</h4> 
   </div> 
   <div class="readable-text" id="p168"> 
    <p>We update the split weekly along with new data and labels (so that each validation set always consists of a whole week). This will help us catch local changes and trends in model performance.</p> 
   </div> 
   <div class="readable-text intended-text" id="p169"> 
    <p>Additionally, we have a separate holdout set as a benchmark (a “golden set”). We update it every 3 months. It helps us track long-term improvements in our system.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p170"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_73"><span class="num-string">7.5.2</span> Validation schemas for PhotoStock Inc. </h3> 
  </div> 
  <div class="readable-text" id="p171"> 
   <p>We will now add the information on validation schemas for PhotoStock Inc. </p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p172"> 
    <h4 class="readable-text-h4 sigil_not_in_toc">Design document: PhotoStock Inc.</h4> 
   </div> 
   <div class="readable-text" id="p173"> 
    <h4 class="readable-text-h4 sigil_not_in_toc">IV. Validation schema</h4> 
   </div> 
   <div class="readable-text" id="p174"> 
    <p>Search query is the main object of validation. There are four main caveats to keep in mind when planning a validation strategy for the PhotoStock Inc. search engine:</p> 
   </div> 
   <ul> 
    <li class="readable-text" id="p175"> Validation and test sets should be representative of the production data; in other words, they should represent real user queries. </li> 
    <li class="readable-text" id="p176"> Validation and test sets should be diverse; in other words, they should cover as wide a range of topics and contexts as possible. </li> 
    <li class="readable-text" id="p177"> Queries by the same user should only appear in either the training, validation, or test sets but not in multiple sets, so we avoid data leakage. </li> 
    <li class="readable-text" id="p178"> Duplicate queries should be removed from the dataset to avoid data leakage. </li> 
   </ul> 
   <div class="readable-text" id="p179"> 
    <p>Thus, we suggest using the following splitting strategy:</p> 
   </div> 
   <ol> 
    <li class="readable-text" id="p180"> Group queries by user; each query is assigned to a user once. If another user has the same query, it’s ignored. </li> 
    <li class="readable-text" id="p181"> Random-split users into training, validation, and test sets with a fixed ratio (to be determined; we don’t know what ratio is the best, but we can start with 90/5/5). </li> 
    <li class="readable-text" id="p182"> Assign new users to their split once and never change it. </li> 
   </ol> 
   <div class="readable-text" id="p183"> 
    <p>Random split assignment should address potential distribution skewness in the data. For example, we may guess there is a seasonality effect in searches (weekend users are amateurs, while weekday users are professionals) and there is some distribution drift over time (new topics emerge; old topics fade away). The random split should address those issues, although additional analysis is required to confirm that.</p> 
   </div> 
   <div class="readable-text intended-text" id="p184"> 
    <p>To assign splits to users, we suggest using a deterministic bucketing approach: we split users into buckets based on their <code>user_id</code> <code>hash</code> and then assign each bucket to a split. This approach is universal because it allows the split ratio to change in the future. For example, if we want to increase the size of the validation set, we can just assign more buckets to the validation set from the training set.</p> 
   </div> 
   <div class="readable-text intended-text" id="p185"> 
    <p>The following is an example of the bucketing approach:</p> 
   </div> 
   <div class="browsable-container listing-container" id="p186"> 
    <div class="code-area-container"> 
     <pre class="code-area">def assign_bucket(user_id):
    _hash = sha1(user_id.encode()).hexdigest()
    return int(_hash, 16) % n_buckets

def assign_split(user_id):
    bucket = assign_bucket(user_id)
    if bucket &lt; n_buckets * train_ratio:
        return 'train'
    elif bucket &lt; n_buckets * (train_ratio + val_ratio):
        return 'val'
    else:
        return 'test'</pre>  
    </div> 
   </div> 
   <div class="readable-text" id="p187"> 
    <p>For the initial project phase, we don’t plan to add more subsets (e.g., “the golden set”), although we can’t exclude this possibility in the future.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p188"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_74">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p189"> Use validation schemas as a way to measure your model’s predictive power accurately. </li> 
   <li class="readable-text" id="p190"> Try to avoid using the same validation split repeatedly for evaluation and searching for optimal hyperparameters, as it may lead to biased/overfitted and nonrobust results. </li> 
   <li class="readable-text" id="p191"> Try to design a validation schema to reflect how the model is applied in practice. </li> 
   <li class="readable-text" id="p192"> When looking for a needed number of K folds, base your choice on the following three variables: bias, variance, and computation time. </li> 
   <li class="readable-text" id="p193"> To do this, consider how the data differs between seen and unseen data (whether there are groups, classes, time, or other essential properties you should take into account). </li> 
   <li class="readable-text" id="p194"> Design a nonstandard schema to fit a particular problem if necessary. </li> 
   <li class="readable-text" id="p195"> Remember that different schemas for different goals can work together nicely. </li> 
  </ul>
 </div></div></body></html>