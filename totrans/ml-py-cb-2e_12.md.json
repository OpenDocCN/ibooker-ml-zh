["```py\n# Load libraries\nimport numpy as np\nfrom sklearn import linear_model, datasets\nfrom sklearn.model_selection import GridSearchCV\n\n# Load data\niris = datasets.load_iris()\nfeatures = iris.data\ntarget = iris.target\n\n# Create logistic regression\nlogistic = linear_model.LogisticRegression(max_iter=500, solver='liblinear')\n\n# Create range of candidate penalty hyperparameter values\npenalty = ['l1','l2']\n\n# Create range of candidate regularization hyperparameter values\nC = np.logspace(0, 4, 10)\n\n# Create dictionary of hyperparameter candidates\nhyperparameters = dict(C=C, penalty=penalty)\n\n# Create grid search\ngridsearch = GridSearchCV(logistic, hyperparameters, cv=5, verbose=0)\n\n# Fit grid search\nbest_model = gridsearch.fit(features, target)\n\n# Show the best model\nprint(best_model.best_estimator_)\n```", "```py\nLogisticRegression(C=7.742636826811269, max_iter=500, penalty='l1',\n                   solver='liblinear')\n```", "```py\nnp.logspace(0, 4, 10)\n```", "```py\narray([1.00000000e+00, 2.78255940e+00, 7.74263683e+00, 2.15443469e+01,\n       5.99484250e+01, 1.66810054e+02, 4.64158883e+02, 1.29154967e+03,\n       3.59381366e+03, 1.00000000e+04])\n```", "```py\n# View best hyperparameters\nprint('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\nprint('Best C:', best_model.best_estimator_.get_params()['C'])\n```", "```py\nBest Penalty: l1\nBest C: 7.742636826811269\n```", "```py\n# Predict target vector\nbest_model.predict(features)\n```", "```py\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n```", "```py\n# Load libraries\nfrom scipy.stats import uniform\nfrom sklearn import linear_model, datasets\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Load data\niris = datasets.load_iris()\nfeatures = iris.data\ntarget = iris.target\n\n# Create logistic regression\nlogistic = linear_model.LogisticRegression(max_iter=500, solver='liblinear')\n\n# Create range of candidate regularization penalty hyperparameter values\npenalty = ['l1', 'l2']\n\n# Create distribution of candidate regularization hyperparameter values\nC = uniform(loc=0, scale=4)\n\n# Create hyperparameter options\nhyperparameters = dict(C=C, penalty=penalty)\n\n# Create randomized search\nrandomizedsearch = RandomizedSearchCV(\n    logistic, hyperparameters, random_state=1, n_iter=100, cv=5, verbose=0,\n    n_jobs=-1)\n\n# Fit randomized search\nbest_model = randomizedsearch.fit(features, target)\n\n# Print best model\nprint(best_model.best_estimator_)\n```", "```py\nLogisticRegression(C=1.668088018810296, max_iter=500, penalty='l1',\n                   solver='liblinear')\n```", "```py\n# Define a uniform distribution between 0 and 4, sample 10 values\nuniform(loc=0, scale=4).rvs(10)\n```", "```py\narray([3.95211699, 0.30693116, 2.88237794, 3.00392864, 0.43964702,\n       1.46670526, 0.27841863, 2.56541664, 2.66475584, 0.79611958])\n```", "```py\n# View best hyperparameters\nprint('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\nprint('Best C:', best_model.best_estimator_.get_params()['C'])\n```", "```py\nBest Penalty: l1\nBest C: 1.668088018810296\n```", "```py\n# Predict target vector\nbest_model.predict(features)\n```", "```py\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2,\n       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n```", "```py\n# Load libraries\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\n# Set random seed\nnp.random.seed(0)\n\n# Load data\niris = datasets.load_iris()\nfeatures = iris.data\ntarget = iris.target\n\n# Create a pipeline\npipe = Pipeline([(\"classifier\", RandomForestClassifier())])\n\n# Create dictionary with candidate learning algorithms and their hyperparameters\nsearch_space = [{\"classifier\": [LogisticRegression(max_iter=500,\n       solver='liblinear')],\n                 \"classifier__penalty\": ['l1', 'l2'],\n                 \"classifier__C\": np.logspace(0, 4, 10)},\n                {\"classifier\": [RandomForestClassifier()],\n                 \"classifier__n_estimators\": [10, 100, 1000],\n                 \"classifier__max_features\": [1, 2, 3]}]\n\n# Create grid search\ngridsearch = GridSearchCV(pipe, search_space, cv=5, verbose=0)\n\n# Fit grid search\nbest_model = gridsearch.fit(features, target)\n\n# Print best model\nprint(best_model.best_estimator_)\n```", "```py\nPipeline(steps=[('classifier',\n                 LogisticRegression(C=7.742636826811269, max_iter=500,\n                                    penalty='l1', solver='liblinear'))])\n```", "```py\n{'classifier': [LogisticRegression(max_iter=500, solver='liblinear')],\n 'classifier__penalty': ['l1', 'l2'],\n 'classifier__C': np.logspace(0, 4, 10)}\n```", "```py\n{'classifier': [RandomForestClassifier()],\n 'classifier__n_estimators': [10, 100, 1000],\n 'classifier__max_features': [1, 2, 3]}\n```", "```py\n# View best model\nprint(best_model.best_estimator_.get_params()[\"classifier\"])\n```", "```py\nLogisticRegression(C=7.742636826811269, max_iter=500, penalty='l1',\n                   solver='liblinear')\n```", "```py\n# Predict target vector\nbest_model.predict(features)\n```", "```py\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n```", "```py\n# Load libraries\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# Set random seed\nnp.random.seed(0)\n\n# Load data\niris = datasets.load_iris()\nfeatures = iris.data\ntarget = iris.target\n\n# Create a preprocessing object that includes StandardScaler features and PCA\npreprocess = FeatureUnion([(\"std\", StandardScaler()), (\"pca\", PCA())])\n\n# Create a pipeline\npipe = Pipeline([(\"preprocess\", preprocess),\n                 (\"classifier\", LogisticRegression(max_iter=1000,\n                     solver='liblinear'))])\n\n# Create space of candidate values\nsearch_space = [{\"preprocess__pca__n_components\": [1, 2, 3],\n                 \"classifier__penalty\": [\"l1\", \"l2\"],\n                 \"classifier__C\": np.logspace(0, 4, 10)}]\n\n# Create grid search\nclf = GridSearchCV(pipe, search_space, cv=5, verbose=0, n_jobs=-1)\n\n# Fit grid search\nbest_model = clf.fit(features, target)\n\n# Print best model\nprint(best_model.best_estimator_)\n```", "```py\nPipeline(steps=[('preprocess',\n                 FeatureUnion(transformer_list=[('std', StandardScaler()),\n                                                ('pca', PCA(n_components=1))])),\n                ('classifier',\n                 LogisticRegression(C=7.742636826811269, max_iter=1000,\n                                    penalty='l1', solver='liblinear'))])\n```", "```py\n# View best n_components\nbest_model.best_estimator_.get_params()['preprocess__pca__n_components']\n```", "```py\n1\n```", "```py\n# Load libraries\nimport numpy as np\nfrom sklearn import linear_model, datasets\nfrom sklearn.model_selection import GridSearchCV\n\n# Load data\niris = datasets.load_iris()\nfeatures = iris.data\ntarget = iris.target\n\n# Create logistic regression\nlogistic = linear_model.LogisticRegression(max_iter=500, solver='liblinear')\n\n# Create range of candidate regularization penalty hyperparameter values\npenalty = [\"l1\", \"l2\"]\n\n# Create range of candidate values for C\nC = np.logspace(0, 4, 1000)\n\n# Create hyperparameter options\nhyperparameters = dict(C=C, penalty=penalty)\n\n# Create grid search\ngridsearch = GridSearchCV(logistic, hyperparameters, cv=5, n_jobs=-1, verbose=1)\n\n# Fit grid search\nbest_model = gridsearch.fit(features, target)\n\n# Print best model\nprint(best_model.best_estimator_)\n```", "```py\nFitting 5 folds for each of 2000 candidates, totalling 10000 fits\nLogisticRegression(C=5.926151812475554, max_iter=500, penalty='l1',\n                   solver='liblinear')\n```", "```py\n# Create grid search using one core\nclf = GridSearchCV(logistic, hyperparameters, cv=5, n_jobs=1, verbose=1)\n\n# Fit grid search\nbest_model = clf.fit(features, target)\n\n# Print best model\nprint(best_model.best_estimator_)\n```", "```py\nFitting 5 folds for each of 2000 candidates, totalling 10000 fits\nLogisticRegression(C=5.926151812475554, max_iter=500, penalty='l1',\n                   solver='liblinear')\n```", "```py\n# Load libraries\nfrom sklearn import linear_model, datasets\n\n# Load data\niris = datasets.load_iris()\nfeatures = iris.data\ntarget = iris.target\n\n# Create cross-validated logistic regression\nlogit = linear_model.LogisticRegressionCV(Cs=100, max_iter=500,\n       solver='liblinear')\n\n# Train model\nlogit.fit(features, target)\n\n# Print model\nprint(logit)\n```", "```py\nLogisticRegressionCV(Cs=100, max_iter=500, solver='liblinear')\n```", "```py\n# Load libraries\nimport numpy as np\nfrom sklearn import linear_model, datasets\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\n\n# Load data\niris = datasets.load_iris()\nfeatures = iris.data\ntarget = iris.target\n\n# Create logistic regression\nlogistic = linear_model.LogisticRegression(max_iter=500, solver='liblinear')\n\n# Create range of 20 candidate values for C\nC = np.logspace(0, 4, 20)\n\n# Create hyperparameter options\nhyperparameters = dict(C=C)\n\n# Create grid search\ngridsearch = GridSearchCV(logistic, hyperparameters, cv=5, n_jobs=-1, verbose=0)\n\n# Conduct nested cross-validation and output the average score\ncross_val_score(gridsearch, features, target).mean()\n```", "```py\n0.9733333333333334\n```", "```py\ngridsearch = GridSearchCV(logistic, hyperparameters, cv=5, verbose=1)\n```", "```py\nbest_model = gridsearch.fit(features, target)\n```", "```py\nFitting 5 folds for each of 20 candidates, totalling 100 fits\n```", "```py\nscores = cross_val_score(gridsearch, features, target)\n```", "```py\nFitting 5 folds for each of 20 candidates, totalling 100 fits\nFitting 5 folds for each of 20 candidates, totalling 100 fits\nFitting 5 folds for each of 20 candidates, totalling 100 fits\nFitting 5 folds for each of 20 candidates, totalling 100 fits\nFitting 5 folds for each of 20 candidates, totalling 100 fits\n```"]