["```py\n# Load libraries\nfrom sklearn import datasets\nfrom sklearn.feature_selection import VarianceThreshold\n\n# Import some data to play with\niris = datasets.load_iris()\n\n# Create features and target\nfeatures = iris.data\ntarget = iris.target\n\n# Create thresholder\nthresholder = VarianceThreshold(threshold=.5)\n\n# Create high variance feature matrix\nfeatures_high_variance = thresholder.fit_transform(features)\n\n# View high variance feature matrix\nfeatures_high_variance[0:3]\n```", "```py\narray([[ 5.1,  1.4,  0.2],\n       [ 4.9,  1.4,  0.2],\n       [ 4.7,  1.3,  0.2]])\n```", "```py\n# View variances\nthresholder.fit(features).variances_\n```", "```py\narray([0.68112222, 0.18871289, 3.09550267, 0.57713289])\n```", "```py\n# Load library\nfrom sklearn.preprocessing import StandardScaler\n\n# Standardize feature matrix\nscaler = StandardScaler()\nfeatures_std = scaler.fit_transform(features)\n\n# Caculate variance of each feature\nselector = VarianceThreshold()\nselector.fit(features_std).variances_\n```", "```py\narray([1., 1., 1., 1.])\n```", "```py\n# Load library\nfrom sklearn.feature_selection import VarianceThreshold\n\n# Create feature matrix with:\n# Feature 0: 80% class 0\n# Feature 1: 80% class 1\n# Feature 2: 60% class 0, 40% class 1\nfeatures = [[0, 1, 0],\n            [0, 1, 1],\n            [0, 1, 0],\n            [0, 1, 1],\n            [1, 0, 0]]\n\n# Run threshold by variance\nthresholder = VarianceThreshold(threshold=(.75 * (1 - .75)))\nthresholder.fit_transform(features)\n```", "```py\narray([[0],\n       [1],\n       [0],\n       [1],\n       [0]])\n```", "```py\n# Load libraries\nimport pandas as pd\nimport numpy as np\n\n# Create feature matrix with two highly correlated features\nfeatures = np.array([[1, 1, 1],\n                     [2, 2, 0],\n                     [3, 3, 1],\n                     [4, 4, 0],\n                     [5, 5, 1],\n                     [6, 6, 0],\n                     [7, 7, 1],\n                     [8, 7, 0],\n                     [9, 7, 1]])\n\n# Convert feature matrix into DataFrame\ndataframe = pd.DataFrame(features)\n\n# Create correlation matrix\ncorr_matrix = dataframe.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape),\n                          k=1).astype(bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n\n# Drop features\ndataframe.drop(dataframe.columns[to_drop], axis=1).head(3)\n```", "```py\n# Correlation matrix\ndataframe.corr()\n```", "```py\n# Upper triangle of correlation matrix\nupper\n```", "```py\n# Load libraries\nfrom sklearn.datasets import load_iris\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2, f_classif\n\n# Load data\niris = load_iris()\nfeatures = iris.data\ntarget = iris.target\n\n# Convert to categorical data by converting data to integers\nfeatures = features.astype(int)\n\n# Select two features with highest chi-squared statistics\nchi2_selector = SelectKBest(chi2, k=2)\nfeatures_kbest = chi2_selector.fit_transform(features, target)\n\n# Show results\nprint(\"Original number of features:\", features.shape[1])\nprint(\"Reduced number of features:\", features_kbest.shape[1])\n```", "```py\nOriginal number of features: 4\nReduced number of features: 2\n```", "```py\n# Select two features with highest F-values\nfvalue_selector = SelectKBest(f_classif, k=2)\nfeatures_kbest = fvalue_selector.fit_transform(features, target)\n\n# Show results\nprint(\"Original number of features:\", features.shape[1])\nprint(\"Reduced number of features:\", features_kbest.shape[1])\n```", "```py\nOriginal number of features: 4\nReduced number of features: 2\n```", "```py\n# Load library\nfrom sklearn.feature_selection import SelectPercentile\n\n# Select top 75% of features with highest F-values\nfvalue_selector = SelectPercentile(f_classif, percentile=75)\nfeatures_kbest = fvalue_selector.fit_transform(features, target)\n\n# Show results\nprint(\"Original number of features:\", features.shape[1])\nprint(\"Reduced number of features:\", features_kbest.shape[1])\n```", "```py\nOriginal number of features: 4\nReduced number of features: 3\n```", "```py\n# Load libraries\nimport warnings\nfrom sklearn.datasets import make_regression\nfrom sklearn.feature_selection import RFECV\nfrom sklearn import datasets, linear_model\n\n# Suppress an annoying but harmless warning\nwarnings.filterwarnings(action=\"ignore\", module=\"scipy\",\n                        message=\"^internal gelsd\")\n\n# Generate features matrix, target vector, and the true coefficients\nfeatures, target = make_regression(n_samples = 10000,\n                                   n_features = 100,\n                                   n_informative = 2,\n                                   random_state = 1)\n\n# Create a linear regression\nols = linear_model.LinearRegression()\n\n# Recursively eliminate features\nrfecv = RFECV(estimator=ols, step=1, scoring=\"neg_mean_squared_error\")\nrfecv.fit(features, target)\nrfecv.transform(features)\n```", "```py\narray([[ 0.00850799,  0.7031277 ,  1.52821875],\n       [-1.07500204,  2.56148527, -0.44567768],\n       [ 1.37940721, -1.77039484, -0.74675125],\n       ...,\n       [-0.80331656, -1.60648007,  0.52231601],\n       [ 0.39508844, -1.34564911,  0.4228057 ],\n       [-0.55383035,  0.82880112,  1.73232647]])\n```", "```py\n# Number of best features\nrfecv.n_features_\n```", "```py\n3\n```", "```py\n# Which categories are best\nrfecv.support_\n```", "```py\narray([False, False, False, False, False,  True, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False,  True, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False,  True, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False])\n```", "```py\n# Rank features best (1) to worst\nrfecv.ranking_\n```", "```py\narray([11, 92, 96, 87, 46,  1, 48, 23, 16,  2, 66, 83, 33, 27, 70, 75, 29,\n       84, 54, 88, 37, 42, 85, 62, 74, 50, 80, 10, 38, 59, 79, 57, 44,  8,\n       82, 45, 89, 69, 94,  1, 35, 47, 39,  1, 34, 72, 19,  4, 17, 91, 90,\n       24, 32, 13, 49, 26, 12, 71, 68, 40,  1, 43, 63, 28, 73, 58, 21, 67,\n        1, 95, 77, 93, 22, 52, 30, 60, 81, 14, 86, 18, 15, 41,  7, 53, 65,\n       51, 64,  6,  9, 20,  5, 55, 56, 25, 36, 61, 78, 31,  3, 76])\n```"]