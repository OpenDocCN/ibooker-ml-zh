- en: Chapter 10\. Dimensionality Reduction Using Feature Selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 10.0 Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 9](ch09.xhtml#dimensionality-reduction-using-feature-extraction),
    we discussed how to reduce the dimensionality of our feature matrix by creating
    new features with (ideally) similar abilities to train quality models but with
    significantly fewer dimensions. This is called *feature extraction*. In this chapter
    we will cover an alternative approach: selecting high-quality, informative features
    and dropping less useful features. This is called *feature selection*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three types of feature selection methods: filter, wrapper, and embedded.
    *Filter methods* select the best features by examining their statistical properties.
    Methods where we explicitly set a threshold for a statistic or manually select
    the number of features we want to keep are examples of feature selection by filtering.
    Wrapper methods use trial and error to find the subset of features that produces
    models with the highest quality predictions. *Wrapper methods* are often the most
    effective, as they find the best result through actual experimentation as opposed
    to naive assumptions. Finally, *embedded methods* select the best feature subset
    as part of, as an extension of, a learning algorithm’s training process.'
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, we’d describe all three methods in this chapter. However, since embedded
    methods are closely intertwined with specific learning algorithms, they are difficult
    to explain prior to a deeper dive into the algorithms themselves. Therefore, in
    this chapter we cover only filter and wrapper feature selection methods, leaving
    the discussion of particular embedded methods until the chapters where those learning
    algorithms are discussed in depth.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1 Thresholding Numerical Feature Variance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have a set of numerical features and want to filter out those with low variance
    (i.e., likely containing little information).
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Select a subset of features with variances above a given threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Variance thresholding* (VT) is an example of feature selection by filtering,
    and one of the most basic approaches to feature selection. It is motivated by
    the idea that features with low variance are likely less interesting (and less
    useful) than features with high variance. VT first calculates the variance of
    each feature:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mrow><mi>V</mi> <mi>a</mi> <mi>r</mi></mrow> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover> <msup><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>-</mo><mi>μ</mi><mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math display="inline"><mi>x</mi></math> is the feature vector, <math
    display="inline"><msub><mi>x</mi><mi>i</mi></msub></math> is an individual feature
    value, and <math display="inline"><mi>μ</mi></math> is that feature’s mean value.
    Next, it drops all features whose variance does not meet that threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keep two things in mind when employing VT. First, the variance is not centered;
    that is, it is in the squared unit of the feature itself. Therefore, VT will not
    work when feature sets contain different units (e.g., one feature is in years
    while another is in dollars). Second, the variance threshold is selected manually,
    so we have to use our own judgment for a good value to select (or use a model
    selection technique described in [Chapter 12](ch12.xhtml#model-selection)). We
    can see the variance for each feature using `variances_`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, if the features have been standardized (to mean zero and unit variance),
    then for obvious reasons VT will not work correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 10.2 Thresholding Binary Feature Variance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have a set of binary categorical features and want to filter out those with
    low variance (i.e., likely containing little information).
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Select a subset of features with a Bernoulli random variable variance above
    a given threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As with numerical features, one strategy for selecting highly informative categorical
    features and filtering out less informative ones is to examine their variances.
    In binary features (i.e., Bernoulli random variables), variance is calculated
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mo form="prefix">Var</mo> <mo>(</mo> <mi>x</mi>
    <mo>)</mo> <mo>=</mo> <mi>p</mi> <mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>p</mi> <mo>)</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math display="inline"><mi>p</mi></math> is the proportion of observations
    of class `1`. Therefore, by setting <math display="inline"><mi>p</mi></math>,
    we can remove features where the vast majority of observations are one class.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3 Handling Highly Correlated Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have a feature matrix and suspect some features are highly correlated.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use a correlation matrix to check for highly correlated features. If highly
    correlated features exist, consider dropping one of the correlated features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '|  | 0 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 3 | 1 |'
  prefs: []
  type: TYPE_TB
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One problem we often run into in machine learning is highly correlated features.
    If two features are highly correlated, then the information they contain is very
    similar, and it is likely redundant to include both features. In the case of simple
    models like linear regression, failing to remove such features violates the assumptions
    of linear regression and can result in an artificially inflated R-squared value.
    The solution to highly correlated features is simple: remove one of them from
    the feature set. Removing highly correlated features by setting a correlation
    threshold is another example of filtering.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our solution, first we create a correlation matrix of all features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '|  | 0 | 1 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1.000000 | 0.976103 | 0.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.976103 | 1.000000 | -0.034503 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.000000 | -0.034503 | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: 'Second, we look at the upper triangle of the correlation matrix to identify
    pairs of highly correlated features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '|  | 0 | 1 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | NaN | 0.976103 | 0.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | NaN | NaN | 0.034503 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | NaN | NaN | NaN |'
  prefs: []
  type: TYPE_TB
- en: Third, we remove one feature from each of those pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 10.4 Removing Irrelevant Features for Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have a categorical target vector and want to remove uninformative features.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If the features are categorical, calculate a chi-square (<math display="inline"><msup><mi>χ</mi>
    <mn>2</mn></msup></math> ) statistic between each feature and the target vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'If the features are quantitative, compute the ANOVA F-value between each feature
    and the target vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of selecting a specific number of features, we can use `SelectPercentile`
    to select the top *n* percent of features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Chi-square statistics examine the independence of two categorical vectors.
    That is, the statistic is the difference between the observed number of observations
    in each class of a categorical feature and what we would expect if that feature
    were independent (i.e., no relationship) of the target vector:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msup><mi>χ</mi> <mn>2</mn></msup> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover> <mfrac><msup><mrow><mo>(</mo><msub><mi>O</mi>
    <mi>i</mi></msub> <mo>-</mo><msub><mi>E</mi> <mi>i</mi></msub> <mo>)</mo></mrow>
    <mn>2</mn></msup> <msub><mi>E</mi> <mi>i</mi></msub></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math display="inline"><msub><mi>O</mi><mi>i</mi></msub></math> is the
    number of observed observations in class <math display="inline"><mi>i</mi></math>,
    and <math display="inline"><msub><mi>E</mi><mi>i</mi></msub></math> is the number
    of expected observations in class <math display="inline"><mi>i</mi></math>.
  prefs: []
  type: TYPE_NORMAL
- en: A chi-squared statistic is a single number that tells you how much difference
    exists between your observed counts and the counts you would expect if there were
    no relationship at all in the population. By calculating the chi-squared statistic
    between a feature and the target vector, we obtain a measurement of the independence
    between the two. If the target is independent of the feature variable, then it
    is irrelevant for our purposes because it contains no information we can use for
    classification. On the other hand, if the two features are highly dependent, they
    likely are very informative for training our model.
  prefs: []
  type: TYPE_NORMAL
- en: To use chi-squared in feature selection, we calculate the chi-squared statistic
    between each feature and the target vector, then select the features with the
    best chi-square statistics. In scikit-learn, we can use `SelectKBest` to select
    them. The parameter `k` determines the number of features we want to keep and
    filters out the least informative features.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that chi-square statistics can be calculated only between
    two categorical vectors. For this reason, chi-squared for feature selection requires
    that both the target vector and the features are categorical. However, if we have
    a numerical feature we can use the chi-squared technique by first transforming
    the quantitative feature into a categorical feature. Finally, to use our chi-squared
    approach, all values need to be nonnegative.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, if we have a numerical feature, we can use `f_classif` to calculate
    the ANOVA F-value statistic with each feature and the target vector. F-value scores
    examine if, when we group the numerical feature by the target vector, the means
    for each group are significantly different. For example, if we had a binary target
    vector, gender, and a quantitative feature, test scores, the F-value score would
    tell us if the mean test score for men is different than the mean test score for
    women. If it is not, then test score doesn’t help us predict gender and therefore
    the feature is irrelevant.
  prefs: []
  type: TYPE_NORMAL
- en: 10.5 Recursively Eliminating Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to automatically select the best features to keep.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use scikit-learn’s `RFECV` to conduct *recursive feature elimination* (RFE)
    using cross-validation (CV). That is, use the wrapper feature selection method
    and repeatedly train a model, each time removing a feature until model performance
    (e.g., accuracy) becomes worse. The remaining features are the best:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have conducted RFE, we can see the number of features we should keep:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also see which of those features we should keep:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We can even view the rankings of the features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is likely the most advanced recipe in this book up to this point, combining
    a number of topics we have yet to address in detail. However, the intuition is
    straightforward enough that we can address it here rather than holding off until
    a later chapter. The idea behind RFE is to train a model repeatedly, updating
    the *weights* or *coefficients* of that model each time. The first time we train
    the model, we include all the features. Then, we find the feature with the smallest
    parameter (notice that this assumes the features are either rescaled or standardized),
    meaning it is less important, and remove that feature from the feature set.
  prefs: []
  type: TYPE_NORMAL
- en: 'The obvious question then is: how many features should we keep? We can (hypothetically)
    repeat this loop until we only have one feature left. A better approach requires
    that we include a new concept called *cross-validation*. We will discuss CV in
    detail in the next chapter, but here is the general idea.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given data containing (1) a target we want to predict, and (2) a feature matrix,
    first we split the data into two groups: a training set and a test set. Second,
    we train our model using the training set. Third, we pretend that we do not know
    the target of the test set and apply our model to its features to predict the
    values of the test set. Finally, we compare our predicted target values with the
    true target values to evaluate our model.'
  prefs: []
  type: TYPE_NORMAL
- en: We can use CV to find the optimum number of features to keep during RFE. Specifically,
    in RFE with CV, after every iteration we use cross-validation to evaluate our
    model. If CV shows that our model improved after we eliminated a feature, then
    we continue on to the next loop. However, if CV shows that our model got worse
    after we eliminated a feature, we put that feature back into the feature set and
    select those features as the best.
  prefs: []
  type: TYPE_NORMAL
- en: In scikit-learn, RFE with CV is implemented using `RFECV`, which contains a
    number of important parameters. The `estimator` parameter determines the type
    of model we want to train (e.g., linear regression), the `step` parameter sets
    the number or proportion of features to drop during each loop, and the `scoring`
    parameter sets the metric of quality we use to evaluate our model during cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[scikit-learn documentation: Recursive feature elimination with cross-validation](https://oreil.ly/aV-Fz)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
