- en: Chapter 8\. Hands-On Autoencoder
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章 自动编码器实战
- en: In this chapter, we will build applications using various versions of autoencoders,
    including undercomplete, overcomplete, sparse, denoising, and variational autoencoders.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将构建使用各种版本的自动编码器的应用程序，包括欠完备、过完备、稀疏、去噪和变分自动编码器。
- en: To start, let’s return to the credit card fraud detection problem we introduced
    in [Chapter 3](ch03.html#Chapter_3). For this problem, we have 284,807 credit
    card transactions, of which only 492 are fraudulent. Using a supervised model,
    we achieved an average precision of 0.82, which is very impressive. We can find
    well over 80% of the fraud with an over 80% precision. Using an unsupervised model,
    we achieved an average precision of 0.69, which is very good considering we did
    not use labels. We can find over 75% of the fraud with an over 75% precision.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们回到我们在 [第3章](ch03.html#Chapter_3) 中介绍的信用卡欺诈检测问题。对于这个问题，我们有 284,807 笔信用卡交易，其中只有
    492 笔是欺诈性的。使用监督模型，我们实现了平均精度为 0.82，这非常令人印象深刻。我们可以找到超过 80% 的欺诈，并且精度超过 80%。使用无监督模型，我们实现了平均精度为
    0.69，考虑到我们没有使用标签，这也是非常好的。我们可以找到超过 75% 的欺诈，并且精度超过 75%。
- en: Let’s see how this same problem can be solved using an autoencoder, which is
    also an unsupervised algorithm but one that uses a neural network.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用自动编码器来解决同样的问题，它也是一种无监督算法，但使用了神经网络。
- en: Data Preparation
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'Let’s first load the necessary libaries:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先加载必要的库：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, load the dataset and prepare it for use. We will create a `dataX` matrix
    with all the PCA components and the feature `Amount`, but drop `Class` and `Time`.
    We will store the `Class` labels in the `dataY` matrix. We will also scale the
    features in the `dataX` matrix so that all the features have a mean of zero and
    standard deviation of one:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，加载数据集并准备使用。我们将创建一个 `dataX` 矩阵，其中包含所有的PCA成分和特征 `Amount`，但排除 `Class` 和 `Time`。我们将把
    `Class` 标签存储在 `dataY` 矩阵中。我们还将对 `dataX` 矩阵中的特征进行缩放，使所有特征的平均值为零，标准差为一。
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As we did in [Chapter 3](ch03.html#Chapter_3), we will create a training set
    with two-thirds of the data and the labels and a test set with one-third of the
    data and the labels.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在 [第3章](ch03.html#Chapter_3) 中所做的那样，我们将创建一个训练集，其中包含三分之二的数据和标签，并创建一个测试集，其中包含三分之一的数据和标签。
- en: 'Let’s store the training set and the test set as *X_train_AE* and *X_test_AE*,
    respectively. We will use these in the autoencoders soon:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将训练集和测试集分别存储为 *X_train_AE* 和 *X_test_AE*，我们很快将在自动编码器中使用它们。
- en: '[PRE2]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let’s also use reuse the function we introduced earlier in the book, called
    `anomalyScores`, to calculate the reconstruction error between the original feature
    matrix and the newly reconstructed feature matrix. The function takes the sum
    of squared errors and normalizes them to a range between zero and one.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们还要重用本书中早期介绍的函数 `anomalyScores`，来计算原始特征矩阵与新重构特征矩阵之间的重构误差。该函数计算平方误差的总和，并将其归一化到零到一的范围内。
- en: 'This is a crucial function. The transactions with errors close to one are the
    ones that are most anomalous (i.e., have the highest reconstruction error) and,
    therefore, are most likely to be fraudulent. The transactions with errors close
    to zero have the lowest reconstruction error and are most likely to be normal:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个关键的函数。误差接近于一的交易最异常（即具有最高的重构误差），因此最可能是欺诈性的。误差接近于零的交易具有最低的重构误差，最可能是正常的。
- en: '[PRE3]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We will also reuse the function to plot the precision-recall curve, the average
    precision, and the ROC curve. This function is called `plotResults`:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将重用一个名为 `plotResults` 的函数来绘制精确率-召回率曲线、平均精度和ROC曲线。
- en: '[PRE4]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The Components of an Autoencoder
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动编码器的组成部分
- en: First, let’s build a very simple autoencoder with the input layer, a single
    hidden layer, and the output layer. We will feed the original feature matrix *x*
    into the autoencoder—this is represented by the input layer. Then, an activation
    function will be applied to the input layer, generating the hidden layer. This
    activation function is called *f* and represents the *encoder* portion of the
    autoencoder. The hidden layer is called *h* (which is equal to *f(x)*) and represents
    the newly learned representation.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们构建一个非常简单的自动编码器，包括输入层、单隐藏层和输出层。我们将原始特征矩阵 *x* 输入到自动编码器中——这由输入层表示。然后，激活函数将应用于输入层，生成隐藏层。这个激活函数称为
    *f*，代表自动编码器的 *编码器* 部分。隐藏层称为 *h*（等于 *f(x)*），代表新学习到的表示。
- en: Next, an activation function is applied to the hidden layer (i.e., the newly
    learned representation) to reconstruct the original observations. This activation
    function is called *g* and represents the *decoder* portion of the autoencoder.
    The output layer is called *r* (which is equal to *g(h)*) and represents the newly
    reconstructed observations. To calculate the reconstruction error, we will compare
    the newly constructed observations *r* with the original ones *x*.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，激活函数应用于隐藏层（即新学到的表示），以重构原始观测数据。这个激活函数称为*g*，代表自动编码器的*解码器*部分。输出层称为*r*（等于*g(h)*），代表新重构的观测数据。为了计算重构误差，我们将比较新构建的观测数据*r*与原始观测数据*x*。
- en: Activation Functions
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: Before we decide the number of nodes to use in this single hidden layer autoencoder,
    let’s discuss activation functions.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们决定在这个单隐藏层自动编码器中使用的节点数之前，让我们讨论一下激活函数。
- en: A neural network learns the weights to apply to the nodes at each of the layers
    but whether the nodes will be activated or not (for use in the next layer) is
    determined by the activation function. In other words, an activation function
    is applied to the weighted input (plus bias, if any) at each layer. We will call
    the weighted input plus bias *Y*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络学习在每个层的节点上应用的权重，但节点是否激活（用于下一层）由激活函数决定。换句话说，激活函数应用于每层的加权输入（加上偏置，如果有的话）。我们将加权输入加偏置称为*Y*。
- en: 'The activation function takes in *Y* and either activates (if *Y* is above
    a certain threshold) or does not. If activated, the information in a given node
    is passed to the next layer; otherwise, it is not. However, we do not want simple
    binary activations. Instead, we want a range of activation values. To do this,
    we can choose a linear activation function or a nonlinear activation function.
    The linear activation function is unbounded. It can generate activation values
    between negative infinity and positive infinity. Common nonlinear activation functions
    include sigmoid, hyperbolic tangent (or tanh for short), rectified linear unit
    (or ReLu for short), and softmax:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数接收*Y*，如果*Y*超过某个阈值，则激活；否则，不激活。如果激活，则给定节点中的信息传递到下一层；否则，不传递。但是，我们不希望简单的二进制激活。相反，我们希望一系列激活值。为此，我们可以选择线性激活函数或非线性激活函数。线性激活函数是无界的。它可以生成介于负无穷到正无穷之间的激活值。常见的非线性激活函数包括sigmoid、双曲正切（或简称tanh）、修正线性单元（或简称ReLu）和softmax：
- en: Sigmoid function
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid函数
- en: The sigmoid function is bounded and can generate activation values between zero
    and one.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid函数是有界的，并且可以生成介于零和一之间的激活值。
- en: Tanh function
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Tanh函数
- en: The tanh function is also bounded and can generate activation values between
    negative one and positive one. Its gradient is steeper than that of the sigmoid
    function.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: tanh函数也是有界的，并且可以生成介于负一到正一之间的激活值。其梯度比sigmoid函数更陡。
- en: ReLu function
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ReLu函数
- en: The ReLu function has an interesting property. If *Y* is positive, ReLu will
    return *Y*. Otherwise, it will return zero. Therefore, ReLu is unbounded for positive
    values of *Y*.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ReLu函数具有一个有趣的性质。如果*Y*是正的，ReLu将返回*Y*。否则，将返回零。因此，对于正值的*Y*，ReLu是无界的。
- en: Softmax function
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax函数
- en: The softmax function is used as the final activation function in a neural network
    for classification problems because it normalizes classification probabilities
    to values that add up to a probability of one.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: softmax函数用作神经网络中分类问题的最终激活函数，因为它将分类概率归一化为总和为一的值。
- en: Of all these functions, the linear activation function is the simplest and least
    computationally expensive. ReLu is the next least computationally expensive, followed
    by the others.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些函数中，线性激活函数是最简单且计算开销最小的。ReLu是接下来计算开销第二小的，其它则依次类推。
- en: Our First Autoencoder
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们的第一个自动编码器
- en: Let’s start with a two-layer autoencoder with a linear activation function for
    both the encoder and the decoder functions. Note that only the number of hidden
    layers plus the output layer count toward the *number of layers* in a neural network.
    Since we have a single hidden layer, this is known as a two-layer neural network.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个具有线性激活函数的两层自动编码器开始。请注意，只有隐藏层的数量加上输出层计入神经网络的*层数*。由于我们有一个隐藏层，因此这被称为两层神经网络。
- en: To build this using TensorFlow and Keras, we must first call the *Sequential
    model API*. The Sequential model is a linear stack of layers, and we will pass
    the types of layers we want into the model before we compile the model and train
    on our data.^([1](ch08.html#idm140637541443984))
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 TensorFlow 和 Keras 构建这一过程，我们首先需要调用*Sequential model API*。Sequential 模型是层的线性堆叠，在编译模型并在数据上进行训练之前，我们将把我们想要的层类型传递到模型中。^([1](ch08.html#idm140637541443984))
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Once we call the Sequential model, we then need to specify the input shape by
    designating the number of dimensions, which should match the number of dimensions
    in the original feature matrix, *dataX*. This number is 29.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们调用了 Sequential 模型，我们接下来需要指定输入形状，即指定与原始特征矩阵*dataX*中维度数量相匹配的维度数，这个数字是29。
- en: We also need to specify the activation function (also known as the encoder function)
    applied to the input layer and the number of nodes we want the hidden layer to
    have. We will pass *linear* as the activation function.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要指定应用于输入层的激活函数（也称为编码器函数）以及我们希望隐藏层具有的节点数。我们将使用*linear*作为激活函数。
- en: 'To start, let’s use a complete autoencoder, where the number of nodes in the
    hidden layer equals the number of nodes in the input layer, which is 29\. All
    of this is done using a single line of code:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们使用一个完整的自编码器，其中隐藏层中的节点数等于输入层中的节点数，即29。所有这些都可以使用一行代码完成：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Similarly, we need to specify the activation function (also known as the decoder
    function) applied to the hidden layer to reconstruct the observations and the
    number of dimensions we want the output layer to have. Since we want the final
    reconstructed matrix to have the same dimensions as the original matrix, the dimension
    needs to be 29\. And, we will use a linear activation function for the decoder,
    too:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们需要指定应用于隐藏层的激活函数（也称为解码器函数），以重构观察结果，并且我们希望输出层具有的维数。由于我们希望最终重构的矩阵与原始矩阵具有相同的维度，维数需要为29。此外，我们还将在解码器中使用线性激活函数：
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Next, we will need to compile the layers we have designed for the neural network.
    This requires us to select a *loss function* (also known as the *objective function*)
    to guide the learning of the weights, an *optimizer* to set the process by which
    the weights are learned, and a list of *metrics* to output to help us evaluate
    the goodness of the neural network.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要编译我们为神经网络设计的层。这需要我们选择一个*损失函数*（也称为*目标函数*）来指导权重的学习，一个*优化器*来设定权重学习的过程，并列出一系列*度量标准*以帮助我们评估神经网络的好坏。
- en: Loss Function
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: Let’s start with the loss function. Recall that we are evaluating the model
    based on the reconstruction error between the newly reconstructed matrix of features
    based on the autoencoder and the original feature matrix that we feed into the
    autoencoder.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从损失函数开始。回想一下，我们根据自编码器基于重构后的特征矩阵与我们输入自编码器的原始特征矩阵之间的重构误差来评估模型。
- en: Therefore, we want to use *mean squared error* as the evaluation metric. (For
    our custom evaluation function, we use sum of squared errors, which is similar.).^([2](ch08.html#idm140637541363344))
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们希望将*均方误差*作为评估指标。（对于我们自定义的评估函数，我们使用平方误差之和，这类似。）^([2](ch08.html#idm140637541363344))
- en: Optimizer
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化器
- en: Neural networks train for many rounds (known as *epochs*). In each of these
    epochs, the neural network readjusts its learned weights to reduce its loss from
    the previous epoch. The process for learning these weights is set by the optimizer.
    We want a process that helps the neural network efficiently learn the optimal
    weights for the various nodes across all the layers that minimizes the loss function
    we have chosen.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络训练多个回合（称为*epochs*）。在每个回合中，神经网络调整其学习的权重，以减少与上一个回合相比的损失。设置学习这些权重的过程由优化器决定。我们希望找到一个过程，帮助神经网络高效地学习各层节点的最佳权重，从而最小化我们选择的损失函数。
- en: To learn the optimal weights, the neural network needs to adjust its “guess”
    for the optimal weights in an intelligent way. One approach is to iteratively
    move the weights in the direction that helps reduce the loss function incrementally.
    But an even better approach is to move the weights in this direction but with
    a degree of randomness—in other words, to move the weights stochastically.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要学习最佳权重，神经网络需要智能地调整其对最佳权重的“猜测”。一种方法是迭代地朝着有助于逐步减少损失函数的方向移动权重。但更好的方法是以一定的随机性朝着这个方向移动权重，换句话说，随机地移动权重。
- en: Although there is more to this, this process is known as *stochastic gradient
    descent* (or SGD for short), the most commonly used optimizer in training neural
    networks.^([3](ch08.html#idm140637541313568)) SGD has a single learning rate,
    known as *alpha*, for all the weight updates that it makes, and this learning
    rate does not change during training. However, in most cases, it’s better to adjust
    the learning rate over the course of the training. For example, in the earlier
    epochs, it makes more sense to adjust the weights by a large degree—in other words,
    to have a large learning rate or alpha.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管还有更多内容，这个过程被称为*随机梯度下降*（或简称SGD），是训练神经网络中最常用的优化器。^([3](ch08.html#idm140637541313568))
    SGD具有一个称为*alpha*的单一学习率，用于所有权重更新，而这个学习率在训练过程中不会改变。然而，在大多数情况下，调整学习率是更好的选择。例如，在早期的epochs中，通过较大的程度调整权重更为合理，换句话说，具有较大的学习率或alpha。
- en: In later epochs, when the weights are more optimal, it makes more sense to adjust
    the weights by a small degree to delicately fine-tune the weights than to take
    massive steps in one direction or another. Therefore, an even better optimzer
    than SGD is the *Adam optimization algorithm*, which is derived from adaptive
    moment estimation. The Adam optimizer dynamically adjusts the learning rate over
    the course of the training process, unlike SGD, and is the optimizer we will use.^([4](ch08.html#idm140637541306896))
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续的epochs中，当权重更加优化时，微调权重的程度比单向或另一方向上的大步调整更为合理。因此，比SGD更好的优化器是*Adam优化算法*，它源自自适应矩估计。Adam优化器动态调整学习率，而不像SGD那样在训练过程中保持不变，并且这是我们将使用的优化器。^([4](ch08.html#idm140637541306896))
- en: For this optimizer, we can set the alpha, which sets the pace at which weights
    are updated. Larger alpha values result in faster initial learning before the
    learning rate is updated.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个优化器，我们可以设置α，这决定了权重更新的速度。较大的α值在更新学习率之前会导致更快的初始学习速度。
- en: Training the Model
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: Finally, we need to choose the evaluation metric, which we will set to `accuracy`
    to keep things simple:^([5](ch08.html#idm140637541302672))
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要选择评估指标，我们将其设置为`accuracy`以保持简单：^([5](ch08.html#idm140637541302672))
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Next, we need to select the number of epochs and the batch size and then begin
    the training process by calling the method *fit*. The number of epochs determines
    the number of times the training occurs over the entire dataset we pass into the
    neural network. We will set this to 10 to start.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要选择epoch数量和批次大小，然后通过调用*fit*方法开始训练过程。epoch数量决定了整个传递到神经网络中的数据集训练次数。我们将这个设置为10来开始。
- en: The batch sets the number of samples the neural network trains on before making
    the next gradient update. If the batch is equal to the total number of observations,
    the neural network will make a gradient update once every epoch. Otherwise, it
    will make updates multiple times per epoch. We will set this to a generic 32 samples
    to start.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 批次设置了神经网络在进行下一个梯度更新之前训练的样本数量。如果批次等于观察的总数，神经网络将每个epoch仅进行一次梯度更新。否则，它将在每个epoch中进行多次更新。我们将这个设置为通用的32个样本来开始。
- en: Into the fit method, we will pass in the initial input matrix, *x*, and the
    target matrix, *y*. In our case, both *x* and *y* will be the original feature
    matrix, *X_train_AE*, because we want to compare the output of the autoencoder—the
    reconstructed feature matrix—with the original feature matrix to calculate the
    reconstruction error.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在fit方法中，我们将传入初始输入矩阵*x*和目标矩阵*y*。在我们的案例中，*x*和*y*都将是原始特征矩阵*X_train_AE*，因为我们希望比较自编码器的输出——重构特征矩阵与原始特征矩阵来计算重构误差。
- en: 'Remember, this is a purely unsupervised solution so we will not use the *y*
    matrix at all. We will also validate our model as we go by testing the reconstruction
    error on the entire training matrix:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，这是一个完全无监督的解决方案，所以我们根本不会使用*y*矩阵。我们将在整个训练矩阵上测试重构误差来验证我们的模型：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Since this a complete autoencoder—where the hidden layer has the same number
    of dimensions as the input layer—the loss is very low, for both the training and
    validation sets:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个完整的自编码器——隐藏层与输入层具有相同的维数，因此对于训练集和验证集，损失都非常低：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This is not optimal—the autoencoder has reconstructed the original feature matrix
    too precisely, memorizing the inputs.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是最优的——自编码器对原始特征矩阵进行了过于精确的重构，记住了输入。
- en: Recall that the autoencoder is meant to learn a new representation that captures
    the most salient information in the original input matrix while dropping the less
    relevant information. Simply memorizing the inputs—also known as learning the
    *identity function*—will not result in new and improved representation learning.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 请回想一下，自编码器旨在学习一个新的表示，捕捉原始输入矩阵中最显著的信息，同时丢弃不太相关的信息。简单地记忆输入——也称为学习*恒等函数*——不会带来新的和改进的表示学习。
- en: Evaluating on the Test Set
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在测试集上评估
- en: 'Let’s use the test set to evaluate just how successively this autoencoder can
    identify fraud in the credit card transactions dataset. We will use the `predict`
    method to do this:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用测试集来评估这个自编码器在识别信用卡交易中的欺诈问题上的成功程度。我们将使用`predict`方法来完成这个任务：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As seen in [Figure 8-1](#evaluation_metrics_of_complete_autoencoder), the average
    precision is 0.30, which is not very good. The best average precision using unsupervised
    learning from [Chapter 4](ch04.html#Chapter_4) was 0.69, and the supervised system
    had an average precision of 0.82\. However, each training process will yield slightly
    different results for the trained autoencoder, so you may not see the same performance
    for your run.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图8-1](#evaluation_metrics_of_complete_autoencoder)所示，平均精度为0.30，这并不是很好的结果。在第4章的无监督学习中，使用无监督学习的最佳平均精度为0.69，有监督系统的平均精度为0.82。然而，每次训练过程将为训练后的自编码器产生略有不同的结果，因此您可能不会在您的运行中看到相同的性能。
- en: To get a better sense of how a two-layer complete autoencoder performs on the
    test set, let’s run this training process ten separate times and store the average
    precision on the test set for each run. We will assess how good this complete
    autoencoder is at capturing fraud based on the mean of the average precision from
    these 10 runs.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地了解两层完整自编码器在测试集上的表现，让我们分别运行这个训练过程十次，并存储每次运行在测试集上的平均精度。我们将根据这10次运行的平均精度来评估这个完整自编码器在捕捉欺诈方面的能力。
- en: '![Evaluation Metrics of Complete Autoencoder](assets/hulp_0801.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![完整自编码器的评估指标](assets/hulp_0801.png)'
- en: Figure 8-1\. Evaluation metrics of complete autoencoder
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 第8-1图。完整自编码器的评估指标
- en: 'To consolidate our work thus far, here is the code to simulate 10 runs from
    start to finish:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结我们迄今为止的工作，这里是从头到尾模拟10次运行的代码：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The following code summarizes the results for the 10 runs. The mean of the average
    precision is 0.30, but the average precision ranges from a low of 0.02 to .72\.
    The *coefficient of variation* (defined as the standard deviation divided by the
    mean over 10 runs) is 0.88.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码总结了这10次运行的结果。平均精度为0.30，但平均精度从0.02到0.72不等。*变异系数*（定义为10次运行中标准差除以平均值）为0.88。
- en: '[PRE13]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Let’s try to improve our results by building variations of this autoencoder.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试通过构建这个自编码器的变种来改进我们的结果。
- en: Two-Layer Undercomplete Autoencoder with Linear Activation Function
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 具有线性激活函数的两层欠完整自编码器
- en: Let’s try an undercomplete autoencoder rather than a complete one.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一个欠完整自编码器，而不是完整的自编码器。
- en: Compared to the previous autoencoder, the only thing that changes is the number
    of nodes in the hidden layer. Instead of setting this to the number of original
    dimensions (29), we will set the nodes to 20\. In other words, this autoencoder
    is a constrained autoencoder. The encoder function is forced to capture the information
    in the input layer with a fewer number of nodes, and the decoder has to take this
    new representation to reconstruct the original matrix.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 与先前的自编码器相比，唯一变化的是隐藏层中节点的数量。不再将其设置为原始维度的数量（29），我们将节点数设置为20。换句话说，这个自编码器是一个受限制的自编码器。编码器函数被迫用较少的节点捕捉输入层中的信息，解码器则必须将这个新的表示用于重构原始矩阵。
- en: 'We should expect the loss here to be higher compared to that of the complete
    autoencoder. Let’s run the code. We will perform 10 independent runs to test how
    well the various undercomplete autoencoders are at catching fraud:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该预期这里的损失比完整自编码器的损失更高。让我们运行代码。我们将执行10次独立运行，以测试各种欠完整自编码器在捕捉欺诈方面的表现：
- en: '[PRE14]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As the following shows, the losses of the undercomplete autoencoder are considerably
    higher than those of the complete autoencoder. It is clear that the autoencoder
    learns a representation that is new and more constrained than the original input
    matrix—the autoencoder did not simply memorize the inputs:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，欠完整自编码器的损失远高于完整自编码器的损失。显然，自编码器学习了一个比原始输入矩阵更加新颖和受限制的表示——自编码器并非简单地记忆输入：
- en: '[PRE15]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This is how an autoencoder should work—it should learn a new representation.
    [Figure 8-2](#evaluation_metrics_of_undercomplete_autoencoder_with_20_nodes) shows
    how effective this new representation is at identifying fraud.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这是自编码器应该工作的方式——它应该学习一个新的表示。[图 8-2](#evaluation_metrics_of_undercomplete_autoencoder_with_20_nodes)
    显示了这种新表示在识别欺诈方面的有效性。
- en: '![Evaluation Metrics of Undercomplete Autoencoder with 20 Nodes](assets/hulp_0802.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![使用20个节点的欠完全自编码器的评估指标](assets/hulp_0802.png)'
- en: Figure 8-2\. Evaluation metrics of undercomplete autoencoder with 20 nodes
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-2\. 使用20个节点的欠完全自编码器的评估指标
- en: The average precision is 0.29, similar to that of the complete autoencoder.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 平均精度为0.29，与完全自编码器的类似。
- en: The following code shows the distribution of average precisions across the 10
    runs. The mean of the average precision is 0.31, but the dispersion is very tight
    (as the coefficient of variation 0.03 indicates). This is a considerably more
    stable system than the one designed with a complete autoencoder.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码显示了10次运行中平均精度的分布。平均精度的均值为0.31，但离散度非常小（如0.03的离散系数所示）。这比使用完全自编码器设计的系统稳定得多。
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: But we are still stuck at a fairly mediocre average precision. Why did the undercomplete
    autoencoder not perform better? It could be that this undercomplete autoencoder
    does not have enough nodes. Or, maybe we need to train using more hidden layers.
    Let’s experiment with these two changes, one by one.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们仍然陷入相当平庸的平均精度。为什么欠完全自编码器表现不佳呢？可能是因为这个欠完全自编码器节点不够。或者，我们可能需要使用更多隐藏层进行训练。让我们逐个尝试这两种变化。
- en: Increasing the Number of Nodes
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增加节点数量
- en: 'The following code displays the training losses when using a two-layer undercomplete
    autocoder with 27 nodes instead of just 20:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码显示了使用27个节点的两层欠完全自编码器的训练损失：
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[Figure 8-3](#evaluation_metrics_of_undercomplete_autoencoder_with_27_nodes)
    displays the average precision, precision-recall curve, and auROC curve.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8-3](#evaluation_metrics_of_undercomplete_autoencoder_with_27_nodes) 展示了平均精度、精确率-召回率曲线和auROC曲线。'
- en: '![Evaluation Metrics of Undercomplete Autoencoder with 27 Nodes](assets/hulp_0803.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![使用27个节点的欠完全自编码器的评估指标](assets/hulp_0803.png)'
- en: Figure 8-3\. Evaluation metrics of undercomplete autoencoder with 27 nodes
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-3\. 使用27个节点的欠完全自编码器的评估指标
- en: The average precision improves considerably to 0.70\. This is better than the
    average precision of the complete autoencoder and better than the best unsupervised
    learning solution from [Chapter 4](ch04.html#Chapter_4).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 平均精度显著提高至0.70\. 这比完全自编码器的平均精度更好，也比[第四章](ch04.html#Chapter_4)中最佳的无监督学习解决方案更好。
- en: The following code summarizes the distribution of average precision across the
    10 runs. The mean of the average precision is 0.53, considerably better than the
    ~0.30 average precision earlier. The dispersion of average precision is reasonably
    good, with a coefficient of variation of 0.50.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码总结了10次运行中平均精度的分布。平均精度的均值为0.53，比之前的约0.30平均精度好得多。平均精度的离散度也相当好，离散系数为0.50。
- en: '[PRE18]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We have a clear improvement over our previous autoencoder-based anomaly detection
    system.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在先前基于自编码器的异常检测系统上有了明显改进。
- en: Adding More Hidden Layers
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加更多隐藏层
- en: Let’s see if we can improve our results by adding an extra hidden layer to the
    autoencoder. We will continue to use linear activation functions for now.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看通过向自编码器添加额外的隐藏层是否可以改善我们的结果。目前我们将继续使用线性激活函数。
- en: Note
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Experimentation is a major part of discovering the best neural network architecture
    for the problem you have to solve. Some changes you make will lead to better results,
    others to worse. Knowing how to modify the neural network and the hyperparameters
    as part of your search to improve the solution is very important.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 实验是发现解决问题的最佳神经网络架构的重要组成部分。您所做的一些更改会带来更好的结果，而另一些则会带来更糟糕的结果。了解如何在搜索过程中修改神经网络和超参数以改进解决方案是非常重要的。
- en: Instead of a single hidden layer with 27 nodes, we will use one hidden layer
    with 28 nodes and another with 27 nodes. This is only a slight variation from
    the one we used previously. This is now a three-layer neural network since we
    have two hidden layers plus the output layer. The input layer does not “count”
    toward this number.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将不再使用27个节点的单隐藏层，而是使用一个28个节点的隐藏层和一个27个节点的隐藏层。这只是与先前使用的稍微不同。由于我们有两个隐藏层加上输出层，所以现在是一个三层神经网络。输入层不算在这个数目中。
- en: 'This additional hidden layer requires just one additional line of code, as
    shown here:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这个额外的隐藏层只需要添加一行代码，如下所示：
- en: '[PRE19]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The following code summarizes the distribution of average precisions across
    the 10 runs. The mean of the average precision is 0.36, worse than the 0.53 we
    just achieved. The dispersion of average precision is also worse, with a coefficient
    of variation of 0.94 (higher is worse):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码总结了10次运行中平均精度的分布。平均精度的平均值为0.36，比刚刚取得的0.53还要差。平均精度的离散度也更差，变异系数为0.94（越高越差）：
- en: '[PRE20]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Nonlinear Autoencoder
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非线性自编码器
- en: Now let’s build an undercomplete autoencoder using a nonlinear activation function.
    We will use ReLu, but you are welcome to experiment with tanh, sigmoid, and the
    other nonlinear activation functions.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用非线性激活函数来构建一个欠完备自编码器。我们将使用ReLu，但您也可以尝试tanh、sigmoid和其他非线性激活函数。
- en: 'We will include three hidden layers, with 27, 22, and 27 nodes, respectively.
    Conceptually, the first two activation functions (applied on the input and first
    hidden layer) perform the encoding, creating the second hidden layer with 22 nodes.
    Then, the next two activation functions perform the decoding, reconstructing the
    22-node representation to the original number of dimensions, 29:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将包含三个隐藏层，分别有27、22和27个节点。在概念上，前两个激活函数（应用于输入和第一个隐藏层）执行编码，创建具有22个节点的第二个隐藏层。然后，接下来的两个激活函数执行解码，将22节点的表示重构为原始维度的数量，即29：
- en: '[PRE21]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The following code shows the losses from this autoencoder, and [Figure 8-4](#evaluation_metrics_of_undercomplete_autoencoder_with_three_hidden_layers_and_relu_activation_function)
    shows the average precision, the precision-recall curve, and the auROC curve:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码显示了这个自编码器的损失，而[图 8-4](#evaluation_metrics_of_undercomplete_autoencoder_with_three_hidden_layers_and_relu_activation_function)显示了平均精度、精确率-召回率曲线和auROC曲线：
- en: '[PRE22]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![Evaluation Metrics of Undercomplete Autoencoder with Three Hidden Layers
    and ReLu Activation Function](assets/hulp_0804.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![三层隐藏层和ReLu激活函数下的欠完备自编码器评估指标](assets/hulp_0804.png)'
- en: Figure 8-4\. Evaluation metrics of undercomplete autoencoder with three hidden
    layers and ReLu activation function
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-4\. 三层隐藏层和ReLu激活函数下的欠完备自编码器评估指标
- en: The results are considerably worse.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显著更差。
- en: 'The following code summarizes the distribution of average precisions across
    the 10 runs. The mean of the average precision is 0.22, worse than the 0.53 we
    achieved earlier. The dispersion of average precisions is very tight, with a coefficient
    of variation of 0.06:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码总结了10次运行中平均精度的分布。平均精度的平均值为0.22，比之前的0.53要差。平均精度的离散度非常小，变异系数为0.06：
- en: '[PRE23]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: These results are much worse than those from a simple autoencoder using a linear
    activation function. It could be that—for this dataset—a linear, undercomplete
    autoencoder is the best solution.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果比使用线性激活函数的简单自编码器要糟糕得多。也许对于这个数据集来说，一个线性的、欠完备的自编码器是最佳解决方案。
- en: For other datasets, that may not always be the case. As always, experimentation
    is required to find the optimal solution. Change the number of nodes, the number
    of hidden layers, and the mix of activation functions, and see how much better
    or worse the solutions become.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他数据集，情况可能并非总是如此。和往常一样，需要进行实验以找到最优解。改变节点数、隐藏层数和激活函数的组合，看看解决方案变得更好或更差了多少。
- en: This type of experimentation is known as *hyperparameter optimization*. You
    are adjusting the hyperparameters—the number of nodes, the number of layers, and
    the mix of activation functions—in search of the optimal solution.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的实验被称为*超参数优化*。您正在调整超参数——节点数、隐藏层数和激活函数的组合，以寻找最优解。
- en: Overcomplete Autoencoder with Linear Activation
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 具有线性激活的过完备自编码器
- en: Now let’s highlight the problem with overcomplete autoencoders. Overcomplete
    autoencoders have more nodes in the hidden layer than either the input or output
    layer. Because the *capacity* of the neural network model is so high, the autoencoder
    simply memorizes the observations it trains on.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来强调一下过完备自编码器的问题。过完备自编码器的隐藏层中的节点数比输入层或输出层都要多。由于神经网络模型的*容量*非常高，自编码器只是简单地记忆训练过的观测结果。
- en: In other words, the autoencoder learns the *identity function*, which is exactly
    what we want to avoid. The autoencoder will overfit the training data and will
    perform very poorly in separating fraudulent credit card transactions from normal
    ones.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，自编码器学习了*恒等函数*，这正是我们想要避免的。自编码器会对训练数据过拟合，并且在区分欺诈信用卡交易和正常交易方面表现非常差。
- en: Recall that we need the autoencoder to learn the salient aspects of the credit
    card transactions in the training set so that it learns what the normal transactions
    look like—without memorizing the information in the less normal, rare fraudulent
    ones.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们需要自编码器在训练集中学习信用卡交易的显著特征，这样它才能学习到正常交易的样子，而不是死记硬背不太正常和稀少的欺诈交易的信息。
- en: 'Only if the autoencoder is able to lose some of the information in the training
    set will it be able to separate the fraudulent transactions from the normal ones:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当自编码器能够丢失一些训练集中的信息时，它才能够分离欺诈交易和正常交易：
- en: '[PRE24]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The following code shows the losses from this autoencoder, and [Figure 8-5](#evaluation_metrics_of_overcomplete_autoencoder_with_single_hidden_layer_and_linear_activation_function)
    displays the average precision, the precision-recall curve, and the auROC curve:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码显示了这个自编码器的损失，并且[图8-6](#evaluation_metrics_of_overcomplete_autoencoder_with_single_hidden_layer_dropout_and_linear_activation_function)显示了平均精度、精确率-召回率曲线和auROC曲线：
- en: '[PRE25]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![Evaluation Metrics of Overcomplete Autoencoder with Single Hidden Layer and
    Linear Activation Function](assets/hulp_0805.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![单隐藏层和线性激活函数的过度完备自编码器的评估指标](assets/hulp_0805.png)'
- en: Figure 8-5\. Evaluation metrics of overcomplete autoencoder with single hidden
    layer and linear activation function
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-5. 单隐藏层、线性激活函数的过度完备自编码器的评估指标
- en: As expected, the losses are very low, and the overfit overcomplete autoencoder
    has very poor performance in detecting the fraudulent credit card transactions.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，损失非常低，而且过度完备的自编码器在检测欺诈信用卡交易方面表现非常糟糕。
- en: 'The following code summarizes the distribution of average precision across
    the 10 runs. The mean of the average precision is 0.31, worse than the 0.53 we
    achieved earlier. The dispersion of average precision is not very tight, with
    a coefficient of variation of 0.89:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码总结了10次运行中平均精度的分布。平均精度的均值为0.31，比我们之前实现的0.53要差。平均精度的离散度不是很紧，变异系数为0.89：
- en: '[PRE26]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Overcomplete Autoencoder with Linear Activation and Dropout
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用线性激活和丢弃的过度完备自编码器
- en: One way to improve the overcomplete autoencoder solution is to use a regularization
    technique to reduce the overfitting. One such technique is known as *dropout*.
    With dropout, we force the autoencoder to drop out some defined percentage of
    units from the layers in the neural network.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 改进过度完备自编码器解决方案的一种方法是使用正则化技术来减少过拟合。其中一种技术被称为*丢弃*。使用丢弃时，我们强制自编码器从神经网络中的层中丢弃一定百分比的单元。
- en: With this new constraint, the overcomplete autoencoder cannot simply memorize
    the credit card transactions in the training set. Instead, the autoencoder has
    to generalize a bit more. The autoencoder is forced to learn more of the salient
    features in the dataset and lose some of the less salient information.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个新的约束条件，过度完备自编码器就不能简单地记住训练集中的信用卡交易了。相反，自编码器必须更多地进行泛化。自编码器被迫学习数据集中更显著的特征，并丢失一些不太显著的信息。
- en: We will use a dropout percentage of 10%, which we will apply to the hidden layer.
    In other words, 10% of the neurons are dropped. The higher the dropout percentage,
    the stronger the regularization. This is done with just a single additional line
    of code.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用10%的丢弃率，将其应用于隐藏层。换句话说，10%的神经元会被丢弃。丢弃率越高，正则化效果越强。这只需要一行额外的代码。
- en: 'Let’s see if this improves the results:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这是否能改善结果：
- en: '[PRE27]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The following code shows the losses from this autoencoder, and [Figure 8-6](#evaluation_metrics_of_overcomplete_autoencoder_with_single_hidden_layer_dropout_and_linear_activation_function)
    displays the average precision, the precision-recall curve, and the auROC curve:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码显示了这个自编码器的损失，并且[图8-6](#evaluation_metrics_of_overcomplete_autoencoder_with_single_hidden_layer_dropout_and_linear_activation_function)显示了平均精度、精确率-召回率曲线和auROC曲线：
- en: '[PRE28]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![Evaluation Metrics of Overcomplete Autoencoder with Single Hidden Layer,
    Dropout, and Linear Activation Function](assets/hulp_0806.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![单隐藏层、丢弃和线性激活函数的过度完备自编码器的评估指标](assets/hulp_0806.png)'
- en: Figure 8-6\. Evaluation metrics of overcomplete autoencoder with single hidden
    layer, dropout, and linear activation function
  id: totrans-146
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-6\. 具有单隐藏层、丢弃率和线性激活函数的过完备自编码器的评估指标
- en: As expected, the losses are very low, and the overfit overcomplete autoencoder
    has very poor performance in detecting the fraudulent credit card transactions.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，损失非常低，而且过拟合的过完备自编码器在检测欺诈信用卡交易方面表现非常差。
- en: 'The following code summarizes the distribution of average precision across
    the 10 runs. The mean of the average precision is 0.21, worse than the 0.53 we
    achieved earlier. The coefficient of variation is 0.40:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码总结了在 10 次运行中平均精度的分布。平均精度的均值为 0.21，比我们之前达到的 0.53 差。变异系数为 0.40：
- en: '[PRE29]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Sparse Overcomplete Autoencoder with Linear Activation
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 具有线性激活的稀疏过完备自编码器
- en: Another regularization technique is *sparsity*. We can force the autoencoder
    to take the sparsity of the matrix into consideration such that the majority of
    the autoencoder’s neurons are inactive most of the time—in other words, they do
    not fire. This makes it harder for the autoencoder to memorize the identity function
    even when the autoencoder is overcomplete because most of the nodes cannot fire
    and, therefore, cannot overfit the observations as easily.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种正则化技术是*稀疏性*。我们可以强制自编码器考虑矩阵的稀疏性，使得大多数自编码器的神经元大部分时间处于非活跃状态——换句话说，它们不会激活。这使得即使自编码器是过完备的，也更难记忆恒等函数，因为大多数节点无法激活，因此不能像以前那样轻易地过拟合观察结果。
- en: We will use a single hidden layer overcomplete autoencoder with 40 nodes like
    before but with just the sparsity penalty, not dropout.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与之前相同的单隐藏层过完备自编码器，有 40 个节点，但只有稀疏性惩罚，而没有丢弃。
- en: 'Let’s see if the results improve from the 0.21 average precision we had earlier:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看结果是否从之前的 0.21 平均精度有所提高：
- en: '[PRE30]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The following code shows the losses from this autoencoder, and [Figure 8-7](#evaluation_metrics_of_sparse_overcomplete_autoencoder_with_single_hidden_layer_and_linear_activation_function)
    displays the average precision, the precision-recall curve, and the auROC curve:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了这个自编码器的损失，而[图 8-7](#evaluation_metrics_of_sparse_overcomplete_autoencoder_with_single_hidden_layer_and_linear_activation_function)则展示了平均精度、精确-召回曲线和
    auROC 曲线：
- en: '[PRE31]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![Evaluation Metrics of Sparse Overcomplete Autoencoder with Single Hidden
    Layer and Linear Activation Function](assets/hulp_0807.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![具有单隐藏层和线性激活函数的稀疏过完备自编码器的评估指标](assets/hulp_0807.png)'
- en: Figure 8-7\. Evaluation metrics of sparse overcomplete autoencoder with single
    hidden layer and linear activation function
  id: totrans-158
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-7\. 具有单隐藏层和线性激活函数的稀疏过完备自编码器的评估指标
- en: 'The following code summarizes the distribution of average precision across
    the 10 runs. The mean of the average precision is 0.21, worse than the 0.53 we
    achieved earlier. The coefficient of variation is 0.99:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码总结了在 10 次运行中平均精度的分布。平均精度的均值为 0.21，比我们之前达到的 0.53 差。变异系数为 0.99：
- en: '[PRE32]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Sparse Overcomplete Autoencoder with Linear Activation and Dropout
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 具有线性激活和丢弃的稀疏过完备自编码器
- en: 'Of course, we can combine the regularization techniques to improve the solution.
    Here is a sparse overcomplete autoencoder with linear activation, 40 nodes in
    the single hidden layer, and dropout of 5%:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们可以结合正则化技术来改善解决方案。这里是一个具有线性激活、单隐藏层中有 40 个节点和 5% 丢弃率的稀疏过完备自编码器：
- en: '[PRE33]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The following training data shows the losses from this autoencoder, and [Figure 8-8](#evaluation_metrics_of_sparse_overcomplete_autoencoder_with_single_hidden_layer_dropout_and_linear_activation)
    displays the average precision, the precision-recall curve, and the auROC curve:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 以下训练数据显示了这个自编码器的损失，而[图 8-8](#evaluation_metrics_of_sparse_overcomplete_autoencoder_with_single_hidden_layer_dropout_and_linear_activation)则展示了平均精度、精确-召回曲线和
    auROC 曲线：
- en: '[PRE34]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![Evaluation Metrics of Sparse Overcomplete Autoencoder with Single Hidden
    Layer, Dropout, and Linear Activation Function](assets/hulp_0808.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![具有单隐藏层、丢弃率和线性激活函数的稀疏过完备自编码器的评估指标](assets/hulp_0808.png)'
- en: Figure 8-8\. Evaluation metrics of sparse overcomplete autoencoder with single
    hidden layer, dropout, and linear activation function
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-8\. 具有单隐藏层、丢弃率和线性激活函数的稀疏过完备自编码器的评估指标
- en: 'The following code summarizes the distribution of average precision across
    the 10 runs. The mean of the average precision is 0.24, worse than the 0.53 we
    achieved earlier. The coefficient of variation is 0.62:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码总结了在 10 次运行中平均精度的分布。平均精度的均值为 0.24，比我们之前达到的 0.53 差。变异系数为 0.62：
- en: '[PRE35]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Working with Noisy Datasets
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理嘈杂数据集
- en: A common problem with real-world data is noisiness data is often distorted in
    some way because of data quality issues from data capture, data migration, data
    transformation, etc. We need autoencoders to be robust enough against such noise
    so that they are not fooled and can learn from the truly important underlying
    structure in the data.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 实际数据的一个常见问题是数据的嘈杂性，数据通常因为数据捕获、数据迁移、数据转换等问题而畸变。我们需要自编码器足够健壮，以便不被这种噪声所迷惑，并能够从数据中学习到真正重要的潜在结构。
- en: 'To simulate this noise, let’s add a Gaussian random matrix of noise to our
    credit card transactions dataset and then train an autoencoder on this noisy training
    set. Then, we will see how well the autoencoder does in predicting fraud on the
    noisy test set:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟这种噪声，让我们向我们的信用卡交易数据集添加一个高斯随机噪声矩阵，然后在这个嘈杂的训练集上训练一个自编码器。然后，我们将看看这个自编码器在嘈杂的测试集上预测欺诈交易的表现：
- en: '[PRE36]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Denoising Autoencoder
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 去噪自编码器
- en: Compared to the original, nondistorted dataset, the penalty for overfitting
    to the noisy dataset of credit card transactions is much higher. There is enough
    noise in the dataset that an autoencoder that fits too well to the noisy data
    will have a poor time detecting fraudulent transactions from normal ones.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始的非失真数据集相比，对信用卡交易嘈杂数据集的过拟合惩罚要高得多。数据集中有足够的噪声，以至于一个对噪声数据拟合得太好的自编码器很难从正常交易和欺诈交易中检测出欺诈交易。
- en: This should make sense. We need an autoencoder that fits well enough to the
    data so that it is able to reconstruct most of the observations well enough but
    not so well enough that it accidentally reconstructs the noise, too. In other
    words, we want the autoencoder to learn the underlying structure but forget the
    noise in the data.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该是有道理的。我们需要一个自编码器，它能够很好地适应数据，以便能够足够好地重构大部分观测值，但又不能够过于好，以至于意外地重构了噪音。换句话说，我们希望自编码器能够学习到潜在的结构，但忽略数据中的噪音。
- en: Let’s try a few options from what has worked well so far. First, we will try
    a single hidden layer, 27-node undercomplete autoencoder with linear activation.
    Next, we will try a single hidden layer, 40-node sparse overcomplete autoencoder
    with dropout. And, finally, we will use an autoencoder with a nonlinear activation
    function.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从到目前为止表现良好的选项中尝试几个。首先，我们将尝试一个单隐藏层、27节点的欠完全自编码器，采用线性激活。接下来，我们将尝试一个单隐藏层、40节点的稀疏过完备自编码器，带有dropout。最后，我们将使用一个带有非线性激活函数的自编码器。
- en: Two-Layer Denoising Undercomplete Autoencoder with Linear Activation
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 两层去噪欠完全自编码器，采用线性激活
- en: On the noisy dataset, the single hidden layer autoencoder with linear activation
    and 27 nodes had an average precision of 0.69\. Let’s see how well it does on
    the noisy dataset. This autoencoder—because it is working with a noisy dataset
    and trying to denoise it—is known as a *denoising autoencoder*.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在嘈杂的数据集上，具有线性激活和27个节点的单隐藏层自编码器的平均精度为0.69。让我们看看它在嘈杂的数据集上表现如何。这种自编码器——因为它正在处理一个嘈杂的数据集并试图去噪它——被称为*去噪自编码器*。
- en: 'The code is similar to what we had before except now we are applying it to
    the noisy training and test datasets, `X_train_AE_noisy` and `X_test_AE_noisy`,
    respectively:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 代码与之前类似，只是现在我们将其应用于嘈杂的训练和测试数据集`X_train_AE_noisy`和`X_test_AE_noisy`：
- en: '[PRE37]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The following training data shows the losses from this autoencoder, and [Figure 8-9](#evaluation_metrics_of_denoising_undercomplete_autoencoder_with_single_hidden_layer_and_linear_activation_function)
    displays the average precision, the precision-recall curve, and the auROC curve:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 以下训练数据显示了这个自编码器的损失，而[图 8-9](#evaluation_metrics_of_denoising_undercomplete_autoencoder_with_single_hidden_layer_and_linear_activation_function)展示了平均精度、精确率-召回率曲线和auROC曲线：
- en: '[PRE38]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '![Evaluation Metrics of Denoising Undercomplete Autoencoder with Single Hidden
    Layer and Linear Activation Function](assets/hulp_0809.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图 8-9](assets/hulp_0809.png)。去噪欠完全自编码器的评估指标，采用单隐藏层和线性激活函数'
- en: Figure 8-9\. Evaluation metrics of denoising undercomplete autoencoder with
    single hidden layer and linear activation function
  id: totrans-185
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-9。去噪欠完全自编码器的评估指标，采用单隐藏层和线性激活函数
- en: 'The mean average precision is now 0.28\. You can see just how difficult it
    is for the linear autoencoder to denoise this noisy dataset:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 平均精度现在为0.28。您可以看出，线性自编码器在去噪这个嘈杂的数据集上是多么困难：
- en: '[PRE39]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: It struggles with separating the true underlying structure in the data from
    the Gaussian noise we added.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 它在将数据中真实的潜在结构与我们添加的高斯噪声分离方面存在困难。
- en: Two-Layer Denoising Overcomplete Autoencoder with Linear Activation
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 具有线性激活函数的两层降噪过完备自编码器
- en: Let’s now try a single hidden layer overcomplete autoencoder with 40 nodes,
    a sparsity regularizer, and dropout of 0.05%.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试一个单隐藏层过完备自编码器，有40个节点，稀疏性正则化器，以及0.05%的Dropout。
- en: 'This had an average precision of 0.56 on the original dataset:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始数据集上，这个模型的平均精度为0.56：
- en: '[PRE40]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The following training data shows the losses from this autoencoder, and [Figure 8-10](#evaluation_metrics_of_denoising_overcomplete_autoencoder_with_dropout_and_linear_activation_function)
    displays the average precision, the precision-recall curve, and the auROC curve:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 以下训练数据显示了该自编码器的损失，而[图8-10](#evaluation_metrics_of_denoising_overcomplete_autoencoder_with_dropout_and_linear_activation_function)展示了平均精度、精确率-召回率曲线和auROC曲线：
- en: '[PRE41]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '![Evaluation Metrics of Denoising Overcomplete Autoencoder with Dropout and
    Linear Activation Function](assets/hulp_0810.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![使用Dropout和线性激活函数的降噪过完备自编码器的评估指标](assets/hulp_0810.png)'
- en: Figure 8-10\. Evaluation metrics of denoising overcomplete autoencoder with
    dropout and linear activation function
  id: totrans-196
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-10\. 使用Dropout和线性激活函数的降噪过完备自编码器的评估指标
- en: 'The following code summarizes the distribution of average precision across
    the 10 runs. The mean of the average precision is 0.10, worse than the 0.53 we
    achieved earlier. The coefficient of variation is 0.83:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码总结了10次运行中平均精度的分布情况。平均精度的均值为0.10，比我们之前达到的0.53差。变异系数为0.83：
- en: '[PRE42]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Two-Layer Denoising Overcomplete Autoencoder with ReLu Activation
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 具有ReLU激活的两层降噪过完备自编码器
- en: 'Finally, let’s see how the same autoencoder fares using ReLu as the activation
    function instead of a linear activation function. Recall that the nonlinear activation
    function autoencoder did not perform quite as well as the one with linear activation
    on the original dataset:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看同一个自编码器使用ReLU作为激活函数而不是线性激活函数时的表现。回想一下，非线性激活函数的自编码器在原始数据集上的表现不如线性激活函数的表现：
- en: '[PRE43]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The following training data shows the losses from this autoencoder, and [Figure 8-11](#evaluation_metrics_of_denoising_overcomplete_autoencoder_with_dropout_and_relu_activation_function)
    displays the average precision, the precision-recall curve, and the auROC curve:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 以下训练数据显示了该自编码器的损失，而[图8-11](#evaluation_metrics_of_denoising_overcomplete_autoencoder_with_dropout_and_relu_activation_function)展示了平均精度、精确率-召回率曲线和auROC曲线：
- en: '[PRE44]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![Evaluation Metrics of Denoising Overcomplete Autoencoder with Dropout and
    ReLU Activation Function](assets/hulp_0811.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![使用Dropout和ReLU激活函数的降噪过完备自编码器的评估指标](assets/hulp_0811.png)'
- en: Figure 8-11\. Evaluation metrics of denoising overcomplete autoencoder with
    dropout and ReLU activation function
  id: totrans-205
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-11\. 使用Dropout和ReLU激活函数的降噪过完备自编码器的评估指标
- en: 'The following code summarizes the distribution of average precision across
    the 10 runs. The mean of the average precision is 0.20, worse than the 0.53 we
    achieved earlier. The coefficient of variation is 0.55:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码总结了10次运行中平均精度的分布情况。平均精度的均值为0.20，比我们之前达到的0.53差。变异系数为0.55：
- en: '[PRE45]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: You can experiment with the number of nodes, layers, degree of sparsity, dropout
    percentage, and the activation functions to see if you can improve the results
    from here.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以尝试不同的节点数、层数、稀疏度、Dropout 百分比和激活函数，看看能否改善结果。
- en: Conclusion
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, we returned to the credit card fraud problem from earlier in
    the book to develop a neural network-based unsupervised fraud detection solution.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们回顾了本书早期提到的信用卡欺诈问题，并开发了基于神经网络的无监督欺诈检测解决方案。
- en: To find the optimal architecture for our autoencoder, we experimented with a
    variety of autoencoders. We tried complete, undercomplete, and overcomplete autoencoders
    with either a single or a few hidden layers. We also used both linear and nonlinear
    activation functions and employed two major types of regularization, sparsity
    and dropout.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到我们自编码器的最优结构，我们尝试了各种自编码器。我们尝试了完备、欠完备和过完备的自编码器，有单层或几层隐藏层。我们还使用了线性和非线性激活函数，并应用了两种主要的正则化方法，稀疏性和Dropout。
- en: We found that a pretty simple two-layer undercomplete neural network with linear
    activation worked best on the original credit card dataset, but we needed a sparse
    two-layer overcomplete autoencoder with linear activation and dropout to address
    the noise in the noisy credit card dataset.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，在原始信用卡数据集上，一个相当简单的两层欠完备神经网络，使用线性激活效果最佳，但在嘈杂的信用卡数据集中，我们需要一个稀疏的两层过完备自编码器，配备线性激活和dropout来处理噪声。
- en: A lot of our experiments were based on trial and error—for each experiment,
    we adjusted several hyperparameters and compared results with previous iterations.
    It is possible that an even better autoencoder-based fraud detection solution
    exists, and I encourage you to experiment on your own to see what you find.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的许多实验都基于试错法进行——每次实验中，我们调整了几个超参数，并将结果与先前的迭代进行比较。可能存在更好的基于自编码器的欺诈检测解决方案，我鼓励您进行自己的实验，看看您能找到什么。
- en: So far in this book, we have viewed supervised and unsupervised as separate
    and distinct approaches, but in [Chapter 9](ch09.html#Chapter_9), we will explore
    how to employ both supervised and unsupervised approaches jointly to develop a
    so-called semisupervised solution that is better than either standalone approach.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 至此，本书中我们将监督学习和无监督学习视为独立且不同的方法，但在[第9章](ch09.html#Chapter_9)中，我们将探讨如何同时使用监督和无监督方法，开发一个称为半监督解决方案，其表现优于任何单独的方法。
- en: ^([1](ch08.html#idm140637541443984-marker)) Visit the official documentation
    for more on the [Keras Sequential model](http://bit.ly/2FZbUrq).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch08.html#idm140637541443984-marker)) 欲了解更多关于[Keras Sequential model](http://bit.ly/2FZbUrq)的信息，请访问官方文档。
- en: ^([2](ch08.html#idm140637541363344-marker)) For more on loss functions, refer
    to the [official Keras documentation](https://keras.io/losses/).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch08.html#idm140637541363344-marker)) 欲了解更多关于损失函数的信息，请参阅[官方Keras文档](https://keras.io/losses/)。
- en: ^([3](ch08.html#idm140637541313568-marker)) Consult Wikipedia for more on [stochastic
    gradient descent](http://bit.ly/2G3Ak30).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch08.html#idm140637541313568-marker)) 请查阅维基百科，了解更多关于[随机梯度下降](http://bit.ly/2G3Ak30)的信息。
- en: ^([4](ch08.html#idm140637541306896-marker)) For more information on optimizers,
    refer to the [documentation](https://keras.io/optimizers/).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch08.html#idm140637541306896-marker)) 欲了解更多有关优化器的信息，请参阅[文档](https://keras.io/optimizers/)。
- en: ^([5](ch08.html#idm140637541302672-marker)) For more on evaluation metrics,
    refer to the [documentation](https://keras.io/metrics/).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch08.html#idm140637541302672-marker)) 欲了解更多评估指标，请参阅[文档](https://keras.io/metrics/)。
