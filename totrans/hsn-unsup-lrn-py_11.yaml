- en: Chapter 8\. Hands-On Autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will build applications using various versions of autoencoders,
    including undercomplete, overcomplete, sparse, denoising, and variational autoencoders.
  prefs: []
  type: TYPE_NORMAL
- en: To start, let’s return to the credit card fraud detection problem we introduced
    in [Chapter 3](ch03.html#Chapter_3). For this problem, we have 284,807 credit
    card transactions, of which only 492 are fraudulent. Using a supervised model,
    we achieved an average precision of 0.82, which is very impressive. We can find
    well over 80% of the fraud with an over 80% precision. Using an unsupervised model,
    we achieved an average precision of 0.69, which is very good considering we did
    not use labels. We can find over 75% of the fraud with an over 75% precision.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how this same problem can be solved using an autoencoder, which is
    also an unsupervised algorithm but one that uses a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Data Preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s first load the necessary libaries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, load the dataset and prepare it for use. We will create a `dataX` matrix
    with all the PCA components and the feature `Amount`, but drop `Class` and `Time`.
    We will store the `Class` labels in the `dataY` matrix. We will also scale the
    features in the `dataX` matrix so that all the features have a mean of zero and
    standard deviation of one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As we did in [Chapter 3](ch03.html#Chapter_3), we will create a training set
    with two-thirds of the data and the labels and a test set with one-third of the
    data and the labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s store the training set and the test set as *X_train_AE* and *X_test_AE*,
    respectively. We will use these in the autoencoders soon:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Let’s also use reuse the function we introduced earlier in the book, called
    `anomalyScores`, to calculate the reconstruction error between the original feature
    matrix and the newly reconstructed feature matrix. The function takes the sum
    of squared errors and normalizes them to a range between zero and one.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a crucial function. The transactions with errors close to one are the
    ones that are most anomalous (i.e., have the highest reconstruction error) and,
    therefore, are most likely to be fraudulent. The transactions with errors close
    to zero have the lowest reconstruction error and are most likely to be normal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also reuse the function to plot the precision-recall curve, the average
    precision, and the ROC curve. This function is called `plotResults`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The Components of an Autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, let’s build a very simple autoencoder with the input layer, a single
    hidden layer, and the output layer. We will feed the original feature matrix *x*
    into the autoencoder—this is represented by the input layer. Then, an activation
    function will be applied to the input layer, generating the hidden layer. This
    activation function is called *f* and represents the *encoder* portion of the
    autoencoder. The hidden layer is called *h* (which is equal to *f(x)*) and represents
    the newly learned representation.
  prefs: []
  type: TYPE_NORMAL
- en: Next, an activation function is applied to the hidden layer (i.e., the newly
    learned representation) to reconstruct the original observations. This activation
    function is called *g* and represents the *decoder* portion of the autoencoder.
    The output layer is called *r* (which is equal to *g(h)*) and represents the newly
    reconstructed observations. To calculate the reconstruction error, we will compare
    the newly constructed observations *r* with the original ones *x*.
  prefs: []
  type: TYPE_NORMAL
- en: Activation Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we decide the number of nodes to use in this single hidden layer autoencoder,
    let’s discuss activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: A neural network learns the weights to apply to the nodes at each of the layers
    but whether the nodes will be activated or not (for use in the next layer) is
    determined by the activation function. In other words, an activation function
    is applied to the weighted input (plus bias, if any) at each layer. We will call
    the weighted input plus bias *Y*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The activation function takes in *Y* and either activates (if *Y* is above
    a certain threshold) or does not. If activated, the information in a given node
    is passed to the next layer; otherwise, it is not. However, we do not want simple
    binary activations. Instead, we want a range of activation values. To do this,
    we can choose a linear activation function or a nonlinear activation function.
    The linear activation function is unbounded. It can generate activation values
    between negative infinity and positive infinity. Common nonlinear activation functions
    include sigmoid, hyperbolic tangent (or tanh for short), rectified linear unit
    (or ReLu for short), and softmax:'
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid function
  prefs: []
  type: TYPE_NORMAL
- en: The sigmoid function is bounded and can generate activation values between zero
    and one.
  prefs: []
  type: TYPE_NORMAL
- en: Tanh function
  prefs: []
  type: TYPE_NORMAL
- en: The tanh function is also bounded and can generate activation values between
    negative one and positive one. Its gradient is steeper than that of the sigmoid
    function.
  prefs: []
  type: TYPE_NORMAL
- en: ReLu function
  prefs: []
  type: TYPE_NORMAL
- en: The ReLu function has an interesting property. If *Y* is positive, ReLu will
    return *Y*. Otherwise, it will return zero. Therefore, ReLu is unbounded for positive
    values of *Y*.
  prefs: []
  type: TYPE_NORMAL
- en: Softmax function
  prefs: []
  type: TYPE_NORMAL
- en: The softmax function is used as the final activation function in a neural network
    for classification problems because it normalizes classification probabilities
    to values that add up to a probability of one.
  prefs: []
  type: TYPE_NORMAL
- en: Of all these functions, the linear activation function is the simplest and least
    computationally expensive. ReLu is the next least computationally expensive, followed
    by the others.
  prefs: []
  type: TYPE_NORMAL
- en: Our First Autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s start with a two-layer autoencoder with a linear activation function for
    both the encoder and the decoder functions. Note that only the number of hidden
    layers plus the output layer count toward the *number of layers* in a neural network.
    Since we have a single hidden layer, this is known as a two-layer neural network.
  prefs: []
  type: TYPE_NORMAL
- en: To build this using TensorFlow and Keras, we must first call the *Sequential
    model API*. The Sequential model is a linear stack of layers, and we will pass
    the types of layers we want into the model before we compile the model and train
    on our data.^([1](ch08.html#idm140637541443984))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Once we call the Sequential model, we then need to specify the input shape by
    designating the number of dimensions, which should match the number of dimensions
    in the original feature matrix, *dataX*. This number is 29.
  prefs: []
  type: TYPE_NORMAL
- en: We also need to specify the activation function (also known as the encoder function)
    applied to the input layer and the number of nodes we want the hidden layer to
    have. We will pass *linear* as the activation function.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, let’s use a complete autoencoder, where the number of nodes in the
    hidden layer equals the number of nodes in the input layer, which is 29\. All
    of this is done using a single line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we need to specify the activation function (also known as the decoder
    function) applied to the hidden layer to reconstruct the observations and the
    number of dimensions we want the output layer to have. Since we want the final
    reconstructed matrix to have the same dimensions as the original matrix, the dimension
    needs to be 29\. And, we will use a linear activation function for the decoder,
    too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will need to compile the layers we have designed for the neural network.
    This requires us to select a *loss function* (also known as the *objective function*)
    to guide the learning of the weights, an *optimizer* to set the process by which
    the weights are learned, and a list of *metrics* to output to help us evaluate
    the goodness of the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Loss Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start with the loss function. Recall that we are evaluating the model
    based on the reconstruction error between the newly reconstructed matrix of features
    based on the autoencoder and the original feature matrix that we feed into the
    autoencoder.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we want to use *mean squared error* as the evaluation metric. (For
    our custom evaluation function, we use sum of squared errors, which is similar.).^([2](ch08.html#idm140637541363344))
  prefs: []
  type: TYPE_NORMAL
- en: Optimizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Neural networks train for many rounds (known as *epochs*). In each of these
    epochs, the neural network readjusts its learned weights to reduce its loss from
    the previous epoch. The process for learning these weights is set by the optimizer.
    We want a process that helps the neural network efficiently learn the optimal
    weights for the various nodes across all the layers that minimizes the loss function
    we have chosen.
  prefs: []
  type: TYPE_NORMAL
- en: To learn the optimal weights, the neural network needs to adjust its “guess”
    for the optimal weights in an intelligent way. One approach is to iteratively
    move the weights in the direction that helps reduce the loss function incrementally.
    But an even better approach is to move the weights in this direction but with
    a degree of randomness—in other words, to move the weights stochastically.
  prefs: []
  type: TYPE_NORMAL
- en: Although there is more to this, this process is known as *stochastic gradient
    descent* (or SGD for short), the most commonly used optimizer in training neural
    networks.^([3](ch08.html#idm140637541313568)) SGD has a single learning rate,
    known as *alpha*, for all the weight updates that it makes, and this learning
    rate does not change during training. However, in most cases, it’s better to adjust
    the learning rate over the course of the training. For example, in the earlier
    epochs, it makes more sense to adjust the weights by a large degree—in other words,
    to have a large learning rate or alpha.
  prefs: []
  type: TYPE_NORMAL
- en: In later epochs, when the weights are more optimal, it makes more sense to adjust
    the weights by a small degree to delicately fine-tune the weights than to take
    massive steps in one direction or another. Therefore, an even better optimzer
    than SGD is the *Adam optimization algorithm*, which is derived from adaptive
    moment estimation. The Adam optimizer dynamically adjusts the learning rate over
    the course of the training process, unlike SGD, and is the optimizer we will use.^([4](ch08.html#idm140637541306896))
  prefs: []
  type: TYPE_NORMAL
- en: For this optimizer, we can set the alpha, which sets the pace at which weights
    are updated. Larger alpha values result in faster initial learning before the
    learning rate is updated.
  prefs: []
  type: TYPE_NORMAL
- en: Training the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, we need to choose the evaluation metric, which we will set to `accuracy`
    to keep things simple:^([5](ch08.html#idm140637541302672))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Next, we need to select the number of epochs and the batch size and then begin
    the training process by calling the method *fit*. The number of epochs determines
    the number of times the training occurs over the entire dataset we pass into the
    neural network. We will set this to 10 to start.
  prefs: []
  type: TYPE_NORMAL
- en: The batch sets the number of samples the neural network trains on before making
    the next gradient update. If the batch is equal to the total number of observations,
    the neural network will make a gradient update once every epoch. Otherwise, it
    will make updates multiple times per epoch. We will set this to a generic 32 samples
    to start.
  prefs: []
  type: TYPE_NORMAL
- en: Into the fit method, we will pass in the initial input matrix, *x*, and the
    target matrix, *y*. In our case, both *x* and *y* will be the original feature
    matrix, *X_train_AE*, because we want to compare the output of the autoencoder—the
    reconstructed feature matrix—with the original feature matrix to calculate the
    reconstruction error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember, this is a purely unsupervised solution so we will not use the *y*
    matrix at all. We will also validate our model as we go by testing the reconstruction
    error on the entire training matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Since this a complete autoencoder—where the hidden layer has the same number
    of dimensions as the input layer—the loss is very low, for both the training and
    validation sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This is not optimal—the autoencoder has reconstructed the original feature matrix
    too precisely, memorizing the inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the autoencoder is meant to learn a new representation that captures
    the most salient information in the original input matrix while dropping the less
    relevant information. Simply memorizing the inputs—also known as learning the
    *identity function*—will not result in new and improved representation learning.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating on the Test Set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s use the test set to evaluate just how successively this autoencoder can
    identify fraud in the credit card transactions dataset. We will use the `predict`
    method to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As seen in [Figure 8-1](#evaluation_metrics_of_complete_autoencoder), the average
    precision is 0.30, which is not very good. The best average precision using unsupervised
    learning from [Chapter 4](ch04.html#Chapter_4) was 0.69, and the supervised system
    had an average precision of 0.82\. However, each training process will yield slightly
    different results for the trained autoencoder, so you may not see the same performance
    for your run.
  prefs: []
  type: TYPE_NORMAL
- en: To get a better sense of how a two-layer complete autoencoder performs on the
    test set, let’s run this training process ten separate times and store the average
    precision on the test set for each run. We will assess how good this complete
    autoencoder is at capturing fraud based on the mean of the average precision from
    these 10 runs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluation Metrics of Complete Autoencoder](assets/hulp_0801.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. Evaluation metrics of complete autoencoder
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To consolidate our work thus far, here is the code to simulate 10 runs from
    start to finish:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The following code summarizes the results for the 10 runs. The mean of the average
    precision is 0.30, but the average precision ranges from a low of 0.02 to .72\.
    The *coefficient of variation* (defined as the standard deviation divided by the
    mean over 10 runs) is 0.88.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Let’s try to improve our results by building variations of this autoencoder.
  prefs: []
  type: TYPE_NORMAL
- en: Two-Layer Undercomplete Autoencoder with Linear Activation Function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s try an undercomplete autoencoder rather than a complete one.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the previous autoencoder, the only thing that changes is the number
    of nodes in the hidden layer. Instead of setting this to the number of original
    dimensions (29), we will set the nodes to 20\. In other words, this autoencoder
    is a constrained autoencoder. The encoder function is forced to capture the information
    in the input layer with a fewer number of nodes, and the decoder has to take this
    new representation to reconstruct the original matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'We should expect the loss here to be higher compared to that of the complete
    autoencoder. Let’s run the code. We will perform 10 independent runs to test how
    well the various undercomplete autoencoders are at catching fraud:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'As the following shows, the losses of the undercomplete autoencoder are considerably
    higher than those of the complete autoencoder. It is clear that the autoencoder
    learns a representation that is new and more constrained than the original input
    matrix—the autoencoder did not simply memorize the inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This is how an autoencoder should work—it should learn a new representation.
    [Figure 8-2](#evaluation_metrics_of_undercomplete_autoencoder_with_20_nodes) shows
    how effective this new representation is at identifying fraud.
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluation Metrics of Undercomplete Autoencoder with 20 Nodes](assets/hulp_0802.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. Evaluation metrics of undercomplete autoencoder with 20 nodes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The average precision is 0.29, similar to that of the complete autoencoder.
  prefs: []
  type: TYPE_NORMAL
- en: The following code shows the distribution of average precisions across the 10
    runs. The mean of the average precision is 0.31, but the dispersion is very tight
    (as the coefficient of variation 0.03 indicates). This is a considerably more
    stable system than the one designed with a complete autoencoder.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: But we are still stuck at a fairly mediocre average precision. Why did the undercomplete
    autoencoder not perform better? It could be that this undercomplete autoencoder
    does not have enough nodes. Or, maybe we need to train using more hidden layers.
    Let’s experiment with these two changes, one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the Number of Nodes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following code displays the training losses when using a two-layer undercomplete
    autocoder with 27 nodes instead of just 20:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 8-3](#evaluation_metrics_of_undercomplete_autoencoder_with_27_nodes)
    displays the average precision, precision-recall curve, and auROC curve.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluation Metrics of Undercomplete Autoencoder with 27 Nodes](assets/hulp_0803.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-3\. Evaluation metrics of undercomplete autoencoder with 27 nodes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The average precision improves considerably to 0.70\. This is better than the
    average precision of the complete autoencoder and better than the best unsupervised
    learning solution from [Chapter 4](ch04.html#Chapter_4).
  prefs: []
  type: TYPE_NORMAL
- en: The following code summarizes the distribution of average precision across the
    10 runs. The mean of the average precision is 0.53, considerably better than the
    ~0.30 average precision earlier. The dispersion of average precision is reasonably
    good, with a coefficient of variation of 0.50.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We have a clear improvement over our previous autoencoder-based anomaly detection
    system.
  prefs: []
  type: TYPE_NORMAL
- en: Adding More Hidden Layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s see if we can improve our results by adding an extra hidden layer to the
    autoencoder. We will continue to use linear activation functions for now.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Experimentation is a major part of discovering the best neural network architecture
    for the problem you have to solve. Some changes you make will lead to better results,
    others to worse. Knowing how to modify the neural network and the hyperparameters
    as part of your search to improve the solution is very important.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of a single hidden layer with 27 nodes, we will use one hidden layer
    with 28 nodes and another with 27 nodes. This is only a slight variation from
    the one we used previously. This is now a three-layer neural network since we
    have two hidden layers plus the output layer. The input layer does not “count”
    toward this number.
  prefs: []
  type: TYPE_NORMAL
- en: 'This additional hidden layer requires just one additional line of code, as
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code summarizes the distribution of average precisions across
    the 10 runs. The mean of the average precision is 0.36, worse than the 0.53 we
    just achieved. The dispersion of average precision is also worse, with a coefficient
    of variation of 0.94 (higher is worse):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Nonlinear Autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let’s build an undercomplete autoencoder using a nonlinear activation function.
    We will use ReLu, but you are welcome to experiment with tanh, sigmoid, and the
    other nonlinear activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will include three hidden layers, with 27, 22, and 27 nodes, respectively.
    Conceptually, the first two activation functions (applied on the input and first
    hidden layer) perform the encoding, creating the second hidden layer with 22 nodes.
    Then, the next two activation functions perform the decoding, reconstructing the
    22-node representation to the original number of dimensions, 29:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code shows the losses from this autoencoder, and [Figure 8-4](#evaluation_metrics_of_undercomplete_autoencoder_with_three_hidden_layers_and_relu_activation_function)
    shows the average precision, the precision-recall curve, and the auROC curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![Evaluation Metrics of Undercomplete Autoencoder with Three Hidden Layers
    and ReLu Activation Function](assets/hulp_0804.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-4\. Evaluation metrics of undercomplete autoencoder with three hidden
    layers and ReLu activation function
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The results are considerably worse.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code summarizes the distribution of average precisions across
    the 10 runs. The mean of the average precision is 0.22, worse than the 0.53 we
    achieved earlier. The dispersion of average precisions is very tight, with a coefficient
    of variation of 0.06:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: These results are much worse than those from a simple autoencoder using a linear
    activation function. It could be that—for this dataset—a linear, undercomplete
    autoencoder is the best solution.
  prefs: []
  type: TYPE_NORMAL
- en: For other datasets, that may not always be the case. As always, experimentation
    is required to find the optimal solution. Change the number of nodes, the number
    of hidden layers, and the mix of activation functions, and see how much better
    or worse the solutions become.
  prefs: []
  type: TYPE_NORMAL
- en: This type of experimentation is known as *hyperparameter optimization*. You
    are adjusting the hyperparameters—the number of nodes, the number of layers, and
    the mix of activation functions—in search of the optimal solution.
  prefs: []
  type: TYPE_NORMAL
- en: Overcomplete Autoencoder with Linear Activation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let’s highlight the problem with overcomplete autoencoders. Overcomplete
    autoencoders have more nodes in the hidden layer than either the input or output
    layer. Because the *capacity* of the neural network model is so high, the autoencoder
    simply memorizes the observations it trains on.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, the autoencoder learns the *identity function*, which is exactly
    what we want to avoid. The autoencoder will overfit the training data and will
    perform very poorly in separating fraudulent credit card transactions from normal
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that we need the autoencoder to learn the salient aspects of the credit
    card transactions in the training set so that it learns what the normal transactions
    look like—without memorizing the information in the less normal, rare fraudulent
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Only if the autoencoder is able to lose some of the information in the training
    set will it be able to separate the fraudulent transactions from the normal ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code shows the losses from this autoencoder, and [Figure 8-5](#evaluation_metrics_of_overcomplete_autoencoder_with_single_hidden_layer_and_linear_activation_function)
    displays the average precision, the precision-recall curve, and the auROC curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![Evaluation Metrics of Overcomplete Autoencoder with Single Hidden Layer and
    Linear Activation Function](assets/hulp_0805.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-5\. Evaluation metrics of overcomplete autoencoder with single hidden
    layer and linear activation function
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As expected, the losses are very low, and the overfit overcomplete autoencoder
    has very poor performance in detecting the fraudulent credit card transactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code summarizes the distribution of average precision across
    the 10 runs. The mean of the average precision is 0.31, worse than the 0.53 we
    achieved earlier. The dispersion of average precision is not very tight, with
    a coefficient of variation of 0.89:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Overcomplete Autoencoder with Linear Activation and Dropout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One way to improve the overcomplete autoencoder solution is to use a regularization
    technique to reduce the overfitting. One such technique is known as *dropout*.
    With dropout, we force the autoencoder to drop out some defined percentage of
    units from the layers in the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: With this new constraint, the overcomplete autoencoder cannot simply memorize
    the credit card transactions in the training set. Instead, the autoencoder has
    to generalize a bit more. The autoencoder is forced to learn more of the salient
    features in the dataset and lose some of the less salient information.
  prefs: []
  type: TYPE_NORMAL
- en: We will use a dropout percentage of 10%, which we will apply to the hidden layer.
    In other words, 10% of the neurons are dropped. The higher the dropout percentage,
    the stronger the regularization. This is done with just a single additional line
    of code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see if this improves the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code shows the losses from this autoencoder, and [Figure 8-6](#evaluation_metrics_of_overcomplete_autoencoder_with_single_hidden_layer_dropout_and_linear_activation_function)
    displays the average precision, the precision-recall curve, and the auROC curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![Evaluation Metrics of Overcomplete Autoencoder with Single Hidden Layer,
    Dropout, and Linear Activation Function](assets/hulp_0806.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-6\. Evaluation metrics of overcomplete autoencoder with single hidden
    layer, dropout, and linear activation function
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As expected, the losses are very low, and the overfit overcomplete autoencoder
    has very poor performance in detecting the fraudulent credit card transactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code summarizes the distribution of average precision across
    the 10 runs. The mean of the average precision is 0.21, worse than the 0.53 we
    achieved earlier. The coefficient of variation is 0.40:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Sparse Overcomplete Autoencoder with Linear Activation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another regularization technique is *sparsity*. We can force the autoencoder
    to take the sparsity of the matrix into consideration such that the majority of
    the autoencoder’s neurons are inactive most of the time—in other words, they do
    not fire. This makes it harder for the autoencoder to memorize the identity function
    even when the autoencoder is overcomplete because most of the nodes cannot fire
    and, therefore, cannot overfit the observations as easily.
  prefs: []
  type: TYPE_NORMAL
- en: We will use a single hidden layer overcomplete autoencoder with 40 nodes like
    before but with just the sparsity penalty, not dropout.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see if the results improve from the 0.21 average precision we had earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code shows the losses from this autoencoder, and [Figure 8-7](#evaluation_metrics_of_sparse_overcomplete_autoencoder_with_single_hidden_layer_and_linear_activation_function)
    displays the average precision, the precision-recall curve, and the auROC curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![Evaluation Metrics of Sparse Overcomplete Autoencoder with Single Hidden
    Layer and Linear Activation Function](assets/hulp_0807.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-7\. Evaluation metrics of sparse overcomplete autoencoder with single
    hidden layer and linear activation function
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The following code summarizes the distribution of average precision across
    the 10 runs. The mean of the average precision is 0.21, worse than the 0.53 we
    achieved earlier. The coefficient of variation is 0.99:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Sparse Overcomplete Autoencoder with Linear Activation and Dropout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Of course, we can combine the regularization techniques to improve the solution.
    Here is a sparse overcomplete autoencoder with linear activation, 40 nodes in
    the single hidden layer, and dropout of 5%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The following training data shows the losses from this autoencoder, and [Figure 8-8](#evaluation_metrics_of_sparse_overcomplete_autoencoder_with_single_hidden_layer_dropout_and_linear_activation)
    displays the average precision, the precision-recall curve, and the auROC curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '![Evaluation Metrics of Sparse Overcomplete Autoencoder with Single Hidden
    Layer, Dropout, and Linear Activation Function](assets/hulp_0808.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-8\. Evaluation metrics of sparse overcomplete autoencoder with single
    hidden layer, dropout, and linear activation function
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The following code summarizes the distribution of average precision across
    the 10 runs. The mean of the average precision is 0.24, worse than the 0.53 we
    achieved earlier. The coefficient of variation is 0.62:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Working with Noisy Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common problem with real-world data is noisiness data is often distorted in
    some way because of data quality issues from data capture, data migration, data
    transformation, etc. We need autoencoders to be robust enough against such noise
    so that they are not fooled and can learn from the truly important underlying
    structure in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To simulate this noise, let’s add a Gaussian random matrix of noise to our
    credit card transactions dataset and then train an autoencoder on this noisy training
    set. Then, we will see how well the autoencoder does in predicting fraud on the
    noisy test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Denoising Autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Compared to the original, nondistorted dataset, the penalty for overfitting
    to the noisy dataset of credit card transactions is much higher. There is enough
    noise in the dataset that an autoencoder that fits too well to the noisy data
    will have a poor time detecting fraudulent transactions from normal ones.
  prefs: []
  type: TYPE_NORMAL
- en: This should make sense. We need an autoencoder that fits well enough to the
    data so that it is able to reconstruct most of the observations well enough but
    not so well enough that it accidentally reconstructs the noise, too. In other
    words, we want the autoencoder to learn the underlying structure but forget the
    noise in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try a few options from what has worked well so far. First, we will try
    a single hidden layer, 27-node undercomplete autoencoder with linear activation.
    Next, we will try a single hidden layer, 40-node sparse overcomplete autoencoder
    with dropout. And, finally, we will use an autoencoder with a nonlinear activation
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Two-Layer Denoising Undercomplete Autoencoder with Linear Activation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On the noisy dataset, the single hidden layer autoencoder with linear activation
    and 27 nodes had an average precision of 0.69\. Let’s see how well it does on
    the noisy dataset. This autoencoder—because it is working with a noisy dataset
    and trying to denoise it—is known as a *denoising autoencoder*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is similar to what we had before except now we are applying it to
    the noisy training and test datasets, `X_train_AE_noisy` and `X_test_AE_noisy`,
    respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The following training data shows the losses from this autoencoder, and [Figure 8-9](#evaluation_metrics_of_denoising_undercomplete_autoencoder_with_single_hidden_layer_and_linear_activation_function)
    displays the average precision, the precision-recall curve, and the auROC curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '![Evaluation Metrics of Denoising Undercomplete Autoencoder with Single Hidden
    Layer and Linear Activation Function](assets/hulp_0809.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-9\. Evaluation metrics of denoising undercomplete autoencoder with
    single hidden layer and linear activation function
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The mean average precision is now 0.28\. You can see just how difficult it
    is for the linear autoencoder to denoise this noisy dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: It struggles with separating the true underlying structure in the data from
    the Gaussian noise we added.
  prefs: []
  type: TYPE_NORMAL
- en: Two-Layer Denoising Overcomplete Autoencoder with Linear Activation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s now try a single hidden layer overcomplete autoencoder with 40 nodes,
    a sparsity regularizer, and dropout of 0.05%.
  prefs: []
  type: TYPE_NORMAL
- en: 'This had an average precision of 0.56 on the original dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The following training data shows the losses from this autoencoder, and [Figure 8-10](#evaluation_metrics_of_denoising_overcomplete_autoencoder_with_dropout_and_linear_activation_function)
    displays the average precision, the precision-recall curve, and the auROC curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '![Evaluation Metrics of Denoising Overcomplete Autoencoder with Dropout and
    Linear Activation Function](assets/hulp_0810.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-10\. Evaluation metrics of denoising overcomplete autoencoder with
    dropout and linear activation function
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The following code summarizes the distribution of average precision across
    the 10 runs. The mean of the average precision is 0.10, worse than the 0.53 we
    achieved earlier. The coefficient of variation is 0.83:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Two-Layer Denoising Overcomplete Autoencoder with ReLu Activation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finally, let’s see how the same autoencoder fares using ReLu as the activation
    function instead of a linear activation function. Recall that the nonlinear activation
    function autoencoder did not perform quite as well as the one with linear activation
    on the original dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The following training data shows the losses from this autoencoder, and [Figure 8-11](#evaluation_metrics_of_denoising_overcomplete_autoencoder_with_dropout_and_relu_activation_function)
    displays the average precision, the precision-recall curve, and the auROC curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '![Evaluation Metrics of Denoising Overcomplete Autoencoder with Dropout and
    ReLU Activation Function](assets/hulp_0811.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-11\. Evaluation metrics of denoising overcomplete autoencoder with
    dropout and ReLU activation function
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The following code summarizes the distribution of average precision across
    the 10 runs. The mean of the average precision is 0.20, worse than the 0.53 we
    achieved earlier. The coefficient of variation is 0.55:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: You can experiment with the number of nodes, layers, degree of sparsity, dropout
    percentage, and the activation functions to see if you can improve the results
    from here.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we returned to the credit card fraud problem from earlier in
    the book to develop a neural network-based unsupervised fraud detection solution.
  prefs: []
  type: TYPE_NORMAL
- en: To find the optimal architecture for our autoencoder, we experimented with a
    variety of autoencoders. We tried complete, undercomplete, and overcomplete autoencoders
    with either a single or a few hidden layers. We also used both linear and nonlinear
    activation functions and employed two major types of regularization, sparsity
    and dropout.
  prefs: []
  type: TYPE_NORMAL
- en: We found that a pretty simple two-layer undercomplete neural network with linear
    activation worked best on the original credit card dataset, but we needed a sparse
    two-layer overcomplete autoencoder with linear activation and dropout to address
    the noise in the noisy credit card dataset.
  prefs: []
  type: TYPE_NORMAL
- en: A lot of our experiments were based on trial and error—for each experiment,
    we adjusted several hyperparameters and compared results with previous iterations.
    It is possible that an even better autoencoder-based fraud detection solution
    exists, and I encourage you to experiment on your own to see what you find.
  prefs: []
  type: TYPE_NORMAL
- en: So far in this book, we have viewed supervised and unsupervised as separate
    and distinct approaches, but in [Chapter 9](ch09.html#Chapter_9), we will explore
    how to employ both supervised and unsupervised approaches jointly to develop a
    so-called semisupervised solution that is better than either standalone approach.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch08.html#idm140637541443984-marker)) Visit the official documentation
    for more on the [Keras Sequential model](http://bit.ly/2FZbUrq).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch08.html#idm140637541363344-marker)) For more on loss functions, refer
    to the [official Keras documentation](https://keras.io/losses/).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch08.html#idm140637541313568-marker)) Consult Wikipedia for more on [stochastic
    gradient descent](http://bit.ly/2G3Ak30).
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch08.html#idm140637541306896-marker)) For more information on optimizers,
    refer to the [documentation](https://keras.io/optimizers/).
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch08.html#idm140637541302672-marker)) For more on evaluation metrics,
    refer to the [documentation](https://keras.io/metrics/).
  prefs: []
  type: TYPE_NORMAL
