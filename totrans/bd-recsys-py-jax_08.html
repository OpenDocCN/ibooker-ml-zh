<html><head></head><body><section data-pdf-bookmark="Chapter 6. Data Processing" data-type="chapter" epub:type="chapter"><div class="chapter" id="data-processing">&#13;
<h1><span class="label">Chapter 6. </span>Data Processing</h1>&#13;
&#13;
&#13;
<p>In the trivial recommender that we defined in <a data-type="xref" href="ch01.html#CH0">Chapter 1</a>, we used the method <code>get_availability</code>; and in the MPIR, we used the method <code>get_item_popularities</code>. We hoped the choice of naming would provide sufficient context about their function, but we did not focus on the implementation details. Now we will start unpacking the details of some of this complexity and present the toolsets for online and offline collectors.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Hydrating Your System" data-type="sect1"><div class="sect1" id="id50">&#13;
<h1>Hydrating Your System</h1>&#13;
&#13;
<p>Getting data into the pipeline<a data-primary="hydration" data-seealso="data processing" data-type="indexterm" id="id518"/><a data-primary="data processing" data-secondary="importing data with PySpark" data-type="indexterm" id="DPpyspark06"/> is punnily referred to as <em>hydration</em>. The ML and data fields have a lot of water-themed naming conventions; <a href="https://oreil.ly/XVlzd">“(Data ∩ Water) Terms”</a> by Pardis Noorzad covers this topic.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="PySpark" data-type="sect2"><div class="sect2" id="id51">&#13;
<h2>PySpark</h2>&#13;
&#13;
<p>Spark<a data-primary="Spark" data-type="indexterm" id="spark06"/><a data-primary="PySpark" data-secondary="importing data with" data-type="indexterm" id="PSimport06"/> is an extremely general computing library, with APIs for Java, Python, SQL, and Scala. PySpark’s role in many ML pipelines is for data processing and transforming the large-scale datasets.</p>&#13;
&#13;
<p>Let’s return to the data structure we introduced for our recommendation problem; recall that the user-item matrix is the linear-algebraic representation of all the triples of users, items, and the user’s rating of the item. These triples are not naturally occurring in the wild. Most commonly, you begin with log files from your system; for example, Bookshop.org may have something that looks like this:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">	<code class="s1">'page_view_id'</code><code class="p">:</code> <code class="s1">'d15220a8e9a8e488162af3120b4396a9ca1'</code><code class="p">,</code>&#13;
	<code class="s1">'anonymous_id'</code><code class="p">:</code> <code class="s1">'e455d516-3c08-4b6f-ab12-77f930e2661f'</code><code class="p">,</code>&#13;
	<code class="s1">'view_tstamp'</code><code class="p">:</code> <code class="mi">2020</code><code class="o">-</code><code class="mi">10</code><code class="o">-</code><code class="mi">29</code> <code class="mi">17</code><code class="p">:</code><code class="mi">44</code><code class="p">:</code><code class="mi">41</code><code class="o">+</code><code class="mi">00</code><code class="p">:</code><code class="mi">00</code><code class="p">,</code>&#13;
	<code class="s1">'page_url'</code><code class="p">:</code> <code class="s1">'https://bookshop.org/lists/best-sellers-of-the-week'</code><code class="p">,</code>&#13;
	<code class="s1">'page_url_host'</code><code class="p">:</code> <code class="s1">'bookshop.org'</code><code class="p">,</code>&#13;
	<code class="s1">'page_url_path'</code><code class="p">:</code> <code class="s1">'/lists/bookshop-org-best-sellers-of-the-week'</code><code class="p">,</code>&#13;
	<code class="s1">'page_title'</code><code class="p">:</code> <code class="s1">'Best Sellers of the Week'</code><code class="p">,</code>&#13;
	<code class="s1">'page_url_query'</code><code class="p">:</code> <code class="kc">None</code><code class="p">,</code>&#13;
	<code class="s1">'authenticated_user_id'</code><code class="p">:</code> <code class="mf">15822493.0</code><code class="p">,</code>&#13;
	<code class="s1">'url_report_id'</code><code class="p">:</code> <code class="mf">511629659.0</code><code class="p">,</code>&#13;
	<code class="s1">'is_profile_page'</code><code class="p">:</code> <code class="kc">False</code><code class="p">,</code>&#13;
	<code class="s1">'product_viewed'</code><code class="p">:</code> <code class="s1">'list'</code><code class="p">,</code></pre>&#13;
&#13;
<p>This is a made-up log file that may look similar to the backend data for Bookshop.org’s best sellers of the week. These are the kinds of events that you consume from engineering and are likely stored in your columnar database. For data like this, utilizing SQL syntax will be our entry point.</p>&#13;
&#13;
<p>PySpark provides a convenient SQL API. Based on your infrastructure, this API will allow you to write what looks like SQL queries against a potentially massive dataset.</p>&#13;
<div data-type="note" epub:type="note"><h1>Example Schemas</h1>&#13;
<p>These example database schemas are only guesses at what Bookshop.org may use, but they are modeled on the authors’ experience of looking at hundreds of database schemas at multiple companies over many years. Additionally, we attempt to distill these schemas to the components relevant to our topic. In real systems, you’d expect much more complexity but the same essential parts. Each data warehouse and event stream will have its own quirks. Please consult a data engineer near you.</p>&#13;
</div>&#13;
&#13;
<p>Let’s use Spark to query the preceding logs:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">user_item_view_counts_qry</code> <code class="o">=</code> <code class="s2">"""</code>&#13;
<code class="s2">SELECT</code>&#13;
<code class="s2">  page_views.authenticated_user_id</code>&#13;
<code class="s2">  , page_views.page_url_path</code>&#13;
<code class="s2">  , COUNT(DISTINCT page_views.page_view_id) AS count_views</code>&#13;
&#13;
<code class="s2">FROM prod.page_views</code>&#13;
<code class="s2">JOIN prod.dim_users</code>&#13;
<code class="s2">	ON page_views.authenticated_user_id = dim_users.authenticated_user_id</code>&#13;
&#13;
<code class="s2">WHERE DATE page_views.view_tstamp &gt;= '2017-01-01'</code>&#13;
<code class="s2">	AND dim_users.country_code = 'US'</code>&#13;
&#13;
<code class="s2">GROUP BY</code>&#13;
<code class="s2">  page_views.authenticated_user_id</code>&#13;
<code class="s2">  , page_views.page_url_path</code>&#13;
&#13;
<code class="s2">ORDER BY 3, page_views.authenticated_user_id</code>&#13;
<code class="s2">"""</code>&#13;
&#13;
<code class="n">user_item_view_counts_sdf</code> <code class="o">=</code> <code class="n">spark</code><code class="o">.</code><code class="n">sql</code><code class="p">(</code><code class="n">user_item_view_counts_qry</code><code class="p">)</code></pre>&#13;
&#13;
<p>This is a simple SQL query, assuming the preceding log schema, that would allow us to see, for each user-item pair, how many times that user has viewed that pair. The convenience of writing pure SQL here means that we can use our experience in columnar databases to quickly ramp up on Spark.</p>&#13;
&#13;
<p>The major advantage of Spark, however, is not yet on display. When executing the preceding code in a Spark session, this query will not be immediately run. It will be staged for execution, but Spark waits until you use this data downstream in a way that <em>requires immediate execution</em> before it begins doing so. This is called<a data-primary="lazy evaluation" data-type="indexterm" id="id519"/> <em>lazy evaluation</em>, and it allows you to work on your data object without every change and interaction immediately being applied. For more details, it’s worth consulting a more in-depth guide like <a class="orm:hideurl" href="https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/"><em>Learning Spark</em></a> by Jules Damji et al. (O’Reilly), but there’s one more important characteristic of the Spark paradigm that is essential to discuss.</p>&#13;
&#13;
<p>Spark is natively a distributed computing language. In particular, this means that the preceding query—even after we force it to execute—will store its data on multiple computers. Spark works via a<a data-primary="driver program" data-type="indexterm" id="id520"/> <em>driver program</em> in your program or notebook, which drives a<a data-primary="cluster manager" data-type="indexterm" id="id521"/> <em>cluster manager</em>, which in turn coordinates<a data-primary="executors" data-type="indexterm" id="id522"/> <em>executors</em> on<a data-primary="worker nodes" data-type="indexterm" id="id523"/> <em>worker nodes.</em> When we query data with Spark, instead of all that data being returned into a DataFrame in memory on the computer we’re using, parts of that data are sent to memory on the executors. And when we do a transformation on the DataFrame, it is applied appropriately on the pieces of the DataFrame that are stored on each of the executors.</p>&#13;
&#13;
<p>If this sounds a bit like magic, that’s because it’s obscuring a lot of technical details behind several convenience layers. Spark is a layer of technology that allows the ML engineer to program as if they’re working on one machine, and have those changes take effect on an entire cluster of machines. It’s not important to understand the network structure when querying, but it is important to be aware of some of these details in case things go wrong; the ability to understand what the error output is referring to is crucial in troubleshooting. This is all summarized in <a data-type="xref" href="#fig:sparkitecture">Figure 6-1</a>, which is a diagram from the  <a href="https://oreil.ly/89kAm">Spark documentation</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig:sparkitecture">&#13;
<img alt="Sparkitecture" src="assets/brpj_0601.png"/>&#13;
<h6><span class="label">Figure 6-1. </span>Component architecture of Spark 3.0</h6>&#13;
</div></figure>&#13;
&#13;
<p>It’s important to note that all this does not come for free; both lazy evaluation and distributed DataFrames come at the cost of needing additional thought when writing programs. Even though Spark makes a lot of this work far easier, understanding how to write efficient code in this paradigm that works with the architecture but still achieves complicated goals can require a year’s worth of experience.</p>&#13;
&#13;
<p>Returning to recommendation systems—and in particular, the offline collector—we want to use PySpark to build the types of datasets needed to train our models. One simple thing to do with PySpark is to transform our logs data into the appropriate form for training a model. In our simple query, we applied a few filters to our data and grouped by user and item to get the number of views. A variety of other tasks may fit naturally into this paradigm—perhaps adding user or item features stored in other databases, or high-level aggregations.</p>&#13;
&#13;
<p>In our MPIR, we asked for <code>get_item_popularities</code>; and we sort of assumed a few things:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>This would return the number of times each item was chosen.</p>&#13;
</li>&#13;
<li>&#13;
<p>This method would be fast.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The second point is important if the endpoint is going to be called in real time. So how might Spark come into play?</p>&#13;
&#13;
<p>First, let’s assume we have a lot of data, enough that we can’t get it all to fit into our little MacBook Pro’s memory. Additionally, let’s continue to use the preceding schema. We can write an even simpler query:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">item_popularity_qry</code> <code class="o">=</code> <code class="s2">"""</code>&#13;
<code class="s2">SELECT</code>&#13;
<code class="s2">  page_views.page_url_path</code>&#13;
<code class="s2">  , COUNT(DISTINCT page_views.authenticated_user_id) AS count_viewers</code>&#13;
&#13;
<code class="s2">FROM prod.page_views</code>&#13;
<code class="s2">JOIN prod.dim_users</code>&#13;
<code class="s2">	ON page_views.authenticated_user_id = dim_users.authenticated_user_id</code>&#13;
&#13;
<code class="s2">WHERE DATE page_views.view_tstamp &gt;= '2017-01-01'</code>&#13;
<code class="s2">	AND dim_users.country_code = 'US'</code>&#13;
&#13;
<code class="s2">GROUP BY</code>&#13;
<code class="s2">  page_views.page_url_path</code>&#13;
&#13;
<code class="s2">ORDER BY 2</code>&#13;
<code class="s2">"""</code>&#13;
&#13;
<code class="n">item_view_counts_sdf</code> <code class="o">=</code> <code class="n">spark</code><code class="o">.</code><code class="n">sql</code><code class="p">(</code><code class="n">item_popularity_qry</code><code class="p">)</code></pre>&#13;
&#13;
<p>We can now write this aggregated list of <code>(item, count)</code> pairs to an app database to serve <code>get_item_popularities</code> (something that doesn’t require us to do any parsing when this is called), or potentially we can take a subset of the top-<math alttext="upper N">&#13;
  <mi>N</mi>&#13;
</math> of this list and store it in memory to get the best items with respect to a particular ranking. Either way, we’ve separated concerns of parsing all our log data, and doing aggregation, from the <code>get_item_popularities</code> function call in real time.</p>&#13;
&#13;
<p>This example used an overly simple data aggregation, one just as easy to do in something like PostgreSQL, so why bother? The first reason is scalability. Spark is really built to scale horizontally, which means that as the data we need to access grows, we merely add more worker nodes.</p>&#13;
&#13;
<p>The second reason is that PySpark is more than just SparkSQL; anyone who’s  done complicated SQL queries can probably agree that the power and flexibility of SQL is enormous, but frequently certain tasks that you want to achieve require a lot of creativity to carry out in the fully SQL environment. PySpark gives you all the expressiveness of pandas DataFrames, Python functions and classes, and a simple interface to apply Python code to the PySpark data structure’s user-defined functions (UDFs). UDFs are similar to lambda functions that you’d use in pandas, but they’re built and optimized for PySpark DataFrames. As you’ve probably experienced when writing ML programs in smaller data regimes, at some point you switch away from using only SQL to using pandas API functions to perform data transformations—so too will you appreciate this power at the Spark data scale.</p>&#13;
&#13;
<p>PySpark allows you to write what looks very much like Python and pandas code and have that code executed in a distributed fashion! You don’t need to write code to specify which worker nodes operations should happen; that’s handled for you by PySpark. This framework isn’t perfect; some things you expect to work may require a bit of care, and optimization of your code can require an additional level of abstraction, but generally, PySpark gives you a rapid way to move your code from one node to a cluster and utilize that power.</p>&#13;
&#13;
<p>To illustrate something a bit more useful in PySpark, let’s return to collaborative filtering (CF) and compute some features more relevant for ranking.<a data-primary="" data-startref="DPpyspark06" data-type="indexterm" id="id524"/><a data-primary="" data-startref="PSimport06" data-type="indexterm" id="id525"/><a data-primary="" data-startref="spark06" data-type="indexterm" id="id526"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Example: User Similarity in PySpark" data-type="sect2"><div class="sect2" id="id52">&#13;
<h2>Example: User Similarity in PySpark</h2>&#13;
&#13;
<p>A<a data-primary="PySpark" data-secondary="user similarity in" data-type="indexterm" id="PSusersim06"/><a data-primary="similarity" data-secondary="user similarity" data-type="indexterm" id="Suser06"/><a data-primary="user similarity" data-secondary="in PySpark" data-secondary-sortas="PySpark" data-type="indexterm" id="USpyspark06"/> user similarity table allows you to map a user to other users who are relevant to the recommender. This recalls the assumption that two similar users like similar things, and thus you can recommend to both users the items that one hasn’t seen. Constructing this user similarity table is an example of a PySpark job that you might see as part of the offline collector’s responsibility. Even though in many cases ratings would continue to stream in all the time, for the purposes of large offline jobs, we often think of a daily batch to update the essential tables for our model. In practice, in many cases this daily batch job suffices to provide features that are good enough for most of the ML work downstream. Other important paradigms exist, but those frequently <em>marry</em> the more frequent updates with these daily batch jobs, instead of totally eliminating them.</p>&#13;
&#13;
<p>This architecture of daily batch jobs with smaller, more frequent batch jobs is called the<a data-primary="Lambda architecture" data-type="indexterm" id="id527"/> <em>lambda architecture</em>, and we’ll get more into the details of how and why later. In brief, the two layers—batch and speed—which are distinguished (inversely) by the frequency of processing and the volume per run of data they process. Note that the speed layer may have varying frequencies associated with it, and it’s possible to have different speed layers for hourly, and another speed layer for minute-frequency jobs that do different things. <a data-type="xref" href="#fig:lamb-architecture">Figure 6-2</a> provides an overview of the architecture.</p>&#13;
&#13;
<figure><div class="figure" id="fig:lamb-architecture">&#13;
<img alt="LambdaArchitecture" src="assets/brpj_0602.png"/>&#13;
<h6><span class="label">Figure 6-2. </span>Overview of a lambda architecture</h6>&#13;
</div></figure>&#13;
&#13;
<p>In the case of user similarity, let’s work on a batch job implementation of computing a daily table. First we’ll need to get ratings from our schema before today. We’ll also include a few other filters that simulate how this query might look in real life:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">user_item_ratings_qry</code> <code class="o">=</code> <code class="s2">"""</code>&#13;
<code class="s2">SELECT</code>&#13;
<code class="s2">  book_ratings.book_id</code>&#13;
<code class="s2">  book_ratings.user_id</code>&#13;
<code class="s2">  , book_ratings.rating_value</code>&#13;
<code class="s2">  , book_ratings.rating_tstamp</code>&#13;
&#13;
<code class="s2">FROM prod.book_ratings</code>&#13;
<code class="s2">JOIN prod.dim_users</code>&#13;
<code class="s2">	ON book_ratings.user_id = dim_users.user_id</code>&#13;
<code class="s2">JOIN prod.dim_books</code>&#13;
<code class="s2">	ON book_ratings.book_id = dim_books.dim_books</code>&#13;
&#13;
<code class="s2">WHERE</code>&#13;
<code class="s2">	DATE book_ratings.rating_tstamp</code>&#13;
<code class="s2">		BETWEEN (DATE '2017-01-01')</code>&#13;
<code class="s2">		 AND (CAST(current_timestamp() as DATE)</code>&#13;
<code class="s2">  AND book_ratings.rating_value IS NOT NULL</code>&#13;
<code class="s2">	AND dim_users.country_code = 'US'</code>&#13;
<code class="s2">  AND dim_books.book_active</code>&#13;
<code class="s2">"""</code>&#13;
&#13;
<code class="n">user_item_ratings_sdf</code> <code class="o">=</code> <code class="n">spark</code><code class="o">.</code><code class="n">sql</code><code class="p">(</code><code class="n">user_item_ratings_qry</code><code class="p">)</code></pre>&#13;
&#13;
<p>As before, utilizing the SQL syntax to get the dataset into a Spark DataFrame is the first step, but now we have additional work on the PySpark side. A common pattern is to get the dataset you want to work with via simple SQL syntax and logic, and then use the PySpark API to do more detailed data processing.</p>&#13;
&#13;
<p>Let’s first observe that we have no assumptions about uniqueness of a user-item rating. For the sake of this table, let’s decide that we’ll use the most recent rating for a pair:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">pyspark.sql.window</code> <code class="kn">import</code> <code class="n">Window</code>&#13;
&#13;
<code class="n">windows</code> <code class="o">=</code> <code class="n">Window</code><code class="p">()</code><code class="o">.</code><code class="n">partitionBy</code><code class="p">(</code>&#13;
	<code class="p">[</code><code class="s1">'book_id'</code><code class="p">,</code> <code class="s1">'user_id'</code><code class="p">]</code>&#13;
<code class="p">)</code><code class="o">.</code><code class="n">orderBy</code><code class="p">(</code>&#13;
	<code class="n">col</code><code class="p">(</code><code class="s2">"rating_tstamp"</code><code class="p">)</code><code class="o">.</code><code class="n">desc</code><code class="p">()</code>&#13;
<code class="p">)</code>&#13;
&#13;
<code class="n">user_item_ratings_sdf</code><code class="o">.</code><code class="n">withColumn</code><code class="p">(</code>&#13;
	<code class="s2">"current_rating"</code><code class="p">,</code>&#13;
	<code class="n">first</code><code class="p">(</code>&#13;
		<code class="n">user_item_ratings_sdf</code><code class="p">(</code><code class="s2">"rating_tstamp"</code><code class="p">)</code>&#13;
	<code class="p">)</code><code class="o">.</code><code class="n">over</code><code class="p">(</code><code class="n">windows</code><code class="p">)</code><code class="o">.</code><code class="k">as</code><code class="p">(</code><code class="s2">"max_rating_tstamp"</code><code class="p">)</code>&#13;
<code class="p">)</code><code class="o">.</code><code class="n">filter</code><code class="p">(</code><code class="s2">"rating_tstamp = max_rating_tstamp"</code><code class="p">)</code></pre>&#13;
&#13;
<p>We’ll now use <code>current_rating</code> as our ratings column for the purpose of downstream calculation. Recall from before our ratings-based definition of user similarity:</p>&#13;
<div data-type="equation">&#13;
<math alttext="normal upper U normal upper S normal i normal m Subscript upper A comma upper B Baseline equals StartFraction sigma-summation Underscript x element-of script upper R Subscript upper A comma upper B Baseline Endscripts left-parenthesis r Subscript upper A comma x Baseline minus r overbar Subscript upper A Baseline right-parenthesis left-parenthesis r Subscript upper B comma x Baseline minus r overbar Subscript upper B Baseline right-parenthesis Over StartRoot sigma-summation Underscript x element-of script upper R Subscript upper A comma upper B Baseline Endscripts left-parenthesis r Subscript upper A comma x Baseline minus r overbar Subscript upper A Baseline right-parenthesis squared EndRoot StartRoot sigma-summation Underscript x element-of script upper R Subscript upper A comma upper B Baseline Endscripts left-parenthesis r Subscript upper B comma x Baseline minus r overbar Subscript upper B Baseline right-parenthesis squared EndRoot EndFraction" display="block">&#13;
  <mrow>&#13;
    <msub><mi> USim </mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow> </msub>&#13;
    <mo>=</mo>&#13;
    <mfrac><mrow><msub><mo>∑</mo> <mrow><mi>x</mi><mo>∈</mo><msub><mi>ℛ</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow> </msub></mrow> </msub><mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>A</mi><mo>,</mo><mi>x</mi></mrow> </msub><mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>A</mi> </msub><mo>)</mo></mrow><mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>B</mi><mo>,</mo><mi>x</mi></mrow> </msub><mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>B</mi> </msub><mo>)</mo></mrow></mrow> <mrow><msqrt><mrow><msub><mo>∑</mo> <mrow><mi>x</mi><mo>∈</mo><msub><mi>ℛ</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow> </msub></mrow> </msub><msup><mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>A</mi><mo>,</mo><mi>x</mi></mrow> </msub><mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>A</mi> </msub><mo>)</mo></mrow> <mn>2</mn> </msup></mrow></msqrt><msqrt><mrow><msub><mo>∑</mo> <mrow><mi>x</mi><mo>∈</mo><msub><mi>ℛ</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow> </msub></mrow> </msub><msup><mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>B</mi><mo>,</mo><mi>x</mi></mrow> </msub><mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>B</mi> </msub><mo>)</mo></mrow> <mn>2</mn> </msup></mrow></msqrt></mrow></mfrac>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>The important values we’ll need are as follows:</p>&#13;
<dl>&#13;
<dt><math alttext="r Subscript left-parenthesis minus comma minus right-parenthesis">&#13;
  <msub><mi>r</mi> <mrow><mo>(</mo><mo>-</mo><mo>,</mo><mo>-</mo><mo>)</mo></mrow> </msub>&#13;
</math></dt>&#13;
<dd>&#13;
<p>The rating corresponding to a user-item pair</p>&#13;
</dd>&#13;
<dt><math alttext="r overbar Subscript left-parenthesis minus right-parenthesis">&#13;
  <msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mrow><mo>(</mo><mo>-</mo><mo>)</mo></mrow> </msub>&#13;
</math></dt>&#13;
<dd>&#13;
<p>The average rating across all items for a user</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p class="less_space pagebreak-before">The rows are already the <math alttext="r Subscript left-parenthesis minus comma minus right-parenthesis">&#13;
  <msub><mi>r</mi> <mrow><mo>(</mo><mo>-</mo><mo>,</mo><mo>-</mo><mo>)</mo></mrow> </msub>&#13;
</math> values, so let’s compute user average ratings, <math alttext="r overbar Subscript left-parenthesis minus right-parenthesis">&#13;
  <msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mrow><mo>(</mo><mo>-</mo><mo>)</mo></mrow> </msub>&#13;
</math>, and the rating deviations:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">pyspark.sql.window</code> <code class="kn">import</code> <code class="n">Window</code>&#13;
<code class="kn">from</code> <code class="nn">pyspark.sql</code> <code class="kn">import</code> <code class="n">functions</code> <code class="k">as</code> <code class="n">F</code>&#13;
&#13;
<code class="n">user_partition</code> <code class="o">=</code> <code class="n">Window</code><code class="o">.</code><code class="n">partitionBy</code><code class="p">(</code><code class="s1">'user_id'</code><code class="p">)</code>&#13;
&#13;
<code class="n">user_item_ratings_sdf</code> <code class="o">=</code> <code class="n">user_item_ratings_sdf</code><code class="o">.</code><code class="n">withColumn</code><code class="p">(</code>&#13;
	<code class="s2">"user_average_rating"</code><code class="p">,</code>&#13;
	<code class="n">F</code><code class="o">.</code><code class="n">avg</code><code class="p">(</code><code class="s2">"current_rating"</code><code class="p">)</code><code class="o">.</code><code class="n">over</code><code class="p">(</code><code class="n">user_partition</code><code class="p">)</code>&#13;
<code class="p">)</code>&#13;
&#13;
<code class="n">user_item_ratings_sdf</code> <code class="o">=</code> <code class="n">user_item_ratings_sdf</code><code class="o">.</code><code class="n">withColumn</code><code class="p">(</code>&#13;
	<code class="s2">"rating_deviation_from_user_mean"</code><code class="p">,</code>&#13;
	<code class="n">F</code><code class="o">.</code><code class="n">col</code><code class="p">(</code><code class="s2">"current_rating"</code><code class="p">)</code> <code class="o">-</code> <code class="n">F</code><code class="o">.</code><code class="n">col</code><code class="p">(</code><code class="s2">"user_average_rating"</code><code class="p">)</code>&#13;
<code class="p">)</code></pre>&#13;
&#13;
<p>Now our schema should look like this (we’ve formatted it slightly nicer than the default Spark output):</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="o">+-------+-------+------------+-------------+</code>&#13;
<code class="o">|</code><code class="n">book_id</code><code class="o">|</code><code class="n">user_id</code><code class="o">|</code><code class="n">rating_value</code><code class="o">|</code><code class="n">rating_tstamp</code><code class="o">|</code>&#13;
<code class="o">+-------+-------+------------+-------------+</code>&#13;
<code class="o">+-------------+-------------------+-------------------------------+</code>&#13;
<code class="n">current_rating</code><code class="o">|</code><code class="n">user_average_rating</code><code class="o">|</code><code class="n">rating_deviation_from_user_mean</code><code class="o">|</code>&#13;
<code class="o">+-------------+-------------------+-------------------------------+</code></pre>&#13;
&#13;
<p>Let’s finish creating a dataset that contains our User Similarity calculations:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">user_pair_item_rating_deviations</code> <code class="o">=</code> <code class="n">user_item_ratings_sdf</code><code class="o">.</code><code class="n">alias</code><code class="p">(</code><code class="s2">"left_ratings"</code><code class="p">)</code>&#13;
<code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">user_item_ratings_sdf</code><code class="o">.</code><code class="n">alias</code><code class="p">(</code><code class="s2">"right_ratings"</code><code class="p">),</code>&#13;
  <code class="p">(</code>&#13;
<code class="n">F</code><code class="o">.</code><code class="n">col</code><code class="p">(</code><code class="s2">"left_ratings.book_id"</code><code class="p">)</code> <code class="o">==</code> <code class="n">F</code><code class="o">.</code><code class="n">col</code><code class="p">(</code><code class="s2">"right_ratings.book_id"</code><code class="p">)</code> <code class="o">&amp;</code>\&#13;
<code class="n">F</code><code class="o">.</code><code class="n">col</code><code class="p">(</code><code class="s2">"left_ratings.user_id"</code><code class="p">)</code> <code class="o">!=</code> <code class="n">F</code><code class="o">.</code><code class="n">col</code><code class="p">(</code><code class="s2">"right_ratings.user_id"</code><code class="p">)</code>&#13;
	<code class="p">),</code>&#13;
	<code class="s2">"inner"</code>&#13;
<code class="p">)</code><code class="o">.</code><code class="n">select</code><code class="p">(</code>&#13;
	<code class="n">F</code><code class="o">.</code><code class="n">col</code><code class="p">(</code><code class="s2">"left_ratings.book_id"</code><code class="p">),</code>&#13;
	<code class="n">F</code><code class="o">.</code><code class="n">col</code><code class="p">(</code><code class="s2">"left_ratings.user_id"</code><code class="p">)</code><code class="o">.</code><code class="n">alias</code><code class="p">(</code><code class="s2">"user_id_1"</code><code class="p">),</code>&#13;
	<code class="n">F</code><code class="o">.</code><code class="n">col</code><code class="p">(</code><code class="s2">"right_ratings.user_id"</code><code class="p">)</code><code class="o">.</code><code class="n">alias</code><code class="p">(</code><code class="s2">"user_id_2"</code><code class="p">),</code>&#13;
  <code class="n">F</code><code class="o">.</code><code class="n">col</code><code class="p">(</code><code class="s2">"left_ratings.rating_deviation_from_user_mean"</code><code class="p">)</code><code class="o">.</code><code class="n">alias</code><code class="p">(</code><code class="s2">"dev_1"</code><code class="p">),</code>&#13;
  <code class="n">F</code><code class="o">.</code><code class="n">col</code><code class="p">(</code><code class="s2">"right_ratings.rating_deviation_from_user_mean"</code><code class="p">)</code><code class="o">.</code><code class="n">alias</code><code class="p">(</code><code class="s2">"dev_2"</code><code class="p">)</code>&#13;
<code class="p">)</code><code class="o">.</code><code class="n">withColumn</code><code class="p">(</code>&#13;
	<code class="s1">'dev_product'</code><code class="p">,</code>&#13;
	<code class="n">F</code><code class="o">.</code><code class="n">col</code><code class="p">(</code><code class="s2">"dev_1"</code><code class="p">)</code><code class="o">*</code><code class="n">F</code><code class="o">.</code><code class="n">col</code><code class="p">(</code><code class="s2">"dev_2"</code><code class="p">)</code>&#13;
<code class="p">)</code>&#13;
&#13;
<code class="n">user_similarities_sdf</code> <code class="o">=</code> <code class="n">user_pair_item_rating_deviations</code><code class="o">.</code><code class="n">groupBy</code><code class="p">(</code>&#13;
	<code class="s2">"user_id_1"</code><code class="p">,</code> <code class="s2">"user_id_2"</code>&#13;
<code class="p">)</code><code class="o">.</code><code class="n">agg</code><code class="p">(</code>&#13;
	<code class="nb">sum</code><code class="p">(</code><code class="s1">'dev_product'</code><code class="p">)</code><code class="o">.</code><code class="n">alias</code><code class="p">(</code><code class="s2">"dev_product_sum"</code><code class="p">),</code>&#13;
	<code class="nb">sum</code><code class="p">(</code><code class="n">F</code><code class="o">.</code><code class="n">pow</code><code class="p">(</code><code class="n">F</code><code class="o">.</code><code class="n">col</code><code class="p">(</code><code class="s2">"dev_1"</code><code class="p">),</code><code class="mi">2</code><code class="p">))</code><code class="o">.</code><code class="n">alias</code><code class="p">(</code><code class="s2">"sum_of_sqrd_devs_1"</code><code class="p">),</code>&#13;
	<code class="nb">sum</code><code class="p">(</code><code class="n">F</code><code class="o">.</code><code class="n">pow</code><code class="p">(</code><code class="n">F</code><code class="o">.</code><code class="n">col</code><code class="p">(</code><code class="s2">"dev_2"</code><code class="p">),</code><code class="mi">2</code><code class="p">))</code><code class="o">.</code><code class="n">alias</code><code class="p">(</code><code class="s2">"sum_of_sqrd_devs_2"</code><code class="p">)</code>&#13;
<code class="p">)</code><code class="o">.</code><code class="n">withColumn</code><code class="p">(</code>&#13;
	<code class="s2">"user_similarity"</code><code class="p">,</code>&#13;
	<code class="p">(</code>&#13;
		<code class="n">F</code><code class="o">.</code><code class="n">col</code><code class="p">(</code><code class="s2">"dev_product_sum"</code><code class="p">)</code> <code class="o">/</code> <code class="p">(</code>&#13;
			<code class="n">F</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">F</code><code class="o">.</code><code class="n">col</code><code class="p">(</code><code class="s2">"sum_of_sqrd_devs_2"</code><code class="p">))</code> <code class="o">*</code>&#13;
			<code class="n">F</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">F</code><code class="o">.</code><code class="n">col</code><code class="p">(</code><code class="s2">"sum_of_sqrd_devs_2"</code><code class="p">))</code>&#13;
		<code class="p">)</code>&#13;
	<code class="p">)</code>&#13;
<code class="p">)</code></pre>&#13;
&#13;
<p>In constructing this dataset, we begin by taking a self-join, which avoids matching the same users with themselves but rather joins on books that match. As we do this join, we take the rating deviation from the user’s mean ratings that we computed previously. We also use this opportunity to multiply them together for the numerator in our user similarity function. In the last step, we’ll <code>groupBy</code> again so that we can sum over all matching book IDs (by <code>groupBy</code> on <code>user_id_1</code> and <code>user_id_2</code>); we sum the product and the powers of each set of deviations so that we can finally divide and generate a new column for our user similarity.</p>&#13;
&#13;
<p>While this computation isn’t particularly complex, let’s take note of a few things that we might appreciate. First, we built our user similarity matrix in full from our records. This matrix may now be stored in a faster-access format so that if we wish to do operations in real time, it’s ready to go. Second, we did all these data transformations in Spark, so we can run these operations on massive datasets and let Spark handle the parallelization onto the cluster. We even were able to do this while writing code that looks a lot like pandas and SQL. Finally, all our operations were columnar and required no iteration-based calculation. This means this code will scale much better than some approaches. This also ensures that Spark can parallelize our code well, and we can expect good performance.</p>&#13;
&#13;
<p>We’ve seen how PySpark can be used to prepare our user similarity matrix. We have this definition of affinity estimating the appropriateness of an item for a user; we can collect each of those scores into a tabular form—user rows and item columns—to yield a matrix. As an exercise, can you take this matrix and generate the affinity matrix?</p>&#13;
<div data-type="equation">&#13;
<math alttext="normal upper A normal f normal f Subscript upper A comma i Baseline equals r overbar Subscript upper A Baseline plus StartFraction sigma-summation Underscript upper U element-of script upper N left-parenthesis upper A right-parenthesis Endscripts normal upper U normal upper S normal i normal m Subscript upper A comma upper U Baseline asterisk left-parenthesis r Subscript upper U comma i Baseline minus r overbar Subscript upper A Baseline right-parenthesis Over sigma-summation Underscript upper U element-of script upper N left-parenthesis upper A right-parenthesis Endscripts normal upper U normal upper S normal i normal m Subscript upper A comma upper U Baseline EndFraction" display="block">&#13;
  <mrow>&#13;
    <msub><mi> Aff </mi> <mrow><mi>A</mi><mo>,</mo><mi>i</mi></mrow> </msub>&#13;
    <mo>=</mo>&#13;
    <msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>A</mi> </msub>&#13;
    <mo>+</mo>&#13;
    <mfrac><mrow><msub><mo>∑</mo> <mrow><mi>U</mi><mo>∈</mo><mi>𝒩</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow> </msub><msub><mi> USim </mi> <mrow><mi>A</mi><mo>,</mo><mi>U</mi></mrow> </msub><mo>*</mo><mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>U</mi><mo>,</mo><mi>i</mi></mrow> </msub><mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>A</mi> </msub><mo>)</mo></mrow></mrow> <mrow><msub><mo>∑</mo> <mrow><mi>U</mi><mo>∈</mo><mi>𝒩</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow> </msub><msub><mi> USim </mi> <mrow><mi>A</mi><mo>,</mo><mi>U</mi></mrow> </msub></mrow></mfrac>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Feel free to assume that <math alttext="script upper N left-parenthesis upper A right-parenthesis">&#13;
  <mrow>&#13;
    <mi>𝒩</mi>&#13;
    <mo>(</mo>&#13;
    <mi>A</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math> is just the five nearest neighbors to <math alttext="upper A">&#13;
  <mi>A</mi>&#13;
</math> with respect to user similarity.<a data-primary="" data-startref="PSusersim06" data-type="indexterm" id="id528"/><a data-primary="" data-startref="Suser06" data-type="indexterm" id="id529"/><a data-primary="" data-startref="USpyspark06" data-type="indexterm" id="id530"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="less_space pagebreak-before" data-pdf-bookmark="DataLoaders" data-type="sect2"><div class="sect2" id="id53">&#13;
<h2>DataLoaders</h2>&#13;
&#13;
<p>DataLoaders<a data-primary="data processing" data-secondary="prescribing how data is batched" data-type="indexterm" id="DPdload06"/><a data-primary="DataLoaders" data-type="indexterm" id="dataloaders06"/> is a programming paradigm originating from PyTorch, but it has been embraced in other gradient-optimized ML workflows. As we begin to integrate gradient-based learning into our recommendation system architectures, we will face challenges in our MLOps tooling. The first is related to training data size and available memory. DataLoaders are a way to prescribe how data is batched and sent to the training loop efficiently; as datasets get large, careful scheduling of these training sets can have major effects on learning. But why must we think about <em>batches</em> of data? That’s because we’ll use a variant of gradient descent appropriate for large amounts of data.</p>&#13;
&#13;
<p>First, let’s review the basics of<a data-primary="mini-batched gradient descent" data-type="indexterm" id="id531"/><a data-primary="gradient descent" data-secondary="mini-batched gradient descent" data-type="indexterm" id="id532"/> <em>mini-batched gradient descent</em>. During training via gradient descent, we make a forward pass of our training sample through our model to yield a prediction, and we then compute the error and the appropriate gradient backward through our model to update parameters. Batched gradient descent takes all our data in a single pass to compute the gradient for the training set and push it back through; this implies you have the entire training dataset in memory. As the dataset scales, this ranges from expensive to impossible; to avoid this, we can instead compute gradients of the loss function for only a subset of the dataset at a time. The simplest paradigm for this, called<a data-primary="SGD (stochastic gradient descent)" data-type="indexterm" id="id533"/><a data-primary="stochastic gradient descent (SGD)" data-type="indexterm" id="id534"/><a data-primary="gradient descent" data-secondary="stochastic gradient descent" data-type="indexterm" id="id535"/> <em>stochastic gradient descent</em> (SGD), computes these gradients and parameter updates one sample at a time. The mini-batched version performs our batched gradient descent, but over a series of subsets to form a partition of our dataset. In mathematical notation, we write the update rule in terms of Jacobians on the smaller batches:</p>&#13;
<div data-type="equation">&#13;
<math alttext="theta equals theta minus eta asterisk nabla Subscript theta Baseline upper J left-parenthesis theta semicolon x Superscript left-parenthesis i colon i plus n right-parenthesis Baseline semicolon y Superscript left-parenthesis i colon i plus n right-parenthesis Baseline right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <mi>θ</mi>&#13;
    <mo>=</mo>&#13;
    <mi>θ</mi>&#13;
    <mo>-</mo>&#13;
    <mi>η</mi>&#13;
    <mo>*</mo>&#13;
    <msub><mi>∇</mi> <mi>θ</mi> </msub>&#13;
    <mi>J</mi>&#13;
    <mfenced close=")" open="(" separators="">&#13;
      <mi>θ</mi>&#13;
      <mo>;</mo>&#13;
      <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>:</mo><mi>i</mi><mo>+</mo><mi>n</mi><mo>)</mo></mrow> </msup>&#13;
      <mo>;</mo>&#13;
      <msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>:</mo><mi>i</mi><mo>+</mo><mi>n</mi><mo>)</mo></mrow> </msup>&#13;
    </mfenced>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>This optimization serves a few purposes. First, it requires only potentially small subsets of our data held in memory during the steps. Second, it requires far fewer passes than the purely iterative version in SGD. Third, the gradient operating on these mini-batches can be organized as a Jacobian, and thus we have linear-algebraic operations that may be highly optimized.</p>&#13;
<div data-type="warning" epub:type="warning"><h1>Jacobians</h1>&#13;
<p>The<a data-primary="Jacobian matrix" data-type="indexterm" id="id536"/> mathematical notion of a Jacobian in the simplest sense is an organizational tool for a set of vector derivatives with relevant indexes. You may recall that for functions of several variables, you can take the derivative <em>with respect to</em> each of those variables. For a single multivariable scalar function, the Jacobian is simply the row vector of first derivatives of the function—which happens to be the transpose of the gradient.</p>&#13;
&#13;
<p>This is the simplest case; the gradient of a multivariable scalar function may be written as a Jacobian. However, once we have a vector of (vector) derivatives, we can write that as a matrix; the utility here is really only in the notation, though. When you collect a series of multivariable scalar functions into a vector of functions, the associated vector of gradients is a vector of vectors of derivatives. This is called a <em>Jacobian matrix</em>, and it generalizes the gradient to vector-valued functions. As you’ve likely realized, layers of neural networks are a great source of vector-valued functions for which you’d like to derivate.</p>&#13;
</div>&#13;
&#13;
<p>If you’re convinced mini-batches are useful, it’s time to discuss <em>DataLoaders</em>—a simple PyTorch API for facilitating mini-batch access from a large dataset. The key parameters for a DataLoader are <code>batch_size</code>, <code>shuffle</code>, and <code>num_workers</code>. The batch size is easy to understand: it’s the number of samples included in each batch (often an integer factor of the total size of the dataset). Often a shuffle operation is applied in serving up these batches; the shuffle allows batches in each epoch to be shown to the network in a randomized order; this is intended to improve robustness. Finally, <code>num_workers</code> is a parallelization parameter for the CPU’s batch generation.</p>&#13;
&#13;
<p>The utility of a DataLoader is really best understood via demonstration:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">params</code> <code class="o">=</code> <code class="p">{</code>&#13;
         <code class="s1">'batch_size'</code><code class="p">:</code> <code class="n">_</code><code class="p">,</code>&#13;
         <code class="s1">'shuffle'</code><code class="p">:</code> <code class="n">_</code><code class="p">,</code>&#13;
         <code class="s1">'num_workers'</code><code class="p">:</code> <code class="n">_</code>&#13;
<code class="p">}</code>&#13;
&#13;
<code class="n">training_generator</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">utils</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">DataLoader</code><code class="p">(</code><code class="n">training_set</code><code class="p">,</code> <code class="n">params</code><code class="p">)</code>&#13;
&#13;
<code class="n">validation_generator</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">utils</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">DataLoader</code><code class="p">(</code><code class="n">validation_set</code><code class="p">,</code> <code class="n">params</code><code class="p">)</code>&#13;
&#13;
<code class="o">//</code> <code class="n">Loop</code> <code class="n">over</code> <code class="n">epochs</code>&#13;
<code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">max_epochs</code><code class="p">):</code>&#13;
    <code class="o">//</code> <code class="n">Training</code>&#13;
    <code class="k">for</code> <code class="n">local_batch</code><code class="p">,</code> <code class="n">local_labels</code> <code class="ow">in</code> <code class="n">training_generator</code><code class="p">:</code>&#13;
&#13;
        <code class="o">//</code> <code class="n">Model</code> <code class="n">computations</code>&#13;
        <code class="p">[</code><code class="o">...</code><code class="p">]</code>&#13;
&#13;
    <code class="o">//</code> <code class="n">Validation</code>&#13;
    <code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">set_grad_enabled</code><code class="p">(</code><code class="kc">False</code><code class="p">):</code>&#13;
        <code class="k">for</code> <code class="n">local_batch</code><code class="p">,</code> <code class="n">local_labels</code> <code class="ow">in</code> <code class="n">validation_generator</code><code class="p">:</code>&#13;
&#13;
            <code class="o">//</code> <code class="n">Model</code> <code class="n">computations</code>&#13;
            <code class="p">[</code><code class="o">...</code><code class="p">]</code></pre>&#13;
&#13;
<p>The first important detail in this code is that any of its generators will be reading in mini-batches from your total dataset  and can be instructed to load those batches in parallel. Note also that any differential steps in the model computations will now be operating on these mini-batches.</p>&#13;
&#13;
<p>It’s easy to think of DataLoaders as merely a tool for code cleanliness (which, admittedly, it does improve), but it’s important to not underestimate how the control of batch order, parallelization, and shape are significant features for training your model. Lastly, the structure of your code now looks like batch gradient descent, but it is taking advantage of mini-batching, further exposing what your code actually does instead of the steps necessary to do it.<a data-primary="" data-startref="dataloaders06" data-type="indexterm" id="id537"/><a data-primary="" data-startref="DPdload06" data-type="indexterm" id="id538"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Database Snapshots" data-type="sect2"><div class="sect2" id="id54">&#13;
<h2>Database Snapshots</h2>&#13;
&#13;
<p>Let’s<a data-primary="data processing" data-secondary="snapshotting production databases" data-type="indexterm" id="id539"/><a data-primary="snapshots" data-type="indexterm" id="id540"/> round out this section by stepping back from these fancy technologies to discuss something important and classic: snapshotting a production database.</p>&#13;
&#13;
<p>An extremely likely scenario is that the engineers (potentially also you) who have built the recommendations server are writing their logs and other application data to an SQL database. More likely than not, this database architecture and deployment are optimized for fast querying by the application across its most common use cases. As we’ve discussed, those logs may be in an event-style schema, and there are other tables that may require aggregation and roll-up to make any sense. For example, a <em>current inventory</em> table may require knowledge of start-of-day inventory and then aggregate a list of purchase events.</p>&#13;
&#13;
<p>All told, the production SQL database is usually a crucial component in the stack that’s geared to specific use. As the downstream consumer of this data, you may find yourself wanting different schemas, wanting lots of access to this database, and performing serious operations on this data. The most common paradigm is <em>database snapshotting</em>. Snapshotting is a functionality provided by various flavors of SQL to performantly make a clone of a database. While this snapshotting may take form in a variety of ways, let’s focus on a few that serve to simplify our systems and ensure they have the necessary data on hand:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>A daily table snapshot may be tied to an <code>as_of</code> field, or <em>the state of this table on this day</em>.</p>&#13;
</li>&#13;
<li>&#13;
<p>A daily table snapshot may be limited by time to see <em>what records have been added today</em>.</p>&#13;
</li>&#13;
<li>&#13;
<p>An event table snapshot may be used to feed a set of events into an event stream processor like Segment (note that you may also set up live event streams like Kafka).</p>&#13;
</li>&#13;
<li>&#13;
<p>An hourly aggregated table can be used for status logging or monitoring.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>In general, the paradigm is usually to operate on snapshots for downstream data processing. Many of the kinds of data processing we mentioned earlier—like computing user similarity—are operations that may require significant data reads.<a data-primary="recommendation systems" data-secondary="tips for building" data-type="indexterm" id="id541"/> <em>It’s important to not build ML applications that require extensive querying on the production database</em>, because doing so would likely decrease performance of the app and result in a slower user experience. This decrease will undermine the improvement made possible by your recommendations.</p>&#13;
&#13;
<p>Once you’ve snapshotted the tables you’re interested in, you can often find a collection of data pipelines useful to transform that data into even more specific tables in your<a data-primary="data processing" data-secondary="data warehouses" data-type="indexterm" id="id542"/> <em>data warehouse</em> (where you should be doing most of your work anyway). Tools like<a data-primary="Dagster" data-type="indexterm" id="id543"/><a data-primary="Airflow" data-type="indexterm" id="id544"/> Dagster, dbt, Apache Airflow, Argo, and Luigi are popular data-pipeline and workflow orchestration tools for extract, transform, load (ETL) operations.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Data Structures for Learning and Inference" data-type="sect1"><div class="sect1" id="id307">&#13;
<h1>Data Structures for Learning and Inference</h1>&#13;
&#13;
<p>This section introduces three important data structures that will enable our recommendation system to perform complex operations quickly. The goal of each structure is to sacrifice precision as little as possible, while speeding up access to the data in real time. As you’ll see, these data structures form the backbone of the real-time inference pipeline and approximate what takes place in the batch pipeline as accurately as possible.</p>&#13;
&#13;
<p>The three data structures are as follows:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Vector search/ANN index</p>&#13;
</li>&#13;
<li>&#13;
<p>Bloom filters for candidate filtering</p>&#13;
</li>&#13;
<li>&#13;
<p>Feature stores</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>So far, we’ve discussed the necessary components for getting data flowing in your system. These help organize data to make it more accessible during the learning and inference processes. Also, we’ll find some shortcuts to speed up inference during retrieval. Vector search will allow us to identify similar items at scale. Bloom filters will allow us to rapidly evaluate many criteria for excluding results. Feature stores will provide us with  necessary data about users for recommendation inference.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Vector Search" data-type="sect2"><div class="sect2" id="id55">&#13;
<h2>Vector Search</h2>&#13;
&#13;
<p>We have discussed user similarity and item similarity in terms of understanding the relationships between those entities, but we haven’t talked about any<a data-primary="acceleration structures" data-secondary="terminology related to" data-type="indexterm" id="id545"/><a data-primary="data processing" data-secondary="acceleration structures for" data-type="indexterm" id="id546"/><a data-primary="vector search" data-type="indexterm" id="id547"/> <em>acceleration structures</em> for these processes.</p>&#13;
&#13;
<p>First let’s discuss a bit of terminology;. If we think of a collection of vectors that represent entities with a similarity metric provided by a distance function, we refer to this as a<a data-primary="latent space" data-secondary="utilizing" data-type="indexterm" id="id548"/> <em>latent space.</em> The simple goal is to utilize our latent space and its associated similarity metric (or complementary distance metric) to be able to retrieve <em>similar</em> items quickly. In our previous examples with similarity, we talked about neighborhoods of users and how they can be utilized to build an affinity score between users and unseen items. But how do you find the neighborhood?</p>&#13;
&#13;
<p>To understand this, recall that we defined neighborhoods of an element <math alttext="x">&#13;
  <mi>x</mi>&#13;
</math>, written <math alttext="script upper N left-parenthesis x right-parenthesis">&#13;
  <mrow>&#13;
    <mi>𝒩</mi>&#13;
    <mo>(</mo>&#13;
    <mi>x</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>, as the set of <math alttext="k">&#13;
  <mi>k</mi>&#13;
</math> elements in the latent space with the maximum similarity; or said differently, the set of <math alttext="j">&#13;
  <mi>j</mi>&#13;
</math>th order statistics for <math alttext="j less-than-or-equal-to k">&#13;
  <mrow>&#13;
    <mi>j</mi>&#13;
    <mo>≤</mo>&#13;
    <mi>k</mi>&#13;
  </mrow>&#13;
</math> from the sample of item similarities to <math alttext="x">&#13;
  <mi>x</mi>&#13;
</math>. These <em><math alttext="k">&#13;
  <mi>k</mi>&#13;
</math>-nearest neighbors</em>, as they’re often called, will be used as the set of elements considered similar to <math alttext="x">&#13;
  <mi>x</mi>&#13;
</math>.</p>&#13;
&#13;
<p>These<a data-primary="collaborative filtering (CF)" data-secondary="vectors from" data-type="indexterm" id="id549"/> vectors from CF yield a few other useful side effects:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>A simple recommender that randomly samples unseen items from a user neighborhood’s liked items</p>&#13;
</li>&#13;
<li>&#13;
<p>Predictions about features of a user, from known features of users in the neighborhood</p>&#13;
</li>&#13;
<li>&#13;
<p>User segmentation via taste similarity</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>So how can we speed up these processes? One of the first significant improvements in this area came from inverted indices. Utilizing inverted indices is at its core carefully constructing a large hash between tokens of the query (for text-based search) and the candidates.</p>&#13;
&#13;
<p>This approach is great for tokenizable entities like sentences or small-lexicon collections. Given the ability to look up items that share one or many tokens with the query, you can even use a general latent embedding to rank the candidate responses by similarity. This approach deserves extra consideration as you scale: it incurs a speed cost because it entails two steps, and because the similarity distribution may not be well correlated with the token similarity required to return many more candidates than we need.</p>&#13;
&#13;
<p>Classic approaches to building a search system are based on large lookup tables and feel deterministic. As we move toward ANN lookup, we want to relax some of that strong deterministic behavior and introduce data structures that make assumptions to <em>prune</em> these large indices. Instead of building indices for only tokenizable <span class="keep-together">components</span> of your elements, you could precompute the <em>k</em>-d tree and use the indices as the index. The <em>k</em>-d tree would precompute the nearest neighbors in a batch process (which may be slow), to populate a top-<em>k</em> response for fast lookup. <em>k</em>-d trees are an efficient data structure for encoding the preceding neighborhoods but are notoriously slow to read from in higher dimensions. Using them instead to build inverted indices, though, can be a great improvement.</p>&#13;
&#13;
<p>More recently, explicitly using vector databases with vector search is becoming much more possible and feasible. Elasticsearch has added this capability; <a href="https://oreil.ly/AZ-Ai">Faiss</a> is a Python library that helps you implement this functionality in your systems; <a href="https://oreil.ly/LSaos">Pinecone</a> is a vector-database system explicitly targeting this goal; and <a href="https://oreil.ly/Z6la_">Weaviate</a> is a native vector-database architecture that allows you to layer the previous token-based inverted indices and vector similarity search.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Approximate Nearest Neighbors" data-type="sect2"><div class="sect2" id="Nearest-Neighbors">&#13;
<h2>Approximate Nearest Neighbors</h2>&#13;
&#13;
<p>What<a data-primary="data processing" data-secondary="approximating nearest neighbors" data-type="indexterm" id="id550"/><a data-primary="approximate nearest neighbors (ANN)" data-type="indexterm" id="id551"/> are this element’s <em>k</em>-nearest neighbors? Incredibly, approximate nearest neighbors (ANN) can get very high accuracy compared to the actual nearest neighbors, and you get there faster with head-spinning speedups. You often are satisfied with approximate solutions to these problems.</p>&#13;
&#13;
<p>One open source library that specializes in these approximations is <a href="https://oreil.ly/i5LyM">PyNNDescent</a>, which uses clever speedups via both optimized implementation and careful mathematical tricks. With ANN, you are opened up to two strategies as discussed:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The<a data-primary="pre-indexing" data-type="indexterm" id="id552"/> pre-index can be dramatically improved.</p>&#13;
</li>&#13;
<li>&#13;
<p>On queries without a pre-indexing option, you can still expect good performance.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>In practice, these similarity lookups are incredibly important for making your applications actually work. While we’ve mostly talked about recommendations for full known catalogs of items, we cannot assume this in other recommendation contexts. These include  the following:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Query-based recommendations (like search)</p>&#13;
</li>&#13;
<li>&#13;
<p>Contextual recommendations</p>&#13;
</li>&#13;
<li>&#13;
<p>Cold-starting new items</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>As we go, you will see more and more references to similarity in spaces and nearest neighbors; at each of those moments, think: “I know how to make this fast!”</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Bloom Filters" data-type="sect2"><div class="sect2" id="id57">&#13;
<h2>Bloom Filters</h2>&#13;
&#13;
<p><em>Bloom filters</em> are<a data-primary="data processing" data-secondary="testing for set inclusion" data-type="indexterm" id="DPsetinc06"/><a data-primary="testing" data-secondary="for set inclusions" data-secondary-sortas="set inclusions" data-type="indexterm" id="id553"/><a data-primary="bloom filters" data-type="indexterm" id="bloomfilters06"/> probabilistic data structures that allow us to test for set inclusion very efficiently but with a downside: set exclusion is deterministic, but set inclusion is probabilistic. <em>In practice, this means that asking the question “Is <math alttext="x">&#13;
  <mi>x</mi>&#13;
</math> in this set” never results in a false negative but may result in a false positive!</em> Note that this type-I error increases as the size of the bloom increases.</p>&#13;
&#13;
<p>Via vector search, we have identified a large pool of potential recommendations for the user. From this pool, we need to do some immediate elimination. The most obvious type of high-level filtering that’s essential is to remove those items that the <em>user has previously not shown interest in or has already purchased.</em> You’ve probably had the experience of being recommended the same item, over and over, and thinking, “I don’t want this; stop showing me this.” From the simple CF models we’ve introduced, you may now see why this could happen.</p>&#13;
&#13;
<p>The system has identified a set of items via CF that you’re more likely to pick. Without any outside influence, those computations will continue to return the same results, and you’ll never escape those recommendations. As the system designer, you may start with a heuristic:</p>&#13;
<blockquote>&#13;
<p>If the user has seen this item recommended three times and never clicked, let’s not show it to them anymore.</p></blockquote>&#13;
&#13;
<p>This is a totally reasonable strategy to improve<a data-primary="freshness" data-type="indexterm" id="id554"/> <em>freshness</em> (the idea of ensuring users see new item recommendations) in your recommendation system. While this is a simple strategy to improve your recommendations, how might you implement this at scale?</p>&#13;
&#13;
<p>A bloom filter may be used by defining the sets in question with the following: “Has this user seen this item recommended three times and never clicked?” Bloom filters have a caveat that they’re additive only: once something is in the bloom, you can’t remove it. This is not a problem when observing a binary state like this heuristic.</p>&#13;
&#13;
<p>Let’s construct a user-item ID to use as our hash in the bloom. Remember that the key feature of the bloom filter is to quickly determine whether the hashed item is in the bloom. When we observe a user-item pair that satisfies the preceding criteria, take that pair as an ID and hash it. Now, because that hashed pair can be easily reconstructed from a list of items for a user, we have a very fast way to filter.</p>&#13;
&#13;
<p>Let’s discuss a few technical details on this topic. First, you might want to do a variety of kinds of filtering—maybe freshness is one, and another may be items the user has already bought, and a third could exclude items that have sold out.</p>&#13;
&#13;
<p>Here it would be good to implement each of these filters independently; the first two can follow our user-item ID hashing as before, and the third one can be a hash only on item IDs.</p>&#13;
&#13;
<p>Another consideration is populating the bloom filters. It’s best practice to build these blooms from a database during the offline batch jobs. On whatever schedule your batch training is run, rebuild your blooms from the records storage to ensure you’re keeping your blooms accurate. Remember that blooms don’t allow deletion, so in the previous example, if an item goes from sold out to restocked, your batch refresh of your blooms can pick up the availability again. In between batch retraining, adding to a bloom is also very performant, so you can continue to add to the bloom as you observe more data that needs to be considered for the filtering in real time. Be sure these transactions are logged to a table, though! That logging will be important when you want to refresh.<a data-primary="" data-startref="DPsetinc06" data-type="indexterm" id="id555"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Fun Aside: Bloom Filters as the Recommendation System" data-type="sect2"><div class="sect2" id="id58">&#13;
<h2>Fun Aside: Bloom Filters as the Recommendation System</h2>&#13;
&#13;
<p>Bloom filters<a data-primary="recommendation systems" data-secondary="bloom filters as" data-type="indexterm" id="id556"/> not only provide an effective way to eliminate some recommendations based on conditions for inclusion, but can also be used to do the recommending itself! In particular, <a href="https://oreil.ly/VsvN2">“An Item/User Representation for Recommender&#13;
Systems Based on Bloom Filters”</a> by Manuel Pozo et al. shows that for high-dimensional feature sets with a lot of sparsity (as we discussed in <a data-type="xref" href="ch03.html#ch:math">Chapter 3</a>), the type of hashing bloom filters do can help overcome some of the key challenges in defining good similarity functions!</p>&#13;
&#13;
<p>Let’s observe that we can do two natural operations on sets via the bloom filter data structures. First, consider two sets <math alttext="upper A">&#13;
  <mi>A</mi>&#13;
</math> and <math alttext="upper B">&#13;
  <mi>B</mi>&#13;
</math>, and associate to them bloom filters <math alttext="script upper B script upper F Subscript upper A">&#13;
  <msub><mi>ℬℱ</mi> <mi>A</mi> </msub>&#13;
</math> and <math alttext="script upper B script upper F Subscript upper B">&#13;
  <msub><mi>ℬℱ</mi> <mi>B</mi> </msub>&#13;
</math>. Then what’s the definition of <math alttext="upper A intersection upper B">&#13;
  <mrow>&#13;
    <mi>A</mi>&#13;
    <mo>∩</mo>&#13;
    <mi>B</mi>&#13;
  </mrow>&#13;
</math>? Can we come up with a bloom filter for this intersection? Yep! Recall that our bloom filters are guaranteed to tell us when an element is not contained in the set, but if an element is in the set, the bloom filter can respond with only a certain probability. In this case, we’d simply look for elements that are <em>in</em> according to <math alttext="script upper B script upper F Subscript upper A">&#13;
  <msub><mi>ℬℱ</mi> <mi>A</mi> </msub>&#13;
</math> <em>AND</em> <em>in</em> according to <math alttext="script upper B script upper F Subscript upper B">&#13;
  <msub><mi>ℬℱ</mi> <mi>B</mi> </msub>&#13;
</math>. Of course, the set of items returned as <em>in</em> each set is larger than the actual set (i.e., <math alttext="upper A subset-of script upper B script upper F Subscript upper A">&#13;
  <mrow>&#13;
    <mi>A</mi>&#13;
    <mo>⊂</mo>&#13;
    <msub><mi>ℬℱ</mi> <mi>A</mi> </msub>&#13;
  </mrow>&#13;
</math>), so the intersection will also be larger:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper A intersection upper B subset-of script upper B script upper F Subscript upper A intersection script upper B script upper F Subscript upper B" display="block">&#13;
  <mrow>&#13;
    <mi>A</mi>&#13;
    <mo>∩</mo>&#13;
    <mi>B</mi>&#13;
    <mo>⊂</mo>&#13;
    <msub><mi>ℬℱ</mi> <mi>A</mi> </msub>&#13;
    <mo>∩</mo>&#13;
    <msub><mi>ℬℱ</mi> <mi>B</mi> </msub>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Note that you can compute the exact difference in cardinality via information about your choice of hashing functions. Also note that the equation is an abuse of notation by calling <math alttext="script upper B script upper F Subscript upper A">&#13;
  <msub><mi>ℬℱ</mi> <mi>A</mi> </msub>&#13;
</math> the set of things returned by the bloom filter corresponding to <math alttext="upper A">&#13;
  <mi>A</mi>&#13;
</math>.</p>&#13;
&#13;
<p>Second, we also need to construct the union. This is similarly easy by considering elements that are <em>in</em> according to <math alttext="script upper B script upper F Subscript upper A">&#13;
  <msub><mi>ℬℱ</mi> <mi>A</mi> </msub>&#13;
</math> <em>OR</em> <em>in</em> according to <math alttext="script upper B script upper F Subscript upper B">&#13;
  <msub><mi>ℬℱ</mi> <mi>B</mi> </msub>&#13;
</math>. And so, similarly:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper A union upper B subset-of script upper B script upper F Subscript upper A union script upper B script upper F Subscript upper B" display="block">&#13;
  <mrow>&#13;
    <mi>A</mi>&#13;
    <mo>∪</mo>&#13;
    <mi>B</mi>&#13;
    <mo>⊂</mo>&#13;
    <msub><mi>ℬℱ</mi> <mi>A</mi> </msub>&#13;
    <mo>∪</mo>&#13;
    <msub><mi>ℬℱ</mi> <mi>B</mi> </msub>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Now, if we consider items <math alttext="upper X">&#13;
  <mi>X</mi>&#13;
</math> and <math alttext="upper Y">&#13;
  <mi>Y</mi>&#13;
</math> as concatenated vectors of potentially many features, and hash those concatenated features, we are representing each of them as the bitwise vectors of our bloom. From before, we saw that the intersection of two blooms makes sense, and in fact is equivalent to the bitwise <em>AND</em> of their bloom representations. This means two items’ feature similarities can be expressed by the bitwise <em>and</em> similarity of their bloom hashes:</p>&#13;
<div data-type="equation">&#13;
<math alttext="normal s normal i normal m left-parenthesis upper X comma upper Y right-parenthesis equals StartAbsoluteValue script upper B script upper F left-parenthesis upper X right-parenthesis intersection script upper B script upper F left-parenthesis upper Y right-parenthesis EndAbsoluteValue equals script upper B script upper F left-parenthesis upper X right-parenthesis asterisk Subscript normal b normal i normal t normal w normal i normal s normal e Baseline script upper B script upper F left-parenthesis upper X right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <mi> sim </mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>X</mi>&#13;
      <mo>,</mo>&#13;
      <mi>Y</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mrow>&#13;
      <mo>|</mo>&#13;
      <mi>ℬℱ</mi>&#13;
      <mrow>&#13;
        <mo>(</mo>&#13;
        <mi>X</mi>&#13;
        <mo>)</mo>&#13;
      </mrow>&#13;
      <mo>∩</mo>&#13;
      <mi>ℬℱ</mi>&#13;
      <mrow>&#13;
        <mo>(</mo>&#13;
        <mi>Y</mi>&#13;
        <mo>)</mo>&#13;
      </mrow>&#13;
      <mo>|</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mi>ℬℱ</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>X</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <msub><mo>*</mo> <mi> bitwise </mi> </msub>&#13;
    <mi>ℬℱ</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>X</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>For static datasets, this method has real advantages, including speed, scalability, and performance. Limitations are based on a variety of features and on the ability to change the set of possible items. Later we will discuss <em>locally sensitive hashing</em>, which further iterates on lookup speed with lower risks of collision in high-dimensional spaces, and some similar ideas will reemerge.<a data-primary="" data-startref="bloomfilters06" data-type="indexterm" id="id557"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Feature Stores" data-type="sect2"><div class="sect2" id="feature-stores">&#13;
<h2>Feature Stores</h2>&#13;
&#13;
<p>So far, we have focused on<a data-primary="data processing" data-secondary="feature stores" data-type="indexterm" id="DPfeature06"/><a data-primary="feature stores" data-type="indexterm" id="featstor06"/> recommendation systems that we might call <em>pure collaborative filtering</em>. We’ve made use of the user- or item-similarity data only when attempting to make good recommendations. If you’ve been wondering, “Hey, what about information about the actual users and items?” your curiosity will now be sated.</p>&#13;
&#13;
<p>There are a huge variety of reasons you could be interested in features in addition to your previous CF methods. Let’s list a few high-level concerns:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>You may wish to show new users a specific set of items first.</p>&#13;
</li>&#13;
<li>&#13;
<p>You may wish to consider geographic boundaries in your recommendations.</p>&#13;
</li>&#13;
<li>&#13;
<p>Distinguishing between children and adults may be important for the types of recommendations they’re given.</p>&#13;
</li>&#13;
<li>&#13;
<p>Item features may be used to ensure high-level diversity in the recommendations (more to come in <a data-type="xref" href="ch15.html#Diversity">Chapter 15</a>).</p>&#13;
</li>&#13;
<li>&#13;
<p>User features can enable various kinds of experimental testing.</p>&#13;
</li>&#13;
<li>&#13;
<p>Item features could be used to group items into sets for contextual recommendations (more to come in <a data-type="xref" href="ch15.html#Diversity">Chapter 15</a>).</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>In addition to these issues, another kind of feature is often essential: real-time features. While the point of feature stores is to provide real-time access to all the necessary features, it’s worthwhile to distinguish stable features that change infrequently from real-time features that we anticipate will change often.</p>&#13;
&#13;
<p>Some important examples of a real-time feature store are dynamic prices, current item availability,<a data-primary="trending status" data-type="indexterm" id="id558"/> <em>trending</em> status, wish-list status, and so on. These features may change throughout the day, and we want their values in the feature store to be mutable in real-time via other services and systems. Therefore, the real-time feature store will need to provide API access for feature mutation. This is something you may not want to provide for <em>stable</em> features.</p>&#13;
&#13;
<p>When we design our feature store, we’re likely to want the stable features to be built from data warehouse tables via ETLs and transformations, and we likely want the real-time features to be built this way as well, but on a faster schedule or allowing API access for mutation. In either case, the key quality of a feature store is <em>very fast read access</em>. It’s often a good idea to separately build feature stores for offline training of models that can be built in test to ensure support for new models.</p>&#13;
&#13;
<p>So how might the architecture and implementation look? See <a data-type="xref" href="#fig:feat-store">Figure 6-3</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig:feat-store">&#13;
<img alt="Feature Store" src="assets/brpj_0603.png"/>&#13;
<h6><span class="label">Figure 6-3. </span>Demonstration of a feature store</h6>&#13;
</div></figure>&#13;
&#13;
<p>Designing a feature store involves designing pipelines that define and <em>transform the features into that store</em> (coordinated via things like Airflow, Luigi, Argo, etc.) and often look similar to the type of data pipelines used in building our collector. One additional complication that the feature store needs to concern itself with is a speed layer. During our discussion of the lambda architecture earlier in this chapter, we mentioned that we can think of batch data processing for the collector and a more rapid speed layer for intermediary updates, but this is even more important for the feature store. The feature store may also need a<a data-primary="streaming layer" data-type="indexterm" id="id559"/> <em>streaming layer</em>. This layer operates on continuous streams of data and can perform data transformations on those; it then writes the appropriate output to the online feature store in real time. This adds complexity because data transformations on streaming data present a very different set of challenges and often require different algorithmic strategies. Some technologies that help here are Spark Streaming and Kinesis. You’ll also need to configure the system to properly handle the data stream, the most common of which is<a data-primary="Kafka" data-type="indexterm" id="id560"/> Kafka. Data streaming layers involve many components and architectural considerations that fall outside our scope; if you’re considering getting started with Kafka, check out <a class="orm:hideurl" href="https://www.oreilly.com/library/view/kafka-the-definitive/9781492043072/"><em>Kafka: The Definitive Guide</em></a>  by Gwen Shapira et al. (O’Reilly).</p>&#13;
&#13;
<p>A feature store also needs a<a data-primary="storage layer" data-type="indexterm" id="id561"/> <em>storage layer</em>; many approaches exist here, but using a NoSQL database is common, especially in the online feature store. The reason is faster retrieval and the nature of the data storage. Feature stores for recommendation systems tend to be very key based (i.e., <em>get the features for this user</em>, or <em>get the features for this item</em>), which lend themselves well to key-value stores. Some example technologies here are DynamoDB, Redis, and Cassandra. The storage layer for an offline feature store may simply be an SQL-style database to reduce complexity, but instead you’ll pay a tax of a delta between offline and online. This delta and others like it are called <a href="https://oreil.ly/IcE1R"><em>training-serving skew</em></a>.</p>&#13;
&#13;
<p>A unique but essential aspect of feature stores is the<a data-primary="registries" data-type="indexterm" id="id562"/> <em>registry</em>. A registry is incredibly useful for a feature store because it coordinates existing features and information on how they’re defined. A more sophisticated instance of a registry also includes input and output schemas with typing, and distributional expectations. These are contracts that the data pipelines must adhere to and satisfy to avoid populating your feature store with garbage data. Additionally, the registry’s definitions allow parallel data scientists and ML engineers to develop new features, use one another’s features, and generally understand the assumptions of features their models may utilize.</p>&#13;
&#13;
<p>One important advantage of these registries is that they incentivize alignment between teams and developers. In particular, if you decide you care about <em>country</em> for your user, and you see a feature <em>country</em> in the registry, you’re more likely to use that (or ask the developer who’s assigned to this feature in the registry) than to make a new one from scratch. Practically, data scientists make hundreds of small decisions and assumptions when defining their models, and  this removes some of that load that’s relying on the existing resources.</p>&#13;
<div data-type="note" epub:type="note"><h1>Model Registries</h1>&#13;
<p>A<a data-primary="models" data-secondary="model registries" data-type="indexterm" id="id563"/> closely related concept to feature registries is model registries. The concepts have a lot in common, but we caution you to think of them differently. A great model registry can have type contracts for the input and output of your models, and can serve many of the same benefits around alignment and clarity. A feature registry should really be focused on definitions of the business logic and features. Because feature engineering can also be model driven, speaking clearly about the differences between these two things can be challenging, so to sum it up, we’ll focus on what they serve: a model registry concerns itself with ML models and the relevant metadata, whereas a feature registry concerns itself with features that models will use.</p>&#13;
</div>&#13;
&#13;
<p>Finally, we need to talk about <em>serving</em> these features. Backed by the appropriately performant storage layer, we need to serve via API request the necessary feature vectors. Those feature vectors are details about the user that the model will need when serving recommendations—for example, the user’s location or content age restrictions. The API can serve back the entire set of features for the key or allow for more specification. Often the responses are JSON serialized for fast data transfer. It’s important that the features being served are the <em>most up-to-date set of features</em>, and latency here is expected to be &lt; 100 ms for more serious industrial applications.</p>&#13;
&#13;
<p>One important caveat here is that for offline training, these feature stores need to accommodate<a data-primary="time travel" data-type="indexterm" id="id564"/> <em>time travel</em>. Because our goal during training is to give the model the appropriate data to learn in the <em>most generalizable way</em>, when training our model, it’s crucial to not give it access to features out of time. This is called <em>data leakage</em> and can cause massive divergence in performance between training and production. The feature store for offline training thus must have knowledge of the features through time, so that during training, a time index may be provided to get the features as they were then. These <code>as_of</code> keys can be tied to the historical training data as we <em>replay</em> the history of what the user-item interactions looked like.</p>&#13;
&#13;
<p>With these pieces in place—and the important monitoring this system needs—you’ll be able to serve offline and online features to your models. In <a data-type="xref" href="part03.html#ranking">Part III</a>, you will see model architectures that make use of them.<a data-primary="" data-startref="DPfeature06" data-type="indexterm" id="id565"/><a data-primary="" data-startref="featstor06" data-type="indexterm" id="id566"/></p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id567">&#13;
<h1>Data Leakage</h1>&#13;
<p>You’re<a data-primary="data processing" data-secondary="data leakage" data-type="indexterm" id="id568"/> likely familiar with the concept of leakage based on what you know about ML: corrupted performance metrics result because the training of the model had access to data that was supposed to be reserved for model performance evaluation.</p>&#13;
&#13;
<p>Data leakage in ML is divided into<a data-primary="feature leakage" data-type="indexterm" id="id569"/><a data-primary="training" data-secondary="training example leakage" data-type="indexterm" id="id570"/> <em>feature leakage</em> and <em>training example leakage</em>. For recommendation systems, data leakage has the additional challenge of temporal leakage, or nonstationarity leakage. The real danger is that in recommendation systems, we see the same observational unit, a user, over and over, and observe a datum each time we see them. When we see them, other aspects of the system may have changed, and in reality we want to use features in our model that are the most up to date as of that observation. Both in features and training examples, to avoid leakage we need to always be thinking of our system’s timeline. This is why data preparation for recommendation systems is inherently time dependent. You will see in <a data-type="xref" href="ch11.html#PersonalRecMetrics">Chapter 11</a> that our accuracy metrics will need to explicitly consider train-test splitting with respect to this time axis, and then be further grouped by the user. This also means that training recommendation systems often requires more resources than many other task types.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id308">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>We’ve discussed not only the crucial components necessary to hydrate your system and serve recommendations, but also some of the engineering building blocks needed to make those components a reality. Equipped with data loaders, embeddings, feature stores, and retrieval mechanisms, we are ready to start constructing our pipeline and system topology.</p>&#13;
&#13;
<p>In the next chapter, we’ll focus our sights on MLOps and the rest of the engineering work required to build and iterate on these systems. It’s going to be important for us to think carefully about deployment and monitoring so our recommendation systems are constrained to life in IPython Notebooks.</p>&#13;
&#13;
<p>Continue onward to see the architectural considerations to move to production.</p>&#13;
</div></section>&#13;
</div></section></body></html>