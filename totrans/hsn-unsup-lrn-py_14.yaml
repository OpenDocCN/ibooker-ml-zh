- en: Chapter 10\. Recommender Systems Using Restricted Boltzmann Machines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章。使用受限玻尔兹曼机的推荐系统
- en: Earlier in this book, we used unsupervised learning to learn the underlying
    (hidden) structure in unlabeled data. Specifically, we performed dimensionality
    reduction, reducing a high-dimensional dataset to one with much fewer dimensions,
    and built an anomaly detection system. We also performed clustering, grouping
    objects together based on how similar or dissimilar they were to each other.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的早期，我们使用无监督学习来学习未标记数据中的潜在（隐藏）结构。具体而言，我们进行了降维，将高维数据集减少到具有更少维度的数据集，并构建了异常检测系统。我们还进行了聚类，根据对象彼此之间的相似性或不相似性将它们分组。
- en: Now, we will move into *generative unsupervised models*, which involve learning
    a probability distribution from an original dataset and using it to make inferences
    about never-before-seen data. In later chapters, we will use such models to generate
    seemingly real data, which at times is virtually indistinguishable from the original
    data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将进入*生成式无监督模型*，这涉及从原始数据集学习概率分布，并用它对以前未见过的数据进行推断。在后面的章节中，我们将使用这些模型生成看似真实的数据，有时几乎无法与原始数据区分开来。
- en: Until now, we have looked at mostly *discriminative models* that learn to separate
    observations based on what the algorithms learn from the data; these discriminative
    models do not learn the probability distribution from the data. Discriminative
    models include supervised ones such as the logistic regression and decision trees
    from [Chapter 2](ch02.html#Chapter_2) as well as clustering methods such as *k*-means
    and hierarchical clustering from [Chapter 5](ch05.html#Chapter_5).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们主要研究了*判别模型*，这些模型根据算法从数据中学到的内容来分离观察结果；这些判别模型不会从数据中学习概率分布。判别模型包括监督学习模型，如逻辑回归和决策树（来自[第2章](ch02.html#Chapter_2)），以及聚类方法，如*k*-均值和层次聚类（来自[第5章](ch05.html#Chapter_5)）。
- en: Let’s start with the simplest of the generative unsupervised models known as
    the *restricted Boltzmann machine*.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从最简单的生成式无监督模型开始，即*受限玻尔兹曼机*。
- en: Boltzmann Machines
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玻尔兹曼机
- en: '*Boltzmann machines* were first invented in 1985 by Geoffrey Hinton (then a
    professor at Carnegie Mellon University and now one of the fathers of the deep
    learning movement, a professor at the University of Toronto, and a machine learning
    researcher at Google) and Terry Sejnowski (who was a professor at John Hopkins
    University at the time).'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*玻尔兹曼机*最早由Geoffrey Hinton（当时是卡内基梅隆大学的教授，现在是深度学习运动的先驱之一，多伦多大学的教授，以及谷歌的机器学习研究员）和Terry
    Sejnowski（当时是约翰霍普金斯大学的教授）于1985年发明。'
- en: Boltzmann machines—of the unrestricted type—consist of a neural network with
    an input layer and one or several hidden layers. The neurons or units in the neural
    network make stochastic decisions about whether to turn on or not based on the
    data fed in during training and the cost function the Boltzmann machine is trying
    to minimize. With this training, the Boltzmann machine discovers interesting features
    about the data, which helps model the complex underlying relationships and patterns
    present in the data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 玻尔兹曼机——无限制型——由具有输入层和一个或多个隐藏层的神经网络组成。神经网络中的神经元或单元根据训练中输入的数据和玻尔兹曼机试图最小化的成本函数，做出是否启动的随机决策。通过这种训练，玻尔兹曼机发现数据的有趣特征，有助于模拟数据中复杂的潜在关系和模式。
- en: However, these unrestricted Boltzmann machines use neural networks with neurons
    that are connected not only to other neurons in other layers but also to neurons
    within the same layer. That, coupled with the presence of many hidden layers,
    makes training an unrestricted Boltzmann machine very inefficient. Unrestricted
    Boltzmann machines had little commercial success during the 1980s and 1990s as
    a result.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些无限制的玻尔兹曼机使用神经网络，其中神经元不仅连接到其他层中的神经元，而且连接到同一层中的神经元。这与许多隐藏层的存在一起，使得无限制的玻尔兹曼机的训练效率非常低。由于这个原因，无限制的玻尔兹曼机在20世纪80年代和90年代几乎没有商业成功。
- en: Restricted Boltzmann Machines
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 受限玻尔兹曼机
- en: In the 2000s, Geoffrey Hinton and others began to have commercial success by
    using a modified version of the original unrestricted Boltzmann machines. These
    *restricted Boltzmann machines (RBMs)* have an input layer (also referred to as
    the *visible layer*) and just a single hidden layer, and the connections among
    neurons are restricted such that neurons are connected only to the neurons in
    other layers but not to neurons within the same layer. In other words, there are
    no visible-visible connections and no hidden-hidden connections.^([1](ch10.html#idm140637537216896))
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在2000年代，Geoffrey Hinton等人开始通过使用修改后的原始无限制玻尔兹曼机取得商业成功。这些*受限玻尔兹曼机（RBM）*具有一个输入层（也称为*可见层*）和一个单独的隐藏层，神经元之间的连接受限，使得神经元仅连接到其他层的神经元，而不连接同一层的神经元。换句话说，没有可见-可见的连接和隐藏-隐藏的连接。^([1](ch10.html#idm140637537216896))
- en: Geoffrey Hinton also demonstrated that such simple RBMs could be stacked on
    top of each other so that the output of the hidden layer of one RBM can be fed
    into the input layer of another RBM. This sort of RBM stacking can be repeated
    many times to learn progressively more nuanced hidden representations of the original
    data. This network of many RBMs can be viewed as one deep, multilayered neural
    network model—and thus the field of deep learning took off, starting in 2006.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Geoffrey Hinton还展示了这样简单的受限玻尔兹曼机（RBM）可以堆叠在一起，以便一个RBM的隐藏层的输出可以被馈送到另一个RBM的输入层。这种RBM堆叠可以多次重复，以逐步学习原始数据更细致的隐藏表示。这种多个RBM组成的网络可以看作是一个深层、多层次的神经网络模型——因此，深度学习领域从2006年开始蓬勃发展。
- en: Note that RBMs use a *stochastic* approach to learning the underlying structure
    of data, whereas autoencoders, for example, use a *deterministic* approach.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，RBM使用*随机*方法来学习数据的潜在结构，而例如自编码器则使用*确定性*方法。
- en: Recommender Systems
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推荐系统
- en: In this chapter, we will use RBMs to build a *recommender system*, one of the
    most successful applications of machine learning to date and widely used in industry
    to help predict user preferences for movies, music, books, news, search, shopping,
    digital advertising, and online dating.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用RBM构建一个*推荐系统*，这是迄今为止最成功的机器学习应用之一，在行业中广泛用于帮助预测用户对电影、音乐、书籍、新闻、搜索、购物、数字广告和在线约会的偏好。
- en: There are two major categories of recommender systems—*collaborative filtering*
    recommender systems and *content-based filtering* recommender systems. Collaborative
    filtering involves building a recommender system from a user’s past behavior and
    those of other users to which the user is similar to. This recommender system
    can then predict items that the user may have an interest in even though the user
    has never expressed explicit interest. Movie recommendations on Netflix rely on
    collaborative filtering.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统有两大主要类别——*协同过滤*推荐系统和*基于内容*的推荐系统。协同过滤涉及根据用户的过去行为以及与用户相似的其他用户的行为来构建推荐系统。这种推荐系统可以预测用户可能感兴趣的项目，即使用户从未明确表达过兴趣。Netflix上的电影推荐就依赖于协同过滤。
- en: Content-based filtering involves learning the distinct properties of an item
    to recommend additional items with similar properties. Music recommendations on
    Pandora rely on content-based filtering.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 基于内容的过滤涉及学习一个项目的独特属性，以推荐具有类似属性的其他项目。Pandora上的音乐推荐就依赖于基于内容的过滤。
- en: Collaborative Filtering
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 协同过滤
- en: Content-based filtering is not commonly used because it is a rather difficult
    task to learn the distinct properties of items—this level of understanding is
    very challenging for artificial machines to achieve currently. It is much easier
    to collect and analyze a large amount of information on users’ behaviors and preferences
    and make predictions based on this. Therefore, collaborative filtering is much
    more widely used and is the type of recommender system we will focus on here.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 基于内容的过滤并不常用，因为学习项目的独特属性是一个相当困难的任务——目前人工机器很难达到这种理解水平。收集和分析大量关于用户行为和偏好的信息，并基于此进行预测，要容易得多。因此，协同过滤更广泛地被使用，也是我们这里将重点关注的推荐系统类型。
- en: Collaborative filtering requires no knowledge of the underlying items themselves.
    Rather, collaborative filtering assumes that users that agreed in the past will
    agree in the future and that user preferences remain stable over time. By modeling
    how similar users are to other users, collaborative filtering can make pretty
    powerful recommendations. Moreover, collaborative filtering does not have to rely
    on *explicit data* (i.e., ratings that users provide). Rather, it can work with
    *implicit data* such as how long or how often a user views or clicks on a particular
    item. For example, in the past Netflix asked users to rate movies but now uses
    implicit user behavior to make inferences about user likes and dislikes.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 协同过滤不需要了解底层物品本身。相反，协同过滤假设在过去达成一致的用户将来也会达成一致，并且用户的偏好随时间保持稳定。通过建模用户与其他用户的相似性，协同过滤可以进行相当强大的推荐。此外，协同过滤不必依赖于*显式数据*（即用户提供的评分）。相反，它可以使用*隐式数据*，例如用户观看或点击特定项目的时间长短或频率来推断用户的喜好和厌恶。例如，过去Netflix要求用户对电影进行评分，但现在使用用户的隐式行为来推断用户的喜好和厌恶。
- en: However, collaborative filtering has its challenges. First, it requires a lot
    of user data to make good recommendations. Second, it is a very computationally
    demanding task. Third, the datasets are generally very sparse since users will
    have exhibited preferences for only a small fraction of all the items in the universe
    of possible items. Assuming we have enough data, there are techniques we can use
    to handle the sparsity of the data and efficiently solve the problem, which we
    will cover in this chapter.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，协同过滤也存在其挑战。首先，它需要大量用户数据来进行良好的推荐。其次，这是一个非常计算密集的任务。第三，数据集通常非常稀疏，因为用户只对可能物品宇宙中的一小部分物品展现了偏好。假设我们有足够的数据，我们可以使用技术来处理数据的稀疏性并高效解决这个问题，我们将在本章中进行讨论。
- en: The Netflix Prize
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Netflix奖励
- en: In 2006, Netflix sponsored a three-year-long competition to improve its movie
    recommender system. The company offered a grand prize of one million dollars to
    the team that could improve the accuracy of its existing recommender system by
    at least 10%. It also released a dataset of over 100 million movie ratings. In
    September 2009, BellKor’s Pramatic Chaos team won the prize, using an ensemble
    of many different algorithmic approaches.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 2006年，Netflix赞助了一场为期三年的比赛，旨在改进其电影推荐系统。该公司向那支能将其现有推荐系统的准确性提高至少10%的团队提供了100万美元的大奖。它还发布了一个包含超过1亿部电影评分的数据集。2009年9月，BellKor的Pramatic
    Chaos团队赢得了这一奖项，他们使用了多种不同算法方法的集成。
- en: Such a high-profile competition with a rich dataset and meaningful prize energized
    the machine learning community and led to substantial progress in recommender
    system research, which paved the way for better recommender systems in industry
    over the past several years.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这样一场备受关注的比赛，拥有丰富的数据集和有意义的奖金，激励了机器学习社区，并推动了推荐系统研究的实质性进展，为工业界在过去几年里开发出更好的推荐系统铺平了道路。
- en: In this chapter, we will use a similar movie rating dataset to build our own
    recommender system using RBMs.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用一个类似的电影评分数据集来构建我们自己的推荐系统，使用RBM（Restricted Boltzmann Machines）。
- en: MovieLens Dataset
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MovieLens数据集
- en: Instead of the 100 million ratings Netflix dataset, we will use a smaller movie
    ratings dataset known as the *MovieLens 20M Dataset*, provided by GroupLens, a
    research lab in the Department of Computer Science and Engineering at the University
    of Minnesota, Twin Cities. The data contains 20,000,263 ratings across 27,278
    movies created by 138,493 users from January 9, 1995 to March 31, 2015\. Of users
    who rated at least 20 movies each, we will select a subset at random.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于Netflix的1亿条评分数据集，我们将使用一个更小的电影评分数据集，称为*MovieLens 20M数据集*，由明尼苏达大学双城分校计算机科学与工程系的研究实验室GroupLens提供。该数据集包含了从1995年1月9日到2015年3月31日，138,493位用户对27,278部电影进行的20,000,263次评分。我们将随机选择至少评分了20部电影的用户子集。
- en: This dataset is more manageable to work with than the 100 million ratings dataset
    from Netflix. Because the file size exceeds one hundred megabytes, the file is
    not accessible on GitHub. You will need to download the file directly from the
    [MovieLens website](http://bit.ly/2G0ZHCn).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集比Netflix的1亿条评分数据集更易于处理。由于文件大小超过了100兆字节，该文件在GitHub上不可访问。您需要直接从[MovieLens网站](http://bit.ly/2G0ZHCn)下载该文件。
- en: Data Preparation
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'As before, let’s load in the necessary libraries:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，让我们加载必要的库：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we will load in the ratings dataset and convert the fields into the appropriate
    data types. We have just a few fields. The user ID, the movie ID, the rating provided
    by the user for the movie, and the timestamp of the rating provided:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将加载评分数据集并将字段转换为适当的数据类型。我们只有几个字段。用户ID，电影ID，用户为电影提供的评分，以及提供评分的时间戳：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[Table 10-1](#movielens_ratings_data) shows a partial view of the data.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 10-1](#movielens_ratings_data)展示了数据的部分视图。'
- en: Table 10-1\. MovieLens ratings data
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 表10-1. MovieLens评分数据
- en: '|  | userId | movieId | rating | timestamp |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | 用户ID | 电影ID | 评分 | 时间戳 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 1 | 2 | 3.5 | 2005-04-02 23:53:47 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 2 | 3.5 | 2005-04-02 23:53:47 |'
- en: '| 1 | 1 | 29 | 3.5 | 2005-04-02 23:31:16 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 29 | 3.5 | 2005-04-02 23:31:16 |'
- en: '| 2 | 1 | 32 | 3.5 | 2005-04-02 23:33:39 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1 | 32 | 3.5 | 2005-04-02 23:33:39 |'
- en: '| 3 | 1 | 47 | 3.5 | 2005-04-02 23:32:07 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1 | 47 | 3.5 | 2005-04-02 23:32:07 |'
- en: '| 4 | 1 | 50 | 3.5 | 2005-04-02 23:29:40 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 1 | 50 | 3.5 | 2005-04-02 23:29:40 |'
- en: '| 5 | 1 | 112 | 3.5 | 2004-09-10 03:09:00 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 1 | 112 | 3.5 | 2004-09-10 03:09:00 |'
- en: '| 6 | 1 | 151 | 4.0 | 2004-09-10 03:08:54 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 1 | 151 | 4.0 | 2004-09-10 03:08:54 |'
- en: '| 7 | 1 | 223 | 4.0 | 2005-04-02 23:46:13 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 1 | 223 | 4.0 | 2005-04-02 23:46:13 |'
- en: '| 8 | 1 | 253 | 4.0 | 2005-04-02 23:35:40 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 1 | 253 | 4.0 | 2005-04-02 23:35:40 |'
- en: '| 9 | 1 | 260 | 4.0 | 2005-04-02 23:33:46 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 1 | 260 | 4.0 | 2005-04-02 23:33:46 |'
- en: '| 10 | 1 | 293 | 4.0 | 2005-04-02 23:31:43 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 1 | 293 | 4.0 | 2005-04-02 23:31:43 |'
- en: '| 11 | 1 | 296 | 4.0 | 2005-04-02 23:32:47 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 1 | 296 | 4.0 | 2005-04-02 23:32:47 |'
- en: '| 12 | 1 | 318 | 4.0 | 2005-04-02 23:33:18 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 1 | 318 | 4.0 | 2005-04-02 23:33:18 |'
- en: '| 13 | 1 | 337 | 3.5 | 2004-09-10 03:08:29 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 13 | 1 | 337 | 3.5 | 2004-09-10 03:08:29 |'
- en: 'Let’s confirm the number of unique users, unique movies, and total ratings,
    and we will also calculate the average number of ratings provided by users:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们确认唯一用户数、唯一电影数和总评分数，还将计算用户提供的平均评分数量：
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The data is as we expected:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 数据正如我们所预期的那样：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: To reduce the complexity and size of this dataset, let’s focus on the top one
    thousand most rated movies. This will reduce the number of ratings from about
    ~20 million to about ~12.8 million.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少这个数据集的复杂性和大小，让我们集中于排名前一千的电影。这将把评分数从约20百万减少到约12.8百万。
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We will also take a sample of one thousand users at random and filter the dataset
    for just these users. This will reduce the number of ratings from ~12.8 million
    to just 90,213\. This number is sufficient to demonstrate collaborative filtering:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将随机抽取一千名用户的样本，并仅过滤这些用户的数据集。这将把评分数从约12.8百万减少到90,213个。这个数量足以展示协同过滤的效果：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s also reindex `movieID` and `userID` to a range of 1 to 1,000 for our
    reduced dataset:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，让我们重新索引`movieID`和`userID`到1到1,000的范围内，用于我们的简化数据集：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s calculate the number of unique users, unique movies, total ratings, and
    average number of ratings per user for our reduced dataset:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算我们简化数据集中的唯一用户数、唯一电影数、总评分数以及每个用户的平均评分数量：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The results are as expected:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如预期：
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s generate a test set and a validation set from this reduced dataset so
    that each holdout set is 5% of the reduced dataset:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从这个简化的数据集中生成一个测试集和一个验证集，使得每个留出集占简化数据集的5%：
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following shows the sizes of the train, validation, and test sets:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 下面显示了训练集、验证集和测试集的大小：
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Define the Cost Function: Mean Squared Error'
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义成本函数：均方误差
- en: Now we are ready to work with the data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好处理这些数据了。
- en: 'First, let’s create a matrix *m* x *n*, where *m* are the users and *n* are
    the movies. This will be a sparsely populated matrix because users rate only a
    fraction of the movies. For example, a matrix with one thousand users and one
    thousand movies will have only 81,191 ratings in the training set. If each of
    the one thousand users rated each of the one thousand movies, we would have a
    matrix with one million ratings, but users rate only a small subset of movies
    on average, so we have only 81,191 ratings on the training set. The rest (nearly
    92% of the values in the matrix) will be zeros:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建一个*m* x *n*的矩阵，其中*m*是用户数，*n*是电影数。这将是一个稀疏填充的矩阵，因为用户只对电影的一小部分进行评分。例如，一个拥有一千个用户和一千部电影的矩阵在训练集中只有81,191个评分。如果每个一千个用户都对每一千部电影进行评分，我们将得到一个百万个评分的矩阵，但是平均而言用户只对少数电影进行评分，因此我们在训练集中只有81,191个评分。其余的值（矩阵中近92%的值）将为零：
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We will generate similar matrices for the validation set and the test set,
    which will be even sparser, of course:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为验证集和测试集生成类似的矩阵，它们会更加稀疏，当然：
- en: '[PRE12]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Before we build our recommender systems, let’s define the cost function that
    we will use to judge the goodness of our model. We will use *mean squared error
    (MSE)*, one of the simplest cost functions in machine learning. MSE measures the
    averaged squared error between the predicted values and the actual values. To
    calculate the MSE, we need two vectors of size *[n,1]*, where *n* is the number
    of ratings we are predicting—4,511 for the validation set. One vector has the
    actual ratings, and the other vector has the predictions.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建推荐系统之前，让我们定义我们将用来评判模型好坏的成本函数。我们将使用*均方误差（MSE）*，这是机器学习中最简单的成本函数之一。MSE测量了预测值与实际值之间的平均平方误差。要计算MSE，我们需要两个大小为*[n,1]*的向量，其中*n*是我们正在预测评分的数量
    —— 对于验证集是 4,511。一个向量包含实际评分，另一个向量包含预测值。
- en: 'Let’s first flatten the sparse matrix with the ratings for the validation set.
    This will be the vector of actual ratings:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先将验证集中带有评分的稀疏矩阵展平。这将是实际评分的向量：
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Perform Baseline Experiments
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进行基准实验
- en: 'As a baseline, let’s predict an average rating of 3.5 for the validation set
    and calculate the MSE:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 作为基准，让我们预测验证集的平均评分为 3.5，并计算MSE：
- en: '[PRE14]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The MSE of this very naive prediction is 1.05\. This is our baseline:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这种非常天真预测的MSE是 1.05。这是我们的基准：
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let’s see if we can improve our results by predicting a user’s rating for a
    given movie based on that user’s average rating for all other movies:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看是否可以通过预测用户对给定电影的评分来改善结果，基于该用户对所有其他电影的平均评分：
- en: '[PRE16]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The MSE improves to 0.909:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 均方误差（MSE）改善到 0.909：
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, let’s predict a user’s rating for a given movie based on the average rating
    all other users have given that movie:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们基于所有其他用户对该电影的平均评分来预测用户对给定电影的评分：
- en: '[PRE18]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The MSE of this approach is 0.914, similar to that found using user average:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的MSE为 0.914，与使用用户平均值发现的MSE类似：
- en: '[PRE19]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Matrix Factorization
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矩阵分解
- en: Before we build a recommender system using RBMs, let’s first build one using
    *matrix factorization*, one of the most successful and popular collaborative filtering
    algorithms today. Matrix factorization decomposes the user-item matrix into a
    product of two lower dimensionality matrices. Users are represented in lower dimensional
    latent space, and so are the items.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用RBM构建推荐系统之前，让我们首先使用*矩阵分解*来构建一个。矩阵分解将用户-物品矩阵分解为两个较低维度矩阵的乘积。用户在较低维度潜在空间中表示，物品也是如此。
- en: Assume our user-item matrix is *R*, with *m* users and *n* items. Matrix factorization
    will create two lower dimensionality matrices, *H* and *W*. *H* is an "*m* users”
    x "*k* latent factors” matrix, and *W* is a "*k* latent factors” x "*n* items”
    matrix.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的用户-物品矩阵是 *R*，有 *m* 个用户和 *n* 个物品。矩阵分解将创建两个较低维度的矩阵，*H* 和 *W*。*H* 是一个 "*m*
    用户" x "*k* 潜在因子" 的矩阵，*W* 是一个 "*k* 潜在因子" x "*n* 物品" 的矩阵。
- en: 'The ratings are computed by matrix multiplication: *R* = *H__W*.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 评分通过矩阵乘法计算：*R* = *H__W*。
- en: The number of *k* latent factors determines the capacity of the model. The higher
    the *k*, the greater the capacity of the model. By increasing *k*, we can improve
    the personalization of rating predictions for users, but, if *k* is too high,
    the model will overfit the data.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*k* 潜在因子的数量决定了模型的容量。*k* 越高，模型的容量越大。通过增加*k*，我们可以提高对用户评分预测的个性化能力，但如果*k*过高，模型将过度拟合数据。'
- en: All of this should be familiar to you. Matrix factorization learns representations
    for the users and items in a lower dimensional space and makes predictions based
    on the newly learned representations.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些对你来说应该是熟悉的。矩阵分解学习了用户和物品在较低维度空间中的表示，并基于新学到的表示进行预测。
- en: One Latent Factor
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个潜在因子
- en: Let’s start with the simplest form of matrix factorization—with just one latent
    factor. We will use Keras to perform our matrix factorization.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从最简单的矩阵分解形式开始 —— 只使用一个潜在因子。我们将使用Keras来执行我们的矩阵分解。
- en: 'First, we need to define the graph. The input is the one-dimensional vector
    of users for the user embedding and the one-dimensional vector of movies for the
    movie embedding. We will embed these input vectors into a latent space of one
    and then flatten them. To generate the output vector *product*, we will take the
    dot product of the movie vector and user vector. We will use the *Adam optimizer*
    to minimize our cost fuction, which is defined as the `mean_squared_error`:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要定义图表。输入是用户嵌入的一维向量和电影嵌入的一维向量。我们将这些输入向量嵌入到一个潜在空间中，然后展平它们。为了生成输出向量 *product*，我们将采用电影向量和用户向量的点积。我们将使用
    *Adam优化器* 来最小化我们的损失函数，该损失函数定义为 `mean_squared_error`：
- en: '[PRE20]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Let’s train the model by feeding in the user and movie vectors from the training
    dataset. We will also evaluate the model on the validation set while we train.
    The MSE will be calculated against the actual ratings we have.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过训练集中的用户和电影向量来训练模型。我们还将在训练过程中对验证集进行评估。我们将根据实际评分计算MSE。
- en: 'We will train for one hundred epochs and store the history of the training
    and validation results. Let’s also plot the results:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练一百个epochs，并记录训练和验证结果的历史。让我们也来绘制结果：
- en: '[PRE21]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[Figure 10-1](#plot_of_validation_mse_using_mf_and_one_latent_factor) shows
    the results.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-1](#plot_of_validation_mse_using_mf_and_one_latent_factor) 展示了结果。'
- en: '![Plot of Validation MSE using MF and One Latent Factor](assets/hulp_1001.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![使用MF和一个潜在因子的验证MSE图](assets/hulp_1001.png)'
- en: Figure 10-1\. Plot of validation MSE using matrix factorization and one latent
    factor
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-1\. 使用矩阵因子化和一个潜在因子的验证MSE图
- en: The minimum MSE using matrix factorization and one latent factor is 0.796\.
    This is a better MSE than our user average and movie average approaches from earlier.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 使用矩阵因子化和一个潜在因子的最小MSE为0.796。这比之前的用户平均和电影平均方法更好。
- en: Let’s see if we can do even better by increasing the number of latent factors
    (i.e., the capacity of the model).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 看看我们是否可以通过增加潜在因子的数量（即模型的容量）来进一步改进。
- en: Three Latent Factors
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 三个潜在因子
- en: '[Figure 10-2](#plot_of_validation_mse_using_mf_and_three_latent_factors) displays
    the results of using three latent factors.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-2](#plot_of_validation_mse_using_mf_and_three_latent_factors) 展示了使用三个潜在因子的结果。'
- en: '![Plot of Validation MSE using MF and Three Latent Factors](assets/hulp_1002.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![使用MF和三个潜在因子的验证MSE图](assets/hulp_1002.png)'
- en: Figure 10-2\. Plot of validation MSE using matrix factorization and three latent
    factors
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-2\. 使用矩阵因子化和三个潜在因子的验证MSE图
- en: The minimum MSE is 0.765, which is better than the one using one latent factor
    and the best yet.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最小MSE为0.765，比使用一个潜在因子更好。
- en: Five Latent Factors
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 五个潜在因子
- en: Let’s now build a matrix factorization model using five latent factors (see
    [Figure 10-3](#plot_of_validation_mse_using_mf_and_five_latent_factors) for the
    results).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们构建一个使用五个潜在因子的矩阵因子化模型（参见 [图 10-3](#plot_of_validation_mse_using_mf_and_five_latent_factors)
    的结果）。
- en: '![Plot of Validation MSE using MF and Five Latent Factors](assets/hulp_1003.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![使用MF和五个潜在因子的验证MSE图](assets/hulp_1003.png)'
- en: Figure 10-3\. Plot of validation MSE using matrix factorization and five latent
    factors
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-3\. 使用矩阵因子化和五个潜在因子的验证MSE图
- en: The minimum MSE fails to improve, and there are clear signs of overfitting after
    the first 25 epochs or so. The validation error troughs and then begins to increase.
    Adding more capacity to the matrix factorization model will not help much more.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最小MSE未能改进，在前25个epochs左右明显出现过拟合迹象。验证误差下降然后开始增加。增加矩阵因子化模型的容量将不会帮助太多。
- en: Collaborative Filtering Using RBMs
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RBM进行协同过滤
- en: Let’s turn back to RBMs again. Recall that RBMs have two layers—the input/visible
    layer and the hidden layer. The neurons in each layer communicate with neurons
    in the other layer but not with neurons in the same layer. In other words, there
    is no intralayer communication among the neurons—this is the *restricted* bit
    of RBMs.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次回到RBM。回想一下，RBM有两层——输入/可见层和隐藏层。每一层中的神经元与另一层中的神经元进行通信，但不与同一层中的神经元进行通信。换句话说，神经元之间没有同层通信——这就是RBM中“限制”的一部分。
- en: Another important feature of RBMs is that the communication between layers happens
    in both directions—not just in one direction. For example, with autoencoders,
    the neurons communicate with the next layer, passing information only in a feedforward
    way.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: RBM的另一个重要特征是层之间的通信是双向的，而不仅仅是单向的。例如，对于自编码器，神经元只能通过前向传递与下一层通信。
- en: With RBMs, the neurons in the visible layer communicate with the hidden layer,
    and then the hidden layer passes back information to the visibile layer, going
    back and forth several times. RBMs perform this communication—the passes back
    and forth between the visible and hidden layer—to develop a generative model such
    that the reconstructions from the outputs of the hidden layer are similar to the
    original inputs.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 RBM，可见层中的神经元与隐藏层通信，然后隐藏层将信息传回可见层，来回多次交换。 RBM 执行此通信——在可见层和隐藏层之间来回传递——以开发生成模型，使得从隐藏层输出的重构与原始输入相似。
- en: In other words, the RBMs are trying to create a generative model that will help
    predict whether a user will like a movie that the user has never seen based on
    how similar the movie is to other movies the user has rated and based on how similar
    the user is to the other users that have rated that movie.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，RBM 正在尝试创建一个生成模型，该模型将根据用户评分的电影之间的相似性以及用户与其他评分该电影的用户的相似性，帮助预测用户是否会喜欢用户从未看过的电影。
- en: The visible layer will have X neurons, where X is the number of movies in the
    dataset. Each neuron will have a normalized rating value from zero to one, where
    zero means the user has not seen the movie. The closer the normalized rating value
    is to one, the more the user likes the movie represented by the neuron.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 可见层将有 X 个神经元，其中 X 是数据集中电影的数量。 每个神经元将具有从零到一的归一化评分值，其中零表示用户未看过电影。 归一化评分值越接近一，表示用户越喜欢神经元表示的电影。
- en: The neurons in the visible layer will communicate with the neurons in the hidden
    layer, which will try to learn the underlying, latent features that characterize
    the user-movie preferences.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 可见层中的神经元将与隐藏层中的神经元通信，后者将试图学习表征用户-电影偏好的潜在特征。
- en: Note that RBMs are also referred to as *symmetrical bipartite, bidirectional
    graphs*—symmetrical because each visible node is connected to each hidden node,
    bipartite because there are two layers of nodes, and bidirectional because the
    communication happens in both directions.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，RBM 也被称为*对称的二分图、双向图*——对称是因为每个可见节点与每个隐藏节点相连，二分是因为有两层节点，双向是因为通信是双向的。
- en: RBM Neural Network Architecture
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RBM 神经网络架构
- en: For our movie-recommender system, we have an *m* x *n* matrix with *m* users
    and *n* movies. To train the RBM, we pass along a batch of *k* users with their
    *n* movie ratings into the neural network and train for a certain number of *epochs*.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的电影推荐系统，我们有一个*m* x *n*矩阵，其中*m*为用户数，*n*为电影数。 要训练 RBM，我们将一批*k*用户及其*n*电影评分传递到神经网络，并训练一定数量的*epochs*。
- en: Each input *x* that is passed into the neural network represents a single user’s
    rating preferences for all *n* movies, where *n* is one thousand in our example.
    Therefore, the visible layer has *n* nodes, one for each movie.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 每个传入神经网络的输入*x*表示单个用户对所有*n*部电影的评分偏好，例如，我们的示例中*n*为一千。 因此，可见层有*n*个节点，每个节点对应一个电影。
- en: We can specify the number of nodes in the hidden layer, which will generally
    be fewer than the nodes in the visible layer to force the hidden layer to learn
    the most salient aspects of the original input as efficiently as possible.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以指定隐藏层中节点的数量，通常比可见层中的节点少，以尽可能有效地让隐藏层学习原始输入的最显著方面。
- en: Each input *v0* is multiplied by its respective weight *W*. The weights are
    learned by the connections from the visible layer to the hidden layer. Then we
    add a bias vector at the hidden layer called *hb*. The bias ensures that at least
    some of the neurons fire. This *W*v0+hb* result is passed through an activation
    function.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输入*v0*都与其相应的权重*W*相乘。 权重是从可见层到隐藏层的连接学习的。 然后我们在隐藏层添加一个称为*hb*的偏置向量。 偏置确保至少有一些神经元会激活。
    这个*W*v0+hb*结果通过激活函数传递。
- en: After this, we will take a sample of the outputs generated via a process known
    as *Gibbs sampling*. In other words, the activation of the hidden layer results
    in final outputs that are generated stochastically. This level of randomness helps
    build a better-performing and more robust generative model.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将通过一种称为*Gibbs sampling*的过程对生成的输出样本进行采样。 换句话说，隐藏层的激活结果以随机方式生成最终输出。 这种随机性有助于构建性能更好、更强大的生成模型。
- en: Next, the output after Gibbs sampling—known as *h0*—is passed back through the
    neural network in the opposite direction in what is called a *backward pass*.
    In the backward pass, the activations in the *forward pass* after Gibbs sampling
    are fed into the hidden layer and multiplied by the same weights *W* as before.
    We then add a new bias vector at the visible layer called *vb*.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，吉布斯采样后的输出—称为*h0*—通过神经网络反向传播回去，进行所谓的*反向传播*。在反向传播中，吉布斯采样后的前向传播中的激活被馈送到隐藏层，并与之前相同的权重*W*相乘。然后我们在可见层添加一个新的称为*vb*的偏置向量。
- en: This *W_h0+vb* is passed through an activation function, and then we perform
    Gibbs sampling. The output of this is *v1*, which is then passed as the new input
    into the visible layer and through the neural network as another forward pass.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这个*W_h0+vb*通过激活函数传递，并进行吉布斯采样。这个输出是*v1*，然后作为新的输入传递到可见层和神经网络中，进行另一次前向传播。
- en: The RBM goes through a series of forward and backward passes like this to learn
    the optimal weights as it attempts to build a robust generative model. RBMs are
    the first type of *generative learning* model that we have explored. By performing
    Gibbs sampling and retraining weights via forward and backward passes, RBMs are
    trying to learn the *probability distribution* of the original input. Specifically,
    RBMs minimize the *Kullback–Leibler divergence*, which measures how one probability
    distribution is different from another; in this case, RBMs are minimizing the
    probability distribution of the original input from the probability distribution
    of the reconstructed data.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: RBM通过一系列前向和反向传播的步骤来学习最优权重，试图构建一个健壮的生成模型。RBM是我们探索的第一种*生成学习*模型。通过执行吉布斯采样和通过前向和反向传播重新训练权重，RBM试图学习原始输入的*概率分布*。具体来说，RBM最小化*Kullback–Leibler散度*，该散度用于衡量一个概率分布与另一个之间的差异；在这种情况下，RBM最小化原始输入的概率分布与重建数据的概率分布之间的差异。
- en: By iteratively readjusting the weights in the neural net, the RBM learns to
    approximate the original data as best as possible.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 通过迭代调整神经网络中的权重，受限玻尔兹曼机（RBM）学习尽可能地逼近原始数据。
- en: With this newly learned probability distribution, RBMs are able to make predictions
    about never-before-seen data. In this case, the RBM we design will attempt to
    predict ratings for movies that the user has never seen based on the user’s similarity
    to other users and the ratings those movies have received by the other users.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个新学习到的概率分布，RBM能够对以前未见过的数据进行预测。在这种情况下，我们设计的RBM将尝试基于用户与其他用户的相似性及其他用户对这些电影的评分来预测用户从未看过的电影的评分。
- en: Build the Components of the RBM Class
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建RBM类的组件
- en: First, we will initialize the class with a few parameters; these are the input
    size of the RBM, the output size, the learning rate, the number of epochs to train
    for, and the batch size during the training process.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将用几个参数初始化这个类；这些参数包括RBM的输入大小、输出大小、学习率、训练时的周期数以及训练过程中的批处理大小。
- en: 'We will also create zero matrices for the weight matrix, the hidden bias vector,
    and the visible bias vector:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还将创建用于权重矩阵、隐藏偏置向量和可见偏置向量的零矩阵： '
- en: '[PRE22]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Next, let’s define functions for the forward pass, the backward pass, and the
    sampling of data during each of these passes back and forth.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们定义前向传播、反向传播和在这些传播过程中数据抽样的函数。
- en: 'Here is the forward pass, where *h* is the hidden layer and *v* is the visible
    layer:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是前向传播，其中*h*是隐藏层，*v*是可见层：
- en: '[PRE23]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Here is the backward pass:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是反向传播的过程：
- en: '[PRE24]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here is the sampling function:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是抽样函数的定义：
- en: '[PRE25]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now we need a function that performs that training. Since we are using TensorFlow,
    we first need to create placeholders for the TensorFlow graph, which we will use
    when we feed data into the TensorFlow session.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要一个函数来执行训练。由于我们使用TensorFlow，我们首先需要为TensorFlow图创建占位符，在我们将数据馈送到TensorFlow会话中时使用。
- en: 'We will have placeholders for the weights matrix, the hidden bias vector, and
    the visible bias vector. We will also need to initialize the values for these
    three using zeros. And, we will need one set to hold the current values and one
    set to hold the previous values:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为权重矩阵、隐藏偏置向量和可见偏置向量创建占位符。我们还需要用零初始化这三者的值。此外，我们需要一个集合来保存当前值和一个集合来保存先前的值：
- en: '[PRE26]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Likewise, we need a placeholder for the visible layer. The hidden layer is
    derived from matrix multiplication of the visible layer and the weights matrix
    and the matrix addition of the hidden bias vector:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们需要一个可见层的占位符。隐藏层是从可见层和权重矩阵的矩阵乘法以及隐藏偏置向量的矩阵加法导出的：
- en: '[PRE27]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'During the backward pass, we take the hidden layer output, multiply it with
    the transpose of the weights matrix used during the forward pass, and add the
    visible bias vector. Note that the weights matrix is the same during both the
    forward and the backward pass. Then, we perform the forward pass again:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播期间，我们取隐藏层输出，与正向传播期间使用的权重矩阵的转置相乘，并加上可见偏置向量。请注意，权重矩阵在正向和反向传播期间是相同的。然后，我们再次执行正向传播：
- en: '[PRE28]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: To update the weights, we perform constrastive divergence.^([2](ch10.html#idm140637534592544))
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 要更新权重，我们执行对比散度[^2]。
- en: We also define the error as MSE.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将误差定义为MSE。
- en: '[PRE29]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: With this, we are ready to initialize the TensorFlow session with the variables
    we have just defined.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们可以使用刚刚定义的变量初始化TensorFlow会话。
- en: Once we call *sess.run*, we can feed in batches of data to begin the training.
    During the training, forward and backward passes will be made, and the RBM will
    update weights based on how the generated data compares to the original input.
    We will print the reconstruction error from each epoch.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们调用*sess.run*，我们可以输入数据批次开始训练。在训练过程中，将进行前向和反向传播，并根据生成数据与原始输入的比较更新RBM权重。我们将打印每个epoch的重构误差。
- en: '[PRE30]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Train RBM Recommender System
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练RBM推荐系统
- en: 'To train the RBM, let’s create a NumPy array called `inputX` from `ratings_train`
    and convert these values to float32\. We will also define the RBM to take in a
    one thousand-dimensional input, output a one thousand-dimensional output, use
    a learning rate of 0.3, train for five hundred epochs, and use a batch size of
    two hundred. These parameters are just preliminary parameter choices; you should
    be able to find more optimal parameters with experimentation, which is encouraged:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练RBM，让我们从`ratings_train`创建一个名为`inputX`的NumPy数组，并将这些值转换为float32。我们还将定义RBM以接受一千维的输入，输出一千维的输出，使用学习率为0.3，训练五百个epoch，并使用批量大小为两百。这些参数只是初步的参数选择；您应该通过实验找到更优的参数，鼓励进行实验：
- en: '[PRE31]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Let’s begin training:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始训练：
- en: '[PRE32]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[Figure 10-4](#plot_of_rvm_errors) displays the plot of the reconstruction
    errors.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10-4](#plot_of_rvm_errors) 显示了重构误差的图。'
- en: '![Plot of RBM Errors](assets/hulp_1004.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![RBM错误图](assets/hulp_1004.png)'
- en: Figure 10-4\. Plot of RBM errors
  id: totrans-169
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-4\. RBM错误图
- en: The error terms generally decrease the longer we train.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 长时间训练后，误差项通常会减少。
- en: 'Now let’s take the RBM model we developed to predict the ratings for users
    in the validation set (which has the same users as the training set):'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将开发的RBM模型应用于预测验证集中用户的评分（该验证集与训练集中的用户相同）：
- en: '[PRE33]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, let’s convert the predictions into an array and calculate the MSE against
    the true validation ratings:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将预测转换为数组，并根据真实验证评分计算MSE：
- en: '[PRE34]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The following code displays the MSE on the validation set:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了验证集上的MSE：
- en: '[PRE35]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This MSE is a starting point and will likely improve with greater experimentation.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这个MSE是一个起点，随着更多的实验，可能会有所改进。
- en: Conclusion
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, we explored restricted Boltzmann machines and used them to
    build a recommender system for movie ratings. The RBM recommender we built learned
    the probability distribution of ratings of movies for users given their previous
    ratings and the ratings of users to which they were most similar to. We then used
    the learned probability distribution to predict ratings on never-before-seen movies.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了受限玻尔兹曼机，并用它们构建了一个电影评分的推荐系统。我们构建的RBM推荐系统学习了给定用户之前评分和他们最相似用户的评分情况下电影评分的概率分布。然后，我们使用学习的概率分布来预测以前未见的电影的评分。
- en: In [Chapter 11](ch11.html#Chapter_11), we will stack RBMs together to build
    deep belief networks and use them to perform even more powerful unsupervised learning
    tasks.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第11章](ch11.html#Chapter_11)，我们将堆叠RBM以构建深度信念网络，并使用它们执行更强大的无监督学习任务。
- en: ^([1](ch10.html#idm140637537216896-marker)) The most common training algorithm
    for this class of RBMs is known as the gradient-based contrastive divergence algorithm.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch10.html#idm140637537216896-marker)) 这类RBM的最常见训练算法被称为基于梯度的对比散度算法。
- en: ^([2](ch10.html#idm140637534592544-marker)) For more on this topic, see the
    paper [“On Contrastive Divergence Learning”](http://bit.ly/2RukFuX).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch10.html#idm140637534592544-marker)) 更多关于这个主题的内容，请参阅论文[“对比散度学习”](http://bit.ly/2RukFuX)。
