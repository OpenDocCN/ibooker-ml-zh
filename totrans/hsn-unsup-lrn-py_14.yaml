- en: Chapter 10\. Recommender Systems Using Restricted Boltzmann Machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier in this book, we used unsupervised learning to learn the underlying
    (hidden) structure in unlabeled data. Specifically, we performed dimensionality
    reduction, reducing a high-dimensional dataset to one with much fewer dimensions,
    and built an anomaly detection system. We also performed clustering, grouping
    objects together based on how similar or dissimilar they were to each other.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will move into *generative unsupervised models*, which involve learning
    a probability distribution from an original dataset and using it to make inferences
    about never-before-seen data. In later chapters, we will use such models to generate
    seemingly real data, which at times is virtually indistinguishable from the original
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Until now, we have looked at mostly *discriminative models* that learn to separate
    observations based on what the algorithms learn from the data; these discriminative
    models do not learn the probability distribution from the data. Discriminative
    models include supervised ones such as the logistic regression and decision trees
    from [Chapter 2](ch02.html#Chapter_2) as well as clustering methods such as *k*-means
    and hierarchical clustering from [Chapter 5](ch05.html#Chapter_5).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with the simplest of the generative unsupervised models known as
    the *restricted Boltzmann machine*.
  prefs: []
  type: TYPE_NORMAL
- en: Boltzmann Machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Boltzmann machines* were first invented in 1985 by Geoffrey Hinton (then a
    professor at Carnegie Mellon University and now one of the fathers of the deep
    learning movement, a professor at the University of Toronto, and a machine learning
    researcher at Google) and Terry Sejnowski (who was a professor at John Hopkins
    University at the time).'
  prefs: []
  type: TYPE_NORMAL
- en: Boltzmann machines—of the unrestricted type—consist of a neural network with
    an input layer and one or several hidden layers. The neurons or units in the neural
    network make stochastic decisions about whether to turn on or not based on the
    data fed in during training and the cost function the Boltzmann machine is trying
    to minimize. With this training, the Boltzmann machine discovers interesting features
    about the data, which helps model the complex underlying relationships and patterns
    present in the data.
  prefs: []
  type: TYPE_NORMAL
- en: However, these unrestricted Boltzmann machines use neural networks with neurons
    that are connected not only to other neurons in other layers but also to neurons
    within the same layer. That, coupled with the presence of many hidden layers,
    makes training an unrestricted Boltzmann machine very inefficient. Unrestricted
    Boltzmann machines had little commercial success during the 1980s and 1990s as
    a result.
  prefs: []
  type: TYPE_NORMAL
- en: Restricted Boltzmann Machines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the 2000s, Geoffrey Hinton and others began to have commercial success by
    using a modified version of the original unrestricted Boltzmann machines. These
    *restricted Boltzmann machines (RBMs)* have an input layer (also referred to as
    the *visible layer*) and just a single hidden layer, and the connections among
    neurons are restricted such that neurons are connected only to the neurons in
    other layers but not to neurons within the same layer. In other words, there are
    no visible-visible connections and no hidden-hidden connections.^([1](ch10.html#idm140637537216896))
  prefs: []
  type: TYPE_NORMAL
- en: Geoffrey Hinton also demonstrated that such simple RBMs could be stacked on
    top of each other so that the output of the hidden layer of one RBM can be fed
    into the input layer of another RBM. This sort of RBM stacking can be repeated
    many times to learn progressively more nuanced hidden representations of the original
    data. This network of many RBMs can be viewed as one deep, multilayered neural
    network model—and thus the field of deep learning took off, starting in 2006.
  prefs: []
  type: TYPE_NORMAL
- en: Note that RBMs use a *stochastic* approach to learning the underlying structure
    of data, whereas autoencoders, for example, use a *deterministic* approach.
  prefs: []
  type: TYPE_NORMAL
- en: Recommender Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will use RBMs to build a *recommender system*, one of the
    most successful applications of machine learning to date and widely used in industry
    to help predict user preferences for movies, music, books, news, search, shopping,
    digital advertising, and online dating.
  prefs: []
  type: TYPE_NORMAL
- en: There are two major categories of recommender systems—*collaborative filtering*
    recommender systems and *content-based filtering* recommender systems. Collaborative
    filtering involves building a recommender system from a user’s past behavior and
    those of other users to which the user is similar to. This recommender system
    can then predict items that the user may have an interest in even though the user
    has never expressed explicit interest. Movie recommendations on Netflix rely on
    collaborative filtering.
  prefs: []
  type: TYPE_NORMAL
- en: Content-based filtering involves learning the distinct properties of an item
    to recommend additional items with similar properties. Music recommendations on
    Pandora rely on content-based filtering.
  prefs: []
  type: TYPE_NORMAL
- en: Collaborative Filtering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Content-based filtering is not commonly used because it is a rather difficult
    task to learn the distinct properties of items—this level of understanding is
    very challenging for artificial machines to achieve currently. It is much easier
    to collect and analyze a large amount of information on users’ behaviors and preferences
    and make predictions based on this. Therefore, collaborative filtering is much
    more widely used and is the type of recommender system we will focus on here.
  prefs: []
  type: TYPE_NORMAL
- en: Collaborative filtering requires no knowledge of the underlying items themselves.
    Rather, collaborative filtering assumes that users that agreed in the past will
    agree in the future and that user preferences remain stable over time. By modeling
    how similar users are to other users, collaborative filtering can make pretty
    powerful recommendations. Moreover, collaborative filtering does not have to rely
    on *explicit data* (i.e., ratings that users provide). Rather, it can work with
    *implicit data* such as how long or how often a user views or clicks on a particular
    item. For example, in the past Netflix asked users to rate movies but now uses
    implicit user behavior to make inferences about user likes and dislikes.
  prefs: []
  type: TYPE_NORMAL
- en: However, collaborative filtering has its challenges. First, it requires a lot
    of user data to make good recommendations. Second, it is a very computationally
    demanding task. Third, the datasets are generally very sparse since users will
    have exhibited preferences for only a small fraction of all the items in the universe
    of possible items. Assuming we have enough data, there are techniques we can use
    to handle the sparsity of the data and efficiently solve the problem, which we
    will cover in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The Netflix Prize
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In 2006, Netflix sponsored a three-year-long competition to improve its movie
    recommender system. The company offered a grand prize of one million dollars to
    the team that could improve the accuracy of its existing recommender system by
    at least 10%. It also released a dataset of over 100 million movie ratings. In
    September 2009, BellKor’s Pramatic Chaos team won the prize, using an ensemble
    of many different algorithmic approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Such a high-profile competition with a rich dataset and meaningful prize energized
    the machine learning community and led to substantial progress in recommender
    system research, which paved the way for better recommender systems in industry
    over the past several years.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will use a similar movie rating dataset to build our own
    recommender system using RBMs.
  prefs: []
  type: TYPE_NORMAL
- en: MovieLens Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Instead of the 100 million ratings Netflix dataset, we will use a smaller movie
    ratings dataset known as the *MovieLens 20M Dataset*, provided by GroupLens, a
    research lab in the Department of Computer Science and Engineering at the University
    of Minnesota, Twin Cities. The data contains 20,000,263 ratings across 27,278
    movies created by 138,493 users from January 9, 1995 to March 31, 2015\. Of users
    who rated at least 20 movies each, we will select a subset at random.
  prefs: []
  type: TYPE_NORMAL
- en: This dataset is more manageable to work with than the 100 million ratings dataset
    from Netflix. Because the file size exceeds one hundred megabytes, the file is
    not accessible on GitHub. You will need to download the file directly from the
    [MovieLens website](http://bit.ly/2G0ZHCn).
  prefs: []
  type: TYPE_NORMAL
- en: Data Preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As before, let’s load in the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will load in the ratings dataset and convert the fields into the appropriate
    data types. We have just a few fields. The user ID, the movie ID, the rating provided
    by the user for the movie, and the timestamp of the rating provided:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[Table 10-1](#movielens_ratings_data) shows a partial view of the data.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 10-1\. MovieLens ratings data
  prefs: []
  type: TYPE_NORMAL
- en: '|  | userId | movieId | rating | timestamp |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 2 | 3.5 | 2005-04-02 23:53:47 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 29 | 3.5 | 2005-04-02 23:31:16 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1 | 32 | 3.5 | 2005-04-02 23:33:39 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1 | 47 | 3.5 | 2005-04-02 23:32:07 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1 | 50 | 3.5 | 2005-04-02 23:29:40 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 1 | 112 | 3.5 | 2004-09-10 03:09:00 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 1 | 151 | 4.0 | 2004-09-10 03:08:54 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 1 | 223 | 4.0 | 2005-04-02 23:46:13 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 1 | 253 | 4.0 | 2005-04-02 23:35:40 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 1 | 260 | 4.0 | 2005-04-02 23:33:46 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 1 | 293 | 4.0 | 2005-04-02 23:31:43 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 1 | 296 | 4.0 | 2005-04-02 23:32:47 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 1 | 318 | 4.0 | 2005-04-02 23:33:18 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | 1 | 337 | 3.5 | 2004-09-10 03:08:29 |'
  prefs: []
  type: TYPE_TB
- en: 'Let’s confirm the number of unique users, unique movies, and total ratings,
    and we will also calculate the average number of ratings provided by users:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The data is as we expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: To reduce the complexity and size of this dataset, let’s focus on the top one
    thousand most rated movies. This will reduce the number of ratings from about
    ~20 million to about ~12.8 million.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also take a sample of one thousand users at random and filter the dataset
    for just these users. This will reduce the number of ratings from ~12.8 million
    to just 90,213\. This number is sufficient to demonstrate collaborative filtering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s also reindex `movieID` and `userID` to a range of 1 to 1,000 for our
    reduced dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s calculate the number of unique users, unique movies, total ratings, and
    average number of ratings per user for our reduced dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s generate a test set and a validation set from this reduced dataset so
    that each holdout set is 5% of the reduced dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following shows the sizes of the train, validation, and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the Cost Function: Mean Squared Error'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we are ready to work with the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s create a matrix *m* x *n*, where *m* are the users and *n* are
    the movies. This will be a sparsely populated matrix because users rate only a
    fraction of the movies. For example, a matrix with one thousand users and one
    thousand movies will have only 81,191 ratings in the training set. If each of
    the one thousand users rated each of the one thousand movies, we would have a
    matrix with one million ratings, but users rate only a small subset of movies
    on average, so we have only 81,191 ratings on the training set. The rest (nearly
    92% of the values in the matrix) will be zeros:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We will generate similar matrices for the validation set and the test set,
    which will be even sparser, of course:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Before we build our recommender systems, let’s define the cost function that
    we will use to judge the goodness of our model. We will use *mean squared error
    (MSE)*, one of the simplest cost functions in machine learning. MSE measures the
    averaged squared error between the predicted values and the actual values. To
    calculate the MSE, we need two vectors of size *[n,1]*, where *n* is the number
    of ratings we are predicting—4,511 for the validation set. One vector has the
    actual ratings, and the other vector has the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first flatten the sparse matrix with the ratings for the validation set.
    This will be the vector of actual ratings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Perform Baseline Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a baseline, let’s predict an average rating of 3.5 for the validation set
    and calculate the MSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The MSE of this very naive prediction is 1.05\. This is our baseline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see if we can improve our results by predicting a user’s rating for a
    given movie based on that user’s average rating for all other movies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The MSE improves to 0.909:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s predict a user’s rating for a given movie based on the average rating
    all other users have given that movie:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The MSE of this approach is 0.914, similar to that found using user average:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Matrix Factorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we build a recommender system using RBMs, let’s first build one using
    *matrix factorization*, one of the most successful and popular collaborative filtering
    algorithms today. Matrix factorization decomposes the user-item matrix into a
    product of two lower dimensionality matrices. Users are represented in lower dimensional
    latent space, and so are the items.
  prefs: []
  type: TYPE_NORMAL
- en: Assume our user-item matrix is *R*, with *m* users and *n* items. Matrix factorization
    will create two lower dimensionality matrices, *H* and *W*. *H* is an "*m* users”
    x "*k* latent factors” matrix, and *W* is a "*k* latent factors” x "*n* items”
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ratings are computed by matrix multiplication: *R* = *H__W*.'
  prefs: []
  type: TYPE_NORMAL
- en: The number of *k* latent factors determines the capacity of the model. The higher
    the *k*, the greater the capacity of the model. By increasing *k*, we can improve
    the personalization of rating predictions for users, but, if *k* is too high,
    the model will overfit the data.
  prefs: []
  type: TYPE_NORMAL
- en: All of this should be familiar to you. Matrix factorization learns representations
    for the users and items in a lower dimensional space and makes predictions based
    on the newly learned representations.
  prefs: []
  type: TYPE_NORMAL
- en: One Latent Factor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start with the simplest form of matrix factorization—with just one latent
    factor. We will use Keras to perform our matrix factorization.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to define the graph. The input is the one-dimensional vector
    of users for the user embedding and the one-dimensional vector of movies for the
    movie embedding. We will embed these input vectors into a latent space of one
    and then flatten them. To generate the output vector *product*, we will take the
    dot product of the movie vector and user vector. We will use the *Adam optimizer*
    to minimize our cost fuction, which is defined as the `mean_squared_error`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Let’s train the model by feeding in the user and movie vectors from the training
    dataset. We will also evaluate the model on the validation set while we train.
    The MSE will be calculated against the actual ratings we have.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will train for one hundred epochs and store the history of the training
    and validation results. Let’s also plot the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 10-1](#plot_of_validation_mse_using_mf_and_one_latent_factor) shows
    the results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Plot of Validation MSE using MF and One Latent Factor](assets/hulp_1001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-1\. Plot of validation MSE using matrix factorization and one latent
    factor
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The minimum MSE using matrix factorization and one latent factor is 0.796\.
    This is a better MSE than our user average and movie average approaches from earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see if we can do even better by increasing the number of latent factors
    (i.e., the capacity of the model).
  prefs: []
  type: TYPE_NORMAL
- en: Three Latent Factors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Figure 10-2](#plot_of_validation_mse_using_mf_and_three_latent_factors) displays
    the results of using three latent factors.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Plot of Validation MSE using MF and Three Latent Factors](assets/hulp_1002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-2\. Plot of validation MSE using matrix factorization and three latent
    factors
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The minimum MSE is 0.765, which is better than the one using one latent factor
    and the best yet.
  prefs: []
  type: TYPE_NORMAL
- en: Five Latent Factors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s now build a matrix factorization model using five latent factors (see
    [Figure 10-3](#plot_of_validation_mse_using_mf_and_five_latent_factors) for the
    results).
  prefs: []
  type: TYPE_NORMAL
- en: '![Plot of Validation MSE using MF and Five Latent Factors](assets/hulp_1003.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-3\. Plot of validation MSE using matrix factorization and five latent
    factors
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The minimum MSE fails to improve, and there are clear signs of overfitting after
    the first 25 epochs or so. The validation error troughs and then begins to increase.
    Adding more capacity to the matrix factorization model will not help much more.
  prefs: []
  type: TYPE_NORMAL
- en: Collaborative Filtering Using RBMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s turn back to RBMs again. Recall that RBMs have two layers—the input/visible
    layer and the hidden layer. The neurons in each layer communicate with neurons
    in the other layer but not with neurons in the same layer. In other words, there
    is no intralayer communication among the neurons—this is the *restricted* bit
    of RBMs.
  prefs: []
  type: TYPE_NORMAL
- en: Another important feature of RBMs is that the communication between layers happens
    in both directions—not just in one direction. For example, with autoencoders,
    the neurons communicate with the next layer, passing information only in a feedforward
    way.
  prefs: []
  type: TYPE_NORMAL
- en: With RBMs, the neurons in the visible layer communicate with the hidden layer,
    and then the hidden layer passes back information to the visibile layer, going
    back and forth several times. RBMs perform this communication—the passes back
    and forth between the visible and hidden layer—to develop a generative model such
    that the reconstructions from the outputs of the hidden layer are similar to the
    original inputs.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, the RBMs are trying to create a generative model that will help
    predict whether a user will like a movie that the user has never seen based on
    how similar the movie is to other movies the user has rated and based on how similar
    the user is to the other users that have rated that movie.
  prefs: []
  type: TYPE_NORMAL
- en: The visible layer will have X neurons, where X is the number of movies in the
    dataset. Each neuron will have a normalized rating value from zero to one, where
    zero means the user has not seen the movie. The closer the normalized rating value
    is to one, the more the user likes the movie represented by the neuron.
  prefs: []
  type: TYPE_NORMAL
- en: The neurons in the visible layer will communicate with the neurons in the hidden
    layer, which will try to learn the underlying, latent features that characterize
    the user-movie preferences.
  prefs: []
  type: TYPE_NORMAL
- en: Note that RBMs are also referred to as *symmetrical bipartite, bidirectional
    graphs*—symmetrical because each visible node is connected to each hidden node,
    bipartite because there are two layers of nodes, and bidirectional because the
    communication happens in both directions.
  prefs: []
  type: TYPE_NORMAL
- en: RBM Neural Network Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For our movie-recommender system, we have an *m* x *n* matrix with *m* users
    and *n* movies. To train the RBM, we pass along a batch of *k* users with their
    *n* movie ratings into the neural network and train for a certain number of *epochs*.
  prefs: []
  type: TYPE_NORMAL
- en: Each input *x* that is passed into the neural network represents a single user’s
    rating preferences for all *n* movies, where *n* is one thousand in our example.
    Therefore, the visible layer has *n* nodes, one for each movie.
  prefs: []
  type: TYPE_NORMAL
- en: We can specify the number of nodes in the hidden layer, which will generally
    be fewer than the nodes in the visible layer to force the hidden layer to learn
    the most salient aspects of the original input as efficiently as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Each input *v0* is multiplied by its respective weight *W*. The weights are
    learned by the connections from the visible layer to the hidden layer. Then we
    add a bias vector at the hidden layer called *hb*. The bias ensures that at least
    some of the neurons fire. This *W*v0+hb* result is passed through an activation
    function.
  prefs: []
  type: TYPE_NORMAL
- en: After this, we will take a sample of the outputs generated via a process known
    as *Gibbs sampling*. In other words, the activation of the hidden layer results
    in final outputs that are generated stochastically. This level of randomness helps
    build a better-performing and more robust generative model.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the output after Gibbs sampling—known as *h0*—is passed back through the
    neural network in the opposite direction in what is called a *backward pass*.
    In the backward pass, the activations in the *forward pass* after Gibbs sampling
    are fed into the hidden layer and multiplied by the same weights *W* as before.
    We then add a new bias vector at the visible layer called *vb*.
  prefs: []
  type: TYPE_NORMAL
- en: This *W_h0+vb* is passed through an activation function, and then we perform
    Gibbs sampling. The output of this is *v1*, which is then passed as the new input
    into the visible layer and through the neural network as another forward pass.
  prefs: []
  type: TYPE_NORMAL
- en: The RBM goes through a series of forward and backward passes like this to learn
    the optimal weights as it attempts to build a robust generative model. RBMs are
    the first type of *generative learning* model that we have explored. By performing
    Gibbs sampling and retraining weights via forward and backward passes, RBMs are
    trying to learn the *probability distribution* of the original input. Specifically,
    RBMs minimize the *Kullback–Leibler divergence*, which measures how one probability
    distribution is different from another; in this case, RBMs are minimizing the
    probability distribution of the original input from the probability distribution
    of the reconstructed data.
  prefs: []
  type: TYPE_NORMAL
- en: By iteratively readjusting the weights in the neural net, the RBM learns to
    approximate the original data as best as possible.
  prefs: []
  type: TYPE_NORMAL
- en: With this newly learned probability distribution, RBMs are able to make predictions
    about never-before-seen data. In this case, the RBM we design will attempt to
    predict ratings for movies that the user has never seen based on the user’s similarity
    to other users and the ratings those movies have received by the other users.
  prefs: []
  type: TYPE_NORMAL
- en: Build the Components of the RBM Class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we will initialize the class with a few parameters; these are the input
    size of the RBM, the output size, the learning rate, the number of epochs to train
    for, and the batch size during the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will also create zero matrices for the weight matrix, the hidden bias vector,
    and the visible bias vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s define functions for the forward pass, the backward pass, and the
    sampling of data during each of these passes back and forth.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the forward pass, where *h* is the hidden layer and *v* is the visible
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the backward pass:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the sampling function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Now we need a function that performs that training. Since we are using TensorFlow,
    we first need to create placeholders for the TensorFlow graph, which we will use
    when we feed data into the TensorFlow session.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will have placeholders for the weights matrix, the hidden bias vector, and
    the visible bias vector. We will also need to initialize the values for these
    three using zeros. And, we will need one set to hold the current values and one
    set to hold the previous values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Likewise, we need a placeholder for the visible layer. The hidden layer is
    derived from matrix multiplication of the visible layer and the weights matrix
    and the matrix addition of the hidden bias vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'During the backward pass, we take the hidden layer output, multiply it with
    the transpose of the weights matrix used during the forward pass, and add the
    visible bias vector. Note that the weights matrix is the same during both the
    forward and the backward pass. Then, we perform the forward pass again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: To update the weights, we perform constrastive divergence.^([2](ch10.html#idm140637534592544))
  prefs: []
  type: TYPE_NORMAL
- en: We also define the error as MSE.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: With this, we are ready to initialize the TensorFlow session with the variables
    we have just defined.
  prefs: []
  type: TYPE_NORMAL
- en: Once we call *sess.run*, we can feed in batches of data to begin the training.
    During the training, forward and backward passes will be made, and the RBM will
    update weights based on how the generated data compares to the original input.
    We will print the reconstruction error from each epoch.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Train RBM Recommender System
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To train the RBM, let’s create a NumPy array called `inputX` from `ratings_train`
    and convert these values to float32\. We will also define the RBM to take in a
    one thousand-dimensional input, output a one thousand-dimensional output, use
    a learning rate of 0.3, train for five hundred epochs, and use a batch size of
    two hundred. These parameters are just preliminary parameter choices; you should
    be able to find more optimal parameters with experimentation, which is encouraged:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s begin training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 10-4](#plot_of_rvm_errors) displays the plot of the reconstruction
    errors.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Plot of RBM Errors](assets/hulp_1004.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-4\. Plot of RBM errors
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The error terms generally decrease the longer we train.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s take the RBM model we developed to predict the ratings for users
    in the validation set (which has the same users as the training set):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s convert the predictions into an array and calculate the MSE against
    the true validation ratings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code displays the MSE on the validation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This MSE is a starting point and will likely improve with greater experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored restricted Boltzmann machines and used them to
    build a recommender system for movie ratings. The RBM recommender we built learned
    the probability distribution of ratings of movies for users given their previous
    ratings and the ratings of users to which they were most similar to. We then used
    the learned probability distribution to predict ratings on never-before-seen movies.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 11](ch11.html#Chapter_11), we will stack RBMs together to build
    deep belief networks and use them to perform even more powerful unsupervised learning
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch10.html#idm140637537216896-marker)) The most common training algorithm
    for this class of RBMs is known as the gradient-based contrastive divergence algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch10.html#idm140637534592544-marker)) For more on this topic, see the
    paper [“On Contrastive Divergence Learning”](http://bit.ly/2RukFuX).
  prefs: []
  type: TYPE_NORMAL
