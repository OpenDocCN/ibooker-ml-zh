<html><head></head><body><section data-pdf-bookmark="Chapter 18. Clustering" data-type="chapter" epub:type="chapter"><div class="chapter" id="idm46066882100744">&#13;
<h1><span class="label">Chapter 18. </span>Clustering</h1>&#13;
&#13;
&#13;
<p><a data-primary="clustering" data-type="indexterm" id="ix_ch18-asciidoc0"/>Clustering is an unsupervised machine learning technique used to divide a group into cohorts. It is unsupervised because we don’t give the model any labels; it just inspects the features and determines which samples are similar and belong in a cluster. In this chapter, we will look at the K-means and hierarchical clustering methods. We will also explore the Titanic dataset again using various techniques.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="K-Means" data-type="sect1"><div class="sect1" id="idm46066881683896">&#13;
<h1>K-Means</h1>&#13;
&#13;
<p><a data-primary="clustering" data-secondary="k-means" data-type="indexterm" id="ix_ch18-asciidoc1"/><a data-primary="k-means clustering" data-type="indexterm" id="ix_ch18-asciidoc2"/>The K-means algorithm requires the user to pick the number of clusters or “k.” It then randomly chooses k centroids and assigns each sample to a cluster based on a distance metric from the centroid. Following the assignment, it recalculates the centroids based on the center of every sample assigned to a label. It then repeats assigning samples to clusters based on the new centroids. After a few iterations it should converge.</p>&#13;
&#13;
<p>Because clustering uses distance metrics to determine which samples are similar, the behavior may change depending on the scale of the data. You can standardize the data and put all of the features on the same scale. Some have suggested that a SME might advise against standardizing if the scale hints that some features have more importance. We will standardize the data here in this example.</p>&#13;
&#13;
<p>In this example, we will cluster the Titanic passengers. We will start with two clusters to see if the clustering can tease apart survival (we won’t leak the survival data into the clustering and will only use <code>X</code>, not <code>y</code>).</p>&#13;
&#13;
<p>Unsupervised algorithms have a <code>.fit</code> method and a <code>.predict</code> method. We only pass <code>X</code> into <code>.fit</code>:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.cluster</code> <code class="kn">import</code> <code class="n">KMeans</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">X_std</code> <code class="o">=</code> <code class="n">preprocessing</code><code class="o">.</code><code class="n">StandardScaler</code><code class="p">()</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">X</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">km</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">km</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_std</code><code class="p">)</code>&#13;
<code class="go">KMeans(algorithm='auto', copy_x=True,</code>&#13;
<code class="go">  init='k-means', max_iter=300,</code>&#13;
<code class="go">  n_clusters=2, n_init=10, n_jobs=1,</code>&#13;
<code class="go">  precompute_distances='auto',</code>&#13;
<code class="go">  random_state=42, tol=0.0001, verbose=0)</code></pre>&#13;
&#13;
<p>After the model is trained, we can call the <code>.predict</code> method to assign new samples to a cluster:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">X_km</code> <code class="o">=</code> <code class="n">km</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">X_km</code>&#13;
<code class="go">array([1, 1, 1, ..., 1, 1, 1], dtype=int32)</code></pre>&#13;
&#13;
<p>Instance parameters:</p>&#13;
<dl>&#13;
<dt><code>n_clusters=8</code></dt>&#13;
<dd>&#13;
<p>Number of clusters to create.</p>&#13;
</dd>&#13;
<dt><code>init='kmeans++'</code></dt>&#13;
<dd>&#13;
<p>Initialization method.</p>&#13;
</dd>&#13;
<dt><code>n_init=10</code></dt>&#13;
<dd>&#13;
<p>Number of times to run the algorithm with different centroids. Best score will win.</p>&#13;
</dd>&#13;
<dt><code>max_iter=300</code></dt>&#13;
<dd>&#13;
<p>Number of iterations for a run.</p>&#13;
</dd>&#13;
<dt><code>tol=0.0001</code></dt>&#13;
<dd>&#13;
<p>Tolerance until convergence.</p>&#13;
</dd>&#13;
<dt><code>precompute_distances='auto'</code></dt>&#13;
<dd>&#13;
<p>Precompute distances (takes more memory but is faster). <code>auto</code> will precompute if <code>n_samples</code> * <code>n_clusters</code> is less than or equal to 12 million.</p>&#13;
</dd>&#13;
<dt><code>verbose=0</code></dt>&#13;
<dd>&#13;
<p>Verbosity.</p>&#13;
</dd>&#13;
<dt><code>random_state=None</code></dt>&#13;
<dd>&#13;
<p>Random seed.</p>&#13;
</dd>&#13;
<dt><code>copy_x=True</code></dt>&#13;
<dd>&#13;
<p>Copy data before computing.</p>&#13;
</dd>&#13;
<dt><code>n_jobs=1</code></dt>&#13;
<dd>&#13;
<p>Number of CPUs to use.</p>&#13;
</dd>&#13;
<dt><code>algorithm='auto'</code></dt>&#13;
<dd>&#13;
<p>K-means algorithm. <code>'full'</code> works with sparse data, but <code>'elkan'</code> is more efficient. <code>'auto'</code> uses <code>'elkan'</code> with dense data.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Attributes:</p>&#13;
<dl>&#13;
<dt><code>cluster_centers_</code></dt>&#13;
<dd>&#13;
<p>Coordinates of centers</p>&#13;
</dd>&#13;
<dt><code>labels_</code></dt>&#13;
<dd>&#13;
<p>Labels for samples</p>&#13;
</dd>&#13;
<dt><code>inertia_</code></dt>&#13;
<dd>&#13;
<p>Sum of squared distance to cluster centroid</p>&#13;
</dd>&#13;
<dt><code>n_iter_</code></dt>&#13;
<dd>&#13;
<p>Number of iterations</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>If you don’t know ahead of time how many clusters you need, you can run the algorithm with a range of sizes and evaluate various metrics. It can be tricky.</p>&#13;
&#13;
<p>You can roll your own elbow plot using the <code>.inertia_</code> calculation. Look for where the curve bends as that is potentially a good choice for the number of clusters. In this case, the curve is smooth, but after eight there doesn’t seem to be much improvement (see <a data-type="xref" href="#idkm1">Figure 18-1</a>).</p>&#13;
&#13;
<p>For plots without an elbow, we have a few options. We can use other metrics, some of which are shown below. We can also inspect a visualization of the clustering and see if clusters are visible. We can add features to the data and see if that helps with clustering.</p>&#13;
&#13;
<p>Here is the code for an elbow plot:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">inertias</code> <code class="o">=</code> <code class="p">[]</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">sizes</code> <code class="o">=</code> <code class="nb">range</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">12</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">k</code> <code class="ow">in</code> <code class="n">sizes</code><code class="p">:</code>&#13;
<code class="gp">... </code>    <code class="n">k2</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">,</code> <code class="n">n_clusters</code><code class="o">=</code><code class="n">k</code><code class="p">)</code>&#13;
<code class="gp">... </code>    <code class="n">k2</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>&#13;
<code class="gp">... </code>    <code class="n">inertias</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">k2</code><code class="o">.</code><code class="n">inertia_</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">inertias</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">sizes</code><code class="p">)</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s">"K"</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s">"Inertia"</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1801.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="idkm1">&#13;
<img alt="Elbow plot that is looking rather smooth." src="assets/mlpr_1801.png"/>&#13;
<h6><span class="label">Figure 18-1. </span>Elbow plot that is looking rather smooth.</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-primary="clustering" data-secondary="metrics" data-type="indexterm" id="idm46066881399688"/><a data-primary="metrics" data-secondary="clustering" data-type="indexterm" id="idm46066881398712"/><a data-primary="scikit-learn" data-secondary="clustering metrics" data-type="indexterm" id="idm46066881397768"/>Scikit-learn has other clustering metrics when the ground truth labels are not known. We can calculate and plot those as well. <a data-primary="silhouette coefficient" data-type="indexterm" id="idm46066881396552"/>The <em>Silhouette Coefficient</em> is a value between -1 and 1. The higher the score, the better. 1 indicates tight clusters, and 0 means overlapping clusters. From that measure, two clusters gives us the best score.</p>&#13;
&#13;
<p><a data-primary="Calinski-Harabasz Index" data-type="indexterm" id="idm46066881394856"/>The <em>Calinski-Harabasz Index</em> is the ratio of between-cluster dispersion and within-cluster dispersion. A higher score is better. Two clusters gives the best score for this metric.</p>&#13;
&#13;
<p><a data-primary="Davis-Bouldin Index" data-type="indexterm" id="idm46066881393160"/>The <em>Davis-Bouldin Index</em> is the average similarity between each cluster and the closest cluster. Scores range from 0 and up. 0 indicates better clustering.</p>&#13;
&#13;
<p>Here we will plot inertia, the silhouette coefficient, the Calinski-Harabasz Index, and the Davies-Bouldin Index over a range of cluster sizes to see if there is a clear size of clusters for the data (see <a data-type="xref" href="#idkm2">Figure 18-2</a>). It appears that most of these metrics agree on two clusters:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">metrics</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">inertias</code> <code class="o">=</code> <code class="p">[]</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">sils</code> <code class="o">=</code> <code class="p">[]</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">chs</code> <code class="o">=</code> <code class="p">[]</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">dbs</code> <code class="o">=</code> <code class="p">[]</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">sizes</code> <code class="o">=</code> <code class="nb">range</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">12</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">k</code> <code class="ow">in</code> <code class="n">sizes</code><code class="p">:</code>&#13;
<code class="gp">... </code>    <code class="n">k2</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">,</code> <code class="n">n_clusters</code><code class="o">=</code><code class="n">k</code><code class="p">)</code>&#13;
<code class="gp">... </code>    <code class="n">k2</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_std</code><code class="p">)</code>&#13;
<code class="gp">... </code>    <code class="n">inertias</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">k2</code><code class="o">.</code><code class="n">inertia_</code><code class="p">)</code>&#13;
<code class="gp">... </code>    <code class="n">sils</code><code class="o">.</code><code class="n">append</code><code class="p">(</code>&#13;
<code class="gp">... </code>        <code class="n">metrics</code><code class="o">.</code><code class="n">silhouette_score</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">k2</code><code class="o">.</code><code class="n">labels_</code><code class="p">)</code>&#13;
<code class="gp">... </code>    <code class="p">)</code>&#13;
<code class="gp">... </code>    <code class="n">chs</code><code class="o">.</code><code class="n">append</code><code class="p">(</code>&#13;
<code class="gp">... </code>        <code class="n">metrics</code><code class="o">.</code><code class="n">calinski_harabasz_score</code><code class="p">(</code>&#13;
<code class="gp">... </code>            <code class="n">X</code><code class="p">,</code> <code class="n">k2</code><code class="o">.</code><code class="n">labels_</code>&#13;
<code class="gp">... </code>        <code class="p">)</code>&#13;
<code class="gp">... </code>    <code class="p">)</code>&#13;
<code class="gp">... </code>    <code class="n">dbs</code><code class="o">.</code><code class="n">append</code><code class="p">(</code>&#13;
<code class="gp">... </code>        <code class="n">metrics</code><code class="o">.</code><code class="n">davies_bouldin_score</code><code class="p">(</code>&#13;
<code class="gp">... </code>            <code class="n">X</code><code class="p">,</code> <code class="n">k2</code><code class="o">.</code><code class="n">labels_</code>&#13;
<code class="gp">... </code>        <code class="p">)</code>&#13;
<code class="gp">... </code>    <code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code>&#13;
<code class="gp">... </code>        <code class="p">{</code>&#13;
<code class="gp">... </code>            <code class="s">"inertia"</code><code class="p">:</code> <code class="n">inertias</code><code class="p">,</code>&#13;
<code class="gp">... </code>            <code class="s">"silhouette"</code><code class="p">:</code> <code class="n">sils</code><code class="p">,</code>&#13;
<code class="gp">... </code>            <code class="s">"calinski"</code><code class="p">:</code> <code class="n">chs</code><code class="p">,</code>&#13;
<code class="gp">... </code>            <code class="s">"davis"</code><code class="p">:</code> <code class="n">dbs</code><code class="p">,</code>&#13;
<code class="gp">... </code>            <code class="s">"k"</code><code class="p">:</code> <code class="n">sizes</code><code class="p">,</code>&#13;
<code class="gp">... </code>        <code class="p">}</code>&#13;
<code class="gp">... </code>    <code class="p">)</code>&#13;
<code class="gp">... </code>    <code class="o">.</code><code class="n">set_index</code><code class="p">(</code><code class="s">"k"</code><code class="p">)</code>&#13;
<code class="gp">... </code>    <code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">,</code> <code class="n">subplots</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">layout</code><code class="o">=</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">2</code><code class="p">))</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1802.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="idkm2">&#13;
<img alt="Cluster metrics. These metrics mostly agree on two clusters." src="assets/mlpr_1802.png"/>&#13;
<h6><span class="label">Figure 18-2. </span>Cluster metrics. These metrics mostly agree on two <span class="keep-together">clusters.</span></h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-primary="Yellowbrick" data-secondary="silhouette score visualizer" data-type="indexterm" id="idm46066881124184"/>Another technique for determining clusters is to visualize the silhouette scores for each cluster. Yellowbrick has a visualizer for this (see <a data-type="xref" href="#id56">Figure 18-3</a>).</p>&#13;
&#13;
<p>The vertical dotted red line in this plot is the average score. One way to interpret it is to make sure that each cluster bumps out above the average, and the cluster scores look decent. Make sure you are using the same x limits (<code>ax.set_xlim</code>). I would choose two clusters from these plots:<a data-startref="ix_ch18-asciidoc2" data-type="indexterm" id="idm46066881121016"/><a data-startref="ix_ch18-asciidoc1" data-type="indexterm" id="idm46066881120312"/></p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">yellowbrick.cluster.silhouette</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">SilhouetteVisualizer</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">axes</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">12</code><code class="p">,</code> <code class="mi">8</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">axes</code> <code class="o">=</code> <code class="n">axes</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="mi">4</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">k</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">6</code><code class="p">)):</code>&#13;
<code class="gp">... </code>    <code class="n">ax</code> <code class="o">=</code> <code class="n">axes</code><code class="p">[</code><code class="n">i</code><code class="p">]</code>&#13;
<code class="gp">... </code>    <code class="n">sil</code> <code class="o">=</code> <code class="n">SilhouetteVisualizer</code><code class="p">(</code>&#13;
<code class="gp">... </code>        <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="n">k</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">),</code>&#13;
<code class="gp">... </code>        <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="p">)</code>&#13;
<code class="gp">... </code>    <code class="n">sil</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_std</code><code class="p">)</code>&#13;
<code class="gp">... </code>    <code class="n">sil</code><code class="o">.</code><code class="n">finalize</code><code class="p">()</code>&#13;
<code class="gp">... </code>    <code class="n">ax</code><code class="o">.</code><code class="n">set_xlim</code><code class="p">(</code><code class="o">-</code><code class="mf">0.2</code><code class="p">,</code> <code class="mf">0.8</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">plt</code><code class="o">.</code><code class="n">tight_layout</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1803.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id56">&#13;
<img alt="Yellowbrick silhouette visualizer" src="assets/mlpr_1803.png"/>&#13;
<h6><span class="label">Figure 18-3. </span>Yellowbrick silhouette visualizer</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Agglomerative (Hierarchical) Clustering" data-type="sect1"><div class="sect1" id="idm46066881682984">&#13;
<h1>Agglomerative (Hierarchical) Clustering</h1>&#13;
&#13;
<p><a data-primary="agglomerative (hierarchical) clustering" data-type="indexterm" id="ix_ch18-asciidoc3"/><a data-primary="clustering" data-secondary="agglomerative" data-type="indexterm" id="ix_ch18-asciidoc4"/><a data-primary="hierarchical (agglomerative) clustering" data-type="indexterm" id="ix_ch18-asciidoc5"/>Agglomerative clustering is another methodology. You start off with each sample in its own cluster. Then you combine the “nearest” clusters. Repeat until done while keeping track of the nearest sizes.</p>&#13;
&#13;
<p><a data-primary="dendrograms" data-type="indexterm" id="ix_ch18-asciidoc6"/>When you have finished this, you will have a <em>dendrogram</em>, or a tree that tracks when clusters were created and what the distance metric was. <a data-primary="scipy" data-type="indexterm" id="idm46066880838936"/>You can use the scipy library to visualize the dendrogram.</p>&#13;
&#13;
<p>We can use scipy to create a dendrogram (see <a data-type="xref" href="#id59">Figure 18-4</a>). As you can see, if you have many samples the leaf nodes are hard to read:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">scipy.cluster</code> <code class="kn">import</code> <code class="n">hierarchy</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">dend</code> <code class="o">=</code> <code class="n">hierarchy</code><code class="o">.</code><code class="n">dendrogram</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">hierarchy</code><code class="o">.</code><code class="n">linkage</code><code class="p">(</code><code class="n">X_std</code><code class="p">,</code> <code class="n">method</code><code class="o">=</code><code class="s">"ward"</code><code class="p">)</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1804.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id59">&#13;
<img alt="Scipy hierarchical clustering dendrogram" src="assets/mlpr_1804.png"/>&#13;
<h6><span class="label">Figure 18-4. </span>Scipy hierarchical clustering dendrogram</h6>&#13;
</div></figure>&#13;
&#13;
<p>Once you have the dendrogram, you have all the clusters (from one to the size of the samples). The heights represent how similar clusters are when they are joined. In order to find how many clusters are in the data, you would want to “cut” a horizontal line through where it would cross the tallest lines.</p>&#13;
&#13;
<p>In this case, it looks like when you perform that cut, you have three clusters.</p>&#13;
&#13;
<p>The previous plot was a little noisy with all of the samples in it. You can also use the <code>truncate_mode</code> parameter to combine the leaves into a single node (see <a data-type="xref" href="#id60">Figure 18-5</a>):</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">scipy.cluster</code> <code class="kn">import</code> <code class="n">hierarchy</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">dend</code> <code class="o">=</code> <code class="n">hierarchy</code><code class="o">.</code><code class="n">dendrogram</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">hierarchy</code><code class="o">.</code><code class="n">linkage</code><code class="p">(</code><code class="n">X_std</code><code class="p">,</code> <code class="n">method</code><code class="o">=</code><code class="s">"ward"</code><code class="p">),</code>&#13;
<code class="gp">... </code>    <code class="n">truncate_mode</code><code class="o">=</code><code class="s">"lastp"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">p</code><code class="o">=</code><code class="mi">20</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">show_contracted</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1805.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id60">&#13;
<img alt="Truncated hierarchical clustering dendrogram. If we cut across the largest vertical lines, we get three clusters." src="assets/mlpr_1805.png"/>&#13;
<h6><span class="label">Figure 18-5. </span>Truncated hierarchical clustering dendrogram. If we cut across the largest vertical lines, we get three clusters.</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-primary="scikit-learn" data-secondary="clustering models" data-type="indexterm" id="idm46066880686136"/>Once we know how many clusters we need, we can use scikit-learn to create a model:<a data-startref="ix_ch18-asciidoc6" data-type="indexterm" id="idm46066880685032"/></p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.cluster</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">AgglomerativeClustering</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">ag</code> <code class="o">=</code> <code class="n">AgglomerativeClustering</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">n_clusters</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">affinity</code><code class="o">=</code><code class="s">"euclidean"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">linkage</code><code class="o">=</code><code class="s">"ward"</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">ag</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">)</code></pre>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p><a data-primary="fastcluster" data-type="indexterm" id="idm46066880573528"/>The <a href="https://oreil.ly/OuNuo">fastcluster package</a> provides an optimized agglomerative clustering package if the scikit-learn implementation is too slow.<a data-startref="ix_ch18-asciidoc5" data-type="indexterm" id="idm46066880571960"/><a data-startref="ix_ch18-asciidoc4" data-type="indexterm" id="idm46066880571256"/><a data-startref="ix_ch18-asciidoc3" data-type="indexterm" id="idm46066880570584"/></p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Understanding Clusters" data-type="sect1"><div class="sect1" id="idm46066881116456">&#13;
<h1>Understanding Clusters</h1>&#13;
&#13;
<p><a data-primary="clustering" data-secondary="understanding clusters" data-type="indexterm" id="ix_ch18-asciidoc7"/>Using K-means on the Titanic dataset, we will make two clusters.&#13;
We can use the grouping functionality in pandas to examine the differences in the clusters. The code below examines the mean and variance for each feature. It appears that the mean value for pclass varies quite a bit.</p>&#13;
&#13;
<p>I’m sticking the survival data back in to see if the clustering was related to that:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">km</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">km</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_std</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">labels</code> <code class="o">=</code> <code class="n">km</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_std</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">X</code><code class="o">.</code><code class="n">assign</code><code class="p">(</code><code class="n">cluster</code><code class="o">=</code><code class="n">labels</code><code class="p">,</code> <code class="n">survived</code><code class="o">=</code><code class="n">y</code><code class="p">)</code>&#13;
<code class="gp">... </code>    <code class="o">.</code><code class="n">groupby</code><code class="p">(</code><code class="s">"cluster"</code><code class="p">)</code>&#13;
<code class="gp">... </code>    <code class="o">.</code><code class="n">agg</code><code class="p">([</code><code class="s">"mean"</code><code class="p">,</code> <code class="s">"var"</code><code class="p">])</code>&#13;
<code class="gp">... </code>    <code class="o">.</code><code class="n">T</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="go">cluster                 0         1</code>&#13;
<code class="go">pclass     mean  0.526538 -1.423831</code>&#13;
<code class="go">           var   0.266089  0.136175</code>&#13;
<code class="go">age        mean -0.280471  0.921668</code>&#13;
<code class="go">           var   0.653027  1.145303</code>&#13;
<code class="go">sibsp      mean -0.010464 -0.107849</code>&#13;
<code class="go">           var   1.163848  0.303881</code>&#13;
<code class="go">parch      mean  0.387540  0.378453</code>&#13;
<code class="go">           var   0.829570  0.540587</code>&#13;
<code class="go">fare       mean -0.349335  0.886400</code>&#13;
<code class="go">           var   0.056321  2.225399</code>&#13;
<code class="go">sex_male   mean  0.678986  0.552486</code>&#13;
<code class="go">           var   0.218194  0.247930</code>&#13;
<code class="go">embarked_Q mean  0.123548  0.016575</code>&#13;
<code class="go">           var   0.108398  0.016345</code>&#13;
<code class="go">embarked_S mean  0.741288  0.585635</code>&#13;
<code class="go">           var   0.191983  0.243339</code>&#13;
<code class="go">survived   mean  0.596685  0.299894</code>&#13;
<code class="go">           var   0.241319  0.210180</code></pre>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p><a data-primary="Jupyter" data-secondary="cluster summary with" data-type="indexterm" id="idm46066880650408"/>In Jupyter you can tack on the following code&#13;
to a DataFrame, and it will highlight the high and low values of each row. This is useful for visually seeing which values stand out in the above cluster summary:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="go">.style.background_gradient(cmap='RdBu', axis=1)</code></pre>&#13;
</div>&#13;
&#13;
<p>In <a data-type="xref" href="#idclex">Figure 18-6</a> we plot a bar plot of the means for each cluster:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">... </code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">X</code><code class="o">.</code><code class="n">assign</code><code class="p">(</code><code class="n">cluster</code><code class="o">=</code><code class="n">labels</code><code class="p">,</code> <code class="n">survived</code><code class="o">=</code><code class="n">y</code><code class="p">)</code>&#13;
<code class="gp">... </code>    <code class="o">.</code><code class="n">groupby</code><code class="p">(</code><code class="s">"cluster"</code><code class="p">)</code>&#13;
<code class="gp">... </code>    <code class="o">.</code><code class="n">mean</code><code class="p">()</code>&#13;
<code class="gp">... </code>    <code class="o">.</code><code class="n">T</code><code class="o">.</code><code class="n">plot</code><code class="o">.</code><code class="n">bar</code><code class="p">(</code><code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="s">"images/mlpr_1806.png"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">bbox_inches</code><code class="o">=</code><code class="s">"tight"</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="idclex">&#13;
<img alt="Mean values of each cluster" src="assets/mlpr_1806.png"/>&#13;
<h6><span class="label">Figure 18-6. </span>Mean values of each cluster</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-primary="principal component analysis (PCA)" data-secondary="component plotting with clustering" data-type="indexterm" id="idm46066880368104"/><a data-primary="seaborn" data-secondary="PCA component plotting with clustering" data-type="indexterm" id="idm46066880367192"/>I also like to plot the PCA components, but colored by the cluster label (see <a data-type="xref" href="#idclpca">Figure 18-7</a>).&#13;
Here we use Seaborn to do that. It is also interesting to change the values&#13;
for <code>hue</code> to dive into the features that are distinct for the clusters.</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">sns</code><code class="o">.</code><code class="n">scatterplot</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="s">"PC1"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="s">"PC2"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">data</code><code class="o">=</code><code class="n">X</code><code class="o">.</code><code class="n">assign</code><code class="p">(</code>&#13;
<code class="gp">... </code>        <code class="n">PC1</code><code class="o">=</code><code class="n">X_pca</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code>&#13;
<code class="gp">... </code>        <code class="n">PC2</code><code class="o">=</code><code class="n">X_pca</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code>&#13;
<code class="gp">... </code>        <code class="n">cluster</code><code class="o">=</code><code class="n">labels</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="p">),</code>&#13;
<code class="gp">... </code>    <code class="n">hue</code><code class="o">=</code><code class="s">"cluster"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">alpha</code><code class="o">=</code><code class="mf">0.5</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="s">"images/mlpr_1807.png"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">bbox_inches</code><code class="o">=</code><code class="s">"tight"</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="idclpca">&#13;
<img alt="PCA plot of clusters" src="assets/mlpr_1807.png"/>&#13;
<h6><span class="label">Figure 18-7. </span>PCA plot of clusters</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-primary="pandas" data-secondary="feature examination in clusters" data-type="indexterm" id="idm46066880222248"/>If we want to examine a single feature, we can use the pandas <code>.describe</code> method:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">X</code><code class="o">.</code><code class="n">assign</code><code class="p">(</code><code class="n">cluster</code><code class="o">=</code><code class="n">label</code><code class="p">)</code>&#13;
<code class="gp">... </code>    <code class="o">.</code><code class="n">groupby</code><code class="p">(</code><code class="s">"cluster"</code><code class="p">)</code>&#13;
<code class="gp">... </code>    <code class="o">.</code><code class="n">age</code><code class="o">.</code><code class="n">describe</code><code class="p">()</code>&#13;
<code class="gp">... </code>    <code class="o">.</code><code class="n">T</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="go">cluster           0           1</code>&#13;
<code class="go">count    362.000000  947.000000</code>&#13;
<code class="go">mean       0.921668   -0.280471</code>&#13;
<code class="go">std        1.070188    0.808101</code>&#13;
<code class="go">min       -2.160126   -2.218578</code>&#13;
<code class="go">25%        0.184415   -0.672870</code>&#13;
<code class="go">50%        0.867467   -0.283195</code>&#13;
<code class="go">75%        1.665179    0.106480</code>&#13;
<code class="go">max        4.003228    3.535618</code></pre>&#13;
&#13;
<p><a data-primary="surrogate models" data-type="indexterm" id="idm46066880220024"/>We can also create a surrogate model to explain the clusters. Here we use a decision tree to explain them. This also shows that pclass (which had a large difference in the mean) is very important:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">dt</code> <code class="o">=</code> <code class="n">tree</code><code class="o">.</code><code class="n">DecisionTreeClassifier</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">dt</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">labels</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">col</code><code class="p">,</code> <code class="n">val</code> <code class="ow">in</code> <code class="nb">sorted</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="nb">zip</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code> <code class="n">dt</code><code class="o">.</code><code class="n">feature_importances_</code><code class="p">),</code>&#13;
<code class="gp">... </code>    <code class="n">key</code><code class="o">=</code><code class="k">lambda</code> <code class="n">col_val</code><code class="p">:</code> <code class="n">col_val</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code>&#13;
<code class="gp">... </code>    <code class="n">reverse</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">):</code>&#13;
<code class="gp">... </code>    <code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s">"{col:10}{val:10.3f}"</code><code class="p">)</code>&#13;
<code class="go">pclass         0.902</code>&#13;
<code class="go">age            0.074</code>&#13;
<code class="go">sex_male       0.016</code>&#13;
<code class="go">embarked_S     0.003</code>&#13;
<code class="go">fare           0.003</code>&#13;
<code class="go">parch          0.003</code>&#13;
<code class="go">sibsp          0.000</code>&#13;
<code class="go">embarked_Q     0.000</code></pre>&#13;
&#13;
<p>And we can visualize the decisions in <a data-type="xref" href="#iddtsurr">Figure 18-8</a>. It shows that pclass is the first feature the surrogate looks at to make a <span class="keep-together">decision</span><a data-startref="ix_ch18-asciidoc7" data-type="indexterm" id="idm46066880030360"/>:<a data-startref="ix_ch18-asciidoc0" data-type="indexterm" id="idm46066880029560"/></p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">dot_data</code> <code class="o">=</code> <code class="n">StringIO</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">tree</code><code class="o">.</code><code class="n">export_graphviz</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">dt</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">out_file</code><code class="o">=</code><code class="n">dot_data</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">feature_names</code><code class="o">=</code><code class="n">X</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">class_names</code><code class="o">=</code><code class="p">[</code><code class="s">"0"</code><code class="p">,</code> <code class="s">"1"</code><code class="p">],</code>&#13;
<code class="gp">... </code>    <code class="n">max_depth</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">filled</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">g</code> <code class="o">=</code> <code class="n">pydotplus</code><code class="o">.</code><code class="n">graph_from_dot_data</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">dot_data</code><code class="o">.</code><code class="n">getvalue</code><code class="p">()</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">g</code><code class="o">.</code><code class="n">write_png</code><code class="p">(</code><code class="s">"images/mlpr_1808.png"</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="iddtsurr">&#13;
<img alt="Decision tree explaining the clustering" src="assets/mlpr_1808.png"/>&#13;
<h6><span class="label">Figure 18-8. </span>Decision tree explaining the clustering</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>