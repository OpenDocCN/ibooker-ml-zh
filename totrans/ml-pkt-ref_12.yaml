- en: Chapter 12\. Metrics and Classification Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We’ll cover the following metrics and evaluation tools in this chapter: confusion
    matrices, various metrics, a classification report, and some visualizations.'
  prefs: []
  type: TYPE_NORMAL
- en: This will be evaluated as a decision tree model that predicts Titanic survival.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion Matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A confusion matrix can aid in understanding how a classifier performs.
  prefs: []
  type: TYPE_NORMAL
- en: 'A binary classifier can have four classification results: true positives (TP),
    true negatives (TN), false positives (FP), and false negatives (FN). The first
    two are correct classifications.'
  prefs: []
  type: TYPE_NORMAL
- en: Here is a common example for remembering the other results. Assuming positive
    means pregnant and negative is not pregnant, a false positive is like claiming
    a man is pregnant. A false negative is claiming that a pregnant woman is not (when
    she is clearly showing) (see [Figure 12-1](#iderr3)). These last two types of
    errors are referred to as *type 1* and *type 2* errors, respectively (see [Table 12-1](#table_12_1)).
  prefs: []
  type: TYPE_NORMAL
- en: Another way to remember these is that P (for false positive) has one straight
    line in it (type 1 error), and N (for false negative) has two vertical lines in
    it.
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification errors.](assets/mlpr_1201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-1\. Classification errors.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Table 12-1\. Binary classification results from a confusion matrix
  prefs: []
  type: TYPE_NORMAL
- en: '| Actual | Predicted negative | Predicted positive |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Actual negative | True negative | False positive (type 1) |'
  prefs: []
  type: TYPE_TB
- en: '| Actual positive | False negative (type 2) | True positive |'
  prefs: []
  type: TYPE_TB
- en: 'Here is the pandas code to calculate the classification results. The comments
    show the results. We will use these variables to calculate other metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Well-behaving classifiers ideally have high counts in the true diagonal. We
    can create a DataFrame using the sklearn `confusion_matrix` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Yellowbrick has a plot for the confusion matrix (see [Figure 12-2](#id30)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![Confusion matrix. The upper left and lower right are correct classifications.
    The lower left is false negative. The upper right is false positive.](assets/mlpr_1202.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-2\. Confusion matrix. The upper left and lower right are correct classifications.
    The lower left is false negative. The upper right is false positive.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `sklearn.metrics` module implements many common classification metrics,
    including:'
  prefs: []
  type: TYPE_NORMAL
- en: '`''accuracy''`'
  prefs: []
  type: TYPE_NORMAL
- en: Percent of correct predictions
  prefs: []
  type: TYPE_NORMAL
- en: '`''average_precision''`'
  prefs: []
  type: TYPE_NORMAL
- en: Precision recall curve summary
  prefs: []
  type: TYPE_NORMAL
- en: '`''f1''`'
  prefs: []
  type: TYPE_NORMAL
- en: Harmonic mean of precision and recall
  prefs: []
  type: TYPE_NORMAL
- en: '`''neg_log_loss''`'
  prefs: []
  type: TYPE_NORMAL
- en: Logistic or cross-entropy loss (model must support `predict_proba`)
  prefs: []
  type: TYPE_NORMAL
- en: '`''precision''`'
  prefs: []
  type: TYPE_NORMAL
- en: Ability to find only relevant samples (not label a negative as a positive)
  prefs: []
  type: TYPE_NORMAL
- en: '`''recall''`'
  prefs: []
  type: TYPE_NORMAL
- en: Ability to find all positive samples
  prefs: []
  type: TYPE_NORMAL
- en: '`''roc_auc''`'
  prefs: []
  type: TYPE_NORMAL
- en: Area under the receiver operator characteristic curve
  prefs: []
  type: TYPE_NORMAL
- en: These strings can be used as the `scoring` parameter when doing grid search,
    or you can use functions from the `sklearn.metrics` module that have the same
    names as the strings but end in `_score`. See the following note for examples.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '`''f1''`, `''precision''`, and `''recall''` all support the following suffixes
    for multiclass classifers:'
  prefs: []
  type: TYPE_NORMAL
- en: '`''_micro''`'
  prefs: []
  type: TYPE_NORMAL
- en: Global weighted average of metric
  prefs: []
  type: TYPE_NORMAL
- en: '`''_macro''`'
  prefs: []
  type: TYPE_NORMAL
- en: Unweighted average of metric
  prefs: []
  type: TYPE_NORMAL
- en: '`''_weighted''`'
  prefs: []
  type: TYPE_NORMAL
- en: Multiclass weighted average of metric
  prefs: []
  type: TYPE_NORMAL
- en: '`''_samples''`'
  prefs: []
  type: TYPE_NORMAL
- en: Per sample metric
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Accuracy is the percentage of correct classifications:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: What is good accuracy? It depends. If I’m predicting fraud (which usually is
    a rare event, say 1 in 10,000), I can get very high accuracy by always predicting
    not fraud. But this model is not very useful. Looking at other metrics and the
    cost of predicting a false positive and a false negative can help us determine
    if a model is decent.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use sklearn to calculate it for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Recall
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recall (also called *sensitivity*) is the percentage of positive values correctly
    classified. (How many relevant results are returned?)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Precision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Precision is the percent of positive predictions that were correct (TP divided
    by (TP + FP)). (How relevant are the results?)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: F1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'F1 is the harmonic mean of recall and precision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Classification Report
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Yellowbrick has a classification report showing precision, recall, and f1 scores
    for both positive and negative values (see [Figure 12-3](#id31)). This is colored,
    and the redder the cell (closer to one), the better the score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![Classification report.](assets/mlpr_1203.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-3\. Classification report.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: ROC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A ROC curve illustrates how the classifier performs by tracking the true positive
    rate (recall/sensitivity) as the false positive rate (inverted specificity) changes
    (see [Figure 12-4](#id32)).
  prefs: []
  type: TYPE_NORMAL
- en: 'A rule of thumb is that the plot should bulge out toward the top-left corner.
    A plot that is to the left and above another plot indicates better performance.
    The diagonal in this plot indicates the behavior of a random guessing classifier.
    By taking the AUC, you get a metric for evaluating the performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Yellowbrick can plot this for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![ROC curve.](assets/mlpr_1204.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-4\. ROC curve.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Precision-Recall Curve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ROC curve may be overly optimistic for imbalanced classes. Another option
    for evaluating classifiers is using a precision-recall curve (see [Figure 12-5](#id33)).
    Classification is a balancing act of finding everything you need (recall) while
    limiting the junk results (precision). This is typically a trade-off. As recall
    goes up, precision usually goes down and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a Yellowbrick precision-recall curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![Precision-recall curve.](assets/mlpr_1205.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-5\. Precision-recall curve.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Cumulative Gains Plot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A cumulative gains plot can be used to evaluate a binary classifier. It models
    the true positive rate (sensitivity) against the support rate (fraction of positive
    predictions). The intuition behind this plot is to sort all classifications by
    predicted probability. Ideally there would be a clean cut that divides positive
    from negative samples. If the first 10% of the predictions has 30% of the positive
    samples, you would plot a point from (0,0) to (.1, .3). You continue this process
    through all of the samples (see [Figure 12-6](#idcgc1)).
  prefs: []
  type: TYPE_NORMAL
- en: A common use for this is determining customer response. The cumulative gains
    curve plots the support or predicted positive rate along the x-axis. Our chart
    labels this as “Percentage of sample”. It plots the sensitivity or true positive
    rate along the y-axis. This is labeled as “Gain” in our plot.
  prefs: []
  type: TYPE_NORMAL
- en: If you wanted to contact 90% of the customers that would respond (sensitivity),
    you can trace from .9 on the y-axis to the right until you hit that curve. The
    x-axis at that point will indicate how many total customers you need to contact
    (support) to get to 90%.
  prefs: []
  type: TYPE_NORMAL
- en: In this case we aren’t contacting customers that would respond to a survey but
    predicting survival on the Titanic. If we ordered all passengers on the Titanic
    according to our model by how likely they are to survive, if you took the first
    65% of them, you would have 90% of the survivors. If you have an associated cost
    per contact and revenue per response, you can calculate what the best number is.
  prefs: []
  type: TYPE_NORMAL
- en: In general, a model that is to the left and above another model is a better
    model. The best models are lines that go up to the top (if 10% of the samples
    are positive, it would hit at (.1, 1)) and then directly to the right. If the
    plot is below the baseline, we would do better to randomly assign labels that
    use our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The [scikit-plot library](https://oreil.ly/dg0iQ) can create a cumulative gains
    plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![Cumulative gains plot. If we ordered people on the Titanic according to our
    model, looking at 20% of them we would get 40% of the survivors.](assets/mlpr_1206.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-6\. Cumulative gains plot. If we ordered people on the Titanic according
    to our model, looking at 20% of them we would get 40% of the survivors.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Lift Curve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A lift curve is another way of looking at the information in a cumulative gains
    plot. The *lift* is how much better we are doing than the baseline model. In our
    plot below, we can see that if we sorted our Titanic passengers by the survival
    probability and took the first 20% of them, our lift would be about 2.2 times
    (the gain divided by sample percent) better than randomly choosing survivors (see
    [Figure 12-7](#idlc1)). (We would get 2.2 times as many survivors.)
  prefs: []
  type: TYPE_NORMAL
- en: 'The scikit-plot library can create a lift curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![lift curve.](assets/mlpr_1207.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-7\. Lift curve.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Class Balance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Yellowbrick has a simple bar plot to view the class sizes. When the relative
    class sizes are different, accuracy is not a good evaluation metric (see [Figure 12-8](#id34)).
    When splitting up the data into training and test sets, use *stratified sampling*
    so the sets keep a relative proportion of the classes. (The `test_train_split`
    function does this when you set the `stratify` parameter to the labels.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![A slight class imbalance.](assets/mlpr_1208.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-8\. A slight class imbalance.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Class Prediction Error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The class prediction error plot from Yellowbrick is a bar chart that visualizes
    a confusion matrix (see [Figure 12-9](#id35)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![Class prediction error. At the top of the left bar are people who died, but
    we predicted that they survived (false positive). At the bottom of the right bar
    are people who survived, but the model predicted death (false negative).](assets/mlpr_1209.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-9\. Class prediction error. At the top of the left bar are people
    who died, but we predicted that they survived (false positive). At the bottom
    of the right bar are people who survived, but the model predicted death (false
    negative).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Discrimination Threshold
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most binary classifiers that predict probability have a *discrimination threshold*
    of 50%. If the predicted probability is above 50%, the classifier assigns a positive
    label. [Figure 12-10](#id36) moves that threshold value between 0 and 100 and
    shows the impact to precision, recall, f1, and queue rate.
  prefs: []
  type: TYPE_NORMAL
- en: This plot can be useful to view the trade-off between precision and recall.
    Assume we are looking for fraud (and considering fraud to be the positive classification).
    To get high recall (catch all of the fraud), we can just classify everything as
    fraud. But in a bank situation, this would not be profitable and would require
    an army of workers. To get high precision (only catch fraud if it is fraud), we
    could have a model that only triggers on cases of extreme fraud. But this would
    miss much of the fraud that might not be as obvious. There is a trade-off here.
  prefs: []
  type: TYPE_NORMAL
- en: The *queue rate* is the percent of predictions above the threshold. You can
    consider this to be the percent of cases to review if you are dealing with fraud.
  prefs: []
  type: TYPE_NORMAL
- en: If you have the cost for positive, negative, and erroneous calculations, you
    can determine what threshold you are comfortable with.
  prefs: []
  type: TYPE_NORMAL
- en: The following plot is useful to see what discrimination threshold will maximize
    the f1 score or adjust precision or recall to an acceptable number when coupled
    with the queue rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yellowbrick provides this visualizer. This visualizer shuffles the data and
    runs 50 trials by default, splitting out 10% for validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![Discrimination threshold.](assets/mlpr_1210.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-10\. Discrimination threshold.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
