- en: Chapter 12\. Metrics and Classification Evaluation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 12 章。指标和分类评估
- en: 'We’ll cover the following metrics and evaluation tools in this chapter: confusion
    matrices, various metrics, a classification report, and some visualizations.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中我们将涵盖以下指标和评估工具：混淆矩阵、各种指标、分类报告和一些可视化。
- en: This will be evaluated as a decision tree model that predicts Titanic survival.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这将作为一个预测泰坦尼克号生存的决策树模型进行评估。
- en: Confusion Matrix
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: A confusion matrix can aid in understanding how a classifier performs.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵有助于理解分类器的表现。
- en: 'A binary classifier can have four classification results: true positives (TP),
    true negatives (TN), false positives (FP), and false negatives (FN). The first
    two are correct classifications.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 二元分类器可以有四种分类结果：真正例（TP）、真反例（TN）、假正例（FP）和假反例（FN）。前两者是正确分类。
- en: Here is a common example for remembering the other results. Assuming positive
    means pregnant and negative is not pregnant, a false positive is like claiming
    a man is pregnant. A false negative is claiming that a pregnant woman is not (when
    she is clearly showing) (see [Figure 12-1](#iderr3)). These last two types of
    errors are referred to as *type 1* and *type 2* errors, respectively (see [Table 12-1](#table_12_1)).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个常见的例子，用于记忆其他结果。假设正表示怀孕，负表示未怀孕，假阳性就像声称一个男性怀孕。假阴性是声称一个怀孕的女性不是（当她显然有显示）（见[图
    12-1](#iderr3)）。这些错误称为 *类型 1* 和 *类型 2* 错误，分别参见[表 12-1](#table_12_1)。
- en: Another way to remember these is that P (for false positive) has one straight
    line in it (type 1 error), and N (for false negative) has two vertical lines in
    it.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 记住这些的另一种方法是，P（表示假阳性）中有一条直线（类型 1 错误），而 N（表示假阴性）中有两条竖线。
- en: '![Classification errors.](assets/mlpr_1201.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![分类错误。](assets/mlpr_1201.png)'
- en: Figure 12-1\. Classification errors.
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-1\. 分类错误。
- en: Table 12-1\. Binary classification results from a confusion matrix
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12-1\. 从混淆矩阵中得出的二元分类结果
- en: '| Actual | Predicted negative | Predicted positive |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| 实际 | 预测为负 | 预测为正 |'
- en: '| --- | --- | --- |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Actual negative | True negative | False positive (type 1) |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 实际负样本 | 真阴性 | 假阳性（类型 1） |'
- en: '| Actual positive | False negative (type 2) | True positive |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 实际正样本 | 假阴性（类型 2） | 真正例 |'
- en: 'Here is the pandas code to calculate the classification results. The comments
    show the results. We will use these variables to calculate other metrics:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是计算分类结果的 pandas 代码。注释显示了结果。我们将使用这些变量来计算其他指标：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Well-behaving classifiers ideally have high counts in the true diagonal. We
    can create a DataFrame using the sklearn `confusion_matrix` function:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 良好表现的分类器理想情况下在真实对角线上有高计数。我们可以使用 sklearn 的 `confusion_matrix` 函数创建一个 DataFrame：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Yellowbrick has a plot for the confusion matrix (see [Figure 12-2](#id30)):'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Yellowbrick 有一个混淆矩阵的绘图（见[图 12-2](#id30)）：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Confusion matrix. The upper left and lower right are correct classifications.
    The lower left is false negative. The upper right is false positive.](assets/mlpr_1202.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![混淆矩阵。左上角和右下角是正确分类。左下角是假阴性。右上角是假阳性。](assets/mlpr_1202.png)'
- en: Figure 12-2\. Confusion matrix. The upper left and lower right are correct classifications.
    The lower left is false negative. The upper right is false positive.
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-2\. 混淆矩阵。左上角和右下角是正确分类。左下角是假阴性。右上角是假阳性。
- en: Metrics
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指标
- en: 'The `sklearn.metrics` module implements many common classification metrics,
    including:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn.metrics` 模块实现了许多常见的分类度量，包括：'
- en: '`''accuracy''`'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`''accuracy''`'
- en: Percent of correct predictions
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 正确预测的百分比
- en: '`''average_precision''`'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`''average_precision''`'
- en: Precision recall curve summary
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 精确率-召回率曲线总结
- en: '`''f1''`'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`''f1''`'
- en: Harmonic mean of precision and recall
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 精度和召回率的调和平均数
- en: '`''neg_log_loss''`'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`''neg_log_loss''`'
- en: Logistic or cross-entropy loss (model must support `predict_proba`)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑或交叉熵损失（模型必须支持 `predict_proba`）
- en: '`''precision''`'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`''precision''`'
- en: Ability to find only relevant samples (not label a negative as a positive)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 能够仅找到相关样本（不将负样本误标为正样本）
- en: '`''recall''`'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`''recall''`'
- en: Ability to find all positive samples
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 能够找到所有正样本
- en: '`''roc_auc''`'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`''roc_auc''`'
- en: Area under the receiver operator characteristic curve
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ROC 曲线下的面积
- en: These strings can be used as the `scoring` parameter when doing grid search,
    or you can use functions from the `sklearn.metrics` module that have the same
    names as the strings but end in `_score`. See the following note for examples.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这些字符串可以作为网格搜索中的 `scoring` 参数使用，或者你可以使用 `sklearn.metrics` 模块中具有相同名称但以 `_score`
    结尾的函数。详见下面的示例。
- en: Note
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '`''f1''`, `''precision''`, and `''recall''` all support the following suffixes
    for multiclass classifers:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`''f1''`, `''precision''` 和 `''recall''` 都支持多类分类器的以下后缀：'
- en: '`''_micro''`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`''_micro''`'
- en: Global weighted average of metric
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 全局加权平均指标
- en: '`''_macro''`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`''_macro''`'
- en: Unweighted average of metric
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 指标的未加权平均
- en: '`''_weighted''`'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`''_weighted''`'
- en: Multiclass weighted average of metric
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 多类加权平均指标
- en: '`''_samples''`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`''_samples''`'
- en: Per sample metric
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 每个样本的指标
- en: Accuracy
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准确率
- en: 'Accuracy is the percentage of correct classifications:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率是正确分类的百分比：
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: What is good accuracy? It depends. If I’m predicting fraud (which usually is
    a rare event, say 1 in 10,000), I can get very high accuracy by always predicting
    not fraud. But this model is not very useful. Looking at other metrics and the
    cost of predicting a false positive and a false negative can help us determine
    if a model is decent.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是良好的准确率？这取决于情况。如果我在预测欺诈（通常是罕见事件，比如一万分之一），我可以通过始终预测不是欺诈来获得非常高的准确率。但这种模型并不是很有用。查看其他指标以及预测假阳性和假阴性的成本可以帮助我们确定模型是否合适。
- en: 'We can use sklearn to calculate it for us:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用sklearn来计算它：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Recall
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 召回率
- en: Recall (also called *sensitivity*) is the percentage of positive values correctly
    classified. (How many relevant results are returned?)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率（也称为*灵敏度*）是正确分类的正值的百分比。（返回多少相关的结果？）
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Precision
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精度
- en: Precision is the percent of positive predictions that were correct (TP divided
    by (TP + FP)). (How relevant are the results?)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 精度是正确预测的正预测的百分比（TP除以（TP + FP））。（结果有多相关？）
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: F1
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: F1
- en: 'F1 is the harmonic mean of recall and precision:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: F1是召回率和精度的调和平均值：
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Classification Report
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类报告
- en: 'Yellowbrick has a classification report showing precision, recall, and f1 scores
    for both positive and negative values (see [Figure 12-3](#id31)). This is colored,
    and the redder the cell (closer to one), the better the score:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Yellowbrick有一个分类报告，显示正负值的精度、召回率和F1分数（见[图12-3](#id31)）。颜色标记，红色越深（接近1），得分越好：
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Classification report.](assets/mlpr_1203.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![分类报告。](assets/mlpr_1203.png)'
- en: Figure 12-3\. Classification report.
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-3\. 分类报告。
- en: ROC
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ROC
- en: A ROC curve illustrates how the classifier performs by tracking the true positive
    rate (recall/sensitivity) as the false positive rate (inverted specificity) changes
    (see [Figure 12-4](#id32)).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线说明分类器在真正例率（召回率/灵敏度）随假正例率（倒置特异性）变化时的表现（见[图12-4](#id32)）。
- en: 'A rule of thumb is that the plot should bulge out toward the top-left corner.
    A plot that is to the left and above another plot indicates better performance.
    The diagonal in this plot indicates the behavior of a random guessing classifier.
    By taking the AUC, you get a metric for evaluating the performance:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一个经验法则是图形应该朝向左上角凸出。一个位于另一个图形左侧且上方的图形表示性能更好。这个图中的对角线表示随机猜测分类器的行为。通过计算AUC，您可以得到一个评估性能的度量：
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Yellowbrick can plot this for us:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Yellowbrick可以为我们绘制这个图：
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![ROC curve.](assets/mlpr_1204.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![ROC曲线。](assets/mlpr_1204.png)'
- en: Figure 12-4\. ROC curve.
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-4\. ROC曲线。
- en: Precision-Recall Curve
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精度-召回曲线
- en: The ROC curve may be overly optimistic for imbalanced classes. Another option
    for evaluating classifiers is using a precision-recall curve (see [Figure 12-5](#id33)).
    Classification is a balancing act of finding everything you need (recall) while
    limiting the junk results (precision). This is typically a trade-off. As recall
    goes up, precision usually goes down and vice versa.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线对于不平衡类可能过于乐观。评估分类器的另一种选择是使用精度-召回曲线（见[图12-5](#id33)）。分类是在找到所有需要的内容（召回率）和限制垃圾结果（精度）之间进行权衡。这通常是一个权衡。随着召回率的提高，精度通常会下降，反之亦然。
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here is a Yellowbrick precision-recall curve:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个Yellowbrick精度-召回曲线：
- en: '[PRE12]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![Precision-recall curve.](assets/mlpr_1205.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![精度-召回曲线。](assets/mlpr_1205.png)'
- en: Figure 12-5\. Precision-recall curve.
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-5\. 精度-召回曲线。
- en: Cumulative Gains Plot
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 累积增益图
- en: A cumulative gains plot can be used to evaluate a binary classifier. It models
    the true positive rate (sensitivity) against the support rate (fraction of positive
    predictions). The intuition behind this plot is to sort all classifications by
    predicted probability. Ideally there would be a clean cut that divides positive
    from negative samples. If the first 10% of the predictions has 30% of the positive
    samples, you would plot a point from (0,0) to (.1, .3). You continue this process
    through all of the samples (see [Figure 12-6](#idcgc1)).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 累积增益图可用于评估二元分类器。它将真正率（灵敏度）模型化为正预测的分数率。该图背后的直觉是按预测概率对所有分类进行排序。理想情况下，应有一个清晰的分界线，将正样本与负样本分开。如果前10%的预测具有30%的正样本，则应绘制从（0,0）到（.1，.3）的点。继续这个过程直至所有样本（见图12-6）。
- en: A common use for this is determining customer response. The cumulative gains
    curve plots the support or predicted positive rate along the x-axis. Our chart
    labels this as “Percentage of sample”. It plots the sensitivity or true positive
    rate along the y-axis. This is labeled as “Gain” in our plot.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常用于确定客户反应。累积增益曲线沿 x 轴绘制支持或预测的正率。我们的图表将其标记为“样本百分比”。它沿 y 轴绘制灵敏度或真正率。在我们的图中标记为“增益”。
- en: If you wanted to contact 90% of the customers that would respond (sensitivity),
    you can trace from .9 on the y-axis to the right until you hit that curve. The
    x-axis at that point will indicate how many total customers you need to contact
    (support) to get to 90%.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想联系90%会响应的客户（灵敏度），您可以从y轴上的0.9追溯到右侧，直到碰到该曲线。此时的x轴指示您需要联系多少总客户（支持），以达到90%。
- en: In this case we aren’t contacting customers that would respond to a survey but
    predicting survival on the Titanic. If we ordered all passengers on the Titanic
    according to our model by how likely they are to survive, if you took the first
    65% of them, you would have 90% of the survivors. If you have an associated cost
    per contact and revenue per response, you can calculate what the best number is.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们不联系会对调查做出反应的客户，而是预测泰坦尼克号上的生存。如果按照我们的模型将泰坦尼克号的所有乘客排序，根据其生存可能性，如果你拿前65%的乘客，你将得到90%的幸存者。如果有每次联系的相关成本和每次响应的收入，您可以计算出最佳数量是多少。
- en: In general, a model that is to the left and above another model is a better
    model. The best models are lines that go up to the top (if 10% of the samples
    are positive, it would hit at (.1, 1)) and then directly to the right. If the
    plot is below the baseline, we would do better to randomly assign labels that
    use our model.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，处于左上方的模型比另一个模型更好。最佳模型是上升到顶部的线（如果样本的10%为正，它将在（.1, 1）处达到）。然后直接到右侧。如果图表在基线以下，我们最好随机分配标签以使用我们的模型。
- en: 'The [scikit-plot library](https://oreil.ly/dg0iQ) can create a cumulative gains
    plot:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[scikit-plot 库](https://oreil.ly/dg0iQ)可以创建一个累积增益图：'
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![Cumulative gains plot. If we ordered people on the Titanic according to our
    model, looking at 20% of them we would get 40% of the survivors.](assets/mlpr_1206.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![累积增益图。如果我们根据我们的模型对泰坦尼克号上的人进行排序，查看其中的20%，我们将获得40%的幸存者。](assets/mlpr_1206.png)'
- en: Figure 12-6\. Cumulative gains plot. If we ordered people on the Titanic according
    to our model, looking at 20% of them we would get 40% of the survivors.
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-6\. 累积增益图。如果我们根据我们的模型对泰坦尼克号上的人进行排序，查看其中的20%，我们将获得40%的幸存者。
- en: Lift Curve
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 抬升曲线
- en: A lift curve is another way of looking at the information in a cumulative gains
    plot. The *lift* is how much better we are doing than the baseline model. In our
    plot below, we can see that if we sorted our Titanic passengers by the survival
    probability and took the first 20% of them, our lift would be about 2.2 times
    (the gain divided by sample percent) better than randomly choosing survivors (see
    [Figure 12-7](#idlc1)). (We would get 2.2 times as many survivors.)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 抬升曲线是查看累积增益图中信息的另一种方式。抬升是我们比基线模型做得更好的程度。在我们的图中，我们可以看到，如果按照生存概率对泰坦尼克号乘客进行排序并取前20%的人，我们的提升将约为基线模型的2.2倍（增益除以样本百分比）好（见图12-7）。
    （我们将获得2.2倍的幸存者。）
- en: 'The scikit-plot library can create a lift curve:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-plot 库可以创建一个抬升曲线：
- en: '[PRE14]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![lift curve.](assets/mlpr_1207.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![抬升曲线。](assets/mlpr_1207.png)'
- en: Figure 12-7\. Lift curve.
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-7\. 抬升曲线。
- en: Class Balance
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 类别平衡
- en: Yellowbrick has a simple bar plot to view the class sizes. When the relative
    class sizes are different, accuracy is not a good evaluation metric (see [Figure 12-8](#id34)).
    When splitting up the data into training and test sets, use *stratified sampling*
    so the sets keep a relative proportion of the classes. (The `test_train_split`
    function does this when you set the `stratify` parameter to the labels.)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Yellowbrick 提供了一个简单的柱状图，用于查看类别大小。当相对类别大小不同时，准确率不是一个良好的评估指标（见[图 12-8](#id34)）。在将数据分成训练集和测试集时，请使用*分层抽样*，以保持类别的相对比例（当你将
    `stratify` 参数设置为标签时，`test_train_split` 函数会执行此操作）。
- en: '[PRE15]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![A slight class imbalance.](assets/mlpr_1208.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![轻微的类别不平衡。](assets/mlpr_1208.png)'
- en: Figure 12-8\. A slight class imbalance.
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-8\. 轻微的类别不平衡。
- en: Class Prediction Error
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 类预测错误
- en: 'The class prediction error plot from Yellowbrick is a bar chart that visualizes
    a confusion matrix (see [Figure 12-9](#id35)):'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Yellowbrick 的类预测错误图是一个柱状图，用于可视化混淆矩阵（参见[图 12-9](#id35)）：
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![Class prediction error. At the top of the left bar are people who died, but
    we predicted that they survived (false positive). At the bottom of the right bar
    are people who survived, but the model predicted death (false negative).](assets/mlpr_1209.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![类预测错误。在左侧条的顶部是死亡者，但我们预测他们幸存（假阳性）。在右侧条的底部是幸存者，但模型预测为死亡（假阴性）。](assets/mlpr_1209.png)'
- en: Figure 12-9\. Class prediction error. At the top of the left bar are people
    who died, but we predicted that they survived (false positive). At the bottom
    of the right bar are people who survived, but the model predicted death (false
    negative).
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-9\. 类预测错误。在左侧条的顶部是死亡者，但我们预测他们幸存（假阳性）。在右侧条的底部是幸存者，但模型预测为死亡（假阴性）。
- en: Discrimination Threshold
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 判别阈值
- en: Most binary classifiers that predict probability have a *discrimination threshold*
    of 50%. If the predicted probability is above 50%, the classifier assigns a positive
    label. [Figure 12-10](#id36) moves that threshold value between 0 and 100 and
    shows the impact to precision, recall, f1, and queue rate.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数预测概率的二元分类器具有50%的*判别阈值*。如果预测概率高于50%，分类器会分配正标签。[图 12-10](#id36) 在0到100之间移动该阈值，并显示对精确度、召回率、f1
    和队列率的影响。
- en: This plot can be useful to view the trade-off between precision and recall.
    Assume we are looking for fraud (and considering fraud to be the positive classification).
    To get high recall (catch all of the fraud), we can just classify everything as
    fraud. But in a bank situation, this would not be profitable and would require
    an army of workers. To get high precision (only catch fraud if it is fraud), we
    could have a model that only triggers on cases of extreme fraud. But this would
    miss much of the fraud that might not be as obvious. There is a trade-off here.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图表对于查看精确度和召回率之间的权衡是有用的。假设我们正在寻找欺诈（并将欺诈视为正分类）。为了获得高召回率（捕捉到所有的欺诈），我们可以将所有东西都分类为欺诈。但在银行情境下，这不会盈利，而且需要大量的工作人员。为了获得高精确度（只有在确实是欺诈时才捕捉到欺诈），我们可以有一个只对极端欺诈案例触发的模型。但这会错过许多不那么明显的欺诈行为。这里存在一种权衡。
- en: The *queue rate* is the percent of predictions above the threshold. You can
    consider this to be the percent of cases to review if you are dealing with fraud.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*队列率*是高于阈值的预测百分比。如果您正在处理欺诈案件，可以将其视为需要审查的案例百分比。'
- en: If you have the cost for positive, negative, and erroneous calculations, you
    can determine what threshold you are comfortable with.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有正、负和错误计算的成本，您可以确定您可以接受的阈值。
- en: The following plot is useful to see what discrimination threshold will maximize
    the f1 score or adjust precision or recall to an acceptable number when coupled
    with the queue rate.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表有助于查看在与队列率结合时，哪个判别阈值能够最大化 f1 得分或调整精确度或召回率至可接受水平。
- en: 'Yellowbrick provides this visualizer. This visualizer shuffles the data and
    runs 50 trials by default, splitting out 10% for validation:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Yellowbrick 提供了这个可视化工具。默认情况下，这个可视化工具对数据进行洗牌，并运行50次试验，其中分离出10%作为验证集：
- en: '[PRE17]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Discrimination threshold.](assets/mlpr_1210.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![判别阈值。](assets/mlpr_1210.png)'
- en: Figure 12-10\. Discrimination threshold.
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-10\. 判别阈值。
