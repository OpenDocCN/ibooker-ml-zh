<html><head></head><body><section data-pdf-bookmark="Chapter 3. Classification Walkthrough: Titanic Dataset" data-type="chapter" epub:type="chapter"><div class="chapter" id="idm46066913332968">&#13;
<h1><span class="label">Chapter 3. </span>Classification Walkthrough: <span class="keep-together">Titanic Dataset</span></h1>&#13;
&#13;
&#13;
<p><a data-primary="classification" data-secondary="walkthrough with Titanic dataset" data-type="indexterm" id="ix_ch03-asciidoc0"/><a data-primary="Titanic dataset, classification walkthrough with" data-type="indexterm" id="ix_ch03-asciidoc1"/>This chapter will walk through a common classification problem using the&#13;
<a href="https://oreil.ly/PjceO">Titanic dataset</a>. Later chapters will dive into and expand on the common&#13;
steps performed during an analysis.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Project Layout Suggestion" data-type="sect1"><div class="sect1" id="idm46066913327336">&#13;
<h1>Project Layout Suggestion</h1>&#13;
&#13;
<p><a data-primary="classification" data-secondary="project layout suggestion" data-type="indexterm" id="idm46066913326168"/><a data-primary="Jupyter" data-secondary="for exploratory data analysis" data-type="indexterm" id="idm46066913325128"/>An excellent tool for performing exploratory data analysis is <a href="https://jupyter.org">Jupyter</a>.&#13;
Jupyter is an open-source notebook environment that supports Python and other languages. It&#13;
allows you to create <em>cells</em> of code or Markdown content.</p>&#13;
&#13;
<p>I tend to use Jupyter in two modes. One is for exploratory data analysis and quickly trying things out.&#13;
The other is more of a deliverable style where I format a report using Markdown cells and insert code&#13;
cells to illustrate important points or discoveries. If you aren’t careful, your notebooks might need some refactoring and&#13;
application of software engineering practices (remove globals, use&#13;
functions and classes, etc.).</p>&#13;
&#13;
<p><a data-primary="cookiecutter" data-type="indexterm" id="idm46066913321512"/>The <a href="https://oreil.ly/86jL3">cookiecutter data science package</a> suggests&#13;
a layout to create an analysis that allows&#13;
for easy reproduction and sharing code.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Imports" data-type="sect1"><div class="sect1" id="idm46066913319576">&#13;
<h1>Imports</h1>&#13;
&#13;
<p><a data-primary="classification" data-secondary="imports" data-type="indexterm" id="idm46066913318408"/><a data-primary="pandas" data-secondary="imports with" data-type="indexterm" id="idm46066913317208"/>This example is based mostly on <a href="http://pandas.pydata.org/">pandas</a>, <a href="https://scikit-learn.org/">scikit-learn</a>, and <a href="http://www.scikit-yb.org/">Yellowbrick</a>. The&#13;
pandas library gives us tooling for easy data munging. <a data-primary="scikit-learn" data-secondary="imports with" data-type="indexterm" id="idm46066913313944"/>The scikit-learn&#13;
library has great predictive modeling, and <a data-primary="Yellowbrick" data-secondary="imports with" data-type="indexterm" id="idm46066913312760"/>Yellowbrick is a visualization&#13;
library for evaluating models:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="kn">as</code> <code class="nn">plt</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">pandas</code> <code class="kn">as</code> <code class="nn">pd</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">ensemble</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">preprocessing</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">tree</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">auc</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">confusion_matrix</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">roc_auc_score</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">roc_curve</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">train_test_split</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">StratifiedKFold</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">yellowbrick.classifier</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">ConfusionMatrix</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">ROCAUC</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">yellowbrick.model_selection</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">LearningCurve</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code></pre>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p><a data-primary="star imports, avoiding" data-type="indexterm" id="idm46066911917176"/>You might find documentation and examples online that include star imports like:</p>&#13;
&#13;
<pre data-type="programlisting">from pandas import *</pre>&#13;
&#13;
<p>Refrain from using star imports. Being explicit makes your code easier&#13;
to understand.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Ask a Question" data-type="sect1"><div class="sect1" id="idm46066905740328">&#13;
<h1>Ask a Question</h1>&#13;
&#13;
<p><a data-primary="classification" data-secondary="asking a question to create predictive model for" data-type="indexterm" id="idm46066905739320"/>In this example, we want to create a predictive model to answer a question.&#13;
It will classify&#13;
whether an individual survives the Titanic ship catastrophe based on&#13;
individual and trip characteristics. This is a toy example, but it&#13;
serves as a pedagogical tool for showing many steps of modeling.&#13;
Our model should be able to take passenger information and&#13;
predict whether that passenger would survive on the Titanic.</p>&#13;
&#13;
<p>This is a classification question, as we are predicting a label for survival;&#13;
either they survived or they died.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Terms for Data" data-type="sect1"><div class="sect1" id="idm46066905737144">&#13;
<h1>Terms for Data</h1>&#13;
&#13;
<p><a data-primary="classification" data-secondary="terms for data" data-type="indexterm" id="idm46066905735896"/><a data-primary="data" data-secondary="terms for" data-type="indexterm" id="idm46066905735048"/>We typically train a model with a matrix of data. (I prefer to use pandas DataFrames because it is very nice to have column labels, but numpy arrays work as well.)</p>&#13;
&#13;
<p>For supervised learning, such as regression or classification, our intent is to have a fuction that transforms features into a label. If we were to write this as an algebra formula, it would look like this:</p>&#13;
<pre>y = f(X)</pre>&#13;
&#13;
<p>X is a matrix. <a data-primary="sample (term)" data-type="indexterm" id="idm46066905732296"/>Each row represents a <em>sample</em> of data or information about an individual. <a data-primary="feature" data-secondary="column as" data-type="indexterm" id="idm46066905731176"/>Every column in X is a <em>feature</em>. The output of our function, y, is a vector that contains labels (for classification) or values (for regression) (see <a data-type="xref" href="#idx1">Figure 3-1</a>).</p>&#13;
&#13;
<figure><div class="figure" id="idx1">&#13;
<img alt="Structured data layout." src="assets/mlpr_0301.png"/>&#13;
<h6><span class="label">Figure 3-1. </span>Structured data layout.</h6>&#13;
</div></figure>&#13;
&#13;
<p>This is standard naming procedure for naming the data and the output. If you read academic papers or even look at the documentation for libraries, they follow this convention.&#13;
In Python, we use the variable name <code>X</code> to hold the sample data even though capitalization of variables is a violation of standard naming conventions (PEP 8). Don’t worry, everyone does it, and if you were to name your variable <code>x</code>, they might look at you funny. The variable <code>y</code> stores the labels or targets.</p>&#13;
&#13;
<p><a data-type="xref" href="#table_3_1">Table 3-1</a> shows a basic dataset with two samples and three features for each sample.</p>&#13;
<table id="table_3_1">&#13;
<caption class="width-full"><span class="label">Table 3-1. </span>Samples (rows) and features (columns)</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>pclass</th>&#13;
<th>age</th>&#13;
<th>sibsp</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>1</p></td>&#13;
<td><p>29</p></td>&#13;
<td><p>0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>1</p></td>&#13;
<td><p>2</p></td>&#13;
<td><p>1</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Gather Data" data-type="sect1"><div class="sect1" id="idm46066905715352">&#13;
<h1>Gather Data</h1>&#13;
&#13;
<p><a data-primary="classification" data-secondary="gathering data" data-type="indexterm" id="idm46066905713976"/><a data-primary="data" data-secondary="gathering" data-type="indexterm" id="idm46066905713000"/>We are going to load an Excel file (make sure you have pandas and xlrd<sup><a data-type="noteref" href="ch03.html#idm46066905711928" id="idm46066905711928-marker">1</a></sup>&#13;
installed) with the Titanic features. It has many columns, including a survived&#13;
column that contains the label of what happened to an individual:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">url</code> <code class="o">=</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="s">"http://biostat.mc.vanderbilt.edu/"</code>&#13;
<code class="gp">... </code>    <code class="s">"wiki/pub/Main/DataSets/titanic3.xls"</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_excel</code><code class="p">(</code><code class="n">url</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">orig_df</code> <code class="o">=</code> <code class="n">df</code></pre>&#13;
&#13;
<p>The following columns are included in the dataset:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>pclass -  Passenger class (1 = 1st, 2 = 2nd, 3 = 3rd)</p>&#13;
</li>&#13;
<li>&#13;
<p>survival - Survival (0 = No, 1 = Yes)</p>&#13;
</li>&#13;
<li>&#13;
<p>name - Name</p>&#13;
</li>&#13;
<li>&#13;
<p>sex - Sex</p>&#13;
</li>&#13;
<li>&#13;
<p>age - Age</p>&#13;
</li>&#13;
<li>&#13;
<p>sibsp - Number of siblings/spouses aboard</p>&#13;
</li>&#13;
<li>&#13;
<p>parch - Number of parents/children aboard</p>&#13;
</li>&#13;
<li>&#13;
<p>ticket - Ticket number</p>&#13;
</li>&#13;
<li>&#13;
<p>fare - Passenger fare</p>&#13;
</li>&#13;
<li>&#13;
<p>cabin - Cabin</p>&#13;
</li>&#13;
<li>&#13;
<p>embarked - Point of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)</p>&#13;
</li>&#13;
<li>&#13;
<p>boat - Lifeboat</p>&#13;
</li>&#13;
<li>&#13;
<p>body - Body identification number</p>&#13;
</li>&#13;
<li>&#13;
<p>home.dest - Home/destination</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Pandas can read this spreadsheet and convert it into a DataFrame for us.&#13;
We will need to spot-check the data and ensure that it is OK for&#13;
performing analysis.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Clean Data" data-type="sect1"><div class="sect1" id="idm46066905678376">&#13;
<h1>Clean Data</h1>&#13;
&#13;
<p><a data-primary="classification" data-secondary="cleaning data" data-type="indexterm" id="ix_ch03-asciidoc2"/><a data-primary="cleaning data" data-type="indexterm" id="ix_ch03-asciidoc3"/><a data-primary="data" data-secondary="cleaning" data-type="indexterm" id="ix_ch03-asciidoc4"/>Once we have the data, we need to ensure that it is in a format that we can use to create a model. <a data-primary="scikit-learn" data-secondary="numeric features with" data-type="indexterm" id="idm46066905672584"/>Most scikit-learn models require that our features be numeric (integer or float). In addition, many models fail if they are passed missing values (<code>NaN</code> in pandas or numpy). <a data-primary="preprocessing data" data-secondary="standardizing" data-type="indexterm" id="idm46066905670968"/><a data-primary="standardizing data" data-type="indexterm" id="idm46066905669960"/>Some models perform better if the data is <em>standardized</em> (given a mean value of 0 and a standard deviation of 1). We will deal with these issues using pandas or scikit-learn. In addition, the Titanic dataset has <em>leaky</em> features.</p>&#13;
&#13;
<p><a data-primary="leaky features" data-secondary="defined" data-type="indexterm" id="idm46066905667992"/>Leaky features are variables that contain information about the future or target. There’s nothing bad in having data about the target, and we often have that data during model creation time. However, if those variables are not available when we perform a prediction on a new sample, we should remove them from the model as they are leaking data from the future.</p>&#13;
&#13;
<p>Cleaning the data can take a bit of time. <a data-primary="subject matter expert (SME)" data-secondary="and cleaning data" data-type="indexterm" id="idm46066905666056"/>It helps to have access&#13;
to a subject matter expert (SME) who can provide guidance on&#13;
dealing with outliers or missing data.</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">df</code><code class="o">.</code><code class="n">dtypes</code>&#13;
<code class="go">pclass         int64</code>&#13;
<code class="go">survived       int64</code>&#13;
<code class="go">name          object</code>&#13;
<code class="go">sex           object</code>&#13;
<code class="go">age          float64</code>&#13;
<code class="go">sibsp          int64</code>&#13;
<code class="go">parch          int64</code>&#13;
<code class="go">ticket        object</code>&#13;
<code class="go">fare         float64</code>&#13;
<code class="go">cabin         object</code>&#13;
<code class="go">embarked      object</code>&#13;
<code class="go">boat          object</code>&#13;
<code class="go">body         float64</code>&#13;
<code class="go">home.dest     object</code>&#13;
<code class="go">dtype: object</code></pre>&#13;
&#13;
<p><a data-primary="types, for storage of columns of data" data-type="indexterm" id="idm46066905619912"/>We typically see <code>int64</code>, <code>float64</code>, <code>datetime64[ns]</code>, or <code>object</code>.&#13;
These are the types that pandas uses to store a column of data.&#13;
<code>int64</code> and <code>float64</code> are numeric types. <code>datetime64[ns]</code> holds&#13;
date and time data. <code>object</code> typically means that it is holding&#13;
string data, though it could be a combination of string and other&#13;
types.</p>&#13;
&#13;
<p><a data-primary="CSV files" data-type="indexterm" id="idm46066905606200"/>When reading from CSV files, pandas will try to coerce data&#13;
into the appropriate type, but will fall back to <code>object</code>. Reading&#13;
data from spreadsheets, databases, or other systems may provide&#13;
better types in the DataFrame. In any case, it is worthwhile&#13;
to look through the data and ensure that the types make sense.</p>&#13;
&#13;
<p>Integer types are typically fine. Float types might have some&#13;
missing values. Date and string types will need to be converted&#13;
or used to feature engineer numeric types. String types that&#13;
have low cardinality are called categorical columns, and&#13;
it might be worthwhile to create dummy columns from them (the <code>pd.get_dummies</code> function&#13;
takes care of this).</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p><a data-primary="pandas" data-secondary="int64 vs. Int64 types" data-type="indexterm" id="idm46066905602616"/>Up to pandas 0.23, if the type is <code>int64</code>, we are guaranteed that there&#13;
are no missing values. If the type is <code>float64</code>, the values might be all floats, but also could be&#13;
integer-like numbers with missing values. The pandas library converts integer values that have missing numbers to floats, as this type supports missing values. The <code>object</code> typically means&#13;
string types (or both string and numeric).</p>&#13;
&#13;
<p>As of pandas 0.24, there is a new <code>Int64</code> type (notice the capitalization).&#13;
This is not the default integer type, but you can coerce to this type and have&#13;
support for missing <span class="keep-together">numbers.</span></p>&#13;
</div>&#13;
&#13;
<p><a data-primary="pandas" data-secondary="profile report with" data-type="indexterm" id="idm46066905598104"/>The pandas-profiling library includes a profile report. You can generate this report in a notebook. It will summarize the types of the columns and allow you to view details of quantile statistics, descriptive statistics, a histogram, common values, and extreme values (see Figures <a data-type="xref" data-xrefstyle="select:labelnumber" href="#pp1">3-2</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#pp2">3-3</a>):</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">pandas_profiling</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">pandas_profiling</code><code class="o">.</code><code class="n">ProfileReport</code><code class="p">(</code><code class="n">df</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="pp1">&#13;
<img alt="Pandas-profiling summary." src="assets/mlpr_0302.png"/>&#13;
<h6><span class="label">Figure 3-2. </span>Pandas-profiling summary.</h6>&#13;
</div></figure>&#13;
&#13;
<figure><div class="figure" id="pp2">&#13;
<img alt="Pandas-profiling variable details." src="assets/mlpr_0303.png"/>&#13;
<h6><span class="label">Figure 3-3. </span>Pandas-profiling variable details.</h6>&#13;
</div></figure>&#13;
&#13;
<p>Use the <code>.shape</code> attribute of the DataFrame to inspect the number of rows and columns:</p>&#13;
&#13;
<pre data-type="programlisting">&gt;&gt;&gt; df.shape&#13;
(1309, 14)</pre>&#13;
&#13;
<p>Use the <code>.describe</code> method to get summary stats as well as see&#13;
the count of nonnull data. The default behavior of this method&#13;
is to only report on numeric columns. Here the output is truncated&#13;
to only show the first two columns:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">df</code><code class="o">.</code><code class="n">describe</code><code class="p">()</code><code class="o">.</code><code class="n">iloc</code><code class="p">[:,</code> <code class="p">:</code><code class="mi">2</code><code class="p">]</code>&#13;
<code class="go">            pclass     survived</code>&#13;
<code class="go">count  1309.000000  1309.000000</code>&#13;
<code class="go">mean      2.294882     0.381971</code>&#13;
<code class="go">std       0.837836     0.486055</code>&#13;
<code class="go">min       1.000000     0.000000</code>&#13;
<code class="go">25%       2.000000     0.000000</code>&#13;
<code class="go">50%       3.000000     0.000000</code>&#13;
<code class="go">75%       3.000000     1.000000</code>&#13;
<code class="go">max       3.000000     1.000000</code></pre>&#13;
&#13;
<p>The count statistic only includes values that are not NaN, so it&#13;
is useful for checking whether a column is missing data. It is also&#13;
a good idea to spot-check the minimum and maximum values to see if&#13;
there are outliers. Summary statistics are one way to do this. Plotting&#13;
a histogram or a box plot is a visual representation that we will see later.</p>&#13;
&#13;
<p><a data-primary="missing data" data-secondary="cleaning data" data-type="indexterm" id="ix_ch03-asciidoc5"/>We will need to deal with missing data.&#13;
Use the <code>.isnull</code> method to find columns or rows with missing values.&#13;
Calling <code>.isnull</code> on a DataFrame returns a new DataFrame with every&#13;
cell containing a <code>True</code> or <code>False</code> value. In Python, these values&#13;
evaluate to <code>1</code> and <code>0</code>, respectively. This allows us to sum them&#13;
up or even calculate the percent missing (by calculating the mean).</p>&#13;
&#13;
<p>The code indicates the count of missing data in each column:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">df</code><code class="o">.</code><code class="n">isnull</code><code class="p">()</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code>&#13;
<code class="go">pclass          0</code>&#13;
<code class="go">survived        0</code>&#13;
<code class="go">name            0</code>&#13;
<code class="go">sex             0</code>&#13;
<code class="go">age           263</code>&#13;
<code class="go">sibsp           0</code>&#13;
<code class="go">parch           0</code>&#13;
<code class="go">ticket          0</code>&#13;
<code class="go">fare            1</code>&#13;
<code class="go">cabin        1014</code>&#13;
<code class="go">embarked        2</code>&#13;
<code class="go">boat          823</code>&#13;
<code class="go">body         1188</code>&#13;
<code class="go">home.dest     564</code>&#13;
<code class="go">dtype: int64</code></pre>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p><a data-primary="null values, percentage of" data-type="indexterm" id="idm46066905488296"/>Replace <code>.sum</code> with <code>.mean</code> to get the percentage of null values.&#13;
By default, calling these methods will apply the operation along axis 0, which is along the index. If you want to get the counts of missing features for each sample, you can apply this along axis 1 (along the columns):</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">df</code><code class="o">.</code><code class="n">isnull</code><code class="p">()</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">loc</code><code class="p">[:</code><code class="mi">10</code><code class="p">]</code>&#13;
<code class="go">0    1</code>&#13;
<code class="go">1    1</code>&#13;
<code class="go">2    2</code>&#13;
<code class="go">3    1</code>&#13;
<code class="go">4    2</code>&#13;
<code class="go">5    1</code>&#13;
<code class="go">6    1</code>&#13;
<code class="go">7    2</code>&#13;
<code class="go">8    1</code>&#13;
<code class="go">9    2</code>&#13;
<code class="go">dtype: int64</code></pre>&#13;
</div>&#13;
&#13;
<p><a data-primary="subject matter expert (SME)" data-secondary="and missing data" data-type="indexterm" id="idm46066905402824"/>A SME can help in determining what to do with missing data.&#13;
The age column might be useful, so keeping it and interpolating values&#13;
could provide some signal to the model. Columns where most of the values&#13;
are missing (cabin, boat, and body) tend to not provide value and can&#13;
be dropped.</p>&#13;
&#13;
<p>The body column&#13;
(body identification number) is missing for many rows. We should&#13;
drop this column at any rate because it leaks data. This column&#13;
indicates that the passenger did not survive; by necessity&#13;
our model could use that to cheat. We will pull it out. (If&#13;
we are creating a model to predict if a passenger would die, knowing&#13;
that they had a body identification number a priori would let us&#13;
know they were already dead. We want our model to not know that&#13;
information and make the prediction based on the other columns.)&#13;
Likewise, the boat column leaks the reverse information&#13;
(that a passenger survived).</p>&#13;
&#13;
<p>Let’s look at some of the rows with missing data. We can create a boolean array&#13;
(a series with <code>True</code> or <code>False</code> to indicate if the row has missing data)&#13;
and use it to inspect rows that are missing data:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">mask</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">isnull</code><code class="p">()</code><code class="o">.</code><code class="n">any</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">mask</code><code class="o">.</code><code class="n">head</code><code class="p">()</code>  <code class="c"># rows</code>&#13;
<code class="go">0    True</code>&#13;
<code class="go">1    True</code>&#13;
<code class="go">2    True</code>&#13;
<code class="go">3    True</code>&#13;
<code class="go">4    True</code>&#13;
<code class="go">dtype: bool</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">df</code><code class="p">[</code><code class="n">mask</code><code class="p">]</code><code class="o">.</code><code class="n">body</code><code class="o">.</code><code class="n">head</code><code class="p">()</code>&#13;
<code class="go">0      NaN</code>&#13;
<code class="go">1      NaN</code>&#13;
<code class="go">2      NaN</code>&#13;
<code class="go">3    135.0</code>&#13;
<code class="go">4      NaN</code>&#13;
<code class="go">Name: body, dtype: float64</code></pre>&#13;
&#13;
<p>We will impute (or derive values for) the missing values for the age column later.</p>&#13;
&#13;
<p>Columns with type of <code>object</code> tend to be categorical (but they may also be high cardinality string data, or a mix of column types). For <code>object</code> columns that we believe to be categorical, use the <code>.value_counts</code> method to examine the counts of the values:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">df</code><code class="o">.</code><code class="n">sex</code><code class="o">.</code><code class="n">value_counts</code><code class="p">(</code><code class="n">dropna</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code>&#13;
<code class="go">male      843</code>&#13;
<code class="go">female    466</code>&#13;
<code class="go">Name: sex, dtype: int64</code></pre>&#13;
&#13;
<p>Remember that pandas typically ignores null or NaN values. If you want to include those, use&#13;
<code>dropna=False</code> to also show counts for NaN:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">df</code><code class="o">.</code><code class="n">embarked</code><code class="o">.</code><code class="n">value_counts</code><code class="p">(</code><code class="n">dropna</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code>&#13;
<code class="go">S      914</code>&#13;
<code class="go">C      270</code>&#13;
<code class="go">Q      123</code>&#13;
<code class="go">NaN      2</code>&#13;
<code class="go">Name: embarked, dtype: int64</code></pre>&#13;
&#13;
<p>We have a couple of options for dealing with missing embarked values. Using S might seem logical as that is the most common value. We could dig into the data and try and determine if another option is better. We could also drop those two values. Or, because this is categorical, we can ignore them and use pandas to create dummy columns if these two samples will just have 0 entries for every option. We will use this latter choice for this feature<a data-startref="ix_ch03-asciidoc5" data-type="indexterm" id="idm46066905303592"/>.<a data-startref="ix_ch03-asciidoc4" data-type="indexterm" id="idm46066905302856"/><a data-startref="ix_ch03-asciidoc3" data-type="indexterm" id="idm46066905302216"/><a data-startref="ix_ch03-asciidoc2" data-type="indexterm" id="idm46066905301544"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Create Features" data-type="sect1"><div class="sect1" id="idm46066905677432">&#13;
<h1>Create Features</h1>&#13;
&#13;
<p><a data-primary="classification" data-secondary="feature creation" data-type="indexterm" id="ix_ch03-asciidoc6"/><a data-primary="feature" data-secondary="creating" data-type="indexterm" id="ix_ch03-asciidoc7"/>We can drop columns that have no variance or no signal. There aren’t features like that in this dataset, but if there was a column called “is human” that had 1 for every sample this column would not be providing any information.</p>&#13;
&#13;
<p>Alternatively, unless we are using NLP or extracting data out of text columns where every value is different, a model will not be able to take advantage of this column. The name column is an example of this. Some have pulled out the title t from the name and treated it as categorical.</p>&#13;
&#13;
<p><a data-primary="columns" data-secondary="dropping" data-type="indexterm" id="idm46066905233336"/><a data-primary="leaky features" data-secondary="dropping columns with" data-type="indexterm" id="idm46066905232360"/>We also want to drop columns that leak information. Both boat and body columns leak whether a passenger survived.</p>&#13;
&#13;
<p>The pandas <code>.drop</code>&#13;
method can drop either rows or columns:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">name</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">name</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">name</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">3</code><code class="p">)</code>&#13;
<code class="go">0      Allen, Miss. Elisabeth Walton</code>&#13;
<code class="go">1     Allison, Master. Hudson Trevor</code>&#13;
<code class="go">2       Allison, Miss. Helen Loraine</code>&#13;
<code class="go">Name: name, dtype: object</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">columns</code><code class="o">=</code><code class="p">[</code>&#13;
<code class="gp">... </code>        <code class="s">"name"</code><code class="p">,</code>&#13;
<code class="gp">... </code>        <code class="s">"ticket"</code><code class="p">,</code>&#13;
<code class="gp">... </code>        <code class="s">"home.dest"</code><code class="p">,</code>&#13;
<code class="gp">... </code>        <code class="s">"boat"</code><code class="p">,</code>&#13;
<code class="gp">... </code>        <code class="s">"body"</code><code class="p">,</code>&#13;
<code class="gp">... </code>        <code class="s">"cabin"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="p">]</code>&#13;
<code class="gp">... </code><code class="p">)</code></pre>&#13;
&#13;
<p>We need to create dummy columns from string columns. This will create&#13;
new columns for sex and embarked. Pandas has a convenient <code>get_dummies</code>&#13;
function for that:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">get_dummies</code><code class="p">(</code><code class="n">df</code><code class="p">)</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">df</code><code class="o">.</code><code class="n">columns</code>&#13;
<code class="go">Index(['pclass', 'survived', 'age', 'sibsp',</code>&#13;
<code class="go">    'parch', 'fare', 'sex_female', 'sex_male',</code>&#13;
<code class="go">    'embarked_C', 'embarked_Q', 'embarked_S'],</code>&#13;
<code class="go">    dtype='object')</code></pre>&#13;
&#13;
<p>At this point the sex_male and sex_female columns are perfectly inverse correlated. Typically we remove any columns with perfect or very high positive or negative correlation. Multicollinearity can impact interpretation of feature importance and coefficients in some models. Here is code to remove the sex_male column:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="n">columns</code><code class="o">=</code><code class="s">"sex_male"</code><code class="p">)</code></pre>&#13;
&#13;
<p>Alternatively, we can add a <code>drop_first=True</code> parameter to the <code>get_dummies</code> call:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">get_dummies</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="n">drop_first</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">df</code><code class="o">.</code><code class="n">columns</code>&#13;
<code class="go">Index(['pclass', 'survived', 'age', 'sibsp',</code>&#13;
<code class="go">    'parch', 'fare', 'sex_male',</code>&#13;
<code class="go">    'embarked_Q', 'embarked_S'],</code>&#13;
<code class="go">    dtype='object')</code></pre>&#13;
&#13;
<p>Create a DataFrame (<code>X</code>) with the features and a series (<code>y</code>) with the&#13;
labels. We could also use numpy arrays, but then we don’t have column&#13;
names:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">y</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">survived</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">X</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="n">columns</code><code class="o">=</code><code class="s">"survived"</code><code class="p">)</code></pre>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p><a data-primary="pyjanitor" data-type="indexterm" id="idm46066905019912"/>We can use the <a href="https://oreil.ly/_IWbA">pyjanitor library</a> to replace the last two lines:<a data-startref="ix_ch03-asciidoc7" data-type="indexterm" id="idm46066905018456"/><a data-startref="ix_ch03-asciidoc6" data-type="indexterm" id="idm46066905017752"/></p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">janitor</code> <code class="kn">as</code> <code class="nn">jn</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">jn</code><code class="o">.</code><code class="n">get_features_targets</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">df</code><code class="p">,</code> <code class="n">target_columns</code><code class="o">=</code><code class="s">"survived"</code>&#13;
<code class="gp">... </code><code class="p">)</code></pre>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Sample Data" data-type="sect1"><div class="sect1" id="idm46066905300744">&#13;
<h1>Sample Data</h1>&#13;
&#13;
<p><a data-primary="classification" data-secondary="sampling data" data-type="indexterm" id="idm46066904965000"/><a data-primary="data" data-secondary="sampling" data-type="indexterm" id="idm46066904964024"/><a data-primary="sampling data" data-type="indexterm" id="idm46066904963080"/>We always want to train and test on different data. Otherwise you don’t&#13;
really know how well your model generalizes to data that it hasn’t seen&#13;
before. We’ll use scikit-learn to pull out 30% for testing (using&#13;
<code>random_state=42</code> to remove an element of randomness if we start&#13;
comparing different models):</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">model_selection</code><code class="o">.</code><code class="n">train_test_split</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="mf">0.3</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code>&#13;
<code class="gp">... </code><code class="p">)</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Impute Data" data-type="sect1"><div class="sect1" id="idm46066904933112">&#13;
<h1>Impute Data</h1>&#13;
&#13;
<p><a data-primary="classification" data-secondary="imputing data" data-type="indexterm" id="idm46066904883736"/><a data-primary="data" data-secondary="imputing" data-type="indexterm" id="idm46066904882760"/><a data-primary="imputing data" data-type="indexterm" id="idm46066904881816"/>The age column has missing values. We need to impute age from the numeric values. We only want to impute on the training set&#13;
and then use that imputer to fill in the date for the test set. Otherwise we&#13;
are leaking data (cheating by giving future information to the model).</p>&#13;
&#13;
<p>Now that we have test and train data, we can impute missing values on the&#13;
training set, and use the trained imputers to fill in the test dataset.&#13;
<a data-primary="fancyimpute" data-type="indexterm" id="idm46066904879960"/>The <a href="https://oreil.ly/Vlf9e">fancyimpute library</a>&#13;
has many algorithms that it implements. Sadly, most of these algorithms are not&#13;
implemented in an <em>inductive</em> manner. This means that you cannot call <code>.fit</code>&#13;
and then <code>.transform</code>, which means you cannot impute for new data based&#13;
on how the model was trained.</p>&#13;
&#13;
<p>The <code>IterativeImputer</code> class (which was in fancyimpute but has been migrated to&#13;
scikit-learn) does support inductive mode. To use it we need to add a&#13;
special experimental import (as of scikit-learn version 0.21.2):</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.experimental</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">enable_iterative_imputer</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">impute</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">num_cols</code> <code class="o">=</code> <code class="p">[</code>&#13;
<code class="gp">... </code>    <code class="s">"pclass"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="s">"age"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="s">"sibsp"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="s">"parch"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="s">"fare"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="s">"sex_female"</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">]</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">imputer</code> <code class="o">=</code> <code class="n">impute</code><code class="o">.</code><code class="n">IterativeImputer</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">imputed</code> <code class="o">=</code> <code class="n">imputer</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">X_train</code><code class="p">[</code><code class="n">num_cols</code><code class="p">]</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">X_train</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code> <code class="n">num_cols</code><code class="p">]</code> <code class="o">=</code> <code class="n">imputed</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">imputed</code> <code class="o">=</code> <code class="n">imputer</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_test</code><code class="p">[</code><code class="n">num_cols</code><code class="p">])</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">X_test</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code> <code class="n">num_cols</code><code class="p">]</code> <code class="o">=</code> <code class="n">imputed</code></pre>&#13;
&#13;
<p>If we wanted to impute with the median, we can use pandas to do that:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">meds</code> <code class="o">=</code> <code class="n">X_train</code><code class="o">.</code><code class="n">median</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">X_train</code> <code class="o">=</code> <code class="n">X_train</code><code class="o">.</code><code class="n">fillna</code><code class="p">(</code><code class="n">meds</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">X_test</code> <code class="o">=</code> <code class="n">X_test</code><code class="o">.</code><code class="n">fillna</code><code class="p">(</code><code class="n">meds</code><code class="p">)</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Normalize Data" data-type="sect1"><div class="sect1" id="idm46066904669416">&#13;
<h1>Normalize Data</h1>&#13;
&#13;
<p><a data-primary="classification" data-secondary="normalizing data" data-type="indexterm" id="idm46066904737288"/><a data-primary="normalizing data" data-seealso="preprocessing data" data-type="indexterm" id="idm46066904736312"/><a data-primary="preprocessing data" data-type="indexterm" id="idm46066904735368"/>Normalizing or preprocessing the data will help many models perform better after this is done.&#13;
Particularly those that depend on a distance metric to determine similarity. (Note that tree models, which treat each feature on its own, don’t have this requirement.)</p>&#13;
&#13;
<p><a data-primary="preprocessing data" data-secondary="standardizing" data-type="indexterm" id="idm46066904723640"/><a data-primary="standardizing data" data-type="indexterm" id="idm46066904722776"/>We are going to standardize the data for the preprocessing. Standardizing is translating the data so that it has a mean value of zero and a standard deviation of one. This way models don’t treat variables with larger scales as more important than smaller scaled variables. I’m going to stick the result (numpy array) back into a pandas&#13;
DataFrame for easier manipulation (and to keep column names).</p>&#13;
&#13;
<p>I also normally don’t standardize dummy columns, so I will ignore those:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">cols</code> <code class="o">=</code> <code class="s">"pclass,age,sibsp,fare"</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s">","</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">sca</code> <code class="o">=</code> <code class="n">preprocessing</code><code class="o">.</code><code class="n">StandardScaler</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">X_train</code> <code class="o">=</code> <code class="n">sca</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">X_train</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="n">cols</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">X_test</code> <code class="o">=</code> <code class="n">sca</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">X_test</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="n">cols</code><code class="p">)</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Refactor" data-type="sect1"><div class="sect1" id="idm46066904611816">&#13;
<h1>Refactor</h1>&#13;
&#13;
<p><a data-primary="classification" data-secondary="refactoring code" data-type="indexterm" id="idm46066904568248"/><a data-primary="code, refactoring" data-type="indexterm" id="idm46066904567304"/><a data-primary="refactoring" data-type="indexterm" id="idm46066904566632"/>At this point I like to refactor my code. I typically make two functions.&#13;
One for general cleaning, and another for dividing up into a training and testing set&#13;
and to perform mutations that need to happen differently on those sets:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="k">def</code> <code class="nf">tweak_titanic</code><code class="p">(</code><code class="n">df</code><code class="p">):</code>&#13;
<code class="gp">... </code>    <code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code>&#13;
<code class="gp">... </code>        <code class="n">columns</code><code class="o">=</code><code class="p">[</code>&#13;
<code class="gp">... </code>            <code class="s">"name"</code><code class="p">,</code>&#13;
<code class="gp">... </code>            <code class="s">"ticket"</code><code class="p">,</code>&#13;
<code class="gp">... </code>            <code class="s">"home.dest"</code><code class="p">,</code>&#13;
<code class="gp">... </code>            <code class="s">"boat"</code><code class="p">,</code>&#13;
<code class="gp">... </code>            <code class="s">"body"</code><code class="p">,</code>&#13;
<code class="gp">... </code>            <code class="s">"cabin"</code><code class="p">,</code>&#13;
<code class="gp">... </code>        <code class="p">]</code>&#13;
<code class="gp">... </code>    <code class="p">)</code><code class="o">.</code><code class="n">pipe</code><code class="p">(</code><code class="n">pd</code><code class="o">.</code><code class="n">get_dummies</code><code class="p">,</code> <code class="n">drop_first</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>&#13;
<code class="gp">... </code>    <code class="k">return</code> <code class="n">df</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="k">def</code> <code class="nf">get_train_test_X_y</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">df</code><code class="p">,</code> <code class="n">y_col</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="mf">0.3</code><code class="p">,</code> <code class="n">std_cols</code><code class="o">=</code><code class="bp">None</code>&#13;
<code class="gp">... </code><code class="p">):</code>&#13;
<code class="gp">... </code>    <code class="n">y</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="n">y_col</code><code class="p">]</code>&#13;
<code class="gp">... </code>    <code class="n">X</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="n">columns</code><code class="o">=</code><code class="n">y_col</code><code class="p">)</code>&#13;
<code class="gp">... </code>    <code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">model_selection</code><code class="o">.</code><code class="n">train_test_split</code><code class="p">(</code>&#13;
<code class="gp">... </code>        <code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="n">size</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code>&#13;
<code class="gp">... </code>    <code class="p">)</code>&#13;
<code class="gp">... </code>    <code class="n">cols</code> <code class="o">=</code> <code class="n">X</code><code class="o">.</code><code class="n">columns</code>&#13;
<code class="gp">... </code>    <code class="n">num_cols</code> <code class="o">=</code> <code class="p">[</code>&#13;
<code class="gp">... </code>        <code class="s">"pclass"</code><code class="p">,</code>&#13;
<code class="gp">... </code>        <code class="s">"age"</code><code class="p">,</code>&#13;
<code class="gp">... </code>        <code class="s">"sibsp"</code><code class="p">,</code>&#13;
<code class="gp">... </code>        <code class="s">"parch"</code><code class="p">,</code>&#13;
<code class="gp">... </code>        <code class="s">"fare"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="p">]</code>&#13;
<code class="gp">... </code>    <code class="n">fi</code> <code class="o">=</code> <code class="n">impute</code><code class="o">.</code><code class="n">IterativeImputer</code><code class="p">()</code>&#13;
<code class="gp">... </code>    <code class="n">X_train</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code>&#13;
<code class="gp">... </code>        <code class="p">:,</code> <code class="n">num_cols</code>&#13;
<code class="gp">... </code>    <code class="p">]</code> <code class="o">=</code> <code class="n">fi</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">[</code><code class="n">num_cols</code><code class="p">])</code>&#13;
<code class="gp">... </code>    <code class="n">X_test</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code> <code class="n">num_cols</code><code class="p">]</code> <code class="o">=</code> <code class="n">fi</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code>&#13;
<code class="gp">... </code>        <code class="n">X_test</code><code class="p">[</code><code class="n">num_cols</code><code class="p">]</code>&#13;
<code class="gp">... </code>    <code class="p">)</code>&#13;
<code class="gp">...</code>&#13;
<code class="gp">... </code>    <code class="k">if</code> <code class="n">std_cols</code><code class="p">:</code>&#13;
<code class="gp">... </code>        <code class="n">std</code> <code class="o">=</code> <code class="n">preprocessing</code><code class="o">.</code><code class="n">StandardScaler</code><code class="p">()</code>&#13;
<code class="gp">... </code>        <code class="n">X_train</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code>&#13;
<code class="gp">... </code>            <code class="p">:,</code> <code class="n">std_cols</code>&#13;
<code class="gp">... </code>        <code class="p">]</code> <code class="o">=</code> <code class="n">std</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code>&#13;
<code class="gp">... </code>            <code class="n">X_train</code><code class="p">[</code><code class="n">std_cols</code><code class="p">]</code>&#13;
<code class="gp">... </code>        <code class="p">)</code>&#13;
<code class="gp">... </code>        <code class="n">X_test</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code>&#13;
<code class="gp">... </code>            <code class="p">:,</code> <code class="n">std_cols</code>&#13;
<code class="gp">... </code>        <code class="p">]</code> <code class="o">=</code> <code class="n">std</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_test</code><code class="p">[</code><code class="n">std_cols</code><code class="p">])</code>&#13;
<code class="gp">...</code>&#13;
<code class="gp">... </code>    <code class="k">return</code> <code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">ti_df</code> <code class="o">=</code> <code class="n">tweak_titanic</code><code class="p">(</code><code class="n">orig_df</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">std_cols</code> <code class="o">=</code> <code class="s">"pclass,age,sibsp,fare"</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s">","</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">get_train_test_X_y</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">ti_df</code><code class="p">,</code> <code class="s">"survived"</code><code class="p">,</code> <code class="n">std_cols</code><code class="o">=</code><code class="n">std_cols</code>&#13;
<code class="gp">... </code><code class="p">)</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Baseline Model" data-type="sect1"><div class="sect1" id="idm46066904558184">&#13;
<h1>Baseline Model</h1>&#13;
&#13;
<p><a data-primary="baseline model" data-type="indexterm" id="idm46066904557144"/><a data-primary="classification" data-secondary="baseline model" data-type="indexterm" id="idm46066904556440"/>Creating a baseline model that does something really simple can give us&#13;
something to compare our model to. Note that using the default <code>.score</code>&#13;
result gives us the accuracy which can be misleading. A problem where a positive case is 1 in 10,000 can easily get over 99% accuracy by always predicting negative.</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.dummy</code> <code class="kn">import</code> <code class="n">DummyClassifier</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">bm</code> <code class="o">=</code> <code class="n">DummyClassifier</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">bm</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">bm</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>  <code class="c"># accuracy</code>&#13;
<code class="go">0.5292620865139949</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">metrics</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">metrics</code><code class="o">.</code><code class="n">precision_score</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">y_test</code><code class="p">,</code> <code class="n">bm</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="go">0.4027777777777778</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Various Families" data-type="sect1"><div class="sect1" id="idm46066904185688">&#13;
<h1>Various Families</h1>&#13;
&#13;
<p><a data-primary="classification" data-secondary="algorithm families for" data-type="indexterm" id="idm46066904126568"/>This code tries a variety of algorithm families. The “No Free Lunch” theorem&#13;
states that no algorithm performs well on all data. However, for some finite&#13;
set of data, there may be an algorithm that does well on that set.&#13;
(A popular choice for structured learning these days is a tree-boosted&#13;
algorithm such as XGBoost.)</p>&#13;
&#13;
<p><a data-primary="k-fold cross-validation" data-type="indexterm" id="idm46066904124872"/>Here we&#13;
use a few different families and compare the AUC score and standard&#13;
deviation using k-fold cross-validation. An algorithm that has a&#13;
slightly smaller average score but tighter standard deviation might be a&#13;
better choice.</p>&#13;
&#13;
<p>Because we are using k-fold cross-validation, we will feed the model all of <code>X</code> and <code>y</code>:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">X</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">([</code><code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">])</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">y</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">([</code><code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code><code class="p">])</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">model_selection</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.dummy</code> <code class="kn">import</code> <code class="n">DummyClassifier</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">LogisticRegression</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="kn">import</code> <code class="n">DecisionTreeClassifier</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.neighbors</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">KNeighborsClassifier</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.naive_bayes</code> <code class="kn">import</code> <code class="n">GaussianNB</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="kn">import</code> <code class="n">SVC</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">RandomForestClassifier</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">xgboost</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">model</code> <code class="ow">in</code> <code class="p">[</code>&#13;
<code class="gp">... </code>    <code class="n">DummyClassifier</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">LogisticRegression</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">DecisionTreeClassifier</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">KNeighborsClassifier</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">GaussianNB</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">SVC</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">RandomForestClassifier</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">xgboost</code><code class="o">.</code><code class="n">XGBClassifier</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">]:</code>&#13;
<code class="gp">... </code>    <code class="n">cls</code> <code class="o">=</code> <code class="n">model</code><code class="p">()</code>&#13;
<code class="gp">... </code>    <code class="n">kfold</code> <code class="o">=</code> <code class="n">model_selection</code><code class="o">.</code><code class="n">KFold</code><code class="p">(</code>&#13;
<code class="gp">... </code>        <code class="n">n_splits</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code>&#13;
<code class="gp">... </code>    <code class="p">)</code>&#13;
<code class="gp">... </code>    <code class="n">s</code> <code class="o">=</code> <code class="n">model_selection</code><code class="o">.</code><code class="n">cross_val_score</code><code class="p">(</code>&#13;
<code class="gp">... </code>        <code class="n">cls</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">scoring</code><code class="o">=</code><code class="s">"roc_auc"</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="n">kfold</code>&#13;
<code class="gp">... </code>    <code class="p">)</code>&#13;
<code class="gp">... </code>    <code class="k">print</code><code class="p">(</code>&#13;
<code class="gp">... </code>        <code class="n">f</code><code class="s">"{model.__name__:22}  AUC: "</code>&#13;
<code class="gp">... </code>        <code class="n">f</code><code class="s">"{s.mean():.3f} STD: {s.std():.2f}"</code>&#13;
<code class="gp">... </code>    <code class="p">)</code>&#13;
<code class="go">DummyClassifier         AUC: 0.511  STD: 0.04</code>&#13;
<code class="go">LogisticRegression      AUC: 0.843  STD: 0.03</code>&#13;
<code class="go">DecisionTreeClassifier  AUC: 0.761  STD: 0.03</code>&#13;
<code class="go">KNeighborsClassifier    AUC: 0.829  STD: 0.05</code>&#13;
<code class="go">GaussianNB              AUC: 0.818  STD: 0.04</code>&#13;
<code class="go">SVC                     AUC: 0.838  STD: 0.05</code>&#13;
<code class="go">RandomForestClassifier  AUC: 0.829  STD: 0.04</code>&#13;
<code class="go">XGBClassifier           AUC: 0.864  STD: 0.04</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Stacking" data-type="sect1"><div class="sect1" id="idm46066904121528">&#13;
<h1>Stacking</h1>&#13;
&#13;
<p><a data-primary="classification" data-secondary="stacking" data-type="indexterm" id="idm46066904120488"/><a data-primary="stacking classifier" data-type="indexterm" id="idm46066904119512"/>If you were going down the Kaggle route (or want maximum performance at&#13;
the cost of interpretability), <em>stacking</em> is an option. A&#13;
stacking classifier takes other models and uses their&#13;
output to predict a target or label. We will use the&#13;
previous models’ outputs and combine them to see if a stacking&#13;
classifier can do better:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">mlxtend.classifier</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">StackingClassifier</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">clfs</code> <code class="o">=</code> <code class="p">[</code>&#13;
<code class="gp">... </code>    <code class="n">x</code><code class="p">()</code>&#13;
<code class="gp">... </code>    <code class="k">for</code> <code class="n">x</code> <code class="ow">in</code> <code class="p">[</code>&#13;
<code class="gp">... </code>        <code class="n">LogisticRegression</code><code class="p">,</code>&#13;
<code class="gp">... </code>        <code class="n">DecisionTreeClassifier</code><code class="p">,</code>&#13;
<code class="gp">... </code>        <code class="n">KNeighborsClassifier</code><code class="p">,</code>&#13;
<code class="gp">... </code>        <code class="n">GaussianNB</code><code class="p">,</code>&#13;
<code class="gp">... </code>        <code class="n">SVC</code><code class="p">,</code>&#13;
<code class="gp">... </code>        <code class="n">RandomForestClassifier</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="p">]</code>&#13;
<code class="gp">... </code><code class="p">]</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">stack</code> <code class="o">=</code> <code class="n">StackingClassifier</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">classifiers</code><code class="o">=</code><code class="n">clfs</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">meta_classifier</code><code class="o">=</code><code class="n">LogisticRegression</code><code class="p">(),</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">kfold</code> <code class="o">=</code> <code class="n">model_selection</code><code class="o">.</code><code class="n">KFold</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">n_splits</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">s</code> <code class="o">=</code> <code class="n">model_selection</code><code class="o">.</code><code class="n">cross_val_score</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">stack</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">scoring</code><code class="o">=</code><code class="s">"roc_auc"</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="n">kfold</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="k">print</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">f</code><code class="s">"{stack.__class__.__name__}  "</code>&#13;
<code class="gp">... </code>    <code class="n">f</code><code class="s">"AUC: {s.mean():.3f}  STD: {s.std():.2f}"</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="go">StackingClassifier  AUC: 0.804  STD: 0.06</code></pre>&#13;
&#13;
<p>In this case it looks like performance went down a bit, as well as standard&#13;
deviation.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Create Model" data-type="sect1"><div class="sect1" id="idm46066903793256">&#13;
<h1>Create Model</h1>&#13;
&#13;
<p><a data-primary="classification" data-secondary="model creation" data-type="indexterm" id="idm46066903792152"/><a data-primary="model" data-secondary="creating with random forest classifier" data-type="indexterm" id="idm46066903703640"/><a data-primary="random forest" data-secondary="model creation with" data-type="indexterm" id="idm46066903702728"/>I’m going to use a random forest classifier to create a model. It is a&#13;
flexible model that tends to give decent out-of-the-box results. Remember to&#13;
train it (calling <code>.fit</code>) with the training data from the data that we split&#13;
earlier into a training and testing set:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">rf</code> <code class="o">=</code> <code class="n">ensemble</code><code class="o">.</code><code class="n">RandomForestClassifier</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">n_estimators</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
<code class="go">RandomForestClassifier(bootstrap=True,</code>&#13;
<code class="go">   class_weight=None, criterion='gini',</code>&#13;
<code class="go">   max_depth=None, max_features='auto',</code>&#13;
<code class="go">   max_leaf_nodes=None,</code>&#13;
<code class="go">   min_impurity_decrease=0.0,</code>&#13;
<code class="go">   min_impurity_split=None,</code>&#13;
<code class="go">   min_samples_leaf=1, min_samples_split=2,</code>&#13;
<code class="go">   min_weight_fraction_leaf=0.0, n_estimators=10,</code>&#13;
<code class="go">   n_jobs=1, oob_score=False, random_state=42,</code>&#13;
<code class="go">   verbose=0, warm_start=False)</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Evaluate Model" data-type="sect1"><div class="sect1" id="idm46066903700136">&#13;
<h1>Evaluate Model</h1>&#13;
&#13;
<p><a data-primary="classification" data-secondary="model evaluation" data-type="indexterm" id="idm46066903519800"/><a data-primary="model" data-secondary="evaluating" data-type="indexterm" id="idm46066903518824"/>Now that we have a model, we can use the test data to see how well the model generalizes to data that it&#13;
hasn’t seen before. The <code>.score</code> method of a classifier returns the&#13;
average of the prediction accuracy. We want to make sure that we call the&#13;
<code>.score</code> method with the test data (presumably it should perform better with&#13;
the training data):</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">rf</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>&#13;
<code class="go">0.7964376590330788</code></pre>&#13;
&#13;
<p>We can also look at other metrics, such as precision:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">metrics</code><code class="o">.</code><code class="n">precision_score</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">y_test</code><code class="p">,</code> <code class="n">rf</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="go">0.8013698630136986</code></pre>&#13;
&#13;
<p><a data-primary="feature importance" data-secondary="tree-based models" data-type="indexterm" id="idm46066903667816"/>A nice benefit of tree-based models is that you can inspect the feature&#13;
importance. The feature importance tells you how much a feature&#13;
contributes to the model. Note that removing a feature doesn’t mean that&#13;
the score will go down accordingly, as other features might be colinear&#13;
(in this case we could remove either the sex_male or sex_female column as&#13;
they have a perfect negative correlation):</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">col</code><code class="p">,</code> <code class="n">val</code> <code class="ow">in</code> <code class="nb">sorted</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="nb">zip</code><code class="p">(</code>&#13;
<code class="gp">... </code>        <code class="n">X_train</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code>&#13;
<code class="gp">... </code>        <code class="n">rf</code><code class="o">.</code><code class="n">feature_importances_</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="p">),</code>&#13;
<code class="gp">... </code>    <code class="n">key</code><code class="o">=</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code>&#13;
<code class="gp">... </code>    <code class="n">reverse</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)[:</code><code class="mi">5</code><code class="p">]:</code>&#13;
<code class="gp">... </code>    <code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s">"{col:10}{val:10.3f}"</code><code class="p">)</code>&#13;
<code class="go">age            0.277</code>&#13;
<code class="go">fare           0.265</code>&#13;
<code class="go">sex_female     0.240</code>&#13;
<code class="go">pclass         0.092</code>&#13;
<code class="go">sibsp          0.048</code></pre>&#13;
&#13;
<p>The feature importance is calculated by looking at the error increase.&#13;
If removing a feature increases the error in the model, the feature is&#13;
more important.</p>&#13;
&#13;
<p>I really like the SHAP library for exploring what features a&#13;
model deems important, and for explaining predictions.&#13;
This library works with black-box models, and we will show it&#13;
later.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Optimize Model" data-type="sect1"><div class="sect1" id="idm46066903401160">&#13;
<h1>Optimize Model</h1>&#13;
&#13;
<p><a data-primary="classification" data-secondary="model optimization" data-type="indexterm" id="idm46066903400200"/><a data-primary="hyperparameters" data-secondary="model optimization and" data-type="indexterm" id="idm46066903399224"/><a data-primary="optimization, model" data-type="indexterm" id="idm46066903398280"/>Models have <em>hyperparameters</em> that control how they behave. By varying&#13;
the values for these parameters, we change their performance. <a data-primary="sklearn" data-secondary="model optimization" data-type="indexterm" id="idm46066903396936"/>Sklearn&#13;
has a grid search class to evaluate a model with different combinations&#13;
of parameters and return the best result. We can use those parameters to&#13;
instantiate the model class:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">rf4</code> <code class="o">=</code> <code class="n">ensemble</code><code class="o">.</code><code class="n">RandomForestClassifier</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">params</code> <code class="o">=</code> <code class="p">{</code>&#13;
<code class="gp">... </code>    <code class="s">"max_features"</code><code class="p">:</code> <code class="p">[</code><code class="mf">0.4</code><code class="p">,</code> <code class="s">"auto"</code><code class="p">],</code>&#13;
<code class="gp">... </code>    <code class="s">"n_estimators"</code><code class="p">:</code> <code class="p">[</code><code class="mi">15</code><code class="p">,</code> <code class="mi">200</code><code class="p">],</code>&#13;
<code class="gp">... </code>    <code class="s">"min_samples_leaf"</code><code class="p">:</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mf">0.1</code><code class="p">],</code>&#13;
<code class="gp">... </code>    <code class="s">"random_state"</code><code class="p">:</code> <code class="p">[</code><code class="mi">42</code><code class="p">],</code>&#13;
<code class="gp">... </code><code class="p">}</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">cv</code> <code class="o">=</code> <code class="n">model_selection</code><code class="o">.</code><code class="n">GridSearchCV</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">rf4</code><code class="p">,</code> <code class="n">params</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code>&#13;
<code class="gp">... </code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="k">print</code><code class="p">(</code><code class="n">cv</code><code class="o">.</code><code class="n">best_params_</code><code class="p">)</code>&#13;
<code class="go">{'max_features': 'auto', 'min_samples_leaf': 0.1,</code>&#13;
<code class="go"> 'n_estimators': 200, 'random_state': 42}</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rf5</code> <code class="o">=</code> <code class="n">ensemble</code><code class="o">.</code><code class="n">RandomForestClassifier</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="o">**</code><code class="p">{</code>&#13;
<code class="gp">... </code>        <code class="s">"max_features"</code><code class="p">:</code> <code class="s">"auto"</code><code class="p">,</code>&#13;
<code class="gp">... </code>        <code class="s">"min_samples_leaf"</code><code class="p">:</code> <code class="mf">0.1</code><code class="p">,</code>&#13;
<code class="gp">... </code>        <code class="s">"n_estimators"</code><code class="p">:</code> <code class="mi">200</code><code class="p">,</code>&#13;
<code class="gp">... </code>        <code class="s">"random_state"</code><code class="p">:</code> <code class="mi">42</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="p">}</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rf5</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rf5</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>&#13;
<code class="go">0.7888040712468194</code></pre>&#13;
&#13;
<p>We can pass in a <code>scoring</code> parameter to <code>GridSearchCV</code> to optimize for different metrics.&#13;
See <a data-type="xref" href="ch12.html#metrics1">Chapter 12</a> for a list of metrics and their meanings.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Confusion Matrix" data-type="sect1"><div class="sect1" id="idm46066903391880">&#13;
<h1>Confusion Matrix</h1>&#13;
&#13;
<p><a data-primary="classification" data-secondary="confusion matrix and" data-type="indexterm" id="idm46066903214936"/><a data-primary="confusion matrix" data-type="indexterm" id="idm46066903213960"/>A confusion matrix allows us to see the correct classifications as well&#13;
as false positives and false negatives. It may be that we want to&#13;
optimize toward false positives or false negatives, and different models&#13;
or parameters can alter that. <a data-primary="sklearn" data-secondary="for confusion matrix" data-type="indexterm" id="idm46066903212904"/><a data-primary="Yellowbrick" data-secondary="confusion matrix" data-type="indexterm" id="idm46066903211960"/>We can use sklearn to get a text version,&#13;
or Yellowbrick for a plot (see <a data-type="xref" href="#id0">Figure 3-4</a>):</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">confusion_matrix</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_pred</code> <code class="o">=</code> <code class="n">rf5</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">confusion_matrix</code><code class="p">(</code><code class="n">y_test</code><code class="p">,</code> <code class="n">y_pred</code><code class="p">)</code>&#13;
<code class="go">array([[196,  28],</code>&#13;
<code class="go">       [ 55, 114]])</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">mapping</code> <code class="o">=</code> <code class="p">{</code><code class="mi">0</code><code class="p">:</code> <code class="s">"died"</code><code class="p">,</code> <code class="mi">1</code><code class="p">:</code> <code class="s">"survived"</code><code class="p">}</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">6</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">cm_viz</code> <code class="o">=</code> <code class="n">ConfusionMatrix</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">rf5</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">classes</code><code class="o">=</code><code class="p">[</code><code class="s">"died"</code><code class="p">,</code> <code class="s">"survived"</code><code class="p">],</code>&#13;
<code class="gp">... </code>    <code class="n">label_encoder</code><code class="o">=</code><code class="n">mapping</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">cm_viz</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">cm_viz</code><code class="o">.</code><code class="n">poof</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="s">"images/mlpr_0304.png"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">bbox_inches</code><code class="o">=</code><code class="s">"tight"</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id0">&#13;
<img alt="Yellowbrick confusion matrix. This is a useful evaluation tool that presents the predicted class along the bottom and the true class along the side. A good classifier would have all of the values along the diagonal, and zeros in the other cells." src="assets/mlpr_0304.png"/>&#13;
<h6><span class="label">Figure 3-4. </span>Yellowbrick confusion matrix. This is a useful evaluation tool that presents the predicted class along the bottom and the true class along the side. A good classifier would have all of the values along the diagonal, and zeros in the other cells.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="ROC Curve" data-type="sect1"><div class="sect1" id="idm46066903111784">&#13;
<h1>ROC Curve</h1>&#13;
&#13;
<p><a data-primary="classification" data-secondary="ROC curve" data-type="indexterm" id="idm46066903061400"/><a data-primary="receiver operating characteristic (ROC) curve" data-type="indexterm" id="idm46066903060456"/><a data-primary="ROC (receiver operating characteristic) curve" data-type="indexterm" id="idm46066903059816"/>A receiver operating characteristic (ROC) plot is a common tool used to&#13;
evaluate classifiers. <a data-primary="area under the curve (AUC)" data-type="indexterm" id="idm46066903058936"/>By measuring the area under the curve (AUC), we can&#13;
get a metric to compare different classifiers (see <a data-type="xref" href="#id0a">Figure 3-5</a>). It plots the true&#13;
positive rate against the false positive rate. We can use sklearn to&#13;
calculate the AUC:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">y_pred</code> <code class="o">=</code> <code class="n">rf5</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">roc_auc_score</code><code class="p">(</code><code class="n">y_test</code><code class="p">,</code> <code class="n">y_pred</code><code class="p">)</code>&#13;
<code class="go">0.7747781065088757</code></pre>&#13;
&#13;
<p>Or Yellowbrick to visualize the plot:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">6</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">roc_viz</code> <code class="o">=</code> <code class="n">ROCAUC</code><code class="p">(</code><code class="n">rf5</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">roc_viz</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>&#13;
<code class="go">0.8279691030696217</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">roc_viz</code><code class="o">.</code><code class="n">poof</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_0305.png"</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id0a">&#13;
<img alt="ROC curve. This shows the true positive rate against the false positive rate. In general, the further it bulges out the better. Measuring the AUC gives a single number to evaluate. Closer to one is better. Below .5 is a poor model." src="assets/mlpr_0305.png"/>&#13;
<h6><span class="label">Figure 3-5. </span>ROC curve. This shows the true positive rate against the false positive rate. In general, the further it bulges out the better. Measuring the AUC gives a single number to evaluate. Closer to one is better. Below .5 is a poor model.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Learning Curve" data-type="sect1"><div class="sect1" id="idm46066902934680">&#13;
<h1>Learning Curve</h1>&#13;
&#13;
<p><a data-primary="classification" data-secondary="learning curve" data-type="indexterm" id="idm46066902933624"/><a data-primary="learning curve" data-type="indexterm" id="idm46066902932648"/><a data-primary="training data" data-type="indexterm" id="idm46066902931976"/>A learning curve is used to tell us if we have enough training data. It&#13;
trains the model with increasing portions of the data and measures the&#13;
score (see <a data-type="xref" href="#id1a">Figure 3-6</a>). If the cross-validation score continues to climb, then we might&#13;
need to invest in gathering more data. Here is a Yellowbrick&#13;
example:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">numpy</code> <code class="kn">as</code> <code class="nn">np</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">cv</code> <code class="o">=</code> <code class="n">StratifiedKFold</code><code class="p">(</code><code class="mi">12</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">sizes</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="mf">0.3</code><code class="p">,</code> <code class="mf">1.0</code><code class="p">,</code> <code class="mi">10</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lc_viz</code> <code class="o">=</code> <code class="n">LearningCurve</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">rf5</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">cv</code><code class="o">=</code><code class="n">cv</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">train_sizes</code><code class="o">=</code><code class="n">sizes</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">scoring</code><code class="o">=</code><code class="s">"f1_weighted"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">n_jobs</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lc_viz</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lc_viz</code><code class="o">.</code><code class="n">poof</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_0306.png"</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id1a">&#13;
<img alt="This learning curve shows that as we add more training samples, our cross-validation (testing) scores appear to improve." src="assets/mlpr_0306.png"/>&#13;
<h6><span class="label">Figure 3-6. </span>This learning curve shows that as we add more training samples, our cross-validation (testing) scores appear to improve.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Deploy Model" data-type="sect1"><div class="sect1" id="idm46066902797064">&#13;
<h1>Deploy Model</h1>&#13;
&#13;
<p><a data-primary="classification" data-secondary="model deployment" data-type="indexterm" id="idm46066902795864"/><a data-primary="model" data-secondary="deployment" data-type="indexterm" id="idm46066902794888"/>Using Python’s <code>pickle</code> module, we can persist models and load them.&#13;
Once we have a model, we call the <code>.predict</code> method to get a&#13;
classification or regression result:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">pickle</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">pic</code> <code class="o">=</code> <code class="n">pickle</code><code class="o">.</code><code class="n">dumps</code><code class="p">(</code><code class="n">rf5</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rf6</code> <code class="o">=</code> <code class="n">pickle</code><code class="o">.</code><code class="n">loads</code><code class="p">(</code><code class="n">pic</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_pred</code> <code class="o">=</code> <code class="n">rf6</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">roc_auc_score</code><code class="p">(</code><code class="n">y_test</code><code class="p">,</code> <code class="n">y_pred</code><code class="p">)</code>&#13;
<code class="go">0.7747781065088757</code></pre>&#13;
&#13;
<p>Using <a href="https://palletsprojects.com/p/flask">Flask</a> to deploy a web service for prediction is very common. There are now other commercial and open source products coming out that support deployment.<a data-startref="ix_ch03-asciidoc1" data-type="indexterm" id="idm46066902658584"/><a data-startref="ix_ch03-asciidoc0" data-type="indexterm" id="idm46066902657976"/> Among them are <a href="http://clipper.ai/">Clipper</a>, <a href="https://oreil.ly/UfHdP">Pipeline</a>, and <a href="https://oreil.ly/1qYkH">Google’s Cloud Machine Learning Engine</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm46066905711928"><sup><a href="ch03.html#idm46066905711928-marker">1</a></sup> Even though we don’t directly call this library, when we load an Excel file, pandas leverages it behind the scenes.</p></div></div></section></body></html>