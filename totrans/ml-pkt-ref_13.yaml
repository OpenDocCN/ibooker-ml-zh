- en: Chapter 13\. Explaining Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Predictive models have different properties. Some are designed to handle linear
    data. Others can mold to more complex input. Some models can be interpreted very
    easily, others are like black boxes and don’t offer much insight into how the
    prediction is made.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we will look at interpreting different models. We will look
    at some examples using the Titanic data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Regression Coefficients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The intercepts and regression coefficients explain the expected value, and how
    features impact the prediction. A positive coefficient indicates that as a feature’s
    value increases, the prediction increases as well.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Importance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tree-based models in the scikit-learn library include a `.fea⁠ture_``importances_`
    attribute for inspecting how the features of a dataset affect the model. We can
    inspect or plot them.
  prefs: []
  type: TYPE_NORMAL
- en: LIME
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[LIME](https://oreil.ly/shCR_) works to help explain black-box models. It performs
    a *local* interpretation rather than an overall interpretation. It will help explain
    a single sample.'
  prefs: []
  type: TYPE_NORMAL
- en: For a given data point or sample, LIME indicates which features were important
    in determining the result. It does this by perturbing the sample in question and
    fitting a linear model to it. The linear model approximates the model close to
    the sample (see [Figure 13-1](#id38)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example explaining the last sample (which our decision tree predicts
    will survive) from the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: LIME doesn’t like using DataFrames as input. Note that we converted the data
    to numpy arrays using `.values`.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If you are doing this in Jupyter, follow up with this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This will render an HTML version of the explanation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a matplotlib figure if we want to export the explanation (or
    aren’t using Jupyter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![LIME explanation for the Titanic dataset. Features for the sample push the
    prediction toward the right (survival) or left (deceased).](assets/mlpr_1301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-1\. LIME explanation for the Titanic dataset. Features for the sample
    push the prediction toward the right (survival) or left (deceased).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Play around with this and notice that if you switch genders, the results are
    affected. Below we take the second to last row in the training data. The prediction
    for that row is 48% deceased and 52% survived. If we switch the gender, we find
    that the prediction shifts toward 88% deceased:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `.predict_proba` method returns a probability for each label.
  prefs: []
  type: TYPE_NORMAL
- en: Tree Interpretation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For sklearn tree-based models (decision tree, random forest, and extra tree
    models) you can use the [treeinterpreter package](https://oreil.ly/vN1Bl). This
    will calculate the bias and the contribution from each feature. The bias is the
    mean of the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each contribution lists how it contributes to each of the labels. (The bias
    plus the contributions should sum to the prediction.) Since this is a binary classification,
    there are only two. We see that sex_male is the most important, followed by age
    and fare:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This example is for classification, but there is support for regression as well.
  prefs: []
  type: TYPE_NORMAL
- en: Partial Dependence Plots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With feature importance in trees we know that a feature is impacting the outcome,
    but we don’t know how the impact varies as the feature’s value changes. Partial
    dependence plots allow us to visualize the relation between changes in just one
    feature and the outcome. We will use [pdpbox](https://oreil.ly/O9zY2) to visualize
    how age affects survival (see [Figure 13-2](#id39)).
  prefs: []
  type: TYPE_NORMAL
- en: 'This example uses a random forest model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![Partial dependence plot showing what happens to the target as age changes.](assets/mlpr_1302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-2\. Partial dependence plot showing what happens to the target as
    age changes.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can also visualize the interactions between two features (see [Figure 13-3](#id40)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![Partial dependence plot with two features. As fare goes up and sex goes from
    male to female, survival goes up.](assets/mlpr_1303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-3\. Partial dependence plot with two features. As fare goes up and
    sex goes from male to female, survival goes up.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The partial dependence plot pins down a feature value across the samples and
    then averages the result. (Be careful about outliers and means.) Also, this plot
    assumes features are independent. (Not always the case; for example, holding width
    of a sepal steady would probably have an effect on the height.) The pdpbox library
    also prints out the individual conditional expectations to better visualize these
    relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Surrogate Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have a model that is not interpretable (SVM or neural network), you can
    fit an interpretable model (decision tree) to that model. Using the surrogate
    you can examine the feature importances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we create a Support Vector Classifier (SVC), but train a decision tree
    (without a depth limit to overfit and capture what is happening in this model)
    to explain it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Shapley
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The SHapley Additive exPlanations, ([SHAP](https://oreil.ly/QYj-q)) package
    can visualize feature contributions of any model. This is a really nice package
    because not only does it work with most models, it also can explain individual
    predictions and the global feature contributions.
  prefs: []
  type: TYPE_NORMAL
- en: SHAP works for both classification and regression. It generates “SHAP” values.
    For classification models, the SHAP value sums to log odds for binary classification.
    For regression, the SHAP values sum to the target prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'This library requires Jupyter (JavaScript) for interactivity on some of its
    plots. (Some can render static images with matplotlib.) Here is an example for
    sample 20, predicted to die:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In the force plot for sample 20, you can see the “base value.” This is a female
    who is predicted to die (see [Figure 13-4](#id41)). We will use the survival index
    (1) because we want the right-hand side of the plot to be survival. The features
    push this to the right or left. The larger the feature, the more impact it has.
    In this case, the low fare and third class push toward death (the output value
    is below .5):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![Shapley feature contributions for sample 20\. This plot shows the base value
    and the features that push toward death.](assets/mlpr_1304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-4\. Shapley feature contributions for sample 20\. This plot shows
    the base value and the features that push toward death.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You can also visualize the explanations for the entire dataset (rotating them
    by 90 and plotting them along the x axis) (see [Figure 13-5](#id13_5)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![Shapley feature contributions for dataset.](assets/mlpr_1305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-5\. Shapley feature contributions for dataset.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The SHAP library can also generate dependence plots. The following plot (see
    [Figure 13-6](#id43)) visualizes the relationship between age and SHAP value (it
    is colored by pclass, which SHAP chooses automatically; specify a column name
    as an `interaction_index` parameter to choose your own):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![Shapley dependency plot for age. Young and old have a higher rate of survival.
    As age goes up, a lower pclass has more chance of survival.](assets/mlpr_1306.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-6\. Shapley dependency plot for age. Young and old have a higher rate
    of survival. As age goes up, a lower pclass has more chance of survival.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You might get a dependence plot that has vertical lines. Setting the `x_jitter`
    parameter to 1 is useful if you are viewing ordinal categorical features.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we can summarize all of the features. This is a very powerful chart
    to understand. It shows global impact, but also individual impacts. The features
    are ranked by importance. The most important features are at the top.
  prefs: []
  type: TYPE_NORMAL
- en: Also the features are colored according to their value. We can see that a low
    sex_male score (female) has a strong push toward survival, while a high score
    has a less strong push toward death. The age feature is a little harder to interpret.
    That is because young and old values push toward survival, while middle values
    push toward death.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you combine the summary plot with the dependence plot, you can get good
    insight into model behavior (see [Figure 13-7](#id44)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![Shapley summary plot showing most important features at the top. The coloring
    shows how the values of the feature affect the target.](assets/mlpr_1307.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-7\. Shapley summary plot showing most important features at the top.
    The coloring shows how the values of the feature affect the target.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
