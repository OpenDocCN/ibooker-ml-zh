- en: Chapter 20\. Tensors with PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 20.0 Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just as NumPy is a foundational tool for data manipulation in the machine learning
    stack, PyTorch is a foundational tool for working with tensors in the deep learning
    stack. Before moving on to deep learning itself, we should familiarize ourselves
    with PyTorch tensors and create many operations analogous to those performed with
    NumPy in [Chapter 1](ch01.xhtml#vectors-matrices-and-arrays).
  prefs: []
  type: TYPE_NORMAL
- en: Although PyTorch is just one of multiple deep learning libraries, it is significantly
    popular both within academia and industry. PyTorch tensors are *very* similar
    to NumPy arrays. However, they also allow us to perform tensor operations on GPUs
    (hardware specialized for deep learning). In this chapter, we’ll familiarize ourselves
    with the basics of PyTorch tensors and many common low-level operations.
  prefs: []
  type: TYPE_NORMAL
- en: 20.1 Creating a Tensor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to create a tensor.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use Pytorch to create a tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main data structure within PyTorch is a tensor, and in many ways tensors
    are exactly like the multidimensional NumPy arrays used in [Chapter 1](ch01.xhtml#vectors-matrices-and-arrays).
    Just like vectors and arrays, these tensors can be represented horizontally (i.e.,
    rows) or vertically (i.e., columns).
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PyTorch documentation: Tensors](https://oreil.ly/utaTD)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 20.2 Creating a Tensor from NumPy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to create PyTorch tensors from NumPy arrays.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the PyTorch `from_numpy` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we can see, PyTorch is very similar to NumPy syntactically. In addition,
    it easily allows us to convert NumPy arrays to PyTorch tensors that we can use
    on GPUs and other accelerated hardware. At the time of writing, NumPy is mentioned
    frequently in the PyTorch documentation, and PyTorch itself even offers a way
    that PyTorch tensors and NumPy arrays can share the same memory to reduce overhead.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PyTorch documentation: Bridge with NumPy](https://oreil.ly/zEJo6)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 20.3 Creating a Sparse Tensor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given data with very few nonzero values, you want to efficiently represent it
    with a tensor.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the PyTorch `to_sparse` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sparse tensors are memory-efficient ways to represent data composed of mostly
    0s. In [Chapter 1](ch01.xhtml#vectors-matrices-and-arrays) we used `scipy` to
    create a compressed sparse row (CSR) matrix that was no longer a NumPy array.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `torch.Tensor` class allows us to create both regular and sparse matrices
    using the same object. If we inspect the types of the two tensors we just created,
    we can see they’re actually both of the same class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PyTorch documentation: Sparse Tensor](https://oreil.ly/8J3IO)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 20.4 Selecting Elements in a Tensor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need to select specific elements of a tensor.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use NumPy-like indexing and slicing to return elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Like NumPy arrays and most everything in Python, PyTorch tensors are zero-indexed.
    Both indexing and slicing are supported as well. One key difference is that indexing
    a PyTorch tensor to return a single element still returns a tensor as opposed
    to the value of the object itself (which would be in the form of an integer or
    float). Slicing syntax also has parity with NumPy and will return objects of type
    tensor in PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'One key difference is that PyTorch tensors do not yet support negative steps
    when slicing. Therefore, attempting to reverse a tensor using slicing yields an
    error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead, if we wish to reverse a tensor we can use the `flip` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PyTorch documentation: Operations on Tensors](https://oreil.ly/8-xj7)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 20.5 Describing a Tensor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to describe the shape, data type, and format of a tensor along with
    the hardware it’s using.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Inpect the `shape`, `dtype`, `layout`, and `device` attributes of the tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'PyTorch tensors provide a number of helpful attributes for gathering information
    about a given tensor, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Shape
  prefs: []
  type: TYPE_NORMAL
- en: Returns the dimensions of the tensor
  prefs: []
  type: TYPE_NORMAL
- en: Dtype
  prefs: []
  type: TYPE_NORMAL
- en: Returns the data type of objects within the tensor
  prefs: []
  type: TYPE_NORMAL
- en: Layout
  prefs: []
  type: TYPE_NORMAL
- en: Returns the memory layout (most common is `strided` used for dense tensors)
  prefs: []
  type: TYPE_NORMAL
- en: Device
  prefs: []
  type: TYPE_NORMAL
- en: Returns the hardware the tensor is being stored on (CPU/GPU)
  prefs: []
  type: TYPE_NORMAL
- en: Again, the key differentiator between tensors and arrays is an attribute like
    *device*, because tensors provide us with hardware-accelerated options like GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 20.6 Applying Operations to Elements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to apply an operation to all elements in a tensor.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Take advantage of *broadcasting* with PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Basic operations in PyTorch will take advantage of broadcasting to parallelize
    them using accelerated hardware such as GPUs. This is true for supported mathematical
    operators in Python (+, -, ×, /) and other functions inherent to PyTorch. Unlike
    NumPy, PyTorch doesn’t include a `vectorize` method for applying a function over
    all elements in a tensor. However, PyTorch comes equipped with all of the mathematical
    tools necessary to distribute and accelerate the usual operations required for
    deep learning workflows.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PyTorch documentation: Broadcasting Semantics](https://oreil.ly/NsPpa)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Vectorization and Broadcasting with PyTorch](https://oreil.ly/dfzIJ)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 20.7 Finding the Maximum and Minimum Values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to find the maximum or minimum value in a tensor.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the PyTorch `max` and `min` methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `max` and `min` methods of a tensor help us find the largest or smallest
    values in that tensor. These methods work the same across multidimensional tensors
    as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 20.8 Reshaping Tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to change the shape (number of rows and columns) of a tensor without
    changing the element values.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the PyTorch `reshape` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Manipulating the shape of a tensor can be common in the field of deep learning,
    as neurons in a neural network often require tensors of a very specific shape.
    Since the required shape of a tensor can change between neurons in a given neural
    network, it is good to have a low-level understanding of our inputs and outputs
    in deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 20.9 Transposing a Tensor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to transpose a tensor.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the `mT` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transposing with PyTorch is slightly different from NumPy. The `T` method used
    for NumPy arrays is supported in PyTorch only with tensors of two dimensions and
    at the time of writing is deprecated for tensors of other shapes. The `mT` method
    used to transpose batches of tensors is preferred, as it scales to greater than
    two dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'An additional way to transpose PyTorch tensors of any shape is to use the `permute`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: This method also works for one-dimensional tensors (for which the value of the
    tranposed tensor is the same as the original tensor).
  prefs: []
  type: TYPE_NORMAL
- en: 20.10 Flattening a Tensor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to transform a tensor into one dimension.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the `flatten` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Flattening a tensor is a useful technique for reducing a multidimensional tensor
    into one dimension.
  prefs: []
  type: TYPE_NORMAL
- en: 20.11 Calculating Dot Products
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to calculate the dot product of two tensors.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the `dot` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Calculating the dot product of two tensors is a common operation useful in the
    deep learning space as well as the information retrieval space. You may remember
    earlier in the book where we used the dot product of two vectors to perform a
    cosine similarity-based search. Doing this in PyTorch on GPU (instead of with
    NumPy or scikit-learn on CPU) can yield impressive performance benefits on information
    retrieval problems.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Vectorization and Broadcasting with PyTorch](https://oreil.ly/lIjtB)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 20.12 Multiplying Tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to multiply two tensors.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use basic Python arithmetic operators:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PyTorch supports basic arithmetic operators such as ×, +, - and /. Although
    multiplying tensors is probably one of the most common operations used in deep
    learning, it’s useful to know tensors can also be added, subtracted, and divided.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add one tensor to another:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Subtract one tensor from another:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Divide one tensor by another:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
