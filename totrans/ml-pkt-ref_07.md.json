["```py\n>>> X2 = pd.DataFrame(\n...     {\n...         \"a\": range(5),\n...         \"b\": [-100, -50, 0, 200, 1000],\n...     }\n... )\n>>> X2\n a     b\n0  0  -100\n1  1   -50\n2  2     0\n3  3   200\n4  4  1000\n```", "```py\n>>> from sklearn import preprocessing\n>>> std = preprocessing.StandardScaler()\n>>> std.fit_transform(X2)\narray([[-1.41421356, -0.75995002],\n [-0.70710678, -0.63737744],\n [ 0\\.        , -0.51480485],\n [ 0.70710678, -0.02451452],\n [ 1.41421356,  1.93664683]])\n```", "```py\n>>> std.scale_\narray([  1.41421356, 407.92156109])\n>>> std.mean_\narray([  2., 210.])\n>>> std.var_\narray([2.000e+00, 1.664e+05])\n```", "```py\n>>> X_std = (X2 - X2.mean()) / X2.std()\n>>> X_std\n a         b\n0 -1.264911 -0.679720\n1 -0.632456 -0.570088\n2  0.000000 -0.460455\n3  0.632456 -0.021926\n4  1.264911  1.732190\n\n>>> X_std.mean()\na    4.440892e-17\nb    0.000000e+00\ndtype: float64\n\n>>> X_std.std()\na    1.0\nb    1.0\ndtype: float64\n```", "```py\n>>> X3 = X2.copy()\n>>> from fastai.structured import scale_vars\n>>> scale_vars(X3, mapper=None)\n>>> X3.std()\na    1.118034\nb    1.118034\ndtype: float64\n>>> X3.mean()\na    0.000000e+00\nb    4.440892e-17\ndtype: float64\n```", "```py\n>>> from sklearn import preprocessing\n>>> mms = preprocessing.MinMaxScaler()\n>>> mms.fit(X2)\n>>> mms.transform(X2)\narray([[0\\.     , 0\\.     ],\n [0.25   , 0.04545],\n [0.5    , 0.09091],\n [0.75   , 0.27273],\n [1\\.     , 1\\.     ]])\n```", "```py\n>>> (X2 - X2.min()) / (X2.max() - X2.min())\n a         b\n0  0.00  0.000000\n1  0.25  0.045455\n2  0.50  0.090909\n3  0.75  0.272727\n4  1.00  1.000000\n```", "```py\n>>> X_cat = pd.DataFrame(\n...     {\n...         \"name\": [\"George\", \"Paul\"],\n...         \"inst\": [\"Bass\", \"Guitar\"],\n...     }\n... )\n>>> X_cat\n name    inst\n0  George    Bass\n1    Paul  Guitar\n```", "```py\n>>> pd.get_dummies(X_cat, drop_first=True)\n name_Paul  inst_Guitar\n0          0            0\n1          1            1\n```", "```py\n>>> X_cat2 = pd.DataFrame(\n...     {\n...         \"A\": [1, None, 3],\n...         \"names\": [\n...             \"Fred,George\",\n...             \"George\",\n...             \"John,Paul\",\n...         ],\n...     }\n... )\n>>> jn.expand_column(X_cat2, \"names\", sep=\",\")\n A        names  Fred  George  John  Paul\n0  1.0  Fred,George     1       1     0     0\n1  NaN       George     0       1     0     0\n2  3.0    John,Paul     0       0     1     1\n```", "```py\n>>> from sklearn import preprocessing\n>>> lab = preprocessing.LabelEncoder()\n>>> lab.fit_transform(X_cat)\narray([0,1])\n```", "```py\n>>> lab.inverse_transform([1, 1, 0])\narray(['Guitar', 'Guitar', 'Bass'], dtype=object)\n```", "```py\n>>> X_cat.name.astype(\n...     \"category\"\n... ).cat.as_ordered().cat.codes + 1\n0    1\n1    2\ndtype: int8\n```", "```py\n>>> mapping = X_cat.name.value_counts()\n>>> X_cat.name.map(mapping)\n0    1\n1    1\nName: name, dtype: int64\n```", "```py\n>>> from collections import Counter\n>>> c = Counter()\n>>> def triples(val):\n...     for i in range(len(val)):\n...         c[val[i : i + 3]] += 1\n>>> df.name.apply(triples)\n>>> c.most_common(10)\n[(', M', 1282),\n (' Mr', 954),\n ('r. ', 830),\n ('Mr.', 757),\n ('s. ', 460),\n ('n, ', 320),\n (' Mi', 283),\n ('iss', 261),\n ('ss.', 261),\n ('Mis', 260)]\n```", "```py\n>>> df.name.str.extract(\n...     \"([A-Za-z]+)\\.\", expand=False\n... ).head()\n0      Miss\n1    Master\n2      Miss\n3        Mr\n4       Mrs\nName: name, dtype: object\n```", "```py\n>>> df.name.str.extract(\n...     \"([A-Za-z]+)\\.\", expand=False\n... ).value_counts()\nMr          757\nMiss        260\nMrs         197\nMaster       61\nDr            8\nRev           8\nCol           4\nMlle          2\nMs            2\nMajor         2\nDona          1\nDon           1\nLady          1\nCountess      1\nCapt          1\nSir           1\nMme           1\nJonkheer      1\nName: name, dtype: int64\n```", "```py\n>>> import category_encoders as ce\n>>> he = ce.HashingEncoder(verbose=1)\n>>> he.fit_transform(X_cat)\n col_0  col_1  col_2  col_3  col_4  col_5  col_6  col_7\n0      0      0      0      1      0      1      0      0\n1      0      2      0      0      0      0      0      0\n```", "```py\n>>> size_df = pd.DataFrame(\n...     {\n...         \"name\": [\"Fred\", \"John\", \"Matt\"],\n...         \"size\": [\"small\", \"med\", \"xxl\"],\n...     }\n... )\n>>> ore = ce.OrdinalEncoder(\n...     mapping=[\n...         {\n...             \"col\": \"size\",\n...             \"mapping\": {\n...                 \"small\": 1,\n...                 \"med\": 2,\n...                 \"lg\": 3,\n...             },\n...         }\n...     ]\n... )\n>>> ore.fit_transform(size_df)\n name  size\n0  Fred   1.0\n1  John   2.0\n2  Matt  -1.0\n```", "```py\n>>> def get_title(df):\n...     return df.name.str.extract(\n...         \"([A-Za-z]+)\\.\", expand=False\n...     )\n>>> te = ce.TargetEncoder(cols=\"Title\")\n>>> te.fit_transform(\n...     df.assign(Title=get_title), df.survived\n... )[\"Title\"].head()\n0    0.676923\n1    0.508197\n2    0.676923\n3    0.162483\n4    0.786802\nName: Title, dtype: float64\n```", "```py\n>>> from fastai.tabular.transform import (\n...     add_datepart,\n... )\n>>> dates = pd.DataFrame(\n...     {\n...         \"A\": pd.to_datetime(\n...             [\"9/17/2001\", \"Jan 1, 2002\"]\n...         )\n...     }\n... )\n\n>>> add_datepart(dates, \"A\")\n>>> dates.T\n 0           1\nAYear                    2001        2002\nAMonth                      9           1\nAWeek                      38           1\nADay                       17           1\nADayofweek                  0           1\nADayofyear                260           1\nAIs_month_end           False       False\nAIs_month_start         False        True\nAIs_quarter_end         False       False\nAIs_quarter_start       False        True\nAIs_year_end            False       False\nAIs_year_start          False        True\nAElapsed           1000684800  1009843200\n```", "```py\n>>> from pandas.api.types import is_numeric_dtype\n>>> def fix_missing(df, col, name, na_dict):\n...     if is_numeric_dtype(col):\n...         if pd.isnull(col).sum() or (\n...             name in na_dict\n...         ):\n...             df[name + \"_na\"] = pd.isnull(col)\n...             filler = (\n...                 na_dict[name]\n...                 if name in na_dict\n...                 else col.median()\n...             )\n...             df[name] = col.fillna(filler)\n...             na_dict[name] = filler\n...     return na_dict\n>>> data = pd.DataFrame({\"A\": [0, None, 5, 100]})\n>>> fix_missing(data, data.A, \"A\", {})\n{'A': 5.0}\n>>> data\n A   A_na\n0    0.0  False\n1    5.0   True\n2    5.0  False\n3  100.0  False\n```", "```py\n>>> data = pd.DataFrame({\"A\": [0, None, 5, 100]})\n>>> data[\"A_na\"] = data.A.isnull()\n>>> data[\"A\"] = data.A.fillna(data.A.median())\n```", "```py\n>>> agg = (\n...     df.groupby(\"cabin\")\n...     .agg(\"min,max,mean,sum\".split(\",\"))\n...     .reset_index()\n... )\n>>> agg.columns = [\n...     \"_\".join(c).strip(\"_\")\n...     for c in agg.columns.values\n... ]\n>>> agg_df = df.merge(agg, on=\"cabin\")\n```"]