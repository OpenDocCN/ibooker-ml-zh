<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 8. Handling Images" data-type="chapter" epub:type="chapter"><div class="chapter" id="handling-images">
<h1><span class="label">Chapter 8. </span>Handling Images</h1>
<section data-pdf-bookmark="8.0 Introduction" data-type="sect1"><div class="sect1" id="id192">
<h1>8.0 Introduction</h1>
<p>Image <a data-primary="images" data-type="indexterm" id="ix_image_class_ch8"/>classification is one of the most exciting areas of machine learning. The ability of computers to recognize patterns and objects from images is an incredibly powerful tool in our toolkit. However, before we can apply machine learning to images, we often first need to transform the raw images to features usable by our learning algorithms. As with textual data, there are also many pretrained classifiers available for images that we can use to extract features or objects of interest to use as inputs to our own models.</p>
<p>To work with images, we will primarily use the <a data-primary="OpenCV (Open Source Computer Vision Library)" data-type="indexterm" id="ix_opencv_vision_lib"/><a data-primary="cross-validation (CV) of ML models" data-secondary="OpenCV" data-type="indexterm" id="ix_cross_val_cv_open"/>Open Source Computer Vision Library
(OpenCV). While there are a number of good libraries out there, OpenCV
is the most popular and well-documented library for handling images. It can occasionally be challenging to install, but if you run into issues, there are many guides online. This book in particular was written with <code>opencv-python-headless==4.7.0.68</code>. You can also run these chapters with <a href="https://oreil.ly/MLwPython">the ML in Python Cookbook Runner</a> to ensure all commands are reproducible.</p>
<p>Throughout this chapter, we will use as examples a set of images, which is available to download from
<a href="https://oreil.ly/gV5Zc">GitHub</a>.</p>
</div></section>
<section data-pdf-bookmark="8.1 Loading Images" data-type="sect1"><div class="sect1" id="loading-images">
<h1>8.1 Loading Images</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id492">
<h2>Problem</h2>
<p>You want to <a data-primary="images" data-secondary="loading" data-type="indexterm" id="ix_image_load"/><a data-primary="loading data" data-secondary="images" data-type="indexterm" id="ix_load_data_image"/>load an image for preprocessing.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id193">
<h2>Solution</h2>
<p>Use <a data-primary="imread method" data-type="indexterm" id="id1377"/>OpenCV’s <code>imread</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">cv2</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">matplotlib</code> <code class="kn">import</code> <code class="n">pyplot</code> <code class="k">as</code> <code class="n">plt</code>

<code class="c1"># Load image as grayscale</code>
<code class="n">image</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">imread</code><code class="p">(</code><code class="s2">"images/plane.jpg"</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">IMREAD_GRAYSCALE</code><code class="p">)</code></pre>
<p>If we want to view the image, we can use the Python plotting library
Matplotlib:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Show image</code>
<code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s2">"gray"</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 08in01" height="211" src="assets/mpc2_08in01.png" width="335"/>
<h6/>
</div></figure>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id194">
<h2>Discussion</h2>
<p>Fundamentally, images are data, and when we use <code>imread</code>, we convert that
data into a data type we are very familiar with—​a NumPy array:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Show data type</code>
<code class="nb">type</code><code class="p">(</code><code class="n">image</code><code class="p">)</code></pre>
<pre data-type="programlisting">numpy.ndarray</pre>
<p>We have transformed the image into a matrix whose elements correspond to
individual pixels. We can even take a look at the actual values of the
matrix:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Show image data</code>
<code class="n">image</code></pre>
<pre data-type="programlisting">array([[140, 136, 146, ..., 132, 139, 134],
       [144, 136, 149, ..., 142, 124, 126],
       [152, 139, 144, ..., 121, 127, 134],
       ...,
       [156, 146, 144, ..., 157, 154, 151],
       [146, 150, 147, ..., 156, 158, 157],
       [143, 138, 147, ..., 156, 157, 157]], dtype=uint8)</pre>
<p class="less_space pagebreak-before">The resolution of our image is 3600 × 2270, the exact dimensions of
our matrix:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Show dimensions</code>
<code class="n">image</code><code class="o">.</code><code class="n">shape</code></pre>
<pre data-type="programlisting">(2270, 3600)</pre>
<p>What does each element in the matrix actually represent? In grayscale
images, the value of an individual element is the pixel intensity.
Intensity values range from black (0) to white (255). For example, the
intensity of the top leftmost pixel in our image has a value of 140:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Show first pixel</code>
<code class="n">image</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code><code class="mi">0</code><code class="p">]</code></pre>
<pre data-type="programlisting">140</pre>
<p>In a matrix representing a color image, each element actually contains three values corresponding to blue, green, and red values, respectively (BGR):</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load image in color</code>
<code class="n">image_bgr</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">imread</code><code class="p">(</code><code class="s2">"images/plane.jpg"</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">IMREAD_COLOR</code><code class="p">)</code>

<code class="c1"># Show pixel</code>
<code class="n">image_bgr</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code><code class="mi">0</code><code class="p">]</code></pre>
<pre data-type="programlisting">array([195, 144, 111], dtype=uint8)</pre>
<p>One small caveat: by default OpenCV uses BGR, but many image
applications—​including Matplotlib—​use red, green, blue (RGB),
meaning the red and the blue <span class="keep-together">values</span> are swapped. To properly display OpenCV color images in Matplotlib, we first need to convert the color to RGB (apologies to hardcopy readers for whom there are no color images):<a data-primary="" data-startref="ix_cross_val_cv_open" data-type="indexterm" id="id1378"/><a data-primary="" data-startref="ix_image_load" data-type="indexterm" id="id1379"/><a data-primary="" data-startref="ix_load_data_image" data-type="indexterm" id="id1380"/><a data-primary="" data-startref="ix_opencv_vision_lib" data-type="indexterm" id="id1381"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Convert to RGB</code>
<code class="n">image_rgb</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">cvtColor</code><code class="p">(</code><code class="n">image_bgr</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">COLOR_BGR2RGB</code><code class="p">)</code>

<code class="c1"># Show image</code>
<code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image_rgb</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 08in02" height="211" src="assets/mpc2_08in02.png" width="335"/>
<h6/>
</div></figure>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1382">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/N1Ub6">Difference Between RGB and BGR</a></p>
</li>
<li>
<p><a href="https://oreil.ly/OEesQ">RGB color model, Wikipedia</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="8.2 Saving Images" data-type="sect1"><div class="sect1" id="saving-images">
<h1>8.2 Saving Images</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id493">
<h2>Problem</h2>
<p>You want to <a data-primary="images" data-secondary="saving" data-type="indexterm" id="id1383"/>save an image for preprocessing.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id195">
<h2>Solution</h2>
<p>Use <a data-primary="imwrite method" data-type="indexterm" id="id1384"/>OpenCV’s <code>imwrite</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">cv2</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">matplotlib</code> <code class="kn">import</code> <code class="n">pyplot</code> <code class="k">as</code> <code class="n">plt</code>

<code class="c1"># Load image as grayscale</code>
<code class="n">image</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">imread</code><code class="p">(</code><code class="s2">"images/plane.jpg"</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">IMREAD_GRAYSCALE</code><code class="p">)</code>

<code class="c1"># Save image</code>
<code class="n">cv2</code><code class="o">.</code><code class="n">imwrite</code><code class="p">(</code><code class="s2">"images/plane_new.jpg"</code><code class="p">,</code> <code class="n">image</code><code class="p">)</code></pre>
<pre data-type="programlisting">True</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id1385">
<h2>Discussion</h2>
<p>OpenCV’s <code>imwrite</code> saves images to the filepath specified. The format
of the image is defined by the filename’s extension (<em>.jpg</em>,
<em>.png</em>, etc.). One behavior to be careful about: <code>imwrite</code> will
overwrite existing files without outputting an error or asking for
confirmation.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="8.3 Resizing Images" data-type="sect1"><div class="sect1" id="resizing-images">
<h1>8.3 Resizing Images</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id196">
<h2>Problem</h2>
<p>You want to <a data-primary="resizing images" data-type="indexterm" id="ix_resize_image"/><a data-primary="images" data-secondary="resizing" data-type="indexterm" id="ix_image_resize"/>resize an image for further preprocessing.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id1386">
<h2>Solution</h2>
<p>Use <code>resize</code> to change the size of an image:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">cv2</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">matplotlib</code> <code class="kn">import</code> <code class="n">pyplot</code> <code class="k">as</code> <code class="n">plt</code>

<code class="c1"># Load image as grayscale</code>
<code class="n">image</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">imread</code><code class="p">(</code><code class="s2">"images/plane_256x256.jpg"</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">IMREAD_GRAYSCALE</code><code class="p">)</code>

<code class="c1"># Resize image to 50 pixels by 50 pixels</code>
<code class="n">image_50x50</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">resize</code><code class="p">(</code><code class="n">image</code><code class="p">,</code> <code class="p">(</code><code class="mi">50</code><code class="p">,</code> <code class="mi">50</code><code class="p">))</code>

<code class="c1"># View image</code>
<code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image_50x50</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s2">"gray"</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 08in03" height="218" src="assets/mpc2_08in03.png" width="218"/>
<h6/>
</div></figure>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id197">
<h2>Discussion</h2>
<p>Resizing images is a common task in image preprocessing for two reasons. First, images come in all shapes and sizes, and to be usable as features, images must have the same dimensions. Standardizing (resizing) images does come at the cost of losing some information present in the larger image, as can be seen in the picture of the airplane. Images are matrices of
information, and when we reduce the size of the image, we are reducing the size of that matrix and the information it contains. Second, machine learning can require thousands or hundreds of thousands of images. When those images are very large they can take up a lot of memory, and by resizing them we can dramatically reduce memory usage. Some common image sizes for machine learning are 32	× 32, 64	× 64, 96	× 96, and 256	× 256. In essence, the method we choose for image resizing will often be a tradeoff between the statistical <a data-primary="Pillow library" data-type="indexterm" id="id1387"/>performance of our model and computational cost to train it. The <a href="https://oreil.ly/NiJn_">Pillow library offers many options for resizing images</a> for this reason.<a data-primary="" data-startref="ix_image_resize" data-type="indexterm" id="id1388"/><a data-primary="" data-startref="ix_resize_image" data-type="indexterm" id="id1389"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="8.4 Cropping Images" data-type="sect1"><div class="sect1" id="cropping-images">
<h1>8.4 Cropping Images</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id198">
<h2>Problem</h2>
<p>You want to <a data-primary="cropping images" data-type="indexterm" id="id1390"/><a data-primary="images" data-secondary="cropping" data-type="indexterm" id="id1391"/>remove the outer portion of the image to change its
dimensions.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id494">
<h2>Solution</h2>
<p>The image is encoded as a two-dimensional NumPy array, so we can crop the image easily by <a data-primary="slicing" data-secondary="NumPy array" data-type="indexterm" id="id1392"/>slicing the array:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">cv2</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">matplotlib</code> <code class="kn">import</code> <code class="n">pyplot</code> <code class="k">as</code> <code class="n">plt</code>

<code class="c1"># Load image in grayscale</code>
<code class="n">image</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">imread</code><code class="p">(</code><code class="s2">"images/plane_256x256.jpg"</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">IMREAD_GRAYSCALE</code><code class="p">)</code>

<code class="c1"># Select first half of the columns and all rows</code>
<code class="n">image_cropped</code> <code class="o">=</code> <code class="n">image</code><code class="p">[:,:</code><code class="mi">128</code><code class="p">]</code>

<code class="c1"># Show image</code>
<code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image_cropped</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s2">"gray"</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 08in04" height="218" src="assets/mpc2_08in04.png" width="108"/>
<h6/>
</div></figure>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id1393">
<h2>Discussion</h2>
<p>Since OpenCV represents images as a matrix of elements, by selecting the rows and columns we want to keep we can easily crop the image. Cropping can be particularly useful if we know that we want to keep only a certain part of every image. For example, if our images come from a stationary security camera we can crop all the images so they contain only the area of interest.</p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1394">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/8JN5p">Slicing NumPy Arrays</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="8.5 Blurring Images" data-type="sect1"><div class="sect1" id="blurring-images">
<h1>8.5 Blurring Images</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id199">
<h2>Problem</h2>
<p>You want to <a data-primary="images" data-secondary="blurring" data-type="indexterm" id="ix_image_blur"/><a data-primary="blurring images" data-type="indexterm" id="ix_blur_image"/>smooth out an image.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id1395">
<h2>Solution</h2>
<p>To blur an image, each pixel is transformed to be the average value of
its neighbors. This neighbor and the operation performed are
mathematically represented as a kernel (don’t worry if you don’t know what a kernel is). The size of this kernel determines the amount of blurring, with larger kernels producing smoother images. Here we blur an image by averaging the values of a 5 × 5 kernel around each pixel:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">cv2</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">matplotlib</code> <code class="kn">import</code> <code class="n">pyplot</code> <code class="k">as</code> <code class="n">plt</code>

<code class="c1"># Load image as grayscale</code>
<code class="n">image</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">imread</code><code class="p">(</code><code class="s2">"images/plane_256x256.jpg"</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">IMREAD_GRAYSCALE</code><code class="p">)</code>

<code class="c1"># Blur image</code>
<code class="n">image_blurry</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">blur</code><code class="p">(</code><code class="n">image</code><code class="p">,</code> <code class="p">(</code><code class="mi">5</code><code class="p">,</code><code class="mi">5</code><code class="p">))</code>

<code class="c1"># Show image</code>
<code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image_blurry</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s2">"gray"</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 08in05" height="218" src="assets/mpc2_08in05.png" width="218"/>
<h6/>
</div></figure>
<p class="fix_tracking">To highlight the effect of kernel size, here is the same blurring with a 100 × 100 <span class="keep-together">kernel:</span></p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Blur image</code>
<code class="n">image_very_blurry</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">blur</code><code class="p">(</code><code class="n">image</code><code class="p">,</code> <code class="p">(</code><code class="mi">100</code><code class="p">,</code><code class="mi">100</code><code class="p">))</code>

<code class="c1"># Show image</code>
<code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image_very_blurry</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s2">"gray"</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">xticks</code><code class="p">([]),</code> <code class="n">plt</code><code class="o">.</code><code class="n">yticks</code><code class="p">([])</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 08in06" height="218" src="assets/mpc2_08in06.png" width="218"/>
<h6/>
</div></figure>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id200">
<h2>Discussion</h2>
<p>Kernels<a data-primary="kernel functions" data-type="indexterm" id="ix_kernel_func"/> are widely used in image processing to do everything from
sharpening to edge detection and will come up repeatedly in this
chapter. The blurring kernel we used looks like<a data-primary="" data-startref="ix_blur_image" data-type="indexterm" id="id1396"/><a data-primary="" data-startref="ix_image_blur" data-type="indexterm" id="id1397"/> this:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create kernel</code>
<code class="n">kernel</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">ones</code><code class="p">((</code><code class="mi">5</code><code class="p">,</code><code class="mi">5</code><code class="p">))</code> <code class="o">/</code> <code class="mf">25.0</code>

<code class="c1"># Show kernel</code>
<code class="n">kernel</code></pre>
<pre data-type="programlisting">array([[ 0.04,  0.04,  0.04,  0.04,  0.04],
       [ 0.04,  0.04,  0.04,  0.04,  0.04],
       [ 0.04,  0.04,  0.04,  0.04,  0.04],
       [ 0.04,  0.04,  0.04,  0.04,  0.04],
       [ 0.04,  0.04,  0.04,  0.04,  0.04]])</pre>
<p class="fix_tracking2">
The center element in the kernel is the pixel being examined, while the
remaining elements are its neighbors. Since all elements have the same
value (normalized to add up to 1), each has an equal say in the
resulting value of the pixel of interest. We can manually apply a kernel
to an image using <code>filter2D</code> to produce a similar blurring effect:
</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Apply kernel</code>
<code class="n">image_kernel</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">filter2D</code><code class="p">(</code><code class="n">image</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="n">kernel</code><code class="p">)</code>

<code class="c1"># Show image</code>
<code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image_kernel</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s2">"gray"</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">xticks</code><code class="p">([]),</code> <code class="n">plt</code><code class="o">.</code><code class="n">yticks</code><code class="p">([])</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 08in07" height="218" src="assets/mpc2_08in07.png" width="218"/>
<h6/>
</div></figure>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1398">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/9yvdg">Image Kernels Explained Visually</a></p>
</li>
<li>
<p><a href="https://oreil.ly/ByREC">Kernel (image processing), Wikipedia</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="8.6 Sharpening Images" data-type="sect1"><div class="sect1" id="sharpening-images">
<h1>8.6 Sharpening Images</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id201">
<h2>Problem</h2>
<p>You want to <a data-primary="images" data-secondary="sharpening" data-type="indexterm" id="id1399"/><a data-primary="sharpening images" data-type="indexterm" id="id1400"/>sharpen an image.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id202">
<h2>Solution</h2>
<p>Create a kernel that highlights the target pixel. <a data-primary="filter2d method" data-type="indexterm" id="id1401"/>Then apply it to the
image using <code>filter2D</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">cv2</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">matplotlib</code> <code class="kn">import</code> <code class="n">pyplot</code> <code class="k">as</code> <code class="n">plt</code>

<code class="c1"># Load image as grayscale</code>
<code class="n">image</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">imread</code><code class="p">(</code><code class="s2">"images/plane_256x256.jpg"</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">IMREAD_GRAYSCALE</code><code class="p">)</code>

<code class="c1"># Create kernel</code>
<code class="n">kernel</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code><code class="mi">0</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code>
                   <code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code><code class="o">-</code><code class="mi">1</code><code class="p">],</code>
                   <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">]])</code>

<code class="c1"># Sharpen image</code>
<code class="n">image_sharp</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">filter2D</code><code class="p">(</code><code class="n">image</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="n">kernel</code><code class="p">)</code>

<code class="c1"># Show image</code>
<code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image_sharp</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s2">"gray"</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 08in08" height="218" src="assets/mpc2_08in08.png" width="218"/>
<h6/>
</div></figure>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id203">
<h2>Discussion</h2>
<p>Sharpening works similarly to blurring, except instead of using a kernel
to average the neighboring values, we constructed a kernel to highlight
the pixel itself. The resulting effect makes contrasts in edges stand
out more.<a data-primary="" data-startref="ix_kernel_func" data-type="indexterm" id="id1402"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="8.7 Enhancing Contrast" data-type="sect1"><div class="sect1" id="enhancing-contrast">
<h1>8.7 Enhancing Contrast</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id204">
<h2>Problem</h2>
<p>We want to increase the <a data-primary="images" data-secondary="contrast enhancement" data-type="indexterm" id="ix_image_contrast_enhance"/><a data-primary="contrast, enhancing image" data-type="indexterm" id="ix_contrast_enhance"/>contrast between pixels in an image.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id205">
<h2>Solution</h2>
<p><em>Histogram equalization</em> is a <a data-primary="equalizeHist method" data-type="indexterm" id="id1403"/><a data-primary="histogram equalization" data-type="indexterm" id="id1404"/>tool for image processing that can make
objects and shapes stand out. When we have a grayscale image, we can
apply OpenCV’s 
<span class="keep-together"><code>equalizeHist</code></span> directly on the image:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">cv2</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">matplotlib</code> <code class="kn">import</code> <code class="n">pyplot</code> <code class="k">as</code> <code class="n">plt</code>

<code class="c1"># Load image</code>
<code class="n">image</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">imread</code><code class="p">(</code><code class="s2">"images/plane_256x256.jpg"</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">IMREAD_GRAYSCALE</code><code class="p">)</code>

<code class="c1"># Enhance image</code>
<code class="n">image_enhanced</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">equalizeHist</code><code class="p">(</code><code class="n">image</code><code class="p">)</code>

<code class="c1"># Show image</code>
<code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image_enhanced</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s2">"gray"</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 08in09" height="218" src="assets/mpc2_08in09.png" width="218"/>
<h6/>
</div></figure>
<p>However, when we have a color image, we first need to convert the image
to the YUV color format. The Y is the luma, or brightness, and U and V
denote the color. After the conversion, we can apply <code>equalizeHist</code> to
the image and then convert it back to BGR or RGB (apologies to hardcopy readers for whom there are no color images):</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load image</code>
<code class="n">image_bgr</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">imread</code><code class="p">(</code><code class="s2">"images/plane.jpg"</code><code class="p">)</code>

<code class="c1"># Convert to YUV</code>
<code class="n">image_yuv</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">cvtColor</code><code class="p">(</code><code class="n">image_bgr</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">COLOR_BGR2YUV</code><code class="p">)</code>

<code class="c1"># Apply histogram equalization</code>
<code class="n">image_yuv</code><code class="p">[:,</code> <code class="p">:,</code> <code class="mi">0</code><code class="p">]</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">equalizeHist</code><code class="p">(</code><code class="n">image_yuv</code><code class="p">[:,</code> <code class="p">:,</code> <code class="mi">0</code><code class="p">])</code>

<code class="c1"># Convert to RGB</code>
<code class="n">image_rgb</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">cvtColor</code><code class="p">(</code><code class="n">image_yuv</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">COLOR_YUV2RGB</code><code class="p">)</code>

<code class="c1"># Show image</code>
<code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image_rgb</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 08in10" height="211" src="assets/mpc2_08in10.png" width="335"/>
<h6/>
</div></figure>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id206">
<h2>Discussion</h2>
<p>While a detailed explanation of how histogram equalization works is
beyond the scope of this book, the short explanation is that it
transforms the image so that it uses a wider range of pixel intensities.</p>
<p>While the resulting image often does not look “realistic,” we need to
remember that the image is just a visual representation of the
underlying data. If histogram equalization is able to make objects of
interest more distinguishable from other objects or backgrounds (which
is not always the case), then it can be a valuable addition to our
image preprocessing pipeline.<a data-primary="" data-startref="ix_contrast_enhance" data-type="indexterm" id="id1405"/><a data-primary="" data-startref="ix_image_contrast_enhance" data-type="indexterm" id="id1406"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="8.8 Isolating Colors" data-type="sect1"><div class="sect1" id="isolating-colors">
<h1>8.8 Isolating Colors</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id207">
<h2>Problem</h2>
<p>You want to <a data-primary="images" data-secondary="color isolation" data-type="indexterm" id="ix_image_color_isol"/><a data-primary="color isolation, images" data-type="indexterm" id="ix_color_isol"/>isolate a color in an image.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id1407">
<h2>Solution</h2>
<p>Define a range of colors and then apply a mask to the image (apologies to hardcopy readers for whom there are no color images):</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">cv2</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">matplotlib</code> <code class="kn">import</code> <code class="n">pyplot</code> <code class="k">as</code> <code class="n">plt</code>

<code class="c1"># Load image</code>
<code class="n">image_bgr</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">imread</code><code class="p">(</code><code class="s1">'images/plane_256x256.jpg'</code><code class="p">)</code>

<code class="c1"># Convert BGR to HSV</code>
<code class="n">image_hsv</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">cvtColor</code><code class="p">(</code><code class="n">image_bgr</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">COLOR_BGR2HSV</code><code class="p">)</code>

<code class="c1"># Define range of blue values in HSV</code>
<code class="n">lower_blue</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="mi">50</code><code class="p">,</code><code class="mi">100</code><code class="p">,</code><code class="mi">50</code><code class="p">])</code>
<code class="n">upper_blue</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="mi">130</code><code class="p">,</code><code class="mi">255</code><code class="p">,</code><code class="mi">255</code><code class="p">])</code>

<code class="c1"># Create mask</code>
<code class="n">mask</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">inRange</code><code class="p">(</code><code class="n">image_hsv</code><code class="p">,</code> <code class="n">lower_blue</code><code class="p">,</code> <code class="n">upper_blue</code><code class="p">)</code>

<code class="c1"># Mask image</code>
<code class="n">image_bgr_masked</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">bitwise_and</code><code class="p">(</code><code class="n">image_bgr</code><code class="p">,</code> <code class="n">image_bgr</code><code class="p">,</code> <code class="n">mask</code><code class="o">=</code><code class="n">mask</code><code class="p">)</code>

<code class="c1"># Convert BGR to RGB</code>
<code class="n">image_rgb</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">cvtColor</code><code class="p">(</code><code class="n">image_bgr_masked</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">COLOR_BGR2RGB</code><code class="p">)</code>

<code class="c1"># Show image</code>
<code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image_rgb</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 08in11" height="218" src="assets/mpc2_08in11.png" width="218"/>
<h6/>
</div></figure>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id208">
<h2>Discussion</h2>
<p>Isolating colors in OpenCV is straightforward. First we convert an image into HSV (hue, saturation, and value). Second, we define a range of values we want to isolate, which is probably the most difficult and
time-consuming part. Third, we create a mask for the image. Image masking is a common technique meant to extract regions of interest. In this case, our mask keeps only the white areas:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Show image</code>
<code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">mask</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'gray'</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 08in12" height="218" src="assets/mpc2_08in12.png" width="218"/>
<h6/>
</div></figure>
<p>Finally, we apply the mask to the image using <code>bitwise_and</code> and convert
to our desired output format.<a data-primary="" data-startref="ix_color_isol" data-type="indexterm" id="id1408"/><a data-primary="" data-startref="ix_image_color_isol" data-type="indexterm" id="id1409"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="8.9 Binarizing Images" data-type="sect1"><div class="sect1" id="binarizing-images">
<h1>8.9 Binarizing Images</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id209">
<h2>Problem</h2>
<p>Given an <a data-primary="images" data-secondary="binarizing" data-type="indexterm" id="ix_image_bin"/><a data-primary="binarizing images" data-type="indexterm" id="ix_bin_image"/>image, you want to output a simplified version.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id210">
<h2>Solution</h2>
<p><em>Thresholding</em> is the <a data-primary="thresholding" data-secondary="binarizing images" data-type="indexterm" id="ix_thresh_bin_image"/><a data-primary="adaptive thresholding" data-type="indexterm" id="ix_adapt_thresh"/><a data-primary="filter methods" data-secondary="binarizing images" data-type="indexterm" id="ix_filter_bin_image"/>process of setting pixels with intensity greater than some value to be white and less than the value to be black. A more advanced technique is <em>adaptive thresholding</em>, where the threshold value for a pixel is determined by the pixel intensities of its neighbors. This can be helpful when lighting conditions change over different regions in an image:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">cv2</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">matplotlib</code> <code class="kn">import</code> <code class="n">pyplot</code> <code class="k">as</code> <code class="n">plt</code>

<code class="c1"># Load image as grayscale</code>
<code class="n">image_grey</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">imread</code><code class="p">(</code><code class="s2">"images/plane_256x256.jpg"</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">IMREAD_GRAYSCALE</code><code class="p">)</code>

<code class="c1"># Apply adaptive thresholding</code>
<code class="n">max_output_value</code> <code class="o">=</code> <code class="mi">255</code>
<code class="n">neighborhood_size</code> <code class="o">=</code> <code class="mi">99</code>
<code class="n">subtract_from_mean</code> <code class="o">=</code> <code class="mi">10</code>
<code class="n">image_binarized</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">adaptiveThreshold</code><code class="p">(</code><code class="n">image_grey</code><code class="p">,</code>
                                        <code class="n">max_output_value</code><code class="p">,</code>
                                        <code class="n">cv2</code><code class="o">.</code><code class="n">ADAPTIVE_THRESH_GAUSSIAN_C</code><code class="p">,</code>
                                        <code class="n">cv2</code><code class="o">.</code><code class="n">THRESH_BINARY</code><code class="p">,</code>
                                        <code class="n">neighborhood_size</code><code class="p">,</code>
                                        <code class="n">subtract_from_mean</code><code class="p">)</code>

<code class="c1"># Show image</code>
<code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image_binarized</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s2">"gray"</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 08in13" height="180" src="assets/mpc2_08in13.png" width="204"/>
<h6/>
</div></figure>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id211">
<h2>Discussion</h2>
<p>The process of binarizing an image involves converting a greyscale image to its black and white form. Our solution has four important arguments in 
<span class="keep-together"><code>adaptiveThreshold</code>.</span>
<code>max_output_value</code> simply determines the maximum intensity of the output
pixel intensities. <code>cv2.ADAPTIVE_THRESH_GAUSSIAN_C</code> sets a pixel’s
threshold to be a weighted sum of the neighboring pixel intensities. The
weights are determined by a Gaussian window. Alternatively, we could set
the threshold to simply the mean of the neighboring pixels with
<code>cv2.ADAPTIVE_THRESH_MEAN_C</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Apply cv2.ADAPTIVE_THRESH_MEAN_C</code>
<code class="n">image_mean_threshold</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">adaptiveThreshold</code><code class="p">(</code><code class="n">image_grey</code><code class="p">,</code>
                                             <code class="n">max_output_value</code><code class="p">,</code>
                                             <code class="n">cv2</code><code class="o">.</code><code class="n">ADAPTIVE_THRESH_MEAN_C</code><code class="p">,</code>
                                             <code class="n">cv2</code><code class="o">.</code><code class="n">THRESH_BINARY</code><code class="p">,</code>
                                             <code class="n">neighborhood_size</code><code class="p">,</code>
                                             <code class="n">subtract_from_mean</code><code class="p">)</code>

<code class="c1"># Show image</code>
<code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image_mean_threshold</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s2">"gray"</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 08in14" height="182" src="assets/mpc2_08in14.png" width="204"/>
<h6/>
</div></figure>
<p>The last two parameters are the block size (the size of the neighborhood used to determine a pixel’s threshold) and a constant subtracted from the calculated threshold (used to manually fine-tune the threshold).</p>
<p>A major benefit of <a data-primary="denoising an image" data-type="indexterm" id="id1410"/>thresholding is <em>denoising</em> an image—​keeping only the most important elements. For example, thresholding is often applied to photos of printed text to isolate the letters from the page.<a data-primary="" data-startref="ix_bin_image" data-type="indexterm" id="id1411"/><a data-primary="" data-startref="ix_filter_bin_image" data-type="indexterm" id="id1412"/><a data-primary="" data-startref="ix_image_bin" data-type="indexterm" id="id1413"/><a data-primary="" data-startref="ix_thresh_bin_image" data-type="indexterm" id="id1414"/><a data-primary="" data-startref="ix_adapt_thresh" data-type="indexterm" id="id1415"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="8.10 Removing Backgrounds" data-type="sect1"><div class="sect1" id="removing-backgrounds">
<h1>8.10 Removing Backgrounds</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id212">
<h2>Problem</h2>
<p>You want to isolate the <a data-primary="images" data-secondary="background removal" data-type="indexterm" id="ix_image_backg_rem"/><a data-primary="background removal, images" data-type="indexterm" id="ix_backg_image"/>foreground of an image.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id213">
<h2>Solution</h2>
<p>Mark a rectangle around the desired foreground, then run the <a data-primary="GrabCut algorithm" data-type="indexterm" id="ix_grab_cut"/>GrabCut algorithm:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">import</code> <code class="nn">cv2</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">matplotlib</code> <code class="kn">import</code> <code class="n">pyplot</code> <code class="k">as</code> <code class="n">plt</code>

<code class="c1"># Load image and convert to RGB</code>
<code class="n">image_bgr</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">imread</code><code class="p">(</code><code class="s1">'images/plane_256x256.jpg'</code><code class="p">)</code>
<code class="n">image_rgb</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">cvtColor</code><code class="p">(</code><code class="n">image_bgr</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">COLOR_BGR2RGB</code><code class="p">)</code>

<code class="c1"># Rectangle values: start x, start y, width, height</code>
<code class="n">rectangle</code> <code class="o">=</code> <code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">56</code><code class="p">,</code> <code class="mi">256</code><code class="p">,</code> <code class="mi">150</code><code class="p">)</code>

<code class="c1"># Create initial mask</code>
<code class="n">mask</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">(</code><code class="n">image_rgb</code><code class="o">.</code><code class="n">shape</code><code class="p">[:</code><code class="mi">2</code><code class="p">],</code> <code class="n">np</code><code class="o">.</code><code class="n">uint8</code><code class="p">)</code>

<code class="c1"># Create temporary arrays used by grabCut</code>
<code class="n">bgdModel</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">((</code><code class="mi">1</code><code class="p">,</code> <code class="mi">65</code><code class="p">),</code> <code class="n">np</code><code class="o">.</code><code class="n">float64</code><code class="p">)</code>
<code class="n">fgdModel</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">((</code><code class="mi">1</code><code class="p">,</code> <code class="mi">65</code><code class="p">),</code> <code class="n">np</code><code class="o">.</code><code class="n">float64</code><code class="p">)</code>

<code class="c1"># Run grabCut</code>
<code class="n">cv2</code><code class="o">.</code><code class="n">grabCut</code><code class="p">(</code><code class="n">image_rgb</code><code class="p">,</code> <code class="c1"># Our image</code>
            <code class="n">mask</code><code class="p">,</code> <code class="c1"># The Mask</code>
            <code class="n">rectangle</code><code class="p">,</code> <code class="c1"># Our rectangle</code>
            <code class="n">bgdModel</code><code class="p">,</code> <code class="c1"># Temporary array for background</code>
            <code class="n">fgdModel</code><code class="p">,</code> <code class="c1"># Temporary array for background</code>
            <code class="mi">5</code><code class="p">,</code> <code class="c1"># Number of iterations</code>
            <code class="n">cv2</code><code class="o">.</code><code class="n">GC_INIT_WITH_RECT</code><code class="p">)</code> <code class="c1"># Initiative using our rectangle</code>

<code class="c1"># Create mask where sure and likely backgrounds set to 0, otherwise 1</code>
<code class="n">mask_2</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">where</code><code class="p">((</code><code class="n">mask</code><code class="o">==</code><code class="mi">2</code><code class="p">)</code> <code class="o">|</code> <code class="p">(</code><code class="n">mask</code><code class="o">==</code><code class="mi">0</code><code class="p">),</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="s1">'uint8'</code><code class="p">)</code>

<code class="c1"># Multiply image with new mask to subtract background</code>
<code class="n">image_rgb_nobg</code> <code class="o">=</code> <code class="n">image_rgb</code> <code class="o">*</code> <code class="n">mask_2</code><code class="p">[:,</code> <code class="p">:,</code> <code class="n">np</code><code class="o">.</code><code class="n">newaxis</code><code class="p">]</code>

<code class="c1"># Show image</code>
<code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image_rgb_nobg</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 08in15" height="218" src="assets/mpc2_08in15.png" width="218"/>
<h6/>
</div></figure>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id214">
<h2>Discussion</h2>
<p>The first thing we notice is that even though GrabCut did a pretty good job,
there are still areas of background left in the image. We could go back
and manually mark those areas as background, but in the real world we
have thousands of images and manually fixing them individually is not
feasible. Therefore, we would do well by simply accepting that the image
data will still contain some background noise.</p>
<p>In our solution, we start by marking a rectangle around the area
that contains the foreground. GrabCut assumes everything outside this
rectangle to be background and uses that information to figure out what
is likely background inside the square. (To learn how the algorithm does
this, see this explanation from <a href="https://oreil.ly/DTGwb">Itay Blumenthal</a>.)
Then a mask is created that denotes the different definitely/likely
background/foreground regions:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Show mask</code>
<code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">mask</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'gray'</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 08in16" height="218" src="assets/mpc2_08in16.png" width="218"/>
<h6/>
</div></figure>
<p>The black region is the area outside our rectangle that is assumed to be
definitely background. The gray area is what GrabCut considered likely
background, while the white area is likely foreground.</p>
<p>This mask is then used to create a second mask that merges the black and
gray regions:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Show mask</code>
<code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">mask_2</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'gray'</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 08in17" height="218" src="assets/mpc2_08in17.png" width="218"/>
<h6/>
</div></figure>
<p>The second mask is then applied to the image so that only the foreground
remains.<a data-primary="" data-startref="ix_backg_image" data-type="indexterm" id="id1416"/><a data-primary="" data-startref="ix_grab_cut" data-type="indexterm" id="id1417"/><a data-primary="" data-startref="ix_image_backg_rem" data-type="indexterm" id="id1418"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="8.11 Detecting Edges" data-type="sect1"><div class="sect1" id="detecting-edges">
<h1>8.11 Detecting Edges</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id215">
<h2>Problem</h2>
<p>You want to find the <a data-primary="edge detection, images" data-type="indexterm" id="ix_edge_det_image"/><a data-primary="images" data-secondary="edge detection" data-type="indexterm" id="ix_image_edge_det"/>edges in an image.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id216">
<h2>Solution</h2>
<p>Use an edge detection technique like the <a data-primary="Canny edge detector" data-type="indexterm" id="ix_canny_edge"/>Canny edge detector:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">cv2</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">matplotlib</code> <code class="kn">import</code> <code class="n">pyplot</code> <code class="k">as</code> <code class="n">plt</code>

<code class="c1"># Load image as grayscale</code>
<code class="n">image_gray</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">imread</code><code class="p">(</code><code class="s2">"images/plane_256x256.jpg"</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">IMREAD_GRAYSCALE</code><code class="p">)</code>

<code class="c1"># Calculate median intensity</code>
<code class="n">median_intensity</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">median</code><code class="p">(</code><code class="n">image_gray</code><code class="p">)</code>

<code class="c1"># Set thresholds to be one standard deviation above and below median intensity</code>
<code class="n">lower_threshold</code> <code class="o">=</code> <code class="nb">int</code><code class="p">(</code><code class="nb">max</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="p">(</code><code class="mf">1.0</code> <code class="o">-</code> <code class="mf">0.33</code><code class="p">)</code> <code class="o">*</code> <code class="n">median_intensity</code><code class="p">))</code>
<code class="n">upper_threshold</code> <code class="o">=</code> <code class="nb">int</code><code class="p">(</code><code class="nb">min</code><code class="p">(</code><code class="mi">255</code><code class="p">,</code> <code class="p">(</code><code class="mf">1.0</code> <code class="o">+</code> <code class="mf">0.33</code><code class="p">)</code> <code class="o">*</code> <code class="n">median_intensity</code><code class="p">))</code>

<code class="c1"># Apply Canny edge detector</code>
<code class="n">image_canny</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">Canny</code><code class="p">(</code><code class="n">image_gray</code><code class="p">,</code> <code class="n">lower_threshold</code><code class="p">,</code> <code class="n">upper_threshold</code><code class="p">)</code>

<code class="c1"># Show image</code>
<code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image_canny</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s2">"gray"</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 08in18" height="218" src="assets/mpc2_08in18.png" width="218"/>
<h6/>
</div></figure>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id217">
<h2>Discussion</h2>
<p>Edge detection is a major topic of interest in computer vision. Edges
are important because they are areas of high information. For example,
in our image one patch of sky looks very much like another and is unlikely to contain unique or interesting information. However, patches where the background sky meets the airplane contain a lot of information (e.g., an object’s shape). Edge detection allows us to remove low-information areas and isolate the areas of images containing
the most information.</p>
<p>There are many edge detection techniques (Sobel filters, Laplacian edge detector, etc.). However, our solution uses the commonly used Canny edge detector. How the Canny detector works is too detailed for this book, but there is one point that we need to address. The Canny detector requires two parameters denoting low and high gradient threshold values. Potential edge pixels between the low and high thresholds are considered weak edge pixels, while those above the high threshold are considered strong edge pixels. OpenCV’s <code>Canny</code> method includes the low and high thresholds as required parameters. In our solution, we set the lower and upper thresholds to be one standard deviation below and above the image’s median pixel intensity. However, we often get better results if we determine a good pair of low and high threshold values through manual trial and error on a few images before running <code>Canny</code> on our entire collection of images.<a data-primary="" data-startref="ix_canny_edge" data-type="indexterm" id="id1419"/><a data-primary="" data-startref="ix_edge_det_image" data-type="indexterm" id="id1420"/><a data-primary="" data-startref="ix_image_edge_det" data-type="indexterm" id="id1421"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1422">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/gG9xo">Canny Edge Detector, Wikipedia</a></p>
</li>
<li>
<p><a href="https://oreil.ly/YvjM5">Canny Edge Detection Auto Thresholding</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="8.12 Detecting Corners" data-type="sect1"><div class="sect1" id="detecting-corners">
<h1>8.12 Detecting Corners</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id218">
<h2>Problem</h2>
<p>You want to detect the <a data-primary="images" data-secondary="corner detection" data-type="indexterm" id="ix_image_corner_det"/><a data-primary="corner detection, images" data-type="indexterm" id="ix_corner_det"/>corners in an image.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id219">
<h2>Solution</h2>
<p>Use OpenCV’s implementation of the <a data-primary="cornerHarris" data-type="indexterm" id="ix_corner_harris"/><a data-primary="Harris corner detector" data-type="indexterm" id="ix_harris_corner_det"/>Harris corner detector,
<code>cornerHarris</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">cv2</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">matplotlib</code> <code class="kn">import</code> <code class="n">pyplot</code> <code class="k">as</code> <code class="n">plt</code>

<code class="c1"># Load image</code>
<code class="n">image_bgr</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">imread</code><code class="p">(</code><code class="s2">"images/plane_256x256.jpg"</code><code class="p">)</code>
<code class="n">image_gray</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">cvtColor</code><code class="p">(</code><code class="n">image_bgr</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">COLOR_BGR2GRAY</code><code class="p">)</code>
<code class="n">image_gray</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">float32</code><code class="p">(</code><code class="n">image_gray</code><code class="p">)</code>

<code class="c1"># Set corner detector parameters</code>
<code class="n">block_size</code> <code class="o">=</code> <code class="mi">2</code>
<code class="n">aperture</code> <code class="o">=</code> <code class="mi">29</code>
<code class="n">free_parameter</code> <code class="o">=</code> <code class="mf">0.04</code>

<code class="c1"># Detect corners</code>
<code class="n">detector_responses</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">cornerHarris</code><code class="p">(</code><code class="n">image_gray</code><code class="p">,</code>
                                      <code class="n">block_size</code><code class="p">,</code>
                                      <code class="n">aperture</code><code class="p">,</code>
                                      <code class="n">free_parameter</code><code class="p">)</code>

<code class="c1"># Large corner markers</code>
<code class="n">detector_responses</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">dilate</code><code class="p">(</code><code class="n">detector_responses</code><code class="p">,</code> <code class="kc">None</code><code class="p">)</code>

<code class="c1"># Only keep detector responses greater than threshold, mark as white</code>
<code class="n">threshold</code> <code class="o">=</code> <code class="mf">0.02</code>
<code class="n">image_bgr</code><code class="p">[</code><code class="n">detector_responses</code> <code class="o">&gt;</code>
          <code class="n">threshold</code> <code class="o">*</code>
          <code class="n">detector_responses</code><code class="o">.</code><code class="n">max</code><code class="p">()]</code> <code class="o">=</code> <code class="p">[</code><code class="mi">255</code><code class="p">,</code><code class="mi">255</code><code class="p">,</code><code class="mi">255</code><code class="p">]</code>

<code class="c1"># Convert to grayscale</code>
<code class="n">image_gray</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">cvtColor</code><code class="p">(</code><code class="n">image_bgr</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">COLOR_BGR2GRAY</code><code class="p">)</code>

<code class="c1"># Show image</code>
<code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image_gray</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s2">"gray"</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 08in19" height="218" src="assets/mpc2_08in19.png" width="218"/>
<h6/>
</div></figure>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id220">
<h2>Discussion</h2>
<p>The <em>Harris corner detector</em> is a commonly used method of detecting the
intersection of two edges. Our interest in detecting corners is
motivated by the same reason as for detecting edges: corners are
points of high information. A complete explanation of the Harris corner detector is available in the external resources at the end of this recipe, but a simplified explanation is that it looks for windows (also called <em>neighborhoods</em> or <em>patches</em>) where small movements of the window (imagine shaking the window) create big changes in the contents of the pixels inside the window. <code>cornerHarris</code> contains three important parameters that we can use to control the edges detected. First, <code>block_size</code> is the size of the neighbor around each pixel used for corner detection. Second, <code>aperture</code> is the size of the Sobel kernel used (don’t worry if you don’t know what that is), and finally there is a free parameter where larger values correspond to identifying softer corners.</p>
<p>The output is a grayscale image depicting potential corners:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Show potential corners</code>
<code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">detector_responses</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'gray'</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 08in20" height="218" src="assets/mpc2_08in20.png" width="218"/>
<h6/>
</div></figure>
<p>We then apply thresholding to keep only the most likely corners. Alternatively, we can use a similar detector, the <a data-primary="Shi-Tomasi corner detector" data-type="indexterm" id="id1423"/>Shi-Tomasi corner detector, which works in a similar way to the Harris <a data-primary="goodFeaturesToTrack" data-type="indexterm" id="id1424"/>detector (<code>goodFeaturesToTrack</code>) to identify a fixed number of strong corners. <code>goodFeaturesToTrack</code> takes three major parameters—​the number 
<span class="keep-together">of corners</span> to detect, the minimum quality of the corner (0 to 1), and the minimum Euclidean distance between corners:<a data-primary="" data-startref="ix_corner_harris" data-type="indexterm" id="id1425"/><a data-primary="" data-startref="ix_harris_corner_det" data-type="indexterm" id="id1426"/><a data-primary="" data-startref="ix_corner_det" data-type="indexterm" id="id1427"/><a data-primary="" data-startref="ix_image_corner_det" data-type="indexterm" id="id1428"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load images</code>
<code class="n">image_bgr</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">imread</code><code class="p">(</code><code class="s1">'images/plane_256x256.jpg'</code><code class="p">)</code>
<code class="n">image_gray</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">cvtColor</code><code class="p">(</code><code class="n">image_bgr</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">COLOR_BGR2GRAY</code><code class="p">)</code>

<code class="c1"># Number of corners to detect</code>
<code class="n">corners_to_detect</code> <code class="o">=</code> <code class="mi">10</code>
<code class="n">minimum_quality_score</code> <code class="o">=</code> <code class="mf">0.05</code>
<code class="n">minimum_distance</code> <code class="o">=</code> <code class="mi">25</code>

<code class="c1"># Detect corners</code>
<code class="n">corners</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">goodFeaturesToTrack</code><code class="p">(</code><code class="n">image_gray</code><code class="p">,</code>
                                  <code class="n">corners_to_detect</code><code class="p">,</code>
                                  <code class="n">minimum_quality_score</code><code class="p">,</code>
                                  <code class="n">minimum_distance</code><code class="p">)</code>
<code class="n">corners</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">int16</code><code class="p">(</code><code class="n">corners</code><code class="p">)</code>

<code class="c1"># Draw white circle at each corner</code>
<code class="k">for</code> <code class="n">corner</code> <code class="ow">in</code> <code class="n">corners</code><code class="p">:</code>
    <code class="n">x</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">corner</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
    <code class="n">cv2</code><code class="o">.</code><code class="n">circle</code><code class="p">(</code><code class="n">image_bgr</code><code class="p">,</code> <code class="p">(</code><code class="n">x</code><code class="p">,</code><code class="n">y</code><code class="p">),</code> <code class="mi">10</code><code class="p">,</code> <code class="p">(</code><code class="mi">255</code><code class="p">,</code><code class="mi">255</code><code class="p">,</code><code class="mi">255</code><code class="p">),</code> <code class="o">-</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Convert to grayscale</code>
<code class="n">image_rgb</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">cvtColor</code><code class="p">(</code><code class="n">image_bgr</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">COLOR_BGR2GRAY</code><code class="p">)</code>

<code class="c1"># Show image</code>
<code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image_rgb</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'gray'</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 08in21" height="218" src="assets/mpc2_08in21.png" width="218"/>
<h6/>
</div></figure>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1429">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/vLMBj">OpenCV’s cornerHarris</a></p>
</li>
<li>
<p><a href="https://oreil.ly/Ra-x6">OpenCV’s goodFeaturesToTrack</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="8.13 Creating Features for Machine Learning" data-type="sect1"><div class="sect1" id="creating-features-for-machine-learning">
<h1>8.13 Creating Features for Machine Learning</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id495">
<h2>Problem</h2>
<p>You want to <a data-primary="images" data-secondary="converting to observation for ML" data-type="indexterm" id="ix_image_conv_observ"/><a data-primary="observations" data-secondary="converting images to" data-type="indexterm" id="ix_observ_conv_image"/>convert an image into an observation for machine learning.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id496">
<h2>Solution</h2>
<p>Use <a data-primary="flatten method" data-secondary="NumPy array" data-type="indexterm" id="ix_flatten_numpy"/>NumPy’s <code>flatten</code> to convert the multidimensional array containing image data into a vector containing the observation’s values:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">cv2</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">matplotlib</code> <code class="kn">import</code> <code class="n">pyplot</code> <code class="k">as</code> <code class="n">plt</code>

<code class="c1"># Load image as grayscale</code>
<code class="n">image</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">imread</code><code class="p">(</code><code class="s2">"images/plane_256x256.jpg"</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">IMREAD_GRAYSCALE</code><code class="p">)</code>

<code class="c1"># Resize image to 10 pixels by 10 pixels</code>
<code class="n">image_10x10</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">resize</code><code class="p">(</code><code class="n">image</code><code class="p">,</code> <code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">10</code><code class="p">))</code>

<code class="c1"># Convert image data to one-dimensional vector</code>
<code class="n">image_10x10</code><code class="o">.</code><code class="n">flatten</code><code class="p">()</code></pre>
<pre data-type="programlisting">array([133, 130, 130, 129, 130, 129, 129, 128, 128, 127, 135, 131, 131,
       131, 130, 130, 129, 128, 128, 128, 134, 132, 131, 131, 130, 129,
       129, 128, 130, 133, 132, 158, 130, 133, 130,  46,  97,  26, 132,
       143, 141,  36,  54,  91,   9,   9,  49, 144, 179,  41, 142,  95,
        32,  36,  29,  43, 113, 141, 179, 187, 141, 124,  26,  25, 132,
       135, 151, 175, 174, 184, 143, 151,  38, 133, 134, 139, 174, 177,
       169, 174, 155, 141, 135, 137, 137, 152, 169, 168, 168, 179, 152,
       139, 136, 135, 137, 143, 159, 166, 171, 175], dtype=uint8)</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id221">
<h2>Discussion</h2>
<p>Images are presented as a grid of pixels. If an image is in grayscale,
each pixel is presented by one value (i.e., pixel intensity is <code>1</code> if
white, <code>0</code> if black). For example, imagine we have a 10	× 10–pixel
image:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image_10x10</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s2">"gray"</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 08in22" height="218" src="assets/mpc2_08in22.png" width="218"/>
<h6/>
</div></figure>
<p>In this case, the dimensions of the image’s data will be 10	× 10:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">image_10x10</code><code class="o">.</code><code class="n">shape</code></pre>
<pre data-type="programlisting">(10, 10)</pre>
<p>And if we flatten the array, we get a vector of length 100 (10
multiplied by 10):</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">image_10x10</code><code class="o">.</code><code class="n">flatten</code><code class="p">()</code><code class="o">.</code><code class="n">shape</code></pre>
<pre data-type="programlisting">(100,)</pre>
<p>This is the feature data for our image that can be joined with the
vectors from other images to create the data we will feed to our machine
learning algorithms.</p>
<p>If the image is in color, instead of each pixel being represented by one
value, it is represented by multiple values (most often three)
representing the channels (red, green, blue, etc.) that blend to
make the final color of that pixel. For this reason, if our 10	× 10 image is in color, we will have 300 feature values for each observation:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load image in color</code>
<code class="n">image_color</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">imread</code><code class="p">(</code><code class="s2">"images/plane_256x256.jpg"</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">IMREAD_COLOR</code><code class="p">)</code>

<code class="c1"># Resize image to 10 pixels by 10 pixels</code>
<code class="n">image_color_10x10</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">resize</code><code class="p">(</code><code class="n">image_color</code><code class="p">,</code> <code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">10</code><code class="p">))</code>

<code class="c1"># Convert image data to one-dimensional vector, show dimensions</code>
<code class="n">image_color_10x10</code><code class="o">.</code><code class="n">flatten</code><code class="p">()</code><code class="o">.</code><code class="n">shape</code></pre>
<pre data-type="programlisting">(300,)</pre>
<p>One of the major challenges of image processing and computer vision is
that since every pixel location in a collection of images is a feature,
as the images get larger, the number of features explodes:</p>
<pre class="less_space pagebreak-before" data-code-language="python" data-type="programlisting"><code class="c1"># Load image in grayscale</code>
<code class="n">image_256x256_gray</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">imread</code><code class="p">(</code><code class="s2">"images/plane_256x256.jpg"</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">IMREAD_GRAYSCALE</code><code class="p">)</code>

<code class="c1"># Convert image data to one-dimensional vector, show dimensions</code>
<code class="n">image_256x256_gray</code><code class="o">.</code><code class="n">flatten</code><code class="p">()</code><code class="o">.</code><code class="n">shape</code></pre>
<pre data-type="programlisting">(65536,)</pre>
<p>And the number of features grows even larger when the image is in color:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load image in color</code>
<code class="n">image_256x256_color</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">imread</code><code class="p">(</code><code class="s2">"images/plane_256x256.jpg"</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">IMREAD_COLOR</code><code class="p">)</code>

<code class="c1"># Convert image data to one-dimensional vector, show dimensions</code>
<code class="n">image_256x256_color</code><code class="o">.</code><code class="n">flatten</code><code class="p">()</code><code class="o">.</code><code class="n">shape</code></pre>
<pre data-type="programlisting">(196608,)</pre>
<p>As the output shows, even a small color image has almost 200,000
features, which can cause problems when we are training our models because the number of features might far exceed the number of observations.</p>
<p>This problem will motivate dimensionality strategies discussed in a
later chapter, which attempt to reduce the number of features while not
losing an excessive amount of information contained in the data.<a data-primary="" data-startref="ix_flatten_numpy" data-type="indexterm" id="id1430"/><a data-primary="" data-startref="ix_image_conv_observ" data-type="indexterm" id="id1431"/><a data-primary="" data-startref="ix_observ_conv_image" data-type="indexterm" id="id1432"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="8.14 Encoding Color Histograms as Features" data-type="sect1"><div class="sect1" id="encoding-color-histograms-as-features">
<h1>8.14 Encoding Color Histograms as Features</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id1433">
<h2>Problem</h2>
<p>You want to create a set of features representing the colors appearing
in an image.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id222">
<h2>Solution</h2>
<p>Compute the <a data-primary="images" data-secondary="encoding color histograms as features" data-type="indexterm" id="ix_image_color_hist_encode"/><a data-primary="color histograms encoded as features" data-type="indexterm" id="ix_color_hist_encode"/>histograms for each color channel:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">cv2</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">matplotlib</code> <code class="kn">import</code> <code class="n">pyplot</code> <code class="k">as</code> <code class="n">plt</code>

<code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Load image</code>
<code class="n">image_bgr</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">imread</code><code class="p">(</code><code class="s2">"images/plane_256x256.jpg"</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">IMREAD_COLOR</code><code class="p">)</code>

<code class="c1"># Convert to RGB</code>
<code class="n">image_rgb</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">cvtColor</code><code class="p">(</code><code class="n">image_bgr</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">COLOR_BGR2RGB</code><code class="p">)</code>

<code class="c1"># Create a list for feature values</code>
<code class="n">features</code> <code class="o">=</code> <code class="p">[]</code>

<code class="c1"># Calculate the histogram for each color channel</code>
<code class="n">colors</code> <code class="o">=</code> <code class="p">(</code><code class="s2">"r"</code><code class="p">,</code><code class="s2">"g"</code><code class="p">,</code><code class="s2">"b"</code><code class="p">)</code>

<code class="c1"># For each channel: calculate histogram and add to feature value list</code>
<code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">channel</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">colors</code><code class="p">):</code>
    <code class="n">histogram</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">calcHist</code><code class="p">([</code><code class="n">image_rgb</code><code class="p">],</code> <code class="c1"># Image</code>
                        <code class="p">[</code><code class="n">i</code><code class="p">],</code> <code class="c1"># Index of channel</code>
                        <code class="kc">None</code><code class="p">,</code> <code class="c1"># No mask</code>
                        <code class="p">[</code><code class="mi">256</code><code class="p">],</code> <code class="c1"># Histogram size</code>
                        <code class="p">[</code><code class="mi">0</code><code class="p">,</code><code class="mi">256</code><code class="p">])</code> <code class="c1"># Range</code>
    <code class="n">features</code><code class="o">.</code><code class="n">extend</code><code class="p">(</code><code class="n">histogram</code><code class="p">)</code>

<code class="c1"># Create a vector for an observation's feature values</code>
<code class="n">observation</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">features</code><code class="p">)</code><code class="o">.</code><code class="n">flatten</code><code class="p">()</code>

<code class="c1"># Show the observation's value for the first five features</code>
<code class="n">observation</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="mi">5</code><code class="p">]</code></pre>
<pre data-type="programlisting">array([ 1008.,   217.,   184.,   165.,   116.], dtype=float32)</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id223">
<h2>Discussion</h2>
<p>In the RGB color model, each color is the combination of three color
channels (i.e., red, green, blue). In turn, each channel can take on one of 256 values (represented by an integer between 0 and 255). For
example, the top leftmost pixel in our image has the following channel
values:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Show RGB channel values</code>
<code class="n">image_rgb</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code><code class="mi">0</code><code class="p">]</code></pre>
<pre data-type="programlisting">array([107, 163, 212], dtype=uint8)</pre>
<p class="fix_tracking2">
A histogram is a representation of the distribution of values in data.
Here’s a simple example:
</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import pandas</code>
<code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>

<code class="c1"># Create some data</code>
<code class="n">data</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">])</code>

<code class="c1"># Show the histogram</code>
<code class="n">data</code><code class="o">.</code><code class="n">hist</code><code class="p">(</code><code class="n">grid</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 08in23" height="391" src="assets/mpc2_08in23.png" width="529"/>
<h6/>
</div></figure>
<p>In this example, we have some data with two <code>1</code>s, two <code>2</code>s, three
<code>3</code>s, one <code>4</code>, and one <code>5</code>. In the histogram, each bar represents the number of times each value (<code>1</code>, <code>2</code>, etc.) appears in our data.</p>
<p>We can apply this same technique to each of the color channels, but
instead of five possible values, we have 256 (the number of possible
values for a channel value). The x-axis represents the 256 possible
channel values, and the y-axis represents the number of times a particular channel value appears across all pixels in an image (apologies to hardcopy readers for whom there are no color images):</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Calculate the histogram for each color channel</code>
<code class="n">colors</code> <code class="o">=</code> <code class="p">(</code><code class="s2">"r"</code><code class="p">,</code><code class="s2">"g"</code><code class="p">,</code><code class="s2">"b"</code><code class="p">)</code>

<code class="c1"># For each channel: calculate histogram, make plot</code>
<code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">channel</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">colors</code><code class="p">):</code>
    <code class="n">histogram</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">calcHist</code><code class="p">([</code><code class="n">image_rgb</code><code class="p">],</code> <code class="c1"># Image</code>
                        <code class="p">[</code><code class="n">i</code><code class="p">],</code> <code class="c1"># Index of channel</code>
                        <code class="kc">None</code><code class="p">,</code> <code class="c1"># No mask</code>
                        <code class="p">[</code><code class="mi">256</code><code class="p">],</code> <code class="c1"># Histogram size</code>
                        <code class="p">[</code><code class="mi">0</code><code class="p">,</code><code class="mi">256</code><code class="p">])</code> <code class="c1"># Range</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">histogram</code><code class="p">,</code> <code class="n">color</code> <code class="o">=</code> <code class="n">channel</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">xlim</code><code class="p">([</code><code class="mi">0</code><code class="p">,</code><code class="mi">256</code><code class="p">])</code>

<code class="c1"># Show plot</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 08in24" height="391" src="assets/mpc2_08in24.png" width="542"/>
<h6/>
</div></figure>
<p>As we can see in the histogram, barely any pixels contain the blue channel values between 0 and ~180, while many pixels contain blue channel values between ~190 and ~210. This distribution of channel values is shown for all three channels. The histogram, however, is not simply a visualization; it has 256 features for each color channel, making for 768 total features representing the distribution of colors in an image.<a data-primary="" data-startref="ix_color_hist_encode" data-type="indexterm" id="id1434"/><a data-primary="" data-startref="ix_image_color_hist_encode" data-type="indexterm" id="id1435"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1436">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/nPbJT">Histogram, Wikipedia</a></p>
</li>
<li>
<p><a href="https://oreil.ly/h60M5">pandas documentation: Histogram</a></p>
</li>
<li>
<p><a href="https://oreil.ly/BuX1C">OpenCV tutorial: Histogram</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="8.15 Using Pretrained Embeddings as Features" data-type="sect1"><div class="sect1" id="using-pretrained-embeddings-as-a-feature">
<h1>8.15 Using Pretrained Embeddings as Features</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id224">
<h2>Problem</h2>
<p>You want to load pretrained <a data-primary="images" data-secondary="pretrained embeddings as features" data-type="indexterm" id="ix_image_pretrain_embed"/><a data-primary="pretrained models" data-secondary="image embeddings" data-type="indexterm" id="ix_pretrain_image_embed"/><a data-primary="embeddings" data-type="indexterm" id="ix_embed_ch8"/>embeddings from an existing model in PyTorch and use them as input to one of your own models.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id497">
<h2>Solution</h2>
<p>Use <code>torchvision.models</code> to select a <a data-primary="PyTorch" data-secondary="pretrained embeddings" data-type="indexterm" id="ix_pytorch_embed"/>model and then retrieve an embedding from it for a given image:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">cv2</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">from</code> <code class="nn">torchvision</code> <code class="kn">import</code> <code class="n">transforms</code>
<code class="kn">import</code> <code class="nn">torchvision.models</code> <code class="k">as</code> <code class="nn">models</code>

<code class="c1"># Load image</code>
<code class="n">image_bgr</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">imread</code><code class="p">(</code><code class="s2">"images/plane.jpg"</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">IMREAD_COLOR</code><code class="p">)</code>

<code class="c1"># Convert to pytorch data type</code>
<code class="n">convert_tensor</code> <code class="o">=</code> <code class="n">transforms</code><code class="o">.</code><code class="n">ToTensor</code><code class="p">()</code>
<code class="n">pytorch_image</code> <code class="o">=</code> <code class="n">convert_tensor</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">image_rgb</code><code class="p">))</code>

<code class="c1"># Load the pretrained model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">models</code><code class="o">.</code><code class="n">resnet18</code><code class="p">(</code><code class="n">pretrained</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>

<code class="c1"># Select the specific layer of the model we want output from</code>
<code class="n">layer</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">_modules</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s1">'avgpool'</code><code class="p">)</code>

<code class="c1"># Set model to evaluation mode</code>
<code class="n">model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code>

<code class="c1"># Infer the embedding with the no_grad option</code>
<code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
    <code class="n">embedding</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">pytorch_image</code><code class="o">.</code><code class="n">unsqueeze</code><code class="p">(</code><code class="mi">0</code><code class="p">))</code>

<code class="nb">print</code><code class="p">(</code><code class="n">embedding</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code></pre>
<pre data-type="programlisting">torch.Size([1, 1000])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id225">
<h2>Discussion</h2>
<p>In the ML space, <em>transfer learning</em> is often <a data-primary="transfer learning" data-type="indexterm" id="id1437"/>defined as taking information learned from one task and using it as input to another task. Instead of starting from zero, we can use representations already learned from large pretrained image models (such as ResNet) to get a head start on our own machine learning models. More intuitively, you can understand how we could use the weights of a model trained to recognize cats as a good start for a model we want to train to recognize dogs. By sharing information form one model to another, we can leverage the information learned from other datasets and model architectures without the overhead of training a model from scratch.</p>
<p>The entire application of transfer learning in computer vision is outside the scope of this book; however, there are many different ways we can extract embeddings-based representations of images outside of PyTorch. In <a data-primary="TensorFlow" data-type="indexterm" id="id1438"/><a data-primary="tensorflow_hub" data-type="indexterm" id="id1439"/>TensorFlow, another common library for deep learning, we can use <code>tensorflow_hub</code>:<a data-primary="" data-startref="ix_embed_ch8" data-type="indexterm" id="id1440"/><a data-primary="" data-startref="ix_image_pretrain_embed" data-type="indexterm" id="id1441"/><a data-primary="" data-startref="pretrained embeddings as features" data-type="indexterm" id="id1442"/><a data-primary="" data-startref="ix_pytorch_embed" data-type="indexterm" id="id1443"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">cv2</code>
<code class="kn">import</code> <code class="nn">tensorflow</code> <code class="k">as</code> <code class="nn">tf</code>
<code class="kn">import</code> <code class="nn">tensorflow_hub</code> <code class="k">as</code> <code class="nn">hub</code>

<code class="c1"># Load image</code>
<code class="n">image_bgr</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">imread</code><code class="p">(</code><code class="s2">"images/plane.jpg"</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">IMREAD_COLOR</code><code class="p">)</code>
<code class="n">image_rgb</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">cvtColor</code><code class="p">(</code><code class="n">image_bgr</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">COLOR_BGR2RGB</code><code class="p">)</code>

<code class="c1"># Convert to tensorflow data type</code>
<code class="n">tf_image</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">image</code><code class="o">.</code><code class="n">convert_image_dtype</code><code class="p">([</code><code class="n">image_rgb</code><code class="p">],</code> <code class="n">tf</code><code class="o">.</code><code class="n">float32</code><code class="p">)</code>

<code class="c1"># Create the model and get embeddings using the inception V1 model</code>
<code class="n">embedding_model</code> <code class="o">=</code> <code class="n">hub</code><code class="o">.</code><code class="n">KerasLayer</code><code class="p">(</code>
    <code class="s2">"https://tfhub.dev/google/imagenet/inception_v1/feature_vector/5"</code>
<code class="p">)</code>
<code class="n">embeddings</code> <code class="o">=</code> <code class="n">embedding_model</code><code class="p">(</code><code class="n">tf_image</code><code class="p">)</code>

<code class="c1"># Print the shape of the embedding</code>
<code class="nb">print</code><code class="p">(</code><code class="n">embeddings</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code></pre>
<pre data-type="programlisting">(1, 1024)</pre>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1444">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/R8RTk">PyTorch tutorial: Transfer Learning for Computer Vision</a></p>
</li>
<li>
<p><a href="https://oreil.ly/iwHI6">TensorFlow Hub</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="8.16 Detecting Objects with OpenCV" data-type="sect1"><div class="sect1" id="detecting-objects-with-open-cv">
<h1>8.16 Detecting Objects with OpenCV</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id226">
<h2>Problem</h2>
<p>You want to <a data-primary="images" data-secondary="object detection" data-type="indexterm" id="ix_image_obj_det2"/><a data-primary="object detection" data-type="indexterm" id="ix_obj_det_image2"/>detect objects in images using pretrained cascade classifiers with <a data-primary="cross-validation (CV) of ML models" data-secondary="OpenCV" data-type="indexterm" id="ix_cross_val_cv_open2"/><a data-primary="OpenCV (Open Source Computer Vision Library)" data-type="indexterm" id="ix_open_cv2"/>OpenCV.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id227">
<h2>Solution</h2>
<p>Download and run one of <a data-primary="Haar cascade classifiers" data-type="indexterm" id="ix_haar_casc"/><a data-primary="classification and classifiers" data-secondary="Haar cascade classifiers" data-type="indexterm" id="ix_classif_haar_casc"/>OpenCV’s <a href="https://oreil.ly/XlXbm">Haar cascade classifiers</a>. In this case, we use a pretrained face detection model to detect and draw a rectangle around a face in an image:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import libraries</code>
<code class="kn">import</code> <code class="nn">cv2</code>
<code class="kn">from</code> <code class="nn">matplotlib</code> <code class="kn">import</code> <code class="n">pyplot</code> <code class="k">as</code> <code class="n">plt</code>

<code class="c1"># first run:</code>
<code class="c1"># mkdir models &amp;&amp; cd models</code>
<code class="c1"># wget https://tinyurl.com/mrc6jwhp</code>
<code class="n">face_cascade</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">CascadeClassifier</code><code class="p">()</code>
<code class="n">face_cascade</code><code class="o">.</code><code class="n">load</code><code class="p">(</code>
    <code class="n">cv2</code><code class="o">.</code><code class="n">samples</code><code class="o">.</code><code class="n">findFile</code><code class="p">(</code>
        <code class="s2">"models/haarcascade_frontalface_default.xml"</code>
    <code class="p">)</code>
<code class="p">)</code>

<code class="c1"># Load image</code>
<code class="n">image_bgr</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">imread</code><code class="p">(</code><code class="s2">"images/kyle_pic.jpg"</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">IMREAD_COLOR</code><code class="p">)</code>
<code class="n">image_rgb</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">cvtColor</code><code class="p">(</code><code class="n">image_bgr</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">COLOR_BGR2RGB</code><code class="p">)</code>

<code class="c1"># Detect faces and draw a rectangle</code>
<code class="n">faces</code> <code class="o">=</code> <code class="n">face_cascade</code><code class="o">.</code><code class="n">detectMultiScale</code><code class="p">(</code><code class="n">image_rgb</code><code class="p">)</code>
<code class="k">for</code> <code class="p">(</code><code class="n">x</code><code class="p">,</code><code class="n">y</code><code class="p">,</code><code class="n">w</code><code class="p">,</code><code class="n">h</code><code class="p">)</code> <code class="ow">in</code> <code class="n">faces</code><code class="p">:</code>
    <code class="n">cv2</code><code class="o">.</code><code class="n">rectangle</code><code class="p">(</code><code class="n">image_rgb</code><code class="p">,</code> <code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">),</code>
                      <code class="p">(</code><code class="n">x</code> <code class="o">+</code> <code class="n">h</code><code class="p">,</code> <code class="n">y</code> <code class="o">+</code> <code class="n">w</code><code class="p">),</code>
                      <code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">255</code><code class="p">,</code> <code class="mi">0</code><code class="p">),</code> <code class="mi">5</code><code class="p">)</code>

<code class="c1"># Show the image</code>
<code class="n">plt</code><code class="o">.</code><code class="n">subplot</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image_rgb</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 08in25" height="395" src="assets/mpc2_08in25.png" width="414"/>
<h6/>
</div></figure>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id228">
<h2>Discussion</h2>
<p><em>Haar cascade classifiers</em> are machine learning models used to learn a set of image features (specifically Haar features) that can be used to detect objects in images. The features themselves are simple rectangular features that are determined by calculating the difference in sums between rectangular regions. Subsequently, a gradient boosting algorithm is applied to learn the most important features and, finally, create a relatively strong model using cascading classifiers.</p>
<p>While the details of this process are outside the scope of this book, it’s noteworthy that these pretrained models can be easily downloaded from places such as the <a href="https://oreil.ly/273DA">OpenCV GitHub</a> as XML files and applied to images without training a model yourself. This is useful in cases where you want to add simple binary image features such as <code>contains_face</code> (or any other object) to your data.<a data-primary="" data-startref="ix_classif_haar_casc" data-type="indexterm" id="id1445"/><a data-primary="" data-startref="ix_cross_val_cv_open2" data-type="indexterm" id="id1446"/><a data-primary="" data-startref="ix_haar_casc" data-type="indexterm" id="id1447"/><a data-primary="" data-startref="ix_image_obj_det2" data-type="indexterm" id="id1448"/><a data-primary="" data-startref="ix_obj_det_image2" data-type="indexterm" id="id1449"/><a data-primary="" data-startref="ix_open_cv2" data-type="indexterm" id="id1450"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1451">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/dFhu6">OpenCV tutorial: Cascade Classifier</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="8.17 Classifying Images with Pytorch" data-type="sect1"><div class="sect1" id="classifying-images-with-pytorch">
<h1>8.17 Classifying Images with Pytorch</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id498">
<h2>Problem</h2>
<p>You want to <a data-primary="pretrained models" data-secondary="image classification fine-tuning" data-type="indexterm" id="ix_pretrain_image_class"/><a data-primary="image classification" data-secondary="fine-tuning pretrained model" data-type="indexterm" id="ix_image_class_pretrain"/><a data-primary="classification and classifiers" data-secondary="image classification" data-type="indexterm" id="ix_classif_image_class"/><a data-primary="PyTorch" data-secondary="image classification with" data-type="indexterm" id="ix_pyt_image_class"/>classify images using pretrained deep learning models in Pytorch.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id1452">
<h2>Solution</h2>
<p>Use <code>torchvision.models</code> to select a pretrained image classification model and feed the image through it:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">cv2</code>
<code class="kn">import</code> <code class="nn">json</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">from</code> <code class="nn">torchvision</code> <code class="kn">import</code> <code class="n">transforms</code>
<code class="kn">from</code> <code class="nn">torchvision.models</code> <code class="kn">import</code> <code class="n">resnet18</code>
<code class="kn">import</code> <code class="nn">urllib.request</code>

<code class="c1"># Get imagenet classes</code>
<code class="k">with</code> <code class="n">urllib</code><code class="o">.</code><code class="n">request</code><code class="o">.</code><code class="n">urlopen</code><code class="p">(</code>
    <code class="s2">"https://raw.githubusercontent.com/raghakot/keras-vis/master/resources/"</code>
    <code class="p">):</code>
    <code class="n">imagenet_class_index</code> <code class="o">=</code> <code class="n">json</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="n">url</code><code class="p">)</code>

<code class="c1"># Instantiate pretrained model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">resnet18</code><code class="p">(</code><code class="n">pretrained</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>

<code class="c1"># Load image</code>
<code class="n">image_bgr</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">imread</code><code class="p">(</code><code class="s2">"images/plane.jpg"</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">IMREAD_COLOR</code><code class="p">)</code>
<code class="n">image_rgb</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">cvtColor</code><code class="p">(</code><code class="n">image_bgr</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">COLOR_BGR2RGB</code><code class="p">)</code>

<code class="c1"># Convert to pytorch data type</code>
<code class="n">convert_tensor</code> <code class="o">=</code> <code class="n">transforms</code><code class="o">.</code><code class="n">ToTensor</code><code class="p">()</code>
<code class="n">pytorch_image</code> <code class="o">=</code> <code class="n">convert_tensor</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">image_rgb</code><code class="p">))</code>

<code class="c1"># Set model to evaluation mode</code>
<code class="n">model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code>

<code class="c1"># Make a prediction</code>
<code class="n">prediction</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">pytorch_image</code><code class="o">.</code><code class="n">unsqueeze</code><code class="p">(</code><code class="mi">0</code><code class="p">))</code>

<code class="c1"># Get the index of the highest predicted probability</code>
<code class="n">_</code><code class="p">,</code> <code class="n">index</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="n">prediction</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Convert that to a percentage value</code>
<code class="n">percentage</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">functional</code><code class="o">.</code><code class="n">softmax</code><code class="p">(</code><code class="n">prediction</code><code class="p">,</code> <code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)[</code><code class="mi">0</code><code class="p">]</code> <code class="o">*</code> <code class="mi">100</code>

<code class="c1"># Print the name of the item at the index along with the percent confidence</code>
<code class="nb">print</code><code class="p">(</code><code class="n">imagenet_class_index</code><code class="p">[</code><code class="nb">str</code><code class="p">(</code><code class="n">index</code><code class="o">.</code><code class="n">tolist</code><code class="p">()[</code><code class="mi">0</code><code class="p">])][</code><code class="mi">1</code><code class="p">],</code>
    <code class="n">percentage</code><code class="p">[</code><code class="n">index</code><code class="o">.</code><code class="n">tolist</code><code class="p">()[</code><code class="mi">0</code><code class="p">]]</code><code class="o">.</code><code class="n">item</code><code class="p">())</code></pre>
<pre data-type="programlisting">airship 6.0569939613342285</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id229">
<h2>Discussion</h2>
<p>Many pretrained deep learning models for image classification are easily available via both PyTorch and TensorFlow. In this example, we used ResNet18, a deep neural network architecture that was trained on the ImageNet dataset that is 18 layers deep. Deeper ResNet models, such as ResNet101 and ResNet152, are also available in Pytorch—and beyond that there are many other image models to choose from. Models trained on the ImageNet dataset are able to output predicted probabilities for all classes defined in the <code>imagenet_class_index</code> variable in the previous code snippet, which we downloaded from GitHub.</p>
<p>Like the facial recognition example in OpenCV (see <a data-type="xref" href="#detecting-objects-with-open-cv">Recipe 8.16</a>), we can use the predicted image classes as downstream features for future ML models or handy metadata tags that add more information to our images.<a data-primary="" data-startref="ix_image_class_ch8" data-type="indexterm" id="id1453"/><a data-primary="" data-startref="ix_classif_image_class" data-type="indexterm" id="id1454"/><a data-primary="" data-startref="ix_image_class_pretrain" data-type="indexterm" id="id1455"/><a data-primary="" data-startref="ix_pretrain_image_class" data-type="indexterm" id="id1456"/><a data-primary="" data-startref="ix_pyt_image_class" data-type="indexterm" id="id1457"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1458">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/MhlxR">PyTorch documentation: Models and Pre-trained Weights</a></p>
</li>
</ul>
</div></section>
</div></section>
</div></section></div></body></html>