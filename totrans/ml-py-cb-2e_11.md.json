["```py\n# Load libraries\nfrom sklearn import datasets\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\n# Load digits dataset\ndigits = datasets.load_digits()\n\n# Create features matrix\nfeatures = digits.data\n\n# Create target vector\ntarget = digits.target\n\n# Create standardizer\nstandardizer = StandardScaler()\n\n# Create logistic regression object\nlogit = LogisticRegression()\n\n# Create a pipeline that standardizes, then runs logistic regression\npipeline = make_pipeline(standardizer, logit)\n\n# Create k-fold cross-validation\nkf = KFold(n_splits=5, shuffle=True, random_state=0)\n\n# Conduct k-fold cross-validation\ncv_results = cross_val_score(pipeline, # Pipeline\n                             features, # Feature matrix\n                             target, # Target vector\n                             cv=kf, # Performance metric\n                             scoring=\"accuracy\", # Loss function\n                             n_jobs=-1) # Use all CPU cores\n\n# Calculate mean\ncv_results.mean()\n```", "```py\n0.969958217270195\n```", "```py\n# View score for all 5 folds\ncv_results\n```", "```py\narray([0.96111111, 0.96388889, 0.98050139, 0.97214485, 0.97214485])\n```", "```py\n# Import library\nfrom sklearn.model_selection import train_test_split\n\n# Create training and test sets\nfeatures_train, features_test, target_train, target_test = train_test_split(\n    features, target, test_size=0.1, random_state=1)\n\n# Fit standardizer to training set\nstandardizer.fit(features_train)\n\n# Apply to both training and test sets which can then be used to train models\nfeatures_train_std = standardizer.transform(features_train)\nfeatures_test_std = standardizer.transform(features_test)\n```", "```py\n# Create a pipeline\npipeline = make_pipeline(standardizer, logit)\n```", "```py\n# Do k-fold cross-validation\ncv_results = cross_val_score(pipeline, # Pipeline\n                             features, # Feature matrix\n                             target, # Target vector\n                             cv=kf, # Performance metric\n                             scoring=\"accuracy\", # Loss function\n                             n_jobs=-1) # Use all CPU cores\n```", "```py\n# Load libraries\nfrom sklearn.datasets import load_wine\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.model_selection import train_test_split\n\n# Load data\nwine = load_wine()\n\n# Create features\nfeatures, target = wine.data, wine.target\n\n# Make test and training split\nfeatures_train, features_test, target_train, target_test = train_test_split(\n    features, target, random_state=0)\n\n# Create a dummy regressor\ndummy = DummyRegressor(strategy='mean')\n\n# \"Train\" dummy regressor\ndummy.fit(features_train, target_train)\n\n# Get R-squared score\ndummy.score(features_test, target_test)\n```", "```py\n-0.0480213580840978\n```", "```py\n# Load library\nfrom sklearn.linear_model import LinearRegression\n\n# Train simple linear regression model\nols = LinearRegression()\nols.fit(features_train, target_train)\n\n# Get R-squared score\nols.score(features_test, target_test)\n```", "```py\n0.804353263176954\n```", "```py\n# Create dummy regressor that predicts 1s for everything\nclf = DummyRegressor(strategy='constant', constant=1)\nclf.fit(features_train, target_train)\n\n# Evaluate score\nclf.score(features_test, target_test)\n```", "```py\n-0.06299212598425186\n```", "```py\n# Load libraries\nfrom sklearn.datasets import load_iris\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Load data\niris = load_iris()\n\n# Create target vector and feature matrix\nfeatures, target = iris.data, iris.target\n\n# Split into training and test set\nfeatures_train, features_test, target_train, target_test = train_test_split(\n   features, target, random_state=0)\n\n# Create dummy classifier\ndummy = DummyClassifier(strategy='uniform', random_state=1)\n\n# \"Train\" model\ndummy.fit(features_train, target_train)\n\n# Get accuracy score\ndummy.score(features_test, target_test)\n```", "```py\n0.42105263157894735\n```", "```py\n# Load library\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create classifier\nclassifier = RandomForestClassifier()\n\n# Train model\nclassifier.fit(features_train, target_train)\n\n# Get accuracy score\nclassifier.score(features_test, target_test)\n```", "```py\n0.9736842105263158\n```", "```py\n# Load libraries\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_classification\n\n# Generate features matrix and target vector\nX, y = make_classification(n_samples = 10000,\n                           n_features = 3,\n                           n_informative = 3,\n                           n_redundant = 0,\n                           n_classes = 2,\n                           random_state = 1)\n\n# Create logistic regression\nlogit = LogisticRegression()\n\n# Cross-validate model using accuracy\ncross_val_score(logit, X, y, scoring=\"accuracy\")\n```", "```py\narray([0.9555, 0.95  , 0.9585, 0.9555, 0.956 ])\n```", "```py\n# Cross-validate model using precision\ncross_val_score(logit, X, y, scoring=\"precision\")\n```", "```py\narray([0.95963673, 0.94820717, 0.9635996 , 0.96149949, 0.96060606])\n```", "```py\n# Cross-validate model using recall\ncross_val_score(logit, X, y, scoring=\"recall\")\n```", "```py\narray([0.951, 0.952, 0.953, 0.949, 0.951])\n```", "```py\n# Cross-validate model using F1\ncross_val_score(logit, X, y, scoring=\"f1\")\n```", "```py\narray([0.95529884, 0.9500998 , 0.95827049, 0.95520886, 0.95577889])\n```", "```py\n# Load libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Create training and test split\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.1,\n                                                    random_state=1)\n\n# Predict values for training target vector\ny_hat = logit.fit(X_train, y_train).predict(X_test)\n\n# Calculate accuracy\naccuracy_score(y_test, y_hat)\n```", "```py\n0.947\n```", "```py\n# Load libraries\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\n# Create feature matrix and target vector\nfeatures, target = make_classification(n_samples=10000,\n                                       n_features=10,\n                                       n_classes=2,\n                                       n_informative=3,\n                                       random_state=3)\n\n# Split into training and test sets\nfeatures_train, features_test, target_train, target_test = train_test_split(\n    features, target, test_size=0.1, random_state=1)\n\n# Create classifier\nlogit = LogisticRegression()\n\n# Train model\nlogit.fit(features_train, target_train)\n\n# Get predicted probabilities\ntarget_probabilities = logit.predict_proba(features_test)[:,1]\n\n# Create true and false positive rates\nfalse_positive_rate, true_positive_rate, threshold = roc_curve(\n  target_test,\n  target_probabilities\n)\n\n# Plot ROC curve\nplt.title(\"Receiver Operating Characteristic\")\nplt.plot(false_positive_rate, true_positive_rate)\nplt.plot([0, 1], ls=\"--\")\nplt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlabel(\"False Positive Rate\")\nplt.show()\n```", "```py\n# Get predicted probabilities\nlogit.predict_proba(features_test)[0:1]\n```", "```py\narray([[0.86891533, 0.13108467]])\n```", "```py\nlogit.classes_\n```", "```py\narray([0, 1])\n```", "```py\nprint(\"Threshold:\", threshold[124])\nprint(\"True Positive Rate:\", true_positive_rate[124])\nprint(\"False Positive Rate:\", false_positive_rate[124])\n```", "```py\nThreshold: 0.5008252732632008\nTrue Positive Rate: 0.8346938775510204\nFalse Positive Rate: 0.1607843137254902\n```", "```py\nprint(\"Threshold:\", threshold[49])\nprint(\"True Positive Rate:\", true_positive_rate[49])\nprint(\"False Positive Rate:\", false_positive_rate[49])\n```", "```py\nThreshold: 0.8058575028551827\nTrue Positive Rate: 0.5653061224489796\nFalse Positive Rate: 0.052941176470588235\n```", "```py\n# Calculate area under curve\nroc_auc_score(target_test, target_probabilities)\n```", "```py\n0.9073389355742297\n```", "```py\n# Load libraries\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_classification\n\n# Generate features matrix and target vector\nfeatures, target = make_classification(n_samples = 10000,\n                           n_features = 3,\n                           n_informative = 3,\n                           n_redundant = 0,\n                           n_classes = 3,\n                           random_state = 1)\n\n# Create logistic regression\nlogit = LogisticRegression()\n\n# Cross-validate model using accuracy\ncross_val_score(logit, features, target, scoring='accuracy')\n```", "```py\narray([0.841 , 0.829 , 0.8265, 0.8155, 0.82  ])\n```", "```py\n# Cross-validate model using macro averaged F1 score\ncross_val_score(logit, features, target, scoring='f1_macro')\n```", "```py\narray([0.84061272, 0.82895312, 0.82625661, 0.81515121, 0.81992692])\n```", "```py\n# Load libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import datasets\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport pandas as pd\n\n# Load data\niris = datasets.load_iris()\n\n# Create features matrix\nfeatures = iris.data\n\n# Create target vector\ntarget = iris.target\n\n# Create list of target class names\nclass_names = iris.target_names\n\n# Create training and test set\nfeatures_train, features_test, target_train, target_test = train_test_split(\n    features, target, random_state=2)\n\n# Create logistic regression\nclassifier = LogisticRegression()\n\n# Train model and make predictions\ntarget_predicted = classifier.fit(features_train,\n    target_train).predict(features_test)\n\n# Create confusion matrix\nmatrix = confusion_matrix(target_test, target_predicted)\n\n# Create pandas dataframe\ndataframe = pd.DataFrame(matrix, index=class_names, columns=class_names)\n\n# Create heatmap\nsns.heatmap(dataframe, annot=True, cbar=None, cmap=\"Blues\")\nplt.title(\"Confusion Matrix\"), plt.tight_layout()\nplt.ylabel(\"True Class\"), plt.xlabel(\"Predicted Class\")\nplt.show()\n```", "```py\n# Load libraries\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\n\n# Generate features matrix, target vector\nfeatures, target = make_regression(n_samples = 100,\n                                   n_features = 3,\n                                   n_informative = 3,\n                                   n_targets = 1,\n                                   noise = 50,\n                                   coef = False,\n                                   random_state = 1)\n\n# Create a linear regression object\nols = LinearRegression()\n\n# Cross-validate the linear regression using (negative) MSE\ncross_val_score(ols, features, target, scoring='neg_mean_squared_error')\n```", "```py\narray([-1974.65337976, -2004.54137625, -3935.19355723, -1060.04361386,\n       -1598.74104702])\n```", "```py\n# Cross-validate the linear regression using R-squared\ncross_val_score(ols, features, target, scoring='r2')\n```", "```py\narray([0.8622399 , 0.85838075, 0.74723548, 0.91354743, 0.84469331])\n```", "```py\n# Load libraries\nimport numpy as np\nfrom sklearn.metrics import silhouette_score\nfrom sklearn import datasets\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\n# Generate features matrix\nfeatures, _ = make_blobs(n_samples = 1000,\n                         n_features = 10,\n                         centers = 2,\n                         cluster_std = 0.5,\n                         shuffle = True,\n                         random_state = 1)\n\n# Cluster data using k-means to predict classes\nmodel = KMeans(n_clusters=2, random_state=1).fit(features)\n\n# Get predicted classes\ntarget_predicted = model.labels_\n\n# Evaluate model\nsilhouette_score(features, target_predicted)\n```", "```py\n0.8916265564072141\n```", "```py\n# Load libraries\nfrom sklearn.metrics import make_scorer, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.datasets import make_regression\n\n# Generate features matrix and target vector\nfeatures, target = make_regression(n_samples = 100,\n                                   n_features = 3,\n                                   random_state = 1)\n\n# Create training set and test set\nfeatures_train, features_test, target_train, target_test = train_test_split(\n     features, target, test_size=0.10, random_state=1)\n\n# Create custom metric\ndef custom_metric(target_test, target_predicted):\n    # Calculate R-squared score\n    r2 = r2_score(target_test, target_predicted)\n    # Return R-squared score\n    return r2\n\n# Make scorer and define that higher scores are better\nscore = make_scorer(custom_metric, greater_is_better=True)\n\n# Create ridge regression object\nclassifier = Ridge()\n\n# Train ridge regression model\nmodel = classifier.fit(features_train, target_train)\n\n# Apply custom scorer\nscore(model, features_test, target_test)\n```", "```py\n0.9997906102882058\n```", "```py\n# Predict values\ntarget_predicted = model.predict(features_test)\n\n# Calculate R-squared score\nr2_score(target_test, target_predicted)\n```", "```py\n0.9997906102882058\n```", "```py\n# Load libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import learning_curve\n\n# Load data\ndigits = load_digits()\n\n# Create feature matrix and target vector\nfeatures, target = digits.data, digits.target\n\n# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(# Classifier\n                                                        RandomForestClassifier(),\n                                                        # Feature matrix\n                                                        features,\n                                                        # Target vector\n                                                        target,\n                                                        # Number of folds\n                                                        cv=10,\n                                                        # Performance metric\n                                                        scoring='accuracy',\n                                                        # Use all computer cores\n                                                        n_jobs=-1,\n                                                        # Sizes of 50\n                                                        # Training set\n                                                       train_sizes=np.linspace(\n                                                       0.01,\n                                                       1.0,\n                                                       50))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std,\n                 train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std,\n                 test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"),\nplt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()\n```", "```py\n# Load libraries\nfrom sklearn import datasets\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n# Load data\niris = datasets.load_iris()\n\n# Create features matrix\nfeatures = iris.data\n\n# Create target vector\ntarget = iris.target\n\n# Create list of target class names\nclass_names = iris.target_names\n\n# Create training and test set\nfeatures_train, features_test, target_train, target_test = train_test_split(\n    features, target, random_state=0)\n\n# Create logistic regression\nclassifier = LogisticRegression()\n\n# Train model and make predictions\nmodel = classifier.fit(features_train, target_train)\ntarget_predicted = model.predict(features_test)\n\n# Create a classification report\nprint(classification_report(target_test,\n                            target_predicted,\n                            target_names=class_names))\n```", "```py\n              precision    recall  f1-score   support\n\n      setosa       1.00      1.00      1.00        16\n  versicolor       1.00      0.91      0.95        11\n   virginica       0.92      1.00      0.96        11\n\n    accuracy                           0.97        38\n   macro avg       0.97      0.97      0.97        38\nweighted avg       0.98      0.97      0.97        38\n```", "```py\n# Load libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import load_digits\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import validation_curve\n\n# Load data\ndigits = load_digits()\n\n# Create feature matrix and target vector\nfeatures, target = digits.data, digits.target\n\n# Create range of values for parameter\nparam_range = np.arange(1, 250, 2)\n\n# Calculate accuracy on training and test set using range of parameter values\ntrain_scores, test_scores = validation_curve(\n    # Classifier\n    RandomForestClassifier(),\n    # Feature matrix\n    features,\n    # Target vector\n    target,\n    # Hyperparameter to examine\n    param_name=\"n_estimators\",\n    # Range of hyperparameter's values\n    param_range=param_range,\n    # Number of folds\n    cv=3,\n    # Performance metric\n    scoring=\"accuracy\",\n    # Use all computer cores\n    n_jobs=-1)\n\n# Calculate mean and standard deviation for training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Calculate mean and standard deviation for test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# Plot mean accuracy scores for training and test sets\nplt.plot(param_range, train_mean, label=\"Training score\", color=\"black\")\nplt.plot(param_range, test_mean, label=\"Cross-validation score\",\n         color=\"dimgrey\")\n\n# Plot accuracy bands for training and test sets\nplt.fill_between(param_range, train_mean - train_std,\n                 train_mean + train_std, color=\"gray\")\nplt.fill_between(param_range, test_mean - test_std,\n                 test_mean + test_std, color=\"gainsboro\")\n\n# Create plot\nplt.title(\"Validation Curve With Random Forest\")\nplt.xlabel(\"Number Of Trees\")\nplt.ylabel(\"Accuracy Score\")\nplt.tight_layout()\nplt.legend(loc=\"best\")\nplt.show()\n```"]