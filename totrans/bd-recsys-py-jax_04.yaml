- en: Chapter 3\. Mathematical Considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of this book is focused on implementation and on practical considerations
    necessary to get recommendation systems working. In this chapter, you’ll find
    the most abstract and theoretical concepts of the book. The purpose of this chapter
    is to cover a few of the essential ideas that undergird the field. It’s important
    to understand these ideas as they lead to pathological behavior in recommendation
    systems and motivate many architectural decisions.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start by discussing the shape of data you often see in recommendation
    systems, and why that shape can require careful thought. Next we’ll talk about
    the underlying mathematical idea, similarity, that drives most modern recommendation
    systems. We’ll briefly cover a different way of thinking about what a recommender
    does, for those with a more statistical inclination. Finally, we’ll use analogies
    to NLP to formulate the popular approach.
  prefs: []
  type: TYPE_NORMAL
- en: Zipf’s Laws in RecSys and the Matthew Effect
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a great many applications of ML, a caveat is given early: the distribution
    of observations of unique items from a large corpus is modeled by *Zipf’s law*—the
    frequency of occurrence drops exponentially. In recommendation systems, the *Matthew
    effect* appears in the popular item’s click rates or the popular user’s feedback
    rates. For example, popular items have dramatically larger click counts than average,
    and more-engaged users give far more ratings than average.'
  prefs: []
  type: TYPE_NORMAL
- en: The Matthew Effect
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Matthew effect—or *popularity bias*—states that the most popular items continue
    to attract the most attention and widen the gap with other items.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take, for example, the [MovieLens dataset](https://oreil.ly/xiUaq), an extremely
    popular dataset for benchmarking recommendation systems. [Jenny Sheng](https://oreil.ly/Uzm2G)
    observes the behavior shown in [Figure 3-1](#fig:movierank-zipfian) for a number
    of movie ratings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Movierank Zipfian](assets/brpj_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. Zipfian distribution of movie-rank ratings
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: At first glance, the rapid decline in ratings is obvious and stark, but is it
    a problem? Let’s assume our recommender will be built as a user-based collaborative
    filtering (CF) model—as alluded to in [Chapter 2](ch02.html#ch:user-item). Then
    how might these distributions affect the recommender?
  prefs: []
  type: TYPE_NORMAL
- en: 'We will consider the distributional ramifications of this phenomenon. Let the
    probability mass function be described by the simple Zipf’s law:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="f left-parenthesis k comma upper M right-parenthesis equals StartFraction
    1 slash k Over sigma-summation Underscript n equals 1 Overscript upper M Endscripts
    left-parenthesis 1 slash n right-parenthesis EndFraction" display="block"><mrow><mi>f</mi>
    <mrow><mo>(</mo> <mi>k</mi> <mo>,</mo> <mi>M</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mfrac><mrow><mn>1</mn><mo>/</mo><mi>k</mi></mrow> <mrow><msubsup><mo>∑</mo> <mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>M</mi></msubsup> <mrow><mo>(</mo><mn>1</mn><mo>/</mo><mi>n</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: For <math alttext="upper M"><mi>M</mi></math> number of tokens in the corpus
    (in our example, the number of movies), <math alttext="k"><mi>k</mi></math> is
    the rank of a token when sorted by number of occurrences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider users <math alttext="upper A"><mi>A</mi></math> and <math alttext="upper
    B"><mi>B</mi></math> , with <math alttext="upper N Subscript upper A Baseline
    equals StartAbsoluteValue script upper I Subscript upper A Baseline EndAbsoluteValue"><mrow><msub><mi>N</mi>
    <mi>A</mi></msub> <mo>=</mo> <mrow><mo>|</mo> <msub><mi>ℐ</mi> <mi>A</mi></msub>
    <mo>|</mo></mrow></mrow></math> and <math alttext="upper N Subscript upper B Baseline
    equals StartAbsoluteValue script upper I Subscript upper B Baseline EndAbsoluteValue"><mrow><msub><mi>N</mi>
    <mi>B</mi></msub> <mo>=</mo> <mrow><mo>|</mo> <msub><mi>ℐ</mi> <mi>B</mi></msub>
    <mo>|</mo></mrow></mrow></math> ratings, respectively. Observe that the probability
    of <math alttext="upper V Subscript i"><msub><mi>V</mi> <mi>i</mi></msub></math>
    , the <math alttext="i"><mi>i</mi></math> th most popular video, appearing in
    <math alttext="script upper I Subscript upper X"><msub><mi>ℐ</mi> <mi>X</mi></msub></math>
    for a user <math alttext="upper X"><mi>X</mi></math> is given by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper P left-parenthesis i right-parenthesis equals StartFraction
    f left-parenthesis i comma upper M right-parenthesis Over sigma-summation Underscript
    j equals 1 Overscript upper M Endscripts f left-parenthesis j comma upper M right-parenthesis
    EndFraction equals StartFraction 1 slash i Over sigma-summation Underscript j
    equals 1 Overscript upper M Endscripts 1 slash j EndFraction" display="block"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <mi>i</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>f</mi><mo>(</mo><mi>i</mi><mo>,</mo><mi>M</mi><mo>)</mo></mrow>
    <mrow><msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>M</mi></msubsup>
    <mi>f</mi><mrow><mo>(</mo><mi>j</mi><mo>,</mo><mi>M</mi><mo>)</mo></mrow></mrow></mfrac>
    <mo>=</mo> <mfrac><mrow><mn>1</mn><mo>/</mo><mi>i</mi></mrow> <mrow><msubsup><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>M</mi></msubsup> <mn>1</mn><mo>/</mo><mi>j</mi></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus the joint probability of an item appearing in two user’s ratings is shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper P left-parenthesis i squared right-parenthesis equals left-parenthesis
    StartFraction 1 slash i Over sigma-summation Underscript j equals 1 Overscript
    upper M Endscripts 1 slash j EndFraction right-parenthesis squared" display="block"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <msup><mi>i</mi> <mn>2</mn></msup> <mo>)</mo></mrow> <mo>=</mo>
    <msup><mfenced close=")" open="(" separators=""><mfrac><mrow><mn>1</mn><mo>/</mo><mi>i</mi></mrow>
    <mrow><msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>M</mi></msubsup>
    <mn>1</mn><mo>/</mo><mi>j</mi></mrow></mfrac></mfenced> <mn>2</mn></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: In words, the probability of two users sharing an item in their rating sets
    drops off with the square of its popularity rank.
  prefs: []
  type: TYPE_NORMAL
- en: This becomes important when we also consider that our, yet unstated, definition
    of user-based CF is based on similarity in users’ ratings sets. This similarity
    is *the number of jointly rated items by two users, divided by the total number
    of items rated by either.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking this definition, we can, for example, compute the similarity score for
    one shared item among <math alttext="upper A"><mi>A</mi></math> and <math alttext="upper
    B"><mi>B</mi></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="sigma-summation Underscript i equals 1 Overscript upper M Endscripts
    StartFraction upper P left-parenthesis i squared right-parenthesis Over double-vertical-bar
    script upper I Subscript upper A Baseline union script upper I Subscript upper
    B Baseline double-vertical-bar EndFraction" display="block"><mrow><munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>M</mi></munderover> <mfrac><mrow><mi>P</mi><mo>(</mo><msup><mi>i</mi>
    <mn>2</mn></msup> <mo>)</mo></mrow> <mrow><mrow><mo>∥</mo></mrow><msub><mi>ℐ</mi>
    <mi>A</mi></msub> <mo>∪</mo><msub><mi>ℐ</mi> <mi>B</mi></msub> <mrow><mo>∥</mo></mrow></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The average similarity score of two users is then generalized as follows via
    repeated application of the preceding equation:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="sigma-summation Underscript t equals 1 Overscript min left-parenthesis
    upper N Subscript upper A Baseline comma upper N Subscript upper B Baseline right-parenthesis
    Endscripts left-parenthesis product Underscript i Subscript k Baseline equals
    i Subscript k minus 1 Baseline plus 1 Overscript t minus 1 Endscripts sigma-summation
    Underscript i equals 1 Overscript upper M Endscripts left-parenthesis StartStartFraction
    upper P left-parenthesis i Subscript k Baseline Superscript 2 Baseline right-parenthesis
    OverOver StartFraction double-vertical-bar script upper I Subscript upper A Baseline
    union script upper I Subscript upper B Baseline double-vertical-bar Over t EndFraction
    EndEndFraction right-parenthesis right-parenthesis" display="block"><mrow><munderover><mo>∑</mo>
    <mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow> <mrow><mo form="prefix" movablelimits="true">min</mo><mo>(</mo><msub><mi>N</mi>
    <mi>A</mi></msub> <mo>,</mo><msub><mi>N</mi> <mi>B</mi></msub> <mo>)</mo></mrow></munderover>
    <mfenced close=")" open="(" separators=""><munderover><mo>∏</mo> <mrow><msub><mi>i</mi>
    <mi>k</mi></msub> <mo>=</mo><msub><mi>i</mi> <mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo><mn>1</mn></mrow> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></munderover>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>M</mi></munderover>
    <mfenced close=")" open="(" separators=""><mfrac><mrow><mi>P</mi><mo>(</mo><msup><mrow><msub><mi>i</mi>
    <mi>k</mi></msub></mrow> <mn>2</mn></msup> <mo>)</mo></mrow> <mfrac><mrow><mrow><mo>∥</mo></mrow><msub><mi>ℐ</mi>
    <mi>A</mi></msub> <mo>∪</mo><msub><mi>ℐ</mi> <mi>B</mi></msub> <mrow><mo>∥</mo></mrow></mrow>
    <mi>t</mi></mfrac></mfrac></mfenced></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: via repeated application of the preceding observation.
  prefs: []
  type: TYPE_NORMAL
- en: These combinatorial formulas not only indicate the relevance of the Zipfian
    in our algorithms, but we also see an almost direct effect on the output of scores.
    Consider the experiment in [“Quantitative Analysis of Matthew Effect and Sparsity
    Problem of Recommender Systems”](https://oreil.ly/m6iw7) by Hao Wang et al. on
    the [Last.fm dataset](https://oreil.ly/NqJOw). Last.fm is a music-listening tracker
    enabling users to keep track of all the songs they listen to; for Last.fm users,
    the authors demonstrate average similarity scores for pairs of users, and they
    find that this Matthew effect persists into the similarity matrix ([Figure 3-2](#fig:lastfm-matthew-effect)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Last.fm Matthew Effect](assets/brpj_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. Matthew effect as seen on the Last.fm dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Observe the radical difference between “hot” cells and all the others. The bright
    cells are few among the mostly dark, suggesting a difficult combination of some
    extremely popular items among the far more common frequency close to zero. While
    these results might seem scary, later we’ll consider diversity-aware loss functions
    that can mitigate the Matthew effect. A simpler way is to use downstream sampling
    methods, which we will discuss as part of our explore-exploit algorithms. Finally,
    the Matthew effect is only the first of two major impacts of this Zipfian; let’s
    turn our attention to the second.
  prefs: []
  type: TYPE_NORMAL
- en: Sparsity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We must now reckon with sparsity. As the ratings skew more and more toward
    the most popular items, the least popular items are starved for data and recommendations,
    which is called *data sparsity.* This connects to the linear-algebraic definition:
    mostly zeros or not populated elements in a vector. When you consider again our
    user-item matrix, less popular items constitute columns with few entries; these
    are sparse vectors. Similarly, at scale we see that the Matthew effect pushes
    more and more of the total ratings into certain columns, and the matrix becomes
    sparse in the traditional mathematical sense. For this reason, sparsity is an
    extremely well-known challenge for recommendation systems.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, let’s consider the implication on our CF algorithms from these sparse
    ratings. Again observe that the probability of <math alttext="upper V Subscript
    i"><msub><mi>V</mi> <mi>i</mi></msub></math> , the <math alttext="i"><mi>i</mi></math>
    th most popular item, appearing in <math alttext="script upper I Subscript upper
    X"><msub><mi>ℐ</mi> <mi>X</mi></msub></math> for a user <math alttext="upper X"><mi>X</mi></math>
    is given by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper P left-parenthesis i right-parenthesis equals StartFraction
    f left-parenthesis i comma upper M right-parenthesis Over sigma-summation Underscript
    j equals 1 Overscript upper M Endscripts f left-parenthesis j comma upper M right-parenthesis
    EndFraction equals StartFraction 1 slash i Over sigma-summation Underscript j
    equals 1 Overscript upper M Endscripts 1 slash j EndFraction" display="block"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <mi>i</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>f</mi><mo>(</mo><mi>i</mi><mo>,</mo><mi>M</mi><mo>)</mo></mrow>
    <mrow><msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>M</mi></msubsup>
    <mi>f</mi><mrow><mo>(</mo><mi>j</mi><mo>,</mo><mi>M</mi><mo>)</mo></mrow></mrow></mfrac>
    <mo>=</mo> <mfrac><mrow><mn>1</mn><mo>/</mo><mi>i</mi></mrow> <mrow><msubsup><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>M</mi></msubsup> <mn>1</mn><mo>/</mo><mi>j</mi></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Then
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="left-parenthesis upper M minus 1 right-parenthesis asterisk upper
    P left-parenthesis i right-parenthesis" display="block"><mrow><mo>(</mo> <mi>M</mi>
    <mo>-</mo> <mn>1</mn> <mo>)</mo> <mo>*</mo> <mi>P</mi> <mo>(</mo> <mi>i</mi> <mo>)</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'is the expected number of other users who click the <math alttext="i"><mi>i</mi></math>
    th most popular item, so summing over all, <math alttext="i"><mi>i</mi></math>
    yields the total number of other users who will share a rating with <math alttext="upper
    X"><mi>X</mi></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="sigma-summation Underscript i equals 1 Overscript upper M Endscripts
    left-parenthesis upper M minus 1 right-parenthesis asterisk upper P left-parenthesis
    i right-parenthesis" display="block"><mrow><munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>M</mi></munderover> <mrow><mo>(</mo> <mi>M</mi> <mo>-</mo> <mn>1</mn> <mo>)</mo></mrow>
    <mo>*</mo> <mi>P</mi> <mrow><mo>(</mo> <mi>i</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Again, as we pull back to the overall trends, we observe this sparsity sneaking
    into the actual computations for our CF algorithms, consider the trend of users
    of different ranks, and see how much their rankings are used to *collaborate*
    in other users’ rankings ([Figure 3-3](#fig:user-sim-counts)).
  prefs: []
  type: TYPE_NORMAL
- en: '![lastfm user similarity](assets/brpj_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. User similarity counts for the Last.fm dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We see that this is an important result to always be aware of: sparsity pushes
    emphasis onto the most popular users and has the risk of making your recommender
    myopic.'
  prefs: []
  type: TYPE_NORMAL
- en: Item-Based Collaborative Filtering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the equations are different, in this section, they apply similarly to
    item-based CF. Similarity in items exhibits the same inheritance of the Zipfian
    in their scores, and items consulted in the CF process drop off by rank.
  prefs: []
  type: TYPE_NORMAL
- en: User Similarity for Collaborative Filtering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In mathematics, it’s common to hear discussion of *distances*. Even back to
    the Pythagorean theorem, we are taught to think of relationships between points
    as distances or dissimilarity. Indeed, this fundamental idea is canonized in mathematics
    as part of the definition of a metric:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="d left-parenthesis a comma c right-parenthesis less-than-or-equal-to
    d left-parenthesis a comma b right-parenthesis plus d left-parenthesis b comma
    c right-parenthesis" display="block"><mrow><mi>d</mi> <mo>(</mo> <mi>a</mi> <mo>,</mo>
    <mi>c</mi> <mo>)</mo> <mo>≤</mo> <mi>d</mi> <mo>(</mo> <mi>a</mi> <mo>,</mo> <mi>b</mi>
    <mo>)</mo> <mo>+</mo> <mi>d</mi> <mo>(</mo> <mi>b</mi> <mo>,</mo> <mi>c</mi> <mo>)</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'In ML, we often instead concern ourselves with the notion of similarity—an
    extremely related topic. In many cases, we can compute similarity or dissimilarity,
    as they are complements of each other; when <math alttext="d colon upper X times
    upper X right-arrow left-bracket 0 comma 1 right-bracket subset-of double-struck
    upper R"><mrow><mi>d</mi> <mo>:</mo> <mi>X</mi> <mo>×</mo> <mi>X</mi> <mo>→</mo>
    <mo>[</mo> <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo>]</mo> <mo>⊂</mo> <mi>ℝ</mi></mrow></math>
    is a *dissimilarity function*, then we often define the following:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper S i m left-parenthesis a comma b right-parenthesis colon
    equals 1 minus d left-parenthesis a comma b right-parenthesis" display="block"><mrow><mi>S</mi>
    <mi>i</mi> <mi>m</mi> <mo>(</mo> <mi>a</mi> <mo>,</mo> <mi>b</mi> <mo>)</mo> <mo>:</mo>
    <mo>=</mo> <mn>1</mn> <mo>-</mo> <mi>d</mi> <mo>(</mo> <mi>a</mi> <mo>,</mo> <mi>b</mi>
    <mo>)</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: This may seem like a needlessly precise statement, but in fact you’ll see that
    a [variety of options are available for framing similarity](https://oreil.ly/9xAT6).
    Furthermore, sometimes we even formulate similarity measures where the associated
    distance measure does not establish a metric on the set of objects. These so-called
    pseudospaces can still be incredibly important, and we’ll show where they come
    up in [Chapter 10](ch10.html#ch10).
  prefs: []
  type: TYPE_NORMAL
- en: In the literature, you’ll find that papers commonly start by introducing a new
    similarity measure and then training a model you’ve seen before on that new measure.
    As you’ll see, the way you choose to relate objects (users, items, features, etc.)
    can have a large effect on what your algorithms learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, let’s laser in on some specific similarity measures. Consider a classic
    ML problem of clustering: we have a space (usually <math alttext="double-struck
    upper R Superscript n"><msup><mi>ℝ</mi> <mi>n</mi></msup></math> ) in which our
    data is represented and are asked to partition our data into subcollections of
    the population and assign these collections names. Frequently, these collections
    are intended to capture a certain meaning, or at the very least be useful for
    summarizing the collection elements’ features.'
  prefs: []
  type: TYPE_NORMAL
- en: When you do that clustering, you frequently are considering points near to one
    another in that space. Further, if you’re given a new observation and asked to
    assign it to a collection as an inference task, you normally compute the new observation’s
    *nearest neighbors*. This could be the *k*-nearest neighbors or simply the nearest
    neighbor among cluster centers; either way, your task is to use the notion of
    similarity to associate—and thus classify. In CF, this same notion is used to
    relate a user for whom you wish to make recommendations to those you already have
    data from.
  prefs: []
  type: TYPE_NORMAL
- en: Nearest Neighbors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Nearest neighbors* is a catchall term that arises from the simple geometric
    idea that, given some space (points defined by feature vectors) and a point in
    that space, you can find the other points closest to it. This has applications
    in all of ML, including classification, ranking/recommendation, and clustering.
    [“Approximate Nearest Neighbors”](ch06.html#Nearest-Neighbors) provides more details.'
  prefs: []
  type: TYPE_NORMAL
- en: So how can we define similarity for our users in CF? They’re not obviously in
    the same space, so our usual tools seem to be lacking.
  prefs: []
  type: TYPE_NORMAL
- en: Pearson Correlation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our original CF formulation indicated that users with similar tastes collaborate
    to recommend items for one another. Let two users <math alttext="upper A"><mi>A</mi></math>
    and <math alttext="upper B"><mi>B</mi></math> have a set of co-rated items—simply
    the set of items with ratings from each—written as <math alttext="script upper
    R Subscript upper A comma upper B"><msub><mi>ℛ</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub></math>
    , and a rating of item <math alttext="x"><mi>x</mi></math> by user <math alttext="upper
    A"><mi>A</mi></math> written as <math alttext="r Subscript upper A comma x"><msub><mi>r</mi>
    <mrow><mi>A</mi><mo>,</mo><mi>x</mi></mrow></msub></math> . Then the following
    is the sum of deviation from <math alttext="upper A"><mi>A</mi></math> ’s average
    rating over all of its co-rated items with <math alttext="upper B"><mi>B</mi></math>
    :'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="sigma-summation Underscript x element-of script upper R Subscript
    upper A comma upper B Endscripts left-parenthesis r Subscript upper A comma x
    Baseline minus r overbar Subscript upper A Baseline right-parenthesis" display="block"><mrow><munder><mo>∑</mo>
    <mrow><mi>x</mi><mo>∈</mo><msub><mi>ℛ</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub></mrow></munder>
    <mrow><mo>(</mo> <msub><mi>r</mi> <mrow><mi>A</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>-</mo> <msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>A</mi></msub>
    <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: If we think of these ratings as a random variable and consider the analog for
    <math alttext="upper B"><mi>B</mi></math> , the correlation between the jointly
    distributed variables (the population covariance) is our *Pearson correlation:*
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="USim Subscript upper A comma upper B Baseline equals StartFraction
    sigma-summation Underscript x element-of script upper R Subscript upper A comma
    upper B Baseline Endscripts left-parenthesis r Subscript upper A comma x Baseline
    minus r overbar Subscript upper A Baseline right-parenthesis left-parenthesis
    r Subscript upper B comma x Baseline minus r overbar Subscript upper B Baseline
    right-parenthesis Over StartRoot sigma-summation Underscript x element-of script
    upper R Subscript upper A comma upper B Baseline Endscripts left-parenthesis r
    Subscript upper A comma x Baseline minus r overbar Subscript upper A Baseline
    right-parenthesis squared EndRoot StartRoot sigma-summation Underscript x element-of
    script upper R Subscript upper A comma upper B Baseline Endscripts left-parenthesis
    r Subscript upper B comma x Baseline minus r overbar Subscript upper B Baseline
    right-parenthesis squared EndRoot EndFraction" display="block"><mrow><msub><mi>USim</mi>
    <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub> <mo>=</mo> <mfrac><mrow><msub><mo>∑</mo>
    <mrow><mi>x</mi><mo>∈</mo><msub><mi>ℛ</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub></mrow></msub>
    <mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>A</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>A</mi></msub>
    <mo>)</mo></mrow><mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>B</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>B</mi></msub>
    <mo>)</mo></mrow></mrow> <mrow><msqrt><mrow><msub><mo>∑</mo> <mrow><mi>x</mi><mo>∈</mo><msub><mi>ℛ</mi>
    <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub></mrow></msub> <msup><mrow><mo>(</mo><msub><mi>r</mi>
    <mrow><mi>A</mi><mo>,</mo><mi>x</mi></mrow></msub> <mo>-</mo><msub><mover accent="true"><mi>r</mi>
    <mo>¯</mo></mover> <mi>A</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt>
    <msqrt><mrow><msub><mo>∑</mo> <mrow><mi>x</mi><mo>∈</mo><msub><mi>ℛ</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub></mrow></msub>
    <msup><mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>B</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>B</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Keeping in mind a few details here is extremely important:'
  prefs: []
  type: TYPE_NORMAL
- en: This is the similarity of the jointly distributed variables describing the users’
    ratings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We compute this via all co-rated items, so user similarity is defined via item-ratings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is a pairwise similarity measure taking values in [–1,1] <math alttext="element-of
    double-struck upper R"><mrow><mo>∈</mo> <mi>ℝ</mi></mrow></math> .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correlation and Similarity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Part III](part03.html#ranking), you will learn about additional definitions
    of *correlation* and *similarity* that are more well suited for handling ranking
    data and that accommodate implicit rankings in particular.
  prefs: []
  type: TYPE_NORMAL
- en: Ratings via Similarity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we’ve introduced user similarity, let’s use it! For a user <math alttext="upper
    A"><mi>A</mi></math> and item <math alttext="x"><mi>x</mi></math> , we can estimate
    the rating via similar users’ ratings:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper A f f Subscript upper A comma i Baseline equals r overbar
    Subscript upper A Baseline plus StartFraction sigma-summation Underscript upper
    U element-of script upper N left-parenthesis upper A right-parenthesis Endscripts
    USim Subscript upper A comma upper U Baseline asterisk left-parenthesis r Subscript
    upper U comma i Baseline minus r overbar Subscript upper A Baseline right-parenthesis
    Over sigma-summation Underscript upper U element-of script upper N left-parenthesis
    upper A right-parenthesis Endscripts USim Subscript upper A comma upper U Baseline
    EndFraction" display="block"><mrow><mi>A</mi> <mi>f</mi> <msub><mi>f</mi> <mrow><mi>A</mi><mo>,</mo><mi>i</mi></mrow></msub>
    <mo>=</mo> <msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>A</mi></msub>
    <mo>+</mo> <mfrac><mrow><msub><mo>∑</mo> <mrow><mi>U</mi><mo>∈</mo><mi>𝒩</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow></msub>
    <msub><mi>USim</mi> <mrow><mi>A</mi><mo>,</mo><mi>U</mi></mrow></msub> <mo>*</mo><mrow><mo>(</mo><msub><mi>r</mi>
    <mrow><mi>U</mi><mo>,</mo><mi>i</mi></mrow></msub> <mo>-</mo><msub><mover accent="true"><mi>r</mi>
    <mo>¯</mo></mover> <mi>A</mi></msub> <mo>)</mo></mrow></mrow> <mrow><msub><mo>∑</mo>
    <mrow><mi>U</mi><mo>∈</mo><mi>𝒩</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow></msub>
    <msub><mi>USim</mi> <mrow><mi>A</mi><mo>,</mo><mi>U</mi></mrow></msub></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the prediction for user <math alttext="upper A"><mi>A</mi></math> ’s
    rating of item <math alttext="x"><mi>x</mi></math> , which takes <math alttext="upper
    A"><mi>A</mi></math> ’s average adjusted rating of the similarity-weighted average
    ratings of all of <math alttext="upper A"><mi>A</mi></math> ’s neighbors. In other
    words: <math alttext="upper A"><mi>A</mi></math> ’s rating will probably be the
    average of people who have ratings like <math alttext="upper A"><mi>A</mi></math>
    ’s rating, adjusted to how generous <math alttext="upper A"><mi>A</mi></math>
    is with ratings in general. We call this estimate the *user-item affinity score*.'
  prefs: []
  type: TYPE_NORMAL
- en: But wait! What’s <math alttext="script upper N left-parenthesis upper A right-parenthesis"><mrow><mi>𝒩</mi>
    <mo>(</mo> <mi>A</mi> <mo>)</mo></mrow></math> ? It’s the neighborhood of <math
    alttext="upper A"><mi>A</mi></math> , via our USim definition from the preceding
    section. The idea here is that we are aggregating ratings over the local region
    of users identified as similar to our target user by the previous USim metric.
    How many neighbors? How do you pick those neighbors? These will be the subject
    of later chapters; for now, assume they’re <math alttext="k"><mi>k</mi></math>
    -nearest neighbors and assume that some hyperparameter tuning is used to determine
    a good value for <math alttext="k"><mi>k</mi></math> .
  prefs: []
  type: TYPE_NORMAL
- en: Explore-Exploit as a Recommendation System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far we’ve presented two ideas, slightly in tension with each other:'
  prefs: []
  type: TYPE_NORMAL
- en: The MPIR, a simple, easy-to-understand recommender
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Matthew effect in recommendation systems and its runaway behavior in distributions
    of ratings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By now, you likely realize that the MPIR will amplify the Matthew effect and
    that the Matthew effect will drive the MPIR to the trivial recommender in the
    limit. This is the classic difficulty of maximizing a loss function with no randomization:
    it quickly settles into a modal state.'
  prefs: []
  type: TYPE_NORMAL
- en: This problem—and many others like it—encourages some modification to the algorithm
    to prevent this failure mode and continues to expose the algorithm and users to
    other options. The basic strategy for *explore-exploit schemes,* or *multiarmed
    bandits* as they’re called, is to take not only the outcome-maximizing recommendation
    but also a collection of alternative *variants,* and randomly determine which
    to use as a response.
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking a step back: given a collection of variant recommendations, or *arms*,
    <math alttext="upper A"><mi>A</mi></math> , for which the outcome of each recommendation
    is <math alttext="y Subscript t"><msub><mi>y</mi> <mi>t</mi></msub></math> , we
    have a prior reward function <math alttext="upper R left-parenthesis y Subscript
    t Baseline right-parenthesis"><mrow><mi>R</mi> <mo>(</mo> <msub><mi>y</mi> <mi>t</mi></msub>
    <mo>)</mo></mrow></math> . The bandit (called an *agent* in this literature) would
    like to maximize <math alttext="upper R left-parenthesis y Subscript t Baseline
    right-parenthesis"><mrow><mi>R</mi> <mo>(</mo> <msub><mi>y</mi> <mi>t</mi></msub>
    <mo>)</mo></mrow></math> but doesn’t know the distribution of the outcomes <math
    alttext="upper Y Subscript a element-of upper A"><msub><mi>Y</mi> <mrow><mi>a</mi><mo>∈</mo><mi>A</mi></mrow></msub></math>
    . The agent thus assumes some prior distributions for <math alttext="upper Y Subscript
    a element-of upper A"><msub><mi>Y</mi> <mrow><mi>a</mi><mo>∈</mo><mi>A</mi></mrow></msub></math>
    and then collects data to update those distributions; after sufficient observations,
    the agent can estimate the expected values of each distribution, <math alttext="mu
    Subscript a element-of upper A Baseline equals double-struck upper E left-parenthesis
    script upper R left-parenthesis upper Y Subscript a Baseline right-parenthesis
    right-parenthesis"><mrow><msub><mi>μ</mi> <mrow><mi>a</mi><mo>∈</mo><mi>A</mi></mrow></msub>
    <mo>=</mo> <mi>𝔼</mi> <mrow><mo>(</mo> <mi>ℛ</mi> <mrow><mo>(</mo> <msub><mi>Y</mi>
    <mi>a</mi></msub> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math> .'
  prefs: []
  type: TYPE_NORMAL
- en: 'If the agent was able to confidently estimate these reward values, the recommendation
    problem would be solved: at inference, the agent would simply estimate the reward
    values for all variants for the user and select the reward-optimizing *arm.* This
    is, of course, ridiculous in totality, but the basic idea is useful nonetheless:
    hold prior assumptions about what will be the greatest expected reward, and explore
    alternatives with some frequency to continue to update the distributions and refine
    your estimators.'
  prefs: []
  type: TYPE_NORMAL
- en: Even when not explicitly using a multiarmed bandit, this insight is a powerful
    and useful framework for understanding the goal of a recommendation system. Utilizing
    the ideas of prior estimates for good recommendations and exploring other options
    to gain signal is a core idea that’s recurring. Let’s see one practicality of
    this approach.
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="epsilon"><mi>ϵ</mi></math> -greedy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'How often should you explore versus use your reward-optimizing arm? The first
    best algorithm is <math alttext="epsilon"><mi>ϵ</mi></math> -greedy: for <math
    alttext="epsilon element-of left-parenthesis 0 comma 1 right-parenthesis"><mrow><mi>ϵ</mi>
    <mo>∈</mo> <mo>(</mo> <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo>)</mo></mrow></math>
    , at each request the agent has the probability <math alttext="epsilon"><mi>ϵ</mi></math>
    of choosing a random arm and the probability <math alttext="1 minus epsilon"><mrow><mn>1</mn>
    <mo>-</mo> <mi>ϵ</mi></mrow></math> of selecting the currently highest estimated
    reward arm.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take the MPIR and slightly modify it to include some exploration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The only modification to our MPIR is that now we have two cases for each potential
    recommendation from our `max_num_recs`. If a random probability is less than our
    <math alttext="epsilon"><mi>ϵ</mi></math> , we proceed as before and select the
    most popular; otherwise, we select a random recommendation.
  prefs: []
  type: TYPE_NORMAL
- en: Maximizing Reward
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’re interpreting maximization of reward as selecting the most-popular items.
    This is an important assumption, and as we move into more complicated recommenders,
    this will be the crucial assumption that we modify to get different algorithms
    and schemes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s summarize our recommender components again:'
  prefs: []
  type: TYPE_NORMAL
- en: Collector
  prefs: []
  type: TYPE_NORMAL
- en: The collector here need not change; we still want to get the item popularities
    first.
  prefs: []
  type: TYPE_NORMAL
- en: Ranker
  prefs: []
  type: TYPE_NORMAL
- en: The ranker also does not change! We begin by ranking the possible recommendations
    by popularity.
  prefs: []
  type: TYPE_NORMAL
- en: Server
  prefs: []
  type: TYPE_NORMAL
- en: If the collector and ranker remain the same, clearly the server is what must
    be adapted for this new recommender. This is the case; instead of taking the top
    items to fill `max_num_recs`, we now utilize our <math alttext="epsilon"><mi>ϵ</mi></math>
    to determine at each step if the next recommendation added to our list should
    be next in line from the ranker or a random selection. Otherwise, we adhere to
    the same API schema and return the same shape of data.
  prefs: []
  type: TYPE_NORMAL
- en: What Should <math alttext="epsilon"><mi>ϵ</mi></math> Be?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the preceding discussion, <math alttext="epsilon"><mi>ϵ</mi></math> is a
    fixed number for the entire call, but what should the value be? This is actually
    an area of great study, and the general wisdom is to start with large <math alttext="epsilon"><mi>ϵ</mi></math>
    (to encourage more exploration) and then reduce over time. Determining the rate
    at which you decrease it, the starting value, and so on, requires serious thought
    and research. Additionally, this value can be tied into your prediction loop and
    be part of the training process. See [“The Exploration-Exploitation Trade-Off:
    Intuitions and Strategies”](https://oreil.ly/wk-OB) by Joseph Rocca for a deeper
    dive.'
  prefs: []
  type: TYPE_NORMAL
- en: Other—often better—sampling techniques exist for optimization. *Importance sampling*
    can utilize the ranking functions we build later to integrate the explore-exploit
    with what our data has to teach.
  prefs: []
  type: TYPE_NORMAL
- en: The NLP-RecSys Relationship
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s utilize some intuition from a different area of ML, natural language
    processing. One of the fundamental models in NLP is *word2vec*: a sequence-based
    model for language understanding that uses the words that occur in sentences together.'
  prefs: []
  type: TYPE_NORMAL
- en: For *skipgram-word2vec*, the model takes sentences and attempts to learn the
    implicit meaning of their words via their co-occurrence relationships with other
    words in those sentences. Each pair of co-occurring words constitutes a sample
    that is one-hot encoded and sent into a vocabulary-sized layer of neurons, with
    a bottleneck layer and a vocabulary-sized output layer for probabilities that
    words will occur.
  prefs: []
  type: TYPE_NORMAL
- en: Via this network, we reduce the size of our representation to the bottleneck
    dimension and thus find a smaller dimensional representation of all our words
    than the original corpus-sized one-hot embedding. The thinking is that similarity
    of words can now be computed via vector similarity in this new representation
    space.
  prefs: []
  type: TYPE_NORMAL
- en: Why is this related to recommendation systems? Well, because if we take the
    ordered sequence of user-item interactions (e.g., the sequence of movies a user
    has rated), we can utilize the same idea from word2vec to find item similarity
    instead of word similarity. In this analogy, the user history is the *sentence*.
  prefs: []
  type: TYPE_NORMAL
- en: Previously, using our CF similarity, we decided that similar users can help
    inform what a good recommendation for a user should be. In this model, we are
    finding item-item similarity, so instead we assume that a user will like the items
    similar to those previously liked.
  prefs: []
  type: TYPE_NORMAL
- en: Items as Words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may have noticed that natural language models treat words as sequences,
    and in fact, our user history is a sequence too! For now, hold onto this knowledge.
    Later, this will guide us to sequence-based methods for RecSys.
  prefs: []
  type: TYPE_NORMAL
- en: Vector Search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have built a collection of vector representations of our items, and we claim
    that similarity in this space (often called a *latent space*, *representation
    space*, or *ambient space*) means similarity in *likability* to users.
  prefs: []
  type: TYPE_NORMAL
- en: To convert this similarity to a recommendation, consider a user <math alttext="upper
    A"><mi>A</mi></math> with a collection of previously liked items <math alttext="script
    upper R Subscript upper A"><msub><mi>ℛ</mi> <mi>A</mi></msub></math> , and consider
    <math alttext="script upper A equals StartSet v Subscript x Baseline vertical-bar
    x element-of script upper R Subscript upper A Baseline EndSet"><mrow><mi>𝒜</mi>
    <mo>=</mo> <mo>{</mo> <msub><mi>v</mi> <mi>x</mi></msub> <mo>|</mo> <mi>x</mi>
    <mo>∈</mo> <msub><mi>ℛ</mi> <mi>A</mi></msub> <mo>}</mo></mrow></math> the set
    of vectors associated to those items in this latent space. We are looking for
    a new item <math alttext="y"><mi>y</mi></math> that we think is good for <math
    alttext="upper A"><mi>A</mi></math> .
  prefs: []
  type: TYPE_NORMAL
- en: The Old Curse
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These latent spaces tend to be of high dimension, which Euclidean distance famously
    performs poorly in. As regions become sparse, the distance function performance
    decreases; local distances are meaningful, but global distances are not to be
    trusted. Instead, cosine distance shows better performance, but this is a topic
    of deep exploration. Additionally, instead of minimizing the distance, in practice
    it’s better to maximize the similarity.
  prefs: []
  type: TYPE_NORMAL
- en: 'One simple way to use similarity to produce a recommendation is to take the
    closest item to the average of those that <math alttext="upper A"><mi>A</mi></math>
    likes:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="argmax Subscript y Baseline StartSet USim left-parenthesis v
    Subscript y Baseline comma a v g left-parenthesis script upper A right-parenthesis
    right-parenthesis bar y element-of Items EndSet" display="block"><mrow><msub><mi>argmax</mi>
    <mi>y</mi></msub> <mfenced close="}" open="{" separators=""><mi>USim</mi> <mo>(</mo>
    <msub><mi>v</mi> <mi>y</mi></msub> <mo>,</mo> <mi>a</mi> <mi>v</mi> <mi>g</mi>
    <mrow><mo>(</mo> <mi>𝒜</mi> <mo>)</mo></mrow> <mo>)</mo> <mo>∣</mo> <mi>y</mi>
    <mo>∈</mo> <mi>Items</mi></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Here, <math alttext="d left-parenthesis minus comma minus right-parenthesis"><mrow><mi>d</mi>
    <mo>(</mo> <mo>-</mo> <mo>,</mo> <mo>-</mo> <mo>)</mo></mrow></math> is a distance
    function in the latent space (usually cosine distance).
  prefs: []
  type: TYPE_NORMAL
- en: 'The argmax essentially treats all of <math alttext="upper A"><mi>A</mi></math>
    ’s ratings equally and suggests something near those. In practice, this process
    is often fraught. First, you could weight the terms by rating:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="argmax Subscript y Baseline StartSet USim left-parenthesis v
    Subscript y Baseline comma StartFraction sigma-summation Underscript v Subscript
    x Baseline element-of script upper A Endscripts r Subscript x Baseline Over StartAbsoluteValue
    script upper R Subscript script upper A Baseline EndAbsoluteValue EndFraction
    right-parenthesis bar y element-of Items EndSet" display="block"><mrow><msub><mi>argmax</mi>
    <mi>y</mi></msub> <mfenced close="}" open="{" separators=""><mi>USim</mi> <mo>(</mo>
    <msub><mi>v</mi> <mi>y</mi></msub> <mo>,</mo> <mfrac><mrow><msub><mo>∑</mo> <mrow><msub><mi>v</mi>
    <mi>x</mi></msub> <mo>∈</mo><mi>𝒜</mi></mrow></msub> <msub><mi>r</mi> <mi>x</mi></msub></mrow>
    <mrow><mrow><mo>|</mo></mrow><msub><mi>ℛ</mi> <mi>𝒜</mi></msub> <mrow><mo>|</mo></mrow></mrow></mfrac>
    <mo>)</mo> <mo>∣</mo> <mi>y</mi> <mo>∈</mo> <mi>Items</mi></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'This can potentially improve the representativeness of the user feedback in
    the recommendations. Alternatively, you might find that a user rates movies across
    a variety of genres and themes. Averaging here will definitely lead to worse results,
    so maybe you want to simply find recommendations similar to one movie the user
    liked, weighted by that rating:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="argmax Subscript y Baseline StartSet StartFraction USim left-parenthesis
    v Subscript y Baseline comma v Subscript x Baseline right-parenthesis Over r Subscript
    x Baseline EndFraction bar y element-of Items comma v Subscript x Baseline element-of
    script upper A EndSet" display="block"><mrow><msub><mi>argmax</mi> <mi>y</mi></msub>
    <mfenced close="}" open="{" separators=""><mfrac><mrow><mi>USim</mi> <mo>(</mo><msub><mi>v</mi>
    <mi>y</mi></msub> <mo>,</mo><msub><mi>v</mi> <mi>x</mi></msub> <mo>)</mo></mrow>
    <msub><mi>r</mi> <mi>x</mi></msub></mfrac> <mo>∣</mo> <mi>y</mi> <mo>∈</mo> <mi>Items</mi>
    <mo>,</mo> <msub><mi>v</mi> <mi>x</mi></msub> <mo>∈</mo> <mi>𝒜</mi></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, you may even want to do this process several times for different items
    a user liked to get <math alttext="k"><mi>k</mi></math> recommendations:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="min hyphen k StartSet argmax Subscript y Baseline StartSet StartFraction
    USim left-parenthesis v Subscript y Baseline comma v Subscript x Baseline right-parenthesis
    Over r Subscript x Baseline EndFraction bar y element-of Items EndSet bar v Subscript
    x Baseline element-of script upper A EndSet" display="block"><mrow><mi>min</mi>
    <mo>-</mo> <mi>k</mi> <mfenced close="}" open="{" separators=""><msub><mi>argmax</mi>
    <mi>y</mi></msub> <mfenced close="}" open="{" separators=""><mfrac><mrow><mi>USim</mi>
    <mo>(</mo><msub><mi>v</mi> <mi>y</mi></msub> <mo>,</mo><msub><mi>v</mi> <mi>x</mi></msub>
    <mo>)</mo></mrow> <msub><mi>r</mi> <mi>x</mi></msub></mfrac> <mo>∣</mo> <mi>y</mi>
    <mo>∈</mo> <mi>Items</mi></mfenced> <mo>∣</mo> <msub><mi>v</mi> <mi>x</mi></msub>
    <mo>∈</mo> <mi>𝒜</mi></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Now we have <math alttext="k"><mi>k</mi></math> recommendations; each is similar
    to something that the user has liked and is weighted by how much they liked it.
    This approach utilized only an implicit geometry of the items formed by their
    co-occurrences.
  prefs: []
  type: TYPE_NORMAL
- en: Latent spaces and the geometric power that comes with them for recommendations
    will be a through line for the rest of the book. We will often formulate our loss
    functions via these geometries, and we’ll exploit the geometric intuition to brainstorm
    where to expand our technique next.
  prefs: []
  type: TYPE_NORMAL
- en: Nearest-Neighbors Search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A reasonable question to ask is “How do I get these vectors that minimize this
    distance?” In all the preceding schemes, we are computing many distances and then
    finding minimums. In general, the problem of nearest neighbors is an extremely
    important and well-studied question.
  prefs: []
  type: TYPE_NORMAL
- en: While finding the exact nearest neighbors can sometimes be slow, a lot of great
    progress has been made on approximate nearest neighbors (ANN) searches. These
    algorithms not only return very close to the actual nearest neighbors, but they
    also perform orders of complexity faster. In general, when you see us (or other
    publications) computing an <math alttext="argmin"><mi>argmin</mi></math> (the
    argument that minimized the function) over some distances, there’s a good chance
    ANN is what’s used in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recommendation systems in the preceding chapter discussed data distribution
    principles such as Zipf’s law and the Matthew Effect. These principles lead to
    challenges, such as skewed user similarity scores and data sparsity. In the world
    of ML, while the traditional math focuses on distance, the emphasis is on the
    concept of similarity. Different measures of similarity can drastically alter
    algorithm learning outcomes, with clustering being a primary application.
  prefs: []
  type: TYPE_NORMAL
- en: In the realm of recommendations, items are often represented in high-dimensional
    latent spaces. Similarity in these spaces hints at user preferences. Methods include
    recommending items close to a user’s average liked items, and this may be improved
    by adding a weighting by user-rating. However, individual preferences necessitate
    diverse recommendations. Latent spaces continue to be influential, driving recommendation
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Locating these vectors effectively requires the nearest-neighbors search. Though
    exact methods are resource-intensive, approximate nearest-neighbors offer a fast,
    precise solution, providing the foundation for the recommendation systems discussed
    in the current chapter.
  prefs: []
  type: TYPE_NORMAL
