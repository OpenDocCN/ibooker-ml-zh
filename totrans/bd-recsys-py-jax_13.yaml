- en: Chapter 10\. Low-Rank Methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章 低秩方法
- en: In the preceding chapter, we lamented at the challenge of working with so many
    features. By letting each item be its own feature, we were able to express a lot
    of information about user preference and item-affinity correlations, but we were
    in big trouble in terms of the curse of dimensionality. Combine this with the
    reality of very sparse features, and you’re in danger. In this chapter, we’ll
    turn to smaller feature spaces. By representing users and items as low-dimensional
    vectors, we can capture the complex relationships between them in a more efficient
    and effective way. This allows us to generate more personalized and relevant recommendations
    for users while also reducing the computational complexity of the recommendation
    process.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们对处理这么多特征的挑战感到惋惜。通过让每个项目成为其自己的特征，我们能够表达大量关于用户偏好和项目相关性的信息，但在维度灾难方面我们陷入了麻烦。结合非常稀疏特征的现实，您面临危险。在本章中，我们将转向更小的特征空间。通过将用户和项目表示为低维向量，我们可以更有效地捕捉它们之间的复杂关系。这使我们能够为用户生成更个性化和相关的推荐，同时减少推荐过程的计算复杂性。
- en: We will explore the use of low-dimensional embeddings and discuss the benefits
    and some of the implementation details of this approach. We will also look at
    code in JAX that uses modern gradient-based optimization to reduce the dimension
    of your item or user representations.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨使用低维嵌入的方法，并讨论此方法的优势以及部分实现细节。我们还将查看在JAX中使用现代基于梯度的优化来减少项目或用户表示维度的代码。
- en: Latent Spaces
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在空间
- en: You are already familiar with feature spaces, which are usually categorical
    or vector-valued direct representations of the data. This can be the raw red,
    green, and blue values of an image, counts of items in a histogram, or attributes
    of an object like length, width, and height. Latent features, on the other hand,
    do not represent any specific real value feature of the items but are initialized
    randomly and then learned to suit a task. The GloVe embeddings we discussed in
    [Chapter 8](ch08.html#ch:wikipedia-e2e) are one such example of a latent vector
    that was learned to represent the log count of words. Here we will cover more
    ways to generate these latent features or embeddings.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经熟悉特征空间，通常是数据的分类或矢量值直接表示。这可以是图像的原始红色、绿色和蓝色值，直方图中项目的计数，或者对象的属性如长度、宽度和高度。另一方面，潜在特征不代表任何特定的实值特征，而是随机初始化，然后根据任务进行学习。我们在[第8章](ch08.html#ch:wikipedia-e2e)中讨论的GloVe嵌入就是一个学习表示单词对数计数的潜在向量的例子。在这里，我们将介绍生成这些潜在特征或嵌入的更多方式。
- en: Focus on Your “Strangths”
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 专注于您的“优势”
- en: This chapter relies heavily on linear algebra, so it’s good to read up on vectors,
    dot products, and norms of vectors before proceeding. Having an understanding
    of matrices and the rank of matrices will also be useful. Consider [*Linear Algebra
    and Its Applications*](https://oreil.ly/8MBN8) by Gilbert Strang.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章非常依赖于线性代数，因此在继续之前，熟悉向量、点积和向量范数是很好的。了解矩阵及其秩也将非常有用。考虑阅读《线性代数及其应用》（[*Linear Algebra
    and Its Applications*](https://oreil.ly/8MBN8)），作者Gilbert Strang。
- en: One of the reasons latent spaces are so popular is that they are usually lower
    in dimension than the features they represent. For example, if the user-item rating
    matrix or interaction matrix (where the matrix entries are 1 if a user has interacted
    with an item) is *N* × *M* dimensional, then factorizing the matrix into latent
    factors of *N* × *K* and *K* × *M*, where *K* is much smaller than *N* or *M*
    is an approximation of the missing entries because we’re relaxing the factorization.
    *K* being smaller than *N* or *M* is usually called an *information bottleneck*—that
    is, we are forcing the matrix to be made up of a much smaller matrix. This means
    the ML model has to make up missing entries, which might be good for recommender
    systems. As long as users have interacted with enough similar items, by forcing
    the system to have a lot less capacity in terms of degrees of freedom, then factorization
    can completely reconstruct the matrix, and the missing entries tend to get filled
    by similar items.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在空间如此受欢迎的原因之一是它们通常比它们所代表的特征低维。例如，如果用户-物品评分矩阵或交互矩阵（其中矩阵条目为 1，如果用户与物品有交互）是 *N*
    × *M* 维度的，那么将矩阵分解为 *N* × *K* 和 *K* × *M* 的潜在因子，其中 *K* 远小于 *N* 或 *M*，是对缺失条目的一种近似，因为我们放宽了因子分解。
    *K* 比 *N* 或 *M* 小通常被称为*信息瓶颈*—也就是说，我们正在强制矩阵由一个小得多的矩阵组成。这意味着 ML 模型必须填补缺失的条目，这对于推荐系统可能是有利的。只要用户与足够相似的物品有交互，通过强制系统在自由度方面容量大大减少，然后因子分解可以完全重构矩阵，并且缺失的条目往往会被相似的物品填补。
- en: Let’s see what happens, for example, when we factor a user-item matrix of 4
    × 4 into a 4 × 2 and a 2 × 4 vector using SVD.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当我们将一个 4 × 4 的用户-物品矩阵分解为一个 4 × 2 和一个 2 × 4 的向量时，会发生什么。
- en: 'We are supplying a matrix whose rows are users and whose columns are items.
    For example, row 0 is `[1, 0, 0, 1]`, which means user 0 has selected item 0 and
    item 3\. These can be ratings or purchases. Now let’s look at some code:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供的矩阵的行是用户，列是物品。例如，第 0 行是 `[1, 0, 0, 1]`，这意味着用户 0 选择了物品 0 和物品 3。这些可以是评分或购买。现在让我们看一些代码：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Notice that the user in row 1 now has a score for an item in column 3, and the
    user in row 3 now has a positive score for the item in column 2\. This phenomenon
    is generally known as *matrix completion* and is a good property for recommender
    systems to have because now we get to recommend new items to users. The general
    method of forcing the ML to go through a bottleneck that is smaller than the size
    of the matrix that it is trying to reconstruct is known as a *low-rank approximation*
    because the rank of the approximation is 2 but the rank of the original user-item
    matrix is 4.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，现在第 1 行的用户在第 3 列有一个物品的分数，并且第 3 行的用户对第 2 列的物品有正分数。这种现象通常被称为*矩阵补全*，对于推荐系统来说是一个很好的特性，因为现在我们可以向用户推荐新的物品。通过强制让
    ML 通过比其尝试重建的矩阵大小更小的瓶颈的一般方法被称为*低秩近似*，因为近似的秩为 2，但原始用户-物品矩阵的秩为 4。
- en: What Is the Rank of a Matrix?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矩阵的秩是什么？
- en: An *N* × *M* matrix may be considered as *N* row vectors (corresponding to users)
    and *M* column vectors (corresponding to items). When you consider the *N* vectors
    of dimension *M*, the *rank of the matrix* is the volume of the polyhedron defined
    by the *N* vectors in *M* dimensions. This is often different from the way we
    talk about the rank of matrices, however. While it’s the most natural and precise
    definition, we instead say it’s the “minimum number of dimensions necessary to
    represent the vectors of the matrix.”
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*N* × *M* 矩阵可以被视为 *N* 行向量（对应用户）和 *M* 列向量（对应物品）。当你考虑 *M* 维度中 *N* 向量时，*矩阵的秩*
    是由 *N* 个向量在 *M* 维度中定义的多面体的体积。然而，这与我们讨论矩阵秩的方式通常不同。虽然这是最自然和精确的定义，但我们却说它是“表示矩阵向量所需的最小维数。”'
- en: We will cover SVD in more detail later in the chapter. This was just to whet
    your appetite to understand how latent spaces are related to recommender systems.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面更详细地介绍 SVD。这只是为了激发你对理解潜在空间如何与推荐系统相关的兴趣。
- en: Dot Product Similarity
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 点积相似度
- en: In [Chapter 3](ch03.html#ch:math) we introduced similarity measures, but now
    we return to the dot product in the context of similarity because of their increased
    importance in latent spaces. After all, latent spaces are built on the assumption
    that distance is similarity.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第三章](ch03.html#ch:math)中，我们介绍了相似度测量，但现在我们回到点积的概念，因为在潜在空间中它们变得更加重要。毕竟，潜在空间建立在距离即相似性的假设上。
- en: The dot-product similarity is meaningful in recommendation systems because it
    provides a geometric interpretation of the relationship between users and items
    in the latent space (or potentially items and items, users and users, etc.). In
    the context of recommendation systems, the dot product can be seen as a projection
    of one vector onto another, indicating the degree of similarity or alignment between
    the user’s preferences and the item’s characteristics.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在推荐系统中，点积相似度有意义，因为它提供了在潜在空间中用户与物品之间关系的几何解释（或者可能是物品与物品、用户与用户等）。在推荐系统的背景下，点积可以看作是一个向量在另一个向量上的投影，指示用户偏好与物品特性之间的相似度或对齐程度。
- en: 'To understand the geometric significance of the dot product, consider two vectors,
    *u* and *p*, representing the user and the product in the latent space, respectively.
    The dot product of these two vectors can be defined as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解点积的几何意义，考虑两个向量 *u* 和 *p*，分别表示潜在空间中的用户和产品。这两个向量的点积可以定义如下：
- en: <math alttext="u times p equals StartAbsoluteValue EndAbsoluteValue u StartAbsoluteValue
    EndAbsoluteValue StartAbsoluteValue EndAbsoluteValue p StartAbsoluteValue EndAbsoluteValue
    c o s left-parenthesis theta right-parenthesis" display="block"><mrow><mi>u</mi>
    <mo>×</mo> <mi>p</mi> <mo>=</mo> <mo>|</mo> <mo>|</mo> <mi>u</mi> <mo>|</mo> <mo>|</mo>
    <mo>|</mo> <mo>|</mo> <mi>p</mi> <mo>|</mo> <mo>|</mo> <mi>c</mi> <mi>o</mi> <mi>s</mi>
    <mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow></math>
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="u times p equals StartAbsoluteValue EndAbsoluteValue u StartAbsoluteValue
    EndAbsoluteValue StartAbsoluteValue EndAbsoluteValue p StartAbsoluteValue EndAbsoluteValue
    c o s left-parenthesis theta right-parenthesis" display="block"><mrow><mi>u</mi>
    <mo>×</mo> <mi>p</mi> <mo>=</mo> <mo>|</mo> <mo>|</mo> <mi>u</mi> <mo>|</mo> <mo>|</mo>
    <mo>|</mo> <mo>|</mo> <mi>p</mi> <mo>|</mo> <mo>|</mo> <mi>c</mi> <mi>o</mi> <mi>s</mi>
    <mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow></math>
- en: Here, ||u|| and ||p|| represent the magnitudes of the user and product vectors,
    and θ is the angle between them. The dot product is thus a measure of the projection
    of one vector onto another, which is scaled by the magnitudes of both vectors.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，||u|| 和 ||p|| 表示用户向量和产品向量的大小，θ 表示它们之间的角度。因此，点积是一个衡量一个向量在另一个向量上投影的度量，其值受到两个向量大小的影响。
- en: 'The cosine similarity, which is another popular similarity measure in recommendation
    systems, is derived directly from the dot product:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度是推荐系统中另一个流行的相似度度量，直接源于点积：
- en: <math alttext="c o s i n e s i m i l a r i t y left-parenthesis u comma p right-parenthesis
    equals StartFraction left-parenthesis u times p right-parenthesis Over left-parenthesis
    StartAbsoluteValue EndAbsoluteValue u StartAbsoluteValue EndAbsoluteValue StartAbsoluteValue
    EndAbsoluteValue p StartAbsoluteValue EndAbsoluteValue right-parenthesis EndFraction"
    display="block"><mrow><mi>c</mi> <mi>o</mi> <mi>s</mi> <mi>i</mi> <mi>n</mi> <mi>e</mi>
    <mi>s</mi> <mi>i</mi> <mi>m</mi> <mi>i</mi> <mi>l</mi> <mi>a</mi> <mi>r</mi> <mi>i</mi>
    <mi>t</mi> <mi>y</mi> <mrow><mo>(</mo> <mi>u</mi> <mo>,</mo> <mi>p</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mo>(</mo><mi>u</mi><mo>×</mo><mi>p</mi><mo>)</mo></mrow>
    <mrow><mo>(</mo><mo>|</mo><mo>|</mo><mi>u</mi><mo>|</mo><mo>|</mo><mo>|</mo><mo>|</mo><mi>p</mi><mo>|</mo><mo>|</mo><mo>)</mo></mrow></mfrac></mstyle></mrow></math>
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="c o s i n e s i m i l a r i t y left-parenthesis u comma p right-parenthesis
    equals StartFraction left-parenthesis u times p right-parenthesis Over left-parenthesis
    StartAbsoluteValue EndAbsoluteValue u StartAbsoluteValue EndAbsoluteValue StartAbsoluteValue
    EndAbsoluteValue p StartAbsoluteValue EndAbsoluteValue right-parenthesis EndFraction"
    display="block"><mrow><mi>c</mi> <mi>o</mi> <mi>s</mi> <mi>i</mi> <mi>n</mi> <mi>e</mi>
    <mi>s</mi> <mi>i</mi> <mi>m</mi> <mi>i</mi> <mi>l</mi> <mi>a</mi> <mi>r</mi> <mi>i</mi>
    <mi>t</mi> <mi>y</mi> <mrow><mo>(</mo> <mi>u</mi> <mo>,</mo> <mi>p</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mo>(</mo><mi>u</mi><mo>×</mo><mi>p</mi><mo>)</mo></mrow>
    <mrow><mo>(</mo><mo>|</mo><mo>|</mo><mi>u</mi><mo>|</mo><mo>|</mo><mo>|</mo><mo>|</mo><mi>p</mi><mo>|</mo><mo>|</mo><mo>)</mo></mrow></mfrac></mstyle></mrow></math>
- en: The cosine similarity ranges from –1 to 1, where –1 indicates completely dissimilar
    preferences and characteristics. A 0 indicates no similarity, and 1 indicates
    perfect alignment between the user’s preferences and the product’s characteristics.
    In the context of recommendation systems, cosine similarity provides a normalized
    measure of similarity that is invariant to the magnitudes of the user and product
    vectors. Note that the choice of using cosine similarity versus L2 distance depends
    on the type of embeddings you’re using and the way you optimize the computations.
    In practice, the only important feature is often the relative values.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度的范围从 -1 到 1，其中 -1 表示完全不相似的偏好和特性。0 表示没有相似度，1 表示用户偏好与产品特性完全对齐。在推荐系统的背景下，余弦相似度提供了一个归一化的相似度度量，不受用户和产品向量大小的影响。需要注意，选择使用余弦相似度还是
    L2 距离取决于您使用的嵌入类型以及优化计算的方式。在实践中，通常最重要的特征是相对值。
- en: The geometric interpretation of the dot product (and cosine similarity) in recommendation
    systems is that it captures the alignment between user preferences and product
    characteristics. If the angle between the user and product vectors is small, the
    user’s preferences align well with the product’s characteristics, leading to a
    higher similarity score. Conversely, if the angle is large, the user’s preferences
    and product’s characteristics are dissimilar, resulting in a lower similarity
    score. By projecting user and item vectors onto each other, the dot-product similarity
    can capture the degree of alignment between user preferences and item characteristics,
    allowing the recommendation system to identify items that are most likely to be
    relevant and appealing to the user.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在推荐系统中，点积（以及余弦相似度）的几何解释是捕捉用户偏好与产品特性之间的对齐程度。如果用户向量和产品向量之间的角度很小，则用户偏好与产品特性很好地对齐，从而导致更高的相似度分数。相反，如果角度很大，则用户偏好与产品特性不相似，导致较低的相似度分数。通过将用户和物品向量投影到彼此上，点积相似度可以捕捉用户偏好与物品特性之间的对齐程度，从而使推荐系统能够识别最有可能与用户相关和吸引的物品。
- en: Anecdotally, the dot product seems to capture popularity, as very long vectors
    tend to be easy to project on anything that isn’t completely perpendicular or
    pointing away from them. As a result, a trade-off exists between frequently recommending
    popular items with a large vector length and longer-tail items that have smaller
    angular difference with cosine distance.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 传闻中，点积似乎捕捉到了流行度，因为非常长的向量倾向于容易投影到任何不完全垂直或指向远离的物体上。因此，存在一种权衡，即频繁推荐具有较大向量长度的热门物品与具有较小余弦距离角度差的长尾物品之间的权衡。
- en: '[Figure 10-1](#cosine_dot) considers two vectors, *a* and *b*. With cosine
    similarity, the vectors are unit length, so the angle is just the measure of similarity.
    However, with dot product, a very long vector like *c* might be considered more
    similar to *a* than *b* even though the angle between *a* and *b* is smaller because
    of the longer length of *c*. These long vectors tend to be very popular items
    that co-occur with many other items.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-1](#cosine_dot) 考虑了两个向量，*a* 和 *b*。使用余弦相似度时，向量是单位长度的，所以角度就是相似度的度量。然而，使用点积时，一个非常长的向量像
    *c* 可能会被认为比 *b* 更相似 *a*，尽管 *a* 和 *b* 之间的角度更小，因为 *c* 的长度更长。这些长向量往往是与许多其他项目共同出现的非常流行的项目。'
- en: '![Cosine vs Dot product similarity](assets/brpj_1001.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![余弦 vs 点积相似度](assets/brpj_1001.png)'
- en: Figure 10-1\. Cosine versus dot-product similarity
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-1\. 余弦相似度与点积相似度
- en: Co-occurrence Models
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 共现模型
- en: In our Wikipedia co-occurrences examples, we determined that the co-occurrence
    structure between two items could be used to generate measures of similarity.
    We covered how PMI can take the counts of co-occurrence and make recommendations
    based on very high mutual information between an item in the cart and others.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的维基百科共现示例中，我们确定了两个项目之间的共现结构可以用来生成相似度测量。我们讨论了如何使用 PMI 可以获取共现计数并基于购物车中的项与其他项之间非常高的互信息做出推荐。
- en: As we’ve discussed, PMI is not a distance metric but still has important similarity
    measures based on co-occurrence. Let’s return to this topic.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所讨论的，PMI 不是一个距离度量，但基于共现仍具有重要的相似度度量。让我们回到这个话题。
- en: 'Recall from earlier that PMI is defined as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 之前提到过，PMI 的定义如下：
- en: <math alttext="StartFraction p left-parenthesis x Subscript i Baseline comma
    x Subscript j Baseline right-parenthesis Over p left-parenthesis x Subscript i
    Baseline right-parenthesis asterisk p left-parenthesis x Subscript j Baseline
    right-parenthesis EndFraction equals StartFraction left-parenthesis upper C Subscript
    script upper I Baseline right-parenthesis Subscript x Sub Subscript i Subscript
    comma x Sub Subscript j Subscript Baseline asterisk number-sign left-parenthesis
    total interactions right-parenthesis Over number-sign left-parenthesis x Subscript
    i Baseline right-parenthesis asterisk number-sign left-parenthesis x Subscript
    j Baseline right-parenthesis EndFraction" display="block"><mrow><mstyle displaystyle="true"
    scriptlevel="0"><mfrac><mrow><mi>p</mi><mo>(</mo><msub><mi>x</mi> <mi>i</mi></msub>
    <mo>,</mo><msub><mi>x</mi> <mi>j</mi></msub> <mo>)</mo></mrow> <mrow><mi>p</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow><mo>*</mo><mi>p</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow></mrow></mfrac></mstyle> <mo>=</mo> <mstyle
    displaystyle="true" scriptlevel="0"><mfrac><mrow><msub><mfenced close=")" open="("
    separators=""><msub><mi>C</mi> <mi>ℐ</mi></msub></mfenced> <mrow><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>,</mo><msub><mi>x</mi> <mi>j</mi></msub></mrow></msub> <mo>*</mo><mo>#</mo><mfenced
    close=")" open="(" separators=""><mtext>total</mtext><mtext>interactions</mtext></mfenced></mrow>
    <mrow><mo>#</mo><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow><mo>*</mo><mo>#</mo><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow></mrow></mfrac></mstyle></mrow></math>
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartFraction p left-parenthesis x Subscript i Baseline comma
    x Subscript j Baseline right-parenthesis Over p left-parenthesis x Subscript i
    Baseline right-parenthesis asterisk p left-parenthesis x Subscript j Baseline
    right-parenthesis EndFraction equals StartFraction left-parenthesis upper C Subscript
    script upper I Baseline right-parenthesis Subscript x Sub Subscript i Subscript
    comma x Sub Subscript j Subscript Baseline asterisk number-sign left-parenthesis
    total interactions right-parenthesis Over number-sign left-parenthesis x Subscript
    i Baseline right-parenthesis asterisk number-sign left-parenthesis x Subscript
    j Baseline right-parenthesis EndFraction" display="block"><mrow><mstyle displaystyle="true"
    scriptlevel="0"><mfrac><mrow><mi>p</mi><mo>(</mo><msub><mi>x</mi> <mi>i</mi></msub>
    <mo>,</mo><msub><mi>x</mi> <mi>j</mi></msub> <mo>)</mo></mrow> <mrow><mi>p</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow><mo>*</mo><mi>p</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow></mrow></mfrac></mstyle> <mo>=</mo> <mstyle
    displaystyle="true" scriptlevel="0"><mfrac><mrow><msub><mfenced close=")" open="("
    separators=""><msub><mi>C</mi> <mi>ℐ</mi></msub></mfenced> <mrow><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>,</mo><msub><mi>x</mi> <mi>j</mi></msub></mrow></msub> <mo>*</mo><mo>#</mo><mfenced
    close=")" open="(" separators=""><mtext>total</mtext><mtext>interactions</mtext></mfenced></mrow>
    <mrow><mo>#</mo><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow><mo>*</mo><mo>#</mo><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow></mrow></mfrac></mstyle></mrow></math>
- en: 'Now let’s consider the *discrete co-occurrence distribution*, <math alttext="upper
    C upper D Subscript x Sub Subscript i"><mrow><mi>C</mi> <msub><mi>D</mi> <msub><mi>x</mi>
    <mi>i</mi></msub></msub></mrow></math> , defined as the collection of co-occurrences
    over all other <math alttext="x Subscript j"><msub><mi>x</mi> <mi>j</mi></msub></math>
    :'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑 *离散共现分布*，<math alttext="upper C upper D Subscript x Sub Subscript i"><mrow><mi>C</mi>
    <msub><mi>D</mi> <msub><mi>x</mi> <mi>i</mi></msub></msub></mrow></math> ，定义为所有其他
    <math alttext="x Subscript j"><msub><mi>x</mi> <mi>j</mi></msub></math> 的共现的集合：
- en: <math alttext="upper C upper D Subscript x Sub Subscript i Baseline equals left-parenthesis
    upper C Subscript script upper I Baseline right-parenthesis Subscript x Sub Subscript
    i Subscript comma x 1 Baseline comma ellipsis comma left-parenthesis upper C Subscript
    script upper I Baseline right-parenthesis Subscript x Sub Subscript i Subscript
    comma x Sub Subscript i Subscript Baseline comma ellipsis comma left-parenthesis
    upper C Subscript script upper I Baseline right-parenthesis Subscript x Sub Subscript
    i Subscript comma x Sub Subscript upper N Subscript Baseline" display="block"><mrow><mi>C</mi>
    <msub><mi>D</mi> <msub><mi>x</mi> <mi>i</mi></msub></msub> <mo>=</mo> <mrow><msub><mfenced
    close=")" open="(" separators=""><msub><mi>C</mi> <mi>ℐ</mi></msub></mfenced>
    <mrow><msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo><msub><mi>x</mi> <mn>1</mn></msub></mrow></msub>
    <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mfenced close=")" open="(" separators=""><msub><mi>C</mi>
    <mi>ℐ</mi></msub></mfenced> <mrow><msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo><msub><mi>x</mi>
    <mi>i</mi></msub></mrow></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mfenced
    close=")" open="(" separators=""><msub><mi>C</mi> <mi>ℐ</mi></msub></mfenced>
    <mrow><msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo><msub><mi>x</mi> <mi>N</mi></msub></mrow></msub></mrow></mrow></math>
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper C upper D Subscript x Sub Subscript i Baseline equals left-parenthesis
    upper C Subscript script upper I Baseline right-parenthesis Subscript x Sub Subscript
    i Subscript comma x 1 Baseline comma ellipsis comma left-parenthesis upper C Subscript
    script upper I Baseline right-parenthesis Subscript x Sub Subscript i Subscript
    comma x Sub Subscript i Subscript Baseline comma ellipsis comma left-parenthesis
    upper C Subscript script upper I Baseline right-parenthesis Subscript x Sub Subscript
    i Subscript comma x Sub Subscript upper N Subscript Baseline" display="block"><mrow><mi>C</mi>
    <msub><mi>D</mi> <msub><mi>x</mi> <mi>i</mi></msub></msub> <mo>=</mo> <mrow><msub><mfenced
    close=")" open="(" separators=""><msub><mi>C</mi> <mi>ℐ</mi></msub></mfenced>
    <mrow><msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo><msub><mi>x</mi> <mn>1</mn></msub></mrow></msub>
    <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mfenced close=")" open="(" separators=""><msub><mi>C</mi>
    <mi>ℐ</mi></msub></mfenced> <mrow><msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo><msub><mi>x</mi>
    <mi>i</mi></msub></mrow></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mfenced
    close=")" open="(" separators=""><msub><mi>C</mi> <mi>ℐ</mi></msub></mfenced>
    <mrow><msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo><msub><mi>x</mi> <mi>N</mi></msub></mrow></msub></mrow></mrow></math>
- en: 'Here, <math alttext="j element-of 1 ellipsis upper N"><mrow><mi>j</mi> <mo>∈</mo>
    <mn>1</mn> <mo>...</mo> <mi>N</mi></mrow></math> , and <math alttext="upper N"><mi>N</mi></math>
    is the total number of items. This represents the co-occurrence histogram between
    <math alttext="x Subscript i"><msub><mi>x</mi> <mi>i</mi></msub></math> and all
    other items. By introducing this discrete distribution, we can utilize another
    tool: the Hellinger distance.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，<math alttext="j element-of 1 ellipsis upper N"><mrow><mi>j</mi> <mo>∈</mo>
    <mn>1</mn> <mo>...</mo> <mi>N</mi></mrow></math> ，而 <math alttext="upper N"><mi>N</mi></math>
    是所有项目的总数。这表示 <math alttext="x Subscript i"><msub><mi>x</mi> <mi>i</mi></msub></math>
    与所有其他项目之间的共现直方图。通过引入这种离散分布，我们可以利用另一个工具：Hellinger 距离。
- en: 'We can measure distributional distance in a few ways, each with different advantages.
    For our discussion, we will not go deeply into the differences and will stick
    to the simplest but most appropriate. Hellinger distance is defined as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用几种方法来衡量分布距离，每种方法都有不同的优势。在我们的讨论中，我们不会深入探讨这些差异，而是坚持使用最简单但最合适的方法。Hellinger
    距离的定义如下：
- en: <math alttext="upper H left-parenthesis upper P comma upper Q right-parenthesis
    equals NestedStartRoot 1 minus sigma-summation Underscript i Overscript n Endscripts
    StartRoot p Subscript i Baseline q Subscript i Baseline EndRoot NestedEndRoot
    equals StartFraction 1 Over StartRoot 2 EndRoot EndFraction double-vertical-bar
    StartRoot upper P EndRoot minus StartRoot upper Q EndRoot double-vertical-bar
    Subscript 2" display="block"><mrow><mi>H</mi> <mrow><mo>(</mo> <mi>P</mi> <mo>,</mo>
    <mi>Q</mi> <mo>)</mo></mrow> <mo>=</mo> <msqrt><mrow><mn>1</mn> <mo>-</mo> <msubsup><mo>∑</mo>
    <mi>i</mi> <mi>n</mi></msubsup> <msqrt><mrow><msub><mi>p</mi> <mi>i</mi></msub>
    <msub><mi>q</mi> <mi>i</mi></msub></mrow></msqrt></mrow></msqrt> <mo>=</mo> <mstyle
    displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn> <msqrt><mn>2</mn></msqrt></mfrac></mstyle>
    <msub><mfenced close="∥" open="∥" separators=""><msqrt><mi>P</mi></msqrt><mo>-</mo><msqrt><mi>Q</mi></msqrt></mfenced>
    <mn>2</mn></msub></mrow></math>
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper H left-parenthesis upper P comma upper Q right-parenthesis
    equals NestedStartRoot 1 minus sigma-summation Underscript i Overscript n Endscripts
    StartRoot p Subscript i Baseline q Subscript i Baseline EndRoot NestedEndRoot
    equals StartFraction 1 Over StartRoot 2 EndRoot EndFraction double-vertical-bar
    StartRoot upper P EndRoot minus StartRoot upper Q EndRoot double-vertical-bar
    Subscript 2" display="block"><mrow><mi>H</mi> <mrow><mo>(</mo> <mi>P</mi> <mo>,</mo>
    <mi>Q</mi> <mo>)</mo></mrow> <mo>=</mo> <msqrt><mrow><mn>1</mn> <mo>-</mo> <msubsup><mo>∑</mo>
    <mi>i</mi> <mi>n</mi></msubsup> <msqrt><mrow><msub><mi>p</mi> <mi>i</mi></msub>
    <msub><mi>q</mi> <mi>i</mi></msub></mrow></msqrt></mrow></msqrt> <mo>=</mo> <mstyle
    displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn> <msqrt><mn>2</mn></msqrt></mfrac></mstyle>
    <msub><mfenced close="∥" open="∥" separators=""><msqrt><mi>P</mi></msqrt><mo>-</mo><msqrt><mi>Q</mi></msqrt></mfenced>
    <mn>2</mn></msub></mrow></math>
- en: <math alttext="upper P equals mathematical left-angle p Subscript i Baseline
    mathematical right-angle"><mrow><mi>P</mi> <mo>=</mo> <mfenced close="〉" open="〈"
    separators=""><msub><mi>p</mi> <mi>i</mi></msub></mfenced></mrow></math> and <math
    alttext="upper Q equals mathematical left-angle q Subscript i Baseline mathematical
    right-angle"><mrow><mi>Q</mi> <mo>=</mo> <mfenced close="〉" open="〈" separators=""><msub><mi>q</mi>
    <mi>i</mi></msub></mfenced></mrow></math> are two probability density vectors.
    In our setting, <math alttext="upper P"><mi>P</mi></math> and <math alttext="upper
    Q"><mi>Q</mi></math> can be <math alttext="upper C upper D Subscript x Sub Subscript
    i"><mrow><mi>C</mi> <msub><mi>D</mi> <msub><mi>x</mi> <mi>i</mi></msub></msub></mrow></math>
    and <math alttext="upper C upper D Subscript x Sub Subscript j"><mrow><mi>C</mi>
    <msub><mi>D</mi> <msub><mi>x</mi> <mi>j</mi></msub></msub></mrow></math> .
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper P equals mathematical left-angle p Subscript i Baseline
    mathematical right-angle"><mrow><mi>P</mi> <mo>=</mo> <mfenced close="〉" open="〈"
    separators=""><msub><mi>p</mi> <mi>i</mi></msub></mfenced></mrow></math> 和 <math
    alttext="upper Q equals mathematical left-angle q Subscript i Baseline mathematical
    right-angle"><mrow><mi>Q</mi> <mo>=</mo> <mfenced close="〉" open="〈" separators=""><msub><mi>q</mi>
    <mi>i</mi></msub></mfenced></mrow></math> 是两个概率密度向量。在我们的设定中，<math alttext="upper
    P"><mi>P</mi></math> 和 <math alttext="upper Q"><mi>Q</mi></math> 可以是 <math alttext="upper
    C upper D Subscript x Sub Subscript i"><mrow><mi>C</mi> <msub><mi>D</mi> <msub><mi>x</mi>
    <mi>i</mi></msub></msub></mrow></math> 和 <math alttext="upper C upper D Subscript
    x Sub Subscript j"><mrow><mi>C</mi> <msub><mi>D</mi> <msub><mi>x</mi> <mi>j</mi></msub></msub></mrow></math>
    。
- en: The motivation behind this process is that we now have a proper distance between
    items purely based on co-occurrences. We can use any dimension transformation
    or reduction on this geometry. Later we will show dimension-reduction techniques
    that can use an arbitrary distance matrix and reduce the space to a lower-dimensional
    embedding that approximates it.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程背后的动机是，我们现在有了基于共现的物品之间的适当距离。我们可以在这种几何上使用任何维度转换或减少。稍后我们将展示可以使用任意距离矩阵并将空间减少到逼近它的低维嵌入的维度减少技术。
- en: What About Measure Spaces and Information Theory?
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测度空间和信息理论呢？
- en: While we’re discussing distributions, you may find yourself wondering, “Is there
    a distance between distributions such that distributions are points in a latent
    space?” Oh, you weren’t wondering that? Well, OK. We’ll address it anyway.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论分布时，你可能会想知道，“是否存在一种距离使得分布在潜在空间中是点？”哦，你没有想到？好吧，我们无论如何会解决这个问题。
- en: The short answer is that we can measure the differences between distributions.
    The most popular is Kullback–Leibler (KL) divergence, which is usually described
    in a Bayesian sense as the amount of surprise in seeing the distribution P, when
    expecting the distribution Q. However, KL is not a proper distance metric because
    it is asymmetric.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 简短的答案是，我们可以衡量分布之间的差异。最流行的是库尔巴克-莱布勒（KL）散度，通常在贝叶斯意义下描述为预期分布Q时看到分布P的惊讶程度。然而，KL不是一个适当的距离度量，因为它是非对称的。
- en: Another symmetric distance metric that has some nice properties is the Hellinger
    distance. Hellinger distance is effectively the 2-norm measure theoretic distance.
    Additionally, Hellinger distance naturally generalizes to discrete distributions.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个具有良好性质的对称距离度量是海林格距离。海林格距离实际上是二范数的测度论距离。此外，海林格距离自然地推广到离散分布。
- en: 'If this still hasn’t scratched your itch for abstraction, we can also consider
    the total variation distance, which is the limit in the space of Fisher’s exact
    distance measures, which really means that it has all the nice properties of a
    distance of two distributions and no measure would ever consider them more dissimilar.
    Well, all the nice properties except for one: it’s not smooth. If you also want
    smoothness for differentiability, you’ll need to approximate it via an offset.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这仍然没有解决你对抽象的渴望，我们还可以考虑总变分距离，这是在费舍尔精确距离测度空间中的极限，这实际上意味着它具有两个分布距离的所有良好性质，并且没有测量会认为它们更不同。嗯，所有的良好性质除了一个：它不平滑。如果你还希望具有平滑性和可微性，你需要通过偏移来近似它。
- en: If you ever need a distance between distributions, just use Hellinger.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要计算分布之间的距离，可以使用海林格距离。
- en: Reducing the Rank of a Recommender Problem
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 减少推荐问题的排名
- en: We’ve shown that as the number of items and users grows, we rapidly increase
    the dimensionality of our recommender problem. Because we’re representing each
    item and user as a column or vector, this scales like <math alttext="n squared"><msup><mi>n</mi>
    <mn>2</mn></msup></math> . One way to push back against this difficulty is by
    rank reduction; recall our previous discussions about rank reduction via factorization.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经表明，随着物品和用户数量的增加，我们推荐问题的维度迅速增加。因为我们将每个项目和用户表示为一列或向量，这类似于<math alttext="n
    squared"><msup><mi>n</mi> <mn>2</mn></msup></math>的规模。推迟这一困难的一种方法是通过降秩；回想我们之前关于通过因子分解进行降秩的讨论。
- en: 'Like many integers, many matrices can be *factored* into smaller matrices;
    for integers, *smaller* means of smaller value, and for matrices, *smaller* means
    of smaller dimensions. When we factor an <math alttext="upper N times upper M"><mrow><mi>N</mi>
    <mo>×</mo> <mi>M</mi></mrow></math> matrix, we will be looking for two matrices
    <math alttext="upper U Subscript upper N times d"><msub><mi>U</mi> <mrow><mi>N</mi><mo>×</mo><mi>d</mi></mrow></msub></math>
    and <math alttext="upper V Subscript d times upper M"><msub><mi>V</mi> <mrow><mi>d</mi><mo>×</mo><mi>M</mi></mrow></msub></math>
    ; note that when you multiply matrices together, they must share a dimension,
    and that dimension is eliminated, leaving the other two. Here, we’ll consider
    MFs when <math alttext="d less-than-or-equal-to upper N"><mrow><mi>d</mi> <mo>≤</mo>
    <mi>N</mi></mrow></math> and <math alttext="d less-than-or-equal-to upper M"><mrow><mi>d</mi>
    <mo>≤</mo> <mi>M</mi></mrow></math> . By factorizing a matrix, we ask for two
    matrices that together equal, or approximate, the original matrix:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 像许多整数一样，许多矩阵可以被*分解*成更小的矩阵；对于整数来说，*更小*意味着更小的值，对于矩阵来说，*更小*意味着更小的维度。当我们分解一个<math
    alttext="upper N times upper M"><mrow><mi>N</mi> <mo>×</mo> <mi>M</mi></mrow></math>矩阵时，我们会寻找两个矩阵<math
    alttext="upper U Subscript upper N times d"><msub><mi>U</mi> <mrow><mi>N</mi><mo>×</mo><mi>d</mi></mrow></msub></math>和<math
    alttext="upper V Subscript d times upper M"><msub><mi>V</mi> <mrow><mi>d</mi><mo>×</mo><mi>M</mi></mrow></msub></math>；请注意，当您将矩阵相乘时，它们必须共享一个维度，并且该维度被消除，留下另外两个维度。在这里，我们将考虑当<math
    alttext="d less-than-or-equal-to upper N"><mrow><mi>d</mi> <mo>≤</mo> <mi>N</mi></mrow></math>和<math
    alttext="d less-than-or-equal-to upper M"><mrow><mi>d</mi> <mo>≤</mo> <mi>M</mi></mrow></math>时的MF。通过分解矩阵，我们要求两个矩阵，它们可以等于或近似于原始矩阵：
- en: <math alttext="upper A Subscript i comma j Baseline asymptotically-equals mathematical
    left-angle upper U Subscript i Baseline comma upper V Subscript j Baseline mathematical
    right-angle" display="block"><mrow><msub><mi>A</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>≃</mo> <mfenced close="〉" open="〈" separators=""><msub><mi>U</mi> <mi>i</mi></msub>
    <mo>,</mo> <msub><mi>V</mi> <mi>j</mi></msub></mfenced></mrow></math>
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper A Subscript i comma j Baseline asymptotically-equals mathematical
    left-angle upper U Subscript i Baseline comma upper V Subscript j Baseline mathematical
    right-angle" display="block"><mrow><msub><mi>A</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>≃</mo> <mfenced close="〉" open="〈" separators=""><msub><mi>U</mi> <mi>i</mi></msub>
    <mo>,</mo> <msub><mi>V</mi> <mi>j</mi></msub></mfenced></mrow></math>
- en: 'We seek a small value for <math alttext="d"><mi>d</mi></math> to reduce the
    number of latent dimensions. As you may have already noticed, each of the matrices
    <math alttext="upper U Subscript upper N times d"><msub><mi>U</mi> <mrow><mi>N</mi><mo>×</mo><mi>d</mi></mrow></msub></math>
    and <math alttext="upper V Subscript d times upper M"><msub><mi>V</mi> <mrow><mi>d</mi><mo>×</mo><mi>M</mi></mrow></msub></math>
    will correspond to rows or columns of the original ratings matrix. However, they’re
    expressed in fewer dimensions. This utilizes the idea of a *low-dimensional latent
    space*. Intuitively, a latent space seeks to represent the same relationships
    as the full <math alttext="upper N times upper M"><mrow><mi>N</mi> <mo>×</mo>
    <mi>M</mi></mrow></math> dimensional relationships in two sets of relationships:
    items versus latent features, and users versus latent features.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们寻求一个小的值<math alttext="d"><mi>d</mi></math>来减少潜在维度的数量。正如您可能已经注意到的那样，每个矩阵<math
    alttext="upper U Subscript upper N times d"><msub><mi>U</mi> <mrow><mi>N</mi><mo>×</mo><mi>d</mi></mrow></msub></math>和<math
    alttext="upper V Subscript d times upper M"><msub><mi>V</mi> <mrow><mi>d</mi><mo>×</mo><mi>M</mi></mrow></msub></math>将对应于原始评分矩阵的行或列。然而，它们以更少的维度表示。这利用了*低维潜在空间*的概念。直观地说，潜在空间旨在以两组关系表示与完整<math
    alttext="upper N times upper M"><mrow><mi>N</mi> <mo>×</mo> <mi>M</mi></mrow></math>维关系相同的关系：项目与潜在特征之间的关系，以及用户与潜在特征之间的关系。
- en: These methods are also popular in other kinds of ML, but for our case, we’ll
    primarily be looking at factorizing ratings or interaction matrices.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法在其他类型的机器学习中也很受欢迎，但对于我们的案例，我们主要将研究对评分或交互矩阵进行因子分解的问题。
- en: 'Frequently, you must overcome a few challenges when considering MF:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑MF时，您经常必须克服一些挑战：
- en: The matrix you wish to factor is sparse and often non-negative and/or binary.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您希望分解的矩阵是稀疏的，并且通常是非负的和/或二进制的。
- en: The number of nonzero elements in each item vector can vary wildly, as we saw
    in the Matthew effect.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个项目向量中的非零元素数量可能会有很大的变化，正如我们在马太效应中看到的那样。
- en: Factorizing matrices is cubic in complexity.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵分解的复杂度是立方的。
- en: SVD and other full-rank methods don’t work without imputation, which itself
    is complicated.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVD和其他全秩方法在没有填补的情况下无法工作，填补本身也很复杂。
- en: We’ll address these with some alternative optimization methods.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一些替代优化方法来解决这些问题。
- en: Optimizing for MF with ALS
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化MF与ALS
- en: 'The basic optimization we wish to execute is to approximate as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望执行的基本优化如下：
- en: <math alttext="upper A Subscript i comma j Baseline asymptotically-equals mathematical
    left-angle upper U Subscript i Baseline comma upper V Subscript j Baseline mathematical
    right-angle" display="block"><mrow><msub><mi>A</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>≃</mo> <mfenced close="〉" open="〈" separators=""><msub><mi>U</mi> <mi>i</mi></msub>
    <mo>,</mo> <msub><mi>V</mi> <mi>j</mi></msub></mfenced></mrow></math>
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper A Subscript i comma j Baseline asymptotically-equals mathematical
    left-angle upper U Subscript i Baseline comma upper V Subscript j Baseline mathematical
    right-angle" display="block"><mrow><msub><mi>A</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>≃</mo> <mfenced close="〉" open="〈" separators=""><msub><mi>U</mi> <mi>i</mi></msub>
    <mo>,</mo> <msub><mi>V</mi> <mi>j</mi></msub></mfenced></mrow></math>
- en: Notably, if you wish to optimize matrix entries directly, you’ll need to optimize
    <math alttext="d squared asterisk upper N asterisk upper M"><mrow><msup><mi>d</mi>
    <mn>2</mn></msup> <mo>*</mo> <mi>N</mi> <mo>*</mo> <mi>M</mi></mrow></math> elements
    simultaneously corresponding to the numbers of parameters in these factorizations.
    We can easily achieve a significant speedup, however, by alternating between tuning
    one matrix or the other. This is called *alternating least squares*, commonly
    *ALS*, and it is a common approach to this problem. Instead of back-propagating
    updates to all terms in both matrices on each pass, you may update only one of
    the two matrices, which dramatically reduces the number of computations that need
    to take place.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，如果您希望直接优化矩阵条目，则需要同时优化<math alttext="d squared asterisk upper N asterisk
    upper M"><mrow><msup><mi>d</mi> <mn>2</mn></msup> <mo>*</mo> <mi>N</mi> <mo>*</mo>
    <mi>M</mi></mrow></math>个元素，这些元素对应于这些因子化中的参数数量。然而，通过交替调整一个或另一个矩阵，我们可以轻松实现显著的加速。这称为*交替最小二乘*，通常简称为*ALS*，这是解决此问题的一种常见方法。与在每次传递中向两个矩阵的所有项后向传播更新不同，您可以仅更新两个矩阵中的一个，从而大大减少需要进行的计算数量。
- en: 'ALS seeks to switch back and forth between <math alttext="upper U"><mi>U</mi></math>
    and <math alttext="upper V"><mi>V</mi></math> , evaluating on the same loss function
    but updating the weights in only one matrix at a time:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ALS 试图在<math alttext="upper U"><mi>U</mi></math>和<math alttext="upper V"><mi>V</mi></math>之间来回切换，评估相同的损失函数，但每次仅更新一个矩阵的权重：
- en: <math alttext="StartLayout 1st Row 1st Column Blank 2nd Column upper U left-arrow
    upper U minus eta asterisk upper U asterisk nabla upper U asterisk script upper
    D left-parenthesis upper A comma upper U upper V right-parenthesis 2nd Row 1st
    Column Blank 2nd Column upper V left-arrow upper V minus eta asterisk upper V
    asterisk nabla upper V asterisk script upper D left-parenthesis upper A comma
    upper U upper V right-parenthesis EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="left"><mrow><mi>U</mi> <mo>←</mo> <mi>U</mi> <mo>-</mo> <mi>η</mi>
    <mo>*</mo> <mi>U</mi> <mo>*</mo> <mi>∇</mi> <mi>U</mi> <mo>*</mo> <mi>𝒟</mi> <mfenced
    close=")" open="(" separators=""><mi>A</mi> <mo>,</mo> <mi>U</mi> <mi>V</mi></mfenced></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mi>V</mi> <mo>←</mo> <mi>V</mi> <mo>-</mo>
    <mi>η</mi> <mo>*</mo> <mi>V</mi> <mo>*</mo> <mi>∇</mi> <mi>V</mi> <mo>*</mo> <mi>𝒟</mi>
    <mfenced close=")" open="(" separators=""><mi>A</mi> <mo>,</mo> <mi>U</mi> <mi>V</mi></mfenced></mrow></mtd></mtr></mtable></math>
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column Blank 2nd Column upper U left-arrow
    upper U minus eta asterisk upper U asterisk nabla upper U asterisk script upper
    D left-parenthesis upper A comma upper U upper V right-parenthesis 2nd Row 1st
    Column Blank 2nd Column upper V left-arrow upper V minus eta asterisk upper V
    asterisk nabla upper V asterisk script upper D left-parenthesis upper A comma
    upper U upper V right-parenthesis EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="left"><mrow><mi>U</mi> <mo>←</mo> <mi>U</mi> <mo>-</mo> <mi>η</mi>
    <mo>*</mo> <mi>U</mi> <mo>*</mo> <mi>∇</mi> <mi>U</mi> <mo>*</mo> <mi>𝒟</mi> <mfenced
    close=")" open="(" separators=""><mi>A</mi> <mo>,</mo> <mi>U</mi> <mi>V</mi></mfenced></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mi>V</mi> <mo>←</mo> <mi>V</mi> <mo>-</mo>
    <mi>η</mi> <mo>*</mo> <mi>V</mi> <mo>*</mo> <mi>∇</mi> <mi>V</mi> <mo>*</mo> <mi>𝒟</mi>
    <mfenced close=")" open="(" separators=""><mi>A</mi> <mo>,</mo> <mi>U</mi> <mi>V</mi></mfenced></mrow></mtd></mtr></mtable></math>
- en: 'Here, <math alttext="eta"><mi>η</mi></math> is the learning rate and <math
    alttext="script upper D"><mi>𝒟</mi></math> is our chosen distance function. We’ll
    present more details of this distance function momentarily. Before we move on,
    let’s consider a few of the intricacies here:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，<math alttext="eta"><mi>η</mi></math>是学习率，<math alttext="script upper D"><mi>𝒟</mi></math>是我们选择的距离函数。我们稍后会详细介绍这个距离函数。在我们继续之前，让我们考虑这里的一些复杂性：
- en: Each of these update rules requires the gradient with respect to the relevant
    factor matrix.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个更新规则都需要相对于相关因子矩阵的梯度。
- en: We update an entire factor matrix at a time, but we evaluate loss on the product
    of the factor matrices versus the original matrix.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们一次更新一个完整的因子矩阵，但在因子矩阵的乘积与原始矩阵之间评估损失。
- en: We have a mysterious distance function.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有一个神秘的距离函数。
- en: By the way that we’ve constructed this optimization, we’re implicitly assuming
    that we’ll use this process to converge two well-approximating matrices (we often
    also impose a limit on the number of iterations).
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过我们构建这种优化的方式，我们隐含地假设我们将使用这个过程来使两个近似的矩阵收敛（通常还会对迭代次数施加限制）。
- en: In JAX, these optimizations will be straightforward to implement, and we’ll
    see how similar the equational forms and the JAX code look.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在 JAX 中，这些优化将很容易实现，并且我们将看到等式形式和 JAX 代码看起来有多么相似。
- en: Distance Between Matrices
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矩阵之间的距离
- en: 'We can determine the distance between two matrices in a variety of ways. As
    we’ve seen before, different distance measurements for vectors yield different
    interpretations from the underlying space. We won’t have as many complications
    for these computations, but it’s worth a short observation. The most obvious approach
    is one you’ve already seen, *observed mean squared error*:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以以多种方式确定两个矩阵之间的距离。正如我们之前所见，对于向量的不同距离测量从底层空间提供了不同的解释。对于这些计算，我们不会有太多复杂性，但这值得一小段观察。最明显的方法是你已经看到的*观察到的均方误差*：
- en: <math alttext="StartFraction sigma-summation Underscript normal upper Omega
    Endscripts left-parenthesis upper A Subscript i comma j Baseline minus mathematical
    left-angle upper U Subscript i Baseline comma upper V Subscript j Baseline mathematical
    right-angle right-parenthesis squared Over 1 normal upper Omega vertical-bar EndFraction"><mstyle
    displaystyle="true" scriptlevel="0"><mfrac><mrow><msub><mo>∑</mo> <mi>Ω</mi></msub>
    <msup><mfenced close=")" open="(" separators=""><msub><mi>A</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>-</mo><mfenced close="〉" open="〈" separators=""><msub><mi>U</mi> <mi>i</mi></msub>
    <mo>,</mo><msub><mi>V</mi> <mi>j</mi></msub></mfenced></mfenced> <mn>2</mn></msup></mrow>
    <mrow><mn>1</mn><mi>Ω</mi><mo>|</mo></mrow></mfrac></mstyle></math>
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartFraction sigma-summation Underscript normal upper Omega
    Endscripts left-parenthesis upper A Subscript i comma j Baseline minus mathematical
    left-angle upper U Subscript i Baseline comma upper V Subscript j Baseline mathematical
    right-angle right-parenthesis squared Over 1 normal upper Omega vertical-bar EndFraction"><mstyle
    displaystyle="true" scriptlevel="0"><mfrac><mrow><msub><mo>∑</mo> <mi>Ω</mi></msub>
    <msup><mfenced close=")" open="(" separators=""><msub><mi>A</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>-</mo><mfenced close="〉" open="〈" separators=""><msub><mi>U</mi> <mi>i</mi></msub>
    <mo>,</mo><msub><mi>V</mi> <mi>j</mi></msub></mfenced></mfenced> <mn>2</mn></msup></mrow>
    <mrow><mn>1</mn><mi>Ω</mi><mo>|</mo></mrow></mfrac></mstyle></math>
- en: One useful alternative to the observed mean squared error can be used when you
    have a single nonzero entry for a user vector (alternatively, a max rating). In
    that case, you could instead use a cross-entropy loss, which provides a *logistic
    MF*, and thus a probability estimate. For more details on how to implement this,
    see the [“Matrix Factorization for Recommender Systems” tutorial](https://oreil.ly/7qWy6)
    by Kyle Chung.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户向量只有一个非零条目（或者说最大评分）时，观察到的均方误差的一个有用替代方法是使用交叉熵损失，这样可以提供一个*逻辑MF*，从而得到概率估计。有关如何实现此操作的更多详细信息，请参阅Kyle
    Chung的[“推荐系统的矩阵分解”教程](https://oreil.ly/7qWy6)。
- en: In our observed ratings, we expect (and see!) a large number of missing values
    and some item vectors with an overrepresented number of ratings. This suggests
    that we should consider nonuniformly weighted matrices. Next we’ll discuss how
    to account for this and other variants with regularization.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们观察到的评分中，我们预期（并看到！）有大量缺失值和一些具有过多评分的项目向量。这表明我们应该考虑非均匀加权的矩阵。接下来我们将讨论如何通过正则化来考虑这些和其他变体。
- en: Regularization for MF
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MF的正则化
- en: '*Weighted alternating least squares* (WALS) is similar to ALS but attempts
    to resolve these two data issues more gracefully. In WALS, the weight assigned
    to each observed rating is inversely proportional to the number of observed ratings
    for the user or item. Therefore, observed ratings for users or items with few
    ratings are given more weight in the optimization process.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*加权交替最小二乘*（WALS）与ALS类似，但试图更优雅地解决这两个数据问题。在WALS中，分配给每个观察到的评分的权重与用户或项目的观察到的评分数成反比。因此，对于评分较少的用户或项目的观察到的评分在优化过程中被赋予更大的权重。'
- en: 'We can apply these weights as a regularization parameter in our eventual loss
    function:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些权重作为我们最终损失函数中的正则化参数应用：
- en: <math alttext="StartFraction sigma-summation Underscript normal upper Omega
    Endscripts left-parenthesis upper A Subscript i comma j Baseline minus less-than
    upper U Subscript i Baseline comma upper V Subscript j Baseline greater-than right-parenthesis
    squared Over StartAbsoluteValue normal upper Omega EndAbsoluteValue EndFraction
    plus StartFraction 1 Over upper N EndFraction sigma-summation StartAbsoluteValue
    upper U EndAbsoluteValue" display="block"><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><msub><mo>∑</mo>
    <mi>Ω</mi></msub> <msup><mfenced close=")" open="(" separators=""><msub><mi>A</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub> <mo>-</mo><mo><</mo><msub><mi>U</mi>
    <mi>i</mi></msub> <mo>,</mo><msub><mi>V</mi> <mi>j</mi></msub> <mo>></mo></mfenced>
    <mn>2</mn></msup></mrow> <mrow><mo>|</mo><mi>Ω</mi><mo>|</mo></mrow></mfrac></mstyle>
    <mo>+</mo> <mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn> <mi>N</mi></mfrac></mstyle>
    <mo>∑</mo> <mrow><mo>|</mo> <mi>U</mi> <mo>|</mo></mrow></mrow></math>
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartFraction sigma-summation Underscript normal upper Omega
    Endscripts left-parenthesis upper A Subscript i comma j Baseline minus less-than
    upper U Subscript i Baseline comma upper V Subscript j Baseline greater-than right-parenthesis
    squared Over StartAbsoluteValue normal upper Omega EndAbsoluteValue EndFraction
    plus StartFraction 1 Over upper N EndFraction sigma-summation StartAbsoluteValue
    upper U EndAbsoluteValue" display="block"><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><msub><mo>∑</mo>
    <mi>Ω</mi></msub> <msup><mfenced close=")" open="(" separators=""><msub><mi>A</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub> <mo>-</mo><mo><</mo><msub><mi>U</mi>
    <mi>i</mi></msub> <mo>,</mo><msub><mi>V</mi> <mi>j</mi></msub> <mo>></mo></mfenced>
    <mn>2</mn></msup></mrow> <mrow><mo>|</mo><mi>Ω</mi><mo>|</mo></mrow></mfrac></mstyle>
    <mo>+</mo> <mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn> <mi>N</mi></mfrac></mstyle>
    <mo>∑</mo> <mrow><mo>|</mo> <mi>U</mi> <mo>|</mo></mrow></mrow></math>
- en: 'Other regularization methods are important, and also popular, for MF. We’ll
    discuss these two powerful regularization techniques:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其他正则化方法对于MF也很重要，并且很受欢迎。我们将讨论这两种强大的正则化技术：
- en: Weight decay
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重衰减
- en: Gramian regularization
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gramian正则化
- en: As is often the case, *weight decay* is our <math alttext="l squared"><msup><mi>l</mi>
    <mn>2</mn></msup></math> regularization, which in this case is at the level of
    the Frobenius norm, i.e., the magnitude of the weight matrix. An elegant way to
    view this weight decay is that it’s minimizing the magnitude of the *singular
    values*.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，*权重衰减* 是我们的 <math alttext="l squared"><msup><mi>l</mi> <mn>2</mn></msup></math>
    正则化，这种情况下是Frobenius范数的水平，即权重矩阵的大小。一个优雅的观点是，这种权重衰减是在最小化*奇异值*的大小。
- en: Similarly, MF has another regularization technique that looks very standard
    but is quite different in calculation. This is via the *Gramians*—essentially
    regularizing the size of the individual matrix entries, but there’s an elegant
    trick for the optimization. In particular, a Gramian of a matrix <math alttext="upper
    U"><mi>U</mi></math> is the product <math alttext="upper U Superscript upper T
    Baseline upper U"><mrow><msup><mi>U</mi> <mi>T</mi></msup> <mi>U</mi></mrow></math>
    . The eagle-eyed among you may recognize this term as the same term we previously
    used to calculate co-occurrences for binary matrices. The connection is that both
    are simply trying to find efficient representations of dot products between a
    matrix’s rows and columns.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，MF还有另一种看起来非常标准但计算方式完全不同的正则化技术。这是通过Gramians——本质上是正则化单个矩阵条目的大小，但在优化中有一个优雅的技巧。特别地，矩阵
    <math alttext="upper U"><mi>U</mi></math> 的Gramian是乘积 <math alttext="upper U Superscript
    upper T Baseline upper U"><mrow><msup><mi>U</mi> <mi>T</mi></msup> <mi>U</mi></mrow></math>
    。敏锐的读者可能会认出这个术语，因为它与我们先前用于计算二进制矩阵的共现的术语相同。联系在于两者都只是试图找到矩阵行和列之间点积的有效表示方式。
- en: 'These regularizations are the Frobenius terms:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这些正则化是Frobenius项：
- en: <math alttext="upper R left-parenthesis upper U comma upper V right-parenthesis
    equals StartFraction 1 Over upper N EndFraction sigma-summation Underscript i
    Overscript upper N Endscripts StartAbsoluteValue upper U Subscript i Baseline
    EndAbsoluteValue Subscript 2 Superscript 2 Baseline plus StartFraction 1 Over
    upper M EndFraction sigma-summation Underscript j Overscript upper M Endscripts
    StartAbsoluteValue upper V Subscript j Baseline EndAbsoluteValue Subscript 2 Superscript
    2" display="block"><mstyle displaystyle="true" scriptlevel="0"><mrow><mi>R</mi>
    <mrow><mo>(</mo> <mi>U</mi> <mo>,</mo> <mi>V</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mfrac><mn>1</mn> <mi>N</mi></mfrac> <munderover><mo>∑</mo> <mi>i</mi> <mi>N</mi></munderover>
    <mrow><mo>|</mo></mrow> <msub><mi>U</mi> <mi>i</mi></msub> <msubsup><mrow><mo>|</mo></mrow>
    <mn>2</mn> <mn>2</mn></msubsup> <mo>+</mo> <mfrac><mn>1</mn> <mi>M</mi></mfrac>
    <munderover><mo>∑</mo> <mi>j</mi> <mi>M</mi></munderover> <msubsup><mrow><mo>|</mo><msub><mi>V</mi>
    <mi>j</mi></msub> <mo>|</mo></mrow> <mn>2</mn> <mn>2</mn></msubsup></mrow></mstyle></math>
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper R left-parenthesis upper U comma upper V right-parenthesis
    equals StartFraction 1 Over upper N EndFraction sigma-summation Underscript i
    Overscript upper N Endscripts StartAbsoluteValue upper U Subscript i Baseline
    EndAbsoluteValue Subscript 2 Superscript 2 Baseline plus StartFraction 1 Over
    upper M EndFraction sigma-summation Underscript j Overscript upper M Endscripts
    StartAbsoluteValue upper V Subscript j Baseline EndAbsoluteValue Subscript 2 Superscript
    2" display="block"><mstyle displaystyle="true" scriptlevel="0"><mrow><mi>R</mi>
    <mrow><mo>(</mo> <mi>U</mi> <mo>,</mo> <mi>V</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mfrac><mn>1</mn> <mi>N</mi></mfrac> <munderover><mo>∑</mo> <mi>i</mi> <mi>N</mi></munderover>
    <mrow><mo>|</mo></mrow> <msub><mi>U</mi> <mi>i</mi></msub> <msubsup><mrow><mo>|</mo></mrow>
    <mn>2</mn> <mn>2</mn></msubsup> <mo>+</mo> <mfrac><mn>1</mn> <mi>M</mi></mfrac>
    <munderover><mo>∑</mo> <mi>j</mi> <mi>M</mi></munderover> <msubsup><mrow><mo>|</mo><msub><mi>V</mi>
    <mi>j</mi></msub> <mo>|</mo></mrow> <mn>2</mn> <mn>2</mn></msubsup></mrow></mstyle></math>
- en: 'Or expanded, the equation looks like this:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，展开来说，方程看起来像这样：
- en: <math alttext="upper R left-parenthesis upper U comma upper V right-parenthesis
    equals StartFraction 1 Over upper N EndFraction sigma-summation Underscript i
    Overscript upper N Endscripts sigma-summation Underscript k Overscript d Endscripts
    upper U Subscript i comma k Superscript 2 Baseline plus StartFraction 1 Over upper
    M EndFraction sigma-summation Underscript j Overscript upper M Endscripts sigma-summation
    Underscript l Overscript d Endscripts upper V Subscript j comma l Superscript
    2" display="block"><mstyle displaystyle="true" scriptlevel="0"><mrow><mi>R</mi>
    <mrow><mo>(</mo> <mi>U</mi> <mo>,</mo> <mi>V</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mfrac><mn>1</mn> <mi>N</mi></mfrac> <munderover><mo>∑</mo> <mi>i</mi> <mi>N</mi></munderover>
    <munderover><mo>∑</mo> <mi>k</mi> <mi>d</mi></munderover> <msubsup><mi>U</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow> <mn>2</mn></msubsup> <mo>+</mo> <mfrac><mn>1</mn>
    <mi>M</mi></mfrac> <munderover><mo>∑</mo> <mi>j</mi> <mi>M</mi></munderover> <munderover><mo>∑</mo>
    <mi>l</mi> <mi>d</mi></munderover> <msubsup><mi>V</mi> <mrow><mi>j</mi><mo>,</mo><mi>l</mi></mrow>
    <mn>2</mn></msubsup></mrow></mstyle></math>
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper R left-parenthesis upper U comma upper V right-parenthesis
    equals StartFraction 1 Over upper N EndFraction sigma-summation Underscript i
    Overscript upper N Endscripts sigma-summation Underscript k Overscript d Endscripts
    upper U Subscript i comma k Superscript 2 Baseline plus StartFraction 1 Over upper
    M EndFraction sigma-summation Underscript j Overscript upper M Endscripts sigma-summation
    Underscript l Overscript d Endscripts upper V Subscript j comma l Superscript
    2" display="block"><mstyle displaystyle="true" scriptlevel="0"><mrow><mi>R</mi>
    <mrow><mo>(</mo> <mi>U</mi> <mo>,</mo> <mi>V</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mfrac><mn>1</mn> <mi>N</mi></mfrac> <munderover><mo>∑</mo> <mi>i</mi> <mi>N</mi></munderover>
    <munderover><mo>∑</mo> <mi>k</mi> <mi>d</mi></munderover> <msubsup><mi>U</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow> <mn>2</mn></msubsup> <mo>+</mo> <mfrac><mn>1</mn>
    <mi>M</mi></mfrac> <munderover><mo>∑</mo> <mi>j</mi> <mi>M</mi></munderover> <munderover><mo>∑</mo>
    <mi>l</mi> <mi>d</mi></munderover> <msubsup><mi>V</mi> <mrow><mi>j</mi><mo>,</mo><mi>l</mi></mrow>
    <mn>2</mn></msubsup></mrow></mstyle></math>
- en: 'And here are the Gramian terms:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是Gramian项：
- en: <math alttext="StartLayout 1st Row 1st Column upper G left-parenthesis upper
    U comma upper V right-parenthesis 2nd Column colon equals StartFraction 1 Over
    upper N dot upper M EndFraction sigma-summation Underscript i Overscript upper
    N Endscripts sigma-summation Underscript j Overscript upper M Endscripts mathematical
    left-angle upper U Subscript i Baseline comma upper V Subscript j Baseline mathematical
    right-angle squared 2nd Row 1st Column Blank 3rd Row 1st Column Blank 2nd Column
    equals StartFraction 1 Over upper N dot upper M EndFraction asterisk sigma-summation
    Underscript k comma l Overscript d Endscripts left-parenthesis upper U Superscript
    upper T Baseline upper U asterisk upper V Superscript upper T Baseline upper V
    right-parenthesis Subscript k comma l Baseline EndLayout period" display="block"><mrow><mtable><mtr><mtd
    columnalign="left"><mrow><mi>G</mi> <mo>(</mo> <mi>U</mi> <mo>,</mo> <mi>V</mi>
    <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mstyle displaystyle="true" scriptlevel="0"><mrow><mo>:</mo>
    <mo>=</mo> <mfrac><mn>1</mn> <mrow><mi>N</mi><mo>·</mo><mi>M</mi></mrow></mfrac>
    <munderover><mo>∑</mo> <mi>i</mi> <mi>N</mi></munderover> <munderover><mo>∑</mo>
    <mi>j</mi> <mi>M</mi></munderover> <msup><mfenced close="〉" open="〈" separators=""><msub><mi>U</mi>
    <mi>i</mi></msub> <mo>,</mo><msub><mi>V</mi> <mi>j</mi></msub></mfenced> <mn>2</mn></msup></mrow></mstyle></mtd></mtr>
    <mtr><mtd columnalign="left"><mstyle displaystyle="true" scriptlevel="0"><mrow><mo>=</mo>
    <mfrac><mn>1</mn> <mrow><mi>N</mi><mo>·</mo><mi>M</mi></mrow></mfrac> <mo>*</mo>
    <munderover><mo>∑</mo> <mrow><mi>k</mi><mo>,</mo><mi>l</mi></mrow> <mi>d</mi></munderover>
    <msub><mfenced close=")" open="(" separators=""><msup><mi>U</mi> <mi>T</mi></msup>
    <mi>U</mi><mo>*</mo><msup><mi>V</mi> <mi>T</mi></msup> <mi>V</mi></mfenced> <mrow><mi>k</mi><mo>,</mo><mi>l</mi></mrow></msub></mrow></mstyle></mtd></mtr></mtable>
    <mo>.</mo></mrow></math>
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column upper G left-parenthesis upper
    U comma upper V right-parenthesis 2nd Column colon equals StartFraction 1 Over
    upper N dot upper M EndFraction sigma-summation Underscript i Overscript upper
    N Endscripts sigma-summation Underscript j Overscript upper M Endscripts mathematical
    left-angle upper U Subscript i Baseline comma upper V Subscript j Baseline mathematical
    right-angle squared 2nd Row 1st Column Blank 3rd Row 1st Column Blank 2nd Column
    equals StartFraction 1 Over upper N dot upper M EndFraction asterisk sigma-summation
    Underscript k comma l Overscript d Endscripts left-parenthesis upper U Superscript
    upper T Baseline upper U asterisk upper V Superscript upper T Baseline upper V
    right-parenthesis Subscript k comma l Baseline EndLayout period" display="block"><mrow><mtable><mtr><mtd
    columnalign="left"><mrow><mi>G</mi> <mo>(</mo> <mi>U</mi> <mo>,</mo> <mi>V</mi>
    <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mstyle displaystyle="true" scriptlevel="0"><mrow><mo>:</mo>
    <mo>=</mo> <mfrac><mn>1</mn> <mrow><mi>N</mi><mo>·</mo><mi>M</mi></mrow></mfrac>
    <munderover><mo>∑</mo> <mi>i</mi> <mi>N</mi></munderover> <munderover><mo>∑</mo>
    <mi>j</mi> <mi>M</mi></munderover> <msup><mfenced close="〉" open="〈" separators=""><msub><mi>U</mi>
    <mi>i</mi></msub> <mo>,</mo><msub><mi>V</mi> <mi>j</mi></msub></mfenced> <mn>2</mn></msup></mrow></mstyle></mtd></mtr>
    <mtr><mtd columnalign="left"><mstyle displaystyle="true" scriptlevel="0"><mrow><mo>=</mo>
    <mfrac><mn>1</mn> <mrow><mi>N</mi><mo>·</mo><mi>M</mi></mrow></mfrac> <mo>*</mo>
    <munderover><mo>∑</mo> <mrow><mi>k</mi><mo>,</mo><mi>l</mi></mrow> <mi>d</mi></munderover>
    <msub><mfenced close=")" open="(" separators=""><msup><mi>U</mi> <mi>T</mi></msup>
    <mi>U</mi><mo>*</mo><msup><mi>V</mi> <mi>T</mi></msup> <mi>V</mi></mfenced> <mrow><mi>k</mi><mo>,</mo><mi>l</mi></mrow></msub></mrow></mstyle></mtd></mtr></mtable>
    <mo>.</mo></mrow></math>
- en: 'Finally, we have our loss function:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有我们的损失函数：
- en: <math alttext="StartFraction 1 Over StartAbsoluteValue normal upper Omega EndAbsoluteValue
    EndFraction sigma-summation Underscript left-parenthesis i comma j right-parenthesis
    element-of normal upper Omega Endscripts left-parenthesis upper A Subscript i
    j Baseline minus mathematical left-angle upper U Subscript i Baseline comma upper
    V Subscript j Baseline mathematical right-angle right-parenthesis squared plus
    lamda Subscript upper R Baseline left-parenthesis StartFraction 1 Over upper N
    EndFraction sigma-summation Underscript i Overscript upper N Endscripts sigma-summation
    Underscript k Overscript d Endscripts upper U Subscript i comma k Superscript
    2 Baseline plus StartFraction 1 Over upper M EndFraction sigma-summation Underscript
    j Overscript upper M Endscripts sigma-summation Underscript l Overscript d Endscripts
    upper V Subscript j comma l Superscript 2 Baseline right-parenthesis plus lamda
    Subscript upper G Baseline left-parenthesis StartFraction 1 Over upper N dot upper
    M EndFraction asterisk sigma-summation Underscript k comma l Overscript d Endscripts
    left-parenthesis upper U Superscript upper T Baseline upper U asterisk upper V
    Superscript upper T Baseline upper V right-parenthesis Subscript k comma l Baseline
    right-parenthesis" display="block"><mstyle displaystyle="true" scriptlevel="0"><mrow><mfrac><mn>1</mn>
    <mrow><mo>|</mo><mi>Ω</mi><mo>|</mo></mrow></mfrac> <munder><mo>∑</mo> <mrow><mo>(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo>)</mo><mo>∈</mo><mi>Ω</mi></mrow></munder>
    <msup><mrow><mo>(</mo><msub><mi>A</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mo>-</mo><mrow><mo>〈</mo><msub><mi>U</mi> <mi>i</mi></msub> <mo>,</mo><msub><mi>V</mi>
    <mi>j</mi></msub> <mo>〉</mo></mrow><mo>)</mo></mrow> <mn>2</mn></msup> <mo>+</mo>
    <msub><mi>λ</mi> <mi>R</mi></msub> <mfenced close=")" open="(" separators=""><mfrac><mn>1</mn>
    <mi>N</mi></mfrac> <munderover><mo>∑</mo> <mi>i</mi> <mi>N</mi></munderover> <munderover><mo>∑</mo>
    <mi>k</mi> <mi>d</mi></munderover> <msubsup><mi>U</mi> <mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow>
    <mn>2</mn></msubsup> <mo>+</mo> <mfrac><mn>1</mn> <mi>M</mi></mfrac> <munderover><mo>∑</mo>
    <mi>j</mi> <mi>M</mi></munderover> <munderover><mo>∑</mo> <mi>l</mi> <mi>d</mi></munderover>
    <msubsup><mi>V</mi> <mrow><mi>j</mi><mo>,</mo><mi>l</mi></mrow> <mn>2</mn></msubsup></mfenced>
    <mo>+</mo> <msub><mi>λ</mi> <mi>G</mi></msub> <mfenced close=")" open="(" separators=""><mfrac><mn>1</mn>
    <mrow><mi>N</mi><mo>·</mo><mi>M</mi></mrow></mfrac> <mo>*</mo> <munderover><mo>∑</mo>
    <mrow><mi>k</mi><mo>,</mo><mi>l</mi></mrow> <mi>d</mi></munderover> <msub><mfenced
    close=")" open="(" separators=""><msup><mi>U</mi> <mi>T</mi></msup> <mi>U</mi><mo>*</mo><msup><mi>V</mi>
    <mi>T</mi></msup> <mi>V</mi></mfenced> <mrow><mi>k</mi><mo>,</mo><mi>l</mi></mrow></msub></mfenced></mrow></mstyle></math>
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartFraction 1 Over StartAbsoluteValue normal upper Omega EndAbsoluteValue
    EndFraction sigma-summation Underscript left-parenthesis i comma j right-parenthesis
    element-of normal upper Omega Endscripts left-parenthesis upper A Subscript i
    j Baseline minus mathematical left-angle upper U Subscript i Baseline comma upper
    V Subscript j Baseline mathematical right-angle right-parenthesis squared plus
    lamda Subscript upper R Baseline left-parenthesis StartFraction 1 Over upper N
    EndFraction sigma-summation Underscript i Overscript upper N Endscripts sigma-summation
    Underscript k Overscript d Endscripts upper U Subscript i comma k Superscript
    2 Baseline plus StartFraction 1 Over upper M EndFraction sigma-summation Underscript
    j Overscript upper M Endscripts sigma-summation Underscript l Overscript d Endscripts
    upper V Subscript j comma l Superscript 2 Baseline right-parenthesis plus lamda
    Subscript upper G Baseline left-parenthesis StartFraction 1 Over upper N dot upper
    M EndFraction asterisk sigma-summation Underscript k comma l Overscript d Endscripts
    left-parenthesis upper U Superscript upper T Baseline upper U asterisk upper V
    Superscript upper T Baseline upper V right-parenthesis Subscript k comma l Baseline
    right-parenthesis" display="block"><mstyle displaystyle="true" scriptlevel="0"><mrow><mfrac><mn>1</mn>
    <mrow><mo>|</mo><mi>Ω</mi><mo>|</mo></mrow></mfrac> <munder><mo>∑</mo> <mrow><mo>(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo>)</mo><mo>∈</mo><mi>Ω</mi></mrow></munder>
    <msup><mrow><mo>(</mo><msub><mi>A</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mo>-</mo><mrow><mo>〈</mo><msub><mi>U</mi> <mi>i</mi></msub> <mo>,</mo><msub><mi>V</mi>
    <mi>j</mi></msub> <mo>〉</mo></mrow><mo>)</mo></mrow> <mn>2</mn></msup> <mo>+</mo>
    <msub><mi>λ</mi> <mi>R</mi></msub> <mfenced close=")" open="(" separators=""><mfrac><mn>1</mn>
    <mi>N</mi></mfrac> <munderover><mo>∑</mo> <mi>i</mi> <mi>N</mi></munderover> <munderover><mo>∑</mo>
    <mi>k</mi> <mi>d</mi></munderover> <msubsup><mi>U</mi> <mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow>
    <mn>2</mn></msubsup> <mo>+</mo> <mfrac><mn>1</mn> <mi>M</mi></mfrac> <munderover><mo>∑</mo>
    <mi>j</mi> <mi>M</mi></munderover> <munderover><mo>∑</mo> <mi>l</mi> <mi>d</mi></munderover>
    <msubsup><mi>V</mi> <mrow><mi>j</mi><mo>,</mo><mi>l</mi></mrow> <mn>2</mn></msubsup></mfenced>
    <mo>+</mo> <msub><mi>λ</mi> <mi>G</mi></msub> <mfenced close=")" open="(" separators=""><mfrac><mn>1</mn>
    <mrow><mi>N</mi><mo>·</mo><mi>M</mi></mrow></mfrac> <mo>*</mo> <munderover><mo>∑</mo>
    <mrow><mi>k</mi><mo>,</mo><mi>l</mi></mrow> <mi>d</mi></munderover> <msub><mfenced
    close=")" open="(" separators=""><msup><mi>U</mi> <mi>T</mi></msup> <mi>U</mi><mo>*</mo><msup><mi>V</mi>
    <mi>T</mi></msup> <mi>V</mi></mfenced> <mrow><mi>k</mi><mo>,</mo><mi>l</mi></mrow></msub></mfenced></mrow></mstyle></math>
- en: Regularized MF Implementation
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化的MF实现
- en: So far, we’ve written a *lot* of math symbols, but all of those symbols have
    allowed us to arrive at a model that is extremely powerful. *Regularized matrix
    factorization* is an effective model for medium-sized recommender problems. This
    model type is still in production for many serious businesses. One classic issue
    with MF implementations is performance, but because we’re working in JAX, which
    has extremely native GPU support, our implementation can actually be much more
    compact than what you may find in something like a [PyTorch example](https://oreil.ly/U-K-V).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经写了*很多*数学符号，但所有这些符号都使我们能够得出一个非常强大的模型。*正则化矩阵因子分解*是一个有效的中等规模推荐问题模型。这种模型类型在许多严肃的企业中仍然在使用。MF实现的一个经典问题是性能，但因为我们使用JAX，在这里有极好的本地GPU支持，我们的实现实际上可以比像[PyTorch示例](https://oreil.ly/U-K-V)中找到的更紧凑。
- en: Let’s work through how this model would look to predict ratings for a user-item
    matrix via this doubly regularized model with Gramians.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过Gramians的双重正则化模型来预测用户-项目矩阵中的评分会是怎样的模型。
- en: 'First we’ll do the simple setup. This will assume your ratings matrix is already
    on wandb:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将进行简单的设置。这将假设您的评分矩阵已经在wandb上了：
- en: '[PRE1]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that we’ve had to implement our own loss function here. This is a relatively
    straightforward mean square error (MSE) loss, but it’s taking advantage of the
    sparse nature of our matrix. You may notice in the code that we’ve converted the
    matrix to a sparse representation, so it’s important that our loss function cannot
    only take advantage of that representation, but also be written to utilize the
    JAX device arrays and mapping/jitting.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们不得不在这里实现我们自己的损失函数。这是一个相对简单的均方误差（MSE）损失，但它利用了我们矩阵的稀疏性质。你可能会在代码中注意到，我们已经将矩阵转换为稀疏表示，因此我们的损失函数不仅可以利用该表示，还可以编写为利用JAX设备数组和映射/编译的形式。
- en: Is That Loss Function Really Right?
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这个损失函数真的正确吗？
- en: If you’re curious about this loss function that appeared like magic, we understand.
    While writing this book, we were extremely uncertain about what the best implementation
    of this loss function that leverages JAX would look like. There are actually many
    reasonable approaches to this kind of optimization. To that end, we wrote a public
    experiment to benchmark several approaches [on Colab](https://oreil.ly/6zwEX).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对这种像魔术般出现的损失函数感到好奇，我们理解。在写这本书的时候，我们对利用JAX的这种损失函数的最佳实现非常不确定。实际上，有很多合理的方法可以进行这种优化。为此，我们编写了一个公共实验来对几种方法进行基准测试[在Colab上](https://oreil.ly/6zwEX)。
- en: 'Next, we need to build model objects to handle our MF state as we train. This
    code, while essentially mostly template code, will set us up well to feed the
    model into a training loop in a relatively memory-efficient way. This model was
    trained on 100 million entries for a few thousand epochs on a MacBook Pro in less
    than a day:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要构建模型对象来处理我们训练时的MF状态。尽管这段代码基本上大多是模板代码，但它会以相对内存高效的方式为我们设置好，以便将模型馈送到训练循环中。这个模型是在一台MacBook
    Pro上，在不到一天的时间里，对100万条记录进行了几千次迭代的训练：
- en: '[PRE2]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We should also set this up to log nicely to wandb so it’s easy to understand
    what is happening during training:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应该设置好，以便在wandb上进行良好的记录，这样在训练过程中就很容易理解发生了什么：
- en: '[PRE3]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that this code is using `tree_multimap` to handle broadcasting our update
    rule, and we’re using the jitted loss from before in the `omse_loss` call. Also,
    we’re calling `value_and_grad` so we can log the loss to wandb as we go. This
    is a common trick you’ll see for efficiently doing both without a callback.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此代码正在使用`tree_multimap`来处理广播我们的更新规则，并且我们正在调用以前的`omse_loss`调用中的被jit的损失。另外，我们正在调用`value_and_grad`，这样我们就可以在进行时将损失记录到wandb中。这是一个常见的技巧，你会看到这样的技巧既可以高效地做到这两点，又可以不使用回调。
- en: 'You can finish this off and start the training with a sweep:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以完成这个过程，并开始进行扫描：
- en: '[PRE4]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In this case, the hyperparameter optimization (HPO) is over our hyperparameters
    like embedding dimension and the priors (randomized matrices). Up until now, we
    have trained some MF models on our ratings matrix. Let’s now add regularization
    and cross-validation.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，超参数优化（HPO）是关于我们的超参数，如嵌入维度和先验（随机矩阵）的。到目前为止，我们已经在我们的评分矩阵上训练了一些MF模型。现在让我们添加正则化和交叉验证。
- en: 'Let’s translate the preceding math equations directly into code:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接将前述数学方程翻译成代码：
- en: '[PRE5]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We won’t dive super deep into learning rate schedulers, but we will do a simple
    decay:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入研究学习率调度器，但我们会做一个简单的衰减：
- en: '[PRE6]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Our updated train function will incorporate our new regularizations—which come
    with some hyperparameters—and a bit of additional logging setup. This code makes
    it easy to log our experiment as it trains and configures the hyperparameters
    to work with regularization:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们更新的训练函数将整合我们的新正则化 - 这些正则化带有一些超参数 - 以及一些额外的日志设置。这段代码使得在训练过程中记录我们的实验并配置超参数以与正则化一起工作变得容易：
- en: '[PRE7]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The last step is to do this in a way that gives us confidence in the models
    we’re seeing. Unfortunately, setting up cross-validation for MF problems can be
    tricky, so we’ll need to make a few modifications to our data structures:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是以一种让我们对所见到的模型感到自信的方式来进行。不幸的是，为MF问题设置交叉验证可能会很棘手，因此我们需要对我们的数据结构进行一些修改：
- en: '[PRE8]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Each hyperparameter setup should yield loss for each fold, so within *wandb.init*,
    we build a model with each fold:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 每个超参数设置应该为每个fold产生损失，因此在*wandb.init*中，我们为每个fold构建一个模型：
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'At each step, we’d like to not only compute the gradient for the training and
    evaluate on the test but also compute gradients for all folds, evaluate on all
    the tests, and produce the relevant errors:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一步，我们不仅想计算训练的梯度并在测试集上评估，还想为所有折叠计算梯度，评估所有测试，并生成相关的错误：
- en: '[PRE10]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Logging should be losses per fold, and the aggregate loss should be the target
    metric. This is because each fold is an independent optimization of the model
    parameters; however, we wish to see aggregate behavior across the folds:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 日志应为每个折叠的损失，并且聚合损失应为目标指标。这是因为每个折叠都是模型参数的独立优化；但是，我们希望看到跨折叠的聚合行为：
- en: '[PRE11]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We wrap up into one big training method:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将所有内容整合成一个大的训练方法：
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here’s our final sweeps configuration:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的最终扫描配置：
- en: '[PRE13]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: That may seem like a lot of setup, but we’ve really achieved a lot here. We’ve
    initialized the model to optimize the two matrix factors while simultaneously
    keeping the matrix elements and the Gramians small.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎是很多设置工作，但我们在这里确实取得了很多成就。我们已经初始化了模型以优化两个矩阵因子，同时保持矩阵元素和Gramians较小。
- en: This brings us to our lovely images.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这带我们来到我们可爱的图片。
- en: Output from HPO MF
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: HPO MF 的输出
- en: Let’s have a quick look at what the prior work has produced. First, [Figure 10-3](#overallLoss)
    shows that our primary loss function, observed mean square error (OMSE), is rapidly
    decreasing. This is great, but we should take a deeper look.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看看先前的工作产生了什么。首先，[图 10-3](#overallLoss) 显示我们的主要损失函数，观察到的均方误差（OMSE），正在迅速减少。这很好，但我们应该深入研究一下。
- en: '![The loss during training](assets/brpj_1003.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![训练过程中的损失](assets/brpj_1003.png)'
- en: Figure 10-3\. The loss during training
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-3\. 训练过程中的损失
- en: Let’s also have a quick look to ensure that our regularization parameters ([Figure 10-4](#regparams))
    are converging. We can see that our L2 regularization could probably still decrease
    if we were to continue for more epochs.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速查看以确保我们的正则化参数（[图 10-4](#regparams)）正在收敛。我们可以看到，如果我们继续更多的epochs，我们的L2正则化可能仍然可以进一步减少。
- en: '![Regularization Params](assets/brpj_1004.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![正则化参数](assets/brpj_1004.png)'
- en: Figure 10-4\. Regularization parameters
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-4\. 正则化参数
- en: We’d like to see our cross-validation laid out by fold and corresponding loss
    ([Figure 10-5](#CrossVal)). This is a *parallel coordinates chart*; its lines
    correspond to different runs that are in correspondence with different choices
    of parameters, and its vertical axes are different metrics. The far-right heatmap
    axis corresponds to the overall total loss that we’re trying to minimize. In this
    case, we alternate test loss on a fold and total loss on that fold. Lower numbers
    are better, and we hope to see individual lines consistent across their loss per
    fold (otherwise, we may have a skewed dataset). We see that choices of hyperparameters
    can interact with fold behavior, but in all the low-loss scenarios (at the bottom),
    we see a high correlation between performance on different folds (the vertical
    axes in the plot).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望看到我们的交叉验证按折叠展开，并伴随着相应的损失（[图 10-5](#CrossVal)）。这是一个*平行坐标图*；其线条对应不同运行，对应于不同参数选择，并且其垂直轴是不同的指标。最右侧的热图轴对应我们试图最小化的总体总损失。在这种情况下，我们交替在一个折叠上测试损失和该折叠上的总损失。较低的数字更好，我们希望看到各行在其折叠的损失上保持一致（否则，我们可能有一个偏斜的数据集）。我们看到，超参数的选择可以与折叠行为互动，但在所有低损失情景（在底部），我们看到不同折叠性能之间的高相关性。
- en: '![The loss during training](assets/brpj_1005.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![训练过程中的损失](assets/brpj_1005.png)'
- en: Figure 10-5\. The loss during training
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-5\. 训练过程中的损失
- en: Next up, which choices of hyperparameters have a strong effect on performance?
    [Figure 10-6](#HPO) is another parallel coordinates plot with the vertical axes
    corresponding to different hyperparameters. Generally, we’re looking for which
    domains on the vertical axes correspond to low loss on the far-right heatmap.
    We see that some of our hyperparameters like priors distribution and, somewhat
    surprisingly, `ell_2` have virtually no effect. However, small embedding dimension
    and small Gramian weight definitely do. A larger alpha also seems to correlate
    well with good performance.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，哪些超参数选择对性能有较强的影响？[图 10-6](#HPO) 是另一个平行坐标图，其垂直轴对应不同的超参数。一般来说，我们希望垂直轴上的哪些领域对应最右侧热图上的低损失。我们看到，我们的一些超参数，比如先验分布和有些令人惊讶的`ell_2`，几乎没有影响。然而，较小的嵌入维度和较小的Gram权重确实有影响。较大的alpha值似乎也与良好的性能相关。
- en: '![The loss during training](assets/brpj_1006.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![训练过程中的损失](assets/brpj_1006.png)'
- en: Figure 10-6\. The loss by hyperparameter
  id: totrans-141
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-6\. 超参数的损失
- en: Finally, we see that as we do a Bayesian hyperparameter search, we really do
    improve our performance over time. [Figure 10-7](#pareto-curve) is a Pareto plot
    in which each dot in the scatterplot represents one run, and left to right is
    a time axis. The vertical axis is overall total loss, so lower is better, and
    it means that generally we’re converging toward better performance. The line inscribed
    along the bottom of the convex hull of the scatter points is the *Pareto frontier*,
    or the best performance at that x value. Since this is a time-series Pareto plot,
    it merely tracks the best performance in time.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们看到，随着我们进行贝叶斯超参数搜索，我们的性能确实随时间改善。[图 10-7](#pareto-curve) 是一个帕累托图，散点图中的每个点表示一次运行，从左到右是时间轴。垂直轴是总体损失，值越低越好，这意味着通常我们正在朝着更好的性能收敛。沿着散点凸包底部内刻的线是*帕累托前沿*，或该x值下的最佳性能。由于这是一个时间序列帕累托图，它仅跟踪时间内的最佳性能。
- en: You may be wondering how and why we’re able to converge to better loss values
    in time. This is because we’ve conducted a Bayesian hyperparameter search, which
    means we selected our hyperparameters from independent Gaussians, and we updated
    our priors for each parameter based on performance of previous runs. For an introduction
    to this method, see [“Bayesian Hyperparameter Optimization—A Primer”](https://oreil.ly/4nd3D)
    by Robert Mitson. In a real setting, we’d see less monotonic behavior in this
    plot, but we’d always be hoping to improve.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你会想知道我们是如何在时间内收敛到更好的损失值的。这是因为我们进行了贝叶斯超参数搜索，这意味着我们从独立的高斯分布中选择了我们的超参数，并且基于先前运行的表现更新了每个参数的先验信息。关于这种方法的介绍，请参阅Robert
    Mitson的[《贝叶斯超参数优化入门》](https://oreil.ly/4nd3D)。在实际环境中，我们会在这个图中看到较少的单调行为，但我们始终希望有所改善。
- en: '![The loss during training](assets/brpj_1007.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![训练期间的损失](assets/brpj_1007.png)'
- en: Figure 10-7\. The Pareto frontier of the loss values
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-7\. 损失值的帕累托前沿
- en: Prequential validation
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 先验验证
- en: If we were to put the preceding approach into practice, we would need to capture
    our trained models in a model registry for use in production. Best practice is
    to establish a set of explicit evaluations against which to test a selection of
    models. In your basic ML training, you’ve likely been encouraged to think about
    validation datasets; these may take many forms, testing particular subsets of
    instances or features or even distributed across covariates in a known way.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将前述方法付诸实践，我们需要将训练过的模型存储在模型注册表中，以便在生产中使用。最佳做法是建立一组明确的评估标准，以测试选择的模型。在你的基本机器学习培训中，你可能已经被鼓励考虑验证数据集；这些可以采用多种形式，测试特定子集的实例或特征，甚至分布在已知方式下的协变量中。
- en: One useful framing for recommendation systems is to remember that they’re a
    fundamentally sequential dataset. With this in mind, let’s take another look at
    our ratings data. Later we will talk more about sequential recommenders, but while
    we’re talking about validation, it’s useful to mention how to take proper care.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统的一个有用的框架是记住它们是一个基本上是顺序数据集。有了这个想法，让我们再看看我们的评分数据。稍后我们将更多地讨论顺序推荐系统，但在谈论验证时，提到如何适当地处理是有用的。
- en: Notice that all our ratings have an associated timestamp. To build a proper
    validation set, it’s a good idea to take that timestamp from the end of our data.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们所有的评分都有相关的时间戳。为了构建一个合适的验证集，从我们数据的末尾获取时间戳是一个好主意。
- en: However, you might be wondering, “When are different users active?” and “Is
    it possible that the later timestamps are a biased selection of the ratings?”
    These are important questions. To account for these questions, we should do a
    holdout by user.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，你可能会想，“不同用户何时活跃？”以及“后期时间戳是否可能是对评分的有偏选择？”这些都是重要的问题。为了解决这些问题，我们应该按用户进行留出验证。
- en: To create this *prequential dataset*, where the test set follows directly after
    the training set in a chronological sequence, start by deciding on a desired size
    for validation, like 10%. Next, group the data by user. Finally, employ rejection
    sampling, ensuring you don’t use the most recent timestamp as the rejection criterion.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建这个*先验数据集*，其中测试集直接跟在训练集后面，按照时间顺序，首先决定验证的期望大小，例如10%。接下来，按用户分组数据。最后，使用拒绝抽样，确保不使用最近的时间戳作为拒绝条件。
- en: 'Here’s a simple implementation for pandas using rejection sampling. This is
    not the most computationally efficient implementation, but it will get the job
    done:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个使用拒绝抽样在pandas中的简单实现。这不是最高效的实现方式，但可以完成任务：
- en: '[PRE14]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This is an effective and important validation scheme for inherently sequential
    datasets.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种对固有顺序数据集的有效和重要的验证方案。
- en: WSABIE
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: WSABIE
- en: Let’s focus again on optimizations and modifications. Another optimization is
    to treat the MF problem as a single optimization.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次关注优化和修改。另一种优化是将MF问题视为单一优化问题。
- en: 'The paper [“WSABIE: Scaling Up to Large Vocabulary Image Annotation”](https://oreil.ly/1GB3x)
    by Jason Weston et al. also contains a factorization for just the item matrix.
    In this scheme, we replace the user matrix with a weighted sum of the items a
    user has affinity to. We cover web scale annotation by image embedding (WSABIE)
    and Warp loss in [“WARP”](ch12.html#warp). Representing a user as the average
    of items they like is a way of saving space and not needing a separate user matrix
    if there are large numbers of users.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '文章[“WSABIE: Scaling Up to Large Vocabulary Image Annotation”](https://oreil.ly/1GB3x)由Jason
    Weston等人提出的方法也包含了仅对物品矩阵进行因式分解的方法。在这种方案中，我们用用户喜欢的物品的加权和替换用户矩阵。我们覆盖了Web规模的图像嵌入（WSABIE）和在[“WARP”](ch12.html#warp)中的Warp损失。将用户表示为他们喜欢的物品的平均值是一种节省空间的方法，如果有大量用户则不需要单独的用户矩阵。'
- en: Latent Space HPO
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在空间HPO
- en: A completely alternative way to do HPO for RecSys is via the latent spaces themselves!
    [“Hyper-Parameter Optimization for Latent Spaces in Dynamic Recommender Systems”](https://oreil.ly/RLeEC)
    by Bruno Veloso et al. attempts to modify the relative embeddings during each
    step to optimize the embedding model.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种完全不同的为RecSys进行HPO的方法是通过潜在空间本身！[“Dynamic Recommender Systems中的潜在空间超参数优化”](https://oreil.ly/RLeEC)由Bruno
    Veloso等人尝试在每个步骤中修改相对嵌入以优化嵌入模型。
- en: Dimension Reduction
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维
- en: Dimension-reduction techniques are frequently employed in recommendation systems
    to decrease computational complexity and enhance the accuracy of recommendation
    algorithms. In this context, the primary concepts of dimension reduction for recommendation
    systems include MF and SVD.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统经常使用降维技术来减少计算复杂性并增强推荐算法的准确性。在这个背景下，推荐系统的降维主要概念包括MF和SVD。
- en: 'The *matrix factorization method* decomposes the user-item interaction matrix
    <math alttext="left-parenthesis upper A element-of double-struck upper R Superscript
    left-parenthesis m times n right-parenthesis Baseline right-parenthesis"><mrow><mo>(</mo>
    <mi>A</mi> <mo>∈</mo> <msup><mi>ℝ</mi> <mrow><mo>(</mo><mi>m</mi><mo>×</mo><mi>n</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></math> into two lower-dimensional matrices, representing the
    user <math alttext="left-parenthesis upper U element-of double-struck upper R
    Superscript left-parenthesis m times r right-parenthesis Baseline right-parenthesis"><mrow><mo>(</mo>
    <mi>U</mi> <mo>∈</mo> <msup><mi>ℝ</mi> <mrow><mo>(</mo><mi>m</mi><mo>×</mo><mi>r</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></math> and item <math alttext="left-parenthesis upper V element-of
    double-struck upper R Superscript left-parenthesis n times r right-parenthesis
    Baseline right-parenthesis"><mrow><mo>(</mo> <mi>V</mi> <mo>∈</mo> <msup><mi>ℝ</mi>
    <mrow><mo>(</mo><mi>n</mi><mo>×</mo><mi>r</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></math>
    latent factors, respectively. This technique can reveal the underlying data structure
    and offer recommendations based on a user’s previous interactions. Mathematically,
    MF can be represented as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*矩阵分解方法*将用户-物品交互矩阵 <math alttext="left-parenthesis upper A element-of double-struck
    upper R Superscript left-parenthesis m times n right-parenthesis Baseline right-parenthesis"><mrow><mo>(</mo>
    <mi>A</mi> <mo>∈</mo> <msup><mi>ℝ</mi> <mrow><mo>(</mo><mi>m</mi><mo>×</mo><mi>n</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></math> 分解成两个表示用户 <math alttext="left-parenthesis upper U element-of
    double-struck upper R Superscript left-parenthesis m times r right-parenthesis
    Baseline right-parenthesis"><mrow><mo>(</mo> <mi>U</mi> <mo>∈</mo> <msup><mi>ℝ</mi>
    <mrow><mo>(</mo><mi>m</mi><mo>×</mo><mi>r</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></math>
    和物品 <math alttext="left-parenthesis upper V element-of double-struck upper R Superscript
    left-parenthesis n times r right-parenthesis Baseline right-parenthesis"><mrow><mo>(</mo>
    <mi>V</mi> <mo>∈</mo> <msup><mi>ℝ</mi> <mrow><mo>(</mo><mi>n</mi><mo>×</mo><mi>r</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></math> 潜在因子的较低维度矩阵。这种技术可以揭示潜在的数据结构，并根据用户的先前交互提供推荐。在数学上，MF可以表示如下：'
- en: <math alttext="upper A tilde upper U times upper V Superscript left-parenthesis
    upper T right-parenthesis" display="block"><mrow><mi>A</mi> <mo>∼</mo> <mi>U</mi>
    <mo>×</mo> <msup><mi>V</mi> <mrow><mo>(</mo><mi>T</mi><mo>)</mo></mrow></msup></mrow></math>
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper A tilde upper U times upper V Superscript left-parenthesis
    upper T right-parenthesis" display="block"><mrow><mi>A</mi> <mo>∼</mo> <mi>U</mi>
    <mo>×</mo> <msup><mi>V</mi> <mrow><mo>(</mo><mi>T</mi><mo>)</mo></mrow></msup></mrow></math>
- en: '*SVD* is a linear-algebra technique that decomposes a matrix (*A*) into three
    matrices—the left singular vectors (*U*), the singular values (*Σ*), and the right
    singular vectors (*V*). SVD can be utilized for MF in recommendation systems,
    where the user-item interaction matrix is decomposed into a smaller number of
    latent factors. The mathematical representation of SVD is as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '*SVD*是一种线性代数技术，它将矩阵(*A*)分解为三个矩阵—左奇异向量(*U*)，奇异值(*Σ*)和右奇异向量(*V*)。在推荐系统中，SVD可用于MF，其中用户-项目交互矩阵被分解为更少数量的潜在因子。SVD的数学表示如下：'
- en: <math alttext="upper A equals upper U times normal upper Sigma times upper V
    Superscript left-parenthesis upper T right-parenthesis" display="block"><mrow><mi>A</mi>
    <mo>=</mo> <mi>U</mi> <mo>×</mo> <mi>Σ</mi> <mo>×</mo> <msup><mi>V</mi> <mrow><mo>(</mo><mi>T</mi><mo>)</mo></mrow></msup></mrow></math>
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper A equals upper U times normal upper Sigma times upper V
    Superscript left-parenthesis upper T right-parenthesis" display="block"><mrow><mi>A</mi>
    <mo>=</mo> <mi>U</mi> <mo>×</mo> <mi>Σ</mi> <mo>×</mo> <msup><mi>V</mi> <mrow><mo>(</mo><mi>T</mi><mo>)</mo></mrow></msup></mrow></math>
- en: 'In practice, though, rather than using a mathematical library to find the eigenvectors,
    folks might use the [power iteration method](https://oreil.ly/DCsRs) to discover
    the eigenvectors approximately. This method is far more scalable than a full dense
    SVD solution that is optimized for correctness and dense vectors:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作中，人们通常不会使用数学库来找到特征向量，而是可能使用[幂迭代法](https://oreil.ly/DCsRs)来近似地发现特征向量。这种方法比为正确性和密集向量优化的完整稠密SVD解决方案要可扩展得多：
- en: '[PRE15]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Notice that the eigenvector returned by the power iteration is close to the
    first column of <math alttext="upper S"><mi>S</mi></math> , but not quite. This
    is because the method is approximate. It relies on the fact that an eigenvector
    doesn’t change in direction when multiplied by the matrix. So by repeatedly multiplying
    by the matrix, we eventually iterate onto an eigenvector. Also notice that we
    solved for column eigenvectors instead of the row eigenvectors. In this example,
    the columns are users, and the rows are items. It is important to play with transposed
    matrices because a lot of ML involves reshaping and transposing matrices, so getting
    used to them early is an important skill.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，幂迭代返回的特征向量接近于<math alttext="upper S"><mi>S</mi></math>的第一列，但不完全相同。这是因为这种方法是近似的。它依赖于特征向量在乘以矩阵时不改变方向的事实。因此，通过重复乘以矩阵，我们最终迭代到一个特征向量。还要注意，我们解决的是列特征向量，而不是行特征向量。在这个例子中，列是用户，行是项目。重要的是要玩转转置矩阵，因为很多机器学习涉及到重塑和转置矩阵，所以早期熟悉它们是一项重要技能。
- en: Eigenvector Examples
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征向量示例
- en: 'Here’s a nice exercise for you: the second eigenvector is computed by subtracting
    out the first eigenvector after the matrix multiplication. This is telling the
    algorithm to ignore any component along the first eigenvector in order to compute
    the second eigenvector. As a fun exercise, hop over to [Colab](https://oreil.ly/0zmWq)
    and try computing the second eigenvector. Extending this to sparse vector representations
    is another interesting exercise, as it allows you to start computing the eigenvectors
    of sparse matrices, which is usually the form of matrix that recommender systems
    use.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个很好的练习给你：第二个特征向量是通过在矩阵乘法后减去第一个特征向量来计算的。这告诉算法忽略第一个特征向量上的任何成分，以计算第二个特征向量。作为一个有趣的练习，跳到[Colab](https://oreil.ly/0zmWq)上尝试计算第二个特征向量。将这种方法扩展到稀疏向量表示是另一个有趣的练习，因为它允许您开始计算稀疏矩阵的特征向量，这通常是推荐系统使用的矩阵形式。
- en: 'Next, we construct a recommendation for a user by creating a column and then
    taking the dot product with all the eigenvectors and finding the closest. We then
    find all the highest-scoring entries in the eigenvector that the user hasn’t seen
    and return them as recommendations. So in the preceding example, if the eigenvector
    <math alttext="x 1"><msub><mi>x</mi> <mn>1</mn></msub></math> was the closest
    to the user column, then the best item to recommend would be item 3 because it
    is the largest component in the eigenvector and thus rated most highly if the
    user is closest to the eigenvector <math alttext="x 1"><msub><mi>x</mi> <mn>1</mn></msub></math>
    . Here’s what this looks like in code:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过创建一个列并将其与所有特征向量点积，找到最接近的特征向量。然后找到用户尚未看到的特征向量中所有最高评分的条目，并将它们作为推荐返回。因此，在上面的例子中，如果特征向量<math
    alttext="x 1"><msub><mi>x</mi> <mn>1</mn></msub></math>最接近用户列，那么推荐的最佳项目将是项目3，因为它是特征向量中最大的成分，因此如果用户最接近特征向量<math
    alttext="x 1"><msub><mi>x</mi> <mn>1</mn></msub></math>，则评分最高。这段代码如下所示：
- en: '[PRE16]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In this example, a user has downvoted item 0 and item 1\. The closest column
    eigenvector is therefore column 0\. We then select the closest eigenvector to
    the user, order the entries, and recommend item 2 to the user, which is the highest-scoring
    entry that the user has not seen.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，用户对项目0和项目1进行了负面评价。最接近的列特征向量因此是列0。然后我们选择距离用户最近的特征向量，对条目进行排序，并向用户推荐项目2，这是用户尚未看到的最高评分条目。
- en: 'Two techniques aim to extract the most relevant features from the user-item
    interaction matrix and reduce its dimensionality, which can improve performance:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 两种技术旨在从用户-项目交互矩阵中提取最相关的特征并减少其维度，从而改善性能：
- en: Principal component analysis (PCA)
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）
- en: This statistical technique transforms the original high-dimensional data into
    a lower-dimensional representation while retaining the most important information.
    PCA can be applied to the user-item interaction matrix to reduce the number of
    dimensions and improve the computational efficiency of the recommendation algorithm.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这种统计技术将原始高维数据转换为较低维度的表示，同时保留最重要的信息。 PCA 可以应用于用户-项目交互矩阵，以减少维度数量并提高推荐算法的计算效率。
- en: Nonnegative matrix factorization (NMF)
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 非负矩阵分解（NMF）
- en: This technique decomposes the nonnegative user-item interaction matrix <math
    alttext="left-parenthesis upper A element-of double-struck upper R Superscript
    left-parenthesis m times n right-parenthesis asterisk plus Baseline right-parenthesis"><mrow><mo>(</mo>
    <mi>A</mi> <mo>∈</mo> <msup><mi>ℝ</mi> <mrow><mo>(</mo><mi>m</mi><mo>×</mo><mi>n</mi><mo>)</mo><mo>*</mo><mo>+</mo></mrow></msup>
    <mo>)</mo></mrow></math> into two nonnegative matrices <math alttext="left-parenthesis
    upper W element-of double-struck upper R Superscript left-parenthesis m times
    r right-parenthesis asterisk plus"><mrow><mo>(</mo> <mi>W</mi> <mo>∈</mo> <msup><mi>ℝ</mi>
    <mrow><mo>(</mo><mi>m</mi><mo>×</mo><mi>r</mi><mo>)</mo><mo>*</mo><mo>+</mo></mrow></msup></mrow></math>
    and <math alttext="upper H element-of double-struck upper R Superscript left-parenthesis
    r times n right-parenthesis Super Subscript plus Superscript Baseline right-parenthesis"><mrow><mi>H</mi>
    <mo>∈</mo> <msup><mi>ℝ</mi> <msub><mrow><mo>(</mo><mi>r</mi><mo>×</mo><mi>n</mi><mo>)</mo></mrow>
    <mo>+</mo></msub></msup> <mrow><mo>)</mo></mrow></mrow></math> . NMF can be utilized
    for dimension reduction in recommendation systems, where the latent factors are
    nonnegative and interpretable. The mathematical representation of NMF is <math
    alttext="upper A asymptotically-equals upper W times upper H"><mrow><mi>A</mi>
    <mo>≃</mo> <mi>W</mi> <mo>×</mo> <mi>H</mi></mrow></math> .
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 此技术将非负用户-项目交互矩阵 <math alttext="left-parenthesis upper A element-of double-struck
    upper R Superscript left-parenthesis m times n right-parenthesis asterisk plus
    Baseline right-parenthesis"><mrow><mo>(</mo> <mi>A</mi> <mo>∈</mo> <msup><mi>ℝ</mi>
    <mrow><mo>(</mo><mi>m</mi><mo>×</mo><mi>n</mi><mo>)</mo><mo>*</mo><mo>+</mo></mrow></msup>
    <mo>)</mo></mrow></math> 分解为两个非负矩阵 <math alttext="left-parenthesis upper W element-of
    double-struck upper R Superscript left-parenthesis m times r right-parenthesis
    asterisk plus"><mrow><mo>(</mo> <mi>W</mi> <mo>∈</mo> <msup><mi>ℝ</mi> <mrow><mo>(</mo><mi>m</mi><mo>×</mo><mi>r</mi><mo>)</mo><mo>*</mo><mo>+</mo></mrow></msup></mrow></math>
    和 <math alttext="upper H element-of double-struck upper R Superscript left-parenthesis
    r times n right-parenthesis Super Subscript plus Superscript Baseline right-parenthesis"><mrow><mi>H</mi>
    <mo>∈</mo> <msup><mi>ℝ</mi> <msub><mrow><mo>(</mo><mi>r</mi><mo>×</mo><mi>n</mi><mo>)</mo></mrow>
    <mo>+</mo></msub></msup> <mrow><mo>)</mo></mrow></mrow></math> 。 NMF 可以用于推荐系统中的维度缩减，其中潜在因子是非负且可解释的。
    NMF 的数学表示为 <math alttext="upper A asymptotically-equals upper W times upper H"><mrow><mi>A</mi>
    <mo>≃</mo> <mi>W</mi> <mo>×</mo> <mi>H</mi></mrow></math> 。
- en: MF techniques can be further extended to incorporate additional information,
    such as item content or user demographic data, through the use of side information.
    Side information can be employed to augment the user-item interaction matrix,
    allowing for more accurate and personalized recommendations.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: MF 技术可以通过使用附加信息（如项目内容或用户人口统计数据）进一步扩展，通过使用辅助信息。 辅助信息可以用来增强用户-项目交互矩阵，从而实现更准确和个性化的推荐。
- en: Furthermore, MF models can be extended to handle implicit feedback data, where
    the absence of interaction data is not equivalent to the lack of interest. By
    incorporating additional regularization terms into the objective function, MF
    models can learn a more robust representation of the user-item interaction matrix,
    leading to better recommendations for implicit feedback scenarios.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，MF 模型可以扩展到处理隐式反馈数据，其中缺少交互数据并不等同于缺乏兴趣。 通过将额外的正则化项并入目标函数，MF 模型可以学习用户-项目交互矩阵的更强健表示，从而为隐式反馈场景提供更好的推荐。
- en: Consider a recommendation system that employs MF to model the user-item interaction
    matrix. If the system comprises many users and items, the resulting factor matrices
    can be high-dimensional and computationally expensive to process. However, by
    using dimension-reduction techniques like SVD or PCA, the algorithm can reduce
    the dimensionality of the factor matrices while preserving the most important
    information about the user-item interactions. This enables the algorithm to generate
    more efficient and accurate recommendations, even for new users or items with
    limited interaction data.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个利用 MF 模型用户-物品交互矩阵的推荐系统。 如果系统包含许多用户和物品，生成的因子矩阵可能是高维的，且处理起来计算成本高昂。 然而，通过使用奇异值分解（SVD）或主成分分析（PCA）等降维技术，算法可以减少因子矩阵的维度，同时保留关于用户-物品交互的最重要信息。
    这使得算法能够为新用户或具有有限交互数据的物品生成更有效和准确的推荐。
- en: Isometric Embeddings
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 等距嵌入
- en: '*Isometric embeddings* are a specific type of embedding that maintains distances
    between points in high-dimensional space when mapping them onto a lower-dimensional
    space. The term *isometric* signifies that the distances between points in the
    high-dimensional space are preserved precisely in the lower-dimensional space,
    up to a scaling factor.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '*等距嵌入* 是一种特定类型的嵌入，当将点映射到低维空间时，保持高维空间中点之间的距离。 术语*等距* 表示高维空间中的点之间的距离在低维空间中被精确地保留，只有一个缩放因子。'
- en: In contrast to other types of embeddings, such as linear or nonlinear embeddings,
    which may distort the distances between points, isometric embeddings are preferable
    in numerous applications where distance preservation is essential. For example,
    in ML, isometric embeddings can be employed to visualize high-dimensional data
    in two or three dimensions while preserving the relative distances between the
    data points. In NLP, isometric embeddings can be utilized to represent the semantic
    similarities between words or documents while maintaining their relative distances
    in the embedding space.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他类型的嵌入（如线性或非线性嵌入）相比，这种等距嵌入在许多需要保持距离的应用中更为理想。 例如，在机器学习中，等距嵌入可用于在二维或三维中可视化高维数据，同时保持数据点之间的相对距离。
    在自然语言处理中，等距嵌入可用于表示单词或文档之间的语义相似性，同时在嵌入空间中保持它们的相对距离。
- en: 'One popular technique for generating isometric embeddings is *multidimensional
    scaling* (*MDS*). MDS operates by computing pairwise distances between the data
    points in the high-dimensional space and then determining a lower-dimensional
    embedding that preserves these distances. The optimization problem is generally
    formulated as a constrained optimization problem, where the objective is to minimize
    the difference between the pairwise distances in the high-dimensional space and
    the corresponding distances in the lower-dimensional embedding. Mathematically,
    we write: <math alttext="m i n Subscript left-parenthesis upper X right-parenthesis
    Baseline sigma-summation Underscript left-parenthesis i comma j right-parenthesis
    Endscripts left-parenthesis d Subscript i j Baseline minus StartAbsoluteValue
    EndAbsoluteValue x Subscript i Baseline minus x Subscript j Baseline StartAbsoluteValue
    EndAbsoluteValue right-parenthesis squared"><mrow><mi>m</mi> <mi>i</mi> <msub><mi>n</mi>
    <mrow><mo>(</mo><mi>X</mi><mo>)</mo></mrow></msub> <msub><mo>∑</mo> <mrow><mo>(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo>)</mo></mrow></msub>
    <mrow><mo>(</mo></mrow> <msub><mi>d</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mo>-</mo> <mrow><mo>|</mo> <mo>|</mo></mrow> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>-</mo> <msub><mi>x</mi> <mi>j</mi></msub> <msup><mrow><mo>|</mo><mo>|</mo><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></math> .'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 生成等距嵌入的一种流行技术是*多维标度*（*MDS*）。 MDS 通过计算高维空间中数据点之间的成对距离，然后确定一个保持这些距离的低维嵌入来运作。 优化问题通常被制定为约束优化问题，其目标是最小化高维空间中成对距离与低维嵌入中相应距离之间的差异。
    在数学上，我们写成：<math alttext="m i n Subscript left-parenthesis upper X right-parenthesis
    Baseline sigma-summation Underscript left-parenthesis i comma j right-parenthesis
    Endscripts left-parenthesis d Subscript i j Baseline minus StartAbsoluteValue
    EndAbsoluteValue x Subscript i Baseline minus x Subscript j Baseline StartAbsoluteValue
    EndAbsoluteValue right-parenthesis squared"><mrow><mi>最小化</mi> <mi>上标</mi> <msub><mtext>左括号</mtext>
    <mrow><mi>X</mi> <mtext>右括号</mtext></mrow></msub> <msub><mo>Σ</mo> <mrow><mtext>左括号</mtext><mi>i</mi><mo>,</mo><mi>j</mi><mtext>右括号</mtext></mrow></msub>
    <mrow><mtext>左括号</mtext></mrow> <msub><mi>d</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mo>-</mo> <mrow><mtext>绝对值</mtext><mtext>绝对值</mtext></mrow> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>-</mo> <msub><mi>x</mi> <mi>j</mi></msub> <msup><mrow><mtext>绝对值</mtext><mtext>绝对值</mtext></mrow>
    <mn>2</mn></msup></mrow></math>。
- en: Here, <math alttext="d Subscript i j"><msub><mi>d</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub></math>
    denotes the pairwise distances in the high-dimensional space, and <math alttext="x
    Subscript i"><msub><mi>x</mi> <mi>i</mi></msub></math> and <math alttext="x Subscript
    j"><msub><mi>x</mi> <mi>j</mi></msub></math> represent points in the lower-dimensional
    embedding.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，<math alttext="d Subscript i j"><msub><mi>d</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub></math>
    表示高维空间中的成对距离，而 <math alttext="x Subscript i"><msub><mi>x</mi> <mi>i</mi></msub></math>
    和 <math alttext="x Subscript j"><msub><mi>x</mi> <mi>j</mi></msub></math> 表示低维嵌入中的点。
- en: Another approach for generating isometric embeddings is through the use of kernel
    methods, such as kernel PCA or kernel MDS. Kernel methods work by implicitly mapping
    the data points into a higher-dimensional feature space, where the distances between
    the points are easier to compute. The isometric embedding is then calculated in
    the feature space, and the resulting embedding is mapped back to the original
    space.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 生成等距嵌入的另一种方法是使用核方法，如核PCA或核MDS。核方法通过隐式地将数据点映射到更高维的特征空间，在这个空间中，点之间的距离更容易计算。然后在特征空间中计算等距嵌入，并将结果嵌回原始空间。
- en: Isometric embeddings have been employed in recommendation systems to represent
    the user-item interaction matrix in a lower-dimensional space where the distances
    between the items are preserved. By preserving the distances between items in
    the embedding space, the recommendation algorithm can better capture the underlying
    structure of the data and provide more accurate and diverse recommendations.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 等距嵌入已被应用于推荐系统中，用于将用户-物品交互矩阵表示为一个低维空间，其中保留了物品之间的距离。通过在嵌入空间中保持物品之间的距离，推荐算法可以更好地捕捉数据的基本结构，并提供更准确和多样化的推荐。
- en: Isometric embeddings can also be employed to incorporate additional information
    into the recommendation algorithm, such as item content or user demographic data.
    By using isometric embeddings to represent the items and the additional information,
    the algorithm can capture the similarities between items based on both the user-item
    interaction data and the item content or user demographics, leading to more accurate
    and diverse recommendations.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 等距嵌入还可以用于整合额外信息到推荐算法中，例如物品内容或用户人口统计数据。通过使用等距嵌入来表示物品和额外信息，算法可以基于用户-物品交互数据以及物品内容或用户人口统计信息捕捉物品之间的相似性，从而提供更准确和多样化的推荐。
- en: Moreover, isometric embeddings can also be used to address the cold-start problem
    in recommendation systems. By using the isometric embeddings to represent the
    items, the algorithm can make recommendations for new items based on their similarities
    to the existing items in the embedding space, even in the absence of user interactions.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，等距嵌入还可以用于解决推荐系统中的冷启动问题。通过使用等距嵌入来表示物品，算法可以根据它们在嵌入空间中与现有物品的相似性，即使在没有用户交互的情况下，为新物品提供建议。
- en: In summary, isometric embeddings are a valuable technique in recommendation
    systems for representing the user-item interaction matrix in a lower-dimensional
    space where the distances between the items are preserved. Isometric embeddings
    can be generated using MF techniques and can be employed to incorporate additional
    information, address the cold-start problem, and improve the accuracy and diversity
    of recommendations.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，等距嵌入是推荐系统中一种宝贵的技术，用于将用户-物品交互矩阵表示为一个低维空间，其中保留了物品之间的距离。等距嵌入可以使用MF技术生成，并可以用于整合额外信息，解决冷启动问题，提高推荐的准确性和多样性。
- en: Nonlinear Locally Metrizable Embeddings
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非线性局部度量嵌入
- en: '*Nonlinear locally metrizable embeddings* are yet another method to represent
    the user-item interaction matrix in a lower-dimensional space where the local
    distances between nearby items are preserved. By preserving the local distances
    between items in the embedding space, the recommendation algorithm can better
    capture the local structure of the data and provide more accurate and diverse
    recommendations.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '*非线性局部度量嵌入* 是另一种方法，用于将用户-物品交互矩阵表示为一个低维空间，其中保留了附近物品之间的局部距离。通过在嵌入空间中保持物品之间的局部距离，推荐算法可以更好地捕捉数据的局部结构，并提供更准确和多样化的推荐。'
- en: 'Mathematically, let <math alttext="upper X equals x 1 comma x 2 comma period
    period period comma x Subscript n Baseline"><mrow><mi>X</mi> <mo>=</mo> <mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>.</mo>
    <mo>.</mo> <mo>.</mo> <mo>,</mo> <msub><mi>x</mi> <mi>n</mi></msub></mrow></mrow></math>
    be the set of items in the high-dimensional space, and <math alttext="upper Y
    equals y 1 comma y 2 comma period period period comma y Subscript n Baseline"><mrow><mi>Y</mi>
    <mo>=</mo> <mrow><msub><mi>y</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>y</mi>
    <mn>2</mn></msub> <mo>,</mo> <mo>.</mo> <mo>.</mo> <mo>.</mo> <mo>,</mo> <msub><mi>y</mi>
    <mi>n</mi></msub></mrow></mrow></math> be the set of items in the lower-dimensional
    space. The goal of nonlinear locally metrizable embeddings is to find a mapping
    <math alttext="f colon upper X right-arrow upper Y"><mrow><mi>f</mi> <mo>:</mo>
    <mi>X</mi> <mo>→</mo> <mi>Y</mi></mrow></math> that preserves the local distances,
    i.e., for any <math alttext="x Subscript i Baseline comma x Subscript j Baseline
    element-of upper X"><mrow><msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>x</mi>
    <mi>j</mi></msub> <mo>∈</mo> <mi>X</mi></mrow></math> , we have this:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上，设 <math alttext="upper X equals x 1 comma x 2 comma period period period
    comma x Subscript n Baseline"><mrow><mi>X</mi> <mo>=</mo> <mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>.</mo>
    <mo>.</mo> <mo>.</mo> <mo>,</mo> <msub><mi>x</mi> <mi>n</mi></msub></mrow></mrow></math>
    表示高维空间中的项目集合，而 <math alttext="upper Y equals y 1 comma y 2 comma period period
    period comma y Subscript n Baseline"><mrow><mi>Y</mi> <mo>=</mo> <mrow><msub><mi>y</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>y</mi> <mn>2</mn></msub> <mo>,</mo> <mo>.</mo>
    <mo>.</mo> <mo>.</mo> <mo>,</mo> <msub><mi>y</mi> <mi>n</mi></msub></mrow></mrow></math>
    表示低维空间中的项目集合。非线性局部可度量嵌入的目标是找到一个映射 <math alttext="f colon upper X right-arrow upper
    Y"><mrow><mi>f</mi> <mo>:</mo> <mi>X</mi> <mo>→</mo> <mi>Y</mi></mrow></math>
    ，以保持局部距离，即对于任意 <math alttext="x Subscript i Baseline comma x Subscript j Baseline
    element-of upper X"><mrow><msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>x</mi>
    <mi>j</mi></msub> <mo>∈</mo> <mi>X</mi></mrow></math> ，我们有：
- en: <math alttext="d Subscript upper Y Baseline left-parenthesis f left-parenthesis
    x Subscript i Baseline right-parenthesis comma f left-parenthesis x Subscript
    j Baseline right-parenthesis right-parenthesis asymptotically-equals d Subscript
    upper X Baseline left-parenthesis x Subscript i Baseline comma x Subscript j Baseline
    right-parenthesis" display="block"><mrow><msub><mi>d</mi> <mi>Y</mi></msub> <mrow><mo>(</mo>
    <mi>f</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow>
    <mo>,</mo> <mi>f</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>j</mi></msub> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>≃</mo> <msub><mi>d</mi> <mi>X</mi></msub> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>x</mi> <mi>j</mi></msub>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="d Subscript upper Y Baseline left-parenthesis f left-parenthesis
    x Subscript i Baseline right-parenthesis comma f left-parenthesis x Subscript
    j Baseline right-parenthesis right-parenthesis asymptotically-equals d Subscript
    upper X Baseline left-parenthesis x Subscript i Baseline comma x Subscript j Baseline
    right-parenthesis" display="block"><mrow><msub><mi>d</mi> <mi>Y</mi></msub> <mrow><mo>(</mo>
    <mi>f</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow>
    <mo>,</mo> <mi>f</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>j</mi></msub> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>≃</mo> <msub><mi>d</mi> <mi>X</mi></msub> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>x</mi> <mi>j</mi></msub>
    <mo>)</mo></mrow></mrow></math>
- en: 'One popular approach to generating nonlinear locally metrizable embeddings
    in recommendation systems is via autoencoder neural networks. Autoencoders work
    by mapping the high-dimensional user-item interaction matrix onto a lower-dimensional
    space through an encoder network, and then reconstructing the matrix back in the
    high-dimensional space through a decoder network. The encoder and decoder networks
    are trained jointly to minimize the difference between the input data and the
    reconstructed data, with the objective of capturing the underlying structure of
    the data in the embedding space:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在推荐系统中生成非线性局部可度量嵌入的一个流行方法是通过自编码器神经网络。自编码器通过编码器网络将高维用户-项目交互矩阵映射到低维空间，然后通过解码器网络将矩阵重新构建回高维空间。编码器和解码器网络被联合训练，以最小化输入数据和重构数据之间的差异，目的是捕捉嵌入空间中的数据潜在结构：
- en: <math alttext="m i n Subscript left-parenthesis theta comma phi right-parenthesis
    Baseline sigma-summation Underscript left-parenthesis i equals 1 right-parenthesis
    Overscript n Endscripts StartAbsoluteValue EndAbsoluteValue x Subscript i minus
    g Subscript phi Baseline left-parenthesis f Subscript theta Baseline left-parenthesis
    x Subscript i Baseline right-parenthesis right-parenthesis StartAbsoluteValue
    EndAbsoluteValue squared" display="block"><mrow><mi>m</mi> <mi>i</mi> <msub><mi>n</mi>
    <mrow><mo>(</mo><mi>θ</mi><mo>,</mo><mi>φ</mi><mo>)</mo></mrow></msub> <munderover><mo>∑</mo>
    <mrow><mo>(</mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>)</mo></mrow> <mi>n</mi></munderover>
    <mrow><mo>|</mo> <mo>|</mo></mrow> <msub><mi>x</mi> <mi>i</mi></msub> <mo>-</mo>
    <msub><mi>g</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <msub><mi>f</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <msup><mrow><mo>|</mo><mo>|</mo></mrow> <mn>2</mn></msup></mrow></math>
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="m i n Subscript left-parenthesis theta comma phi right-parenthesis
    Baseline sigma-summation Underscript left-parenthesis i equals 1 right-parenthesis
    Overscript n Endscripts StartAbsoluteValue EndAbsoluteValue x Subscript i minus
    g Subscript phi Baseline left-parenthesis f Subscript theta Baseline left-parenthesis
    x Subscript i Baseline right-parenthesis right-parenthesis StartAbsoluteValue
    EndAbsoluteValue squared" display="block"><mrow><mi>m</mi> <mi>i</mi> <msub><mi>n</mi>
    <mrow><mo>(</mo><mi>θ</mi><mo>,</mo><mi>φ</mi><mo>)</mo></mrow></msub> <munderover><mo>∑</mo>
    <mrow><mo>(</mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>)</mo></mrow> <mi>n</mi></munderover>
    <mrow><mo>|</mo> <mo>|</mo></mrow> <msub><mi>x</mi> <mi>i</mi></msub> <mo>-</mo>
    <msub><mi>g</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <msub><mi>f</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <msup><mrow><mo>|</mo><mo>|</mo></mrow> <mn>2</mn></msup></mrow></math>
- en: Here, <math alttext="f Subscript theta"><msub><mi>f</mi> <mi>θ</mi></msub></math>
    denotes the encoder network with parameters <math alttext="theta comma g Subscript
    theta Baseline"><mrow><mi>θ</mi> <mo>,</mo> <msub><mi>g</mi> <mi>θ</mi></msub></mrow></math>
    denotes the decoder network with parameters <math alttext="theta"><mi>θ</mi></math>
    , and <math alttext="StartAbsoluteValue EndAbsoluteValue dot StartAbsoluteValue
    EndAbsoluteValue"><mrow><mo>|</mo> <mo>|</mo> <mo>·</mo> <mo>|</mo> <mo>|</mo></mrow></math>
    represents the Euclidean norm.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，<math alttext="f Subscript theta"><msub><mi>f</mi> <mi>θ</mi></msub></math>
    表示带有参数 <math alttext="theta comma g Subscript theta Baseline"><mrow><mi>θ</mi>
    <mo>,</mo> <msub><mi>g</mi> <mi>θ</mi></msub></mrow></math> 的编码器网络，<math alttext="theta"><mi>θ</mi></math>
    表示具有参数 <math alttext="theta"><mi>θ</mi></math> 的解码器网络，并且 <math alttext="StartAbsoluteValue
    EndAbsoluteValue dot StartAbsoluteValue EndAbsoluteValue"><mrow><mo>|</mo> <mo>|</mo>
    <mo>·</mo> <mo>|</mo> <mo>|</mo></mrow></math> 表示欧几里得范数。
- en: Another approach for generating nonlinear locally metrizable embeddings in recommendation
    systems is through the use of t-distributed stochastic neighbor embedding (t-SNE).
    t-SNE works by modeling the pairwise similarities between the items in the high-dimensional
    space, and then finding a lower-dimensional embedding that preserves these similarities.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在推荐系统中生成非线性局部可度量嵌入的另一种方法是使用 t-分布随机邻域嵌入（t-SNE）。t-SNE 的工作原理是对高维空间中项目之间的成对相似性进行建模，然后找到一个保持这些相似性的低维嵌入。
- en: 'A more popular approach in modern times is UMAP, which instead attempts to
    fit a minimal manifold that preserves density in local neighborhoods. UMAP is
    an essential technique for finding low-dimensional representations in complex
    and high-dimensional latent spaces; find it’s documentation at [*https://oreil.ly/NLqDg*](https://oreil.ly/NLqDg).
    The optimization problem is typically formulated as a cost function *C* that measures
    the difference between the pairwise similarities in the high-dimensional space
    and the corresponding similarities in the lower-dimensional embedding:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代，更流行的方法是 UMAP，它试图在局部邻域中保持密度的最小流形。 UMAP 是在复杂和高维潜在空间中寻找低维表示的重要技术；查阅其文档 [*https://oreil.ly/NLqDg*](https://oreil.ly/NLqDg)。优化问题通常被制定为一个成本函数
    *C*，用于衡量高维空间中成对相似性与较低维度嵌入中相应相似性之间的差异：
- en: <math alttext="dollar-sign upper C left-parenthesis upper Y right-parenthesis
    equals sigma-summation Underscript left-parenthesis i comma j right-parenthesis
    Endscripts p Subscript i j Baseline asterisk l o g left-parenthesis StartFraction
    p Subscript i j Baseline Over q Subscript i j Baseline EndFraction right-parenthesis
    dollar-sign"><mrow><mi>C</mi> <mrow><mo>(</mo> <mi>Y</mi> <mo>)</mo></mrow> <mo>=</mo>
    <msub><mo>∑</mo> <mrow><mo>(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo>)</mo></mrow></msub>
    <msub><mi>p</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub> <mo>*</mo> <mi>l</mi>
    <mi>o</mi> <mi>g</mi> <mrow><mo>(</mo> <mstyle displaystyle="true" scriptlevel="0"><mfrac><msub><mi>p</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub> <msub><mi>q</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub></mfrac></mstyle>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign upper C left-parenthesis upper Y right-parenthesis
    equals sigma-summation Underscript left-parenthesis i comma j right-parenthesis
    Endscripts p Subscript i j Baseline asterisk l o g left-parenthesis StartFraction
    p Subscript i j Baseline Over q Subscript i j Baseline EndFraction right-parenthesis
    dollar-sign"><mrow><mi>C</mi> <mrow><mo>(</mo> <mi>Y</mi> <mo>)</mo></mrow> <mo>=</mo>
    <msub><mo>∑</mo> <mrow><mo>(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo>)</mo></mrow></msub>
    <msub><mi>p</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub> <mo>*</mo> <mi>l</mi>
    <mi>o</mi> <mi>g</mi> <mrow><mo>(</mo> <mstyle displaystyle="true" scriptlevel="0"><mfrac><msub><mi>p</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub> <msub><mi>q</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub></mfrac></mstyle>
    <mo>)</mo></mrow></mrow></math>
- en: Here, <math alttext="p Subscript i j"><msub><mi>p</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub></math>
    denotes the pairwise similarities in the high-dimensional space, <math alttext="q
    Subscript i j"><msub><mi>q</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub></math>
    denotes the pairwise similarities in the lower-dimensional space, and the sum
    is over all pairs of items <math alttext="left-parenthesis i comma j right-parenthesis"><mrow><mo>(</mo>
    <mi>i</mi> <mo>,</mo> <mi>j</mi> <mo>)</mo></mrow></math> .
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，<math alttext="p Subscript i j"><msub><mi>p</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub></math>
    表示高维空间中的成对相似性，<math alttext="q Subscript i j"><msub><mi>q</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub></math>
    表示低维空间中的成对相似性，求和遍历所有项目对<math alttext="left-parenthesis i comma j right-parenthesis"><mrow><mo>(</mo>
    <mi>i</mi> <mo>,</mo> <mi>j</mi> <mo>)</mo></mrow></math> 。
- en: Nonlinear locally metrizable embeddings can also be used to incorporate additional
    information into the recommendation algorithm, such as item content or user demographic
    data. By using nonlinear locally metrizable embeddings to represent the items
    and the additional information, the algorithm can capture the similarities between
    items based on both the user-item interaction data and the item content or user
    demographics, leading to more accurate and diverse recommendations.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性局部可度量嵌入还可用于将额外信息整合到推荐算法中，例如项目内容或用户人口统计数据。通过使用非线性局部可度量嵌入来表示项目和额外信息，算法可以基于用户-项目交互数据以及项目内容或用户人口统计数据捕捉项目之间的相似性，从而实现更准确和多样化的推荐。
- en: Moreover, nonlinear locally metrizable embeddings can also be used to address
    the cold-start problem in recommendation systems. By using the nonlinear locally
    metrizable embeddings to represent the items, the algorithm can make recommendations
    for new items based on their similarities to the existing items in the embedding
    space, even in the absence of user interactions.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，非线性局部可度量嵌入还可用于解决推荐系统中的冷启动问题。通过使用非线性局部可度量嵌入来表示项目，算法可以基于它们在嵌入空间中与现有项目的相似性为新项目提供建议，即使在缺乏用户交互的情况下也能实现。
- en: In summary, nonlinear locally metrizable embeddings are a useful technique in
    recommendation systems for representing the user-item interaction matrix in a
    lower-dimensional space where the local distances between nearby items are preserved.
    Nonlinear locally metrizable embeddings can be generated using techniques such
    as autoencoder neural networks or t-SNE and can be used to incorporate additional
    information, address the cold-start problem, and improve the accuracy and diversity
    of recommendations.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，非线性局部可度量嵌入是推荐系统中的一种有用技术，用于在低维空间中表示用户-项目交互矩阵，保持附近项目之间的局部距离。非线性局部可度量嵌入可以使用自编码器神经网络或
    t-SNE 等技术生成，并可用于整合额外信息，解决冷启动问题，并提高推荐的准确性和多样性。
- en: Centered Kernel Alignment
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 居中核对齐
- en: When training neural networks, the latent space representations at each layer
    are expected to express correlation structures between the incoming signals. Frequently,
    these interstitial representations comprise a sequence of states transitioning
    from the initial layer to the final layer. You may naturally wonder, “How do these
    representations change throughout the layers of the network” and “How similar
    are these layers?” Interestingly, for some architectures, this question may yield
    deep insight into the network’s behavior.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练神经网络时，预期每一层的潜在空间表示能够表达传入信号之间的相关结构。通常，这些层间表示由一系列从初始层到最终层的状态转换组成。你可能会自然而然地想到：“这些表示如何随网络层次变化而变化？”以及“这些层有多相似？”有趣的是，对于某些架构，这个问题可能揭示出网络行为的深刻见解。
- en: This process of comparing layer representations is called *correlation analysis*.
    For an MLP with layers <math alttext="1 comma ellipsis comma upper N"><mrow><mn>1</mn>
    <mo>,</mo> <mo>...</mo> <mo>,</mo> <mi>N</mi></mrow></math> , the correlations
    may be represented by an <math alttext="upper N times upper N"><mrow><mi>N</mi>
    <mo>×</mo> <mi>N</mi></mrow></math> matrix of pairwise relationships. The idea
    is that each layer comprises a series of latent factors, and similar to correlation
    analysis for other features of a dataset, these latent features’ relationships
    may be simply summarized by their covariance.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 比较层表示的这一过程称为*相关分析*。对于具有层 <math alttext="1 comma ellipsis comma upper N"><mrow><mn>1</mn>
    <mo>,</mo> <mo>...</mo> <mo>,</mo> <mi>N</mi></mrow></math> 的 MLP，这些相关性可以通过一个
    <math alttext="upper N times upper N"><mrow><mi>N</mi> <mo>×</mo> <mi>N</mi></mrow></math>
    的矩阵来表示成对关系。其核心思想是，每一层都包含一系列潜在因素，类似于数据集其他特征的相关分析，这些潜在特征的关系可以简单地通过它们的协方差进行总结。
- en: Affinity and p-sale
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 亲和力和 p-sale
- en: 'As you’ve seen, MF is a powerful dimension-reduction technique that can yield
    an estimator for the probability of a sale (often shorted to *p-sale*). In MF,
    the goal has been to decompose this historical data on user behavior and the product
    sales matrix into two lower-dimensional matrices: one that represents user preferences
    and another that represents product characteristics. Now, let’s convert this MF
    model into a sale estimator.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，MF 是一种强大的降维技术，可以生成销售概率的估算器（通常缩写为*p-sale*）。在 MF 中，目标是将用户行为和产品销售矩阵的历史数据分解为两个低维矩阵：一个代表用户偏好，另一个代表产品特征。现在，让我们将这个
    MF 模型转换为销售估算器。
- en: 'Let <math alttext="upper R element-of double-struck upper R Superscript left-parenthesis
    upper M times upper N right-parenthesis"><mrow><mi>R</mi> <mo>∈</mo> <msup><mi>ℝ</mi>
    <mrow><mo>(</mo><mi>M</mi><mo>×</mo><mi>N</mi><mo>)</mo></mrow></msup></mrow></math>
    be the historical data matrix, where *M* is the number of users and *N* is the
    number of products. The MF aims to find two matrices <math alttext="upper U element-of
    double-struck upper R Superscript left-parenthesis upper M times d right-parenthesis"><mrow><mi>U</mi>
    <mo>∈</mo> <msup><mi>ℝ</mi> <mrow><mo>(</mo><mi>M</mi><mo>×</mo><mi>d</mi><mo>)</mo></mrow></msup></mrow></math>
    and <math alttext="upper V element-of double-struck upper R Superscript left-parenthesis
    upper N times d right-parenthesis"><mrow><mi>V</mi> <mo>∈</mo> <msup><mi>ℝ</mi>
    <mrow><mo>(</mo><mi>N</mi><mo>×</mo><mi>d</mi><mo>)</mo></mrow></msup></mrow></math>
    , where <math alttext="d"><mi>d</mi></math> is the dimensionality of the latent
    space, such that:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 让 <math alttext="upper R element-of double-struck upper R Superscript left-parenthesis
    upper M times upper N right-parenthesis"><mrow><mi>R</mi> <mo>∈</mo> <msup><mi>ℝ</mi>
    <mrow><mo>(</mo><mi>M</mi><mo>×</mo><mi>N</mi><mo>)</mo></mrow></msup></mrow></math>
    是历史数据矩阵，其中 *M* 是用户数，*N* 是产品数。MF 的目标是找到两个矩阵 <math alttext="upper U element-of double-struck
    upper R Superscript left-parenthesis upper M times d right-parenthesis"><mrow><mi>U</mi>
    <mo>∈</mo> <msup><mi>ℝ</mi> <mrow><mo>(</mo><mi>M</mi><mo>×</mo><mi>d</mi><mo>)</mo></mrow></msup></mrow></math>
    和 <math alttext="upper V element-of double-struck upper R Superscript left-parenthesis
    upper N times d right-parenthesis"><mrow><mi>V</mi> <mo>∈</mo> <msup><mi>ℝ</mi>
    <mrow><mo>(</mo><mi>N</mi><mo>×</mo><mi>d</mi><mo>)</mo></mrow></msup></mrow></math>
    ，其中 <math alttext="d"><mi>d</mi></math> 是潜在空间的维数，使得：
- en: <math alttext="upper R asymptotically-equals upper U asterisk upper V Superscript
    upper T" display="block"><mrow><mi>R</mi> <mo>≃</mo> <mi>U</mi> <mo>*</mo> <msup><mi>V</mi>
    <mi>T</mi></msup></mrow></math>
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper R asymptotically-equals upper U asterisk upper V Superscript
    upper T" display="block"><mrow><mi>R</mi> <mo>≃</mo> <mi>U</mi> <mo>*</mo> <msup><mi>V</mi>
    <mi>T</mi></msup></mrow></math>
- en: The *probability of a sale*, or equivalently a read, watch, eat, or click, can
    be predicted using MF by first decomposing the historical data matrix into user
    and product matrices, and then calculating a score that represents the likelihood
    of a user purchasing a given product. This score can be calculated using the dot
    product of the corresponding row in the user matrix and the column in the product
    matrix, followed by a logistic function to transform the dot product into a probability
    score.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '*销售的概率*，或等效地说，阅读、观看、吃或点击，可以通过MF预测，首先将历史数据矩阵分解为用户和产品矩阵，然后计算一个表示用户购买特定产品可能性的分数。这个分数可以通过用户矩阵中对应行和产品矩阵中的列的点积计算，然后通过逻辑函数将点积转换为概率分数。'
- en: 'Mathematically, the probability of a sale for a user *u* and a product *p*
    can be represented as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，用户*u*和产品*p*的销售概率可以表示如下：
- en: <math alttext="upper P left-parenthesis u comma p right-parenthesis equals normal
    s normal i normal g normal m normal o normal i normal d left-parenthesis u asterisk
    p Superscript upper T Baseline right-parenthesis" display="block"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <mi>u</mi> <mo>,</mo> <mi>p</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mi>sigmoid</mi> <mrow><mo>(</mo> <mi>u</mi> <mo>*</mo> <msup><mi>p</mi> <mi>T</mi></msup>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper P left-parenthesis u comma p right-parenthesis equals normal
    s normal i normal g normal m normal o normal i normal d left-parenthesis u asterisk
    p Superscript upper T Baseline right-parenthesis" display="block"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <mi>u</mi> <mo>,</mo> <mi>p</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mi>sigmoid</mi> <mrow><mo>(</mo> <mi>u</mi> <mo>*</mo> <msup><mi>p</mi> <mi>T</mi></msup>
    <mo>)</mo></mrow></mrow></math>
- en: 'Here, sigmoid is the logistic function that maps the dot product of the user
    and product vectors to a probability score between 0 and 1:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，sigmoid 是将用户和产品向量的点积映射到介于 0 和 1 之间的概率分数的逻辑函数：
- en: <math alttext="s i g m o i d left-parenthesis x right-parenthesis equals 1 slash
    left-parenthesis 1 plus e x p left-parenthesis negative x right-parenthesis right-parenthesis"
    display="block"><mrow><mi>s</mi> <mi>i</mi> <mi>g</mi> <mi>m</mi> <mi>o</mi> <mi>i</mi>
    <mi>d</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo> <mo>=</mo> <mn>1</mn> <mo>/</mo> <mo>(</mo>
    <mn>1</mn> <mo>+</mo> <mi>e</mi> <mi>x</mi> <mi>p</mi> <mo>(</mo> <mo>-</mo> <mi>x</mi>
    <mo>)</mo> <mo>)</mo></mrow></math>
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="s i g m o i d left-parenthesis x right-parenthesis equals 1 slash
    left-parenthesis 1 plus e x p left-parenthesis negative x right-parenthesis right-parenthesis"
    display="block"><mrow><mi>s</mi> <mi>i</mi> <mi>g</mi> <mi>m</mi> <mi>o</mi> <mi>i</mi>
    <mi>d</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo> <mo>=</mo> <mn>1</mn> <mo>/</mo> <mo>(</mo>
    <mn>1</mn> <mo>+</mo> <mi>e</mi> <mi>x</mi> <mi>p</mi> <mo>(</mo> <mo>-</mo> <mi>x</mi>
    <mo>)</mo> <mo>)</mo></mrow></math>
- en: The <math alttext="p Superscript upper T"><msup><mi>p</mi> <mi>T</mi></msup></math>
    represents the transpose of the product vector. The dot product of the user and
    product vectors is a measure of the similarity between the user’s preferences
    and the product’s characteristics, and the logistic function maps this similarity
    score to a probability score.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="p Superscript upper T"><msup><mi>p</mi> <mi>T</mi></msup></math>表示产品向量的转置。用户和产品向量的点积是用户偏好与产品特性之间相似性的度量，逻辑函数将此相似性分数映射到概率分数。
- en: The user and product matrices can be trained on the historical data by using
    various MF algorithms, such as SVD, NMF, or ALS. Once the matrices are trained,
    the dot product and logistic function can be applied to new user-product pairs
    to predict the probability of a sale. The predicted probabilities can then be
    used to rank and recommend products to the user.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 用户和产品矩阵可以通过使用各种MF算法（如SVD、NMF或ALS）在历史数据上进行训练。一旦矩阵训练好了，可以将点积和逻辑函数应用于新的用户-产品对，以预测销售的概率。预测的概率可以用来对用户进行产品排名和推荐。
- en: It’s worth highlighting that, since the loss function for ALS is convex (meaning
    there is a single global minimum), the convergence can be fast when we fix either
    the user or item matrix. In this method, the user matrix is fixed and the item
    matrix is solved for. Then the item matrix is fixed and the user matrix is solved
    for. The method alternates between the two solutions, and because the loss is
    convex in this regime, the method converges quickly.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，由于ALS的损失函数是凸的（意味着存在唯一的全局最小值），当我们固定用户或物品矩阵时，收敛速度可能会很快。在这种方法中，固定用户矩阵，解决物品矩阵。然后固定物品矩阵，解决用户矩阵。该方法在两种解之间交替，并且因为在这个范围内损失是凸的，所以方法会迅速收敛。
- en: The dot product of the corresponding row in the user matrix and column in the
    product matrix represents the affinity score between the user and the product,
    or how well the user’s preferences match the product’s characteristics. However,
    this score alone may not be a sufficient predictor of whether the user will actually
    purchase the product.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 用户矩阵中对应行和产品矩阵中列的点积表示用户和产品之间的亲和分数，或者说用户偏好与产品特性匹配的程度。然而，仅凭这个分数可能不足以预测用户是否会实际购买产品。
- en: The logistic function applied to the dot product in the MF model transforms
    the affinity score into a probability score, which represents the likelihood of
    a sale. This transformation takes into account additional factors beyond just
    the user’s preferences and the product’s characteristics, such as the overall
    popularity of the product, the user’s purchasing behavior, and any other relevant
    external factors. By incorporating these additional factors, MF is able to better
    predict the probability of a sale, rather than just an affinity score.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在MF模型中，应用于点积的逻辑函数将亲和分数转换为概率分数，该分数表示销售的可能性。此转换考虑了除用户偏好和产品特性外的额外因素，例如产品的整体流行度、用户的购买行为以及其他相关的外部因素。通过整合这些额外因素，MF能够更好地预测销售的概率，而不仅仅是亲和分数。
- en: A comparison library (however, not in JAX) for computing latent embeddings linearly
    is [libFM](http://libfm.org). The formulation for a factorization machine is similar
    to a GloVe embedding in that it also models the interaction between two vectors,
    but the dot product can be used for regression or binary classification tasks.
    The method can also be extended to recommend more than two kinds of items beyond
    user and item.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 一个比较库（不过不在 JAX 中）用于线性计算潜在嵌入是 [libFM](http://libfm.org)。因子机的公式类似于 GloVe 嵌入，它也模拟两个向量之间的交互，但点积可以用于回归或二元分类任务。该方法还可以扩展到推荐超过用户和项目两种类型的物品。
- en: In summary, MF produces probabilities of sale instead of just affinity scores
    by incorporating additional factors beyond the user’s preferences and the product’s
    characteristics, and transforming the affinity score into a probability score
    by using a logistic function.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，矩阵因子化（MF）不仅通过将用户偏好和产品特性之外的因素整合，产生销售概率而非仅仅亲和力评分，还通过使用 logistic 函数将亲和力分数转换为概率分数。
- en: Propensity Weighting for Recommendation System Evaluation
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推荐系统评估的倾向分配加权
- en: As you’ve seen, recommendation systems are evaluated based on user feedback,
    which is collected from the deployed recommendation system. However, this data
    is causally influenced by the deployed system, creating a feedback loop that may
    bias the evaluation of new models. This feedback loop can lead to confounding
    variables, making it difficult to distinguish between user preferences and the
    influence of the deployed system.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，推荐系统是根据用户反馈进行评估的，这些反馈来自部署的推荐系统。然而，这些数据受到部署系统的因果影响，形成了一个反馈循环，可能会偏向评估新模型。这种反馈循环可能导致混杂变量，使得很难区分用户偏好和部署系统影响之间的关系。
- en: If this surprises you, let’s consider for a moment what would have to be true
    for a recommendation system to *not* causally influence the actions users take
    and/or the outcomes that result from those actions. That would require assumptions
    like “the recommendations are completely ignored by the user” and “the system
    makes recommendations at random.” Propensity weighting can mitigate some of the
    worst effects of this problem.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这让您感到惊讶，让我们考虑一下，推荐系统*不*对用户采取的行动和/或这些行动导致的结果产生因果影响，这需要假设像“用户完全忽视推荐”和“系统随机推荐”。倾向分配可以减轻这个问题的一些最坏影响。
- en: 'The performance of a recommender system depends on many factors, including
    user-item characteristics, contextual information, and trends, which can affect
    the quality of the recommendations and the user engagement. However, the influence
    can be mutual: the user interactions influence the recommender, and vice versa.
    Evaluating the causal effect of a recommender system on user behavior and satisfaction
    is therefore a challenging task, as it requires controlling for potential confounding
    factors—those that may affect both the treatment assignment (the recommendation
    strategy) and the outcome of interest (the user’s response to the recommendations).'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统的性能取决于许多因素，包括用户-项目特征、上下文信息和趋势，这些因素可以影响推荐的质量和用户参与度。然而，影响可能是相互的：用户互动影响推荐系统，反之亦然。因此，评估推荐系统对用户行为和满意度的因果效应是一项具有挑战性的任务，因为它需要控制潜在的混杂因素——这些因素可能影响治疗分配（推荐策略）和感兴趣的结果（用户对推荐的响应）。
- en: 'Causal inference provides a framework for addressing these challenges. In the
    context of recommender systems, causal inference can help answer questions such
    as these:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 因果推断为解决这些挑战提供了一个框架。在推荐系统的背景下，因果推断可以帮助回答以下问题：
- en: How does the choice of recommendation strategy affect user engagement, such
    as CTRs, purchase rates, and satisfaction ratings?
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐策略的选择如何影响用户参与度，如点击率、购买率和满意度评分？
- en: What is the optimal recommendation strategy for a given user segment, item category,
    or context?
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于给定用户段、项目类别或上下文，什么是最佳推荐策略？
- en: What are the long-term effects of a recommendation strategy on user retention,
    loyalty, and lifetime value?
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐策略对用户保留、忠诚度和生命周期价值的长期影响是什么？
- en: We’ll round out this chapter by introducing one aspect of causal inference important
    to recommender systems, based on the concept of propensity score. We’ll introduce
    propensity to quantify the adjusted likelihood of some items being shown to the
    user. We’ll then see how this interacts with the famous Simpson’s paradox.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过介绍因果推断中一个对推荐系统至关重要的方面来结束本章，基于倾向得分的概念。我们将介绍倾向性，以量化向用户展示某些项的调整后可能性。接下来我们将看到这如何与著名的辛普森悖论互动。
- en: Propensity
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 倾向性
- en: In many data science problems, we are forced to contend with confounders and,
    notably, the correlation between those confounders and a target outcome. Depending
    on the setting, the confounder may be of a variety of forms. Interestingly, in
    recommendation systems, that confounder can be the system itself! Offline evaluation
    of recommendation systems is subject to confounders derived from the item selection
    behavior of users and the deployed recommendation system.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多数据科学问题中，我们不得不处理混杂因素，特别是混杂因素与目标结果之间的相关性。根据设置的不同，混杂因素可能具有各种形式。有趣的是，在推荐系统中，这种混杂因素可能是系统本身！推荐系统的离线评估受用户的项目选择行为和部署的推荐系统产生的混杂因素的影响。
- en: If this issue seems a bit circular, it kind of is. This is sometimes called
    *closed-loop feedback*. One approach to mitigation is propensity weighting, which
    aims to address this problem by considering each feedback in the corresponding
    stratum based on the estimated propensities. You may recall that *propensity*
    refers to the likelihood of a user seeing an item; by inversely weighting by this,
    we can offset the selection bias. Compared to the standard offline holdout evaluation,
    this method attempts to represent the actual utility of the examined recommendation
    models.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个问题看起来有点循环，它确实有点。这有时被称为*闭环反馈*。缓解的一种方法是倾向加权，通过考虑每个反馈在相应层次上的估计倾向性来解决这个问题。你可能还记得*倾向性*指的是用户看到某个项的可能性；通过其倒数加权，我们可以抵消选择偏差。与标准的离线留置评估相比，这种方法试图代表所考虑的推荐模型的实际效用。
- en: Utilizing Counterfactuals
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用反事实推断
- en: One other approach to mitigating selection bias that we won’t dive into is *counterfactual
    evaluation*, which estimates the actual utility of a recommendation model with
    propensity-weighting techniques more similar to off-policy evaluation approaches
    in reinforcement learning (RL). However, counterfactual evaluation often relies
    on accurate logging propensities in an open-loop setting where some random items
    are exposed to the user, which is not practical for most recommendation problems.
    If you have the option to include randomized recommendations to users for rating,
    this can help de-bias as well. One such setting where these methods may be combined
    is in RL-based recommenders that use explore-exploit methods like a multiarmed
    bandit or other structured randomization.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 缓解选择偏差的另一种方法是*反事实评估*，它使用类似于强化学习中的离线策略评估方法的倾向加权技术来估计推荐模型的实际效用。然而，反事实评估通常依赖于在开环设置中准确记录倾向性，其中一些随机项目会暴露给用户，这对大多数推荐问题来说是不实际的。如果您可以选择向用户提供随机推荐以进行评分，这也可以帮助去偏差。这种方法可能与基于RL的推荐系统结合使用，后者使用探索-利用方法，如多臂老虎机或其他结构化随机化。
- en: '*Inverse propensity scoring* (*IPS*) is a propensity-based evaluation method
    that leverages importance sampling to account for the fact that the feedback collected
    from the deployed recommendation system is not uniformly random. The propensity
    score is a balancing factor that adjusts the observed feedback distribution conditioned
    on the propensity score. The IPS evaluation method is theoretically unbiased if
    open-loop feedback can be sampled from all possible items uniformly at random.
    In [Chapter 3](ch03.html#ch:math), we discussed the Matthew effect, or “the rich
    get richer” for recommendation systems; IPS is one way to combat this effect.
    Note the relationship here between the two ideas of the Matthew effect and Simpson’s
    paradox, when within different strata, selection effects create significant biasing.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '*反向倾向评分*（*IPS*）是一种基于倾向性的评估方法，利用重要性抽样来考虑部署的推荐系统收集的反馈不是均匀随机的事实。倾向性评分是一个平衡因子，它调整了基于倾向性评分的观察到的反馈分布。如果开环反馈可以从所有可能的项目中均匀随机抽样，那么IPS评估方法在理论上是无偏的。在[第三章](ch03.html#ch:math)中，我们讨论了马太效应，或者推荐系统的“富者愈富”现象；IPS是对抗这种效应的一种方式。注意这里马太效应和辛普森悖论之间的关系，当在不同的分层中，选择效应产生了显著的偏倚时，这种关系是存在的。'
- en: Propensity weighting is based on the idea that the probability of an item being
    exposed to a user by the deployed recommendation system (the propensity score)
    affects the feedback that is collected from that user. By reweighting the feedback
    based on the propensity scores, we can adjust for the bias introduced by the deployed
    system and obtain a more accurate evaluation of the new recommendation model.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 倾向性加权基于这样一个思想：部署的推荐系统使一个项目暴露给用户的概率（倾向性评分）会影响到从该用户收集到的反馈。通过根据倾向性评分重新加权反馈，我们可以调整由部署系统引入的偏倚，并获得对新推荐模型更准确的评估。
- en: To apply IPS, we need to estimate the propensity scores for each item-user interaction
    in the collected feedback dataset. This can be done by modeling the probability
    that the deployed system would have exposed the item to the user at the time of
    the interaction. One simple approach is to use the popularity of the item as a
    proxy for its propensity score. However, more sophisticated methods can be used
    to model the propensity scores based on user and item features, as well as the
    context of the interaction.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 要应用IPS，我们需要估计收集到的反馈数据集中每个项目-用户交互的倾向性评分。这可以通过建模部署系统在交互时会暴露项目给用户的概率来实现。一个简单的方法是使用项目的流行度作为其倾向性评分的代理。然而，也可以使用更复杂的方法来基于用户和项目特征以及交互的上下文来建模倾向性评分。
- en: Once the propensity scores are estimated, we can reweight the feedback by using
    importance sampling. Specifically, each feedback is weighted by the inverse of
    its propensity score so that items that are more likely to be exposed by the deployed
    system are downweighted, while items that are less likely to be exposed are upweighted.
    This reweighting process approximates a counterfactual distribution of feedback
    expected from surfacing recommendations from a uniform distribution of popularity.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦估计出倾向性评分，我们可以通过重要性抽样重新加权反馈。具体来说，每个反馈都被其倾向性评分的倒数加权，以便更可能由部署系统暴露的项目被降权，而更不可能被暴露的项目则被升权。这种重新加权过程近似于从人气均匀分布的推荐中获得的反馈的反事实分布。
- en: Finally, we can use the reweighted feedback to evaluate the new recommendation
    model via standard metrics for evaluation, as we’ve seen in this chapter. The
    effectiveness of the new model is then compared to that of the deployed system
    by using the reweighted feedback, providing a fairer and more accurate evaluation
    of the new model’s performance.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用重新加权的反馈通过标准的评估指标来评估新推荐模型，正如我们在本章中看到的那样。然后，通过使用重新加权的反馈，将新模型的效果与部署系统的效果进行比较，提供对新模型性能更公平、更准确的评估。
- en: Simpson’s and Mitigating Confounding
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 辛普森氏和缓解混杂变量
- en: '*Simpson’s paradox* is predicated on the idea of a confounding variable that
    establishes strata within which we see (potentially misleading) covariation. This
    paradox arises when the association between two variables is investigated but
    these variables are strongly influenced by a confounding variable.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '*辛普森悖论*是基于一个混杂变量的概念，该混杂变量建立了我们看到（可能误导性的）协变量的分层。当调查两个变量之间的关联性时，但这些变量受到混杂变量的强烈影响时，就会出现这种悖论。'
- en: In the case of recommendation systems, this confounding variable is the deployed
    model’s characteristics and tendencies of selection. The propensity score is introduced
    as a measure of a system’s deviation from an unbiased open-loop exposure scenario.
    This score allows for the design and analysis of offline evaluation of recommendation
    models based on the observed closed-loop feedback, mimicking some of the particular
    characteristics of the open-loop scenario.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 对于推荐系统，这个混淆变量是部署模型的特征和选择趋势。倾向分数被引入为系统从无偏开环暴露场景偏离的度量。此分数允许基于观察到的闭环反馈设计和分析推荐模型的离线评估，模拟开环场景的某些特定特征。
- en: Traditional descriptions of Simpson’s paradox often suggest stratification,
    a well-known approach to identify and estimate causal effects by first identifying
    the underlying strata before investigating causal effects in each stratum. This
    approach enables the measurement of the potential outcome irrespective of the
    confounding variable. For recommendation systems, this involves stratifying the
    observed outcome based on the possible values of the confounding variable, which
    is the deployed model’s characteristics.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的辛普森悖论描述通常建议分层，这是一种通过首先识别底层分层来识别和估计因果效应的众所周知方法。对于推荐系统，这涉及根据部署模型特征的可能值分层观察结果，该特征是混淆变量。
- en: The user-independent propensity score is estimated via a two-step generative
    process using the prior probability that an item is recommended by the deployed
    model and the conditional probability that the user interacts with the item, given
    that it is recommended. Based on a set of mild assumptions (but too mathematically
    technical to cover here), the user-independent propensity score can be estimated
    using maximum likelihood for each dataset.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 用户独立倾向分数通过使用先验概率来估计，即部署模型推荐项目的先验概率，以及在推荐时用户与项目交互的条件概率进行两步生成过程估计。基于一系列温和假设（但在这里数学上太技术化，不予详细讨论），可以使用每个数据集的最大似然估计用户独立倾向分数。
- en: 'We need to define the user-propensity score <math alttext="p Subscript u comma
    i"><msub><mi>p</mi> <mrow><mi>u</mi><mo>,</mo><mi>i</mi></mrow></msub></math>
    , which indicates the tendency—or frequency—of the deployed model to expose item
    <math alttext="i element-of upper I"><mrow><mi>i</mi> <mo>∈</mo> <mi>I</mi></mrow></math>
    to user <math alttext="u element-of upper U"><mrow><mi>u</mi> <mo>∈</mo> <mi>U</mi></mrow></math>
    . In practice, we marginalize over users to get the user-independent propensity
    score <math alttext="p Subscript asterisk comma i"><msub><mi>p</mi> <mrow><mo>*</mo><mo>,</mo><mi>i</mi></mrow></msub></math>
    . As described in [“Unbiased Offline Recommender Evaluation for Missing-Not-at-Random
    Implicit Feedback”](https://oreil.ly/mpM87) by Longqi Yang et al., the equation
    is as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要定义用户倾向分数 <math alttext="p Subscript u comma i"><msub><mi>p</mi> <mrow><mi>u</mi><mo>,</mo><mi>i</mi></mrow></msub></math>
    ，这表示部署模型暴露项目 <math alttext="i element-of upper I"><mrow><mi>i</mi> <mo>∈</mo>
    <mi>I</mi></mrow></math> 给用户 <math alttext="u element-of upper U"><mrow><mi>u</mi>
    <mo>∈</mo> <mi>U</mi></mrow></math> 的倾向或频率。在实践中，我们对用户进行边际化，得到用户独立倾向分数 <math alttext="p
    Subscript asterisk comma i"><msub><mi>p</mi> <mrow><mo>*</mo><mo>,</mo><mi>i</mi></mrow></msub></math>
    。正如 Longqi Yang 等人在 [“不偏离线推荐系统评估的隐式反馈”](https://oreil.ly/mpM87) 中所述，方程如下：
- en: <math alttext="p Subscript asterisk comma i Baseline alpha left-parenthesis
    n Subscript i Superscript asterisk Baseline right-parenthesis Superscript StartFraction
    gamma plus 1 Over 2 EndFraction" display="block"><mrow><msub><mi>p</mi> <mrow><mo>*</mo><mo>,</mo><mi>i</mi></mrow></msub>
    <mi>α</mi> <msup><mfenced close=")" open="(" separators=""><msubsup><mi>n</mi>
    <mi>i</mi> <mo>*</mo></msubsup></mfenced> <mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>γ</mi><mo>+</mo><mn>1</mn></mrow>
    <mn>2</mn></mfrac></mstyle></msup></mrow></math>
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="p Subscript asterisk comma i Baseline alpha left-parenthesis
    n Subscript i Superscript asterisk Baseline right-parenthesis Superscript StartFraction
    gamma plus 1 Over 2 EndFraction" display="block"><mrow><msub><mi>p</mi> <mrow><mo>*</mo><mo>,</mo><mi>i</mi></mrow></msub>
    <mi>α</mi> <msup><mfenced close=")" open="(" separators=""><msubsup><mi>n</mi>
    <mi>i</mi> <mo>*</mo></msubsup></mfenced> <mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>γ</mi><mo>+</mo><mn>1</mn></mrow>
    <mn>2</mn></mfrac></mstyle></msup></mrow></math>
- en: Here, <math alttext="n Subscript i Superscript asterisk"><msubsup><mi>n</mi>
    <mi>i</mi> <mo>*</mo></msubsup></math> is the total number of times item <math
    alttext="i"><mi>i</mi></math> interacted with, and <math alttext="gamma"><mi>γ</mi></math>
    is a parameter that affects the propensity distributions over items with different
    observed popularity. The power-law parameter <math alttext="gamma"><mi>γ</mi></math>
    affects the propensity distributions over items and depends on the examined dataset;
    we estimate the <math alttext="gamma"><mi>γ</mi></math> parameter by using maximum
    likelihood for each dataset.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，<math alttext="n Subscript i Superscript asterisk"><msubsup><mi>n</mi> <mi>i</mi>
    <mo>*</mo></msubsup></math> 是项目 <math alttext="i"><mi>i</mi></math> 与之交互的总次数，<math
    alttext="gamma"><mi>γ</mi></math> 是影响不同观察流行度的倾向分布的参数。幂律参数 <math alttext="gamma"><mi>γ</mi></math>
    影响项目的倾向分布，并且依赖于研究的数据集；我们使用每个数据集的最大似然估计 <math alttext="gamma"><mi>γ</mi></math>
    参数。
- en: With these estimates for propensity, we can then apply a simple inverse weighting
    <math alttext="w Subscript i Baseline equals StartFraction 1 Over p Subscript
    i Baseline EndFraction"><mrow><msub><mi>w</mi> <mi>i</mi></msub> <mo>=</mo> <mstyle
    displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn> <msub><mi>p</mi> <mi>i</mi></msub></mfrac></mstyle></mrow></math>
    when calculating the effect of feedback. Finally, we can combine these weightings
    with propensity matching, to generate counterfactual recommendations; by collecting
    approximately equal propensity items into strata, we can then use these strata
    as our confounding variable.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些倾向估计，我们可以在计算反馈效果时应用简单的反权重<math alttext="w Subscript i Baseline equals StartFraction
    1 Over p Subscript i Baseline EndFraction"><mrow><msub><mi>w</mi> <mi>i</mi></msub>
    <mo>=</mo> <mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn> <msub><mi>p</mi>
    <mi>i</mi></msub></mfrac></mstyle></mrow></math>。最后，我们可以将这些权重与倾向匹配结合起来，生成反事实推荐；通过收集大致相等的倾向项目到分层中，我们可以将这些分层用作混杂变量。
- en: Doubly Robust Estimation
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双重稳健估计
- en: 'Doubly robust estimation (DRE) is a method that combines two models: one that
    models the probability of receiving the treatment (being recommended an item by
    the deployed model) and one that models the outcome of interest (the user’s feedback
    on the item). The weights used in DRE depend on the predicted probabilities from
    both models. This method has the advantage that it can still provide unbiased
    estimates even if one of the models is misspecified.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 双重稳健估计（DRE）是一种结合两个模型的方法：一个模型建模接受治疗（通过已部署模型推荐项目）的概率，另一个模型建模感兴趣的结果（用户对项目的反馈）。在DRE中使用的权重取决于这两个模型的预测概率。该方法的优点在于，即使其中一个模型被错误规定，它仍然可以提供无偏估计。
- en: 'The structural equations for a doubly robust estimator with propensity score
    weighting and outcome model is as follows:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 具有倾向分数加权和结果模型的双重稳健估计器的结构方程如下：
- en: <math alttext="normal upper Theta equals StartFraction sigma-summation w Subscript
    i Baseline left-parenthesis upper Y Subscript i Baseline minus f left-parenthesis
    upper X Subscript i Baseline right-parenthesis right-parenthesis Over sigma-summation
    w Subscript i Baseline left-parenthesis upper T Subscript i Baseline minus p Subscript
    i Baseline right-parenthesis plus sigma-summation w Subscript i Baseline left-parenthesis
    p Subscript i Baseline left-parenthesis 1 minus p Subscript i Baseline right-parenthesis
    squared left-parenthesis f left-parenthesis upper X Subscript i Baseline right-parenthesis
    minus f Superscript asterisk Baseline left-parenthesis upper X Subscript i Baseline
    right-parenthesis right-parenthesis right-parenthesis EndFraction" display="block"><mrow><mi>Θ</mi>
    <mo>=</mo> <mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mo>∑</mo><msub><mi>w</mi>
    <mi>i</mi></msub> <mfenced close=")" open="(" separators=""><msub><mi>Y</mi> <mi>i</mi></msub>
    <mo>-</mo><mi>f</mi><mrow><mo>(</mo><msub><mi>X</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mfenced></mrow>
    <mrow><mo>∑</mo><msub><mi>w</mi> <mi>i</mi></msub> <mfenced close=")" open="("
    separators=""><msub><mi>T</mi> <mi>i</mi></msub> <mo>-</mo><msub><mi>p</mi> <mi>i</mi></msub></mfenced>
    <mo>+</mo><mo>∑</mo><msub><mi>w</mi> <mi>i</mi></msub> <mfenced close=")" open="("
    separators=""><msub><mi>p</mi> <mi>i</mi></msub> <msup><mrow><mo>(</mo><mn>1</mn><mo>-</mo><msub><mi>p</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup> <mfenced close=")" open="("
    separators=""><mi>f</mi><mrow><mo>(</mo><msub><mi>X</mi> <mi>i</mi></msub> <mo>)</mo></mrow><mo>-</mo><msup><mi>f</mi>
    <mo>*</mo></msup> <mrow><mo>(</mo><msub><mi>X</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mfenced></mfenced></mrow></mfrac></mstyle></mrow></math>
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal upper Theta equals StartFraction sigma-summation w Subscript
    i Baseline left-parenthesis upper Y Subscript i Baseline minus f left-parenthesis
    upper X Subscript i Baseline right-parenthesis right-parenthesis Over sigma-summation
    w Subscript i Baseline left-parenthesis upper T Subscript i Baseline minus p Subscript
    i Baseline right-parenthesis plus sigma-summation w Subscript i Baseline left-parenthesis
    p Subscript i Baseline left-parenthesis 1 minus p Subscript i Baseline right-parenthesis
    squared left-parenthesis f left-parenthesis upper X Subscript i Baseline right-parenthesis
    minus f Superscript asterisk Baseline left-parenthesis upper X Subscript i Baseline
    right-parenthesis right-parenthesis right-parenthesis EndFraction" display="block"><mrow><mi>Θ</mi>
    <mo>=</mo> <mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mo>∑</mo><msub><mi>w</mi>
    <mi>i</mi></msub> <mfenced close=")" open="(" separators=""><msub><mi>Y</mi> <mi>i</mi></msub>
    <mo>-</mo><mi>f</mi><mrow><mo>(</mo><msub><mi>X</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mfenced></mrow>
    <mrow><mo>∑</mo><msub><mi>w</mi> <mi>i</mi></msub> <mfenced close=")" open="("
    separators=""><msub><mi>T</mi> <mi>i</mi></msub> <mo>-</mo><msub><mi>p</mi> <mi>i</mi></msub></mfenced>
    <mo>+</mo><mo>∑</mo><msub><mi>w</mi> <mi>i</mi></msub> <mfenced close=")" open="("
    separators=""><msub><mi>p</mi> <mi>i</mi></msub> <msup><mrow><mo>(</mo><mn>1</mn><mo>-</mo><msub><mi>p</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup> <mfenced close=")" open="("
    separators=""><mi>f</mi><mrow><mo>(</mo><msub><mi>X</mi> <mi>i</mi></msub> <mo>)</mo></mrow><mo>-</mo><msup><mi>f</mi>
    <mo>*</mo></msup> <mrow><mo>(</mo><msub><mi>X</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mfenced></mfenced></mrow></mfrac></mstyle></mrow></math>
- en: Here, <math alttext="upper Y Subscript i"><msub><mi>Y</mi> <mi>i</mi></msub></math>
    is the outcome, <math alttext="upper X Subscript i"><msub><mi>X</mi> <mi>i</mi></msub></math>
    are covariates, <math alttext="upper T Subscript i"><msub><mi>T</mi> <mi>i</mi></msub></math>
    is the treatment, <math alttext="p Subscript i"><msub><mi>p</mi> <mi>i</mi></msub></math>
    is the propensity score, <math alttext="w Subscript i"><msub><mi>w</mi> <mi>i</mi></msub></math>
    is the weight, <math alttext="f left-parenthesis upper X Subscript i Baseline
    right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <msub><mi>X</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow></math> is the outcome model, and <math alttext="f Superscript
    asterisk Baseline left-parenthesis upper X Subscript i Baseline right-parenthesis"><mrow><msup><mi>f</mi>
    <mo>*</mo></msup> <mrow><mo>(</mo> <msub><mi>X</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
    is the estimated outcome model.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，<math alttext="upper Y Subscript i"><msub><mi>Y</mi> <mi>i</mi></msub></math>
    是结果，<math alttext="upper X Subscript i"><msub><mi>X</mi> <mi>i</mi></msub></math>
    是协变量，<math alttext="upper T Subscript i"><msub><mi>T</mi> <mi>i</mi></msub></math>
    是治疗，<math alttext="p Subscript i"><msub><mi>p</mi> <mi>i</mi></msub></math> 是倾向分数，<math
    alttext="w Subscript i"><msub><mi>w</mi> <mi>i</mi></msub></math> 是权重，<math alttext="f
    left-parenthesis upper X Subscript i Baseline right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <msub><mi>X</mi> <mi>i</mi></msub> <mo>)</mo></mrow></math> 是结果模型，<math
    alttext="f Superscript asterisk Baseline left-parenthesis upper X Subscript i
    Baseline right-parenthesis"><mrow><msup><mi>f</mi> <mo>*</mo></msup> <mrow><mo>(</mo>
    <msub><mi>X</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math> 是估计的结果模型。
- en: For a great introduction to these considerations, check out [“Give Me a Robust
    Estimator—and Make It a Double!](https://oreil.ly/sNhvF).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些考虑的很好介绍，请查看[“给我一个稳健的估计器 - 并且要双重！”](https://oreil.ly/sNhvF)。
- en: Summary
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: What a whirlwind! Latent spaces are one of the *most* important aspects of recommendation
    systems. They are the representations that we use to encode our users and items.
    Ultimately, latent spaces are about more than dimension reduction; they are about
    understanding a geometry in which measures of distance encode the meaning relevant
    to your ML task.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 真是一个旋风！潜在空间是推荐系统中*最*重要的方面之一。它们是我们用来编码用户和项目的表示。最终，潜在空间不仅仅是维度减少，它们关乎理解一种几何，其中距离的测量编码与您的机器学习任务相关的含义。
- en: The world of embeddings and encoders runs deep. We haven’t had time to discuss
    CLIP embeddings (image + text) or the Poincaré disk (naturally hierarchical distance
    measures). We didn’t dive deep into UMAP (a nonlinear density-aware dimension-reduction
    technique) or HNSW (a method for retrieval in latent spaces that respects local
    geometry well). Instead, we point you to the (contemporaneously published) [article
    by Vicki Boykis](https://oreil.ly/Rpu23) on embeddings, the [essay and guide to
    constructing embeddings](https://oreil.ly/UDZ1_) by Karel Minařík, or the [beautiful
    visual guide to text embeddings](https://oreil.ly/t1N48) by Meor Amer from Cohere.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入和编码器的世界深不可测。我们还没有时间讨论CLIP嵌入（图像+文本）或Poincaré盘（自然层次距离测量）。我们没有深入讨论UMAP（一种非线性密度感知的降维技术）或HNSW（一种在潜在空间中检索的方法，很好地尊重局部几何关系）。而是指向了Vicki
    Boykis的（同时发布的）[文章](https://oreil.ly/Rpu23)关于嵌入，Karel Minařík的[文章和构建嵌入的指南](https://oreil.ly/UDZ1_)，或者Cohere的Meor
    Amer的[文本嵌入美丽的视觉指南](https://oreil.ly/t1N48)。
- en: We’re now equipped with representations, but next we need to optimize. We’re
    building *personalized* recommendation systems, so let’s define the metrics that
    measure our performance on our task.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经有了表示方法，但接下来我们需要优化。我们正在构建*个性化*推荐系统，因此让我们定义一些度量标准来衡量我们在任务上的表现。
