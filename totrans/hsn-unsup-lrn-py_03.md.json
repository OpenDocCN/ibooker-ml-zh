["```py\n$ git clone https://github.com/aapatel09/handson-unsupervised-learning.git\n$ git lfs pull\n```", "```py\n$ cd handson-unsupervised-learning\n```", "```py\n$ conda create -n unsupervisedLearning python=3.6 anaconda\n```", "```py\n$ activate unsupervisedLearning\n```", "```py\n$ pip install tensorflow\n```", "```py\n$ pip install keras\n```", "```py\ncd xgboost\npip install xgboost-0.6+20171121-cp36-cp36m-win_amd64.whl\n```", "```py\n$ pip install xgboost-0.6+20171121-cp36-cp36m-win_amd64.whl\n```", "```py\n$ pip install lightgbm\n```", "```py\n$ pip install fastcluster\n```", "```py\n$ pip install hdbscan\n```", "```py\n$ pip install tslearn\n```", "```py\n$ jupyter notebook\n```", "```py\n'''Main'''\nimport numpy as np\nimport pandas as pd\nimport os\n\n'''Data Viz'''\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nimport matplotlib as mpl\n\n%matplotlib inline\n\n'''Data Prep'''\nfrom sklearn import preprocessing as pp\nfrom scipy.stats import pearsonr\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n'''Algos'''\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\n```", "```py\ncurrent_path = os.getcwd()\nfile = '\\\\datasets\\\\credit_card_data\\\\credit_card.csv'\ndata = pd.read_csv(current_path + file)\n```", "```py\ndata.head()\n```", "```py\ndata.describe()\n```", "```py\ndata.columns\n```", "```py\nIndex(['Time', 'V1,' 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21',\n'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount', 'Class'],\ndtype='object')\n```", "```py\ndata['Class'].sum()\n```", "```py\nnanCounter = np.isnan(data).sum()\n```", "```py\nTime \t\t0\nV1 \t\t0\nV2 \t\t0\nV3 \t\t0\nV4 \t\t0\nV5 \t\t0\nV6 \t\t0\nV7 \t\t0\nV8 \t\t0\nV9 \t\t0\nV10 \t\t0\nV11 \t\t0\nV12 \t\t0\nV13 \t\t0\nV14 \t\t0\nV15 \t\t0\nV16 \t\t0\nV17 \t\t0\nV18 \t\t0\nV19 \t\t0\nV20 \t\t0\nV21 \t\t0\nV22 \t\t0\nV23 \t\t0\nV24 \t\t0\nV25 \t\t0\nV26 \t\t0\nV27 \t\t0\nV28 \t\t0\nAmount \t0\nClass \t\t0\ndtype: \tint64\n```", "```py\ndistinctCounter = data.apply(lambda x: len(x.unique()))\n```", "```py\nTime \t\t124592\nV1 \t\t275663\nV2 \t\t275663\nV3 \t\t275663\nV4 \t\t275663\nV5 \t\t275663\nV6 \t\t275663\nV7 \t\t275663\nV8 \t\t275663\nV9 \t\t275663\nV10 \t\t275663\nV11 \t\t275663\nV12 \t\t275663\nV13 \t\t275663\nV14 \t\t275663\nV15 \t\t275663\nV16 \t\t275663\nV17 \t\t275663\nV18 \t\t275663\nV19 \t\t275663\nV20 \t\t275663\nV21 \t\t275663\nV22 \t\t275663\nV23 \t\t275663\nV24 \t\t275663\nV25 \t\t275663\nV26 \t\t275663\nV27 \t\t275663\nV28 \t\t275663\nAmount \t32767\nClass \t\t2\ndtype: \tint64\n```", "```py\ndataX = data.copy().drop([‘Class’],axis=1)\ndataY = data[‘Class’].copy()\n```", "```py\nfeaturesToScale = dataX.drop(['Time'],axis=1).columns\nsX = pp.StandardScaler(copy=True)\ndataX.loc[:,featuresToScale] = sX.fit_transform(dataX[featuresToScale])\n```", "```py\ncorrelationMatrix = pd.DataFrame(data=[],index=dataX.columns,\ncolumns=dataX.columns)\nfor i in dataX.columns:\n    for j in dataX.columns:\n        correlationMatrix.loc[i,j] = np.round(pearsonr(dataX.loc[:,i],\n         dataX.loc[:,j])[0],2)\n```", "```py\ncount_classes = pd.value_counts(data['Class'],sort=True).sort_index()\nax = sns.barplot(x=count_classes.index, y=tuple(count_classes/len(data)))\nax.set_title('Frequency Percentage by Class')\nax.set_xlabel('Class')\nax.set_ylabel('Frequency Percentage')\n```", "```py\nX_train, X_test, y_train, y_test = train_test_split(dataX,\n                                    dataY, test_size=0.33,\n                                    random_state=2018, stratify=dataY)\n```", "```py\nk_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2018)\n```", "```py\npenalty = 'l2'\nC = 1.0\nclass_weight = 'balanced'\nrandom_state = 2018\nsolver = 'liblinear'\n\nlogReg = LogisticRegression(penalty=penalty, C=C,\n            class_weight=class_weight, random_state=random_state,\n                            solver=solver, n_jobs=n_jobs)\n```", "```py\ntrainingScores = []\ncvScores = []\npredictionsBasedOnKFolds = pd.DataFrame(data=[],\n                                        index=y_train.index,columns=[0,1])\n\nmodel = logReg\n\nfor train_index, cv_index in k_fold.split(np.zeros(len(X_train))\n                                          ,y_train.ravel()):\n    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], \\\n        X_train.iloc[cv_index,:]\n    y_train_fold, y_cv_fold = y_train.iloc[train_index], \\\n        y_train.iloc[cv_index]\n\n    model.fit(X_train_fold, y_train_fold)\n    loglossTraining = log_loss(y_train_fold,\n                               model.predict_proba(X_train_fold)[:,1])\n    trainingScores.append(loglossTraining)\n\n    predictionsBasedOnKFolds.loc[X_cv_fold.index,:] = \\\n        model.predict_proba(X_cv_fold)\n    loglossCV = log_loss(y_cv_fold,\n                         predictionsBasedOnKFolds.loc[X_cv_fold.index,1])\n    cvScores.append(loglossCV)\n\n    print('Training Log Loss: ', loglossTraining)\n    print('CV Log Loss: ', loglossCV)\n\nloglossLogisticRegression = log_loss(y_train,\n                                     predictionsBasedOnKFolds.loc[:,1])\nprint('Logistic Regression Log Loss: ', loglossLogisticRegression)\n```", "```py\nTraining Log Loss: \t\t0.10080139188958696\nCV Log Loss:\t\t0.10490645274118293\nTraining Log Loss: \t\t0.12098957040484648\nCV Log Loss:\t\t0.11634801169793386\nTraining Log Loss: \t\t0.1074616029843435\nCV Log Loss:\t\t0.10845630232487576\nTraining Log Loss: \t\t0.10228137039781758\nCV Log Loss:\t\t0.10321736161148198\nTraining Log Loss: \t\t0.11476012373315266\nCV Log Loss:\t\t0.1160124452312548\n```", "```py\nLogistic Regression Log Loss: 0.10978811472134588\n```", "```py\npreds = pd.concat([y_train,predictionsBasedOnKFolds.loc[:,1]], axis=1)\npreds.columns = ['trueLabel','prediction']\npredictionsBasedOnKFoldsLogisticRegression = preds.copy()\n\nprecision, recall, thresholds = precision_recall_curve(preds['trueLabel'],\n                                                       preds['prediction'])\n\naverage_precision = average_precision_score(preds['trueLabel'],\n                                            preds['prediction'])\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n```", "```py\nfpr, tpr, thresholds = roc_curve(preds['trueLabel'],preds['prediction'])\n\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic:\n          Area under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()\n```", "```py\nn_estimators = 10\nmax_features = 'auto'\nmax_depth = None\nmin_samples_split = 2\nmin_samples_leaf = 1\nmin_weight_fraction_leaf = 0.0\nmax_leaf_nodes = None\nbootstrap = True\noob_score = False\nn_jobs = -1\nrandom_state = 2018\nclass_weight = 'balanced'\n\nRFC = RandomForestClassifier(n_estimators=n_estimators,\n        max_features=max_features, max_depth=max_depth,\n        min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n        min_weight_fraction_leaf=min_weight_fraction_leaf,\n        max_leaf_nodes=max_leaf_nodes, bootstrap=bootstrap,\n        oob_score=oob_score, n_jobs=n_jobs, random_state=random_state,\n        class_weight=class_weight)\n```", "```py\ntrainingScores = []\ncvScores = []\npredictionsBasedOnKFolds = pd.DataFrame(data=[],\n                                        index=y_train.index,columns=[0,1])\n\nmodel = RFC\n\nfor train_index, cv_index in k_fold.split(np.zeros(len(X_train)),\n                                          y_train.ravel()):\n    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], \\\n        X_train.iloc[cv_index,:]\n    y_train_fold, y_cv_fold = y_train.iloc[train_index], \\\n        y_train.iloc[cv_index]\n\n    model.fit(X_train_fold, y_train_fold)\n    loglossTraining = log_loss(y_train_fold, \\\n                                model.predict_proba(X_train_fold)[:,1])\n    trainingScores.append(loglossTraining)\n\n    predictionsBasedOnKFolds.loc[X_cv_fold.index,:] = \\\n        model.predict_proba(X_cv_fold)\n    loglossCV = log_loss(y_cv_fold, \\\n        predictionsBasedOnKFolds.loc[X_cv_fold.index,1])\n    cvScores.append(loglossCV)\n\n    print('Training Log Loss: ', loglossTraining)\n    print('CV Log Loss: ', loglossCV)\n\nloglossRandomForestsClassifier = log_loss(y_train,\n                                          predictionsBasedOnKFolds.loc[:,1])\nprint('Random Forests Log Loss: ', loglossRandomForestsClassifier)\n```", "```py\nTraining Log Loss: \t\t0.0003951763883952557\nCV Log Loss:\t\t0.014479198936303003\nTraining Log Loss: \t\t0.0004501221178398935\nCV Log Loss:\t\t0.005712702421375242\nTraining Log Loss: \t\t0.00043128813023860164\nCV Log Loss:\t\t0.00908372752510077\nTraining Log Loss: \t\t0.0004341676022058672\nCV Log Loss:\t\t0.013491161736979267\nTraining Log Loss: \t\t0.0004275530435950083\nCV Log Loss:\t\t0.009963232439211515\n```", "```py\nRandom Forests Log Loss: 0.010546004611793962\n```", "```py\nparams_xGB = {\n    'nthread':16, #number of cores\n    'learning rate': 0.3, #range 0 to 1, default 0.3\n    'gamma': 0, #range 0 to infinity, default 0\n        # increase to reduce complexity (increase bias, reduce variance)\n    'max_depth': 6, #range 1 to infinity, default 6\n    'min_child_weight': 1, #range 0 to infinity, default 1\n    'max_delta_step': 0, #range 0 to infinity, default 0\n    'subsample': 1.0, #range 0 to 1, default 1\n        # subsample ratio of the training examples\n    'colsample_bytree': 1.0, #range 0 to 1, default 1\n        # subsample ratio of features\n    'objective':'binary:logistic',\n    'num_class':1,\n    'eval_metric':'logloss',\n    'seed':2018,\n    'silent':1\n}\n```", "```py\ntrainingScores = []\ncvScores = []\npredictionsBasedOnKFolds = pd.DataFrame(data=[],\n                                    index=y_train.index,columns=['prediction'])\n\nfor train_index, cv_index in k_fold.split(np.zeros(len(X_train)),\n                                          y_train.ravel()):\n    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], \\\n        X_train.iloc[cv_index,:]\n    y_train_fold, y_cv_fold = y_train.iloc[train_index], \\\n        y_train.iloc[cv_index]\n\n    dtrain = xgb.DMatrix(data=X_train_fold, label=y_train_fold)\n    dCV = xgb.DMatrix(data=X_cv_fold)\n\n    bst = xgb.cv(params_xGB, dtrain, num_boost_round=2000,\n                 nfold=5, early_stopping_rounds=200, verbose_eval=50)\n\n    best_rounds = np.argmin(bst['test-logloss-mean'])\n    bst = xgb.train(params_xGB, dtrain, best_rounds)\n\n    loglossTraining = log_loss(y_train_fold, bst.predict(dtrain))\n    trainingScores.append(loglossTraining)\n\n    predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'] = \\\n        bst.predict(dCV)\n    loglossCV = log_loss(y_cv_fold, \\\n        predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'])\n    cvScores.append(loglossCV)\n\n    print('Training Log Loss: ', loglossTraining)\n    print('CV Log Loss: ', loglossCV)\n\nloglossXGBoostGradientBoosting = \\\n    log_loss(y_train, predictionsBasedOnKFolds.loc[:,'prediction'])\nprint('XGBoost Gradient Boosting Log Loss: ', loglossXGBoostGradientBoosting)\n```", "```py\nXGBoost Gradient Boosting Log Loss: 0.0029566906288156715\n```", "```py\nparams_lightGB = {\n    'task': 'train',\n    'application':'binary',\n    'num_class':1,\n    'boosting': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'metric_freq':50,\n    'is_training_metric':False,\n    'max_depth':4,\n    'num_leaves': 31,\n    'learning_rate': 0.01,\n    'feature_fraction': 1.0,\n    'bagging_fraction': 1.0,\n    'bagging_freq': 0,\n    'bagging_seed': 2018,\n    'verbose': 0,\n    'num_threads':16\n}\n```", "```py\ntrainingScores = []\ncvScores = []\npredictionsBasedOnKFolds = pd.DataFrame(data=[],\n                                index=y_train.index,columns=['prediction'])\n\nfor train_index, cv_index in k_fold.split(np.zeros(len(X_train)),\n                                          y_train.ravel()):\n    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], \\\n        X_train.iloc[cv_index,:]\n    y_train_fold, y_cv_fold = y_train.iloc[train_index], \\\n        y_train.iloc[cv_index]\n\n    lgb_train = lgb.Dataset(X_train_fold, y_train_fold)\n    lgb_eval = lgb.Dataset(X_cv_fold, y_cv_fold, reference=lgb_train)\n    gbm = lgb.train(params_lightGB, lgb_train, num_boost_round=2000,\n                   valid_sets=lgb_eval, early_stopping_rounds=200)\n\n    loglossTraining = log_loss(y_train_fold, \\\n                gbm.predict(X_train_fold, num_iteration=gbm.best_iteration))\n    trainingScores.append(loglossTraining)\n\n    predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'] = \\\n        gbm.predict(X_cv_fold, num_iteration=gbm.best_iteration)\n    loglossCV = log_loss(y_cv_fold, \\\n        predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'])\n    cvScores.append(loglossCV)\n\n    print('Training Log Loss: ', loglossTraining)\n    print('CV Log Loss: ', loglossCV)\n\nloglossLightGBMGradientBoosting = \\\n    log_loss(y_train, predictionsBasedOnKFolds.loc[:,'prediction'])\nprint('LightGBM gradient boosting Log Loss: ', loglossLightGBMGradientBoosting)\n```", "```py\nLightGBM Gradient Boosting Log Loss: 0.0029732268054261826\n```", "```py\npredictionsTestSetLogisticRegression = \\\n    pd.DataFrame(data=[],index=y_test.index,columns=['prediction'])\npredictionsTestSetLogisticRegression.loc[:,'prediction'] = \\\n    logReg.predict_proba(X_test)[:,1]\nlogLossTestSetLogisticRegression = \\\n    log_loss(y_test, predictionsTestSetLogisticRegression)\n\npredictionsTestSetRandomForests = \\\n    pd.DataFrame(data=[],index=y_test.index,columns=['prediction'])\npredictionsTestSetRandomForests.loc[:,'prediction'] = \\\n    RFC.predict_proba(X_test)[:,1]\nlogLossTestSetRandomForests = \\\n    log_loss(y_test, predictionsTestSetRandomForests)\n\npredictionsTestSetXGBoostGradientBoosting = \\\n    pd.DataFrame(data=[],index=y_test.index,columns=['prediction'])\ndtest = xgb.DMatrix(data=X_test)\npredictionsTestSetXGBoostGradientBoosting.loc[:,'prediction'] = \\\n    bst.predict(dtest)\nlogLossTestSetXGBoostGradientBoosting = \\\n    log_loss(y_test, predictionsTestSetXGBoostGradientBoosting)\n\npredictionsTestSetLightGBMGradientBoosting = \\\n    pd.DataFrame(data=[],index=y_test.index,columns=['prediction'])\npredictionsTestSetLightGBMGradientBoosting.loc[:,'prediction'] = \\\n    gbm.predict(X_test, num_iteration=gbm.best_iteration)\nlogLossTestSetLightGBMGradientBoosting = \\\n    log_loss(y_test, predictionsTestSetLightGBMGradientBoosting)\n```", "```py\nLog Loss of Logistic Regression on Test Set: 0.123732961313\nLog Loss of Random Forests on Test Set: 0.00918192757674\nLog Loss of XGBoost Gradient Boosting on Test Set: 0.00249116807943\nLog Loss of LightGBM Gradient Boosting on Test Set: 0.002376320092424\n```", "```py\npredictionsBasedOnKFoldsFourModels = pd.DataFrame(data=[],index=y_train.index)\npredictionsBasedOnKFoldsFourModels = predictionsBasedOnKFoldsFourModels.join(\n    predictionsBasedOnKFoldsLogisticRegression['prediction'].astype(float), \\\n    how='left').join(predictionsBasedOnKFoldsRandomForests['prediction'] \\\n\t.astype(float),how='left',rsuffix=\"2\").join( \\\n    predictionsBasedOnKFoldsXGBoostGradientBoosting['prediction'] \\\n\t.astype(float), how='left',rsuffix=\"3\").join( \\\n    predictionsBasedOnKFoldsLightGBMGradientBoosting['prediction'] \\\n\t.astype(float), how='left',rsuffix=\"4\")\npredictionsBasedOnKFoldsFourModels.columns = \\\n    ['predsLR','predsRF','predsXGB','predsLightGBM']\n\nX_trainWithPredictions = \\\n    X_train.merge(predictionsBasedOnKFoldsFourModels,\n                  left_index=True,right_index=True)\n```", "```py\nparams_lightGB = {\n    'task': 'train',\n    'application':'binary',\n    'num_class':1,\n    'boosting': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'metric_freq':50,\n    'is_training_metric':False,\n    'max_depth':4,\n    'num_leaves': 31,\n    'learning_rate': 0.01,\n    'feature_fraction': 1.0,\n    'bagging_fraction': 1.0,\n    'bagging_freq': 0,\n    'bagging_seed': 2018,\n    'verbose': 0,\n    'num_threads':16\n}\n```", "```py\ntrainingScores = []\ncvScores = []\npredictionsBasedOnKFoldsEnsemble = \\\n    pd.DataFrame(data=[],index=y_train.index,columns=['prediction'])\n\nfor train_index, cv_index in k_fold.split(np.zeros(len(X_train)), \\\n                                          y_train.ravel()):\n    X_train_fold, X_cv_fold = \\\n        X_trainWithPredictions.iloc[train_index,:], \\\n        X_trainWithPredictions.iloc[cv_index,:]\n    y_train_fold, y_cv_fold = y_train.iloc[train_index], y_train.iloc[cv_index]\n\n    lgb_train = lgb.Dataset(X_train_fold, y_train_fold)\n    lgb_eval = lgb.Dataset(X_cv_fold, y_cv_fold, reference=lgb_train)\n    gbm = lgb.train(params_lightGB, lgb_train, num_boost_round=2000,\n                   valid_sets=lgb_eval, early_stopping_rounds=200)\n\n    loglossTraining = log_loss(y_train_fold, \\\n        gbm.predict(X_train_fold, num_iteration=gbm.best_iteration))\n    trainingScores.append(loglossTraining)\n\n    predictionsBasedOnKFoldsEnsemble.loc[X_cv_fold.index,'prediction'] = \\\n        gbm.predict(X_cv_fold, num_iteration=gbm.best_iteration)\n    loglossCV = log_loss(y_cv_fold, \\\n        predictionsBasedOnKFoldsEnsemble.loc[X_cv_fold.index,'prediction'])\n    cvScores.append(loglossCV)\n\n    print('Training Log Loss: ', loglossTraining)\n    print('CV Log Loss: ', loglossCV)\n\nloglossEnsemble = log_loss(y_train, \\\n        predictionsBasedOnKFoldsEnsemble.loc[:,'prediction'])\nprint('Ensemble Log Loss: ', loglossEnsemble)\n```", "```py\nEnsemble Log Loss: 0.002885415974220497\n```", "```py\n'''Pipeline for New Data'''\n# first, import new data into a dataframe called 'newData'\n# second, scale data\n# newData.loc[:,featuresToScale] = sX.transform(newData[featuresToScale])\n# third, predict using LightGBM\n# gbm.predict(newData, num_iteration=gbm.best_iteration)\n```"]