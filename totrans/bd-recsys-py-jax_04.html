<html><head></head><body><section data-pdf-bookmark="Chapter 3. Mathematical Considerations" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch:math">&#13;
<h1><span class="label">Chapter 3. </span>Mathematical Considerations</h1>&#13;
&#13;
&#13;
<p>Most of this book is focused on implementation and on practical considerations necessary to get recommendation systems working. In this chapter, you‚Äôll find the most abstract and theoretical concepts of the book. The purpose of this chapter is to cover a few of the essential ideas that undergird the field. It‚Äôs important to understand these ideas as they lead to pathological behavior in recommendation systems and motivate many architectural decisions.</p>&#13;
&#13;
<p>We‚Äôll start by discussing the shape of data you often see in recommendation systems, and why that shape can require careful thought. Next we‚Äôll talk about the underlying mathematical idea, similarity, that drives most modern recommendation systems. We‚Äôll briefly cover a different way of thinking about what a recommender does, for those with a more statistical inclination. Finally, we‚Äôll use analogies to NLP to formulate the popular approach.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Zipf‚Äôs Laws in RecSys and the Matthew Effect" data-type="sect1"><div class="sect1" id="id25">&#13;
<h1>Zipf‚Äôs Laws in RecSys and the Matthew Effect</h1>&#13;
&#13;
<p>In<a data-primary="Zipf's law" data-type="indexterm" id="zipfs03"/><a data-primary="Matthew effect" data-type="indexterm" id="mateff03"/><a data-primary="machine learning (ML)" data-secondary="Zipf's law" data-type="indexterm" id="MLzipf03"/><a data-primary="recommendation systems" data-secondary="Matthew effect in" data-type="indexterm" id="RSmatt03"/> a great many applications of ML, a caveat is given early: the distribution of observations of unique items from a large corpus is modeled by <em>Zipf‚Äôs law</em>‚Äîthe frequency of occurrence drops exponentially. In recommendation systems, the <em>Matthew effect</em> appears in the popular item‚Äôs click rates or the popular user‚Äôs feedback rates. For example, popular items have dramatically larger click counts than average, and more-engaged users give far more ratings than average.</p>&#13;
<div data-type="note" epub:type="note"><h1>The Matthew Effect</h1>&#13;
<p>The Matthew effect‚Äîor<a data-primary="popularity bias" data-type="indexterm" id="popbias03"/> <em>popularity bias</em>‚Äîstates that the most popular items continue to attract the most attention and widen the gap with other items.</p>&#13;
</div>&#13;
&#13;
<p>Take, for example, the <a href="https://oreil.ly/xiUaq">MovieLens dataset</a>, an extremely popular dataset for benchmarking recommendation systems. <a href="https://oreil.ly/Uzm2G">Jenny Sheng</a> observes the behavior shown in <a data-type="xref" href="#fig:movierank-zipfian">Figure¬†3-1</a> for a number of movie ratings:</p>&#13;
&#13;
<figure><div class="figure" id="fig:movierank-zipfian">&#13;
<img alt="Movierank Zipfian" src="assets/brpj_0301.png"/>&#13;
<h6><span class="label">Figure 3-1. </span>Zipfian distribution of movie-rank ratings</h6>&#13;
</div></figure>&#13;
&#13;
<p>At first glance, the rapid decline in ratings is obvious and stark, but is it a problem? Let‚Äôs assume our recommender will be built as a user-based collaborative filtering (CF) model‚Äîas alluded to in <a data-type="xref" href="ch02.html#ch:user-item">Chapter¬†2</a>. Then how might these distributions affect the recommender?</p>&#13;
&#13;
<p>We will consider the distributional ramifications of this phenomenon. Let the probability mass function be described by the simple Zipf‚Äôs law:</p>&#13;
<div data-type="equation">&#13;
<math alttext="f left-parenthesis k comma upper M right-parenthesis equals StartFraction 1 slash k Over sigma-summation Underscript n equals 1 Overscript upper M Endscripts left-parenthesis 1 slash n right-parenthesis EndFraction" display="block">&#13;
  <mrow>&#13;
    <mi>f</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>k</mi>&#13;
      <mo>,</mo>&#13;
      <mi>M</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mfrac><mrow><mn>1</mn><mo>/</mo><mi>k</mi></mrow> <mrow><msubsup><mo>‚àë</mo> <mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow> <mi>M</mi> </msubsup><mrow><mo>(</mo><mn>1</mn><mo>/</mo><mi>n</mi><mo>)</mo></mrow></mrow></mfrac>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>For <math alttext="upper M">&#13;
  <mi>M</mi>&#13;
</math> number of tokens in the corpus (in our example, the number of movies), <math alttext="k">&#13;
  <mi>k</mi>&#13;
</math> is the rank of a token when sorted by number of occurrences.</p>&#13;
&#13;
<p>Let‚Äôs consider users <math alttext="upper A">&#13;
  <mi>A</mi>&#13;
</math> and <math alttext="upper B">&#13;
  <mi>B</mi>&#13;
</math>, with <math alttext="upper N Subscript upper A Baseline equals StartAbsoluteValue script upper I Subscript upper A Baseline EndAbsoluteValue">&#13;
  <mrow>&#13;
    <msub><mi>N</mi> <mi>A</mi> </msub>&#13;
    <mo>=</mo>&#13;
    <mrow>&#13;
      <mo>|</mo>&#13;
      <msub><mi>‚Ñê</mi> <mi>A</mi> </msub>&#13;
      <mo>|</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math>  and <math alttext="upper N Subscript upper B Baseline equals StartAbsoluteValue script upper I Subscript upper B Baseline EndAbsoluteValue">&#13;
  <mrow>&#13;
    <msub><mi>N</mi> <mi>B</mi> </msub>&#13;
    <mo>=</mo>&#13;
    <mrow>&#13;
      <mo>|</mo>&#13;
      <msub><mi>‚Ñê</mi> <mi>B</mi> </msub>&#13;
      <mo>|</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math> ratings, respectively. Observe that the probability of <math alttext="upper V Subscript i">&#13;
  <msub><mi>V</mi> <mi>i</mi> </msub>&#13;
</math>, the <math alttext="i">&#13;
  <mi>i</mi>&#13;
</math>th most popular video, appearing in <math alttext="script upper I Subscript upper X">&#13;
  <msub><mi>‚Ñê</mi> <mi>X</mi> </msub>&#13;
</math> for a user <math alttext="upper X">&#13;
  <mi>X</mi>&#13;
</math> is given by the following:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper P left-parenthesis i right-parenthesis equals StartFraction f left-parenthesis i comma upper M right-parenthesis Over sigma-summation Underscript j equals 1 Overscript upper M Endscripts f left-parenthesis j comma upper M right-parenthesis EndFraction equals StartFraction 1 slash i Over sigma-summation Underscript j equals 1 Overscript upper M Endscripts 1 slash j EndFraction" display="block">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>i</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mfrac><mrow><mi>f</mi><mo>(</mo><mi>i</mi><mo>,</mo><mi>M</mi><mo>)</mo></mrow> <mrow><msubsup><mo>‚àë</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>M</mi> </msubsup><mi>f</mi><mrow><mo>(</mo><mi>j</mi><mo>,</mo><mi>M</mi><mo>)</mo></mrow></mrow></mfrac>&#13;
    <mo>=</mo>&#13;
    <mfrac><mrow><mn>1</mn><mo>/</mo><mi>i</mi></mrow> <mrow><msubsup><mo>‚àë</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>M</mi> </msubsup><mn>1</mn><mo>/</mo><mi>j</mi></mrow></mfrac>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Thus the joint probability of an item appearing in two user‚Äôs ratings is shown here:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper P left-parenthesis i squared right-parenthesis equals left-parenthesis StartFraction 1 slash i Over sigma-summation Underscript j equals 1 Overscript upper M Endscripts 1 slash j EndFraction right-parenthesis squared" display="block">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msup><mi>i</mi> <mn>2</mn> </msup>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <msup><mfenced close=")" open="(" separators=""><mfrac><mrow><mn>1</mn><mo>/</mo><mi>i</mi></mrow> <mrow><msubsup><mo>‚àë</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>M</mi> </msubsup><mn>1</mn><mo>/</mo><mi>j</mi></mrow></mfrac></mfenced> <mn>2</mn> </msup>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>In words, the probability of two users sharing an item in their rating sets drops off with the square of its popularity rank.</p>&#13;
&#13;
<p>This becomes important when we also consider that our, yet unstated, definition of user-based CF is based on<a data-primary="similarity" data-secondary="in collaborative filtering" data-secondary-sortas="collaborative filtering" data-type="indexterm" id="id414"/> similarity in users‚Äô ratings sets. This similarity is <em>the number of jointly rated items by two users, divided by the total number of items rated by either.</em></p>&#13;
&#13;
<p>Taking this definition, we can, for example, compute the similarity score for one shared item among <math alttext="upper A">&#13;
  <mi>A</mi>&#13;
</math> and <math alttext="upper B">&#13;
  <mi>B</mi>&#13;
</math>:</p>&#13;
<div data-type="equation">&#13;
<math alttext="sigma-summation Underscript i equals 1 Overscript upper M Endscripts StartFraction upper P left-parenthesis i squared right-parenthesis Over double-vertical-bar script upper I Subscript upper A Baseline union script upper I Subscript upper B Baseline double-vertical-bar EndFraction" display="block">&#13;
  <mrow>&#13;
    <munderover><mo>‚àë</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>M</mi> </munderover>&#13;
    <mfrac><mrow><mi>P</mi><mo>(</mo><msup><mi>i</mi> <mn>2</mn> </msup><mo>)</mo></mrow> <mrow><mrow><mo>‚à•</mo></mrow><msub><mi>‚Ñê</mi> <mi>A</mi> </msub><mo>‚à™</mo><msub><mi>‚Ñê</mi> <mi>B</mi> </msub><mrow><mo>‚à•</mo></mrow></mrow></mfrac>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>The average similarity score of two users is then generalized as follows via repeated application of the preceding equation:</p>&#13;
<div data-type="equation">&#13;
<math alttext="sigma-summation Underscript t equals 1 Overscript min left-parenthesis upper N Subscript upper A Baseline comma upper N Subscript upper B Baseline right-parenthesis Endscripts left-parenthesis product Underscript i Subscript k Baseline equals i Subscript k minus 1 Baseline plus 1 Overscript t minus 1 Endscripts sigma-summation Underscript i equals 1 Overscript upper M Endscripts left-parenthesis StartStartFraction upper P left-parenthesis i Subscript k Baseline Superscript 2 Baseline right-parenthesis OverOver StartFraction double-vertical-bar script upper I Subscript upper A Baseline union script upper I Subscript upper B Baseline double-vertical-bar Over t EndFraction EndEndFraction right-parenthesis right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <munderover><mo>‚àë</mo> <mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow> <mrow><mo form="prefix" movablelimits="true">min</mo><mo>(</mo><msub><mi>N</mi> <mi>A</mi> </msub><mo>,</mo><msub><mi>N</mi> <mi>B</mi> </msub><mo>)</mo></mrow> </munderover>&#13;
    <mfenced close=")" open="(" separators="">&#13;
      <munderover><mo>‚àè</mo> <mrow><msub><mi>i</mi> <mi>k</mi> </msub><mo>=</mo><msub><mi>i</mi> <mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow> </msub><mo>+</mo><mn>1</mn></mrow> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow> </munderover>&#13;
      <munderover><mo>‚àë</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>M</mi> </munderover>&#13;
      <mfenced close=")" open="(" separators="">&#13;
        <mfrac><mrow><mi>P</mi><mo>(</mo><msup><mrow><msub><mi>i</mi> <mi>k</mi> </msub></mrow> <mn>2</mn> </msup><mo>)</mo></mrow> <mfrac><mrow><mrow><mo>‚à•</mo></mrow><msub><mi>‚Ñê</mi> <mi>A</mi> </msub><mo>‚à™</mo><msub><mi>‚Ñê</mi> <mi>B</mi> </msub><mrow><mo>‚à•</mo></mrow></mrow> <mi>t</mi></mfrac></mfrac>&#13;
      </mfenced>&#13;
    </mfenced>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>via repeated application of the preceding observation.</p>&#13;
&#13;
<p>These combinatorial formulas not only indicate the relevance of the Zipfian in our algorithms, but we also see an almost direct effect on the output of scores. Consider the experiment in <a href="https://oreil.ly/m6iw7">‚ÄúQuantitative Analysis of Matthew Effect and Sparsity Problem of Recommender Systems‚Äù</a> by Hao Wang et al. on the <a href="https://oreil.ly/NqJOw">Last.fm dataset</a>. Last.fm is a music-listening tracker enabling users to keep track of all the songs they listen to; for Last.fm users, the authors demonstrate average similarity scores for pairs of users, and they find that this Matthew effect persists into the similarity matrix (<a data-type="xref" href="#fig:lastfm-matthew-effect">Figure¬†3-2</a>).</p>&#13;
&#13;
<figure><div class="figure" id="fig:lastfm-matthew-effect">&#13;
<img alt="Last.fm Matthew Effect" src="assets/brpj_0302.png"/>&#13;
<h6><span class="label">Figure 3-2. </span>Matthew effect as seen on the Last.fm dataset</h6>&#13;
</div></figure>&#13;
&#13;
<p>Observe the radical difference between ‚Äúhot‚Äù cells and all the others. The bright cells are few among the mostly dark, suggesting a difficult combination of some extremely popular items among the far more common frequency close to zero. While these results might seem scary, later we‚Äôll consider diversity-aware loss functions that can mitigate the Matthew effect. A simpler way is to use downstream sampling methods, which we will discuss as part of our explore-exploit algorithms. Finally, the Matthew effect is only the first of two major impacts of this Zipfian; let‚Äôs turn our attention to the second.<a data-primary="" data-startref="zipfs03" data-type="indexterm" id="id415"/><a data-primary="" data-startref="mateff03" data-type="indexterm" id="id416"/><a data-primary="" data-startref="popbias03" data-type="indexterm" id="id417"/><a data-primary="" data-startref="MLzipf03" data-type="indexterm" id="id418"/><a data-primary="" data-startref="RSmatt03" data-type="indexterm" id="id419"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Sparsity" data-type="sect1"><div class="sect1" id="id26">&#13;
<h1>Sparsity</h1>&#13;
&#13;
<p>We<a data-primary="data collection and user logging" data-secondary="data sparsity" data-type="indexterm" id="DCULspars03"/><a data-primary="sparsity" data-type="indexterm" id="spars03"/><a data-primary="recommendation systems" data-secondary="data sparsity" data-type="indexterm" id="RSdspars03"/> must now reckon with sparsity. As the ratings skew more and more toward the most popular items, the least popular items are starved for data and recommendations, which is called <em>data sparsity.</em> This connects to the linear-algebraic definition: mostly zeros or not populated elements in a vector. When you consider again our user-item matrix, less popular items constitute columns with few entries; these are sparse vectors. Similarly, at scale we see that the Matthew effect pushes more and more of the total ratings into certain columns, and the matrix becomes sparse in the traditional mathematical sense. For this reason, sparsity is an extremely well-known challenge for recommendation systems.</p>&#13;
&#13;
<p>As before, let‚Äôs consider the implication on our CF algorithms from these sparse ratings. Again observe that the probability of <math alttext="upper V Subscript i">&#13;
  <msub><mi>V</mi> <mi>i</mi> </msub>&#13;
</math>, the <math alttext="i">&#13;
  <mi>i</mi>&#13;
</math>th most popular item, appearing in <math alttext="script upper I Subscript upper X">&#13;
  <msub><mi>‚Ñê</mi> <mi>X</mi> </msub>&#13;
</math> for a user <math alttext="upper X">&#13;
  <mi>X</mi>&#13;
</math> is given by the following:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper P left-parenthesis i right-parenthesis equals StartFraction f left-parenthesis i comma upper M right-parenthesis Over sigma-summation Underscript j equals 1 Overscript upper M Endscripts f left-parenthesis j comma upper M right-parenthesis EndFraction equals StartFraction 1 slash i Over sigma-summation Underscript j equals 1 Overscript upper M Endscripts 1 slash j EndFraction" display="block">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>i</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mfrac><mrow><mi>f</mi><mo>(</mo><mi>i</mi><mo>,</mo><mi>M</mi><mo>)</mo></mrow> <mrow><msubsup><mo>‚àë</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>M</mi> </msubsup><mi>f</mi><mrow><mo>(</mo><mi>j</mi><mo>,</mo><mi>M</mi><mo>)</mo></mrow></mrow></mfrac>&#13;
    <mo>=</mo>&#13;
    <mfrac><mrow><mn>1</mn><mo>/</mo><mi>i</mi></mrow> <mrow><msubsup><mo>‚àë</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>M</mi> </msubsup><mn>1</mn><mo>/</mo><mi>j</mi></mrow></mfrac>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Then</p>&#13;
<div data-type="equation">&#13;
<math alttext="left-parenthesis upper M minus 1 right-parenthesis asterisk upper P left-parenthesis i right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <mo>(</mo>&#13;
    <mi>M</mi>&#13;
    <mo>-</mo>&#13;
    <mn>1</mn>&#13;
    <mo>)</mo>&#13;
    <mo>*</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>i</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>is the expected number of other users who click the <math alttext="i">&#13;
  <mi>i</mi>&#13;
</math>th most popular item, so summing over all, <math alttext="i">&#13;
  <mi>i</mi>&#13;
</math> yields the total number of other users who will share a rating &#13;
<span class="keep-together">with <math alttext="upper X">&#13;
  <mi>X</mi>&#13;
</math>:</span></p>&#13;
<div data-type="equation">&#13;
<math alttext="sigma-summation Underscript i equals 1 Overscript upper M Endscripts left-parenthesis upper M minus 1 right-parenthesis asterisk upper P left-parenthesis i right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <munderover><mo>‚àë</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>M</mi> </munderover>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>M</mi>&#13;
      <mo>-</mo>&#13;
      <mn>1</mn>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>*</mo>&#13;
    <mi>P</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>i</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Again, as we pull back to the overall trends, we observe this sparsity sneaking into the actual computations for our CF algorithms, consider the trend of users of different ranks, and see how much their rankings are used to <em>collaborate</em> in other users‚Äô rankings (<a data-type="xref" href="#fig:user-sim-counts">Figure¬†3-3</a>).</p>&#13;
&#13;
<figure><div class="figure" id="fig:user-sim-counts">&#13;
<img alt="lastfm user similarity" src="assets/brpj_0303.png"/>&#13;
<h6><span class="label">Figure 3-3. </span>User similarity counts for the Last.fm dataset</h6>&#13;
</div></figure>&#13;
&#13;
<p>We see that this is an important result to always be aware of: sparsity pushes emphasis onto the most popular users and has the risk of making your recommender myopic.</p>&#13;
<div data-type="note" epub:type="note"><h1>Item-Based Collaborative Filtering</h1>&#13;
<p>While<a data-primary="item-based collaborative filtering" data-type="indexterm" id="id420"/><a data-primary="collaborative filtering (CF)" data-secondary="item-based" data-type="indexterm" id="id421"/> the equations are different, in this section, they apply similarly to item-based CF. Similarity in items exhibits the same inheritance of the Zipfian in their scores, and items consulted in the CF process drop off by rank.<a data-primary="" data-startref="DCULspars03" data-type="indexterm" id="id422"/><a data-primary="" data-startref="spars03" data-type="indexterm" id="id423"/><a data-primary="" data-startref="RSdspars03" data-type="indexterm" id="id424"/></p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="User Similarity for Collaborative Filtering" data-type="sect1"><div class="sect1" id="id27">&#13;
<h1>User Similarity for Collaborative Filtering</h1>&#13;
&#13;
<p>In mathematics, it‚Äôs<a data-primary="collaborative filtering (CF)" data-secondary="user similarity for" data-type="indexterm" id="CFusersim03"/><a data-primary="user similarity" data-secondary="in collaborative filtering" data-secondary-sortas="collaborative filtering" data-type="indexterm" id="UScolfil03"/><a data-primary="similarity" data-secondary="user similarity" data-type="indexterm" id="Suser03"/> common to hear discussion of<a data-primary="distances" data-type="indexterm" id="id425"/> <em>distances</em>. Even back to the Pythagorean theorem, we are taught to think of relationships between points as distances or dissimilarity. Indeed, this fundamental idea is canonized in mathematics as part of the definition of a metric:</p>&#13;
<div data-type="equation">&#13;
<math alttext="d left-parenthesis a comma c right-parenthesis less-than-or-equal-to d left-parenthesis a comma b right-parenthesis plus d left-parenthesis b comma c right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <mi>d</mi>&#13;
    <mo>(</mo>&#13;
    <mi>a</mi>&#13;
    <mo>,</mo>&#13;
    <mi>c</mi>&#13;
    <mo>)</mo>&#13;
    <mo>‚â§</mo>&#13;
    <mi>d</mi>&#13;
    <mo>(</mo>&#13;
    <mi>a</mi>&#13;
    <mo>,</mo>&#13;
    <mi>b</mi>&#13;
    <mo>)</mo>&#13;
    <mo>+</mo>&#13;
    <mi>d</mi>&#13;
    <mo>(</mo>&#13;
    <mi>b</mi>&#13;
    <mo>,</mo>&#13;
    <mi>c</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>In<a data-primary="dissimilarity function" data-type="indexterm" id="id426"/> ML, we often instead concern ourselves with the notion of similarity‚Äîan extremely related topic. In many cases, we can compute similarity or dissimilarity, as they are complements of each other; when <math alttext="d colon upper X times upper X right-arrow left-bracket 0 comma 1 right-bracket subset-of double-struck upper R">&#13;
  <mrow>&#13;
    <mi>d</mi>&#13;
    <mo>:</mo>&#13;
    <mi>X</mi>&#13;
    <mo>√ó</mo>&#13;
    <mi>X</mi>&#13;
    <mo>‚Üí</mo>&#13;
    <mo>[</mo>&#13;
    <mn>0</mn>&#13;
    <mo>,</mo>&#13;
    <mn>1</mn>&#13;
    <mo>]</mo>&#13;
    <mo>‚äÇ</mo>&#13;
    <mi>‚Ñù</mi>&#13;
  </mrow>&#13;
</math> is a <em>dissimilarity function</em>, then we often define the following:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper S i m left-parenthesis a comma b right-parenthesis colon equals 1 minus d left-parenthesis a comma b right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <mi>S</mi>&#13;
    <mi>i</mi>&#13;
    <mi>m</mi>&#13;
    <mo>(</mo>&#13;
    <mi>a</mi>&#13;
    <mo>,</mo>&#13;
    <mi>b</mi>&#13;
    <mo>)</mo>&#13;
    <mo>:</mo>&#13;
    <mo>=</mo>&#13;
    <mn>1</mn>&#13;
    <mo>-</mo>&#13;
    <mi>d</mi>&#13;
    <mo>(</mo>&#13;
    <mi>a</mi>&#13;
    <mo>,</mo>&#13;
    <mi>b</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>This may seem like a needlessly precise statement, but in fact you‚Äôll see that a <a href="https://oreil.ly/9xAT6">variety of options are available for framing similarity</a>. Furthermore, sometimes we even formulate similarity measures where the associated distance measure does not establish a metric on the set of objects. These so-called pseudospaces can still be incredibly important, and we‚Äôll show where they come up in <a data-type="xref" href="ch10.html#ch10">Chapter¬†10</a>.</p>&#13;
&#13;
<p>In the literature, you‚Äôll find that papers commonly start by introducing a new similarity measure and then training a model you‚Äôve seen before on that new measure. As you‚Äôll see, the way you choose to relate objects (users, items, features, etc.) can have a large effect on what your algorithms learn.</p>&#13;
&#13;
<p>For now, let‚Äôs laser in on some specific similarity measures. Consider a classic ML problem of clustering: we have a space (usually <math alttext="double-struck upper R Superscript n">&#13;
  <msup><mi>‚Ñù</mi> <mi>n</mi> </msup>&#13;
</math>) in which our data is represented and are asked to partition our data into subcollections of the population and assign these collections names. Frequently, these collections are intended to capture a certain meaning, or at the very least be useful for summarizing the collection elements‚Äô features.</p>&#13;
&#13;
<p>When you do that clustering, you frequently are considering points near to one another in that space. Further, if you‚Äôre given a new observation and asked to assign it to a collection as an inference task, you normally compute the new observation‚Äôs<a data-primary="nearest neighbors" data-secondary="in collaborative filtering" data-secondary-sortas="collaborative filetering" data-type="indexterm" id="id427"/> <em>nearest neighbors</em>. This could be the <em>k</em>-nearest neighbors or simply the nearest neighbor among cluster centers; either way, your task is to use the notion of similarity to associate‚Äîand thus classify. In CF, this same notion is used to relate a user for whom you wish to make recommendations to those you already have data from.</p>&#13;
<div data-type="note" epub:type="note" id="nearest_neighbors"><h1>Nearest Neighbors</h1>&#13;
<p><em>Nearest neighbors</em> is<a data-primary="nearest neighbors" data-secondary="definition of term" data-type="indexterm" id="id428"/> a catchall term that arises from the simple geometric idea that, given some space (points defined by feature vectors) and a point in that space, you can find the other points closest to it. This has applications in all of ML, including classification, ranking/recommendation, and clustering.  <a data-type="xref" href="ch06.html#Nearest-Neighbors">‚ÄúApproximate Nearest Neighbors‚Äù</a> provides more details.</p>&#13;
</div>&#13;
&#13;
<p>So how can we define similarity for our users in CF? They‚Äôre not obviously in the same space, so our usual tools seem to be lacking.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Pearson Correlation" data-type="sect2"><div class="sect2" id="id28">&#13;
<h2>Pearson Correlation</h2>&#13;
&#13;
<p>Our<a data-primary="Pearson correlation" data-type="indexterm" id="id429"/><a data-primary="correlation" data-secondary="Pearson correlation" data-type="indexterm" id="id430"/> original CF formulation indicated that users with similar tastes collaborate to recommend items for one another. Let two users <math alttext="upper A">&#13;
  <mi>A</mi>&#13;
</math> and <math alttext="upper B">&#13;
  <mi>B</mi>&#13;
</math> have a set of co-rated items‚Äîsimply the set of items with ratings from each‚Äîwritten as <math alttext="script upper R Subscript upper A comma upper B">&#13;
  <msub><mi>‚Ñõ</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow> </msub>&#13;
</math>, and a rating of item <math alttext="x">&#13;
  <mi>x</mi>&#13;
</math> by user <math alttext="upper A">&#13;
  <mi>A</mi>&#13;
</math> written as <math alttext="r Subscript upper A comma x">&#13;
  <msub><mi>r</mi> <mrow><mi>A</mi><mo>,</mo><mi>x</mi></mrow> </msub>&#13;
</math>. Then the following is the sum of deviation from <math alttext="upper A">&#13;
  <mi>A</mi>&#13;
</math>‚Äôs average rating over all of its co-rated items with <math alttext="upper B">&#13;
  <mi>B</mi>&#13;
</math>:</p>&#13;
<div data-type="equation">&#13;
<math alttext="sigma-summation Underscript x element-of script upper R Subscript upper A comma upper B Endscripts left-parenthesis r Subscript upper A comma x Baseline minus r overbar Subscript upper A Baseline right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <munder><mo>‚àë</mo> <mrow><mi>x</mi><mo>‚àà</mo><msub><mi>‚Ñõ</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow> </msub></mrow> </munder>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>r</mi> <mrow><mi>A</mi><mo>,</mo><mi>x</mi></mrow> </msub>&#13;
      <mo>-</mo>&#13;
      <msub><mover accent="true"><mi>r</mi> <mo>¬Ø</mo></mover> <mi>A</mi> </msub>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>If we think of these ratings as a random variable and consider the analog for <math alttext="upper B">&#13;
  <mi>B</mi>&#13;
</math>, the correlation between the jointly distributed variables (the population covariance) is our <em>Pearson correlation:</em></p>&#13;
<div data-type="equation">&#13;
<math alttext="USim Subscript upper A comma upper B Baseline equals StartFraction sigma-summation Underscript x element-of script upper R Subscript upper A comma upper B Baseline Endscripts left-parenthesis r Subscript upper A comma x Baseline minus r overbar Subscript upper A Baseline right-parenthesis left-parenthesis r Subscript upper B comma x Baseline minus r overbar Subscript upper B Baseline right-parenthesis Over StartRoot sigma-summation Underscript x element-of script upper R Subscript upper A comma upper B Baseline Endscripts left-parenthesis r Subscript upper A comma x Baseline minus r overbar Subscript upper A Baseline right-parenthesis squared EndRoot StartRoot sigma-summation Underscript x element-of script upper R Subscript upper A comma upper B Baseline Endscripts left-parenthesis r Subscript upper B comma x Baseline minus r overbar Subscript upper B Baseline right-parenthesis squared EndRoot EndFraction" display="block">&#13;
  <mrow>&#13;
    <msub><mi> USim </mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow> </msub>&#13;
    <mo>=</mo>&#13;
    <mfrac><mrow><msub><mo>‚àë</mo> <mrow><mi>x</mi><mo>‚àà</mo><msub><mi>‚Ñõ</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow> </msub></mrow> </msub><mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>A</mi><mo>,</mo><mi>x</mi></mrow> </msub><mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>¬Ø</mo></mover> <mi>A</mi> </msub><mo>)</mo></mrow><mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>B</mi><mo>,</mo><mi>x</mi></mrow> </msub><mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>¬Ø</mo></mover> <mi>B</mi> </msub><mo>)</mo></mrow></mrow> <mrow><msqrt><mrow><msub><mo>‚àë</mo> <mrow><mi>x</mi><mo>‚àà</mo><msub><mi>‚Ñõ</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow> </msub></mrow> </msub><msup><mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>A</mi><mo>,</mo><mi>x</mi></mrow> </msub><mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>¬Ø</mo></mover> <mi>A</mi> </msub><mo>)</mo></mrow> <mn>2</mn> </msup></mrow></msqrt><msqrt><mrow><msub><mo>‚àë</mo> <mrow><mi>x</mi><mo>‚àà</mo><msub><mi>‚Ñõ</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow> </msub></mrow> </msub><msup><mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>B</mi><mo>,</mo><mi>x</mi></mrow> </msub><mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>¬Ø</mo></mover> <mi>B</mi> </msub><mo>)</mo></mrow> <mn>2</mn> </msup></mrow></msqrt></mrow></mfrac>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p class="less_space pagebreak-before">Keeping in mind a few details here is extremely important:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>This is the similarity of the jointly distributed variables describing the users‚Äô ratings.</p>&#13;
</li>&#13;
<li>&#13;
<p>We compute this via all co-rated items, so user similarity is defined via item-ratings.</p>&#13;
</li>&#13;
<li>&#13;
<p>This is a pairwise similarity measure taking values in [‚Äì1,1] <math alttext="element-of double-struck upper R">&#13;
  <mrow>&#13;
    <mo>‚àà</mo>&#13;
    <mi>‚Ñù</mi>&#13;
  </mrow>&#13;
</math>.</p>&#13;
</li>&#13;
</ul>&#13;
<div data-type="tip"><h1>Correlation and Similarity</h1>&#13;
<p>In <a data-type="xref" href="part03.html#ranking">Part¬†III</a>, you will learn about additional definitions of <em>correlation</em> and <em>similarity</em> that are more well suited for handling ranking data and that accommodate implicit rankings in particular.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Ratings via Similarity" data-type="sect2"><div class="sect2" id="id29">&#13;
<h2>Ratings via Similarity</h2>&#13;
&#13;
<p>Now<a data-primary="ratings" data-secondary="via similarity" data-secondary-sortas="similarity" data-type="indexterm" id="id431"/> that we‚Äôve introduced user similarity, let‚Äôs use it! For a user <math alttext="upper A">&#13;
  <mi>A</mi>&#13;
</math> and item <math alttext="x">&#13;
  <mi>x</mi>&#13;
</math>, we can estimate the rating via similar users‚Äô ratings:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper A f f Subscript upper A comma i Baseline equals r overbar Subscript upper A Baseline plus StartFraction sigma-summation Underscript upper U element-of script upper N left-parenthesis upper A right-parenthesis Endscripts USim Subscript upper A comma upper U Baseline asterisk left-parenthesis r Subscript upper U comma i Baseline minus r overbar Subscript upper A Baseline right-parenthesis Over sigma-summation Underscript upper U element-of script upper N left-parenthesis upper A right-parenthesis Endscripts USim Subscript upper A comma upper U Baseline EndFraction" display="block">&#13;
  <mrow>&#13;
    <mi>A</mi>&#13;
    <mi>f</mi>&#13;
    <msub><mi>f</mi> <mrow><mi>A</mi><mo>,</mo><mi>i</mi></mrow> </msub>&#13;
    <mo>=</mo>&#13;
    <msub><mover accent="true"><mi>r</mi> <mo>¬Ø</mo></mover> <mi>A</mi> </msub>&#13;
    <mo>+</mo>&#13;
    <mfrac><mrow><msub><mo>‚àë</mo> <mrow><mi>U</mi><mo>‚àà</mo><mi>ùí©</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow> </msub><msub><mi> USim </mi> <mrow><mi>A</mi><mo>,</mo><mi>U</mi></mrow> </msub><mo>*</mo><mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>U</mi><mo>,</mo><mi>i</mi></mrow> </msub><mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>¬Ø</mo></mover> <mi>A</mi> </msub><mo>)</mo></mrow></mrow> <mrow><msub><mo>‚àë</mo> <mrow><mi>U</mi><mo>‚àà</mo><mi>ùí©</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow> </msub><msub><mi> USim </mi> <mrow><mi>A</mi><mo>,</mo><mi>U</mi></mrow> </msub></mrow></mfrac>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>This is the prediction for user <math alttext="upper A">&#13;
  <mi>A</mi>&#13;
</math>‚Äôs rating of item <math alttext="x">&#13;
  <mi>x</mi>&#13;
</math>, which takes <math alttext="upper A">&#13;
  <mi>A</mi>&#13;
</math>‚Äôs average adjusted rating of the similarity-weighted average ratings of all of <math alttext="upper A">&#13;
  <mi>A</mi>&#13;
</math>‚Äôs neighbors. In other words: <math alttext="upper A">&#13;
  <mi>A</mi>&#13;
</math>‚Äôs rating will probably be the average of people who have ratings like <math alttext="upper A">&#13;
  <mi>A</mi>&#13;
</math>‚Äôs rating, adjusted to how generous <math alttext="upper A">&#13;
  <mi>A</mi>&#13;
</math> is with ratings in general. We call this estimate the<a data-primary="user-item affinity score" data-type="indexterm" id="id432"/> <em>user-item affinity score</em>.</p>&#13;
&#13;
<p>But wait! What‚Äôs <math alttext="script upper N left-parenthesis upper A right-parenthesis">&#13;
  <mrow>&#13;
    <mi>ùí©</mi>&#13;
    <mo>(</mo>&#13;
    <mi>A</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>? It‚Äôs the neighborhood of <math alttext="upper A">&#13;
  <mi>A</mi>&#13;
</math>, via our USim definition from the preceding section. The idea here is that we are aggregating ratings over the local region of users identified as similar to our target user by the previous USim metric. How many neighbors? How do you pick those neighbors? These will be the subject of later chapters; for now, assume they‚Äôre <math alttext="k">&#13;
  <mi>k</mi>&#13;
</math>-nearest neighbors and assume that some hyperparameter tuning is used to determine a good value for <math alttext="k">&#13;
  <mi>k</mi>&#13;
</math>.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id433">&#13;
<h1>Correlation Metric Spaces</h1>&#13;
<p>You<a data-primary="correlation" data-secondary="correlation metric spaces" data-type="indexterm" id="id434"/><a data-primary="metric spaces" data-type="indexterm" id="id435"/> might wonder, ‚ÄúDoes this Pearson correlation yield a metric space under a transformation?‚Äù The answer is yes, but clearly defining the metric space is a bit more complicated than our simple definition. While the preceding equation can get us a distance, it‚Äôs not good enough to get us a metric space without a more novel transformation.</p>&#13;
&#13;
<p>In particular, for <math alttext="upper P left-parenthesis upper A comma upper B right-parenthesis">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>A</mi>&#13;
    <mo>,</mo>&#13;
    <mi>B</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>, the previously defined correlation, <math alttext="1 minus upper P left-parenthesis upper A comma upper B right-parenthesis">&#13;
  <mrow>&#13;
    <mn>1</mn>&#13;
    <mo>-</mo>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>A</mi>&#13;
    <mo>,</mo>&#13;
    <mi>B</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math> yields a distance that satisfies all metric properties <em>except</em> the<a data-primary="triangle inequality" data-type="indexterm" id="id436"/> triangle inequality. There are several known ways to adjust this, though: <math alttext="StartRoot 1 minus upper P left-parenthesis upper A comma upper B right-parenthesis squared EndRoot">&#13;
  <msqrt>&#13;
    <mrow>&#13;
      <mn>1</mn>&#13;
      <mo>-</mo>&#13;
      <mi>P</mi>&#13;
      <msup><mrow><mo>(</mo><mi>A</mi><mo>,</mo><mi>B</mi><mo>)</mo></mrow> <mn>2</mn> </msup>&#13;
    </mrow>&#13;
  </msqrt>&#13;
</math> is the most common. For a survey, see <a href="https://oreil.ly/6bmIp">‚ÄúMetric Distances Derived from Cosine Similarity and Pearson and Spearman Correlations‚Äù</a> by Stijn van Dongen and Anton J. Enright.<a data-primary="" data-startref="CFusersim03" data-type="indexterm" id="id437"/><a data-primary="" data-startref="UScolfil03" data-type="indexterm" id="id438"/><a data-primary="" data-startref="Suser03" data-type="indexterm" id="id439"/></p>&#13;
</div></aside>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Explore-Exploit as a Recommendation System" data-type="sect1"><div class="sect1" id="id30">&#13;
<h1>Explore-Exploit as a Recommendation System</h1>&#13;
&#13;
<p>So far<a data-primary="recommendation systems" data-secondary="explore-exploit as" data-type="indexterm" id="RSexplore03"/><a data-primary="explore-exploit recommenders" data-type="indexterm" id="exexrec03"/> we‚Äôve presented two ideas, slightly in tension with each other:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The MPIR, a simple, easy-to-understand recommender</p>&#13;
</li>&#13;
<li>&#13;
<p>The Matthew effect in recommendation systems and its runaway behavior in distributions of ratings</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>By now, you likely realize that the MPIR will amplify the Matthew effect and that the Matthew effect will drive the MPIR to the trivial recommender in the limit. This is the classic difficulty of maximizing a loss function with no randomization: it quickly settles into a modal state.</p>&#13;
&#13;
<p>This problem‚Äîand many others like it‚Äîencourages some modification to the algorithm to prevent this failure mode and continues to expose the algorithm and users to other options. The basic strategy for <em>explore-exploit schemes,</em> or<a data-primary="multiarmed bandits" data-type="indexterm" id="id440"/> <em>multiarmed bandits</em> as they‚Äôre called, is to take not only the outcome-maximizing recommendation but also a collection of alternative <em>variants,</em> and randomly determine which to use as a response.</p>&#13;
&#13;
<p>Taking a step back: given a collection of variant recommendations, or  <em>arms</em>, <math alttext="upper A">&#13;
  <mi>A</mi>&#13;
</math>, for which the outcome of each recommendation is <math alttext="y Subscript t">&#13;
  <msub><mi>y</mi> <mi>t</mi> </msub>&#13;
</math>, we have a prior reward function <math alttext="upper R left-parenthesis y Subscript t Baseline right-parenthesis">&#13;
  <mrow>&#13;
    <mi>R</mi>&#13;
    <mo>(</mo>&#13;
    <msub><mi>y</mi> <mi>t</mi> </msub>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>. The bandit (called an<a data-primary="agents" data-type="indexterm" id="id441"/> <em>agent</em> in this literature) would like to maximize <math alttext="upper R left-parenthesis y Subscript t Baseline right-parenthesis">&#13;
  <mrow>&#13;
    <mi>R</mi>&#13;
    <mo>(</mo>&#13;
    <msub><mi>y</mi> <mi>t</mi> </msub>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math> but doesn‚Äôt know the distribution of the outcomes <math alttext="upper Y Subscript a element-of upper A">&#13;
  <msub><mi>Y</mi> <mrow><mi>a</mi><mo>‚àà</mo><mi>A</mi></mrow> </msub>&#13;
</math>. The agent thus assumes some prior distributions for <math alttext="upper Y Subscript a element-of upper A">&#13;
  <msub><mi>Y</mi> <mrow><mi>a</mi><mo>‚àà</mo><mi>A</mi></mrow> </msub>&#13;
</math> and then collects data to update those distributions; after sufficient observations, the agent can estimate the expected values of each distribution, <math alttext="mu Subscript a element-of upper A Baseline equals double-struck upper E left-parenthesis script upper R left-parenthesis upper Y Subscript a Baseline right-parenthesis right-parenthesis">&#13;
  <mrow>&#13;
    <msub><mi>Œº</mi> <mrow><mi>a</mi><mo>‚àà</mo><mi>A</mi></mrow> </msub>&#13;
    <mo>=</mo>&#13;
    <mi>ùîº</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>‚Ñõ</mi>&#13;
      <mrow>&#13;
        <mo>(</mo>&#13;
        <msub><mi>Y</mi> <mi>a</mi> </msub>&#13;
        <mo>)</mo>&#13;
      </mrow>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math>.</p>&#13;
&#13;
<p>If the agent was able to confidently estimate these reward values, the recommendation problem would be solved: at inference, the agent would simply estimate the reward values for all variants for the user and select the reward-optimizing <em>arm.</em> This is, of course, ridiculous in totality, but the basic idea is useful nonetheless: hold prior assumptions about what will be the greatest expected reward, and explore alternatives with some frequency to continue to update the distributions and refine your estimators.</p>&#13;
&#13;
<p>Even when not explicitly using a multiarmed bandit, this insight is a powerful and useful framework for understanding the goal of a recommendation system. Utilizing the ideas of prior estimates for good recommendations and exploring other options to gain signal is a core idea that‚Äôs recurring. Let‚Äôs see one practicality of this approach.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="&#10;  œµ&#10;-greedy" data-type="sect2"><div class="sect2" id="id31">&#13;
<h2><math alttext="epsilon">&#13;
  <mi>œµ</mi>&#13;
</math>-greedy</h2>&#13;
&#13;
<p>How often should you explore versus use your reward-optimizing arm? The first best algorithm is <math alttext="epsilon">&#13;
  <mi>œµ</mi>&#13;
</math>-greedy: for <math alttext="epsilon element-of left-parenthesis 0 comma 1 right-parenthesis">&#13;
  <mrow>&#13;
    <mi>œµ</mi>&#13;
    <mo>‚àà</mo>&#13;
    <mo>(</mo>&#13;
    <mn>0</mn>&#13;
    <mo>,</mo>&#13;
    <mn>1</mn>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>, at each request the agent has the  probability <math alttext="epsilon">&#13;
  <mi>œµ</mi>&#13;
</math> of choosing a random arm and the  probability <math alttext="1 minus epsilon">&#13;
  <mrow>&#13;
    <mn>1</mn>&#13;
    <mo>-</mo>&#13;
    <mi>œµ</mi>&#13;
  </mrow>&#13;
</math> of selecting the currently highest estimated reward arm.</p>&#13;
&#13;
<p>Let‚Äôs take the MPIR and slightly modify it to include some exploration:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">jax</code> <code class="kn">import</code> <code class="n">random</code>&#13;
<code class="n">key</code> <code class="o">=</code> <code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>&#13;
&#13;
&#13;
<code class="k">def</code> <code class="nf">get_item_popularities</code><code class="p">()</code> <code class="o">-&gt;</code> <code class="n">Optional</code><code class="p">[</code><code class="n">Dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="nb">int</code><code class="p">]]:</code>&#13;
    <code class="o">...</code>&#13;
        <code class="c1"># Dict of pairs: (item-identifier, count item chosen)</code>&#13;
        <code class="k">return</code> <code class="n">item_choice_counts</code>&#13;
    <code class="k">return</code> <code class="kc">None</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">get_most_popular_recs_ep_greedy</code><code class="p">(</code>&#13;
    <code class="n">max_num_recs</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code>&#13;
    <code class="n">epsilon</code><code class="p">:</code> <code class="nb">float</code>&#13;
<code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Optional</code><code class="p">[</code><code class="n">List</code><code class="p">[</code><code class="nb">str</code><code class="p">]]:</code>&#13;
    <code class="k">assert</code> <code class="n">epsilon</code><code class="o">&lt;</code><code class="mf">1.0</code>&#13;
    <code class="k">assert</code> <code class="n">epsilon</code><code class="o">&gt;</code><code class="mi">0</code>&#13;
&#13;
    <code class="n">items_popularity_dict</code> <code class="o">=</code> <code class="n">get_item_popularities</code><code class="p">()</code>&#13;
    <code class="k">if</code> <code class="n">items_popularity_dict</code><code class="p">:</code>&#13;
        <code class="n">sorted_items</code> <code class="o">=</code> <code class="nb">sorted</code><code class="p">(</code>&#13;
            <code class="n">items_popularity_dict</code><code class="o">.</code><code class="n">items</code><code class="p">(),</code>&#13;
            <code class="n">key</code><code class="o">=</code><code class="k">lambda</code> <code class="n">item</code><code class="p">:</code> <code class="n">item</code><code class="p">[</code><code class="mi">1</code><code class="p">]),</code>&#13;
            <code class="n">reverse</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>&#13;
        <code class="p">)</code>&#13;
        <code class="n">top_items</code> <code class="o">=</code> <code class="p">[</code><code class="n">i</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="n">sorted_items</code><code class="p">]</code>&#13;
        <code class="n">recommendations</code> <code class="o">=</code> <code class="p">[]</code>&#13;
        <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">max_num_recs</code><code class="p">):</code> <code class="c1"># we wish to return max_num_recs</code>&#13;
            <code class="k">if</code> <code class="n">random</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="n">key</code><code class="p">)</code><code class="o">&gt;</code><code class="n">epsilon</code><code class="p">:</code> <code class="c1"># if greater than epsilon, exploit</code>&#13;
                <code class="n">recommendations</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">top_items</code><code class="o">.</code><code class="n">pop</code><code class="p">(</code><code class="mi">0</code><code class="p">))</code>&#13;
            <code class="k">else</code><code class="p">:</code> <code class="c1"># otherwise, explore</code>&#13;
                <code class="n">explore_choice</code> <code class="o">=</code> <code class="n">random</code><code class="o">.</code><code class="n">randint</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code class="nb">len</code><code class="p">(</code><code class="n">top_items</code><code class="p">))</code>&#13;
                <code class="n">recommendations</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">top_items</code><code class="o">.</code><code class="n">pop</code><code class="p">(</code><code class="n">explore_choice</code><code class="p">))</code>&#13;
        <code class="k">return</code> <code class="n">recommendations</code>&#13;
&#13;
    <code class="k">return</code> <code class="kc">None</code></pre>&#13;
&#13;
<p>The only modification to our MPIR is that now we have two cases for each potential recommendation from our <code>max_num_recs</code>. If a random probability is less than our <math alttext="epsilon">&#13;
  <mi>œµ</mi>&#13;
</math>, we proceed as before and select the most popular; otherwise, we select a random recommendation.</p>&#13;
<div data-type="note" epub:type="note"><h1>Maximizing Reward</h1>&#13;
<p>We‚Äôre interpreting<a data-primary="maximization of reward" data-type="indexterm" id="id442"/> maximization of reward as selecting the most-popular items. This is an important assumption, and as we move into more complicated recommenders, this will be the crucial assumption that we modify to get different algorithms and schemes.</p>&#13;
</div>&#13;
&#13;
<p>Now let‚Äôs summarize our recommender components again:</p>&#13;
<dl>&#13;
<dt>Collector</dt>&#13;
<dd>&#13;
<p>The collector<a data-primary="collector" data-secondary="in explore-exploit systems" data-secondary-sortas="explore-exploit systems" data-type="indexterm" id="id443"/> here need not change; we still want to get the item popularities first.</p>&#13;
</dd>&#13;
<dt>Ranker</dt>&#13;
<dd>&#13;
<p>The ranker<a data-primary="ranker" data-secondary="in explore-exploit systems" data-secondary-sortas="explore-exploit systems" data-type="indexterm" id="id444"/> also does not change! We begin by ranking the possible recommendations by popularity.</p>&#13;
</dd>&#13;
<dt>Server</dt>&#13;
<dd>&#13;
<p>If<a data-primary="server" data-secondary="in explore-exploit systems" data-secondary-sortas="explore-exploit systems" data-type="indexterm" id="id445"/> the collector and ranker remain the same, clearly the server is what must be adapted for this new recommender. This is the case; instead of taking the top items to fill <code>max_num_recs</code>, we now utilize our <math alttext="epsilon">&#13;
  <mi>œµ</mi>&#13;
</math> to determine at each step if the next recommendation added to our list should be next in line from the ranker or a random selection. Otherwise, we adhere to the same API schema and return the same shape of data.</p>&#13;
</dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="What Should &#10;  œµ&#10; Be?" data-type="sect2"><div class="sect2" id="id32">&#13;
<h2>What Should <math alttext="epsilon">&#13;
  <mi>œµ</mi>&#13;
</math> Be?</h2>&#13;
&#13;
<p>In the preceding discussion, <math alttext="epsilon">&#13;
  <mi>œµ</mi>&#13;
</math> is a fixed number for the entire call, but what should the value be? This is actually an area of great study, and the general wisdom is to start with large <math alttext="epsilon">&#13;
  <mi>œµ</mi>&#13;
</math> (to encourage more exploration) and then reduce over time. Determining the rate at which you decrease it, the starting value, and so on, requires serious thought and research. Additionally, this value can be tied into your prediction loop and be part of the training process. See <a href="https://oreil.ly/wk-OB">‚ÄúThe Exploration-Exploitation Trade-Off: Intuitions and Strategies‚Äù</a> by Joseph Rocca for a deeper dive.</p>&#13;
&#13;
<p>Other‚Äîoften better‚Äîsampling techniques exist for optimization.<a data-primary="importance sampling" data-type="indexterm" id="id446"/><a data-primary="sampling techniques" data-type="indexterm" id="id447"/> <em>Importance sampling</em> can utilize the ranking functions we build later to integrate the explore-exploit with what our data has to teach.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The NLP-RecSys Relationship" data-type="sect1"><div class="sect1" id="id33">&#13;
<h1>The NLP-RecSys Relationship</h1>&#13;
&#13;
<p>Let‚Äôs<a data-primary="natural language processing (NLP)" data-type="indexterm" id="NLPrecsys03"/><a data-primary="RecSys" data-secondary="relationship to natural language processing" data-type="indexterm" id="RSnlp03"/> utilize some intuition from a different area of ML, natural language processing. One<a data-primary="word2vec model" data-type="indexterm" id="id448"/> of the fundamental models in NLP is <em>word2vec</em>: a sequence-based model for language understanding that uses the words that occur in sentences together.</p>&#13;
&#13;
<p>For<a data-primary="skipgram-word2vec model" data-type="indexterm" id="id449"/> <em>skipgram-word2vec</em>, the model takes sentences and attempts to learn the implicit meaning of their words via their co-occurrence relationships with other words in those sentences. Each pair of co-occurring words constitutes a sample that is one-hot encoded and sent into a vocabulary-sized layer of neurons, with a bottleneck layer and a vocabulary-sized output layer for probabilities that words will occur.</p>&#13;
&#13;
<p>Via this network, we reduce the size of our representation to the bottleneck dimension and thus find a smaller dimensional representation of all our words than the original corpus-sized one-hot embedding. The thinking is that similarity of words can now be computed via vector similarity in this new representation space.</p>&#13;
&#13;
<p>Why is this related to recommendation systems? Well, because if we take the ordered sequence of user-item interactions (e.g., the sequence of movies a user has rated), we can utilize the same idea from word2vec to find item similarity instead of word similarity. In this analogy, the user history is the <em>sentence</em>.</p>&#13;
&#13;
<p>Previously, using our CF similarity, we decided that similar users can help inform what a good recommendation for a user should be. In this model, we are finding item-item similarity, so instead we assume that a user will like the items similar to those previously liked.</p>&#13;
<div data-type="note" epub:type="note"><h1>Items as Words</h1>&#13;
<p>You may have noticed that natural language models treat words as sequences, and in fact, our user history is a sequence too! For now, hold onto this knowledge. Later, this will guide us to sequence-based methods for RecSys.</p>&#13;
</div>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Vector Search" data-type="sect2"><div class="sect2" id="id34">&#13;
<h2>Vector Search</h2>&#13;
&#13;
<p>We<a data-primary="vector search" data-type="indexterm" id="id450"/> have built a collection of vector representations of our items, and we claim that similarity in this space (often called a<a data-primary="latent space" data-secondary="vector search and" data-type="indexterm" id="id451"/><a data-primary="ambient space" data-type="indexterm" id="id452"/><a data-primary="representation space" data-type="indexterm" id="id453"/> <em>latent space</em>, <em>representation space</em>, or <em>ambient space</em>) means similarity in <em>likability</em><a data-primary="likability" data-type="indexterm" id="id454"/> to users.</p>&#13;
&#13;
<p>To convert this<a data-primary="similarity" data-secondary="converting to recommendations" data-type="indexterm" id="id455"/> similarity to a recommendation, consider a user <math alttext="upper A">&#13;
  <mi>A</mi>&#13;
</math> with a collection of previously liked items <math alttext="script upper R Subscript upper A">&#13;
  <msub><mi>‚Ñõ</mi> <mi>A</mi> </msub>&#13;
</math>, and consider <math alttext="script upper A equals StartSet v Subscript x Baseline vertical-bar x element-of script upper R Subscript upper A Baseline EndSet">&#13;
  <mrow>&#13;
    <mi>ùíú</mi>&#13;
    <mo>=</mo>&#13;
    <mo>{</mo>&#13;
    <msub><mi>v</mi> <mi>x</mi> </msub>&#13;
    <mo>|</mo>&#13;
    <mi>x</mi>&#13;
    <mo>‚àà</mo>&#13;
    <msub><mi>‚Ñõ</mi> <mi>A</mi> </msub>&#13;
    <mo>}</mo>&#13;
  </mrow>&#13;
</math> the set of vectors associated to those items in this latent space. We are looking for a new item <math alttext="y">&#13;
  <mi>y</mi>&#13;
</math> that we think is good for <math alttext="upper A">&#13;
  <mi>A</mi>&#13;
</math>.</p>&#13;
<div data-type="warning" epub:type="warning"><h1>The Old Curse</h1>&#13;
<p>These latent spaces tend to be of high dimension, which Euclidean distance famously performs poorly in. As regions become sparse, the distance function performance decreases; local distances are meaningful, but global distances are not to be trusted. Instead, cosine distance shows better performance, but this is a topic of deep exploration. Additionally, instead of minimizing the distance, in practice it‚Äôs better to maximize the similarity.</p>&#13;
</div>&#13;
&#13;
<p>One simple way to use similarity to produce a recommendation is to take the closest item to the average of those that <math alttext="upper A">&#13;
  <mi>A</mi>&#13;
</math> likes:</p>&#13;
<div data-type="equation">&#13;
<math alttext="argmax Subscript y Baseline StartSet USim left-parenthesis v Subscript y Baseline comma a v g left-parenthesis script upper A right-parenthesis right-parenthesis bar y element-of Items EndSet" display="block">&#13;
  <mrow>&#13;
    <msub><mi> argmax </mi> <mi>y</mi> </msub>&#13;
    <mfenced close="}" open="{" separators="">&#13;
      <mi> USim </mi>&#13;
      <mo>(</mo>&#13;
      <msub><mi>v</mi> <mi>y</mi> </msub>&#13;
      <mo>,</mo>&#13;
      <mi>a</mi>&#13;
      <mi>v</mi>&#13;
      <mi>g</mi>&#13;
      <mrow>&#13;
        <mo>(</mo>&#13;
        <mi>ùíú</mi>&#13;
        <mo>)</mo>&#13;
      </mrow>&#13;
      <mo>)</mo>&#13;
      <mo>‚à£</mo>&#13;
      <mi>y</mi>&#13;
      <mo>‚àà</mo>&#13;
      <mi> Items </mi>&#13;
    </mfenced>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Here, <math alttext="d left-parenthesis minus comma minus right-parenthesis">&#13;
  <mrow>&#13;
    <mi>d</mi>&#13;
    <mo>(</mo>&#13;
    <mo>-</mo>&#13;
    <mo>,</mo>&#13;
    <mo>-</mo>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math> is a distance function in the latent space (usually cosine distance).</p>&#13;
&#13;
<p>The argmax essentially treats all of <math alttext="upper A">&#13;
  <mi>A</mi>&#13;
</math>‚Äôs ratings equally and suggests something near those. In practice, this process is often fraught. First, you could weight the terms by rating:</p>&#13;
<div data-type="equation">&#13;
<math alttext="argmax Subscript y Baseline StartSet USim left-parenthesis v Subscript y Baseline comma StartFraction sigma-summation Underscript v Subscript x Baseline element-of script upper A Endscripts r Subscript x Baseline Over StartAbsoluteValue script upper R Subscript script upper A Baseline EndAbsoluteValue EndFraction right-parenthesis bar y element-of Items EndSet" display="block">&#13;
  <mrow>&#13;
    <msub><mi> argmax </mi> <mi>y</mi> </msub>&#13;
    <mfenced close="}" open="{" separators="">&#13;
      <mi> USim </mi>&#13;
      <mo>(</mo>&#13;
      <msub><mi>v</mi> <mi>y</mi> </msub>&#13;
      <mo>,</mo>&#13;
      <mfrac><mrow><msub><mo>‚àë</mo> <mrow><msub><mi>v</mi> <mi>x</mi> </msub><mo>‚àà</mo><mi>ùíú</mi></mrow> </msub><msub><mi>r</mi> <mi>x</mi> </msub></mrow> <mrow><mrow><mo>|</mo></mrow><msub><mi>‚Ñõ</mi> <mi>ùíú</mi> </msub><mrow><mo>|</mo></mrow></mrow></mfrac>&#13;
      <mo>)</mo>&#13;
      <mo>‚à£</mo>&#13;
      <mi>y</mi>&#13;
      <mo>‚àà</mo>&#13;
      <mi> Items </mi>&#13;
    </mfenced>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>This can potentially improve the representativeness of the user feedback in the recommendations. Alternatively, you might find that a user rates movies across a variety of genres and themes. Averaging here will definitely lead to worse results, so maybe you want to simply find recommendations similar to one movie the user liked, weighted by that rating:</p>&#13;
<div data-type="equation">&#13;
<math alttext="argmax Subscript y Baseline StartSet StartFraction USim left-parenthesis v Subscript y Baseline comma v Subscript x Baseline right-parenthesis Over r Subscript x Baseline EndFraction bar y element-of Items comma v Subscript x Baseline element-of script upper A EndSet" display="block">&#13;
  <mrow>&#13;
    <msub><mi> argmax </mi> <mi>y</mi> </msub>&#13;
    <mfenced close="}" open="{" separators="">&#13;
      <mfrac><mrow><mi> USim </mi><mo>(</mo><msub><mi>v</mi> <mi>y</mi> </msub><mo>,</mo><msub><mi>v</mi> <mi>x</mi> </msub><mo>)</mo></mrow> <msub><mi>r</mi> <mi>x</mi> </msub></mfrac>&#13;
      <mo>‚à£</mo>&#13;
      <mi>y</mi>&#13;
      <mo>‚àà</mo>&#13;
      <mi> Items </mi>&#13;
      <mo>,</mo>&#13;
      <msub><mi>v</mi> <mi>x</mi> </msub>&#13;
      <mo>‚àà</mo>&#13;
      <mi>ùíú</mi>&#13;
    </mfenced>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Finally, you may even want to do this process several times for different items a user liked to get <math alttext="k">&#13;
  <mi>k</mi>&#13;
</math> recommendations:</p>&#13;
<div data-type="equation">&#13;
<math alttext="min hyphen k StartSet argmax Subscript y Baseline StartSet StartFraction USim left-parenthesis v Subscript y Baseline comma v Subscript x Baseline right-parenthesis Over r Subscript x Baseline EndFraction bar y element-of Items EndSet bar v Subscript x Baseline element-of script upper A EndSet" display="block">&#13;
  <mrow>&#13;
    <mi> min </mi>&#13;
    <mo>-</mo>&#13;
    <mi>k</mi>&#13;
    <mfenced close="}" open="{" separators="">&#13;
      <msub><mi> argmax </mi> <mi>y</mi> </msub>&#13;
      <mfenced close="}" open="{" separators="">&#13;
        <mfrac><mrow><mi> USim </mi><mo>(</mo><msub><mi>v</mi> <mi>y</mi> </msub><mo>,</mo><msub><mi>v</mi> <mi>x</mi> </msub><mo>)</mo></mrow> <msub><mi>r</mi> <mi>x</mi> </msub></mfrac>&#13;
        <mo>‚à£</mo>&#13;
        <mi>y</mi>&#13;
        <mo>‚àà</mo>&#13;
        <mi> Items </mi>&#13;
      </mfenced>&#13;
      <mo>‚à£</mo>&#13;
      <msub><mi>v</mi> <mi>x</mi> </msub>&#13;
      <mo>‚àà</mo>&#13;
      <mi>ùíú</mi>&#13;
    </mfenced>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Now we have <math alttext="k">&#13;
  <mi>k</mi>&#13;
</math> recommendations; each is similar to something that the user has liked and is weighted by how much they liked it. This approach utilized only an implicit geometry of the items formed by their co-occurrences.</p>&#13;
&#13;
<p>Latent spaces and the geometric power that comes with them for recommendations will be a through line for the rest of the book. We will often formulate our loss functions via these geometries, and we‚Äôll exploit the geometric intuition to brainstorm where to expand our technique next.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Nearest-Neighbors Search" data-type="sect2"><div class="sect2" id="id35">&#13;
<h2>Nearest-Neighbors Search</h2>&#13;
&#13;
<p>A<a data-primary="nearest neighbors" data-secondary="nearest-neighbors search" data-type="indexterm" id="id456"/> reasonable question to ask is ‚ÄúHow do I get these vectors that minimize this distance?‚Äù In all the preceding schemes, we are computing many distances and then finding minimums. In general, the problem of nearest neighbors is an extremely important and well-studied question.</p>&#13;
&#13;
<p>While finding the exact nearest neighbors can sometimes be slow, a lot of great progress has been made on<a data-primary="approximate nearest neighbors (ANN)" data-type="indexterm" id="id457"/> approximate nearest neighbors (ANN) searches. These algorithms not only return very close to the actual nearest neighbors, but they also perform orders of complexity faster. In general, when you see us (or other publications) computing an <math alttext="argmin">&#13;
  <mi> argmin </mi>&#13;
</math> (the argument that minimized the function) over some distances, there‚Äôs a good chance ANN is what‚Äôs used in practice.<a data-primary="" data-startref="NLPrecsys03" data-type="indexterm" id="id458"/><a data-primary="" data-startref="RSnlp03" data-type="indexterm" id="id459"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id303">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>Recommendation systems in the preceding chapter discussed data distribution principles such as Zipf‚Äôs law and the Matthew Effect. These principles lead to challenges, such as skewed user similarity scores and data sparsity. In the world of ML, while the traditional math focuses on distance, the emphasis is on the concept of similarity. Different measures of similarity can drastically alter algorithm learning outcomes, with clustering being a primary application.</p>&#13;
&#13;
<p>In the realm of recommendations, items are often represented in high-dimensional latent spaces. Similarity in these spaces hints at user preferences. Methods include recommending items close to a user‚Äôs average liked items, and this may be improved by adding a weighting by user-rating. However, individual preferences necessitate diverse recommendations. Latent spaces continue to be influential, driving recommendation techniques.</p>&#13;
&#13;
<p>Locating these vectors effectively requires the nearest-neighbors search. Though exact methods are resource-intensive, approximate nearest-neighbors offer a fast, precise solution, providing the foundation for the recommendation systems discussed in the current chapter.</p>&#13;
</div></section>&#13;
</div></section></body></html>