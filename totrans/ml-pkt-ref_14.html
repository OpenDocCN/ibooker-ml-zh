<html><head></head><body><section data-pdf-bookmark="Chapter 14. Regression" data-type="chapter" epub:type="chapter"><div class="chapter" id="idm46066890687896">&#13;
<h1><span class="label">Chapter 14. </span>Regression</h1>&#13;
&#13;
&#13;
<p><a data-primary="regression" data-type="indexterm" id="ix_ch14-asciidoc0"/>Regression <a data-primary="supervised learning" data-secondary="regression and" data-type="indexterm" id="idm46066889454376"/>is a supervised machine learning process. It is similar to&#13;
classification, but rather than predicting a label, we try to predict&#13;
a continuous value. If you are trying to predict a number, then use regression.</p>&#13;
&#13;
<p>It turns out that sklearn supports many of the&#13;
same classification models for regression problems. In fact, the&#13;
API is the same, calling <code>.fit</code>, <code>.score</code>, and <code>.predict</code>. This is also true for the next-generation boosting libraries, XGBoost and LightGBM.</p>&#13;
&#13;
<p>Though there are similarities with the classification models and hyperparameters,&#13;
the evaluation metrics are different for regression. This chapter will review many&#13;
of the types of regression models. We will use the <a href="https://oreil.ly/b2bKQ">Boston housing dataset</a> to explore them.</p>&#13;
&#13;
<p>Here we load the data, create a split version for training and testing, and&#13;
create another split version with standardized data:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">pandas</code> <code class="kn">as</code> <code class="nn">pd</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_boston</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">model_selection</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">preprocessing</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">b</code> <code class="o">=</code> <code class="n">load_boston</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">bos_X</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">b</code><code class="o">.</code><code class="n">data</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="n">b</code><code class="o">.</code><code class="n">feature_names</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">bos_y</code> <code class="o">=</code> <code class="n">b</code><code class="o">.</code><code class="n">target</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">bos_X_train</code><code class="p">,</code> <code class="n">bos_X_test</code><code class="p">,</code> <code class="n">bos_y_train</code><code class="p">,</code> <code class="n">bos_y_test</code> <code class="o">=</code> <code class="n">model_selection</code><code class="o">.</code><code class="n">train_test_split</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">bos_X</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">bos_y</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">test_size</code><code class="o">=</code><code class="mf">0.3</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">bos_sX</code> <code class="o">=</code> <code class="n">preprocessing</code><code class="o">.</code><code class="n">StandardScaler</code><code class="p">()</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">bos_X</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">bos_sX_train</code><code class="p">,</code> <code class="n">bos_sX_test</code><code class="p">,</code> <code class="n">bos_sy_train</code><code class="p">,</code> <code class="n">bos_sy_test</code> <code class="o">=</code> <code class="n">model_selection</code><code class="o">.</code><code class="n">train_test_split</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">bos_sX</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">bos_y</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">test_size</code><code class="o">=</code><code class="mf">0.3</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code></pre>&#13;
&#13;
<p>Here are descriptions of the features of the housing dataset taken from the dataset:</p>&#13;
<dl>&#13;
<dt>CRIM</dt>&#13;
<dd>&#13;
<p>Per capita crime rate by town</p>&#13;
</dd>&#13;
<dt>ZN</dt>&#13;
<dd>&#13;
<p>Proportion of residential land zoned for lots over 25,000 square feet</p>&#13;
</dd>&#13;
<dt>INDUS</dt>&#13;
<dd>&#13;
<p>Proportion of nonretail business acres per town</p>&#13;
</dd>&#13;
<dt>CHAS</dt>&#13;
<dd>&#13;
<p>Charles River dummy variable (1 if tract bounds river; 0&#13;
otherwise)</p>&#13;
</dd>&#13;
<dt>NOX</dt>&#13;
<dd>&#13;
<p>Nitric oxides concentration (parts per 10 million)</p>&#13;
</dd>&#13;
<dt>RM</dt>&#13;
<dd>&#13;
<p>Average number of rooms per dwelling</p>&#13;
</dd>&#13;
<dt>AGE</dt>&#13;
<dd>&#13;
<p>Proportion of owner-occupied units built prior to 1940</p>&#13;
</dd>&#13;
<dt>DIS</dt>&#13;
<dd>&#13;
<p>Weighted distances to five Boston employment centers</p>&#13;
</dd>&#13;
<dt>RAD</dt>&#13;
<dd>&#13;
<p>Index of accessibility to radial highways</p>&#13;
</dd>&#13;
<dt>TAX</dt>&#13;
<dd>&#13;
<p>Full-value property tax rate per $10,000</p>&#13;
</dd>&#13;
<dt>PTRATIO</dt>&#13;
<dd>&#13;
<p>Pupil-teacher ratio by town</p>&#13;
</dd>&#13;
<dt>B</dt>&#13;
<dd>&#13;
<p>1000(Bk - 0.63)^2, where Bk is the proportion of black people by town (this dataset is from 1978)</p>&#13;
</dd>&#13;
<dt>LSTAT</dt>&#13;
<dd>&#13;
<p>Percent lower status of the population</p>&#13;
</dd>&#13;
<dt>MEDV</dt>&#13;
<dd>&#13;
<p>Median value of owner-occupied homes in increments of $1000</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Baseline Model" data-type="sect1"><div class="sect1" id="idm46066889409032">&#13;
<h1>Baseline Model</h1>&#13;
&#13;
<p><a data-primary="baseline regression model" data-type="indexterm" id="idm46066889219304"/><a data-primary="regression" data-secondary="baseline model" data-type="indexterm" id="idm46066889218632"/>A baseline regression model will give us something to compare our other&#13;
models to. <a data-primary="coefficient of determination" data-type="indexterm" id="idm46066889217560"/><a data-primary="sklearn" data-secondary="coefficient of determination" data-type="indexterm" id="idm46066889216920"/>In sklearn, the default result of the <code>.score</code> method&#13;
is the <em>coefficient of determination</em> (r² or R²). This number explains the percent&#13;
of variation of the input data that the prediction captures.&#13;
The value is typically between 0 and 1, but it can be negative in the case&#13;
of particulary bad models.</p>&#13;
&#13;
<p>The default strategy of the <code>DummyRegressor</code> is to predict the mean value of the training set. We can see that this model does not perform very well:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.dummy</code> <code class="kn">import</code> <code class="n">DummyRegressor</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">dr</code> <code class="o">=</code> <code class="n">DummyRegressor</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">dr</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">bos_X_train</code><code class="p">,</code> <code class="n">bos_y_train</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">dr</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">bos_X_test</code><code class="p">,</code> <code class="n">bos_y_test</code><code class="p">)</code>&#13;
<code class="go">-0.03469753992352409</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Linear Regression" data-type="sect1"><div class="sect1" id="idm46066889211704">&#13;
<h1>Linear Regression</h1>&#13;
&#13;
<p><a data-primary="linear regression" data-type="indexterm" id="ix_ch14-asciidoc1"/><a data-primary="regression" data-secondary="linear" data-type="indexterm" id="ix_ch14-asciidoc2"/><a data-primary="simple linear regression" data-type="indexterm" id="ix_ch14-asciidoc3"/>Simple linear regression is taught in math and beginning statistics courses.&#13;
It tries to fit a form of the formula y = mx + b while minimizing the square of the&#13;
errors. When solved, we have an intercept and coefficient. The intercept gives a&#13;
base value for a prediction modified by adding the product of the coefficient&#13;
and the input.</p>&#13;
&#13;
<p>This form can be generalized to higher dimensions. In that case each feature&#13;
has a coefficient. The larger the absolute value of the coefficient, the more impact the feature has on the target.</p>&#13;
&#13;
<p>This model assumes that the prediction is a linear combination of the inputs. For some datasets, this is not flexible enough. Complexity can be added by transforming the features (the sklearn <code>preprocessing.PolynomialFeatures</code> transformer can create polynomial combinations of the features). If&#13;
this leads to overfitting, ridge and lasso regression may be used to&#13;
regularize the estimator.</p>&#13;
&#13;
<p><a data-primary="heteroscedasticity" data-secondary="defined" data-type="indexterm" id="idm46066889118248"/>This model is also susceptible to <em>heteroscedasticity</em>. This is the idea&#13;
that as the input values change in size, the error of the prediction (or&#13;
the residuals) often changes as well. If you plot the input against the&#13;
residuals, you will see a fan&#13;
or cone shape. We will see examples of that later.</p>&#13;
&#13;
<p><a data-primary="multicollinearity" data-type="indexterm" id="idm46066889116216"/>Another issue to be aware of is <em>multicollinearity</em>.&#13;
If columns have high correlation, it can hinder interpretation&#13;
of the coefficients. This usually does not impact the model, only&#13;
coefficient meaning.</p>&#13;
&#13;
<p>A linear regression model has the following properties:</p>&#13;
<dl>&#13;
<dt>Runtime efficiency</dt>&#13;
<dd>&#13;
<p>Use <code>n_jobs</code> to speed up performance.</p>&#13;
</dd>&#13;
<dt>Preprocess data</dt>&#13;
<dd>&#13;
<p>Standardize data before training the model.</p>&#13;
</dd>&#13;
<dt>Prevent overfitting</dt>&#13;
<dd>&#13;
<p>You can simplify the model by not using or adding polynomial features.</p>&#13;
</dd>&#13;
<dt>Interpret results</dt>&#13;
<dd>&#13;
<p>Can interpret results as weights for feature contribution, but assumes normality and independence of features. You might want to remove colinear features to improve interpretability. R² will tell you how much of the total variance of the outcome is explained by the model.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Here is a sample run with the default data:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">LinearRegression</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lr</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lr</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">bos_X_train</code><code class="p">,</code> <code class="n">bos_y_train</code><code class="p">)</code>&#13;
<code class="go">LinearRegression(copy_X=True, fit_intercept=True,</code>&#13;
<code class="go">  n_jobs=1, normalize=False)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lr</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">bos_X_test</code><code class="p">,</code> <code class="n">bos_y_test</code><code class="p">)</code>&#13;
<code class="go">0.7109203586326287</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lr</code><code class="o">.</code><code class="n">coef_</code>&#13;
<code class="go">array([-1.32774155e-01,  3.57812335e-02,</code>&#13;
<code class="go">  4.99454423e-02,  3.12127706e+00,</code>&#13;
<code class="go">  -1.54698463e+01,  4.04872721e+00,</code>&#13;
<code class="go">  -1.07515901e-02, -1.38699758e+00,</code>&#13;
<code class="go">  2.42353741e-01, -8.69095363e-03,</code>&#13;
<code class="go">  -9.11917342e-01,  1.19435253e-02,</code>&#13;
<code class="go">  -5.48080157e-01])</code></pre>&#13;
&#13;
<p>Instance parameters:</p>&#13;
<dl>&#13;
<dt><code>n_jobs=None</code></dt>&#13;
<dd>&#13;
<p>Number of CPUs to use. <code>-1</code> is all.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Attributes after fitting:</p>&#13;
<dl>&#13;
<dt><code>coef_</code></dt>&#13;
<dd>&#13;
<p>Linear regression coefficients</p>&#13;
</dd>&#13;
<dt><code>intercept_</code></dt>&#13;
<dd>&#13;
<p>Intercept of the linear model</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>The <code>.intercept_</code> value is the expected mean value. You can see how scaling the data affects the coefficients. The sign of the coefficients explains the direction of the relation between the feature and the target. A positive sign indicates that as the feature increases, the label increases. A negative sign indicates that as the feature increases, the label decreases. The larger the absolute value of the coefficient, the more impact it has:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">lr2</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lr2</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">bos_sX_train</code><code class="p">,</code> <code class="n">bos_sy_train</code><code class="p">)</code>&#13;
<code class="go">LinearRegression(copy_X=True, fit_intercept=True,</code>&#13;
<code class="go">  n_jobs=1, normalize=False)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lr2</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">bos_sX_test</code><code class="p">,</code> <code class="n">bos_sy_test</code><code class="p">)</code>&#13;
<code class="go">0.7109203586326278</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lr2</code><code class="o">.</code><code class="n">intercept_</code>&#13;
<code class="go">22.50945471291039</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lr2</code><code class="o">.</code><code class="n">coef_</code>&#13;
<code class="go">array([-1.14030209,  0.83368112,  0.34230461,</code>&#13;
<code class="go">  0.792002, -1.7908376, 2.84189278, -0.30234582,</code>&#13;
<code class="go">  -2.91772744,  2.10815064, -1.46330017,</code>&#13;
<code class="go">  -1.97229956,  1.08930453, -3.91000474])</code></pre>&#13;
&#13;
<p><a data-primary="Yellowbrick" data-secondary="coefficient visualization" data-type="indexterm" id="idm46066889021000"/>You can use Yellowbrick to visualize coefficients (see <a data-type="xref" href="#id45">Figure 14-1</a>). Because the scaled Boston&#13;
data is a numpy array rather than a pandas DataFrame, we need to pass the&#13;
<code>labels</code> parameter if we want to use the column names:<a data-startref="ix_ch14-asciidoc3" data-type="indexterm" id="idm46066888974584"/><a data-startref="ix_ch14-asciidoc2" data-type="indexterm" id="idm46066888973880"/><a data-startref="ix_ch14-asciidoc1" data-type="indexterm" id="idm46066888973208"/></p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">yellowbrick.features</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">FeatureImportances</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fi_viz</code> <code class="o">=</code> <code class="n">FeatureImportances</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">lr2</code><code class="p">,</code> <code class="n">labels</code><code class="o">=</code><code class="n">bos_X</code><code class="o">.</code><code class="n">columns</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fi_viz</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">bos_sX</code><code class="p">,</code> <code class="n">bos_sy</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fi_viz</code><code class="o">.</code><code class="n">poof</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="s">"images/mlpr_1401.png"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">bbox_inches</code><code class="o">=</code><code class="s">"tight"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id45">&#13;
<img alt="Feature importance. This indicates that RM (number of rooms) increases the price, age doesn't really matter, and LSTAT (percent of low status in population) lowers the price." src="assets/mlpr_1401.png"/>&#13;
<h6><span class="label">Figure 14-1. </span>Feature importance. This indicates that RM (number of rooms) increases the price, age doesn’t really matter, and LSTAT (percent of low status in population) lowers the price.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="SVMs" data-type="sect1"><div class="sect1" id="idm46066889124744">&#13;
<h1>SVMs</h1>&#13;
&#13;
<p><a data-primary="regression" data-secondary="SVMs and" data-type="indexterm" id="ix_ch14-asciidoc4"/><a data-primary="support vector machines (SVMs)" data-type="indexterm" id="ix_ch14-asciidoc5"/>Support vector machines can perform regression as well.</p>&#13;
&#13;
<p>SVMs have the following properties:</p>&#13;
<dl>&#13;
<dt>Runtime efficiency</dt>&#13;
<dd>&#13;
<p>The scikit-learn implementation is O(n⁴), so it can be hard to scale to large sizes. Using a linear kernel or the <code>LinearSVR</code> model can improve the runtime performance at perhaps the cost of accuracy. Upping the <code>cache_size</code> parameter can bring that down to O(n³).</p>&#13;
</dd>&#13;
<dt>Preprocess data</dt>&#13;
<dd>&#13;
<p>The algorithm is not scale invariant, so standardizing the data is highly recommended.</p>&#13;
</dd>&#13;
<dt>Prevent overfitting</dt>&#13;
<dd>&#13;
<p>The <code>C</code> (penalty parameter) controls regularization. A smaller value allows for a smaller margin in the hyperplane. A higher value for <code>gamma</code> will tend to overfit the training data. The <code>LinearSVR</code> model supports a <code>loss</code> and <code>penalty</code> parameter for regularization. The <code>epsilon</code> parameter can be raised (with 0 you should expect overfitting).</p>&#13;
</dd>&#13;
<dt>Interpret results</dt>&#13;
<dd>&#13;
<p>Inspect <code>.support_vectors_</code>, though these are hard to interpret. With linear kernels, you can inspect <code>.coef_</code>.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Here is an example of using the library:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="kn">import</code> <code class="n">SVR</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">svr</code> <code class="o">=</code> <code class="n">SVR</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">svr</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">bos_sX_train</code><code class="p">,</code> <code class="n">bos_sy_train</code><code class="p">)</code>&#13;
<code class="go">SVR(C=1.0, cache_size=200, coef0=0.0, degree=3,</code>&#13;
<code class="go">  epsilon=0.1, gamma='auto', kernel='rbf',</code>&#13;
<code class="go">  max_iter=-1, shrinking=True, tol=0.001,</code>&#13;
<code class="go">  verbose=False)</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">svr</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">bos_sX_test</code><code class="p">,</code> <code class="n">bos_sy_test</code><code class="p">)</code>&#13;
<code class="go">0.6555356362002485</code></pre>&#13;
&#13;
<p>Instance parameters:</p>&#13;
<dl>&#13;
<dt><code>C=1.0</code></dt>&#13;
<dd>&#13;
<p>The penalty parameter. The smaller the value, the tighter the decision boundary (more overfitting).</p>&#13;
</dd>&#13;
<dt><code>cache_size=200</code></dt>&#13;
<dd>&#13;
<p>Cache size (MB). Bumping this up can improve training time on large datasets.</p>&#13;
</dd>&#13;
<dt><code>coef0=0.0</code></dt>&#13;
<dd>&#13;
<p>Independent term for poly and sigmoid kernels.</p>&#13;
</dd>&#13;
<dt><code>epsilon=0.1</code></dt>&#13;
<dd>&#13;
<p>Defines a margin of tolerance where no penalty is given to errors. Should be smaller for larger datasets.</p>&#13;
</dd>&#13;
<dt><code>degree=3</code></dt>&#13;
<dd>&#13;
<p>Degree for polynomial kernel.</p>&#13;
</dd>&#13;
<dt><code>gamma='auto'</code></dt>&#13;
<dd>&#13;
<p>Kernel coefficient. Can be a number, <code>'scale'</code> (default in 0.22, 1 / (<code>num features</code> * <code>X.std()</code>) ), or <code>'auto'</code> (default prior, 1 / <code>num_features</code>). A lower value leads to overfitting the training data.</p>&#13;
</dd>&#13;
<dt><code>kernel='rbf'</code></dt>&#13;
<dd>&#13;
<p>Kernel type: <code>'linear'</code>, <code>'poly'</code>, <code>'rbf'</code> (default), <code>'sigmoid'</code>, <code>'precomputed'</code>, or a function.</p>&#13;
</dd>&#13;
<dt><code>max_iter=-1</code></dt>&#13;
<dd>&#13;
<p>Maximum number of iterations for solver. -1 for no limit.</p>&#13;
</dd>&#13;
<dt><code>probability=False</code></dt>&#13;
<dd>&#13;
<p>Enable probability estimation. Slows down training.</p>&#13;
</dd>&#13;
<dt><code>random_state=None</code></dt>&#13;
<dd>&#13;
<p>Random seed.</p>&#13;
</dd>&#13;
<dt><code>shrinking=True</code></dt>&#13;
<dd>&#13;
<p>Use shrinking heuristic.</p>&#13;
</dd>&#13;
<dt><code>tol=0.001</code></dt>&#13;
<dd>&#13;
<p>Stopping tolerance.</p>&#13;
</dd>&#13;
<dt><code>verbose=False</code></dt>&#13;
<dd>&#13;
<p>Verbosity.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Attributes after fitting:<a data-startref="ix_ch14-asciidoc5" data-type="indexterm" id="idm46066888817208"/><a data-startref="ix_ch14-asciidoc4" data-type="indexterm" id="idm46066888742296"/></p>&#13;
<dl>&#13;
<dt><code>support_</code></dt>&#13;
<dd>&#13;
<p>Support vector indices</p>&#13;
</dd>&#13;
<dt><code>support_vectors_</code></dt>&#13;
<dd>&#13;
<p>Support vectors</p>&#13;
</dd>&#13;
<dt><code>coef_</code></dt>&#13;
<dd>&#13;
<p>Coefficients (for linear) kernel</p>&#13;
</dd>&#13;
<dt><code>intercept_</code></dt>&#13;
<dd>&#13;
<p>Constant for decision function</p>&#13;
</dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="K-Nearest Neighbor" data-type="sect1"><div class="sect1" id="idm46066888867592">&#13;
<h1>K-Nearest Neighbor</h1>&#13;
&#13;
<p><a data-primary="k-nearest neighbor (KNN)" data-secondary="for regression" data-type="indexterm" id="ix_ch14-asciidoc6"/><a data-primary="regression" data-secondary="k-nearest neighbor" data-type="indexterm" id="ix_ch14-asciidoc7"/>The KNN model also supports regression by finding k neighbor targets&#13;
to the sample for which you want to predict. For regression, this model averages the targets&#13;
together to determine a prediction.</p>&#13;
&#13;
<p>Nearest neighbor models have the following properties:</p>&#13;
<dl>&#13;
<dt>Runtime efficiency</dt>&#13;
<dd>&#13;
<p>Training runtime is O(1), but there is a trade-off as the sample data needs to be stored. Testing runtime is O(Nd), where N is the number of training examples and d is dimensionality.</p>&#13;
</dd>&#13;
<dt>Preprocess data</dt>&#13;
<dd>&#13;
<p>Yes, distance-based calculations perform better when standardized.</p>&#13;
</dd>&#13;
<dt>Prevent overfitting</dt>&#13;
<dd>&#13;
<p>Raise <code>n_neighbors</code>. Change <code>p</code> for L1 or L2 metric.</p>&#13;
</dd>&#13;
<dt>Interpret results</dt>&#13;
<dd>&#13;
<p>Interpret the k-nearest neighbors to the sample (using the <code>.kneighbors</code> method). Those neighbors (if you can explain them) explain your result.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Here is an example of using the model:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.neighbors</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">KNeighborsRegressor</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">knr</code> <code class="o">=</code> <code class="n">KNeighborsRegressor</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">knr</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">bos_sX_train</code><code class="p">,</code> <code class="n">bos_sy_train</code><code class="p">)</code>&#13;
<code class="go">KNeighborsRegressor(algorithm='auto',</code>&#13;
<code class="go">  leaf_size=30, metric='minkowski',</code>&#13;
<code class="go">  metric_params=None, n_jobs=1, n_neighbors=5,</code>&#13;
<code class="go">  p=2, weights='uniform')</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">knr</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">bos_sX_test</code><code class="p">,</code> <code class="n">bos_sy_test</code><code class="p">)</code>&#13;
<code class="go">0.747112767457727</code></pre>&#13;
&#13;
<p>Attributes:</p>&#13;
<dl>&#13;
<dt><code>algorithm='auto'</code></dt>&#13;
<dd>&#13;
<p>Can be <code>'brute'</code>, <code>'ball_tree'</code>, or <code>'kd_tree'</code>.</p>&#13;
</dd>&#13;
<dt><code>leaf_size=30</code></dt>&#13;
<dd>&#13;
<p>Used for tree algorithms.</p>&#13;
</dd>&#13;
<dt><code>metric='minkowski'</code></dt>&#13;
<dd>&#13;
<p>Distance metric.</p>&#13;
</dd>&#13;
<dt><code>metric_params=None</code></dt>&#13;
<dd>&#13;
<p>Additional dictionary of parameters for custom metric function.</p>&#13;
</dd>&#13;
<dt><code>n_jobs=1</code></dt>&#13;
<dd>&#13;
<p>Number of CPUs.</p>&#13;
</dd>&#13;
<dt><code>n_neighbors=5</code></dt>&#13;
<dd>&#13;
<p>Number of neighbors.</p>&#13;
</dd>&#13;
<dt><code>p=2</code></dt>&#13;
<dd>&#13;
<p>Minkowski power parameter. 1 = manhattan (L1). 2 = euclidean (L2).</p>&#13;
</dd>&#13;
<dt><code>weights='uniform'</code></dt>&#13;
<dd>&#13;
<p>Can be <code>'distance'</code>, in which case, closer points have more influence.<a data-startref="ix_ch14-asciidoc7" data-type="indexterm" id="idm46066888653144"/><a data-startref="ix_ch14-asciidoc6" data-type="indexterm" id="idm46066888652440"/></p>&#13;
</dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Decision Tree" data-type="sect1"><div class="sect1" id="idm46066888666328">&#13;
<h1>Decision Tree</h1>&#13;
&#13;
<p><a data-primary="decision tree" data-secondary="for regression" data-type="indexterm" id="ix_ch14-asciidoc8"/><a data-primary="regression" data-secondary="decision tree" data-type="indexterm" id="ix_ch14-asciidoc9"/>Decision trees support classification and regression. At each level of the&#13;
tree, various splits on features are evaluated. The split that will produce the&#13;
lowest error (impurity) is chosen. The <code>criterion</code> parameter can be adjusted to determine the metric for impurity.</p>&#13;
&#13;
<p>Decision trees have the following properties:</p>&#13;
<dl>&#13;
<dt>Runtime efficiency</dt>&#13;
<dd>&#13;
<p>For creation, loop over each of the m features we have to sort all n samples: O(mn log n). For predicting, you walk the tree: O(height).</p>&#13;
</dd>&#13;
<dt>Preprocess data</dt>&#13;
<dd>&#13;
<p>Scaling not necessary. Need to get rid of missing values and convert to numeric.</p>&#13;
</dd>&#13;
<dt>Prevent overfitting</dt>&#13;
<dd>&#13;
<p>Set <code>max_depth</code> to a lower number, raise <span class="keep-together"><code>min_impurity_decrease</code></span>.</p>&#13;
</dd>&#13;
<dt>Interpret results</dt>&#13;
<dd>&#13;
<p>Can step through the tree of choices. Because there are steps, a tree is bad at dealing with linear relationships (a small change in the values of a feature can cause a completely different tree to be formed). The tree is also highly dependent on the training data. A small change can change the whole tree.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Here is an example using the scikit-learn library:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="kn">import</code> <code class="n">DecisionTreeRegressor</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">dtr</code> <code class="o">=</code> <code class="n">DecisionTreeRegressor</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">dtr</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">bos_X_train</code><code class="p">,</code> <code class="n">bos_y_train</code><code class="p">)</code>&#13;
<code class="go">DecisionTreeRegressor(criterion='mse',</code>&#13;
<code class="go">  max_depth=None, max_features=None,</code>&#13;
<code class="go">  max_leaf_nodes=None, min_impurity_decrease=0.0,</code>&#13;
<code class="go">  min_impurity_split=None, min_samples_leaf=1,</code>&#13;
<code class="go">  min_samples_split=2,</code>&#13;
<code class="go">  min_weight_fraction_leaf=0.0, presort=False,</code>&#13;
<code class="go">  random_state=42, splitter='best')</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">dtr</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">bos_X_test</code><code class="p">,</code> <code class="n">bos_y_test</code><code class="p">)</code>&#13;
<code class="go">0.8426751288675483</code></pre>&#13;
&#13;
<p>Instance parameters:</p>&#13;
<dl>&#13;
<dt><code>criterion='mse'</code></dt>&#13;
<dd>&#13;
<p>Splitting function. Default is mean squared error (L2 loss). <code>'friedman_mse'</code> or <code>'mae'</code> (L1 loss).</p>&#13;
</dd>&#13;
<dt><code>max_depth=None</code></dt>&#13;
<dd>&#13;
<p>Depth of tree. Default will build until leaves contain less than <code>min_samples_split</code>.</p>&#13;
</dd>&#13;
<dt><code>max_features=None</code></dt>&#13;
<dd>&#13;
<p>Number of features to examine for split. Default is all.</p>&#13;
</dd>&#13;
<dt><code>max_leaf_nodes=None</code></dt>&#13;
<dd>&#13;
<p>Limit number of leaves. Default is unlimited.</p>&#13;
</dd>&#13;
<dt><code>min_impurity_decrease=0.0</code></dt>&#13;
<dd>&#13;
<p>Split node if a split will decrease impurity &gt;= value.</p>&#13;
</dd>&#13;
<dt><code>min_impurity_split=None</code></dt>&#13;
<dd>&#13;
<p>Deprecated.</p>&#13;
</dd>&#13;
<dt><code>min_samples_leaf=1</code></dt>&#13;
<dd>&#13;
<p>Minimum number of samples at each leaf.</p>&#13;
</dd>&#13;
<dt><code>min_samples_split=2</code></dt>&#13;
<dd>&#13;
<p>Minimum number of samples required to split a node.</p>&#13;
</dd>&#13;
<dt><code>min_weight_fraction_leaf=0.0</code></dt>&#13;
<dd>&#13;
<p>Minimum sum of weights required for leaf nodes.</p>&#13;
</dd>&#13;
<dt><code>presort=False</code></dt>&#13;
<dd>&#13;
<p>May speed up training with small dataset or restricted depth if set to <code>True</code>.</p>&#13;
</dd>&#13;
<dt><code>random_state=None</code></dt>&#13;
<dd>&#13;
<p>Random seed.</p>&#13;
</dd>&#13;
<dt><code>splitter='best'</code></dt>&#13;
<dd>&#13;
<p>Use <code>'random'</code> or <code>'best'</code>.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Attributes after fitting:</p>&#13;
<dl>&#13;
<dt><code>feature_importances_</code></dt>&#13;
<dd>&#13;
<p>Array of Gini importance</p>&#13;
</dd>&#13;
<dt><code>max_features_</code></dt>&#13;
<dd>&#13;
<p>Computed value of <code>max_features</code></p>&#13;
</dd>&#13;
<dt><code>n_outputs_</code></dt>&#13;
<dd>&#13;
<p>Number of outputs</p>&#13;
</dd>&#13;
<dt><code>n_features_</code></dt>&#13;
<dd>&#13;
<p>Number of features</p>&#13;
</dd>&#13;
<dt><code>tree_</code></dt>&#13;
<dd>&#13;
<p>Underlying tree object</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>View the tree (see <a data-type="xref" href="#tree2">Figure 14-2</a>):</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">pydotplus</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">io</code> <code class="kn">import</code> <code class="n">StringIO</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="kn">import</code> <code class="n">export_graphviz</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">dot_data</code> <code class="o">=</code> <code class="n">StringIO</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">tree</code><code class="o">.</code><code class="n">export_graphviz</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">dtr</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">out_file</code><code class="o">=</code><code class="n">dot_data</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">feature_names</code><code class="o">=</code><code class="n">bos_X</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">filled</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">g</code> <code class="o">=</code> <code class="n">pydotplus</code><code class="o">.</code><code class="n">graph_from_dot_data</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">dot_data</code><code class="o">.</code><code class="n">getvalue</code><code class="p">()</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">g</code><code class="o">.</code><code class="n">write_png</code><code class="p">(</code><code class="s">"images/mlpr_1402.png"</code><code class="p">)</code></pre>&#13;
&#13;
<p>For Jupyter, use:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="go">from IPython.display import Image</code>&#13;
<code class="go">Image(g.create_png())</code></pre>&#13;
<div class="landscape">&#13;
&#13;
<figure><div class="figure" id="tree2">&#13;
<img alt="Decision tree." src="assets/mlpr_1402.png"/>&#13;
<h6><span class="label">Figure 14-2. </span>Decision tree.</h6>&#13;
</div></figure>&#13;
</div>&#13;
&#13;
<p>This plot was a little wide. On a computer you can zoom in on portions of it. You can also limit the depth of the chart (see <a data-type="xref" href="#tree3">Figure 14-3</a>). (It turns out that the most important features are typically near the top of the tree.) We will use the <code>max_depth</code> parameter to do this:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">dot_data</code> <code class="o">=</code> <code class="n">StringIO</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">tree</code><code class="o">.</code><code class="n">export_graphviz</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">dtr</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">max_depth</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">out_file</code><code class="o">=</code><code class="n">dot_data</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">feature_names</code><code class="o">=</code><code class="n">bos_X</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">filled</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">g</code> <code class="o">=</code> <code class="n">pydotplus</code><code class="o">.</code><code class="n">graph_from_dot_data</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">dot_data</code><code class="o">.</code><code class="n">getvalue</code><code class="p">()</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">g</code><code class="o">.</code><code class="n">write_png</code><code class="p">(</code><code class="s">"images/mlpr_1403.png"</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="tree3">&#13;
<img alt="The first two layers of a decision tree." src="assets/mlpr_1403.png"/>&#13;
<h6><span class="label">Figure 14-3. </span>The first two layers of a decision tree.</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-primary="dtreeviz" data-type="indexterm" id="idm46066888321528"/>We can also use the dtreeviz package to view a scatter plot at each of the nodes of the tree (see <a data-type="xref" href="#treedtviz2">Figure 14-4</a>). We will use a tree limited to a depth of two so we can see the details:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">dtr3</code> <code class="o">=</code> <code class="n">DecisionTreeRegressor</code><code class="p">(</code><code class="n">max_depth</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">dtr3</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">bos_X_train</code><code class="p">,</code> <code class="n">bos_y_train</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">viz</code> <code class="o">=</code> <code class="n">dtreeviz</code><code class="o">.</code><code class="n">trees</code><code class="o">.</code><code class="n">dtreeviz</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">dtr3</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">bos_X</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">bos_y</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">target_name</code><code class="o">=</code><code class="s">"price"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">feature_names</code><code class="o">=</code><code class="n">bos_X</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">viz</code></pre>&#13;
&#13;
<figure><div class="figure" id="treedtviz2">&#13;
<img alt="Regression with dtviz." src="assets/mlpr_1404.png"/>&#13;
<h6><span class="label">Figure 14-4. </span>Regression with dtviz.</h6>&#13;
</div></figure>&#13;
&#13;
<p>Feature importance:<a data-startref="ix_ch14-asciidoc9" data-type="indexterm" id="idm46066888227304"/><a data-startref="ix_ch14-asciidoc8" data-type="indexterm" id="idm46066888226568"/></p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">col</code><code class="p">,</code> <code class="n">val</code> <code class="ow">in</code> <code class="nb">sorted</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="nb">zip</code><code class="p">(</code>&#13;
<code class="gp">... </code>        <code class="n">bos_X</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code> <code class="n">dtr</code><code class="o">.</code><code class="n">feature_importances_</code>&#13;
<code class="gp">... </code>    <code class="p">),</code>&#13;
<code class="gp">... </code>    <code class="n">key</code><code class="o">=</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code>&#13;
<code class="gp">... </code>    <code class="n">reverse</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)[:</code><code class="mi">5</code><code class="p">]:</code>&#13;
<code class="gp">... </code>    <code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s">"{col:10}{val:10.3f}"</code><code class="p">)</code>&#13;
<code class="go">RM             0.574</code>&#13;
<code class="go">LSTAT          0.191</code>&#13;
<code class="go">DIS            0.110</code>&#13;
<code class="go">CRIM           0.061</code>&#13;
<code class="go">RAD            0.018</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Random Forest" data-type="sect1"><div class="sect1" id="idm46066888204808">&#13;
<h1>Random Forest</h1>&#13;
&#13;
<p><a data-primary="random forest" data-secondary="for regression" data-type="indexterm" id="ix_ch14-asciidoc10"/><a data-primary="regression" data-secondary="random forest" data-type="indexterm" id="ix_ch14-asciidoc11"/>Decision trees are good because they are explainable, but they have a&#13;
tendency to overfit. A random forest trades some of the explainability&#13;
for a model that tends to generalize better. This model can also be used for regression.</p>&#13;
&#13;
<p>Random forests have the following properties:</p>&#13;
<dl>&#13;
<dt>Runtime efficiency</dt>&#13;
<dd>&#13;
<p>Need to create j random trees. This can be done in parallel using <code>n_jobs</code>. Complexity for each tree is O(mn log n), where n is the number of samples and m is the number of features. For creation, loop over each of the m features, and sort all n samples: O(mn log n). For predicting, you walk the tree: O(height).</p>&#13;
</dd>&#13;
<dt>Preprocess data</dt>&#13;
<dd>&#13;
<p>Not necessary as long as the input is numeric and not missing values.</p>&#13;
</dd>&#13;
<dt>Prevent overfitting</dt>&#13;
<dd>&#13;
<p>Add more trees (<code>n_estimators</code>). Use lower <code>max_depth</code>.</p>&#13;
</dd>&#13;
<dt>Interpret results</dt>&#13;
<dd>&#13;
<p>Supports feature importance, but we don’t have a single decision tree that we can walk through. Can inspect single trees from the ensemble.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Here is an example of using the model:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">RandomForestRegressor</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rfr</code> <code class="o">=</code> <code class="n">RandomForestRegressor</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">,</code> <code class="n">n_estimators</code><code class="o">=</code><code class="mi">100</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rfr</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">bos_X_train</code><code class="p">,</code> <code class="n">bos_y_train</code><code class="p">)</code>&#13;
<code class="go">RandomForestRegressor(bootstrap=True,</code>&#13;
<code class="go">  criterion='mse', max_depth=None,</code>&#13;
<code class="go">  max_features='auto', max_leaf_nodes=None,</code>&#13;
<code class="go">  min_impurity_decrease=0.0,</code>&#13;
<code class="go">  min_impurity_split=None,_samples_leaf=1,</code>&#13;
<code class="go">  min_samples_split=2,</code>&#13;
<code class="go">  min_weight_fraction_leaf=0.0,</code>&#13;
<code class="go">  n_estimators=100, n_jobs=1,</code>&#13;
<code class="go">  oob_score=False, random_state=42,</code>&#13;
<code class="go">  verbose=0, warm_start=False)</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rfr</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">bos_X_test</code><code class="p">,</code> <code class="n">bos_y_test</code><code class="p">)</code>&#13;
<code class="go">0.8641887615545837</code></pre>&#13;
&#13;
<p>Instance parameters (these options mirror the decision tree):</p>&#13;
<dl>&#13;
<dt><code>bootstrap=True</code></dt>&#13;
<dd>&#13;
<p>Bootstrap when building trees.</p>&#13;
</dd>&#13;
<dt><code>criterion='mse'</code></dt>&#13;
<dd>&#13;
<p>Splitting function, <code>'mae'</code>.</p>&#13;
</dd>&#13;
<dt><code>max_depth=None</code></dt>&#13;
<dd>&#13;
<p>Depth of tree. Default will build until leaves contain less than <code>min_samples_split</code>.</p>&#13;
</dd>&#13;
<dt><code>max_features='auto'</code></dt>&#13;
<dd>&#13;
<p>Number of features to examine for split. Default is all.</p>&#13;
</dd>&#13;
<dt><code>max_leaf_nodes=None</code></dt>&#13;
<dd>&#13;
<p>Limit number of leaves. Default is unlimited.</p>&#13;
</dd>&#13;
<dt><code>min_impurity_decrease=0.0</code></dt>&#13;
<dd>&#13;
<p>Split node if a split will decrease impurity by this value or more.</p>&#13;
</dd>&#13;
<dt><code>min_impurity_split=None</code></dt>&#13;
<dd>&#13;
<p>Deprecated.</p>&#13;
</dd>&#13;
<dt><code>min_samples_leaf=1</code></dt>&#13;
<dd>&#13;
<p>Minimum number of samples at each leaf.</p>&#13;
</dd>&#13;
<dt><code>min_samples_split=2</code></dt>&#13;
<dd>&#13;
<p>Minimum number of samples required to split a node.</p>&#13;
</dd>&#13;
<dt><code>min_weight_fraction_leaf=0.0</code> </dt>&#13;
<dd>&#13;
<p>Minimum sum total of weights required for leaf nodes.</p>&#13;
</dd>&#13;
<dt><code>n_estimators=10</code></dt>&#13;
<dd>&#13;
<p>Number of trees in the forest.</p>&#13;
</dd>&#13;
<dt><code>n_jobs=None</code></dt>&#13;
<dd>&#13;
<p>Number of jobs for fitting and predicting. (<code>None</code> means 1.)</p>&#13;
</dd>&#13;
<dt><code>oob_score=False</code></dt>&#13;
<dd>&#13;
<p>Whether to use OOB samples to estimate score on unseen data.</p>&#13;
</dd>&#13;
<dt><code>random_state=None</code></dt>&#13;
<dd>&#13;
<p>Random seed.</p>&#13;
</dd>&#13;
<dt><code>verbose=0</code></dt>&#13;
<dd>&#13;
<p>Verbosity.</p>&#13;
</dd>&#13;
<dt><code>warm_start=False</code></dt>&#13;
<dd>&#13;
<p>Fit a new forest or use existing one.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Attributes after fitting:</p>&#13;
<dl>&#13;
<dt><code>estimators_</code></dt>&#13;
<dd>&#13;
<p>Collection of trees</p>&#13;
</dd>&#13;
<dt><code>feature_importances_</code></dt>&#13;
<dd>&#13;
<p>Array of Gini importance</p>&#13;
</dd>&#13;
<dt><code>n_classes_</code></dt>&#13;
<dd>&#13;
<p>Number of classes</p>&#13;
</dd>&#13;
<dt><code>n_features_</code></dt>&#13;
<dd>&#13;
<p>Number of features</p>&#13;
</dd>&#13;
<dt><code>oob_score_</code></dt>&#13;
<dd>&#13;
<p>Score of the training dataset using OOB estimate</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Feature importance:<a data-startref="ix_ch14-asciidoc11" data-type="indexterm" id="idm46066888000120"/><a data-startref="ix_ch14-asciidoc10" data-type="indexterm" id="idm46066887999384"/></p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">col</code><code class="p">,</code> <code class="n">val</code> <code class="ow">in</code> <code class="nb">sorted</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="nb">zip</code><code class="p">(</code>&#13;
<code class="gp">... </code>        <code class="n">bos_X</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code> <code class="n">rfr</code><code class="o">.</code><code class="n">feature_importances_</code>&#13;
<code class="gp">... </code>    <code class="p">),</code>&#13;
<code class="gp">... </code>    <code class="n">key</code><code class="o">=</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code>&#13;
<code class="gp">... </code>    <code class="n">reverse</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)[:</code><code class="mi">5</code><code class="p">]:</code>&#13;
<code class="gp">... </code>    <code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s">"{col:10}{val:10.3f}"</code><code class="p">)</code>&#13;
<code class="go">RM             0.505</code>&#13;
<code class="go">LSTAT          0.283</code>&#13;
<code class="go">DIS            0.115</code>&#13;
<code class="go">CRIM           0.029</code>&#13;
<code class="go">PTRATIO        0.016</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="XGBoost Regression" data-type="sect1"><div class="sect1" id="idm46066888650728">&#13;
<h1>XGBoost Regression</h1>&#13;
&#13;
<p><a data-primary="regression" data-secondary="XGBoost for" data-type="indexterm" id="ix_ch14-asciidoc12"/><a data-primary="XGBoost" data-secondary="for regression" data-type="indexterm" id="ix_ch14-asciidoc13"/>The XGBoost library also supports regression. It builds a simple decision tree, then “boosts” it by adding subsequent trees. Each tree tries to correct the residuals of the previous output. In practice, this works quite well on structured data.</p>&#13;
&#13;
<p>It has the following properties:</p>&#13;
<dl>&#13;
<dt>Runtime efficiency</dt>&#13;
<dd>&#13;
<p>XGBoost is parallelizeable. Use the <code>n_jobs</code> option to indicate the number of CPUs. Use GPU for even better <span class="keep-together">performance.</span></p>&#13;
</dd>&#13;
<dt>Preprocess data</dt>&#13;
<dd>&#13;
<p>No scaling necessary with tree models. Need to encode categorical data. Supports missing data!</p>&#13;
</dd>&#13;
<dt>Prevent overfitting</dt>&#13;
<dd>&#13;
<p>The <code>early_stopping_rounds=N</code> parameter can be set to stop training if there is no improvement after N rounds. L1 and L2 regularization are controlled by <code>reg_alpha</code> and <code>reg_lambda</code>, respectively. Higher numbers mean more conservative.</p>&#13;
</dd>&#13;
<dt>Interpret results</dt>&#13;
<dd>&#13;
<p>Has feature importance.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Here is an example using the library:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">xgr</code> <code class="o">=</code> <code class="n">xgb</code><code class="o">.</code><code class="n">XGBRegressor</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">xgr</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">bos_X_train</code><code class="p">,</code> <code class="n">bos_y_train</code><code class="p">)</code>&#13;
<code class="go">XGBRegressor(base_score=0.5, booster='gbtree',</code>&#13;
<code class="go">  colsample_bylevel=1, colsample_bytree=1,</code>&#13;
<code class="go">  gamma=0, learning_rate=0.1, max_delta_step=0,</code>&#13;
<code class="go">  max_depth=3, min_child_weight=1, missing=None,</code>&#13;
<code class="go">  n_estimators=100, n_jobs=1, nthread=None,</code>&#13;
<code class="go">  objective='reg:linear', random_state=42,</code>&#13;
<code class="go">  reg_alpha=0, reg_lambda=1, scale_pos_weight=1,</code>&#13;
<code class="go">  seed=None, silent=True, subsample=1)</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">xgr</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">bos_X_test</code><code class="p">,</code> <code class="n">bos_y_test</code><code class="p">)</code>&#13;
<code class="go">0.871679473122472</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">xgr</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">bos_X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[[</code><code class="mi">0</code><code class="p">]])</code>&#13;
<code class="go">array([27.013563], dtype=float32)</code></pre>&#13;
&#13;
<p>Instance parameters:</p>&#13;
<dl>&#13;
<dt><code>max_depth=3</code></dt>&#13;
<dd>&#13;
<p>Maximum depth.</p>&#13;
</dd>&#13;
<dt><code>learning_rate=0.1</code></dt>&#13;
<dd>&#13;
<p>Learning rate (eta) for boosting (between 0 and 1). After each boost step, the newly added weights are scaled by this factor. The lower the value, the more conservative, but will also need more trees to converge. In the call to <code>.train</code>, you can pass a <code>learning_rates</code> parameter, which is a list of rates at each round (i.e., <code>[.1]*100 + [.05]*100</code>).</p>&#13;
</dd>&#13;
<dt><code>n_estimators=100</code></dt>&#13;
<dd>&#13;
<p>Number of rounds or boosted trees.</p>&#13;
</dd>&#13;
<dt><code>silent=True</code></dt>&#13;
<dd>&#13;
<p>Whether to print messages while running boosting.</p>&#13;
</dd>&#13;
<dt><code>objective="reg:linear"</code></dt>&#13;
<dd>&#13;
<p>Learning task or callable for classification.</p>&#13;
</dd>&#13;
<dt><code>booster="gbtree"</code></dt>&#13;
<dd>&#13;
<p>Can be <code>'gbtree'</code>, <code>'gblinear'</code>, or <code>'dart'</code>. The <code>'dart'</code> option adds dropout (drops random trees to prevent overfitting). The <code>'gblinear'</code> option creates a regularized linear model (read not a tree but similar to lasso regression).</p>&#13;
</dd>&#13;
<dt><code>nthread=None</code></dt>&#13;
<dd>&#13;
<p>Deprecated.</p>&#13;
</dd>&#13;
<dt><code>n_jobs=1</code></dt>&#13;
<dd>&#13;
<p>Number of threads to use.</p>&#13;
</dd>&#13;
<dt><code>gamma=0</code></dt>&#13;
<dd>&#13;
<p>Minimum loss reduction needed to further split a leaf.</p>&#13;
</dd>&#13;
<dt><code>min_child_weight=1</code></dt>&#13;
<dd>&#13;
<p>Minimum value for sum of hessian for a child.</p>&#13;
</dd>&#13;
<dt><code>max_delta_step=0</code></dt>&#13;
<dd>&#13;
<p>Make update more conservative. Set 1 to 10 for imbalanced classes.</p>&#13;
</dd>&#13;
<dt><code>subsample=1</code></dt>&#13;
<dd>&#13;
<p>Fraction of samples to use for next boosting round.</p>&#13;
</dd>&#13;
<dt><code>colsample_bytree=1</code></dt>&#13;
<dd>&#13;
<p>Fraction of columns to use for boosting round.</p>&#13;
</dd>&#13;
<dt><code>colsample_bylevel=1</code></dt>&#13;
<dd>&#13;
<p>Fraction of columns to use for level in tree.</p>&#13;
</dd>&#13;
<dt><code>colsample_bynode=1</code></dt>&#13;
<dd>&#13;
<p>Fraction of columns to use for split (node in tree).</p>&#13;
</dd>&#13;
<dt><code>reg_alpha=0</code></dt>&#13;
<dd>&#13;
<p>L1 regularization (mean of weights). Increase to be more conservative.</p>&#13;
</dd>&#13;
<dt><code>reg_lambda=1</code></dt>&#13;
<dd>&#13;
<p>L2 regularization (root of squared weights). Increase to be more conservative.</p>&#13;
</dd>&#13;
<dt><code>base_score=.5</code></dt>&#13;
<dd>&#13;
<p>Initial prediction.</p>&#13;
</dd>&#13;
<dt><code>seed=None</code></dt>&#13;
<dd>&#13;
<p>Deprecated.</p>&#13;
</dd>&#13;
<dt><code>random_state=0</code></dt>&#13;
<dd>&#13;
<p>Random seed.</p>&#13;
</dd>&#13;
<dt><code>missing=None</code></dt>&#13;
<dd>&#13;
<p>Value to interpret for missing. <code>None</code> means <code>np.nan</code>.</p>&#13;
</dd>&#13;
<dt><code>importance_type='gain'</code></dt>&#13;
<dd>&#13;
<p>The feature importance type: <code>'gain'</code>, <code>'weight'</code>, <code>'cover'</code>, <code>'total_gain'</code>, or <code>'total_cover'</code>.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Attributes:</p>&#13;
<dl>&#13;
<dt><code>coef_</code></dt>&#13;
<dd>&#13;
<p>Coefficients for gblinear learners (<code>booster = 'gblinear'</code>)</p>&#13;
</dd>&#13;
<dt><code>intercept_</code></dt>&#13;
<dd>&#13;
<p>Intercept for gblinear learners</p>&#13;
</dd>&#13;
<dt><code>feature_importances_</code></dt>&#13;
<dd>&#13;
<p>Feature importances for gbtree learners</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p><a data-primary="feature importance" data-secondary="XGBoost" data-type="indexterm" id="idm46066887767112"/>Feature importance is the average gain across all the nodes where the feature is used:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">col</code><code class="p">,</code> <code class="n">val</code> <code class="ow">in</code> <code class="nb">sorted</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="nb">zip</code><code class="p">(</code>&#13;
<code class="gp">... </code>        <code class="n">bos_X</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code> <code class="n">xgr</code><code class="o">.</code><code class="n">feature_importances_</code>&#13;
<code class="gp">... </code>    <code class="p">),</code>&#13;
<code class="gp">... </code>    <code class="n">key</code><code class="o">=</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code>&#13;
<code class="gp">... </code>    <code class="n">reverse</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)[:</code><code class="mi">5</code><code class="p">]:</code>&#13;
<code class="gp">... </code>    <code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s">"{col:10}{val:10.3f}"</code><code class="p">)</code>&#13;
<code class="go">DIS            0.187</code>&#13;
<code class="go">CRIM           0.137</code>&#13;
<code class="go">RM             0.137</code>&#13;
<code class="go">LSTAT          0.134</code>&#13;
<code class="go">AGE            0.110</code></pre>&#13;
&#13;
<p>XGBoost includes plotting facilities for feature importance. Note that the <code>importance_type</code> parameter changes the values in this plot (see <a data-type="xref" href="#idfir2">Figure 14-5</a>). The default is using weight to determine feature importance:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">xgb</code><code class="o">.</code><code class="n">plot_importance</code><code class="p">(</code><code class="n">xgr</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1405.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="idfir2">&#13;
<img alt="Feature importance using weight (how many times a feature is split on in the trees)." src="assets/mlpr_1405.png"/>&#13;
<h6><span class="label">Figure 14-5. </span>Feature importance using weight (how many times a feature is split on in the trees).</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-primary="Yellowbrick" data-secondary="feature importance visualization" data-type="indexterm" id="idm46066887656856"/>Using Yellowbrick to plot feature importances (it will normalize the <code>feature_importances_</code> attribute) (see <a data-type="xref" href="#idfir1">Figure 14-6</a>):</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fi_viz</code> <code class="o">=</code> <code class="n">FeatureImportances</code><code class="p">(</code><code class="n">xgr</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fi_viz</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">bos_X_train</code><code class="p">,</code> <code class="n">bos_y_train</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fi_viz</code><code class="o">.</code><code class="n">poof</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1406.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="idfir1">&#13;
<img alt="Feature importance using relative importance of gain (percent importance of the most important feature)." src="assets/mlpr_1406.png"/>&#13;
<h6><span class="label">Figure 14-6. </span>Feature importance using relative importance of gain (percent importance of the most important feature).</h6>&#13;
</div></figure>&#13;
&#13;
<p>XGBoost provides both a textual representation of the trees&#13;
and a graphical one. Here is the text representation:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">booster</code> <code class="o">=</code> <code class="n">xgr</code><code class="o">.</code><code class="n">get_booster</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="k">print</code><code class="p">(</code><code class="n">booster</code><code class="o">.</code><code class="n">get_dump</code><code class="p">()[</code><code class="mi">0</code><code class="p">])</code>&#13;
<code class="go">0:[LSTAT&lt;9.72500038] yes=1,no=2,missing=1</code>&#13;
<code class="go"> 1:[RM&lt;6.94099998] yes=3,no=4,missing=3</code>&#13;
<code class="go">  3:[DIS&lt;1.48494995] yes=7,no=8,missing=7</code>&#13;
<code class="go">   7:leaf=3.9599998</code>&#13;
<code class="go">   8:leaf=2.40158272</code>&#13;
<code class="go">  4:[RM&lt;7.43700027] yes=9,no=10,missing=9</code>&#13;
<code class="go">   9:leaf=3.22561002</code>&#13;
<code class="go">   10:leaf=4.31580687</code>&#13;
<code class="go"> 2:[LSTAT&lt;16.0849991] yes=5,no=6,missing=5</code>&#13;
<code class="go">  5:[B&lt;116.024994] yes=11,no=12,missing=11</code>&#13;
<code class="go">   11:leaf=1.1825</code>&#13;
<code class="go">   12:leaf=1.99701393</code>&#13;
<code class="go">  6:[NOX&lt;0.603000045] yes=13,no=14,missing=13</code>&#13;
<code class="go">   13:leaf=1.6868</code>&#13;
<code class="go">   14:leaf=1.18572915</code></pre>&#13;
&#13;
<p>The leaf values can be interpreted as the sum of the <code>base_score</code> and the leaf. (To validate this, call <code>.predict</code> with the <code>ntree_limit=1</code> parameter to limit the model to using the result of the first tree.)</p>&#13;
&#13;
<p>Here is a graphical version of the tree (see <a data-type="xref" href="#idr27">Figure 14-7</a>):<a data-startref="ix_ch14-asciidoc13" data-type="indexterm" id="idm46066887454808"/><a data-startref="ix_ch14-asciidoc12" data-type="indexterm" id="idm46066887454104"/></p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="go">fig, ax = plt.subplots(figsize=(6, 4))</code>&#13;
<code class="go">xgb.plot_tree(xgr, ax=ax, num_trees=0)</code>&#13;
<code class="go">fig.savefig('images/mlpr_1407.png', dpi=300)</code></pre>&#13;
&#13;
<figure><div class="figure" id="idr27">&#13;
<img alt="XGBoost tree." src="assets/mlpr_1407.png"/>&#13;
<h6><span class="label">Figure 14-7. </span>XGBoost tree.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="LightGBM Regression" data-type="sect1"><div class="sect1" id="idm46066887911864">&#13;
<h1>LightGBM Regression</h1>&#13;
&#13;
<p><a data-primary="LightGBM" data-secondary="for regression" data-type="indexterm" id="ix_ch14-asciidoc14"/><a data-primary="regression" data-secondary="LightGBM for" data-type="indexterm" id="ix_ch14-asciidoc15"/>The gradient boosting tree library, LightGBM, also supports regression. As mentioned in the classification chapter, it can be faster than XGBoost for creating trees due to the sampling mechanism used to determine node splits.</p>&#13;
&#13;
<p>Also, remember that it grows trees depth first, so limiting depth may harm the model. It has the following properties:</p>&#13;
<dl>&#13;
<dt>Runtime efficiency</dt>&#13;
<dd>&#13;
<p>Can take advantage of multiple CPUs. By using binning, can be 15 times faster than XGBoost.</p>&#13;
</dd>&#13;
<dt>Preprocess data</dt>&#13;
<dd>&#13;
<p>Has some support for encoding categorical columns as integers (or pandas <code>Categorical</code> type), but AUC appears to suffer compared to one-hot encoding.</p>&#13;
</dd>&#13;
<dt>Prevent overfitting</dt>&#13;
<dd>&#13;
<p>Lower <code>num_leaves</code>. Increase <code>min_data_in_leaf</code>. Use <code>min_gain_to_split</code> with <code>lambda_l1</code> or <code>lambda_l2</code>.</p>&#13;
</dd>&#13;
<dt>Interpret results</dt>&#13;
<dd>&#13;
<p>Feature importance is available. Individual trees are weak and tend to be hard to interpret.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Here is an example of using the model:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">lightgbm</code> <code class="kn">as</code> <code class="nn">lgb</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lgr</code> <code class="o">=</code> <code class="n">lgb</code><code class="o">.</code><code class="n">LGBMRegressor</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lgr</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">bos_X_train</code><code class="p">,</code> <code class="n">bos_y_train</code><code class="p">)</code>&#13;
<code class="go">LGBMRegressor(boosting_type='gbdt',</code>&#13;
<code class="go">  class_weight=None, colsample_bytree=1.0,</code>&#13;
<code class="go">  learning_rate=0.1, max_depth=-1,</code>&#13;
<code class="go">  min_child_samples=20, min_child_weight=0.001,</code>&#13;
<code class="go">  min_split_gain=0.0, n_estimators=100,</code>&#13;
<code class="go">  n_jobs=-1, num_leaves=31, objective=None,</code>&#13;
<code class="go">  random_state=42, reg_alpha=0.0,</code>&#13;
<code class="go">  reg_lambda=0.0, silent=True, subsample=1.0,</code>&#13;
<code class="go">  subsample_for_bin=200000, subsample_freq=0)</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lgr</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">bos_X_test</code><code class="p">,</code> <code class="n">bos_y_test</code><code class="p">)</code>&#13;
<code class="go">0.847729219534575</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lgr</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">bos_X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[[</code><code class="mi">0</code><code class="p">]])</code>&#13;
<code class="go">array([30.31689569])</code></pre>&#13;
&#13;
<p class="pagebreak-before">Instance parameters:</p>&#13;
<dl>&#13;
<dt><code>boosting_type='gbdt'</code></dt>&#13;
<dd>&#13;
<p>Can be <code>'gbdt'</code> (gradient boosting), <code>'rf'</code> (random forest), <code>'dart'</code> (dropouts meet multiple additive regression trees), or <code>'goss'</code> (gradient-based, one-sided sampling).</p>&#13;
</dd>&#13;
<dt><code>num_leaves=31</code></dt>&#13;
<dd>&#13;
<p>Maximum tree leaves.</p>&#13;
</dd>&#13;
<dt><code>max_depth=-1</code></dt>&#13;
<dd>&#13;
<p>Maximum tree depth. -1 is unlimited. Larger depths tend to overfit more.</p>&#13;
</dd>&#13;
<dt><code>learning_rate=0.1</code></dt>&#13;
<dd>&#13;
<p>Range (0, 1.0]. Learning rate for boosting. A smaller value slows down overfitting as the boosting rounds have less impact. A smaller number should give better performance but will require more <code>num_iterations</code>.</p>&#13;
</dd>&#13;
<dt><code>n_estimators=100</code></dt>&#13;
<dd>&#13;
<p>Number of trees or boosting rounds.</p>&#13;
</dd>&#13;
<dt><code>subsample_for_bin=200000</code></dt>&#13;
<dd>&#13;
<p>Samples required to create bins.</p>&#13;
</dd>&#13;
<dt><code>objective=None</code></dt>&#13;
<dd>&#13;
<p><code>None</code> - Does regression by default. Can be a function or string.</p>&#13;
</dd>&#13;
<dt><code>min_split_gain=0.0</code></dt>&#13;
<dd>&#13;
<p>Loss reduction required to partition leaf.</p>&#13;
</dd>&#13;
<dt><code>min_child_weight=0.001</code></dt>&#13;
<dd>&#13;
<p>Sum of hessian weight required for a leaf. Larger will be more conservative.</p>&#13;
</dd>&#13;
<dt><code>min_child_samples=20</code></dt>&#13;
<dd>&#13;
<p>Number of samples required for a leaf. Lower numbers mean more overfitting.</p>&#13;
</dd>&#13;
<dt><code>subsample=1.0</code></dt>&#13;
<dd>&#13;
<p>Fraction of samples to use for the next round.</p>&#13;
</dd>&#13;
<dt><code>subsample_freq=0</code></dt>&#13;
<dd>&#13;
<p>Subsample frequency. Change to 1 to enable.</p>&#13;
</dd>&#13;
<dt><code>colsample_bytree=1.0</code></dt>&#13;
<dd>&#13;
<p>Range (0, 1.0]. Select percent of features for each boosting round.</p>&#13;
</dd>&#13;
<dt><code>reg_alpha=0.0</code></dt>&#13;
<dd>&#13;
<p>L1 regularization (mean of weights). Increase to be more conservative.</p>&#13;
</dd>&#13;
<dt><code>reg_lambda=0.0</code></dt>&#13;
<dd>&#13;
<p>L2 regularization (root of squared weights). Increase to be more conservative.</p>&#13;
</dd>&#13;
<dt><code>random_state=42</code></dt>&#13;
<dd>&#13;
<p>Random seed.</p>&#13;
</dd>&#13;
<dt><code>n_jobs=-1</code></dt>&#13;
<dd>&#13;
<p>Number of threads.</p>&#13;
</dd>&#13;
<dt><code>silent=True</code></dt>&#13;
<dd>&#13;
<p>Verbose mode.</p>&#13;
</dd>&#13;
<dt><code>importance_type='split'</code></dt>&#13;
<dd>&#13;
<p>Determines how importance is calculated: <code><em>split</em></code> (times a feature was used) or <code><em>gain</em></code> (total gains of splits when a feature was used).</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p><a data-primary="feature importance" data-secondary="LightGBM" data-type="indexterm" id="idm46066887337032"/>LightGBM supports feature importance. The <code>importance_type</code> parameter determines how this is calculated (the default is based on how many times a feature was used):</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">col</code><code class="p">,</code> <code class="n">val</code> <code class="ow">in</code> <code class="nb">sorted</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="nb">zip</code><code class="p">(</code>&#13;
<code class="gp">... </code>        <code class="n">bos_X</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code> <code class="n">lgr</code><code class="o">.</code><code class="n">feature_importances_</code>&#13;
<code class="gp">... </code>    <code class="p">),</code>&#13;
<code class="gp">... </code>    <code class="n">key</code><code class="o">=</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code>&#13;
<code class="gp">... </code>    <code class="n">reverse</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)[:</code><code class="mi">5</code><code class="p">]:</code>&#13;
<code class="gp">... </code>    <code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s">"{col:10}{val:10.3f}"</code><code class="p">)</code>&#13;
<code class="go">LSTAT        226.000</code>&#13;
<code class="go">RM           199.000</code>&#13;
<code class="go">DIS          172.000</code>&#13;
<code class="go">AGE          130.000</code>&#13;
<code class="go">B            121.000</code></pre>&#13;
&#13;
<p>Feature importance plot showing how many times a feature is used (see <a data-type="xref" href="#id23_2">Figure 14-8</a>):</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lgb</code><code class="o">.</code><code class="n">plot_importance</code><code class="p">(</code><code class="n">lgr</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">tight_layout</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1408.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id23_2">&#13;
<img alt="Feature importance showing how many times a feature is used." src="assets/mlpr_1408.png"/>&#13;
<h6><span class="label">Figure 14-8. </span>Feature importance showing how many times a feature is used.</h6>&#13;
</div></figure>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>In Jupyter, use the following command to view a tree<a data-startref="ix_ch14-asciidoc15" data-type="indexterm" id="idm46066887131480"/><a data-startref="ix_ch14-asciidoc14" data-type="indexterm" id="idm46066887130776"/>:<a data-startref="ix_ch14-asciidoc0" data-type="indexterm" id="idm46066887129976"/></p>&#13;
&#13;
<pre data-type="programlisting">lgb.create_tree_digraph(lgbr)</pre>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>