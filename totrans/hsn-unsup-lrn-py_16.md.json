["```py\n'''Main'''\nimport numpy as np\nimport pandas as pd\nimport os, time, re\nimport pickle, gzip, datetime\n\n'''Data Viz'''\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nimport matplotlib as mpl\nfrom mpl_toolkits.axes_grid1 import Grid\n\n%matplotlib inline\n\n'''Data Prep and Model Evaluation'''\nfrom sklearn import preprocessing as pp\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss, accuracy_score\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score, mean_squared_error\n\n'''Algos'''\nimport lightgbm as lgb\n\n'''TensorFlow and Keras'''\nimport tensorflow as tf\nimport keras\nfrom keras import backend as K\nfrom keras.models import Sequential, Model\nfrom keras.layers import Activation, Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.layers import LeakyReLU, Reshape, UpSampling2D, Conv2DTranspose\nfrom keras.layers import BatchNormalization, Input, Lambda\nfrom keras.layers import Embedding, Flatten, dot\nfrom keras import regularizers\nfrom keras.losses import mse, binary_crossentropy\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.optimizers import Adam, RMSprop\nfrom tensorflow.examples.tutorials.mnist import input_data\n```", "```py\n# Load the datasets\ncurrent_path = os.getcwd()\nfile = '\\\\datasets\\\\mnist_data\\\\mnist.pkl.gz'\nf = gzip.open(current_path+file, 'rb')\ntrain_set, validation_set, test_set = pickle.load(f, encoding='latin1')\nf.close()\n\nX_train, y_train = train_set[0], train_set[1]\nX_validation, y_validation = validation_set[0], validation_set[1]\nX_test, y_test = test_set[0], test_set[1]\n\nX_train_keras = X_train.reshape(50000,28,28,1)\nX_validation_keras = X_validation.reshape(10000,28,28,1)\nX_test_keras = X_test.reshape(10000,28,28,1)\n\ny_train_keras = to_categorical(y_train)\ny_validation_keras = to_categorical(y_validation)\ny_test_keras = to_categorical(y_test)\n\n# Create Pandas DataFrames from the datasets\ntrain_index = range(0,len(X_train))\nvalidation_index = range(len(X_train),len(X_train)+len(X_validation))\ntest_index = range(len(X_train)+len(X_validation),len(X_train)+ \\\n                   len(X_validation)+len(X_test))\n\nX_train = pd.DataFrame(data=X_train,index=train_index)\ny_train = pd.Series(data=y_train,index=train_index)\n\nX_validation = pd.DataFrame(data=X_validation,index=validation_index)\ny_validation = pd.Series(data=y_validation,index=validation_index)\n\nX_test = pd.DataFrame(data=X_test,index=test_index)\ny_test = pd.Series(data=y_test,index=test_index)\n\ndef view_digit(X, y, example):\n    label = y.loc[example]\n    image = X.loc[example,:].values.reshape([28,28])\n    plt.title('Example: %d Label: %d' % (example, label))\n    plt.imshow(image, cmap=plt.get_cmap('gray'))\n    plt.show()\n```", "```py\nmodel = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same',\n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same',\n                 activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same',\n                 activation ='relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same',\n                 activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))\n```", "```py\n# Train CNN\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(X_train_keras, y_train_keras,\n          validation_data=(X_validation_keras, y_validation_keras), \\\n          epochs=100)\n```", "```py\ndef generator(self, depth=256, dim=7, dropout=0.3, momentum=0.8, \\\n              window=5, input_dim=100, output_depth=1):\n    if self.G:\n        return self.G\n    self.G = Sequential()\n    self.G.add(Dense(dim*dim*depth, input_dim=input_dim))\n    self.G.add(BatchNormalization(momentum=momentum))\n    self.G.add(Activation('relu'))\n    self.G.add(Reshape((dim, dim, depth)))\n    self.G.add(Dropout(dropout))\n```", "```py\n    self.G.add(UpSampling2D())\n    self.G.add(Conv2DTranspose(int(depth/2), window, padding='same'))\n    self.G.add(BatchNormalization(momentum=momentum))\n    self.G.add(Activation('relu'))\n\n    self.G.add(UpSampling2D())\n    self.G.add(Conv2DTranspose(int(depth/4), window, padding='same'))\n    self.G.add(BatchNormalization(momentum=momentum))\n    self.G.add(Activation('relu'))\n\n    self.G.add(Conv2DTranspose(int(depth/8), window, padding='same'))\n    self.G.add(BatchNormalization(momentum=momentum))\n    self.G.add(Activation('relu'))\n```", "```py\n    self.G.add(Conv2DTranspose(output_depth, window, padding='same'))\n    self.G.add(Activation('sigmoid'))\n    self.G.summary()\n    return self.G\n```", "```py\ndef discriminator(self, depth=64, dropout=0.3, alpha=0.3):\n    if self.D:\n        return self.D\n    self.D = Sequential()\n    input_shape = (self.img_rows, self.img_cols, self.channel)\n    self.D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,\n        padding='same'))\n    self.D.add(LeakyReLU(alpha=alpha))\n    self.D.add(Dropout(dropout))\n\n    self.D.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n    self.D.add(LeakyReLU(alpha=alpha))\n    self.D.add(Dropout(dropout))\n\n    self.D.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n    self.D.add(LeakyReLU(alpha=alpha))\n    self.D.add(Dropout(dropout))\n\n    self.D.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n    self.D.add(LeakyReLU(alpha=alpha))\n    self.D.add(Dropout(dropout))\n\n    self.D.add(Flatten())\n    self.D.add(Dense(1))\n    self.D.add(Activation('sigmoid'))\n    self.D.summary()\n    return self.D\n```", "```py\ndef discriminator_model(self):\n    if self.DM:\n        return self.DM\n    optimizer = RMSprop(lr=0.0002, decay=6e-8)\n    self.DM = Sequential()\n    self.DM.add(self.discriminator())\n    self.DM.compile(loss='binary_crossentropy', \\\n                    optimizer=optimizer, metrics=['accuracy'])\n    return self.DM\n\ndef adversarial_model(self):\n    if self.AM:\n        return self.AM\n    optimizer = RMSprop(lr=0.0001, decay=3e-8)\n    self.AM = Sequential()\n    self.AM.add(self.generator())\n    self.AM.add(self.discriminator())\n    self.AM.compile(loss='binary_crossentropy', \\\n                    optimizer=optimizer, metrics=['accuracy'])\n    return self.AM\n```", "```py\nclass MNIST_DCGAN(object):\n    def __init__(self, x_train):\n        self.img_rows = 28\n        self.img_cols = 28\n        self.channel = 1\n\n        self.x_train = x_train\n\n        self.DCGAN = DCGAN()\n        self.discriminator =  self.DCGAN.discriminator_model()\n        self.adversarial = self.DCGAN.adversarial_model()\n        self.generator = self.DCGAN.generator()\n```", "```py\ndef train(self, train_steps=2000, batch_size=256, save_interval=0):\n    noise_input = None\n    if save_interval>0:\n        noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n    for i in range(train_steps):\n        images_train = self.x_train[np.random.randint(0,\n            self.x_train.shape[0], size=batch_size), :, :, :]\n        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n        images_fake = self.generator.predict(noise)\n        x = np.concatenate((images_train, images_fake))\n        y = np.ones([2*batch_size, 1])\n        y[batch_size:, :] = 0\n\n        d_loss = self.discriminator.train_on_batch(x, y)\n\n        y = np.ones([batch_size, 1])\n        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n        a_loss = self.adversarial.train_on_batch(noise, y)\n        log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n        log_mesg = \"%s [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], \\\n                                                  a_loss[1])\n        print(log_mesg)\n        if save_interval>0:\n            if (i+1)%save_interval==0:\n                self.plot_images(save2file=True, \\\n                    samples=noise_input.shape[0],\\\n                    noise=noise_input, step=(i+1))\n```", "```py\ndef plot_images(self, save2file=False, fake=True, samples=16, \\\n                noise=None, step=0):\n    filename = 'mnist.png'\n    if fake:\n        if noise is None:\n            noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n        else:\n            filename = \"mnist_%d.png\" % step\n        images = self.generator.predict(noise)\n    else:\n        i = np.random.randint(0, self.x_train.shape[0], samples)\n        images = self.x_train[i, :, :, :]\n\n    plt.figure(figsize=(10,10))\n    for i in range(images.shape[0]):\n        plt.subplot(4, 4, i+1)\n        image = images[i, :, :, :]\n        image = np.reshape(image, [self.img_rows, self.img_cols])\n        plt.imshow(image, cmap='gray')\n        plt.axis('off')\n    plt.tight_layout()\n    if save2file:\n        plt.savefig(filename)\n        plt.close('all')\n    else:\n        plt.show()\n```", "```py\n# Initialize MNIST_DCGAN and train\nmnist_dcgan = MNIST_DCGAN(X_train_keras)\ntimer = ElapsedTimer()\nmnist_dcgan.train(train_steps=10000, batch_size=256, save_interval=500)\n```", "```py\n0:  [D loss: 0.692640, acc: 0.527344] [A loss: 1.297974, acc: 0.000000]\n1:  [D loss: 0.651119, acc: 0.500000] [A loss: 0.920461, acc: 0.000000]\n2:  [D loss: 0.735192, acc: 0.500000] [A loss: 1.289153, acc: 0.000000]\n3:  [D loss: 0.556142, acc: 0.947266] [A loss: 1.218020, acc: 0.000000]\n4:  [D loss: 0.492492, acc: 0.994141] [A loss: 1.306247, acc: 0.000000]\n5:  [D loss: 0.491894, acc: 0.916016] [A loss: 1.722399, acc: 0.000000]\n6:  [D loss: 0.607124, acc: 0.527344] [A loss: 1.698651, acc: 0.000000]\n7:  [D loss: 0.578594, acc: 0.921875] [A loss: 1.042844, acc: 0.000000]\n8:  [D loss: 0.509973, acc: 0.587891] [A loss: 1.957741, acc: 0.000000]\n9:  [D loss: 0.538314, acc: 0.896484] [A loss: 1.133667, acc: 0.000000]\n10: [D loss: 0.510218, acc: 0.572266] [A loss: 1.855000, acc: 0.000000]\n11: [D loss: 0.501239, acc: 0.923828] [A loss: 1.098140, acc: 0.000000]\n12: [D loss: 0.509211, acc: 0.519531] [A loss: 1.911793, acc: 0.000000]\n13: [D loss: 0.482305, acc: 0.923828] [A loss: 1.187290, acc: 0.000000]\n14: [D loss: 0.395886, acc: 0.900391] [A loss: 1.465053, acc: 0.000000]\n15: [D loss: 0.346876, acc: 0.992188] [A loss: 1.443823, acc: 0.000000]\n```", "```py\n9985: [D loss: 0.696480, acc: 0.521484] [A loss: 0.955954, acc: 0.125000]\n9986: [D loss: 0.716583, acc: 0.472656] [A loss: 0.761385, acc: 0.363281]\n9987: [D loss: 0.710941, acc: 0.533203] [A loss: 0.981265, acc: 0.074219]\n9988: [D loss: 0.703731, acc: 0.515625] [A loss: 0.679451, acc: 0.558594]\n9989: [D loss: 0.722460, acc: 0.492188] [A loss: 0.899768, acc: 0.125000]\n9990: [D loss: 0.691914, acc: 0.539062] [A loss: 0.726867, acc: 0.464844]\n9991: [D loss: 0.716197, acc: 0.500000] [A loss: 0.932500, acc: 0.144531]\n9992: [D loss: 0.689704, acc: 0.548828] [A loss: 0.734389, acc: 0.414062]\n9993: [D loss: 0.714405, acc: 0.517578] [A loss: 0.850408, acc: 0.218750]\n9994: [D loss: 0.690414, acc: 0.550781] [A loss: 0.766320, acc: 0.355469]\n9995: [D loss: 0.709792, acc: 0.511719] [A loss: 0.960070, acc: 0.105469]\n9996: [D loss: 0.695851, acc: 0.500000] [A loss: 0.774395, acc: 0.324219]\n9997: [D loss: 0.712254, acc: 0.521484] [A loss: 0.853828, acc: 0.183594]\n9998: [D loss: 0.702689, acc: 0.529297] [A loss: 0.802785, acc: 0.308594]\n9999: [D loss: 0.698032, acc: 0.517578] [A loss: 0.810278, acc: 0.304688]\n```"]