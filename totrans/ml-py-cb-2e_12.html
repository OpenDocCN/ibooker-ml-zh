<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 12. Model Selection" data-type="chapter" epub:type="chapter"><div class="chapter" id="model-selection">
<h1><span class="label">Chapter 12. </span>Model Selection</h1>
<section data-pdf-bookmark="12.0 Introduction" data-type="sect1"><div class="sect1" id="id284">
<h1>12.0 Introduction</h1>
<p>In <a data-primary="model selection" data-type="indexterm" id="ix_mod_sel_ch12"/><a data-primary="hyperparameters" data-secondary="versus parameters" data-secondary-sortas="parameters" data-type="indexterm" id="id1642"/><a data-primary="parameters" data-secondary="versus hyperparameters" data-secondary-sortas="hyperparameters" data-type="indexterm" id="id1643"/>machine learning, we use training algorithms to learn the parameters of a model by minimizing some loss function. However, many learning algorithms (e.g., support vector classifier and random forests) have additional <em>hyperparameters</em> that are <a data-primary="hyperparameters" data-seealso="model selection" data-type="indexterm" id="id1644"/>defined by the user and affect how the model will learn its parameters. As we mentioned earlier in the book, <em>parameters</em> (also sometimes called model weights) are what models learn during the training process, whereas hyperparameters are provided manually by us (the users).</p>
<p>For example, random forests are collections of decision trees (hence the word <em>forest</em>); however, the number of decision trees in the forest is not learned by the algorithm and must be set prior to fitting. This is often referred to as <em>hyperparameter tuning</em>, <em>hyperparameter optimization</em>, or <em>model selection</em>. Additionally, we might want to try multiple learning algorithms (for example, trying both support vector classifier and random forests to see which learning method produces the best model).</p>
<p>While there is widespread terminology variation in this
area, in this book we refer to selecting both the best learning
algorithm and its best hyperparameters as model selection. The reason is
straightforward: imagine we have data and want to train a support
vector classifier with 10 candidate hyperparameter values and a random
forest classifier with 10 candidate hyperparameter values. The result is
that we are trying to select the best model from a set of 20 candidate
models. In this chapter, we will cover techniques to efficiently select
the best model from the set of candidates.</p>
<p>Throughout this chapter we will refer to specific hyperparameters, such
as C (the inverse of regularization strength). Don’t worry if you don’t
know what the hyperparameters are. We will cover them in later
chapters. Instead, just treat hyperparameters like the settings for the
learning algorithm that we must choose before starting training. In general, finding the model and associated hyperparameters that yield the best performance is the result of experimentation—trying a bunch of things out and seeing what works best.</p>
</div></section>
<section data-pdf-bookmark="12.1 Selecting the Best Models Using Exhaustive Search" data-type="sect1"><div class="sect1" id="selecting-best-models-using-exhaustive-search">
<h1>12.1 Selecting the Best Models Using Exhaustive Search</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id285">
<h2>Problem</h2>
<p>You want to <a data-primary="model selection" data-secondary="exhaustive search method" data-type="indexterm" id="ix_mod_sel_ex_search"/><a data-primary="exhaustive search method, model selection" data-type="indexterm" id="ix_search_mod_sel"/><a data-primary="cross-validation (CV) of ML models" data-secondary="GridSearchCV" data-type="indexterm" id="ix_cross_val_grid"/>select the best model by searching over a range of
hyperparameters.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id286">
<h2>Solution</h2>
<p>Use <a data-primary="GridSearchCV" data-type="indexterm" id="ix_grid_search_cv"/>scikit-learn’s <code>GridSearchCV</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">linear_model</code><code class="p">,</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">GridSearchCV</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create logistic regression</code>
<code class="n">logistic</code> <code class="o">=</code> <code class="n">linear_model</code><code class="o">.</code><code class="n">LogisticRegression</code><code class="p">(</code><code class="n">max_iter</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code> <code class="n">solver</code><code class="o">=</code><code class="s1">'liblinear'</code><code class="p">)</code>

<code class="c1"># Create range of candidate penalty hyperparameter values</code>
<code class="n">penalty</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'l1'</code><code class="p">,</code><code class="s1">'l2'</code><code class="p">]</code>

<code class="c1"># Create range of candidate regularization hyperparameter values</code>
<code class="n">C</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">logspace</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">10</code><code class="p">)</code>

<code class="c1"># Create dictionary of hyperparameter candidates</code>
<code class="n">hyperparameters</code> <code class="o">=</code> <code class="nb">dict</code><code class="p">(</code><code class="n">C</code><code class="o">=</code><code class="n">C</code><code class="p">,</code> <code class="n">penalty</code><code class="o">=</code><code class="n">penalty</code><code class="p">)</code>

<code class="c1"># Create grid search</code>
<code class="n">gridsearch</code> <code class="o">=</code> <code class="n">GridSearchCV</code><code class="p">(</code><code class="n">logistic</code><code class="p">,</code> <code class="n">hyperparameters</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">verbose</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Fit grid search</code>
<code class="n">best_model</code> <code class="o">=</code> <code class="n">gridsearch</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>

<code class="c1"># Show the best model</code>
<code class="nb">print</code><code class="p">(</code><code class="n">best_model</code><code class="o">.</code><code class="n">best_estimator_</code><code class="p">)</code></pre>
<pre data-type="programlisting">LogisticRegression(C=7.742636826811269, max_iter=500, penalty='l1',
                   solver='liblinear')</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id287">
<h2>Discussion</h2>
<p><code>GridSearchCV</code> is a brute-force approach to model selection using
cross-validation. Specifically, a user defines sets of possible values
for one or multiple hyperparameters, and then <code>GridSearchCV</code> trains a model using every value and/or combination of values. The model with the best performance score is selected as the best model.</p>
<p>For example, in our <a data-primary="hyperparameters" data-secondary="regularization penalty" data-type="indexterm" id="id1645"/><a data-primary="regularization penalty hyperparameter" data-type="indexterm" id="id1646"/><a data-primary="C hyperparameter" data-secondary="GridSearchCV" data-type="indexterm" id="id1647"/>solution we used logistic regression as our learning algorithm and tuned two hyperparameters: C and the regularization penalty. We also specified two other parameters, the solver and max iterations. Don’t worry if you don’t know what these terms mean; we cover them in the next few chapters. Just realize that C and the regularization penalty can take a range of values, which have to be specified prior to training. For C, we define 10 possible values:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">np</code><code class="o">.</code><code class="n">logspace</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">10</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([1.00000000e+00, 2.78255940e+00, 7.74263683e+00, 2.15443469e+01,
       5.99484250e+01, 1.66810054e+02, 4.64158883e+02, 1.29154967e+03,
       3.59381366e+03, 1.00000000e+04])</pre>
<p>Similarly, we define two possible values for the regularization
penalty: <code>['l1', 'l2']</code>. For each combination of C and regularization
penalty values, we train the model and evaluate it using k-fold
cross-validation. In our solution, we have 10 possible values of C, 2
possible values of regularization penalty, and 5 folds. They create

<span class="keep-together">10 × 2 × 5 = 100</span> candidate models, from which the
best is selected.</p>
<p>Once <code>GridSearchCV</code> is complete, we can see the hyperparameters of
the best model:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View best hyperparameters</code>
<code class="nb">print</code><code class="p">(</code><code class="s1">'Best Penalty:'</code><code class="p">,</code> <code class="n">best_model</code><code class="o">.</code><code class="n">best_estimator_</code><code class="o">.</code><code class="n">get_params</code><code class="p">()[</code><code class="s1">'penalty'</code><code class="p">])</code>
<code class="nb">print</code><code class="p">(</code><code class="s1">'Best C:'</code><code class="p">,</code> <code class="n">best_model</code><code class="o">.</code><code class="n">best_estimator_</code><code class="o">.</code><code class="n">get_params</code><code class="p">()[</code><code class="s1">'C'</code><code class="p">])</code></pre>
<pre data-type="programlisting">Best Penalty: l1
Best C: 7.742636826811269</pre>
<p>By default, after identifying the best hyperparameters, <code>GridSearchCV</code>
will retrain a model using the best hyperparameters on the entire
dataset (rather than leaving a fold out for cross-validation). We can
use this model to predict values like any other scikit-learn model:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Predict target vector</code>
<code class="n">best_model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">features</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])</pre>
<p>One <code>GridSearchCV</code> parameter is worth noting: <code>verbose</code>. While mostly
unnecessary, it can be reassuring during long searching processes to
receive an indication that the search is progressing. The <code>verbose</code>
parameter determines the number of messages outputted during the search,
with <code>0</code> showing no output, and <code>1</code> to <code>3</code> outputting additional messages.<a data-primary="" data-startref="ix_cross_val_grid" data-type="indexterm" id="id1648"/><a data-primary="" data-startref="ix_search_mod_sel" data-type="indexterm" id="id1649"/><a data-primary="" data-startref="ix_grid_search_cv" data-type="indexterm" id="id1650"/><a data-primary="" data-startref="ix_mod_sel_ex_search" data-type="indexterm" id="id1651"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1652">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/XlMPG">scikit-learn documentation: GridSearchCV</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="12.2 Selecting the Best Models Using Randomized Search" data-type="sect1"><div class="sect1" id="selecting-best-models-using-randomized-search">
<h1>12.2 Selecting the Best Models Using Randomized Search</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id513">
<h2>Problem</h2>
<p>You want a <a data-primary="model selection" data-secondary="randomized search method" data-type="indexterm" id="ix_mod_sel_rand_search"/><a data-primary="cross-validation (CV) of ML models" data-secondary="RandomizedSearchCV" data-type="indexterm" id="ix_cross_val_rand_search"/>computationally cheaper method than exhaustive search to
select the best model.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id288">
<h2>Solution</h2>
<p>Use <a data-primary="RandomizedSearchCV" data-type="indexterm" id="ix_rand_search_cv"/>scikit-learn’s <code>RandomizedSearchCV</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">scipy.stats</code> <code class="kn">import</code> <code class="n">uniform</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">linear_model</code><code class="p">,</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">RandomizedSearchCV</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create logistic regression</code>
<code class="n">logistic</code> <code class="o">=</code> <code class="n">linear_model</code><code class="o">.</code><code class="n">LogisticRegression</code><code class="p">(</code><code class="n">max_iter</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code> <code class="n">solver</code><code class="o">=</code><code class="s1">'liblinear'</code><code class="p">)</code>

<code class="c1"># Create range of candidate regularization penalty hyperparameter values</code>
<code class="n">penalty</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'l1'</code><code class="p">,</code> <code class="s1">'l2'</code><code class="p">]</code>

<code class="c1"># Create distribution of candidate regularization hyperparameter values</code>
<code class="n">C</code> <code class="o">=</code> <code class="n">uniform</code><code class="p">(</code><code class="n">loc</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">scale</code><code class="o">=</code><code class="mi">4</code><code class="p">)</code>

<code class="c1"># Create hyperparameter options</code>
<code class="n">hyperparameters</code> <code class="o">=</code> <code class="nb">dict</code><code class="p">(</code><code class="n">C</code><code class="o">=</code><code class="n">C</code><code class="p">,</code> <code class="n">penalty</code><code class="o">=</code><code class="n">penalty</code><code class="p">)</code>

<code class="c1"># Create randomized search</code>
<code class="n">randomizedsearch</code> <code class="o">=</code> <code class="n">RandomizedSearchCV</code><code class="p">(</code>
    <code class="n">logistic</code><code class="p">,</code> <code class="n">hyperparameters</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">n_iter</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">verbose</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code>
    <code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Fit randomized search</code>
<code class="n">best_model</code> <code class="o">=</code> <code class="n">randomizedsearch</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>

<code class="c1"># Print best model</code>
<code class="nb">print</code><code class="p">(</code><code class="n">best_model</code><code class="o">.</code><code class="n">best_estimator_</code><code class="p">)</code></pre>
<pre data-type="programlisting">LogisticRegression(C=1.668088018810296, max_iter=500, penalty='l1',
                   solver='liblinear')</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id289">
<h2>Discussion</h2>
<p>In <a data-type="xref" href="#selecting-best-models-using-exhaustive-search">Recipe 12.1</a>, we used <code>GridSearchCV</code> on a user-defined set of
hyperparameter values to search for the best model according to a score
function. A more efficient method than <code>GridSearchCV</code>’s brute-force
search is to search over a specific number of random combinations of
hyperparameter values from user-supplied distributions (e.g., normal,
uniform). scikit-learn implements this randomized search technique
with <code>RandomizedSearchCV</code>.</p>
<p>With <code>RandomizedSearchCV</code>, if we specify a distribution, scikit-learn
will randomly sample without replacement hyperparameter values from that
distribution. As an example of the general concept, here we randomly
sample 10 values from a uniform distribution ranging from 0 to 4:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Define a uniform distribution between 0 and 4, sample 10 values</code>
<code class="n">uniform</code><code class="p">(</code><code class="n">loc</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">scale</code><code class="o">=</code><code class="mi">4</code><code class="p">)</code><code class="o">.</code><code class="n">rvs</code><code class="p">(</code><code class="mi">10</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([3.95211699, 0.30693116, 2.88237794, 3.00392864, 0.43964702,
       1.46670526, 0.27841863, 2.56541664, 2.66475584, 0.79611958])</pre>
<p>Alternatively, if we specify a list of values, such as two regularization
penalty hyperparameter values <code>['l1', 'l2']</code>, <code>RandomizedSearchCV</code> will
randomly sample with replacement from the list.</p>
<p>Just like with <code>GridSearchCV</code>, we can see the hyperparameter values of
the best model:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View best hyperparameters</code>
<code class="nb">print</code><code class="p">(</code><code class="s1">'Best Penalty:'</code><code class="p">,</code> <code class="n">best_model</code><code class="o">.</code><code class="n">best_estimator_</code><code class="o">.</code><code class="n">get_params</code><code class="p">()[</code><code class="s1">'penalty'</code><code class="p">])</code>
<code class="nb">print</code><code class="p">(</code><code class="s1">'Best C:'</code><code class="p">,</code> <code class="n">best_model</code><code class="o">.</code><code class="n">best_estimator_</code><code class="o">.</code><code class="n">get_params</code><code class="p">()[</code><code class="s1">'C'</code><code class="p">])</code></pre>
<pre data-type="programlisting">Best Penalty: l1
Best C: 1.668088018810296</pre>
<p>And just like with <code>GridSearchCV</code>, after the search is complete
<code>RandomizedSearchCV</code> fits a new model using the best hyperparameters on
the entire dataset. We can use this model like any other in
scikit-learn; for example, to make predictions:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Predict target vector</code>
<code class="n">best_model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">features</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2,
       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])</pre>
<p>The number of sampled combinations of hyperparameters (i.e., the number
of candidate models trained) is specified with the <code>n_iter</code> (number of
iterations) setting. It’s worth noting that <code>RandomizedSearchCV</code> isn’t inherently faster than <code>GridSearchCV</code>, but it often achieves comparable performance to <code>GridSearchCV</code> in less time just by testing fewer combinations.<a data-primary="" data-startref="ix_cross_val_rand_search" data-type="indexterm" id="id1653"/><a data-primary="" data-startref="ix_mod_sel_rand_search" data-type="indexterm" id="id1654"/><a data-primary="" data-startref="ix_rand_search_cv" data-type="indexterm" id="id1655"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1656">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/rpiSs">scikit-learn documentation: RandomizedSearchCV</a></p>
</li>
<li>
<p><a href="https://oreil.ly/iBcbo">Random Search for Hyper-Parameter Optimization</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="12.3 Selecting the Best Models from Multiple Learning Algorithms" data-type="sect1"><div class="sect1" id="selecting-best-models-from-multiple-learning-algorithms">
<h1>12.3 Selecting the Best Models from Multiple Learning Algorithms</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id290">
<h2>Problem</h2>
<p>You want to <a data-primary="multiple learning algorithms" data-type="indexterm" id="ix_multi_learn_alg"/><a data-primary="model selection" data-secondary="with multiple learning algorithms" data-secondary-sortas="multiple learning algorithms" data-type="indexterm" id="ix_mod_sel_multi_learn"/>select the best model by searching over a range of learning
algorithms and their respective hyperparameters.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id291">
<h2>Solution</h2>
<p>Create a <a data-primary="GridSearchCV" data-type="indexterm" id="ix_grid_search_cv2"/><a data-primary="cross-validation (CV) of ML models" data-secondary="GridSearchCV" data-type="indexterm" id="ix_cross_val_grid_cv"/>dictionary of candidate learning algorithms and their
hyperparameters to use as the search space for <code>GridSearchCV</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LogisticRegression</code>
<code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">RandomForestClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">GridSearchCV</code>
<code class="kn">from</code> <code class="nn">sklearn.pipeline</code> <code class="kn">import</code> <code class="n">Pipeline</code>

<code class="c1"># Set random seed</code>
<code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create a pipeline</code>
<code class="n">pipe</code> <code class="o">=</code> <code class="n">Pipeline</code><code class="p">([(</code><code class="s2">"classifier"</code><code class="p">,</code> <code class="n">RandomForestClassifier</code><code class="p">())])</code>

<code class="c1"># Create dictionary with candidate learning algorithms and their hyperparameters</code>
<code class="n">search_space</code> <code class="o">=</code> <code class="p">[{</code><code class="s2">"classifier"</code><code class="p">:</code> <code class="p">[</code><code class="n">LogisticRegression</code><code class="p">(</code><code class="n">max_iter</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code>
       <code class="n">solver</code><code class="o">=</code><code class="s1">'liblinear'</code><code class="p">)],</code>
                 <code class="s2">"classifier__penalty"</code><code class="p">:</code> <code class="p">[</code><code class="s1">'l1'</code><code class="p">,</code> <code class="s1">'l2'</code><code class="p">],</code>
                 <code class="s2">"classifier__C"</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">logspace</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">10</code><code class="p">)},</code>
                <code class="p">{</code><code class="s2">"classifier"</code><code class="p">:</code> <code class="p">[</code><code class="n">RandomForestClassifier</code><code class="p">()],</code>
                 <code class="s2">"classifier__n_estimators"</code><code class="p">:</code> <code class="p">[</code><code class="mi">10</code><code class="p">,</code> <code class="mi">100</code><code class="p">,</code> <code class="mi">1000</code><code class="p">],</code>
                 <code class="s2">"classifier__max_features"</code><code class="p">:</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">]}]</code>

<code class="c1"># Create grid search</code>
<code class="n">gridsearch</code> <code class="o">=</code> <code class="n">GridSearchCV</code><code class="p">(</code><code class="n">pipe</code><code class="p">,</code> <code class="n">search_space</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">verbose</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Fit grid search</code>
<code class="n">best_model</code> <code class="o">=</code> <code class="n">gridsearch</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>

<code class="c1"># Print best model</code>
<code class="nb">print</code><code class="p">(</code><code class="n">best_model</code><code class="o">.</code><code class="n">best_estimator_</code><code class="p">)</code></pre>
<pre data-type="programlisting">Pipeline(steps=[('classifier',
                 LogisticRegression(C=7.742636826811269, max_iter=500,
                                    penalty='l1', solver='liblinear'))])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id292">
<h2>Discussion</h2>
<p>In the previous two recipes, we found the best model by searching over
possible hyperparameter values of a learning algorithm. However, what if
we are not certain which learning algorithm to use? scikit-learn allows us to include learning algorithms as part of the
search space. <a data-primary="classifier__[hyperparameter name] format" data-type="indexterm" id="id1657"/>In our solution we define a search space that includes two
learning algorithms: logistic regression and random forest classifier.
Each learning algorithm has its own hyperparameters, and we define
their candidate values using the format
<code>classifier__[<em>hyperparameter name</em>]</code>. For example, for our logistic regression, to define the set of possible values for regularization hyperparameter space, <code>C</code>, and potential types of regularization penalties, <code>penalty</code>, we create a 
<span class="keep-together">dictionary:</span></p>
<pre data-type="programlisting">{'classifier': [LogisticRegression(max_iter=500, solver='liblinear')],
 'classifier__penalty': ['l1', 'l2'],
 'classifier__C': np.logspace(0, 4, 10)}</pre>
<p>We can also create a similar dictionary for the random forest
hyperparameters:</p>
<pre data-type="programlisting">{'classifier': [RandomForestClassifier()],
 'classifier__n_estimators': [10, 100, 1000],
 'classifier__max_features': [1, 2, 3]}</pre>
<p>After the search is complete, we can use <code>best_estimator_</code> to view the
best model’s learning algorithm and hyperparameters:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View best model</code>
<code class="nb">print</code><code class="p">(</code><code class="n">best_model</code><code class="o">.</code><code class="n">best_estimator_</code><code class="o">.</code><code class="n">get_params</code><code class="p">()[</code><code class="s2">"classifier"</code><code class="p">])</code></pre>
<pre data-type="programlisting">LogisticRegression(C=7.742636826811269, max_iter=500, penalty='l1',
                   solver='liblinear')</pre>
<p>Just like with the last two recipes, once we have fit the model
selection search, we can use this best model just like any other
scikit-learn model:<a data-primary="" data-startref="ix_mod_sel_multi_learn" data-type="indexterm" id="id1658"/><a data-primary="" data-startref="ix_multi_learn_alg" data-type="indexterm" id="id1659"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Predict target vector</code>
<code class="n">best_model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">features</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])</pre>
</div></section>
</div></section>
<section data-pdf-bookmark="12.4 Selecting the Best Models When Preprocessing" data-type="sect1"><div class="sect1" id="selecting-best-models-when-preprocessing">
<h1>12.4 Selecting the Best Models When Preprocessing</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id514">
<h2>Problem</h2>
<p>You want to include a <a data-primary="preprocessing data" data-secondary="model selection" data-type="indexterm" id="ix_preproc_mod_sel"/><a data-primary="model selection" data-secondary="in preprocessing" data-secondary-sortas="preprocessing" data-type="indexterm" id="ix_mod_sel_preproc"/>preprocessing step during model selection.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id1660">
<h2>Solution</h2>
<p>Create a pipeline that includes the preprocessing step and any of its
parameters:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LogisticRegression</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">GridSearchCV</code>
<code class="kn">from</code> <code class="nn">sklearn.pipeline</code> <code class="kn">import</code> <code class="n">Pipeline</code><code class="p">,</code> <code class="n">FeatureUnion</code>
<code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">PCA</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>

<code class="c1"># Set random seed</code>
<code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create a preprocessing object that includes StandardScaler features and PCA</code>
<code class="n">preprocess</code> <code class="o">=</code> <code class="n">FeatureUnion</code><code class="p">([(</code><code class="s2">"std"</code><code class="p">,</code> <code class="n">StandardScaler</code><code class="p">()),</code> <code class="p">(</code><code class="s2">"pca"</code><code class="p">,</code> <code class="n">PCA</code><code class="p">())])</code>

<code class="c1"># Create a pipeline</code>
<code class="n">pipe</code> <code class="o">=</code> <code class="n">Pipeline</code><code class="p">([(</code><code class="s2">"preprocess"</code><code class="p">,</code> <code class="n">preprocess</code><code class="p">),</code>
                 <code class="p">(</code><code class="s2">"classifier"</code><code class="p">,</code> <code class="n">LogisticRegression</code><code class="p">(</code><code class="n">max_iter</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code>
                     <code class="n">solver</code><code class="o">=</code><code class="s1">'liblinear'</code><code class="p">))])</code>

<code class="c1"># Create space of candidate values</code>
<code class="n">search_space</code> <code class="o">=</code> <code class="p">[{</code><code class="s2">"preprocess__pca__n_components"</code><code class="p">:</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">],</code>
                 <code class="s2">"classifier__penalty"</code><code class="p">:</code> <code class="p">[</code><code class="s2">"l1"</code><code class="p">,</code> <code class="s2">"l2"</code><code class="p">],</code>
                 <code class="s2">"classifier__C"</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">logspace</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">10</code><code class="p">)}]</code>

<code class="c1"># Create grid search</code>
<code class="n">clf</code> <code class="o">=</code> <code class="n">GridSearchCV</code><code class="p">(</code><code class="n">pipe</code><code class="p">,</code> <code class="n">search_space</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">verbose</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Fit grid search</code>
<code class="n">best_model</code> <code class="o">=</code> <code class="n">clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>

<code class="c1"># Print best model</code>
<code class="nb">print</code><code class="p">(</code><code class="n">best_model</code><code class="o">.</code><code class="n">best_estimator_</code><code class="p">)</code></pre>
<pre data-type="programlisting">Pipeline(steps=[('preprocess',
                 FeatureUnion(transformer_list=[('std', StandardScaler()),
                                                ('pca', PCA(n_components=1))])),
                ('classifier',
                 LogisticRegression(C=7.742636826811269, max_iter=1000,
                                    penalty='l1', solver='liblinear'))])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id293">
<h2>Discussion</h2>
<p>Very often we will need to preprocess our data before using it to train
a model. We have to be careful to properly handle preprocessing when
conducting model selection. First, <code>GridSearchCV</code> uses cross-validation to determine which model has the highest performance. However, in cross-validation, we are in effect pretending that the fold held out as the test set is not seen, and thus not part of fitting any preprocessing steps (e.g., scaling or standardization). For this reason, we cannot preprocess the data and then run <code>GridSearchCV</code>. Rather, the preprocessing steps must be a part of the set of actions taken by <code>GridSearchCV</code>.</p>
<p>This might appear complex, but <a data-primary="FeatureUnion" data-type="indexterm" id="id1661"/>scikit-learn makes it simple. <code>FeatureUnion</code> allows us to combine multiple preprocessing actions properly. In our solution, we use 
<span class="keep-together"><code>FeatureUnion</code></span> to combine two preprocessing steps: standardize the feature values 
<span class="keep-together">(<code>StandardScaler</code>)</span> and principal component analysis (<code>PCA</code>). This object is called 
<span class="keep-together"><code>preprocess</code></span> and contains both of our preprocessing steps. We then include 
<span class="keep-together"><code>preprocess</code></span> in a pipeline with our learning algorithm. The result is that this allows us to outsource the proper (and confusing) handling of fitting, transforming, and training the models with combinations of hyperparameters to scikit-learn.</p>
<p>Second, some <a data-primary="parameters" data-secondary="preprocessing" data-type="indexterm" id="id1662"/><a data-primary="preprocessing data" data-secondary="parameters for" data-type="indexterm" id="id1663"/>preprocessing methods have their own parameters, which often have to be supplied by the user. For example, dimensionality reduction using PCA requires the user to define the number of principal components to use to produce the transformed feature set. Ideally, we would choose the number of components that produces a model with the greatest performance for some evaluation test metric.</p>
<p>Luckily, scikit-learn makes this easy. When we include candidate component values in the search space, they are treated like any other hyperparameter to be searched over. In our solution, we defined <code>features__pca__n_components': [1, 2, 3]</code> in the search space to indicate that we want to discover if one, two, or three principal components produce the best model.<a data-primary="" data-startref="ix_cross_val_grid_cv" data-type="indexterm" id="id1664"/><a data-primary="" data-startref="ix_grid_search_cv2" data-type="indexterm" id="id1665"/><a data-primary="" data-startref="ix_mod_sel_preproc" data-type="indexterm" id="id1666"/><a data-primary="" data-startref="ix_preproc_mod_sel" data-type="indexterm" id="id1667"/></p>
<p>After model selection is complete, we can view the preprocessing values
that produced the best model. For example, we can see the best number of
principal 
<span class="keep-together">components:</span></p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View best n_components</code>
<code class="n">best_model</code><code class="o">.</code><code class="n">best_estimator_</code><code class="o">.</code><code class="n">get_params</code><code class="p">()[</code><code class="s1">'preprocess__pca__n_components'</code><code class="p">]</code></pre>
<pre data-type="programlisting">1</pre>
</div></section>
</div></section>
<section data-pdf-bookmark="12.5 Speeding Up Model Selection with Parallelization" data-type="sect1"><div class="sect1" id="speeding-up-model-selection-with-parallelization">
<h1>12.5 Speeding Up Model Selection with Parallelization</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id294">
<h2>Problem</h2>
<p>You need to <a data-primary="model selection" data-secondary="parallelization to speed up" data-type="indexterm" id="ix_mod_sel_par"/><a data-primary="parallelization, to speed up model selection" data-type="indexterm" id="ix_para_mod_sel"/><a data-primary="performance" data-secondary="parallelization for model selection" data-type="indexterm" id="ix_perf_para_mod_sel"/>speed up model selection.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id295">
<h2>Solution</h2>
<p>Use all the <a data-primary="n_jobs=-1 setting" data-type="indexterm" id="id1668"/>cores in your machine by setting <code>n_jobs=-1</code>, which enables you to train multiple models simultaneously:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">linear_model</code><code class="p">,</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">GridSearchCV</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create logistic regression</code>
<code class="n">logistic</code> <code class="o">=</code> <code class="n">linear_model</code><code class="o">.</code><code class="n">LogisticRegression</code><code class="p">(</code><code class="n">max_iter</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code> <code class="n">solver</code><code class="o">=</code><code class="s1">'liblinear'</code><code class="p">)</code>

<code class="c1"># Create range of candidate regularization penalty hyperparameter values</code>
<code class="n">penalty</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"l1"</code><code class="p">,</code> <code class="s2">"l2"</code><code class="p">]</code>

<code class="c1"># Create range of candidate values for C</code>
<code class="n">C</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">logspace</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">1000</code><code class="p">)</code>

<code class="c1"># Create hyperparameter options</code>
<code class="n">hyperparameters</code> <code class="o">=</code> <code class="nb">dict</code><code class="p">(</code><code class="n">C</code><code class="o">=</code><code class="n">C</code><code class="p">,</code> <code class="n">penalty</code><code class="o">=</code><code class="n">penalty</code><code class="p">)</code>

<code class="c1"># Create grid search</code>
<code class="n">gridsearch</code> <code class="o">=</code> <code class="n">GridSearchCV</code><code class="p">(</code><code class="n">logistic</code><code class="p">,</code> <code class="n">hyperparameters</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">,</code> <code class="n">verbose</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Fit grid search</code>
<code class="n">best_model</code> <code class="o">=</code> <code class="n">gridsearch</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>

<code class="c1"># Print best model</code>
<code class="nb">print</code><code class="p">(</code><code class="n">best_model</code><code class="o">.</code><code class="n">best_estimator_</code><code class="p">)</code></pre>
<pre data-type="programlisting">Fitting 5 folds for each of 2000 candidates, totalling 10000 fits
LogisticRegression(C=5.926151812475554, max_iter=500, penalty='l1',
                   solver='liblinear')</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id296">
<h2>Discussion</h2>
<p>In the recipes in this chapter, we have kept the number of candidate
models small to make the code complete quickly. However, in the real
world we may have many thousands or tens of thousands of models
to train. As a result, it can take many hours to find the best
model.<a data-primary="" data-startref="ix_mod_sel_par" data-type="indexterm" id="id1669"/><a data-primary="" data-startref="ix_para_mod_sel" data-type="indexterm" id="id1670"/><a data-primary="" data-startref="ix_perf_para_mod_sel" data-type="indexterm" id="id1671"/></p>
<p>
To speed up the process, scikit-learn lets us train multiple
models simultaneously. Without going into too much technical detail,
scikit-learn can simultaneously train models up to the number of cores on the machine. Most modern laptops have at least four cores, so (assuming you’re currently on a laptop) we can potentially train four models at the same time. This will dramatically increase the speed of our model selection process. The parameter <code>n_jobs</code> defines the number of models to train in parallel.
</p>
<p>In our solution, we set <code>n_jobs</code> to <code>-1</code>, which tells scikit-learn to use
<em>all</em> cores. However, by default <code>n_jobs</code> is set to <code>1</code>, meaning it uses only one core. To demonstrate this, if we run the same <code>GridSearchCV</code> as
in the solution, but with <code>n_jobs=1</code>, we can see it takes significantly longer to find the best model (note that exact time will depend on your computer):</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create grid search using one core</code>
<code class="n">clf</code> <code class="o">=</code> <code class="n">GridSearchCV</code><code class="p">(</code><code class="n">logistic</code><code class="p">,</code> <code class="n">hyperparameters</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">verbose</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Fit grid search</code>
<code class="n">best_model</code> <code class="o">=</code> <code class="n">clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>

<code class="c1"># Print best model</code>
<code class="nb">print</code><code class="p">(</code><code class="n">best_model</code><code class="o">.</code><code class="n">best_estimator_</code><code class="p">)</code></pre>
<pre data-type="programlisting">Fitting 5 folds for each of 2000 candidates, totalling 10000 fits
LogisticRegression(C=5.926151812475554, max_iter=500, penalty='l1',
                   solver='liblinear')</pre>
</div></section>
</div></section>
<section data-pdf-bookmark="12.6 Speeding Up Model Selection Using Algorithm-Specific Methods" data-type="sect1"><div class="sect1" id="speeding-up-model-selection-algorithm-specific-methods">
<h1>12.6 Speeding Up Model Selection Using Algorithm-Specific Methods</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id297">
<h2>Problem</h2>
<p>You need to <a data-primary="performance" data-secondary="algorithm-specific methods for model selection" data-type="indexterm" id="id1672"/><a data-primary="algorithm-specific methods, speeding up model selection" data-type="indexterm" id="id1673"/><a data-primary="model selection" data-secondary="algorithm-specific methods to speed up" data-type="indexterm" id="id1674"/>speed up model selection without using additional compute power.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id298">
<h2>Solution</h2>
<p>If you are <a data-primary="C hyperparameter" data-secondary="LogisticRegressionCV" data-type="indexterm" id="id1675"/><a data-primary="LogisticRegressionCV" data-type="indexterm" id="id1676"/>using a select number of learning algorithms, use
scikit-learn’s model-specific cross-validation hyperparameter tuning, <code>LogisticRegressionCV</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">linear_model</code><code class="p">,</code> <code class="n">datasets</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create cross-validated logistic regression</code>
<code class="n">logit</code> <code class="o">=</code> <code class="n">linear_model</code><code class="o">.</code><code class="n">LogisticRegressionCV</code><code class="p">(</code><code class="n">Cs</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">max_iter</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code>
       <code class="n">solver</code><code class="o">=</code><code class="s1">'liblinear'</code><code class="p">)</code>

<code class="c1"># Train model</code>
<code class="n">logit</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>

<code class="c1"># Print model</code>
<code class="nb">print</code><code class="p">(</code><code class="n">logit</code><code class="p">)</code></pre>
<pre data-type="programlisting">LogisticRegressionCV(Cs=100, max_iter=500, solver='liblinear')</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id1677">
<h2>Discussion</h2>
<p class="fix_tracking2">
Sometimes the characteristics of a learning algorithm allow us to
search for the best hyperparameters significantly faster than either
brute-force or randomized model search methods. In scikit-learn, many
learning algorithms (e.g., ridge, lasso, and elastic net regression) have
an algorithm-specific cross-validation method to take advantage of this.
For example, <code>LogisticRegression</code> is used to conduct a standard logistic
regression classifier, while <code>LogisticRegressionCV</code> implements an
efficient cross-validated logistic regression classifier that can identify the optimum value of the hyperparameter C.
</p>
<p>scikit-learn’s <code>LogisticRegressionCV</code> method includes a parameter <code>Cs</code>.
If supplied a list, <code>Cs</code> contains the candidate hyperparameter values to
select from. If supplied an integer, the parameter <code>Cs</code> generates a list of that number of candidate values. The candidate values are drawn logarithmically from a range between 0.0001 and 10,0000 (a range of reasonable values for C).</p>
<p>However, a major downside to <code>LogisticRegressionCV</code> is that it can only
search a range of values for C. In <a data-type="xref" href="#selecting-best-models-using-exhaustive-search">Recipe 12.1</a> our possible hyperparameter space included both C and another hyperparameter (the
regularization penalty norm). This limitation is common to many of
scikit-learn’s model-specific cross-validated approaches.</p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1678">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/uguJi">scikit-learn documentation: LogisticRegressionCV</a></p>
</li>
<li>
<p><a href="https://oreil.ly/6xfn6">scikit-learn documentation: Model specific cross-validation</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="12.7 Evaluating Performance After Model Selection" data-type="sect1"><div class="sect1" id="evaluating-performance-after-model-selection">
<h1>12.7 Evaluating Performance After Model Selection</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id515">
<h2>Problem</h2>
<p>You want to <a data-primary="performance" data-secondary="evaluation after selection" data-type="indexterm" id="ix_perf_eval_sel"/><a data-primary="cross-validation (CV) of ML models" data-secondary="performance evaluation after model selection" data-type="indexterm" id="ix_cross_val_perf_eval"/><a data-primary="model selection" data-secondary="performance evaluation after selection" data-type="indexterm" id="ix_mod_sel_perf_eval"/>evaluate the performance of a model found through model
selection.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id299">
<h2>Solution</h2>
<p>Use <a data-primary="nested cross-validation after model selection" data-type="indexterm" id="id1679"/>nested cross-validation to avoid biased evaluation:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">linear_model</code><code class="p">,</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">GridSearchCV</code><code class="p">,</code> <code class="n">cross_val_score</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create logistic regression</code>
<code class="n">logistic</code> <code class="o">=</code> <code class="n">linear_model</code><code class="o">.</code><code class="n">LogisticRegression</code><code class="p">(</code><code class="n">max_iter</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code> <code class="n">solver</code><code class="o">=</code><code class="s1">'liblinear'</code><code class="p">)</code>

<code class="c1"># Create range of 20 candidate values for C</code>
<code class="n">C</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">logspace</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">20</code><code class="p">)</code>

<code class="c1"># Create hyperparameter options</code>
<code class="n">hyperparameters</code> <code class="o">=</code> <code class="nb">dict</code><code class="p">(</code><code class="n">C</code><code class="o">=</code><code class="n">C</code><code class="p">)</code>

<code class="c1"># Create grid search</code>
<code class="n">gridsearch</code> <code class="o">=</code> <code class="n">GridSearchCV</code><code class="p">(</code><code class="n">logistic</code><code class="p">,</code> <code class="n">hyperparameters</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">,</code> <code class="n">verbose</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Conduct nested cross-validation and output the average score</code>
<code class="n">cross_val_score</code><code class="p">(</code><code class="n">gridsearch</code><code class="p">,</code> <code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code></pre>
<pre data-type="programlisting">0.9733333333333334</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id300">
<h2>Discussion</h2>
<p>Nested cross-validation during model selection is a difficult concept
for many people to grasp the first time. Remember that in k-fold
cross-validation, we train our model on <em>k–1</em> folds of the data, use this model to make predictions on the remaining fold, and then evaluate our model on how well its predictions compare to the true values. We then repeat this process <em>k</em> times.</p>
<p>In the model selection searches described in this chapter (i.e.,
<code>GridSearchCV</code> and <code>RandomizedSearchCV</code>), we used cross-validation to
evaluate which hyperparameter values produced the best models. However,
a nuanced and generally underappreciated problem arises: since we used
the data to select the best hyperparameter values, we cannot use that
same data to evaluate the model’s performance. The solution? Wrap the
cross-validation used for model search in another cross-validation! In
nested cross-validation, the “inner” cross-validation selects the best
model, while the “outer” cross-validation provides an unbiased
evaluation of the model’s performance. In our solution, the inner
cross-validation is our <code>GridSearchCV</code> object, which we then wrap in an
outer cross-validation using <code>cross_val_score</code>.</p>
<p>If you are confused, try a simple experiment. First, set <code>verbose=1</code> so
we can see what is happening:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">gridsearch</code> <code class="o">=</code> <code class="n">GridSearchCV</code><code class="p">(</code><code class="n">logistic</code><code class="p">,</code> <code class="n">hyperparameters</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">verbose</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code></pre>
<p>Next, run <code>gridsearch.fit(features, target)</code>, which is our inner
cross-validation used to find the best model:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">best_model</code> <code class="o">=</code> <code class="n">gridsearch</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
<pre data-type="programlisting">Fitting 5 folds for each of 20 candidates, totalling 100 fits</pre>
<p>From the output you can see the inner cross-validation trained 20
candidate models five times, totaling 100 models. Next, nest <code>clf</code> inside
a new cross-validation, which defaults to five folds:<a data-primary="" data-startref="ix_mod_sel_ch12" data-type="indexterm" id="id1680"/><a data-primary="" data-startref="ix_perf_eval_sel" data-type="indexterm" id="id1681"/><a data-primary="" data-startref="ix_cross_val_perf_eval" data-type="indexterm" id="id1682"/><a data-primary="" data-startref="ix_mod_sel_perf_eval" data-type="indexterm" id="id1683"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="n">scores</code> <code class="o">=</code> <code class="n">cross_val_score</code><code class="p">(</code><code class="n">gridsearch</code><code class="p">,</code> <code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
<pre data-type="programlisting">Fitting 5 folds for each of 20 candidates, totalling 100 fits
Fitting 5 folds for each of 20 candidates, totalling 100 fits
Fitting 5 folds for each of 20 candidates, totalling 100 fits
Fitting 5 folds for each of 20 candidates, totalling 100 fits
Fitting 5 folds for each of 20 candidates, totalling 100 fits</pre>
<p>The output shows that the inner cross-validation trained 20 models five
times to find the best model, and this model was evaluated using an
outer five-fold cross-validation, creating a total of 500 models
trained.</p>
</div></section>
</div></section>
</div></section></div></body></html>