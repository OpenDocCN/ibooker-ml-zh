- en: Chapter 16\. Logistic Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 16.0 Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Despite being called a regression, *logistic regression* is actually a widely
    used supervised classification technique. Logistic regression (and its extensions,
    like multinomial logistic regression) is a straightforward, well-understood approach
    to predicting the probability that an observation is of a certain class. In this
    chapter, we will cover training a variety of classifiers using logistic regression
    in scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: 16.1 Training a Binary Classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to train a simple classifier model.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Train a logistic regression in scikit-learn using `LogisticRegression`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Despite having “regression” in its name, a logistic regression is actually
    a widely used binary classifier (i.e., the target vector can take only two values).
    In a logistic regression, a linear model (e.g., *β[0] + β[1]x*) is included in
    a logistic (also called sigmoid) function, <math display="inline"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><mi>z</mi></mrow></msup></mrow></mfrac></math> , such that:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>=</mo> <mn>1</mn> <mo>∣</mo> <mi>X</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn>
    <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub> <mi>x</mi><mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math display="inline"><mi>P</mi><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>=</mo><mn>1</mn><mo>∣</mo><mi>X</mi><mo>)</mo></mrow></math> is the probability
    of the <math display="inline"><mi>i</mi></math>th observation’s target value,
    <math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math>, being class 1;
    <math display="inline"><mi>X</mi></math> is the training data; <math display="inline"><msub><mi>β</mi><mn>0</mn></msub></math>
    and <math display="inline"><msub><mi>β</mi><mn>1</mn></msub></math> are the parameters
    to be learned; and <math display="inline"><mi>e</mi></math> is Euler’s number.
    The effect of the logistic function is to constrain the value of the function’s
    output to between 0 and 1, so that it can be interpreted as a probability. If
    <math display="inline"><mi>P</mi><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>=</mo><mn>1</mn><mo>∣</mo><mi>X</mi><mo>)</mo></mrow></math> is greater than
    0.5, class 1 is predicted; otherwise, class 0 is predicted.
  prefs: []
  type: TYPE_NORMAL
- en: 'In scikit-learn, we can train a logistic regression model using `LogisticRegression`.
    Once it is trained, we can use the model to predict the class of new observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, our observation was predicted to be class 1. Additionally,
    we can see the probability that an observation is a member of each class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Our observation had a 17.7% chance of being class 0 and an 82.2% chance of being
    class 1.
  prefs: []
  type: TYPE_NORMAL
- en: 16.2 Training a Multiclass Classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given more than two classes, you need to train a classifier model.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Train a logistic regression in scikit-learn with `LogisticRegression` using
    one-vs-rest or multinomial methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On their own, logistic regressions are only binary classifiers, meaning they
    cannot handle target vectors with more than two classes. However, two clever extensions
    to logistic regression do just that. First, in *one-vs-rest* logistic regression
    (OvR) a separate model is trained for each class predicted, whether an observation
    is that class or not (thus making it a binary classification problem). It assumes
    that each classification problem (e.g., class 0 or not) is independent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, in *multinomial logistic regression* (MLR), the logistic function
    we saw in [Recipe 16.1](#training_binary_classifier) is replaced with a softmax
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>=</mo> <mi>k</mi> <mo>∣</mo> <mi>X</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><msup><mi>e</mi>
    <mrow><msub><mi>β</mi> <mi>k</mi></msub> <msub><mi>x</mi> <mi>i</mi></msub></mrow></msup>
    <mrow><msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>K</mi></msubsup>
    <msup><mi>e</mi> <mrow><msub><mi>β</mi> <mi>j</mi></msub> <msub><mi>x</mi> <mi>i</mi></msub></mrow></msup></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math display="inline"><mi>P</mi><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>=</mo><mi>k</mi><mo>∣</mo><mi>X</mi><mo>)</mo></mrow></math> is the probability
    of the <math display="inline"><mi>i</mi></math> th observation’s target value,
    <math display="inline"><msub><mi>y</mi> <mi>i</mi></msub></math> , being in class
    <math display="inline"><mi>k</mi></math> , and <math display="inline"><mi>K</mi></math>
    is the total number of classes. One practical advantage of MLR is that its predicted
    probabilities using the `predict_proba` method are more reliable (i.e., better
    calibrated).
  prefs: []
  type: TYPE_NORMAL
- en: When using `LogisticRegression` we can select which of the two techniques we
    want, with OvR (`ovr`) being the default argument. We can switch to MLR by setting
    the argument to `multinomial`.
  prefs: []
  type: TYPE_NORMAL
- en: 16.3 Reducing Variance Through Regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to reduce the variance of your logistic regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Tune the regularization strength hyperparameter, `C`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Regularization* is a method of penalizing complex models to reduce their variance.
    Specifically, a penalty term is added to the loss function we are trying to minimize,
    typically the L1 and L2 penalties. In the L1 penalty:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>α</mi> <munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>p</mi></munderover> <mfenced close="|" open="|" separators=""><msub><mover
    accent="true"><mi>β</mi> <mo>^</mo></mover> <mi>j</mi></msub></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'where <math display="inline"><msub><mover accent="true"><mi>β</mi><mo>^</mo></mover>
    <mi>j</mi></msub></math> is the parameters of the <math display="inline"><mi>j</mi></math>th
    of <math display="inline"><mi>p</mi></math> features being learned, and <math
    display="inline"><mi>α</mi></math> is a hyperparameter denoting the regularization
    strength. With the L2 penalty:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>α</mi> <munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>p</mi></munderover> <msup><msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover>
    <mi>j</mi></msub> <mn>2</mn></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Higher values of <math display="inline"><mi>α</mi></math> increase the penalty
    for larger parameter values (i.e., more complex models). scikit-learn follows
    the common method of using <math display="inline"><mi>C</mi></math> instead of
    <math display="inline"><mi>α</mi></math> where <math display="inline"><mi>C</mi></math>
    is the inverse of the regularization strength: <math display="inline"><mrow><mi>C</mi><mo>=</mo><mfrac><mn>1</mn>
    <mi>α</mi></mfrac></mrow></math>. To reduce variance while using logistic regression,
    we can treat <math display="inline"><mi>C</mi></math> as a hyperparameter to be
    tuned to find the value of <math display="inline"><mi>C</mi></math> that creates
    the best model. In scikit-learn we can use the `LogisticRegressionCV` class to
    efficiently tune <math display="inline"><mi>C</mi></math>. `LogisticRegressionCV`’s
    parameter `Cs` can either accept a range of values for <math display="inline"><mi>C</mi></math>
    to search over (if a list of floats is supplied as an argument) or, if supplied
    an integer, will generate a list of that many candidate values drawn from a logarithmic
    scale between –10,000 and 10,000.'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, `LogisticRegressionCV` does not allow us to search over different
    penalty terms. To do this we have to use the less efficient model selection techniques
    discussed in [Chapter 12](ch12.xhtml#model-selection).
  prefs: []
  type: TYPE_NORMAL
- en: 16.4 Training a Classifier on Very Large Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to train a simple classifier model on a very large set of data.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Train a logistic regression in scikit-learn with `LogisticRegression` using
    the *stochastic average gradient* (SAG) solver:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: scikit-learn’s `LogisticRegression` offers a number of techniques for training
    a logistic regression, called *solvers*. Most of the time scikit-learn will select
    the best solver automatically for us or warn us that we cannot do something with
    that solver. However, there is one particular case we should be aware of.
  prefs: []
  type: TYPE_NORMAL
- en: While an exact explanation is beyond the bounds of this book (for more information
    see Mark Schmidt’s slides in the [“See Also”](#see-also-ch16a) section of this
    recipe), stochastic average gradient descent allows us to train a model much faster
    than other solvers when our data is very large. However, it is also very sensitive
    to feature scaling, so standardizing our features is particularly important. We
    can set our learning algorithm to use this solver by setting `solver="sag"`.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Minimizing Finite Sums with the Stochastic Average Gradient Algorithm, Mark
    Schmidt](https://oreil.ly/K5rEG)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 16.5 Handling Imbalanced Classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to train a simple classifier model.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Train a logistic regression in scikit-learn using `LogisticRegression`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Like many other learning algorithms in scikit-learn, `LogisticRegression` comes
    with a built-in method of handling imbalanced classes. If we have highly imbalanced
    classes and have not addressed it during preprocessing, we have the option of
    using the `class_weight` parameter to weight the classes to make certain we have
    a balanced mix of each class. Specifically, the `balanced` argument will automatically
    weigh classes inversely proportional to their frequency:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>w</mi> <mi>j</mi></msub> <mo>=</mo> <mfrac><mi>n</mi>
    <mrow><mi>k</mi><msub><mi>n</mi> <mi>j</mi></msub></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math display="inline"><msub><mi>w</mi><mi>j</mi></msub></math> is the
    weight to class <math display="inline"><mi>j</mi></math>, <math display="inline"><mi>n</mi></math>
    is the number of observations, <math display="inline"><msub><mi>n</mi><mi>j</mi></msub></math>
    is the number of observations in class <math display="inline"><mi>j</mi></math>,
    and <math display="inline"><mi>k</mi></math> is the total number of classes.
  prefs: []
  type: TYPE_NORMAL
