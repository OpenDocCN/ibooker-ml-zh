- en: Chapter 16\. Logistic Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 16.0 Introduction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Despite being called a regression, *logistic regression* is actually a widely
    used supervised classification technique. Logistic regression (and its extensions,
    like multinomial logistic regression) is a straightforward, well-understood approach
    to predicting the probability that an observation is of a certain class. In this
    chapter, we will cover training a variety of classifiers using logistic regression
    in scikit-learn.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 16.1 Training a Binary Classifier
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to train a simple classifier model.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Train a logistic regression in scikit-learn using `LogisticRegression`:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Discussion
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Despite having “regression” in its name, a logistic regression is actually
    a widely used binary classifier (i.e., the target vector can take only two values).
    In a logistic regression, a linear model (e.g., *β[0] + β[1]x*) is included in
    a logistic (also called sigmoid) function, <math display="inline"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><mi>z</mi></mrow></msup></mrow></mfrac></math> , such that:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>=</mo> <mn>1</mn> <mo>∣</mo> <mi>X</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn>
    <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub> <mi>x</mi><mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>=</mo> <mn>1</mn> <mo>∣</mo> <mi>X</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn>
    <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub> <mi>x</mi><mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>
- en: where <math display="inline"><mi>P</mi><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>=</mo><mn>1</mn><mo>∣</mo><mi>X</mi><mo>)</mo></mrow></math> is the probability
    of the <math display="inline"><mi>i</mi></math>th observation’s target value,
    <math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math>, being class 1;
    <math display="inline"><mi>X</mi></math> is the training data; <math display="inline"><msub><mi>β</mi><mn>0</mn></msub></math>
    and <math display="inline"><msub><mi>β</mi><mn>1</mn></msub></math> are the parameters
    to be learned; and <math display="inline"><mi>e</mi></math> is Euler’s number.
    The effect of the logistic function is to constrain the value of the function’s
    output to between 0 and 1, so that it can be interpreted as a probability. If
    <math display="inline"><mi>P</mi><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>=</mo><mn>1</mn><mo>∣</mo><mi>X</mi><mo>)</mo></mrow></math> is greater than
    0.5, class 1 is predicted; otherwise, class 0 is predicted.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'In scikit-learn, we can train a logistic regression model using `LogisticRegression`.
    Once it is trained, we can use the model to predict the class of new observations:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In this example, our observation was predicted to be class 1. Additionally,
    we can see the probability that an observation is a member of each class:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Our observation had a 17.7% chance of being class 0 and an 82.2% chance of being
    class 1.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 16.2 Training a Multiclass Classifier
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given more than two classes, you need to train a classifier model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Train a logistic regression in scikit-learn with `LogisticRegression` using
    one-vs-rest or multinomial methods:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Discussion
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On their own, logistic regressions are only binary classifiers, meaning they
    cannot handle target vectors with more than two classes. However, two clever extensions
    to logistic regression do just that. First, in *one-vs-rest* logistic regression
    (OvR) a separate model is trained for each class predicted, whether an observation
    is that class or not (thus making it a binary classification problem). It assumes
    that each classification problem (e.g., class 0 or not) is independent.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 单独来看，逻辑回归只是二元分类器，意味着它不能处理目标向量超过两个类。然而，逻辑回归的两个巧妙扩展可以做到。首先，在 *一对多* 逻辑回归（OvR）中，为每个预测的类别训练一个单独的模型，无论观察结果是否属于该类（从而将其转化为二元分类问题）。它假设每个分类问题（例如，类别
    0 或非类别 0）是独立的。
- en: 'Alternatively, in *multinomial logistic regression* (MLR), the logistic function
    we saw in [Recipe 16.1](#training_binary_classifier) is replaced with a softmax
    function:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，*多项式逻辑回归*（MLR）中，我们在 [配方 16.1](#training_binary_classifier) 中看到的逻辑函数被 softmax
    函数取代：
- en: <math display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>=</mo> <mi>k</mi> <mo>∣</mo> <mi>X</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><msup><mi>e</mi>
    <mrow><msub><mi>β</mi> <mi>k</mi></msub> <msub><mi>x</mi> <mi>i</mi></msub></mrow></msup>
    <mrow><msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>K</mi></msubsup>
    <msup><mi>e</mi> <mrow><msub><mi>β</mi> <mi>j</mi></msub> <msub><mi>x</mi> <mi>i</mi></msub></mrow></msup></mrow></mfrac></mrow></math>
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>=</mo> <mi>k</mi> <mo>∣</mo> <mi>X</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><msup><mi>e</mi>
    <mrow><msub><mi>β</mi> <mi>k</mi></msub> <msub><mi>x</mi> <mi>i</mi></msub></mrow></msup>
    <mrow><msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>K</mi></msubsup>
    <msup><mi>e</mi> <mrow><msub><mi>β</mi> <mi>j</mi></msub> <msub><mi>x</mi> <mi>i</mi></msub></mrow></msup></mrow></mfrac></mrow></math>
- en: where <math display="inline"><mi>P</mi><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>=</mo><mi>k</mi><mo>∣</mo><mi>X</mi><mo>)</mo></mrow></math> is the probability
    of the <math display="inline"><mi>i</mi></math> th observation’s target value,
    <math display="inline"><msub><mi>y</mi> <mi>i</mi></msub></math> , being in class
    <math display="inline"><mi>k</mi></math> , and <math display="inline"><mi>K</mi></math>
    is the total number of classes. One practical advantage of MLR is that its predicted
    probabilities using the `predict_proba` method are more reliable (i.e., better
    calibrated).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math display="inline"><mi>P</mi><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>=</mo><mi>k</mi><mo>∣</mo><mi>X</mi><mo>)</mo></mrow></math> 是第 <math display="inline"><mi>i</mi></math>
    个观察目标值 <math display="inline"><msub><mi>y</mi> <mi>i</mi></msub></math> 属于类别 <math
    display="inline"><mi>k</mi></math> 的概率，<math display="inline"><mi>K</mi></math>
    是总类别数。MLR 的一个实际优势是，使用 `predict_proba` 方法预测的概率更可靠（即更好地校准）。
- en: When using `LogisticRegression` we can select which of the two techniques we
    want, with OvR (`ovr`) being the default argument. We can switch to MLR by setting
    the argument to `multinomial`.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 `LogisticRegression` 时，我们可以选择我们想要的两种技术之一，OvR (`ovr`) 是默认参数。我们可以通过设置参数为 `multinomial`
    切换到 MLR。
- en: 16.3 Reducing Variance Through Regularization
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 16.3 通过正则化减少方差
- en: Problem
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You need to reduce the variance of your logistic regression model.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要减少逻辑回归模型的方差。
- en: Solution
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Tune the regularization strength hyperparameter, `C`:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 调整正则化强度超参数 `C`：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Discussion
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: '*Regularization* is a method of penalizing complex models to reduce their variance.
    Specifically, a penalty term is added to the loss function we are trying to minimize,
    typically the L1 and L2 penalties. In the L1 penalty:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*正则化* 是一种惩罚复杂模型以减少其方差的方法。具体来说，是向我们试图最小化的损失函数中添加一个惩罚项，通常是 L1 和 L2 惩罚。在 L1 惩罚中：'
- en: <math display="block"><mrow><mi>α</mi> <munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>p</mi></munderover> <mfenced close="|" open="|" separators=""><msub><mover
    accent="true"><mi>β</mi> <mo>^</mo></mover> <mi>j</mi></msub></mfenced></mrow></math>
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>α</mi> <munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>p</mi></munderover> <mfenced close="|" open="|" separators=""><msub><mover
    accent="true"><mi>β</mi> <mo>^</mo></mover> <mi>j</mi></msub></mfenced></mrow></math>
- en: 'where <math display="inline"><msub><mover accent="true"><mi>β</mi><mo>^</mo></mover>
    <mi>j</mi></msub></math> is the parameters of the <math display="inline"><mi>j</mi></math>th
    of <math display="inline"><mi>p</mi></math> features being learned, and <math
    display="inline"><mi>α</mi></math> is a hyperparameter denoting the regularization
    strength. With the L2 penalty:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math display="inline"><msub><mover accent="true"><mi>β</mi><mo>^</mo></mover>
    <mi>j</mi></msub></math> 是正在学习的第 <math display="inline"><mi>j</mi></math> 个特征的参数，<math
    display="inline"><mi>α</mi></math> 是表示正则化强度的超参数。使用 L2 惩罚时：
- en: <math display="block"><mrow><mi>α</mi> <munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>p</mi></munderover> <msup><msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover>
    <mi>j</mi></msub> <mn>2</mn></msup></mrow></math>
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>α</mi> <munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>p</mi></munderover> <msup><msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover>
    <mi>j</mi></msub> <mn>2</mn></msup></mrow></math>
- en: 'Higher values of <math display="inline"><mi>α</mi></math> increase the penalty
    for larger parameter values (i.e., more complex models). scikit-learn follows
    the common method of using <math display="inline"><mi>C</mi></math> instead of
    <math display="inline"><mi>α</mi></math> where <math display="inline"><mi>C</mi></math>
    is the inverse of the regularization strength: <math display="inline"><mrow><mi>C</mi><mo>=</mo><mfrac><mn>1</mn>
    <mi>α</mi></mfrac></mrow></math>. To reduce variance while using logistic regression,
    we can treat <math display="inline"><mi>C</mi></math> as a hyperparameter to be
    tuned to find the value of <math display="inline"><mi>C</mi></math> that creates
    the best model. In scikit-learn we can use the `LogisticRegressionCV` class to
    efficiently tune <math display="inline"><mi>C</mi></math>. `LogisticRegressionCV`’s
    parameter `Cs` can either accept a range of values for <math display="inline"><mi>C</mi></math>
    to search over (if a list of floats is supplied as an argument) or, if supplied
    an integer, will generate a list of that many candidate values drawn from a logarithmic
    scale between –10,000 and 10,000.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 较高的 <math display="inline"><mi>α</mi></math> 值增加了较大参数值的惩罚（即更复杂的模型）。scikit-learn
    遵循使用 <math display="inline"><mi>C</mi></math> 而不是 <math display="inline"><mi>α</mi></math>
    的常见方法，其中 <math display="inline"><mi>C</mi></math> 是正则化强度的倒数：<math display="inline"><mrow><mi>C</mi><mo>=</mo><mfrac><mn>1</mn>
    <mi>α</mi></mfrac></mrow></math>。为了在使用逻辑回归时减少方差，我们可以将 <math display="inline"><mi>C</mi></math>
    视为一个超参数，用于调整以找到创建最佳模型的 <math display="inline"><mi>C</mi></math> 的值。在 scikit-learn
    中，我们可以使用 `LogisticRegressionCV` 类来高效地调整 <math display="inline"><mi>C</mi></math>。`LogisticRegressionCV`
    的参数 `Cs` 可以接受一个值范围供 <math display="inline"><mi>C</mi></math> 搜索（如果提供一个浮点数列表作为参数），或者如果提供一个整数，则会在对数尺度的
    -10,000 到 10,000 之间生成相应数量的候选值列表。
- en: Unfortunately, `LogisticRegressionCV` does not allow us to search over different
    penalty terms. To do this we have to use the less efficient model selection techniques
    discussed in [Chapter 12](ch12.xhtml#model-selection).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，`LogisticRegressionCV` 不允许我们在不同的惩罚项上搜索。为了做到这一点，我们必须使用在 [第 12 章](ch12.xhtml#model-selection)
    讨论的效率较低的模型选择技术。
- en: 16.4 Training a Classifier on Very Large Data
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 16.4 在非常大的数据上训练分类器
- en: Problem
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You need to train a simple classifier model on a very large set of data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要在非常大的数据集上训练一个简单的分类器模型。
- en: Solution
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Train a logistic regression in scikit-learn with `LogisticRegression` using
    the *stochastic average gradient* (SAG) solver:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 *stochastic average gradient*（SAG）求解器在 scikit-learn 中训练逻辑回归：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Discussion
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: scikit-learn’s `LogisticRegression` offers a number of techniques for training
    a logistic regression, called *solvers*. Most of the time scikit-learn will select
    the best solver automatically for us or warn us that we cannot do something with
    that solver. However, there is one particular case we should be aware of.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 的 `LogisticRegression` 提供了一些训练逻辑回归的技术，称为 *solvers*。大多数情况下，scikit-learn
    会自动为我们选择最佳的求解器，或者警告我们无法使用某个求解器来做某事。然而，有一个特定的情况我们应该注意。
- en: While an exact explanation is beyond the bounds of this book (for more information
    see Mark Schmidt’s slides in the [“See Also”](#see-also-ch16a) section of this
    recipe), stochastic average gradient descent allows us to train a model much faster
    than other solvers when our data is very large. However, it is also very sensitive
    to feature scaling, so standardizing our features is particularly important. We
    can set our learning algorithm to use this solver by setting `solver="sag"`.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管详细解释超出了本书的范围（更多信息请参见 Mark Schmidt 在本章 “参见” 部分的幻灯片），随机平均梯度下降使我们能够在数据非常大时比其他求解器更快地训练模型。然而，它对特征缩放非常敏感，因此标准化我们的特征特别重要。我们可以通过设置
    `solver="sag"` 来让我们的学习算法使用这个求解器。
- en: See Also
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Minimizing Finite Sums with the Stochastic Average Gradient Algorithm, Mark
    Schmidt](https://oreil.ly/K5rEG)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用随机平均梯度算法最小化有限和，Mark Schmidt](https://oreil.ly/K5rEG)'
- en: 16.5 Handling Imbalanced Classes
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 16.5 处理不平衡的类
- en: Problem
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You need to train a simple classifier model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要训练一个简单的分类器模型。
- en: Solution
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Train a logistic regression in scikit-learn using `LogisticRegression`:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 中的 `LogisticRegression` 训练逻辑回归模型：
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Discussion
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Like many other learning algorithms in scikit-learn, `LogisticRegression` comes
    with a built-in method of handling imbalanced classes. If we have highly imbalanced
    classes and have not addressed it during preprocessing, we have the option of
    using the `class_weight` parameter to weight the classes to make certain we have
    a balanced mix of each class. Specifically, the `balanced` argument will automatically
    weigh classes inversely proportional to their frequency:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 `scikit-learn` 中许多其他学习算法一样，`LogisticRegression` 自带处理不平衡类别的方法。如果我们的类别高度不平衡，在预处理过程中没有处理它，我们可以使用
    `class_weight` 参数来加权这些类别，以确保每个类别的混合平衡。具体地，`balanced` 参数将自动根据其频率的倒数加权类别：
- en: <math display="block"><mrow><msub><mi>w</mi> <mi>j</mi></msub> <mo>=</mo> <mfrac><mi>n</mi>
    <mrow><mi>k</mi><msub><mi>n</mi> <mi>j</mi></msub></mrow></mfrac></mrow></math>
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>w</mi> <mi>j</mi></msub> <mo>=</mo> <mfrac><mi>n</mi>
    <mrow><mi>k</mi><msub><mi>n</mi> <mi>j</mi></msub></mrow></mfrac></mrow></math>
- en: where <math display="inline"><msub><mi>w</mi><mi>j</mi></msub></math> is the
    weight to class <math display="inline"><mi>j</mi></math>, <math display="inline"><mi>n</mi></math>
    is the number of observations, <math display="inline"><msub><mi>n</mi><mi>j</mi></msub></math>
    is the number of observations in class <math display="inline"><mi>j</mi></math>,
    and <math display="inline"><mi>k</mi></math> is the total number of classes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math display="inline"><msub><mi>w</mi><mi>j</mi></msub></math> 是类别 <math
    display="inline"><mi>j</mi></math> 的权重，<math display="inline"><mi>n</mi></math>
    是观测数量，<math display="inline"><msub><mi>n</mi><mi>j</mi></msub></math> 是类别 <math
    display="inline"><mi>j</mi></math> 中的观测数量，<math display="inline"><mi>k</mi></math>
    是总类别数量。
