- en: Chapter 9\. Semisupervised Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章. 半监督学习
- en: Until now, we have viewed supervised learning and unsupervised learning as two
    separate and distinct branches of machine learning. Supervised learning is appropriate
    when our dataset is labeled, and unsupervised learning is necessary when our dataset
    is unlabeled.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们将监督学习和无监督学习视为机器学习的两个独立而不同的分支。当我们的数据集有标签时，适合使用监督学习，当数据集没有标签时，需要使用无监督学习。
- en: In the real world, the distinction is not quite so clear. Datasets are usually
    partially labeled, and we want to efficiently label the unlabeled observations
    while leveraging the information in the labeled set. With supervised learning,
    we would have to toss away the majority of the dataset because it is unlabeled.
    With unsupervised learning, we would have the majority of the data to work with
    but would not know how to take advantage of the few labels we have.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，区分并不是那么清晰。数据集通常是部分标记的，我们希望在利用标记集中的信息的同时，有效地标记未标记的观察结果。使用监督学习，我们必须丢弃大多数未标记的数据集。使用无监督学习，我们会有大部分数据可供使用，但不知道如何利用我们拥有的少量标记。
- en: The field of *semisupervised learning* blends the benefits of both supervised
    and unsupervised learning, taking advantage of the few labels that are available
    to uncover structure in a dataset and help label the rest.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*半监督学习*领域融合了监督学习和无监督学习的优点，利用少量可用标记来揭示数据集的结构并帮助标记其余部分。'
- en: We will continue to use the credit card transactions dataset in this chapter
    to showcase semisupervised learning.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续使用信用卡交易数据集来展示半监督学习。
- en: Data Preparation
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'As before, let’s load in the necessary libraries and prepare the data. This
    should be pretty familiar by now:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 像之前一样，让我们加载必要的库并准备数据。现在这应该很熟悉了：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As before, we will generate a training and test set. But we will drop 90% of
    the fraudulent credit card transactions from the training set to simulate how
    to work with *partially* *labeled* datasets.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 像之前一样，我们将生成一个训练集和一个测试集。但是我们会从训练集中删除90%的欺诈信用卡交易，以模拟如何处理*部分*有*标记*的数据集。
- en: 'While this may seem like a very aggressive move, real-world problems involving
    payment fraud have similarly low incidences of fraud (as little as 1 fraud per
    10,000 cases). By removing 90% of the labels from the training set, we are simulating
    this type of phenomenon:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这看起来可能是一个非常激进的举措，但涉及支付欺诈的真实世界问题同样具有很低的欺诈率（每10,000例中可能只有1例欺诈）。通过从训练集中删除90%的标签，我们正在模拟这种现象：
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We will also reuse the `anomalyScores` and `plotResults` functions:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将重用`anomalyScores`和`plotResults`函数：
- en: '[PRE2]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Finally, here’s a new function called `precisionAnalysis` to help us assess
    the precision of our models at a certain level of recall. Specifically, we will
    determine what the model’s precision is to catch 75% of the fraudulent credit
    card transactions in the test set. The higher the precision, the better the model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这里有一个新函数叫做`precisionAnalysis`，帮助我们在某个召回率水平上评估模型的精度。具体来说，我们将确定模型在测试集中捕捉到75%的欺诈信用卡交易的精度。精度越高，模型越好。
- en: 'This is a reasonable benchmark. In other words, we want to catch 75% of the
    fraud with as high of a precision as possible. If we do not achieve a high enough
    precision, we will unnecessarily reject good credit card transactions, potentially
    angering our customer base:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个合理的基准。换句话说，我们希望能够捕捉到75%的欺诈行为，并且尽可能高精度。如果我们没有达到足够高的精度，我们将不必要地拒绝良好的信用卡交易，可能会激怒我们的客户群体：
- en: '[PRE4]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Supervised Model
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督模型
- en: To benchmark our semisupervised model, let’s first see how well a supervised
    model and a unsupervised model do in isolation.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对我们的半监督模型进行基准测试，让我们先看看单独使用监督模型和无监督模型的效果如何。
- en: 'We will start with a supervised learning solution based on light gradient boosting
    like the one that performed best in [Chapter 2](ch02.html#Chapter_2). We will
    use *k*-fold cross-validation to create five folds:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从基于轻量梯度提升的监督学习解决方案开始，就像在[第2章](ch02.html#Chapter_2)中表现最佳的那个。我们将使用*k*-折交叉验证来创建五个折叠：
- en: '[PRE5]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s next set the parameters for gradient boosting:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，设定梯度提升的参数：
- en: '[PRE6]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, let’s train the algorithm:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们训练算法：
- en: '[PRE7]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We will now use this model to predict the fraud on the test set of credit card
    transactions.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用这个模型来预测信用卡交易测试集上的欺诈行为。
- en: '[Figure 9-1](#results_of_supervised_model) displays the results.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-1](#results_of_supervised_model)展示了结果。'
- en: '![Results of Supervised Model](assets/hulp_0901.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![监督模型的结果](assets/hulp_0901.png)'
- en: Figure 9-1\. Results of supervised model
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-1\. 监督模型的结果
- en: The average precision on the test based on the precision-recall curve is 0.62\.
    To catch 75% of the fraud, we have a precision of just 0.5%.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 基于精度-召回曲线的测试平均精度为0.62。要捕捉75%的欺诈案例，我们的精度仅为0.5%。
- en: Unsupervised Model
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督模型
- en: Now let’s build a fraud detection solution using unsupervised learning. Specifically,
    we will build a sparse two-layer overcomplete autoencoder with a linear activation
    function. We will have 40 nodes in the hidden layer and a dropout of 2%.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用无监督学习构建欺诈检测解决方案。具体来说，我们将构建一个稀疏的两层过完备自动编码器，使用线性激活函数。我们将在隐藏层中有40个节点，并且2%的丢失率。
- en: However, we will adjust our training set by *oversampling* the number of fraudulent
    cases we have. Oversampling is a technique used to adjust the class distribution
    in a given dataset. We want to add more fraudulent cases to our dataset so that
    the autoencoder we train has an easier time separating the normal/nonfraudulent
    transactions from the abnormal/fraudulent ones.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们将通过*过采样*我们拥有的欺诈案例来调整我们的训练集。过采样是一种用于调整给定数据集中类分布的技术。我们希望向我们的数据集中添加更多的欺诈案例，以便我们训练的自动编码器更容易将正常/非欺诈交易与异常/欺诈交易区分开来。
- en: Recall that after having dropped 90% of the fraudulent cases from the training
    set, we have just 33 fraudulent cases left. We will take the 33 fraudulent cases,
    duplicate these 100 times, and then append them to the training set. We will also
    keep copies of the nonoversampled training set so we can use them for the rest
    of our machine learning pipeline.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在从训练集中删除90%欺诈案例后，我们只剩下33个欺诈案例。我们将取这33个欺诈案例，复制它们100次，然后添加到训练集中。我们还会保留非过采样训练集的副本，以便在机器学习流水线的其余部分使用它们。
- en: 'Remember we do not touch the test set—there is no oversampling with the test
    set, just the training set:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们不会触及测试集——测试集不进行过采样，只有训练集进行过采样：
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s now train our autoencoder:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们训练我们的自动编码器：
- en: '[PRE9]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[Figure 9-2](#results_of_unsupervised_model) displays the results.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-2](#results_of_unsupervised_model) 展示了结果。'
- en: '![Results of Unsupervised Model](assets/hulp_0902.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![无监督模型的结果](assets/hulp_0902.png)'
- en: Figure 9-2\. Results of unsupervised model
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-2\. 无监督模型的结果
- en: The average precision on the test based on the precision-recall curve is 0.57\.
    To catch 75% of the fraud, we have a precision of just 45%. While the average
    precision of the unsupervised solution is similar to the average precision of
    the supervised solution, the precision of 45% at 75% recall is better.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 基于精度-召回曲线的测试平均精度为0.57。要捕捉75%的欺诈案例，我们的精度仅为45%。虽然无监督解决方案的平均精度与监督解决方案相似，但在75%召回率下的45%精度更佳。
- en: However, the unsupervised solution by itself is still not great.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，单独的无监督解决方案仍然不是很好。
- en: Semisupervised Model
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 半监督模型
- en: Now, let’s take the representation learned by the autoencoder (the hidden layer),
    combine it with the original training set, and feed this into the gradient boosting
    algorithm. This a semisupervised approach, taking advantage of supervised and
    unsupervised learning.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们取自动编码器学到的表示（隐藏层），将其与原始训练集结合起来，并将其馈送到梯度提升算法中。这是一种半监督方法，充分利用了监督和无监督学习。
- en: 'To get the hidden layer, we call the `Model()` class from the Keras API and
    use the `get_layer` function:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取隐藏层，我们从Keras API中调用`Model()`类并使用`get_layer`函数：
- en: '[PRE10]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let’s store these autoencoder representations into DataFrames and then combine
    them with the original training set:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这些自动编码器表示存储到DataFrame中，然后将它们与原始训练集结合起来：
- en: '[PRE11]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We will now train the gradient boosting model on this new training set of 69
    features (29 from the original dataset and 40 from the autoencoder’s representation):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将在这个新的69个特征的训练集上训练梯度提升模型（29个来自原始数据集，40个来自自动编码器的表示）：
- en: '[PRE12]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[Figure 9-3](#results_of_semi_supervised_model) displays the results.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-3](#results_of_semi_supervised_model) 展示了结果。'
- en: '![Results of Semisupervised Model](assets/hulp_0903.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![半监督模型的结果](assets/hulp_0903.png)'
- en: Figure 9-3\. Results of semisupervised model
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-3\. 半监督模型的结果
- en: The average precision on the test set based on the precision-recall curve is
    0.78\. This is a good bit higher than both the supervised and the unsupervised
    models.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 基于精度-召回曲线的测试集平均精度为0.78。这比监督和无监督模型都高出许多。
- en: To catch 75% of the fraud, we have a precision of 92%. This is a considerable
    improvement. With this level of precision, the payment processor should feel comfortable
    rejecting transactions that the model flags as potentially fraudulent. Less than
    one in ten will be wrong, and we will catch approximately 75% of the fraud.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要捕获75%的欺诈，我们的精度达到了92%。这是一个显著的改进。在这种精度水平下，支付处理器应该对拒绝模型标记为潜在欺诈的交易感到放心。不到十分之一会出错，而我们将捕获大约75%的欺诈行为。
- en: The Power of Supervised and Unsupervised
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习和非监督学习的威力
- en: In this semisupervised credit card fraud detection solution, both supervised
    learning and unsupervised learning have important roles to play. One way to explore
    this is by analyzing which features the final gradient boosting model found most
    important.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种半监督信用卡欺诈检测解决方案中，监督学习和非监督学习都发挥了重要作用。探索的一种方式是分析最终梯度提升模型发现的最重要的特征是哪些。
- en: 'Let’s find and store those feature importance values from the model we just
    trained:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从刚刚训练的模型中找出并存储这些特征重要性数值：
- en: '[PRE13]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[Table 9-1](#feature_importantce_from_semi_supervised_model) shows some of
    the most important features, sorted in descending order.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 9-1](#feature_importantce_from_semi_supervised_model)显示了按降序排列的一些最重要的特征。'
- en: Table 9-1\. Feature importance from semisupervised model
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 表9-1\. 半监督模型的特征重要性
- en: '|  | featImportance |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | featImportance |'
- en: '| --- | --- |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| V28 | 0.047843 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| V28 | 0.047843 |'
- en: '| Amount | 0.037263 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| Amount | 0.037263 |'
- en: '| 21 | 0.030244 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 21 | 0.030244 |'
- en: '| V21 | 0.029624 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| V21 | 0.029624 |'
- en: '| V26 | 0.029469 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| V26 | 0.029469 |'
- en: '| V12 | 0.028334 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| V12 | 0.028334 |'
- en: '| V27 | 0.028024 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| V27 | 0.028024 |'
- en: '| 6 | 0.027405 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 0.027405 |'
- en: '| 28 | 0.026941 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 28 | 0.026941 |'
- en: '| 36 | 0.024050 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 36 | 0.024050 |'
- en: '| 5 | 0.022347 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.022347 |'
- en: As you can see here, some of the top features are features the hidden layer
    learned by the autoencoder (the non “V” features) while others are the principal
    components from the original dataset (the “V” features) as well as the amount
    of the transaction.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在这里所看到的，一些顶级特征是自动编码器学习的隐藏层特征（非“V”特征），而其他特征则是原始数据集的主要成分（“V”特征）以及交易金额。
- en: Conclusion
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The semisupervised model trounces the performance of both the standalone supervised
    model and the standalone unsupervised model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督模型击败了独立的监督模型和独立的非监督模型的性能。
- en: We just scratched the surface of what’s possible with semisupervised learning,
    but this should help reframe the conversation from debating between supervised
    and unsupervised learning to combining supervised and unsupervised learning in
    the search for an optimal applied solution.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只是初步探讨了半监督学习的潜力，但这应该有助于从辩论监督和非监督学习之间的选择转变为在寻找最佳应用解决方案中结合监督和非监督学习。
