- en: Chapter 9\. Semisupervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until now, we have viewed supervised learning and unsupervised learning as two
    separate and distinct branches of machine learning. Supervised learning is appropriate
    when our dataset is labeled, and unsupervised learning is necessary when our dataset
    is unlabeled.
  prefs: []
  type: TYPE_NORMAL
- en: In the real world, the distinction is not quite so clear. Datasets are usually
    partially labeled, and we want to efficiently label the unlabeled observations
    while leveraging the information in the labeled set. With supervised learning,
    we would have to toss away the majority of the dataset because it is unlabeled.
    With unsupervised learning, we would have the majority of the data to work with
    but would not know how to take advantage of the few labels we have.
  prefs: []
  type: TYPE_NORMAL
- en: The field of *semisupervised learning* blends the benefits of both supervised
    and unsupervised learning, taking advantage of the few labels that are available
    to uncover structure in a dataset and help label the rest.
  prefs: []
  type: TYPE_NORMAL
- en: We will continue to use the credit card transactions dataset in this chapter
    to showcase semisupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Data Preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As before, let’s load in the necessary libraries and prepare the data. This
    should be pretty familiar by now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As before, we will generate a training and test set. But we will drop 90% of
    the fraudulent credit card transactions from the training set to simulate how
    to work with *partially* *labeled* datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'While this may seem like a very aggressive move, real-world problems involving
    payment fraud have similarly low incidences of fraud (as little as 1 fraud per
    10,000 cases). By removing 90% of the labels from the training set, we are simulating
    this type of phenomenon:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also reuse the `anomalyScores` and `plotResults` functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Finally, here’s a new function called `precisionAnalysis` to help us assess
    the precision of our models at a certain level of recall. Specifically, we will
    determine what the model’s precision is to catch 75% of the fraudulent credit
    card transactions in the test set. The higher the precision, the better the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a reasonable benchmark. In other words, we want to catch 75% of the
    fraud with as high of a precision as possible. If we do not achieve a high enough
    precision, we will unnecessarily reject good credit card transactions, potentially
    angering our customer base:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Supervised Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To benchmark our semisupervised model, let’s first see how well a supervised
    model and a unsupervised model do in isolation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start with a supervised learning solution based on light gradient boosting
    like the one that performed best in [Chapter 2](ch02.html#Chapter_2). We will
    use *k*-fold cross-validation to create five folds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s next set the parameters for gradient boosting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s train the algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We will now use this model to predict the fraud on the test set of credit card
    transactions.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 9-1](#results_of_supervised_model) displays the results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Results of Supervised Model](assets/hulp_0901.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-1\. Results of supervised model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The average precision on the test based on the precision-recall curve is 0.62\.
    To catch 75% of the fraud, we have a precision of just 0.5%.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let’s build a fraud detection solution using unsupervised learning. Specifically,
    we will build a sparse two-layer overcomplete autoencoder with a linear activation
    function. We will have 40 nodes in the hidden layer and a dropout of 2%.
  prefs: []
  type: TYPE_NORMAL
- en: However, we will adjust our training set by *oversampling* the number of fraudulent
    cases we have. Oversampling is a technique used to adjust the class distribution
    in a given dataset. We want to add more fraudulent cases to our dataset so that
    the autoencoder we train has an easier time separating the normal/nonfraudulent
    transactions from the abnormal/fraudulent ones.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that after having dropped 90% of the fraudulent cases from the training
    set, we have just 33 fraudulent cases left. We will take the 33 fraudulent cases,
    duplicate these 100 times, and then append them to the training set. We will also
    keep copies of the nonoversampled training set so we can use them for the rest
    of our machine learning pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember we do not touch the test set—there is no oversampling with the test
    set, just the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now train our autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 9-2](#results_of_unsupervised_model) displays the results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Results of Unsupervised Model](assets/hulp_0902.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-2\. Results of unsupervised model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The average precision on the test based on the precision-recall curve is 0.57\.
    To catch 75% of the fraud, we have a precision of just 45%. While the average
    precision of the unsupervised solution is similar to the average precision of
    the supervised solution, the precision of 45% at 75% recall is better.
  prefs: []
  type: TYPE_NORMAL
- en: However, the unsupervised solution by itself is still not great.
  prefs: []
  type: TYPE_NORMAL
- en: Semisupervised Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let’s take the representation learned by the autoencoder (the hidden layer),
    combine it with the original training set, and feed this into the gradient boosting
    algorithm. This a semisupervised approach, taking advantage of supervised and
    unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the hidden layer, we call the `Model()` class from the Keras API and
    use the `get_layer` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s store these autoencoder representations into DataFrames and then combine
    them with the original training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now train the gradient boosting model on this new training set of 69
    features (29 from the original dataset and 40 from the autoencoder’s representation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 9-3](#results_of_semi_supervised_model) displays the results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Results of Semisupervised Model](assets/hulp_0903.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-3\. Results of semisupervised model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The average precision on the test set based on the precision-recall curve is
    0.78\. This is a good bit higher than both the supervised and the unsupervised
    models.
  prefs: []
  type: TYPE_NORMAL
- en: To catch 75% of the fraud, we have a precision of 92%. This is a considerable
    improvement. With this level of precision, the payment processor should feel comfortable
    rejecting transactions that the model flags as potentially fraudulent. Less than
    one in ten will be wrong, and we will catch approximately 75% of the fraud.
  prefs: []
  type: TYPE_NORMAL
- en: The Power of Supervised and Unsupervised
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this semisupervised credit card fraud detection solution, both supervised
    learning and unsupervised learning have important roles to play. One way to explore
    this is by analyzing which features the final gradient boosting model found most
    important.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s find and store those feature importance values from the model we just
    trained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[Table 9-1](#feature_importantce_from_semi_supervised_model) shows some of
    the most important features, sorted in descending order.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 9-1\. Feature importance from semisupervised model
  prefs: []
  type: TYPE_NORMAL
- en: '|  | featImportance |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| V28 | 0.047843 |'
  prefs: []
  type: TYPE_TB
- en: '| Amount | 0.037263 |'
  prefs: []
  type: TYPE_TB
- en: '| 21 | 0.030244 |'
  prefs: []
  type: TYPE_TB
- en: '| V21 | 0.029624 |'
  prefs: []
  type: TYPE_TB
- en: '| V26 | 0.029469 |'
  prefs: []
  type: TYPE_TB
- en: '| V12 | 0.028334 |'
  prefs: []
  type: TYPE_TB
- en: '| V27 | 0.028024 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 0.027405 |'
  prefs: []
  type: TYPE_TB
- en: '| 28 | 0.026941 |'
  prefs: []
  type: TYPE_TB
- en: '| 36 | 0.024050 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 0.022347 |'
  prefs: []
  type: TYPE_TB
- en: As you can see here, some of the top features are features the hidden layer
    learned by the autoencoder (the non “V” features) while others are the principal
    components from the original dataset (the “V” features) as well as the amount
    of the transaction.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The semisupervised model trounces the performance of both the standalone supervised
    model and the standalone unsupervised model.
  prefs: []
  type: TYPE_NORMAL
- en: We just scratched the surface of what’s possible with semisupervised learning,
    but this should help reframe the conversation from debating between supervised
    and unsupervised learning to combining supervised and unsupervised learning in
    the search for an optimal applied solution.
  prefs: []
  type: TYPE_NORMAL
