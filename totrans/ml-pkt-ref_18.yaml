- en: Chapter 18\. Clustering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第18章 聚类
- en: Clustering is an unsupervised machine learning technique used to divide a group
    into cohorts. It is unsupervised because we don’t give the model any labels; it
    just inspects the features and determines which samples are similar and belong
    in a cluster. In this chapter, we will look at the K-means and hierarchical clustering
    methods. We will also explore the Titanic dataset again using various techniques.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一种无监督机器学习技术，用于将群体分成几个组。它是无监督的，因为我们没有给模型任何标签；它只是检查特征并确定哪些样本相似并属于一个簇。在本章中，我们将研究K均值和层次聚类方法。我们还将再次使用各种技术探索泰坦尼克号数据集。
- en: K-Means
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K均值
- en: The K-means algorithm requires the user to pick the number of clusters or “k.”
    It then randomly chooses k centroids and assigns each sample to a cluster based
    on a distance metric from the centroid. Following the assignment, it recalculates
    the centroids based on the center of every sample assigned to a label. It then
    repeats assigning samples to clusters based on the new centroids. After a few
    iterations it should converge.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: K均值算法需要用户选择簇的数量或“k”。然后它随机选择k个质心，并根据从质心到每个样本的距离度量将每个样本分配给一个簇。分配完成后，它根据分配给一个标签的每个样本的中心重新计算质心。然后它根据新的质心再次将样本分配到簇中。经过几次迭代后，它应该会收敛。
- en: Because clustering uses distance metrics to determine which samples are similar,
    the behavior may change depending on the scale of the data. You can standardize
    the data and put all of the features on the same scale. Some have suggested that
    a SME might advise against standardizing if the scale hints that some features
    have more importance. We will standardize the data here in this example.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 因为聚类使用距离度量来确定哪些样本相似，所以行为可能会根据数据的规模而变化。您可以标准化数据并使所有特征处于相同的比例。有些人建议，如果规模提示某些特征更重要，SME可能会建议不要标准化。我们将在这个例子中对数据进行标准化。
- en: In this example, we will cluster the Titanic passengers. We will start with
    two clusters to see if the clustering can tease apart survival (we won’t leak
    the survival data into the clustering and will only use `X`, not `y`).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将对泰坦尼克号乘客进行聚类。我们将从两个簇开始，看看聚类是否能够分开生存（我们不会将生存数据泄漏到聚类中，只使用`X`，而不是`y`）。
- en: 'Unsupervised algorithms have a `.fit` method and a `.predict` method. We only
    pass `X` into `.fit`:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督算法具有`.fit`方法和`.predict`方法。我们只将`X`传递给`.fit`：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After the model is trained, we can call the `.predict` method to assign new
    samples to a cluster:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练后，我们可以调用`.predict`方法将新样本分配给一个簇：
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Instance parameters:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 实例参数：
- en: '`n_clusters=8`'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_clusters=8`'
- en: Number of clusters to create.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 创建的簇的数量。
- en: '`init=''kmeans++''`'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '`init=''kmeans++''`'
- en: Initialization method.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化方法。
- en: '`n_init=10`'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_init=10`'
- en: Number of times to run the algorithm with different centroids. Best score will
    win.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 使用不同质心运行算法的次数。最佳得分将获胜。
- en: '`max_iter=300`'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_iter=300`'
- en: Number of iterations for a run.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 运行的迭代次数。
- en: '`tol=0.0001`'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`tol=0.0001`'
- en: Tolerance until convergence.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 收敛的公差。
- en: '`precompute_distances=''auto''`'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`precompute_distances=''auto''`'
- en: Precompute distances (takes more memory but is faster). `auto` will precompute
    if `n_samples` * `n_clusters` is less than or equal to 12 million.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 预计算距离（需要更多内存但更快）。如果`n_samples` * `n_clusters`小于或等于1200万，`auto`将预先计算。
- en: '`verbose=0`'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '`verbose=0`'
- en: Verbosity.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 冗余性。
- en: '`random_state=None`'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`random_state=None`'
- en: Random seed.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 随机种子。
- en: '`copy_x=True`'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`copy_x=True`'
- en: Copy data before computing.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算之前复制数据。
- en: '`n_jobs=1`'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_jobs=1`'
- en: Number of CPUs to use.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用的CPU数量。
- en: '`algorithm=''auto''`'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`algorithm=''auto''`'
- en: K-means algorithm. `'full'` works with sparse data, but `'elkan'` is more efficient.
    `'auto'` uses `'elkan'` with dense data.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: K均值算法。`'full'`适用于稀疏数据，但`'elkan'`更高效。`'auto'`在密集数据中使用`'elkan'`。
- en: 'Attributes:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 属性：
- en: '`cluster_centers_`'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`cluster_centers_`'
- en: Coordinates of centers
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 中心的坐标
- en: '`labels_`'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`labels_`'
- en: Labels for samples
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 样本的标签
- en: '`inertia_`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`inertia_`'
- en: Sum of squared distance to cluster centroid
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 到聚类质心的平方距离之和
- en: '`n_iter_`'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_iter_`'
- en: Number of iterations
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代次数
- en: If you don’t know ahead of time how many clusters you need, you can run the
    algorithm with a range of sizes and evaluate various metrics. It can be tricky.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果事先不知道需要多少个簇，可以以一系列大小运行算法并评估各种指标。这可能有些棘手。
- en: You can roll your own elbow plot using the `.inertia_` calculation. Look for
    where the curve bends as that is potentially a good choice for the number of clusters.
    In this case, the curve is smooth, but after eight there doesn’t seem to be much
    improvement (see [Figure 18-1](#idkm1)).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `.inertia_` 计算自己的肘部图。寻找曲线弯曲的位置，因为那可能是选择聚类数量的一个良好选择。在这种情况下，曲线很平滑，但在八个之后似乎没有太多改善（见
    [Figure 18-1](#idkm1)）。
- en: For plots without an elbow, we have a few options. We can use other metrics,
    some of which are shown below. We can also inspect a visualization of the clustering
    and see if clusters are visible. We can add features to the data and see if that
    helps with clustering.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于没有肘部的图，我们有几个选择。我们可以使用其他指标，其中一些如下所示。我们还可以检查聚类的可视化，看看聚类是否可见。我们可以向数据添加特征，看看是否有帮助进行聚类。
- en: 'Here is the code for an elbow plot:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是肘部图的代码：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Elbow plot that is looking rather smooth.](assets/mlpr_1801.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![看起来相当平滑的肘部图。](assets/mlpr_1801.png)'
- en: Figure 18-1\. Elbow plot that is looking rather smooth.
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 18-1\. 看起来相当平滑的肘部图。
- en: Scikit-learn has other clustering metrics when the ground truth labels are not
    known. We can calculate and plot those as well. The *Silhouette Coefficient* is
    a value between -1 and 1\. The higher the score, the better. 1 indicates tight
    clusters, and 0 means overlapping clusters. From that measure, two clusters gives
    us the best score.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当地面真实标签未知时，Scikit-learn 具有其他聚类指标。我们也可以计算并绘制这些指标。*轮廓系数* 是介于 -1 和 1 之间的值。得分越高越好。1
    表示紧密的聚类，0 表示重叠的聚类。根据这个度量，两个聚类给我们带来了最佳分数。
- en: The *Calinski-Harabasz Index* is the ratio of between-cluster dispersion and
    within-cluster dispersion. A higher score is better. Two clusters gives the best
    score for this metric.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*Calinski-Harabasz 指数* 是介于类间离散度和类内离散度之间的比率。分数越高越好。对于这个指标，两个聚类给出了最佳分数。'
- en: The *Davis-Bouldin Index* is the average similarity between each cluster and
    the closest cluster. Scores range from 0 and up. 0 indicates better clustering.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*Davis-Bouldin 指数* 是每个聚类与最接近的聚类之间相似性的平均值。分数从 0 开始。0 表示更好的聚类。'
- en: 'Here we will plot inertia, the silhouette coefficient, the Calinski-Harabasz
    Index, and the Davies-Bouldin Index over a range of cluster sizes to see if there
    is a clear size of clusters for the data (see [Figure 18-2](#idkm2)). It appears
    that most of these metrics agree on two clusters:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将绘制惯性、轮廓系数、Calinski-Harabasz 指数和 Davies-Bouldin 指数在一系列聚类大小上的情况，以查看数据是否有明确的聚类大小（见
    [Figure 18-2](#idkm2)）。大多数这些指标都同意选择两个聚类：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![Cluster metrics. These metrics mostly agree on two clusters.](assets/mlpr_1802.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![聚类指标。这些指标大多数同意选择两个聚类。](assets/mlpr_1802.png)'
- en: Figure 18-2\. Cluster metrics. These metrics mostly agree on two clusters.
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 18-2\. 聚类指标。这些指标大多数同意选择两个聚类。
- en: Another technique for determining clusters is to visualize the silhouette scores
    for each cluster. Yellowbrick has a visualizer for this (see [Figure 18-3](#id56)).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种确定聚类的技术是可视化每个聚类的轮廓分数。Yellowbrick 有一个此类的可视化工具（见 [Figure 18-3](#id56)）。
- en: 'The vertical dotted red line in this plot is the average score. One way to
    interpret it is to make sure that each cluster bumps out above the average, and
    the cluster scores look decent. Make sure you are using the same x limits (`ax.set_xlim`).
    I would choose two clusters from these plots:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图中，垂直的虚线是平均分数。一种解释方法是确保每个聚类都突出于平均水平之上，并且聚类分数看起来还不错。确保使用相同的 x 轴限制 (`ax.set_xlim`)。我会从这些图中选择两个聚类：
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Yellowbrick silhouette visualizer](assets/mlpr_1803.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![Yellowbrick 轮廓可视化器](assets/mlpr_1803.png)'
- en: Figure 18-3\. Yellowbrick silhouette visualizer
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 18-3\. Yellowbrick 轮廓可视化器
- en: Agglomerative (Hierarchical) Clustering
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚合（层次）聚类
- en: Agglomerative clustering is another methodology. You start off with each sample
    in its own cluster. Then you combine the “nearest” clusters. Repeat until done
    while keeping track of the nearest sizes.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合聚类是另一种方法。你从每个样本单独的聚类开始。然后将“最近的”聚类组合起来。重复此过程，同时跟踪最近的大小。
- en: When you have finished this, you will have a *dendrogram*, or a tree that tracks
    when clusters were created and what the distance metric was. You can use the scipy
    library to visualize the dendrogram.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当您完成这些操作时，您将得到一棵*dendrogram*，或者一棵跟踪聚类创建时间和距离度量的树。您可以使用 scipy 库可视化这棵树。
- en: 'We can use scipy to create a dendrogram (see [Figure 18-4](#id59)). As you
    can see, if you have many samples the leaf nodes are hard to read:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 scipy 创建一棵树状图（见 [Figure 18-4](#id59)）。如您所见，如果样本很多，则叶节点很难阅读：
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Scipy hierarchical clustering dendrogram](assets/mlpr_1804.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![Scipy 层次聚类树状图](assets/mlpr_1804.png)'
- en: Figure 18-4\. Scipy hierarchical clustering dendrogram
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 18-4\. Scipy 层次聚类树状图
- en: Once you have the dendrogram, you have all the clusters (from one to the size
    of the samples). The heights represent how similar clusters are when they are
    joined. In order to find how many clusters are in the data, you would want to
    “cut” a horizontal line through where it would cross the tallest lines.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您有了树状图，您就拥有了所有的聚类（从样本数为一到样本数为止）。高度表示加入时相似聚类的相似程度。为了找出数据中有多少个聚类，您需要在最高的线交叉处“切割”一条水平线。
- en: In this case, it looks like when you perform that cut, you have three clusters.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，当您执行该切割时，看起来有三个聚类。
- en: 'The previous plot was a little noisy with all of the samples in it. You can
    also use the `truncate_mode` parameter to combine the leaves into a single node
    (see [Figure 18-5](#id60)):'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个图表太嘈杂了，其中包含了所有的样本。您还可以使用`truncate_mode`参数将叶子节点合并为单个节点（参见[图 18-5](#id60)）：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Truncated hierarchical clustering dendrogram. If we cut across the largest
    vertical lines, we get three clusters.](assets/mlpr_1805.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![截断的层次聚类树状图。如果我们在最大的垂直线上切割，我们将得到三个聚类。](assets/mlpr_1805.png)'
- en: Figure 18-5\. Truncated hierarchical clustering dendrogram. If we cut across
    the largest vertical lines, we get three clusters.
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 18-5\. 截断的层次聚类树状图。如果我们在最大的垂直线上切割，我们将得到三个聚类。
- en: 'Once we know how many clusters we need, we can use scikit-learn to create a
    model:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们知道需要多少个聚类，我们可以使用scikit-learn创建一个模型：
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The [fastcluster package](https://oreil.ly/OuNuo) provides an optimized agglomerative
    clustering package if the scikit-learn implementation is too slow.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[fastcluster包](https://oreil.ly/OuNuo) 提供了一个优化的凝聚聚类包，如果scikit-learn的实现速度太慢的话。'
- en: Understanding Clusters
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解聚类
- en: Using K-means on the Titanic dataset, we will make two clusters. We can use
    the grouping functionality in pandas to examine the differences in the clusters.
    The code below examines the mean and variance for each feature. It appears that
    the mean value for pclass varies quite a bit.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在Titanic数据集上使用K-means，我们将得到两个聚类。我们可以使用pandas中的分组功能来检查聚类之间的差异。下面的代码检查每个特征的均值和方差。看起来pclass的均值变化相当大。
- en: 'I’m sticking the survival data back in to see if the clustering was related
    to that:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我将生存数据重新放回，看看聚类是否与此相关：
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'In Jupyter you can tack on the following code to a DataFrame, and it will highlight
    the high and low values of each row. This is useful for visually seeing which
    values stand out in the above cluster summary:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在Jupyter中，您可以将以下代码添加到DataFrame中，并突出显示每行的高低值。这对于直观地查看哪些值在上述聚类摘要中显著是有用的：
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In [Figure 18-6](#idclex) we plot a bar plot of the means for each cluster:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 18-6](#idclex)中，我们绘制了每个聚类的均值条形图：
- en: '[PRE10]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![Mean values of each cluster](assets/mlpr_1806.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![每个聚类的均值](assets/mlpr_1806.png)'
- en: Figure 18-6\. Mean values of each cluster
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 18-6\. 每个聚类的均值
- en: I also like to plot the PCA components, but colored by the cluster label (see
    [Figure 18-7](#idclpca)). Here we use Seaborn to do that. It is also interesting
    to change the values for `hue` to dive into the features that are distinct for
    the clusters.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我还喜欢绘制PCA组件，但是按聚类标签着色（见[图 18-7](#idclpca)）。在这里，我们使用Seaborn来执行此操作。将`hue`的值更改为深入研究聚类中显著的特征也很有趣。
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![PCA plot of clusters](assets/mlpr_1807.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![聚类的PCA图](assets/mlpr_1807.png)'
- en: Figure 18-7\. PCA plot of clusters
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 18-7\. 聚类的PCA图
- en: 'If we want to examine a single feature, we can use the pandas `.describe` method:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想检查单个特征，可以使用pandas的`.describe`方法：
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can also create a surrogate model to explain the clusters. Here we use a
    decision tree to explain them. This also shows that pclass (which had a large
    difference in the mean) is very important:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以创建一个替代模型来解释这些聚类。在这里，我们使用决策树来解释它们。这还显示了pclass（其均值差异很大）非常重要：
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'And we can visualize the decisions in [Figure 18-8](#iddtsurr). It shows that
    pclass is the first feature the surrogate looks at to make a decision:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过[图 18-8](#iddtsurr)来可视化决策。它显示pclass是第一个特征，用于做出决策：
- en: '[PRE14]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![Decision tree explaining the clustering](assets/mlpr_1808.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![解释聚类的决策树](assets/mlpr_1808.png)'
- en: Figure 18-8\. Decision tree explaining the clustering
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 18-8\. 解释聚类的决策树
