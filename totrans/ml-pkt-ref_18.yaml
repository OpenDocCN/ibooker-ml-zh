- en: Chapter 18\. Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering is an unsupervised machine learning technique used to divide a group
    into cohorts. It is unsupervised because we don’t give the model any labels; it
    just inspects the features and determines which samples are similar and belong
    in a cluster. In this chapter, we will look at the K-means and hierarchical clustering
    methods. We will also explore the Titanic dataset again using various techniques.
  prefs: []
  type: TYPE_NORMAL
- en: K-Means
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The K-means algorithm requires the user to pick the number of clusters or “k.”
    It then randomly chooses k centroids and assigns each sample to a cluster based
    on a distance metric from the centroid. Following the assignment, it recalculates
    the centroids based on the center of every sample assigned to a label. It then
    repeats assigning samples to clusters based on the new centroids. After a few
    iterations it should converge.
  prefs: []
  type: TYPE_NORMAL
- en: Because clustering uses distance metrics to determine which samples are similar,
    the behavior may change depending on the scale of the data. You can standardize
    the data and put all of the features on the same scale. Some have suggested that
    a SME might advise against standardizing if the scale hints that some features
    have more importance. We will standardize the data here in this example.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we will cluster the Titanic passengers. We will start with
    two clusters to see if the clustering can tease apart survival (we won’t leak
    the survival data into the clustering and will only use `X`, not `y`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Unsupervised algorithms have a `.fit` method and a `.predict` method. We only
    pass `X` into `.fit`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After the model is trained, we can call the `.predict` method to assign new
    samples to a cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Instance parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_clusters=8`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of clusters to create.
  prefs: []
  type: TYPE_NORMAL
- en: '`init=''kmeans++''`'
  prefs: []
  type: TYPE_NORMAL
- en: Initialization method.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_init=10`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of times to run the algorithm with different centroids. Best score will
    win.
  prefs: []
  type: TYPE_NORMAL
- en: '`max_iter=300`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of iterations for a run.
  prefs: []
  type: TYPE_NORMAL
- en: '`tol=0.0001`'
  prefs: []
  type: TYPE_NORMAL
- en: Tolerance until convergence.
  prefs: []
  type: TYPE_NORMAL
- en: '`precompute_distances=''auto''`'
  prefs: []
  type: TYPE_NORMAL
- en: Precompute distances (takes more memory but is faster). `auto` will precompute
    if `n_samples` * `n_clusters` is less than or equal to 12 million.
  prefs: []
  type: TYPE_NORMAL
- en: '`verbose=0`'
  prefs: []
  type: TYPE_NORMAL
- en: Verbosity.
  prefs: []
  type: TYPE_NORMAL
- en: '`random_state=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Random seed.
  prefs: []
  type: TYPE_NORMAL
- en: '`copy_x=True`'
  prefs: []
  type: TYPE_NORMAL
- en: Copy data before computing.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_jobs=1`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of CPUs to use.
  prefs: []
  type: TYPE_NORMAL
- en: '`algorithm=''auto''`'
  prefs: []
  type: TYPE_NORMAL
- en: K-means algorithm. `'full'` works with sparse data, but `'elkan'` is more efficient.
    `'auto'` uses `'elkan'` with dense data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cluster_centers_`'
  prefs: []
  type: TYPE_NORMAL
- en: Coordinates of centers
  prefs: []
  type: TYPE_NORMAL
- en: '`labels_`'
  prefs: []
  type: TYPE_NORMAL
- en: Labels for samples
  prefs: []
  type: TYPE_NORMAL
- en: '`inertia_`'
  prefs: []
  type: TYPE_NORMAL
- en: Sum of squared distance to cluster centroid
  prefs: []
  type: TYPE_NORMAL
- en: '`n_iter_`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of iterations
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t know ahead of time how many clusters you need, you can run the
    algorithm with a range of sizes and evaluate various metrics. It can be tricky.
  prefs: []
  type: TYPE_NORMAL
- en: You can roll your own elbow plot using the `.inertia_` calculation. Look for
    where the curve bends as that is potentially a good choice for the number of clusters.
    In this case, the curve is smooth, but after eight there doesn’t seem to be much
    improvement (see [Figure 18-1](#idkm1)).
  prefs: []
  type: TYPE_NORMAL
- en: For plots without an elbow, we have a few options. We can use other metrics,
    some of which are shown below. We can also inspect a visualization of the clustering
    and see if clusters are visible. We can add features to the data and see if that
    helps with clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code for an elbow plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![Elbow plot that is looking rather smooth.](assets/mlpr_1801.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-1\. Elbow plot that is looking rather smooth.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Scikit-learn has other clustering metrics when the ground truth labels are not
    known. We can calculate and plot those as well. The *Silhouette Coefficient* is
    a value between -1 and 1\. The higher the score, the better. 1 indicates tight
    clusters, and 0 means overlapping clusters. From that measure, two clusters gives
    us the best score.
  prefs: []
  type: TYPE_NORMAL
- en: The *Calinski-Harabasz Index* is the ratio of between-cluster dispersion and
    within-cluster dispersion. A higher score is better. Two clusters gives the best
    score for this metric.
  prefs: []
  type: TYPE_NORMAL
- en: The *Davis-Bouldin Index* is the average similarity between each cluster and
    the closest cluster. Scores range from 0 and up. 0 indicates better clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we will plot inertia, the silhouette coefficient, the Calinski-Harabasz
    Index, and the Davies-Bouldin Index over a range of cluster sizes to see if there
    is a clear size of clusters for the data (see [Figure 18-2](#idkm2)). It appears
    that most of these metrics agree on two clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![Cluster metrics. These metrics mostly agree on two clusters.](assets/mlpr_1802.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-2\. Cluster metrics. These metrics mostly agree on two clusters.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Another technique for determining clusters is to visualize the silhouette scores
    for each cluster. Yellowbrick has a visualizer for this (see [Figure 18-3](#id56)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The vertical dotted red line in this plot is the average score. One way to
    interpret it is to make sure that each cluster bumps out above the average, and
    the cluster scores look decent. Make sure you are using the same x limits (`ax.set_xlim`).
    I would choose two clusters from these plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![Yellowbrick silhouette visualizer](assets/mlpr_1803.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-3\. Yellowbrick silhouette visualizer
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Agglomerative (Hierarchical) Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Agglomerative clustering is another methodology. You start off with each sample
    in its own cluster. Then you combine the “nearest” clusters. Repeat until done
    while keeping track of the nearest sizes.
  prefs: []
  type: TYPE_NORMAL
- en: When you have finished this, you will have a *dendrogram*, or a tree that tracks
    when clusters were created and what the distance metric was. You can use the scipy
    library to visualize the dendrogram.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use scipy to create a dendrogram (see [Figure 18-4](#id59)). As you
    can see, if you have many samples the leaf nodes are hard to read:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![Scipy hierarchical clustering dendrogram](assets/mlpr_1804.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-4\. Scipy hierarchical clustering dendrogram
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once you have the dendrogram, you have all the clusters (from one to the size
    of the samples). The heights represent how similar clusters are when they are
    joined. In order to find how many clusters are in the data, you would want to
    “cut” a horizontal line through where it would cross the tallest lines.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, it looks like when you perform that cut, you have three clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The previous plot was a little noisy with all of the samples in it. You can
    also use the `truncate_mode` parameter to combine the leaves into a single node
    (see [Figure 18-5](#id60)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![Truncated hierarchical clustering dendrogram. If we cut across the largest
    vertical lines, we get three clusters.](assets/mlpr_1805.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-5\. Truncated hierarchical clustering dendrogram. If we cut across
    the largest vertical lines, we get three clusters.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Once we know how many clusters we need, we can use scikit-learn to create a
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The [fastcluster package](https://oreil.ly/OuNuo) provides an optimized agglomerative
    clustering package if the scikit-learn implementation is too slow.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using K-means on the Titanic dataset, we will make two clusters. We can use
    the grouping functionality in pandas to examine the differences in the clusters.
    The code below examines the mean and variance for each feature. It appears that
    the mean value for pclass varies quite a bit.
  prefs: []
  type: TYPE_NORMAL
- en: 'I’m sticking the survival data back in to see if the clustering was related
    to that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In Jupyter you can tack on the following code to a DataFrame, and it will highlight
    the high and low values of each row. This is useful for visually seeing which
    values stand out in the above cluster summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In [Figure 18-6](#idclex) we plot a bar plot of the means for each cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![Mean values of each cluster](assets/mlpr_1806.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-6\. Mean values of each cluster
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: I also like to plot the PCA components, but colored by the cluster label (see
    [Figure 18-7](#idclpca)). Here we use Seaborn to do that. It is also interesting
    to change the values for `hue` to dive into the features that are distinct for
    the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![PCA plot of clusters](assets/mlpr_1807.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-7\. PCA plot of clusters
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If we want to examine a single feature, we can use the pandas `.describe` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also create a surrogate model to explain the clusters. Here we use a
    decision tree to explain them. This also shows that pclass (which had a large
    difference in the mean) is very important:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'And we can visualize the decisions in [Figure 18-8](#iddtsurr). It shows that
    pclass is the first feature the surrogate looks at to make a decision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![Decision tree explaining the clustering](assets/mlpr_1808.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-8\. Decision tree explaining the clustering
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
