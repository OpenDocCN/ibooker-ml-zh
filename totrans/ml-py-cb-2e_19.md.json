["```py\n# Load libraries\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\n\n# Load data\niris = datasets.load_iris()\nfeatures = iris.data\n\n# Standardize features\nscaler = StandardScaler()\nfeatures_std = scaler.fit_transform(features)\n\n# Create k-means object\ncluster = KMeans(n_clusters=3, random_state=0, n_init=\"auto\")\n\n# Train model\nmodel = cluster.fit(features_std)\n```", "```py\n# View predicted class\nmodel.labels_\n```", "```py\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n       1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2,\n       2, 1, 2, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2,\n       2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1], dtype=int32)\n```", "```py\n# View true class\niris.target\n```", "```py\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n```", "```py\n# Create new observation\nnew_observation = [[0.8, 0.8, 0.8, 0.8]]\n\n# Predict observation's cluster\nmodel.predict(new_observation)\n```", "```py\narray([2], dtype=int32)\n```", "```py\n# View cluster centers\nmodel.cluster_centers_\n```", "```py\narray([[-1.01457897,  0.85326268, -1.30498732, -1.25489349],\n       [-0.01139555, -0.87600831,  0.37707573,  0.31115341],\n       [ 1.16743407,  0.14530299,  1.00302557,  1.0300019 ]])\n```", "```py\n# Load libraries\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import MiniBatchKMeans\n\n# Load data\niris = datasets.load_iris()\nfeatures = iris.data\n\n# Standardize features\nscaler = StandardScaler()\nfeatures_std = scaler.fit_transform(features)\n\n# Create k-mean object\ncluster = MiniBatchKMeans(n_clusters=3, random_state=0, batch_size=100,\n       n_init=\"auto\")\n\n# Train model\nmodel = cluster.fit(features_std)\n```", "```py\n# Load libraries\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import MeanShift\n\n# Load data\niris = datasets.load_iris()\nfeatures = iris.data\n\n# Standardize features\nscaler = StandardScaler()\nfeatures_std = scaler.fit_transform(features)\n\n# Create mean shift object\ncluster = MeanShift(n_jobs=-1)\n\n# Train model\nmodel = cluster.fit(features_std)\n```", "```py\n# Load libraries\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import DBSCAN\n\n# Load data\niris = datasets.load_iris()\nfeatures = iris.data\n\n# Standardize features\nscaler = StandardScaler()\nfeatures_std = scaler.fit_transform(features)\n\n# Create DBSCAN object\ncluster = DBSCAN(n_jobs=-1)\n\n# Train model\nmodel = cluster.fit(features_std)\n```", "```py\n# Show cluster membership\nmodel.labels_\n```", "```py\narray([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1,\n        0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  1,\n        1,  1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1,  1,  1,  1,  1,  1,\n       -1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n       -1,  1, -1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1, -1,  1, -1,  1,\n        1,  1,  1, -1, -1, -1, -1, -1,  1,  1,  1,  1, -1,  1,  1, -1, -1,\n       -1,  1,  1, -1,  1,  1, -1,  1,  1,  1, -1, -1, -1,  1,  1,  1, -1,\n       -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1])\n```", "```py\n# Load libraries\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Load data\niris = datasets.load_iris()\nfeatures = iris.data\n\n# Standardize features\nscaler = StandardScaler()\nfeatures_std = scaler.fit_transform(features)\n\n# Create agglomerative clustering object\ncluster = AgglomerativeClustering(n_clusters=3)\n\n# Train model\nmodel = cluster.fit(features_std)\n```", "```py\n# Show cluster membership\nmodel.labels_\n```", "```py\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,\n       1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0,\n       2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 2, 0, 0, 2,\n       2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n```"]