- en: 9 Error analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 错误分析
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Learning curve analysis
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习曲线分析
- en: Residual analysis
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 剩余分析
- en: Finding commonalities in residuals
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在残差中寻找共性
- en: Once we’ve assembled the initial building blocks, which include gathering the
    first dataset, picking the metrics, defining the evaluation procedure, and training
    the baseline, we are ready to start the iterative adjustments process. Just as
    backpropagation in neural networks calculates the direction of the fastest loss
    reduction and passes it backward from layer to layer, error analysis finds the
    fastest improvement for the whole system.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们组装了初始的构建块，包括收集第一个数据集、选择指标、定义评估程序和训练基线，我们就准备好开始迭代调整过程。正如神经网络中的反向传播计算最快损失减少的方向并将它从层到层传递回去一样，错误分析找到了整个系统的最快改进方式。
- en: Error analysis steps up as the compass that guides the iterative updates of
    your system. It helps you understand the error dynamics during the training phase
    (learning curve analysis) and the distribution of errors after the prediction
    phase (residual analysis). By analyzing these errors, you can identify commonalities,
    trends, and patterns that inform improvements to your machine learning (ML) system.
    In this chapter, we will examine its crucial stages and types and provide examples
    that we hope will give you a better understanding of the subject.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 错误分析成为指导系统迭代更新的指南针。它帮助你在训练阶段（学习曲线分析）和预测阶段之后（残差分析）理解错误动态。通过分析这些错误，你可以识别出共性、趋势和模式，这些可以指导你改进机器学习（ML）系统。在本章中，我们将考察其关键阶段和类型，并提供我们希望有助于你更好地理解这一主题的示例。
- en: Error analysis is often skipped when ML systems are designed for a reason that
    seems somewhat legit at first glance—this step is not part of *building the system*
    per se. However, time spent on error analysis is always a good investment as it
    reveals weak spots and suggests ways of improving the system. Leaving this step
    out of this book would be a huge mistake from our side.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 错误分析在为机器学习系统设计时常常被忽略，这看似是一个合理的理由——因为这个步骤本身并不属于*构建系统*的过程。然而，在错误分析上花费的时间总是值得的，因为它揭示了系统的弱点，并提出了改进系统的方法。如果我们在这本书中省略这一步骤，那将是我们的一大失误。
- en: 9.1 Learning curve analysis
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 学习曲线分析
- en: 'Learning curve analysis evaluates the learning process by plotting and analyzing
    the learning curve, showing the relationship between the model’s training performance
    and the amount of training data used. Learning curve analysis is designed to answer
    two vital questions:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 学习曲线分析通过绘制和分析学习曲线来评估学习过程，显示了模型训练性能与所用训练数据量之间的关系。学习曲线分析旨在回答两个关键问题：
- en: Does the model converge?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型是否收敛？
- en: If so, have we avoided underfitting or overfitting issues?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果是这样，我们是否避免了欠拟合或过拟合问题？
- en: If both questions lead to negative answers, there is no need for the rest of
    the analysis.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这两个问题都得到了否定的答案，就没有必要进行剩余的分析。
- en: Before we get into details, what is a learning curve? The term was coined in
    behavioral psychology, where it is used to display the learning progress of a
    person or animal observed over time (see figure 9.1). For instance, we may analyze
    the number of mistakes made by a subject in every new test iteration or study
    how much time it takes for a mouse to find a path through the labyrinth compared
    to the trial number.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入细节之前，什么是学习曲线？这个术语起源于行为心理学，在那里它被用来显示在一段时间内观察到的个人或动物的学习进度（见图9.1）。例如，我们可能分析受试者在每次新测试迭代中犯的错误数量，或者研究老鼠在迷宫中找到路径所需的时间与试验次数的比较。
- en: '![figure](../Images/CH09_F01_Babushkin.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH09_F01_Babushkin.png)'
- en: Figure 9.1 Basic representation of a learning curve
  id: totrans-15
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.1 学习曲线的基本表示
- en: 'In ML, a learning curve is essentially a graphical representation that shows
    the dependency of a chosen metric on a specific numerical property, such as the
    number of iterations, dataset size, or model complexity. Let’s do a brief breakdown
    of all three properties:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，学习曲线本质上是一种图形表示，显示了所选指标对特定数值属性（如迭代次数、数据集大小或模型复杂性）的依赖性。让我们简要地分解这三个属性：
- en: '*Number of iterations—*This kind of curve depicts an evolution of the loss
    or metric during training and helps examine the model’s convergence. In some sources,
    it is referred to as a loss curve or a convergence curve. A good example of iterations
    is the number of training epochs in neural networks.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*迭代次数—*这种曲线描绘了训练过程中损失或指标的变化，有助于检查模型是否收敛。在某些资料中，它被称为损失曲线或收敛曲线。迭代的一个好例子是神经网络中的训练轮数。'
- en: '*Model complexity—*This type of learning curve shows how performance varies
    based on changes in the complexity of your model. As its complexity increases,
    the model tends to fit the training data better but may start to generalize poorly
    on the unseen data. Examples of parameters for model complexity are the tree depth,
    the number of features, and the number of layers in a neural network.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型复杂度—*这种类型的学习曲线显示了模型性能如何随着模型复杂度的变化而变化。随着复杂度的增加，模型倾向于更好地拟合训练数据，但可能开始对未见过的数据泛化不佳。模型复杂度的参数示例包括树深度、特征数量以及神经网络中的层数。'
- en: '*Dataset size—*This learning curve reveals how the number of samples in the
    training dataset affects the model’s performance. It is helpful in determining
    whether the model would benefit from more data.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据集大小—*这条学习曲线揭示了训练数据集中样本数量如何影响模型的表现。这有助于确定模型是否能够从更多数据中受益。'
- en: These properties reveal the three most common types of learning curves. Before
    diving into each, we should recall “the quest for the grail of machine learning,”
    the overfitting and underfitting problem, sometimes referred to as the bias–variance
    tradeoff (see figure 9.2).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特性揭示了三种最常见的学习曲线类型。在深入探讨每一种之前，我们应该回顾“机器学习的圣杯追求”，即过拟合和欠拟合问题，有时也被称为偏差-方差权衡（见图9.2）。
- en: '![figure](../Images/CH09_F02_Babushkin.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_F02_Babushkin.png)'
- en: Figure 9.2 With an increasing number of model parameters, training error tends
    to become lower and lower while minimizing bias. At the same time, the model variance
    increases, providing us with a U-shaped validation error.
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.2 随着模型参数数量的增加，训练误差趋于越来越低，同时最小化偏差。同时，模型方差增加，为我们提供了一个U形的验证误差。
- en: 9.1.1 Overfitting and underfitting
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.1 过拟合和欠拟合
- en: Overfitting happens when a model conveys a great performance on training data
    and a bad performance on unseen data. Usually, this happens when it learns the
    training data so well that it becomes too specialized and fails to generalize,
    focusing too much on insignificant details and patterns that are not present in
    the new data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合发生在模型在训练数据上表现出色，而在未见过的数据上表现不佳的情况下。通常，这是因为它对训练数据学得太好，变得过于专业化，无法泛化，过分关注新数据中不存在的细微细节和模式。
- en: On the other hand, underfitting occurs when the model is too simple and misses
    some important relationships between features and the target variable, resulting
    in poor performance on both the training data and new data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，当模型过于简单，错过了特征与目标变量之间的一些重要关系时，就会发生欠拟合，导致在训练数据和新的数据上表现都较差。
- en: Both are strongly related to the bias–variance tradeoff, which is the balance
    between the model’s complexity and the amount of input data. The greater the model’s
    capacity to capture useful signals from data, the lower the bias and the higher
    the risk of overfitting. On the other hand, reducing a variance requires decreasing
    complexity, thus leading to a more biased model.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种情况都与偏差-方差权衡密切相关，这是模型复杂度与输入数据量之间的平衡。模型从数据中捕捉有用信号的能力越强，偏差就越低，过拟合的风险就越高。另一方面，减少方差需要降低复杂度，这会导致模型偏差增加。
- en: '*Bias* is an error caused by the low capacity of the model to capture useful
    signals in the data. In other words, the model is biased toward its simplified
    assumptions about the data. When the model is biased, we call it *underfitting*.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*偏差*是由模型捕捉数据中有用信号的能力低而引起的错误。换句话说，模型倾向于对其关于数据的简化假设。当模型有偏差时，我们称之为*欠拟合*。'
- en: '*Variance* is an error caused by the model’s high sensitivity to small fluctuations
    in the training set. The model is poorly generalized on new data, which, in terms
    of the model’s parameters, is highly varied from what it has seen in the training
    set. Often, high variance is a primary reason behind *overfitting* (assuming nothing
    is broken in other parts of the system).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*方差*是由模型对训练集中微小波动的过高敏感性引起的错误。模型在新数据上的泛化能力较差，从模型参数的角度来看，这些数据与训练集中看到的数据差异很大。通常，高方差是*过拟合*（假设系统其他部分没有问题）的主要原因。'
- en: A good learning algorithm is expected to minimize both bias and variance simultaneously.
    The bias–variance tradeoff, however, vividly demonstrates that reducing variance
    often involves increasing bias and vice versa. The quest here is to find the right
    balance between the two. This is when learning-curve analysis can guide us.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的学习算法应该同时最小化偏差和方差。然而，偏差-方差权衡却生动地展示了减少方差往往涉及增加偏差，反之亦然。这里的追求就是在这两者之间找到合适的平衡。这时，学习曲线分析就能为我们提供指导。
- en: Keep in mind that the model’s redundant complexity (i.e., high variance) is
    not the only reason behind overfitting. Some other possible scenarios include
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，模型的冗余复杂性（即高方差）并不是过拟合的唯一原因。其他可能的情况包括
- en: '*Data leakage* (using information that is not supposed to be known during inference)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据泄露*（在推理过程中使用不应知道的信息）'
- en: '*Noisy or highly granular features* that force the model to capture irrelevant
    patterns'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*噪声或高度细粒度的特征*迫使模型捕捉无关的模式'
- en: '*Existence of outliers* that have a large impact on the loss function'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*异常值的存在*对损失函数有重大影响'
- en: Overall *poor ability* of the model to extrapolate
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型整体*外推能力*较差
- en: Training sets and validation sets simply belonging to*different distributions*
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集和验证集简单地属于*different distributions*
- en: Regardless of a given case, learning curves are an effective tool to detect
    underfitting and overfitting. Armed with the knowledge about overfitting and underfitting,
    we are ready to go through different types of curves and the hints they give us
    in this quest.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 不论是哪种情况，学习曲线都是检测欠拟合和过拟合的有效工具。掌握了关于过拟合和欠拟合的知识，我们就准备好去分析不同类型的曲线以及它们在这个追求中给出的提示。
- en: 9.1.2 Loss curve
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.2 损失曲线
- en: A loss curve (also referred to as a convergence curve or learning curve) based
    on learning iterations is the first thing that comes to mind for ML engineers
    when they hear the term “learning curve.” It shows how much the algorithm is improving
    as it puts more and more learning effort into the task.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当机器学习工程师听到“学习曲线”这个词时，首先想到的是基于学习迭代的损失曲线（也称为收敛曲线或学习曲线）。它显示了算法随着将越来越多的学习努力投入到任务中而不断改进的情况。
- en: The curve plots the loss (or metric) on the vertical axis and the number of
    training iterations (or epochs) on the horizontal axis. As the model trains, the
    loss should decrease, ideally forming a downward slope toward the bottom of the
    curve.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 曲线在垂直轴上绘制损失（或指标），在水平轴上绘制训练迭代次数（或时期）。随着模型的训练，损失应该减少，理想情况下形成一个向曲线底部的下降斜率。
- en: In contrast to learning curves where axis X is the dataset size or the model
    complexity (which we will talk about soon), the iteration-wise curve requires
    only one training run, which makes its tracking practical even for large datasets
    when a single training run takes hours or even days. The loss curve helps keep
    your finger on the pulse all the way until the end of training. If you’ve run
    just 10 training epochs out of 200, you can already get insights on whether the
    loss value is progressing as expected or if there are issues that make further
    training pointless.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 与学习曲线不同，其中X轴是数据集大小或模型复杂性（我们很快就会讨论），迭代曲线只需要一次训练运行，这使得即使对于大型数据集，当单次训练运行需要数小时甚至数天时，其跟踪也是实用的。损失曲线有助于在整个训练过程中保持对脉搏的把握。如果你只运行了200个训练时期中的10个，你就可以了解损失值是否按预期进展，或者是否存在使进一步训练无意义的问题。
- en: Be sure you track loss curves, collect them for all conducted experiments, and
    make them available for future analysis. Incorporating loss-curve monitoring into
    the training pipeline in the early stages of its building is a valuable one-time
    effort because you will need these insights for all future experiments, and it
    is helpful for the overall pipeline’s reproducibility (we’ll have an in-depth
    look into this subject in chapter 10).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一定要跟踪损失曲线，收集所有进行的实验的损失曲线，并使它们可用于未来的分析。在构建训练流程的早期阶段就将其纳入损失曲线监控是一个非常有价值的单次努力，因为您将需要这些洞察来用于所有未来的实验，并且对整个流程的可重复性有帮助（我们将在第10章深入探讨这个主题）。
- en: 9.1.3 Interpreting loss curves
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.3 解释损失曲线
- en: There are several main patterns in the behavior of loss curves that deviate
    from what is expected by design. Let’s conduct a brief analysis of each pattern
    and see how we can interpret different patterns we may encounter while debugging
    the system (what conclusions can be made and what steps should be taken to debug
    detected issues).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 损失曲线的行为存在几种主要模式，与设计期望不符。让我们简要分析每种模式，并看看我们如何解释在调试系统时可能遇到的不同模式（可以得出什么结论，以及应采取哪些步骤来调试检测到的问题）。
- en: Pattern 1
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模式1
- en: 'Pattern 1 shows that the loss curve diverges (not converging to the desired
    loss value and oscillating instead; see figure 9.3). How can we try to make the
    training process more stable? Consider the following:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 模式1表明损失曲线发散（没有收敛到期望的损失值，而是振荡；参见图9.3）。我们如何尝试使训练过程更稳定？考虑以下因素：
- en: Check if features and targets are correlated in any way—or if samples and labels
    are passed to the model in the right order.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查特征和目标是否以任何方式相关——或者样本和标签是否按正确顺序传递给模型。
- en: Reduce the learning rate to prevent the model from bouncing around in the parameter
    space.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低学习率以防止模型在参数空间中弹跳。
- en: Reduce the dataset size to a single batch (or 10–100 samples) and check if the
    model is able to overfit them.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据集大小减少到单个批次（或10-100个样本），并检查模型是否能够过度拟合它们。
- en: Start with a simpler model and add complexity incrementally. Each time, check
    whether it outperforms the constant baseline or rule-based baseline.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从一个更简单的模型开始，逐步增加复杂性。每次，检查它是否优于恒定基线或基于规则的基线。
- en: '![figure](../Images/CH09_F03_Babushkin.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_F03_Babushkin.png)'
- en: Figure 9.3 Loss is oscillating, which demonstrates a lack of convergence.
  id: totrans-51
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.3 损失在振荡，这表明没有收敛。
- en: Pattern 2
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模式2
- en: Pattern 2 shows a loss explosion or trending toward NaN (not a number) (see
    figure 9.4). This behavior indicates computational problems when either the gradient
    is exploded (in which case solutions like gradient clipping, lower learning rate,
    or different weights initialization techniques may help) or some mathematical
    problems emerge (e.g., division by zero, the logarithm of zero or negative numbers,
    or NaNs in data—thus, it usually indicates an error in implementation or lack
    of data preprocessing).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 模式2表明损失爆炸或趋向于NaN（不是一个数字）（参见图9.4）。这种行为表明，当梯度爆炸（在这种情况下，梯度裁剪、降低学习率或不同的权重初始化技术等解决方案可能有所帮助）或出现某些数学问题时（例如，除以零、零或负数的对数、或数据中的NaN——因此，这通常表明实现错误或数据预处理不足）。
- en: '![figure](../Images/CH09_F04_Babushkin.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_F04_Babushkin.png)'
- en: Figure 9.4 A model was converging until something went wrong.
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.4 模型在出现错误之前一直在收敛。
- en: Pattern 3
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模式3
- en: Pattern 3 shows that the loss decreases, but the metric is contradictory (see
    figure 9.5). If the model continues improving based on the loss but the metric
    is stuck, it may signal that the chosen metric is inadequate for the problem or
    poorly implemented. Typically, it happens in classification and other related
    tasks where we use metrics that include a certain threshold.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 模式3表明损失在下降，但指标是矛盾的（参见图9.5）。如果模型根据损失继续改进，但指标停滞不前，这可能表明所选指标不适合该问题或实现不当。通常，这种情况发生在分类和其他相关任务中，我们使用包括一定阈值的指标。
- en: '![figure](../Images/CH09_F05_Babushkin.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_F05_Babushkin.png)'
- en: Figure 9.5 Loss decreases while the metric stays constantly low.
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.5 损失在下降，而指标保持恒定低。
- en: Campfire story from Valerii
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Valerii的篝火故事
- en: When I was working at a company providing messaging services, one of the tasks
    we had was the improvement of existing antispam and antifraud systems. The main
    challenge we faced was that the model, which had been trained on a given dataset,
    showed promising metrics during offline testing but didn’t perform as expected
    after deployment.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当我在一家提供消息服务公司的公司工作时，我们的一项任务是改进现有的反垃圾邮件和反欺诈系统。我们面临的主要挑战是，在给定数据集上训练的模型在离线测试期间显示出有希望的指标，但在部署后并没有达到预期的效果。
- en: 'After digging into possible causes, we found three main problems:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入调查可能的原因后，我们发现存在三个主要问题：
- en: We didn’t use proper metrics for offline testing. For example, such metrics
    as precision at defined recall are not that useful for fraud detection as they
    are class sensitive (precision), while specificity at a defined recall yielded
    better results (see chapter 5). However, recall is somewhat of Schrödinger’s cat
    itself, as we never had its full picture for fraud (there were fraud cases we
    missed, and we didn’t know how many of them); thus, our recall could be calculated
    only on a subset of known fraud cases.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们没有使用适当的指标进行离线测试。例如，像在定义的召回率下的精确度这样的指标对于欺诈检测并不那么有用，因为它们是类别敏感的（精确度），而在定义的召回率下的特异性则产生了更好的结果（参见第5章）。然而，召回率本身就像薛定谔的猫一样，因为我们从未完全了解欺诈的全貌（我们错过了欺诈案例，也不知道有多少）；因此，我们的召回率只能基于已知欺诈案例的子集来计算。
- en: The second problem was hiding in performance evaluation. We conducted it on
    a point estimate basis, but reality has a tendency to deviate from point estimates,
    and given the scale of 100 billion events per day, even a 0.1% deviation would
    translate to a significant number (100 million) of events being misclassified
    compared to what we expected.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个问题隐藏在性能评估中。我们基于点估计进行评估，但现实往往偏离点估计，考虑到每天有1000亿个事件这一规模，即使是0.1%的偏差也会导致与预期相比有显著的错误分类事件数量（1亿）。
- en: The third problem lay in the fact that we evaluated the system through a spam/nonspam
    binary classification, using log loss as the loss function. What we really needed
    was to keep users happy and reduce spam to the appropriate levels (there is always
    spam, but sometimes it’s not a big deal, and other times it is a problem), excluding
    such cases as receiving 100,000 messages from a single number within 1 second.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个问题在于我们通过垃圾邮件/非垃圾邮件的二分类来评估系统，使用对数损失作为损失函数。我们真正需要的是让用户满意并将垃圾邮件减少到适当的水平（总是会有垃圾邮件，但有时并不是大问题，有时则是个问题），排除像在1秒内从单个号码接收10万条消息这样的情况。
- en: While the first two problems were challenging but somewhat manageable, the third
    problem was extremely complex, demanding a proper hierarchy of metrics, a custom
    loss function, and continuous experimentation.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前两个问题具有挑战性但尚可管理，但第三个问题极其复杂，需要适当的指标层次结构、自定义损失函数以及持续实验。
- en: 'That is the essence of ML: we train a model using specific loss, we measure
    its performance using different sets of metrics in the hope of achieving something
    different from the first and second, and sooner or later, we will face a plateau
    where the first or even second element improves but the third does not react.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是机器学习的本质：我们使用特定的损失函数训练模型，我们使用不同的指标集来衡量其性能，希望得到与第一和第二次不同的结果，迟早我们会遇到一个平台期，其中第一个甚至第二个元素有所改善，但第三个元素没有反应。
- en: The most important lesson we learned from this case was that while you can adjust
    your error analysis until it reaches perfection, it will not save you from a fiasco
    if you incorrectly pick metrics in the first place.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从这个案例中学到的最重要的教训是，尽管你可以调整你的错误分析直到达到完美，但如果一开始就错误地选择了指标，这并不能让你免于灾难。
- en: Pattern 4
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模式 4
- en: Pattern 4 shows the converging training curve with unexpected loss values. While
    the curvature of the training curve appears promising, the observed values are
    perplexing. To identify such anomalies in advance, it is advisable to do a sanity
    check by running a simple unit on a single batch that asserts if the loss falls
    within the expected range. Often, the reason behind such an issue lies in scaling
    transformations (e.g., normalization of an image or a mask in segmentation).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 模式 4显示了训练曲线的收敛以及意外的损失值。虽然训练曲线的曲率看起来很有希望，但观察到的值令人困惑。为了提前识别这样的异常，建议通过在一个批次上运行简单的单元测试来执行合理性检查，以确认损失是否在预期的范围内。通常，这种问题的原因在于缩放变换（例如，图像或分割中的掩模归一化）。
- en: Pattern 5
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模式 5
- en: '![figure](../Images/CH09_F06_Babushkin.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_F06_Babushkin.png)'
- en: Figure 9.6 Loss is decreasing for training but not validation, reflecting potential
    overfit.
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.6 训练损失在下降，但验证损失没有下降，反映了潜在的过拟合。
- en: Pattern 5 shows that the training loss decreases while the validation loss increases
    (see figure 9.6). This is a classic textbook example of overfitting due to high
    variance. In these cases, you should restrict the model’s capacity, either by
    reducing its complexity directly or by increasing regularization.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 模式5显示，训练损失在下降，而验证损失在上升（见图9.6）。这是由于高方差导致的过拟合的经典教科书示例。在这些情况下，你应该限制模型的容量，要么直接减少其复杂性，要么通过增加正则化来实现。
- en: 9.1.4 Model-wise learning curve
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.4 模型级学习曲线
- en: After we ensure the model converges and the training loss reaches the plateau
    with no drastic overfitting or underfitting, we can wrap up our learning-curve
    analysis and move on. This is especially relevant at the stage of initial deployment.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们确保模型收敛并且训练损失达到平台期，没有剧烈的过拟合或欠拟合之后，我们可以结束学习曲线分析并继续前进。这在初始部署阶段尤其相关。
- en: 'However, if we face overfitting/underfitting issues or there is enough time
    to experiment with the optimal model size, that is when the second type of learning
    curve comes into play (see figure 9.7):'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们面临过拟合/欠拟合问题，或者有足够的时间实验最佳模型大小，那么第二种类型的学习曲线就派上用场了（见图9.7）：
- en: First, we pick a hyperparameter that represents a varying model complexity.
    Again, it may be the tree depth in gradient boosting, a regularization strength,
    a number of features, or a number of layers in a deep neural network.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们选择一个代表可变模型复杂性的超参数。再次，它可能是梯度提升中的树深度，正则化强度，特征数量，或者深度神经网络中的层数。
- en: We define a grid for this hyperparameter (e.g., 2, 3, 4, …, 16 for the tree
    depth; 10^(-2), 10^(-1), 1, 10, 10², 10³ for regularization term).
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为这个超参数定义一个网格（例如，树深度的2，3，4，……，16；正则化项的10^(-2)，10^(-1)，1，10，10²，10³）。
- en: We train each model until convergence and capture the final loss/metric values.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们训练每个模型直到收敛，并捕获最终的损失/指标值。
- en: '![figure](../Images/CH09_F07_Babushkin.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH09_F07_Babushkin.png)'
- en: Figure 9.7 Finding the optimal model complexity based on learning curves
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.7 基于学习曲线寻找最佳模型复杂性
- en: Now we map these values on the vertical axis and corresponding hyperparameter
    values on the horizontal axis. This learning curve helps us easily see what range
    of model complexity (determined by this hyperparameter) is optimal for the given
    data.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将这些值映射到垂直轴上，并将相应的超参数值映射到水平轴上。这个学习曲线帮助我们轻松地看到对于给定的数据，哪个模型复杂度范围（由这个超参数决定）是最优的。
- en: 9.1.5 Sample-wise learning curve
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.5 样本级学习曲线
- en: 'Finally, let’s vary the dataset size. We discussed this technique in detail
    in chapter 6, section 6.4\. In short, we keep the validation set unchanged and
    probe different numbers of samples in the training set: 100, 1,000, 10,000, and
    so on. As in the model-wise learning curve, we train the model until it reaches
    convergence and plot training and validation learning curves.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们改变数据集的大小。我们在第6章第6.4节中详细讨论了这项技术。简而言之，我们保持验证集不变，并在训练集中探测不同数量的样本：100，1,000，10,000等等。就像在模型级学习曲线中一样，我们训练模型直到其收敛，并绘制训练和验证学习曲线。
- en: If we extrapolate the validation metric, we can estimate how much new data we
    need to increase the metric by 1% and vice versa. If we expect to gather N more
    samples of data, we can forecast what metric gain it’ll give.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们外推验证指标，我们可以估计需要多少新数据才能使指标增加1%，反之亦然。如果我们预计收集N个更多数据样本，我们可以预测这将带来多少指标提升。
- en: Besides this extrapolation, the sample-wise learning curve also serves the purpose
    of revealing overfitting and underfitting. Specifically, what insights do we get
    by analyzing training and validation curves?
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这种外推之外，样本级学习曲线还起到揭示过拟合和欠拟合的作用。具体来说，通过分析训练和验证曲线，我们能得到哪些见解？
- en: If the curves almost converge (there is a small or no gap between curves at
    the maximum number of samples), the model generalizes well, and there’s no need
    to add more samples to the dataset, as it will not increase the model’s performance.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果曲线几乎收敛（在最大样本数时曲线之间存在小的或没有差距），则模型泛化良好，无需向数据集中添加更多样本，因为这不会提高模型性能。
- en: Specifically, if the training and validation curves almost converge but the
    loss level remains high in both, it reports a high bias problem (underfitting).
    In this scenario, increasing the dataset size will not help either. What can be
    fruitful is using a more complicated model.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具体来说，如果训练和验证曲线几乎收敛，但损失水平在两者中都保持较高，则报告高偏差问题（欠拟合）。在这种情况下，增加数据集大小也不会有所帮助。可能有益的是使用更复杂的模型。
- en: If there is a large gap between the curves, it signals either a high variance
    problem or simply a difference between the training set and validation set. In
    the first case, we should reduce model complexity or gather more data to combat
    this problem. In the second case, we need to examine the data-splitting procedure
    and ensure it fairly represents real-world scenarios the model will encounter.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果曲线之间存在较大差距，这表明存在高方差问题或训练集和验证集之间的简单差异。在前一种情况下，我们应该减少模型复杂性或收集更多数据来解决这个问题。在后一种情况下，我们需要检查数据拆分过程，并确保它公平地代表模型将遇到的现实世界场景。
- en: A sample-wise learning curve indicates whether the current bottleneck in the
    system is the amount of data or not. Understanding the metric dependency on dataset
    size guides our next steps in improving the system, which could include a combination
    of gathering more data and investing effort in feature engineering and model hyperparameter
    tuning.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 按样本阶段的学习曲线表明当前系统中的瓶颈是否是数据量。理解指标对数据集大小的依赖性指导我们下一步改进系统，这可能包括收集更多数据和投入精力进行特征工程以及模型超参数调整。
- en: 9.1.6 Double descent
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.6 双重下降
- en: The bias–variance tradeoff runs like clockwork for classical ML models and deep
    neural networks of moderate size. However, for modern overparametrized deep neural
    networks, things get more perplexing.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差-方差权衡在经典机器学习模型和适度规模的深度神经网络中像时钟一样运行。然而，对于现代过参数化的深度神经网络，事情变得更加复杂。
- en: 'There is a phenomenon called *double descent*, where the test error first gets
    better, then worse, and then better again. Surprisingly, researchers found different
    regimes of double descent corresponding to all three learning curves: epoch-wise,
    model-wise, and sample-wise.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一种称为*双重下降*的现象，其中测试误差首先变好，然后变差，然后再变好。令人惊讶的是，研究人员发现与所有三个学习曲线相对应的不同双重下降阶段：按时间阶段、按模型阶段和按样本阶段。
- en: 'It is still an open question what the mechanism behind the double descent is,
    why it occurs, and whether it means that overfitting for large deep neural networks
    is not an issue. The common hypothesis behind the double descent is the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 双重下降背后的机制是什么，为什么它会发生，以及它是否意味着大型深度神经网络的重过拟合不是一个问题，这仍然是一个未解之谜。双重下降背后的常见假设如下：
- en: If the model’s capacity (the number of parameters) is lower than the dataset
    size, it tries to approximate the data, leading to the classical regime where
    the bias–variance tradeoff takes place. We call this model *underparametrized*.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型的容量（参数数量）低于数据集大小，它试图逼近数据，导致偏差-方差权衡发生的经典阶段。我们称这种模型为*欠参数化*。
- en: At the interpolation threshold, the model has sufficient ability to fit the
    training data perfectly and reach zero bias. There is effectively only one such
    model in this parameter space. Forcing this model to fit even slightly noisy labels
    will destroy its global structure.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在插值阈值处，模型有足够的能力完美地拟合训练数据并达到零偏差。在这个参数空间中实际上只有一个这样的模型。强迫这个模型拟合甚至稍微有噪声的标签将破坏其全局结构。
- en: However, in an *overparametrized* regime, there are many models of this kind.
    Some of them not only interpolate the train set but also perform well on the test
    set. It turns out that stochastic gradient descent leads to such “good models”
    for reasons we don’t yet understand.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，在过参数化阶段，有许多这样的模型。其中一些不仅插值训练集，而且在测试集上表现良好。结果发现，随机梯度下降由于我们尚未理解的原因导致这些“好模型”。
- en: For further details, we recommend reading “Deep Double Descent” by OpenAI ([https://openai.com/index/deep-double-descent/](https://openai.com/index/deep-double-descent/)).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更详细的信息，我们建议阅读OpenAI的《深度双重下降》（[https://openai.com/index/deep-double-descent/](https://openai.com/index/deep-double-descent/))。
- en: The modern scaling laws of large neural networks redefine our modeling strategies.
    The double-descent phenomenon may surprise those who are only familiar with the
    classical bias–variance tradeoff, and it is crucial to consider it when selecting
    a model for the system (especially when working with large convolutional networks
    and transformers), as well as when determining training and debugging procedures.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 大型神经网络的现代缩放定律重新定义了我们的建模策略。双下降现象可能会让那些只熟悉经典偏差-方差权衡的人感到惊讶，在选择系统模型时（尤其是在处理大型卷积网络和变压器时）以及确定训练和调试程序时，考虑这一点至关重要。
- en: We only mention this here to highlight that most of the heuristics described
    previously are not solid laws set in stone. As with many things in ML design,
    they reveal signals—often useful ones—but may not be an exact fit for a particular
    problem.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里只提到这一点，以强调之前描述的大多数启发式方法并不是一成不变的定律。就像机器学习设计中许多事情一样，它们揭示了信号——通常是很有用的信号——但可能并不完全适合特定的问题。
- en: 9.2 Residual analysis
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 残差分析
- en: Certainly, coming up with new ideas is important, but even more important, to
    understand the results.— Ilya Sutskever
  id: totrans-103
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当然，提出新想法很重要，但更重要的是理解结果。——伊利亚·苏茨克维
- en: 'Once we’ve ensured that a ML model has converged and is not plagued by underfitting
    or overfitting, the next step in model debugging is to perform residual analysis.
    This involves studying individual predictions made by the model compared to their
    corresponding true labels. Residual analysis involves calculating the differences
    between the predicted and actual values, known as *residuals* (see figure 9.8):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们确保了机器学习模型已经收敛并且没有受到欠拟合或过拟合的困扰，模型调试的下一步就是进行残差分析。这涉及到研究模型做出的单个预测与其相应的真实标签之间的差异。残差分析包括计算预测值和实际值之间的差异，称为*残差*（见图9.8）：
- en: '*residual[i] = y_pred[i] – y_true[i]*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*residual[i] = y_pred[i] – y_true[i]*'
- en: First, what exactly are residuals? In the narrow sense, residuals are simply
    the differences between predicted and true values in regression. In a wider sense,
    residuals can be any sample-wise differences or errors between model predictions
    and ground truth.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，残差究竟是什么？在狭义上，残差仅仅是回归中预测值和真实值之间的差异。在广义上，残差可以是模型预测和真实值之间的任何样本级差异或误差。
- en: '![figure](../Images/CH09_F08_Babushkin.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_F08_Babushkin.png)'
- en: 'Figure 9.8 Basic case: single regressor x. Each vertical line represents a
    residual error (e) or, simply, residual. Lines above the regressor have negative
    signs (the actual value is higher than the predicted value), and lines below the
    regressor have positive signs.'
  id: totrans-108
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.8 基本情况：单个回归器x。每条垂直线代表一个残差误差（e）或简单地说是残差。回归器上方的线带有负号（实际值高于预测值），而回归器下方的线带有正号。
- en: Thus, to align residuals to a specific loss, you may prefer using a single term
    of the loss sum as a pseudo-residual instead of raw differences, which are a squared
    error for mean squared error, a logarithmic error for root mean squared logarithmic
    error, or a class label multiplied by the predicted probability of this class
    for LogLoss.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了将残差与特定的损失函数对齐，你可能更愿意使用损失总和的单个项作为伪残差，而不是原始差异，这些差异对于均方误差是平方误差，对于均方对数误差是对数误差，对于LogLoss是类别标签乘以该类别的预测概率。
- en: Often, residuals are associated only with regression and classification tasks.
    But what about residuals outside of regression and classification tasks? Looking
    more broadly, we will find equivalent tools in almost any ML-related task.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，残差仅与回归和分类任务相关联。但回归和分类任务之外的残差又如何呢？从更广泛的角度来看，我们几乎可以在任何与机器学习相关的任务中找到等效的工具。
- en: For instance, in the search engine context, true labels are often mappings from
    a search query to the top N most relevant documents or products. To calculate
    residuals in this context, we can measure the difference between the rank predicted
    by the model and the true rank of each item in the top N list (see figure 9.9).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在搜索引擎的背景下，真实标签通常是从搜索查询到前N个最相关文档或产品的映射。为了计算这种背景下的残差，我们可以测量模型预测的排名与列表中每个项目的真实排名之间的差异（见图9.9）。
- en: '![figure](../Images/CH09_F09_Babushkin.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_F09_Babushkin.png)'
- en: 'Figure 9.9 Example of residuals for the image segmentation problem (image source:
    [https://arxiv.org/abs/1810.13230](https://arxiv.org/abs/1810.13230))'
  id: totrans-113
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.9 图像分割问题的残差示例（图片来源：[https://arxiv.org/abs/1810.13230](https://arxiv.org/abs/1810.13230))
- en: In image segmentation, we can compute the differences between predicted and
    ground truth masks for each image, which yields 2D residuals that highlight which
    parts of an object are not covered by the mask or are covered incorrectly.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像分割中，我们可以计算每个图像的预测和真实掩码之间的差异，这会产生二维残差，突出显示哪些对象的部分没有被掩码覆盖或被错误地覆盖。
- en: 9.2.1 Goals of residual analysis
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.1 残差分析的目标
- en: Residual analysis helps identify patterns in the errors made by the model so
    that we can detect clear directions for improving the system. Whereas the overall
    error of a model is usually represented by a single number, such as a loss or
    metric, the residual analysis does the opposite. It examines the raw differences
    between predictions and true labels, providing a more fine-grained diagnostic
    of the model’s performance.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 残差分析有助于识别模型所犯错误中的模式，以便我们可以检测到改进系统的明确方向。与模型的整体误差通常用一个单一的数字表示，如损失或度量不同，残差分析则相反。它检查预测值和真实标签之间的原始差异，为模型性能提供更细致的诊断。
- en: 'Along with that, there are other main purposes of residual analysis:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，残差分析还有其他主要目的：
- en: '*Verify model assumptions.* First, it challenges our basic assumptions about
    the model. Do residuals follow a normal distribution? Are the model’s predictions
    biased or not? If we identify any significant discrepancies, we may need to reevaluate
    our approach or choose a different model.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*验证模型假设。* 首先，它挑战了我们关于模型的基本假设。残差是否遵循正态分布？模型的预测是否存在偏差？如果我们发现任何显著的差异，我们可能需要重新评估我们的方法或选择不同的模型。'
- en: '*Detect sources of metric change.* The overall performance may increase or
    may remain unchanged. Either way, capturing a significant change in the metric
    distribution is possible. Which data samples show varying residual patterns with
    different models? In which data subset do we have the greatest number of wrong
    answers? What samples affect the final score the most?'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*检测度量变化的原因。* 整体性能可能提高或保持不变。无论如何，捕捉度量分布的显著变化是可能的。哪些数据样本在不同模型中显示出不同的残差模式？在哪个数据子集中，我们错误答案的数量最多？哪些样本对最终分数的影响最大？'
- en: '*Ensure fairness of residuals.* Residual analysis enables us to evaluate if
    the model treats every sample fairly and has the same distribution across different
    cohorts. If we identify any significant skewness or disparities, we can make respective
    adjustments to ensure that the model is unbiased and treats all samples equally.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*确保残差的公平性。* 残差分析使我们能够评估模型是否公平地对待每个样本，并在不同的群体中具有相同的分布。如果我们发现任何显著的偏斜或不平等，我们可以做出相应的调整，以确保模型无偏见并平等地对待所有样本。'
- en: '*Perform worst-case and best-case analysis.* Is there a commonality between
    the top N samples with the biggest residuals? What should we change so that our
    model performs better in these cases? What about a top N list with the smallest
    residuals?'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*执行最坏情况和最好情况分析。* 最大的N个残差样本之间是否存在共性？我们应该改变什么，以便我们的模型在这些情况下表现更好？对于最小的N个残差样本呢？'
- en: '*Examine corner cases.* How does our model perform on users with the shortest
    or longest history or on shortest/longest audio records, texts, and sessions,
    depending on the problem we solve? How does it deal with items with the lowest/highest
    price, zero stocks, or highest revenue? We must be familiar with business cases
    and the nature of data to evaluate all possible pitfalls.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*检查边界情况。* 对于具有最短或最长历史记录的用户，或者根据我们解决的问题，最短/最长的音频记录、文本和会话，我们的模型表现如何？它如何处理价格最低/最高的项目、零库存或最高收入的项目？我们必须熟悉业务案例和数据性质，以评估所有可能的陷阱。'
- en: These questions are the essence of the residual analysis, and finding answers
    to them closes the feedback loop of offline evaluation. The earlier we start capturing
    hard samples (and loss curves) for conducted experiments, the better. It is a
    good practice to collect 10 to 20 objects with the largest residuals after training
    as attendant artifacts. It is difficult to overestimate the value of thinking
    through such steps in a training pipeline in a design document.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题是残差分析的核心，找到这些问题的答案就关闭了离线评估的反馈循环。我们越早开始收集进行实验的硬样本（和损失曲线），就越好。在设计文档中，收集训练后具有最大残差的10到20个对象作为附属工件是一种好习惯。很难高估在设计文档的训练流程中思考这些步骤的价值。
- en: In the project’s later phases, we transform it into a part of automatic reports
    for every trained model, along with model drift monitoring and data quality reports.
    Let’s say the metric increased in most cases but dropped a bit in a crucial segment.
    Depending on our policy, we may either reject this version or simply pay extra
    attention to this change in forthcoming iterations.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在项目的后期阶段，我们将它转化为每个训练模型的自动报告的一部分，包括模型漂移监控和数据质量报告。假设大多数情况下指标有所增加，但在一个关键部分略有下降。根据我们的政策，我们可能要么拒绝这个版本，要么在未来的迭代中额外关注这种变化。
- en: 9.2.2 Model assumptions
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.2 模型假设
- en: Whichever model we train, we have prior assumptions about its predictions, biases,
    and residual distribution. The assumption check helps us ensure that we picked
    the right model, collected enough data, and engineered the right features. If
    the assumptions reveal an unexpected pattern, it may prompt the exploration of
    alternative solutions.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们训练哪种模型，我们对其预测、偏差和残差分布都有先验假设。假设检查帮助我们确保我们选择了正确的模型，收集了足够的数据，并构建了正确的特征。如果假设揭示出意外的模式，可能会促使我们探索替代解决方案。
- en: Again, from the design perspective, we need to figure out in advance what we
    assume to be true about the model’s predictions or, specifically, residuals and
    express it via corresponding unit tests. It will prevent unexpected model behavior
    after the next deployment. In the following chapter, we will dive into a more
    holistic overview of tests and their role in the training pipeline. Let’s explore
    two different examples to see how assumptions can be applied.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，从设计角度来看，我们需要提前确定我们对模型预测或，具体来说，残差的真实假设是什么，并通过相应的单元测试来表达。这将防止在下次部署后出现意外的模型行为。在下一章中，我们将深入探讨测试的更全面概述及其在训练流程中的作用。让我们探索两个不同的例子，看看假设是如何应用的。
- en: Example 1\. Linear regression assumptions
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例 1\. 线性回归假设
- en: Suppose we solve the demand forecasting problem using a simple linear regression.
    What are the key assumptions we make here?
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们使用简单的线性回归来解决需求预测问题。我们在这里做出了哪些关键假设？
- en: '*Linearity**—*The relationship between predictors (x) and target (y) is linear.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*线性**—*预测变量（x）和目标（y）之间的关系是线性的。'
- en: '*Strict exogeneity**—*Residuals should be zero-centered.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*严格外生性**—*残差应该是零中心的。'
- en: '*Normality**—*Residuals are assumed to be normally distributed.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*正态性**—*假设残差是正态分布的。'
- en: '*Homoscedasticity**—*The variance of the residual is the same for any value
    of X.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*同方差性**—*残差的方差对于任何X的值都是相同的。'
- en: '*Independence**—*Residual error terms should be independent.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*独立性**—*残差误差项应该是独立的。'
- en: After fitting our model, we check whether these assumptions hold true. Potential
    problems include
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在拟合我们的模型之后，我们检查这些假设是否成立。潜在问题包括
- en: '*Nonlinearity* in X-Y relationships'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: X-Y关系中的 *非线性*
- en: '*Bias* in residuals'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*偏差* 在残差中'
- en: '*Heteroscedasticity:* nonconstant variance of error terms'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*异方差性:* 错误项的非常数方差'
- en: 'Presence of data points with *extremely high influence*: outliers in predicted
    values (y) or in regressors (x)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存在具有 *极高影响* 的数据点：预测值（y）或回归变量（x）中的异常值
- en: 'To check regression assumptions, we’ll examine the distribution of residuals.
    For this purpose, we plot residuals in four different ways and build so-called
    *diagnostic plots* (see figures 9.10 and 9.11):'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查回归假设，我们将检查残差的分布。为此，我们以四种不同的方式绘制残差，并构建所谓的 *诊断图*（见图9.10和9.11）：
- en: '*Residuals vs. fitted**—*Utilized to evaluate the assumptions of a linear relationship.
    A horizontal line that lacks distinct patterns suggests a linear relationship,
    which is favorable. No difference between the solid and dashed lines means strong
    linear dependence.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*残差与拟合**—*用于评估线性关系的假设。一条没有明显模式的水平线表明存在线性关系，这是有利的。实线和虚线之间没有差异意味着强烈的线性依赖。'
- en: '*Normal quantile-quantile (Q-Q) plot**—*Used to examine whether the residuals
    are normally distributed. We plot quantiles of the standard normal distribution
    as x-coordinates and quantiles of standardized residuals (residuals after subtracting
    the mean and dividing by standard deviation) as y-coordinates. If the resulting
    points are close to the straight line (dotted line on the plot), residuals follow
    a normal distribution.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*正态分位数-分位数（Q-Q）图**—*用于检查残差是否正态分布。我们将标准正态分布的分位数作为x坐标，将标准化残差（减去均值并除以标准差后的残差）的分位数作为y坐标。如果得到的点接近直线（图上的虚线），则残差遵循正态分布。'
- en: '*Scale-location**—*Used to assess the homogeneity of variance among the residuals.
    A horizontal line with evenly dispersed points is a strong sign of homoscedasticity.
    This is not the case in our example, where we have a heteroscedasticity problem
    (higher fitted values have higher variance).'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*尺度-位置**—*用于评估残差中方差的一致性。一条水平线且点均匀分布是同方差性的强烈迹象。在我们的例子中并非如此，我们有一个异方差性问题（拟合值越高，方差越大）。'
- en: '*Residuals vs. leverage**—*Used to identify influential cases, meaning extreme
    values that could affect the regression results when included or excluded from
    the analysis. *Leverage* refers to the extent to which the coefficients in the
    regression model would change if we removed a particular observation from the
    dataset. There is a commonly used measurement for influential data points called
    Cook’s Distance.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*残差与杠杆率**—*用于识别影响较大的案例，即可能影响回归结果时包含或排除分析中的极端值。*杠杆率*指的是如果我们从数据集中移除特定观测值，回归模型中的系数会改变的幅度。有一个常用的测量影响数据点的指标，称为库克距离（Cook’s
    Distance）。'
- en: '![figure](../Images/CH09_F10_Babushkin.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH09_F10_Babushkin.png)'
- en: 'Figure 9.10 Four diagnostic plots for linear regression’s residual analysis
    in both cases (Case 1: Assumptions are met)'
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.10 两种情况（情况1：假设成立）下线性回归残差分析的四个诊断图
- en: '![figure](../Images/CH09_F11_Babushkin.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH09_F11_Babushkin.png)'
- en: 'Figure 9.11 Four diagnostic plots for linear regression’s residual analysis
    in both cases (Case 2: Assumptions are not met)'
  id: totrans-148
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.11 两种情况（情况2：假设不成立）下线性回归残差分析的四个诊断图
- en: Sometimes it is beneficial to force the model to follow our assumptions more
    strictly by incorporating our prior knowledge directly into the model.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 有时通过直接将我们的先验知识纳入模型，强制模型更严格地遵循我们的假设是有益的。
- en: Example 2\. Attention plot
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例2. 注意力图
- en: Imagine you’re the product owner of a banking application, and your next big
    update is to add a voice assistant. After looking into your “inner circle” of
    specialists, you hire Stacy, a world-class master in text-to-speech (TTS) systems.
    After several weeks of work, Stacy builds a first version of a voice synthesis
    system.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你是银行应用程序的产品所有者，你的下一个重大更新是添加语音助手。在调查了你的“内圈”专家后，你聘请了Stacy，一位在文本到语音（TTS）系统方面世界级的专家。经过几周的工作，Stacy
    构建了一个语音合成系统的第一个版本。
- en: 'In the realm of TTS tasks, there exists a fundamental assumption: the order
    of characters in a text should progress linearly over time in the corresponding
    audio segments. When we read a text, it’s natural to assume that the text’s position
    aligns closely with the audio we hear. This stands in contrast to other sequence-to-sequence
    tasks, like machine translation, where an attention module is necessary to resolve
    word alignment between languages with different syntax or token ordering, such
    as English and Chinese.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在TTS任务领域，存在一个基本假设：文本中字符的顺序应该在对应音频段中随时间线性增长。当我们阅读文本时，自然会假设文本的位置与听到的音频紧密对齐。这与其他序列到序列任务形成对比，例如机器翻译，在这些任务中，需要一个注意力模块来解决不同句法或标记顺序的语言之间的词对齐，例如英语和中文。
- en: To assess the validity of this assumption, Stacy employs an *attention plot*—a
    visual representation that depicts the activation map between audio frames (x-axis)
    and characters (y-axis). By observing this plot at regular intervals during training,
    Stacy aims to evaluate how closely it resembles a nearly diagonal matrix.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估这个假设的有效性，Stacy 使用了一个*注意力图*——一种视觉表示，描述了音频帧（x轴）和字符（y轴）之间的激活映射。通过在训练过程中定期观察这个图，Stacy
    旨在评估它多么接近一个几乎对角线的矩阵。
- en: To force the attention matrix to exhibit a near-diagonal pattern, Stacy employs
    a technique known as *guided attention*. Whenever the attention matrix deviates
    significantly from the diagonal, it is penalized using an auxiliary loss. This
    heuristic not only accelerates the training process but also steers the model
    toward meaningful solutions that align with the underlying assumption from the
    start (see figure 9.12).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使注意力矩阵呈现出接近对角线的模式，Stacy 采用了一种称为*引导注意力*的技术。每当注意力矩阵与对角线显著偏离时，它就会使用辅助损失进行惩罚。这种启发式方法不仅加速了训练过程，而且从一开始就引导模型朝着与潜在假设一致的有意义解决方案发展（见图9.12）。
- en: '![figure](../Images/CH09_F12_Babushkin.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH09_F12_Babushkin.png)'
- en: Figure 9.12 Attention plot evolution through training without (left) and with
    (right) guided attention loss
  id: totrans-156
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.12 无（左）和有（右）引导注意力损失的注意力图训练演变
- en: 'The deviation of the attention plot from the diagonal matrix is nothing but
    residuals. Residuals of moderate size are appropriate: some characters people
    speak more quickly than others; therefore, the plot will not represent a straight
    line. However, large residuals reveal the specific sounds or character combinations
    that the model can’t learn well.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力图与对角矩阵的偏差不过是残差。适度的残差是合适的：人们说话的速度有快有慢；因此，图表不会代表一条直线。然而，大的残差揭示了模型无法很好地学习的特定声音或字符组合。
- en: Armed with this knowledge, Stacy could shape a forthcoming data-gathering strategy
    to address the model’s difficulties and improve its performance.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 带着这些知识，Stacy可以制定一个即将到来的数据收集策略，以解决模型的困难并提高其性能。
- en: To learn more about the architecture of a typical TTS network, including details
    on the construction of the attention module in this case, we recommend studying
    the paper “Efficiently Trainable Text-to-Speech System Based on Deep Convolutional
    Networks with Guided Attention” ([https://arxiv.org/abs/1710.08969](https://arxiv.org/abs/1710.08969)).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于典型TTS网络架构的信息，包括此情况下注意力模块构建的细节，我们建议研究论文“基于深度卷积网络和引导注意力的高效可训练语音合成系统”（[https://arxiv.org/abs/1710.08969](https://arxiv.org/abs/1710.08969)）。
- en: 9.2.3 Residual distribution
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.3 残差分布
- en: If none of the model’s assumptions are met, it is a sign to adjust the training
    pipeline, collect more data, engineer new features, or explore alternative models
    and losses. But how do we determine the necessary improvement steps? The guess-and-check
    approach may seem tempting, but we don’t recommend it for exploring the solution
    space.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型的所有假设都不成立，那么这是一个调整训练流程、收集更多数据、工程新特征或探索替代模型和损失的信号。但我们是怎样确定必要的改进步骤的呢？猜测和检查的方法可能看起来很有吸引力，但我们不建议用它来探索解决方案空间。
- en: Let’s, for example, violate the normality assumption for the same linear regression
    (figures 9.13–9.15).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们违反相同线性回归的正态性假设（图9.13-9.15）。
- en: '![figure](../Images/CH09_F13_Babushkin.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_F13_Babushkin.png)'
- en: Figure 9.13 In Case 1, we observe a normal residual distribution when linearity
    assumptions are met.
  id: totrans-164
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.13 在案例1中，当线性假设成立时，我们观察到正态的残差分布。
- en: '![figure](../Images/CH09_F14_Babushkin.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_F14_Babushkin.png)'
- en: Figure 9.14 Case 2 displays nonnormal residual distribution when linearity assumptions
    are not met due to the log-normal distribution of the target (there is a clear
    skewness in the distribution).
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.14案例2展示了当目标变量的对数正态分布导致线性假设不成立时的非正态残差分布（分布中存在明显的偏斜）。
- en: '![figure](../Images/CH09_F15_Babushkin.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_F15_Babushkin.png)'
- en: Figure 9.15 In Case 3, there’s a nonnormal residual distribution for linear
    regression when linearity assumptions are not met due to the nonmonotonic dependence
    of the target from the regressors.
  id: totrans-168
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.15 在案例3中，由于目标变量对回归器的非单调依赖性，当线性假设不成立时，线性回归存在非正态的残差分布。
- en: 'In this example, we are lucky and instantly see what’s wrong with the model:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们很幸运，立刻就看到了模型的问题：
- en: '*Case 2—*It seems we don’t consider the distribution of the target variable.
    Entities like revenue, sales, and prices follow a log-normal distribution, whereas
    the mean squared error or mean absolute error that the regression models minimize
    is not suitable (at least not directly). To overcome this issue, applying logarithm
    transform to the target often helps.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*案例2—*我们似乎没有考虑目标变量的分布。像收入、销售额和价格这样的实体遵循对数正态分布，而回归模型最小化的均方误差或平均绝对误差（至少不是直接）不合适。为了克服这个问题，通常对目标变量应用对数变换有帮助。'
- en: '*Case 3—*Residuals form multiple clusters. In this case, transforming the target
    will not be of any use. The target variable dependence on features is not monotonic.
    What can help in this case is either trying a model that can catch nonmonotonic
    dependencies or engineering new features that will help the linear model to reduce
    nonmonotonic dependencies to monotonic and even linear ones.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*案例3—*残差形成多个簇。在这种情况下，转换目标变量将没有任何帮助。目标变量对特征依赖性不是单调的。在这种情况下，可以尝试一个能够捕捉非单调依赖性的模型，或者工程新的特征，帮助线性模型将非单调依赖性降低到单调甚至线性。'
- en: 9.2.4 Fairness of residuals
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.4 残差的公平性
- en: In ML, fairness is an indicator of inequality among data. How do individual
    samples contribute to a loss or metric? By what cost do we increase the metric?
    Does the new model add inequality among residuals or reduce it? Basically, *fairness*
    is another term for defining the skewness of the residual distribution.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习（ML）中，公平性是数据中不平等的一个指标。个体样本是如何贡献到损失或指标的？我们通过什么成本来增加指标？新的模型是在残差中增加不平等还是减少它？基本上，“公平性”是定义残差分布偏斜的另一个术语。
- en: Not every metric change is meant to be equal. Some improvements are distributed
    uniformly among all samples, while others add significant growth in one stratum
    and provoke a decrease in the rest. The concept of “fairness” in residual analysis
    pushes us toward a more holistic model evaluation procedure, far beyond estimating
    single-valued metrics.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 并非每一次指标的变化都意味着平等。一些改进在所有样本中均匀分布，而其他改进在一个阶层中增加显著增长，同时引起其他阶层的减少。残差分析中的“公平性”概念推动我们朝着更全面的模型评估程序发展，远超估计单一值指标。
- en: To get a better understanding of what fairness is, consider figure 9.16.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解什么是公平性，请考虑图9.16。
- en: '![figure](../Images/CH09_F16_Babushkin.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_F16_Babushkin.png)'
- en: Figure 9.16 Fair vs. unfair residual distribution
  id: totrans-177
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.16 公平与不公平的残差分布
- en: In this simplified example, we have two models. Both decrease the mean absolute
    error (MAE) by 20%. However, we prefer to deploy the first model because it reduces
    absolute residuals uniformly among all 10 samples. In contrast, the second model
    drastically improves metrics on one half and reduces them on the other half. In
    this case, we add inequality to the residual distribution, so we call this distribution
    unfair.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简化的例子中，我们有两个模型。它们都通过20%降低了平均绝对误差（MAE）。然而，我们更愿意部署第一个模型，因为它在所有10个样本中均匀地减少了绝对残差。相比之下，第二个模型在一半的样本上大幅提高了指标，而在另一半上减少了指标。在这种情况下，我们在残差分布中增加了不平等，因此我们称这种分布为不公平。
- en: One way to assess fairness quantitatively, instead of relying purely on visualizations,
    is to use the Gini index from economics (see figure 9.17). To compute it, the
    residuals should be sorted based on their absolute values, and then the cumulative
    proportion of the absolute values should be divided by the cumulative proportion
    of the number of residuals.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 评估公平性的一个定量方法，而不是完全依赖可视化，是使用经济学中的基尼指数（见图9.17）。为了计算它，残差应根据它们的绝对值进行排序，然后绝对值的累积比例应除以残差数量的累积比例。
- en: '![figure](../Images/CH09_F17_Babushkin.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_F17_Babushkin.png)'
- en: 'Figure 9.17 Graphical representation of the Gini coefficient: the graph shows
    that the Gini coefficient equals the area marked A divided by the sum of the areas
    marked A and B—that is, Gini = A/(A + B). It is also equal to 2A and to 1 − 2B
    because A + B = 0.5 (since the axes scale from 0 to 1).'
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.17 基尼系数的图形表示：该图显示基尼系数等于标记为A的面积除以标记为A和B的面积之和——即，基尼系数 = A/(A + B)。它也等于2A和1
    − 2B，因为A + B = 0.5（因为坐标轴的刻度从0到1）。
- en: For total fairness (Gini = 0.0), almost all residuals have the same contribution
    to the total error. For total inequity (Gini = 1.0), a single residual is stealing
    the covers. Those are two extremes that you will rarely face in real life, while
    the common values are always somewhere in between.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 对于完全公平（基尼系数 = 0.0），几乎所有残差对总误差的贡献相同。对于完全不平等（基尼系数 = 1.0），单个残差在窃取覆盖。这些是你在现实生活中很少会遇到的两极，而常见的值总是在两者之间。
- en: There are two main reasons to care about fairness. First, we want the model
    to have high performance among all users, items, or other entities it will be
    deployed on, not just a fraction of them. This also includes reducing overall
    inequality in residuals step by step in each iteration of the system.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 关注公平性的两个主要原因。首先，我们希望模型在所有用户、物品或其他实体上都有高性能，而不仅仅是其中的一部分。这也包括在系统的每次迭代中逐步减少残差的整体不平等。
- en: Second, we want to not only improve the overall error numbers but also reduce
    each residual. If we increase the search engine quality by 5%, it reduces prediction
    quality on some segments of users by 20%, which damages the users’ experience.
    The gains we get by improving the average quality may be easily neutralized by
    the increase in the churn of these users.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们不仅希望提高整体误差数值，还希望减少每个残差。如果我们通过提高搜索引擎质量5%来减少预测质量，那么它将使用户的一些部分的预测质量降低20%，这会损害用户体验。通过提高平均质量所获得的收益可能会被这些用户流失率的增加所抵消。
- en: In the long term, we chase close-to-equal growth among all strata. There are
    exceptions to every rule, and fairness is not that critical for every single project
    and every metric. We should pay attention to the error cost produced by residual
    distribution tails. This should define our tradeoff between average and sample-wise
    improvements.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 从长远来看，我们追求所有层级的接近相等增长。每条规则都有例外，公平性对于每个项目和每个指标来说并不那么关键。我们应该注意残差分布尾部的误差成本。这应该定义我们平均和样本改进之间的权衡。
- en: 9.2.5 Underprediction and overprediction
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.5 低估和过度预测
- en: In regression tasks, we often split residuals by sign—positive residuals indicate
    overprediction (predicted values are greater than true values), while negative
    residuals indicate underprediction.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归任务中，我们通常根据符号来分割残差——正残差表示过度预测（预测值大于真实值），而负残差表示低估预测。
- en: Depending on the problem we are set to solve, one or another bias of the model
    is preferred. For instance, if we are building a demand forecasting system, missed
    profit is a less desirable outcome than moderate overstocks. On the other hand,
    if we predict a client’s creditworthiness for a bank, we are better off underestimating
    it than overestimating it.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们要解决的问题，模型的一个或另一个偏差可能更受欢迎。例如，如果我们正在构建一个需求预测系统，错失的利润不如适度的过剩库存那么令人不快。另一方面，如果我们为银行预测客户的信用度，我们最好低估它而不是高估它。
- en: Therefore, the cost of an error is often asymmetric, and it should tell us residuals
    of which sign and size we should pay attention to the most.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，错误的成本往往是非对称的，它应该告诉我们应该最关注哪些符号和大小残差。
- en: 9.2.6 Elasticity curves
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.6 弹性曲线
- en: One of the demand-specific error analysis tools is the elasticity plot. It is
    not a universal tool applicable for any ML system, but because it is crucial for
    pricing-related applications, it is worth our attention. Although we discussed
    most of the curve-shaped ways of analysis earlier, this example belongs here as
    it can be seen as a special case of residual analysis.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 需求特定错误分析工具之一是弹性图。它不是一个适用于任何机器学习系统的通用工具，但由于它对于定价相关应用至关重要，值得我们关注。尽管我们之前讨论了大多数曲线形状的分析方法，但这个例子属于这里，因为它可以被视为残差分析的一个特例。
- en: One of the core model assumptions behind demand forecasting is price–demand
    dependency—the higher the price, the lower the demand, and vice versa. This is
    true for almost all kinds of products (except some special cases like Veblen and
    Giffen goods, if you recall Microeconomics 101).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 需求预测背后的核心模型假设之一是价格-需求依赖性——价格越高，需求越低，反之亦然。这几乎适用于所有类型的商品（除非是一些特殊案例，如凡勃伦商品和吉芬商品，如果你还记得微观经济学101的话）。
- en: An elasticity plot is a special case for a more generic concept called a *partial
    dependence plot*, in which we vary some features and analyze how the predicted
    outcome changes. It is used for model interpretability (which we will cover in
    chapter 11).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 弹性图是更通用概念“部分依赖图”的一个特例，其中我们改变一些特征并分析预测结果如何变化。它用于模型可解释性（我们将在第11章中介绍）。
- en: 'There are two ways to plot the elasticity curve for our model:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方式来绘制我们模型的弹性曲线：
- en: 'Using training data (with real prices known):'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用训练数据（已知真实价格）：
- en: Taking the sales history of a particular SKU
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 取特定SKU的销售历史
- en: Predicting sales for each data point (Y)
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测每个数据点的销售（Y）
- en: Taking the historical price for each data point (X)
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 取每个数据点（X）的历史价格
- en: Plotting X-Y (price → predicted sales) dependency
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绘制X-Y（价格→预测销售）依赖关系图
- en: 'Using a mixture of synthetic and real data:'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用合成数据和真实数据的混合：
- en: Taking the last price for a particular SKU
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 取特定SKU的最后价格
- en: Multiplying this price by different coefficients (–20%, –19%, –18%, …, +19%,
    +20%)
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将此价格乘以不同的系数（-20%，-19%，-18%，…，+19%，+20%）
- en: Recalculating all price-based features and predicting sales (Y) for each new
    row
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新计算所有基于价格的特征，并预测每行的新销售（Y）
- en: Plotting X-Y (price → predicted sales) dependency
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绘制X-Y（价格→预测销售）依赖关系图
- en: 'As we mentioned before, in a perfect case, the plot demonstrates an inverse
    dependency: the higher the price, the lower the sales. However, if this plot demonstrates
    the opposite, is noisy (partly or fully nonmonotonic), or has any other controversial
    patterns, it will signal one of the following:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，在理想情况下，图表显示了反向依赖性：价格越高，销量越低。然而，如果这个图表显示了相反的情况，是嘈杂的（部分或全部非单调），或者有任何其他争议性的模式，它将表明以下情况之一：
- en: We don’t have enough price variability for this SKU to capture its elasticity
    (e.g., a short history of sales).
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于这个SKU，我们没有足够的价格变化来捕捉其弹性（例如，销售历史很短）。
- en: The sales for this SKU are way too stochastic. For instance, this SKU is often
    affected by promo campaigns, seasonality, or other external factors).
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个SKU的销售非常随机。例如，这个SKU经常受到促销活动、季节性或其他外部因素的影响）。
- en: The model can’t capture it for whatever reason (“a hard case”).
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于某种原因（“一个难题”），模型无法捕捉到它。
- en: The “better” the plot (more monotonous in the negative direction), the more
    we can rely on the model’s predictions for these SKUs. If the elasticity is “bad,”
    it signals that forecasting is not reliable and should be further investigated
    rather than deployed for these “hard” SKUs (see figures 9.18 and 9.19).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: “更好”的图表（在负方向上更单调）意味着我们可以更多地依赖模型对这些SKU的预测。如果弹性“不好”，这表明预测不可靠，应该对这些“困难”SKU进行进一步调查，而不是部署（见图9.18和9.19）。
- en: '![figure](../Images/CH09_F18_Babushkin.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_F18_Babushkin.png)'
- en: Figure 9.18 Theoretical example of elastic demand
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.18 需求弹性的理论示例
- en: '![figure](../Images/CH09_F19_Babushkin.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_F19_Babushkin.png)'
- en: Figure 9.19 Theoretical example of inelastic demand
  id: totrans-213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.19 需求无弹性的理论示例
- en: Plotting elasticity not only for predicted sales but for actual sales as well
    is helpful. It is also helpful to understand whether this particular SKU reveals
    distinct elasticity. If it doesn’t, we should not expect it for the predicted
    demand.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅为预测销售绘制弹性曲线，还为实际销售绘制弹性曲线是有帮助的。了解这个特定的SKU是否显示出独特的弹性也很重要。如果它没有，我们不应该期望预测需求会有弹性。
- en: If you feel like you need more context on the price elasticity concept, we recommend
    the article “Forecasting with Price Elasticity of Demand” ([https://mng.bz/GN5v](https://mng.bz/GN5v)).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要更多关于价格弹性概念的信息，我们推荐阅读文章“使用需求价格弹性进行预测”([https://mng.bz/GN5v](https://mng.bz/GN5v))。
- en: 'A friend of ours recently told his own campfire story about using an elasticity
    curves application. He had been working on a pricing problem, and their solution
    was effectively a glorified elasticity plot. They built a gradient-boosting model
    that predicted the number of sold items using various features, including price-based
    ones and estimated sales for different possible prices. Their first model revealed
    a surprising pattern: instead of having a smooth look, the plot had a visible
    “ladder” of steps. After deeper analysis, they realized that the origin of the
    steps was related to the features they used; continuous variables were split into
    buckets with low cardinality (e.g., only 256 buckets for all possible prices across
    all the items on the marketplace), and it limited the model’s sensitivity. After
    the number of buckets was increased, the model was able to capture more detailed
    patterns, and the elasticity curve became smooth, improving the overall system
    performance.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的一位朋友最近讲述了他使用弹性曲线应用的故事。他一直在处理一个定价问题，他们的解决方案实际上是一个美化的弹性图。他们构建了一个梯度提升模型，使用各种特征来预测销售数量，包括基于价格的特性和对不同可能价格的估计销售。他们的第一个模型揭示了一个令人惊讶的模式：图表看起来并不平滑，而是有一个明显的“梯子”状步骤。经过更深入的分析，他们意识到这些步骤的起源与使用的特征有关；连续变量被分割成低基数（例如，对于市场上所有商品的可能的全部价格，只有256个桶），这限制了模型敏感性。在增加桶的数量后，模型能够捕捉到更详细的模式，弹性曲线变得平滑，从而提高了整体系统性能。
- en: 9.3 Finding commonalities in residuals
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 在残差中寻找共同点
- en: 'Now that we’ve examined residual distribution as a whole, it’s time to investigate
    the patterns and trends in residual subgroups. To do that, we approach the problem
    from both ends (see figure 9.20):'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经整体考察了残差分布，是时候研究残差子组中的模式和趋势了。为此，我们从两端来解决这个问题（见图9.20）：
- en: We group samples by their residuals and analyze features in each group.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们根据样本的残差将样本分组，并分析每个组中的特征。
- en: We group samples by their features and analyze residuals in each group.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们根据样本的特征将样本分组，并分析每个组中的残差。
- en: '![figure](../Images/CH09_F20_Babushkin.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_F20_Babushkin.png)'
- en: 'Figure 9.20 Two approaches to finding commonalities in residuals: picking subsets
    by residuals rank or by sample attributes'
  id: totrans-222
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.20 寻找残差共同点的两种方法：通过残差排名或通过样本属性选择子集
- en: Grouping residuals by their values includes best-case and worst-case analysis;
    it also covers overprediction and underprediction problems. Grouping residuals
    by their properties produces group analysis and corner case analysis.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 按值分组残差包括最坏/最好情况分析；它还涵盖了过度预测和不足预测问题。按属性分组残差产生分组分析和边界情况分析。
- en: 9.3.1 Worst/best-case analysis
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.1 最坏/最好情况分析
- en: The goal of worst/best-case analysis is to define typical cases where the model
    works fine and where we should avoid making decisions based on this model. What
    do residuals have in common in these extreme cases?
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 最坏/最好情况分析的目标是定义模型运行良好的典型情况以及我们应该避免基于此模型做出决策的情况。在这些极端情况下，残差有什么共同之处？
- en: Campfire story from Valerii
  id: totrans-226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Valerii 的篝火故事
- en: 'Getting back to a case mentioned in previous chapters, once we deployed a dynamic
    pricing system in a large marketplace that set a price for an item based on predicted
    demand. We started to encounter moments when it failed to forecast sales accurately.
    We decided to focus on the top 200 products by their residual size. Soon we realized
    the most distinguishable clusters of problems:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 回到前几章提到的案例，一旦我们在一个大市场中部署了一个基于预测需求的动态定价系统，我们就开始遇到它无法准确预测销售的情况。我们决定专注于剩余量最大的前200个产品。很快，我们就意识到最明显的问题集群：
- en: The first cluster revolved around *new products* that had recently been added
    to the marketplace’s assortment matrix. This issue is known as the *cold start*
    problem. These items posed a challenge as the lack of historical data hindered
    accurate predictions. It became clear that relying solely on our ML model would
    not suffice in such cases. Instead, we needed to develop heuristics that would
    use the warming up of sales for products within the same category to establish
    a solid foundation for forecasting.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个集群围绕着最近添加到市场组合矩阵中的**新产品**，这个问题被称为**冷启动**问题。这些商品由于缺乏历史数据而难以准确预测，构成了挑战。很明显，仅依靠我们的机器学习模型在这种情况下是不够的。相反，我们需要开发启发式方法，利用同一类别内产品的销售升温来为预测建立一个坚实的基础。
- en: Another cluster emerged from the *electronic devices category**,* revealing
    a different predicament. These products exhibited sparse sales over time, making
    it difficult to depend on our model’s predictions. Recognizing this, we made a
    crucial decision to exclude these items from our pilot and explore alternative
    ways of improving the prediction quality. We contemplated the idea of splitting
    the model into larger categories, believing it would capture the specific dynamics
    within each group more effectively.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个集群来自**电子设备类别**，揭示了不同的困境。这些产品在时间上的销售稀疏，使得依赖我们模型的预测变得困难。意识到这一点，我们做出了一个关键的决定，将这批商品排除在我们的试点之外，并探索提高预测质量的其他方法。我们考虑了将模型分成更大类别的想法，相信这将更有效地捕捉每个组内的特定动态。
- en: However, the most significant bias was influenced by *marketing activities**—*big
    sales, promotional codes, and discounts. The model struggled to account for these
    factors, resulting in noticeable underpredictions represented by large negative
    residuals. To rectify this bias, we incorporated a promo calendar into our feature
    set. By doing so, we could empower the model to make corresponding corrections,
    thus enhancing the accuracy of predictions.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，最大的偏差是由**营销活动**——**大促销、促销代码和折扣——引起的。模型难以解释这些因素，导致明显的不足预测，表现为大的负残差。为了纠正这种偏差，我们将促销日历纳入我们的特征集。通过这样做，我们使模型能够做出相应的调整，从而提高预测的准确性。
- en: In addition to identifying the areas where our model fell short, we also investigated
    residuals that were close to zero to determine the boundaries of our model’s applicability.
    This analysis helped us understand where we could have confidence in the model’s
    predictions, where the quality might be satisfactory but within acceptable limits,
    and where it became risky to rely on the model’s forecasts.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 除了确定我们的模型不足的领域外，我们还研究了接近零的残差，以确定我们模型适用性的边界。这种分析帮助我们了解在哪些情况下我们可以对模型的预测有信心，在哪些情况下质量可能令人满意但处于可接受范围内，以及在哪些情况下依赖模型的预测变得风险。
- en: By examining these residual patterns, we gained a comprehensive understanding
    of the strengths and limitations of our dynamic pricing system, enabling us to
    make informed decisions about its rollout and ensure its appropriate usage.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查这些残差模式，我们全面了解了我们的动态定价系统的优势和局限性，使我们能够就其推广做出明智的决定，并确保其适当的使用。
- en: 9.3.2 Adversarial validation
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.2 对抗验证
- en: If the manual worst-case analysis won’t provide new insights, a “machine learning
    model for analyzing machine learning models” could help. In chapter 7, we discussed
    a concept called adversarial validation. It was derived from ML competitions and
    is used to check whether the distribution of the two datasets differs. Often we
    concatenate, train, and test datasets with labels 0 and 1.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如果手动最坏情况分析不会提供新的见解，一个“用于分析机器学习模型的机器学习模型”可能会有所帮助。在第7章中，我们讨论了一个称为对抗验证的概念。它源于机器学习竞赛，用于检查两个数据集的分布是否不同。通常，我们会将带有标签0和1的数据集连接起来，进行训练和测试。
- en: 'Adversarial validation can be easily transferred to the rails of residual analysis:
    we set 0 for “good” samples of data and 1 for “bad” samples (e.g., taking the
    top-N% biggest residuals in the second case). We should try different thresholds
    for our particular set.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗验证可以轻松转移到残差分析的轨道上：我们将“好”样本数据设置为0，“坏”样本数据设置为1（例如，在第二种情况下，取最大的N%残差）。我们应该尝试为我们的特定集合尝试不同的阈值。
- en: 'The rest of the algorithm is similar: we fit a simple classifier (e.g., logistic
    regression) on these labels and calculate the receiver operating characteristic
    area under the curve (ROC AUC). If two classes are separable (area under the curve
    is significantly greater than 0.5), then we analyze the model’s weights, and this
    gives us a hint of which exact features best distinguish our “worst” cases from
    others.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的其余部分类似：我们在这些标签上拟合一个简单的分类器（例如，逻辑回归）并计算接收者操作特征曲线下的面积（ROC AUC）。如果两个类别是可分离的（曲线下的面积显著大于0.5），那么我们分析模型的权重，这为我们提供了关于哪些确切特征最能区分我们的“最坏”案例的线索。
- en: Sometimes we find no easily defined patterns during worst-case analysis. This
    is fine. It means that we have already captured the most low-hanging fruits in
    the model improvement space.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 有时在最坏情况分析中找不到容易定义的模式。这是可以的。这意味着我们已经在模型改进空间中捕获了最大的低垂之果。
- en: 9.3.3 Variety of group analysis
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.3 组分析的多样性
- en: Group analysis enables the identification of distinct patterns and trends within
    the residuals of various groups, segments, classes, cohorts, or clusters. For
    instance, in both binary and multiclass classification scenarios, one effective
    approach involves splitting the residuals by classes (i.e., by target variable),
    allowing for separate analysis of the residuals in each group.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 组分析能够识别各种群体、段、类别、队列或集群的残差中的独特模式和趋势。例如，在二进制和多类分类场景中，一种有效的方法是按类别（即目标变量）拆分残差，允许对每个组中的残差进行单独分析。
- en: Many typical applications processing tabular data, such as fraud detection systems,
    rely on grouping samples based on specific characteristics, such as geography
    or traffic source. By analyzing the residuals within each segment, it becomes
    possible to uncover common biases present in the model’s predictions. These insights
    can then guide further model refinement by incorporating more relevant features
    or adjusting the weights of existing features.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 许多处理表格数据的典型应用，如欺诈检测系统，依赖于根据特定特征（如地理位置或流量来源）对样本进行分组。通过分析每个段内的残差，可以揭示模型预测中存在的共同偏差。这些见解可以指导通过纳入更多相关特征或调整现有特征的权重来进一步改进模型。
- en: When dealing with the data in the form of text, images, or videos, there may
    be no distinct cohorts or groups by default. In such cases, an alternative method
    involves manually classifying a set of N residuals and assigning labels to each
    issue encountered (e.g., identifying images that are too dark or blurry or flagging
    texts with specific wording or style). This process allows for the discovery of
    specific problematic clusters where the model underperforms. Consequently, it
    provides guidance on what type of data should be collected next to improve the
    system.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理文本、图像或视频形式的数据时，默认情况下可能没有明显的群体或分组。在这种情况下，一种替代方法涉及手动对一组N个残差进行分类，并为遇到的每个问题分配标签（例如，识别过暗或模糊的图像或标记具有特定措辞或风格的文本）。这个过程允许发现模型表现不佳的具体问题集群。因此，它提供了关于应收集哪种类型的数据以改进系统的指导。
- en: 9.3.4 Corner-case analysis
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.4 边缘情况分析
- en: Corner-case analysis aims to test the model in rare circumstances. Typically,
    we would like to have a benchmark, a fixed set of already-captured corner cases,
    to quickly examine the behavior of each new model.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘情况分析旨在测试模型在罕见情况下。通常，我们希望有一个基准，一个固定集合的已捕获的边缘情况，以便快速检查每个新模型的行为。
- en: 'Here are some ideas for what we can check during corner-case analysis:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些关于我们在边缘情况分析期间可以检查的内容的想法：
- en: '*Forecasting models*—Users/items with short or no history, users/items with
    extremely large numbers of actions/sales, highest and lowest values in feature
    X, users/items with rare actions/sales'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预测模型*——历史短暂或无历史的用户/物品、动作/销售数量极多的用户/物品、特征X的最高和最低值、动作/销售稀有的用户/物品'
- en: '*Image segmentation*—Bad-quality images, low-resolution images, high-resolution
    images, occlusions and reflections, abnormal lighting conditions, multiple objects
    in one image, no objects in the image'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图像分割*——低质量图像、低分辨率图像、高分辨率图像、遮挡和反射、异常光照条件、一张图像中的多个对象、图像中没有对象'
- en: '*Language models*—Shortest and longest texts, jokes, offensive topics, simple
    arithmetic, text with typos, text with N different languages, extensive usage
    of emojis'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语言模型*——最短和最长的文本、笑话、冒犯性话题、简单的算术、带错别字的文本、包含N种不同语言的文本、大量使用表情符号'
- en: '*Voice recognition*—Shortest or longest audio, bad-quality audio, audio with
    no voice, music instead of audio, samples with pronunciation that are too fast
    or too slow, loud environment, silent voice, samples with multiple speakers (aka
    “cocktail party”)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语音识别*——最短或最长的音频、低质量音频、无语音音频、音乐而不是音频、发音过快或过慢的样本、嘈杂的环境、无声的语音、包含多个说话者的样本（即“鸡尾酒会”）'
- en: While the best/worst-case analyses ask which data our model digests excellently
    or badly, the corner-case analysis and cohort analysis ask about the model’s performance
    on predefined subsets of data.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 当最佳/最坏情况分析询问我们的模型在哪些数据上表现优秀或糟糕时，边缘情况分析和队列分析则询问模型在预定义数据子集上的性能。
- en: Campfire story from Arseny
  id: totrans-250
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 阿尔谢尼的篝火故事
- en: 'When I was working for the augmented-reality company, one important piece of
    the system was a key-point detector based on a deep learning model. The task was
    to identify several key points that were later used to understand the object coordinates.
    The training pipeline was written with proper diagnostic tools from the very beginning,
    so we identified at an early stage that certain samples with high loss demonstrated
    one common pattern—some images contained mirrors or other reflecting surfaces
    (even a puddle on a rainy day!), and the model could pick not the key point versus
    its reflection. It meant we needed additional properties from the system: choose
    the “real” object, remember it between frames, and ignore key points that belong
    to mirrored objects. Early understanding of this problem helped us tinker with
    a solution that could mitigate the reflected key points situation.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 当我在增强现实公司工作时，系统的一个重要部分是基于深度学习模型的关键点检测器。任务是识别几个关键点，这些关键点后来被用来理解物体坐标。从一开始，训练流程就使用了适当的诊断工具，所以我们早期就发现某些损失较高的样本表现出一个共同的模式——某些图像包含镜子或其他反射表面（甚至雨天的一个水坑！），模型无法区分关键点与其反射。这意味着我们需要从系统中获取额外的属性：选择“真实”的物体，记住它，并忽略属于反射物体的关键点。对这一问题的早期理解帮助我们调整解决方案，以减轻反射关键点的情况。
- en: '![sidebar figure](../Images/CH09_F21_Babushkin.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![侧边栏图像](../Images/CH09_F21_Babushkin.png)'
- en: Model detects the real foot, not the reflection
  id: totrans-253
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型检测到真实的脚，而不是反射
- en: '9.4 Design document: Error analysis'
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 设计文档：错误分析
- en: Because we’re convinced that error analysis should be among the essential elements
    of ML system design, we will include this phase in both our design documents.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们坚信错误分析应该是机器学习系统设计的基本要素之一，所以我们将在我们的设计文档中包含这一阶段。
- en: 9.4.1 Error analysis for Supermegaretail
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.1 超级大零售的错误分析
- en: We start off with Supermegaretail, where we will suggest the approach that will
    help the company achieve its main goal—to reduce the gap between delivered and
    sold items, making it as narrow as possible while avoiding out-of-stock situations.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从超级大零售开始，我们将提出一种帮助公司实现其主要目标的方法——尽可能缩小交付和销售物品之间的差距，同时避免缺货情况。
- en: 'Design document: Supermegaretail'
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设计文档：超级大零售
- en: VI. Error analysis
  id: totrans-259
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI. 错误分析
- en: Remember that we have six quantile losses for 1.5th, 25th, 50th, 75th, 95th,
    and 99th quantiles of the target and corresponding six models for each. The constant
    baseline estimates these quantiles for each product based on the last N days of
    its sales. These baselines already have some residual distribution with some specific
    bias that is helpful to consider.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们有针对目标值1.5分位数、25分位数、50分位数、75分位数、95分位数和99分位数的六个分位数损失，以及每个分位数对应的六个模型。常量基线根据产品过去N天的销售情况来估计每个产品的这些分位数。这些基线已经具有一些特定的残差分布和一些有用的特定偏差。
- en: Comparing more complex models (linear models and gradient boosting) with these
    dummy baselines will give us an understanding of whether we are moving in the
    right direction in modeling and feature engineering.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 将更复杂的模型（线性模型和梯度提升）与这些基线模型进行比较，将帮助我们了解我们在建模和特征工程方面是否朝着正确的方向前进。
- en: i. Learning-curve analysis
  id: totrans-262
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: i. 学习曲线分析
- en: i. Convergence analysis
  id: totrans-263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: i. 收敛分析
- en: A step-wise learning curve based on the number of iterations comes into play
    only when we start experimenting with the gradient-boosting algorithm. The key
    questions we should answer when examining the loss curve are
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始尝试梯度提升算法时，基于迭代次数的逐步学习曲线才发挥作用。在检查损失曲线时，我们应该回答的关键问题是
- en: Does the model converge at all?
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型是否真的收敛？
- en: Does the model beat baseline metrics (quantile loss, mean absolute percent error,
    etc.)?
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型是否优于基线指标（分位数损失，平均绝对百分比误差等）？
- en: Are issues like underfitting/overfitting presented or not?
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否存在欠拟合/过拟合等问题？
- en: Once we ensure the model converges, we can pick a sufficient number of trees
    on a rough grid (500-1,000-2,000-3,000-5,000) and fixate for future experiments.
    For simpler baselines, convergence analysis is not needed.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们确保模型收敛，我们可以在一个粗略的网格（500-1,000-2,000-3,000-5,000）上选择足够多的树，并固定用于未来的实验。对于更简单的基线，不需要进行收敛分析。
- en: ii. Model complexity
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ii. 模型复杂性
- en: We will use a model-wise learning curve to decide an optimal number of features
    and overall model complexity.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用模型级学习曲线来决定最佳特征数量和整体模型复杂性。
- en: 'Let’s say we fixate all hyperparameters except the number of lags we use: the
    more we take, the more complicated patterns and seasonalities our model can capture—and
    the easier it will be to overfit training data. Should it be N – 1, N – 2, N –
    3 days? or N – 1, N – 2, …, N – 30 days? The optimal number can be determined
    by the “model size vs. error size” plot.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们固定所有超参数，除了我们使用的滞后数：我们取的越多，模型可以捕捉到的复杂模式和季节性就越多——并且更容易过拟合训练数据。应该是N - 1，N -
    2，N - 3天？还是N - 1，N - 2，……，N - 30天？最佳数量可以通过“模型大小与错误大小”图来确定。
- en: Similarly, we can optimize window sizes. For instance, windows “7/14/21/…” are
    more granular than “30/60/90/…” ones. The appropriate level of granularity can
    be chosen, again, by using a model-wise learning curve.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以优化窗口大小。例如，“7/14/21/…”这样的窗口比“30/60/90/…”更细粒度。通过使用模型级学习曲线，我们可以选择适当的粒度级别。
- en: In the same fashion, we tweak other key hyperparameters of the model during
    the initial adjustments—for instance, regularization term size.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 以同样的方式，我们在初始调整期间调整模型的其它关键超参数——例如，正则化项的大小。
- en: iii. Dataset size
  id: totrans-274
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: iii. 数据集大小
- en: Do we need to use all the available data to train the model? How many last months
    are enough and relevant? Do we need to utilize all (day, store, item) data points,
    or can we downsample 20%/10%/5% of them without noticeable downgrading in metrics?
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否需要使用所有可用数据来训练模型？需要多少个月的数据足够且相关？我们需要利用所有（日，店铺，商品）数据点，还是可以下采样20%/10%/5%而不会在指标上明显下降？
- en: 'Here comes the rescue: the sample-wise learning curve analysis that determines
    how many samples are necessary for the error on the validation set to reach a
    plateau.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 救援来了：样本级学习曲线分析，它决定了验证集上的错误达到平台期所需的样本数量。
- en: We should make an important design decision of whether to use (day, store, item)
    as an object of the dataset or move to less granular (week, store, item). The
    last option reduces the number of required computations by a factor of 7, while
    model performance can either be left unchanged or even be increased.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该做出一个重要的设计决策，即是否将（日，店铺，商品）作为数据集的对象，或者转向更粗粒度（周，店铺，商品）。最后一个选项可以将所需的计算量减少7倍，同时模型性能可以保持不变，甚至可能提高。
- en: This design decision affects not only the demand forecasting service speed and
    performance but also the overall product (a stock management system), drastically
    reshaping the landscape of its possible use cases. Therefore, despite the possible
    advantages, this decision should be agreed upon with our product managers, users
    (category managers), and stakeholders.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这个设计决策不仅会影响预测服务的速度和性能，还会影响整体产品（库存管理系统），极大地重塑了其可能的用例。因此，尽管可能有优势，这个决策应该与我们的产品经理、用户（品类经理）和利益相关者达成一致。
- en: ii. Residual analysis
  id: totrans-279
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ii. 残差分析
- en: 'Remember that we have an asymmetric cost function: overstock is far less harmful
    than out-of-stock problems. We have either expired goods or missed profit. The
    uncovered demand problem is a much worse scenario, and in the long run, it is
    expressed in customers’ dissatisfaction and an increased risk that they will pass
    to competitors.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 记住我们有一个非对称的成本函数：过剩库存远比缺货问题危害小。我们可能面临过期商品或错失利润。未满足的需求问题是一个更糟糕的情况，从长远来看，它体现在顾客的不满和增加的风险，即他们可能会转向竞争对手。
- en: i. Residual distribution
  id: totrans-281
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: i. 残差分布
- en: 'The mentioned peculiarity of the demand should guide us throughout the residual
    analysis of our forecasting model: positive residuals (overprediction) are more
    preferred than negative ones (underpredictions). However, too much overprediction
    is bad as well.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 提到的需求特性应指导我们在预测模型残差分析中的整个过程：正残差（高估）比负残差（低估）更受欢迎。然而，过度高估同样不好。
- en: 'Therefore, we plot the distribution of the residuals along with their bias
    (a simple average among raw residuals). We expect this to be true in one of the
    following possible scenarios:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们绘制了残差的分布图，以及它们的偏差（原始残差中的简单平均值）。我们期望在以下可能的场景之一中这是正确的：
- en: A small positive bias reveals slight overprediction, which is the desirable
    outcome. If, in addition, residuals are not widely spread (low variance), we get
    a perfect scenario.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小的正偏差表明略微高估，这是理想的结果。如果，此外，残差分布不广（方差低），我们就会得到一个完美的场景。
- en: Equally spread residuals in both negative and positive directions would be okay
    but is less preferred than the previous scenario. We should force the model to
    produce more optimistic forecasts to ensure we minimize missed profit.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在负向和正向方向上均匀分布的残差是可以接受的，但不如前一种情况受欢迎。我们应该迫使模型产生更乐观的预测，以确保我们最小化错失的利润。
- en: The worst scenario is when we have a skew in favor of negative residuals. It
    means our model tends to increase customers’ dissatisfaction. This would definitely
    be a red flag for the current model version deployment.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最糟糕的情况是我们对负残差有偏差。这意味着我们的模型倾向于增加顾客的不满。这肯定是对当前模型版本部署的一个红旗。
- en: If we have a skew but favor positive residuals, this is unambiguously a good
    case for Supermegaretail as well and, hence, is less preferred than the first
    case.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们有偏差但有利于正残差，这无疑是Supermegaretail的好案例，因此不如第一种情况受欢迎。
- en: These scenarios are applicable when we try to estimate unbiased demand (we use
    median prediction for that). But as mentioned, we also have several other models
    for other quantiles (1.5%, 25%, 75%, 95%, 99%).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这些场景适用于我们试图估计无偏需求（我们使用中位数预测）时。但如前所述，我们还有其他几个模型用于其他分位数（1.5%，25%，75%，95%，99%）。
- en: '![figure](../Images/CH09_UN01_Babushkin.png)![figure](../Images/CH09_UN02_Babushkin.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH09_UN01_Babushkin.png)![figure](../Images/CH09_UN02_Babushkin.png)'
- en: 'For each of them, we analyze the basic assumption behind each model—for example:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一个模型，我们分析其背后的基本假设——例如：
- en: Is it true that 95% of residuals are positive for a model that predicts a 95%
    quantile?
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于预测95%分位数的模型，95%的残差都是正的吗？
- en: Is it true that 75% of residuals are negative for a model that predicts a 25%
    quantile?
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于预测25%分位数的模型，75%的残差都是负的吗？
- en: ii. Elasticity
  id: totrans-293
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ii. 弹性
- en: We should validate the elasticity assumptions using elasticity curves. There
    is no solid understanding of whether all the goods are expected to demonstrate
    elasticity, and this needs to be confirmed with stakeholders.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该使用弹性曲线验证弹性假设。对于所有商品是否都期望表现出弹性，没有明确的理解，这需要与利益相关者确认。
- en: 'If we face problems related to elasticity, we have two options to improve the
    elasticity capturing:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们面临与弹性相关的问题，我们有两种选择来提高弹性捕捉：
- en: '*Postprocessing* (fast, simple, ad hoc solution)—We can apply an additional
    model (e.g., isotonic regression) for prediction postprocessing to calibrate forecasts.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*后处理*（快速、简单、临时解决方案）——我们可以应用一个额外的模型（例如，等调回归）进行预测后处理以校准预测。'
- en: '*Improving the model* (slow, hard, generic solution)—This requires additional
    modeling, feature engineering, data preprocessing, etc. There is no predefined
    set of actions that will solve the problem for sure.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*改进模型*（慢速、困难、通用解决方案）——这需要额外的建模、特征工程、数据预处理等。没有一组预定义的操作可以肯定地解决问题。'
- en: iii. Best-case vs. worst-case vs. corner-case
  id: totrans-298
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: iii. 最佳情况 vs. 最坏情况 vs. 边缘情况
- en: 'Each time we roll out a new version of the model, we automatically report its
    performance in best/worst/corner cases and save the top-N% cases as artifacts
    of the training pipeline. The following is a draft of a checklist of questions
    for which we should find answers in this report:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 每次我们推出模型的新版本时，我们会自动报告其在最佳/最差/边缘情况下的性能，并将前N%的案例保存为训练管道的工件。以下是一份清单草案，其中我们应在报告中找到答案：
- en: What’s the model’s prediction error when the sales history of an item is short?
    Are the residuals mostly positive or mostly negative?
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当一个物品的销售历史较短时，模型的预测误差是多少？残差主要是正的还是主要是负的？
- en: What about items with a high price or with a low price?
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于价格高或价格低的物品怎么办？
- en: How does prediction error depend on weekends/holidays/promotion days?
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测误差如何依赖于周末/假日/促销日？
- en: What are the commonalities among the items with almost zero residuals? Is a
    long sales history necessarily required for them? How long should the sales history
    be to get acceptable performance? Does the model require other conditions that
    can help us to distinguish those cases where we are certain about the quality
    of the forecast?
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几乎没有残差的物品之间有哪些共同点？是否必须要求它们有长期的销售历史？为了获得可接受的表现，销售历史应该有多长？模型是否需要其他条件，以帮助我们区分那些我们对预测质量有把握的情况？
- en: What are the commonalities among the items with the largest negative residuals?
    We would 100% prefer to exclude these cases or whole categories from A/B-testing
    groups or pilots. We should also focus on these items when we start to improve
    the model.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有最大负残差的物品之间有哪些共同点？我们100%希望排除这些案例或整个类别从A/B测试组或试点中。当我们开始改进模型时，我们也应该关注这些物品。
- en: What do the items with the largest positive residuals have in common?
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有最大正残差的物品有哪些共同点？
- en: 9.4.2 Error analysis for PhotoStock Inc.
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.2 PhotoStock Inc.的错误分析
- en: Now we get back to PhotoStock Inc., which requires a modern search tool able
    to find the most relevant shots based on customers’ text queries while providing
    excellent performance and displaying the most relevant images in stock.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们回到PhotoStock Inc.，它需要一个现代的搜索工具，能够根据客户的文本查询找到最相关的照片，同时提供出色的性能并显示最相关的库存图像。
- en: 'Design document: PhotoStock Inc.'
  id: totrans-308
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设计文档：PhotoStock Inc.
- en: VI. Error analysis
  id: totrans-309
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI. 错误分析
- en: To enable early diagnostics of potential problems, we should include tools for
    error analysis from the very beginning. In this section of the document, we want
    to plan in advance some parts we want to focus on.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够尽早诊断潜在问题，我们应该从一开始就包括错误分析工具。在本节文档中，我们希望提前规划一些我们想要关注的部分。
- en: i. Learning curve analysis
  id: totrans-311
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: i. 学习曲线分析
- en: Loss curves should be enabled for sanity checks and further tuning of vital
    hyperparameters like early stopping threshold, learning rate, and many others.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该启用损失曲线以进行合理性检查和进一步调整关键超参数，如早期停止阈值、学习率等。
- en: Given our loss is composite (contains multiple components; see the previous
    Metrics and Losses section), we need to be able to see the loss curves per component
    to adapt its weights.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们的损失是复合的（包含多个组件；参见之前的指标和损失部分），我们需要能够看到每个组件的损失曲线，以便调整其权重。
- en: It should be possible to train the model on subsamples of data to draw sample-size
    learning curves later and estimate how new data improves the overall performance.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该能够在数据子样本上训练模型，以便稍后绘制样本大小学习曲线，并估计新数据如何提高整体性能。
- en: In parallel with loss curves, there should be metric curves to ensure they’re
    fairly correlated.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与损失曲线并行，应该有度量曲线以确保它们有公平的相关性。
- en: Given the dataset may be shared, we need to be able to see the curves per shard
    as well.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于数据集可能被共享，我们需要能够看到每个分片（shard）的曲线。
- en: ii. Residual analysis
  id: totrans-317
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ii. 残差分析
- en: For each training epoch, we should report the most interesting samples, such
    as samples with the highest/lowest loss overall and per component.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个训练周期，我们应该报告最有趣的样本，例如整体和每个组件损失最高/最低的样本。
- en: For each displayed sample, metadata should be available, so we report not only
    the search query and relevant images but also category, tags, query geo, query
    lang, and other attributes that may arise later.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个显示的样本，应该提供元数据，因此我们不仅报告搜索查询和相关的图像，还包括类别、标签、查询地理、查询语言和其他可能出现的属性。
- en: 'After training each candidate model (the model that is considered to be good
    enough to be used for the real system), we suggest the following procedure:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练每个候选模型（被认为是足够好，可以用于实际系统的模型）之后，我们建议以下程序：
- en: Sample 100 results with high loss/metric.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 样本100个高损失/指标的结果。
- en: For each one, suggest a short hypothesis about how this sample is outstanding
    (e.g., the suggested image is blurry, the image description is overoptimized,
    the query is too short, etc.) and group the results by these resolutions. Further
    analysis should be performed every time new steps of system improvement are planned,
    as it is a significant source of the signal.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每一个样本，建议提出一个简短假设来说明这个样本为何突出（例如，建议的图像模糊，图像描述过度优化，查询太短等），并按这些分辨率对这些结果进行分组。在计划系统改进的新步骤时，应进行进一步分析，因为它是一个重要的信号来源。
- en: In the future, we can consider applying interpretability techniques here as
    well because, at some point, questions like “Why image X is semantically close
    to image Y” will arise. However, from the current perspective, it can be postponed.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来，我们可以考虑在这里也应用可解释性技术，因为，在某个时候，诸如“为什么图像X在语义上与图像Y相似”这样的问题将会出现。然而，从当前的角度来看，它可以推迟。
- en: Summary
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Don’t hesitate to apply error analysis while designing your ML system, as it
    will help you reveal its weak spots and suggest ways of improving it.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在设计你的机器学习系统时，不要犹豫应用错误分析，因为它将帮助你揭示其弱点并提出改进的方法。
- en: Learning curve analysis is a vital first step for defining the efficiency of
    your model. If the model does not converge and there are overfitting and/or underfitting
    issues, there is no need for the rest of the analysis.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习曲线分析是定义你模型效率的重要第一步。如果模型没有收敛，并且存在过拟合和/或欠拟合问题，就没有必要进行其他分析。
- en: The presence of either overfitting or underfitting is an indicator of a possible
    imbalance between the model’s complexity and the amount of input data.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过拟合或欠拟合的存在是模型复杂性与输入数据量之间可能不平衡的指标。
- en: Depending on the type of loss curve you’re observing during error analysis,
    there is a certain list of actions you need to take to debug detected issues.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据你在错误分析期间观察到的损失曲线类型，你需要采取一系列行动来调试检测到的问题。
- en: Designed to calculate the differences between the predicted and actual values,
    residual analysis is essential for verifying model assumptions, detecting sources
    of metric changes, ensuring the fairness of residuals, performing worst-case and
    best-case analysis, and examining corner cases.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计用于计算预测值和实际值之间差异的残差分析对于验证模型假设、检测指标变化的原因、确保残差的公平性、执行最坏情况和最佳情况分析以及检查边缘情况至关重要。
