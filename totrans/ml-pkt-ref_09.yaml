- en: Chapter 9\. Imbalanced Classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are classifying data, and the classes are not relatively balanced in
    size, the bias toward more popular classes can carry over into your model. For
    example, if you have 1 positive case and 99 negative cases, you can get 99% accuracy
    simply by classifying everything as negative. There are various options for dealing
    with *imbalanced classes*.
  prefs: []
  type: TYPE_NORMAL
- en: Use a Different Metric
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One hint is to use a measure other than accuracy (AUC is a good choice) for
    calibrating models. Precision and recall are also better options when the target
    sizes are different. However, there are other options to consider as well.
  prefs: []
  type: TYPE_NORMAL
- en: Tree-based Algorithms and Ensembles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tree-based models may perform better depending on the distribution of the smaller
    class. If they tend to be clustered, they can be classified easier.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble methods can further aid in pulling out the minority classes. Bagging
    and boosting are options found in tree models like random forests and Extreme
    Gradient Boosting (XGBoost).
  prefs: []
  type: TYPE_NORMAL
- en: Penalize Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many scikit-learn classification models support the `class_weight` parameter.
    Setting this to `'balanced'` will attempt to regularize minority classes and incentivize
    the model to classify them correctly. Alternatively, you can grid search and specify
    the weight options by passing in a dictionary mapping class to weight (give higher
    weight to smaller classes).
  prefs: []
  type: TYPE_NORMAL
- en: The [XGBoost](https://xgboost.readthedocs.io) library has the `max_delta_step`
    parameter, which can be set from 1 to 10 to make the update step more conservative.
    It also has the `scale_pos_weight` parameter that sets the ratio of negative to
    positive samples (for binary classes). Also, the `eval_metric` should be set to
    `'auc'` rather than the default value of `'error'` for classification.
  prefs: []
  type: TYPE_NORMAL
- en: The KNN model has a `weights` parameter that can bias neighbors that are closer.
    If the minority class samples are close together, setting this parameter to `'distance'`
    may improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: Upsampling Minority
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can upsample the minority class in a couple of ways. Here is an sklearn
    implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also use the imbalanced-learn library to randomly sample with replacement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Generate Minority Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The imbalanced-learn library can also generate new samples of minority classes
    with both the Synthetic Minority Over-sampling Technique (SMOTE) and Adaptive
    Synthetic (ADASYN) sampling approach algorithms. SMOTE works by choosing one of
    its k-nearest neighbors, connecting a line to one of them, and choosing a point
    along that line. ADASYN is similar to SMOTE, but generates more samples from those
    that are harder to learn. The classes in imbanced-learn are named `over_sampling.SMOTE`
    and `over_sampling.ADASYN`.
  prefs: []
  type: TYPE_NORMAL
- en: Downsampling Majority
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another method to balance classes is to downsample majority classes. Here is
    an sklearn example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Don’t use replacement when downsampling.
  prefs: []
  type: TYPE_NORMAL
- en: 'The imbalanced-learn library also implements various downsampling algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ClusterCentroids`'
  prefs: []
  type: TYPE_NORMAL
- en: This class uses K-means to synthesize data with the centroids.
  prefs: []
  type: TYPE_NORMAL
- en: '`RandomUnderSampler`'
  prefs: []
  type: TYPE_NORMAL
- en: This class randomly selects samples.
  prefs: []
  type: TYPE_NORMAL
- en: '`NearMiss`'
  prefs: []
  type: TYPE_NORMAL
- en: This class uses nearest neighbors to downsample.
  prefs: []
  type: TYPE_NORMAL
- en: '`TomekLink`'
  prefs: []
  type: TYPE_NORMAL
- en: This class downsamples by removing samples that are close to each other.
  prefs: []
  type: TYPE_NORMAL
- en: '`EditedNearestNeighbours`'
  prefs: []
  type: TYPE_NORMAL
- en: This class removes samples that have neighbors that are either not in the majority
    or all of the same class.
  prefs: []
  type: TYPE_NORMAL
- en: '`RepeatedNearestNeighbours`'
  prefs: []
  type: TYPE_NORMAL
- en: This class repeatedly calls the `EditedNearestNeighbours`.
  prefs: []
  type: TYPE_NORMAL
- en: '`AllKNN`'
  prefs: []
  type: TYPE_NORMAL
- en: This class is similar but increases the number of nearest neighbors during the
    iterations of downsampling.
  prefs: []
  type: TYPE_NORMAL
- en: '`CondensedNearestNeighbour`'
  prefs: []
  type: TYPE_NORMAL
- en: This class picks one sample of the class to be downsampled, then iterates through
    the other samples of the class, and if KNN doesn’t misclassify, it adds that sample.
  prefs: []
  type: TYPE_NORMAL
- en: '`OneSidedSelection`'
  prefs: []
  type: TYPE_NORMAL
- en: This classremoves noisy samples.
  prefs: []
  type: TYPE_NORMAL
- en: '`NeighbourhoodCleaningRule`'
  prefs: []
  type: TYPE_NORMAL
- en: This class uses `EditedNearestNeighbours` results and applies KNN to it.
  prefs: []
  type: TYPE_NORMAL
- en: '`InstanceHardnessThreshold`'
  prefs: []
  type: TYPE_NORMAL
- en: This class trains a model, then removes samples with low probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: All of these classes support the `.fit_sample` method.
  prefs: []
  type: TYPE_NORMAL
- en: Upsampling Then Downsampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The imbalanced-learn library implements `SMOTEENN` and `SMOTETomek`, which both
    upsample and then apply downsampling to clean up the data.
  prefs: []
  type: TYPE_NORMAL
