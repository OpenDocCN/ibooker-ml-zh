- en: Chapter 15\. K-Nearest Neighbors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 15.0 Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The k-nearest neighbors (KNN) classifier is one of the simplest yet most commonly
    used classifiers in supervised machine learning. KNN is often considered a lazy
    learner; it doesn’t technically train a model to make predictions. Instead an
    observation is predicted to be the same class as that of the largest proportion
    of the *k* nearest observations.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if an observation with an unknown class is surrounded by an observation
    of class 1, then the observation is classified as class 1\. In this chapter we
    will explore how to use scikit-learn to create and use a KNN classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 15.1 Finding an Observation’s Nearest Neighbors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to find an observation’s *k* nearest observations (neighbors).
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use scikit-learn’s `NearestNeighbors`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our solution we used the dataset of iris flowers. We created an observation,
    `new_observation`, with some values and then found the two observations that are
    closest to our observation. `indices` contains the locations of the observations
    in our dataset that are closest, so `X[indices]` displays the values of those
    observations. Intuitively, distance can be thought of as a measure of similarity,
    so the two closest observations are the two flowers most similar to the flower
    we created.
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we measure distance? scikit-learn offers a wide variety of distance
    metrics, <math display="inline"><mi>d</mi></math>, including Euclidean:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>d</mi> <mrow><mi>e</mi><mi>u</mi><mi>c</mi><mi>l</mi><mi>i</mi><mi>d</mi><mi>e</mi><mi>a</mi><mi>n</mi></mrow></msub>
    <mo>=</mo> <msqrt><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'and Manhattan distance:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>d</mi> <mrow><mi>m</mi><mi>a</mi><mi>n</mi><mi>h</mi><mi>a</mi><mi>t</mi><mi>t</mi><mi>a</mi><mi>n</mi></mrow></msub>
    <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <mfenced close="|" open="|" separators=""><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>-</mo> <msub><mi>y</mi> <mi>i</mi></msub></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, `NearestNeighbors` uses Minkowski distance:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>d</mi> <mrow><mi>m</mi><mi>i</mi><mi>n</mi><mi>k</mi><mi>o</mi><mi>w</mi><mi>s</mi><mi>k</mi><mi>i</mi></mrow></msub>
    <mo>=</mo> <msup><mfenced close=")" open="(" separators=""><munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover> <msup><mfenced
    close="|" open="|" separators=""><msub><mi>x</mi> <mi>i</mi></msub> <mo>-</mo><msub><mi>y</mi>
    <mi>i</mi></msub></mfenced> <mi>p</mi></msup></mfenced> <mrow><mn>1</mn><mo>/</mo><mi>p</mi></mrow></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math> and <math
    display="inline"><msub><mi>y</mi><mi>i</mi></msub></math> are the two observations
    we are calculating the distance between. Minkowski includes a hyperparameter,
    <math display="inline"><mi>p</mi></math>, where <math display="inline"><mi>p</mi></math>
    = 1 is Manhattan distance and <math display="inline"><mi>p</mi></math> = 2 is
    Euclidean distance, and so on. By default in scikit-learn <math display="inline"><mi>p</mi></math>
    = 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can set the distance metric using the `metric` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `distance` variable we created contains the actual distance measurement
    to each of the two nearest neighbors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition, we can use `kneighbors_graph` to create a matrix indicating each
    observation’s nearest neighbors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: When we are finding nearest neighbors or using any learning algorithm based
    on distance, it is important to transform features so that they are on the same
    scale. This is because the distance metrics treat all features as if they were
    on the same scale, but if one feature is in millions of dollars and a second feature
    is in percentages, the distance calculated will be biased toward the former. In
    our solution we addressed this potential issue by standardizing the features using
    `StandardScaler`.
  prefs: []
  type: TYPE_NORMAL
- en: 15.2 Creating a K-Nearest Neighbors Classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given an observation of unknown class, you need to predict its class based on
    the class of its neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If the dataset is not very large, use `KNeighborsClassifier`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In KNN, given an observation, <math display="inline"><msub><mi>x</mi><mi>u</mi></msub></math>,
    with an unknown target class, the algorithm first identifies the <math display="inline"><mi>k</mi></math>
    closest observations (sometimes called <math display="inline"><msub><mi>x</mi><mi>u</mi></msub></math>’s
    *neighborhood*) based on some distance metric (e.g., Euclidean distance), then
    these <math display="inline"><mi>k</mi></math> observations “vote” based on their
    class, and the class that wins the vote is <math display="inline"><msub><mi>x</mi><mi>u</mi></msub></math>’s
    predicted class. More formally, the probability <math display="inline"><msub><mi>x</mi><mi>u</mi></msub></math>
    of some class <math display="inline"><mi>j</mi></math> is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mfrac><mn>1</mn> <mi>k</mi></mfrac> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>∈</mo><mi>ν</mi></mrow></munderover> <mi>I</mi> <mrow><mo>(</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo> <mi>j</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'where ν is the <math display="inline"><mi>k</mi></math> observation in <math
    display="inline"><msub><mi>x</mi><mi>u</mi></msub></math>’s neighborhood, <math
    display="inline"><msub><mi>y</mi><mi>i</mi></msub></math> is the class of the
    *i*th observation, and <math display="inline"><mi>I</mi></math> is an indicator
    function (i.e., 1 is true, 0 otherwise). In scikit-learn we can see these probabilities
    using `predict_proba`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The class with the highest probability becomes the predicted class. For example,
    in the preceding output, the first observation should be class 1 (*Pr* = 0.6)
    while the second observation should be class 2 (*Pr* = 1), and this is just what
    we see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '`KNeighborsClassifier` contains a number of important parameters to consider.
    First, `metric` sets the distance metric used. Second, `n_jobs` determines how
    many of the computer’s cores to use. Because making a prediction requires calculating
    the distance from a point to every single point in the data, using multiple cores
    is highly recommended. Third, `algorithm` sets the method used to calculate the
    nearest neighbors. While there are real differences in the algorithms, by default
    `KNeighborsClassifier` attempts to auto-select the best algorithm so you often
    don’t need to worry about this parameter. Fourth, by default `KNeighborsClassifier`
    works how we described previously, with each observation in the neighborhood getting
    one vote; however, if we set the `weights` parameter to `distance`, the closer
    observations’ votes are weighted more than observations farther away. Intuitively
    this make sense, since more similar neighbors might tell us more about an observation’s
    class than others.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, because distance calculations treat all features as if they are on
    the same scale, it is important to standardize the features prior to using a KNN
    classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 15.3 Identifying the Best Neighborhood Size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to select the best value for *k* in a k-nearest neighbors classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use model selection techniques like `GridSearchCV`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The size of *k* has real implications in KNN classifiers. In machine learning
    we are trying to find a balance between bias and variance, and in few places is
    that as explicit as the value of *k*. If *k* = *n*, where *n* is the number of
    observations, then we have high bias but low variance. If *k* = 1, we will have
    low bias but high variance. The best model will come from finding the value of
    *k* that balances this bias-variance trade-off. In our solution, we used `GridSearchCV`
    to conduct five-fold cross-validation on KNN classifiers with different values
    of *k*. When that is completed, we can see the *k* that produces the best model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 15.4 Creating a Radius-Based Nearest Neighbors Classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given an observation of unknown class, you need to predict its class based on
    the class of all observations within a certain distance.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use `RadiusNeighborsClassifier`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In KNN classification, an observation’s class is predicted from the classes
    of its *k* neighbors. A less common technique is classification in a *radius-based
    nearest neighbor* (RNN) classifier, where an observation’s class is predicted
    from the classes of all observations within a given radius *r*.
  prefs: []
  type: TYPE_NORMAL
- en: In scikit-learn, `RadiusNeighborsClassifier` is very similar to `KNeighbors​Classi⁠fier`,
    with the exception of two parameters. First, in `RadiusNeighbors​Clas⁠sifier`
    we need to specify the radius of the fixed area used to determine if an observation
    is a neighbor using `radius`. Unless there is some substantive reason for setting
    `radius` to some value, it’s best to treat it like any other hyperparameter and
    tune it during model selection. The second useful parameter is `outlier_label`,
    which indicates what label to give an observation that has no observations within
    the radius—​which itself can be a useful tool for identifying outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 15.5 Finding Approximate Nearest Neighbors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You want to fetch nearest neighbors for big data at low latency:'
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use an *approximate nearest neighbors* (ANN) based search with Facebook’s `faiss`
    library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: KNN is a great approach to finding the most similar observations in a set of
    small data. However, as the size of our data increases, so does the time it takes
    to compute the distance between any one observation and all other points in our
    dataset. Large scale ML systems such as search or recommendation engines often
    use some form of vector similarity measure to retrieve similar observations. But
    at scale in real time, where we need results in less than 100 ms, KNN becomes
    infeasible to run.
  prefs: []
  type: TYPE_NORMAL
- en: ANN helps us overcome this problem by sacrificing some of the quality of the
    exact nearest neighbors search in favor of speed. This is to say that although
    the order and items in the first 10 nearest neighbors of an ANN search may not
    match the first 10 results from an exact KNN search, we get those first 10 nearest
    neighbors much faster.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we use an ANN approach called inverted file index (IVF). This
    approach works by using clustering to limit the scope of the search space for
    our nearest neighbors search. IVF uses Voronoi tessellations to partition our
    search space into a number of distinct areas (or clusters). And when we go to
    find nearest neighbors, we visit a limited number of clusters to find similar
    observations, as opposed to conducting a comparison across every point in our
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How Voronoi tessellations are created from data is best visualized using simple
    data. Take a scatter plot of random data visualized in two dimensions, as shown
    in [Figure 15-1](#fig1501).
  prefs: []
  type: TYPE_NORMAL
- en: '![mpc2 1501](assets/mpc2_1501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-1\. A scatter plot of randomly generated two-dimensional data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using Voronoi tessellations, we can create a number of subspaces, each of which
    contains only a small subset of the total observations we want to search, as shown
    in [Figure 15-2](#fig1502).
  prefs: []
  type: TYPE_NORMAL
- en: '![mpc2 1502](assets/mpc2_1502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-2\. Randomly generated two-dimensional data separated into a number
    of different subspaces
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `nlist` parameter in the `Faiss` library lets us define the number of clusters
    we want to create. An additional parameter, `nprobe`, can be used at query time
    to define the number of clusters we want to search to retrieve nearest neighbors
    for a given observation. Increasing both `nlist` and `nprobe` can result in higher
    quality neighbors at the cost of larger computational effort and thus a longer
    runtime for IVF indices. Decreasing each of these parameters will have the inverse
    effect, and your code will run faster but at the risk of returning lower quality
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Notice this example returns the exact same output as the first recipe in this
    chapter. This is because we are working with very small data and using only three
    clusters, which makes it unlikely our ANN results will differ significantly from
    our KNN results.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Nearest Neighbor Indexes for Similarity Search (different ANN index types)](https://oreil.ly/DVqgn)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 15.6 Evaluating Approximate Nearest Neighbors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You want to see how your ANN compares to exact nearest neighbors (KNN):'
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Compute the recall @k nearest neighbors of the ANN as compared to the KNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Recall @k* is most simply defined as the number of items returned by the ANN
    at some *k* nearest neighbors that also appear in the exact nearest neighbors
    at the same *k*, divided by *k*. In this example, at 10 nearest neighbors we have
    100% recall, which means that our ANN is returning the same indices as our KNN
    at k=10 (though not necessarily in the same order).'
  prefs: []
  type: TYPE_NORMAL
- en: Recall is a common metric to use when evaluating ANNs against exact nearest
    neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Google’s note on ANN for its Vertex Matching Engine Service](https://oreil.ly/-COc9)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
