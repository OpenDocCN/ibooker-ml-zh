<html><head></head><body><section data-pdf-bookmark="Chapter 11. Personalized Recommendation Metrics" data-type="chapter" epub:type="chapter"><div class="chapter" id="PersonalRecMetrics">&#13;
<h1><span class="label">Chapter 11. </span>Personalized Recommendation Metrics</h1>&#13;
&#13;
&#13;
<p>Having<a data-primary="recommendation systems" data-secondary="ranking metrics" data-type="indexterm" id="RSrankingm11"/> explored the powerful methodologies of MF and neural networks in the context of personalization, we are now equipped with potent tools to craft sophisticated recommendation systems. However, the order of recommendations in a list may have a profound impact on user engagement and satisfaction.</p>&#13;
&#13;
<p>Our journey so far has primarily been focused on predicting what a user may like, using latent factors or deep learning architectures. However, the manner in which we present these predictions, or more formally, how we rank these recommendations, holds paramount significance. Therefore, this chapter will shift our gaze from the prediction problem and will unravel the complex landscape of ranking in recommendation systems.</p>&#13;
&#13;
<p>This chapter is dedicated to understanding key ranking metrics including mean average precision (mAP), mean reciprocal rank (MRR), and normalized discounted cumulative gain (NDCG). Each of these metrics takes a unique approach toward quantifying the quality of our rankings, catering to different aspects of the user interaction.</p>&#13;
&#13;
<p>We’ll dive into the intricacies of these metrics, unveiling their computational details and discussing their interpretation, covering their strengths and weaknesses, and pointing out their specific relevance to various personalization scenarios.</p>&#13;
&#13;
<p>This exploration forms an integral part of the evaluation process in recommendation systems. It not only gives us a robust framework to measure the performance of our system but also provides essential insights into understanding how different algorithms might perform in online settings. This will lay the foundation for future discussions on algorithmic bias, diversity in recommendations, and a multistakeholder approach to recommendation systems.</p>&#13;
&#13;
<p>In essence, the knowledge garnered in this chapter will be instrumental in fine-tuning our recommendation system, ensuring that we don’t just predict well but also recommend in a way that truly resonates with individual user preferences and behaviors.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Environments" data-type="sect1"><div class="sect1" id="id216">&#13;
<h1>Environments</h1>&#13;
&#13;
<p>Before<a data-primary="ranking metrics" data-secondary="evaluating recommendation systems" data-type="indexterm" id="RMeval11"/><a data-primary="recommendation systems" data-secondary="evaluating" data-type="indexterm" id="RSeval11"/> we dig into defining the key metrics, we’re going to spend a few moments discussing the kinds of evaluation we can do. Evaluation for recommendation systems, as you’ll soon see, is frequently characterized by how <em>relevant</em> the recommendations are for a user. This is similar to search metrics, but we add in additional factors to account for <em>where</em> in the list the most relevant items are.</p>&#13;
&#13;
<p>For an extremely comprehensive view on evaluation of recommender systems, the recent project <a href="https://oreil.ly/b6mPy">RecList</a> builds a useful checklist-based framework for organizing metrics and evaluations.</p>&#13;
&#13;
<p>Often you’ll hear about evaluating recommenders in a few setups:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Online/offline</p>&#13;
</li>&#13;
<li>&#13;
<p>User/item</p>&#13;
</li>&#13;
<li>&#13;
<p>A/B</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Each setup provides slightly different kinds of evaluations and tells you different things. Let’s quickly break down the differences to set some assumptions about terminology.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Online and Offline" data-type="sect2"><div class="sect2" id="id126">&#13;
<h2>Online and Offline</h2>&#13;
&#13;
<p>When<a data-primary="environments" data-secondary="online and offline evaluation" data-type="indexterm" id="id965"/><a data-primary="offline evaluation" data-type="indexterm" id="id966"/> we refer to online versus offline recommenders, we are referring to <em>when</em> you’re running evals. In <em>offline evaluation</em>, you start with a test/evaluation dataset, outside your production system, and compute a set of metrics. This is often the simplest recommender to set up but has the highest expectation of existing data. Using historical data, you construct a set of relevant responses, which you can then use during simulated inference. This approach is the most similar to other kinds of traditional ML, although with slightly different computations for the error.</p>&#13;
&#13;
<p>When we’re training large models, these datasets are similar to an offline dataset. We previously saw prequential data, which is much more relevant in recommendation systems than in lots of other ML applications. Sometimes you’ll hear people say that “all recommenders are sequential recommenders” because of the importance of historical exposure to the recommender problem.</p>&#13;
&#13;
<p class="less_space pagebreak-before"><em>Online evaluation</em> takes<a data-primary="online evaluation" data-type="indexterm" id="id967"/> place during inference, usually in production. The tricky part is that you essentially never know the counterfactual outcomes. You can compute specific metrics on the online rankings: frequency and distributions of covariates, CTR/success rate, or time on platform, but ultimately these are different from the offline metrics.</p>&#13;
<div data-type="tip"><h1>Bootstrapping from Historical Evaluation Data</h1>&#13;
<p>One<a data-primary="environments" data-secondary="historical evaluation data" data-type="indexterm" id="id968"/><a data-primary="historical evaluation data" data-type="indexterm" id="id969"/> of the most common questions from people building a recommender from scratch is “Where do you get the initial training data?” This is a hard problem. Ultimately, you have to be clever to come up with a useful dataset. Consider our co-occurrence data in the Wikipedia recommender; we didn’t require any user interactions to get to a set of data to build a recommender. Bootstrapping from item to item is the most popular strategy, but you can use  other tricks as well. The simplest way to start moving into user-item recommenders is to simply ask the user questions. If you ask for preference information across a set of item features, you can build simple models that start to incorporate this.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="User Versus Item Metrics" data-type="sect2"><div class="sect2" id="id127">&#13;
<h2>User Versus Item Metrics</h2>&#13;
&#13;
<p>Because<a data-primary="environments" data-secondary="user versus item metrics" data-type="indexterm" id="id970"/><a data-primary="user metrics" data-type="indexterm" id="id971"/><a data-primary="item metrics" data-type="indexterm" id="id972"/><a data-primary="metrics" data-secondary="user versus item metrics" data-type="indexterm" id="id973"/> recommender systems are personalization machines, it can be easy to think that we always want to be making recommendations for the user and measuring the performance as such. Subtleties exist, though. We want to be sure individual items are getting a fair chance, and sometimes looking at the other side of the equation can help assess this. In other words, are the items getting recommended frequently enough to have a chance to find their niche? We should explicitly compute our metrics over user <em>and</em> item axes.</p>&#13;
&#13;
<p>Another aspect of item-side metrics is for set-based recommenders. The other items that are recommended in context can have a significant effect on the performance of a recommendation. As a result, we should be careful to measure the pairwise item metrics in our large-scale evaluations.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="A/B Testing" data-type="sect2"><div class="sect2" id="id128">&#13;
<h2>A/B Testing</h2>&#13;
&#13;
<p>It’s<a data-primary="environments" data-secondary="A/B testing" data-type="indexterm" id="id974"/><a data-primary="A/B testing" data-type="indexterm" id="id975"/><a data-primary="testing" data-secondary="A/B testing" data-type="indexterm" id="id976"/> good to use randomized, controlled trials to evaluate how your new recommendation model is performing. For recommendations, this is quite tricky. At the end of this chapter, you’ll see some of the nuance, but for now, let’s consider a quick reminder of how to think about A/B testing in a closed-loop paradigm.</p>&#13;
&#13;
<p class="less_space pagebreak-before">A/B tests ultimately attempt to estimate the effect size of swapping one model in for another; effect size estimation is the process of measuring the causal impact of an intervention on a target metric. First, we would need to deploy two recommender models. We’d also hope that there’s a reasonable randomization of users into each of the recommenders. However, what’s the randomization unit? It’s easy to quickly assume it’s the user, but what has changed about the recommender? Has the recommender changed in a way that covaries with some properties of the distribution—e.g., have you built a new recommender that is less friendly toward seasonal TV specials just as we enter into the second week of November?</p>&#13;
&#13;
<p>Another consideration with this sort of testing for recommendation systems is the long-term compounding effects. A frequent rejoinder about a series of positive A/B test outcomes over several years is “Have you tested the first recommender against the last?” This is because populations change, both the users and the items. As you also vary the recommender system, you frequently find yourself in a double-blind situation where you’ve never seen this user or item population with any other recommender. If all the effect sizes of every A/B test were additive across the industry, the world GDP would likely be two to three times as large.</p>&#13;
&#13;
<p>The way to guard against protests like this is via a<a data-primary="long-term holdout" data-type="indexterm" id="id977"/> <em>long-term holdout</em>, a random subset of users (continually being added to) who will not be upgraded to new models through time. By measuring the target metrics on this set versus the most cutting-edge model in production, you’re always able to understand the long-term effects of your work. The downside of a long-term holdout? It’s hard to maintain, and it’s hard to sacrifice some of the effects of your work on a subset of the population.</p>&#13;
&#13;
<p>Now let’s finally get to the metrics already!<a data-primary="" data-startref="RMeval11" data-type="indexterm" id="id978"/><a data-primary="" data-startref="RSeval11" data-type="indexterm" id="id979"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Recall and Precision" data-type="sect1"><div class="sect1" id="id129">&#13;
<h1>Recall and Precision</h1>&#13;
&#13;
<p>Let’s<a data-primary="ranking metrics" data-secondary="recall and precision" data-type="indexterm" id="RMrecall11"/><a data-primary="recall" data-type="indexterm" id="recall11"/><a data-primary="precision" data-type="indexterm" id="precision11"/> begin by considering four recommender problems and how each may have different implications for the kind of results you want.</p>&#13;
&#13;
<p>First, let’s consider entering a bookstore and looking for a book by a popular author. We would say this is the recommender problem:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Provides a lot of recommendations</p>&#13;
</li>&#13;
<li>&#13;
<p>Offers few possible relevant results</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Additionally, if the bookstore has a good selection, we’d expect that <em>all</em> the relevant results are contained in the recommendations because bookstores often carry most or all of an author’s oeuvre once they’ve become popular. However, many of the recommendations—the books in the bookstore–are simply not relevant for this search.</p>&#13;
&#13;
<p class="less_space pagebreak-before">Second, let’s consider looking for a gas station nearby on a mapping app while in a large metro. We expect that a lot of gas stations are relatively close by, but you would probably consider only the first couple—or maybe even only one, the first one that you see. Thus a recommender for this problem has the following:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Many relevant results</p>&#13;
</li>&#13;
<li>&#13;
<p>Few useful recommendations</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>In the first scenario, the relevant results may be fully contained in the recommendations, and in the second scenario, the recommendations may be fully contained in the relevant results.</p>&#13;
&#13;
<p>Let’s now look at more common scenarios.</p>&#13;
&#13;
<p>For our third example, consider that you’re searching on a streaming video platform for something to watch tonight when you’re feeling romantic. Streaming platforms tend to show a lot of recommendations—pages and pages from this one theme or another. But on this night, and on just this platform, only a couple of movies or TV shows might really fit what you’re looking for. Our recommender, then, does the following:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Provides many recommendations</p>&#13;
</li>&#13;
<li>&#13;
<p>Offers only a few that are actually relevant</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>However, importantly, not all relevant results will be in the recommendations! As we know, different platforms have different media, so some of the relevant results won’t appear in the recommendations no matter how many we look at.</p>&#13;
&#13;
<p>Fourth, and finally, you’re a high-end coffee lover with distinguished tastes headed into the local roaster for a third-wave, single-origin coffee. As an experienced coffee connoisseur, you love high-quality coffees from all over the world and enjoy most but not all origins. On any given day, your local cafe has only a few single-origin hand-brewed options. Despite your worldly palette, there are some popular terroirs that you just don’t love. This little recommendation brew bar can be described as follows:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Provides a few recommendations</p>&#13;
</li>&#13;
<li>&#13;
<p>Offers many possible recommendations that are relevant</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>On any given day, only some of the few recommendations may be relevant to you.</p>&#13;
&#13;
<p class="less_space pagebreak-before">So those are our matching four scenarios. For the latter two, the intersection between recommendation and relevance may be proportionally small or large—or even empty! The main idea is that the full size of the smaller sample is not always <span class="keep-together">in use.</span></p>&#13;
&#13;
<p>Now that we’ve worked through a few examples, let’s see how they relate to the core metrics for a recommender: precision and recall <em>@ k</em> (<a data-type="xref" href="#recall_sets">Figure 11-1</a>). Focusing on examples 3 and 4, we can see that only some of the recommendations intersect with the options that are relevant. And only some of the relevant options intersect with the recommendations. It’s often overlooked, but in fact <em>these two ratios define our metrics</em>—let’s go!</p>&#13;
&#13;
<figure><div class="figure" id="recall_sets">&#13;
<img alt="The sets in a retreival problem" src="assets/brpj_1101.png"/>&#13;
<h6><span class="label">Figure 11-1. </span>Recall and precision sets</h6>&#13;
</div></figure>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="@ k" data-type="sect2"><div class="sect2" id="id130">&#13;
<h2>@ k</h2>&#13;
&#13;
<p>In<a data-primary="@ k" data-type="indexterm" id="id980"/> much of this chapter and RecSys metrics discussion, we say things like <em>@ k</em>. This means “at <em>k</em>,” which should really be “in <em>k</em>” or “out of <em>k</em>.” These are simply the size of the set of recommendations. We often anchor the customer experience on how many recommendations we can show the user without the experience suffering. We also need to know the cardinality of the set of relevant items, which we call <em>@ r</em>. Note that while it may not feel like it’s possible to ever know this number, we assume this refers to “known relevant” options via our training or test data.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="less_space pagebreak-before" data-pdf-bookmark="Precision at k" data-type="sect2"><div class="sect2" id="id314">&#13;
<h2>Precision at k</h2>&#13;
&#13;
<p><em>Precision</em> is the ratio of the size of the set of relevant recommendations to <em>k</em>, the size of the set of recommendations.</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper P r e c i s i o n commercial-at k equals StartFraction n u m Subscript r e l e v a n t Baseline Over left-parenthesis k right-parenthesis EndFraction" display="block">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mi>r</mi>&#13;
    <mi>e</mi>&#13;
    <mi>c</mi>&#13;
    <mi>i</mi>&#13;
    <mi>s</mi>&#13;
    <mi>i</mi>&#13;
    <mi>o</mi>&#13;
    <mi>n</mi>&#13;
    <mo>@</mo>&#13;
    <mi>k</mi>&#13;
    <mo>=</mo>&#13;
    <mstyle displaystyle="true" scriptlevel="0">&#13;
      <mfrac><mrow><mi>n</mi><mi>u</mi><msub><mi>m</mi> <mrow><mi>r</mi><mi>e</mi><mi>l</mi><mi>e</mi><mi>v</mi><mi>a</mi><mi>n</mi><mi>t</mi></mrow> </msub></mrow> <mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></mfrac>&#13;
    </mstyle>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Notice that the size of the relevant items doesn’t appear in the formula. That’s OK; the size of the intersection is still dependent on the size of the set of relevant items.</p>&#13;
&#13;
<p>Looking at our examples, 2 technically has the highest precision, but it’s a bit of a red herring because of the number of relevant results. This is one reason  precision is not the most common metric for evaluating recommendation systems.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Recall at k" data-type="sect2"><div class="sect2" id="id131">&#13;
<h2>Recall at k</h2>&#13;
&#13;
<p><em>Recall</em> is the ratio of the size of the set of relevant recommendations to <em>r</em>, the size of the set of relevant items.</p>&#13;
&#13;
<p>But wait! If the ratio is the relevant recommendations over the relevant items, where is <em>k</em>? <em>k</em> is still important here because the size of the set of recommendations constrains the possible size of the intersection. Recall that these ratios are operating on that intersection that is always dependent on <em>k</em>. This means you often consider the max of <em>r</em> and <em>k</em>.</p>&#13;
&#13;
<p>In scenario 3, we hope that some of the movies that fit our heart’s desire will be on the right streaming platform. The number of these divided by the count of all the media anywhere is the <em>recall</em>. If all your relevant movies are on this platform, you might call that<a data-primary="total recall" data-type="indexterm" id="id981"/> <em>total recall</em>.</p>&#13;
&#13;
<p>Scenario 4’s café experience shows that recall is sometimes the inverse probability of an avoid; because you like so many coffees, we might instead find it easier to talk about what you don’t like. In this case, the number of avoids in the offering will have a large effect on the recall:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper R e c a l l commercial-at k equals StartFraction left-parenthesis k minus upper A v o i d commercial-at k right-parenthesis Over n u m Subscript r e l e v a n t Baseline EndFraction" display="block">&#13;
  <mrow>&#13;
    <mi>R</mi>&#13;
    <mi>e</mi>&#13;
    <mi>c</mi>&#13;
    <mi>a</mi>&#13;
    <mi>l</mi>&#13;
    <mi>l</mi>&#13;
    <mo>@</mo>&#13;
    <mi>k</mi>&#13;
    <mo>=</mo>&#13;
    <mstyle displaystyle="true" scriptlevel="0">&#13;
      <mfrac><mrow><mo>(</mo><mi>k</mi><mo>-</mo><mi>A</mi><mi>v</mi><mi>o</mi><mi>i</mi><mi>d</mi><mo>@</mo><mi>k</mi><mo>)</mo></mrow> <mrow><mi>n</mi><mi>u</mi><msub><mi>m</mi> <mrow><mi>r</mi><mi>e</mi><mi>l</mi><mi>e</mi><mi>v</mi><mi>a</mi><mi>n</mi><mi>t</mi></mrow> </msub></mrow></mfrac>&#13;
    </mstyle>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>This is the core mathematical definition for recall and is often one of the first measurements we’ll consider because it’s a pure estimate of how your retrieval is performing.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="R-precision" data-type="sect2"><div class="sect2" id="id132">&#13;
<h2>R-precision</h2>&#13;
&#13;
<p>If<a data-primary="top-r recommendations" data-type="indexterm" id="id982"/><a data-primary="r-precision" data-type="indexterm" id="id983"/> we also have a ranking on our recommendations, we can take the ratio of relevant recommendations to <em>r</em> in the <em>top-r</em> recommendations. This improves this metric in cases where <em>r</em> is very small, as in examples 1 and 3.<a data-primary="" data-startref="RMrecall11" data-type="indexterm" id="id984"/><a data-primary="" data-startref="recall11" data-type="indexterm" id="id985"/><a data-primary="" data-startref="precision11" data-type="indexterm" id="id986"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="mAP, MMR, NDCG" data-type="sect1"><div class="sect1" id="id217">&#13;
<h1>mAP, MMR, NDCG</h1>&#13;
&#13;
<p>Having<a data-primary="ranking metrics" data-secondary="ordering of recommendations" data-type="indexterm" id="id987"/> delved into the reliable domains of precision@<em>k</em> and recall@<em>k</em>, we’ve gained valuable insights into the quality of our recommendation systems. However, these metrics, as crucial as they are, can sometimes fall short in capturing an important aspect of these systems: <em>the order of recommendations</em>.</p>&#13;
&#13;
<p>In recommendation systems, the ordering in which we present suggestions carries significant weight and needs to be evaluated to ensure that it’s effective.</p>&#13;
&#13;
<p>That’s why we’ll now journey beyond precision@<em>k</em> and recall@<em>k</em> to explore some key ranking-sensitive metrics—namely, mean average precision (mAP), mean reciprocal rank (MRR), and normalized discounted cumulative gain (NDCG). These metrics consider not only whether our recommendations are relevant but also whether they are well-ordered.</p>&#13;
&#13;
<p>The mAP metric lends importance to each relevant document and its position, and MRR concentrates on the rank of the first relevant item. NDCG gives more importance to relevant documents at higher ranks. By understanding these metrics, you’ll have an even more robust set of tools to evaluate and refine your recommendation systems.</p>&#13;
&#13;
<p>So, let’s carry on with our exploration, striking a balance between precision and comprehensibility. By the end of this section, you will be well equipped to handle these essential evaluation methods in a confident and knowledgeable manner.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="mAP" data-type="sect2"><div class="sect2" id="id133">&#13;
<h2>mAP</h2>&#13;
&#13;
<p>This<a data-primary="ranking metrics" data-secondary="mean average precision (mAP)" data-type="indexterm" id="id988"/><a data-primary="mean average precision (mAP)" data-type="indexterm" id="id989"/> vital metric in recommendation systems is particularly adept at accounting for the rank of relevant items. If, in a list of five items, the relevant ones are found at positions 2, 3, and 5, mAP would be calculated by computing precision@2, precision@3, and precision@5 and then taking an average of these values. The strength of mAP lies in its sensitivity to the ordering of relevant items, providing a higher score when these items are ranked higher.</p>&#13;
&#13;
<p>Consider an example with two recommendation algorithms A and B:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>For algorithm A, we compute the mAP as follows:</p>&#13;
&#13;
<p>(precision@2 + precision@3 + precision@5) / 3 = (1/2 + 2/3 + 3/5) / 3 = 0.6</p>&#13;
</li>&#13;
<li>&#13;
<p>For algorithm B, which perfectly ranks the items, we calculate mAP as follows:</p>&#13;
&#13;
<p>mAP = (precision@1 + precision@2 + precision@3) / 3 = (1/1 + 2/2 + 3/3) / 3 = 1</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The generalized formula for mAP across a set of queries Q is shown here:</p>&#13;
<div data-type="equation">&#13;
<math alttext="dollar-sign m upper A upper P equals StartFraction 1 Over StartAbsoluteValue upper Q EndAbsoluteValue EndFraction sigma-summation Underscript q equals 1 Overscript StartAbsoluteValue upper Q EndAbsoluteValue Endscripts StartFraction 1 Over m Subscript q Baseline EndFraction sigma-summation Underscript k equals 1 Overscript n Endscripts upper P left-parenthesis k right-parenthesis asterisk r e l left-parenthesis k right-parenthesis dollar-sign">&#13;
  <mrow>&#13;
    <mi>m</mi>&#13;
    <mi>A</mi>&#13;
    <mi>P</mi>&#13;
    <mo>=</mo>&#13;
    <mfrac><mn>1</mn> <mrow><mo>|</mo><mi>Q</mi><mo>|</mo></mrow></mfrac>&#13;
    <msubsup><mo>∑</mo> <mrow><mi>q</mi><mo>=</mo><mn>1</mn></mrow> <mrow><mo>|</mo><mi>Q</mi><mo>|</mo></mrow> </msubsup>&#13;
    <mfrac><mn>1</mn> <msub><mi>m</mi> <mi>q</mi> </msub></mfrac>&#13;
    <msubsup><mo>∑</mo> <mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </msubsup>&#13;
    <mi>P</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>k</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>*</mo>&#13;
    <mi>r</mi>&#13;
    <mi>e</mi>&#13;
    <mi>l</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>k</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Here, <math alttext="StartAbsoluteValue upper Q EndAbsoluteValue">&#13;
  <mrow>&#13;
    <mo>|</mo>&#13;
    <mi>Q</mi>&#13;
    <mo>|</mo>&#13;
  </mrow>&#13;
</math> is the total number of queries, <math alttext="m Subscript q">&#13;
  <msub><mi>m</mi> <mi>q</mi> </msub>&#13;
</math> is the number of relevant documents for a specific query <math alttext="q">&#13;
  <mi>q</mi>&#13;
</math>, <math alttext="upper P left-parenthesis k right-parenthesis">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>k</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math> stands for the precision at the _k_th cutoff, and <math alttext="r e l left-parenthesis k right-parenthesis">&#13;
  <mrow>&#13;
    <mi>r</mi>&#13;
    <mi>e</mi>&#13;
    <mi>l</mi>&#13;
    <mo>(</mo>&#13;
    <mi>k</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math> is an indicator function equating to 1 if the item at rank <math alttext="k">&#13;
  <mi>k</mi>&#13;
</math> is relevant, and 0 otherwise.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="MRR" data-type="sect2"><div class="sect2" id="id134">&#13;
<h2>MRR</h2>&#13;
&#13;
<p>Another<a data-primary="mean reciprocal rank (MRR)" data-type="indexterm" id="id990"/><a data-primary="ranking metrics" data-secondary="mean reciprocal rank (MRR)" data-type="indexterm" id="id991"/> effective metric used in recommendation systems is MRR. Unlike MAP, which considers all relevant items, MRR primarily focuses on the position of the first relevant item in the recommendation list. It’s computed as the reciprocal of the rank where the first relevant item appears.</p>&#13;
&#13;
<p>Consequently, MRR can reach its maximum value of 1 if the first item in the list is relevant. If the first relevant item is found farther down the list, MRR takes a value less than 1. For instance, if the first relevant item is positioned at rank 2, the MRR would be 1/2.</p>&#13;
&#13;
<p>Let’s look at this in the context of the recommendation algorithms A and B that we used earlier:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>For algorithm A, the first relevant item is at rank 2, so the MRR equals 1/2 = 0.5.</p>&#13;
</li>&#13;
<li>&#13;
<p>For algorithm B, which perfectly ranked the items, the first relevant item is at rank 1, so the MRR equals 1/1 = 1.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Extending this to multiple queries, the general formula for MRR is as follows:</p>&#13;
<div data-type="equation">&#13;
<math alttext="dollar-sign upper M upper R upper R equals StartFraction 1 Over StartAbsoluteValue upper Q EndAbsoluteValue EndFraction sigma-summation Underscript i equals 1 Overscript StartAbsoluteValue upper Q EndAbsoluteValue Endscripts StartFraction 1 Over r a n k Subscript i Baseline EndFraction dollar-sign">&#13;
  <mrow>&#13;
    <mi>M</mi>&#13;
    <mi>R</mi>&#13;
    <mi>R</mi>&#13;
    <mo>=</mo>&#13;
    <mfrac><mn>1</mn> <mrow><mo>|</mo><mi>Q</mi><mo>|</mo></mrow></mfrac>&#13;
    <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mrow><mo>|</mo><mi>Q</mi><mo>|</mo></mrow> </msubsup>&#13;
    <mfrac><mn>1</mn> <mrow><mi>r</mi><mi>a</mi><mi>n</mi><msub><mi>k</mi> <mi>i</mi> </msub></mrow></mfrac>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Here, |<em>Q</em>| represents the total number of queries, and <math alttext="r a n k Subscript i">&#13;
  <mrow>&#13;
    <mi>r</mi>&#13;
    <mi>a</mi>&#13;
    <mi>n</mi>&#13;
    <msub><mi>k</mi> <mi>i</mi> </msub>&#13;
  </mrow>&#13;
</math> is the position of the first relevant item in the list for the _i_th query. This metric provides valuable insight into how well a recommendation algorithm delivers a relevant recommendation right at the top of the list.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="NDCG" data-type="sect2"><div class="sect2" id="id135">&#13;
<h2>NDCG</h2>&#13;
&#13;
<p>To<a data-primary="normalized discounted cumulative gain (NDCG)" data-type="indexterm" id="id992"/><a data-primary="ranking metrics" data-secondary="normalized discounted cumulative gain (NDCG)" data-type="indexterm" id="id993"/> further refine our understanding of ranking metrics, let’s step into the world of NDCG. Like mAP and MRR, NDCG also acknowledges the rank order of relevant items but introduces a twist. It discounts the relevance of items as we move down the list, signifying that items appearing earlier in the list are more valuable than those ranked lower.</p>&#13;
&#13;
<p>NDCG begins with the concept of<a data-primary="CG (cumulative gain)" data-type="indexterm" id="id994"/><a data-primary="cumulative gain (CG)" data-type="indexterm" id="id995"/> cumulative gain (CG), which is simply the sum of the relevance scores of the top <em>k</em> items in the list. Discounted cumulative gain (DCG) goes a step further, discounting the relevance of each item based on its position. NDCG, then, is the DCG value normalized by the<a data-primary="ideal DCG (IDCG)" data-type="indexterm" id="id996"/> ideal DCG (IDCG), the DCG that we would obtain if all relevant items appeared at the very top of the list.</p>&#13;
&#13;
<p>Assuming we have five items in our list and a specific user for whom the relevant items are found at positions 2 and 3, the IDCG@<em>k</em> would be (1/log(1 + 1) + 1/log(2 + 1)) = 1.5 + 0.63 = 2.13.</p>&#13;
&#13;
<p>Let’s put this into the context of our example algorithms A and B.</p>&#13;
<dl>&#13;
<dt>For algorithm A</dt>&#13;
<dd>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>DCG@5 = 1/log(2 + 1) + 1/log(3 + 1) + 1/log(5 + 1) = 0.63 + 0.5 + 0.39 = 1.52</p>&#13;
</li>&#13;
<li>&#13;
<p>NDCG@5 = DCG@5 / IDCG@5 = 1.52 / 2.13 = 0.71</p>&#13;
</li>&#13;
</ul>&#13;
</dd>&#13;
<dt>For algorithm B</dt>&#13;
<dd>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>DCG@5 = 1/log(1 + 1) + 1/log(2 + 1) + 1/log(3 + 1) = 1 + 0.63 + 0.5 = 2.13</p>&#13;
</li>&#13;
<li>&#13;
<p>NDCG@5 = DCG@5 / IDCG@5 = 2.13 / 2.13 = 1</p>&#13;
</li>&#13;
</ul>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>The general formula for NDCG can be represented as</p>&#13;
<ul class="simplelist">&#13;
			<li><math alttext="upper N upper D upper C upper G commercial-at k equals StartFraction upper D upper C upper G commercial-at k Over upper I upper D upper C upper G commercial-at k EndFraction">&#13;
  <mrow>&#13;
    <mi>N</mi>&#13;
    <mi>D</mi>&#13;
    <mi>C</mi>&#13;
    <mi>G</mi>&#13;
    <mo>@</mo>&#13;
    <mi>k</mi>&#13;
    <mo>=</mo>&#13;
    <mfrac><mrow><mi>D</mi><mi>C</mi><mi>G</mi><mo>@</mo><mi>k</mi></mrow> <mrow><mi>I</mi><mi>D</mi><mi>C</mi><mi>G</mi><mo>@</mo><mi>k</mi></mrow></mfrac>&#13;
  </mrow>&#13;
</math>&#13;
			</li>&#13;
			</ul>&#13;
&#13;
<p>where</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><math alttext="upper D upper C upper G commercial-at k equals sigma-summation Underscript i equals 1 Overscript k Endscripts StartFraction r e l Subscript i Baseline Over l o g 2 left-parenthesis i plus 1 right-parenthesis EndFraction">&#13;
  <mrow>&#13;
    <mi>D</mi>&#13;
    <mi>C</mi>&#13;
    <mi>G</mi>&#13;
    <mo>@</mo>&#13;
    <mi>k</mi>&#13;
    <mo>=</mo>&#13;
    <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>k</mi> </msubsup>&#13;
    <mfrac><mrow><mi>r</mi><mi>e</mi><msub><mi>l</mi> <mi>i</mi> </msub></mrow> <mrow><mi>l</mi><mi>o</mi><msub><mi>g</mi> <mn>2</mn> </msub><mrow><mo>(</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></mrow></mfrac>&#13;
  </mrow>&#13;
</math></p>&#13;
</li>&#13;
<li>&#13;
<p><math alttext="upper I upper D upper C upper G commercial-at k equals sigma-summation Underscript i equals 1 Overscript StartAbsoluteValue script upper R EndAbsoluteValue Endscripts StartFraction 1 Over l o g 2 left-parenthesis i plus 1 right-parenthesis EndFraction">&#13;
  <mrow>&#13;
    <mi>I</mi>&#13;
    <mi>D</mi>&#13;
    <mi>C</mi>&#13;
    <mi>G</mi>&#13;
    <mo>@</mo>&#13;
    <mi>k</mi>&#13;
    <mo>=</mo>&#13;
    <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mrow><mo>|</mo><mi>ℛ</mi><mo>|</mo></mrow> </msubsup>&#13;
    <mfrac><mn>1</mn> <mrow><mi>l</mi><mi>o</mi><msub><mi>g</mi> <mn>2</mn> </msub><mrow><mo>(</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></mrow></mfrac>&#13;
  </mrow>&#13;
</math></p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>and <math alttext="script upper R">&#13;
  <mi>ℛ</mi>&#13;
</math> is the set of relevant documents.</p>&#13;
&#13;
<p>This metric gives us a normalized score for how well our recommendation algorithm ranks relevant items, discounting as we move further down the list.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="mAP Versus NDCG?" data-type="sect2"><div class="sect2" id="id218">&#13;
<h2>mAP Versus NDCG?</h2>&#13;
&#13;
<p>Both mAP and NDCG<a data-primary="ranking metrics" data-secondary="mAP versus NDCG" data-type="indexterm" id="id997"/> are holistic metrics that offer a comprehensive perspective of ranking quality by incorporating all relevant items and their respective ranks. However, the interpretability and use cases of these metrics can vary based on the specifics of the recommendation context and the nature of relevance.</p>&#13;
&#13;
<p>While MRR does not consider all relevant items, it does provide an interpretable insight into an algorithm’s performance, highlighting the average rank of the first relevant item. This can be particularly useful when the topmost recommendations hold significant value.</p>&#13;
&#13;
<p>mAP, on the other hand, is a rich evaluation measure that effectively represents the area under the precision-recall curve. Its average aspect confers an intuitive interpretation related to the trade-off between precision and recall across different rank cutoffs.</p>&#13;
&#13;
<p>NDCG introduces a robust consideration of the relevance of each item and is sensitive to the rank order, employing a logarithmic discount factor to quantify the diminishing significance of items as we move down the list. This allows it to handle scenarios in which items can have varying degrees of relevance, extending beyond binary relevance often used in mAP and MRR. However, this versatility of NDCG can also limit its interpretability because of the complexity of the logarithmic discount.</p>&#13;
&#13;
<p>Further, although NDCG is well equipped for use cases where items carry distinct relevance weights, procuring accurate ground-truth relevance scores can pose a significant challenge in practical applications. This imposes a limitation on the real-world usefulness of NDCG.</p>&#13;
&#13;
<p>Cumulatively, these metrics form the backbone of offline evaluation methodologies for recommendation algorithms. As we advance in our exploration, we’ll cover online evaluations, discuss strategies to assess and mitigate algorithmic bias, understand the importance of ensuring diversity in recommendations, and optimize recommendation systems to cater to various stakeholders in the ecosystem.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Correlation Coefficients" data-type="sect2"><div class="sect2" id="id219">&#13;
<h2>Correlation Coefficients</h2>&#13;
&#13;
<p>While<a data-primary="ranking metrics" data-secondary="correlation coefficients" data-type="indexterm" id="id998"/><a data-primary="correlation" data-secondary="correlation coefficients" data-type="indexterm" id="id999"/> correlation coefficients like Pearson’s or Spearman’s can be employed to evaluate the similarity between two rankings (for instance, between the predicted and the ground-truth rankings), they do not provide the exact same information as mAP, MRR, or NDCG.</p>&#13;
&#13;
<p>Correlation coefficients are typically used to measure the degree of linear association between two continuous variables, and in the context of ranking, they can indicate the overall similarity between two ordered lists. However, they do not directly account for aspects such as the relevance of individual items, the position of relevant items, or varying degrees of relevance among items, which are integral to mAP, MRR, and NDCG.</p>&#13;
&#13;
<p>For example, say a user has interacted with five items in the past. A recommender system might predict that the user will interact with these items again but rank them in the opposite order of importance. Even though the system has correctly identified the items of interest, the reversed ranking would lead to poor performance as measured by mAP, MRR, or NDCG, but a high negative correlation coefficient would be obtained because of the linear relationship.</p>&#13;
&#13;
<p>As a result, while correlation coefficients can provide a high-level understanding of ranking performance, they are not sufficient substitutes for the more detailed information provided by metrics like mAP, MRR, and NDCG.</p>&#13;
&#13;
<p>To utilize correlation coefficients in the context of ranking, it would be essential to pair them with other metrics that account for the specific nuances of the recommendation problem, such as the relevance of individual items and their positions in the ranking.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="RMSE from Affinity" data-type="sect1"><div class="sect1" id="id136">&#13;
<h1>RMSE from Affinity</h1>&#13;
&#13;
<p>Root mean square error (RMSE)<a data-primary="ranking metrics" data-secondary="RMSE from affinity" data-type="indexterm" id="id1000"/><a data-primary="root mean square error (RMSE)" data-type="indexterm" id="id1001"/> and ranking metrics like mAP, MRR, and  NDCG offer fundamentally different perspectives when evaluating a recommendation system that outputs affinity scores.</p>&#13;
&#13;
<p>RMSE is a popular metric for quantifying prediction error. It computes the square root of the average of squared differences between the predicted affinity scores and the true values. Lower RMSE signifies better predictive accuracy. However, RMSE treats the problem as a standard regression task and disregards the inherent ranking structure in recommendation systems.</p>&#13;
&#13;
<p>Conversely, mAP, MRR, and NDCG are explicitly designed to evaluate the quality of rankings, which is essential in a recommendation system. In essence, while RMSE measures the closeness of predicted affinity scores to actual values, mAP, MRR, and NDCG assess the ranking quality by considering the positions of relevant items. Therefore, if your main concern is ranking items rather than predicting precise affinity scores, these ranking metrics are generally more appropriate.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Integral Forms: AUC and cAUC" data-type="sect1"><div class="sect1" id="id220">&#13;
<h1>Integral Forms: AUC and cAUC</h1>&#13;
&#13;
<p>When<a data-primary="ranking metrics" data-secondary="AUC and cAUC" data-type="indexterm" id="id1002"/> it comes to recommendation systems, we are producing a ranked list of items for each user. As you’ve seen, these rankings are based on affinity, the probability or level of preference that the user has for each item. Given this framework, several metrics have been developed to evaluate the quality of these ranked lists. One such metric is the AUC-ROC, which is complemented by mAP, MRR, and NDCG. Let’s take a closer look at understanding these.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Recommendation Probabilities to AUC-ROC" data-type="sect2"><div class="sect2" id="id137">&#13;
<h2>Recommendation Probabilities to AUC-ROC</h2>&#13;
&#13;
<p>In<a data-primary="area under the receiver operating characteristic curve (AUC-ROC)" data-type="indexterm" id="id1003"/><a data-primary="AUC-ROC (area under the receiver operating characteristic curve)" data-type="indexterm" id="id1004"/> a binary classification setup, the area <em>under the receiver operating characteristic curve</em> (AUC-ROC) measures the ability of the recommendation model to distinguish between positive (relevant) and negative (irrelevant) instances. It is calculated by plotting the<a data-primary="TPR (true positive rate)" data-type="indexterm" id="id1005"/><a data-primary="true positive rate (TPR)" data-type="indexterm" id="id1006"/> true positive rate (TPR) against the<a data-primary="false positive rate (FPR)" data-type="indexterm" id="id1007"/><a data-primary="FPR (false positive rate)" data-type="indexterm" id="id1008"/> false positive rate (FPR) at various threshold settings and then computing the area under this curve.</p>&#13;
&#13;
<p>In the context of recommendations, you can think of these “thresholds” as varying the number of top items recommended to a user. The AUC-ROC metric becomes an evaluation of how well your model ranks relevant items over irrelevant ones, irrespective of the actual rank position. In other words, AUC-ROC effectively quantifies the likelihood that a randomly chosen relevant item is ranked higher than a randomly chosen irrelevant one by the model. This, however, doesn’t account for the actual position or order of items in the list, only the relative ranking of positive versus negative instances. The affinity of a calibrated item may be interpreted as a confidence measure by the model that an item is relevant, and when considering historical data, even uncalibrated affinity scores may make a great suggestion for the number of recommendations necessary to find something useful.</p>&#13;
&#13;
<p>One serious implementation of these affinity scores might be to show users only items over a particular score and otherwise tell them to come back later or use exploration methods to improve the data. For example, if you sold hygiene products and were considering asking customers to add some Aesop soap during checkout, you may wish to evaluate the Aesop ROC and make this suggestion only when the observed affinity passed the learned threshold. You’ll also see these concepts used later in <a data-type="xref" href="ch14.html#InvHealth">“Inventory Health”</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Comparison to Other Metrics" data-type="sect2"><div class="sect2" id="id138">&#13;
<h2>Comparison to Other Metrics</h2>&#13;
&#13;
<p>Let’s put these in context with the other metrics:</p>&#13;
<dl>&#13;
<dt>mAP</dt>&#13;
<dd>&#13;
<p>This<a data-primary="mean average precision (mAP)" data-type="indexterm" id="id1009"/><a data-primary="mAP (mean average precision)" data-type="indexterm" id="id1010"/> metric expands on the idea of precision at a specific cutoff in the ranked list to provide an overall measure of model performance. It does this by averaging the precision values computed at the ranks where each relevant item is found. Unlike AUC-ROC, mAP puts emphasis on the higher-ranked items and is more sensitive to changes at the top of the ranking.</p>&#13;
</dd>&#13;
<dt>MRR</dt>&#13;
<dd>&#13;
<p>Unlike<a data-primary="mean reciprocal rank (MRR)" data-type="indexterm" id="id1011"/><a data-primary="MRR (mean reciprocal rank)" data-type="indexterm" id="id1012"/> AUC-ROC and mAP, which consider all relevant items in the list, MRR focuses only on the rank of the first relevant item in the list. It is a measure of how quickly the model can find a relevant item. If the model consistently places a relevant item at the top of the list, it will have a higher MRR.</p>&#13;
</dd>&#13;
<dt>NDCG</dt>&#13;
<dd>&#13;
<p>This<a data-primary="normalized discounted cumulative gain (NDCG)" data-type="indexterm" id="id1013"/><a data-primary="NDCG (normalized discounted cumulative gain)" data-type="indexterm" id="id1014"/> metric evaluates the quality of the ranking by not only considering the order of recommendations but also taking into account the graded relevance of items (which the previous metrics don’t). NDCG discounts items further down the list, rewarding relevant items that appear near the top of the list.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>AUC-ROC provides a valuable aggregate measure of a model’s ability to differentiate between relevant and irrelevant items; mAP, MRR, and NDCG offer a more nuanced evaluation of the model’s ranking quality, considering factors like position bias and varying degrees of relevance.</p>&#13;
&#13;
<p>Note that we sometimes compute the AUC per customer and then average. That’s customer AUC (cAUC), which can often provide a good expectation for a user’s experience.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="BPR" data-type="sect1"><div class="sect1" id="id139">&#13;
<h1>BPR</h1>&#13;
&#13;
<p><em>Bayesian personalized ranking</em> (BPR) presents<a data-primary="ranking metrics" data-secondary="Bayesian personalized ranking (BPR)" data-type="indexterm" id="id1015"/><a data-primary="Bayesian personalized ranking (BPR)" data-type="indexterm" id="id1016"/><a data-primary="BPR (Bayesian personalized ranking)" data-type="indexterm" id="id1017"/> a Bayesian approach to the task of item ranking in recommendation systems, effectively providing a probability framework to model the personalized ranking process. Instead of transforming the item recommendation problem into a binary classification problem (relevant or not), BPR focuses on pairwise preferences: given two items, which does the user prefer? This approach aligns better with the nature of implicit feedback that is common in recommendation systems.</p>&#13;
&#13;
<p>The BPR model uses a pairwise loss function that takes into account the relative order of a positive item and a negative item for a specific user. It seeks to maximize the posterior probability of the observed rankings being correct. The model is typically optimized using stochastic gradient descent or a variant thereof. It’s important to note that BPR (unlike other metrics we’ve discussed, including AUC-ROC, mAP, MRR, and NDCG) is a model training objective rather than an evaluation metric. Therefore, while the aforementioned metrics evaluate a model’s performance post-training, BPR provides a mechanism to guide the model learning process in a way that directly optimizes for the ranking task. A much deeper discussion of these topics is in <a href="https://oreil.ly/NwCYa">“BPR: Bayesian Personalized Ranking from Implicit Feedback”</a> by Steffen Rendle et al.<a data-primary="" data-startref="RSrankingm11" data-type="indexterm" id="id1018"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="less_space pagebreak-before" data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id315">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>Now that you know how to evaluate the performance of the recommendation systems that you train, you may be wondering how to actually train them. You may have noticed that many of the metrics we introduced would not make very good loss functions; they involve a lot of simultaneous observations about sets and lists of items. This would unfortunately make the signal that the recommender would be learning from highly combinatorial. Additionally, the metrics we’ve presented really have two aspects to consider: the binary metric associated to recall, and the rank weighting.</p>&#13;
&#13;
<p>In the next chapter, you’re going to learn some loss functions that make excellent training objectives. The importance of these, we’re sure, won’t be lost on you.</p>&#13;
</div></section>&#13;
</div></section></body></html>