["```py\n'''Main'''\nimport numpy as np\nimport pandas as pd\nimport os, time, re\nimport pickle, gzip\n\n'''Data Viz'''\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nimport matplotlib as mpl\n\n%matplotlib inline\n\n'''Data Prep and Model Evaluation'''\nfrom sklearn import preprocessing as pp\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\n\n'''Algos'''\nimport lightgbm as lgb\n\n'''TensorFlow and Keras'''\nimport tensorflow as tf\nimport keras\nfrom keras import backend as K\nfrom keras.models import Sequential, Model\nfrom keras.layers import Activation, Dense, Dropout\nfrom keras.layers import BatchNormalization, Input, Lambda\nfrom keras import regularizers\nfrom keras.losses import mse, binary_crossentropy\n```", "```py\ndata = pd.read_csv('creditcard.csv')\ndataX = data.copy().drop(['Class','Time'],axis=1)\ndataY = data['Class'].copy()\nfeaturesToScale = dataX.columns\nsX = pp.StandardScaler(copy=True, with_mean=True, with_std=True)\ndataX.loc[:,featuresToScale] = sX.fit_transform(dataX[featuresToScale])\n```", "```py\nX_train, X_test, y_train, y_test = \\\n    train_test_split(dataX, dataY, test_size=0.33, \\\n                     random_state=2018, stratify=dataY)\n\nX_train_AE = X_train.copy()\nX_test_AE = X_test.copy()\n```", "```py\ndef anomalyScores(originalDF, reducedDF):\n    loss = np.sum((np.array(originalDF) - \\\n                   np.array(reducedDF))**2, axis=1)\n    loss = pd.Series(data=loss,index=originalDF.index)\n    loss = (loss-np.min(loss))/(np.max(loss)-np.min(loss))\n    return loss\n```", "```py\ndef plotResults(trueLabels, anomalyScores, returnPreds = False):\n    preds = pd.concat([trueLabels, anomalyScores], axis=1)\n    preds.columns = ['trueLabel', 'anomalyScore']\n    precision, recall, thresholds = \\\n        precision_recall_curve(preds['trueLabel'], \\\n                               preds['anomalyScore'])\n    average_precision = average_precision_score( \\\n                        preds['trueLabel'], preds['anomalyScore'])\n\n    plt.step(recall, precision, color='k', alpha=0.7, where='post')\n    plt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.ylim([0.0, 1.05])\n    plt.xlim([0.0, 1.0])\n\n    plt.title('Precision-Recall curve: Average Precision = \\\n {0:0.2f}'.format(average_precision))\n\n    fpr, tpr, thresholds = roc_curve(preds['trueLabel'], \\\n                                     preds['anomalyScore'])\n    areaUnderROC = auc(fpr, tpr)\n\n    plt.figure()\n    plt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\n    plt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic: Area under the \\\n curve = {0:0.2f}'.format(areaUnderROC))\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    if returnPreds==True:\n        return preds\n```", "```py\n# Model one\n# Two layer complete autoencoder with linear activation\n\n# Call neural network API\nmodel = Sequential()\n```", "```py\nmodel.add(Dense(units=29, activation='linear',input_dim=29))\n```", "```py\nmodel.add(Dense(units=29, activation='linear'))\n```", "```py\nmodel.compile(optimizer='adam',\n              loss='mean_squared_error',\n              metrics=['accuracy'])\n```", "```py\nnum_epochs = 10\nbatch_size = 32\n\nhistory = model.fit(x=X_train_AE, y=X_train_AE,\n                    epochs=num_epochs,\n                    batch_size=batch_size,\n                    shuffle=True,\n                    validation_data=(X_train_AE, X_train_AE),\n                    verbose=1)\n```", "```py\nTraining history of complete autoencoder\n\nTrain on 190820 samples, validate on 190820 samples\nEpoch 1/10\n190820/190820 [==============================] - 29s 154us/step - loss: 0.1056\n- acc: 0.8728 - val_loss: 0.0013 - val_acc: 0.9903\nEpoch 2/10\n190820/190820 [==============================] - 27s 140us/step - loss: 0.0012\n- acc: 0.9914 - val_loss: 1.0425e-06 - val_acc: 0.9995\nEpoch 3/10\n190820/190820 [==============================] - 23s 122us/step - loss: 6.6244\ne-04 - acc: 0.9949 - val_loss: 5.2491e-04 - val_acc: 0.9913\nEpoch 4/10\n190820/190820 [==============================] - 23s 119us/step - loss: 0.0016\n- acc: 0.9929 - val_loss: 2.2246e-06 - val_acc: 0.9995\nEpoch 5/10\n190820/190820 [==============================] - 23s 119us/step - loss: 5.7424\ne-04 - acc: 0.9943 - val_loss: 9.0811e-05 - val_acc: 0.9970\nEpoch 6/10\n190820/190820 [==============================] - 22s 118us/step - loss: 5.4950\ne-04 - acc: 0.9941 - val_loss: 6.0598e-05 - val_acc: 0.9959\nEpoch 7/10\n190820/190820 [==============================] - 22s 117us/step - loss: 5.2291\ne-04 - acc: 0.9946 - val_loss: 0.0023 - val_acc: 0.9675\nEpoch 8/10\n190820/190820 [==============================] - 22s 117us/step - loss: 6.5130\ne-04 - acc: 0.9932 - val_loss: 4.5059e-04 - val_acc: 0.9945\nEpoch 9/10\n190820/190820 [==============================] - 23s 122us/step - loss: 4.9077\ne-04 - acc: 0.9952 - val_loss: 7.2591e-04 - val_acc: 0.9908\nEpoch 10/10\n190820/190820 [==============================] - 23s 118us/step - loss: 6.1469\ne-04 - acc: 0.9945 - val_loss: 4.4131e-06 - val_acc: 0.9991\n```", "```py\npredictions = model.predict(X_test, verbose=1)\nanomalyScoresAE = anomalyScores(X_test, predictions)\npreds = plotResults(y_test, anomalyScoresAE, True)\n```", "```py\n# 10 runs - We will capture mean of average precision\ntest_scores = []\nfor i in range(0,10):\n    # Call neural network API\n    model = Sequential()\n\n    # Apply linear activation function to input layer\n    # Generate hidden layer with 29 nodes, the same as the input layer\n    model.add(Dense(units=29, activation='linear',input_dim=29))\n\n    # Apply linear activation function to hidden layer\n    # Generate output layer with 29 nodes\n    model.add(Dense(units=29, activation='linear'))\n\n    # Compile the model\n    model.compile(optimizer='adam',\n                  loss='mean_squared_error',\n                  metrics=['accuracy'])\n\n    # Train the model\n    num_epochs = 10\n    batch_size = 32\n\n    history = model.fit(x=X_train_AE, y=X_train_AE,\n                        epochs=num_epochs,\n                        batch_size=batch_size,\n                        shuffle=True,\n                        validation_data=(X_train_AE, X_train_AE),\n                        verbose=1)\n\n    # Evaluate on test set\n    predictions = model.predict(X_test, verbose=1)\n    anomalyScoresAE = anomalyScores(X_test, predictions)\n    preds, avgPrecision = plotResults(y_test, anomalyScoresAE, True)\n    test_scores.append(avgPrecision)\n\nprint(\"Mean average precision over 10 runs: \", np.mean(test_scores))\ntest_scores\n```", "```py\nMean average precision over 10 runs: 0.30108318944579776\nCoefficient of variation over 10 runs: 0.8755095071789248\n\n[0.25468022666666157,\n0.092705950994909,\n0.716481644928299,\n0.01946589342639965,\n0.25623865457838263,\n0.33597083510378234,\n0.018757053070824415,\n0.6188569405068724,\n0.6720552647581304,\n0.025619070873716072]\n```", "```py\n# 10 runs - We will capture mean of average precision\ntest_scores = []\nfor i in range(0,10):\n    # Call neural network API\n    model = Sequential()\n\n    # Apply linear activation function to input layer\n    # Generate hidden layer with 20 nodes\n    model.add(Dense(units=20, activation='linear',input_dim=29))\n\n    # Apply linear activation function to hidden layer\n    # Generate output layer with 29 nodes\n    model.add(Dense(units=29, activation='linear'))\n\n    # Compile the model\n    model.compile(optimizer='adam',\n                  loss='mean_squared_error',\n                  metrics=['accuracy'])\n\n    # Train the model\n    num_epochs = 10\n    batch_size = 32\n\n    history = model.fit(x=X_train_AE, y=X_train_AE,\n                        epochs=num_epochs,\n                        batch_size=batch_size,\n                        shuffle=True,\n                        validation_data=(X_train_AE, X_train_AE),\n                        verbose=1)\n\n    # Evaluate on test set\n    predictions = model.predict(X_test, verbose=1)\n    anomalyScoresAE = anomalyScores(X_test, predictions)\n    preds, avgPrecision = plotResults(y_test, anomalyScoresAE, True)\n    test_scores.append(avgPrecision)\n\nprint(\"Mean average precision over 10 runs: \", np.mean(test_scores))\ntest_scores\n```", "```py\nTraining history of undercomplete autoencoder with 20 nodes\n\nTrain on 190820 samples, validate on 190820 samples\nEpoch 1/10\n190820/190820 [==============================] - 28s 145us/step - loss: 0.3588\n- acc: 0.5672 - val_loss: 0.2789 - val_acc: 0.6078\nEpoch 2/10\n190820/190820 [==============================] - 29s 153us/step - loss: 0.2817\n- acc: 0.6032 - val_loss: 0.2757 - val_acc: 0.6115\nEpoch 3/10\n190820/190820 [==============================] - 28s 147us/step - loss: 0.2793\n- acc: 0.6147 - val_loss: 0.2755 - val_acc: 0.6176\nEpoch 4/10\n190820/190820 [==============================] - 30s 155us/step - loss: 0.2784\n- acc: 0.6164 - val_loss: 0.2750 - val_acc: 0.6167\nEpoch 5/10\n190820/190820 [==============================] - 29s 152us/step - loss: 0.2786\n- acc: 0.6188 - val_loss: 0.2746 - val_acc: 0.6126\nEpoch 6/10\n190820/190820 [==============================] - 29s 151us/step - loss: 0.2776\n- acc: 0.6140 - val_loss: 0.2752 - val_acc: 0.6043\nEpoch 7/10\n190820/190820 [==============================] - 30s 156us/step - loss: 0.2775\n- acc: 0.5947 - val_loss: 0.2745 - val_acc: 0.5946\nEpoch 8/10\n190820/190820 [==============================] - 29s 149us/step - loss: 0.2770\n- acc: 0.5903 - val_loss: 0.2740 - val_acc: 0.5882\nEpoch 9/10\n190820/190820 [==============================] - 29s 153us/step - loss: 0.2768\n- acc: 0.5921 - val_loss: 0.2770 - val_acc: 0.5801\nEpoch 10/10\n190820/190820 [==============================] - 29s 150us/step - loss: 0.2767\n- acc: 0.5803 - val_loss: 0.2744 - val_acc: 0.5743\n93987/93987[==============================] - 3s 36us/step\n```", "```py\nMean average precision over 10 runs: 0.30913783987972737\nCoefficient of variation over 10 runs: 0.032251659812254876\n\n[0.2886910204920736,\n0.3056142045082387,\n0.31658073591381186,\n0.30590858583039254,\n0.31824197682595556,\n0.3136952374067599,\n0.30888135217515555,\n0.31234000424933206,\n0.29695149753706923,\n0.3244746838584846]\n```", "```py\nTraining history of undercomplete autoencoder with 27 nodes\n\nTrain on 190820 samples, validate on 190820 samples\n\nEpoch 1/10\n190820/190820 [==============================] - 29s 150us/step - loss: 0.1169\n- acc: 0.8224 - val_loss: 0.0368 - val_acc: 0.8798\nEpoch 2/10\n190820/190820 [==============================] - 29s 154us/step - loss: 0.0388\n- acc: 0.8610 - val_loss: 0.0360 - val_acc: 0.8530\nEpoch 3/10\n190820/190820 [==============================] - 30s 156us/step - loss: 0.0382\n- acc: 0.8680 - val_loss: 0.0359 - val_acc: 0.8745\nEpoch 4/10\n190820/190820 [==============================] - 30s 156us/step - loss: 0.0371\n- acc: 0.8811 - val_loss: 0.0353 - val_acc: 0.9021\nEpoch 5/10\n190820/190820 [==============================] - 30s 155us/step - loss: 0.0373\n- acc: 0.9114 - val_loss: 0.0352 - val_acc: 0.9226\nEpoch 6/10\n190820/190820 [==============================] - 30s 155us/step - loss: 0.0377\n- acc: 0.9361 - val_loss: 0.0370 - val_acc: 0.9416\nEpoch 7/10\n190820/190820 [==============================] - 30s 156us/step - loss: 0.0361\n- acc: 0.9448 - val_loss: 0.0358 - val_acc: 0.9378\nEpoch 8/10\n190820/190820 [==============================] - 30s 156us/step - loss: 0.0354\n- acc: 0.9521 - val_loss: 0.0350 - val_acc: 0.9503\nEpoch 9/10\n190820/190820 [==============================] - 29s 153us/step - loss: 0.0352\n- acc: 0.9613 - val_loss: 0.0349 - val_acc: 0.9263\nEpoch 10/10\n190820/190820 [==============================] - 29s 153us/step - loss: 0.0353\n- acc: 0.9566 - val_loss: 0.0343 - val_acc: 0.9477\n93987/93987[==============================] - 4s 39us/step\n```", "```py\nMean average precision over 10 runs: 0.5273341559141779\nCoefficient of variation over 10 runs: 0.5006880691999009\n\n[0.689799495450694,\n0.7092146840717755,\n0.7336692377321005,\n0.6154173765950426,\n0.7068800243349335,\n0.35250757724667586,\n0.6904117414832501,\n0.02335388808244066,\n0.690798140588336,\n0.061289393556529626]\n```", "```py\n# Model two\n# Three layer undercomplete autoencoder with linear activation\n# With 28 and 27 nodes in the two hidden layers, respectively\n\nmodel = Sequential()\nmodel.add(Dense(units=28, activation='linear',input_dim=29))\nmodel.add(Dense(units=27, activation='linear'))\nmodel.add(Dense(units=29, activation='linear'))\n```", "```py\nMean average precision over 10 runs: 0.36075271075596366\nCoefficient of variation over 10 runs: 0.9361649046827353\n\n[0.02259626054852924,\n0.6984699403560997,\n0.011035001202665167,\n0.06621450000830197,\n0.008916986608776182,\n0.705399684020873,\n0.6995233144849828,\n0.008263068338243631,\n0.6904537524978872,\n0.6966545994932775]\n```", "```py\nmodel = Sequential()\nmodel.add(Dense(units=27, activation='relu',input_dim=29))\nmodel.add(Dense(units=22, activation='relu'))\nmodel.add(Dense(units=27, activation='relu'))\nmodel.add(Dense(units=29, activation='relu'))\n```", "```py\nTraining history of undercomplete autoencoder with three hidden layers and ReLu\nactivation function\n\nTrain on 190820 samples, validate on 190820 samples\n\nEpoch 1/10\n190820/190820 [==============================] - 32s 169us/step - loss: 0.7010\n- acc: 0.5626 - val_loss: 0.6339 - val_acc: 0.6983\nEpoch 2/10\n190820/190820 [==============================] - 33s 174us/step - loss: 0.6302\n- acc: 0.7132 - val_loss: 0.6219 - val_acc: 0.7465\nEpoch 3/10\n190820/190820 [==============================] - 34s 177us/step - loss: 0.6224\n- acc: 0.7367 - val_loss: 0.6198 - val_acc: 0.7528\nEpoch 4/10\n190820/190820 [==============================] - 34s 179us/step - loss: 0.6227\n- acc: 0.7380 - val_loss: 0.6205 - val_acc: 0.7471\nEpoch 5/10\n190820/190820 [==============================] - 33s 174us/step - loss: 0.6206\n- acc: 0.7452 - val_loss: 0.6202 - val_acc: 0.7353\nEpoch 6/10\n190820/190820 [==============================] - 33s 175us/step - loss: 0.6206\n- acc: 0.7458 - val_loss: 0.6192 - val_acc: 0.7485\nEpoch 7/10\n190820/190820 [==============================] - 33s 174us/step - loss: 0.6199\n- acc: 0.7481 - val_loss: 0.6239 - val_acc: 0.7308\nEpoch 8/10\n190820/190820 [==============================] - 33s 175us/step - loss: 0.6203\n- acc: 0.7497 - val_loss: 0.6183 - val_acc: 0.7626\nEpoch 9/10\n190820/190820 [==============================] - 34s 177us/step - loss: 0.6197\n- acc: 0.7491 - val_loss: 0.6188 - val_acc: 0.7531\nEpoch 10/10\n190820/190820 [==============================] - 34s 177us/step - loss: 0.6201\n- acc: 0.7486 - val_loss: 0.6188 - val_acc: 0.7540\n93987/93987 [==============================] - 5s 48 us/step\n```", "```py\nMean average precision over 10 runs:    0.2232934196381843\nCoefficient of variation over 10 runs:   0.060779960264380296\n\n[0.22598829389665595,\n0.22616147166925166,\n0.22119489753135715,\n0.2478548473814437,\n0.2251289336369011,\n0.2119454446242229,\n0.2126914064768752,\n0.24581338950742185,\n0.20665608837737512,\n0.20949942328033827]\n```", "```py\nmodel = Sequential()\nmodel.add(Dense(units=40, activation='linear',input_dim=29))\nmodel.add(Dense(units=29, activation='linear'))\n```", "```py\nTraining history of overcomplete autoencoder with single hidden layer and\n linear activation function\n\nTrain on 190820 samples, validate on 190820 samples\nEpoch 1/10\n190820/190820 [==============================] - 31s 161us/step - loss: 0.0498\n- acc: 0.9438 - val_loss: 9.2301e-06 - val_acc: 0.9982\nEpoch 2/10\n190820/190820 [==============================] - 33s 171us/step - loss: 0.0014\n- acc: 0.9925 - val_loss: 0.0019 - val_acc: 0.9909\nEpoch 3/10\n190820/190820 [==============================] - 33s 172us/step - loss: 7.6469\ne-04 - acc: 0.9947 - val_loss: 4.5314e-05 - val_acc: 0.9970\nEpoch 4/10\n190820/190820 [==============================] - 35s 182us/step - loss: 0.0010\n- acc: 0.9930 - val_loss: 0.0039 - val_acc: 0.9859\nEpoch 5/10\n190820/190820 [==============================] - 32s 166us/step - loss: 0.0012\n- acc: 0.9924 - val_loss: 8.5141e-04 - val_acc: 0.9886\nEpoch 6/10\n190820/190820 [==============================] - 31s 163us/step - loss: 5.0655\ne-04 - acc: 0.9955 - val_loss: 8.2359e-04 - val_acc: 0.9910\nEpoch 7/10\n190820/190820 [==============================] - 30s 156us/step - loss: 7.6046\ne-04 - acc: 0.9930 - val_loss: 0.0045 - val_acc: 0.9933\nEpoch 8/10\n190820/190820 [==============================] - 30s 157us/step - loss: 9.1609\ne-04 - acc: 0.9930 - val_loss: 7.3662e-04 - val_acc: 0.9872\nEpoch 9/10\n190820/190820 [==============================] - 30s 158us/step - loss: 7.6287\ne-04 - acc: 0.9929 - val_loss: 2.5671e-04 - val_acc: 0.9940\nEpoch 10/10\n190820/190820 [==============================] - 30s 157us/step - loss: 7.0697\ne-04 - acc: 0.9928 - val_loss: 4.5272e-06 - val_acc: 0.9994\n93987/93987[==============================] - 4s 48us/step\n```", "```py\nMean average precision over 10 runs: 0.3061984081568074\nCoefficient of variation over 10 runs: 0.8896921668864564\n\n[0.03394897465567298,\n0.14322827274920255,\n0.03610123178524601,\n0.019735235731640446,\n0.012571999125881402,\n0.6788921569665146,\n0.5411349583727725,\n0.388474572258503,\n0.7089617645810736,\n0.4989349153415674]\n```", "```py\nmodel = Sequential()\nmodel.add(Dense(units=40, activation='linear', input_dim=29))\nmodel.add(Dropout(0.10))\nmodel.add(Dense(units=29, activation='linear'))\n```", "```py\nTraining history of overcomplete autoencoder with single hidden layer,\ndropout, and linear activation function\n\nTrain on 190820 samples, validate on 190820 samples\nEpoch 1/10\n190820/190820 [==============================] - 27s 141us/step - loss: 0.1358\n- acc: 0.7430 - val_loss: 0.0082 - val_acc: 0.9742\nEpoch 2/10\n190820/190820 [==============================] - 28s 146us/step - loss: 0.0782\n- acc: 0.7849 - val_loss: 0.0094 - val_acc: 0.9689\nEpoch 3/10\n190820/190820 [==============================] - 28s 149us/step - loss: 0.0753\n- acc: 0.7858 - val_loss: 0.0102 - val_acc: 0.9672\nEpoch 4/10\n190820/190820 [==============================] - 28s 148us/step - loss: 0.0772\n- acc: 0.7864 - val_loss: 0.0093 - val_acc: 0.9677\nEpoch 5/10\n190820/190820 [==============================] - 28s 147us/step - loss: 0.0813\n- acc: 0.7843 - val_loss: 0.0108 - val_acc: 0.9631\nEpoch 6/10\n190820/190820 [==============================] - 28s 149us/step - loss: 0.0756\n- acc: 0.7844 - val_loss: 0.0095 - val_acc: 0.9654\nEpoch 7/10\n190820/190820 [==============================] - 29s 150us/step - loss: 0.0743\n- acc: 0.7850 - val_loss: 0.0077 - val_acc: 0.9768\nEpoch 8/10\n190820/190820 [==============================] - 29s 150us/step - loss: 0.0767\n- acc: 0.7840 - val_loss: 0.0070 - val_acc: 0.9759\nEpoch 9/10\n190820/190820 [==============================] - 29s 150us/step - loss: 0.0762\n- acc: 0.7851 - val_loss: 0.0072 - val_acc: 0.9733\nEpoch 10/10\n190820/190820 [==============================] - 29s 151us/step - loss: 0.0756\n- acc: 0.7849 - val_loss: 0.0067 - val_acc: 0.9749\n93987/93987 [==============================] - 3s 32us/step\n```", "```py\nMean average precision over 10 runs: 0.21150415381770646\nCoefficient of variation over 10 runs: 0.40295807771579256\n\n[0.22549974304927337,\n0.22451178120391296,\n0.17243952488912334,\n0.2533716906936315,\n0.13251890273915556,\n0.1775116247503748,\n0.4343283958332979,\n0.10469065867732033,\n0.19480068075466764,\n0.19537213558630712]\n```", "```py\nmodel = Sequential()\n    model.add(Dense(units=40, activation='linear',  \\\n        activity_regularizer=regularizers.l1(10e-5), input_dim=29))\nmodel.add(Dense(units=29, activation='linear'))\n```", "```py\nTraining history of sparse overcomplete autoencoder with single hidden layer\nand linear activation function\n\nTrain on 190820 samples, validate on 190820 samples\nEpoch 1/10\n190820/190820 [==============================] - 27s 142us/step - loss: 0.0985\n- acc: 0.9380 - val_loss: 0.0369 - val_acc: 0.9871\nEpoch 2/10\n190820/190820 [==============================] - 26s 136us/step - loss: 0.0284\n- acc: 0.9829 - val_loss: 0.0261 - val_acc: 0.9698\nEpoch 3/10\n190820/190820 [==============================] - 26s 136us/step - loss: 0.0229\n- acc: 0.9816 - val_loss: 0.0169 - val_acc: 0.9952\nEpoch 4/10\n190820/190820 [==============================] - 26s 137us/step - loss: 0.0201\n- acc: 0.9821 - val_loss: 0.0147 - val_acc: 0.9943\nEpoch 5/10\n190820/190820 [==============================] - 26s 137us/step - loss: 0.0183\n- acc: 0.9810 - val_loss: 0.0142 - val_acc: 0.9842\nEpoch 6/10\n190820/190820 [==============================] - 26s 137us/step - loss: 0.0206\n- acc: 0.9774 - val_loss: 0.0158 - val_acc: 0.9906\nEpoch 7/10\n190820/190820 [==============================] - 26s 136us/step - loss: 0.0169\n- acc: 0.9816 - val_loss: 0.0124 - val_acc: 0.9866\nEpoch 8/10\n190820/190820 [==============================] - 26s 137us/step - loss: 0.0165\n- acc: 0.9795 - val_loss: 0.0208 - val_acc: 0.9537\nEpoch 9/10\n190820/190820 [==============================] - 26s 136us/step - loss: 0.0164\n- acc: 0.9801 - val_loss: 0.0105 - val_acc: 0.9965\nEpoch 10/10\n190820/190820 [==============================] - 27s 140us/step - loss: 0.0167\n- acc: 0.9779 - val_loss: 0.0102 - val_acc: 0.9955\n93987/93987 [==============================] - 3s 32us/step\n```", "```py\nMean average precision over 10 runs: 0.21373659011504448\nCoefficient of variation over 10 runs: 0.9913040763536749\n\n[0.1370972172100049,\n0.28328895710699215,\n0.6362677613798704,\n0.3467265637372019,\n0.5197889253491589,\n0.01871495737323161,\n0.0812609121251577,\n0.034749761900336684,\n0.04846036143317335,\n0.031010483535317393]\n```", "```py\nmodel = Sequential()\n    model.add(Dense(units=40, activation='linear',  \\\n        activity_regularizer=regularizers.l1(10e-5), input_dim=29))\n    model.add(Dropout(0.05))\nmodel.add(Dense(units=29, activation='linear'))\n```", "```py\nTraining history of sparse overcomplete autoencoder with single hidden layer,\ndropout, and linear activation function\n\nTrain on 190820 samples, validate on 190820 samples\nEpoch 1/10\n190820/190820 [==============================] - 31s 162us/step - loss: 0.1477\n- acc: 0.8150 - val_loss: 0.0506 - val_acc: 0.9727\nEpoch 2/10\n190820/190820 [==============================] - 29s 154us/step - loss: 0.0756\n- acc: 0.8625 - val_loss: 0.0344 - val_acc: 0.9788\nEpoch 3/10\n190820/190820 [==============================] - 29s 152us/step - loss: 0.0687\n- acc: 0.8612 - val_loss: 0.0291 - val_acc: 0.9790\nEpoch 4/10\n190820/190820 [==============================] - 29s 154us/step - loss: 0.0644\n- acc: 0.8606 - val_loss: 0.0274 - val_acc: 0.9734\nEpoch 5/10\n190820/190820 [==============================] - 31s 163us/step - loss: 0.0630\n- acc: 0.8597 - val_loss: 0.0242 - val_acc: 0.9746\nEpoch 6/10\n190820/190820 [==============================] - 31s 162us/step - loss: 0.0609\n- acc: 0.8600 - val_loss: 0.0220 - val_acc: 0.9800\nEpoch 7/10\n190820/190820 [==============================] - 30s 156us/step - loss: 0.0624\n- acc: 0.8581 - val_loss: 0.0289 - val_acc: 0.9633\nEpoch 8/10\n190820/190820 [==============================] - 29s 154us/step - loss: 0.0589\n- acc: 0.8588 - val_loss: 0.0574 - val_acc: 0.9366\nEpoch 9/10\n190820/190820 [==============================] - 29s 154us/step - loss: 0.0596\n- acc: 0.8571 - val_loss: 0.0206 - val_acc: 0.9752\nEpoch 10/10\n190820/190820 [==============================] - 31s 165us/step - loss: 0.0593\n- acc: 0.8590 - val_loss: 0.0204 - val_acc: 0.9808\n93987/93987 [==============================] - 4s 38us/step\n```", "```py\nMean average precision over 10 runs: 0.2426994231628755\nCoefifcient of variation over 10 runs: 0.6153219870606188\n\n[0.6078198313533932,\n0.20862366991302814,\n0.25854513247057875,\n0.08496595007072019,\n0.26313491674585093,\n0.17001322998258625,\n0.15338215561753896,\n0.1439107390306835,\n0.4073422280287587,\n0.1292563784156162]\n```", "```py\nnoise_factor = 0.50\nX_train_AE_noisy = X_train_AE.copy() + noise_factor * \\\n np.random.normal(loc=0.0, scale=1.0, size=X_train_AE.shape)\nX_test_AE_noisy = X_test_AE.copy() + noise_factor * \\\n np.random.normal(loc=0.0, scale=1.0, size=X_test_AE.shape)\n```", "```py\nfor i in range(0,10):\n    # Call neural network API\n    model = Sequential()\n\n    # Generate hidden layer with 27 nodes using linear activation\n    model.add(Dense(units=27, activation='linear', input_dim=29))\n\n    # Generate output layer with 29 nodes\n    model.add(Dense(units=29, activation='linear'))\n\n    # Compile the model\n    model.compile(optimizer='adam',\n                  loss='mean_squared_error',\n                  metrics=['accuracy'])\n\n    # Train the model\n    num_epochs = 10\n    batch_size = 32\n\n    history = model.fit(x=X_train_AE_noisy, y=X_train_AE_noisy,\n                        epochs=num_epochs,\n                        batch_size=batch_size,\n                        shuffle=True,\n                        validation_data=(X_train_AE, X_train_AE),\n                        verbose=1)\n\n    # Evaluate on test set\n    predictions = model.predict(X_test_AE_noisy, verbose=1)\n    anomalyScoresAE = anomalyScores(X_test, predictions)\n    preds, avgPrecision = plotResults(y_test, anomalyScoresAE, True)\n    test_scores.append(avgPrecision)\n    model.reset_states()\n\nprint(\"Mean average precision over 10 runs: \", np.mean(test_scores))\ntest_scores\n```", "```py\nTraining history of denoising undercomplete autoencoder with single hidden layer\nand linear activation function\n\nTrain on 190820 samples, validate on 190820 samples\nEpoch 1/10\n190820/190820 [==============================] - 25s 133us/step - loss: 0.1733\n- acc: 0.7756 - val_loss: 0.0356 - val_acc: 0.9123\nEpoch 2/10\n190820/190820 [==============================] - 24s 126us/step - loss: 0.0546\n- acc: 0.8793 - val_loss: 0.0354 - val_acc: 0.8973\nEpoch 3/10\n190820/190820 [==============================] - 24s 126us/step - loss: 0.0531\n- acc: 0.8764 - val_loss: 0.0350 - val_acc: 0.9399\nEpoch 4/10\n190820/190820 [==============================] - 24s 126us/step - loss: 0.0525\n- acc: 0.8879 - val_loss: 0.0342 - val_acc: 0.9573\nEpoch 5/10\n190820/190820 [==============================] - 24s 126us/step - loss: 0.0530\n- acc: 0.8910 - val_loss: 0.0347 - val_acc: 0.9503\nEpoch 6/10\n190820/190820 [==============================] - 24s 126us/step - loss: 0.0524\n- acc: 0.8889 - val_loss: 0.0350 - val_acc: 0.9138\nEpoch 7/10\n190820/190820 [==============================] - 24s 126us/step - loss: 0.0531\n- acc: 0.8845 - val_loss: 0.0343 - val_acc: 0.9280\nEpoch 8/10\n190820/190820 [==============================] - 24s 126us/step - loss: 0.0530\n- acc: 0.8798 - val_loss: 0.0339 - val_acc: 0.9507\nEpoch 9/10\n190820/190820 [==============================] - 24s 126us/step - loss: 0.0526\n- acc: 0.8877 - val_loss: 0.0337 - val_acc: 0.9611\nEpoch 10/10\n190820/190820 [==============================] - 24s 127us/step - loss: 0.0528\n- acc: 0.8885 - val_loss: 0.0352 - val_acc: 0.9474\n93987/93987 [==============================] - 3s 34us/step\n```", "```py\nMean average precision over 10 runs: 0.2825997155005206\nCoeficient of variation over 10 runs: 1.1765416185187383\n\n[0.6929639885685303,\n0.008450118408150287,\n0.6970753417267612,\n0.011820311633718597,\n0.008924124892696377,\n0.010639537507746342,\n0.6884911855668772,\n0.006549332886020607,\n0.6805304226634528,\n0.02055279115125298]\n```", "```py\nmodel = Sequential()\nmodel.add(Dense(units=40, activation='linear',\n activity_regularizer=regularizers.l1(10e-5),\n                input_dim=29))\nmodel.add(Dropout(0.05))\nmodel.add(Dense(units=29, activation='linear'))\n```", "```py\nTraining history of denoising overcomplete autoencoder with dropout and linear\nactivation function\n\nTrain on 190820 samples, validate on 190820 samples\nEpoch 1/10\n190820/190820 [==============================] - 28s 145us/step - loss: 0.1726\n- acc: 0.8035 - val_loss: 0.0432 - val_acc: 0.9781\nEpoch 2/10\n190820/190820 [==============================] - 26s 138us/step - loss: 0.0868\n- acc: 0.8490 - val_loss: 0.0307 - val_acc: 0.9775\nEpoch 3/10\n190820/190820 [==============================] - 26s 138us/step - loss: 0.0809\n- acc: 0.8455 - val_loss: 0.0445 - val_acc: 0.9535\nEpoch 4/10\n190820/190820 [==============================] - 26s 138us/step - loss: 0.0777\n- acc: 0.8438 - val_loss: 0.0257 - val_acc: 0.9709\nEpoch 5/10\n190820/190820 [==============================] - 27s 139us/step - loss: 0.0748\n- acc: 0.8434 - val_loss: 0.0219 - val_acc: 0.9787\nEpoch 6/10\n190820/190820 [==============================] - 26s 138us/step - loss: 0.0746\n- acc: 0.8425 - val_loss: 0.0210 - val_acc: 0.9794\nEpoch 7/10\n190820/190820 [==============================] - 26s 138us/step - loss: 0.0713\n- acc: 0.8437 - val_loss: 0.0294 - val_acc: 0.9503\nEpoch 8/10\n190820/190820 [==============================] - 26s 138us/step - loss: 0.0708\n- acc: 0.8426 - val_loss: 0.0276 - val_acc: 0.9606\nEpoch 9/10\n190820/190820 [==============================] - 26s 139us/step - loss: 0.0704\n- acc: 0.8428 - val_loss: 0.0180 - val_acc: 0.9811\nEpoch 10/10\n190820/190820 [==============================] - 27s 139us/step - loss: 0.0702\n- acc: 0.8424 - val_loss: 0.0185 - val_acc: 0.9710\n93987/93987 [==============================] - 4s 38us/step\n```", "```py\nMean average precision over 10 runs: 0.10112931070692295\nCoefficient of variation over 10 runs: 0.8343774832756188\n\n[0.08283546387140524,\n0.043070120657586454,\n0.018901753737287603,\n0.02381040174486509,\n0.16038446580196433,\n0.03461061251209459,\n0.17847771715513427,\n0.2483282420447288,\n0.012981344347664117,\n0.20789298519649893]\n```", "```py\nmodel = Sequential()\n    model.add(Dense(units=40, activation='relu',  \\\n        activity_regularizer=regularizers.l1(10e-5), input_dim=29))\n    model.add(Dropout(0.05))\nmodel.add(Dense(units=29, activation='relu'))\n```", "```py\nTraining history of denoising overcomplete autoencoder with dropout and ReLU\nactivation function\"\n\nTrain on 190820 samples, validate on 190820 samples\nEpoch 1/10\n190820/190820 [==============================] - 29s 153us/step - loss: 0.3049\n- acc: 0.6454 - val_loss: 0.0841 - val_acc: 0.8873\nEpoch 2/10\n190820/190820 [==============================] - 27s 143us/step - loss: 0.1806\n- acc: 0.7193 - val_loss: 0.0606 - val_acc: 0.9012\nEpoch 3/10\n190820/190820 [==============================] - 27s 143us/step - loss: 0.1626\n- acc: 0.7255 - val_loss: 0.0500 - val_acc: 0.9045\nEpoch 4/10\n190820/190820 [==============================] - 27s 143us/step - loss: 0.1567\n- acc: 0.7294 - val_loss: 0.0445 - val_acc: 0.9116\nEpoch 5/10\n190820/190820 [==============================] - 27s 143us/step - loss: 0.1484\n- acc: 0.7309 - val_loss: 0.0433 - val_acc: 0.9136\nEpoch 6/10\n190820/190820 [==============================] - 27s 144us/step - loss: 0.1467\n- acc: 0.7311 - val_loss: 0.0375 - val_acc: 0.9101\nEpoch 7/10\n190820/190820 [==============================] - 27s 143us/step - loss: 0.1427\n- acc: 0.7335 - val_loss: 0.0384 - val_acc: 0.9013\nEpoch 8/10\n190820/190820 [==============================] - 27s 143us/step - loss: 0.1397\n- acc: 0.7307 - val_loss: 0.0337 - val_acc: 0.9145\nEpoch 9/10\n190820/190820 [==============================] - 27s 143us/step - loss: 0.1361\n- acc: 0.7322 - val_loss: 0.0343 - val_acc: 0.9066\nEpoch 10/10\n190820/190820 [==============================] - 27s 144us/step - loss: 0.1349\n- acc: 0.7331 - val_loss: 0.0325 - val_acc: 0.9107\n93987/93987 [==============================] - 4s 41us/step\n```", "```py\nMean average precision over 10 runs: 0.1969608394689088\nCoefficient of variation over 10 runs: 0.5566706365802669\n\n[0.22960316854089222,\n0.37609633487223315,\n0.11429775486529765,\n0.10208135698072755,\n0.4002384343852861,\n0.13317480663248088,\n0.15764518571284625,\n0.2406315655171392,\n0.05080529996343734,\n0.1650344872187474]\n```"]