["```py\n# Import libraries\nimport torch\n\n# Create a torch tensor that requires gradients\nt = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n\n# Perform a tensor operation simulating \"forward propagation\"\ntensor_sum = t.sum()\n\n# Perform back propagation\ntensor_sum.backward()\n\n# View the gradients\nt.grad\n```", "```py\ntensor([1., 1., 1.])\n```", "```py\nimport torch\n\ntensor = torch.tensor([1.0,2.0,3.0], requires_grad=True)\ntensor.numpy()\n```", "```py\nRuntimeError: Can't call numpy() on Tensor that requires grad. Use\n    tensor.detach().numpy() instead.\n```", "```py\n# Load libraries\nfrom sklearn import preprocessing\nimport numpy as np\n\n# Create feature\nfeatures = np.array([[-100.1, 3240.1],\n                     [-200.2, -234.1],\n                     [5000.5, 150.1],\n                     [6000.6, -125.1],\n                     [9000.9, -673.1]])\n\n# Create scaler\nscaler = preprocessing.StandardScaler()\n\n# Convert to a tensor\nfeatures_standardized_tensor = torch.from_numpy(features)\n\n# Show features\nfeatures_standardized_tensor\n```", "```py\ntensor([[-100.1000, 3240.1000],\n        [-200.2000, -234.1000],\n        [5000.5000,  150.1000],\n        [6000.6000, -125.1000],\n        [9000.9000, -673.1000]], dtype=torch.float64)\n```", "```py\n# Load library\nimport torch\n\n# Create features\ntorch_features = torch.tensor([[-100.1, 3240.1],\n                               [-200.2, -234.1],\n                               [5000.5, 150.1],\n                               [6000.6, -125.1],\n                               [9000.9, -673.1]], requires_grad=True)\n\n# Compute the mean and standard deviation\nmean = torch_features.mean(0, keepdim=True)\nstandard_deviation = torch_features.std(0, unbiased=False, keepdim=True)\n\n# Standardize the features using the mean and standard deviation\ntorch_features_standardized = torch_features - mean\ntorch_features_standardized /= standard_deviation\n\n# Show standardized features\ntorch_features_standardized\n```", "```py\ntensor([[-1.1254,  1.9643],\n        [-1.1533, -0.5007],\n        [ 0.2953, -0.2281],\n        [ 0.5739, -0.4234],\n        [ 1.4096, -0.8122]], grad_fn=<DivBackward0>)\n```", "```py\n# Import libraries\nimport torch\nimport torch.nn as nn\n\n# Define a neural network\nclass SimpleNeuralNet(nn.Module):\n    def __init__(self):\n        super(SimpleNeuralNet, self).__init__()\n        self.fc1 = nn.Linear(10, 16)\n        self.fc2 = nn.Linear(16, 16)\n        self.fc3 = nn.Linear(16, 1)\n\n    def forward(self, x):\n        x = nn.functional.relu(self.fc1(x))\n        x = nn.functional.relu(self.fc2(x))\n        x = nn.functional.sigmoid(self.fc3(x))\n        return x\n\n# Initialize the neural network\nnetwork = SimpleNeuralNet()\n\n# Define loss function, optimizer\nloss_criterion = nn.BCELoss()\noptimizer = torch.optim.RMSprop(network.parameters())\n\n# Show the network\nnetwork\n```", "```py\nSimpleNeuralNet(\n  (fc1): Linear(in_features=10, out_features=16, bias=True)\n  (fc2): Linear(in_features=16, out_features=16, bias=True)\n  (fc3): Linear(in_features=16, out_features=1, bias=True)\n)\n```", "```py\n# Import libraries\nimport torch\n\n# Define a neural network using `Sequential`\nclass SimpleNeuralNet(nn.Module):\n    def __init__(self):\n        super(SimpleNeuralNet, self).__init__()\n        self.sequential = torch.nn.Sequential(\n            torch.nn.Linear(10, 16),\n            torch.nn.ReLU(),\n            torch.nn.Linear(16,16),\n            torch.nn.ReLU(),\n            torch.nn.Linear(16, 1),\n            torch.nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.sequential(x)\n        return x\n\n# Instantiate and view the network\nSimpleNeuralNet()\n```", "```py\nSimpleNeuralNet(\n  (sequential): Sequential(\n    (0): Linear(in_features=10, out_features=16, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=16, out_features=16, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=16, out_features=1, bias=True)\n    (5): Sigmoid()\n  )\n)\n```", "```py\n# Import libraries\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import RMSprop\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\n# Create training and test sets\nfeatures, target = make_classification(n_classes=2, n_features=10,\n    n_samples=1000)\nfeatures_train, features_test, target_train, target_test = train_test_split(\n    features, target, test_size=0.1, random_state=1)\n\n# Set random seed\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n# Convert data to PyTorch tensors\nx_train = torch.from_numpy(features_train).float()\ny_train = torch.from_numpy(target_train).float().view(-1, 1)\nx_test = torch.from_numpy(features_test).float()\ny_test = torch.from_numpy(target_test).float().view(-1, 1)\n\n# Define a neural network using `Sequential`\nclass SimpleNeuralNet(nn.Module):\n    def __init__(self):\n        super(SimpleNeuralNet, self).__init__()\n        self.sequential = torch.nn.Sequential(\n            torch.nn.Linear(10, 16),\n            torch.nn.ReLU(),\n            torch.nn.Linear(16,16),\n            torch.nn.ReLU(),\n            torch.nn.Linear(16, 1),\n            torch.nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.sequential(x)\n        return x\n\n# Initialize neural network\nnetwork = SimpleNeuralNet()\n\n# Define loss function, optimizer\ncriterion = nn.BCELoss()\noptimizer = RMSprop(network.parameters())\n\n# Define data loader\ntrain_data = TensorDataset(x_train, y_train)\ntrain_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n\n# Compile the model using torch 2.0's optimizer\nnetwork = torch.compile(network)\n\n# Train neural network\nepochs = 3\nfor epoch in range(epochs):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        optimizer.zero_grad()\n        output = network(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n    print(\"Epoch:\", epoch+1, \"\\tLoss:\", loss.item())\n\n# Evaluate neural network\nwith torch.no_grad():\n    output = network(x_test)\n    test_loss = criterion(output, y_test)\n    test_accuracy = (output.round() == y_test).float().mean()\n    print(\"Test Loss:\", test_loss.item(), \"\\tTest Accuracy:\",\n        test_accuracy.item())\n```", "```py\nEpoch: 1 \tLoss: 0.19006995856761932\nEpoch: 2 \tLoss: 0.14092367887496948\nEpoch: 3 \tLoss: 0.03935524448752403\nTest Loss: 0.06877756118774414 \tTest Accuracy: 0.9700000286102295\n```", "```py\n# Import libraries\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import RMSprop\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nN_CLASSES=3\nEPOCHS=3\n\n# Create training and test sets\nfeatures, target = make_classification(n_classes=N_CLASSES, n_informative=9,\n    n_redundant=0, n_features=10, n_samples=1000)\nfeatures_train, features_test, target_train, target_test = train_test_split(\n    features, target, test_size=0.1, random_state=1)\n\n# Set random seed\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n# Convert data to PyTorch tensors\nx_train = torch.from_numpy(features_train).float()\ny_train = torch.nn.functional.one_hot(torch.from_numpy(target_train).long(),\n    num_classes=N_CLASSES).float()\nx_test = torch.from_numpy(features_test).float()\ny_test = torch.nn.functional.one_hot(torch.from_numpy(target_test).long(),\n    num_classes=N_CLASSES).float()\n\n# Define a neural network using `Sequential`\nclass SimpleNeuralNet(nn.Module):\n    def __init__(self):\n        super(SimpleNeuralNet, self).__init__()\n        self.sequential = torch.nn.Sequential(\n            torch.nn.Linear(10, 16),\n            torch.nn.ReLU(),\n            torch.nn.Linear(16,16),\n            torch.nn.ReLU(),\n            torch.nn.Linear(16,3),\n            torch.nn.Softmax()\n        )\n\n    def forward(self, x):\n        x = self.sequential(x)\n        return x\n\n# Initialize neural network\nnetwork = SimpleNeuralNet()\n\n# Define loss function, optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = RMSprop(network.parameters())\n\n# Define data loader\ntrain_data = TensorDataset(x_train, y_train)\ntrain_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n\n# Compile the model using torch 2.0's optimizer\nnetwork = torch.compile(network)\n\n# Train neural network\nfor epoch in range(EPOCHS):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        optimizer.zero_grad()\n        output = network(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n    print(\"Epoch:\", epoch+1, \"\\tLoss:\", loss.item())\n\n# Evaluate neural network\nwith torch.no_grad():\n    output = network(x_test)\n    test_loss = criterion(output, y_test)\n    test_accuracy = (output.round() == y_test).float().mean()\n    print(\"Test Loss:\", test_loss.item(), \"\\tTest Accuracy:\",\n        test_accuracy.item())\n```", "```py\nEpoch: 1 \tLoss: 0.8022041916847229\nEpoch: 2 \tLoss: 0.775616466999054\nEpoch: 3 \tLoss: 0.7751263380050659\nTest Loss: 0.8105319142341614 \tTest Accuracy: 0.8199999928474426\n```", "```py\n# View target matrix\ny_train\n```", "```py\ntensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [1., 0., 0.],\n        ...,\n        [0., 1., 0.],\n        [1., 0., 0.],\n        [0., 0., 1.]])\n```", "```py\n# Import libraries\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import RMSprop\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\n\nEPOCHS=5\n\n# Create training and test sets\nfeatures, target = make_regression(n_features=10, n_samples=1000)\nfeatures_train, features_test, target_train, target_test = train_test_split(\n    features, target, test_size=0.1, random_state=1)\n\n# Set random seed\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n# Convert data to PyTorch tensors\nx_train = torch.from_numpy(features_train).float()\ny_train = torch.from_numpy(target_train).float().view(-1,1)\nx_test = torch.from_numpy(features_test).float()\ny_test = torch.from_numpy(target_test).float().view(-1,1)\n\n# Define a neural network using `Sequential`\nclass SimpleNeuralNet(nn.Module):\n    def __init__(self):\n        super(SimpleNeuralNet, self).__init__()\n        self.sequential = torch.nn.Sequential(\n            torch.nn.Linear(10, 16),\n            torch.nn.ReLU(),\n            torch.nn.Linear(16,16),\n            torch.nn.ReLU(),\n            torch.nn.Linear(16,1),\n        )\n\n    def forward(self, x):\n        x = self.sequential(x)\n        return x\n\n# Initialize neural network\nnetwork = SimpleNeuralNet()\n\n# Define loss function, optimizer\ncriterion = nn.MSELoss()\noptimizer = RMSprop(network.parameters())\n\n# Define data loader\ntrain_data = TensorDataset(x_train, y_train)\ntrain_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n\n# Compile the model using torch 2.0's optimizer\nnetwork = torch.compile(network)\n\n# Train neural network\nfor epoch in range(EPOCHS):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        optimizer.zero_grad()\n        output = network(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n    print(\"Epoch:\", epoch+1, \"\\tLoss:\", loss.item())\n\n# Evaluate neural network\nwith torch.no_grad():\n    output = network(x_test)\n    test_loss = float(criterion(output, y_test))\n    print(\"Test MSE:\", test_loss)\n```", "```py\nEpoch: 1 \tLoss: 10764.02734375\nEpoch: 2 \tLoss: 1356.510009765625\nEpoch: 3 \tLoss: 504.9664306640625\nEpoch: 4 \tLoss: 199.11314392089844\nEpoch: 5 \tLoss: 191.20834350585938\nTest MSE: 162.24497985839844\n```", "```py\n# Import libraries\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import RMSprop\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\n# Create training and test sets\nfeatures, target = make_classification(n_classes=2, n_features=10,\n    n_samples=1000)\nfeatures_train, features_test, target_train, target_test = train_test_split(\n    features, target, test_size=0.1, random_state=1)\n\n# Set random seed\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n# Convert data to PyTorch tensors\nx_train = torch.from_numpy(features_train).float()\ny_train = torch.from_numpy(target_train).float().view(-1, 1)\nx_test = torch.from_numpy(features_test).float()\ny_test = torch.from_numpy(target_test).float().view(-1, 1)\n\n# Define a neural network using `Sequential`\nclass SimpleNeuralNet(nn.Module):\n    def __init__(self):\n        super(SimpleNeuralNet, self).__init__()\n        self.sequential = torch.nn.Sequential(\n            torch.nn.Linear(10, 16),\n            torch.nn.ReLU(),\n            torch.nn.Linear(16,16),\n            torch.nn.ReLU(),\n            torch.nn.Linear(16, 1),\n            torch.nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.sequential(x)\n        return x\n\n# Initialize neural network\nnetwork = SimpleNeuralNet()\n\n# Define loss function, optimizer\ncriterion = nn.BCELoss()\noptimizer = RMSprop(network.parameters())\n\n# Define data loader\ntrain_data = TensorDataset(x_train, y_train)\ntrain_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n\n# Compile the model using torch 2.0's optimizer\nnetwork = torch.compile(network)\n\n# Train neural network\nepochs = 3\nfor epoch in range(epochs):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        optimizer.zero_grad()\n        output = network(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n    print(\"Epoch:\", epoch+1, \"\\tLoss:\", loss.item())\n\n# Evaluate neural network\nwith torch.no_grad():\n    predicted_class = network.forward(x_train).round()\n\npredicted_class[0]\n```", "```py\nEpoch: 1 \tLoss: 0.19006995856761932\nEpoch: 2 \tLoss: 0.14092367887496948\nEpoch: 3 \tLoss: 0.03935524448752403\ntensor([1.])\n```", "```py\n# Load libraries\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import RMSprop\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create training and test sets\nfeatures, target = make_classification(n_classes=2, n_features=10,\n    n_samples=1000)\nfeatures_train, features_test, target_train, target_test = train_test_split(\n    features, target, test_size=0.1, random_state=1)\n\n# Set random seed\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n# Convert data to PyTorch tensors\nx_train = torch.from_numpy(features_train).float()\ny_train = torch.from_numpy(target_train).float().view(-1, 1)\nx_test = torch.from_numpy(features_test).float()\ny_test = torch.from_numpy(target_test).float().view(-1, 1)\n\n# Define a neural network using `Sequential`\nclass SimpleNeuralNet(nn.Module):\n    def __init__(self):\n        super(SimpleNeuralNet, self).__init__()\n        self.sequential = torch.nn.Sequential(\n            torch.nn.Linear(10, 16),\n            torch.nn.ReLU(),\n            torch.nn.Linear(16,16),\n            torch.nn.ReLU(),\n            torch.nn.Linear(16, 1),\n            torch.nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.sequential(x)\n        return x\n\n# Initialize neural network\nnetwork = SimpleNeuralNet()\n\n# Define loss function, optimizer\ncriterion = nn.BCELoss()\noptimizer = RMSprop(network.parameters())\n\n# Define data loader\ntrain_data = TensorDataset(x_train, y_train)\ntrain_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n\n# Compile the model using torch 2.0's optimizer\nnetwork = torch.compile(network)\n\n# Train neural network\nepochs = 8\ntrain_losses = []\ntest_losses = []\nfor epoch in range(epochs):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        optimizer.zero_grad()\n        output = network(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n    with torch.no_grad():\n        train_output = network(x_train)\n        train_loss = criterion(output, target)\n        train_losses.append(train_loss.item())\n\n        test_output = network(x_test)\n        test_loss = criterion(test_output, y_test)\n        test_losses.append(test_loss.item())\n\n# Visualize loss history\nepochs = range(0, epochs)\nplt.plot(epochs, train_losses, \"r--\")\nplt.plot(epochs, test_losses, \"b-\")\nplt.legend([\"Training Loss\", \"Test Loss\"])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.show();\n```", "```py\n# Import libraries\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import RMSprop\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\n# Create training and test sets\nfeatures, target = make_classification(n_classes=2, n_features=10,\n    n_samples=1000)\nfeatures_train, features_test, target_train, target_test = train_test_split(\n    features, target, test_size=0.1, random_state=1)\n\n# Set random seed\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n# Convert data to PyTorch tensors\nx_train = torch.from_numpy(features_train).float()\ny_train = torch.from_numpy(target_train).float().view(-1, 1)\nx_test = torch.from_numpy(features_test).float()\ny_test = torch.from_numpy(target_test).float().view(-1, 1)\n\n# Define a neural network using `Sequential`\nclass SimpleNeuralNet(nn.Module):\n    def __init__(self):\n        super(SimpleNeuralNet, self).__init__()\n        self.sequential = torch.nn.Sequential(\n            torch.nn.Linear(10, 16),\n            torch.nn.ReLU(),\n            torch.nn.Linear(16,16),\n            torch.nn.ReLU(),\n            torch.nn.Linear(16, 1),\n            torch.nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.sequential(x)\n        return x\n\n# Initialize neural network\nnetwork = SimpleNeuralNet()\n\n# Define loss function, optimizer\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(network.parameters(), lr=1e-4, weight_decay=1e-5)\n\n# Define data loader\ntrain_data = TensorDataset(x_train, y_train)\ntrain_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n\n# Compile the model using torch 2.0's optimizer\nnetwork = torch.compile(network)\n\n# Train neural network\nepochs = 100\nfor epoch in range(epochs):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        optimizer.zero_grad()\n        output = network(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n# Evaluate neural network\nwith torch.no_grad():\n    output = network(x_test)\n    test_loss = criterion(output, y_test)\n    test_accuracy = (output.round() == y_test).float().mean()\n    print(\"Test Loss:\", test_loss.item(), \"\\tTest Accuracy:\",\n        test_accuracy.item())\n```", "```py\nTest Loss: 0.4030887186527252 \tTest Accuracy: 0.9599999785423279\n```", "```py\n# Import libraries\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import RMSprop\nimport lightning as pl\nfrom lightning.pytorch.callbacks.early_stopping import EarlyStopping\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\n# Create training and test sets\nfeatures, target = make_classification(n_classes=2, n_features=10,\n    n_samples=1000)\nfeatures_train, features_test, target_train, target_test = train_test_split(\n    features, target, test_size=0.1, random_state=1)\n\n# Set random seed\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n# Convert data to PyTorch tensors\nx_train = torch.from_numpy(features_train).float()\ny_train = torch.from_numpy(target_train).float().view(-1, 1)\nx_test = torch.from_numpy(features_test).float()\ny_test = torch.from_numpy(target_test).float().view(-1, 1)\n\n# Define a neural network using `Sequential`\nclass SimpleNeuralNet(nn.Module):\n    def __init__(self):\n        super(SimpleNeuralNet, self).__init__()\n        self.sequential = torch.nn.Sequential(\n            torch.nn.Linear(10, 16),\n            torch.nn.ReLU(),\n            torch.nn.Linear(16,16),\n            torch.nn.ReLU(),\n            torch.nn.Linear(16, 1),\n            torch.nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.sequential(x)\n        return x\n\nclass LightningNetwork(pl.LightningModule):\n    def __init__(self, network):\n        super().__init__()\n        self.network = network\n        self.criterion = nn.BCELoss()\n        self.metric = nn.functional.binary_cross_entropy\n\n    def training_step(self, batch, batch_idx):\n        # training_step defines the train loop.\n        data, target = batch\n        output = self.network(data)\n        loss = self.criterion(output, target)\n        self.log(\"val_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=1e-3)\n\n# Define data loader\ntrain_data = TensorDataset(x_train, y_train)\ntrain_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n\n# Initialize neural network\nnetwork = LightningNetwork(SimpleNeuralNet())\n\n# Train network\ntrainer = pl.Trainer(callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\",\n    patience=3)], max_epochs=1000)\ntrainer.fit(model=network, train_dataloaders=train_loader)\n```", "```py\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n  | Name      | Type            | Params\n----------------------------------------------\n0 | network   | SimpleNeuralNet | 465\n1 | criterion | BCELoss         | 0\n----------------------------------------------\n465       Trainable params\n0         Non-trainable params\n465       Total params\n0.002     Total estimated model params size (MB)\n/usr/local/lib/python3.10/site-packages/lightning/pytorch/trainer/\n    connectors/data_connector.py:224: PossibleUserWarning:\n    The dataloader, train_dataloader, does not have many workers which\n    may be a bottleneck. Consider increasing the value of the `num_workers`\n    argument (try 7 which is the number of cpus on this machine)\n    in the `DataLoader` init to improve performance.\n  rank_zero_warn(\n/usr/local/lib/python3.10/site-packages/lightning/pytorch/trainer/\n    trainer.py:1609: PossibleUserWarning: The number of training batches (9)\n    is smaller than the logging interval Trainer(log_every_n_steps=50).\n    Set a lower value for log_every_n_steps if you want to see logs\n    for the training epoch.\n  rank_zero_warn(\nEpoch 23: 100%|███████████████| 9/9 [00:00<00:00, 59.29it/s, loss=0.147, v_num=5]\n```", "```py\n# Train network\ntrainer = pl.Trainer(max_epochs=1000)\ntrainer.fit(model=network, train_dataloaders=train_loader)\n```", "```py\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n  | Name      | Type            | Params\n----------------------------------------------\n0 | network   | SimpleNeuralNet | 465\n1 | criterion | BCELoss         | 0\n----------------------------------------------\n465       Trainable params\n0         Non-trainable params\n465       Total params\n0.002     Total estimated model params size (MB)\nEpoch 999: 100%|████████████| 9/9 [00:01<00:00,  7.95it/s, loss=0.00188, v_num=6]\n`Trainer.fit` stopped: `max_epochs=1000` reached.\nEpoch 999: 100%|████████████| 9/9 [00:01<00:00,  7.80it/s, loss=0.00188, v_num=6]\n```", "```py\n# Load libraries\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import RMSprop\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\n# Create training and test sets\nfeatures, target = make_classification(n_classes=2, n_features=10,\n    n_samples=1000)\nfeatures_train, features_test, target_train, target_test = train_test_split(\n    features, target, test_size=0.1, random_state=1)\n\n# Set random seed\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n# Convert data to PyTorch tensors\nx_train = torch.from_numpy(features_train).float()\ny_train = torch.from_numpy(target_train).float().view(-1, 1)\nx_test = torch.from_numpy(features_test).float()\ny_test = torch.from_numpy(target_test).float().view(-1, 1)\n\n# Define a neural network using `Sequential`\nclass SimpleNeuralNet(nn.Module):\n    def __init__(self):\n        super(SimpleNeuralNet, self).__init__()\n        self.sequential = torch.nn.Sequential(\n            torch.nn.Linear(10, 16),\n            torch.nn.ReLU(),\n            torch.nn.Linear(16,16),\n            torch.nn.ReLU(),\n            torch.nn.Linear(16, 1),\n            torch.nn.Dropout(0.1), # Drop 10% of neurons\n            torch.nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        x = self.sequential(x)\n        return x\n\n# Initialize neural network\nnetwork = SimpleNeuralNet()\n\n# Define loss function, optimizer\ncriterion = nn.BCELoss()\noptimizer = RMSprop(network.parameters())\n\n# Define data loader\ntrain_data = TensorDataset(x_train, y_train)\ntrain_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n\n# Compile the model using torch 2.0's optimizer\nnetwork = torch.compile(network)\n\n# Train neural network\nepochs = 3\nfor epoch in range(epochs):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        optimizer.zero_grad()\n        output = network(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n    print(\"Epoch:\", epoch+1, \"\\tLoss:\", loss.item())\n\n# Evaluate neural network\nwith torch.no_grad():\n    output = network(x_test)\n    test_loss = criterion(output, y_test)\n    test_accuracy = (output.round() == y_test).float().mean()\n    print(\"Test Loss:\", test_loss.item(), \"\\tTest Accuracy:\",\n        test_accuracy.item())\n```", "```py\nEpoch: 1 \tLoss: 0.18791493773460388\nEpoch: 2 \tLoss: 0.17331615090370178\nEpoch: 3 \tLoss: 0.1384529024362564\nTest Loss: 0.12702330946922302 \tTest Accuracy: 0.9100000262260437\n```", "```py\n# Load libraries\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import RMSprop\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\n# Create training and test sets\nfeatures, target = make_classification(n_classes=2, n_features=10,\n    n_samples=1000)\nfeatures_train, features_test, target_train, target_test = train_test_split(\n    features, target, test_size=0.1, random_state=1)\n\n# Set random seed\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n# Convert data to PyTorch tensors\nx_train = torch.from_numpy(features_train).float()\ny_train = torch.from_numpy(target_train).float().view(-1, 1)\nx_test = torch.from_numpy(features_test).float()\ny_test = torch.from_numpy(target_test).float().view(-1, 1)\n\n# Define a neural network using `Sequential`\nclass SimpleNeuralNet(nn.Module):\n    def __init__(self):\n        super(SimpleNeuralNet, self).__init__()\n        self.sequential = torch.nn.Sequential(\n            torch.nn.Linear(10, 16),\n            torch.nn.ReLU(),\n            torch.nn.Linear(16,16),\n            torch.nn.ReLU(),\n            torch.nn.Linear(16, 1),\n            torch.nn.Dropout(0.1), # Drop 10% of neurons\n            torch.nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        x = self.sequential(x)\n        return x\n\n# Initialize neural network\nnetwork = SimpleNeuralNet()\n\n# Define loss function, optimizer\ncriterion = nn.BCELoss()\noptimizer = RMSprop(network.parameters())\n\n# Define data loader\ntrain_data = TensorDataset(x_train, y_train)\ntrain_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n\n# Compile the model using torch 2.0's optimizer\nnetwork = torch.compile(network)\n\n# Train neural network\nepochs = 5\nfor epoch in range(epochs):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        optimizer.zero_grad()\n        output = network(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        # Save the model at the end of every epoch\n        torch.save(\n            {\n                'epoch': epoch,\n                'model_state_dict': network.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': loss,\n            },\n            \"model.pt\"\n        )\n    print(\"Epoch:\", epoch+1, \"\\tLoss:\", loss.item())\n```", "```py\nEpoch: 1 \tLoss: 0.18791493773460388\nEpoch: 2 \tLoss: 0.17331615090370178\nEpoch: 3 \tLoss: 0.1384529024362564\nEpoch: 4 \tLoss: 0.1435958743095398\nEpoch: 5 \tLoss: 0.17967987060546875\n```", "```py\n# Load libraries\nfrom functools import partial\nimport numpy as np\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import RMSprop\nfrom torch.utils.data import random_split, DataLoader, TensorDataset\nfrom ray import tune\nfrom ray.tune import CLIReporter\nfrom ray.tune.schedulers import ASHAScheduler\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\n# Create training and test sets\nfeatures, target = make_classification(n_classes=2, n_features=10,\n    n_samples=1000)\nfeatures_train, features_test, target_train, target_test = train_test_split(\n    features, target, test_size=0.1, random_state=1)\n\n# Set random seed\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n# Convert data to PyTorch tensors\nx_train = torch.from_numpy(features_train).float()\ny_train = torch.from_numpy(target_train).float().view(-1, 1)\nx_test = torch.from_numpy(features_test).float()\ny_test = torch.from_numpy(target_test).float().view(-1, 1)\n\n# Define a neural network using `Sequential`\nclass SimpleNeuralNet(nn.Module):\n    def __init__(self, layer_size_1=10, layer_size_2=10):\n        super(SimpleNeuralNet, self).__init__()\n        self.sequential = torch.nn.Sequential(\n            torch.nn.Linear(10, layer_size_1),\n            torch.nn.ReLU(),\n            torch.nn.Linear(layer_size_1, layer_size_2),\n            torch.nn.ReLU(),\n            torch.nn.Linear(layer_size_2, 1),\n            torch.nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.sequential(x)\n        return x\n\nconfig = {\n    \"layer_size_1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n    \"layer_size_2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n    \"lr\": tune.loguniform(1e-4, 1e-1),\n}\n\nscheduler = ASHAScheduler(\n    metric=\"loss\",\n    mode=\"min\",\n    max_t=1000,\n    grace_period=1,\n    reduction_factor=2\n)\n\nreporter = CLIReporter(\n    parameter_columns=[\"layer_size_1\", \"layer_size_2\", \"lr\"],\n    metric_columns=[\"loss\"]\n)\n\n# # Train neural network\ndef train_model(config, epochs=3):\n    network = SimpleNeuralNet(config[\"layer_size_1\"], config[\"layer_size_2\"])\n\n    criterion = nn.BCELoss()\n    optimizer = optim.SGD(network.parameters(), lr=config[\"lr\"], momentum=0.9)\n\n    train_data = TensorDataset(x_train, y_train)\n    train_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n\n    # Compile the model using torch 2.0's optimizer\n    network = torch.compile(network)\n\n    for epoch in range(epochs):\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            output = network(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n            tune.report(loss=(loss.item()))\n\nresult = tune.run(\n    train_model,\n    resources_per_trial={\"cpu\": 2},\n    config=config,\n    num_samples=1,\n    scheduler=scheduler,\n    progress_reporter=reporter\n)\n\nbest_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\nprint(\"Best trial config: {}\".format(best_trial.config))\nprint(\"Best trial final validation loss: {}\".format(\n    best_trial.last_result[\"loss\"]))\n\nbest_trained_model = SimpleNeuralNet(best_trial.config[\"layer_size_1\"],\n    best_trial.config[\"layer_size_2\"])\n```", "```py\n== Status ==\nCurrent time: 2023-03-05 23:31:33 (running for 00:00:00.07)\nMemory usage on this node: 1.7/15.6 GiB\nUsing AsyncHyperBand: num_stopped=0\nBracket: Iter 512.000: None | Iter 256.000: None | Iter 128.000: None |\n    Iter 64.000: None | Iter 32.000: None | Iter 16.000: None |\n    Iter 8.000: None | Iter 4.000: None | Iter 2.000: None |\n    Iter 1.000: None\nResources requested: 2.0/7 CPUs, 0/0 GPUs, 0.0/8.95 GiB heap,\n    0.0/4.48 GiB objects\nResult logdir: /root/ray_results/train_model_2023-03-05_23-31-33\nNumber of trials: 1/1 (1 RUNNING)\n...\n```", "```py\n# Load libraries\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim import RMSprop\nfrom torchviz import make_dot\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\n# Create training and test sets\nfeatures, target = make_classification(n_classes=2, n_features=10,\n    n_samples=1000)\nfeatures_train, features_test, target_train, target_test = train_test_split(\n    features, target, test_size=0.1, random_state=1)\n\n# Set random seed\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n# Convert data to PyTorch tensors\nx_train = torch.from_numpy(features_train).float()\ny_train = torch.from_numpy(target_train).float().view(-1, 1)\nx_test = torch.from_numpy(features_test).float()\ny_test = torch.from_numpy(target_test).float().view(-1, 1)\n\n# Define a neural network using Sequential\nclass SimpleNeuralNet(nn.Module):\n    def __init__(self):\n        super(SimpleNeuralNet, self).__init__()\n        self.sequential = torch.nn.Sequential(\n            torch.nn.Linear(10, 16),\n            torch.nn.ReLU(),\n            torch.nn.Linear(16,16),\n            torch.nn.ReLU(),\n            torch.nn.Linear(16, 1),\n            torch.nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.sequential(x)\n        return x\n\n# Initialize neural network\nnetwork = SimpleNeuralNet()\n\n# Define loss function, optimizer\ncriterion = nn.BCELoss()\noptimizer = RMSprop(network.parameters())\n\n# Define data loader\ntrain_data = TensorDataset(x_train, y_train)\ntrain_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n\n# Compile the model using torch 2.0's optimizer\nnetwork = torch.compile(network)\n\n# Train neural network\nepochs = 3\nfor epoch in range(epochs):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        optimizer.zero_grad()\n        output = network(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\nmake_dot(output.detach(), params=dict(\n    list(\n        network.named_parameters()\n        )\n      )\n    ).render(\n        \"simple_neural_network\",\n        format=\"png\"\n)\n```", "```py\n'simple_neural_network.png'\n```"]