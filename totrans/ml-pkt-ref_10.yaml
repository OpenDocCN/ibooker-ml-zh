- en: Chapter 10\. Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classification is a *supervised learning* mechanism for labeling a sample based
    on the features. Supervised learning means that we have labels for classification
    or numbers for regression that the algorithm should learn.
  prefs: []
  type: TYPE_NORMAL
- en: We will look at various classification models in this chapter. Sklearn implements
    many common and useful models. We will also see some that are not in sklearn,
    but implement the sklearn interface. Because they follow the same interface, it
    is easy to try different families of models and see how well they perform.
  prefs: []
  type: TYPE_NORMAL
- en: In sklearn, we create a model instance and call the `.fit` method on it with
    the training data and training labels. We can now call the `.predict` method (or
    the `.predict_proba` or the `.predict_``log_proba` methods) with the fitted model.
    To evaluate the model, we use the `.score` with testing data and testing labels.
  prefs: []
  type: TYPE_NORMAL
- en: The bigger challenge is usually arranging data in a form that will work with
    sklearn. The data (`X`) should be an (m by n) numpy array (or pandas DataFrame)
    with m rows of sample data each with n features (columns). The label (`y`) is
    a vector (or pandas series) of size m with a value (class) for each sample.
  prefs: []
  type: TYPE_NORMAL
- en: The `.score` method returns the mean accuracy, which by itself might not be
    sufficient to evaluate a classifier. We will see other evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: We will look at many models and discuss their efficiency, the preprocessing
    techniques they require, how to prevent overfitting, and if the model supports
    intuitive interpretation of results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The general methods that sklearn type models implement are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`fit(X, y[, sample_weight])`'
  prefs: []
  type: TYPE_NORMAL
- en: Fit a model
  prefs: []
  type: TYPE_NORMAL
- en: '`predict(X)`'
  prefs: []
  type: TYPE_NORMAL
- en: Predict classes
  prefs: []
  type: TYPE_NORMAL
- en: '`predict_log_proba(X)`'
  prefs: []
  type: TYPE_NORMAL
- en: Predict log probability
  prefs: []
  type: TYPE_NORMAL
- en: '`predict_proba(X)`'
  prefs: []
  type: TYPE_NORMAL
- en: Predict probability
  prefs: []
  type: TYPE_NORMAL
- en: '`score(X, y[, sample_weight])`'
  prefs: []
  type: TYPE_NORMAL
- en: Get accuracy
  prefs: []
  type: TYPE_NORMAL
- en: Logistic Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logistic regression estimates probabilities by using a logistic function. (Careful;
    even though it has regression in the name, it is used for classification.) This
    has been the standard classification model for most sciences.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some model characteristics that we will include for each
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: Runtime efficiency
  prefs: []
  type: TYPE_NORMAL
- en: Can use `n_jobs` if not using `'liblinear'` solver.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocess data
  prefs: []
  type: TYPE_NORMAL
- en: If `solver` is set to `'sag'` or `'saga'`, standardize so that convergence works.
    Can handle sparse input.
  prefs: []
  type: TYPE_NORMAL
- en: Prevent overfitting
  prefs: []
  type: TYPE_NORMAL
- en: The `C` parameter controls regularization. (Lower `C` is more regularization,
    higher means less.) Can specify `penalty` to `'l1'` or `'l2'` (the default).
  prefs: []
  type: TYPE_NORMAL
- en: Interpret results
  prefs: []
  type: TYPE_NORMAL
- en: The `.coef_` attribute of the fitted model shows the decision function coefficients.
    A change in x one unit changes the log odds ratio by the coefficient. The `.intercept_`
    attribute is the inverse log odds of the baseline condition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example using this model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Instance parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`penalty=''l2''`'
  prefs: []
  type: TYPE_NORMAL
- en: Penalization norm, `'l1'` or `'l2'`.
  prefs: []
  type: TYPE_NORMAL
- en: '`dual=False`'
  prefs: []
  type: TYPE_NORMAL
- en: Use dual formulation (only with `'l2'` and `'liblinear'`).
  prefs: []
  type: TYPE_NORMAL
- en: '`C=1.0`'
  prefs: []
  type: TYPE_NORMAL
- en: Positive float. Inverse regularization strength. Smaller is stronger regularization.
  prefs: []
  type: TYPE_NORMAL
- en: '`fit_intercept=True`'
  prefs: []
  type: TYPE_NORMAL
- en: Add bias to the decision function.
  prefs: []
  type: TYPE_NORMAL
- en: '`intercept_scaling=1`'
  prefs: []
  type: TYPE_NORMAL
- en: If `fit_intercept` and `'liblinear'`, scale the intercept.
  prefs: []
  type: TYPE_NORMAL
- en: '`max_iter=100`'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: '`multi_class=''ovr''`'
  prefs: []
  type: TYPE_NORMAL
- en: Use one versus rest for each class, or for `'multinomial'`, train one class.
  prefs: []
  type: TYPE_NORMAL
- en: '`class_weight=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Dictionary or `'balanced'`.
  prefs: []
  type: TYPE_NORMAL
- en: '`solver=''liblinear''`'
  prefs: []
  type: TYPE_NORMAL
- en: '`''liblinear''` is good for small data. `''newton-cg''`, `''sag''`, `''saga''`,
    and `''lbfgs''` are for multiclass data. `''liblinear''` and `''saga''` only work
    with `''l1''` penalty. The others work with `''l2''`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`tol=0.0001`'
  prefs: []
  type: TYPE_NORMAL
- en: Stopping tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: '`verbose=0`'
  prefs: []
  type: TYPE_NORMAL
- en: Be verbose (if nonzero int).
  prefs: []
  type: TYPE_NORMAL
- en: '`warm_start=False`'
  prefs: []
  type: TYPE_NORMAL
- en: If `True`, remember previous fit.
  prefs: []
  type: TYPE_NORMAL
- en: '`njobs=1`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of CPUs to use. `-1` is all. Only works with `multi_class='over'` and
    `solver` is not `'liblinear'`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attributes after fitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '`coef_`'
  prefs: []
  type: TYPE_NORMAL
- en: Decision function coefficients
  prefs: []
  type: TYPE_NORMAL
- en: '`intercept_`'
  prefs: []
  type: TYPE_NORMAL
- en: Intercept of the decision function
  prefs: []
  type: TYPE_NORMAL
- en: '`n_iter_`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of iterations
  prefs: []
  type: TYPE_NORMAL
- en: 'The intercept is the log odds of the baseline condition. We can convert it
    back to a percent accuracy (proportion):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the inverse logit function, we see that the baseline for survival is
    34%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can inspect the coefficients. The inverse logit of the coefficients gives
    the proportion of the positive cases. In this case, if fare goes up, we are more
    likely to survive. If sex is male, we are less likely to survive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Yellowbrick can also visualize the coefficients. This visualizer has a `relative=True`
    parameter that makes the largest value be 100 (or -100), and the others are the
    percentages of that (see [Figure 10-1](#idlr1_2)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![Feature importance (relative to largest absolute regression coefficient).](assets/mlpr_1001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-1\. Feature importance (relative to largest absolute regression coefficient).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Naive Bayes is a probabilistic classifier that assumes independence between
    the features of the data. It is popular for text classification applications,
    such as catching spam. One advantage of this model is that because it assumes
    feature independence, it can train a model with a small number of samples. (A
    downside is that it can’t capture the interactions between features.) This simple
    model can also work with data that has many features. As such, it serves as a
    good baseline model.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three classes in sklearn: `GaussianNB`, `MultinomialNB`, and `BernoulliNB`.
    The first assumes a Gaussian distribution (continuous features with a normal distribution),
    the second is for discrete occurrence counts, and the third is for discrete Boolean
    features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This model has the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Runtime efficiency
  prefs: []
  type: TYPE_NORMAL
- en: Training O(Nd), where N is the number of training examples and d is dimensionality.
    Testing O(cd), where c is the number of classes.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocess data
  prefs: []
  type: TYPE_NORMAL
- en: Assumes that data is independent. Should perform better after removing colinear
    columns. For continuous numerical data, might be good to bin data. Gaussian assumes
    normal distribution, and you might need to transform data to convert to normal
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Prevent overfitting
  prefs: []
  type: TYPE_NORMAL
- en: Exhibits high bias and low variance (ensembles won’t reduce variance).
  prefs: []
  type: TYPE_NORMAL
- en: Interpret results
  prefs: []
  type: TYPE_NORMAL
- en: Percentage is the likelihood that a sample belongs to a class based on priors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example using this model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Instance parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`priors=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Prior probabilities of classes.
  prefs: []
  type: TYPE_NORMAL
- en: '`var_smoothing=1e-9`'
  prefs: []
  type: TYPE_NORMAL
- en: Added to variance for stable calculations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attributes after fitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '`class_prior_`'
  prefs: []
  type: TYPE_NORMAL
- en: Probabilities of classes
  prefs: []
  type: TYPE_NORMAL
- en: '`class_count_`'
  prefs: []
  type: TYPE_NORMAL
- en: Counts of classes
  prefs: []
  type: TYPE_NORMAL
- en: '`theta_`'
  prefs: []
  type: TYPE_NORMAL
- en: Mean of each column per class
  prefs: []
  type: TYPE_NORMAL
- en: '`sigma_`'
  prefs: []
  type: TYPE_NORMAL
- en: Variance of each column per class
  prefs: []
  type: TYPE_NORMAL
- en: '`epsilon_`'
  prefs: []
  type: TYPE_NORMAL
- en: Additive value to each variance
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: These models are susceptible to the *zero probability problem*. If you try to
    classify a new sample that has no training data, it will have a zero probability.
    One solution is to use *Laplace smoothing*. Sklearn controls this with the `alpha`
    parameter, which defaults to `1` and enables smoothing on the `MultinomialNB`
    and `BernoulliNB` models.
  prefs: []
  type: TYPE_NORMAL
- en: Support Vector Machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Support Vector Machine (SVM) is an algorithm that tries to fit a line (or
    plane or hyperplane) between the different classes that maximizes the distance
    from the line to the points of the classes. In this way it tries to find a robust
    separation between the classes. The *support vectors* are the points of the edge
    of the dividing hyperplane.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are a few different SVM implementations in sklearn. `SVC` wraps the `libsvm`
    library, while `LinearSVC` wraps the `liblinear` library.
  prefs: []
  type: TYPE_NORMAL
- en: There is also the `linear_model.SGDClassifier`, which implements SVM when using
    the default `loss` parameter. This chapter will describe the first implementation.
  prefs: []
  type: TYPE_NORMAL
- en: SVM generally performs well and can support linear spaces or nonlinear spaces
    by using a *kernel trick*. The kernel trick is the idea that we can create a decision
    boundary in a new dimension by minimizing a formula that is easier to calculate
    than actually mapping the points to the new dimension. The default kernel is the
    Radial Basis Function (`'rbf'`), which is controlled by the `gamma` parameter
    and can map an input space into a high dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: 'SVMs have the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Runtime efficiency
  prefs: []
  type: TYPE_NORMAL
- en: The scikit-learn implementation is O(n⁴), so it can be hard to scale to large
    sizes. Using a linear kernel or the `LinearSVC` model can improve the runtime
    performance at perhaps the cost of accuracy. Upping the `cache_size` parameter
    can bring that down to O(n³).
  prefs: []
  type: TYPE_NORMAL
- en: Preprocess data
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm is not scale invariant. Standardizing the data is highly recommended.
  prefs: []
  type: TYPE_NORMAL
- en: Prevent overfitting
  prefs: []
  type: TYPE_NORMAL
- en: The `C` (penalty parameter) controls regularization. A smaller value allows
    for a smaller margin in the hyperplane. A higher value for `gamma` will tend to
    overfit the training data. The `LinearSVC` model supports a `loss` and `penalty`
    parameter to support regularization.
  prefs: []
  type: TYPE_NORMAL
- en: Interpret results
  prefs: []
  type: TYPE_NORMAL
- en: Inspect `.support_vectors_`, though these are hard to explain. With linear kernels,
    you can inspect `.coef_`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example using scikit-learn’s SVM implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: To get probability, use `probability=True`, which will slow down fitting of
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: This is similar to a perceptron, but will find the maximum margin. If the data
    is not linearly separable, it will minimize the error. Alternatively, a different
    kernel may be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instance parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`C=1.0`'
  prefs: []
  type: TYPE_NORMAL
- en: The penalty parameter. The smaller the value, the tighter the decision boundary
    (more overfitting).
  prefs: []
  type: TYPE_NORMAL
- en: '`cache_size=200`'
  prefs: []
  type: TYPE_NORMAL
- en: Cache size (MB). Bumping this up can improve training time on large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '`class_weight=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Dictionary or `'balanced'`. Use dictionary to set `C` for each class.
  prefs: []
  type: TYPE_NORMAL
- en: '`coef0=0.0`'
  prefs: []
  type: TYPE_NORMAL
- en: Independent term for poly and sigmoid kernels.
  prefs: []
  type: TYPE_NORMAL
- en: '`decision_function_shape=''ovr''`'
  prefs: []
  type: TYPE_NORMAL
- en: Use one versus rest (`'ovr'`) or one versus one.
  prefs: []
  type: TYPE_NORMAL
- en: '`degree=3`'
  prefs: []
  type: TYPE_NORMAL
- en: Degree for polynomial kernel.
  prefs: []
  type: TYPE_NORMAL
- en: '`gamma=''auto''`'
  prefs: []
  type: TYPE_NORMAL
- en: Kernel coefficient. Can be a number, `'scale'` (default in 0.22, 1 / (`num features`
    * `X.std()` ) ), or `'auto'` (default prior, 1 / `num features`). A lower value
    leads to overfitting the training data.
  prefs: []
  type: TYPE_NORMAL
- en: '`kernel=''rbf''`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kernel type: `''linear''`, `''poly''`, `''rbf''` (default), `''sigmoid''`,
    `''precomputed''`, or a function.'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_iter=-1`'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum number of iterations for solver. -1 for no limit.
  prefs: []
  type: TYPE_NORMAL
- en: '`probability=False`'
  prefs: []
  type: TYPE_NORMAL
- en: Enable probability estimation. Slows down training.
  prefs: []
  type: TYPE_NORMAL
- en: '`random_state=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Random seed.
  prefs: []
  type: TYPE_NORMAL
- en: '`shrinking=True`'
  prefs: []
  type: TYPE_NORMAL
- en: Use shrinking heuristic.
  prefs: []
  type: TYPE_NORMAL
- en: '`tol=0.001`'
  prefs: []
  type: TYPE_NORMAL
- en: Stopping tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: '`verbose=False`'
  prefs: []
  type: TYPE_NORMAL
- en: Verbosity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attributes after fitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '`support_`'
  prefs: []
  type: TYPE_NORMAL
- en: Support vector indices
  prefs: []
  type: TYPE_NORMAL
- en: '`support_vectors_`'
  prefs: []
  type: TYPE_NORMAL
- en: Support vectors
  prefs: []
  type: TYPE_NORMAL
- en: '`n_support_vectors_`'
  prefs: []
  type: TYPE_NORMAL
- en: Count of per-class support vectors
  prefs: []
  type: TYPE_NORMAL
- en: '`coef_`'
  prefs: []
  type: TYPE_NORMAL
- en: Coefficients (for linear) kernel
  prefs: []
  type: TYPE_NORMAL
- en: K-Nearest Neighbor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The K-Nearest Neighbor (KNN) algorithm classifies based on distance to some
    number (k) of training samples. The algorithm family is called *instance-based*
    learning as there are no parameters to learn. This model assumes that distance
    is sufficient for inference; otherwise it makes no assumptions about the underlying
    data or its distributions.
  prefs: []
  type: TYPE_NORMAL
- en: The tricky part is selecting the appropriate k value. Also, the curse of dimensionality
    can hamper distance metrics as there is little difference in high dimensions between
    nearest and farthest neighbor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nearest neighbor models have the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Runtime efficiency
  prefs: []
  type: TYPE_NORMAL
- en: Training O(1), but need to store data. Testing O(Nd) where N is the number of
    training examples and d is dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocess data
  prefs: []
  type: TYPE_NORMAL
- en: Yes, distance-based calculations perform better when standardized.
  prefs: []
  type: TYPE_NORMAL
- en: Prevent overfitting
  prefs: []
  type: TYPE_NORMAL
- en: Raise `n_neighbors`. Change `p` for L1 or L2 metric.
  prefs: []
  type: TYPE_NORMAL
- en: Interpret results
  prefs: []
  type: TYPE_NORMAL
- en: Interpret the k-nearest neighbors to the sample (using the `.kneighbors` method).
    Those neighbors (if you can explain them) explain your result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of using the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`algorithm=''auto''`'
  prefs: []
  type: TYPE_NORMAL
- en: Can be `'brute'`, `'ball_tree'`, or `'kd_tree'`.
  prefs: []
  type: TYPE_NORMAL
- en: '`leaf_size=30`'
  prefs: []
  type: TYPE_NORMAL
- en: Used for tree algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '`metric=''minkowski''`'
  prefs: []
  type: TYPE_NORMAL
- en: Distance metric.
  prefs: []
  type: TYPE_NORMAL
- en: '`metric_params=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Additional dictionary of parameters for custom metric function.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_jobs=1`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_neighbors=5`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: '`p=2`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Minkowski power parameter: 1 = manhattan (L1). 2 = Euclidean (L2).'
  prefs: []
  type: TYPE_NORMAL
- en: '`weights=''uniform''`'
  prefs: []
  type: TYPE_NORMAL
- en: Can be `'distance'`, in which case, closer points have more influence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Distance metrics include: `''euclidean''`, `''manhattan''`, `''chebyshev''`,
    `''minkowski''`, `''wminkowski''`, `''seuclidean''`, `''mahalanobis''`, `''haversine''`,
    `''hamming''`, `''canberra''`, `''braycurtis''`, `''jaccard''`, `''matching''`,
    `''dice''`, `''rogerstanimoto''`, `''russellrao''`, `''sokalmichener''`, `''sokalsneath''`,
    or a callable (user defined).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If k is an even number and the neighbors are split, the result depends on the
    order of the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Decision Tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A decision tree is like going to a doctor who asks a series of questions to
    determine the cause of your symptoms. We can use a process to create a decision
    tree and have a series of questions to predict a target class. The advantages
    of this model include support for nonnumeric data (in some implementations), little
    data preparation (no need for scaling), support for dealing with nonlinear relationships,
    feature importances are revealed, and it is easy to explain.
  prefs: []
  type: TYPE_NORMAL
- en: The default algorithm used for creation is called the classification and regression
    tree (CART). It uses the Gini impurity or index measure to construct decisions.
    This is done by looping over the features and finding the value that gives the
    lowest probability of misclassifying.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The default values will lead to a fully grown (read overfit) tree. Use a mechanism
    such as `max_depth` and cross-validation to control for this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Decision trees have the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Runtime efficiency
  prefs: []
  type: TYPE_NORMAL
- en: For creation, loop over each of the m features, and sort all n samples, O(mn
    log n). For predicting, you walk the tree, O( height).
  prefs: []
  type: TYPE_NORMAL
- en: Preprocess data
  prefs: []
  type: TYPE_NORMAL
- en: Scaling is not necessary. Need to get rid of missing values and convert to numeric.
  prefs: []
  type: TYPE_NORMAL
- en: Prevent overfitting
  prefs: []
  type: TYPE_NORMAL
- en: Set `max_depth` to a lower number, raise `min_``impurity_decrease`.
  prefs: []
  type: TYPE_NORMAL
- en: Interpret results
  prefs: []
  type: TYPE_NORMAL
- en: Can step through the tree of choices. Because there are steps, a tree is bad
    at dealing with linear relationships (a small change in a number can go down a
    different path). The tree is also highly dependent on the training data. A small
    change can change the whole tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example using the scikit-learn library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Instance parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`class_weight=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Weights for class in dictionary. `'balanced'` will set values to the inverse
    proportion of class frequencies. Default is a value of 1 for each class. For multiclass,
    need a list of dictionaries, one-versus-rest (OVR) for each class.
  prefs: []
  type: TYPE_NORMAL
- en: '`criterion=''gini''`'
  prefs: []
  type: TYPE_NORMAL
- en: Splitting function, `'gini'` or `'entropy'`.
  prefs: []
  type: TYPE_NORMAL
- en: '`max_depth=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Depth of tree. Default will build until the leaves contain less than `min_samples_split`.
  prefs: []
  type: TYPE_NORMAL
- en: '`max_features=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of features to examine for split. Default is all.
  prefs: []
  type: TYPE_NORMAL
- en: '`max_leaf_nodes=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Limit the number of leaves. Default is unlimited.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_impurity_decrease=0.0`'
  prefs: []
  type: TYPE_NORMAL
- en: Split node if a split will decrease impurity >= value.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_impurity_split=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Deprecated.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_samples_leaf=1`'
  prefs: []
  type: TYPE_NORMAL
- en: Minimum number of samples at each leaf.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_samples_split=2`'
  prefs: []
  type: TYPE_NORMAL
- en: Minimum number of samples required to split a node.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_weight_fraction_leaf=0.0`'
  prefs: []
  type: TYPE_NORMAL
- en: Minimum sum total of weights required for leaf nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '`presort=False`'
  prefs: []
  type: TYPE_NORMAL
- en: May speed up training with a small dataset or restricted depth if set to `True`.
  prefs: []
  type: TYPE_NORMAL
- en: '`random_state=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Random seed.
  prefs: []
  type: TYPE_NORMAL
- en: '`splitter=''best''`'
  prefs: []
  type: TYPE_NORMAL
- en: Use `'random'` or `'best'`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attributes after fitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '`classes_`'
  prefs: []
  type: TYPE_NORMAL
- en: Class labels
  prefs: []
  type: TYPE_NORMAL
- en: '`feature_importances_`'
  prefs: []
  type: TYPE_NORMAL
- en: Array of Gini importance
  prefs: []
  type: TYPE_NORMAL
- en: '`n_classes_`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of classes
  prefs: []
  type: TYPE_NORMAL
- en: '`n_features_`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of features
  prefs: []
  type: TYPE_NORMAL
- en: '`tree_`'
  prefs: []
  type: TYPE_NORMAL
- en: Underlying tree object
  prefs: []
  type: TYPE_NORMAL
- en: 'View the tree with this code (see [Figure 10-2](#id21)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'For Jupyter, use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![Decision Tree.](assets/mlpr_1002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-2\. Decision tree.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The [dtreeviz package](https://github.com/parrt/dtreeviz) can aid in understanding
    how the decision tree works. It creates a tree with labeled histograms, which
    gives valuable insight (see [Figure 10-3](#iddtviz)). Here is an example. In Jupyter
    we can just display the `viz` object directly. If we are working from a script,
    we can call the `.save` method to create a PDF, SVG, or PNG:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![dtreeviz output.](assets/mlpr_1003.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-3\. dtreeviz output.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Feature importance showing Gini importance (reduction of error by using that
    feature):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use Yellowbrick to visualize feature importance (see [Figure 10-4](#id22)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![Feature importance (Gini coefficient) for decision tree (normalized to male
    importance).](assets/mlpr_1004.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-4\. Feature importance (Gini coefficient) for decision tree (normalized
    to male importance).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Random Forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A random forest is an ensemble of decision trees. It uses *bagging* to correct
    the tendency of decision trees to overfit. By creating many trees trained on random
    subsamples of the samples and random features of the data, the variance is lowered.
  prefs: []
  type: TYPE_NORMAL
- en: Because they train on subsamples of the data, random forests can evaluate OOB
    error and evaluate performance. They can also track feature importance by averaging
    the feature importance over all of the trees.
  prefs: []
  type: TYPE_NORMAL
- en: The intuition for understanding bagging comes from a 1785 essay by Marquis de
    Condorcet. The essence is that if you are creating a jury, you should add anyone
    who has a greater than 50% chance of delivering the correct verdict and then average
    their decisions. Every time you add another member (and their selection process
    is independent of the others), you will get a better result.
  prefs: []
  type: TYPE_NORMAL
- en: The idea with random forests is to create a “forest” of decision trees trained
    on different columns of the training data. If each tree has a better than 50%
    chance of correct classification, you should incorporate its prediction. The random
    forest has been an excellent tool for both classification and regression, though
    it has recently fallen out of favor for gradient-boosted trees.
  prefs: []
  type: TYPE_NORMAL
- en: 'It has the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Runtime efficiency
  prefs: []
  type: TYPE_NORMAL
- en: Need to create j random trees. This can be done in parallel using `n_jobs`.
    Complexity for each tree is O(mn log n), where n is the number of samples and
    m is the number of features. For creation, loop over each of the m features, and
    sort all n samples, O(mn log n). For predicting, walk the tree O( height).
  prefs: []
  type: TYPE_NORMAL
- en: Preprocess data
  prefs: []
  type: TYPE_NORMAL
- en: Not necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Prevent overfitting
  prefs: []
  type: TYPE_NORMAL
- en: Add more trees (`n_estimators`). Use lower `max_depth`.
  prefs: []
  type: TYPE_NORMAL
- en: Interpret results
  prefs: []
  type: TYPE_NORMAL
- en: Supports feature importance, but we don’t have a single decision tree that we
    can walk through. Can inspect single trees from the ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Instance parameters (these options mirror the decision tree):'
  prefs: []
  type: TYPE_NORMAL
- en: '`bootstrap=True`'
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrap when building trees.
  prefs: []
  type: TYPE_NORMAL
- en: '`class_weight=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Weights for class in dictionary. `'balanced'` will set values to the inverse
    proportion of class frequencies. Default is a value of 1 for each class. For multiclass,
    need a list of dictionaries (OVR) for each class.
  prefs: []
  type: TYPE_NORMAL
- en: '`criterion=''gini''`'
  prefs: []
  type: TYPE_NORMAL
- en: Splitting function, `'gini'` or `'entropy'`.
  prefs: []
  type: TYPE_NORMAL
- en: '`max_depth=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Depth of tree. Default will build until leaves contain less than `min_samples_split`.
  prefs: []
  type: TYPE_NORMAL
- en: '`max_features=''auto''`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of features to examine for split. Default is all.
  prefs: []
  type: TYPE_NORMAL
- en: '`max_leaf_nodes=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Limit the number of leaves. Default is unlimited.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_impurity_decrease=0.0`'
  prefs: []
  type: TYPE_NORMAL
- en: Split node if a split will decrease impurity >= value.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_impurity_split=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Deprecated.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_samples_leaf=1`'
  prefs: []
  type: TYPE_NORMAL
- en: Minimum number of samples at each leaf.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_samples_split=2`'
  prefs: []
  type: TYPE_NORMAL
- en: Minimum number of samples required to split a node. `min_weight_fraction_leaf=0.0`-
    Minimum sum total of weights required for leaf nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '* `n_estimators=10`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of trees in the forest.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_jobs=1`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of jobs for fitting and predicting.
  prefs: []
  type: TYPE_NORMAL
- en: '`oob_score=False`'
  prefs: []
  type: TYPE_NORMAL
- en: Whether to estimate `oob_score`.
  prefs: []
  type: TYPE_NORMAL
- en: '`random_state=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Random seed.
  prefs: []
  type: TYPE_NORMAL
- en: '`verbose=0`'
  prefs: []
  type: TYPE_NORMAL
- en: Verbosity.
  prefs: []
  type: TYPE_NORMAL
- en: '`warm_start=False`'
  prefs: []
  type: TYPE_NORMAL
- en: Fit a new forest or use the existing one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attributes after fitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '`classes_`'
  prefs: []
  type: TYPE_NORMAL
- en: Class labels.
  prefs: []
  type: TYPE_NORMAL
- en: '`feature_importances_`'
  prefs: []
  type: TYPE_NORMAL
- en: Array of Gini importance.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_classes_`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of classes.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_features_`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of features.
  prefs: []
  type: TYPE_NORMAL
- en: '`oob_score_`'
  prefs: []
  type: TYPE_NORMAL
- en: OOB score. Average accuracy for each observation not used in trees.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature importance showing Gini importance (reduction of error by using that
    feature):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The random forest classifier computes the feature importance by determining
    the *mean decrease in impurity* for each feature (also known as Gini importance).
    Features that reduce uncertainty in classification receive higher scores.
  prefs: []
  type: TYPE_NORMAL
- en: 'These numbers might be off if features vary in scale or cardinality of categorical
    columns. A more reliable score is *permutation importance* (where each column
    has its values permuted and the drop in accuracy is measured). An even more reliable
    mechanism is *drop column importance* (where each column is dropped and the model
    is re-evaluated), but sadly this requires creating a new model for each column
    that is dropped. See the `importances` function in the `rfpimp` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: XGBoost
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although sklearn has a `GradientBoostedClassifier`, it is better to use a third-party
    implementation that uses extreme boosting. These tend to provide better results.
  prefs: []
  type: TYPE_NORMAL
- en: '[XGBoost](https://oreil.ly/WBo0g) is a popular library outside of scikit-learn.
    It creates a weak tree and then “boosts” the subsequent trees to reduce the residual
    errors. It tries to capture and address any patterns in the errors until they
    appear to be random.'
  prefs: []
  type: TYPE_NORMAL
- en: 'XGBoost has the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Runtime efficiency
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost is parallelizeable. Use the `n_jobs` option to indicate the number of
    CPUs. Use GPU for even better performance.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocess data
  prefs: []
  type: TYPE_NORMAL
- en: No scaling necessary with tree models. Need to encode categorical data.
  prefs: []
  type: TYPE_NORMAL
- en: Prevent overfitting
  prefs: []
  type: TYPE_NORMAL
- en: The `early_stopping_rounds=N` parameter can be set to stop training if there
    is no improvement after N rounds. L1 and L2 regularization are controlled by `reg_alpha`
    and `reg_lambda`, respectively. Higher numbers are more conservative.
  prefs: []
  type: TYPE_NORMAL
- en: Interpret results
  prefs: []
  type: TYPE_NORMAL
- en: Has feature importance.
  prefs: []
  type: TYPE_NORMAL
- en: 'XGBoost has an extra parameter for the `.fit` method. The `early_stopping_rounds`
    parameter can be combined with the `eval_set` parameter to tell XGBoost to stop
    creating trees if the evaluation metric has not improved after that many boosting
    rounds. The `eval_metric` can also be set to one of the following: `''rmse''`,
    `''mae''`, `''logloss''`, `''error''` (default), `''auc''`, `''aucpr''`, as well
    as a custom function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example using the library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Instance parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_depth=3`'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum depth.
  prefs: []
  type: TYPE_NORMAL
- en: '`learning_rate=0.1`'
  prefs: []
  type: TYPE_NORMAL
- en: Learning rate (also called eta) for boosting (between 0 and 1). After each boost
    step, the newly added weights are scaled by this factor. The lower the value,
    the more conservative, but will also need more trees to converge. In the call
    to `.train`, you can pass a `learning_rates` parameter, which is a list of rates
    at each round (i.e., `[.1]*100 + [.05]*100`).
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators=100`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of rounds or boosted trees.
  prefs: []
  type: TYPE_NORMAL
- en: '`silent=True`'
  prefs: []
  type: TYPE_NORMAL
- en: Opposite of verbose. Whether to print messages while running boosting.
  prefs: []
  type: TYPE_NORMAL
- en: '`objective=''binary:logistic''`'
  prefs: []
  type: TYPE_NORMAL
- en: Learning task or callable for classification.
  prefs: []
  type: TYPE_NORMAL
- en: '`booster=''gbtree''`'
  prefs: []
  type: TYPE_NORMAL
- en: Can be `'gbtree'`, `'gblinear'`, or `'dart'`.
  prefs: []
  type: TYPE_NORMAL
- en: '`nthread=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Deprecated.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_jobs=1`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of threads to use.
  prefs: []
  type: TYPE_NORMAL
- en: '`gamma=0`'
  prefs: []
  type: TYPE_NORMAL
- en: Controls pruning. Range is 0 to infinite. Minimum loss reduction needed to further
    split a leaf. Higher gamma is more conservative. If training and test scores are
    diverging, insert a higher number (around 10). If training and test scores are
    close, use a lower number.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_child_weight=1`'
  prefs: []
  type: TYPE_NORMAL
- en: Minimum value for sum of hessian for a child.
  prefs: []
  type: TYPE_NORMAL
- en: '`max_delta_step=0`'
  prefs: []
  type: TYPE_NORMAL
- en: Make update more conservative. Set 1 to 10 for imbalanced classes.
  prefs: []
  type: TYPE_NORMAL
- en: '`subsample=1`'
  prefs: []
  type: TYPE_NORMAL
- en: Fraction of samples to use for next round.
  prefs: []
  type: TYPE_NORMAL
- en: '`colsample_bytree=1`'
  prefs: []
  type: TYPE_NORMAL
- en: Fraction of columns to use for round.
  prefs: []
  type: TYPE_NORMAL
- en: '`colsample_bylevel=1`'
  prefs: []
  type: TYPE_NORMAL
- en: Fraction of columns to use for level.
  prefs: []
  type: TYPE_NORMAL
- en: '`colsample_bynode=1`'
  prefs: []
  type: TYPE_NORMAL
- en: Fraction of columns to use for node.
  prefs: []
  type: TYPE_NORMAL
- en: '`reg_alpha=0`'
  prefs: []
  type: TYPE_NORMAL
- en: L1 regularization (mean of weights) encourages sparsity. Increase to be more
    conservative.
  prefs: []
  type: TYPE_NORMAL
- en: '`reg_lambda=1`'
  prefs: []
  type: TYPE_NORMAL
- en: L2 regularization (root of squared weights) encourages small weights. Increase
    to be more conservative.
  prefs: []
  type: TYPE_NORMAL
- en: '`scale_pos_weight=1`'
  prefs: []
  type: TYPE_NORMAL
- en: Ratio of negative/positive weight.
  prefs: []
  type: TYPE_NORMAL
- en: '`base_score=.5`'
  prefs: []
  type: TYPE_NORMAL
- en: Initial prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '`seed=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Deprecated.
  prefs: []
  type: TYPE_NORMAL
- en: '`random_state=0`'
  prefs: []
  type: TYPE_NORMAL
- en: Random seed.
  prefs: []
  type: TYPE_NORMAL
- en: '`missing=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Value to interpret for `missing`. `None` means `np.nan`.
  prefs: []
  type: TYPE_NORMAL
- en: '`importance_type=''gain''`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The feature importance type: `''gain''`, `''weight''`, `''cover''`, `''total_gain''`,
    or `''total_cover''`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`coef_`'
  prefs: []
  type: TYPE_NORMAL
- en: Coefficients for gblinear learners
  prefs: []
  type: TYPE_NORMAL
- en: '`feature_importances_`'
  prefs: []
  type: TYPE_NORMAL
- en: Feature importances for gbtree learners
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature importance is the average gain across all the nodes where the feature
    is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'XGBoost can plot the feature importance (see [Figure 10-5](#id25)). It has
    an `importance_type` parameter. The default value is `"weight"`, which is the
    number of times a feature appears in a tree. It can also be `"gain"`, which shows
    the average gain when the feature is used, or `"cover"`, which is the number of
    samples affected by a split:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![Feature importance showing weight (how many times a feature appears in the
    trees).](assets/mlpr_1005.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-5\. Feature importance showing weight (how many times a feature appears
    in the trees).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can plot this in Yellowbrick, which normalizes the values (see [Figure 10-6](#id26)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![Yellowbrick feature importance for XGBoost (normalized to 100).](assets/mlpr_1006.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-6\. Yellowbrick feature importance for XGBoost (normalized to 100).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'XGBoost provides both a textual representation of the trees and a graphical
    one. Here is the text representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The value in the leaf is the score for class 1\. It can be converted into a
    probability using the logistic function. If the decisions fell through to leaf
    7, the probability of class 1 is 53%. This is the score from a single tree. If
    our model had 100 trees, you would sum up each leaf value and get the probability
    with the logistic function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the graphical version of the first tree in the model (see [Figure 10-7](#id27)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![Tree of XGBoost.](assets/mlpr_1007.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-7\. Tree of XGBoost.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The [xgbfir package](https://oreil.ly/kPnRv) is a library built on top of XGBoost.
    This library gives various measures about feature importance. What is unique is
    that it provides these measures about the columns, and also pairs of columns,
    so you can see the interactions. In addition, you can get information about triplets
    (three-column) interactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The measures it provides are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Gain`'
  prefs: []
  type: TYPE_NORMAL
- en: Total gain of each feature or feature interaction
  prefs: []
  type: TYPE_NORMAL
- en: '`FScore`'
  prefs: []
  type: TYPE_NORMAL
- en: Amount of possible splits taken on a feature or feature interaction
  prefs: []
  type: TYPE_NORMAL
- en: '`wFScore`'
  prefs: []
  type: TYPE_NORMAL
- en: Amount of possible splits taken on a feature or feature interaction, weighted
    by the probability of the splits to take place
  prefs: []
  type: TYPE_NORMAL
- en: '`Average wFScore`'
  prefs: []
  type: TYPE_NORMAL
- en: '`wFScore` divided by `FScore`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Average Gain`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Gain` divided by `FScore`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Expected Gain`'
  prefs: []
  type: TYPE_NORMAL
- en: Total gain of each feature or feature interaction weighted by the probability
    to gather the gain
  prefs: []
  type: TYPE_NORMAL
- en: 'The interface is simply an export to a spreadsheet, so we will use pandas to
    read them back in. Here is the column importance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: From this table, we see sex_male ranks high in gain, average wFScore, average
    gain, and expected gain, whereas fare tops out in FScore and wFScore.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at pairs of column interactions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Here we see that the top two interactions involve the sex_male column in combination
    with pclass and age. If you were only able to make a model with two features,
    you would probably want to choose pclass and sex_male.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s look at triplets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This is only showing the first triplet due to space limitations, but the spreadsheet
    has many more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Gradient Boosted with LightGBM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LightGBM is an implementation by Microsoft. LightGBM uses a sampling mechanism
    to deal with continuous values. This allows quicker creation of trees (than say
    XGBoost), and reduces memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: LightGBM also grows trees depth first (*leaf-wise* rather than *level-wise*).
    Because of this, rather than using `max_depth` to control overfitting, use `num_leaves`
    (where this value is < 2^(`max_depth`)).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Installation of this library currently requires having a compiler and is a little
    more involved than just a `pip install`.
  prefs: []
  type: TYPE_NORMAL
- en: 'It has the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Runtime efficiency
  prefs: []
  type: TYPE_NORMAL
- en: Can take advantage of multiple CPUs. By using binning, can be 15 times faster
    than XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocess data
  prefs: []
  type: TYPE_NORMAL
- en: Has some support for encoding categorical columns as integers (or pandas `Categorical`
    type), but AUC appears to suffer compared to one-hot encoding.
  prefs: []
  type: TYPE_NORMAL
- en: Prevent overfitting
  prefs: []
  type: TYPE_NORMAL
- en: Lower `num_leaves`. Increase `min_data_in_leaf`. Use `min_gain_to_split` with
    `lambda_l1` or `lambda_l2`.
  prefs: []
  type: TYPE_NORMAL
- en: Interpret results
  prefs: []
  type: TYPE_NORMAL
- en: Feature importance is available. Individual trees are weak and tend to be hard
    to interpret.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example using the library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Instance parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`boosting_type=''gbdt''`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Can be: `''gbdt''` (gradient boosting), `''rf''` (random forest), `''dart''`
    (dropouts meet multiple additive regression trees), or `''goss''` (gradient-based,
    one-sided sampling).'
  prefs: []
  type: TYPE_NORMAL
- en: '`class_weight=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Dictionary or `'balanced'`. Use dictionary to set weight for each class label
    when doing multiclass problems. For binary problems, use `is_unbalance` or `scale_pos_weight`.
  prefs: []
  type: TYPE_NORMAL
- en: '`colsample_bytree=1.0`'
  prefs: []
  type: TYPE_NORMAL
- en: Range (0, 1.0]. Select percent of features for each boosting round.
  prefs: []
  type: TYPE_NORMAL
- en: '`importance_type=''split''`'
  prefs: []
  type: TYPE_NORMAL
- en: How to calculate feature importance. `'split'` means number of times a feature
    is used. `'gain'` is total gains of splits for a feature.
  prefs: []
  type: TYPE_NORMAL
- en: '`learning_rate=0.1`'
  prefs: []
  type: TYPE_NORMAL
- en: Range (0, 1.0]. Learning rate for boosting. A smaller value slows down overfitting
    as boosting rounds have less impact. A smaller number should give better performance
    but will require more `num_iterations`.
  prefs: []
  type: TYPE_NORMAL
- en: '`max_depth=-1`'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum tree depth. -1 is unlimited. Larger depths tend to overfit more.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_child_samples=20`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of samples required for a leaf. Lower numbers mean more overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_child_weight=0.001`'
  prefs: []
  type: TYPE_NORMAL
- en: Sum of hessian weight required for a leaf.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_split_gain=0.0`'
  prefs: []
  type: TYPE_NORMAL
- en: Loss reduction required to partition leaf.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators=100`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of trees or boosting rounds.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_jobs=-1`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of threads.
  prefs: []
  type: TYPE_NORMAL
- en: '`num_leaves=31`'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum tree leaves.
  prefs: []
  type: TYPE_NORMAL
- en: '`objective=None`'
  prefs: []
  type: TYPE_NORMAL
- en: '`None` is `''binary''` or `''multiclass''` for classifier. Can be a function
    or string.'
  prefs: []
  type: TYPE_NORMAL
- en: '`random_state=42`'
  prefs: []
  type: TYPE_NORMAL
- en: Random seed.
  prefs: []
  type: TYPE_NORMAL
- en: '`reg_alpha=0.0`'
  prefs: []
  type: TYPE_NORMAL
- en: L1 regularization (mean of weights). Increase to be more conservative.
  prefs: []
  type: TYPE_NORMAL
- en: '`reg_lambda=0.0`'
  prefs: []
  type: TYPE_NORMAL
- en: L2 regularization (root of squared weights). Increase to be more conservative.
  prefs: []
  type: TYPE_NORMAL
- en: '`silent=True`'
  prefs: []
  type: TYPE_NORMAL
- en: Verbose mode.
  prefs: []
  type: TYPE_NORMAL
- en: '`subsample=1.0`'
  prefs: []
  type: TYPE_NORMAL
- en: Fraction of samples to use for next round.
  prefs: []
  type: TYPE_NORMAL
- en: '`subsample_for_bin=200000`'
  prefs: []
  type: TYPE_NORMAL
- en: Samples required to create bins.
  prefs: []
  type: TYPE_NORMAL
- en: '`subsample_freq=0`'
  prefs: []
  type: TYPE_NORMAL
- en: Subsample frequency. Change to 1 to enable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature importance based on `''splits''` (number of times a product is used):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The LightGBM library supports creating a feature importance plot (see [Figure 10-8](#id23)).
    The default is based on `''splits''`, the number of times a feature is used. You
    can specify `''importance_type''` if you want to change it to `''gain''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![Feature Importance splits for LightGBM.](assets/mlpr_1008.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-8\. Feature importance splits for LightGBM.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As of version 0.9, Yellowbrick doesn’t work with LightGBM for creating feature
    importance plots.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a tree of the decisions as well (see [Figure 10-9](#id24)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![LightGBM tree.](assets/mlpr_1009.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-9\. LightGBM tree.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In Jupyter, use the following command to view a tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: TPOT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[TPOT](https://oreil.ly/NFJvl) uses a genetic algorithm to try different models
    and ensembles. This can take hours to days to run as it considers multiple models
    and preprocessing steps, as well as the hyperparameters for said models, and ensembling
    options. On a typical machine, a generation may take five or more minutes to run.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It has the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Runtime efficiency
  prefs: []
  type: TYPE_NORMAL
- en: Can take hours or days. Use `n_jobs=-1` to use all CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocess data
  prefs: []
  type: TYPE_NORMAL
- en: You need to remove NaN and categorical data.
  prefs: []
  type: TYPE_NORMAL
- en: Prevent overfitting
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, results should use cross-validation to minimize overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Interpret results
  prefs: []
  type: TYPE_NORMAL
- en: Depends on the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of using the library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Instance parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`generations=100`'
  prefs: []
  type: TYPE_NORMAL
- en: Iterations to run.
  prefs: []
  type: TYPE_NORMAL
- en: '`population_size=100`'
  prefs: []
  type: TYPE_NORMAL
- en: Population size for genetic programming. Larger size usually performs better
    but takes more memory and time.
  prefs: []
  type: TYPE_NORMAL
- en: '`offspring_size=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Offspring for each generation. Default is `population_size`.
  prefs: []
  type: TYPE_NORMAL
- en: '`mutation_rate=.9`'
  prefs: []
  type: TYPE_NORMAL
- en: Mutation rate for algorithm [0, 1]. Default is .9.
  prefs: []
  type: TYPE_NORMAL
- en: '`crossover_rate=.1`'
  prefs: []
  type: TYPE_NORMAL
- en: Cross-over rate (how many pipelines to breed in a generation). Range [0, 1].
    Default is .1.
  prefs: []
  type: TYPE_NORMAL
- en: '`scoring=''accuracy''`'
  prefs: []
  type: TYPE_NORMAL
- en: Scoring mechanism. Uses sklearn strings.
  prefs: []
  type: TYPE_NORMAL
- en: '`cv=5`'
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation folds.
  prefs: []
  type: TYPE_NORMAL
- en: '`subsample=1`'
  prefs: []
  type: TYPE_NORMAL
- en: Subsample training instances. Range [0, 1]. Default is 1.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_jobs=1`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of CPUs to use, -1 for all cores.
  prefs: []
  type: TYPE_NORMAL
- en: '`max_time_mins=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum amount of minutes to run.
  prefs: []
  type: TYPE_NORMAL
- en: '`max_eval_time_mins=5`'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum amount of minutes to evaluate a single pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '`random_state=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Random seed.
  prefs: []
  type: TYPE_NORMAL
- en: '`config_dict`'
  prefs: []
  type: TYPE_NORMAL
- en: Configuration options for optimization.
  prefs: []
  type: TYPE_NORMAL
- en: '`warm_start=False`'
  prefs: []
  type: TYPE_NORMAL
- en: Reuse previous calls to `.fit`.
  prefs: []
  type: TYPE_NORMAL
- en: '`memory=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Can cache pipelines. `'auto'` or a path will persist in a directory.
  prefs: []
  type: TYPE_NORMAL
- en: '`use_dask=False`'
  prefs: []
  type: TYPE_NORMAL
- en: Use dask.
  prefs: []
  type: TYPE_NORMAL
- en: '`periodic_checkpoint_folder=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Path to a folder where the best pipeline will be persisted periodically.
  prefs: []
  type: TYPE_NORMAL
- en: '`early_stop=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Stop after running this many generations with no improvement.
  prefs: []
  type: TYPE_NORMAL
- en: '`verbosity=0`'
  prefs: []
  type: TYPE_NORMAL
- en: 0 = none, 1 = minimal, 2 = high, or 3 = all. 2 and higher shows a progress bar.
  prefs: []
  type: TYPE_NORMAL
- en: '`disable_update_check=False`'
  prefs: []
  type: TYPE_NORMAL
- en: Disable version check.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`evaluated_individuals_`'
  prefs: []
  type: TYPE_NORMAL
- en: Dictionary with all pipelines that were evaluated.
  prefs: []
  type: TYPE_NORMAL
- en: '`fitted_pipeline_`'
  prefs: []
  type: TYPE_NORMAL
- en: Best pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'After you are done, you can export the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The result might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
