- en: Chapter 6\. Handling Text
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 6.0 Introduction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unstructured text data, like the contents of a book or a tweet, is both one
    of the most interesting sources of features and one of the most complex to handle.
    In this chapter, we will cover strategies for transforming text into information-rich
    features and use some out-of-the-box features (termed *embeddings*) that have
    become increasingly ubiquitous in tasks that involve natural language processing
    (NLP).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This is not to say that the recipes covered here are comprehensive. Entire academic
    disciplines focus on handling unstructured data such as text. In this chapter,
    we will cover some commonly used techniques; knowledge of these will add valuable
    tools to our preprocessing toolbox. In addition to many generic text processing
    recipes, we’ll also demonstrate how you can import and leverage some pretrained
    machine learning models to generate richer text features.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Cleaning Text
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have some unstructured text data and want to complete some basic cleaning.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the following example, we look at the text for three books and clean it
    by using Python’s core string operations, in particular `strip`, `replace`, and
    `split`:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We also create and apply a custom transformation function:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, we can use regular expressions to make powerful string operations:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Discussion
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some text data will need to be cleaned before we can use it to build features,
    or be preprocessed in some way prior to being fed into an algorithm. Most basic
    text cleaning can be completed using Python’s standard string operations. In the
    real world, we will most likely define a custom cleaning function (e.g., `capitalizer`)
    combining some cleaning tasks and apply that to the text data. Although cleaning
    strings can remove some information, it makes the data much easier to work with.
    Strings have many inherent methods that are useful for cleaning and processing;
    some additional examples can be found here:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: See Also
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Beginners Tutorial for Regular Expressions in Python](https://oreil.ly/hSqsa)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.2 Parsing and Cleaning HTML
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have text data with HTML elements and want to extract just the text.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use Beautiful Soup’s extensive set of options to parse and extract from HTML:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Discussion
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite the strange name, Beautiful Soup is a powerful Python library designed
    for scraping HTML. Typically Beautiful Soup is used to process HTML during live
    web scraping, but we can just as easily use it to extract text data embedded in
    static HTML. The full range of Beautiful Soup operations is beyond the scope of
    this book, but even the method we use in our solution shows how easy it can be
    to parse HTML and extract information from specific tags using `find()`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Beautiful Soup](https://oreil.ly/vh8h3)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.3 Removing Punctuation
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have a feature of text data and want to remove punctuation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Define a function that uses `translate` with a dictionary of punctuation characters:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Discussion
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Python `translate` method is popular due to its speed. In our solution,
    first we created a dictionary, `punctuation`, with all punctuation characters
    according to Unicode as its keys and `None` as its values. Next we translated
    all characters in the string that are in `punctuation` into `None`, effectively
    removing them. There are more readable ways to remove punctuation, but this somewhat
    hacky solution has the advantage of being far faster than alternatives.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: It is important to be conscious of the fact that punctuation contains information
    (e.g., “Right?” versus “Right!”). Removing punctuation can be a necessary evil
    when we need to manually create features; however, if the punctuation is important
    we should make sure to take that into account. Depending on the downstream task
    we’re trying to accomplish, punctuation might contain important information we
    want to keep (e.g., using a “?” to classify if some text contains a question).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Tokenizing Text
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have text and want to break it up into individual words.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Natural Language Toolkit for Python (NLTK) has a powerful set of text manipulation
    operations, including word tokenizing:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can also tokenize into sentences:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Discussion
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Tokenization*, especially word tokenization, is a common task after cleaning
    text data because it is the first step in the process of turning the text into
    data we will use to construct useful features. Some pretrained NLP models (such
    as Google’s BERT) utilize model-specific tokenization techniques; however, word-level
    tokenization is still a fairly common tokenization approach before getting features
    from individual words.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Removing Stop Words
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given tokenized text data, you want to remove extremely common words (e.g.,
    *a*, *is*, *of*, *on*) that contain little informational value.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use NLTK’s `stopwords`:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Discussion
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While “stop words” can refer to any set of words we want to remove before processing,
    frequently the term refers to extremely common words that themselves contain little
    information value. Whether or not you choose to remove stop words will depend
    on your individual use case. NLTK has a list of common stop words that we can
    use to find and remove stop words in our tokenized words:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note that NLTK’s `stopwords` assumes the tokenized words are all lowercased.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 6.6 Stemming Words
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have tokenized words and want to convert them into their root forms.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use NLTK’s `PorterStemmer`:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Discussion
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Stemming* reduces a word to its stem by identifying and removing affixes (e.g.,
    gerunds) while keeping the root meaning of the word. For example, both “tradition”
    and “traditional” have “tradit” as their stem, indicating that while they are
    different words, they represent the same general concept. By stemming our text
    data, we transform it to something less readable but closer to its base meaning
    and thus more suitable for comparison across observations. NLTK’s `PorterStemmer`
    implements the widely used Porter stemming algorithm to remove or replace common
    suffixes to produce the word stem.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[The Porter Stemming Algorithm](https://oreil.ly/Z4NTp)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.7 Tagging Parts of Speech
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have text data and want to tag each word or character with its part of speech.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use NLTK’s pretrained parts-of-speech tagger:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output is a list of tuples with the word and the tag of the part of speech.
    NLTK uses the Penn Treebank parts for speech tags. Some examples of the Penn Treebank
    tags are:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '| Tag | Part of speech |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
- en: '| NNP | Proper noun, singular |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
- en: '| NN | Noun, singular or mass |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
- en: '| RB | Adverb |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
- en: '| VBD | Verb, past tense |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
- en: '| VBG | Verb, gerund or present participle |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
- en: '| JJ | Adjective |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
- en: '| PRP | Personal pronoun |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
- en: 'Once the text has been tagged, we can use the tags to find certain parts of
    speech. For example, here are all nouns:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'A more realistic situation would be to have data where every observation contains
    a tweet, and we want to convert those sentences into features for individual parts
    of speech (e.g., a feature with `1` if a proper noun is present, and `0` otherwise):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Using `classes_` we can see that each feature is a part-of-speech tag:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Discussion
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If our text is English and not on a specialized topic (e.g., medicine) the simplest
    solution is to use NLTK’s pretrained parts-of-speech tagger. However, if `pos_tag`
    is not very accurate, NLTK also gives us the ability to train our own tagger.
    The major downside of training a tagger is that we need a large corpus of text
    where the tag of each word is known. Constructing this tagged corpus is obviously
    labor intensive and is probably going to be a last resort.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Alphabetical list of part-of-speech tags used in the Penn Treebank Project](https://oreil.ly/31xKf)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.8 Performing Named-Entity Recognition
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to perform named-entity recognition in freeform text (such as “Person,”
    “State,” etc.).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use spaCy’s default named-entity recognition pipeline and models to extract
    entites from text:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Discussion
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Named-entity recognition is the process of recognizing specific entities from
    text. Tools like spaCy offer preconfigured pipelines, and even pretrained or fine-tuned
    machine learning models that can easily identify these entities. In this case,
    we use spaCy to identify a person (“Elon Musk”), organization (“Twitter”), and
    money value (“21B”) from the raw text. Using this information, we can extract
    structured information from the unstructured textual data. This information can
    then be used in downstream machine learning models or data analysis.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Training a custom named-entity recognition model is outside the scope of this
    example; however, it is often done using deep learning and other NLP techniques.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[spaCy Named Entity Recognition documentation](https://oreil.ly/cN8KM)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Named-entity recognition, Wikipedia](https://oreil.ly/G8WDF)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.9 Encoding Text as a Bag of Words
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have text data and want to create a set of features indicating the number
    of times an observation’s text contains a particular word.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use scikit-learn’s `CountVectorizer`:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This output is a sparse array, which is often necessary when we have a large
    amount of text. However, in our toy example we can use `toarray` to view a matrix
    of word counts for each observation:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can use the `get_feature_names` method to view the word associated with
    each feature:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Note that the `I` from `I love Brazil` is not considered a token because the
    default `token_pattern` only considers tokens of two or more alphanumeric characters.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'Still, this might be confusing so, for the sake of clarity, here is what the
    feature matrix looks like with the words as column names (each row is one observation):'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '| beats | best | both | brazil | germany | is | love | sweden |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 | 2 | 0 | 0 | 1 | 0 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 0 | 0 | 0 | 1 | 0 | 1 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 1 | 0 | 1 | 0 | 0 | 0 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
- en: Discussion
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most common methods of transforming text into features is using a
    bag-of-words model. Bag-of-words models output a feature for every unique word
    in text data, with each feature containing a count of occurrences in observations.
    For example, in our solution, the sentence “I love Brazil. Brazil!” has a value
    of `2` in the “brazil” feature because the word *brazil* appears two times.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: The text data in our solution was purposely small. In the real world, a single
    observation of text data could be the contents of an entire book! Since our bag-of-words
    model creates a feature for every unique word in the data, the resulting matrix
    can contain thousands of features. This means the size of the matrix can sometimes
    become very large in memory. Luckily, we can exploit a common characteristic of
    bag-of-words feature matrices to reduce the amount of data we need to store.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Most words likely do not occur in most observations, and therefore bag-of-words
    feature matrices will contain mostly 0s as values. We call these types of matrices
    *sparse*. Instead of storing all values of the matrix, we can store only nonzero
    values and then assume all other values are 0\. This will save memory when we
    have large feature matrices. One of the nice features of `CountVectorizer` is
    that the output is a sparse matrix by default.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '`CountVectorizer` comes with a number of useful parameters to make it easy
    to create bag-of-words feature matrices. First, while by default every feature
    is a word, that does not have to be the case. Instead we can set every feature
    to be the combination of two words (called a 2-gram) or even three words (3-gram).
    `ngram_range` sets the minimum and maximum size of our *n*-grams. For example,
    `(2,3)` will return all 2-grams and 3-grams. Second, we can easily remove low-information
    filler words by using `stop_words`, either with a built-in list or a custom list.
    Finally, we can restrict the words or phrases we want to consider to a certain
    list of words using `vocabulary`. For example, we could create a bag-of-words
    feature matrix only for occurrences of country names:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: See Also
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[*n*-gram, Wikipedia](https://oreil.ly/XWIrM)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Bag of Words Meets Bags of Popcorn](https://oreil.ly/IiyRV)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.10 Weighting Word Importance
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want a bag of words with words weighted by their importance to an observation.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Compare the frequency of the word in a document (a tweet, movie review, speech
    transcript, etc.) with the frequency of the word in all other documents using
    term frequency-inverse document frequency (<math display="inline"><mtext class="left_paren"
    fontstyle="italic">tf-idf</mtext></math>). scikit-learn makes this easy with `TfidfVectorizer`:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Just as in [Recipe 6.9](#encoding-text-as-a-bag-of-words), the output is a
    sparse matrix. However, if we want to view the output as a dense matrix, we can
    use `toarray`:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '`vocabulary_` shows us the word of each feature:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Discussion
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The more a word appears in a document, the more likely it is that the word is
    important to that document. For example, if the word *economy* appears frequently,
    it is evidence that the document might be about economics. We call this *term
    frequency* (<math display="inline"><mtext class="left_paren" fontstyle="italic">tf</mtext></math>).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, if a word appears in many documents, it is likely less important
    to any individual document. For example, if every document in some text data contains
    the word *after* then it is probably an unimportant word. We call this *document
    frequency* (<math display="inline"><mtext class="left_paren" fontstyle="italic">df</mtext></math>).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'By combining these two statistics, we can assign a score to every word representing
    how important that word is in a document. Specifically, we multiply <math display="inline"><mtext
    fontstyle="italic">tf</mtext></math> to the inverse of document frequency (<math
    display="inline"><mtext class="left_paren" fontstyle="italic">idf</mtext></math>):'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合这两个统计量，我们可以为每个单词分配一个分数，代表该单词在文档中的重要性。具体来说，我们将 <math display="inline"><mtext
    fontstyle="italic">tf</mtext></math> 乘以文档频率的倒数 <math display="inline"><mtext class="left_paren"
    fontstyle="italic">idf</mtext></math>：
- en: <math display="block"><mrow><mtext class="left_paren" fontstyle="italic">tf-idf</mtext>
    <mo>(</mo> <mi>t</mi> <mo>,</mo> <mi>d</mi> <mo>)</mo> <mo>=</mo> <mi>t</mi> <mi>f</mi>
    <mo class="left_paren">(</mo> <mi>t</mi> <mo>,</mo> <mi>d</mi> <mo>)</mo> <mo>×</mo>
    <mtext class="left_paren" fontstyle="italic">idf</mtext> <mo>(</mo> <mi class="left_paren">t</mi>
    <mo>)</mo></mrow></math>
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext class="left_paren" fontstyle="italic">tf-idf</mtext>
    <mo>(</mo> <mi>t</mi> <mo>,</mo> <mi>d</mi> <mo>)</mo> <mo>=</mo> <mi>t</mi> <mi>f</mi>
    <mo class="left_paren">(</mo> <mi>t</mi> <mo>,</mo> <mi>d</mi> <mo>)</mo> <mo>×</mo>
    <mtext class="left_paren" fontstyle="italic">idf</mtext> <mo>(</mo> <mi class="left_paren">t</mi>
    <mo>)</mo></mrow></math>
- en: 'where <math display="inline"><mi>t</mi></math> is a word (term) and <math display="inline"><mi>d</mi></math>
    is a document. There are a number of variations in how <math display="inline"><mtext
    fontstyle="italic">tf</mtext></math> and <math display="inline"><mtext fontstyle="italic">idf</mtext></math>
    are calculated. In scikit-learn, <math display="inline"><mtext fontstyle="italic">tf</mtext></math>
    is simply the number of times a word appears in the document, and <math display="inline"><mtext
    fontstyle="italic">idf</mtext></math> is calculated as:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math display="inline"><mi>t</mi></math> 是一个单词（术语），<math display="inline"><mi>d</mi></math>
    是一个文档。关于如何计算 <math display="inline"><mtext fontstyle="italic">tf</mtext></math>
    和 <math display="inline"><mtext fontstyle="italic">idf</mtext></math> 有许多不同的变体。在
    scikit-learn 中，<math display="inline"><mtext fontstyle="italic">tf</mtext></math>
    简单地是单词在文档中出现的次数，<math display="inline"><mtext fontstyle="italic">idf</mtext></math>
    计算如下：
- en: <math display="block"><mrow><mtext class="left_paren" fontstyle="italic">idf</mtext>
    <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>l</mi> <mi>o</mi>
    <mi>g</mi> <mfrac><mrow><mn>1</mn><mo>+</mo><msub><mi>n</mi> <mi>d</mi></msub></mrow>
    <mrow><mn>1</mn><mo>+</mo><mtext class="left_paren" fontstyle="italic">df</mtext><mo>(</mo><mi>d</mi><mo>,</mo><mi>t</mi><mo>)</mo></mrow></mfrac>
    <mo>+</mo> <mn>1</mn></mrow></math>
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext class="left_paren" fontstyle="italic">idf</mtext>
    <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>l</mi> <mi>o</mi>
    <mi>g</mi> <mfrac><mrow><mn>1</mn><mo>+</mo><msub><mi>n</mi> <mi>d</mi></msub></mrow>
    <mrow><mn>1</mn><mo>+</mo><mtext class="left_paren" fontstyle="italic">df</mtext><mo>(</mo><mi>d</mi><mo>,</mo><mi>t</mi><mo>)</mo></mrow></mfrac>
    <mo>+</mo> <mn>1</mn></mrow></math>
- en: where <math display="inline"><msub><mi>n</mi><mi>d</mi></msub></math> is the
    number of documents, and <math display="inline"><mtext class="left_paren" fontstyle="italic">df</mtext><mo>(</mo><mi>d</mi><mo>,</mo><mi>t</mi><mo>)</mo></math>
    is term <math display="inline"><mi>t</mi></math>’s document frequency (i.e., the
    number of documents where the term appears).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math display="inline"><msub><mi>n</mi><mi>d</mi></msub></math> 是文档数量，<math
    display="inline"><mtext class="left_paren" fontstyle="italic">df</mtext><mo>(</mo><mi>d</mi><mo>,</mo><mi>t</mi><mo>)</mo></math>
    是术语 <math display="inline"><mi>t</mi></math> 的文档频率（即术语出现的文档数量）。
- en: By default, scikit-learn then normalizes the <math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math>
    vectors using the Euclidean norm (L2 norm). The higher the resulting value, the
    more important the word is to a document.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，scikit-learn 使用欧几里得范数（L2 范数）对 <math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math>
    向量进行归一化。结果值越高，单词对文档的重要性越大。
- en: See Also
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '[scikit-learn documentation: *tf–idf* term weighting](https://oreil.ly/40WeT)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[scikit-learn 文档: *tf–idf* 术语加权](https://oreil.ly/40WeT)'
- en: 6.11 Using Text Vectors to Calculate Text Similarity in a Search Query
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6.11 使用文本向量计算搜索查询中的文本相似度
- en: Problem
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to use <math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math>
    vectors to implement a text search function in Python.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 您想要使用 <math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math>
    向量来实现 Python 中的文本搜索功能。
- en: Solution
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Calculate the cosine similarity between <math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math>
    vectors using scikit-learn:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 计算 <math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math>
    向量之间的余弦相似度：
- en: '[PRE50]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Discussion
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Text vectors are incredibly useful for NLP use cases such as search engines.
    After calculating the <math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math>
    vectors of a set of sentences or documents, we can use the same `tfidf` object
    to vectorize future sets of text. Then, we can compute cosine similarity between
    our input vector and the matrix of other vectors and sort by the most relevant
    documents.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 文本向量对于诸如搜索引擎之类的 NLP 用例非常有用。计算了一组句子或文档的 <math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math>
    向量后，我们可以使用相同的 `tfidf` 对象来向量化未来的文本集。然后，我们可以计算输入向量与其他向量矩阵之间的余弦相似度，并按最相关的文档进行排序。
- en: Cosine similarities take on the range of [0, 1.0], with 0 being least similar
    and 1 being most similar. Since we’re using <math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math>
    vectors to compute the similarity between vectors, the frequency of a word’s occurrence
    is also taken into account. However, with a small corpus (set of documents) even
    “frequent” words may not appear frequently. In this example, “Sweden is best”
    is the most relevant text to our search query “Brazil is the best”. Since the
    query mentions Brazil, we might expect “I love Brazil. Brazil!” to be the most
    relevant; however, “Sweden is best” is the most similar due to the words “is”
    and “best”. As the number of documents we add to our corpus increases, less important
    words will be weighted less and have less effect on our cosine similarity calculation.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度的取值范围为[0, 1.0]，其中0表示最不相似，1表示最相似。由于我们使用<math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math>向量来计算向量之间的相似度，单词出现的频率也被考虑在内。然而，在一个小的语料库（文档集合）中，即使是“频繁”出现的词语也可能不频繁出现。在这个例子中，“瑞典是最好的”是最相关的文本，与我们的搜索查询“巴西是最好的”最相似。由于查询提到了巴西，我们可能期望“我爱巴西。巴西！”是最相关的；然而，由于“是”和“最好”，“瑞典是最好的”是最相似的。随着我们向语料库中添加的文档数量的增加，不重要的词语将被加权较少，对余弦相似度计算的影响也将减小。
- en: See Also
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Cosine Similarity, Geeks for Geeks](https://oreil.ly/-5Odv)'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[余弦相似度，Geeks for Geeks](https://oreil.ly/-5Odv)'
- en: '[Nvidia Gave Me a $15K Data Science Workstation—Here’s What I Did with It (building
    a Pubmed search engine in Python)](https://oreil.ly/pAxbR)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Nvidia给了我一台价值15000美元的数据科学工作站——这是我在其中做的事情（用Python构建Pubmed搜索引擎）](https://oreil.ly/pAxbR)'
- en: 6.12 Using a Sentiment Analysis Classifier
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6.12 使用情感分析分类器
- en: Problem
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to classify the sentiment of some text to use as a feature or in downstream
    data analysis.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望对一些文本的情感进行分类，以便作为特征或在下游数据分析中使用。
- en: Solution
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the `transformers` library’s sentiment classifier.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`transformers`库的情感分类器。
- en: '[PRE52]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Discussion
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: The `transformers` library is an extremely popular library for NLP tasks and
    contains a number of easy-to-use APIs for training models or using pretrained
    ones. We’ll talk more about NLP and this library in [Chapter 22](ch22.xhtml#neural-networks-for-unstructured-data),
    but this example serves as a high-level introduction to the power of using pretrained
    classifiers in your machine learning pipelines to generate features, classify
    text, or analyze unstructured data.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`库是一个极为流行的自然语言处理任务库，包含许多易于使用的API，用于训练模型或使用预训练模型。我们将在[第22章](ch22.xhtml#neural-networks-for-unstructured-data)更详细地讨论NLP和这个库，但这个例子作为使用预训练分类器在您的机器学习流水线中生成特征、分类文本或分析非结构化数据的强大工具的高级介绍。'
- en: See Also
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Hugging Face Transformers Quick Tour](https://oreil.ly/7hT6W)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hugging Face Transformers快速导览](https://oreil.ly/7hT6W)'
