- en: Chapter 6\. Handling Text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 6.0 Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unstructured text data, like the contents of a book or a tweet, is both one
    of the most interesting sources of features and one of the most complex to handle.
    In this chapter, we will cover strategies for transforming text into information-rich
    features and use some out-of-the-box features (termed *embeddings*) that have
    become increasingly ubiquitous in tasks that involve natural language processing
    (NLP).
  prefs: []
  type: TYPE_NORMAL
- en: This is not to say that the recipes covered here are comprehensive. Entire academic
    disciplines focus on handling unstructured data such as text. In this chapter,
    we will cover some commonly used techniques; knowledge of these will add valuable
    tools to our preprocessing toolbox. In addition to many generic text processing
    recipes, we’ll also demonstrate how you can import and leverage some pretrained
    machine learning models to generate richer text features.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Cleaning Text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have some unstructured text data and want to complete some basic cleaning.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the following example, we look at the text for three books and clean it
    by using Python’s core string operations, in particular `strip`, `replace`, and
    `split`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We also create and apply a custom transformation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can use regular expressions to make powerful string operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some text data will need to be cleaned before we can use it to build features,
    or be preprocessed in some way prior to being fed into an algorithm. Most basic
    text cleaning can be completed using Python’s standard string operations. In the
    real world, we will most likely define a custom cleaning function (e.g., `capitalizer`)
    combining some cleaning tasks and apply that to the text data. Although cleaning
    strings can remove some information, it makes the data much easier to work with.
    Strings have many inherent methods that are useful for cleaning and processing;
    some additional examples can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Beginners Tutorial for Regular Expressions in Python](https://oreil.ly/hSqsa)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.2 Parsing and Cleaning HTML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have text data with HTML elements and want to extract just the text.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use Beautiful Soup’s extensive set of options to parse and extract from HTML:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite the strange name, Beautiful Soup is a powerful Python library designed
    for scraping HTML. Typically Beautiful Soup is used to process HTML during live
    web scraping, but we can just as easily use it to extract text data embedded in
    static HTML. The full range of Beautiful Soup operations is beyond the scope of
    this book, but even the method we use in our solution shows how easy it can be
    to parse HTML and extract information from specific tags using `find()`.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Beautiful Soup](https://oreil.ly/vh8h3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.3 Removing Punctuation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have a feature of text data and want to remove punctuation.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Define a function that uses `translate` with a dictionary of punctuation characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Python `translate` method is popular due to its speed. In our solution,
    first we created a dictionary, `punctuation`, with all punctuation characters
    according to Unicode as its keys and `None` as its values. Next we translated
    all characters in the string that are in `punctuation` into `None`, effectively
    removing them. There are more readable ways to remove punctuation, but this somewhat
    hacky solution has the advantage of being far faster than alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to be conscious of the fact that punctuation contains information
    (e.g., “Right?” versus “Right!”). Removing punctuation can be a necessary evil
    when we need to manually create features; however, if the punctuation is important
    we should make sure to take that into account. Depending on the downstream task
    we’re trying to accomplish, punctuation might contain important information we
    want to keep (e.g., using a “?” to classify if some text contains a question).
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Tokenizing Text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have text and want to break it up into individual words.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Natural Language Toolkit for Python (NLTK) has a powerful set of text manipulation
    operations, including word tokenizing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also tokenize into sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Tokenization*, especially word tokenization, is a common task after cleaning
    text data because it is the first step in the process of turning the text into
    data we will use to construct useful features. Some pretrained NLP models (such
    as Google’s BERT) utilize model-specific tokenization techniques; however, word-level
    tokenization is still a fairly common tokenization approach before getting features
    from individual words.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Removing Stop Words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given tokenized text data, you want to remove extremely common words (e.g.,
    *a*, *is*, *of*, *on*) that contain little informational value.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use NLTK’s `stopwords`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While “stop words” can refer to any set of words we want to remove before processing,
    frequently the term refers to extremely common words that themselves contain little
    information value. Whether or not you choose to remove stop words will depend
    on your individual use case. NLTK has a list of common stop words that we can
    use to find and remove stop words in our tokenized words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Note that NLTK’s `stopwords` assumes the tokenized words are all lowercased.
  prefs: []
  type: TYPE_NORMAL
- en: 6.6 Stemming Words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have tokenized words and want to convert them into their root forms.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use NLTK’s `PorterStemmer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Stemming* reduces a word to its stem by identifying and removing affixes (e.g.,
    gerunds) while keeping the root meaning of the word. For example, both “tradition”
    and “traditional” have “tradit” as their stem, indicating that while they are
    different words, they represent the same general concept. By stemming our text
    data, we transform it to something less readable but closer to its base meaning
    and thus more suitable for comparison across observations. NLTK’s `PorterStemmer`
    implements the widely used Porter stemming algorithm to remove or replace common
    suffixes to produce the word stem.'
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[The Porter Stemming Algorithm](https://oreil.ly/Z4NTp)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.7 Tagging Parts of Speech
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have text data and want to tag each word or character with its part of speech.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use NLTK’s pretrained parts-of-speech tagger:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is a list of tuples with the word and the tag of the part of speech.
    NLTK uses the Penn Treebank parts for speech tags. Some examples of the Penn Treebank
    tags are:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Tag | Part of speech |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| NNP | Proper noun, singular |'
  prefs: []
  type: TYPE_TB
- en: '| NN | Noun, singular or mass |'
  prefs: []
  type: TYPE_TB
- en: '| RB | Adverb |'
  prefs: []
  type: TYPE_TB
- en: '| VBD | Verb, past tense |'
  prefs: []
  type: TYPE_TB
- en: '| VBG | Verb, gerund or present participle |'
  prefs: []
  type: TYPE_TB
- en: '| JJ | Adjective |'
  prefs: []
  type: TYPE_TB
- en: '| PRP | Personal pronoun |'
  prefs: []
  type: TYPE_TB
- en: 'Once the text has been tagged, we can use the tags to find certain parts of
    speech. For example, here are all nouns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'A more realistic situation would be to have data where every observation contains
    a tweet, and we want to convert those sentences into features for individual parts
    of speech (e.g., a feature with `1` if a proper noun is present, and `0` otherwise):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `classes_` we can see that each feature is a part-of-speech tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If our text is English and not on a specialized topic (e.g., medicine) the simplest
    solution is to use NLTK’s pretrained parts-of-speech tagger. However, if `pos_tag`
    is not very accurate, NLTK also gives us the ability to train our own tagger.
    The major downside of training a tagger is that we need a large corpus of text
    where the tag of each word is known. Constructing this tagged corpus is obviously
    labor intensive and is probably going to be a last resort.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Alphabetical list of part-of-speech tags used in the Penn Treebank Project](https://oreil.ly/31xKf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.8 Performing Named-Entity Recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to perform named-entity recognition in freeform text (such as “Person,”
    “State,” etc.).
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use spaCy’s default named-entity recognition pipeline and models to extract
    entites from text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Named-entity recognition is the process of recognizing specific entities from
    text. Tools like spaCy offer preconfigured pipelines, and even pretrained or fine-tuned
    machine learning models that can easily identify these entities. In this case,
    we use spaCy to identify a person (“Elon Musk”), organization (“Twitter”), and
    money value (“21B”) from the raw text. Using this information, we can extract
    structured information from the unstructured textual data. This information can
    then be used in downstream machine learning models or data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Training a custom named-entity recognition model is outside the scope of this
    example; however, it is often done using deep learning and other NLP techniques.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[spaCy Named Entity Recognition documentation](https://oreil.ly/cN8KM)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Named-entity recognition, Wikipedia](https://oreil.ly/G8WDF)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.9 Encoding Text as a Bag of Words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have text data and want to create a set of features indicating the number
    of times an observation’s text contains a particular word.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use scikit-learn’s `CountVectorizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This output is a sparse array, which is often necessary when we have a large
    amount of text. However, in our toy example we can use `toarray` to view a matrix
    of word counts for each observation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the `get_feature_names` method to view the word associated with
    each feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Note that the `I` from `I love Brazil` is not considered a token because the
    default `token_pattern` only considers tokens of two or more alphanumeric characters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Still, this might be confusing so, for the sake of clarity, here is what the
    feature matrix looks like with the words as column names (each row is one observation):'
  prefs: []
  type: TYPE_NORMAL
- en: '| beats | best | both | brazil | germany | is | love | sweden |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 | 2 | 0 | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 0 | 0 | 0 | 1 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 1 | 0 | 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most common methods of transforming text into features is using a
    bag-of-words model. Bag-of-words models output a feature for every unique word
    in text data, with each feature containing a count of occurrences in observations.
    For example, in our solution, the sentence “I love Brazil. Brazil!” has a value
    of `2` in the “brazil” feature because the word *brazil* appears two times.
  prefs: []
  type: TYPE_NORMAL
- en: The text data in our solution was purposely small. In the real world, a single
    observation of text data could be the contents of an entire book! Since our bag-of-words
    model creates a feature for every unique word in the data, the resulting matrix
    can contain thousands of features. This means the size of the matrix can sometimes
    become very large in memory. Luckily, we can exploit a common characteristic of
    bag-of-words feature matrices to reduce the amount of data we need to store.
  prefs: []
  type: TYPE_NORMAL
- en: Most words likely do not occur in most observations, and therefore bag-of-words
    feature matrices will contain mostly 0s as values. We call these types of matrices
    *sparse*. Instead of storing all values of the matrix, we can store only nonzero
    values and then assume all other values are 0\. This will save memory when we
    have large feature matrices. One of the nice features of `CountVectorizer` is
    that the output is a sparse matrix by default.
  prefs: []
  type: TYPE_NORMAL
- en: '`CountVectorizer` comes with a number of useful parameters to make it easy
    to create bag-of-words feature matrices. First, while by default every feature
    is a word, that does not have to be the case. Instead we can set every feature
    to be the combination of two words (called a 2-gram) or even three words (3-gram).
    `ngram_range` sets the minimum and maximum size of our *n*-grams. For example,
    `(2,3)` will return all 2-grams and 3-grams. Second, we can easily remove low-information
    filler words by using `stop_words`, either with a built-in list or a custom list.
    Finally, we can restrict the words or phrases we want to consider to a certain
    list of words using `vocabulary`. For example, we could create a bag-of-words
    feature matrix only for occurrences of country names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[*n*-gram, Wikipedia](https://oreil.ly/XWIrM)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Bag of Words Meets Bags of Popcorn](https://oreil.ly/IiyRV)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.10 Weighting Word Importance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want a bag of words with words weighted by their importance to an observation.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Compare the frequency of the word in a document (a tweet, movie review, speech
    transcript, etc.) with the frequency of the word in all other documents using
    term frequency-inverse document frequency (<math display="inline"><mtext class="left_paren"
    fontstyle="italic">tf-idf</mtext></math>). scikit-learn makes this easy with `TfidfVectorizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Just as in [Recipe 6.9](#encoding-text-as-a-bag-of-words), the output is a
    sparse matrix. However, if we want to view the output as a dense matrix, we can
    use `toarray`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '`vocabulary_` shows us the word of each feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The more a word appears in a document, the more likely it is that the word is
    important to that document. For example, if the word *economy* appears frequently,
    it is evidence that the document might be about economics. We call this *term
    frequency* (<math display="inline"><mtext class="left_paren" fontstyle="italic">tf</mtext></math>).
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, if a word appears in many documents, it is likely less important
    to any individual document. For example, if every document in some text data contains
    the word *after* then it is probably an unimportant word. We call this *document
    frequency* (<math display="inline"><mtext class="left_paren" fontstyle="italic">df</mtext></math>).
  prefs: []
  type: TYPE_NORMAL
- en: 'By combining these two statistics, we can assign a score to every word representing
    how important that word is in a document. Specifically, we multiply <math display="inline"><mtext
    fontstyle="italic">tf</mtext></math> to the inverse of document frequency (<math
    display="inline"><mtext class="left_paren" fontstyle="italic">idf</mtext></math>):'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext class="left_paren" fontstyle="italic">tf-idf</mtext>
    <mo>(</mo> <mi>t</mi> <mo>,</mo> <mi>d</mi> <mo>)</mo> <mo>=</mo> <mi>t</mi> <mi>f</mi>
    <mo class="left_paren">(</mo> <mi>t</mi> <mo>,</mo> <mi>d</mi> <mo>)</mo> <mo>×</mo>
    <mtext class="left_paren" fontstyle="italic">idf</mtext> <mo>(</mo> <mi class="left_paren">t</mi>
    <mo>)</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'where <math display="inline"><mi>t</mi></math> is a word (term) and <math display="inline"><mi>d</mi></math>
    is a document. There are a number of variations in how <math display="inline"><mtext
    fontstyle="italic">tf</mtext></math> and <math display="inline"><mtext fontstyle="italic">idf</mtext></math>
    are calculated. In scikit-learn, <math display="inline"><mtext fontstyle="italic">tf</mtext></math>
    is simply the number of times a word appears in the document, and <math display="inline"><mtext
    fontstyle="italic">idf</mtext></math> is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext class="left_paren" fontstyle="italic">idf</mtext>
    <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>l</mi> <mi>o</mi>
    <mi>g</mi> <mfrac><mrow><mn>1</mn><mo>+</mo><msub><mi>n</mi> <mi>d</mi></msub></mrow>
    <mrow><mn>1</mn><mo>+</mo><mtext class="left_paren" fontstyle="italic">df</mtext><mo>(</mo><mi>d</mi><mo>,</mo><mi>t</mi><mo>)</mo></mrow></mfrac>
    <mo>+</mo> <mn>1</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math display="inline"><msub><mi>n</mi><mi>d</mi></msub></math> is the
    number of documents, and <math display="inline"><mtext class="left_paren" fontstyle="italic">df</mtext><mo>(</mo><mi>d</mi><mo>,</mo><mi>t</mi><mo>)</mo></math>
    is term <math display="inline"><mi>t</mi></math>’s document frequency (i.e., the
    number of documents where the term appears).
  prefs: []
  type: TYPE_NORMAL
- en: By default, scikit-learn then normalizes the <math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math>
    vectors using the Euclidean norm (L2 norm). The higher the resulting value, the
    more important the word is to a document.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[scikit-learn documentation: *tf–idf* term weighting](https://oreil.ly/40WeT)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.11 Using Text Vectors to Calculate Text Similarity in a Search Query
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to use <math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math>
    vectors to implement a text search function in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Calculate the cosine similarity between <math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math>
    vectors using scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text vectors are incredibly useful for NLP use cases such as search engines.
    After calculating the <math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math>
    vectors of a set of sentences or documents, we can use the same `tfidf` object
    to vectorize future sets of text. Then, we can compute cosine similarity between
    our input vector and the matrix of other vectors and sort by the most relevant
    documents.
  prefs: []
  type: TYPE_NORMAL
- en: Cosine similarities take on the range of [0, 1.0], with 0 being least similar
    and 1 being most similar. Since we’re using <math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math>
    vectors to compute the similarity between vectors, the frequency of a word’s occurrence
    is also taken into account. However, with a small corpus (set of documents) even
    “frequent” words may not appear frequently. In this example, “Sweden is best”
    is the most relevant text to our search query “Brazil is the best”. Since the
    query mentions Brazil, we might expect “I love Brazil. Brazil!” to be the most
    relevant; however, “Sweden is best” is the most similar due to the words “is”
    and “best”. As the number of documents we add to our corpus increases, less important
    words will be weighted less and have less effect on our cosine similarity calculation.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Cosine Similarity, Geeks for Geeks](https://oreil.ly/-5Odv)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Nvidia Gave Me a $15K Data Science Workstation—Here’s What I Did with It (building
    a Pubmed search engine in Python)](https://oreil.ly/pAxbR)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.12 Using a Sentiment Analysis Classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to classify the sentiment of some text to use as a feature or in downstream
    data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use the `transformers` library’s sentiment classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `transformers` library is an extremely popular library for NLP tasks and
    contains a number of easy-to-use APIs for training models or using pretrained
    ones. We’ll talk more about NLP and this library in [Chapter 22](ch22.xhtml#neural-networks-for-unstructured-data),
    but this example serves as a high-level introduction to the power of using pretrained
    classifiers in your machine learning pipelines to generate features, classify
    text, or analyze unstructured data.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Hugging Face Transformers Quick Tour](https://oreil.ly/7hT6W)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
