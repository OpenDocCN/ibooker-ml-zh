["```py\nimport numpy as np\n\na = np.array([\n    [1, 0, 0 ,1],\n    [1, 0, 0 ,0],\n    [0, 1, 1, 0],\n    [0, 1, 0, 0]]\n)\n\nu, s, v = np.linalg.svd(a, full_matrices=False)\n\n# Set the last two eigenvalues to 0.\ns[2:4] = 0\nprint(s)\nb = np.dot(u * s, v)\nprint(b)\n\n# These are the eigenvalues with the smallest two set to 0.\ns = [1.61803399 1.61803399 0.         0.        ]\n\n# This is the newly reconstructed matrix.\nb = [[1.17082039 0.         0.         0.7236068 ]\n [0.7236068  0.         0.         0.4472136 ]\n [0.         1.17082039 0.7236068  0.        ]\n [0.         0.7236068  0.4472136  0.        ]]\n```", "```py\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport pandas as pd\nimport os, json, wandb, math\n\nfrom jax import grad, jit\nfrom jax import random\nfrom jax.experimental import sparse\n\nkey = random.PRNGKey(0)\n\nwandb.login()\nrun = wandb.init(\n    # Set entity to specify your username or team name\n    entity=\"wandb-un\",\n    # Set the project where this run will be logged\n    project=\"jax-mf\",\n    # associate the runs to the right dataset\n    config={\n      \"dataset\": \"MF-Dataset\",\n    }\n)\n\n# note that we assume the dataset is a ratings table stored in wandb\nartifact = run.use_artifact('stored-dataset:latest')\nratings_artifact = artifact.download()\nratings_artifact_blob = json.load(\n    open(\n        os.path.join(\n            ratings_artifact,\n            'ratings.table.json'\n        )\n    )\n)\n\nratings_artifact_blob.keys()\n# ['_type', 'column_types', 'columns', 'data', 'ncols', 'nrows']\n\nratings = pd.DataFrame( # user_id, item_id, rating, unix_timestamp\n    data=ratings_artifact_blob['data'],\n    columns=ratings_artifact_blob['columns']\n)\n\ndef start_pipeline(df):\n    return df.copy()\n\ndef column_as_type(df, column: str, cast_type):\n    df[column] = df[column].astype(cast_type)\n    return df\n\ndef rename_column_value(df, target_column, prior_val, post_val):\n    df[target_column] = df[target_column].replace({prior_val: post_val})\n    return df\n\ndef split_dataframe(df, holdout_fraction=0.1):\n    \"\"\"Splits a DataFrame into training and test sets.\n Args:\n df: a dataframe.\n holdout_fraction: fraction of dataframe rows to use in the test set.\n Returns:\n train: dataframe for training\n test: dataframe for testing\n \"\"\"\n    test = df.sample(frac=holdout_fraction, replace=False)\n    train = df[~df.index.isin(test.index)]\n    return train, test\n\nall_rat = (ratings\n    .pipe(start_pipeline)\n    .pipe(column_as_type, column='user_id', cast_type=int)\n    .pipe(column_as_type, column='item_id', cast_type=int)\n)\n\ndef ratings_to_sparse_array(ratings_df, user_dim, item_dim):\n    indices = (np.array(ratings_df['user_id']), np.array(ratings_df['item_id']))\n    values = jnp.array(ratings_df['rating'])\n\n    return {\n        'indices': indices,\n        'values': values,\n        'shape': [user_dim, item_dim]\n    }\n\ndef random_normal(pr_key, shape, mu=0, sigma=1, ):\n    return (mu + sigma * random.normal(pr_key, shape=shape))\n\nx = random_normal(\n    pr_key = random.PRNGKey(1701),\n    shape=(10000,),\n    mu = 1.0,\n    sigma = 3.0,\n) # these hyperparameters are pretty meaningless\n\ndef sp_mse_loss(A, params):\n    U, V = params['users'], params['items']\n    rows, columns = A['indices']\n    estimator = -(U @ V.T)[(rows, columns)]\n    square_err = jax.tree_map(\n        lambda x: x**2,\n        A['values']+estimator\n    )\n    return jnp.mean(square_err)\n\nomse_loss = jit(sp_mse_loss)\n```", "```py\nclass CFModel(object):\n    \"\"\"Simple class that represents a collaborative filtering model\"\"\"\n    def __init__(\n          self,\n          metrics: dict,\n          embeddings: dict,\n          ground_truth: dict,\n          embeddings_parameters: dict,\n          prng_key=None\n    ):\n        \"\"\"Initializes a CFModel.\n Args:\n \"\"\"\n        self._metrics = metrics\n        self._embeddings = embeddings\n        self._ground_truth = ground_truth\n        self._embeddings_parameters = embeddings_parameters\n\n        if prng_key is None:\n            prng_key = random.PRNGKey(0)\n        self._prng_key = prng_key\n\n    @property\n    def embeddings(self):\n        \"\"\"The embeddings dictionary.\"\"\"\n        return self._embeddings\n\n    @embeddings.setter\n    def embeddings(self, value):\n        self._embeddings = value\n\n    @property\n    def metrics(self):\n        \"\"\"The metrics dictionary.\"\"\"\n        return self._metrics\n\n    @property\n    def ground_truth(self):\n        \"\"\"The train/test dictionary.\"\"\"\n        return self._ground_truth\n\n    def reset_embeddings(self):\n        \"\"\"Clear out embeddings state.\"\"\"\n\n        prng_key1, prng_key2 = random.split(self._prng_key, 2)\n\n        self._embeddings['users'] = random_normal(\n            prng_key1,\n            [\n              self._embeddings_parameters['user_dim'],\n              self._embeddings_parameters['embedding_dim']\n            ],\n            mu=0,\n            sigma=self._embeddings_parameters['init_stddev'],\n        )\n        self._embeddings['items'] = random_normal(\n            prng_key2,\n            [\n              self._embeddings_parameters['item_dim'],\n              self._embeddings_parameters['embedding_dim']],\n            mu=0,\n            sigma=self._embeddings_parameters['init_stddev'],\n        )\n\ndef model_constructor(\n    ratings_df,\n    user_dim,\n    item_dim,\n    embedding_dim=3,\n    init_stddev=1.,\n    holdout_fraction=0.2,\n    prng_key=None,\n    train_set=None,\n    test_set=None,\n):\n    if prng_key is None:\n      prng_key = random.PRNGKey(0)\n\n    prng_key1, prng_key2 = random.split(prng_key, 2)\n\n    if (train_set is None) and (test_set is None):\n        train, test = (ratings_df\n            .pipe(start_pipeline)\n            .pipe(split_dataframe, holdout_fraction=holdout_fraction)\n        )\n\n        A_train = (train\n            .pipe(start_pipeline)\n            .pipe(ratings_to_sparse_array, user_dim=user_dim, item_dim=item_dim)\n        )\n        A_test = (test\n            .pipe(start_pipeline)\n            .pipe(ratings_to_sparse_array, user_dim=user_dim, item_dim=item_dim)\n        )\n    elif (train_set is None) ^ (test_set is None):\n        raise('Must send train and test if sending one')\n    else:\n        A_train, A_test = train_set, test_set\n\n    U = random_normal(\n        prng_key1,\n        [user_dim, embedding_dim],\n        mu=0,\n        sigma=init_stddev,\n    )\n    V = random_normal(\n        prng_key2,\n        [item_dim, embedding_dim],\n        mu=0,\n        sigma=init_stddev,\n    )\n\n    train_loss = omse_loss(A_train, {'users': U, 'items': V})\n    test_loss = omse_loss(A_test, {'users': U, 'items': V})\n\n    metrics = {\n        'train_error': train_loss,\n        'test_error': test_loss\n    }\n    embeddings = {'users': U, 'items': V}\n    ground_truth = {\n        \"A_train\": A_train,\n        \"A_test\": A_test\n    }\n    return CFModel(\n        metrics=metrics,\n        embeddings=embeddings,\n        ground_truth=ground_truth,\n        embeddings_parameters={\n            'user_dim': user_dim,\n            'item_dim': item_dim,\n            'embedding_dim': embedding_dim,\n            'init_stddev': init_stddev,\n        },\n        prng_key=prng_key,\n    )\n\nmf_model = model_constructor(all_rat, user_count, item_count)\n```", "```py\ndef train():\n  run_config = { # These will be hyperparameters we will tune via wandb\n      'emb_dim': 10, # Latent dimension\n      'prior_std': 0.1, # Std dev around 0 for weights initialization\n      'alpha': 1.0, # Learning rate\n      'steps': 1500, # Number of training steps\n  }\n\n  with wandb.init() as run:\n    run_config.update(run.config)\n    model_object = model_constructor(\n        ratings_df=all_rat,\n        user_dim=user_count,\n        item_dim=item_count,\n        embedding_dim=run_config['emb_dim'],\n        init_stddev=run_config['prior_std'],\n        prng_key=random.PRNGKey(0),\n        train_set=mf_model.ground_truth['A_train'],\n        test_set=mf_model.ground_truth['A_test']\n    )\n    model_object.reset_embeddings() # Ensure we are starting from priors\n    alpha, steps = run_config['alpha'], run_config['steps']\n    print(run_config)\n    grad_fn = jax.value_and_grad(omse_loss, 1)\n    for i in range(steps):\n      # We perform one gradient update\n      loss_val, grads = grad_fn(\n          model_object.ground_truth['A_train'],\n          model_object.embeddings\n      )\n      model_object.embeddings = jax.tree_multimap(\n          lambda p, g: p - alpha * g,\n          # Basic update rule; JAX handles broadcasting for us\n          model_object.embeddings,\n          grads\n      )\n      if i % 1000 == 0: # Most output in wandb; little bit of logging\n        print(f'Loss step {i}: ', loss_val)\n        print(f\"\"\"Test loss: {\n            omse_loss(\n                model_object.ground_truth['A_train'],\n                model_object.embeddings\n            )}\"\"\")\n\n      wandb.log({\n          \"Train omse\": loss_val,\n          \"Test omse\": omse_loss(\n              model_object.ground_truth['A_test'],\n              model_object.embeddings\n           )\n      })\n```", "```py\nsweep_config = {\n    \"name\" : \"mf-test-sweep\",\n    \"method\" : \"random\",\n    \"parameters\" : {\n        \"steps\" : {\n            \"min\": 1000,\n            \"max\": 3000,\n        },\n        \"alpha\" :{\n            \"min\": 0.6,\n            \"max\": 1.75\n        },\n        \"emb_dim\" :{\n            \"min\": 3,\n            \"max\": 10\n        },\n        \"prior_std\" :{\n            \"min\": .5,\n            \"max\": 2.0\n        },\n    },\n    \"metric\" : {\n        'name': 'Test omse',\n        'goal': 'minimize'\n    }\n}\n\nsweep_id = wandb.sweep(sweep_config, project=\"jax-mf\", entity=\"wandb-un\")\n\nwandb.init()\ntrain()\n\ncount = 50\nwandb.agent(sweep_id, function=train, count=count)\n```", "```py\ndef ell_two_regularization_term(params, dimensions):\n    U, V = params['users'], params['items']\n    N, M = dimensions['users'], dimensions['items']\n    user_sq = jnp.multiply(U, U)\n    item_sq = jnp.multiply(V, V)\n    return (jnp.sum(user_sq)/N + jnp.sum(item_sq)/M)\n\nl2_loss = jit(ell_two_regularization_term)\n\ndef gramian_regularization_term(params, dimensions):\n    U, V = params['users'], params['items']\n    N, M = dimensions['users'], dimensions['items']\n    gr_user = U.T @ U\n    gr_item = V.T @ V\n    gr_square = jnp.multiply(gr_user, gr_item)\n    return (jnp.sum(gr_square)/(N*M))\n\ngr_loss = jit(gramian_regularization_term)\n\ndef regularized_omse(A, params, dimensions, hyperparams):\n  lr, lg = hyperparams['ell_2'], hyperparams['gram']\n  losses = {\n      'omse': sp_mse_loss(A, params),\n      'l2_loss': l2_loss(params, dimensions),\n      'gr_loss': gr_loss(params, dimensions),\n  }\n  losses.update({\n      'total_loss': losses['omse'] + lr*losses['l2_loss'] + lg*losses['gr_loss']\n  })\n  return losses['total_loss'], losses\n\nreg_loss_observed = jit(regularized_omse)\n```", "```py\ndef lr_decay(\n    step_num,\n    base_learning_rate,\n    decay_pct = 0.5,\n    period_length = 100.0\n):\n    return base_learning_rate * math.pow(\n        decay_pct,\n        math.floor((1+step_num)/period_length)\n    )\n```", "```py\ndef train_with_reg_loss():\n    run_config = { # These will be hyperparameters we will tune via wandb\n        'emb_dim': None,\n        'prior_std': None,\n        'alpha': None, # Learning rate\n        'steps': None,\n        'ell_2': 1, #l2 regularization penalization weight\n        'gram': 1, #gramian regularization penalization weight\n        'decay_pct': 0.5,\n        'period_length': 100.0\n    }\n\n    with wandb.init() as run:\n        run_config.update(run.config)\n        model_object = model_constructor(\n            ratings_df=all_rat,\n            user_dim=942,\n            item_dim=1681,\n            embedding_dim=run_config['emb_dim'],\n            init_stddev=run_config['prior_std'],\n            prng_key=random.PRNGKey(0),\n            train_set=mf_model.ground_truth['A_train'],\n            test_set=mf_model.ground_truth['A_test']\n        )\n        model_object.reset_embeddings() # Ensure we start from priors\n\n        alpha, steps = run_config['alpha'], run_config['steps']\n        print(run_config)\n\n        grad_fn = jax.value_and_grad(\n            reg_loss_observed,\n            1,\n            has_aux=True\n        ) # Tell JAX to expect an aux dict as output\n\n        for i in range(steps):\n            (total_loss_val, loss_dict), grads = grad_fn(\n                model_object.ground_truth['A_train'],\n                model_object.embeddings,\n                dimensions={'users': user_count, 'items': item_count},\n                hyperparams={\n                    'ell_2': run_config['ell_2'],\n                    'gram': run_config['gram']\n                } # JAX carries our loss dict along for logging\n            )\n\n            model_object.embeddings = jax.tree_multimap(\n                lambda p, g: p - lr_decay(\n                    i,\n                    alpha,\n                    run_config['decay_pct'],\n                    run_config['period_length']\n                ) * g, # update with decay\n                model_object.embeddings,\n                grads\n            )\n            if i % 1000 == 0:\n                print(f'Loss step {i}:')\n                print(loss_dict)\n                print(f\"\"\"Test loss: {\n                    omse_loss(model_object.ground_truth['A_test'],\n                    model_object.embeddings)}\"\"\")\n\n            loss_dict.update( # wandb takes the entire loss dictionary\n                {\n                    \"Test omse\": omse_loss(\n                        model_object.ground_truth['A_test'],\n                        model_object.embeddings\n                    ),\n                    \"learning_rate\": lr_decay(i, alpha),\n                }\n            )\n            wandb.log(loss_dict)\n\n sweep_config = {\n    \"name\" : \"mf-HPO-with-reg\",\n    \"method\" : \"random\",\n    \"parameters\" : {\n      \"steps\": {\n        \"value\": 2000\n      },\n      \"alpha\" :{\n        \"min\": 0.6,\n        \"max\": 2.25\n      },\n      \"emb_dim\" :{\n        \"min\": 15,\n        \"max\": 80\n      },\n      \"prior_std\" :{\n        \"min\": .5,\n        \"max\": 2.0\n      },\n      \"ell_2\" :{\n        \"min\": .05,\n        \"max\": 0.5\n      },\n      \"gram\" :{\n        \"min\": .1,\n        \"max\": .75\n      },\n      \"decay_pct\" :{\n        \"min\": .2,\n        \"max\": .8\n      },\n      \"period_length\" :{\n        \"min\": 50,\n        \"max\": 500\n      }\n    },\n    \"metric\" : {\n      'name': 'Test omse',\n      'goal': 'minimize'\n    }\n  }\n\n  sweep_id = wandb.sweep(\n      sweep_config,\n      project=\"jax-mf\",\n      entity=\"wandb-un\"\n  )\n\nrun_config = { # These will be hyperparameters we will tune via wandb\n      'emb_dim': 10, # Latent dimension\n      'prior_std': 0.1,\n      'alpha': 1.0, # Learning rate\n      'steps': 1000, # Number of training steps\n      'ell_2': 1, #l2 regularization penalization weight\n      'gram': 1, #gramian regularization penalization weight\n      'decay_pct': 0.5,\n      'period_length': 100.0\n  }\n\ntrain_with_reg_loss()\n```", "```py\ndef sparse_array_concatenate(sparse_array_iterable):\n    return {\n        'indices': tuple(\n            map(\n                jnp.concatenate,\n                zip(*(x['indices'] for x in sparse_array_iterable)))\n            ),\n        'values': jnp.concatenate(\n            [x['values'] for x in sparse_array_iterable]\n        ),\n    }\n\nclass jax_df_Kfold(object):\n    \"\"\"Simple class that handles Kfold\n splitting of a matrix as a dataframe and stores as sparse jarrays\"\"\"\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        user_dim: int,\n        item_dim: int,\n        k: int = 5,\n        prng_key=random.PRNGKey(0)\n    ):\n        self._df = df\n        self._num_folds = k\n        self._split_idxes = jnp.array_split(\n            random.permutation(\n                prng_key,\n                df.index.to_numpy(),\n                axis=0,\n                independent=True\n            ),\n            self._num_folds\n        )\n\n        self._fold_arrays = dict()\n\n        for fold_index in range(self._num_folds):\n        # let's create sparse jax arrays for each fold piece\n            self._fold_arrays[fold_index] = (\n                self._df[\n                    self._df.index.isin(self._split_idxes[fold_index])\n                ].pipe(start_pipeline)\n                .pipe(\n                    ratings_to_sparse_array,\n                    user_dim=user_dim,\n                    item_dim=item_dim\n                )\n            )\n\n    def get_fold(self, fold_index: int):\n        assert(self._num_folds > fold_index)\n        test = self._fold_arrays[fold_index]\n        train = sparse_array_concatenate(\n            [v for k,v in self._fold_arrays.items() if k != fold_index]\n        )\n        return train, test\n```", "```py\nfor j in num_folds:\n  train, test = folder.get_fold(j)\n  model_object_dict[j] = model_constructor(\n          ratings_df=all_rat,\n          user_dim=user_count,\n          item_dim=item_count,\n          embedding_dim=run_config['emb_dim'],\n          init_stddev=run_config['prior_std'],\n          prng_key=random.PRNGKey(0),\n          train_set=train,\n          test_set=test\n      )\n```", "```py\nfor i in range(steps):\n    loss_dict = {\"learning_rate\": step_decay(i)}\n    for j, M in model_object_dict.items():\n        (total_loss_val, fold_loss_dict), grads = grad_fn(\n          M.ground_truth['A_train'],\n          M.embeddings,\n          dimensions={'users': 942, 'items': 1681},\n          hyperparams={'ell_2': run_config['ell_2'], 'gram': run_config['gram']}\n        )\n\n        M.embeddings = jax.tree_multimap(\n            lambda p, g: p - step_decay(i) * g,\n            M.embeddings,\n            grads\n        )\n```", "```py\n        fold_loss_dict = {f'{k}_fold-{j}': v for k, v in fold_loss_dict.items()}\n        fold_loss_dict.update(\n                  {\n                      f\"Test omse_fold-{j}\": omse_loss(\n                        M.ground_truth['A_test'],\n                        M.embeddings\n                      ),\n                  }\n              )\n\n        loss_dict.update(fold_loss_dict)\n\n    loss_dict.update({\n      \"Test omse_mean\": jnp.mean(\n        [v for k,v in loss_dict.items() if k.startswith('Test omse_fold-')]\n      )\n    })\n    wandb.log(loss_dict)\n```", "```py\ndef train_with_reg_loss_CV():\n    run_config = { # These will be hyperparameters we will tune via wandb\n        'emb_dim': None, # Latent dimension\n        'prior_std': None,\n        # Standard deviation around 0 that our weights are initialized to\n        'alpha': None, # Learning rate\n        'steps': None, # Number of training steps\n        'num_folds': None, # Number of CV Folds\n        'ell_2': 1, #hyperparameter for l2 regularization penalization weight\n        'gram': 1, #hyperparameter for gramian regularization penalization weight\n    }\n\n    with wandb.init() as run:\n        run_config.update(run.config) # This is how the wandb agent passes params\n        model_object_dict = dict()\n\n        for j in range(run_config['num_folds']):\n            train, test = folder.get_fold(j)\n            model_object_dict[j] = model_constructor(\n                ratings_df=all_rat,\n                user_dim=942,\n                item_dim=1681,\n                embedding_dim=run_config['emb_dim'],\n                init_stddev=run_config['prior_std'],\n                prng_key=random.PRNGKey(0),\n                train_set=train,\n                test_set=test\n            )\n            model_object_dict[j].reset_embeddings()\n            # Ensure we are starting from priors\n\n        alpha, steps = run_config['alpha'], run_config['steps']\n        print(run_config)\n\n        grad_fn = jax.value_and_grad(reg_loss_observed, 1, has_aux=True)\n        # Tell JAX to expect an aux dict as output\n\n        for i in range(steps):\n            loss_dict = {\n              \"learning_rate\": lr_decay(\n                i,\n                alpha,\n                decay_pct=.75,\n                period_length=250\n              )\n            }\n            for j, M in model_object_dict.items():\n            # Iterate through folds\n\n                (total_loss_val, fold_loss_dict), grads = grad_fn(\n                # compute gradients for one fold\n                    M.ground_truth['A_train'],\n                    M.embeddings,\n                    dimensions={'users': 942, 'items': 1681},\n                    hyperparams={\n                      'ell_2': run_config['ell_2'],\n                      'gram': run_config['gram']\n                    }\n                )\n\n                M.embeddings = jax.tree_multimap(\n                # update weights for one fold\n                    lambda p, g: p - lr_decay(\n                      i,\n                      alpha,\n                      decay_pct=.75,\n                      period_length=250\n                    ) * g,\n                    M.embeddings,\n                    grads\n                )\n\n                fold_loss_dict = {\n                  f'{k}_fold-{j}':\n                  v for k, v in fold_loss_dict.items()\n                }\n                fold_loss_dict.update( # loss calculation within fold\n                    {\n                        f\"Test omse_fold-{j}\": omse_loss(\n                          M.ground_truth['A_test'],\n                          M.embeddings\n                        ),\n                    }\n                )\n\n                loss_dict.update(fold_loss_dict)\n\n            loss_dict.update({ # average loss over all folds\n                \"Test omse_mean\": np.mean(\n                    [v for k,v in loss_dict.items()\n                    if k.startswith('Test omse_fold-')]\n                ),\n                \"test omse_max\": np.max(\n                    [v for k,v in loss_dict.items()\n                    if k.startswith('Test omse_fold-')]\n                ),\n                \"test omse_min\": np.min(\n                    [v for k,v in loss_dict.items()\n                    if k.startswith('Test omse_fold-')]\n                )\n            })\n            wandb.log(loss_dict)\n\n            if i % 1000 == 0:\n                print(f'Loss step {i}:')\n                print(loss_dict)\n```", "```py\nsweep_config = {\n    \"name\" : \"mf-HPO-CV\",\n    \"method\" : \"random\",\n    \"parameters\" : {\n      \"steps\": {\n        \"value\": 2000\n      },\n      \"num_folds\": {\n        \"value\": 5\n      },\n      \"alpha\" :{\n        \"min\": 2.0,\n        \"max\": 3.0\n      },\n      \"emb_dim\" :{\n        \"min\": 15,\n        \"max\": 70\n      },\n      \"prior_std\" :{\n        \"min\": .75,\n        \"max\": 1.0\n      },\n      \"ell_2\" :{\n        \"min\": .05,\n        \"max\": 0.5\n      },\n      \"gram\" :{\n        \"min\": .1,\n        \"max\": .6\n      },\n    },\n    \"metric\" : {\n      'name': 'Test omse_mean',\n      'goal': 'minimize'\n    }\n  }\n\n  sweep_id = wandb.sweep(sweep_config, project=\"jax-mf\", entity=\"wandb-un\")\n\nwandb.agent(sweep_id, function=train_with_reg_loss_CV, count=count)\n```", "```py\ndef prequential_validation_set(df, holdout_perc=0.1):\n    '''\n We utilize rejection sampling.\n\n Assign a probability to all observations, if they lie below the\n sample percentage AND they're the most recent still in the set, include.\n\n Otherwise return them and repeat.\n Each time, take no more than the remaining necessary to fill the count.\n '''\n    count = int(len(df)*holdout_perc)\n    sample = []\n    while count >0:\n      df['p'] = np.random.rand(len(df),1) #generate probabilities\n      x = list(\n          df.loc[~df.index.isin(sample)] # exclude already selected\n          .sort_values(['unix_timestamp'], ascending=False)\n          .groupby('user_id').head(1) # only allow the first in each group\n          .query(\"p < @holdout_perc\").index # grab the indices\n      )\n      rnd.shuffle(x) # ensure our previous sorting doesn't bias the users subset\n      sample += x[:count] # add observations up to the remaining needed\n      count -= len(x[:count]) # decrement the remaining needed\n\n    df.drop(columns=['p'], inplace=True)\n\n    test = df.iloc[sample]\n    train = df[~df.index.isin(test.index)]\n    return train, test\n```", "```py\nimport jax\nimport jax.numpy as jnp\n\ndef power_iteration(a: jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Returns an eigenvector of the matrix a.\n Args:\n a: a n x m matrix\n \"\"\"\n  key = jax.random.PRNGKey(0)\n  x = jax.random.normal(key, shape=(a.shape[1], 1))\n  for i in range(100):\n    x = a @ x\n    x = x / jnp.linalg.norm(x)\n  return x.T\n\nkey = jax.random.PRNGKey(123)\nA = jax.random.normal(key, shape=[4, 4])\nprint(A)\n[[ 0.52830553  0.3722206  -1.2219944  -0.10314374]\n [ 1.4722222   0.47889313 -1.2940298   1.0449569 ]\n [ 0.23724185  0.3545859  -0.172465   -1.8011322 ]\n [ 0.4864215   0.08039388 -1.2540827   0.72071517]]\nS, _, _ = jnp.linalg.svd(A)\nprint(S)\n[[-0.375782    0.40269807  0.44086716 -0.70870167]\n [-0.753597    0.0482972  -0.65527284  0.01940039]\n [ 0.2040088   0.91405433 -0.15798494  0.31293103]\n [-0.49925917 -0.00250015  0.5927009   0.6320123 ]]\nx1 = power_iteration(A)\nprint(x1.T)\n[[-0.35423845]\n [-0.8332922 ]\n [ 0.16189891]\n [-0.39233655]]\n```", "```py\nimport jax\nimport jax.numpy as jnp\n\ndef recommend_items(eigenvectors: jnp.ndarray, user:jnp.ndarray) -> jnp.ndarray:\n  \"\"\"Returns an ordered list of recommend items for the user.\n Args:\n eigenvectors: a nxm eigenvector matrix\n user: a user vector of size m.\n \"\"\"\n  score_eigenvectors = jnp.matmul(eigenvectors.T, user)\n  which_eigenvector = jnp.argmax(score_eigenvectors)\n  closest_eigenvector = eigenvectors.T[which_eigenvector]\n  scores, items = jax.lax.top_k(closest_eigenvector, 3)\n  return scores, items\n\nS = jnp.array(\n[[-0.375782,    0.40269807],\n [-0.753597,    0.0482972],\n [ 0.2040088,   0.91405433],\n [-0.49925917, -0.00250015]])\nu = jnp.array([-1, -1, 0, 0]).reshape(4, 1)\nscores, items = recommend_items(S, u)\nprint(scores)\n[ 0.2040088  -0.375782   -0.49925917]\nprint(items)\n[2 0 3]\n```"]