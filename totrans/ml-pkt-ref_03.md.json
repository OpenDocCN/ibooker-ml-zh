["```py\n>>> import matplotlib.pyplot as plt\n>>> import pandas as pd\n>>> from sklearn import (\n...     ensemble,\n...     preprocessing,\n...     tree,\n... )\n>>> from sklearn.metrics import (\n...     auc,\n...     confusion_matrix,\n...     roc_auc_score,\n...     roc_curve,\n... )\n>>> from sklearn.model_selection import (\n...     train_test_split,\n...     StratifiedKFold,\n... )\n>>> from yellowbrick.classifier import (\n...     ConfusionMatrix,\n...     ROCAUC,\n... )\n>>> from yellowbrick.model_selection import (\n...     LearningCurve,\n... )\n```", "```py\nfrom pandas import *\n```", "```py\ny = f(X)\n```", "```py\n>>> url = (\n...     \"http://biostat.mc.vanderbilt.edu/\"\n...     \"wiki/pub/Main/DataSets/titanic3.xls\"\n... )\n>>> df = pd.read_excel(url)\n>>> orig_df = df\n```", "```py\n>>> df.dtypes\npclass         int64\nsurvived       int64\nname          object\nsex           object\nage          float64\nsibsp          int64\nparch          int64\nticket        object\nfare         float64\ncabin         object\nembarked      object\nboat          object\nbody         float64\nhome.dest     object\ndtype: object\n```", "```py\n>>> import pandas_profiling\n>>> pandas_profiling.ProfileReport(df)\n```", "```py\n>>> df.shape\n(1309, 14)\n```", "```py\n>>> df.describe().iloc[:, :2]\n pclass     survived\ncount  1309.000000  1309.000000\nmean      2.294882     0.381971\nstd       0.837836     0.486055\nmin       1.000000     0.000000\n25%       2.000000     0.000000\n50%       3.000000     0.000000\n75%       3.000000     1.000000\nmax       3.000000     1.000000\n```", "```py\n>>> df.isnull().sum()\npclass          0\nsurvived        0\nname            0\nsex             0\nage           263\nsibsp           0\nparch           0\nticket          0\nfare            1\ncabin        1014\nembarked        2\nboat          823\nbody         1188\nhome.dest     564\ndtype: int64\n```", "```py\n>>> df.isnull().sum(axis=1).loc[:10]\n0    1\n1    1\n2    2\n3    1\n4    2\n5    1\n6    1\n7    2\n8    1\n9    2\ndtype: int64\n```", "```py\n>>> mask = df.isnull().any(axis=1)\n\n>>> mask.head()  # rows\n0    True\n1    True\n2    True\n3    True\n4    True\ndtype: bool\n\n>>> df[mask].body.head()\n0      NaN\n1      NaN\n2      NaN\n3    135.0\n4      NaN\nName: body, dtype: float64\n```", "```py\n>>> df.sex.value_counts(dropna=False)\nmale      843\nfemale    466\nName: sex, dtype: int64\n```", "```py\n>>> df.embarked.value_counts(dropna=False)\nS      914\nC      270\nQ      123\nNaN      2\nName: embarked, dtype: int64\n```", "```py\n>>> name = df.name\n>>> name.head(3)\n0      Allen, Miss. Elisabeth Walton\n1     Allison, Master. Hudson Trevor\n2       Allison, Miss. Helen Loraine\nName: name, dtype: object\n\n>>> df = df.drop(\n...     columns=[\n...         \"name\",\n...         \"ticket\",\n...         \"home.dest\",\n...         \"boat\",\n...         \"body\",\n...         \"cabin\",\n...     ]\n... )\n```", "```py\n>>> df = pd.get_dummies(df)\n\n>>> df.columns\nIndex(['pclass', 'survived', 'age', 'sibsp',\n 'parch', 'fare', 'sex_female', 'sex_male',\n 'embarked_C', 'embarked_Q', 'embarked_S'],\n dtype='object')\n```", "```py\n>>> df = df.drop(columns=\"sex_male\")\n```", "```py\n>>> df = pd.get_dummies(df, drop_first=True)\n\n>>> df.columns\nIndex(['pclass', 'survived', 'age', 'sibsp',\n 'parch', 'fare', 'sex_male',\n 'embarked_Q', 'embarked_S'],\n dtype='object')\n```", "```py\n>>> y = df.survived\n>>> X = df.drop(columns=\"survived\")\n```", "```py\n>>> import janitor as jn\n>>> X, y = jn.get_features_targets(\n...     df, target_columns=\"survived\"\n... )\n```", "```py\n>>> X_train, X_test, y_train, y_test = model_selection.train_test_split(\n...     X, y, test_size=0.3, random_state=42\n... )\n```", "```py\n>>> from sklearn.experimental import (\n...     enable_iterative_imputer,\n... )\n>>> from sklearn import impute\n>>> num_cols = [\n...     \"pclass\",\n...     \"age\",\n...     \"sibsp\",\n...     \"parch\",\n...     \"fare\",\n...     \"sex_female\",\n... ]\n\n>>> imputer = impute.IterativeImputer()\n>>> imputed = imputer.fit_transform(\n...     X_train[num_cols]\n... )\n>>> X_train.loc[:, num_cols] = imputed\n>>> imputed = imputer.transform(X_test[num_cols])\n>>> X_test.loc[:, num_cols] = imputed\n```", "```py\n>>> meds = X_train.median()\n>>> X_train = X_train.fillna(meds)\n>>> X_test = X_test.fillna(meds)\n```", "```py\n>>> cols = \"pclass,age,sibsp,fare\".split(\",\")\n>>> sca = preprocessing.StandardScaler()\n>>> X_train = sca.fit_transform(X_train)\n>>> X_train = pd.DataFrame(X_train, columns=cols)\n>>> X_test = sca.transform(X_test)\n>>> X_test = pd.DataFrame(X_test, columns=cols)\n```", "```py\n>>> def tweak_titanic(df):\n...     df = df.drop(\n...         columns=[\n...             \"name\",\n...             \"ticket\",\n...             \"home.dest\",\n...             \"boat\",\n...             \"body\",\n...             \"cabin\",\n...         ]\n...     ).pipe(pd.get_dummies, drop_first=True)\n...     return df\n\n>>> def get_train_test_X_y(\n...     df, y_col, size=0.3, std_cols=None\n... ):\n...     y = df[y_col]\n...     X = df.drop(columns=y_col)\n...     X_train, X_test, y_train, y_test = model_selection.train_test_split(\n...         X, y, test_size=size, random_state=42\n...     )\n...     cols = X.columns\n...     num_cols = [\n...         \"pclass\",\n...         \"age\",\n...         \"sibsp\",\n...         \"parch\",\n...         \"fare\",\n...     ]\n...     fi = impute.IterativeImputer()\n...     X_train.loc[\n...         :, num_cols\n...     ] = fi.fit_transform(X_train[num_cols])\n...     X_test.loc[:, num_cols] = fi.transform(\n...         X_test[num_cols]\n...     )\n...\n...     if std_cols:\n...         std = preprocessing.StandardScaler()\n...         X_train.loc[\n...             :, std_cols\n...         ] = std.fit_transform(\n...             X_train[std_cols]\n...         )\n...         X_test.loc[\n...             :, std_cols\n...         ] = std.transform(X_test[std_cols])\n...\n...     return X_train, X_test, y_train, y_test\n\n>>> ti_df = tweak_titanic(orig_df)\n>>> std_cols = \"pclass,age,sibsp,fare\".split(\",\")\n>>> X_train, X_test, y_train, y_test = get_train_test_X_y(\n...     ti_df, \"survived\", std_cols=std_cols\n... )\n```", "```py\n>>> from sklearn.dummy import DummyClassifier\n>>> bm = DummyClassifier()\n>>> bm.fit(X_train, y_train)\n>>> bm.score(X_test, y_test)  # accuracy\n0.5292620865139949\n\n>>> from sklearn import metrics\n>>> metrics.precision_score(\n...     y_test, bm.predict(X_test)\n... )\n0.4027777777777778\n```", "```py\n>>> X = pd.concat([X_train, X_test])\n>>> y = pd.concat([y_train, y_test])\n>>> from sklearn import model_selection\n>>> from sklearn.dummy import DummyClassifier\n>>> from sklearn.linear_model import (\n...     LogisticRegression,\n... )\n>>> from sklearn.tree import DecisionTreeClassifier\n>>> from sklearn.neighbors import (\n...     KNeighborsClassifier,\n... )\n>>> from sklearn.naive_bayes import GaussianNB\n>>> from sklearn.svm import SVC\n>>> from sklearn.ensemble import (\n...     RandomForestClassifier,\n... )\n>>> import xgboost\n\n>>> for model in [\n...     DummyClassifier,\n...     LogisticRegression,\n...     DecisionTreeClassifier,\n...     KNeighborsClassifier,\n...     GaussianNB,\n...     SVC,\n...     RandomForestClassifier,\n...     xgboost.XGBClassifier,\n... ]:\n...     cls = model()\n...     kfold = model_selection.KFold(\n...         n_splits=10, random_state=42\n...     )\n...     s = model_selection.cross_val_score(\n...         cls, X, y, scoring=\"roc_auc\", cv=kfold\n...     )\n...     print(\n...         f\"{model.__name__:22}  AUC: \"\n...         f\"{s.mean():.3f} STD: {s.std():.2f}\"\n...     )\nDummyClassifier         AUC: 0.511  STD: 0.04\nLogisticRegression      AUC: 0.843  STD: 0.03\nDecisionTreeClassifier  AUC: 0.761  STD: 0.03\nKNeighborsClassifier    AUC: 0.829  STD: 0.05\nGaussianNB              AUC: 0.818  STD: 0.04\nSVC                     AUC: 0.838  STD: 0.05\nRandomForestClassifier  AUC: 0.829  STD: 0.04\nXGBClassifier           AUC: 0.864  STD: 0.04\n```", "```py\n>>> from mlxtend.classifier import (\n...     StackingClassifier,\n... )\n>>> clfs = [\n...     x()\n...     for x in [\n...         LogisticRegression,\n...         DecisionTreeClassifier,\n...         KNeighborsClassifier,\n...         GaussianNB,\n...         SVC,\n...         RandomForestClassifier,\n...     ]\n... ]\n>>> stack = StackingClassifier(\n...     classifiers=clfs,\n...     meta_classifier=LogisticRegression(),\n... )\n>>> kfold = model_selection.KFold(\n...     n_splits=10, random_state=42\n... )\n>>> s = model_selection.cross_val_score(\n...     stack, X, y, scoring=\"roc_auc\", cv=kfold\n... )\n>>> print(\n...     f\"{stack.__class__.__name__}  \"\n...     f\"AUC: {s.mean():.3f}  STD: {s.std():.2f}\"\n... )\nStackingClassifier  AUC: 0.804  STD: 0.06\n```", "```py\n>>> rf = ensemble.RandomForestClassifier(\n...     n_estimators=100, random_state=42\n... )\n>>> rf.fit(X_train, y_train)\nRandomForestClassifier(bootstrap=True,\n class_weight=None, criterion='gini',\n max_depth=None, max_features='auto',\n max_leaf_nodes=None,\n min_impurity_decrease=0.0,\n min_impurity_split=None,\n min_samples_leaf=1, min_samples_split=2,\n min_weight_fraction_leaf=0.0, n_estimators=10,\n n_jobs=1, oob_score=False, random_state=42,\n verbose=0, warm_start=False)\n```", "```py\n>>> rf.score(X_test, y_test)\n0.7964376590330788\n```", "```py\n>>> metrics.precision_score(\n...     y_test, rf.predict(X_test)\n... )\n0.8013698630136986\n```", "```py\n>>> for col, val in sorted(\n...     zip(\n...         X_train.columns,\n...         rf.feature_importances_,\n...     ),\n...     key=lambda x: x[1],\n...     reverse=True,\n... )[:5]:\n...     print(f\"{col:10}{val:10.3f}\")\nage            0.277\nfare           0.265\nsex_female     0.240\npclass         0.092\nsibsp          0.048\n```", "```py\n>>> rf4 = ensemble.RandomForestClassifier()\n>>> params = {\n...     \"max_features\": [0.4, \"auto\"],\n...     \"n_estimators\": [15, 200],\n...     \"min_samples_leaf\": [1, 0.1],\n...     \"random_state\": [42],\n... }\n>>> cv = model_selection.GridSearchCV(\n...     rf4, params, n_jobs=-1\n... ).fit(X_train, y_train)\n>>> print(cv.best_params_)\n{'max_features': 'auto', 'min_samples_leaf': 0.1,\n 'n_estimators': 200, 'random_state': 42}\n\n>>> rf5 = ensemble.RandomForestClassifier(\n...     **{\n...         \"max_features\": \"auto\",\n...         \"min_samples_leaf\": 0.1,\n...         \"n_estimators\": 200,\n...         \"random_state\": 42,\n...     }\n... )\n>>> rf5.fit(X_train, y_train)\n>>> rf5.score(X_test, y_test)\n0.7888040712468194\n```", "```py\n>>> from sklearn.metrics import confusion_matrix\n>>> y_pred = rf5.predict(X_test)\n>>> confusion_matrix(y_test, y_pred)\narray([[196,  28],\n [ 55, 114]])\n\n>>> mapping = {0: \"died\", 1: \"survived\"}\n>>> fig, ax = plt.subplots(figsize=(6, 6))\n>>> cm_viz = ConfusionMatrix(\n...     rf5,\n...     classes=[\"died\", \"survived\"],\n...     label_encoder=mapping,\n... )\n>>> cm_viz.score(X_test, y_test)\n>>> cm_viz.poof()\n>>> fig.savefig(\n...     \"images/mlpr_0304.png\",\n...     dpi=300,\n...     bbox_inches=\"tight\",\n... )\n```", "```py\n>>> y_pred = rf5.predict(X_test)\n>>> roc_auc_score(y_test, y_pred)\n0.7747781065088757\n```", "```py\n>>> fig, ax = plt.subplots(figsize=(6, 6))\n>>> roc_viz = ROCAUC(rf5)\n>>> roc_viz.score(X_test, y_test)\n0.8279691030696217\n>>> roc_viz.poof()\n>>> fig.savefig(\"images/mlpr_0305.png\")\n```", "```py\n>>> import numpy as np\n>>> fig, ax = plt.subplots(figsize=(6, 4))\n>>> cv = StratifiedKFold(12)\n>>> sizes = np.linspace(0.3, 1.0, 10)\n>>> lc_viz = LearningCurve(\n...     rf5,\n...     cv=cv,\n...     train_sizes=sizes,\n...     scoring=\"f1_weighted\",\n...     n_jobs=4,\n...     ax=ax,\n... )\n>>> lc_viz.fit(X, y)\n>>> lc_viz.poof()\n>>> fig.savefig(\"images/mlpr_0306.png\")\n```", "```py\n>>> import pickle\n>>> pic = pickle.dumps(rf5)\n>>> rf6 = pickle.loads(pic)\n>>> y_pred = rf6.predict(X_test)\n>>> roc_auc_score(y_test, y_pred)\n0.7747781065088757\n```"]