["```py\n>>> from sklearn.linear_model import (\n...     LogisticRegression,\n... )\n>>> lr = LogisticRegression(random_state=42)\n>>> lr.fit(X_train, y_train)\nLogisticRegression(C=1.0, class_weight=None,\n dual=False, fit_intercept=True,\n intercept_scaling=1, max_iter=100,\n multi_class='ovr', n_jobs=1, penalty='l2',\n random_state=42, solver='liblinear',\n tol=0.0001, verbose=0, warm_start=False)\n>>> lr.score(X_test, y_test)\n0.8040712468193384\n\n>>> lr.predict(X.iloc[[0]])\narray([1])\n>>> lr.predict_proba(X.iloc[[0]])\narray([[0.08698937, 0.91301063]])\n>>> lr.predict_log_proba(X.iloc[[0]])\narray([[-2.4419694 , -0.09100775]])\n>>> lr.decision_function(X.iloc[[0]])\narray([2.35096164])\n```", "```py\n>>> lr.intercept_\narray([-0.62386001])\n```", "```py\n>>> def inv_logit(p):\n...     return np.exp(p) / (1 + np.exp(p))\n\n>>> inv_logit(lr.intercept_)\narray([0.34890406])\n```", "```py\n>>> cols = X.columns\n>>> for col, val in sorted(\n...     zip(cols, lr.coef_[0]),\n...     key=lambda x: x[1],\n...     reverse=True,\n... ):\n...     print(\n...         f\"{col:10}{val:10.3f} {inv_logit(val):10.3f}\"\n...     )\nfare           0.104      0.526\nparch         -0.062      0.485\nsibsp         -0.274      0.432\nage           -0.296      0.427\nembarked_Q    -0.504      0.377\nembarked_S    -0.507      0.376\npclass        -0.740      0.323\nsex_male      -2.400      0.083\n```", "```py\n>>> from yellowbrick.features.importances import (\n...     FeatureImportances,\n... )\n>>> fig, ax = plt.subplots(figsize=(6, 4))\n>>> fi_viz = FeatureImportances(lr)\n>>> fi_viz.fit(X, y)\n>>> fi_viz.poof()\n>>> fig.savefig(\"images/mlpr_1001.png\", dpi=300)\n```", "```py\n>>> from sklearn.naive_bayes import GaussianNB\n>>> nb = GaussianNB()\n>>> nb.fit(X_train, y_train)\nGaussianNB(priors=None, var_smoothing=1e-09)\n>>> nb.score(X_test, y_test)\n0.7837150127226463\n\n>>> nb.predict(X.iloc[[0]])\narray([1])\n>>> nb.predict_proba(X.iloc[[0]])\narray([[2.17472227e-08, 9.99999978e-01]])\n>>> nb.predict_log_proba(X.iloc[[0]])\narray([[-1.76437798e+01, -2.17472227e-08]])\n```", "```py\n>>> from sklearn.svm import SVC\n>>> svc = SVC(random_state=42, probability=True)\n>>> svc.fit(X_train, y_train)\nSVC(C=1.0, cache_size=200, class_weight=None,\n coef0=0.0, decision_function_shape='ovr',\n degree=3, gamma='auto', kernel='rbf',\n max_iter=-1, probability=True, random_state=42,\n shrinking=True, tol=0.001, verbose=False)\n>>> svc.score(X_test, y_test)\n0.8015267175572519\n\n>>> svc.predict(X.iloc[[0]])\narray([1])\n>>> svc.predict_proba(X.iloc[[0]])\narray([[0.15344656, 0.84655344]])\n>>> svc.predict_log_proba(X.iloc[[0]])\narray([[-1.87440289, -0.16658195]])\n```", "```py\n>>> from sklearn.neighbors import (\n...     KNeighborsClassifier,\n... )\n>>> knc = KNeighborsClassifier()\n>>> knc.fit(X_train, y_train)\nKNeighborsClassifier(algorithm='auto',\n leaf_size=30, metric='minkowski',\n metric_params=None, n_jobs=1, n_neighbors=5,\n p=2, weights='uniform')\n>>> knc.score(X_test, y_test)\n0.7837150127226463\n\n>>> knc.predict(X.iloc[[0]])\narray([1])\n\n>>> knc.predict_proba(X.iloc[[0]])\narray([[0., 1.]])\n```", "```py\n>>> from sklearn.tree import DecisionTreeClassifier\n>>> dt = DecisionTreeClassifier(\n...     random_state=42, max_depth=3\n... )\n>>> dt.fit(X_train, y_train)\nDecisionTreeClassifier(class_weight=None,\n criterion='gini', max_depth=None,\n max_features=None, max_leaf_nodes=None,\n min_impurity_decrease=0.0,\n min_impurity_split=None,\n min_samples_leaf=1, min_samples_split=2,\n min_weight_fraction_leaf=0.0, presort=False,\n random_state=42, splitter='best')\n\n>>> dt.score(X_test, y_test)\n0.8142493638676844\n\n>>> dt.predict(X.iloc[[0]])\narray([1])\n>>> dt.predict_proba(X.iloc[[0]])\narray([[0.02040816, 0.97959184]])\n>>> dt.predict_log_proba(X.iloc[[0]])\narray([[-3.8918203 , -0.02061929]])\n```", "```py\n>>> import pydotplus\n>>> from io import StringIO\n>>> from sklearn.tree import export_graphviz\n>>> dot_data = StringIO()\n>>> tree.export_graphviz(\n...     dt,\n...     out_file=dot_data,\n...     feature_names=X.columns,\n...     class_names=[\"Died\", \"Survived\"],\n...     filled=True,\n... )\n>>> g = pydotplus.graph_from_dot_data(\n...     dot_data.getvalue()\n... )\n>>> g.write_png(\"images/mlpr_1002.png\")\n```", "```py\nfrom IPython.display import Image\nImage(g.create_png())\n```", "```py\n>>> viz = dtreeviz.trees.dtreeviz(\n...     dt,\n...     X,\n...     y,\n...     target_name=\"survived\",\n...     feature_names=X.columns,\n...     class_names=[\"died\", \"survived\"],\n... )\n>>> viz\n```", "```py\n>>> for col, val in sorted(\n...     zip(X.columns, dt.feature_importances_),\n...     key=lambda x: x[1],\n...     reverse=True,\n... )[:5]:\n...     print(f\"{col:10}{val:10.3f}\")\nsex_male       0.607\npclass         0.248\nsibsp          0.052\nfare           0.050\nage            0.043\n```", "```py\n>>> from yellowbrick.features.importances import (\n...     FeatureImportances,\n... )\n>>> fig, ax = plt.subplots(figsize=(6, 4))\n>>> fi_viz = FeatureImportances(dt)\n>>> fi_viz.fit(X, y)\n>>> fi_viz.poof()\n>>> fig.savefig(\"images/mlpr_1004.png\", dpi=300)\n```", "```py\n>>> from sklearn.ensemble import (\n...     RandomForestClassifier,\n... )\n>>> rf = RandomForestClassifier(random_state=42)\n>>> rf.fit(X_train, y_train)\nRandomForestClassifier(bootstrap=True,\n class_weight=None, criterion='gini',\n max_depth=None, max_features='auto',\n max_leaf_nodes=None, min_impurity_decrease=0.0,\n min_impurity_split=None, min_samples_leaf=1,\n min_samples_split=2,\n min_weight_fraction_leaf=0.0,\n n_estimators=10, n_jobs=1, oob_score=False,\n random_state=42, verbose=0, warm_start=False)\n>>> rf.score(X_test, y_test)\n0.7862595419847328\n\n>>> rf.predict(X.iloc[[0]])\narray([1])\n>>> rf.predict_proba(X.iloc[[0]])\narray([[0., 1.]])\n>>> rf.predict_log_proba(X.iloc[[0]])\narray([[-inf,   0.]])\n```", "```py\n>>> for col, val in sorted(\n...     zip(X.columns, rf.feature_importances_),\n...     key=lambda x: x[1],\n...     reverse=True,\n... )[:5]:\n...     print(f\"{col:10}{val:10.3f}\")\nage            0.285\nfare           0.268\nsex_male       0.232\npclass         0.077\nsibsp          0.059\n```", "```py\n>>> import rfpimp\n>>> rf = RandomForestClassifier(random_state=42)\n>>> rf.fit(X_train, y_train)\n>>> rfpimp.importances(\n...     rf, X_test, y_test\n... ).Importance\nFeature\nsex_male      0.155216\nfare          0.043257\nage           0.033079\npclass        0.027990\nparch         0.020356\nembarked_Q    0.005089\nsibsp         0.002545\nembarked_S    0.000000\nName: Importance, dtype: float64\n```", "```py\n>>> import xgboost as xgb\n>>> xgb_class = xgb.XGBClassifier(random_state=42)\n>>> xgb_class.fit(\n...     X_train,\n...     y_train,\n...     early_stopping_rounds=10,\n...     eval_set=[(X_test, y_test)],\n... )\nXGBClassifier(base_score=0.5, booster='gbtree',\n colsample_bylevel=1, colsample_bytree=1, gamma=0,\n learning_rate=0.1, max_delta_step=0, max_depth=3,\n min_child_weight=1, missing=None,\n n_estimators=100, n_jobs=1, nthread=None,\n objective='binary:logistic', random_state=42,\n reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n seed=None, silent=True, subsample=1)\n\n>>> xgb_class.score(X_test, y_test)\n0.7862595419847328\n\n>>> xgb_class.predict(X.iloc[[0]])\narray([1])\n>>> xgb_class.predict_proba(X.iloc[[0]])\narray([[0.06732017, 0.93267983]], dtype=float32)\n```", "```py\n>>> for col, val in sorted(\n...     zip(\n...         X.columns,\n...         xgb_class.feature_importances_,\n...     ),\n...     key=lambda x: x[1],\n...     reverse=True,\n... )[:5]:\n...     print(f\"{col:10}{val:10.3f}\")\nfare           0.420\nage            0.309\npclass         0.071\nsex_male       0.066\nsibsp          0.050\n```", "```py\n>>> fig, ax = plt.subplots(figsize=(6, 4))\n>>> xgb.plot_importance(xgb_class, ax=ax)\n>>> fig.savefig(\"images/mlpr_1005.png\", dpi=300)\n```", "```py\n>>> fig, ax = plt.subplots(figsize=(6, 4))\n>>> fi_viz = FeatureImportances(xgb_class)\n>>> fi_viz.fit(X, y)\n>>> fi_viz.poof()\n>>> fig.savefig(\"images/mlpr_1006.png\", dpi=300)\n```", "```py\n>>> booster = xgb_class.get_booster()\n>>> print(booster.get_dump()[0])\n0:[sex_male<0.5] yes=1,no=2,missing=1\n 1:[pclass<0.23096] yes=3,no=4,missing=3\n 3:[fare<-0.142866] yes=7,no=8,missing=7\n 7:leaf=0.132530\n 8:leaf=0.184\n 4:[fare<-0.19542] yes=9,no=10,missing=9\n 9:leaf=0.024598\n 10:leaf=-0.1459\n 2:[age<-1.4911] yes=5,no=6,missing=5\n 5:[sibsp<1.81278] yes=11,no=12,missing=11\n 11:leaf=0.13548\n 12:leaf=-0.15000\n 6:[pclass<-0.95759] yes=13,no=14,missing=13\n 13:leaf=-0.06666\n 14:leaf=-0.1487\n```", "```py\n>>> # score from first tree leaf 7\n>>> 1 / (1 + np.exp(-1 * 0.1238))\n0.5309105310475829\n```", "```py\n>>> fig, ax = plt.subplots(figsize=(6, 4))\n>>> xgb.plot_tree(xgb_class, ax=ax, num_trees=0)\n>>> fig.savefig(\"images/mlpr_1007.png\", dpi=300)\n```", "```py\n>>> import xgbfir\n>>> xgbfir.saveXgbFI(\n...     xgb_class,\n...     feature_names=X.columns,\n...     OutputXlsxFile=\"fir.xlsx\",\n... )\n>>> pd.read_excel(\"/tmp/surv-fir.xlsx\").head(3).T\n 0         1         2\nInteraction         sex_male    pclass      fare\nGain                 1311.44   585.794   544.884\nFScore                    42        45       267\nwFScore              39.2892   21.5038    128.33\nAverage wFScore     0.935458  0.477861  0.480636\nAverage Gain         31.2247   13.0177   2.04076\nExpected Gain        1307.43   229.565   236.738\nGain Rank                  1         2         3\nFScore Rank                4         3         1\nwFScore Rank               3         4         1\nAvg wFScore Rank           1         5         4\nAvg Gain Rank              1         2         4\nExpected Gain Rank         1         3         2\nAverage Rank         1.83333   3.16667       2.5\nAverage Tree Index   32.2381   20.9778   51.9101\nAverage Tree Depth  0.142857   1.13333   1.50562\n```", "```py\n>>> pd.read_excel(\n...     \"fir.xlsx\",\n...     sheet_name=\"Interaction Depth 1\",\n... ).head(2).T\nInteraction         pclass|sex_male  age|sex_male\nGain                        2090.27       964.046\nFScore                           35            18\nwFScore                     14.3608       9.65915\nAverage wFScore            0.410308      0.536619\nAverage Gain                 59.722       53.5581\nExpected Gain                827.49        616.17\nGain Rank                         1             2\nFScore Rank                       5            10\nwFScore Rank                      4             8\nAvg wFScore Rank                  8             5\nAvg Gain Rank                     1             2\nExpected Gain Rank                1             2\nAverage Rank                3.33333       4.83333\nAverage Tree Index          18.9714       38.1111\nAverage Tree Depth                1       1.11111\n```", "```py\n>>> pd.read_excel(\n...     \"fir.xlsx\",\n...     sheet_name=\"Interaction Depth 2\",\n... ).head(1).T\n 0\nInteraction         fare|pclass|sex_male\nGain                             2973.16\nFScore                                44\nwFScore                          8.92572\nAverage wFScore                 0.202857\nAverage Gain                     67.5719\nExpected Gain                    549.145\nGain Rank                              1\nFScore Rank                            1\nwFScore Rank                           4\nAvg wFScore Rank                      21\nAvg Gain Rank                          3\nExpected Gain Rank                     2\nAverage Rank                     5.33333\nAverage Tree Index               16.6591\nAverage Tree Depth                     2\n```", "```py\n>>> pd.read_excel(\n...     \"/tmp/surv-fir.xlsx\",\n...     sheet_name=\"Interaction Depth 2\",\n... )[[\"Interaction\", \"Gain\"]].head()\n Interaction         Gain\n0  fare|pclass|sex_male  2973.162529\n1   age|pclass|sex_male  1621.945151\n2    age|sex_male|sibsp  1042.320428\n3     age|fare|sex_male   366.860828\n4    fare|fare|sex_male   196.224791\n```", "```py\n>>> import lightgbm as lgb\n>>> lgbm_class = lgb.LGBMClassifier(\n...     random_state=42\n... )\n>>> lgbm_class.fit(X_train, y_train)\nLGBMClassifier(boosting_type='gbdt',\n class_weight=None, colsample_bytree=1.0,\n learning_rate=0.1, max_depth=-1,\n min_child_samples=20, min_child_weight=0.001,\n min_split_gain=0.0, n_estimators=100,\n n_jobs=-1, num_leaves=31, objective=None,\n random_state=42, reg_alpha=0.0, reg_lambda=0.0,\n silent=True, subsample=1.0,\n subsample_for_bin=200000, subsample_freq=0)\n\n>>> lgbm_class.score(X_test, y_test)\n0.7964376590330788\n\n>>> lgbm_class.predict(X.iloc[[0]])\narray([1])\n>>> lgbm_class.predict_proba(X.iloc[[0]])\narray([[0.01637168, 0.98362832]])\n```", "```py\n>>> for col, val in sorted(\n...     zip(cols, lgbm_class.feature_importances_),\n...     key=lambda x: x[1],\n...     reverse=True,\n... )[:5]:\n...     print(f\"{col:10}{val:10.3f}\")\nfare        1272.000\nage         1182.000\nsibsp        118.000\npclass       115.000\nsex_male     110.000\n```", "```py\n>>> fig, ax = plt.subplots(figsize=(6, 4))\n>>> lgb.plot_importance(lgbm_class, ax=ax)\n>>> fig.tight_layout()\n>>> fig.savefig(\"images/mlpr_1008.png\", dpi=300)\n```", "```py\n>>> fig, ax = plt.subplots(figsize=(6, 4))\n>>> lgb.plot_tree(lgbm_class, tree_index=0, ax=ax)\n>>> fig.savefig(\"images/mlpr_1009.png\", dpi=300)\n```", "```py\nlgb.create_tree_digraph(lgbm_class)\n```", "```py\n>>> from tpot import TPOTClassifier\n>>> tc = TPOTClassifier(generations=2)\n>>> tc.fit(X_train, y_train)\n>>> tc.score(X_test, y_test)\n0.7888040712468194\n\n>>> tc.predict(X.iloc[[0]])\narray([1])\n>>> tc.predict_proba(X.iloc[[0]])\narray([[0.07449919, 0.92550081]])\n```", "```py\n>>> tc.export(\"tpot_exported_pipeline.py\")\n```", "```py\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import \\\n    train_test_split\nfrom sklearn.pipeline import make_pipeline, \\\n    make_union\nfrom sklearn.preprocessing import Normalizer\nfrom tpot.builtins import StackingEstimator\n\n# NOTE: Make sure that the class is labeled\n# 'target' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE',\n    sep='COLUMN_SEPARATOR', dtype=np.float64)\nfeatures = tpot_data.drop('target', axis=1).values\ntraining_features, testing_features, \\\n    training_target, testing_target = \\\n    train_test_split(features,\n       tpot_data['target'].values, random_state=42)\n\n# Score on the training set was:0.8122535043953432\nexported_pipeline = make_pipeline(\n  Normalizer(norm=\"max\"),\n  StackingEstimator(\n    estimator=ExtraTreesClassifier(bootstrap=True,\n      criterion=\"gini\", max_features=0.85,\n      min_samples_leaf=2, min_samples_split=19,\n      n_estimators=100)),\n  ExtraTreesClassifier(bootstrap=False,\n    criterion=\"entropy\", max_features=0.3,\n    min_samples_leaf=13, min_samples_split=9,\n    n_estimators=100)\n)\n\nexported_pipeline.fit(training_features,\n  training_target)\nresults = exported_pipeline.predict(\n  testing_features)\n```"]