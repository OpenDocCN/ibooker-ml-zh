<html><head></head><body><section data-pdf-bookmark="Chapter 14. Business Logic" data-type="chapter" epub:type="chapter"><div class="chapter" id="HardRanking">&#13;
<h1><span class="label">Chapter 14. </span>Business Logic</h1>&#13;
&#13;
&#13;
<p>By<a data-primary="business logic" data-secondary="challenges of recommendation systems" data-type="indexterm" id="id1098"/><a data-primary="recommendation systems" data-secondary="challenges of" data-type="indexterm" id="id1099"/> now, you may be thinking, “Yes, our algorithmic ranking and recommendation has arrived! Personalization for every user with latent understanding is how we run our business.” Unfortunately, the business is rarely this simple.</p>&#13;
&#13;
<p>Let’s take a really straightforward example, a recipe recommendation system. Consider a user who simply hates grapefruit (one of the authors of this book <em>really</em> does) but may love a set of other ingredients that go well with grapefruit: asparagus, avocado, banana, butter, cashews, champagne, chicken, coconut, crab, fish, ginger, hazelnut, honey, lemon, lime, melon, mint, olive oil, onion, orange, pecan, pineapple, raspberry, rum, salmon, seaweed, shrimp, star anise, strawberry, tarragon, tomato, vanilla, wine, and yogurt. These ingredients are the <em>most</em> popular to pair with grapefruit, and the user loves almost all of these.</p>&#13;
&#13;
<p>What’s the right way for the recommender to handle this case? It may seem like this is something that collaborative filtering (CF), latent features, or hybrid recommendations would catch. However, if the user likes all these shared flavors, the item-based CF model would not catch this well. Similarly, if the user truly <em>hates</em> grapefruit, latent features may not be sufficient to truly avoid it.</p>&#13;
&#13;
<p>In this case, the simple approach is a<a data-primary="hard avoids" data-type="indexterm" id="id1100"/><a data-primary="avoids" data-secondary="hard avoids" data-type="indexterm" id="id1101"/> great one: <em>hard avoids</em>. In this chapter, we’ll talk about some of the intricacies of business logic intersecting the output of your recommendation system.</p>&#13;
&#13;
<p>Instead of attempting to learn exceptions as part of the latent features that the model utilizes when making recommendations, it’s more consistent and simple to integrate these business rules as an external step via deterministic logic. As an example: the model could remove all grapefruit cocktails that are retrieved instead of attempting to learn to rank them lower.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Hard Ranking" data-type="sect1"><div class="sect1" id="id158">&#13;
<h1>Hard Ranking</h1>&#13;
&#13;
<p>You<a data-primary="business logic" data-secondary="hard rankings" data-type="indexterm" id="id1102"/><a data-primary="hard rankings/ratings" data-type="indexterm" id="id1103"/><a data-primary="ratings" data-secondary="hard and soft ratings" data-type="indexterm" id="id1104"/> can come up with a lot of examples of these phenomena when you start thinking of situations similar to our grapefruit scenario. <em>Hard ranking</em> usually refers to one of two kinds of special ranking rules:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Explicitly removing some items from the list before ranking.</p>&#13;
</li>&#13;
<li>&#13;
<p>Using a categorical feature to rank the results by category. (Note that this can even be done for multiple features to achieve a hierarchical hard ranking.)</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Have you ever observed any of the following?</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>A user bought a sofa. The system continues to recommend sofas to this user even though  they won’t need a sofa for the next five years.</p>&#13;
</li>&#13;
<li>&#13;
<p>A user buys a birthday gift for a friend interested in gardening. Then the ecommerce site keeps recommending gardening tools despite the user having no interest in it.</p>&#13;
</li>&#13;
<li>&#13;
<p>A parent wants to buy a toy for their child. But when the parent goes to the website where they usually buy toys, the site recommends several toys for a child a few years younger—the parent hasn’t purchased from the site since the child was that age.</p>&#13;
</li>&#13;
<li>&#13;
<p>A runner experiences serious knee pain and determines they can no longer go on long runs. They switch to cycling, which is lower impact. However, their local meetup recommendations are still all running oriented.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>All of these cases can be relatively easy to deal with via deterministic logic. For these situations,  we would prefer <em>not</em> to try to learn these rules via ML. We should assume that for these types of scenarios, we will get low signal about these preferences: negative implicit feedback is often lower in relevance, and many of the situations listed are represented by details that you want the system to learn once and for all. Additionally, in some of the previous examples, it can be upsetting or harmful to a relationship with a user to have the preferences not respected.</p>&#13;
&#13;
<p>The name for these preferences is<a data-primary="avoids" data-secondary="alternate terms for" data-type="indexterm" id="id1105"/><a data-primary="constraints" data-type="indexterm" id="id1106"/><a data-primary="overrides" data-type="indexterm" id="id1107"/><a data-primary="hard rules" data-type="indexterm" id="id1108"/> <em>avoids</em>—or sometimes constraints, overrides, or hard rules. You should think of them as explicit expectations of the system: “Don’t show me recipes with grapefruit,” “No more sofas,” “I don’t like gardening,” “My child is older than 10 now,” and “Don’t show me trail runs.”</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Learned Avoids" data-type="sect1"><div class="sect1" id="id159">&#13;
<h1>Learned Avoids</h1>&#13;
&#13;
<p>Not<a data-primary="business logic" data-secondary="learned avoids" data-type="indexterm" id="id1109"/> all business rules are such obvious avoids that derive from explicit user feedback, and some derive from explicit feedback not directly related to specific items. It’s important to include a wide variety of avoids when considering serving recommendations.</p>&#13;
&#13;
<p>For the sake of simplicity, let’s assume you’re building a fashion recommender system. Examples of more subtle avoids include the following:</p>&#13;
<dl>&#13;
<dt>Already owned items</dt>&#13;
<dd>&#13;
<p>These are items that users really need to purchase only once—for example, clothing users have bought through your platform or told you they already own. Creating a<a data-primary="virtual closets" data-type="indexterm" id="id1110"/> <em>virtual closet</em> might be a way to ask users to tell you what they have, to assist in these avoids.</p>&#13;
</dd>&#13;
<dt>Disliked features</dt>&#13;
<dd>&#13;
<p>These are features of items that the user can indicate disinterest in. During an onboarding questionnaire, you may ask users if they like polka dots or if they have a favorite color palette. These are explicitly indicated pieces of feedback that can be used for avoids.</p>&#13;
</dd>&#13;
<dt>Ignored categories</dt>&#13;
<dd>&#13;
<p>This is a  category or group of items that doesn’t resonate with the user. This can be implicit but learned outside the primary recommender model. Maybe the user has never clicked the Dresses category on your ecommerce website because they don’t enjoy wearing them.</p>&#13;
</dd>&#13;
<dt>Low-quality items</dt>&#13;
<dd>&#13;
<p>Over time, you’ll learn that some items are simply low quality for most users. You can detect this via a high number of returns or low ratings from buyers. These items ultimately should be removed from inventory, but in the meantime, it’s important to include them as avoids for all but the strongest signal of match.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>These additional avoids can be implemented easily during the serving stage and can even include simple models. Training linear models to capture some of these rules and then applying them during serving can be a useful and reliable mechanism for improving ranking. Note that the small models perform very fast inference, so little negative impact usually results from including them in the pipeline. For larger-scale behavior trends or higher-order factors, we expect our core recommendation models to learn these ideas.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Hand-Tuned Weights" data-type="sect1"><div class="sect1" id="id160">&#13;
<h1>Hand-Tuned Weights</h1>&#13;
&#13;
<p>On<a data-primary="business logic" data-secondary="hand-tuned weights" data-type="indexterm" id="id1111"/><a data-primary="hand-tuned weights" data-type="indexterm" id="id1112"/> the other side of the spectrum of avoids is <em>hand-tuned ranking</em>. This technique was popular in earlier days of search ranking, when humans would use analytics and observation to determine what they thought were the most important features in a ranking and then craft a multiobjective ranker. For example, flower stores may rank higher in early May as many users search for Mother’s Day gifts. Since there could be many variable elements to track, these kinds of approaches don’t scale well and have been largely deemphasized in modern recommendation ranking.</p>&#13;
&#13;
<p>However, hand-tuned ranking can be incredibly useful as an<a data-primary="avoids" data-secondary="hand-tuned rankings as" data-type="indexterm" id="id1113"/> <em>avoid</em>. While technically it’s not an avoid, we sometimes still call it that. An example of this in practice is to know that new users like to start with a lower-priced item while they’re learning whether your shipping is trustworthy. A useful technique is to then uprank lower-priced items before the first order.</p>&#13;
&#13;
<p>While it may feel bad to consider building a hand-tuned ranking, it’s important to not count this technique out. It has a place and is often a great place to start. One interesting human-in-the-loop application of this kind of technique is for hand-tuned ranking by experts. Back to our fashion recommender, a style expert may know that this summer’s trending color is mauve, especially among the younger generation. Then can positively influence user satisfaction if the expert ranks these mauve items up for users in the right age persona.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Inventory Health" data-type="sect1"><div class="sect1" id="InvHealth">&#13;
<h1>Inventory Health</h1>&#13;
&#13;
<p>A<a data-primary="business logic" data-secondary="inventory health" data-type="indexterm" id="id1114"/><a data-primary="inventory health" data-type="indexterm" id="id1115"/> unique and somewhat contentious side of hard ranking is inventory health. Notoriously hard to define, <em>inventory health</em> estimates how good the existing inventory is for satisfying user demand.</p>&#13;
&#13;
<p>Let’s<a data-primary="demand forecast" data-type="indexterm" id="id1116"/><a data-primary="forecasting" data-type="indexterm" id="id1117"/><a data-primary="affinity" data-type="indexterm" id="id1118"/> take a quick look at one way to define inventory health, via affinity scores and forecasting. We can do this by leveraging a demand forecast, which is an incredibly powerful and popular way to optimize the business: what are the expected sales in each category over the next <em>N</em> time periods? Building these forecasting models is outside the scope of this book, but the core ideas are well captured in the famous book <a href="https://otexts.com/fpp3/">“Forecasting: Principles and Practice”</a> by Rob Hyndman and George Athanasopoulos (Otexts). For the sake of our discussion, assume that you’re able to roughly approximate the number of socks you’ll sell over the next month, broken down by size and usage type. This can be a really instructive estimate for the number of socks of various types you should have on hand.</p>&#13;
&#13;
<p>However, it doesn’t stop there; inventory may be finite, and in practice inventory is often a major constraint on businesses that sell physical goods. With that caveat, we have to turn to the other side of the market demand. If our demand outstrips our availability, we are ultimately disappointing users who don’t have access to the item they desired.</p>&#13;
&#13;
<p>Let’s take an example of selling bagels; you’ve calculated average demand for poppy seed, onion, asiago cheese, and egg. On any given day, many customers will come to buy a bagel with a clear preference in mind, but will you have enough of that bagel? Every bagel you don’t sell is wasted; people like fresh bagels. This means that the bagels you recommend to each person are dependent on good inventory. Some users are less picky; they can get one of two or three of the options and be just as happy. In that case, it’s better to give them another bagel option and save the lowest inventory for the picky ones. This is a kind of model refinement called<a data-primary="optimization" data-secondary="global optimization" data-type="indexterm" id="id1119"/><a data-primary="global optimization" data-type="indexterm" id="id1120"/> <em>optimization</em>, which has a huge number of techniques. We won’t get into optimization techniques, but books on mathematical optimization or operations research will provide direction. <em>Algorithms for Optimization</em> by Mykel J. Kochenderfer and Tim A. Wheeler (MIT Press) is a good place to start.</p>&#13;
&#13;
<p>Inventory health ties back to hard ranking, because actively managing inventory as part of your recommendations is an incredibly important and powerful tool. Ultimately, inventory optimization will degrade the perceived performance of your recommendations, but by including it as part of your business rules, the overall health of your business and recommender system improves. This is why it is sometimes called <em>global optimization</em>.</p>&#13;
&#13;
<p>The reason that these methods stir up heated discussions is that not everyone agrees that the quality of recommendations for some users should be depressed to improve those for the “greater good.” Health of the marketplace and average satisfaction are useful metrics to consider, but ensure that these are aligned with the north-star metrics for the recommendation system at large.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Implementing Avoids" data-type="sect1"><div class="sect1" id="id162">&#13;
<h1>Implementing Avoids</h1>&#13;
&#13;
<p>The<a data-primary="business logic" data-secondary="implementing avoids" data-type="indexterm" id="BLavoid14"/><a data-primary="avoids" data-secondary="implementing" data-type="indexterm" id="Aimpl14"/> simplest approach to handling avoids is via downstream filtering. To do this, you’ll want to apply the avoid rules for the user before the recommendations are passed along from the ranker to the user. Implementing this approach looks something like this:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">filter_dataframe</code><code class="p">(</code><code class="n">df</code><code class="p">:</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">,</code> <code class="n">filter_dict</code><code class="p">:</code> <code class="nb">dict</code><code class="p">):</code>&#13;
    <code class="sd">"""</code>&#13;
<code class="sd">    Filter a dataframe to exclude rows where columns have certain values.</code>&#13;
&#13;
<code class="sd">    Args:</code>&#13;
<code class="sd">        df (pd.DataFrame): Input dataframe.</code>&#13;
<code class="sd">        filter_dict (dict): Dictionary where keys are column names</code>&#13;
<code class="sd">        and values are the values to exclude.</code>&#13;
&#13;
<code class="sd">    Returns:</code>&#13;
<code class="sd">        pd.DataFrame: Filtered dataframe.</code>&#13;
<code class="sd">    """</code>&#13;
    <code class="k">for</code> <code class="n">col</code><code class="p">,</code> <code class="n">val</code> <code class="ow">in</code> <code class="n">filter_dict</code><code class="o">.</code><code class="n">items</code><code class="p">():</code>&#13;
        <code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">df</code><code class="p">[</code><code class="n">col</code><code class="p">]</code> <code class="o">!=</code> <code class="n">val</code><code class="p">]</code>&#13;
    <code class="k">return</code> <code class="n">df</code>&#13;
&#13;
<code class="n">filter_dict</code> <code class="o">=</code> <code class="p">{</code><code class="s1">'column1'</code><code class="p">:</code> <code class="s1">'value1'</code><code class="p">,</code> <code class="s1">'column2'</code><code class="p">:</code> <code class="s1">'value2'</code><code class="p">,</code> <code class="s1">'column3'</code><code class="p">:</code> <code class="s1">'value3'</code><code class="p">}</code>&#13;
&#13;
<code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">pipe</code><code class="p">(</code><code class="n">filter_dataframe</code><code class="p">,</code> <code class="n">filter_dict</code><code class="p">)</code></pre>&#13;
&#13;
<p>Admittedly, this<a data-primary="JAX framework" data-secondary="implementing avoids in" data-type="indexterm" id="id1121"/> is a trivial but also relatively naive attempt at avoids. First, working purely in pandas will limit some of the scalability of your recommender, so let’s convert this to JAX:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">jax</code>&#13;
<code class="kn">import</code> <code class="nn">jax.numpy</code> <code class="k">as</code> <code class="nn">jnp</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">filter_jax_array</code><code class="p">(</code><code class="n">arr</code><code class="p">:</code> <code class="n">jnp</code><code class="o">.</code><code class="n">array</code><code class="p">,</code> <code class="n">col_indices</code><code class="p">:</code> <code class="nb">list</code><code class="p">,</code> <code class="n">values</code><code class="p">:</code> <code class="nb">list</code><code class="p">):</code>&#13;
    <code class="sd">"""</code>&#13;
<code class="sd">    Filter a jax array to exclude rows where certain columns have certain values.</code>&#13;
&#13;
<code class="sd">    Args:</code>&#13;
<code class="sd">        arr (jnp.array): Input array.</code>&#13;
<code class="sd">        col_indices (list): List of column indices to filter on.</code>&#13;
<code class="sd">        values (list): List of corresponding values to exclude.</code>&#13;
&#13;
<code class="sd">    Returns:</code>&#13;
<code class="sd">        jnp.array: Filtered array.</code>&#13;
<code class="sd">    """</code>&#13;
    <code class="k">assert</code> <code class="nb">len</code><code class="p">(</code><code class="n">col_indices</code><code class="p">)</code> <code class="o">==</code> <code class="nb">len</code><code class="p">(</code><code class="n">values</code><code class="p">),</code>&#13;
&#13;
    <code class="n">masks</code> <code class="o">=</code> <code class="p">[</code><code class="n">arr</code><code class="p">[:,</code> <code class="n">col</code><code class="p">]</code> <code class="o">!=</code> <code class="n">val</code> <code class="k">for</code> <code class="n">col</code><code class="p">,</code> <code class="n">val</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">col_indices</code><code class="p">,</code> <code class="n">values</code><code class="p">)]</code>&#13;
    <code class="n">total_mask</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">logical_and</code><code class="p">(</code><code class="o">*</code><code class="n">masks</code><code class="p">)</code>&#13;
&#13;
    <code class="k">return</code> <code class="n">arr</code><code class="p">[</code><code class="n">total_mask</code><code class="p">]</code></pre>&#13;
&#13;
<p>But there are deeper issues. The next issue you may face is where that collection of avoids is stored. An obvious place is somewhere like a NoSQL database keyed on users, and then you can get all of the avoids as a simple lookup. This is a natural use of feature stores, as you saw in <a data-type="xref" href="ch06.html#feature-stores">“Feature Stores”</a>. Some avoids may be applied in real time, while others are learned upon user onboarding. Feature stores are a great place to house avoids.</p>&#13;
&#13;
<p>The next potential gotcha with our naive filter is that it doesn’t naturally extend to covariate avoids, or more complicated avoid scenarios. Some avoids are actually dependent on context—a user who doesn’t wear white after Labor Day, users who don’t eat meat on Fridays, or coffee-processing methods that don’t mesh well with certain brewers. All of these require conditional logic. You might think that your powerful and effective recommendation system model can certainly learn these details, but this is true only sometimes. The reality is that many of these kinds of considerations are lower signal than the large-scale concepts your recommendation system should be learning, and thus are hard to learn consistently. Additionally, these kinds of rules are often ones you should require, as opposed to remain optimistic about. For that reason, you often should explicitly specify such restrictions.</p>&#13;
&#13;
<p>This specification can often be achieved by explicit deterministic algorithms that impose these requirements. For the coffee problem, one of the authors hand-built a decision stump to handle a few bad combinations of coffee roast features and <span class="keep-together">brewers—<em>anaerobic espresso?! Yuck!</em></span></p>&#13;
&#13;
<p>Our other two examples (not wearing white after Labor Day and not eating meat on Fridays), however, are a bit more nuanced. An explicit algorithmic approach may be tricky to handle. How do we know that a user doesn’t eat meat on Fridays during one period of the year?<a data-primary="" data-startref="Aimpl14" data-type="indexterm" id="id1122"/><a data-primary="" data-startref="BLavoid14" data-type="indexterm" id="id1123"/></p>&#13;
&#13;
<p>For these use cases, model-based avoids can impose these requirements.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Model-Based Avoids" data-type="sect1"><div class="sect1" id="id229">&#13;
<h1>Model-Based Avoids</h1>&#13;
&#13;
<p>In<a data-primary="business logic" data-secondary="model-based avoids" data-type="indexterm" id="id1124"/><a data-primary="models" data-secondary="model-based avoids" data-type="indexterm" id="id1125"/><a data-primary="avoids" data-secondary="model-based" data-type="indexterm" id="id1126"/> our quest to include more complicated rules and potentially learn them, we may sound like we’re back in the realm of retrieval. Unfortunately, even with models like wide-and-deep with lots of parameters doing both user modeling and item modeling, learning such high-level relationships can be tricky.</p>&#13;
&#13;
<p>While most of this book has focused on working fairly large and deep, this part of recommendation systems is well suited for simple models. For feature-based binary predictions (should this be recommended), we certainly have a zoo of good options. The best approach would obviously depend heavily on the number of features involved in implementing the avoid you wish to capture. It’s useful to remember that many avoids that we’re considering in this section start out as assumptions or hypotheses: we think some users may not wear white after Labor Day, and then attempt to find features that model this outcome well. In this way, it can be more tractable using extremely simple regression models to find covarying features with the outcome in question.</p>&#13;
&#13;
<p>Another related piece of this puzzle is<a data-primary="latent space" data-secondary="latent representations" data-type="indexterm" id="id1127"/> latent representations. For our Friday vegetarians, we may be trying to infer a particular persona that we know has this rule. That persona is a latent feature that we hope to map from other attributes. It’s important to be careful with this kind of modeling (in general, personas can be a bit nuanced and worthy of thoughtful decision making), but it can be quite helpful. It may seem like the user-modeling parts of your large recommender model should learn these—and they can! A useful trick is to pull forward personas learned from that model and regress them against hypothesized avoids to allow for more signal. However, the other model doesn’t always learn these personas because our loss functions for retrieval relevance (and downstream for ranking) are attempting to parse out relevance for individual users from the latent persona features—which may predict these avoids only amid context features.</p>&#13;
&#13;
<p>All in all, implementing the avoids is both very easy and very hard. When building production recommendation systems, the journey is not over when you get to serving; many models factor into the final step of the process.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id319">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>Sometimes you need to rely on more classic approaches to ensuring that the recommendations you’re sending downstream are satisfying essential rules of your business. Learning explicit or subtle lessons from your users can be turned into simple strategies to continue to delight them.</p>&#13;
&#13;
<p>However, this is not the end of our serving challenge. Another kind of downstream consideration is related to the kind of filtering we’ve done here but derives from user preference and human behavior. Ensuring that recommendations are not repeated, rote, and redundant is the subject of the next chapter on diversity in recommendations. We will also discuss how to balance multiple priorities simultaneously when determining exactly what to serve.</p>&#13;
</div></section>&#13;
</div></section></body></html>