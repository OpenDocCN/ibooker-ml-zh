<html><head></head><body><section data-pdf-bookmark="Chapter 16. Acceleration Structures" data-type="chapter" epub:type="chapter"><div class="chapter" id="acceleration_structures">&#13;
<h1><span class="label">Chapter 16. </span>Acceleration Structures</h1>&#13;
&#13;
&#13;
<p>So<a data-primary="acceleration structures" data-secondary="definition of term" data-type="indexterm" id="id1164"/> what are acceleration structures? In computer science terminology, when you try to rank every item in a corpus one by one, the typical amount of time it would take if there are <em>N</em> items is proportional to <em>N</em>. This is called <a href="https://oreil.ly/9-ton">big <em>O</em> notation</a>. So if you have a user vector and you have a corpus of <em>N</em> items, it would take typically <em>O</em>(<em>N</em>) time to score all the items in the corpus for one user. This is usually tractable if <em>N</em> is small and can fit into GPU RAM, typically <em>N</em> &lt; 1 million items or so. However, if we have a very large corpus of, say, a billion items, it might take a very long time if we also have to make recommendations for a billion users. Then in big <em>O</em> notation it would be <span><em>O</em>(10<sup>18</sup>)</span> dot products to score a billion items for each and every one of a billion users.</p>&#13;
&#13;
<p>In<a data-primary="acceleration structures" data-secondary="strategies for" data-type="indexterm" id="id1165"/> this chapter, we will try to reduce the <em>O</em>(<em>N</em> * <em>M</em>) time to something sublinear in the number of items <em>N</em> and the number of users <em>M</em>. We will discuss strategies including the following:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Sharding</p>&#13;
</li>&#13;
<li>&#13;
<p>Locality sensitive hashing</p>&#13;
</li>&#13;
<li>&#13;
<p><em>k</em>-d Trees</p>&#13;
</li>&#13;
<li>&#13;
<p>Hierarchical k-means</p>&#13;
</li>&#13;
<li>&#13;
<p>Cheaper retrieval methods</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>We’ll also cover the trade-offs related to each strategy and what they could be used for. For all the following examples, we assume that the user and items are represented by embedding vectors of the same size and that the affinity between the user and items is a simple dot product, cosine distance, or Euclidean distance. If we were to use a neural network like a two-tower model to score the user and item, then possibly the only method that could be used to speed things up would be sharding or some kind of cheaper pre-filtering method.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Sharding" data-type="sect1"><div class="sect1" id="id169">&#13;
<h1>Sharding</h1>&#13;
&#13;
<p><em>Sharding</em> is<a data-primary="acceleration structures" data-secondary="sharding" data-type="indexterm" id="id1166"/><a data-primary="sharding" data-type="indexterm" id="id1167"/> probably the simplest strategy to <a href="https://oreil.ly/ul_IK">divide and conquer</a>. Suppose you have <em>k</em> machines, <em>N</em> items, and <em>M</em> users. Using a sharding strategy, you can reduce the runtime to <em>O</em>(<em>N</em> * <em>M</em> / <em>k</em>). You can do this by assigning each item a unique identifier, so you have tuples of (<em><code>unique_id</code></em>, <em><code>item_vector</code></em>). Then, by simply taking <code>machine_id = unique_id % K</code>, we can assign a subset of the corpus to a different machine.</p>&#13;
&#13;
<p>When a user needs a recommendation, we can then compute the top-scoring recommendations either ahead of time or on demand by distributing the workload onto <em>k</em> machines, thus making the computation <em>k</em> times faster, except for the overhead in gathering the top results on the server and ordering them jointly. Note that if you want, say, 100 top-scoring items, you would still have to obtain the top 100 results from each shard, collate them together, and then sort all the results jointly if you want to have the same results as in a brute-force method of scoring the entire corpus.</p>&#13;
&#13;
<p>Sharding is useful in the sense that it can be combined with any of the other acceleration methods and is not dependent on the representation having any specific form, such as being a single vector.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Locality Sensitive Hashing" data-type="sect1"><div class="sect1" id="id170">&#13;
<h1>Locality Sensitive Hashing</h1>&#13;
&#13;
<p><em>Locality sensitive hashing</em> (LSH) is<a data-primary="LSH (locality sensitive hashing)" data-type="indexterm" id="LSH16"/><a data-primary="acceleration structures" data-secondary="locality sensitive hashing (LSH)" data-type="indexterm" id="ASlocality16"/><a data-primary="locality sensitive hashing (LSH)" data-type="indexterm" id="locality16"/><a data-primary="hashing" data-secondary="locality sensitive hashing (LSH)" data-type="indexterm" id="Hlocality16"/> an interesting technique that converts a vector into a token-based representation. This is powerful because if CPUs are readily available, we can use them to compute the similarity between vectors by using cheaper integer arithmetic operations such as XOR and bit counting with specialized assembly instructions rather than floating-point operations. Integer operations tend to be much faster on CPUs than floating-point operations, so we can compute similarity between items much faster than using vector operations.</p>&#13;
&#13;
<p>The other benefit is that once items are represented as a series of tokens, a regular search engine database would be able to store and retrieve these items by using token matching. Regular hashing, on the other hand, tends to result in vastly different hash codes if a slight change occurs in the input. This is not a criticism of the hash functions; they just have different uses for different kinds of data.</p>&#13;
&#13;
<p>Let’s walk through a couple of ways to convert a vector into a hash. LSH is different from regular hashing in that small perturbations to a vector should result in the same hash bits as the hash of the original vector. This is an important property as it allows us to look up the neighborhood of a vector by using fast methods such as hash maps. One simple hashing method is called <a href="https://oreil.ly/_1Bd8">the Power of Comparative Reasoning</a>, or Winner Take All hashing. In this hashing scheme, the vector is first permuted using a known, reproducible permutation. We can generate this known permutation by simply shuffling the indices of all the vector dimensions with a random-number generator that accepts a seed and reliably reproduces the same exact shuffle sequence. It is important that the permutation is stable over different versions of Python, as we want to reproduce the hashing operation when generating the hashes as well as during retrieval time. Since we are using JAX’s random library and JAX is careful about the reproducibility of permutations, we just directly use the permutation function in JAX. The hash code computation after that is simply a comparison between adjacent dimensions of the permuted vector, as shown in <a data-type="xref" href="#ex-16-1">Example 16-1</a>.</p>&#13;
<div data-type="example" id="ex-16-1">&#13;
<h5><span class="label">Example 16-1. </span>Winner take all</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">compute_wta_hash</code><code class="p">(</code><code class="n">x</code><code class="p">):</code>&#13;
  <code class="sd">"""Example code to compute some Winner take all hash vectors</code>&#13;
<code class="sd">  Args:</code>&#13;
<code class="sd">    x: a vector</code>&#13;
<code class="sd">  Result:</code>&#13;
<code class="sd">    hash: a hash code</code>&#13;
<code class="sd">  """</code>&#13;
  <code class="n">key</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="mi">1337</code><code class="p">)</code>&#13;
  <code class="n">permuted</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">permutation</code><code class="p">(</code><code class="n">key</code><code class="p">,</code> <code class="n">x</code><code class="p">)</code>&#13;
&#13;
  <code class="n">hash1</code> <code class="o">=</code> <code class="n">permuted</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">&gt;</code> <code class="n">permuted</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>&#13;
  <code class="n">hash2</code> <code class="o">=</code> <code class="n">permuted</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code> <code class="o">&gt;</code> <code class="n">permuted</code><code class="p">[</code><code class="mi">2</code><code class="p">]</code>&#13;
&#13;
  <code class="k">return</code> <code class="p">(</code><code class="n">hash1</code><code class="p">,</code> <code class="n">hash2</code><code class="p">)</code>&#13;
&#13;
<code class="n">x1</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">])</code>&#13;
<code class="n">x2</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code> <code class="mf">2.5</code><code class="p">,</code> <code class="mi">3</code><code class="p">])</code>&#13;
<code class="n">x3</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="mi">3</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">1</code><code class="p">])</code>&#13;
<code class="n">x1_hash</code> <code class="o">=</code> <code class="n">compute_wta_hash</code><code class="p">(</code><code class="n">x1</code><code class="p">)</code>&#13;
<code class="n">x2_hash</code> <code class="o">=</code> <code class="n">compute_wta_hash</code><code class="p">(</code><code class="n">x2</code><code class="p">)</code>&#13;
<code class="n">x3_hash</code> <code class="o">=</code> <code class="n">compute_wta_hash</code><code class="p">(</code><code class="n">x3</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="n">x1_hash</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="n">x2_hash</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="n">x3_hash</code><code class="p">)</code>&#13;
&#13;
<code class="p">(</code><code class="n">Array</code><code class="p">(</code><code class="kc">False</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="nb">bool</code><code class="p">),</code> <code class="n">Array</code><code class="p">(</code><code class="kc">True</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="nb">bool</code><code class="p">))</code>&#13;
<code class="p">(</code><code class="n">Array</code><code class="p">(</code><code class="kc">False</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="nb">bool</code><code class="p">),</code> <code class="n">Array</code><code class="p">(</code><code class="kc">True</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="nb">bool</code><code class="p">))</code>&#13;
<code class="p">(</code><code class="n">Array</code><code class="p">(</code><code class="kc">True</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="nb">bool</code><code class="p">),</code> <code class="n">Array</code><code class="p">(</code><code class="kc">False</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="nb">bool</code><code class="p">))</code></pre></div>&#13;
&#13;
<p>As you can see, the vector <code>x2</code> is slightly different from <code>x1</code> and results in the same hash code of <code>01</code>, whereas <code>x3</code> is different and results in a hash code of <code>10</code>.&#13;
The <a href="https://oreil.ly/RF-x1">Hamming distance</a> of<a data-primary="Hamming distance" data-type="indexterm" id="id1168"/> the hash code is then used to compute the distance between two vectors, as shown in <a data-type="xref" href="#example-16-2">Example 16-2</a>. The distance is simply the XOR of the two hash codes, which results in 1 whenever the bits disagree, followed by bit counting.</p>&#13;
<div data-type="example" id="example-16-2">&#13;
<h5><span class="label">Example 16-2. </span>Hamming function</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">x</code> <code class="o">=</code> <code class="mi">16</code>&#13;
<code class="n">y</code> <code class="o">=</code> <code class="mi">15</code>&#13;
<code class="n">hamming_xy</code> <code class="o">=</code> <code class="nb">int</code><code class="o">.</code><code class="n">bit_count</code><code class="p">(</code><code class="n">x</code> <code class="o">^</code> <code class="n">y</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="n">hamming_xy</code><code class="p">)</code>&#13;
<code class="mi">5</code></pre></div>&#13;
&#13;
<p>Using the Hamming distance as shown here results in some speedup in the distance computation, but the major speedup will come from using the hash codes in a hash map. For example, we could break up the hash code into 8-bit chunks and store the corpus into shards keyed by each 8-bit chunk, which results in a 256× speedup because we have to look only in the hash map that has the same key as the query vector for nearest neighbors.</p>&#13;
&#13;
<p>This has a drawback in terms of recall, though, because all 8 bits have to match in order for an item to be retrieved that matches the query vector. A tradeoff exists between the number of bits of the hash code used in hashing and the Hamming distance computation. The larger the number of bits, the faster the search, because the corpus is divided into smaller and smaller chunks. However, the drawback is that more and more bits have to match, and thus all the hash code bits in a nearby vector in the original space might not match and thus might not be retrieved.</p>&#13;
&#13;
<p>The remedy is to have multiple hash codes with different random-number generators and repeat this process a few times with different random seeds. This extra step is left as an exercise for you.</p>&#13;
&#13;
<p>Another<a data-primary="Johnson-Lindenstrauss lemma" data-type="indexterm" id="id1169"/> common way to compute hash bits uses the <a href="https://oreil.ly/vbAGn">Johnson-Lindenstrauss lemma</a>, which is a fancy way of saying that two vectors, when multiplied by the same random Gaussian matrix, tend to end up in a similar location. However, the L2 distances are preserved, which means this hash function works better when using Euclidean distance to train the embeddings rather than dot products. In this scheme, only the hash code computation differs; the Hamming distance treatment is exactly the same.</p>&#13;
&#13;
<p>The speedup from LSH is directly proportional to the number of bits of the hash code that have to be an exact match. Suppose only 8 bits of the hash code are used in the hash map; then the speedup is <span>2<sup>8</sup></span>, or 256 times the original. The trade-off for the speed is having to store the hash map in memory.<a data-primary="" data-startref="ASlocality16" data-type="indexterm" id="id1170"/><a data-primary="" data-startref="locality16" data-type="indexterm" id="id1171"/><a data-primary="" data-startref="Hlocality16" data-type="indexterm" id="id1172"/><a data-primary="" data-startref="LSH16" data-type="indexterm" id="id1173"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="k-d Trees" data-type="sect1"><div class="sect1" id="id171">&#13;
<h1>k-d Trees</h1>&#13;
&#13;
<p>A common strategy for speeding up computation in computer science is<a data-primary="divide and conquer strategy" data-type="indexterm" id="id1174"/><a data-primary="k-d trees" data-type="indexterm" id="kdtrees16"/><a data-primary="acceleration structures" data-secondary="k-d trees" data-type="indexterm" id="ASkdtree16"/> <em>divide and conquer</em>. In this scheme, the data is recursively partitioned into two halves, and only the half that is relevant to the search query is searched. In contrast to a linear <em>O</em>(<em>n</em>) in the number of items in the corpus scheme, a divide-and-conquer algorithm would be able to query a corpus in <em>O</em>(log2(<em>n</em>)) time, which is a substantial speedup if <em>n</em> is large.</p>&#13;
&#13;
<p>One such binary tree for vector spaces is called a <a href="https://oreil.ly/z0vFO"><em>k</em>-d tree</a>. Typically, to build a <em>k</em>-d tree, we compute the bounding box of all the points in the collection, find the longest edge of the bounding box and split it down the middle of that edge in the splitting dimension, and then partition the collection into two halves. If the median is used, the collection is more or less divided into two equal-numbered items; we say <em>more or less</em> because there might be ties along that split dimension. The recursive process stops when a small number of items is left in the leaf node. Many implementations of <em>k</em>-d trees exist—for example,  <a href="https://oreil.ly/iZZD9">SciPy’s <em>k</em>-d tree</a>.</p>&#13;
&#13;
<p>Although the speedup is substantial, this method tends to work when the number of feature dimensions of the vector is low. Also, similar to other methods, <em>k</em>-d trees work best when the L2 distance is the metric used for the embedding. Losses in retrieval can occur if the dot product was used for the similarity metric, as the <em>k</em>-d tree makes more sense for Euclidean space partitioning.</p>&#13;
&#13;
<p><a data-type="xref" href="#example-16-3">Example 16-3</a> provides sample code for splitting a batch of points along the largest dimension.</p>&#13;
<div data-type="example" id="example-16-3">&#13;
<h5><span class="label">Example 16-3. </span>Partitioning via a k-d tree</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">jax</code>&#13;
<code class="kn">import</code> <code class="nn">jax.numpy</code> <code class="k">as</code> <code class="nn">jnp</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">kdtree_partition</code><code class="p">(</code><code class="n">x</code><code class="p">:</code> <code class="n">jnp</code><code class="o">.</code><code class="n">ndarray</code><code class="p">):</code>&#13;
  <code class="sd">"""Finds the split plane and value for a batch of vectors x."""</code>&#13;
  <code class="c1"># First, find the bounding box.</code>&#13;
  <code class="n">bbox_min</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">min</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>&#13;
  <code class="n">bbox_max</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>&#13;
  <code class="c1"># Return the largest split dimension and value.</code>&#13;
  <code class="n">diff</code> <code class="o">=</code> <code class="n">bbox_max</code> <code class="o">-</code> <code class="n">bbox_min</code>&#13;
  <code class="n">split_dim</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">diff</code><code class="p">)</code>&#13;
  <code class="n">split_value</code> <code class="o">=</code> <code class="mf">0.5</code> <code class="o">*</code> <code class="p">(</code><code class="n">bbox_min</code><code class="p">[</code><code class="n">split_dim</code><code class="p">]</code> <code class="o">+</code> <code class="n">bbox_max</code><code class="p">[</code><code class="n">split_dim</code><code class="p">])</code>&#13;
  <code class="k">return</code> <code class="n">split_dim</code><code class="p">,</code> <code class="n">split_value</code>&#13;
&#13;
<code class="n">key</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>&#13;
<code class="n">x</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">normal</code><code class="p">(</code><code class="n">key</code><code class="p">,</code> <code class="p">[</code><code class="mi">256</code><code class="p">,</code> <code class="mi">3</code><code class="p">])</code> <code class="o">*</code> <code class="n">jnp</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">2</code><code class="p">])</code>&#13;
<code class="n">split_dim</code><code class="p">,</code> <code class="n">split_value</code> <code class="o">=</code> <code class="n">kdtree_partition</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="s2">"Split dimension </code><code class="si">%d</code><code class="s2"> at value </code><code class="si">%f</code><code class="s2">"</code> <code class="o">%</code> <code class="p">(</code><code class="n">split_dim</code><code class="p">,</code> <code class="n">split_value</code><code class="p">))</code>&#13;
&#13;
<code class="c1"># Partition the points into two groups, the left subtree</code>&#13;
<code class="c1"># has all the elements left of the splitting plane.</code>&#13;
<code class="n">left</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">where</code><code class="p">(</code><code class="n">x</code><code class="p">[:,</code> <code class="n">split_dim</code><code class="p">]</code> <code class="o">&lt;</code> <code class="n">split_value</code><code class="p">)</code>&#13;
<code class="n">right</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">where</code><code class="p">(</code><code class="n">x</code><code class="p">[:,</code> <code class="n">split_dim</code><code class="p">]</code> <code class="o">&gt;=</code> <code class="n">split_value</code><code class="p">)</code>&#13;
&#13;
<code class="n">Split</code> <code class="n">dimension</code> <code class="mi">1</code> <code class="n">at</code> <code class="n">value</code> <code class="o">-</code><code class="mf">0.352623</code></pre></div>&#13;
&#13;
<p>As you can see from the code, the <em>k</em>-d tree partitioning code can be as simple as splitting along the middle longest dimension. Other possibilities are splitting along the median of the longest dimension or <a href="https://oreil.ly/BxAf7">using a surface area heuristic</a>.</p>&#13;
&#13;
<p>A <em>k</em>-d tree is constructed by repeatedly partitioning the data along only one spatial dimension at a time (usually along the largest axis aligned to the spread of data); see <a data-type="xref" href="#kdtree_construction">Figure 16-1</a>.</p>&#13;
&#13;
<figure><div class="figure" id="kdtree_construction">&#13;
<img alt="KD-Tree construction" src="assets/brpj_1601.png"/>&#13;
<h6><span class="label">Figure 16-1. </span>k-d tree construction’s initial bounding box</h6>&#13;
</div></figure>&#13;
&#13;
<p>Partitions are recursively subdivided again, usually along the longest axis, until the number of points in the partition is fewer than a chosen small number; see <a data-type="xref" href="#kdtree_recursive">Figure 16-2</a>.</p>&#13;
&#13;
<p>The <em>k</em>-d tree lookup time is <em>O</em>(log2(<em>n</em>)) in <em>n</em>, the number of items in the corpus. The tree also requires a small overhead of memory to store the tree itself, which is  dominated by the number of leaf nodes, so it would be best to have a minimal number of items in a leaf to prevent splits that are too fine.</p>&#13;
&#13;
<figure><div class="figure" id="kdtree_recursive">&#13;
<img alt="KD-Tree recursive step" src="assets/brpj_1602.png"/>&#13;
<h6><span class="label">Figure 16-2. </span>k-d tree construction recursively partitioned</h6>&#13;
</div></figure>&#13;
&#13;
<p>From the root node, repeatedly check whether the query point (e.g., the item we are seeking nearest neighbors for) is in the left or right child of the root node, as shown in <a data-type="xref" href="#kdtree_query">Figure 16-3</a>. For example, use <code>go_left = x[split_dim] &lt; value_split[dim]</code>. In binary tree convention, the left child contains all points whose value at the split dimension are less than the split value. Hence if the query point’s value at the split dimension is less than the split value we go left, otherwise we go right. Recursively descend down the tree until reaching the leaf node; then exhaustively compute distances to all items in the leaf node.</p>&#13;
&#13;
<figure><div class="figure" id="kdtree_query">&#13;
<img alt="KD-Tree Query" src="assets/brpj_1603.png"/>&#13;
<h6><span class="label">Figure 16-3. </span>k-d tree query</h6>&#13;
</div></figure>&#13;
&#13;
<p class="less_space pagebreak-before">A <em>k</em>-d tree has a potential drawback. If an item is close to a splitting plane, that item would be considered on the other side of the tree. As a result, the item would not be considered as a nearest neighbor candidate. In some implementations of <em>k</em>-d trees, called<a data-primary="spill trees" data-type="indexterm" id="id1175"/> <em>spill trees</em>, both sides of a splitting plane are visited if the query point is close enough to the plane’s decision boundary. This change increases runtime a little bit for the benefit of more recall.<a data-primary="" data-startref="kdtrees16" data-type="indexterm" id="id1176"/><a data-primary="" data-startref="ASkdtree16" data-type="indexterm" id="id1177"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Hierarchical k-means" data-type="sect1"><div class="sect1" id="id172">&#13;
<h1>Hierarchical k-means</h1>&#13;
&#13;
<p>Another<a data-primary="acceleration structures" data-secondary="k-means clustering" data-type="indexterm" id="id1178"/><a data-primary="k-means clustering" data-type="indexterm" id="id1179"/><a data-primary="hierarchical k-means" data-type="indexterm" id="id1180"/> divide-and-conquer strategy that does scale to higher feature dimensions is <em>k-means clustering</em>. In this scheme, the corpus is clustered into <em>k</em> clusters and then recursively clustered into <em>k</em> more clusters until each cluster is smaller than a defined limit.</p>&#13;
&#13;
<p>An implementation of <em>k</em>-means can be found at <a href="https://oreil.ly/E45Lo">scikit-learn’s web page</a>.</p>&#13;
&#13;
<p>To build the clustering, first create cluster centroids at random from existing points (<a data-type="xref" href="#kmeans_construction">Figure 16-4</a>).</p>&#13;
&#13;
<figure><div class="figure" id="kmeans_construction">&#13;
<img alt="Kmeans Initialization" src="assets/brpj_1604.png"/>&#13;
<h6><span class="label">Figure 16-4. </span>k-means initialization</h6>&#13;
</div></figure>&#13;
&#13;
<p>Next, we assign all points to the cluster they are closest to. Then for each cluster, we take the average of all the assigned points as the new cluster center. We repeat until done, which can be a fixed number of steps. <a data-type="xref" href="#kmeans_construction2">Figure 16-5</a> illustrates this process. The output is then <em>k</em> cluster centers of points. The process can be repeated again for each cluster center, splitting again into <em>k</em> more clusters.</p>&#13;
&#13;
<figure><div class="figure" id="kmeans_construction2">&#13;
<img alt="Kmeans Clustering" src="assets/brpj_1605.png"/>&#13;
<h6><span class="label">Figure 16-5. </span>k-means clustering</h6>&#13;
</div></figure>&#13;
&#13;
<p>Again, the speedup is <em>O</em>(log(<em>n</em>)) in the number of items, but <em>k</em>-means is better adapted to clustering higher-dimensional data points than <em>k</em>-d trees.</p>&#13;
&#13;
<p>The querying for a <em>k</em>-means cluster is rather straightforward. You can find the closest cluster to the query point and then repeat the process for all subclusters until a leaf node is found; then all the items in the leaf node are scored against the query point.</p>&#13;
&#13;
<p>An alternative to <em>k</em>-means is to perform<a data-primary="singular value decomposition (SVD)" data-type="indexterm" id="id1181"/> SVD and use the first <em>k</em> eigenvectors as the clustering criteria. The use of SVD is interesting in that there exists closed form and approximate methods like <a href="https://oreil.ly/ZgZ2-">power iteration</a> for computing the eigenvectors. Using the dot product to compute affinity might be better suited to vectors trained using the dot product as the affinity metric.</p>&#13;
&#13;
<p>To learn more on this topic, you can consult <a href="https://oreil.ly/rMg-3">“Label Partitioning for Sublinear Ranking”</a> by Jason Weston et al. (including one of this book’s authors). The paper compares LSH, SVD, and hierarchical <em>k</em>-means. You’ll find a comparison of the speedup and the loss in retrieval, with the brute-force as a baseline.</p>&#13;
<div data-type="tip"><h1>Graph-Based ANN</h1>&#13;
<p>An<a data-primary="approximate nearest neighbors (ANN)" data-type="indexterm" id="id1182"/><a data-primary="graph-based ANN" data-type="indexterm" id="id1183"/> emerging trend in ANNs is using graph-based methods. Lately,<a data-primary="hierarchical navigable small worlds" data-type="indexterm" id="id1184"/> <em>hierarchical navigable small worlds</em> is a particularly popular approach. This <a href="https://oreil.ly/Z2ohy">graph algorithm</a> encodes proximity in multilayer structures and then relies on the common maxim that “the number of connectivity steps from one node to another is often surprisingly small.” In graph-based ANN methods, you often find one neighbor, and then traverse the edges connected to that neighbor to rapidly find others.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Cheaper Retrieval Methods" data-type="sect1"><div class="sect1" id="id232">&#13;
<h1>Cheaper Retrieval Methods</h1>&#13;
&#13;
<p>If<a data-primary="acceleration structures" data-secondary="cheaper retrieval methods" data-type="indexterm" id="id1185"/><a data-primary="retrieval" data-secondary="cheaper retrieval methods" data-type="indexterm" id="id1186"/> your corpus has the ability to do an item-wise cheap retrieval method, one way to speed up searches is to use the cheap retrieval method to obtain a small subset of items and then use the more expensive vector-based methods to rank the subset. One such cheap retrieval method is to make a posting list of the top co-occurrences of one item with another. Then when it comes to generating the candidates for ranking, gather all the top co-occurring items together (from a user’s preferred items, for example) and then score them together with the ML model. In this way, we do not have to score the entire corpus with the ML model but just a small subset.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id321">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>In this chapter, we showed a few ways to speed up the retrieval and scoring of items in a corpus, given a query vector, without losing too much in terms of recall and while still maintaining precision. No ANN method is perfect, as the acceleration structures depend on the distribution of the data, and this varies from dataset to dataset. We hope that this chapter provides a launching pad for you to explore various ways to make retrieval faster and sublinear in the number of items in the corpus.</p>&#13;
</div></section>&#13;
</div></section></body></html>