<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 13. Linear Regression" data-type="chapter" epub:type="chapter"><div class="chapter" id="linear-regression">
<h1><span class="label">Chapter 13. </span>Linear Regression</h1>
<section data-pdf-bookmark="13.0 Introduction" data-type="sect1"><div class="sect1" id="id301">
<h1>13.0 Introduction</h1>
<p><em>Linear regression</em> is one of the <a data-primary="linear regression" data-type="indexterm" id="ix_lin_reg_ch13"/><a data-primary="predictions and predicting" data-secondary="linear regression" data-type="indexterm" id="ix_predict_lin_reg"/>simplest supervised learning algorithms in our toolkit. If you have ever taken an introductory statistics course in college, likely the final topic you covered was linear regression. Linear regression and its extensions continue to be a common and useful method of making predictions when the target vector is a quantitative value (e.g., home price, age). In this chapter we will cover a variety of linear regression methods (and some extensions) for creating well-performing prediction models.</p>
</div></section>
<section data-pdf-bookmark="13.1 Fitting a Line" data-type="sect1"><div class="sect1" id="fitting-a-line">
<h1>13.1 Fitting a Line</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id516">
<h2>Problem</h2>
<p>You want to <a data-primary="linear regression" data-secondary="fitting a line" data-type="indexterm" id="ix_lin_reg_fit_data"/><a data-primary="fitting of data to a line" data-secondary="linear regression" data-type="indexterm" id="ix_fit_data_lin_reg"/>train a model that represents a linear relationship between
the feature and target vector.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id302">
<h2>Solution</h2>
<p>Use a <a data-primary="LinearRegression" data-type="indexterm" id="id1684"/>linear regression (in scikit-learn, <code>LinearRegression</code>):</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LinearRegression</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_regression</code>

<code class="c1"># Generate features matrix, target vector</code>
<code class="n">features</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="n">make_regression</code><code class="p">(</code><code class="n">n_samples</code> <code class="o">=</code> <code class="mi">100</code><code class="p">,</code>
                                   <code class="n">n_features</code> <code class="o">=</code> <code class="mi">3</code><code class="p">,</code>
                                   <code class="n">n_informative</code> <code class="o">=</code> <code class="mi">2</code><code class="p">,</code>
                                   <code class="n">n_targets</code> <code class="o">=</code> <code class="mi">1</code><code class="p">,</code>
                                   <code class="n">noise</code> <code class="o">=</code> <code class="mf">0.2</code><code class="p">,</code>
                                   <code class="n">coef</code> <code class="o">=</code> <code class="kc">False</code><code class="p">,</code>
                                   <code class="n">random_state</code> <code class="o">=</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Create linear regression</code>
<code class="n">regression</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">()</code>

<code class="c1"># Fit the linear regression</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">regression</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id303">
<h2>Discussion</h2>
<p>Linear regression assumes that the relationship between the features and
the target vector is approximately linear. <a data-primary="effect in linear regression" data-type="indexterm" id="id1685"/>That is, the <em>effect</em> (also
called <em>coefficient</em>, <em>weight</em>, or <em>parameter</em>) of the features on the target vector is constant. In our solution, for the sake of explanation, we have trained our model using only three features. This means our linear model will be:</p>
<div data-type="equation">
<math display="block">
<mrow>
<mover accent="true"><mi>y</mi> <mo>^</mo></mover>
<mo>=</mo>
<msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>0</mn> </msub>
<mo>+</mo>
<msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>1</mn> </msub>
<msub><mi>x</mi> <mn>1</mn> </msub>
<mo>+</mo>
<msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>2</mn> </msub>
<msub><mi>x</mi> <mn>2</mn> </msub>
<mo>+</mo>
<msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>3</mn> </msub>
<msub><mi>x</mi> <mn>3</mn> </msub>
<mo>+</mo>
<mi>ϵ</mi>
</mrow>
</math>
</div>
<p>where  <math display="inline"><mover accent="true"><mi>y</mi> <mo>^</mo></mover></math> is our target, <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math> is the data for a single feature, <math display="inline"><msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>1</mn> </msub></math>, <math display="inline"><msub><mover accent="true"><mi>β</mi><mo>^</mo></mover> <mn>2</mn> </msub></math>, and <math display="inline"><msub><mover accent="true"><mi>β</mi><mo>^</mo></mover> <mn>3</mn> </msub></math> are the coefficients identified by fitting
the model, and <math display="inline"><mi>ϵ</mi></math> is the error. After we have fit
our model, we can view the value of each parameter. For example,
<math display="inline"> <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>0</mn></msub></math>, also called the <em>bias</em> or <em>intercept</em>, can be viewed using <code>intercept_</code>:<a data-primary="bias or intercept, linear regression" data-type="indexterm" id="id1686"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View the intercept</code>
<code class="n">model</code><code class="o">.</code><code class="n">intercept_</code></pre>
<pre data-type="programlisting">-0.009650118178816669</pre>
<p>And <math display="inline"><msub><mover accent="true"><mi>β</mi><mo>^</mo></mover> <mn>1</mn> </msub></math> and <math display="inline"><msub><mover accent="true"><mi>β</mi><mo>^</mo></mover> <mn>2</mn> </msub></math> are shown using <code>coef_</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View the feature coefficients</code>
<code class="n">model</code><code class="o">.</code><code class="n">coef_</code></pre>
<pre data-type="programlisting">array([1.95531234e-02, 4.42087450e+01, 5.81494563e+01])</pre>
<p>In our dataset, the target value is a randomly generated continuous variable:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># First value in the target vector</code>
<code class="n">target</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code></pre>
<pre data-type="programlisting">-20.870747595269407</pre>
<p>Using the <code>predict</code> method, we can predict the output based on the input features:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Predict the target value of the first observation</code>
<code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">features</code><code class="p">)[</code><code class="mi">0</code><code class="p">]</code></pre>
<pre data-type="programlisting">-20.861927709296808</pre>
<p>Not bad! Our model was off only by about 0.01!</p>
<p>The major advantage of linear regression is its interpretability, in
large part because the coefficients of the model are the effect of a one-unit change on the target vector. Our model’s coefficient of the first feature was ~–0.02, meaning that we have the change in target for each additional unit change in the first feature.</p>
<p>Using the <code>score</code> function<a data-primary="score function, linear regression" data-type="indexterm" id="id1687"/>, we can also see how well our model performed on the data:</p>
<pre data-type="programlisting" id="source"># Print the score of the model on the training data
print(model.score(features, target))</pre>
<pre data-type="programlisting">0.9999901732607787</pre>
<p>The default score for linear regression in scikit learn is R<sup>2</sup>, which ranges from 0.0 (worst) to 1.0 (best). As we can see in this example, we are very close to the perfect value of 1.0. However it’s worth noting that we are evaluating this model on data it has already seen (the training data), where typically we’d evaluate on a held-out test set of data instead. Nonetheless, such a high score would bode well for our model in a real setting.<a data-primary="" data-startref="ix_fit_data_lin_reg" data-type="indexterm" id="id1688"/><a data-primary="" data-startref="ix_lin_reg_fit_data" data-type="indexterm" id="id1689"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="13.2 Handling Interactive Effects" data-type="sect1"><div class="sect1" id="handling-interactive-effects">
<h1>13.2 Handling Interactive Effects</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id517">
<h2>Problem</h2>
<p>You have a <a data-primary="linear regression" data-secondary="interactive effects, handling" data-type="indexterm" id="ix_lin_reg_inter_effect"/>feature whose effect on the target variable depends on
another feature.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id304">
<h2>Solution</h2>
<p>Create an <a data-primary="PolynomialFeatures" data-type="indexterm" id="ix_poly_nom_feat2"/>interaction term to capture that dependence using
scikit-learn’s 
<span class="keep-together"><code>PolynomialFeatures</code>:</span></p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LinearRegression</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">PolynomialFeatures</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_regression</code>

<code class="c1"># Generate features matrix, target vector</code>
<code class="n">features</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="n">make_regression</code><code class="p">(</code><code class="n">n_samples</code> <code class="o">=</code> <code class="mi">100</code><code class="p">,</code>
                                   <code class="n">n_features</code> <code class="o">=</code> <code class="mi">2</code><code class="p">,</code>
                                   <code class="n">n_informative</code> <code class="o">=</code> <code class="mi">2</code><code class="p">,</code>
                                   <code class="n">n_targets</code> <code class="o">=</code> <code class="mi">1</code><code class="p">,</code>
                                   <code class="n">noise</code> <code class="o">=</code> <code class="mf">0.2</code><code class="p">,</code>
                                   <code class="n">coef</code> <code class="o">=</code> <code class="kc">False</code><code class="p">,</code>
                                   <code class="n">random_state</code> <code class="o">=</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Create interaction term</code>
<code class="n">interaction</code> <code class="o">=</code> <code class="n">PolynomialFeatures</code><code class="p">(</code>
    <code class="n">degree</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">include_bias</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code> <code class="n">interaction_only</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">features_interaction</code> <code class="o">=</code> <code class="n">interaction</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Create linear regression</code>
<code class="n">regression</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">()</code>

<code class="c1"># Fit the linear regression</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">regression</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_interaction</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id305">
<h2>Discussion</h2>
<p>Sometimes a feature’s effect on our target variable is at least
partially dependent on another feature. For example, imagine a simple
coffee-based example where we have two binary features—​the presence
of sugar (<code>sugar</code>) and whether or not we have stirred (<code>stirred</code>)—and
we want to predict if the coffee tastes sweet. Just putting sugar in the
coffee (<code>sugar=1, stirred=0</code>) won’t make the coffee taste sweet (all the
sugar is at the bottom!) and just stirring the coffee without adding
sugar (<code>sugar=0, stirred=1</code>) won’t make it sweet either. Instead it is
the interaction of putting sugar in the coffee <em>and</em> stirring the coffee
(<code>sugar=1, stirred=1</code>) that will make a coffee taste sweet. The effects
of <code>sugar</code> and <code>stirred</code> on sweetness are dependent on each other. <a data-primary="interaction effect, linear regression" data-type="indexterm" id="id1690"/>In this
case we say there is an <em>interaction effect</em> between the features <code>sugar</code>
and <code>stirred</code>.</p>
<p>We can account for interaction effects by including a new feature comprising the product of corresponding values from the interacting features:</p>
<div data-type="equation">
<math display="block">
<mrow>
<mover accent="true"><mi>y</mi> <mo>^</mo></mover>
<mo>=</mo>
<msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>0</mn> </msub>
<mo>+</mo>
<msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>1</mn> </msub>
<msub><mi>x</mi> <mn>1</mn> </msub>
<mo>+</mo>
<msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>2</mn> </msub>
<msub><mi>x</mi> <mn>2</mn> </msub>
<mo>+</mo>
<msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>3</mn> </msub>
<msub><mi>x</mi> <mn>1</mn> </msub>
<msub><mi>x</mi> <mn>2</mn> </msub>
<mo>+</mo>
<mi>ϵ</mi>
</mrow>
</math>
</div>
<p>where <math display="inline"><msub><mi>x</mi><mn>1</mn></msub></math> and <math display="inline"><msub><mi>x</mi><mn>2</mn></msub></math> are the values of the <code>sugar</code> and <code>stirred</code>, respectively, and <math display="inline"><msub><mi>x</mi><mn>1</mn></msub><msub><mi>x</mi><mn>2</mn></msub></math> represents the interaction between the two.</p>
<p>In our solution, we used a dataset containing only two features. Here is
the first observation’s values for each of those features:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View the feature values for first observation</code>
<code class="n">features</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code></pre>
<pre data-type="programlisting">array([0.0465673 , 0.80186103])</pre>
<p>To create an interaction term, we simply multiply those two values
together for every observation:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import library</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>

<code class="c1"># For each observation, multiply the values of the first and second feature</code>
<code class="n">interaction_term</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">multiply</code><code class="p">(</code><code class="n">features</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">features</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">])</code></pre>
<p>We can then view the interaction term for the first observation:</p>
<pre class="less_space pagebreak-before" data-code-language="python" data-type="programlisting"><code class="c1"># View interaction term for first observation</code>
<code class="n">interaction_term</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code></pre>
<pre data-type="programlisting">0.037340501965846186</pre>
<p>However, while often we will have a substantive reason for believing
there is an interaction between two features, sometimes we will not. In
those cases it can be useful to use scikit-learn’s <code>PolynomialFeatures</code>
to create interaction terms for all combinations of features. We can
then use model selection strategies to identify the combination of
features and interaction terms that produces the best model.</p>
<p>To create interaction terms using <code>PolynomialFeatures</code>, there are three
important parameters we must set. Most important, <code>interaction_only=True</code> tells 
<span class="keep-together"><code>PolynomialFeatures</code></span> to return only interaction terms (and not polynomial features, which we will discuss in
<a data-type="xref" href="#fitting-a-non-linear-relationship">Recipe 13.3</a>). By default, <code>PolynomialFeatures</code> will add a feature
containing 1s called a <em>bias</em>. We can <a data-primary="bias or intercept, linear regression" data-type="indexterm" id="id1691"/>prevent that with
<code>include_bias=False</code>. Finally, the <code>degree</code> parameter determines the
maximum number of features to create interaction terms from (in case we
wanted to create an interaction term that is the combination of three
features). We can see the output of <code>PolynomialFeatures</code> from our
solution by checking to see if the first observation’s feature values
and interaction term value match our manually calculated version:<a data-primary="" data-startref="ix_lin_reg_inter_effect" data-type="indexterm" id="id1692"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View the values of the first observation</code>
<code class="n">features_interaction</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code></pre>
<pre data-type="programlisting">array([0.0465673 , 0.80186103, 0.0373405 ])</pre>
</div></section>
</div></section>
<section data-pdf-bookmark="13.3 Fitting a Nonlinear Relationship" data-type="sect1"><div class="sect1" id="fitting-a-non-linear-relationship">
<h1>13.3 Fitting a Nonlinear Relationship</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id518">
<h2>Problem</h2>
<p>You want to <a data-primary="linear regression" data-secondary="fitting nonlinear relationship" data-type="indexterm" id="ix_lin_reg_nonlin"/><a data-primary="fitting of data to a line" data-secondary="linear regression" data-type="indexterm" id="ix_fit_data_lin_reg2"/>model a nonlinear relationship.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id306">
<h2>Solution</h2>
<p>Create a <a data-primary="regression and regressors" data-secondary="polynomial regression" data-type="indexterm" id="ix_regress_poly"/><a data-primary="polynomial regression" data-type="indexterm" id="ix_poly_reg"/>polynomial regression by including polynomial features in a
linear regression model:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LinearRegression</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">PolynomialFeatures</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_regression</code>

<code class="c1"># Generate features matrix, target vector</code>
<code class="n">features</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="n">make_regression</code><code class="p">(</code><code class="n">n_samples</code> <code class="o">=</code> <code class="mi">100</code><code class="p">,</code>
                                   <code class="n">n_features</code> <code class="o">=</code> <code class="mi">3</code><code class="p">,</code>
                                   <code class="n">n_informative</code> <code class="o">=</code> <code class="mi">2</code><code class="p">,</code>
                                   <code class="n">n_targets</code> <code class="o">=</code> <code class="mi">1</code><code class="p">,</code>
                                   <code class="n">noise</code> <code class="o">=</code> <code class="mf">0.2</code><code class="p">,</code>
                                   <code class="n">coef</code> <code class="o">=</code> <code class="kc">False</code><code class="p">,</code>
                                   <code class="n">random_state</code> <code class="o">=</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Create polynomial features x^2 and x^3</code>
<code class="n">polynomial</code> <code class="o">=</code> <code class="n">PolynomialFeatures</code><code class="p">(</code><code class="n">degree</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">include_bias</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>
<code class="n">features_polynomial</code> <code class="o">=</code> <code class="n">polynomial</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Create linear regression</code>
<code class="n">regression</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">()</code>

<code class="c1"># Fit the linear regression</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">regression</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_polynomial</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id307">
<h2>Discussion</h2>
<p>So far we have discussed modeling only linear relationships. An example
of a linear relationship would be the number of stories a building has
and the building’s height. In linear regression, we assume the effect of
number of stories and building height is approximately constant, meaning
a 20-story building will be roughly twice as high as a 10-story
building, which will be roughly twice as high as a 5-story building.
Many relationships of interest, however, are not strictly linear.</p>
<p>Often we want to model a nonlinear relationship—for example, the
relationship between the number of hours a student studies and the score
she gets on a test. Intuitively, we can imagine there is a big
difference in test scores between students who study for one hour
compared to students who did not study at all. However, there is a
much smaller difference in test scores between a student who studied for
99 hours and a student who studied for 100 hours. The effect that one hour of
studying has on a student’s test score decreases as the number of hours
increases.</p>
<p>Polynomial regression is an extension of linear regression that allows us
to model nonlinear relationships. To create a polynomial regression,
convert the linear function we used in <a data-type="xref" href="#fitting-a-line">Recipe 13.1</a>:</p>
<div data-type="equation">
<math display="block">
<mrow>
<mover accent="true"><mi>y</mi> <mo>^</mo></mover>
<mo>=</mo>
<msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>0</mn> </msub>
<mo>+</mo>
<msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>1</mn> </msub>
<msub><mi>x</mi> <mn>1</mn> </msub>
<mo>+</mo>
<mi>ϵ</mi>
</mrow>
</math>
</div>
<p>into a polynomial function by adding polynomial features:</p>
<div data-type="equation">
<math display="block">
<mrow>
<mover accent="true"><mi>y</mi> <mo>^</mo></mover>
<mo>=</mo>
<msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>0</mn> </msub>
<mo>+</mo>
<msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>1</mn> </msub>
<msub><mi>x</mi> <mn>1</mn> </msub>
<mo>+</mo>
<msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>2</mn> </msub>
<msup><msub><mi>x</mi> <mn>1</mn></msub> <mn>2</mn> </msup>
<mo>+</mo>
<mo>.</mo>
<mo>.</mo>
<mo>.</mo>
<mo>+</mo>
<msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mi>d</mi> </msub>
<msup><msub><mi>x</mi> <mn>1</mn></msub> <mi>d</mi></msup>
<mo>+</mo>
<mi>ϵ</mi>
</mrow>
</math>
</div>
<p>where <math display="inline"><mi>d</mi></math> is the degree of the polynomial. How are we able
to use a linear regression for a nonlinear function? The answer is that
we do not change how the linear regression fits the model but rather
only add polynomial features. That is, the linear regression does
not “know” that the <math display="inline"><msup><mi>x</mi><mn>2</mn></msup></math> is a quadratic transformation of <math display="inline"><mi>x</mi></math>. It just considers it one more variable.</p>
<p>A more practical description might be in order. To model
nonlinear relationships, we can create new features that raise an
existing feature, <math display="inline"><mi>x</mi></math>, up to some power:
<math display="inline"><msup><mi>x</mi><mn>2</mn></msup></math>, <math display="inline"><msup><mi>x</mi><mn>3</mn></msup></math>, and so on. The more of these new features we add, the more flexible the “line” created by our model. To make this more explicit, imagine we want to create a polynomial to the third degree. For the sake of simplicity, we will focus on only one observation (the first observation in the dataset), <math display="inline"><mi>x</mi></math>[0]:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View first observation</code>
<code class="n">features</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code></pre>
<pre data-type="programlisting">array([-0.61175641])</pre>
<p>To create a polynomial feature, we would raise the first observation’s
value to the second degree, <math display="inline"><msup><msub><mi>x</mi> <mn>1</mn></msub> <mn>2</mn> </msup></math>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View first observation raised to the second power, x^2</code>
<code class="n">features</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">**</code><code class="mi">2</code></pre>
<pre data-type="programlisting">array([0.37424591])</pre>
<p>This would be our new feature. We would then also raise the
first observation’s value to the third degree, <math display="inline"><msup><msub><mi>x</mi> <mn>1</mn></msub> <mn>3</mn> </msup></math>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View first observation raised to the third power, x^3</code>
<code class="n">features</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">**</code><code class="mi">3</code></pre>
<pre data-type="programlisting">array([-0.22894734])</pre>
<p>By including all three features (<math display="inline"><mi>x</mi></math>, <math display="inline"><msup><mi>x</mi><mn>2</mn></msup></math>,
and <math display="inline"><msup><mi>x</mi><mn>3</mn></msup></math>) in our feature matrix and then running a linear regression, we have conducted a polynomial regression:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View the first observation's values for x, x^2, and x^3</code>
<code class="n">features_polynomial</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code></pre>
<pre data-type="programlisting">array([-0.61175641,  0.37424591, -0.22894734])</pre>
<p><code>PolynomialFeatures</code> has two important parameters. First, <code>degree</code>
determines the maximum number of degrees for the polynomial features.
For example, <code>degree=3</code> will generate <math display="inline"><msup><mi>x</mi><mn>2</mn></msup></math> and
<math display="inline"><msup><mi>x</mi><mn>3</mn></msup></math>. Second, by default <code>PolynomialFeatures</code> includes a feature containing only 1s (called a bias). We can remove that by setting <code>include_bias=False</code>.<a data-primary="" data-startref="ix_poly_nom_feat2" data-type="indexterm" id="id1693"/><a data-primary="" data-startref="ix_fit_data_lin_reg2" data-type="indexterm" id="id1694"/><a data-primary="" data-startref="ix_lin_reg_nonlin" data-type="indexterm" id="id1695"/><a data-primary="" data-startref="ix_poly_reg" data-type="indexterm" id="id1696"/><a data-primary="" data-startref="ix_regress_poly" data-type="indexterm" id="id1697"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="13.4 Reducing Variance with Regularization" data-type="sect1"><div class="sect1" id="reducing-variance-with-regularization">
<h1>13.4 Reducing Variance with Regularization</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id519">
<h2>Problem</h2>
<p>You want to <a data-primary="regularization" data-secondary="reducing variance with" data-type="indexterm" id="ix_regul_red_var"/><a data-primary="linear regression" data-secondary="regularization to reduce variance" data-type="indexterm" id="ix_lin_reg_red_va_regul"/>reduce the variance of your linear regression model.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id308">
<h2>Solution</h2>
<p>Use a <a data-primary="shrinkage penalty" data-type="indexterm" id="id1698"/>learning algorithm that includes a <em>shrinkage penalty</em> (also called <em>regularization</em>) like ridge regression and lasso regression:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">Ridge</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_regression</code>

<code class="c1"># Generate features matrix, target vector</code>
<code class="n">features</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="n">make_regression</code><code class="p">(</code><code class="n">n_samples</code> <code class="o">=</code> <code class="mi">100</code><code class="p">,</code>
                                   <code class="n">n_features</code> <code class="o">=</code> <code class="mi">3</code><code class="p">,</code>
                                   <code class="n">n_informative</code> <code class="o">=</code> <code class="mi">2</code><code class="p">,</code>
                                   <code class="n">n_targets</code> <code class="o">=</code> <code class="mi">1</code><code class="p">,</code>
                                   <code class="n">noise</code> <code class="o">=</code> <code class="mf">0.2</code><code class="p">,</code>
                                   <code class="n">coef</code> <code class="o">=</code> <code class="kc">False</code><code class="p">,</code>
                                   <code class="n">random_state</code> <code class="o">=</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Standardize features</code>
<code class="n">scaler</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>
<code class="n">features_standardized</code> <code class="o">=</code> <code class="n">scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Create ridge regression with an alpha value</code>
<code class="n">regression</code> <code class="o">=</code> <code class="n">Ridge</code><code class="p">(</code><code class="n">alpha</code><code class="o">=</code><code class="mf">0.5</code><code class="p">)</code>

<code class="c1"># Fit the linear regression</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">regression</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_standardized</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id309">
<h2>Discussion</h2>
<p>In <a data-primary="residual sum of squares (RSS)" data-type="indexterm" id="id1699"/><a data-primary="RSS (residual sum of squares)" data-type="indexterm" id="id1700"/>standard linear regression the model trains to minimize the sum of
squared error between the true (<math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math>) and prediction (<math display="inline"><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover> <mi>i</mi> </msub></math>) target values, or residual sum of squares (RSS):</p>
<div data-type="equation">
<math display="block">
<mstyle displaystyle="true" scriptlevel="0">
<mrow>
<mi>R</mi>
<mi>S</mi>
<mi>S</mi>
<mo>=</mo>
<munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </munderover>
<msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi> </msub><mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>i</mi> </msub><mo>)</mo></mrow> <mn>2</mn> </msup>
</mrow>
</mstyle>
</math>
</div>
<p>Regularized <a data-primary="regression and regressors" data-secondary="ridge regression" data-type="indexterm" id="id1701"/><a data-primary="ridge regression" data-type="indexterm" id="id1702"/>regression learners are similar, except they attempt to
minimize RSS <em>and</em> some penalty for the total size of the coefficient
values, called a <em>shrinkage penalty</em> because it attempts to “shrink” the
model. There are two common types of regularized learners for linear
regression: ridge regression and the lasso. The only formal difference
is the type of shrinkage penalty used. In <em>ridge regression</em>, the
shrinkage penalty is a tuning hyperparameter multiplied by the squared sum of all coefficients:</p>
<div data-type="equation">
<math display="block">
<mrow>
<mtext>RSS</mtext>
<mo>+</mo>
<mi>α</mi>
<munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>p</mi> </munderover>
<msup><msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mi>j</mi></msub> <mn>2</mn> </msup>
</mrow>
</math>
</div>
<p>where <math display="inline"><msub><mover accent="true"><mi>β</mi><mo>^</mo></mover> <mi>j</mi> </msub></math> is the coefficient of the
<math display="inline"><mi>j</mi></math>th of <math display="inline"><mi>p</mi></math> features and <math display="inline"><mi>α</mi></math> is a hyperparameter (discussed next). The <em>lasso</em> is similar, except the shrinkage penalty is a tuning hyperparameter multiplied by the sum of the absolute value of all coefficients:</p>
<div data-type="equation">
<math display="block">
<mrow>
<mfrac><mn>1</mn> <mrow><mn>2</mn><mi>n</mi></mrow></mfrac>
<mtext>RSS</mtext>
<mo>+</mo>
<mi>α</mi>
<munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>p</mi> </munderover>
<mfenced close="|" open="|" separators="">
<msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mi>j</mi> </msub>
</mfenced>
</mrow>
</math>
</div>
<p>where <math display="inline"><mi>n</mi></math> is the number of observations. So which one should
we use? As a very general rule of thumb, ridge regression often produces
slightly better predictions than lasso, but lasso (for reasons we
will discuss in <a data-type="xref" href="#reducing-features-with-lasso-regression">Recipe 13.5</a>) produces more interpretable models. If
we want a <a data-primary="elastic net" data-type="indexterm" id="id1703"/>balance between ridge and lasso’s penalty functions we can use
<em>elastic net</em>, which is simply a regression model with both penalties
included. Regardless of which one we use, both ridge and lasso
regressions can penalize large or complex models by including
coefficient values in the loss function we are trying to minimize.</p>
<p>The hyperparameter, <math display="inline"><mi>α</mi></math>, lets us control how much we
penalize the coefficients, with higher values of <math display="inline"><mi>α</mi></math>
creating simpler models. The ideal value of <math display="inline"><mi>α</mi></math>
should be tuned like any other hyperparameter. In scikit-learn,
<math display="inline"><mi>α</mi></math> is set using the <code>alpha</code> parameter.</p>
<p>scikit-learn <a data-primary="cross-validation (CV) of ML models" data-secondary="RidgeCV" data-type="indexterm" id="id1704"/><a data-primary="RidgeCV" data-type="indexterm" id="id1705"/>includes a <code>RidgeCV</code> method that allows us to select the ideal value for <math display="inline"><mi>α</mi></math>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">RidgeCV</code>

<code class="c1"># Create ridge regression with three alpha values</code>
<code class="n">regr_cv</code> <code class="o">=</code> <code class="n">RidgeCV</code><code class="p">(</code><code class="n">alphas</code><code class="o">=</code><code class="p">[</code><code class="mf">0.1</code><code class="p">,</code> <code class="mf">1.0</code><code class="p">,</code> <code class="mf">10.0</code><code class="p">])</code>

<code class="c1"># Fit the linear regression</code>
<code class="n">model_cv</code> <code class="o">=</code> <code class="n">regr_cv</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_standardized</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>

<code class="c1"># View coefficients</code>
<code class="n">model_cv</code><code class="o">.</code><code class="n">coef_</code></pre>
<pre data-type="programlisting">array([1.29223201e-02, 4.40972291e+01, 5.38979372e+01])</pre>
<p>We can then easily view the best model’s <math display="inline"><mi>α</mi></math> value:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View alpha</code>
<code class="n">model_cv</code><code class="o">.</code><code class="n">alpha_</code></pre>
<pre data-type="programlisting">0.1</pre>
<p>One final note: because in linear regression the value of the
coefficients is partially determined by the scale of the feature, and in
regularized models all coefficients are summed together, we must make
sure to standardize the feature prior to training.<a data-primary="" data-startref="ix_lin_reg_red_va_regul" data-type="indexterm" id="id1706"/><a data-primary="" data-startref="ix_regul_red_var" data-type="indexterm" id="id1707"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="13.5 Reducing Features with Lasso Regression" data-type="sect1"><div class="sect1" id="reducing-features-with-lasso-regression">
<h1>13.5 Reducing Features with Lasso Regression</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id520">
<h2>Problem</h2>
<p>You want to <a data-primary="regularization" data-secondary="reducing features with" data-type="indexterm" id="id1708"/>simplify your linear regression model by reducing the number
of 
<span class="keep-together">features.</span></p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id521">
<h2>Solution</h2>
<p>Use a <a data-primary="linear regression" data-secondary="lasso regression to reduce features" data-type="indexterm" id="id1709"/><a data-primary="feature extraction" data-secondary="with lasso regression" data-secondary-sortas="lasso regression" data-type="indexterm" id="id1710"/>lasso regression:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">Lasso</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_regression</code>

<code class="c1"># Generate features matrix, target vector</code>
<code class="n">features</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="n">make_regression</code><code class="p">(</code><code class="n">n_samples</code> <code class="o">=</code> <code class="mi">100</code><code class="p">,</code>
                                   <code class="n">n_features</code> <code class="o">=</code> <code class="mi">3</code><code class="p">,</code>
                                   <code class="n">n_informative</code> <code class="o">=</code> <code class="mi">2</code><code class="p">,</code>
                                   <code class="n">n_targets</code> <code class="o">=</code> <code class="mi">1</code><code class="p">,</code>
                                   <code class="n">noise</code> <code class="o">=</code> <code class="mf">0.2</code><code class="p">,</code>
                                   <code class="n">coef</code> <code class="o">=</code> <code class="kc">False</code><code class="p">,</code>
                                   <code class="n">random_state</code> <code class="o">=</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Standardize features</code>
<code class="n">scaler</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>
<code class="n">features_standardized</code> <code class="o">=</code> <code class="n">scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Create lasso regression with alpha value</code>
<code class="n">regression</code> <code class="o">=</code> <code class="n">Lasso</code><code class="p">(</code><code class="n">alpha</code><code class="o">=</code><code class="mf">0.5</code><code class="p">)</code>

<code class="c1"># Fit the linear regression</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">regression</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_standardized</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id310">
<h2>Discussion</h2>
<p>One interesting characteristic of lasso regression’s penalty is that it
can shrink the coefficients of a model to zero, effectively reducing the
number of features in the model. For example, in our solution we set
<code>alpha</code> to <code>0.5</code>, and we can see that many of the coefficients are 0,
meaning their corresponding features are not used in the model:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View coefficients</code>
<code class="n">model</code><code class="o">.</code><code class="n">coef_</code></pre>
<pre data-type="programlisting">array([-0.        , 43.58618393, 53.39523724])</pre>
<p class="less_space pagebreak-before">However, if we increase α to a much higher value, we
see that literally none of the features are being used:<a data-primary="" data-startref="ix_lin_reg_ch13" data-type="indexterm" id="id1711"/><a data-primary="" data-startref="ix_predict_lin_reg" data-type="indexterm" id="id1712"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create lasso regression with a high alpha</code>
<code class="n">regression_a10</code> <code class="o">=</code> <code class="n">Lasso</code><code class="p">(</code><code class="n">alpha</code><code class="o">=</code><code class="mi">10</code><code class="p">)</code>
<code class="n">model_a10</code> <code class="o">=</code> <code class="n">regression_a10</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_standardized</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>
<code class="n">model_a10</code><code class="o">.</code><code class="n">coef_</code></pre>
<pre data-type="programlisting">array([-0.        , 32.92181899, 42.73086731])</pre>
<p>The practical benefit of this effect is that it means we could
include 100 features in our feature matrix and then, through adjusting
lasso’s α hyperparameter, produce a model that uses
only 10 (for instance) of the most important features. This lets us
reduce variance while improving the interpretability of our model (since
fewer features are easier to explain).</p>
</div></section>
</div></section>
</div></section></div></body></html>