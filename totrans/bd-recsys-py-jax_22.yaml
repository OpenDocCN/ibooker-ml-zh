- en: Chapter 17\. Sequential Recommenders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In our journey so far, you’ve learned about a variety of features that appear
    as explicit or as latent components in the recommendation problem. One kind of
    feature, which has appeared implicitly, is the history of previous recommendations
    and interactions. You may wish to protest here: “All of the work we’ve done so
    far considers the previous recommendations and interactions! We’ve even learned
    about prequential training data.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'That is true, but it fails to account for more explicit relationships between
    the *sequence of recommendations leading up to the inference request*. Let’s look
    at an example to distinguish the two. Your video-streaming website knows that
    you’ve previously seen all of Darren Aronofsky’s films, so when *The Whale* is
    released, the website is very likely to recommend it. But this type of recommendation
    is different from one you might receive after finishing episode 10 of *Succession*.
    You may have been watching Aronofsky films over a long time period—*Pi* many years
    ago and *Black Swan* earlier this year. But you have been watching an episode
    of *Succession* each night this week, and your entire recent history is made up
    of Logan Roy. This latter example is a sequential recommendation problem: using
    the most recent ordered list of interactions to predict what you’ll enjoy next.'
  prefs: []
  type: TYPE_NORMAL
- en: In terms of the modeling objective, the recommenders we’ve seen use pairwise
    relationships between potential recommendations and historical interactions. Sequential
    recommendation aims to predict users’ next actions based on the sequential interactions
    in the past that may be of much higher *order*—i.e., combinations of interactions
    among three or more items. Most sequential recommendation models involve sequential
    data-mining techniques such as Markov chains, recurrent neural networks (RNNs),
    and self-attention. These models usually take into consideration short-term user
    behavior and are less sensitive, even oblivious, to the global user preferences
    that have stabilized over time.
  prefs: []
  type: TYPE_NORMAL
- en: Initial work in sequential recommendations focused on modeling the transitions
    between successive items. These used Markov chains and translation-based methods.
    As deep learning methods showed more and more promise in modeling sequential data—such
    as their biggest success in NLP—there have been many attempts to use neural network
    architectures to model sequential dynamics of a user’s interaction history. Early
    successes in this direction include GRU4Rec using an RNN to model users’ sequential
    interactions. Recently, transformer architectures have demonstrated superior performance
    for sequential data modeling. The transformer architecture lends itself to efficient
    parallelization and is effective at modeling long-range sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Markov Chains
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Despite mining for relationships to historical recommendations, the models we’ve
    been considering often fail to capture sequential patterns in user behavior, thereby
    disregarding the chronological order of user interactions. To address this shortcoming,
    sequential recommender systems were developed, incorporating techniques like Markov
    chains to model the temporal dependencies between items.
  prefs: []
  type: TYPE_NORMAL
- en: A *Markov chain* is a stochastic model that operates on the principle of *memorylessness*.
    It models the probability of transitioning from one state to another—given the
    current state—without considering the sequence of preceding events. Markov chains
    model the sequential behavior of users by considering each state as an item, and
    the transition probabilities as the likelihood of a user interacting with a certain
    item after the current one.
  prefs: []
  type: TYPE_NORMAL
- en: The first-order Markov chain, in which the future state depends solely on the
    current state, was a common strategy in early sequential recommenders. Despite
    its simplicity, the first-order Markov chain is effective in capturing short-term,
    item-to-item transition patterns, improving the quality of recommendations over
    nonsequential methods.
  prefs: []
  type: TYPE_NORMAL
- en: Take, for example, our preceding *Succession* example. If you’re using only
    a first-order Markov chain, a really great heuristic would be “What is the next
    episode in the series, if it’s a series; otherwise, fall back on a collaborative
    filtering (CF) model.” You can see that for a huge percentage of watch hours,
    this naive first-order chain would simply tell the user to watch the next episode
    of a show. Not particularly enlightening, but a good sign. When you abstract this
    out further, you start to get more powerful methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first-order assumption does not always hold in real-world applications,
    as user behavior is often influenced by a longer history of interactions. To overcome
    this limitation, higher-order Markov chains look further back: the next state
    is determined by a set of previous states, providing a richer model of user behavior.
    Nevertheless, it’s crucial to select the appropriate order, as too high an order
    may lead to overfitting and sparsity of the transition matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: Order-Two Markov Chain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s consider an example of an *order-two Markov chain* model using the weather.
    Assume we have three states: sunny ( <math alttext="upper S"><mi>S</mi></math>
    ), cloudy ( <math alttext="upper C"><mi>C</mi></math> ), and rainy ( <math alttext="upper
    R"><mi>R</mi></math> ).'
  prefs: []
  type: TYPE_NORMAL
- en: In an order-two Markov chain, the weather of today ( <math alttext="t"><mi>t</mi></math>
    ) would depend on the weather of yesterday ( <math alttext="t minus 1"><mrow><mi>t</mi>
    <mo>-</mo> <mn>1</mn></mrow></math> ) and the day before yesterday ( <math alttext="t
    minus 2"><mrow><mi>t</mi> <mo>-</mo> <mn>2</mn></mrow></math> ). The transition
    probability can be denoted as <math alttext="upper P left-parenthesis upper S
    Subscript t Baseline vertical-bar upper S Subscript t minus 1 Baseline comma upper
    S Subscript t minus 2 Baseline right-parenthesis"><mrow><mi>P</mi> <mo>(</mo>
    <msub><mi>S</mi> <mi>t</mi></msub> <mo>|</mo> <msub><mi>S</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>,</mo> <msub><mi>S</mi> <mrow><mi>t</mi><mo>-</mo><mn>2</mn></mrow></msub>
    <mo>)</mo></mrow></math> .
  prefs: []
  type: TYPE_NORMAL
- en: 'The Markov chain can be defined by a transition matrix that provides the probabilities
    of transitioning from one state to another. However, because we’re dealing with
    an order-two Markov chain, we would have a transition tensor instead. For simplicity,
    let’s say we have the following transition probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartLayout 1st Row 1st Column upper P left-parenthesis upper
    S vertical-bar upper S comma upper S right-parenthesis 2nd Column equals 0.7 comma
    upper P left-parenthesis upper C vertical-bar upper S comma upper S right-parenthesis
    equals 0.2 comma upper P left-parenthesis upper R vertical-bar upper S comma upper
    S right-parenthesis equals 0.1 comma 2nd Row 1st Column upper P left-parenthesis
    upper S vertical-bar upper S comma upper C right-parenthesis 2nd Column equals
    0.3 comma upper P left-parenthesis upper C vertical-bar upper S comma upper C
    right-parenthesis equals 0.4 comma upper P left-parenthesis upper R vertical-bar
    upper S comma upper C right-parenthesis equals 0.3 comma 3rd Row 1st Column Blank
    2nd Column ellipsis EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mi>P</mi> <mo>(</mo> <mi>S</mi> <mo>|</mo> <mi>S</mi>
    <mo>,</mo> <mi>S</mi> <mo>)</mo></mrow></mtd> <mtd><mrow><mo>=</mo> <mn>0</mn>
    <mo>.</mo> <mn>7</mn> <mo>,</mo> <mi>P</mi> <mo>(</mo> <mi>C</mi> <mo>|</mo> <mi>S</mi>
    <mo>,</mo> <mi>S</mi> <mo>)</mo> <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>2</mn> <mo>,</mo>
    <mi>P</mi> <mo>(</mo> <mi>R</mi> <mo>|</mo> <mi>S</mi> <mo>,</mo> <mi>S</mi> <mo>)</mo>
    <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>1</mn> <mo>,</mo></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><mi>P</mi> <mo>(</mo> <mi>S</mi> <mo>|</mo> <mi>S</mi>
    <mo>,</mo> <mi>C</mi> <mo>)</mo></mrow></mtd> <mtd><mrow><mo>=</mo> <mn>0</mn>
    <mo>.</mo> <mn>3</mn> <mo>,</mo> <mi>P</mi> <mo>(</mo> <mi>C</mi> <mo>|</mo> <mi>S</mi>
    <mo>,</mo> <mi>C</mi> <mo>)</mo> <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>4</mn> <mo>,</mo>
    <mi>P</mi> <mo>(</mo> <mi>R</mi> <mo>|</mo> <mi>S</mi> <mo>,</mo> <mi>C</mi> <mo>)</mo>
    <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>3</mn> <mo>,</mo></mrow></mtd></mtr> <mtr><mtd><mo>...</mo></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: You can visualize these probabilities in a three-dimensional cube. The first
    two dimensions represent the state of today and yesterday, and the third dimension
    represents the possible states of tomorrow.
  prefs: []
  type: TYPE_NORMAL
- en: If the weather was sunny for the last two days and we want to predict the weather
    for tomorrow, we would look at the transition probabilities starting with <math
    alttext="left-parenthesis upper S comma upper S right-parenthesis"><mrow><mo>(</mo>
    <mi>S</mi> <mo>,</mo> <mi>S</mi> <mo>)</mo></mrow></math> , which are <math alttext="upper
    P left-parenthesis upper S vertical-bar upper S comma upper S right-parenthesis
    equals 0.7"><mrow><mi>P</mi> <mo>(</mo> <mi>S</mi> <mo>|</mo> <mi>S</mi> <mo>,</mo>
    <mi>S</mi> <mo>)</mo> <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>7</mn></mrow></math>
    , <math alttext="upper P left-parenthesis upper C vertical-bar upper S comma upper
    S right-parenthesis equals 0.2"><mrow><mi>P</mi> <mo>(</mo> <mi>C</mi> <mo>|</mo>
    <mi>S</mi> <mo>,</mo> <mi>S</mi> <mo>)</mo> <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>2</mn></mrow></math>
    , and <math alttext="upper P left-parenthesis upper R vertical-bar upper S comma
    upper S right-parenthesis equals 0.1"><mrow><mi>P</mi> <mo>(</mo> <mi>R</mi> <mo>|</mo>
    <mi>S</mi> <mo>,</mo> <mi>S</mi> <mo>)</mo> <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>1</mn></mrow></math>
    . Therefore, according to our model, there’s a 70% chance that it will be sunny,
    a 20% chance that it will be cloudy, and a 10% chance that it will be rainy.
  prefs: []
  type: TYPE_NORMAL
- en: The probabilities in the transition matrix (or tensor) are typically estimated
    from data. If you have a historical record of the weather for several years, you
    can count the number of times each transition occurs and divide by the total number
    of transitions to estimate the probability.
  prefs: []
  type: TYPE_NORMAL
- en: This is only a basic demonstration of an order-two Markov chain. In real applications,
    the states might be much more numerous and the transition matrix much larger,
    but the principles remain the same.
  prefs: []
  type: TYPE_NORMAL
- en: Other Markov Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A more advanced Markovian approach is the *Markov decision process* (*MDP*),
    which extends the Markov chain by introducing actions and rewards. In the context
    of recommender systems, each action could represent a recommendation, and the
    reward could be the user’s response to the recommendation. By incorporating user
    feedback, the MDP can learn more personalized recommendation strategies.
  prefs: []
  type: TYPE_NORMAL
- en: MDPs are defined by a tuple <math alttext="left-parenthesis upper S comma upper
    A comma upper P comma upper R right-parenthesis"><mrow><mo>(</mo> <mi>S</mi> <mo>,</mo>
    <mi>A</mi> <mo>,</mo> <mi>P</mi> <mo>,</mo> <mi>R</mi> <mo>)</mo></mrow></math>
    , where <math alttext="upper S"><mi>S</mi></math> is the set of states, <math
    alttext="upper A"><mi>A</mi></math> is the set of actions, <math alttext="upper
    P"><mi>P</mi></math> is the state transition probability matrix, and <math alttext="upper
    R"><mi>R</mi></math> is the reward function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use a simplified MDP for a movie recommender system as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: States ( <math alttext="upper S"><mi>S</mi></math> )
  prefs: []
  type: TYPE_NORMAL
- en: 'These could represent the genres of movies a user has watched in the past.
    For simplicity, let’s say we have three states: Comedy ( <math alttext="upper
    C"><mi>C</mi></math> ), Drama ( <math alttext="upper D"><mi>D</mi></math> ), and
    Action ( <math alttext="upper A"><mi>A</mi></math> ).'
  prefs: []
  type: TYPE_NORMAL
- en: Actions ( <math alttext="upper A"><mi>A</mi></math> )
  prefs: []
  type: TYPE_NORMAL
- en: 'These could represent the movies that can be recommended. For this example,
    let’s say we have five actions (movies): Movies 1, 2, 3, 4, and 5.'
  prefs: []
  type: TYPE_NORMAL
- en: Transition probabilities ( <math alttext="upper P"><mi>P</mi></math> )
  prefs: []
  type: TYPE_NORMAL
- en: This represents the likelihood of transitioning from one state to another, given
    a specific action. For instance, if the user just watched a Drama ( <math alttext="upper
    D"><mi>D</mi></math> ) and we recommend Movie 3 (which is an Action movie), the
    transition probability <math alttext="upper P left-parenthesis upper A vertical-bar
    upper D comma upper M o v i e Baseline 3 right-parenthesis"><mrow><mi>P</mi> <mo>(</mo>
    <mi>A</mi> <mo>|</mo> <mi>D</mi> <mo>,</mo> <mi>M</mi> <mi>o</mi> <mi>v</mi> <mi>i</mi>
    <mi>e</mi> <mn>3</mn> <mo>)</mo></mrow></math> might be 0.6, indicating a 60%
    chance the user will watch another Action movie.
  prefs: []
  type: TYPE_NORMAL
- en: Rewards ( <math alttext="upper R"><mi>R</mi></math> )
  prefs: []
  type: TYPE_NORMAL
- en: This is the feedback from the user after taking an action (recommendation).
    Let’s assume for simplicity that a user’s click on a recommended movie gives a
    reward of +1 and no click is a reward of 0.
  prefs: []
  type: TYPE_NORMAL
- en: The aim of the recommender system in this context is to learn a policy <math
    alttext="pi colon upper S right-arrow upper A"><mrow><mi>π</mi> <mo>:</mo> <mi>S</mi>
    <mo>→</mo> <mi>A</mi></mrow></math> that maximizes the expected cumulative reward.
    A policy dictates which action the agent (the recommender system) should take
    in each state.
  prefs: []
  type: TYPE_NORMAL
- en: This policy can be learned via reinforcement learning algorithms, such as Q-learning
    or policy iteration, which essentially learn the value of taking an action in
    a state (i.e., recommending a movie after the user has watched a certain genre),
    considering the immediate reward and the potential future rewards.
  prefs: []
  type: TYPE_NORMAL
- en: The main challenge in a real-world recommender system scenario is that both
    the state and action spaces are extremely large, and the transition dynamics and
    reward function can be complex and difficult to estimate accurately. But, the
    principles demonstrated in this simple example remain the same.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the promising performance of Markov chain-based recommender systems,
    several challenges remain. The *memorylessness* assumption of the Markov chain
    may not hold in certain scenarios where long-term dependencies exist. Furthermore,
    most Markov chain models treat user-item interactions as binary events (either
    interaction or no interaction), which oversimplifies the variety of interactions
    users may have with items, such as browsing, clicking, and purchasing.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll cover neural networks. We’ll see how some architectures you’re likely
    familiar with can be relevant to learning a sequential recommender task.
  prefs: []
  type: TYPE_NORMAL
- en: RNN and CNN Architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Recurrent neural networks* (RNNs) are a type of neural network architecture
    designed to recognize patterns in sequences of data, such as text, speech, or
    time series data. These networks are *recurrent* in that the outputs from one
    step in the sequence are fed back into the network as inputs while processing
    the next step. This gives RNNs a form of memory, which is helpful for tasks like
    language modeling, where each word depends on the previous words.'
  prefs: []
  type: TYPE_NORMAL
- en: At each time step, an RNN takes in an input (like a word in a sentence) and
    produces an output (like a prediction of the next word). It also updates its internal
    state, which is a representation of what it has “seen” last in the sequence. This
    internal state is passed back into the network when processing the next input.
    As a result, the network can use information from previous steps to influence
    its predictions for the current step. This is what allows RNNs to effectively
    process sequential data.
  prefs: []
  type: TYPE_NORMAL
- en: '[GRU4Rec](https://oreil.ly/OwEFj) used recurrent neural networks to model session-based
    recommendations in one of the first applications of neural network architectures
    to the recommendation problem. A *session* refers to a single contiguous period
    of user interaction, like time spent on a page without the user navigating away
    or turning off their computer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we will see a dramatic advantage of sequential recommendation systems:
    most traditional recommendation methods rely on an explicit user ID to build a
    user-interest profile. However, session-based recommendations operate over anonymous
    user sessions that are often quite short to allow for a profile modeling. Moreover,
    a lot of variance can occur in user motivations in different sessions. A solution
    via user-agnostic recommendation that works for such recommendation situations
    is an item-based model in which an item-item similarity matrix is calculated based
    on items co-occurring within a single session. This precomputed similarity matrix
    is employed at runtime to recommend the most similar item to the one last clicked.
    This approach has obvious limitations such as relying only on the last clicked
    item. To this end, GRU4Rec uses all the items in the session and models the session
    as a sequence of items. The task of recommending items to be added translates
    to the prediction of the next item in the sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the small fixed-size vocabulary of languages, recommendation systems
    are required to reason over a large number of items that grows over time as more
    items are added. To handle this concern, pairwise ranking losses (e.g., BPR) are
    considered. GRU4Rec is further extended in [GRU4Rec+](https://oreil.ly/Y17DB),
    which utilizes a new loss function specifically designed for gains in top-*k*
    recommendation. These loss functions blend deep learning and LTR to address neural
    recommendation settings.
  prefs: []
  type: TYPE_NORMAL
- en: A different approach to neural networks for recommendations adopted CNNs for
    sequential recommendation. We won’t cover the basics of CNNs here, but you can
    consult [“How Do Convolutional Neural Networks Work?”](https://oreil.ly/-jEiE)
    by Brandon Rohrer for the essentials.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s discuss one method that has shown a lot of success, [CosRec](https://oreil.ly/wlCdN),
    as visualized in [Figure 17-1](#CosRecImage). This method (and others) starts
    with a structure similar to that of our MF used throughout most of the book: a
    user-item matrix. We assume that there are two latent factor matrices, <math alttext="upper
    E Subscript script upper I"><msub><mi>E</mi> <mi>ℐ</mi></msub></math> and <math
    alttext="upper E Subscript script upper U"><msub><mi>E</mi> <mi>𝒰</mi></msub></math>
    , but let’s first focus on the item matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each vector in the item matrix is an embedding vector for a single item, but
    we wish to encode sequences: take sequences of length <math alttext="upper L"><mi>L</mi></math>
    and collect those embedding vectors. We now have an <math alttext="upper L times
    upper D"><mrow><mi>L</mi> <mo>×</mo> <mi>D</mi></mrow></math> matrix with a row
    per each item in the sequence. Take adjacent rows as pairs and concatenate them
    for each vector in a three-tensor; this effectively captures the sequence as a
    series of pairwise transitions. This three-tensor can be passed through a vectorized
    2D CNN to yield a vector (of length <math alttext="upper L"><mi>L</mi></math>
    ) that is concatenated with the original user vector and fed through a fully connected
    layer. Finally, binary cross-entropy is our loss function to attempt to predict
    the best recommendation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![CosRec CNN Architecture from Yan et. al. 2019](assets/brpj_1701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-1\. CosRec CNN
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Attention Architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A term that is commonly associated with neural networks and that may ring a
    bell for you by now is *attention*. This is because transformers, in particular
    the kind that appear in large language models (LLMs) like the generalized pretrained
    transformer, have become a central focus among AI users.
  prefs: []
  type: TYPE_NORMAL
- en: We will give an extremely brief, and less technical, introduction to self-attention
    and the transformer here. For a more complete guide on transformers, consult the
    excellent overview in [“Transformers from Scratch”](https://oreil.ly/4PSx-) by
    Brandon Rohrer.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s state the key differentiating assumption about a transformer model:
    the embeddings are positional. We’re hoping to learn not only one embedding for
    every item but also an embedding for every item-position pair. Therefore, when
    an article is the first in a session and the last in a session, those two instances
    are treated as *two separate items*.'
  prefs: []
  type: TYPE_NORMAL
- en: Another important notion is stacking. When building transformers, we often think
    of the architecture as a layer cake, with sections stacked on top of one another.
    The key components are the embeddings, the self-attention layer, the skip-addition,
    and the feed-forward layer. The most complicated operations happen in self-attention,
    so let’s focus on that first. We just discussed the positional embeddings, which
    are sent as a sequence of these embedding vectors; recall that a transformer is
    a sequence-to-sequence model! The skip-addition means that we push the embedding
    forward *around* the self-attention layer (and the feed-forward layer above) and
    add it to the positional output of the attention layer. The feed-forward layer
    is an unexciting multilayer perceptron that stays in the positional columns and
    uses a ReLU or GeLU activation.
  prefs: []
  type: TYPE_NORMAL
- en: ReLU Versus GeLU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ReLU (Rectified Linear Unit) is an activation function defined as <math alttext="f
    left-parenthesis x right-parenthesis equals max left-parenthesis 0 comma x right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo> <mo>=</mo> <mo form="prefix" movablelimits="true">max</mo>
    <mo>(</mo> <mn>0</mn> <mo>,</mo> <mi>x</mi> <mo>)</mo></mrow></math> . GeLU (Gaussian
    Error Linear Unit) is another activation function approximated as <math alttext="f
    left-parenthesis x right-parenthesis equals 0.5 x left-parenthesis 1 plus hyperbolic
    tangent left-parenthesis StartRoot StartFraction 2 Over pi EndFraction EndRoot
    left-parenthesis x plus 0.044715 x cubed right-parenthesis right-parenthesis right-parenthesis"><mrow><mi>f</mi>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mn>0</mn> <mo>.</mo>
    <mn>5</mn> <mi>x</mi> <mfenced close=")" open="(" separators=""><mn>1</mn> <mo>+</mo>
    <mo form="prefix">tanh</mo> <mfenced close=")" open="(" separators=""><msqrt><mfrac><mn>2</mn>
    <mi>π</mi></mfrac></msqrt> <mfenced close=")" open="(" separators=""><mi>x</mi>
    <mo>+</mo> <mn>0</mn> <mo>.</mo> <mn>044715</mn> <msup><mi>x</mi> <mn>3</mn></msup></mfenced></mfenced></mfenced></mrow></math>
    , inspired by the Gaussian cumulative distribution function. The intuition behind
    GeLU is that it tends to allow small values of <math alttext="x"><mi>x</mi></math>
    to pass through while smoothly saturating extreme values, potentially enabling
    better gradient flow for deep models. Both functions introduce nonlinearity in
    neural networks, with GeLU often demonstrating improved learning dynamics over
    ReLU in certain contexts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some quick tips on self-attention:'
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind self-attention is that everything in the sequence affects everything
    else, in some manner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The self-attention layer is learning four weight matrices per head.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The heads are in 1-1 correspondence with the sequence length.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We often call the weight matrices <math alttext="upper Q comma upper K comma
    upper O comma upper V"><mrow><mi>Q</mi> <mo>,</mo> <mi>K</mi> <mo>,</mo> <mi>O</mi>
    <mo>,</mo> <mi>V</mi></mrow></math> . Both <math alttext="upper Q"><mi>Q</mi></math>
    and <math alttext="upper K"><mi>K</mi></math> get crossed with the positional
    embedding, but <math alttext="upper O"><mi>O</mi></math> and <math alttext="upper
    V"><mi>V</mi></math> are first crossed into an embedding-dimension-sized square
    matrix before dotting with the embedding. <math alttext="upper Q ModifyingAbove
    upper E With dot"><mrow><mi>Q</mi> <mover accent="true"><mi>E</mi> <mo>˙</mo></mover></mrow></math>
    and <math alttext="upper K ModifyingAbove upper E With dot"><mrow><mi>K</mi> <mover
    accent="true"><mi>E</mi> <mo>˙</mo></mover></mrow></math> multiply to create the
    eponymous *attention* matrix, over which we take a row-wise softmax to get the
    attention vector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some normalizations exist, but we’ll disregard them as inessential for understanding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we want to speak accurately but briefly about attention, we usually say,
    “It takes a sequence of positional embeddings and mushes them all together to
    learn how they’re related.”
  prefs: []
  type: TYPE_NORMAL
- en: Self-Attentive Sequential Recommendation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[SASRec](https://oreil.ly/aKKzg) is the first transformer model we’ll consider.
    This autoregressive sequential model (similar to a causal language model) predicts
    the next user interaction from past user interactions. Inspired by the success
    of the transformer models in sequential mining tasks, the self-attention-based
    architecture is used for sequential recommendation.'
  prefs: []
  type: TYPE_NORMAL
- en: When we say that the SASRec model is trained in an autoregressive manner, we
    mean that the self-attention is allowed to attend to only the earlier positions
    in the sequence; looking into the future is not permitted. In terms of the mushing
    we referenced earlier, think of this as only mushing forward the influence. Some
    people call this “causal” because it respects the causal arrow of time. The model
    also allows for a learnable positional encoding, which means that the updates
    carry down to the embedding layer. This model uses two transformer blocks.
  prefs: []
  type: TYPE_NORMAL
- en: BERT4Rec
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Inspired by the BERT model in NLP, [BERT4Rec](https://oreil.ly/SH9ON) improves
    upon SASRec by training a bidirectional masked sequential (language) model.
  prefs: []
  type: TYPE_NORMAL
- en: 'While BERT uses a masked language model for pretraining word embeddings, BERT4Rec
    uses this architecture to train end-to-end recommendation systems. It tries to
    predict the masked items in the user-interaction sequence. Similar to the original
    BERT model, the self-attention is bidirectional: it can look at both past and
    future interactions in the action sequence. To prevent leakage of future information
    and to emulate the realistic settings, only the last item in the sequence is masked
    during inference. Using item masking, BERT4Rec outperforms SASRec. However, a
    drawback of the BERT4Rec model is that it is quite compute intensive and requires
    much more training time.'
  prefs: []
  type: TYPE_NORMAL
- en: Recency Sampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sequential recommendation and the adoption of transformer architecture in these
    tasks has seen a lot of interest recently. These deep neural network models like
    BERT4Rec and SASRec have shown improved performance over traditional approaches.
    However, these models suffer from slow training problems. A recently published
    paper—ha ha, get it—addresses the question of improving training efficiency while
    achieving state-of-the-art performance. See [“Effective and Efficient Training
    for Sequential Recommendation Using Recency Sampling”](https://oreil.ly/yV4ro)
    by Aleksandr Petrov and Craig Macdonald for details.
  prefs: []
  type: TYPE_NORMAL
- en: The two training paradigms we’ve just described for sequential models are autoregressive,
    which tries to predict the next item in the user-interaction sequence, and masked,
    which tries to predict masked items in the interaction sequence. The autoregressive
    approach doesn’t use the beginning of the sequence as labels in the training process,
    and thus valuable information is lost. The masked approach, on the other hand,
    is only weakly related to the end goal of the sequential recommendation.
  prefs: []
  type: TYPE_NORMAL
- en: The paper by Petrov and Macdonald proposes a recency-based sampling of positive
    examples from the sequences to build the training data. The sampling is designed
    to give more recent interactions higher chances of being sampled. However, because
    of the probabilistic nature of the sampling mechanism, even the oldest of the
    interactions have nonzero chances of being chosen. An exponential function is
    employed as a sampling routine that interpolates between the masking-based sampling,
    where each interaction has equal probability of being sampled, and autoregressive
    sampling, where items from the end of the sequence are sampled. This showed superior
    performance in sequential recommendation tasks while requiring much less training
    time. Compare this approach to some of the other examples where we saw sampling
    provide significant improvements in training recommender systems!
  prefs: []
  type: TYPE_NORMAL
- en: Merging Static and Sequential
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pinterest recently released [“Rethinking Personalized Ranking at Pinterest:
    An End-to-End Approach”](https://oreil.ly/r_kPN) by Jiajing Xu et al. describing
    its personalized recommendation system, which is built to leverage raw user actions.
    The recommendation task is decomposed into modeling users’ long-term and short-term
    intentions.'
  prefs: []
  type: TYPE_NORMAL
- en: The process of comprehending long-term user interests is accomplished by training
    an end-to-end embedding model, referred to as PinnerFormer, to learn from a user’s
    historical actions on the platform. These actions are subsequently transformed
    into user embeddings, which are designed for optimization based on anticipated
    long-term future user activities.
  prefs: []
  type: TYPE_NORMAL
- en: This procedure employs an adapted transformer model to operate on users’ sequential
    actions with the intent to forecast their long-term future activities. Each user’s
    activity is compiled into a sequence, encompassing their actions over a specific
    time window, such as one year. The graph neural network–based (GNN-based) PinnerSage
    embeddings, in conjunction with relevant metadata (for example, the type of action,
    the timestamp, and so forth), are used to add features to each action in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Distinct from traditional sequential modeling tasks and sequential recommendation
    systems, PinnerFormer is designed to predict extended future user activities rather
    than the immediately subsequent action. This objective is achieved by training
    the model to foresee a user’s positive future interactions over a window of 14
    days following the embedding’s generation. In comparison, traditional sequential
    models would anticipate only the subsequent action.
  prefs: []
  type: TYPE_NORMAL
- en: This alternate approach allows for the embedding generation to occur offline
    in a batch-processing mode, resulting in significant reductions in infrastructure
    needs. In contrast to most traditional sequential modeling systems, which operate
    in real time and incur substantial computational and infrastructure costs, these
    embeddings can be produced in batches (for instance, on a daily basis) rather
    than every time a user performs an action.
  prefs: []
  type: TYPE_NORMAL
- en: A dense all-action loss is introduced in this methodology to facilitate batch
    training of the model. The objective here is not to predict the immediate next
    action but rather all the actions the user will undertake over the subsequent
    <math alttext="k"><mi>k</mi></math> days. The aim is to predict all occurrences
    of a user’s positive interactions at intervals such as <math alttext="upper T
    plus 3"><mrow><mi>T</mi> <mo>+</mo> <mn>3</mn></mrow></math> , <math alttext="upper
    T plus 8"><mrow><mi>T</mi> <mo>+</mo> <mn>8</mn></mrow></math> , and <math alttext="upper
    T plus 12"><mrow><mi>T</mi> <mo>+</mo> <mn>12</mn></mrow></math> , thereby compelling
    the system to learn long-term intentions. While traditionally the last action’s
    embedding is used to make the prediction, the dense all-action loss employs randomly
    selected positions in the action sequence, and the corresponding embedding is
    used to predict all actions for each of those positions.
  prefs: []
  type: TYPE_NORMAL
- en: Based on offline and online experimental results, the use of dense all-action
    loss to train for long-term user actions has significantly bridged the gap between
    batch generation and real-time generation of user embeddings. Moreover, to accommodate
    users’ short-term interests, the transformer model retrieves the most recent actions
    for each user in real time, processing them along with the long-term user embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Transformers and sequential recommendation systems are really at the cutting
    edge of modern recommenders. These days, most research in recommendation systems
    is in the area of sequential datasets, and the hottest recommenders are using
    longer and longer sequences for prediction. Two important projects are worthy
    of attention:'
  prefs: []
  type: TYPE_NORMAL
- en: Transformers4Rec
  prefs: []
  type: TYPE_NORMAL
- en: 'This open source project is geared toward scalable transformer models by the
    NVIDIA Merlin team. For more details, see [“Transformers4Rec: Bridging the Gap
    Between NLP and Sequential/Session-Based Recommendation”](https://oreil.ly/jwWBq)
    by Gabriel de Souza Pereira Moreira et al.'
  prefs: []
  type: TYPE_NORMAL
- en: Monolith
  prefs: []
  type: TYPE_NORMAL
- en: 'Also known as the TikTok For You page recommender, this is one of the most
    popular and exciting recommendation systems at this time. It is a fundamentally
    sequential recommender, with some elegant hybrid approaches. [“Monolith: Real-Time
    Recommendation System with Collisionless Embedding Table”](https://oreil.ly/EADgK)
    by Zhuoran Liu et al. covers the architectural considerations.'
  prefs: []
  type: TYPE_NORMAL
- en: Our final step before this book concludes is to consider a few approaches to
    recommendations. These don’t build exactly on top of what we’ve done but will
    use some of what we’ve done and introduce a few new ideas. Let’s sprint to the
    finish!
  prefs: []
  type: TYPE_NORMAL
