["```py\n>>> import pandas as pd\n>>> from sklearn.datasets import load_boston\n>>> from sklearn import (\n...     model_selection,\n...     preprocessing,\n... )\n>>> b = load_boston()\n>>> bos_X = pd.DataFrame(\n...     b.data, columns=b.feature_names\n... )\n>>> bos_y = b.target\n\n>>> bos_X_train, bos_X_test, bos_y_train, bos_y_test = model_selection.train_test_split(\n...     bos_X,\n...     bos_y,\n...     test_size=0.3,\n...     random_state=42,\n... )\n\n>>> bos_sX = preprocessing.StandardScaler().fit_transform(\n...     bos_X\n... )\n>>> bos_sX_train, bos_sX_test, bos_sy_train, bos_sy_test = model_selection.train_test_split(\n...     bos_sX,\n...     bos_y,\n...     test_size=0.3,\n...     random_state=42,\n... )\n```", "```py\n>>> from sklearn.dummy import DummyRegressor\n>>> dr = DummyRegressor()\n>>> dr.fit(bos_X_train, bos_y_train)\n>>> dr.score(bos_X_test, bos_y_test)\n-0.03469753992352409\n```", "```py\n>>> from sklearn.linear_model import (\n...     LinearRegression,\n... )\n>>> lr = LinearRegression()\n>>> lr.fit(bos_X_train, bos_y_train)\nLinearRegression(copy_X=True, fit_intercept=True,\n n_jobs=1, normalize=False)\n>>> lr.score(bos_X_test, bos_y_test)\n0.7109203586326287\n>>> lr.coef_\narray([-1.32774155e-01,  3.57812335e-02,\n 4.99454423e-02,  3.12127706e+00,\n -1.54698463e+01,  4.04872721e+00,\n -1.07515901e-02, -1.38699758e+00,\n 2.42353741e-01, -8.69095363e-03,\n -9.11917342e-01,  1.19435253e-02,\n -5.48080157e-01])\n```", "```py\n>>> lr2 = LinearRegression()\n>>> lr2.fit(bos_sX_train, bos_sy_train)\nLinearRegression(copy_X=True, fit_intercept=True,\n n_jobs=1, normalize=False)\n>>> lr2.score(bos_sX_test, bos_sy_test)\n0.7109203586326278\n>>> lr2.intercept_\n22.50945471291039\n>>> lr2.coef_\narray([-1.14030209,  0.83368112,  0.34230461,\n 0.792002, -1.7908376, 2.84189278, -0.30234582,\n -2.91772744,  2.10815064, -1.46330017,\n -1.97229956,  1.08930453, -3.91000474])\n```", "```py\n>>> from yellowbrick.features import (\n...     FeatureImportances,\n... )\n>>> fig, ax = plt.subplots(figsize=(6, 4))\n>>> fi_viz = FeatureImportances(\n...     lr2, labels=bos_X.columns\n... )\n>>> fi_viz.fit(bos_sX, bos_sy)\n>>> fi_viz.poof()\n>>> fig.savefig(\n...     \"images/mlpr_1401.png\",\n...     bbox_inches=\"tight\",\n...     dpi=300,\n... )\n```", "```py\n>>> from sklearn.svm import SVR\n>>> svr = SVR()\n>>> svr.fit(bos_sX_train, bos_sy_train)\nSVR(C=1.0, cache_size=200, coef0=0.0, degree=3,\n epsilon=0.1, gamma='auto', kernel='rbf',\n max_iter=-1, shrinking=True, tol=0.001,\n verbose=False)\n\n>>> svr.score(bos_sX_test, bos_sy_test)\n0.6555356362002485\n```", "```py\n>>> from sklearn.neighbors import (\n...     KNeighborsRegressor,\n... )\n>>> knr = KNeighborsRegressor()\n>>> knr.fit(bos_sX_train, bos_sy_train)\nKNeighborsRegressor(algorithm='auto',\n leaf_size=30, metric='minkowski',\n metric_params=None, n_jobs=1, n_neighbors=5,\n p=2, weights='uniform')\n\n>>> knr.score(bos_sX_test, bos_sy_test)\n0.747112767457727\n```", "```py\n>>> from sklearn.tree import DecisionTreeRegressor\n>>> dtr = DecisionTreeRegressor(random_state=42)\n>>> dtr.fit(bos_X_train, bos_y_train)\nDecisionTreeRegressor(criterion='mse',\n max_depth=None, max_features=None,\n max_leaf_nodes=None, min_impurity_decrease=0.0,\n min_impurity_split=None, min_samples_leaf=1,\n min_samples_split=2,\n min_weight_fraction_leaf=0.0, presort=False,\n random_state=42, splitter='best')\n\n>>> dtr.score(bos_X_test, bos_y_test)\n0.8426751288675483\n```", "```py\n>>> import pydotplus\n>>> from io import StringIO\n>>> from sklearn.tree import export_graphviz\n>>> dot_data = StringIO()\n>>> tree.export_graphviz(\n...     dtr,\n...     out_file=dot_data,\n...     feature_names=bos_X.columns,\n...     filled=True,\n... )\n>>> g = pydotplus.graph_from_dot_data(\n...     dot_data.getvalue()\n... )\n>>> g.write_png(\"images/mlpr_1402.png\")\n```", "```py\nfrom IPython.display import Image\nImage(g.create_png())\n```", "```py\n>>> dot_data = StringIO()\n>>> tree.export_graphviz(\n...     dtr,\n...     max_depth=2,\n...     out_file=dot_data,\n...     feature_names=bos_X.columns,\n...     filled=True,\n... )\n>>> g = pydotplus.graph_from_dot_data(\n...     dot_data.getvalue()\n... )\n>>> g.write_png(\"images/mlpr_1403.png\")\n```", "```py\n>>> dtr3 = DecisionTreeRegressor(max_depth=2)\n>>> dtr3.fit(bos_X_train, bos_y_train)\n>>> viz = dtreeviz.trees.dtreeviz(\n...     dtr3,\n...     bos_X,\n...     bos_y,\n...     target_name=\"price\",\n...     feature_names=bos_X.columns,\n... )\n>>> viz\n```", "```py\n>>> for col, val in sorted(\n...     zip(\n...         bos_X.columns, dtr.feature_importances_\n...     ),\n...     key=lambda x: x[1],\n...     reverse=True,\n... )[:5]:\n...     print(f\"{col:10}{val:10.3f}\")\nRM             0.574\nLSTAT          0.191\nDIS            0.110\nCRIM           0.061\nRAD            0.018\n```", "```py\n>>> from sklearn.ensemble import (\n...     RandomForestRegressor,\n... )\n>>> rfr = RandomForestRegressor(\n...     random_state=42, n_estimators=100\n... )\n>>> rfr.fit(bos_X_train, bos_y_train)\nRandomForestRegressor(bootstrap=True,\n criterion='mse', max_depth=None,\n max_features='auto', max_leaf_nodes=None,\n min_impurity_decrease=0.0,\n min_impurity_split=None,_samples_leaf=1,\n min_samples_split=2,\n min_weight_fraction_leaf=0.0,\n n_estimators=100, n_jobs=1,\n oob_score=False, random_state=42,\n verbose=0, warm_start=False)\n\n>>> rfr.score(bos_X_test, bos_y_test)\n0.8641887615545837\n```", "```py\n>>> for col, val in sorted(\n...     zip(\n...         bos_X.columns, rfr.feature_importances_\n...     ),\n...     key=lambda x: x[1],\n...     reverse=True,\n... )[:5]:\n...     print(f\"{col:10}{val:10.3f}\")\nRM             0.505\nLSTAT          0.283\nDIS            0.115\nCRIM           0.029\nPTRATIO        0.016\n```", "```py\n>>> xgr = xgb.XGBRegressor(random_state=42)\n>>> xgr.fit(bos_X_train, bos_y_train)\nXGBRegressor(base_score=0.5, booster='gbtree',\n colsample_bylevel=1, colsample_bytree=1,\n gamma=0, learning_rate=0.1, max_delta_step=0,\n max_depth=3, min_child_weight=1, missing=None,\n n_estimators=100, n_jobs=1, nthread=None,\n objective='reg:linear', random_state=42,\n reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n seed=None, silent=True, subsample=1)\n\n>>> xgr.score(bos_X_test, bos_y_test)\n0.871679473122472\n\n>>> xgr.predict(bos_X.iloc[[0]])\narray([27.013563], dtype=float32)\n```", "```py\n>>> for col, val in sorted(\n...     zip(\n...         bos_X.columns, xgr.feature_importances_\n...     ),\n...     key=lambda x: x[1],\n...     reverse=True,\n... )[:5]:\n...     print(f\"{col:10}{val:10.3f}\")\nDIS            0.187\nCRIM           0.137\nRM             0.137\nLSTAT          0.134\nAGE            0.110\n```", "```py\n>>> fig, ax = plt.subplots(figsize=(6, 4))\n>>> xgb.plot_importance(xgr, ax=ax)\n>>> fig.savefig(\"images/mlpr_1405.png\", dpi=300)\n```", "```py\n>>> fig, ax = plt.subplots(figsize=(6, 4))\n>>> fi_viz = FeatureImportances(xgr)\n>>> fi_viz.fit(bos_X_train, bos_y_train)\n>>> fi_viz.poof()\n>>> fig.savefig(\"images/mlpr_1406.png\", dpi=300)\n```", "```py\n>>> booster = xgr.get_booster()\n>>> print(booster.get_dump()[0])\n0:[LSTAT<9.72500038] yes=1,no=2,missing=1\n 1:[RM<6.94099998] yes=3,no=4,missing=3\n 3:[DIS<1.48494995] yes=7,no=8,missing=7\n 7:leaf=3.9599998\n 8:leaf=2.40158272\n 4:[RM<7.43700027] yes=9,no=10,missing=9\n 9:leaf=3.22561002\n 10:leaf=4.31580687\n 2:[LSTAT<16.0849991] yes=5,no=6,missing=5\n 5:[B<116.024994] yes=11,no=12,missing=11\n 11:leaf=1.1825\n 12:leaf=1.99701393\n 6:[NOX<0.603000045] yes=13,no=14,missing=13\n 13:leaf=1.6868\n 14:leaf=1.18572915\n```", "```py\nfig, ax = plt.subplots(figsize=(6, 4))\nxgb.plot_tree(xgr, ax=ax, num_trees=0)\nfig.savefig('images/mlpr_1407.png', dpi=300)\n```", "```py\n>>> import lightgbm as lgb\n>>> lgr = lgb.LGBMRegressor(random_state=42)\n>>> lgr.fit(bos_X_train, bos_y_train)\nLGBMRegressor(boosting_type='gbdt',\n class_weight=None, colsample_bytree=1.0,\n learning_rate=0.1, max_depth=-1,\n min_child_samples=20, min_child_weight=0.001,\n min_split_gain=0.0, n_estimators=100,\n n_jobs=-1, num_leaves=31, objective=None,\n random_state=42, reg_alpha=0.0,\n reg_lambda=0.0, silent=True, subsample=1.0,\n subsample_for_bin=200000, subsample_freq=0)\n\n>>> lgr.score(bos_X_test, bos_y_test)\n0.847729219534575\n\n>>> lgr.predict(bos_X.iloc[[0]])\narray([30.31689569])\n```", "```py\n>>> for col, val in sorted(\n...     zip(\n...         bos_X.columns, lgr.feature_importances_\n...     ),\n...     key=lambda x: x[1],\n...     reverse=True,\n... )[:5]:\n...     print(f\"{col:10}{val:10.3f}\")\nLSTAT        226.000\nRM           199.000\nDIS          172.000\nAGE          130.000\nB            121.000\n```", "```py\n>>> fig, ax = plt.subplots(figsize=(6, 4))\n>>> lgb.plot_importance(lgr, ax=ax)\n>>> fig.tight_layout()\n>>> fig.savefig(\"images/mlpr_1408.png\", dpi=300)\n```", "```py\nlgb.create_tree_digraph(lgbr)\n```"]