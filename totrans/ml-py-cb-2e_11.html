<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 11. Model Evaluation" data-type="chapter" epub:type="chapter"><div class="chapter" id="model-evaluation">
<h1><span class="label">Chapter 11. </span>Model Evaluation</h1>
<section data-pdf-bookmark="11.0 Introduction" data-type="sect1"><div class="sect1" id="id254">
<h1>11.0 Introduction</h1>
<p>In this <a data-primary="model evaluation" data-type="indexterm" id="ix_mod_eval_ch11"/>chapter we will examine strategies for evaluating the quality of models created through our learning algorithms. It might appear strange to discuss model evaluation before discussing how to create them, but there is a method to our madness. Models are only as useful as the quality of their predictions, and thus, fundamentally, our goal is not to create models (which is easy) but to create high-quality models (which is hard). Therefore, before we explore the myriad learning algorithms, let’s first learn how we can evaluate the models they produce.</p>
</div></section>
<section data-pdf-bookmark="11.1 Cross-Validating Models" data-type="sect1"><div class="sect1" id="cross-validating-models">
<h1>11.1 Cross-Validating Models</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id505">
<h2>Problem</h2>
<p>You want to <a data-primary="cross-validation (CV) of ML models" data-secondary="conducting a CV" data-type="indexterm" id="ix_cross_val_cv_cond"/><a data-primary="model evaluation" data-secondary="cross-validating models" data-type="indexterm" id="ix_mod_eval_cross_val"/><a data-primary="predictions and predicting" data-secondary="cross-validating models" data-type="indexterm" id="ix_predict_cv_mod"/>evaluate how well your classification model generalizes to unforeseen data.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id1529">
<h2>Solution</h2>
<p>Create a pipeline that preprocesses the data, trains the model, and then evaluates it using cross-validation:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">metrics</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">KFold</code><code class="p">,</code> <code class="n">cross_val_score</code>
<code class="kn">from</code> <code class="nn">sklearn.pipeline</code> <code class="kn">import</code> <code class="n">make_pipeline</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LogisticRegression</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>

<code class="c1"># Load digits dataset</code>
<code class="n">digits</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_digits</code><code class="p">()</code>

<code class="c1"># Create features matrix</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">digits</code><code class="o">.</code><code class="n">data</code>

<code class="c1"># Create target vector</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">digits</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create standardizer</code>
<code class="n">standardizer</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>

<code class="c1"># Create logistic regression object</code>
<code class="n">logit</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">()</code>

<code class="c1"># Create a pipeline that standardizes, then runs logistic regression</code>
<code class="n">pipeline</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code><code class="n">standardizer</code><code class="p">,</code> <code class="n">logit</code><code class="p">)</code>

<code class="c1"># Create k-fold cross-validation</code>
<code class="n">kf</code> <code class="o">=</code> <code class="n">KFold</code><code class="p">(</code><code class="n">n_splits</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Conduct k-fold cross-validation</code>
<code class="n">cv_results</code> <code class="o">=</code> <code class="n">cross_val_score</code><code class="p">(</code><code class="n">pipeline</code><code class="p">,</code> <code class="c1"># Pipeline</code>
                             <code class="n">features</code><code class="p">,</code> <code class="c1"># Feature matrix</code>
                             <code class="n">target</code><code class="p">,</code> <code class="c1"># Target vector</code>
                             <code class="n">cv</code><code class="o">=</code><code class="n">kf</code><code class="p">,</code> <code class="c1"># Performance metric</code>
                             <code class="n">scoring</code><code class="o">=</code><code class="s2">"accuracy"</code><code class="p">,</code> <code class="c1"># Loss function</code>
                             <code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code> <code class="c1"># Use all CPU cores</code>

<code class="c1"># Calculate mean</code>
<code class="n">cv_results</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code></pre>
<pre data-type="programlisting">0.969958217270195</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id255">
<h2>Discussion</h2>
<p>At first consideration, evaluating supervised-learning models might
appear straightforward: train a model and then calculate how well it did using some performance metric (accuracy, squared errors, etc.). However, this approach is fundamentally flawed. If we train a model using our data, and then evaluate how well it did on that data, we are not achieving our desired goal. Our goal is not to evaluate how well the model does on our training data, but how well it does on data it has never seen before (e.g., a new customer, a new crime, a new image). For this reason, our method of evaluation should help us understand how well models are able to make predictions from data they have never seen before.</p>
<p>One <a data-primary="hold-out data" data-type="indexterm" id="id1530"/><a data-primary="supervised learning models" data-secondary="importance of test data for evaluating" data-type="indexterm" id="id1531"/><a data-primary="validation data" data-type="indexterm" id="id1532"/><a data-primary="training set" data-type="indexterm" id="ix_train_set_ch11"/><a data-primary="test set" data-type="indexterm" id="ix_test_set_ch11"/>strategy might be to hold off a slice of data for testing. This is
called <em>validation</em> (or <em>hold-out</em>). In validation, our observations (features and targets) are split into two sets, traditionally called the <em>training set</em> and the <em>test set</em>. We take the test set and put it off to the side, pretending that we have never seen it before. Next we train our model using our training set, using the features and target vector to teach the model how to make the best prediction. Finally, we simulate having never-before-seen external data by evaluating how our model performs on our test set. However, the validation approach has two major weaknesses. First, the performance of the model can be highly dependent on which few observations were selected for the test set. Second, the model is not being trained using all the available data, and it’s not being evaluated on all the available data.</p>
<p>A better <a data-primary="k-fold cross-validation (KFCV)" data-type="indexterm" id="ix_kfold_cross_valid2"/>strategy, which overcomes these weaknesses, is called <em>k-fold cross-validation</em> (KFCV). In KFCV, we split the data into <em>k</em> parts called <em>folds</em>. The model is then trained using <em>k – 1</em> folds—​combined into one training set—​and then the last fold is used as a test set. We repeat this <em>k</em> times, each time using a different fold as the test set. The performance on the model for each of the <em>k</em> iterations is then averaged to produce an overall measurement.</p>
<p>In our solution, we conducted k-fold cross-validation using five folds and outputted the evaluation scores to <code>cv_results</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View score for all 5 folds</code>
<code class="n">cv_results</code></pre>
<pre data-type="programlisting">array([0.96111111, 0.96388889, 0.98050139, 0.97214485, 0.97214485])</pre>
<p>There are three important points to consider when we are using KFCV. First, KFCV assumes that each observation was created independently from the other (i.e., the data is independent and identically distributed [IID]). If the data is <a data-primary="IID (independent and identically distributed) data" data-type="indexterm" id="id1533"/><a data-primary="independent and identically distributed (IID) data" data-type="indexterm" id="id1534"/>IID, it is a good idea to shuffle observations when assigning to folds. In scikit-learn we can set <code>shuffle=True</code> to perform shuffling.</p>
<p>Second, when we are using <a data-primary="stratified k-fold cross validation" data-type="indexterm" id="id1535"/>KFCV to evaluate a classifier, it is often
beneficial to have folds containing roughly the same percentage of
observations from each of the different target classes (called <em>stratified k-fold</em>). For example, if our target vector contained gender and 80% of the observations were male, then each fold would contain 80% male and 20% female observations. In scikit-learn, we can conduct stratified k-fold cross-validation by replacing the <code>KFold</code> class with <code>StratifiedKFold</code>.</p>
<p>Finally, when we are using validation sets or cross-validation, it is important to preprocess data based on the training set and then apply those transformations to both the training and test set. For example, when we <code>fit</code> our standardization object, <code>standardizer</code>, we calculate the mean and variance of only the training set. Then we apply that transformation (using <code>transform</code>) to both the training and test sets:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import library</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>

<code class="c1"># Create training and test sets</code>
<code class="n">features_train</code><code class="p">,</code> <code class="n">features_test</code><code class="p">,</code> <code class="n">target_train</code><code class="p">,</code> <code class="n">target_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Fit standardizer to training set</code>
<code class="n">standardizer</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_train</code><code class="p">)</code>

<code class="c1"># Apply to both training and test sets which can then be used to train models</code>
<code class="n">features_train_std</code> <code class="o">=</code> <code class="n">standardizer</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">features_train</code><code class="p">)</code>
<code class="n">features_test_std</code> <code class="o">=</code> <code class="n">standardizer</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">features_test</code><code class="p">)</code></pre>
<p>The reason for this is because we are pretending that the test set is
unknown data. If we fit both our preprocessors using observations from
both training and test sets, some of the information from the test set
leaks into our training set. This rule applies for any preprocessing
step such as feature selection.<a data-primary="" data-startref="ix_test_set_ch11" data-type="indexterm" id="id1536"/><a data-primary="" data-startref="ix_train_set_ch11" data-type="indexterm" id="id1537"/><a data-primary="" data-startref="ix_kfold_cross_valid2" data-type="indexterm" id="id1538"/></p>
<p>scikit-learn’s <code>pipeline</code> package<a data-primary="pipeline package" data-type="indexterm" id="id1539"/><a data-primary="scikit-learn library" data-secondary="pipeline package" data-type="indexterm" id="id1540"/> makes this easy to do while using
cross-validation techniques. We first create a pipeline that
preprocesses the data (e.g., <code>standardizer</code>) and then trains a model
(logistic regression, <code>logit</code>):</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create a pipeline</code>
<code class="n">pipeline</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code><code class="n">standardizer</code><code class="p">,</code> <code class="n">logit</code><code class="p">)</code></pre>
<p>Then we run KFCV using that pipeline and scikit does all the work for
us:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Do k-fold cross-validation</code>
<code class="n">cv_results</code> <code class="o">=</code> <code class="n">cross_val_score</code><code class="p">(</code><code class="n">pipeline</code><code class="p">,</code> <code class="c1"># Pipeline</code>
                             <code class="n">features</code><code class="p">,</code> <code class="c1"># Feature matrix</code>
                             <code class="n">target</code><code class="p">,</code> <code class="c1"># Target vector</code>
                             <code class="n">cv</code><code class="o">=</code><code class="n">kf</code><code class="p">,</code> <code class="c1"># Performance metric</code>
                             <code class="n">scoring</code><code class="o">=</code><code class="s2">"accuracy"</code><code class="p">,</code> <code class="c1"># Loss function</code>
                             <code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code> <code class="c1"># Use all CPU cores</code></pre>
<p><code>cross_val_score</code> comes with <a data-primary="cross_val_score" data-type="indexterm" id="id1541"/>three parameters we have not
discussed, but that are worth noting:</p>
<dl>
<dt><code>cv</code></dt>
<dd>
<p><code>cv</code> determines our cross-validation technique. K-fold is the most common by far, but there are others, such as leave-one-out cross-validation where the number of folds <em>k</em> equals the number of data points in the set.</p>
</dd>
<dt><code>scoring</code></dt>
<dd>
<p><code>scoring</code> defines the metric for success, a number of which are discussed in other recipes in this chapter.</p>
</dd>
<dt><code>n_jobs=-1</code></dt>
<dd>
<p><code>n_jobs=-1</code> tells scikit-learn to use every core available. For example, if your computer has four cores (a common number for laptops), then scikit-learn will use all four cores at once to speed up the operation.</p>
</dd>
</dl>
<p>One small note: when running some of these examples, you may see a warning that says “ConvergenceWarning: lbfgs failed to converge.” The configuration used in these examples is designed to prevent this, but should it still occur, you can ignore it for now. We will troubleshoot issues like this later in the book as we dive into specific types of models.<a data-primary="" data-startref="ix_predict_cv_mod" data-type="indexterm" id="id1542"/><a data-primary="" data-startref="ix_cross_val_cv_cond" data-type="indexterm" id="id1543"/><a data-primary="" data-startref="ix_mod_eval_cross_val" data-type="indexterm" id="id1544"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1545">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/vrGXy">Why Every Statistician Should Know About Cross-Validation</a></p>
</li>
<li>
<p><a href="https://oreil.ly/NE-B8">Cross-Validation Gone Wrong</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="11.2 Creating a Baseline Regression Model" data-type="sect1"><div class="sect1" id="creating-a-baseline-regression-model">
<h1>11.2 Creating a Baseline Regression Model</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id256">
<h2>Problem</h2>
<p>You want a simple <a data-primary="regression and regressors" data-secondary="model evaluation" data-type="indexterm" id="ix_regress_mod_eval"/><a data-primary="baseline regression model" data-type="indexterm" id="ix_baseline_regress"/>baseline regression model to use as a comparison against other models that you train.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id257">
<h2>Solution</h2>
<p>Use <a data-primary="DummyRegressor" data-type="indexterm" id="ix_dum_regress"/><a data-primary="model evaluation" data-secondary="baseline regression model" data-type="indexterm" id="ix_mod_eval_base_reg"/>scikit-learn’s <code>DummyRegressor</code> to create a simple model to use as a
baseline:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_wine</code>
<code class="kn">from</code> <code class="nn">sklearn.dummy</code> <code class="kn">import</code> <code class="n">DummyRegressor</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>

<code class="c1"># Load data</code>
<code class="n">wine</code> <code class="o">=</code> <code class="n">load_wine</code><code class="p">()</code>

<code class="c1"># Create features</code>
<code class="n">features</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="n">wine</code><code class="o">.</code><code class="n">data</code><code class="p">,</code> <code class="n">wine</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Make test and training split</code>
<code class="n">features_train</code><code class="p">,</code> <code class="n">features_test</code><code class="p">,</code> <code class="n">target_train</code><code class="p">,</code> <code class="n">target_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Create a dummy regressor</code>
<code class="n">dummy</code> <code class="o">=</code> <code class="n">DummyRegressor</code><code class="p">(</code><code class="n">strategy</code><code class="o">=</code><code class="s1">'mean'</code><code class="p">)</code>

<code class="c1"># "Train" dummy regressor</code>
<code class="n">dummy</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_train</code><code class="p">,</code> <code class="n">target_train</code><code class="p">)</code>

<code class="c1"># Get R-squared score</code>
<code class="n">dummy</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">features_test</code><code class="p">,</code> <code class="n">target_test</code><code class="p">)</code></pre>
<pre data-type="programlisting">-0.0480213580840978</pre>
<p>To compare, we train our model and evaluate the performance score:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LinearRegression</code>

<code class="c1"># Train simple linear regression model</code>
<code class="n">ols</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">()</code>
<code class="n">ols</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_train</code><code class="p">,</code> <code class="n">target_train</code><code class="p">)</code>

<code class="c1"># Get R-squared score</code>
<code class="n">ols</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">features_test</code><code class="p">,</code> <code class="n">target_test</code><code class="p">)</code></pre>
<pre data-type="programlisting">0.804353263176954</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id258">
<h2>Discussion</h2>
<p><code>DummyRegressor</code> allows us to create a very simple model that we
can use as a baseline to compare against any other models that we train. This can
often be useful to simulate a “naive” existing prediction process in a
product or system. For example, a product might have been originally
hardcoded to assume that all new users will spend $100 in the first
month, regardless of their features. If we encode that assumption into a baseline model, we are able to concretely state the benefits of using a machine learning approach by comparing the dummy model’s <code>score</code> with that of a trained model.</p>
<p><code>DummyRegressor</code> uses the <code>strategy</code> parameter to set the method of making predictions, including the mean or median value in the training set. Furthermore, if we set <code>strategy</code> to <code>constant</code> and use the <code>constant</code> parameter, we can set the dummy regressor to predict some constant value for every observation:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create dummy regressor that predicts 1s for everything</code>
<code class="n">clf</code> <code class="o">=</code> <code class="n">DummyRegressor</code><code class="p">(</code><code class="n">strategy</code><code class="o">=</code><code class="s1">'constant'</code><code class="p">,</code> <code class="n">constant</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="n">clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_train</code><code class="p">,</code> <code class="n">target_train</code><code class="p">)</code>

<code class="c1"># Evaluate score</code>
<code class="n">clf</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">features_test</code><code class="p">,</code> <code class="n">target_test</code><code class="p">)</code></pre>
<pre data-type="programlisting">-0.06299212598425186</pre>
<p>One <a data-primary="R² (coefficient of determination)" data-type="indexterm" id="id1546"/><a data-primary="coefficient of determination (R²)" data-type="indexterm" id="id1547"/>small note regarding <code>score</code>. By default, <code>score</code> returns the
coefficient of determination (R-squared, <math display="inline"><msup><mi>R</mi> <mn>2</mn> </msup></math>) score:</p>
<div data-type="equation">
<math display="block">
<mrow>
<msup><mi>R</mi> <mn>2</mn> </msup>
<mo>=</mo>
<mn>1</mn>
<mo>-</mo>
<mfrac><mrow><msub><mo>∑</mo> <mi>i</mi> </msub><msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi> </msub><mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>i</mi> </msub><mo>)</mo></mrow> <mn>2</mn> </msup></mrow> <mrow><msub><mo>∑</mo> <mi>i</mi> </msub><msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi> </msub><mo>-</mo><mover accent="true"><mi>y</mi> <mo>¯</mo></mover><mo>)</mo></mrow> <mn>2</mn> </msup></mrow></mfrac>
</mrow>
</math>
</div>
<p>where <math display="inline"><msub><mi>y</mi> <mi>i</mi> </msub></math> is the true value of the target observation, <math display="inline"><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover> <mi>i</mi> </msub></math> is the predicted value, and <math display="inline"><mover accent="true"><mi>y</mi><mo>¯</mo></mover></math> is the mean value for the target vector.</p>
<p>The closer <math display="inline"><msup><mi>R</mi> <mn>2</mn> </msup></math> is to 1, the more of the variance in the target vector that is explained by the features.<a data-primary="" data-startref="ix_baseline_regress" data-type="indexterm" id="id1548"/><a data-primary="" data-startref="ix_dum_regress" data-type="indexterm" id="id1549"/><a data-primary="" data-startref="ix_mod_eval_base_reg" data-type="indexterm" id="id1550"/><a data-primary="" data-startref="ix_regress_mod_eval" data-type="indexterm" id="id1551"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="11.3 Creating a Baseline Classification Model" data-type="sect1"><div class="sect1" id="creating-a-baseline-classification-model">
<h1>11.3 Creating a Baseline Classification Model</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id259">
<h2>Problem</h2>
<p>You want a simple <a data-primary="model evaluation" data-secondary="baseline classification model" data-type="indexterm" id="ix_mod_eval_base_class"/><a data-primary="baseline classification model" data-type="indexterm" id="ix_base_class_mod"/><a data-primary="classification and classifiers" data-secondary="baseline classification model" data-type="indexterm" id="ix_classif_base_class_mod"/>baseline classifier to compare against your model.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id260">
<h2>Solution</h2>
<p>Use <a data-primary="DummyClassifier" data-type="indexterm" id="ix_dum_classif"/>scikit-learn’s <code>DummyClassifier</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_iris</code>
<code class="kn">from</code> <code class="nn">sklearn.dummy</code> <code class="kn">import</code> <code class="n">DummyClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">load_iris</code><code class="p">()</code>

<code class="c1"># Create target vector and feature matrix</code>
<code class="n">features</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code><code class="p">,</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Split into training and test set</code>
<code class="n">features_train</code><code class="p">,</code> <code class="n">features_test</code><code class="p">,</code> <code class="n">target_train</code><code class="p">,</code> <code class="n">target_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
   <code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Create dummy classifier</code>
<code class="n">dummy</code> <code class="o">=</code> <code class="n">DummyClassifier</code><code class="p">(</code><code class="n">strategy</code><code class="o">=</code><code class="s1">'uniform'</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># "Train" model</code>
<code class="n">dummy</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_train</code><code class="p">,</code> <code class="n">target_train</code><code class="p">)</code>

<code class="c1"># Get accuracy score</code>
<code class="n">dummy</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">features_test</code><code class="p">,</code> <code class="n">target_test</code><code class="p">)</code></pre>
<pre data-type="programlisting">0.42105263157894735</pre>
<p>By comparing the baseline classifier to our trained classifier, we can
see the 
<span class="keep-together">improvement:</span></p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">RandomForestClassifier</code>

<code class="c1"># Create classifier</code>
<code class="n">classifier</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">()</code>

<code class="c1"># Train model</code>
<code class="n">classifier</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_train</code><code class="p">,</code> <code class="n">target_train</code><code class="p">)</code>

<code class="c1"># Get accuracy score</code>
<code class="n">classifier</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">features_test</code><code class="p">,</code> <code class="n">target_test</code><code class="p">)</code></pre>
<pre data-type="programlisting">0.9736842105263158</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id261">
<h2>Discussion</h2>
<p>A common measure of a classifier’s performance is how much better it is
than random guessing. scikit-learn’s <code>DummyClassifier</code> makes this
comparison easy. The <code>strategy</code> parameter gives us a number of options
for generating values. There are two particularly useful strategies.
First, <code>stratified</code> makes predictions proportional to the class proportions of the training
set’s target vector (e.g., if 20% of the observations
in the training data are women, then <code>DummyClassifier</code> will predict
women 20% of the time). Second, <code>uniform</code> will generate predictions
uniformly at random between the different classes. For example, if 20%
of observations are women and 80% are men, <code>uniform</code> will produce
predictions that are 50% women and 50% men.<a data-primary="" data-startref="ix_base_class_mod" data-type="indexterm" id="id1552"/><a data-primary="" data-startref="ix_classif_base_class_mod" data-type="indexterm" id="id1553"/><a data-primary="" data-startref="ix_dum_classif" data-type="indexterm" id="id1554"/><a data-primary="" data-startref="ix_mod_eval_base_class" data-type="indexterm" id="id1555"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1556">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/bwqQU">scikit-learn documentation: DummyClassifier</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="11.4 Evaluating Binary Classifier Predictions" data-type="sect1"><div class="sect1" id="evaluating-binary-classifier-predictions">
<h1>11.4 Evaluating Binary Classifier Predictions</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id506">
<h2>Problem</h2>
<p>Given a <a data-primary="classification and classifiers" data-secondary="binary classifiers" data-type="indexterm" id="ix_classif_bin_classif"/><a data-primary="predictions and predicting" data-secondary="binary classification evaluation of" data-type="indexterm" id="ix_predict_bin_class"/><a data-primary="model evaluation" data-secondary="binary classifier predictions" data-type="indexterm" id="ix_mod_eval_bin_class"/><a data-primary="binary classifiers" data-secondary="prediction evaluation" data-type="indexterm" id="ix_bin_class_predict"/>trained classification model, you want to evaluate its quality.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id262">
<h2>Solution</h2>
<p>Use <a data-primary="cross_val_score" data-type="indexterm" id="ix_cross_val_score"/>scikit-learn’s <code>cross_val_score</code> to conduct cross-validation while
using the 
<span class="keep-together"><code>scoring</code></span> parameter to define one of a number of
performance metrics, including accuracy, precision, recall, and
<em>F<sub>1</sub></em>. <em>Accuracy</em> is a <a data-primary="accuracy metric" data-type="indexterm" id="ix_acc_metric"/>common performance metric. It is simply the proportion of
observations predicted correctly:</p>
<div data-type="equation">
<math display="block">
<mstyle displaystyle="true" scriptlevel="0">
<mrow>
<mi>A</mi>
<mi>c</mi>
<mi>c</mi>
<mi>u</mi>
<mi>r</mi>
<mi>a</mi>
<mi>c</mi>
<mi>y</mi>
<mo>=</mo>
<mfrac><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi></mrow> <mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac>
</mrow>
</mstyle>
</math>
</div>
<p>where:</p>
<dl>
<dt><math display="inline"><mi>T</mi><mi>P</mi></math></dt>
<dd>
<p>The number of true positives. These are observations that
are part of the <em>positive</em> class (has the disease, purchased the
product, etc.) and that we predicted correctly.</p>
</dd>
<dt><math display="inline"><mi>T</mi><mi>N</mi></math></dt>
<dd>
<p>The number of true negatives. These are observations that
are part of the <em>negative</em> class (does not have the disease, did
not purchase the product, etc.) and that we predicted correctly.</p>
</dd>
<dt><math display="inline"><mi>F</mi><mi>P</mi></math></dt>
<dd>
<p>The number of false positives, also called a <em>Type
I error</em>. These are observations that are predicted to be part of the <em>positive</em> class but
are actually part of the <em>negative</em> class.</p>
</dd>
<dt><math display="inline"><mi>F</mi><mi>N</mi></math></dt>
<dd>
<p>The number of false negatives, also called a <em>Type
II error</em>. These are observations that are predicted to be part of the <em>negative</em> class but are actually part of the <em>positive</em> class.</p>
</dd>
</dl>
<p>We can measure accuracy in three-fold (the default number of folds)
cross-validation by setting <code>scoring="accuracy"</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">cross_val_score</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LogisticRegression</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_classification</code>

<code class="c1"># Generate features matrix and target vector</code>
<code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">make_classification</code><code class="p">(</code><code class="n">n_samples</code> <code class="o">=</code> <code class="mi">10000</code><code class="p">,</code>
                           <code class="n">n_features</code> <code class="o">=</code> <code class="mi">3</code><code class="p">,</code>
                           <code class="n">n_informative</code> <code class="o">=</code> <code class="mi">3</code><code class="p">,</code>
                           <code class="n">n_redundant</code> <code class="o">=</code> <code class="mi">0</code><code class="p">,</code>
                           <code class="n">n_classes</code> <code class="o">=</code> <code class="mi">2</code><code class="p">,</code>
                           <code class="n">random_state</code> <code class="o">=</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Create logistic regression</code>
<code class="n">logit</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">()</code>

<code class="c1"># Cross-validate model using accuracy</code>
<code class="n">cross_val_score</code><code class="p">(</code><code class="n">logit</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">scoring</code><code class="o">=</code><code class="s2">"accuracy"</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([0.9555, 0.95  , 0.9585, 0.9555, 0.956 ])</pre>
<p>The appeal of accuracy is that it has an intuitive and plain English
explanation: the proportion of observations predicted correctly. However, in
the real world, often our data has <a data-primary="imbalanced classes, handling" data-secondary="accuracy paradox with" data-type="indexterm" id="id1557"/>imbalanced classes (e.g., the 99.9% of
observations are of class 1 and only 0.1% are class 2). When in the
presence of imbalanced classes, accuracy suffers from a paradox where a
model is highly accurate but lacks predictive power. For example,
imagine we are trying to predict the presence of a very rare cancer that
occurs in 0.1% of the population. After training our model, we find the
accuracy is at 95%. However, 99.9% of people do not have the cancer: if
we simply created a model that “predicted” that nobody had that form of
cancer, our naive model would be 4.9% more accurate, but it clearly is not able to <em>predict</em> anything. For this reason, we are often motivated
to use other metrics such as precision, recall, and the <em>F<sub>1</sub></em>
score.<a data-primary="" data-startref="ix_acc_metric" data-type="indexterm" id="id1558"/></p>
<p><em>Precision</em> is the <a data-primary="precision metric" data-type="indexterm" id="id1559"/>proportion of every observation predicted to be positive that is actually positive. We can think about it as a measurement noise in our predictions—that is, how likely we are to be right when we predict something is positive. Models with high precision are pessimistic in that they predict an observation is of the positive class only when they are very certain about it. Formally, precision is:</p>
<div data-type="equation">
<math display="block">
<mstyle displaystyle="true" scriptlevel="0">
<mrow>
<mi fontstyle="italic"> Precision </mi>
<mo>=</mo>
<mfrac><mrow><mi>T</mi><mi>P</mi></mrow> <mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac>
</mrow>
</mstyle>
</math>
</div>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Cross-validate model using precision</code>
<code class="n">cross_val_score</code><code class="p">(</code><code class="n">logit</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">scoring</code><code class="o">=</code><code class="s2">"precision"</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([0.95963673, 0.94820717, 0.9635996 , 0.96149949, 0.96060606])</pre>
<p><em>Recall</em> is the <a data-primary="recall metric" data-type="indexterm" id="id1560"/>proportion of every positive observation that is truly positive. Recall measures the model’s ability to identify an observation of the positive class. Models with high recall are optimistic in that they have a low bar for predicting that an observation is in the positive class:</p>
<div data-type="equation">
<math display="block">
<mstyle displaystyle="true" scriptlevel="0">
<mrow>
<mi fontstyle="italic">Recall </mi>
<mo>=</mo>
<mfrac><mrow><mi>T</mi><mi>P</mi></mrow> <mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac>
</mrow>
</mstyle>
</math>
</div>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Cross-validate model using recall</code>
<code class="n">cross_val_score</code><code class="p">(</code><code class="n">logit</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">scoring</code><code class="o">=</code><code class="s2">"recall"</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([0.951, 0.952, 0.953, 0.949, 0.951])</pre>
<p>If this is the <a data-primary="harmonic mean" data-type="indexterm" id="id1561"/>first time you have encountered precision and recall, it is understandable if it takes a little while to fully understand them. This is one of the downsides to accuracy; precision and recall are less intuitive. Almost always we want some kind of balance between precision and recall, and this role is filled by the <em>F<sub>1</sub></em> score. The <em>F<sub>1</sub></em> score is the <em>harmonic mean</em> (a kind of average used for ratios):</p>
<div data-type="equation">
<math display="block">
<mstyle displaystyle="true" scriptlevel="0">
<mrow>
<msub><mi>F</mi> <mn fontstyle="italic">1</mn> </msub>
<mo>=</mo>
<mn>2</mn>
<mo>×</mo>
<mfrac><mrow><mi fontstyle="italic"> Precision </mi><mo>×</mo><mi fontstyle="italic"> Recall </mi></mrow> <mrow><mi fontstyle="italic"> Precision </mi><mo>+</mo><mi fontstyle="italic"> Recall </mi></mrow></mfrac>
</mrow>
</mstyle>
</math>
</div>
<p>This score is a measure of correctness achieved in positive prediction—that is, of observations labeled as positive, how many are actually positive:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Cross-validate model using F1</code>
<code class="n">cross_val_score</code><code class="p">(</code><code class="n">logit</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">scoring</code><code class="o">=</code><code class="s2">"f1"</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([0.95529884, 0.9500998 , 0.95827049, 0.95520886, 0.95577889])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id263">
<h2>Discussion</h2>
<p>As an <a data-primary="F₁ score" data-type="indexterm" id="id1562"/>evaluation metric, accuracy has some valuable properties,
especially its intuitiveness. However, better metrics often involve
using some balance of precision and recall—that is, a trade-off between
the optimism and pessimism of our model. <em>F<sub>1</sub></em> represents a balance between the recall and precision, where the relative contributions of both are equal.</p>
<p>As an alternative to using <code>cross_val_score</code>, if we already have the true y
values and the predicted y values, we can calculate the metrics accuracy and recall directly:<a data-primary="" data-startref="ix_bin_class_predict" data-type="indexterm" id="id1563"/><a data-primary="" data-startref="ix_cross_val_score" data-type="indexterm" id="id1564"/><a data-primary="" data-startref="ix_mod_eval_bin_class" data-type="indexterm" id="id1565"/><a data-primary="" data-startref="ix_predict_bin_class" data-type="indexterm" id="id1566"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">accuracy_score</code>

<code class="c1"># Create training and test split</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">X</code><code class="p">,</code>
                                                    <code class="n">y</code><code class="p">,</code>
                                                    <code class="n">test_size</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code>
                                                    <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Predict values for training target vector</code>
<code class="n">y_hat</code> <code class="o">=</code> <code class="n">logit</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>

<code class="c1"># Calculate accuracy</code>
<code class="n">accuracy_score</code><code class="p">(</code><code class="n">y_test</code><code class="p">,</code> <code class="n">y_hat</code><code class="p">)</code></pre>
<pre data-type="programlisting">0.947</pre>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1567">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/vjgZ-">Accuracy paradox, Wikipedia</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="11.5 Evaluating Binary Classifier Thresholds" data-type="sect1"><div class="sect1" id="evaluating-binary-classifier-thresholds">
<h1>11.5 Evaluating Binary Classifier Thresholds</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id507">
<h2>Problem</h2>
<p>You want to evaluate a <a data-primary="thresholding" data-secondary="binary classifier thresholds" data-type="indexterm" id="ix_thresh_bin_class"/><a data-primary="predictions and predicting" data-secondary="binary classifier thresholds" data-type="indexterm" id="ix_predict_bin_class_th"/><a data-primary="model evaluation" data-secondary="binary classifier thresholds" data-type="indexterm" id="ix_mod_eval_bin_class_th"/><a data-primary="filter methods" data-secondary="binary classifier threshold evaluation" data-type="indexterm" id="ix_filter_meth_bin_class_thresh"/><a data-primary="binary classifiers" data-secondary="thresholding" data-type="indexterm" id="ix_bin_class_thresh"/>binary classifier and various probability
thresholds.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id264">
<h2>Solution</h2>
<p>Use the <em>receiver operating characteristic</em> (ROC) <a data-primary="ROC (receiver operating characteristic) curve" data-type="indexterm" id="ix_roc_rec_op_ch_curve"/><a data-primary="receiver operating characteristic (ROC) curve" data-type="indexterm" id="ix_rec_op_ch_curve_roc"/>curve to evaluate the quality of the binary classifier. In <a data-primary="roc_curve" data-type="indexterm" id="id1568"/>scikit-learn, we can use <code>roc_curve</code> to calculate the true and false positives at each threshold, and then plot them:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_classification</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LogisticRegression</code>
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">roc_curve</code><code class="p">,</code> <code class="n">roc_auc_score</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>

<code class="c1"># Create feature matrix and target vector</code>
<code class="n">features</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="n">make_classification</code><code class="p">(</code><code class="n">n_samples</code><code class="o">=</code><code class="mi">10000</code><code class="p">,</code>
                                       <code class="n">n_features</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code>
                                       <code class="n">n_classes</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>
                                       <code class="n">n_informative</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code>
                                       <code class="n">random_state</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code>

<code class="c1"># Split into training and test sets</code>
<code class="n">features_train</code><code class="p">,</code> <code class="n">features_test</code><code class="p">,</code> <code class="n">target_train</code><code class="p">,</code> <code class="n">target_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Create classifier</code>
<code class="n">logit</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">()</code>

<code class="c1"># Train model</code>
<code class="n">logit</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_train</code><code class="p">,</code> <code class="n">target_train</code><code class="p">)</code>

<code class="c1"># Get predicted probabilities</code>
<code class="n">target_probabilities</code> <code class="o">=</code> <code class="n">logit</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">features_test</code><code class="p">)[:,</code><code class="mi">1</code><code class="p">]</code>

<code class="c1"># Create true and false positive rates</code>
<code class="n">false_positive_rate</code><code class="p">,</code> <code class="n">true_positive_rate</code><code class="p">,</code> <code class="n">threshold</code> <code class="o">=</code> <code class="n">roc_curve</code><code class="p">(</code>
  <code class="n">target_test</code><code class="p">,</code>
  <code class="n">target_probabilities</code>
<code class="p">)</code>

<code class="c1"># Plot ROC curve</code>
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s2">"Receiver Operating Characteristic"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">false_positive_rate</code><code class="p">,</code> <code class="n">true_positive_rate</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">([</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">ls</code><code class="o">=</code><code class="s2">"--"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">([</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">]</code> <code class="p">,</code> <code class="n">c</code><code class="o">=</code><code class="s2">".7"</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code> <code class="p">,</code> <code class="n">c</code><code class="o">=</code><code class="s2">".7"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"True Positive Rate"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"False Positive Rate"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 11in01" height="432" src="assets/mpc2_11in01.png" width="550"/>
<h6/>
</div></figure>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id265">
<h2>Discussion</h2>
<p>The receiver operating characteristic curve is a common method for evaluating the quality of a binary classifier. ROC compares the presence of true positives and false positives at every probability threshold (i.e., the probability at which an observation is predicted to be a class). By plotting the ROC curve, we can see how the model performs. A classifier that predicts every observation correctly would look like the solid light gray line in the ROC output in the previous figure, going straight up to the top immediately. A classifier that predicts at random will appear as the diagonal line. The better the model, the closer it is to the solid line.</p>
<p>Until <a data-primary="probability estimates for prediction" data-type="indexterm" id="ix_prob_est_pred"/>now we have only examined models based on the values they
predict. However, in many learning algorithms, those predicted values are based on probability estimates. That is, each observation is given an explicit probability of belonging in each class. In our <a data-primary="predict_proba method" data-type="indexterm" id="id1569"/>solution, we can use <code>predict_proba</code> to see the predicted probabilities for the first observation:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Get predicted probabilities</code>
<code class="n">logit</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">features_test</code><code class="p">)[</code><code class="mi">0</code><code class="p">:</code><code class="mi">1</code><code class="p">]</code></pre>
<pre data-type="programlisting">array([[0.86891533, 0.13108467]])</pre>
<p>We can see the classes using <code>classes_</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">logit</code><code class="o">.</code><code class="n">classes_</code></pre>
<pre data-type="programlisting">array([0, 1])</pre>
<p>In this example, the first observation has an ~87% chance of being in the negative class (<code>0</code>) and a 13% chance of being in the positive class (<code>1</code>). By default, scikit-learn predicts an observation is part of the positive class if the probability is greater than 0.5 (called the <em>threshold</em>). However, instead of a middle ground, we will often want to explicitly bias our model to use a different threshold for substantive reasons. For example, if a false positive is very costly to our company, we might prefer a model that has a high probability threshold. We fail to predict some positives, but when an observation is predicted to be positive, we can be very confident that the prediction is correct. This <a data-primary="false positive rate (FPR)" data-type="indexterm" id="id1570"/><a data-primary="FPR (false positive rate)" data-type="indexterm" id="id1571"/><a data-primary="TPR (true positive rate)" data-type="indexterm" id="id1572"/><a data-primary="true positive rate (TPR)" data-type="indexterm" id="id1573"/>trade-off is represented in the <em>true positive rate</em> (TPR) and the <em>false positive rate</em> (FPR). The TPR is the number of observations correctly predicted true divided by all true positive observations:</p>
<div data-type="equation">
<math display="block">
<mrow>
<mtext>TPR</mtext>
<mo>=</mo>
<mfrac><mrow><mtext>TP</mtext></mrow> <mrow><mtext>TP</mtext><mo>+</mo><mtext>FN</mtext></mrow></mfrac>
</mrow>
</math>
</div>
<p>The FPR is the number of incorrectly predicted positives divided by all true negative observations:</p>
<div data-type="equation">
<math display="block">
<mrow>
<mtext>FPR</mtext>
<mo>=</mo>
<mfrac><mrow><mtext>FP</mtext></mrow> <mrow><mtext>FP</mtext><mo>+</mo><mtext>TN</mtext></mrow></mfrac>
</mrow>
</math>
</div>
<p>The ROC curve represents the respective TPR and FPR for every probability threshold. For example, in our solution a threshold of roughly 0.50 has a TPR of ~0.83 and an FPR of ~0.16:</p>
<pre data-code-language="python" data-type="programlisting"><code class="nb">print</code><code class="p">(</code><code class="s2">"Threshold:"</code><code class="p">,</code> <code class="n">threshold</code><code class="p">[</code><code class="mi">124</code><code class="p">])</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"True Positive Rate:"</code><code class="p">,</code> <code class="n">true_positive_rate</code><code class="p">[</code><code class="mi">124</code><code class="p">])</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"False Positive Rate:"</code><code class="p">,</code> <code class="n">false_positive_rate</code><code class="p">[</code><code class="mi">124</code><code class="p">])</code></pre>
<pre data-type="programlisting">Threshold: 0.5008252732632008
True Positive Rate: 0.8346938775510204
False Positive Rate: 0.1607843137254902</pre>
<p>However, if we increase the threshold to ~80% (i.e., increase how certain
the model has to be before it predicts an observation as positive) the
TPR drops significantly but so does the FPR:</p>
<pre data-code-language="python" data-type="programlisting"><code class="nb">print</code><code class="p">(</code><code class="s2">"Threshold:"</code><code class="p">,</code> <code class="n">threshold</code><code class="p">[</code><code class="mi">49</code><code class="p">])</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"True Positive Rate:"</code><code class="p">,</code> <code class="n">true_positive_rate</code><code class="p">[</code><code class="mi">49</code><code class="p">])</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"False Positive Rate:"</code><code class="p">,</code> <code class="n">false_positive_rate</code><code class="p">[</code><code class="mi">49</code><code class="p">])</code></pre>
<pre data-type="programlisting">Threshold: 0.8058575028551827
True Positive Rate: 0.5653061224489796
False Positive Rate: 0.052941176470588235</pre>
<p>This is because our higher requirement for being predicted to be in the
positive class has caused the model to not identify a number of positive
observations (the lower TPR) but has also reduced the noise from negative
observations being predicted as positive (the lower FPR).</p>
<p>In addition to being able to visualize the trade-off between TPR and
FPR, the ROC curve can also be used as a general metric for a model.<a data-primary="" data-startref="ix_classif_bin_classif" data-type="indexterm" id="id1574"/><a data-primary="" data-startref="ix_bin_class_thresh" data-type="indexterm" id="id1575"/><a data-primary="" data-startref="ix_filter_meth_bin_class_thresh" data-type="indexterm" id="id1576"/><a data-primary="" data-startref="ix_mod_eval_bin_class_th" data-type="indexterm" id="id1577"/><a data-primary="" data-startref="ix_predict_bin_class_th" data-type="indexterm" id="id1578"/><a data-primary="" data-startref="ix_thresh_bin_class" data-type="indexterm" id="id1579"/> The
better a model is, the higher the curve and thus the greater the area
under the curve. For this reason, it is common to calculate the <a data-primary="area under the ROC curve (AUC ROC)" data-type="indexterm" id="id1580"/>area
under the ROC curve (AUC ROC) to judge the overall quality of a model at
all possible thresholds. The closer the AUC ROC is to 1, the better the
model. In scikit-learn we can calculate the AUC ROC using
<code>roc_auc_score</code>:<a data-primary="" data-startref="ix_rec_op_ch_curve_roc" data-type="indexterm" id="id1581"/><a data-primary="" data-startref="ix_roc_rec_op_ch_curve" data-type="indexterm" id="id1582"/><a data-primary="" data-startref="ix_prob_est_pred" data-type="indexterm" id="id1583"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Calculate area under curve</code>
<code class="n">roc_auc_score</code><code class="p">(</code><code class="n">target_test</code><code class="p">,</code> <code class="n">target_probabilities</code><code class="p">)</code></pre>
<pre data-type="programlisting">0.9073389355742297</pre>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1584">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/0qcpZ">ROC Curves in Python and R</a></p>
</li>
<li>
<p><a href="https://oreil.ly/re7sT">The Area Under an ROC Curve</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="11.6 Evaluating Multiclass Classifier Predictions" data-type="sect1"><div class="sect1" id="evaluating-multi-class-classifier-predictions">
<h1>11.6 Evaluating Multiclass Classifier Predictions</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id266">
<h2>Problem</h2>
<p>You have a <a data-primary="predictions and predicting" data-secondary="multiclass classifier evaluation of" data-type="indexterm" id="ix_predict_multclass_eval"/><a data-primary="model evaluation" data-secondary="multiclass classifier predictions" data-type="indexterm" id="ix_mod_eval_multi_class_pred"/><a data-primary="multiclass classifiers" data-type="indexterm" id="ix_multi_class_pred"/><a data-primary="classification and classifiers" data-secondary="multiclass predictions" data-type="indexterm" id="ix_classif_multi_pred"/>model that predicts three or more classes and want to
evaluate the model’s performance.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id508">
<h2>Solution</h2>
<p>Use <a data-primary="cross-validation (CV) of ML models" data-secondary="multiclassifier predictions" data-type="indexterm" id="ix_cv_multi_class_pred"/>cross-validation with an evaluation metric capable of handling more
than two classes:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">cross_val_score</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LogisticRegression</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_classification</code>

<code class="c1"># Generate features matrix and target vector</code>
<code class="n">features</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="n">make_classification</code><code class="p">(</code><code class="n">n_samples</code> <code class="o">=</code> <code class="mi">10000</code><code class="p">,</code>
                           <code class="n">n_features</code> <code class="o">=</code> <code class="mi">3</code><code class="p">,</code>
                           <code class="n">n_informative</code> <code class="o">=</code> <code class="mi">3</code><code class="p">,</code>
                           <code class="n">n_redundant</code> <code class="o">=</code> <code class="mi">0</code><code class="p">,</code>
                           <code class="n">n_classes</code> <code class="o">=</code> <code class="mi">3</code><code class="p">,</code>
                           <code class="n">random_state</code> <code class="o">=</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Create logistic regression</code>
<code class="n">logit</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">()</code>

<code class="c1"># Cross-validate model using accuracy</code>
<code class="n">cross_val_score</code><code class="p">(</code><code class="n">logit</code><code class="p">,</code> <code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">,</code> <code class="n">scoring</code><code class="o">=</code><code class="s1">'accuracy'</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([0.841 , 0.829 , 0.8265, 0.8155, 0.82  ])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id267">
<h2>Discussion</h2>
<p>When we have balanced classes (i.e., a roughly equal number of
observations in each class of the target vector), <a data-primary="accuracy metric" data-type="indexterm" id="id1585"/>accuracy is—​just
like in the binary class setting—​a simple and interpretable choice
for an evaluation metric. Accuracy is the number of correct predictions
divided by the number of observations and works just as well in the
multiclass as in the binary setting. However, when we have <a data-primary="imbalanced classes, handling" data-secondary="multiclass classifier evaluation" data-type="indexterm" id="id1586"/>imbalanced classes
(a common scenario), we should be inclined to use other evaluation
metrics.</p>
<p>Many of scikit-learn’s built-in metrics are for evaluating binary
classifiers. However, many of these metrics can be extended for use when
we have more than two classes. <a data-primary="precision metric" data-type="indexterm" id="id1587"/><a data-primary="recall metric" data-type="indexterm" id="id1588"/><a data-primary="F₁ score" data-type="indexterm" id="id1589"/>Precision, recall, and <em>F<sub>1</sub></em> scores are
useful metrics that we have already covered in detail in previous
recipes. While all of them were originally designed for binary
classifiers, we can apply them to multiclass settings by treating our
data as a set of binary classes. Doing so enables us to apply the
metrics to each class as if it were the only class in the data, and then
aggregate the evaluation scores for all the classes by averaging them:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Cross-validate model using macro averaged F1 score</code>
<code class="n">cross_val_score</code><code class="p">(</code><code class="n">logit</code><code class="p">,</code> <code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">,</code> <code class="n">scoring</code><code class="o">=</code><code class="s1">'f1_macro'</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([0.84061272, 0.82895312, 0.82625661, 0.81515121, 0.81992692])</pre>
<p>In this code, <code>macro</code> refers to the method used to average the
evaluation scores from the classes. The options are <code>macro</code>, <code>weighted</code>, and <code>micro</code>:</p>
<dl>
<dt><code>macro</code></dt>
<dd>
<p>  Calculate the mean of metric scores for each class, weighting
each class equally.</p>
</dd>
<dt><code>weighted</code></dt>
<dd>
<p>  Calculate the mean of metric scores for each class, weighting
each class proportional to its size in the data.</p>
</dd>
<dt><code>micro</code></dt>
<dd>
<p>  Calculate the mean of metric scores for each observation-class
combination.<a data-primary="" data-startref="ix_classif_multi_pred" data-type="indexterm" id="id1590"/><a data-primary="" data-startref="ix_cv_multi_class_pred" data-type="indexterm" id="id1591"/><a data-primary="" data-startref="ix_mod_eval_multi_class_pred" data-type="indexterm" id="id1592"/><a data-primary="" data-startref="ix_multi_class_pred" data-type="indexterm" id="id1593"/><a data-primary="" data-startref="ix_predict_multclass_eval" data-type="indexterm" id="id1594"/></p>
</dd>
</dl>
</div></section>
</div></section>
<section data-pdf-bookmark="11.7 Visualizing a Classifier’s Performance" data-type="sect1"><div class="sect1" id="visualizing-a-classifiers-performance">
<h1>11.7 Visualizing a Classifier’s Performance</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id509">
<h2>Problem</h2>
<p>Given <a data-primary="model evaluation" data-secondary="classifier performance visualization" data-type="indexterm" id="ix_mod_eval_classif_vis_perf"/><a data-primary="visualization" data-secondary="classifier performance" data-type="indexterm" id="ix_vis_class_perf"/><a data-primary="classification and classifiers" data-secondary="visualizing classifier performance" data-type="indexterm" id="ix_classif_vis_perf"/>predicted classes and true classes of the test data, you want to
visually compare the model’s quality.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id268">
<h2>Solution</h2>
<p>Use a <em>confusion matrix</em>, which <a data-primary="confusion matrices" data-type="indexterm" id="id1595"/>compares predicted classes and true
classes:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>
<code class="kn">import</code> <code class="nn">seaborn</code> <code class="k">as</code> <code class="nn">sns</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LogisticRegression</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">confusion_matrix</code>
<code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>

<code class="c1"># Create features matrix</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>

<code class="c1"># Create target vector</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create list of target class names</code>
<code class="n">class_names</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target_names</code>

<code class="c1"># Create training and test set</code>
<code class="n">features_train</code><code class="p">,</code> <code class="n">features_test</code><code class="p">,</code> <code class="n">target_train</code><code class="p">,</code> <code class="n">target_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>

<code class="c1"># Create logistic regression</code>
<code class="n">classifier</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">()</code>

<code class="c1"># Train model and make predictions</code>
<code class="n">target_predicted</code> <code class="o">=</code> <code class="n">classifier</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_train</code><code class="p">,</code>
    <code class="n">target_train</code><code class="p">)</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">features_test</code><code class="p">)</code>

<code class="c1"># Create confusion matrix</code>
<code class="n">matrix</code> <code class="o">=</code> <code class="n">confusion_matrix</code><code class="p">(</code><code class="n">target_test</code><code class="p">,</code> <code class="n">target_predicted</code><code class="p">)</code>

<code class="c1"># Create pandas dataframe</code>
<code class="n">dataframe</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">matrix</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">class_names</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="n">class_names</code><code class="p">)</code>

<code class="c1"># Create heatmap</code>
<code class="n">sns</code><code class="o">.</code><code class="n">heatmap</code><code class="p">(</code><code class="n">dataframe</code><code class="p">,</code> <code class="n">annot</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">cbar</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s2">"Blues"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s2">"Confusion Matrix"</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">tight_layout</code><code class="p">()</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"True Class"</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Predicted Class"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 11in02" height="444" src="assets/mpc2_11in02.png" width="600"/>
<h6/>
</div></figure>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id269">
<h2>Discussion</h2>
<p>Confusion matrices are an easy, effective visualization of a
classifier’s performance. One of the major benefits of confusion
matrices is their interpretability. Each column of the matrix (often
visualized as a heatmap) represents predicted classes, while every row
shows true classes. The result is that every cell is one possible
combination of predicted and true classes. This is probably best explained using an example. In the <span class="keep-together">solution</span>, the top-left cell is the number of observations predicted to be <em>Iris setosa</em> (indicated by the column) that are actually <em>Iris setosa</em> (indicated by the row). This means the model accurately predicted all <em>Iris setosa</em> flowers. However, the model does not do as well at predicting <em>Iris virginica</em>. The bottom-right cell indicates that the model successfully predicted eleven observations were <em>Iris virginica</em>, but (looking one cell up) predicted one flower to be <em>virginica</em> that was actually <em>Iris versicolor</em>.</p>
<p>There are three things worth noting about confusion matrices. First, a
perfect model will have values along the diagonal and zeros everywhere
else. A bad model will have the observation counts spread
evenly around cells. Second, a confusion matrix lets us see not only where the model was wrong but also how it was wrong. That is, we can look at patterns of misclassification. For example, our model had an easy time differentiating <em>Iris virginica</em> and <em>Iris setosa</em>, but a slightly more difficult time classifying <em>Iris virginica</em> and <em>Iris versicolor</em>. Finally, confusion matrices work with any number of classes (although if we had
one million classes in our target vector, the confusion matrix
visualization might be difficult to read).<a data-primary="" data-startref="ix_classif_vis_perf" data-type="indexterm" id="id1596"/><a data-primary="" data-startref="ix_mod_eval_classif_vis_perf" data-type="indexterm" id="id1597"/><a data-primary="" data-startref="ix_vis_class_perf" data-type="indexterm" id="id1598"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1599">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/tDWPB">Confusion matrix, Wikipedia</a></p>
</li>
<li>
<p><a href="https://oreil.ly/fdsTg">scikit-learn documentation: Confusion Matrix</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="11.8 Evaluating Regression Models" data-type="sect1"><div class="sect1" id="evaluating-regression-models">
<h1>11.8 Evaluating Regression Models</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id510">
<h2>Problem</h2>
<p>You want to <a data-primary="regression and regressors" data-secondary="model evaluation" data-type="indexterm" id="ix_regress_mod_eval2"/><a data-primary="model evaluation" data-secondary="regression models" data-type="indexterm" id="ix_mod_eval_regress_mod"/>evaluate the performance of a regression model.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id270">
<h2>Solution</h2>
<p>Use <em>mean squared error</em> <a data-primary="MSE (mean squared error)" data-type="indexterm" id="ix_mse_mean_sq_err"/><a data-primary="mean squared error (MSE)" data-type="indexterm" id="ix_mean_sq_err_mse"/>(MSE):</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_regression</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">cross_val_score</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LinearRegression</code>

<code class="c1"># Generate features matrix, target vector</code>
<code class="n">features</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="n">make_regression</code><code class="p">(</code><code class="n">n_samples</code> <code class="o">=</code> <code class="mi">100</code><code class="p">,</code>
                                   <code class="n">n_features</code> <code class="o">=</code> <code class="mi">3</code><code class="p">,</code>
                                   <code class="n">n_informative</code> <code class="o">=</code> <code class="mi">3</code><code class="p">,</code>
                                   <code class="n">n_targets</code> <code class="o">=</code> <code class="mi">1</code><code class="p">,</code>
                                   <code class="n">noise</code> <code class="o">=</code> <code class="mi">50</code><code class="p">,</code>
                                   <code class="n">coef</code> <code class="o">=</code> <code class="kc">False</code><code class="p">,</code>
                                   <code class="n">random_state</code> <code class="o">=</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Create a linear regression object</code>
<code class="n">ols</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">()</code>

<code class="c1"># Cross-validate the linear regression using (negative) MSE</code>
<code class="n">cross_val_score</code><code class="p">(</code><code class="n">ols</code><code class="p">,</code> <code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">,</code> <code class="n">scoring</code><code class="o">=</code><code class="s1">'neg_mean_squared_error'</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([-1974.65337976, -2004.54137625, -3935.19355723, -1060.04361386,
       -1598.74104702])</pre>
<p>Another <a data-primary="R² (coefficient of determination)" data-type="indexterm" id="id1600"/><a data-primary="coefficient of determination (R²)" data-type="indexterm" id="id1601"/>common regression metric is the coefficient of determination,
<em>R<sup>2</sup></em>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Cross-validate the linear regression using R-squared</code>
<code class="n">cross_val_score</code><code class="p">(</code><code class="n">ols</code><code class="p">,</code> <code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">,</code> <code class="n">scoring</code><code class="o">=</code><code class="s1">'r2'</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([0.8622399 , 0.85838075, 0.74723548, 0.91354743, 0.84469331])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id271">
<h2>Discussion</h2>
<p>MSE is one of the most common evaluation metrics for regression models.
Formally, MSE is:</p>
<div data-type="equation">
<math display="block">
<mrow>
<mo form="prefix">MSE</mo>
<mo>=</mo>
<mfrac><mn>1</mn> <mi>n</mi></mfrac>
<munderover><mo>∑</mo> <mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mrow> <mi>n</mi> </munderover>
<msup><mrow><mo>(</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover><mi>i</mi></msub><mo>-</mo><msub><mi>y</mi> <mi>i</mi> </msub><mo>)</mo></mrow> <mn>2</mn> </msup>
</mrow>
</math>
</div>
<p>where <math display="inline"><mi>n</mi></math> is the number of observations, <math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math>
is the true value of the target we are trying to predict for observation <math display="inline"><mi>i</mi></math>, and <math display="inline"><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover><mi>i</mi> </msub></math> is the model’s predicted value for <math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math>. MSE is a measurement of the squared sum of all distances between predicted and true values. The higher the value of MSE, the greater the total squared error and thus the worse the model. <a data-primary="error handling" data-secondary="mean squared error" data-type="indexterm" id="id1602"/>There are a number of mathematical benefits to squaring the error term, including that it forces all error values to be positive, but one often unrealized implication is that squaring penalizes a few large errors more than many small errors, even if the absolute value of the errors is the same. For example, imagine two models, A and B, each with two observations:</p>
<ul>
<li>
<p>Model A has errors of 0 and 10, and thus its MSE is
<em>0<sup>2</sup> + 10<sup>2</sup> = 100</em>.</p>
</li>
<li>
<p>Model B has two errors of 5 each, and thus its MSE is
<em>5<sup>2</sup> + 5<sup>2</sup> = 50</em>.</p>
</li>
</ul>
<p class="less_space pagebreak-before">Both models have the same total errors, 10; however, MSE would consider
model A (MSE = 100) worse than model B (MSE = 50). In practice this implication is rarely an issue (and indeed can be theoretically beneficial), and MSE works perfectly fine as an evaluation metric.</p>
<p>One important note: by default, in scikit-learn, arguments of the
<code>scoring</code> parameter assume that higher values are better than lower
values. However, this is not the case for MSE, where higher values mean
a worse model. For this reason, scikit-learn looks at the <em>negative</em> MSE using the <code>neg_mean_squared_error</code> argument.</p>
<p>A <a data-primary="R² (coefficient of determination)" data-type="indexterm" id="id1603"/><a data-primary="coefficient of determination (R²)" data-type="indexterm" id="id1604"/>common alternative regression evaluation metric is the default metric we used in <a data-type="xref" href="#creating-a-baseline-regression-model">Recipe 11.2</a>, <math display="inline"><msup><mi>R</mi><mn>2</mn></msup></math>, which measures the amount of variance in the target vector that is explained by the model.</p>
<div data-type="equation">
<math display="block">
<mrow>
<msup><mi>R</mi> <mn>2</mn> </msup>
<mo>=</mo>
<mn>1</mn>
<mo>-</mo>
<mfrac><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </msubsup><msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi> </msub><mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>i</mi> </msub><mo>)</mo></mrow> <mn>2</mn> </msup></mrow> <mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </msubsup><msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi> </msub><mo>-</mo><mover accent="true"><mi>y</mi> <mo>¯</mo></mover><mo>)</mo></mrow> <mn>2</mn> </msup></mrow></mfrac>
</mrow>
</math>
</div>
<p>where <math display="inline"><msub><mi>y</mi> <mi>i</mi> </msub></math> is the true target value of the
<em>i</em>th observation, <math display="inline"><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover> <mi>i</mi> </msub></math> is the predicted value for the <em>i</em>th observation, and 
<math display="inline"><mover accent="true"><mi>y</mi> <mo>¯</mo></mover></math> is the mean value of the target vector. The closer that <math display="inline"><msup><mi>R</mi><mn>2</mn></msup></math> is to 1.0, the better
the model.<a data-primary="" data-startref="ix_mean_sq_err_mse" data-type="indexterm" id="id1605"/><a data-primary="" data-startref="ix_mod_eval_regress_mod" data-type="indexterm" id="id1606"/><a data-primary="" data-startref="ix_mse_mean_sq_err" data-type="indexterm" id="id1607"/><a data-primary="" data-startref="ix_regress_mod_eval2" data-type="indexterm" id="id1608"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1609">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/MWDlR">Mean squared error, Wikipedia</a></p>
</li>
<li>
<p><a href="https://oreil.ly/lKKWk">Coefficient of determination, Wikipedia</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="11.9 Evaluating Clustering Models" data-type="sect1"><div class="sect1" id="evaluating-clustering-models">
<h1>11.9 Evaluating Clustering Models</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id272">
<h2>Problem</h2>
<p>You have used an <a data-primary="model evaluation" data-secondary="cluster models" data-type="indexterm" id="ix_mod_eval_cluster"/><a data-primary="clustering" data-secondary="model evaluation" data-type="indexterm" id="ix_cluster_mod_eval"/><a data-primary="unsupervised learning models" data-type="indexterm" id="ix_unsup_learn_ch11"/>unsupervised learning algorithm to cluster your data. Now you want to know how well it did.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id273">
<h2>Solution</h2>
<p>Use <em>silhouette coefficients</em> to <a data-primary="silhouette coefficients" data-type="indexterm" id="ix_silhou_coeff"/>measure the quality of the clusters (note that this does not measure predictive performance):</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">silhouette_score</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.cluster</code> <code class="kn">import</code> <code class="n">KMeans</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_blobs</code>

<code class="c1"># Generate features matrix</code>
<code class="n">features</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="n">make_blobs</code><code class="p">(</code><code class="n">n_samples</code> <code class="o">=</code> <code class="mi">1000</code><code class="p">,</code>
                         <code class="n">n_features</code> <code class="o">=</code> <code class="mi">10</code><code class="p">,</code>
                         <code class="n">centers</code> <code class="o">=</code> <code class="mi">2</code><code class="p">,</code>
                         <code class="n">cluster_std</code> <code class="o">=</code> <code class="mf">0.5</code><code class="p">,</code>
                         <code class="n">shuffle</code> <code class="o">=</code> <code class="kc">True</code><code class="p">,</code>
                         <code class="n">random_state</code> <code class="o">=</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Cluster data using k-means to predict classes</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Get predicted classes</code>
<code class="n">target_predicted</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">labels_</code>

<code class="c1"># Evaluate model</code>
<code class="n">silhouette_score</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target_predicted</code><code class="p">)</code></pre>
<pre data-type="programlisting">0.8916265564072141</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id274">
<h2>Discussion</h2>
<p><em>Supervised model evaluation</em> compares <a data-primary="supervised learning models" data-secondary="versus unsupervised models" data-secondary-sortas="unsupervised models" data-type="indexterm" id="id1610"/>predictions (e.g., classes or
quantitative values) with the corresponding true values in the target
vector. However, the most common motivation for using clustering methods is that your data doesn’t have a target vector. A number of clustering evaluation metrics require a target vector, but again, using unsupervised learning approaches like clustering when you have a target vector available to you is probably handicapping yourself unnecessarily.</p>
<p>While we cannot evaluate predictions versus true values if we don’t have a target vector, we can evaluate the nature of the clusters themselves. Intuitively, we can imagine “good” clusters having very small distances between observations in the same cluster (i.e., dense clusters) and large distances between the different clusters (i.e., well-separated clusters). Silhouette coefficients provide a single value measuring both traits. Formally, the <em>i</em>th observation’s silhouette coefficient is:</p>
<div data-type="equation">
<math display="block">
<mrow>
<msub><mi>s</mi> <mi>i</mi> </msub>
<mo>=</mo>
<mfrac><mrow><msub><mi>b</mi> <mi>i</mi> </msub><mo>-</mo><msub><mi>a</mi> <mi>i</mi> </msub></mrow> <mrow><mtext>max</mtext><mo>(</mo><msub><mi>a</mi> <mi>i</mi> </msub><mo>,</mo><msub><mi>b</mi> <mi>i</mi> </msub><mo>)</mo></mrow></mfrac>
</mrow>
</math>
</div>
<p>where <math display="inline"><msub><mi>s</mi><mi>i</mi></msub></math> is the silhouette coefficient for observation <math display="inline"><mi>i</mi></math>, <math display="inline"><msub><mi>a</mi><mi>i</mi></msub></math> is the mean distance between <math display="inline"><mi>i</mi></math> and all observations of the same class, and <math display="inline"><msub><mi>b</mi><mi>i</mi></msub></math> is the mean distance between <math display="inline"><mi>i</mi></math> and all observations from the closest cluster of a different class. The value returned by <code>silhouette_score</code> is the mean silhouette coefficient for all observations. Silhouette coefficients range between –1 and 1, with 1 indicating dense,  well-separated clusters.<a data-primary="" data-startref="ix_cluster_mod_eval" data-type="indexterm" id="id1611"/><a data-primary="" data-startref="ix_mod_eval_cluster" data-type="indexterm" id="id1612"/><a data-primary="" data-startref="ix_silhou_coeff" data-type="indexterm" id="id1613"/><a data-primary="" data-startref="ix_unsup_learn_ch11" data-type="indexterm" id="id1614"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1615">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/gGjQj">scikit-learn documentation: silhouette_score</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="11.10 Creating a Custom Evaluation Metric" data-type="sect1"><div class="sect1" id="creating-custom-evaluation-metric">
<h1>11.10 Creating a Custom Evaluation Metric</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id275">
<h2>Problem</h2>
<p>You want to <a data-primary="model evaluation" data-secondary="custom evaluation metric" data-type="indexterm" id="ix_mod_eval_cust_metric"/><a data-primary="custom evaluation metric, creating" data-type="indexterm" id="ix_cust_eval_metric"/>evaluate a model using a metric you created.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id276">
<h2>Solution</h2>
<p>Create the <a data-primary="make_scorer function" data-type="indexterm" id="ix_make_scorer_func"/><a data-primary="metrics" data-secondary="custom evaluation" data-type="indexterm" id="ix_metric_cust_eval"/>metric as a function and convert it into a scorer function
using scikit-learn’s <code>make_scorer</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">make_scorer</code><code class="p">,</code> <code class="n">r2_score</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">Ridge</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_regression</code>

<code class="c1"># Generate features matrix and target vector</code>
<code class="n">features</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="n">make_regression</code><code class="p">(</code><code class="n">n_samples</code> <code class="o">=</code> <code class="mi">100</code><code class="p">,</code>
                                   <code class="n">n_features</code> <code class="o">=</code> <code class="mi">3</code><code class="p">,</code>
                                   <code class="n">random_state</code> <code class="o">=</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Create training set and test set</code>
<code class="n">features_train</code><code class="p">,</code> <code class="n">features_test</code><code class="p">,</code> <code class="n">target_train</code><code class="p">,</code> <code class="n">target_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
     <code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="mf">0.10</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Create custom metric</code>
<code class="k">def</code> <code class="nf">custom_metric</code><code class="p">(</code><code class="n">target_test</code><code class="p">,</code> <code class="n">target_predicted</code><code class="p">):</code>
    <code class="c1"># Calculate R-squared score</code>
    <code class="n">r2</code> <code class="o">=</code> <code class="n">r2_score</code><code class="p">(</code><code class="n">target_test</code><code class="p">,</code> <code class="n">target_predicted</code><code class="p">)</code>
    <code class="c1"># Return R-squared score</code>
    <code class="k">return</code> <code class="n">r2</code>

<code class="c1"># Make scorer and define that higher scores are better</code>
<code class="n">score</code> <code class="o">=</code> <code class="n">make_scorer</code><code class="p">(</code><code class="n">custom_metric</code><code class="p">,</code> <code class="n">greater_is_better</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>

<code class="c1"># Create ridge regression object</code>
<code class="n">classifier</code> <code class="o">=</code> <code class="n">Ridge</code><code class="p">()</code>

<code class="c1"># Train ridge regression model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">classifier</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_train</code><code class="p">,</code> <code class="n">target_train</code><code class="p">)</code>

<code class="c1"># Apply custom scorer</code>
<code class="n">score</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">features_test</code><code class="p">,</code> <code class="n">target_test</code><code class="p">)</code></pre>
<pre data-type="programlisting">0.9997906102882058</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id277">
<h2>Discussion</h2>
<p>While scikit-learn has a number of built-in metrics for evaluating model
performance, it is often useful to define our own metrics. scikit-learn
makes this easy using <code>make_scorer</code>. First, we define a function that
takes in two arguments—​the ground truth target vector and our predicted
values—​and outputs some score. Second, we use <code>make_scorer</code> to create a
scorer object, making sure to specify whether higher or lower scores are
desirable (using the <code>greater_is_better</code> parameter).</p>
<p>The custom metric in the solution (<code>custom_metric</code>) is a toy
example since it simply wraps a built-in metric for calculating the
<em>R<sup>2</sup></em> score. In a real-world situation, we would replace the <code>custom_metric</code> function with whatever custom metric we wanted. However, we can see that the custom metric that calculates <em>R<sup>2</sup></em> does work by comparing the results to scikit-learn’s <code>r2_score</code> built-in
method:<a data-primary="" data-startref="ix_cust_eval_metric" data-type="indexterm" id="id1616"/><a data-primary="" data-startref="ix_make_scorer_func" data-type="indexterm" id="id1617"/><a data-primary="" data-startref="ix_metric_cust_eval" data-type="indexterm" id="id1618"/><a data-primary="" data-startref="ix_mod_eval_cust_metric" data-type="indexterm" id="id1619"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Predict values</code>
<code class="n">target_predicted</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">features_test</code><code class="p">)</code>

<code class="c1"># Calculate R-squared score</code>
<code class="n">r2_score</code><code class="p">(</code><code class="n">target_test</code><code class="p">,</code> <code class="n">target_predicted</code><code class="p">)</code></pre>
<pre data-type="programlisting">0.9997906102882058</pre>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1620">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/-RqFY">scikit-learn documentation: make_scorer</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="11.11 Visualizing the Effect of Training Set Size" data-type="sect1"><div class="sect1" id="visualizing-effect-of-training-set-size">
<h1>11.11 Visualizing the Effect of Training Set Size</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id278">
<h2>Problem</h2>
<p>You want to <a data-primary="training set" data-type="indexterm" id="ix_train_set_ch112"/><a data-primary="visualization" data-secondary="training set size effect" data-type="indexterm" id="ix_vis_train_set"/><a data-primary="model evaluation" data-secondary="training set size effect visualization" data-type="indexterm" id="ix_model_eval_train_set"/>evaluate the effect of the number of observations in your
training set on some metric (accuracy, <em>F<sub>1</sub></em>, etc.).</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id1621">
<h2>Solution</h2>
<p>Plot the accuracy against the training set size:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>
<code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">RandomForestClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_digits</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">learning_curve</code>

<code class="c1"># Load data</code>
<code class="n">digits</code> <code class="o">=</code> <code class="n">load_digits</code><code class="p">()</code>

<code class="c1"># Create feature matrix and target vector</code>
<code class="n">features</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="n">digits</code><code class="o">.</code><code class="n">data</code><code class="p">,</code> <code class="n">digits</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create CV training and test scores for various training set sizes</code>
<code class="n">train_sizes</code><code class="p">,</code> <code class="n">train_scores</code><code class="p">,</code> <code class="n">test_scores</code> <code class="o">=</code> <code class="n">learning_curve</code><code class="p">(</code><code class="c1"># Classifier</code>
                                                        <code class="n">RandomForestClassifier</code><code class="p">(),</code>
                                                        <code class="c1"># Feature matrix</code>
                                                        <code class="n">features</code><code class="p">,</code>
                                                        <code class="c1"># Target vector</code>
                                                        <code class="n">target</code><code class="p">,</code>
                                                        <code class="c1"># Number of folds</code>
                                                        <code class="n">cv</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code>
                                                        <code class="c1"># Performance metric</code>
                                                        <code class="n">scoring</code><code class="o">=</code><code class="s1">'accuracy'</code><code class="p">,</code>
                                                        <code class="c1"># Use all computer cores</code>
                                                        <code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">,</code>
                                                        <code class="c1"># Sizes of 50</code>
                                                        <code class="c1"># Training set</code>
                                                       <code class="n">train_sizes</code><code class="o">=</code><code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code>
                                                       <code class="mf">0.01</code><code class="p">,</code>
                                                       <code class="mf">1.0</code><code class="p">,</code>
                                                       <code class="mi">50</code><code class="p">))</code>

<code class="c1"># Create means and standard deviations of training set scores</code>
<code class="n">train_mean</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">train_scores</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="n">train_std</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">std</code><code class="p">(</code><code class="n">train_scores</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Create means and standard deviations of test set scores</code>
<code class="n">test_mean</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">test_scores</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="n">test_std</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">std</code><code class="p">(</code><code class="n">test_scores</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Draw lines</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">train_sizes</code><code class="p">,</code> <code class="n">train_mean</code><code class="p">,</code> <code class="s1">'--'</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s2">"#111111"</code><code class="p">,</code>  <code class="n">label</code><code class="o">=</code><code class="s2">"Training score"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">train_sizes</code><code class="p">,</code> <code class="n">test_mean</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s2">"#111111"</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"Cross-validation score"</code><code class="p">)</code>

<code class="c1"># Draw bands</code>
<code class="n">plt</code><code class="o">.</code><code class="n">fill_between</code><code class="p">(</code><code class="n">train_sizes</code><code class="p">,</code> <code class="n">train_mean</code> <code class="o">-</code> <code class="n">train_std</code><code class="p">,</code>
                 <code class="n">train_mean</code> <code class="o">+</code> <code class="n">train_std</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s2">"#DDDDDD"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">fill_between</code><code class="p">(</code><code class="n">train_sizes</code><code class="p">,</code> <code class="n">test_mean</code> <code class="o">-</code> <code class="n">test_std</code><code class="p">,</code>
                 <code class="n">test_mean</code> <code class="o">+</code> <code class="n">test_std</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s2">"#DDDDDD"</code><code class="p">)</code>

<code class="c1"># Create plot</code>
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s2">"Learning Curve"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Training Set Size"</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Accuracy Score"</code><code class="p">),</code>
<code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">(</code><code class="n">loc</code><code class="o">=</code><code class="s2">"best"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">tight_layout</code><code class="p">()</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 11in03" height="441" src="assets/mpc2_11in03.png" width="600"/>
<h6/>
</div></figure>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id279">
<h2>Discussion</h2>
<p><em>Learning curves</em> visualize<a data-primary="learning curves" data-type="indexterm" id="id1622"/> the performance (e.g., accuracy, recall) of a model on the training set and during cross-validation as the number of observations in the training set increases. They are commonly used to determine if our learning algorithms would benefit from gathering additional training data.</p>
<p>In our solution, we plot the accuracy of a random forest classifier at 50 different training set sizes, ranging from 1% of observations to 100%. The increasing accuracy score of the cross-validated models tell us that we would likely benefit from additional observations (although in
practice this might not be feasible).<a data-primary="" data-startref="ix_model_eval_train_set" data-type="indexterm" id="id1623"/><a data-primary="" data-startref="ix_train_set_ch112" data-type="indexterm" id="id1624"/><a data-primary="" data-startref="ix_vis_train_set" data-type="indexterm" id="id1625"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1626">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/jAKwy">scikit-learn documentation: Learning Curve</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="11.12 Creating a Text Report of Evaluation Metrics" data-type="sect1"><div class="sect1" id="creating-a-text-report-of-evaluation-metrics">
<h1>11.12 Creating a Text Report of Evaluation Metrics</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id511">
<h2>Problem</h2>
<p>You want a quick <a data-primary="text" data-secondary="evaluation metrics report" data-type="indexterm" id="ix_text_eval_metric"/><a data-primary="metrics" data-secondary="text report of evaluation" data-type="indexterm" id="ix_metric_text_rpt"/><a data-primary="model evaluation" data-secondary="text report of evaluation metrics" data-type="indexterm" id="ix_mod_eval_text_rpt"/><a data-primary="classification and classifiers" data-secondary="description of performance" data-type="indexterm" id="ix_classif_desc_perf"/>description of a classifier’s performance.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id280">
<h2>Solution</h2>
<p>Use <a data-primary="classification_report" data-type="indexterm" id="id1627"/>scikit-learn’s <code>classification_report</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LogisticRegression</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">classification_report</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>

<code class="c1"># Create features matrix</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>

<code class="c1"># Create target vector</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create list of target class names</code>
<code class="n">class_names</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target_names</code>

<code class="c1"># Create training and test set</code>
<code class="n">features_train</code><code class="p">,</code> <code class="n">features_test</code><code class="p">,</code> <code class="n">target_train</code><code class="p">,</code> <code class="n">target_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Create logistic regression</code>
<code class="n">classifier</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">()</code>

<code class="c1"># Train model and make predictions</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">classifier</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_train</code><code class="p">,</code> <code class="n">target_train</code><code class="p">)</code>
<code class="n">target_predicted</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">features_test</code><code class="p">)</code>

<code class="c1"># Create a classification report</code>
<code class="nb">print</code><code class="p">(</code><code class="n">classification_report</code><code class="p">(</code><code class="n">target_test</code><code class="p">,</code>
                            <code class="n">target_predicted</code><code class="p">,</code>
                            <code class="n">target_names</code><code class="o">=</code><code class="n">class_names</code><code class="p">))</code></pre>
<pre data-type="programlisting">              precision    recall  f1-score   support

      setosa       1.00      1.00      1.00        16
  versicolor       1.00      0.91      0.95        11
   virginica       0.92      1.00      0.96        11

    accuracy                           0.97        38
   macro avg       0.97      0.97      0.97        38
weighted avg       0.98      0.97      0.97        38</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id281">
<h2>Discussion</h2>
<p><code>classification_report</code> provides a quick means for us to see some common
evaluation metrics, including precision, recall, and <em>F<sub>1</sub></em> score (described
in <a data-type="xref" href="#evaluating-binary-classifier-predictions">Recipe 11.4</a>). Support refers to the number of observations
in each class.<a data-primary="" data-startref="ix_mod_eval_text_rpt" data-type="indexterm" id="id1628"/><a data-primary="" data-startref="ix_classif_desc_perf" data-type="indexterm" id="id1629"/><a data-primary="" data-startref="ix_metric_text_rpt" data-type="indexterm" id="id1630"/><a data-primary="" data-startref="ix_text_eval_metric" data-type="indexterm" id="id1631"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1632">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/9mBSF">Precision and
recall, Wikipedia</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="11.13 Visualizing the Effect of Hyperparameter Values" data-type="sect1"><div class="sect1" id="visualizing-effect-of-hyperparameter-values">
<h1>11.13 Visualizing the Effect of Hyperparameter Values</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id512">
<h2>Problem</h2>
<p>You want to <a data-primary="model evaluation" data-secondary="hyperparameter value effects visualization" data-type="indexterm" id="ix_mod_eval_hyper_val_vis"/><a data-primary="visualization" data-secondary="hyperparameter value effects" data-type="indexterm" id="ix_visual_hyper_val"/><a data-primary="hyperparameters" data-secondary="value effects visualization" data-type="indexterm" id="ix_hyperparam_val_vis"/><a data-primary="predictions and predicting" data-secondary="hyperparameter value effects" data-type="indexterm" id="ix_predict_hyper_val"/>understand how the performance of a model changes as the
value of some hyperparameter changes.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id282">
<h2>Solution</h2>
<p>Plot the hyperparameter against the model <a data-primary="accuracy metric" data-type="indexterm" id="ix_acc_metric2"/><a data-primary="validation_curve" data-type="indexterm" id="id1633"/>accuracy (validation curve):</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_digits</code>
<code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">RandomForestClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">validation_curve</code>

<code class="c1"># Load data</code>
<code class="n">digits</code> <code class="o">=</code> <code class="n">load_digits</code><code class="p">()</code>

<code class="c1"># Create feature matrix and target vector</code>
<code class="n">features</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="n">digits</code><code class="o">.</code><code class="n">data</code><code class="p">,</code> <code class="n">digits</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create range of values for parameter</code>
<code class="n">param_range</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">250</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>

<code class="c1"># Calculate accuracy on training and test set using range of parameter values</code>
<code class="n">train_scores</code><code class="p">,</code> <code class="n">test_scores</code> <code class="o">=</code> <code class="n">validation_curve</code><code class="p">(</code>
    <code class="c1"># Classifier</code>
    <code class="n">RandomForestClassifier</code><code class="p">(),</code>
    <code class="c1"># Feature matrix</code>
    <code class="n">features</code><code class="p">,</code>
    <code class="c1"># Target vector</code>
    <code class="n">target</code><code class="p">,</code>
    <code class="c1"># Hyperparameter to examine</code>
    <code class="n">param_name</code><code class="o">=</code><code class="s2">"n_estimators"</code><code class="p">,</code>
    <code class="c1"># Range of hyperparameter's values</code>
    <code class="n">param_range</code><code class="o">=</code><code class="n">param_range</code><code class="p">,</code>
    <code class="c1"># Number of folds</code>
    <code class="n">cv</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code>
    <code class="c1"># Performance metric</code>
    <code class="n">scoring</code><code class="o">=</code><code class="s2">"accuracy"</code><code class="p">,</code>
    <code class="c1"># Use all computer cores</code>
    <code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Calculate mean and standard deviation for training set scores</code>
<code class="n">train_mean</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">train_scores</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="n">train_std</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">std</code><code class="p">(</code><code class="n">train_scores</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Calculate mean and standard deviation for test set scores</code>
<code class="n">test_mean</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">test_scores</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="n">test_std</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">std</code><code class="p">(</code><code class="n">test_scores</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Plot mean accuracy scores for training and test sets</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">param_range</code><code class="p">,</code> <code class="n">train_mean</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"Training score"</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s2">"black"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">param_range</code><code class="p">,</code> <code class="n">test_mean</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"Cross-validation score"</code><code class="p">,</code>
         <code class="n">color</code><code class="o">=</code><code class="s2">"dimgrey"</code><code class="p">)</code>

<code class="c1"># Plot accuracy bands for training and test sets</code>
<code class="n">plt</code><code class="o">.</code><code class="n">fill_between</code><code class="p">(</code><code class="n">param_range</code><code class="p">,</code> <code class="n">train_mean</code> <code class="o">-</code> <code class="n">train_std</code><code class="p">,</code>
                 <code class="n">train_mean</code> <code class="o">+</code> <code class="n">train_std</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s2">"gray"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">fill_between</code><code class="p">(</code><code class="n">param_range</code><code class="p">,</code> <code class="n">test_mean</code> <code class="o">-</code> <code class="n">test_std</code><code class="p">,</code>
                 <code class="n">test_mean</code> <code class="o">+</code> <code class="n">test_std</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s2">"gainsboro"</code><code class="p">)</code>

<code class="c1"># Create plot</code>
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s2">"Validation Curve With Random Forest"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Number Of Trees"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Accuracy Score"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">tight_layout</code><code class="p">()</code>
<code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">(</code><code class="n">loc</code><code class="o">=</code><code class="s2">"best"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 11in04" height="438" src="assets/mpc2_11in04.png" width="600"/>
<h6/>
</div></figure>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id283">
<h2>Discussion</h2>
<p>Most <a data-primary="random forest classifier" data-type="indexterm" id="id1634"/>training algorithms (including many covered in this book) contain
hyperparameters that must be chosen before the training process begins.
For example, a <em>random forest classifier</em> creates a “forest” of decision
trees, each of which votes on the predicted class of an observation. One hyperparameter in random forest classifiers is the number of trees in the forest. Most often hyperparameter values are selected during model selection (see <a data-type="xref" href="ch12.xhtml#model-selection">Chapter 12</a>). However, it is occasionally
useful to visualize how model performance changes as the hyperparameter value changes. In our solution, we plot the changes in accuracy for a
random forest classifier for the training set and during cross-validation as the number of trees increases. When we have a small number of trees, both the training and cross-validation score are low, suggesting the model is underfitted. As the number of trees increases to 250, the accuracy of both levels off, suggesting there is probably not much value in the computational cost of training a massive forest.</p>
<p>In scikit-learn, we can calculate the validation curve using
<code>validation_curve</code>, which contains three important parameters:<a data-primary="" data-startref="ix_mod_eval_ch11" data-type="indexterm" id="id1635"/><a data-primary="" data-startref="ix_acc_metric2" data-type="indexterm" id="id1636"/><a data-primary="" data-startref="ix_predict_hyper_val" data-type="indexterm" id="id1637"/><a data-primary="" data-startref="ix_hyperparam_val_vis" data-type="indexterm" id="id1638"/><a data-primary="" data-startref="ix_mod_eval_hyper_val_vis" data-type="indexterm" id="id1639"/><a data-primary="" data-startref="ix_visual_hyper_val" data-type="indexterm" id="id1640"/></p>
<dl>
<dt><code>param_name</code></dt>
<dd>
<p>Name of the hyperparameter to vary</p>
</dd>
<dt><code>param_range</code></dt>
<dd>
<p>Value of the hyperparameter to use</p>
</dd>
<dt><code>scoring</code></dt>
<dd>
<p>Evaluation metric used to judge to model</p>
</dd>
</dl>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1641">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/FH_kH">scikit-learn documentation: Validation Curve</a></p>
</li>
</ul>
</div></section>
</div></section>
</div></section></div></body></html>