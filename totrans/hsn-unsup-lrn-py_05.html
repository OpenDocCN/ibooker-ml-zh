<html><head></head><body><section data-pdf-bookmark="Chapter 3. Dimensionality Reduction" data-type="chapter" epub:type="chapter"><div class="chapter" id="Chapter_3">&#13;
<h1><span class="label">Chapter 3. </span>Dimensionality Reduction</h1>&#13;
&#13;
&#13;
<p>In<a data-primary="curse of dimensionality" data-type="indexterm" id="idm140637553302272"/> this chapter, we will focus on one of the major challenges in building successful applied machine learning solutions: the curse of dimensionality. Unsupervised learning has a great counter—<em>dimensionality reduction</em>. In this chapter, we will introduce this concept and build from there so that you can develop an intuition for how it all works.</p>&#13;
&#13;
<p>In <a data-type="xref" href="ch04.html#Chapter_4">Chapter 4</a>, we will build our own unsupervised learning solution based on dimensionality reduction—specifically, an unsupervised learning-based credit card fraud detection system (as opposed to the supervised-based system we built in <a data-type="xref" href="ch02.html#Chapter_2">Chapter 2</a>). This type of unsupervised fraud detection is known as anomaly detection, a rapidly growing area in the field of applied unsupervised learning.</p>&#13;
&#13;
<p>But before we build an anomaly detection system, let’s cover dimensionality reduction in this chapter.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Motivation for Dimensionality Reduction" data-type="sect1"><div class="sect1" id="idm140637553297584">&#13;
<h1>The Motivation for Dimensionality Reduction</h1>&#13;
&#13;
<p>As<a data-primary="dimensionality reduction" data-secondary="motivation for" data-type="indexterm" id="idm140637553294560"/> mentioned in <a data-type="xref" href="ch01.html#Chapter_1">Chapter 1</a>, dimensionality reduction helps counteract one of the most commonly occurring problems in machine learning—the curse of dimensionality—in which algorithms cannot effectively and efficiently train on the data because of the sheer size of the feature space.</p>&#13;
&#13;
<p>Dimensionality reduction algorithms project high-dimensional data to a low-dimensional space, retaining as much of the salient information as possible while removing redundant information. Once the data is in the low-dimensional space, machine learning algorithms are able to identify interesting patterns more effectively and efficiently because a lot of the noise has been reduced.</p>&#13;
&#13;
<p>Sometimes, dimensionality reduction is the goal itself—for example, to build anomaly detection systems, as we will show in the next chapter.</p>&#13;
&#13;
<p>Other times, dimensionality reduction is not an end in itself but rather a means to another end. For example, dimensionality reduction is commonly a part of the machine learning pipeline to help solve large-scale, computationally expensive problems involving images, video, speech, and text.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The MNIST Digits Database" data-type="sect2"><div class="sect2" id="idm140637553290096">&#13;
<h2>The MNIST Digits Database</h2>&#13;
&#13;
<p>Before<a data-primary="MNIST digits database" data-type="indexterm" id="mnist03"/><a data-primary="dimensionality reduction" data-secondary="MNIST digits database" data-type="indexterm" id="DRmnist03"/> we introduce the dimensionality reduction algorithms, let’s explore the dataset that we will use in this chapter. We will work with a simple computer vision dataset: the MNIST (Mixed National Institute of Standards and Technology) database of handwritten digits, one of the best known datasets in machine learning. We will use the version of the MNIST dataset publicly available on Yann LeCun’s website.<sup><a data-type="noteref" href="ch03.html#idm140637553285552" id="idm140637553285552-marker">1</a></sup> To make it easier, we will use the pickled version, courtesy of <a href="http://deeplearning.net">deeplearning.net</a>.<sup><a data-type="noteref" href="ch03.html#idm140637553283488" id="idm140637553283488-marker">2</a></sup></p>&#13;
&#13;
<p>This dataset has been divided into three sets—a training set with 50,000 examples, a validation set with 10,000 examples, and a test set with 10,000 examples. We have labels for all the examples.</p>&#13;
&#13;
<p>This dataset consists of 28x28 pixel images of handwritten digits. Every single data point (i.e., every image) can be conveyed as an array of numbers, where each number describes how dark each pixel is. In other words, a 28x28 array of numbers corresponds to a 28x28 pixel image.</p>&#13;
&#13;
<p>To make this simpler, we can flatten each array into a 28x28, or 784, dimensional vector. Each component of the vector is a float between zero and one—representing the intensity of each pixel in the image. Zero stands for black; one stands for white. The labels are numbers between zero and nine, and indicate which digit the image represents.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Data acquisition and exploration" data-type="sect3"><div class="sect3" id="idm140637553279936">&#13;
<h3>Data acquisition and exploration</h3>&#13;
&#13;
<p>Before we work with the dimensionality reduction algorithms, let’s load the libraries we will use:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import libraries</code>&#13;
<code class="sd">'''Main'''</code>&#13;
<code class="kn">import</code> <code class="nn">numpy</code> <code class="kn">as</code> <code class="nn">np</code>&#13;
<code class="kn">import</code> <code class="nn">pandas</code> <code class="kn">as</code> <code class="nn">pd</code>&#13;
<code class="kn">import</code> <code class="nn">os</code><code class="o">,</code> <code class="nn">time</code>&#13;
<code class="kn">import</code> <code class="nn">pickle</code><code class="o">,</code> <code class="nn">gzip</code>&#13;
&#13;
<code class="sd">'''Data Viz'''</code>&#13;
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="kn">as</code> <code class="nn">plt</code>&#13;
<code class="kn">import</code> <code class="nn">seaborn</code> <code class="kn">as</code> <code class="nn">sns</code>&#13;
<code class="n">color</code> <code class="o">=</code> <code class="n">sns</code><code class="o">.</code><code class="n">color_palette</code><code class="p">()</code>&#13;
<code class="kn">import</code> <code class="nn">matplotlib</code> <code class="kn">as</code> <code class="nn">mpl</code>&#13;
&#13;
<code class="o">%</code><code class="n">matplotlib</code> <code class="n">inline</code>&#13;
&#13;
<code class="sd">'''Data Prep and Model Evaluation'''</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">preprocessing</code> <code class="k">as</code> <code class="n">pp</code>&#13;
<code class="kn">from</code> <code class="nn">scipy.stats</code> <code class="kn">import</code> <code class="n">pearsonr</code>&#13;
<code class="kn">from</code> <code class="nn">numpy.testing</code> <code class="kn">import</code> <code class="n">assert_array_almost_equal</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">StratifiedKFold</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">log_loss</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">precision_recall_curve</code><code class="p">,</code> <code class="n">average_precision_score</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">roc_curve</code><code class="p">,</code> <code class="n">auc</code><code class="p">,</code> <code class="n">roc_auc_score</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">confusion_matrix</code><code class="p">,</code> <code class="n">classification_report</code>&#13;
&#13;
<code class="sd">'''Algos'''</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LogisticRegression</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">RandomForestClassifier</code>&#13;
<code class="kn">import</code> <code class="nn">xgboost</code> <code class="kn">as</code> <code class="nn">xgb</code>&#13;
<code class="kn">import</code> <code class="nn">lightgbm</code> <code class="kn">as</code> <code class="nn">lgb</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Load the MNIST datasets" data-type="sect3"><div class="sect3" id="idm140637553276768">&#13;
<h3>Load the MNIST datasets</h3>&#13;
&#13;
<p>Let’s now load the MNIST datasets:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load the datasets</code>&#13;
<code class="n">current_path</code> <code class="o">=</code> <code class="n">os</code><code class="o">.</code><code class="n">getcwd</code><code class="p">()</code>&#13;
<code class="nb">file</code> <code class="o">=</code> <code class="s1">'</code><code class="se">\\</code><code class="s1">datasets</code><code class="se">\\</code><code class="s1">mnist_data</code><code class="se">\\</code><code class="s1">mnist.pkl.gz'</code>&#13;
&#13;
<code class="n">f</code> <code class="o">=</code> <code class="n">gzip</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">current_path</code><code class="o">+</code><code class="nb">file</code><code class="p">,</code> <code class="s1">'rb'</code><code class="p">)</code>&#13;
<code class="n">train_set</code><code class="p">,</code> <code class="n">validation_set</code><code class="p">,</code> <code class="n">test_set</code> <code class="o">=</code> <code class="n">pickle</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="n">f</code><code class="p">,</code> <code class="n">encoding</code><code class="o">=</code><code class="s1">'latin1'</code><code class="p">)</code>&#13;
<code class="n">f</code><code class="o">.</code><code class="n">close</code><code class="p">()</code>&#13;
&#13;
<code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code> <code class="o">=</code> <code class="n">train_set</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">train_set</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>&#13;
<code class="n">X_validation</code><code class="p">,</code> <code class="n">y_validation</code> <code class="o">=</code> <code class="n">validation_set</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">validation_set</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>&#13;
<code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">test_set</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">test_set</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Verify shape of datasets" data-type="sect3"><div class="sect3" id="idm140637552941344">&#13;
<h3>Verify shape of datasets</h3>&#13;
&#13;
<p>Let’s verify the shape of the datasets to make sure they loaded properly:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Verify shape of datasets</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s2">"Shape of X_train: "</code><code class="p">,</code> <code class="n">X_train</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s2">"Shape of y_train: "</code><code class="p">,</code> <code class="n">y_train</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s2">"Shape of X_validation: "</code><code class="p">,</code> <code class="n">X_validation</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s2">"Shape of y_validation: "</code><code class="p">,</code> <code class="n">y_validation</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s2">"Shape of X_test: "</code><code class="p">,</code> <code class="n">X_test</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s2">"Shape of y_test: "</code><code class="p">,</code> <code class="n">y_test</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code></pre>&#13;
&#13;
<p>The following code confirms the shapes of the datasets are as expected:</p>&#13;
&#13;
<pre data-type="programlisting">Shape of X_train:       (50000, 784)&#13;
Shape of y_train:       (50000,)&#13;
Shape of X_validation:  (10000, 784)&#13;
Shape of y_validation:  (10000,)&#13;
Shape of X_test:        (10000, 784)&#13;
Shape of y_test:        (10000,)</pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Create Pandas DataFrames from the datasets" data-type="sect3"><div class="sect3" id="idm140637553212688">&#13;
<h3>Create Pandas DataFrames from the datasets</h3>&#13;
&#13;
<p>Let’s convert the numpy arrays into Pandas DataFrames so they are easier to explore and work with:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create Pandas DataFrames from the datasets</code>&#13;
<code class="n">train_index</code> <code class="o">=</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code><code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">))</code>&#13;
<code class="n">validation_index</code> <code class="o">=</code> <code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">),</code> <code class="o">/</code>&#13;
                         <code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code><code class="o">+</code><code class="nb">len</code><code class="p">(</code><code class="n">X_validation</code><code class="p">))</code>&#13;
<code class="n">test_index</code> <code class="o">=</code> <code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code><code class="o">+</code><code class="nb">len</code><code class="p">(</code><code class="n">X_validation</code><code class="p">),</code> <code class="o">/</code>&#13;
                   <code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code><code class="o">+</code><code class="nb">len</code><code class="p">(</code><code class="n">X_validation</code><code class="p">)</code><code class="o">+</code><code class="nb">len</code><code class="p">(</code><code class="n">X_test</code><code class="p">))</code>&#13;
&#13;
<code class="n">X_train</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">train_index</code><code class="p">)</code>&#13;
<code class="n">y_train</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">y_train</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">train_index</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_validation</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_validation</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">validation_index</code><code class="p">)</code>&#13;
<code class="n">y_validation</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">y_validation</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">validation_index</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_test</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_test</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">test_index</code><code class="p">)</code>&#13;
<code class="n">y_test</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">y_test</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">test_index</code><code class="p">)</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Explore the data" data-type="sect3"><div class="sect3" id="idm140637553209952">&#13;
<h3>Explore the data</h3>&#13;
&#13;
<p>Let’s generate a summary view of the data:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Describe the training matrix</code>&#13;
<code class="n">X_train</code><code class="o">.</code><code class="n">describe</code><code class="p">()</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#data_exploration">Table 3-1</a> displays a summary view of the image data. Many of the values are zeros—in other words, most of the pixels in the images are black. This makes sense since the digits are in white and shown in the middle of the image on a black backdrop.</p>&#13;
<table class="pagebreak-before" id="data_exploration">&#13;
<caption><span class="label">Table 3-1. </span>Data exploration</caption>&#13;
<thead>&#13;
<tr>&#13;
<th/>&#13;
<th>0</th>&#13;
<th>1</th>&#13;
<th>2</th>&#13;
<th>3</th>&#13;
<th>4</th>&#13;
<th>5</th>&#13;
<th>6</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>count</p></td>&#13;
<td><p>50000.0</p></td>&#13;
<td><p>50000.0</p></td>&#13;
<td><p>50000.0</p></td>&#13;
<td><p>50000.0</p></td>&#13;
<td><p>50000.0</p></td>&#13;
<td><p>50000.0</p></td>&#13;
<td><p>50000.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>mean</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>std</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>min</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>25%</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>50%</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>75%</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>max</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
<td><p>0.0</p></td>&#13;
</tr>&#13;
</tbody>&#13;
<tfoot>&#13;
<tr>&#13;
<td colspan="8"><p>8 rows x 784 columns</p></td>&#13;
</tr>&#13;
</tfoot>&#13;
</table>&#13;
&#13;
<p>The labels data is a one-dimensional vector representing the actual content in the image. Labels for the first few images are as follows:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Show the labels</code>&#13;
<code class="n">y_train</code><code class="o">.</code><code class="n">head</code><code class="p">()</code></pre>&#13;
&#13;
<pre data-type="programlisting" id="labels_preview">  0   5&#13;
  1   0&#13;
  2   4&#13;
  3   1&#13;
  4   9&#13;
  dtype: int64</pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Display the images" data-type="sect3"><div class="sect3" id="idm140637552592560">&#13;
<h3>Display the images</h3>&#13;
&#13;
<p>Let’s define a function to view the image along with its label:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">view_digit</code><code class="p">(</code><code class="n">example</code><code class="p">):</code>&#13;
    <code class="n">label</code> <code class="o">=</code> <code class="n">y_train</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>&#13;
    <code class="n">image</code> <code class="o">=</code> <code class="n">X_train</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">example</code><code class="p">,:]</code><code class="o">.</code><code class="n">values</code><code class="o">.</code><code class="n">reshape</code><code class="p">([</code><code class="mi">28</code><code class="p">,</code><code class="mi">28</code><code class="p">])</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s1">'Example: </code><code class="si">%d</code><code class="s1">  Label: </code><code class="si">%d</code><code class="s1">'</code> <code class="o">%</code> <code class="p">(</code><code class="n">example</code><code class="p">,</code> <code class="n">label</code><code class="p">))</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="n">plt</code><code class="o">.</code><code class="n">get_cmap</code><code class="p">(</code><code class="s1">'gray'</code><code class="p">))</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>&#13;
&#13;
<p>A view of the first image—once the 784-dimensional vector is reshaped into a 28 x 28 pixel image—shows the number five (<a data-type="xref" href="#view_the_first_digit">Figure 3-1</a>).<a data-primary="" data-startref="DRmnist03" data-type="indexterm" id="idm140637552523968"/><a data-primary="" data-startref="mnist03" data-type="indexterm" id="idm140637552523056"/></p>&#13;
&#13;
<figure><div class="figure" id="view_the_first_digit">&#13;
<img alt="View the first digit" src="assets/hulp_0301.png"/>&#13;
<h6><span class="label">Figure 3-1. </span>View of the first digit</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Dimensionality Reduction Algorithms" data-type="sect1"><div class="sect1" id="idm140637553289472">&#13;
<h1>Dimensionality Reduction Algorithms</h1>&#13;
&#13;
<p>Now<a data-primary="dimensionality reduction" data-secondary="algorithms for" data-type="indexterm" id="idm140637552353328"/><a data-primary="algorithms" data-secondary="for dimensionality reduction" data-type="indexterm" id="idm140637552352352"/> that we’ve loaded and explored the MNIST digits dataset, let’s move to the dimensionality reduction algorithms. For each algorithm, we will introduce the concept first and then build a deeper understanding by applying the algorithm to the MNIST digits dataset.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Linear Projection vs. Manifold Learning" data-type="sect2"><div class="sect2" id="idm140637552350896">&#13;
<h2>Linear Projection vs. Manifold Learning</h2>&#13;
&#13;
<p>There<a data-primary="linear projection dimensionality reduction" data-type="indexterm" id="idm140637552349632"/> are two major branches of dimensionality reduction. The first is known as <em>linear projection</em>, which involves linearly projecting data from a high-dimensional space to a low-dimensional space. This includes techniques such as <em>principal component analysis, singular value decomposition</em>, and <em>random projection</em>.</p>&#13;
&#13;
<p>The<a data-primary="nonlinear dimensionality reduction" data-type="indexterm" id="idm140637552346704"/><a data-primary="manifold learning" data-type="indexterm" id="idm140637552346000"/> second is known as <em>manifold learning</em>, which is also referred to as <em>nonlinear dimensionality reduction</em>. This involves techniques such as <em>isomap</em>, which learns the <em>curved distance</em> (also called the <em>geodesic distance</em>) between points rather than the <em>Euclidean distance</em>. Other techniques include <em>multidimensional scaling (MDS), locally linear embedding (LLE), t-distributed stochastic neighbor embedding (t-SNE), dictionary learning, random trees embedding</em>, and <em>independent component analysis</em>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Principal Component Analysis" data-type="sect1"><div class="sect1" id="idm140637552341408">&#13;
<h1>Principal Component Analysis</h1>&#13;
&#13;
<p>We<a data-primary="dimensionality reduction" data-secondary="principal component analysis (PCA)" data-type="indexterm" id="DRpca03"/> will explore several versions of PCA, including standard PCA, incremental PCA, sparse PCA, and kernel PCA.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="PCA, the Concept" data-type="sect2"><div class="sect2" id="idm140637552338080">&#13;
<h2>PCA, the Concept</h2>&#13;
&#13;
<p>Let’s<a data-primary="principal component analysis (PCA)" data-secondary="concept of" data-type="indexterm" id="idm140637552336544"/> start with standard PCA, one of the most common linear dimensionality reduction techniques. In PCA, the algorithm finds a low-dimensional representation of the data while retaining as much of the variation (i.e., salient information) as possible.</p>&#13;
&#13;
<p>PCA does this by addressing the correlation among features. If the correlation is very high among a subset of the features, PCA will attempt to combine the highly correlated features and represent this data with a smaller number of linearly uncorrelated features. The algorithm keeps performing this correlation reduction, finding the directions of maximum variance in the original high-dimensional data and projecting them onto a smaller dimensional space. These newly derived components are known as principal components.</p>&#13;
&#13;
<p>With these components, it is possible to reconstruct the original features—not exactly but generally close enough. The PCA algorithm actively attempts to minimize the reconstruction error during its search for the optimal components.</p>&#13;
&#13;
<p>In our MNIST example, the original feature space has 784 dimensions, known as <em>d</em> dimensions. PCA will project the data onto a smaller subspace of <em>k</em> dimensions (where <em>k &lt; d</em>) while retaining as much of the salient information as possible. These <em>k</em> dimensions are known as the principal components.</p>&#13;
&#13;
<p>The number of meaningful principal components we are left with is considerably smaller than the number of dimensions in the original dataset. We lose some of the variance (i.e., information) by moving to this low-dimensional space, but the underlying structure of the data is easier to identify, allowing us to perform tasks like anomaly detection and clustering more effectively and efficiently.</p>&#13;
&#13;
<p>Moreover, by reducing the dimensionality of the data, PCA will reduce the size of the data, improving the performance of machine learning algorithms further along in the machine learning pipeline (for example, for tasks such as image classification).</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>It<a data-primary="feature scaling" data-type="indexterm" id="idm140637552328896"/> is essential to perform feature scaling before running PCA. PCA is very sensitive to the relative ranges of the original features. Generally we must scale the data to make sure the features are in the same relative range. However, for our MNIST digits dataset, the features are already scaled to a range of zero to one, so we can skip this step.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="PCA in Practice" data-type="sect2"><div class="sect2" id="idm140637552327408">&#13;
<h2>PCA in Practice</h2>&#13;
&#13;
<p>Now<a data-primary="principal component analysis (PCA)" data-secondary="in practice" data-type="indexterm" id="idm140637552326032"/> that you have a better grasp of how PCA works, let’s apply PCA to the MNIST digits dataset and see how well PCA captures the most salient information about the digits as its projects the data from the original 784-dimensional space to a lower dimensional space.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before" data-pdf-bookmark="Set the hyperparameters" data-type="sect3"><div class="sect3" id="idm140637552324464">&#13;
<h3>Set the hyperparameters</h3>&#13;
&#13;
<p>Let’s set the hyperparameters for the PCA algorithm:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">PCA</code>&#13;
&#13;
<code class="n">n_components</code> <code class="o">=</code> <code class="mi">784</code>&#13;
<code class="n">whiten</code> <code class="o">=</code> <code class="bp">False</code>&#13;
<code class="n">random_state</code> <code class="o">=</code> <code class="mi">2018</code>&#13;
&#13;
<code class="n">pca</code> <code class="o">=</code> <code class="n">PCA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="n">n_components</code><code class="p">,</code> <code class="n">whiten</code><code class="o">=</code><code class="n">whiten</code><code class="p">,</code> \&#13;
          <code class="n">random_state</code><code class="o">=</code><code class="n">random_state</code><code class="p">)</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Apply PCA" data-type="sect3"><div class="sect3" id="idm140637552321184">&#13;
<h3>Apply PCA</h3>&#13;
&#13;
<p>We will set the number of principal components to the original number of dimensions (i.e., 784). Then, PCA will capture the salient information from the original dimensions and start generating principal components. Once these components are generated, we will determine how many principal components we need to effectively capture most of the variance/information from the original feature set.</p>&#13;
&#13;
<p>Let’s fit and transform our training data, generating these principal components:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">X_train_PCA</code> <code class="o">=</code> <code class="n">pca</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>&#13;
<code class="n">X_train_PCA</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train_PCA</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">train_index</code><code class="p">)</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Evaluate PCA" data-type="sect3"><div class="sect3" id="idm140637552235168">&#13;
<h3>Evaluate PCA</h3>&#13;
&#13;
<p>Because we have not reduced the dimensionality at all (we’ve just transformed the data) the variance/information of the original data captured by the 784 principal components should be 100%:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Percentage of Variance Captured by 784 principal components</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s2">"Variance Explained by all 784 principal components: "</code><code class="p">,</code> \&#13;
      <code class="nb">sum</code><code class="p">(</code><code class="n">pca</code><code class="o">.</code><code class="n">explained_variance_ratio_</code><code class="p">))</code></pre>&#13;
&#13;
<pre data-type="programlisting">Variance Explained by all 784 principal components: 0.9999999999999997</pre>&#13;
&#13;
<p>However, it is important to note that the importance of the 784 principal components varies quite a bit. The importance of the first X principal components are summarized here:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Percentage of Variance Captured by X principal components</code>&#13;
<code class="n">importanceOfPrincipalComponents</code> <code class="o">=</code> \&#13;
    <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">pca</code><code class="o">.</code><code class="n">explained_variance_ratio_</code><code class="p">)</code>&#13;
<code class="n">importanceOfPrincipalComponents</code> <code class="o">=</code> <code class="n">importanceOfPrincipalComponents</code><code class="o">.</code><code class="n">T</code>&#13;
&#13;
<code class="k">print</code><code class="p">(</code><code class="s1">'Variance Captured by First 10 Principal Components: '</code><code class="p">,</code>&#13;
      <code class="n">importanceOfPrincipalComponents</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code><code class="mi">0</code><code class="p">:</code><code class="mi">9</code><code class="p">]</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">values</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s1">'Variance Captured by First 20 Principal Components: '</code><code class="p">,</code>&#13;
      <code class="n">importanceOfPrincipalComponents</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code><code class="mi">0</code><code class="p">:</code><code class="mi">19</code><code class="p">]</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">values</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s1">'Variance Captured by First 50 Principal Components: '</code><code class="p">,</code>&#13;
      <code class="n">importanceOfPrincipalComponents</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code><code class="mi">0</code><code class="p">:</code><code class="mi">49</code><code class="p">]</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">values</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s1">'Variance Captured by First 100 Principal Components: '</code><code class="p">,</code>&#13;
      <code class="n">importanceOfPrincipalComponents</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code><code class="mi">0</code><code class="p">:</code><code class="mi">99</code><code class="p">]</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">values</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s1">'Variance Captured by First 200 Principal Components: '</code><code class="p">,</code>&#13;
      <code class="n">importanceOfPrincipalComponents</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code><code class="mi">0</code><code class="p">:</code><code class="mi">199</code><code class="p">]</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">values</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s1">'Variance Captured by First 300 Principal Components: '</code><code class="p">,</code>&#13;
      <code class="n">importanceOfPrincipalComponents</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code><code class="mi">0</code><code class="p">:</code><code class="mi">299</code><code class="p">]</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">values</code><code class="p">)</code></pre>&#13;
&#13;
<pre data-type="programlisting">Variance Captured by First 10 Principal Components: [0.48876238]&#13;
Variance Captured by First 20 Principal Components: [0.64398025]&#13;
Variance Captured by First 50 Principal Components: [0.8248609]&#13;
Variance Captured by First 100 Principal Components: [0.91465857]&#13;
Variance Captured by First 200 Principal Components: [0.96650076]&#13;
Variance Captured by First 300 Principal Components: [0.9862489]</pre>&#13;
&#13;
<p>The first 10 components in total capture approximately 50% of the variance, the first one hundred components over 90%, and the first three hundred components almost 99% of the variance; the information in the rest of the principal components is of negligible value.</p>&#13;
&#13;
<p>We can also plot the importance of each principal component, ranked from the first principal component to the last. For the sake of readability, just the first 10 components are displayed in <a data-type="xref" href="#importance_of_pca_components">Figure 3-2</a>.</p>&#13;
&#13;
<p>The power of PCA should be more apparent now. With just the first two hundred principal components (far fewer than the original 784 dimensions), we capture over 96% of the variance/information.</p>&#13;
&#13;
<p>PCA allows us to reduce the dimensionality of the original data substantially while retaining most of the salient information. On the PCA-reduced feature set, other machine learning algorithms—downstream in the machine learning pipeline—will have an easier time separating the data points in space (to perform tasks such as anomaly detection and clustering) and will require fewer computational resources.</p>&#13;
&#13;
<figure class="width-75"><div class="figure" id="importance_of_pca_components">&#13;
<img alt="Importance of PCA components" src="assets/hulp_0302.png"/>&#13;
<h6><span class="label">Figure 3-2. </span>Importance of PCA components</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Visualize the separation of points in space" data-type="sect3"><div class="sect3" id="idm140637552220608">&#13;
<h3>Visualize the separation of points in space</h3>&#13;
&#13;
<p>To demonstrate the power of PCA to efficiently and compactly capture the variance/information in data, let’s plot the observations in two dimensions. Specifically, we will display a scatterplot of the first and second principal components and mark the observations by the true label. Let’s create a function for this called <code>scatterPlot</code> because we also need to present visualizations for the other dimensionality algorithms later on:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">scatterPlot</code><code class="p">(</code><code class="n">xDF</code><code class="p">,</code> <code class="n">yDF</code><code class="p">,</code> <code class="n">algoName</code><code class="p">):</code>&#13;
    <code class="n">tempDF</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">xDF</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code><code class="mi">0</code><code class="p">:</code><code class="mi">1</code><code class="p">],</code> <code class="n">index</code><code class="o">=</code><code class="n">xDF</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>&#13;
    <code class="n">tempDF</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">((</code><code class="n">tempDF</code><code class="p">,</code><code class="n">yDF</code><code class="p">),</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">join</code><code class="o">=</code><code class="s2">"inner"</code><code class="p">)</code>&#13;
    <code class="n">tempDF</code><code class="o">.</code><code class="n">columns</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"First Vector"</code><code class="p">,</code> <code class="s2">"Second Vector"</code><code class="p">,</code> <code class="s2">"Label"</code><code class="p">]</code>&#13;
    <code class="n">sns</code><code class="o">.</code><code class="n">lmplot</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="s2">"First Vector"</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s2">"Second Vector"</code><code class="p">,</code> <code class="n">hue</code><code class="o">=</code><code class="s2">"Label"</code><code class="p">,</code> \&#13;
               <code class="n">data</code><code class="o">=</code><code class="n">tempDF</code><code class="p">,</code> <code class="n">fit_reg</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code>&#13;
    <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">gca</code><code class="p">()</code>&#13;
    <code class="n">ax</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="s2">"Separation of Observations using "</code><code class="o">+</code><code class="n">algoName</code><code class="p">)</code>&#13;
&#13;
<code class="n">scatterPlot</code><code class="p">(</code><code class="n">X_train_PCA</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="s2">"PCA"</code><code class="p">)</code></pre>&#13;
&#13;
<p>As seen in <a data-type="xref" href="#separation_of_observations_using_pca">Figure 3-3</a>, with just the top two principal components, PCA does a good job of separating the points in space such that similar points are generally closer to each other than they are to other, less similar points. In other words, images of the same digit are closer to each other than they are to images of other digits.</p>&#13;
&#13;
<p>PCA accomplishes this without using any labels whatsoever. This demonstrates the power of unsupervised learning to capture the underlying structure of data, helping discover hidden patterns in the absence of labels.</p>&#13;
&#13;
<figure><div class="figure" id="separation_of_observations_using_pca">&#13;
<img alt="Separation of Observations Using PCA" src="assets/hulp_0303.png"/>&#13;
<h6><span class="label">Figure 3-3. </span>Separation of observations using PCA</h6>&#13;
</div></figure>&#13;
&#13;
<p>If we run the same two-dimensional scatterplot using two of the most important features from the original 784 feature set—determined by training a supervised learning model—the separation is poor, at best (<a data-type="xref" href="#separation_of_observations_without_pca">Figure 3-4</a>).</p>&#13;
&#13;
<figure><div class="figure" id="separation_of_observations_without_pca">&#13;
<img alt="Separation of Observations Without PCA" src="assets/hulp_0304.png"/>&#13;
<h6><span class="label">Figure 3-4. </span>Separation of observations without PCA</h6>&#13;
</div></figure>&#13;
&#13;
<p>Comparison of Figures <a data-type="xref" data-xrefstyle="select:labelnumber" href="#separation_of_observations_using_pca">3-3</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#separation_of_observations_without_pca">3-4</a> shows just how powerful PCA is in learning the underlying structure of the dataset without using any labels whatsoever—even with just two dimensions, we can start meaningfully separating the images by the digits they display.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Not<a data-primary="principal component analysis (PCA)" data-secondary="benefits of" data-type="indexterm" id="idm140637551725792"/> only does PCA help separate data so that we can discover hidden patterns more readily, it also helps reduce the size of the feature set, making it less costly—both in time and in computational resources—to train machine learning models.</p>&#13;
&#13;
<p>With the MNIST dataset, the reduction in training time will be modest at best since the dataset is very small—we have only 784 features and 50,000 observations. But if the dataset were millions of features and billions of observations, dimensionality reduction would dramatically reduce the training time of the machine learning algorithms further along in the machine learning pipeline.</p>&#13;
&#13;
<p>Lastly, PCA usually throws away some of the information available in the original feature set but does so wisely, capturing the most important elements and tossing the less valuable ones. A model that is trained on a PCA-reduced feature set may not perform quite as well in terms of accuracy as a model that is trained on the full feature set, but both the training and prediction times will be much faster. This is one of the important trade-offs you must consider when choosing whether to use dimensionality reduction in your machine learning product.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Incremental PCA" data-type="sect2"><div class="sect2" id="idm140637552327104">&#13;
<h2>Incremental PCA</h2>&#13;
&#13;
<p>For<a data-primary="principal component analysis (PCA)" data-secondary="incremental PCA" data-type="indexterm" id="idm140637551721376"/><a data-primary="incremental PCA" data-type="indexterm" id="idm140637551720400"/> datasets that are very large and cannot fit in memory, we can perform PCA incrementally in small batches, where each batch is able to fit in memory. The batch size can be either set manually or determined automatically. This batch-based form of PCA is known as <em>incremental PCA</em>. The resulting principal components of PCA and incremental PCA are generally pretty similar (<a data-type="xref" href="#separation_of_observations_using_incremental_pca">Figure 3-5</a>). Here is the code for incremental PCA:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Incremental PCA</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">IncrementalPCA</code>&#13;
&#13;
<code class="n">n_components</code> <code class="o">=</code> <code class="mi">784</code>&#13;
<code class="n">batch_size</code> <code class="o">=</code> <code class="bp">None</code>&#13;
&#13;
<code class="n">incrementalPCA</code> <code class="o">=</code> <code class="n">IncrementalPCA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="n">n_components</code><code class="p">,</code> \&#13;
                                <code class="n">batch_size</code><code class="o">=</code><code class="n">batch_size</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_train_incrementalPCA</code> <code class="o">=</code> <code class="n">incrementalPCA</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>&#13;
<code class="n">X_train_incrementalPCA</code> <code class="o">=</code> \&#13;
    <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train_incrementalPCA</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">train_index</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_validation_incrementalPCA</code> <code class="o">=</code> <code class="n">incrementalPCA</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_validation</code><code class="p">)</code>&#13;
<code class="n">X_validation_incrementalPCA</code> <code class="o">=</code> \&#13;
    <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_validation_incrementalPCA</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">validation_index</code><code class="p">)</code>&#13;
&#13;
<code class="n">scatterPlot</code><code class="p">(</code><code class="n">X_train_incrementalPCA</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="s2">"Incremental PCA"</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="separation_of_observations_using_incremental_pca">&#13;
<img alt="Separation of Observations Using Incremental PCA" src="assets/hulp_0305.png"/>&#13;
<h6><span class="label">Figure 3-5. </span>Separation of observations using incremental PCA</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Sparse PCA" data-type="sect2"><div class="sect2" id="idm140637551910192">&#13;
<h2>Sparse PCA</h2>&#13;
&#13;
<p>The<a data-primary="alpha hyperparameter" data-type="indexterm" id="idm140637551908656"/><a data-primary="principal component analysis (PCA)" data-secondary="sparse PCA" data-type="indexterm" id="idm140637551907920"/><a data-primary="sparse PCA" data-type="indexterm" id="idm140637551907008"/> normal PCA algorithm searches for linear combinations in all the input variables, reducing the original feature space as densely as possible. But for some machine learning problems, some degree of sparsity may be preferred. A version of PCA that retains some degree of sparsity—controlled by a hyperparameter called <em>alpha</em>—is known as <em>sparse PCA</em>. The sparse PCA algorithm searches for linear combinations in just some of the input variables, reducing the original feature space to some degree but not as compactly as normal PCA.</p>&#13;
&#13;
<p>Because this algorithm trains a bit more slowly than normal PCA, we will train on just the first 10,000 examples in our training set (out of the total 50,000 examples). We will continue this practice of training on fewer than the total number of observations when the algorithm training times are slow.</p>&#13;
&#13;
<p>For our purposes (i.e., developing some intuition of how these dimensionality reduction algorithms work), the reduced training process is fine. For a better solution, training on the complete training set is advised:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Sparse PCA</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">SparsePCA</code>&#13;
&#13;
<code class="n">n_components</code> <code class="o">=</code> <code class="mi">100</code>&#13;
<code class="n">alpha</code> <code class="o">=</code> <code class="mf">0.0001</code>&#13;
<code class="n">random_state</code> <code class="o">=</code> <code class="mi">2018</code>&#13;
<code class="n">n_jobs</code> <code class="o">=</code> <code class="o">-</code><code class="mi">1</code>&#13;
&#13;
<code class="n">sparsePCA</code> <code class="o">=</code> <code class="n">SparsePCA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="n">n_components</code><code class="p">,</code> \&#13;
                <code class="n">alpha</code><code class="o">=</code><code class="n">alpha</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="n">random_state</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=</code><code class="n">n_jobs</code><code class="p">)</code>&#13;
&#13;
<code class="n">sparsePCA</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="o">.</code><code class="n">loc</code><code class="p">[:</code><code class="mi">10000</code><code class="p">,:])</code>&#13;
<code class="n">X_train_sparsePCA</code> <code class="o">=</code> <code class="n">sparsePCA</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>&#13;
<code class="n">X_train_sparsePCA</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train_sparsePCA</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">train_index</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_validation_sparsePCA</code> <code class="o">=</code> <code class="n">sparsePCA</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_validation</code><code class="p">)</code>&#13;
<code class="n">X_validation_sparsePCA</code> <code class="o">=</code> \&#13;
    <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_validation_sparsePCA</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">validation_index</code><code class="p">)</code>&#13;
&#13;
<code class="n">scatterPlot</code><code class="p">(</code><code class="n">X_train_sparsePCA</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="s2">"Sparse PCA"</code><code class="p">)</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#separation_of_observations_using_sparse_pca">Figure 3-6</a> shows a two-dimensional scatterplot using the first two principal components using sparse PCA.</p>&#13;
&#13;
<figure><div class="figure" id="separation_of_observations_using_sparse_pca">&#13;
<img alt="Separation of Observations Using Sparse PCA" src="assets/hulp_0306.png"/>&#13;
<h6><span class="label">Figure 3-6. </span>Separation of observations using sparse PCA</h6>&#13;
</div></figure>&#13;
&#13;
<p>Notice that this scatterplot looks different from that of the normal PCA, as expected. Normal and sparse PCA generate principal components differently, and the separation of points is somewhat different, too.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Kernel PCA" data-type="sect2"><div class="sect2" id="idm140637551592880">&#13;
<h2>Kernel PCA</h2>&#13;
&#13;
<p>Normal<a data-primary="principal component analysis (PCA)" data-secondary="kernel PCA" data-type="indexterm" id="idm140637551591312"/><a data-primary="kernel PCA" data-type="indexterm" id="idm140637551590336"/> PCA, incremental PCA, and sparse PCA linearly project the original data onto a lower dimensional space, but there is also a nonlinear form of PCA known as <em>kernel PCA</em>, which runs a similarity function over pairs of original data points in order to perform nonlinear dimensionality reduction.</p>&#13;
&#13;
<p>By learning this similarity function (known as the <em>kernel method</em>), kernel PCA maps the implicit feature space where the majority of data points lie and creates this implicit feature space in a much smaller number of dimensions than the dimensions in the original feature set. This method is especially effective when the original feature set is not linearly separable.</p>&#13;
&#13;
<p>For<a data-primary="gamma coefficient" data-type="indexterm" id="idm140637551538384"/> the kernel PCA algorithm, we need to set the number of components we desire, the type of kernel, and the kernel coefficient, which is known as the <em>gamma</em>. The<a data-primary="radial basis function (RBF) kernel" data-type="indexterm" id="idm140637551537088"/> most popular kernel is the <em>radial basis function kernel</em>, more commonly referred to as the <em>RBF kernel</em>. This is what we will use here:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Kernel PCA</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">KernelPCA</code>&#13;
&#13;
<code class="n">n_components</code> <code class="o">=</code> <code class="mi">100</code>&#13;
<code class="n">kernel</code> <code class="o">=</code> <code class="s1">'rbf'</code>&#13;
<code class="n">gamma</code> <code class="o">=</code> <code class="bp">None</code>&#13;
<code class="n">random_state</code> <code class="o">=</code> <code class="mi">2018</code>&#13;
<code class="n">n_jobs</code> <code class="o">=</code> <code class="mi">1</code>&#13;
&#13;
<code class="n">kernelPCA</code> <code class="o">=</code> <code class="n">KernelPCA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="n">n_components</code><code class="p">,</code> <code class="n">kernel</code><code class="o">=</code><code class="n">kernel</code><code class="p">,</code> \&#13;
                      <code class="n">gamma</code><code class="o">=</code><code class="n">gamma</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=</code><code class="n">n_jobs</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="n">random_state</code><code class="p">)</code>&#13;
&#13;
<code class="n">kernelPCA</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="o">.</code><code class="n">loc</code><code class="p">[:</code><code class="mi">10000</code><code class="p">,:])</code>&#13;
<code class="n">X_train_kernelPCA</code> <code class="o">=</code> <code class="n">kernelPCA</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>&#13;
<code class="n">X_train_kernelPCA</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train_kernelPCA</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">train_index</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_validation_kernelPCA</code> <code class="o">=</code> <code class="n">kernelPCA</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_validation</code><code class="p">)</code>&#13;
<code class="n">X_validation_kernelPCA</code> <code class="o">=</code> \&#13;
    <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_validation_kernelPCA</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">validation_index</code><code class="p">)</code>&#13;
&#13;
<code class="n">scatterPlot</code><code class="p">(</code><code class="n">X_train_kernelPCA</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="s2">"Kernel PCA"</code><code class="p">)</code></pre>&#13;
&#13;
<p>The two-dimensional scatterplot of the kernel PCA is nearly identical to the one of the linear PCA for our MNIST digits dataset (<a data-type="xref" href="#separation_of_observations_using_kernel_pca">Figure 3-7</a>). Learning the RBF kernel does not improve the dimensionality reduction.<a data-primary="" data-startref="DRpca03" data-type="indexterm" id="idm140637551533216"/></p>&#13;
&#13;
<figure><div class="figure" id="separation_of_observations_using_kernel_pca">&#13;
<img alt="Separation of Observations Using Kernel PCA" src="assets/hulp_0307.png"/>&#13;
<h6><span class="label">Figure 3-7. </span>Separation of observations using kernel PCA</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Singular Value Decomposition" data-type="sect1"><div class="sect1" id="idm140637551348720">&#13;
<h1>Singular Value Decomposition</h1>&#13;
&#13;
<p>Another<a data-primary="dimensionality reduction" data-secondary="singular value decomposition (SVD)" data-type="indexterm" id="idm140637551347184"/><a data-primary="singular value decomposition (SVD)" data-type="indexterm" id="idm140637551346240"/> approach to learning the underlying structure of the data is to reduce the rank of the original matrix of features to a smaller rank such that the original matrix can be recreated using a linear combination of some of the vectors in the smaller rank matrix. This is known as <em>singular value decomposition (SVD)</em>.</p>&#13;
&#13;
<p>To generate the smaller rank matrix, SVD keeps the vectors of the original matrix that have the most information (i.e., the highest singular value). The smaller rank matrix captures the most important elements of the original feature space.</p>&#13;
&#13;
<p>This is very similar to PCA. PCA, which uses the eigen-decomposition of the covariance matrix to perform dimensionality reduction. SVD uses singular value decomposition, as its name implies. In fact, PCA involves the use of SVD in its calculation, but much of this discussion is beyond the scope of this book.</p>&#13;
&#13;
<p>Here is how SVD works:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Singular Value Decomposition</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">TruncatedSVD</code>&#13;
&#13;
<code class="n">n_components</code> <code class="o">=</code> <code class="mi">200</code>&#13;
<code class="n">algorithm</code> <code class="o">=</code> <code class="s1">'randomized'</code>&#13;
<code class="n">n_iter</code> <code class="o">=</code> <code class="mi">5</code>&#13;
<code class="n">random_state</code> <code class="o">=</code> <code class="mi">2018</code>&#13;
&#13;
<code class="n">svd</code> <code class="o">=</code> <code class="n">TruncatedSVD</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="n">n_components</code><code class="p">,</code> <code class="n">algorithm</code><code class="o">=</code><code class="n">algorithm</code><code class="p">,</code> \&#13;
                   <code class="n">n_iter</code><code class="o">=</code><code class="n">n_iter</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="n">random_state</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_train_svd</code> <code class="o">=</code> <code class="n">svd</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>&#13;
<code class="n">X_train_svd</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train_svd</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">train_index</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_validation_svd</code> <code class="o">=</code> <code class="n">svd</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_validation</code><code class="p">)</code>&#13;
<code class="n">X_validation_svd</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_validation_svd</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">validation_index</code><code class="p">)</code>&#13;
&#13;
<code class="n">scatterPlot</code><code class="p">(</code><code class="n">X_train_svd</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="s2">"Singular Value Decomposition"</code><code class="p">)</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#separation_of_observations_using_svd">Figure 3-8</a> displays the separation of points that we achieve using the two most important vectors from SVD.</p>&#13;
&#13;
<figure><div class="figure" id="separation_of_observations_using_svd">&#13;
<img alt="Separation of Observations Using SVD" src="assets/hulp_0308.png"/>&#13;
<h6><span class="label">Figure 3-8. </span>Separation of observations using SVD</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Random Projection" data-type="sect1"><div class="sect1" id="idm140637551412048">&#13;
<h1>Random Projection</h1>&#13;
&#13;
<p>Another<a data-primary="dimensionality reduction" data-secondary="random projection" data-type="indexterm" id="idm140637551410720"/><a data-primary="random projection dimensionality reduction" data-type="indexterm" id="idm140637551409696"/><a data-primary="Johnson-Lindenstrauss lemma" data-type="indexterm" id="idm140637551409056"/> linear dimensionality reduction technique is random projection, which relies on the <em>Johnson–Lindenstrauss lemma</em>. According to the Johnson–Lindenstrauss lemma, points in a high-dimensional space can be embedded into a much lower-dimensional space so that distances between the points are nearly preserved. In other words, even as we move from high-dimensional space to low-dimensional space, the relevant structure of the original feature set is preserved.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Gaussian Random Projection" data-type="sect2"><div class="sect2" id="idm140637551407360">&#13;
<h2>Gaussian Random Projection</h2>&#13;
&#13;
<p>There are two versions of random projection—the standard version known as <em>Gaussian random projection</em> and a sparse version known as <em>sparse random projection</em>.</p>&#13;
&#13;
<p>For<a data-primary="Gaussian random projection" data-type="indexterm" id="idm140637551404448"/> Gaussian random projection, we can either specify the number of components we would like to have in the reduced feature space, or we can set the hyperparameter <em>eps</em>. The eps controls the quality of the embedding according to the Johnson–Lindenstrauss lemma, where smaller values generate a higher number of dimensions. In our case, we will set this hyperparameter:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Gaussian Random Projection</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.random_projection</code> <code class="kn">import</code> <code class="n">GaussianRandomProjection</code>&#13;
&#13;
<code class="n">n_components</code> <code class="o">=</code> <code class="s1">'auto'</code>&#13;
<code class="n">eps</code> <code class="o">=</code> <code class="mf">0.5</code>&#13;
<code class="n">random_state</code> <code class="o">=</code> <code class="mi">2018</code>&#13;
&#13;
<code class="n">GRP</code> <code class="o">=</code> <code class="n">GaussianRandomProjection</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="n">n_components</code><code class="p">,</code> <code class="n">eps</code><code class="o">=</code><code class="n">eps</code><code class="p">,</code> \&#13;
                               <code class="n">random_state</code><code class="o">=</code><code class="n">random_state</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_train_GRP</code> <code class="o">=</code> <code class="n">GRP</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>&#13;
<code class="n">X_train_GRP</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train_GRP</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">train_index</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_validation_GRP</code> <code class="o">=</code> <code class="n">GRP</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_validation</code><code class="p">)</code>&#13;
<code class="n">X_validation_GRP</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_validation_GRP</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">validation_index</code><code class="p">)</code>&#13;
&#13;
<code class="n">scatterPlot</code><code class="p">(</code><code class="n">X_train_GRP</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="s2">"Gaussian Random Projection"</code><code class="p">)</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#separation_of_observations_using_gaussian_random_projection">Figure 3-9</a> shows the two-dimensional scatterplot using Gaussian random projection.</p>&#13;
&#13;
<figure><div class="figure" id="separation_of_observations_using_gaussian_random_projection">&#13;
<img alt="Separation of Observations Using Gaussian Random Projection" src="assets/hulp_0309.png"/>&#13;
<h6><span class="label">Figure 3-9. </span>Separation of observations using Gaussian random projection</h6>&#13;
</div></figure>&#13;
&#13;
<p>Although it is a form of linear projection like PCA, random projection is an entirely different family of dimensionality reduction. Thus the random projection scatterplot looks very different from the scatterplots of normal PCA, incremental PCA, sparse PCA, and kernel PCA.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Sparse Random Projection" data-type="sect2"><div class="sect2" id="idm140637551057056">&#13;
<h2>Sparse Random Projection</h2>&#13;
&#13;
<p>Just<a data-primary="sparse random projection" data-type="indexterm" id="idm140637551055520"/> as there is a sparse version of PCA, there is a sparse version of random projection known as sparse random projection. It retains some degree of sparsity in the transformed feature set and is generally much more efficient, transforming the original data into the reduced space much faster than normal Gaussian random projection:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Sparse Random Projection</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.random_projection</code> <code class="kn">import</code> <code class="n">SparseRandomProjection</code>&#13;
&#13;
<code class="n">n_components</code> <code class="o">=</code> <code class="s1">'auto'</code>&#13;
<code class="n">density</code> <code class="o">=</code> <code class="s1">'auto'</code>&#13;
<code class="n">eps</code> <code class="o">=</code> <code class="mf">0.5</code>&#13;
<code class="n">dense_output</code> <code class="o">=</code> <code class="bp">False</code>&#13;
<code class="n">random_state</code> <code class="o">=</code> <code class="mi">2018</code>&#13;
&#13;
<code class="n">SRP</code> <code class="o">=</code> <code class="n">SparseRandomProjection</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="n">n_components</code><code class="p">,</code> \&#13;
        <code class="n">density</code><code class="o">=</code><code class="n">density</code><code class="p">,</code> <code class="n">eps</code><code class="o">=</code><code class="n">eps</code><code class="p">,</code> <code class="n">dense_output</code><code class="o">=</code><code class="n">dense_output</code><code class="p">,</code> \&#13;
        <code class="n">random_state</code><code class="o">=</code><code class="n">random_state</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_train_SRP</code> <code class="o">=</code> <code class="n">SRP</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>&#13;
<code class="n">X_train_SRP</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train_SRP</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">train_index</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_validation_SRP</code> <code class="o">=</code> <code class="n">SRP</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_validation</code><code class="p">)</code>&#13;
<code class="n">X_validation_SRP</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_validation_SRP</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">validation_index</code><code class="p">)</code>&#13;
&#13;
<code class="n">scatterPlot</code><code class="p">(</code><code class="n">X_train_SRP</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="s2">"Sparse Random Projection"</code><code class="p">)</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#separation_of_observations_using_sparse_random_projection">Figure 3-10</a> shows the two-dimensional scatterplot using sparse random projection.</p>&#13;
&#13;
<figure><div class="figure" id="separation_of_observations_using_sparse_random_projection">&#13;
<img alt="Separation of Observations Using Sparse Random Projection" src="assets/hulp_0310.png"/>&#13;
<h6><span class="label">Figure 3-10. </span>Separation of observations using sparse random projection</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Isomap" data-type="sect1"><div class="sect1" id="idm140637551185040">&#13;
<h1>Isomap</h1>&#13;
&#13;
<p>Instead<a data-primary="dimensionality reduction" data-secondary="isometric mapping (Isomap)" data-type="indexterm" id="idm140637551183472"/><a data-primary="isometric mapping (Isomap)" data-type="indexterm" id="idm140637551182480"/> of linearly projecting the data from a high-dimensional space to a low-dimensional space, we can use nonlinear dimensionality reduction methods. These methods are collectively known as manifold learning.</p>&#13;
&#13;
<p>The most vanilla form of manifold learning is known as <em>isometric mapping</em>, or <em>Isomap</em> for short. Like kernel PCA, Isomap learns a new, low-dimensional embedding of the original feature set by calculating the pairwise distances of all the points, where<a data-primary="geodesic distance" data-type="indexterm" id="idm140637551179792"/><a data-primary="Euclidean distance" data-type="indexterm" id="idm140637551179088"/> distance is <em>curved</em> or <em>geodesic distance</em> rather than <em>Euclidean distance</em>. In other words, it learns the intrinsic geometry of the original data based on where each point lies relative to its neighbors on a manifold:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Isomap</code>&#13;
&#13;
<code class="kn">from</code> <code class="nn">sklearn.manifold</code> <code class="kn">import</code> <code class="n">Isomap</code>&#13;
&#13;
<code class="n">n_neighbors</code> <code class="o">=</code> <code class="mi">5</code>&#13;
<code class="n">n_components</code> <code class="o">=</code> <code class="mi">10</code>&#13;
<code class="n">n_jobs</code> <code class="o">=</code> <code class="mi">4</code>&#13;
&#13;
<code class="n">isomap</code> <code class="o">=</code> <code class="n">Isomap</code><code class="p">(</code><code class="n">n_neighbors</code><code class="o">=</code><code class="n">n_neighbors</code><code class="p">,</code> \&#13;
                <code class="n">n_components</code><code class="o">=</code><code class="n">n_components</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=</code><code class="n">n_jobs</code><code class="p">)</code>&#13;
&#13;
<code class="n">isomap</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="mi">5000</code><code class="p">,:])</code>&#13;
<code class="n">X_train_isomap</code> <code class="o">=</code> <code class="n">isomap</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>&#13;
<code class="n">X_train_isomap</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train_isomap</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">train_index</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_validation_isomap</code> <code class="o">=</code> <code class="n">isomap</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_validation</code><code class="p">)</code>&#13;
<code class="n">X_validation_isomap</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_validation_isomap</code><code class="p">,</code> \&#13;
                                   <code class="n">index</code><code class="o">=</code><code class="n">validation_index</code><code class="p">)</code>&#13;
&#13;
<code class="n">scatterPlot</code><code class="p">(</code><code class="n">X_train_isomap</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="s2">"Isomap"</code><code class="p">)</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#separation_of_observations_using_isomap">Figure 3-11</a> shows the two-dimensional scatterplot using Isomap.</p>&#13;
&#13;
<figure><div class="figure" id="separation_of_observations_using_isomap">&#13;
<img alt="Separation of Observations Using Isomap" src="assets/hulp_0311.png"/>&#13;
<h6><span class="label">Figure 3-11. </span>Separation of observations using isomap</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before" data-pdf-bookmark="Multidimensional Scaling" data-type="sect1"><div class="sect1" id="idm140637550777392">&#13;
<h1>Multidimensional Scaling</h1>&#13;
&#13;
<p><em>Multidimensional scaling (MDS)</em> is<a data-primary="dimensionality reduction" data-secondary="multidimensional scaling" data-type="indexterm" id="idm140637550775296"/><a data-primary="multidimensional scaling" data-type="indexterm" id="idm140637550774240"/> a form of nonlinear dimensionality reduction that learns the similarity of points in the original dataset and, using this similarity learning, models this in a lower dimensional space:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Multidimensional Scaling</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.manifold</code> <code class="kn">import</code> <code class="n">MDS</code>&#13;
&#13;
<code class="n">n_components</code> <code class="o">=</code> <code class="mi">2</code>&#13;
<code class="n">n_init</code> <code class="o">=</code> <code class="mi">12</code>&#13;
<code class="n">max_iter</code> <code class="o">=</code> <code class="mi">1200</code>&#13;
<code class="n">metric</code> <code class="o">=</code> <code class="bp">True</code>&#13;
<code class="n">n_jobs</code> <code class="o">=</code> <code class="mi">4</code>&#13;
<code class="n">random_state</code> <code class="o">=</code> <code class="mi">2018</code>&#13;
&#13;
<code class="n">mds</code> <code class="o">=</code> <code class="n">MDS</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="n">n_components</code><code class="p">,</code> <code class="n">n_init</code><code class="o">=</code><code class="n">n_init</code><code class="p">,</code> <code class="n">max_iter</code><code class="o">=</code><code class="n">max_iter</code><code class="p">,</code> \&#13;
          <code class="n">metric</code><code class="o">=</code><code class="n">metric</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=</code><code class="n">n_jobs</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="n">random_state</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_train_mds</code> <code class="o">=</code> <code class="n">mds</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="mi">1000</code><code class="p">,:])</code>&#13;
<code class="n">X_train_mds</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train_mds</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">train_index</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="mi">1001</code><code class="p">])</code>&#13;
&#13;
<code class="n">scatterPlot</code><code class="p">(</code><code class="n">X_train_mds</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="s2">"Multidimensional Scaling"</code><code class="p">)</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#separation_of_observations_using_mds">Figure 3-12</a> displays the two-dimensional scatterplot using MDS.</p>&#13;
&#13;
<figure><div class="figure" id="separation_of_observations_using_mds">&#13;
<img alt="Separation of Observations Using MDS" src="assets/hulp_0312.png"/>&#13;
<h6><span class="label">Figure 3-12. </span>Separation of observations using MDS</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before" data-pdf-bookmark="Locally Linear Embedding" data-type="sect1"><div class="sect1" id="idm140637550611520">&#13;
<h1>Locally Linear Embedding</h1>&#13;
&#13;
<p>Another<a data-primary="locally linear embedding (LLE)" data-type="indexterm" id="idm140637550609712"/><a data-primary="dimensionality reduction" data-secondary="locally linear embedding (LLE)" data-type="indexterm" id="idm140637550608912"/> popular nonlinear dimensionality reduction method is called <em>locally linear embedding (LLE)</em>. This method preserves distances within local neighborhoods as it projects the data from the original feature space to a reduced space. LLE discovers the nonlinear structure in the original, high-dimensional data by segmenting the data into smaller components (i.e., into neighborhoods of points) and modeling each component as a linear embedding.</p>&#13;
&#13;
<p>For this algorithm, we set the number of components we desire and the number of points to consider in a given neighborhood:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Locally Linear Embedding (LLE)</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.manifold</code> <code class="kn">import</code> <code class="n">LocallyLinearEmbedding</code>&#13;
&#13;
<code class="n">n_neighbors</code> <code class="o">=</code> <code class="mi">10</code>&#13;
<code class="n">n_components</code> <code class="o">=</code> <code class="mi">2</code>&#13;
<code class="n">method</code> <code class="o">=</code> <code class="s1">'modified'</code>&#13;
<code class="n">n_jobs</code> <code class="o">=</code> <code class="mi">4</code>&#13;
<code class="n">random_state</code> <code class="o">=</code> <code class="mi">2018</code>&#13;
&#13;
<code class="n">lle</code> <code class="o">=</code> <code class="n">LocallyLinearEmbedding</code><code class="p">(</code><code class="n">n_neighbors</code><code class="o">=</code><code class="n">n_neighbors</code><code class="p">,</code> \&#13;
        <code class="n">n_components</code><code class="o">=</code><code class="n">n_components</code><code class="p">,</code> <code class="n">method</code><code class="o">=</code><code class="n">method</code><code class="p">,</code> \&#13;
        <code class="n">random_state</code><code class="o">=</code><code class="n">random_state</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=</code><code class="n">n_jobs</code><code class="p">)</code>&#13;
&#13;
<code class="n">lle</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="mi">5000</code><code class="p">,:])</code>&#13;
<code class="n">X_train_lle</code> <code class="o">=</code> <code class="n">lle</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>&#13;
<code class="n">X_train_lle</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train_lle</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">train_index</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_validation_lle</code> <code class="o">=</code> <code class="n">lle</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_validation</code><code class="p">)</code>&#13;
<code class="n">X_validation_lle</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_validation_lle</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">validation_index</code><code class="p">)</code>&#13;
&#13;
<code class="n">scatterPlot</code><code class="p">(</code><code class="n">X_train_lle</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="s2">"Locally Linear Embedding"</code><code class="p">)</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#separation_of_observations_using_lle">Figure 3-13</a> shows the two-dimensional scatterplot using LLE.</p>&#13;
&#13;
<figure><div class="figure" id="separation_of_observations_using_lle">&#13;
<img alt="Separation of Observations Using LLE" src="assets/hulp_0313.png"/>&#13;
<h6><span class="label">Figure 3-13. </span>Separation of observations using LLE</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="t-Distributed Stochastic Neighbor Embedding" data-type="sect1"><div class="sect1" id="idm140637550699984">&#13;
<h1>t-Distributed Stochastic Neighbor Embedding</h1>&#13;
&#13;
<p><em>t-distributed stochastic neighbor embedding (t-SNE)</em> is<a data-primary="t-distributed stochastic neighbor embedding" data-type="indexterm" id="idm140637550698160"/><a data-primary="dimensionality reduction" data-secondary="t-distributed stochastic neighbor embedding" data-type="indexterm" id="idm140637550697424"/> a nonlinear dimensionality reduction technique for visualizing high-dimensional data. t-SNE accomplishes this by modeling each high-dimensional point into a two- or three-dimensional space, where similar points are modeled close to each other and dissimilar points are modeled farther away. It does this by constructing two probability distributions, one over pairs of points in the high-dimensional space and another over pairs of points in the low-dimensional space such that similar points have a high probability and dissimilar points have a lower probability. Specifically,<a data-primary="Kullback–Leibler divergence" data-type="indexterm" id="idm140637550695712"/> t-SNE minimizes the <em>Kullback–Leibler divergence</em> between the two probability distributions.</p>&#13;
&#13;
<p>In real-world applications of t-SNE, it is best to use another dimensionality reduction technique (such as PCA, as we do here) to reduce the number of dimensions before applying t-SNE. By applying another form of dimensionality reduction first, we reduce the noise in the features that are fed into t-SNE and speed up the computation of the algorithm:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># t-SNE</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.manifold</code> <code class="kn">import</code> <code class="n">TSNE</code>&#13;
&#13;
<code class="n">n_components</code> <code class="o">=</code> <code class="mi">2</code>&#13;
<code class="n">learning_rate</code> <code class="o">=</code> <code class="mi">300</code>&#13;
<code class="n">perplexity</code> <code class="o">=</code> <code class="mi">30</code>&#13;
<code class="n">early_exaggeration</code> <code class="o">=</code> <code class="mi">12</code>&#13;
<code class="n">init</code> <code class="o">=</code> <code class="s1">'random'</code>&#13;
<code class="n">random_state</code> <code class="o">=</code> <code class="mi">2018</code>&#13;
&#13;
<code class="n">tSNE</code> <code class="o">=</code> <code class="n">TSNE</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="n">n_components</code><code class="p">,</code> <code class="n">learning_rate</code><code class="o">=</code><code class="n">learning_rate</code><code class="p">,</code> \&#13;
            <code class="n">perplexity</code><code class="o">=</code><code class="n">perplexity</code><code class="p">,</code> <code class="n">early_exaggeration</code><code class="o">=</code><code class="n">early_exaggeration</code><code class="p">,</code> \&#13;
            <code class="n">init</code><code class="o">=</code><code class="n">init</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="n">random_state</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_train_tSNE</code> <code class="o">=</code> <code class="n">tSNE</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train_PCA</code><code class="o">.</code><code class="n">loc</code><code class="p">[:</code><code class="mi">5000</code><code class="p">,:</code><code class="mi">9</code><code class="p">])</code>&#13;
<code class="n">X_train_tSNE</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train_tSNE</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">train_index</code><code class="p">[:</code><code class="mi">5001</code><code class="p">])</code>&#13;
&#13;
<code class="n">scatterPlot</code><code class="p">(</code><code class="n">X_train_tSNE</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="s2">"t-SNE"</code><code class="p">)</code></pre>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>t-SNE has a nonconvex cost function, which means that different initializations of the algorithm will generate different results. There is no stable solution.</p>&#13;
</div>&#13;
&#13;
<p><a data-type="xref" href="#separation_of_observations_using_t_sne">Figure 3-14</a> shows the two-dimensional scatterplot of t-SNE.</p>&#13;
&#13;
<figure><div class="figure" id="separation_of_observations_using_t_sne">&#13;
<img alt="Separation of Observations Using t-SNE" src="assets/hulp_0314.png"/>&#13;
<h6><span class="label">Figure 3-14. </span>Separation of observations using t-SNE</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Other Dimensionality Reduction Methods" data-type="sect1"><div class="sect1" id="idm140637550409152">&#13;
<h1>Other Dimensionality Reduction Methods</h1>&#13;
&#13;
<p>We have covered both linear and nonlinear forms of dimensionality reduction. Now we will move to methods that do not rely on any sort of geometry or distance metric.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Dictionary Learning" data-type="sect1"><div class="sect1" id="idm140637550407424">&#13;
<h1>Dictionary Learning</h1>&#13;
&#13;
<p>One<a data-primary="dimensionality reduction" data-secondary="dictionary learning" data-type="indexterm" id="idm140637550405632"/><a data-primary="dictionary learning" data-type="indexterm" id="idm140637550404608"/><a data-primary="atoms" data-type="indexterm" id="idm140637550403936"/> such method is <em>dictionary learning</em>, which learns the sparse representation of the original data. The resulting matrix is known as the <em>dictionary</em>, and the vectors in the dictionary are known as <em>atoms</em>. These atoms are simple, binary vectors, populated by zeros and ones. Each instance in the original data can be reconstructed as a weighted sum of these atoms.</p>&#13;
&#13;
<p>Assuming<a data-primary="undercomplete dictionaries" data-type="indexterm" id="idm140637550401088"/><a data-primary="overcomplete dictionaries" data-type="indexterm" id="idm140637550400384"/> there are <em>d</em> features in the original data and <em>n</em> atoms in the dictionary, we can have a dictionary that is either <em>undercomplete</em>, where <em>n &lt; d</em>, or <em>overcomplete</em>, where <em>n &gt; d</em>. The undercomplete dictionary achieves dimensionality reduction, representing the original data with a fewer number of vectors, which is what we will focus on.<sup><a data-type="noteref" href="ch03.html#idm140637550396848" id="idm140637550396848-marker">3</a></sup></p>&#13;
&#13;
<p>There is a mini-batch version of dictionary learning that we will apply to our dataset of digits. As with the other dimensionality reduction methods, we will set the number of components. We will also set the batch size and the number of iterations to perform the training.</p>&#13;
&#13;
<p>Since we want to visualize the images using a two-dimensional scatterplot, we will learn a very dense dictionary, but, in practice, we would use a much sparser version:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Mini-batch dictionary learning</code>&#13;
&#13;
<code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">MiniBatchDictionaryLearning</code>&#13;
&#13;
<code class="n">n_components</code> <code class="o">=</code> <code class="mi">50</code>&#13;
<code class="n">alpha</code> <code class="o">=</code> <code class="mi">1</code>&#13;
<code class="n">batch_size</code> <code class="o">=</code> <code class="mi">200</code>&#13;
<code class="n">n_iter</code> <code class="o">=</code> <code class="mi">25</code>&#13;
<code class="n">random_state</code> <code class="o">=</code> <code class="mi">2018</code>&#13;
&#13;
<code class="n">miniBatchDictLearning</code> <code class="o">=</code> <code class="n">MiniBatchDictionaryLearning</code><code class="p">(</code> \&#13;
                        <code class="n">n_components</code><code class="o">=</code><code class="n">n_components</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="n">alpha</code><code class="p">,</code> \&#13;
                        <code class="n">batch_size</code><code class="o">=</code><code class="n">batch_size</code><code class="p">,</code> <code class="n">n_iter</code><code class="o">=</code><code class="n">n_iter</code><code class="p">,</code> \&#13;
                        <code class="n">random_state</code><code class="o">=</code><code class="n">random_state</code><code class="p">)</code>&#13;
&#13;
<code class="n">miniBatchDictLearning</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,:</code><code class="mi">10000</code><code class="p">])</code>&#13;
<code class="n">X_train_miniBatchDictLearning</code> <code class="o">=</code> <code class="n">miniBatchDictLearning</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>&#13;
<code class="n">X_train_miniBatchDictLearning</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code> \&#13;
    <code class="n">data</code><code class="o">=</code><code class="n">X_train_miniBatchDictLearning</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">train_index</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_validation_miniBatchDictLearning</code> <code class="o">=</code> \&#13;
    <code class="n">miniBatchDictLearning</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_validation</code><code class="p">)</code>&#13;
<code class="n">X_validation_miniBatchDictLearning</code> <code class="o">=</code> \&#13;
    <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_validation_miniBatchDictLearning</code><code class="p">,</code> \&#13;
    <code class="n">index</code><code class="o">=</code><code class="n">validation_index</code><code class="p">)</code>&#13;
&#13;
<code class="n">scatterPlot</code><code class="p">(</code><code class="n">X_train_miniBatchDictLearning</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> \&#13;
            <code class="s2">"Mini-batch Dictionary Learning"</code><code class="p">)</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#separation_of_observations_using_dictionary_learning">Figure 3-15</a> shows the two-dimensional scatterplot using dictionary learning.</p>&#13;
&#13;
<figure><div class="figure" id="separation_of_observations_using_dictionary_learning">&#13;
<img alt="Separation of Observations Using Dictionary Learning" src="assets/hulp_0315.png"/>&#13;
<h6><span class="label">Figure 3-15. </span>Separation of observations using dictionary learning</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Independent Component Analysis" data-type="sect1"><div class="sect1" id="idm140637550233904">&#13;
<h1>Independent Component Analysis</h1>&#13;
&#13;
<p>One<a data-primary="dimensionality reduction" data-secondary="independent component analysis (ICA)" data-type="indexterm" id="idm140637550232368"/><a data-primary="independent component analysis (ICA)" data-type="indexterm" id="idm140637550231280"/> common problem with unlabeled data is that there are many independent signals embedded together into the features we are given. Using <em>independent component analysis (ICA)</em>, we can separate these blended signals into their individual components. After the separation is complete, we can reconstruct any of the original features by adding together some combination of the individual components we generate. ICA is commonly used in signal processing tasks (for example, to identify the individual voices in an audio clip of a busy coffeehouse).</p>&#13;
&#13;
<p>The following shows how ICA works:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Independent Component Analysis</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">FastICA</code>&#13;
&#13;
<code class="n">n_components</code> <code class="o">=</code> <code class="mi">25</code>&#13;
<code class="n">algorithm</code> <code class="o">=</code> <code class="s1">'parallel'</code>&#13;
<code class="n">whiten</code> <code class="o">=</code> <code class="bp">True</code>&#13;
<code class="n">max_iter</code> <code class="o">=</code> <code class="mi">100</code>&#13;
<code class="n">random_state</code> <code class="o">=</code> <code class="mi">2018</code>&#13;
&#13;
<code class="n">fastICA</code> <code class="o">=</code> <code class="n">FastICA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="n">n_components</code><code class="p">,</code> <code class="n">algorithm</code><code class="o">=</code><code class="n">algorithm</code><code class="p">,</code> \&#13;
                  <code class="n">whiten</code><code class="o">=</code><code class="n">whiten</code><code class="p">,</code> <code class="n">max_iter</code><code class="o">=</code><code class="n">max_iter</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="n">random_state</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_train_fastICA</code> <code class="o">=</code> <code class="n">fastICA</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>&#13;
<code class="n">X_train_fastICA</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train_fastICA</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">train_index</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_validation_fastICA</code> <code class="o">=</code> <code class="n">fastICA</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_validation</code><code class="p">)</code>&#13;
<code class="n">X_validation_fastICA</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_validation_fastICA</code><code class="p">,</code> \&#13;
                                    <code class="n">index</code><code class="o">=</code><code class="n">validation_index</code><code class="p">)</code>&#13;
&#13;
<code class="n">scatterPlot</code><code class="p">(</code><code class="n">X_train_fastICA</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="s2">"Independent Component Analysis"</code><code class="p">)</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#separation_of_observations_using_independent_component_analysis">Figure 3-16</a> shows the two-dimensional scatterplot using ICA.</p>&#13;
&#13;
<figure><div class="figure" id="separation_of_observations_using_independent_component_analysis">&#13;
<img alt="Separation of Observations Using Independent Component Analysis" src="assets/hulp_0316.png"/>&#13;
<h6><span class="label">Figure 3-16. </span>Separation of observations using independent component analysis</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="idm140637549970960">&#13;
<h1>Conclusion</h1>&#13;
&#13;
<p>In this chapter, we introduced and explored a number of dimensionality reduction algorithms starting with linear methods such as PCA and random projection. Then, we switched to nonlinear methods—also known as manifold learning—such as Isomap, multidimensional scaling, LLE, and t-SNE. We also covered nondistance-based methods such as dictionary learning and ICA.</p>&#13;
&#13;
<p>Dimensionality reduction captures the most salient information in a dataset in a small number of dimensions by learning the underlying structure of the data, and it does this without using any labels. By applying these algorithms to the MNIST digits dataset, we were able to meaningfully separate the images based on the digits they represented with just the top two dimensions.</p>&#13;
&#13;
<p>This highlights the power of dimensionality reduction.</p>&#13;
&#13;
<p>In <a data-type="xref" href="ch04.html#Chapter_4">Chapter 4</a>, we will build an applied unsupervised learning solution using these dimensionality reduction algorithms. Specifically, we will revist the fraud detection problem introduced in <a data-type="xref" href="ch02.html#Chapter_2">Chapter 2</a> and attempt to separate fraudulent transactions from normal ones without using labels.<a data-primary="" data-startref="SLdimen03" data-type="indexterm" id="idm140637549965504"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm140637553285552"><sup><a href="ch03.html#idm140637553285552-marker">1</a></sup> The <a href="http://yann.lecun.com/exdb/mnist/">MNIST database of handwritten digits</a>, courtesy of Yann Lecun.</p><p data-type="footnote" id="idm140637553283488"><sup><a href="ch03.html#idm140637553283488-marker">2</a></sup> The <a href="http://deeplearning.net/tutorial/gettingstarted.html">pickled version of the MNIST dataset</a>, courtesy of deeplearning.net.</p><p data-type="footnote" id="idm140637550396848"><sup><a href="ch03.html#idm140637550396848-marker">3</a></sup> The overcomplete dictionary serves a different purpose and has applications such as image compression.</p></div></div></section></body></html>