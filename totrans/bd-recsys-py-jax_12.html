<html><head></head><body><section data-pdf-bookmark="Chapter 9. Feature-Based and Counting-Based Recommendations" data-type="chapter" epub:type="chapter"><div class="chapter" id="feature-counting">&#13;
<h1><span class="label">Chapter 9. </span>Feature-Based and Counting-Based Recommendations</h1>&#13;
&#13;
&#13;
<p>Consider this oversimplified problem: given a bunch of new users, predict which will like our new mega-ultra-fancy-fun-item-of-novelty, or MUFFIN for short. You may start by asking which old users like MUFFIN; do those users have any aspects in common? If so, you could build a model that predicts MUFFIN affinity from those correlated user features.</p>&#13;
&#13;
<p>Alternatively, you could ask, â€œWhat are other items people buy with MUFFIN?â€ If you find that others frequently also ask for JAM (just-awesome-merch), then MUFFIN may be a good suggestion for those who already have JAM. This would be using the co-occurrence of MUFFIN and JAM as a predictor. Similarly, if your friend comes along with tastes similar to yoursâ€”you both like SCONE, JAM, BISCUIT, and TEAâ€”but your friend hasnâ€™t yet had the MUFFIN, if you like MUFFIN, itâ€™s probably a good choice for your friend too. This is using the co-occurrence of items between you and your friend.</p>&#13;
&#13;
<p>These item relationship features will form our first ranking methods in this chapter; so grab a tasty snack and letâ€™s dig in.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Bilinear Factor Models (Metric Learning)" data-type="sect1"><div class="sect1" id="id97">&#13;
<h1>Bilinear Factor Models (Metric Learning)</h1>&#13;
&#13;
<p>As<a data-primary="bilinear factor models" data-secondary="basics of" data-type="indexterm" id="bilinearfm09"/><a data-primary="models" data-secondary="bilinear models" data-type="indexterm" id="Mbilinear09"/><a data-primary="metric learning" data-type="indexterm" id="metricl09"/><a data-primary="feature-based recommendations" data-secondary="bilinear factor models" data-type="indexterm" id="FBRbilinear09"/> per the usual idioms about running in front of horses and walking after the cart, letâ€™s start our journey into ranking systems with what can be considered the<a data-primary="naive ML approaches" data-type="indexterm" id="id790"/><a data-primary="machine learning (ML)" data-secondary="naive ML approaches" data-type="indexterm" id="id791"/> <em>naive</em> ML approaches. Via these approaches, we will start to get a sense of where the rub lies in building recommendation systems and why some of the forthcoming efforts are necessary at all.</p>&#13;
&#13;
<p>Letâ€™s begin again with our basic premise of recommendation problems: to estimate ratings of item <math alttext="x">&#13;
  <mi>x</mi>&#13;
</math> by user <math alttext="i">&#13;
  <mi>i</mi>&#13;
</math> written as <math alttext="r Subscript i comma x">&#13;
  <msub><mi>r</mi> <mrow><mi>i</mi><mo>,</mo><mi>x</mi></mrow> </msub>&#13;
</math>. <em>Note the slight change in notation from earlier for reasons that will become clear momentarily.</em> In a usual ML paradigm, we might claim that estimating this score is done via properties of the item and the user, and frequently those properties would be described as features, and thus <math alttext="bold i">&#13;
  <mi>ğ¢</mi>&#13;
</math> and <math alttext="bold x">&#13;
  <mi>ğ±</mi>&#13;
</math> can be the user and item vectors, respectively, composed of these features.</p>&#13;
&#13;
<p>Now, we consider user <math alttext="i">&#13;
  <mi>i</mi>&#13;
</math> with their collection of previously interacted-with items <math alttext="script upper R Subscript i">&#13;
  <msub><mi>â„›</mi> <mi>i</mi> </msub>&#13;
</math>, and consider <math alttext="script upper I equals StartSet bold x vertical-bar x element-of script upper R Subscript i Baseline EndSet">&#13;
  <mrow>&#13;
    <mi>â„</mi>&#13;
    <mo>=</mo>&#13;
    <mo>{</mo>&#13;
    <mi>ğ±</mi>&#13;
    <mo>|</mo>&#13;
    <mi>x</mi>&#13;
    <mo>âˆˆ</mo>&#13;
    <msub><mi>â„›</mi> <mi>i</mi> </msub>&#13;
    <mo>}</mo>&#13;
  </mrow>&#13;
</math> the set of vectors associated to those items in this feature space. We can then map this collection of vectors to a representation to yield a<a data-primary="content-based feature vector" data-type="indexterm" id="id792"/> <em>content-based feature vector for <math alttext="i">&#13;
  <mi>i</mi>&#13;
</math>.</em> <a data-type="xref" href="#fig:content-feature-vector">FigureÂ 9-1</a> illustrates an example mapping.</p>&#13;
&#13;
<figure><div class="figure" id="fig:content-feature-vector">&#13;
<img alt="Map a user's read books to a single feature vector" src="assets/brpj_0901.png"/>&#13;
<h6><span class="label">Figure 9-1. </span>Content-to-feature vector</h6>&#13;
</div></figure>&#13;
&#13;
<p>This extremely simple approach can turn a collection of item features and user-item interactions into features of the user. Much of the following will be increasingly rich ways of doing this. Thinking very hard about the map, the features, and the requirements for <em>interaction</em> yields many of the key insights in the rest of the book.</p>&#13;
&#13;
<p>Letâ€™s take the preceding mapping, <math alttext="bold i colon equals upper F left-parenthesis script upper I right-parenthesis">&#13;
  <mrow>&#13;
    <mi>ğ¢</mi>&#13;
    <mo>:</mo>&#13;
    <mo>=</mo>&#13;
    <mi>F</mi>&#13;
    <mfenced close=")" open="(">&#13;
      <mi>â„</mi>&#13;
    </mfenced>&#13;
  </mrow>&#13;
</math>, to be a simple aggregation like dimension-wise average. Then recognize that the mapping will provide a vector of the same dimension as the items. Now we have a user vector in the same â€œspaceâ€ as the items, and we can ask a similarity question as we did in our discussion of latent space in <a data-type="xref" href="ch03.html#ch:math">ChapterÂ 3</a>.</p>&#13;
&#13;
<p>We need to move back to the mathematical framings to set up how to use these vectors. Ultimately, weâ€™re now in a latent space with users and items, but how can we do anything with that? Well you may already remember how to compare vector similarity. Letâ€™s define the similarity to be<a data-primary="similarity" data-secondary="cosine similarity" data-type="indexterm" id="id793"/><a data-primary="cosine similarity" data-type="indexterm" id="id794"/> <em>cosine-similarity</em>:</p>&#13;
<div data-type="equation">&#13;
<math alttext="s i m left-parenthesis bold i comma bold x right-parenthesis equals StartFraction bold i dot bold x Over StartAbsoluteValue bold i EndAbsoluteValue asterisk StartAbsoluteValue bold x EndAbsoluteValue EndFraction" display="block">&#13;
  <mrow>&#13;
    <mi>s</mi>&#13;
    <mi>i</mi>&#13;
    <mi>m</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>ğ¢</mi>&#13;
      <mo>,</mo>&#13;
      <mi>ğ±</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mstyle displaystyle="true" scriptlevel="0">&#13;
      <mfrac><mrow><mi>ğ¢</mi><mo>Â·</mo><mi>ğ±</mi></mrow> <mrow><mfenced close="|" open="|"><mi>ğ¢</mi></mfenced><mo>*</mo><mfenced close="|" open="|"><mi>ğ±</mi></mfenced></mrow></mfrac>&#13;
    </mstyle>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>If we precompose our similarity with vector normalization, this is simply the inner productâ€”<em>and this is an essential first step toward recommendation systems</em>. For convenience, letâ€™s always assume this space weâ€™re working in is after normalization, so all similarity measures are done on the unit sphere:</p>&#13;
<div data-type="equation">&#13;
<math alttext="r Subscript i comma x Baseline tilde s i m left-parenthesis bold i comma bold x right-parenthesis equals sigma-summation Underscript k Endscripts bold i Subscript k Baseline asterisk bold x Subscript k" display="block">&#13;
  <mrow>&#13;
    <msub><mi>r</mi> <mrow><mi>i</mi><mo>,</mo><mi>x</mi></mrow> </msub>&#13;
    <mo>âˆ¼</mo>&#13;
    <mi>s</mi>&#13;
    <mi>i</mi>&#13;
    <mi>m</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>ğ¢</mi>&#13;
      <mo>,</mo>&#13;
      <mi>ğ±</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <munder><mo>âˆ‘</mo> <mi>k</mi> </munder>&#13;
    <msub><mi>ğ¢</mi> <mi>k</mi> </msub>&#13;
    <mo>*</mo>&#13;
    <msub><mi>ğ±</mi> <mi>k</mi> </msub>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>This now approximates our ratings. But wait, dear reader, where are the learnable parameters? Letâ€™s go ahead and make this a weighted summation, via a diagonal matrix <math alttext="upper A">&#13;
  <mi>A</mi>&#13;
</math>:</p>&#13;
<div data-type="equation">&#13;
<math alttext="r Subscript i comma x Baseline tilde s i m Superscript upper A Baseline left-parenthesis bold i comma bold x right-parenthesis equals sigma-summation Underscript k Endscripts a Subscript k Baseline asterisk bold i Subscript k Baseline asterisk bold x Subscript k" display="block">&#13;
  <mrow>&#13;
    <msub><mi>r</mi> <mrow><mi>i</mi><mo>,</mo><mi>x</mi></mrow> </msub>&#13;
    <mo>âˆ¼</mo>&#13;
    <mi>s</mi>&#13;
    <mi>i</mi>&#13;
    <msup><mi>m</mi> <mi>A</mi> </msup>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>ğ¢</mi>&#13;
      <mo>,</mo>&#13;
      <mi>ğ±</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <munder><mo>âˆ‘</mo> <mi>k</mi> </munder>&#13;
    <msub><mi>a</mi> <mi>k</mi> </msub>&#13;
    <mo>*</mo>&#13;
    <msub><mi>ğ¢</mi> <mi>k</mi> </msub>&#13;
    <mo>*</mo>&#13;
    <msub><mi>ğ±</mi> <mi>k</mi> </msub>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>This slight generalization already puts us in the world of statistical learning. You can probably already see how <math alttext="upper A">&#13;
  <mi>A</mi>&#13;
</math> can be used to learn which of the dimensions in this space are most important for approximating the ratings, but before we make that precise, letâ€™s generalize yet once more:</p>&#13;
<div data-type="equation">&#13;
<math alttext="r Subscript i comma x Baseline tilde s i m Superscript upper A Baseline left-parenthesis bold i comma bold x right-parenthesis equals sigma-summation Underscript k comma l Endscripts a Subscript k l Baseline asterisk bold i Subscript k Baseline asterisk bold x Subscript l" display="block">&#13;
  <mrow>&#13;
    <msub><mi>r</mi> <mrow><mi>i</mi><mo>,</mo><mi>x</mi></mrow> </msub>&#13;
    <mo>âˆ¼</mo>&#13;
    <mi>s</mi>&#13;
    <mi>i</mi>&#13;
    <msup><mi>m</mi> <mi>A</mi> </msup>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>ğ¢</mi>&#13;
      <mo>,</mo>&#13;
      <mi>ğ±</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <munder><mo>âˆ‘</mo> <mrow><mi>k</mi><mo>,</mo><mi>l</mi></mrow> </munder>&#13;
    <msub><mi>a</mi> <mrow><mi>k</mi><mi>l</mi></mrow> </msub>&#13;
    <mo>*</mo>&#13;
    <msub><mi>ğ¢</mi> <mi>k</mi> </msub>&#13;
    <mo>*</mo>&#13;
    <msub><mi>ğ±</mi> <mi>l</mi> </msub>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>This nets us even more parameters! We see that now <math alttext="s i m Superscript upper A Baseline left-parenthesis bold i comma bold x right-parenthesis equals bold i upper A bold x">&#13;
  <mrow>&#13;
    <mi>s</mi>&#13;
    <mi>i</mi>&#13;
    <msup><mi>m</mi> <mi>A</mi> </msup>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>ğ¢</mi>&#13;
      <mo>,</mo>&#13;
      <mi>ğ±</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mi>ğ¢</mi>&#13;
    <mi>A</mi>&#13;
    <mi>ğ±</mi>&#13;
  </mrow>&#13;
</math>, and we are only one step away from the familiar ground of<a data-primary="linear regression" data-type="indexterm" id="id795"/> linear regression. Currently, our model is in the form of a<a data-primary="bilinear regression" data-type="indexterm" id="id796"/> <em>bilinear regression</em>, so letâ€™s utilize a little linear algebra. For the sake of exposition, let <math alttext="bold i element-of double-struck upper R Superscript n">&#13;
  <mrow>&#13;
    <mi>ğ¢</mi>&#13;
    <mo>âˆˆ</mo>&#13;
    <msup><mi>â„</mi> <mi>n</mi> </msup>&#13;
  </mrow>&#13;
</math>, <math alttext="bold x element-of double-struck upper R Superscript m">&#13;
  <mrow>&#13;
    <mi>ğ±</mi>&#13;
    <mo>âˆˆ</mo>&#13;
    <msup><mi>â„</mi> <mi>m</mi> </msup>&#13;
  </mrow>&#13;
</math>, and <math alttext="upper A element-of double-struck upper R Superscript n times m">&#13;
  <mrow>&#13;
    <mi>A</mi>&#13;
    <mo>âˆˆ</mo>&#13;
    <msup><mi>â„</mi> <mrow><mi>n</mi><mo>Ã—</mo><mi>m</mi></mrow> </msup>&#13;
  </mrow>&#13;
</math>, and then we have this:</p>&#13;
<div data-type="equation">&#13;
<math alttext="bold vect left-parenthesis bold i asterisk bold x Superscript upper T Baseline right-parenthesis element-of double-struck upper R Superscript n asterisk m" display="block">&#13;
  <mrow>&#13;
    <mi>ğ¯ğğœğ­</mi>&#13;
    <mfenced close=")" open="(" separators="">&#13;
      <mi>ğ¢</mi>&#13;
      <mo>*</mo>&#13;
      <msup><mi>ğ±</mi> <mi>T</mi> </msup>&#13;
    </mfenced>&#13;
    <mo>âˆˆ</mo>&#13;
    <msup><mi>â„</mi> <mrow><mi>n</mi><mo>*</mo><mi>m</mi></mrow> </msup>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>We can simplify to the following:</p>&#13;
<div data-type="equation">&#13;
<math alttext="s i m Superscript upper A Baseline left-parenthesis bold i comma bold x right-parenthesis equals bold i upper A bold x equals bold vect left-parenthesis bold i asterisk bold x Superscript upper T Baseline right-parenthesis asterisk bold vect left-parenthesis upper A right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <mi>s</mi>&#13;
    <mi>i</mi>&#13;
    <msup><mi>m</mi> <mi>A</mi> </msup>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>ğ¢</mi>&#13;
      <mo>,</mo>&#13;
      <mi>ğ±</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mi>ğ¢</mi>&#13;
    <mi>A</mi>&#13;
    <mi>ğ±</mi>&#13;
    <mo>=</mo>&#13;
    <mi>ğ¯ğğœğ­</mi>&#13;
    <mfenced close=")" open="(" separators="">&#13;
      <mi>ğ¢</mi>&#13;
      <mo>*</mo>&#13;
      <msup><mi>ğ±</mi> <mi>T</mi> </msup>&#13;
    </mfenced>&#13;
    <mo>*</mo>&#13;
    <mi>ğ¯ğğœğ­</mi>&#13;
    <mfenced close=")" open="(">&#13;
      <mi>A</mi>&#13;
    </mfenced>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>If we make up notation for the right-hand side, youâ€™ll find your friend linear regression waiting for you:</p>&#13;
<div data-type="equation">&#13;
<math alttext="bold v Subscript i x Baseline colon equals bold vect left-parenthesis bold i asterisk bold x Superscript upper T Baseline right-parenthesis comma beta colon equals bold vect left-parenthesis upper A right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <msub><mi>ğ¯</mi> <mrow><mi>i</mi><mi>x</mi></mrow> </msub>&#13;
    <mo>:</mo>&#13;
    <mo>=</mo>&#13;
    <mi>ğ¯ğğœğ­</mi>&#13;
    <mfenced close=")" open="(" separators="">&#13;
      <mi>ğ¢</mi>&#13;
      <mo>*</mo>&#13;
      <msup><mi>ğ±</mi> <mi>T</mi> </msup>&#13;
    </mfenced>&#13;
    <mo>,</mo>&#13;
    <mi>Î²</mi>&#13;
    <mo>:</mo>&#13;
    <mo>=</mo>&#13;
    <mi>ğ¯ğğœğ­</mi>&#13;
    <mfenced close=")" open="(">&#13;
      <mi>A</mi>&#13;
    </mfenced>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Thus:</p>&#13;
<div data-type="equation">&#13;
<math alttext="r Subscript i comma x Baseline tilde s i m Superscript upper A Baseline left-parenthesis bold i comma bold x right-parenthesis equals bold v Subscript i x Baseline beta" display="block">&#13;
  <mrow>&#13;
    <msub><mi>r</mi> <mrow><mi>i</mi><mo>,</mo><mi>x</mi></mrow> </msub>&#13;
    <mo>âˆ¼</mo>&#13;
    <mi>s</mi>&#13;
    <mi>i</mi>&#13;
    <msup><mi>m</mi> <mi>A</mi> </msup>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>ğ¢</mi>&#13;
      <mo>,</mo>&#13;
      <mi>ğ±</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <msub><mi>ğ¯</mi> <mrow><mi>i</mi><mi>x</mi></mrow> </msub>&#13;
    <mi>Î²</mi>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>With this computation behind us, we see that whether we wish to compute binary ratings, ordinal ratings, or likelihood estimation, the tools in our linear models toolbox can enter the party. We have available to us regularization and optimizers and any other fun weâ€™re interested in from the linear models world.</p>&#13;
&#13;
<p>If these equations feel frustrating or painful, let me try to offer you a geometric mental model. Each item and user is in a high-dimensional space, and ultimately weâ€™re trying to figure out which ones are closest to one another. People frequently misunderstand these geometries by imagining the tips of the vectors being near one another; this is not the case. These spaces are extremely high-dimensional, which results in the analogy being far from the truth. Instead, ask if <em>the values are similarly large in some of the vector indices.</em> This is a much simpler, but also more accurate, geometric view: there are some subspaces in the extremely high-dimensional space where the vectors point in the same direction.</p>&#13;
&#13;
<p>This forms the foundation for where we are going but has serious limitations for large-scale recommender problems. You will see, however, that the feature-based learning still has its place in the cold-start regime.</p>&#13;
&#13;
<p>Note that in addition to the preceding approach of building content-based features for a user, we may also have obvious user features that are obtained via queries to the user, or implicitly via other data collection; examples of these features include location, age range, and height.<a data-primary="" data-startref="FBRbilinear09" data-type="indexterm" id="id797"/></p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id798">&#13;
<h1>Is User Space the Same as Item Space?</h1>&#13;
<p>In<a data-primary="user space" data-type="indexterm" id="id799"/><a data-primary="item space" data-type="indexterm" id="id800"/><a data-primary="feature-based recommendations" data-secondary="user space versus item space" data-type="indexterm" id="id801"/> this section, weâ€™ve discussed ways to put users and items in the same latent spaces. We claim that we can make comparisons between users and items by vector operations. In mathematics, vectors are elements of vector spaces, and (finite dimensional) vector spaces are defined by their number of dimensions and the values that the vectors have as elements. For example, if we say itâ€™s a three-dimensional vector space with 8-bit integers, thatâ€™s sufficient to specify a vector space.</p>&#13;
&#13;
<p>However, devilish details are lurking around. First, what does <em>distance</em> mean in a specified vector space? We have many conventional measures, but itâ€™s important to ensure that comparisons between two spaces are utilizing the same definitions of distance. Another consideration is the process by which you define the vectors of the space; if you arrive at your vectors via a dimension reduction from a larger space, there are likely density properties that you can expect not to be present naively. Where this is most relevant for ranking models and recommendation systems is that we frequently arrive at user space and item space separately, and often compute distance between user and item vectors.</p>&#13;
&#13;
<p>Is this OK? In many cases, it lacks firm theoretical footing but works well. One particular case where this <em>does</em> have a firm theoretical footing is MF. Rather than a long digression on geometric algebra, we will give the following guidance: if youâ€™re interested in comparing two vectors that arenâ€™t in the same space, ask yourself if theyâ€™re of the same dimension, if distance is defined the same in both spaces, and if the density priors are similar. In fact, at times, none of these is true and you can <em>still</em> get away with a comparison. But for each of these potential risks, itâ€™s worth a stop-and-think.</p>&#13;
&#13;
<p>One explicit example of a troubling difference in two latent spaces is found in <a href="https://oreil.ly/ReDF6">â€œPoincarÃ© Embeddings for&#13;
Learning Hierarchical Representationsâ€</a> by Maximilian Nickel and Douwe Kiela; this paper provides an interesting way to encode relationships between items in your latent space via the implicit geometry. However, your users may not be encoded in a hyperbolic space. Tread carefully if you compute inner products between these and Euclidean embedded vectors!<a data-primary="" data-startref="bilinearfm09" data-type="indexterm" id="id802"/><a data-primary="" data-startref="metricl09" data-type="indexterm" id="id803"/><a data-primary="" data-startref="Mbilinear09" data-type="indexterm" id="id804"/></p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Feature-Based Warm Starting" data-type="sect1"><div class="sect1" id="id98">&#13;
<h1>Feature-Based Warm Starting</h1>&#13;
&#13;
<p>As<a data-primary="feature-based recommendations" data-secondary="feature-based warm starting" data-type="indexterm" id="featurebasedws09"/><a data-primary="warm starts" data-type="indexterm" id="warmstart09"/> you saw in <a data-type="xref" href="ch07.html#serving-and-architecture">ChapterÂ 7</a>, there are a variety of ways to use features alongside some of the collaborative filtering (CF) and MF approaches weâ€™ve presented. In particular, you saw how encoders built via a two-towers architecture can be used for fast feature-based recommendations in the cold-start scenario. Letâ€™s look into this deeper and think carefully about features for new users or items.</p>&#13;
&#13;
<p>In <a data-type="xref" href="#feature-counting">ChapterÂ 9</a>, we built  our bilinear factor model as a simple regression and, in fact, saw that all the standard ML modeling approaches would apply. However, we took the user embedding to be features learned from item interactions: that is, the content-based feature vector. If our goal is to build a recommendation algorithm that does not need a history of user ratings, obviously this construction will not suffice.</p>&#13;
&#13;
<p>We might begin by asking if the preceding factor regression approach could work in the pure user-feature settingâ€”leave aside worries about the inner product that depended on a mutual embedding and just take everything to be pure matrices. While this is a reasonable idea that can yield some results, we may quickly identify the coarseness of this model: each user would then need to provide answers to queries <math alttext="q Subscript k">&#13;
  <msub><mi>q</mi> <mi>k</mi> </msub>&#13;
</math> such that <math alttext="bold i element-of double-struck upper R Superscript k">&#13;
  <mrow>&#13;
    <mi>ğ¢</mi>&#13;
    <mo>âˆˆ</mo>&#13;
    <msup><mi>â„</mi> <mi>k</mi> </msup>&#13;
  </mrow>&#13;
</math>. Because the<a data-primary="dimensionality" data-type="indexterm" id="id805"/> dimensionality of these user vectors scales linearly with the number of questions weâ€™re willing and able to ask the user, we are passing along the difficulty of the problem to our user experience.</p>&#13;
&#13;
<p class="less_space pagebreak-before">Because we intend on using CF via MF as our core model, weâ€™d really like to find a way to smoothly transition from the feature-based model into this MF, ensuring we take advantage of user/item ratings as they emerge. In <a data-type="xref" href="ch07.html#sec:eval_flywheel">â€œThe Evaluation Flywheelâ€</a>, we discussed using inference results and their subsequent outcomes in real time to update the model, but how do we account for that in the modeling paradigm?</p>&#13;
&#13;
<p>In a latent-factor model obtained via MF, we have the following:</p>&#13;
<div data-type="equation">&#13;
<math alttext="bold u Subscript i Baseline bold v Subscript x" display="block">&#13;
  <mrow>&#13;
    <msub><mi>ğ®</mi> <mi>i</mi> </msub>&#13;
    <msub><mi>ğ¯</mi> <mi>x</mi> </msub>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Here, <math alttext="bold u Subscript i">&#13;
  <msub><mi>ğ®</mi> <mi>i</mi> </msub>&#13;
</math> has a Gaussian prior with zero mean; this is why new users wonâ€™t yield useful ratings before they have interaction data. We thus say that the<a data-primary="user-matrix" data-type="indexterm" id="id806"/> <em>user-matrix</em> has<a data-primary="zero-concentrated priors" data-type="indexterm" id="id807"/> <em>zero-concentrated priors.</em> Our first strategy to including features in our MF is to simply build a better priors distribution.</p>&#13;
&#13;
<p>More mathematically: we learn a regression model <math alttext="upper G left-parenthesis bold i right-parenthesis tilde bold u Subscript i">&#13;
  <mrow>&#13;
    <mi>G</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>ğ¢</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>âˆ¼</mo>&#13;
    <msub><mi>ğ®</mi> <mi>i</mi> </msub>&#13;
  </mrow>&#13;
</math> for initialization of our learned factor matrix, and this means weâ€™re learning the following:</p>&#13;
<div data-type="equation">&#13;
<math alttext="s left-parenthesis i comma x right-parenthesis tilde bold w Subscript i x Baseline gamma plus alpha Subscript i Baseline plus beta Subscript x Baseline plus bold u Subscript i Baseline bold v Subscript x" display="block">&#13;
  <mrow>&#13;
    <mi>s</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>i</mi>&#13;
      <mo>,</mo>&#13;
      <mi>x</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>âˆ¼</mo>&#13;
    <msub><mi>ğ°</mi> <mrow><mi>i</mi><mi>x</mi></mrow> </msub>&#13;
    <mi>Î³</mi>&#13;
    <mo>+</mo>&#13;
    <msub><mi>Î±</mi> <mi>i</mi> </msub>&#13;
    <mo>+</mo>&#13;
    <msub><mi>Î²</mi> <mi>x</mi> </msub>&#13;
    <mo>+</mo>&#13;
    <msub><mi>ğ®</mi> <mi>i</mi> </msub>&#13;
    <msub><mi>ğ¯</mi> <mi>x</mi> </msub>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Here, our <math alttext="bold w Subscript i x Baseline gamma">&#13;
  <mrow>&#13;
    <msub><mi>ğ°</mi> <mrow><mi>i</mi><mi>x</mi></mrow> </msub>&#13;
    <mi>Î³</mi>&#13;
  </mrow>&#13;
</math> is now a standard bilinear feature regression from user and item features, the bias terms are learned to estimate popularity or<a data-primary="rank inflation" data-type="indexterm" id="id808"/> <em>rank inflation</em>, and our familiar MF terms are <math alttext="bold u Subscript i Baseline bold v Subscript x">&#13;
  <mrow>&#13;
    <msub><mi>ğ®</mi> <mi>i</mi> </msub>&#13;
    <msub><mi>ğ¯</mi> <mi>x</mi> </msub>&#13;
  </mrow>&#13;
</math>.</p>&#13;
&#13;
<p>Note that this approach provides a general strategy for including features into an MF model. How we fit the factors-features model is totally up to us, as are the optimization methods we wish to employ.</p>&#13;
&#13;
<p>Also note that instead of regression-based approaches, priors can be established via <em>k</em>-nearest neighbors in a purely feature-based embedding space. This modeling strategy is explored in great detail in  <a href="https://oreil.ly/N4Ast"> â€œEliciting Auxiliary Information for Cold Start User Recommendation: A Surveyâ€</a> by Nor Aniza Abdullah et al. Compare this with the item-item content-based recommender from <a data-type="xref" href="ch05.html#ch:pinterest-content">ChapterÂ 5</a>, where the query is an item and similarity in item space is the link between the last item and the next.</p>&#13;
&#13;
<p>We have established a strategy and a collection of approaches to building our models via features. Weâ€™ve even seen how our MF will fall over for new users, only to be saved by a feature-based model. So why not stick to features? Why introduce factors at all?<a data-primary="" data-startref="featurebasedws09" data-type="indexterm" id="id809"/><a data-primary="" data-startref="warmstart09" data-type="indexterm" id="id810"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="less_space pagebreak-before" data-pdf-bookmark="Segmentation Models and Hybrids" data-type="sect1"><div class="sect1" id="id99">&#13;
<h1>Segmentation Models and Hybrids</h1>&#13;
&#13;
<p>Similar to our preceding discussion of warm-starting via features is the closely related concept of<a data-primary="demographic-based systems" data-type="indexterm" id="id811"/><a data-primary="segmentation models" data-type="indexterm" id="segmod09"/><a data-primary="feature-based recommendations" data-secondary="segmentation models" data-type="indexterm" id="FBRseg09"/> <em>demographic-based systems.</em> Note that <em>demographic</em> in this context need not refer explicitly to personally identifiable information and can refer to the user data collected during the sign-up process. Simple examples from book recommendations might include a userâ€™s favorite genres, self-identified price preference, book-length preferences, and favorite author. Standard methods of clustering-based regression can be helpful in converting a small set of user features into recommendations for new users. For these coarse user features, building simple feature-based models like naive Bayes, can be especially effective.</p>&#13;
&#13;
<p>More generally, given user feature vectors, we can formulate a similarity measure and then user segments to make new-user recommendations. This should feel similar to feature-based recommenders, but instead of requiring usage of user features, we model the userâ€™s containment in a segment and then build our factor model from the segment to different items.</p>&#13;
&#13;
<p>One way to imagine this approach is to consider the modeling problem as estimating the following for C, a user cluster:</p>&#13;
<div data-type="equation">&#13;
<math alttext="r Subscript upper C comma x Baseline colon equals Avg left-parenthesis r Subscript bold i comma x Baseline bar bold i element-of upper C right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <msub><mi>r</mi> <mrow><mi>C</mi><mo>,</mo><mi>x</mi></mrow> </msub>&#13;
    <mo>:</mo>&#13;
    <mo>=</mo>&#13;
    <mtext>Avg</mtext>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>r</mi> <mrow><mi>ğ¢</mi><mo>,</mo><mi>x</mi></mrow> </msub>&#13;
      <mo>âˆ£</mo>&#13;
      <mi>ğ¢</mi>&#13;
      <mo>âˆˆ</mo>&#13;
      <mi>C</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Then we estimate <math alttext="upper P left-parenthesis bold j element-of upper C right-parenthesis">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mo>(</mo>&#13;
    <mi>ğ£</mi>&#13;
    <mo>âˆˆ</mo>&#13;
    <mi>C</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>, the probability a user <math alttext="bold j">&#13;
  <mi>ğ£</mi>&#13;
</math> is a member of <math alttext="upper C">&#13;
  <mi>C</mi>&#13;
</math>. We can easily imagine that we instead wish to use the probability associated with each cluster to build a bagging model, and have each cluster contributed to a weighted average rating.</p>&#13;
&#13;
<p>While these ideas may not seem like interesting extensions to what weâ€™ve built previously, in practice they can be enormously useful for fast, explainable recommendations for new users.</p>&#13;
&#13;
<p>Also note that nothing in this construction is particular to the users; we can consider the<a data-primary="dual model" data-type="indexterm" id="id812"/> <em>dual model</em> that takes the clustering to be at the level of the items and performs a similar process. Combining these models can provide the coarsest model of simply user segments to item groups, and utilizing several of these modeling approaches simultaneously can provide important and flexible models.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Tag-Based Recommenders" data-type="sect2"><div class="sect2" id="id100">&#13;
<h2>Tag-Based Recommenders</h2>&#13;
&#13;
<p>One<a data-primary="tag-based recommenders" data-type="indexterm" id="id813"/><a data-primary="recommendation systems" data-secondary="tag-based recommenders" data-type="indexterm" id="id814"/> special case of the segmentation model for item-based recommenders is a <em>tag-based recommender</em>. This is a quite common first recommender to try when you have some human labels and need to quickly turn it into a working recommender.</p>&#13;
&#13;
<p>Letâ€™s talk through a toy example: you have a personal digital wardrobe, where youâ€™ve logged many features about each article of clothing in your personal closet. You want your fashion recommender to give you suggestions for what else to wear, given that youâ€™ve selected one piece for the day. You wake up and see that itâ€™s rainy outside, so you start by choosing a cozy cardigan. The model youâ€™ve trained has found that cardigan has tags <em>outerwear</em> and <em>cozy</em>, which it knows correlate well with <em>bottoms</em> and <em>warm</em>â€”so itâ€™s likely to recommend heavier jeans today.</p>&#13;
&#13;
<p>The upside of a tag recommender is how explainable and understandable the recommendations are. The downside is that performance is directly tied to the amount of effort thatâ€™s put into tagging items.</p>&#13;
&#13;
<p>Letâ€™s discuss a slightly more involved example of a tag-based recommender that one of the authors built in collaboration with Ashraf Shaik and Eric Bunch for recommending blog posts.</p>&#13;
&#13;
<p>The goal was to warm-start the blog-post recommender by utilizing high-quality tags that classified the blogs into themes. One special aspect of this system was its rich hierarchical tagging maintained by the marketing team. In particular, each <em>tag type</em> had several values, and there were 11 tag types with up to 10 values each. Blogs had values for each tag type and sometimes had multiple tags in a single tag type for the blog. This may sound a bit complicated, but suffice it to say that each blog post could have some of the 47 tags, and the tags were further grouped into types.</p>&#13;
&#13;
<p>One of the first potential tasks is to use those tags to build a simple recommender, and we did, but doing so would mean missing a significant additional opportunity when afforded such high-quality tag data: evaluating our embeddings.</p>&#13;
&#13;
<p>First, we needed to understand how we could build user embeddings. Our plan was to average the blog embeddings a user had seen, a simple CF approach when you have a clear item embedding. Thus we wanted to train the best embedding model possible for these blogs. We started by considering models like BERT but were unsure whether the highly technical content would be meaningfully captured by our embedding model. This led us to realize that we could use the tags as a classifier dataset for our embedding. If we could test several embedding models by training a simple<a data-primary="multilayer perceptron (MLP)" data-type="indexterm" id="id815"/><a data-primary="MLP (multilayer perceptron)" data-type="indexterm" id="id816"/> multilayer perceptron (MLP) to perform<a data-primary="multilabel multiclassification" data-type="indexterm" id="id817"/> multilabel multiclassification for each tag type, where the input features were the embedding dimensions, then our embedding space would capture the content well.</p>&#13;
&#13;
<p>Some of the embedding models were of varying dimensions, and some were quite large, so we also first used a<a data-primary="Uniform Manifold Approximation and Projection (UMAP)" data-type="indexterm" id="id818"/> dimension reduction (UMAP) to a standard size before we trained the MLP. We used <a href="https://oreil.ly/rYGsU">F1 scores</a> to determine which of the embedding models led to the best classification model for tags, and we used visual inspection to ensure the groups were as weâ€™d hoped. This worked quite well and showed that some embeddings were much better than others.<a data-primary="" data-startref="FBRseg09" data-type="indexterm" id="id819"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Hybridization" data-type="sect2"><div class="sect2" id="id101">&#13;
<h2>Hybridization</h2>&#13;
&#13;
<p>You<a data-primary="feature-based recommendations" data-secondary="hybridization" data-type="indexterm" id="id820"/><a data-primary="hybridization" data-type="indexterm" id="id821"/> saw in the previous section how to blend our MF with simpler models by taking priors from the simpler models and learning how to transition away. Coarser approaches to this process of <em>hybridization</em> exist:</p>&#13;
<dl>&#13;
<dt>Weighted combinations of models</dt>&#13;
<dd>&#13;
<p>This<a data-primary="models" data-secondary="weighted combinations of" data-type="indexterm" id="id822"/> approach is incredibly powerful, and the weights can be learned in a standard Bayesian framework.</p>&#13;
</dd>&#13;
<dt>Multilevel modeling</dt>&#13;
<dd>&#13;
<p>This<a data-primary="multilevel modeling" data-type="indexterm" id="id823"/><a data-primary="models" data-secondary="multilevel modeling" data-type="indexterm" id="id824"/> approach can include learning a model to select which recommendation model should be used, and then learning models in each regime. For example, we could use a tree-based model on user features when the user has fewer than 10 historical ratings and then use MF after that. A variety of multilevel approaches exist, including<a data-primary="switching" data-type="indexterm" id="id825"/><a data-primary="cascading" data-type="indexterm" id="id826"/> <em>switching</em> and <em>cascading</em>, which correspond roughly to voting and boosting, respectively.</p>&#13;
</dd>&#13;
<dt>Feature augmentation</dt>&#13;
<dd>&#13;
<p>This<a data-primary="feature augmentation/engineering" data-type="indexterm" id="id827"/> allows multiple vectors of features to be concatenated and a larger model to be learned. By definition, if we wish to combine feature vectors with factor vectors, like those coming from a CF, we will expect substantial nullity. Learning despite that nullity allows a somewhat naive combination of the different kinds of features to be fed into the model and operated on in all regimes of user activity.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>We can combine these models in a variety of useful ways. However, we take the position that instead of more complicated combinations of several models that work well in different paradigms, we will attempt to stick to a relatively straightforward<a data-primary="architectures" data-secondary="model-service architectures" data-type="indexterm" id="id828"/> model-service architecture by doing the following:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Training the best model we can by using MF-based CF</p>&#13;
</li>&#13;
<li>&#13;
<p>Using user and item feature-based models for cold start</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Letâ€™s see why we think feature-based modeling might not be the best strategy, even if we do it via neural networks and latent factor models.<a data-primary="" data-startref="segmod09" data-type="indexterm" id="id829"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Limitations of Bilinear Models" data-type="sect1"><div class="sect1" id="id213">&#13;
<h1>Limitations of Bilinear Models</h1>&#13;
&#13;
<p>We<a data-primary="bilinear factor models" data-secondary="limitations of" data-type="indexterm" id="id830"/><a data-primary="models" data-secondary="bilinear models" data-type="indexterm" id="id831"/><a data-primary="feature-based recommendations" data-secondary="limitations of bilinear models" data-type="indexterm" id="id832"/> started this chapter by describing <em>bilinear modeling</em> approaches, and immediately you should take warningâ€”theyâ€™re linear relationships. You can immediately wonder, â€œAre there really linear relationships between the features of my users and items and the pairwise affinity?â€</p>&#13;
&#13;
<p>The answer to this question might depend on the number of features, or it might not. Either way, skepticism is appropriate, and in practice the answer is overwhelmingly <em>no</em>. You might think, â€œWell then, as it is a linear approximation, MF also cannot succeed,â€ but thatâ€™s not so clear-cut. In fact, MF suggests that the linear relationship is <em>between the latent factors</em>, not the actual features. This subtle difference makes a world of difference.</p>&#13;
&#13;
<p>One important callout before we move on to simpler ideas is that neural networks with nonlinear activation functions can be used to build feature-based methods. This domain has had some successes, but ultimately a surprising and important result is that <a href="https://oreil.ly/rFWaS">neural CF does not outperform matrix factorization</a>. This doesnâ€™t suggest that there are no useful approaches for feature-based models utilizing MLPs, but it does defray some of our worries about MF being <em>too linear</em>. So why not use more feature-based approaches?</p>&#13;
&#13;
<p>The first most obvious challenge for content-based, demographic-based, and any other feature-based method is <em>getting the features</em>. Letâ€™s consider the dual problems:</p>&#13;
<dl>&#13;
<dt>Features for users</dt>&#13;
<dd>&#13;
<p>If we want to collect features for users, we need to either ask them a series of queries or infer those features implicitly. Inferring these via exogenous signals is noisy and limited, but each query that we ask the user increases the likelihood of onboarding drop-off. When we think of user-onboarding funnels, we know that each additional prompt or question incurs another chance that the user will not complete the onboarding. This effect accumulates quickly, and without users making it through the funnel, the recommendation system wonâ€™t be very useful.</p>&#13;
</dd>&#13;
<dt>Features for items</dt>&#13;
<dd>&#13;
<p>On the flip side, creating features for items is a heavily manual task. While many businesses need to do this task to serve other purposes as well, it still incurs a significant cost in many cases. If the features are to be useful, they need to be of high quality, which incurs more debt. But most importantly, if the number of items is extremely large, the cost may quickly get out of reach. For large-scale recommendation problems, manually adding features is simply infeasible. This is where automatic feature-engineering models can help.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Another significant issue in these feature-based models is <em>separability</em> or <em>distinguishability</em>. These models are not useful if the features cannot separate the items or users well. This leads to compounding problems as the cardinality increases.</p>&#13;
&#13;
<p>Finally, in many recommendation problems, we start with the assumption that taste or preference is extremely personal. We fundamentally believe that our interest in a book will have less to do with the number of pages and publication date than how it connects with us and our personal experience (<em>our deepest apologies to anyone who bought this book based on page number and publication date</em>). CFâ€”while simple in conceptâ€”speaks better to these connections via a <em>shared experience network</em>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Counting Recommenders" data-type="sect1"><div class="sect1" id="counting">&#13;
<h1>Counting Recommenders</h1>&#13;
&#13;
<p>Here<a data-primary="recommendation systems" data-secondary="counting recommenders" data-type="indexterm" id="RScounting09"/> we will use the simplest feature type, simple counting. Counting the frequency and pairwise frequencies will provide a simple but useful set of initial models.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Return to the Most-Popular-Item Recommender" data-type="sect2"><div class="sect2" id="id102">&#13;
<h2>Return to the Most-Popular-Item Recommender</h2>&#13;
&#13;
<p>Our<a data-primary="counting recommenders" data-secondary="most-popular-item recommender (MPIR)" data-type="indexterm" id="id833"/><a data-primary="most-popular-item recommender (MPIR)" data-type="indexterm" id="id834"/> super simple scheme from before, implementing the MPIR, provided us with a convenient toy model, but what are the practical considerations of deploying an MPIR? It turns out that the MPIR provides an excellent framework for getting started on a Bayesian approximation approach to recommendations. Note that in this section, weâ€™re not even considering a personalized recommender; everything here is reward maximization across the entire user population. We follow the treatment in <a class="orm:hideurl" href="https://oreil.ly/kKulC"><em>Statistical Methods for Recommender Systems</em></a> by Deepak K. Agarwal and Bee-Chung Chen (Cambridge University Press).</p>&#13;
&#13;
<p>For the sake of simplicity, letâ€™s consider<a data-primary="CTR (click-through rate)" data-type="indexterm" id="id835"/><a data-primary="click-through rate (CTR)" data-type="indexterm" id="id836"/> <em>click-through rate</em> (<em>CTR</em>) as our simple metric to optimize. Our formulation is as follows: we have <math alttext="script upper I equals StartSet i EndSet">&#13;
  <mrow>&#13;
    <mi>â„</mi>&#13;
    <mo>=</mo>&#13;
    <mfenced close="}" open="{">&#13;
      <mi>i</mi>&#13;
    </mfenced>&#13;
  </mrow>&#13;
</math> items available to recommend and initially <em>only one time period</em> in which to do it, and weâ€™re interested in an<a data-primary="allocation plan" data-type="indexterm" id="id837"/> <em>allocation plan</em>, or a set of proportions <math alttext="x Subscript i Baseline comma sigma-summation Underscript i element-of script upper I Endscripts x Subscript i Baseline equals 1 comma">&#13;
  <mrow>&#13;
    <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
    <mo>,</mo>&#13;
    <msub><mo>âˆ‘</mo> <mrow><mi>i</mi><mo>âˆˆ</mo><mi>â„</mi></mrow> </msub>&#13;
    <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
    <mo>=</mo>&#13;
    <mn>1</mn>&#13;
    <mo>,</mo>&#13;
  </mrow>&#13;
</math> for how to recommend items. This can be seen as a very simple<a data-primary="multiarmed bandits" data-type="indexterm" id="id838"/> multiarmed bandit problem with the reward given by the following:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper R left-parenthesis bold x bold comma bold c right-parenthesis equals sigma-summation Underscript i element-of script upper I Endscripts c Subscript i Baseline asterisk left-parenthesis upper N asterisk x Subscript i Baseline right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <mi>R</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>ğ±</mi>&#13;
      <mo>,</mo>&#13;
      <mi>ğœ</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <munder><mo>âˆ‘</mo> <mrow><mi>i</mi><mo>âˆˆ</mo><mi>â„</mi></mrow> </munder>&#13;
    <msub><mi>c</mi> <mi>i</mi> </msub>&#13;
    <mo>*</mo>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>N</mi>&#13;
      <mo>*</mo>&#13;
      <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Here, <math alttext="c Subscript i">&#13;
  <msub><mi>c</mi> <mi>i</mi> </msub>&#13;
</math> represents prior distributions of CTR for each item. Itâ€™s plain to see that maximizing this reward is achieved by allocating all recommendations to the item with greatest <math alttext="p Subscript i">&#13;
  <msub><mi>p</mi> <mi>i</mi> </msub>&#13;
</math>, i.e., picking the most popular item in terms of CTR.</p>&#13;
&#13;
<p>This setup makes it obvious that if we have strong confidence in our priors, this problem seems trivial. So letâ€™s move to a case where we have a mismatch in confidence.</p>&#13;
&#13;
<p>Letâ€™s consider <em>two time periods</em>, <math alttext="upper N 0">&#13;
  <msub><mi>N</mi> <mn>0</mn> </msub>&#13;
</math> and <math alttext="upper N 1">&#13;
  <msub><mi>N</mi> <mn>1</mn> </msub>&#13;
</math>, as indicating the  number of user visits. Note that we think of 0 as the past and 1 as the future in this model. Letâ€™s assume that we offer <em>only two items</em> and that, somewhat mysteriously, for one item we have 100% confidence in its CTR in each time period: <math alttext="q 0">&#13;
  <msub><mi>q</mi> <mn>0</mn> </msub>&#13;
</math>  and <math alttext="q 1">&#13;
  <msub><mi>q</mi> <mn>1</mn> </msub>&#13;
</math> will denote these rates, respectively. In contrast, we have only priors for our second item:  <math alttext="p 0 tilde script upper P left-parenthesis theta 0 right-parenthesis">&#13;
  <mrow>&#13;
    <msub><mi>p</mi> <mn>0</mn> </msub>&#13;
    <mo>âˆ¼</mo>&#13;
    <mi>ğ’«</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>Î¸</mi> <mn>0</mn> </msub>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math>  and <math alttext="p 1 tilde script upper P left-parenthesis theta 1 right-parenthesis">&#13;
  <mrow>&#13;
    <msub><mi>p</mi> <mn>1</mn> </msub>&#13;
    <mo>âˆ¼</mo>&#13;
    <mi>ğ’«</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>Î¸</mi> <mn>1</mn> </msub>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math> will denote these rates, respectively, and we regard <math alttext="theta Subscript i">&#13;
  <msub><mi>Î¸</mi> <mi>i</mi> </msub>&#13;
</math> as a state vector. We again notate the allocations with <math alttext="x Subscript i comma t">&#13;
  <msub><mi>x</mi> <mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow> </msub>&#13;
</math>, where now the second index refers to time period. Then we can simply compute the expected number of clicks as follows:</p>&#13;
<div data-type="equation">&#13;
<math alttext="double-struck upper E left-bracket upper N 0 asterisk x 0 left-parenthesis p 0 minus q 0 right-parenthesis plus upper N 1 asterisk x 1 left-parenthesis p 1 minus q 1 right-parenthesis right-bracket plus q 0 upper N 0 plus q 1 upper N 1" display="block">&#13;
  <mrow>&#13;
    <mi>ğ”¼</mi>&#13;
    <mfenced close="]" open="[" separators="">&#13;
      <msub><mi>N</mi> <mn>0</mn> </msub>&#13;
      <mo>*</mo>&#13;
      <msub><mi>x</mi> <mn>0</mn> </msub>&#13;
      <mfenced close=")" open="(" separators="">&#13;
        <msub><mi>p</mi> <mn>0</mn> </msub>&#13;
        <mo>-</mo>&#13;
        <msub><mi>q</mi> <mn>0</mn> </msub>&#13;
      </mfenced>&#13;
      <mo>+</mo>&#13;
      <msub><mi>N</mi> <mn>1</mn> </msub>&#13;
      <mo>*</mo>&#13;
      <msub><mi>x</mi> <mn>1</mn> </msub>&#13;
      <mfenced close=")" open="(" separators="">&#13;
        <msub><mi>p</mi> <mn>1</mn> </msub>&#13;
        <mo>-</mo>&#13;
        <msub><mi>q</mi> <mn>1</mn> </msub>&#13;
      </mfenced>&#13;
    </mfenced>&#13;
    <mo>+</mo>&#13;
    <msub><mi>q</mi> <mn>0</mn> </msub>&#13;
    <msub><mi>N</mi> <mn>0</mn> </msub>&#13;
    <mo>+</mo>&#13;
    <msub><mi>q</mi> <mn>1</mn> </msub>&#13;
    <msub><mi>N</mi> <mn>1</mn> </msub>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>This is maximized by assuming a distribution for <math alttext="p 1">&#13;
  <msub><mi>p</mi> <mn>1</mn> </msub>&#13;
</math> as a function of <math alttext="x 0">&#13;
  <msub><mi>x</mi> <mn>0</mn> </msub>&#13;
</math> and <math alttext="p 0">&#13;
  <msub><mi>p</mi> <mn>0</mn> </msub>&#13;
</math>. With distributional assumptions that <math alttext="p 0">&#13;
  <msub><mi>p</mi> <mn>0</mn> </msub>&#13;
</math> is gamma distributed and <math alttext="p 1">&#13;
  <msub><mi>p</mi> <mn>1</mn> </msub>&#13;
</math> is normally distributed, we can treat this as a convex optimization problem to maximize the clicks. See <em>Statistical Methods for Recommender Systems</em> for a full treatment of the statistics.</p>&#13;
&#13;
<p>This toy example extends in both dimensions to model larger item sets and more time windows and provides us with relatively straightforward intuition about the relationship between our priors for each item and time step during this step-forward optimization.</p>&#13;
&#13;
<p>Letâ€™s put this recommender in context: weâ€™ve started with item popularity and generalized to a Bayesian recommender that learns with respect to user feedback. You might consider a recommender like this for a very trend-based recommendations context like news; popular stories are often important, but that can change rapidly, and we want to be learning from user behavior.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Correlation Mining" data-type="sect2"><div class="sect2" id="id103">&#13;
<h2>Correlation Mining</h2>&#13;
&#13;
<p>Weâ€™ve<a data-primary="counting recommenders" data-secondary="correlation mining" data-type="indexterm" id="CRcorrelation09"/><a data-primary="correlation" data-secondary="correlation mining" data-type="indexterm" id="cormin09"/> seen ways to use correlations between features of items  and recommendations, but we should not forget to use correlations between items themselves. Think back to our early discussions of cheese in <a data-type="xref" href="ch02.html#ch:user-item">ChapterÂ 2</a> (<a data-type="xref" href="ch02.html#fig:cheese-ratings">FigureÂ 2-1</a>); we said that our CF gave us a way to find mutual cheese tastes to recommend new cheeses. This was built on the notion of ratings, but we can abstract away from the ratings and simply look at the correlations of items a user chooses. You can imagine for an ecommerce bookseller that a userâ€™s choice of one book to read may be useful in recommending othersâ€”even if that user chooses not to rate the first book. We also saw this phenomena in <a data-type="xref" href="ch08.html#ch:wikipedia-e2e">ChapterÂ 8</a> as we used the co-occurrence of tokens in Wikipedia entries.</p>&#13;
&#13;
<p>We introduced the co-occurrence matrix as the multidimensional array of counts where two items, <math alttext="i">&#13;
  <mi>i</mi>&#13;
</math> and <math alttext="j">&#13;
  <mi>j</mi>&#13;
</math>, co-occur. Letâ€™s take a moment to discuss co-occurrence a bit more deeply.</p>&#13;
&#13;
<p>Co-occurrence<a data-primary="co-occurrence" data-secondary="dependent on context" data-type="indexterm" id="id839"/> is context dependent; for our Wikipedia articles, we considered co-occurrence of tokens in an article. In the case of ecommerce, co-occurrence can be two items purchased by the same user. For ads, co-occurrence can be two things that the user clicked, and so on. Mathematically, given users and items, we construct an<a data-primary="incidence vector" data-type="indexterm" id="id840"/> <em>incidence vector</em> for each user, the binary vector of one-hot encoded features for each item that they interacted with. Those vectors are stacked into a vector to yield a <math alttext="number-sign left-parenthesis u s e r s right-parenthesis times number-sign left-parenthesis i t e m s right-parenthesis">&#13;
  <mrow>&#13;
    <mo>#</mo>&#13;
    <mo>(</mo>&#13;
    <mi>u</mi>&#13;
    <mi>s</mi>&#13;
    <mi>e</mi>&#13;
    <mi>r</mi>&#13;
    <mi>s</mi>&#13;
    <mo>)</mo>&#13;
    <mo>Ã—</mo>&#13;
    <mo>#</mo>&#13;
    <mo>(</mo>&#13;
    <mi>i</mi>&#13;
    <mi>t</mi>&#13;
    <mi>e</mi>&#13;
    <mi>m</mi>&#13;
    <mi>s</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math> matrix in which each row is a user, each column is an item, and the elements equal 1 when a user-item pair has interacted.</p>&#13;
&#13;
<p>To be mathematically precise, a<a data-primary="user-item incidence structure" data-type="indexterm" id="id841"/> <em>user-item incidence structure</em> is a collection of sets of user interactions, <math alttext="StartSet y Subscript u Baseline EndSet Subscript u element-of upper U">&#13;
  <msub><mfenced close="}" open="{" separators=""><msub><mi>y</mi> <mi>u</mi> </msub></mfenced> <mrow><mi>u</mi><mo>âˆˆ</mo><mi>U</mi></mrow> </msub>&#13;
</math>, with items <math alttext="StartSet x Subscript i Baseline EndSet Subscript i element-of upper I">&#13;
  <msub><mfenced close="}" open="{" separators=""><msub><mi>x</mi> <mi>i</mi> </msub></mfenced> <mrow><mi>i</mi><mo>âˆˆ</mo><mi>I</mi></mrow> </msub>&#13;
</math>, where <math alttext="upper U">&#13;
  <mi>U</mi>&#13;
</math> indexes users and <math alttext="upper I">&#13;
  <mi>I</mi>&#13;
</math> indexes items.</p>&#13;
&#13;
<p>The associated<a data-primary="user-item incidence matrix" data-type="indexterm" id="id842"/> <em>user-item incidence matrix</em>, <math alttext="script upper U">&#13;
  <mi>ğ’°</mi>&#13;
</math>, is the binary matrix with rows indexed by sets, and columns indexed by nodes, such that elements are as follows:</p>&#13;
<div data-type="equation">&#13;
<math alttext="e Subscript y Sub Subscript u Subscript comma x Sub Subscript i Subscript Baseline equals StartLayout Enlarged left-brace 1st Row 1st Column 1 2nd Column x Subscript i Baseline element-of y Subscript u Baseline 2nd Row 1st Column 0 2nd Column otherwise EndLayout" display="block">&#13;
  <mrow>&#13;
    <msub><mi>e</mi> <mrow><msub><mi>y</mi> <mi>u</mi> </msub><mo>,</mo><msub><mi>x</mi> <mi>i</mi> </msub></mrow> </msub>&#13;
    <mo>=</mo>&#13;
    <mfenced close="" open="{" separators="">&#13;
      <mtable>&#13;
        <mtr>&#13;
          <mtd columnalign="left">&#13;
            <mn>1</mn>&#13;
          </mtd>&#13;
          <mtd columnalign="left">&#13;
            <mrow>&#13;
              <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
              <mo>âˆˆ</mo>&#13;
              <msub><mi>y</mi> <mi>u</mi> </msub>&#13;
            </mrow>&#13;
          </mtd>&#13;
        </mtr>&#13;
        <mtr>&#13;
          <mtd columnalign="left">&#13;
            <mn>0</mn>&#13;
          </mtd>&#13;
          <mtd columnalign="left">&#13;
            <mtext>otherwise</mtext>&#13;
          </mtd>&#13;
        </mtr>&#13;
      </mtable>&#13;
    </mfenced>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>The<a data-primary="co-occurrence" data-secondary="computing of" data-type="indexterm" id="id843"/> <em>co-occurrence of</em> <math alttext="x Subscript a">&#13;
  <msub><mi>x</mi> <mi>a</mi> </msub>&#13;
</math> <em>and</em> <math alttext="x Subscript b">&#13;
  <msub><mi>x</mi> <mi>b</mi> </msub>&#13;
</math> is the order of the set <math alttext="StartSet y Subscript u Baseline bar x Subscript a Baseline element-of y Subscript u Baseline and x Subscript b Baseline element-of y Subscript u Baseline EndSet">&#13;
  <mfenced close="}" open="{" separators="">&#13;
    <msub><mi>y</mi> <mi>u</mi> </msub>&#13;
    <mo>âˆ£</mo>&#13;
    <msub><mi>x</mi> <mi>a</mi> </msub>&#13;
    <mo>âˆˆ</mo>&#13;
    <msub><mi>y</mi> <mi>u</mi> </msub>&#13;
    <mspace width="4.pt"/>&#13;
    <mtext>and</mtext>&#13;
    <mspace width="4.pt"/>&#13;
    <msub><mi>x</mi> <mi>b</mi> </msub>&#13;
    <mo>âˆˆ</mo>&#13;
    <msub><mi>y</mi> <mi>u</mi> </msub>&#13;
  </mfenced>&#13;
</math>. We can also write that as a matrix that can be computed via a simple formula; let <math alttext="upper C Subscript script upper I">&#13;
  <msub><mi>C</mi> <mi>â„</mi> </msub>&#13;
</math> be the co-occurrences matrixâ€”i.e., the matrix with rows and columns indexed by <math alttext="StartSet x Subscript i Baseline EndSet Subscript i element-of upper I">&#13;
  <msub><mfenced close="}" open="{" separators=""><msub><mi>x</mi> <mi>i</mi> </msub></mfenced> <mrow><mi>i</mi><mo>âˆˆ</mo><mi>I</mi></mrow> </msub>&#13;
</math> and with elements that are the co-occurrences of the indices. Then we use the following:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper C Subscript script upper I Baseline equals script upper I Superscript upper T Baseline asterisk script upper I" display="block">&#13;
  <mrow>&#13;
    <msub><mi>C</mi> <mi>â„</mi> </msub>&#13;
    <mo>=</mo>&#13;
    <msup><mi>â„</mi> <mi>T</mi> </msup>&#13;
    <mo>*</mo>&#13;
    <mi>â„</mi>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id844">&#13;
<h1>Higher-Order Co-occurrences</h1>&#13;
<p>You<a data-primary="co-occurrence" data-secondary="higher-order" data-type="indexterm" id="id845"/> could imagine further generalizing this recommender to aggregate across several items the user has seen. In practice, you could consider the last five items the user has interacted with and then compute the conditional-MPIR recommendations for each and union them together.</p>&#13;
&#13;
<p>Alternatively, you could generalize to <em>higher-order</em> co-occurrences. In other words, instead of pairs of items that co-occur, look at triples, quadruples, or more. To read one approach to this generalization, check out <a href="https://oreil.ly/ta8HU">â€œHigher Order Co-occurrence Tensors for Hypergraphs via Face-Splittingâ€</a> by one of the authors.</p>&#13;
</div></aside>&#13;
&#13;
<p>As mentioned in <a data-type="xref" href="ch08.html#CaB">â€œCustomers Also Boughtâ€</a>, we can build a new variant of our MPIR by considering the rows or columns of the co-occurence matrix. The<a data-primary="conditional MPIR" data-type="indexterm" id="id846"/> <em>conditional MPIR</em> is the recommender that returns the max of the elements in the row corresponding to <math alttext="x Subscript i">&#13;
  <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
</math>, given the userâ€™s last interaction was the item <math alttext="x Subscript i">&#13;
  <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
</math>.</p>&#13;
&#13;
<p>In practice, we often think of the row corresponding to <math alttext="x Subscript i">&#13;
  <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
</math> as<a data-primary="basis vector" data-type="indexterm" id="id847"/> a <em>basis vector</em>, i.e., a vector <math alttext="q Subscript x Sub Subscript i">&#13;
  <msub><mi>q</mi> <msub><mi>x</mi> <mi>i</mi> </msub> </msub>&#13;
</math> with one nonzero element in the <math alttext="x Subscript i">&#13;
  <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
</math>th position:</p>&#13;
<div data-type="equation">&#13;
<math alttext="q Subscript x Sub Subscript i Subscript comma j Baseline equals StartLayout Enlarged left-brace 1st Row 1st Column 1 2nd Column j equals x Subscript i Baseline 2nd Row 1st Column 0 2nd Column otherwise EndLayout equals Start 5 By 1 Matrix 1st Row  0 2nd Row  vertical-ellipsis 3rd Row  1 4th Row  vertical-ellipsis 5th Row  0 EndMatrix" display="block">&#13;
  <mrow>&#13;
    <msub><mi>q</mi> <mrow><msub><mi>x</mi> <mi>i</mi> </msub><mo>,</mo><mi>j</mi></mrow> </msub>&#13;
    <mo>=</mo>&#13;
    <mfenced close="" open="{" separators="">&#13;
      <mtable>&#13;
        <mtr>&#13;
          <mtd columnalign="left">&#13;
            <mn>1</mn>&#13;
          </mtd>&#13;
          <mtd columnalign="left">&#13;
            <mrow>&#13;
              <mi>j</mi>&#13;
              <mo>=</mo>&#13;
              <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
            </mrow>&#13;
          </mtd>&#13;
        </mtr>&#13;
        <mtr>&#13;
          <mtd columnalign="left">&#13;
            <mn>0</mn>&#13;
          </mtd>&#13;
          <mtd columnalign="left">&#13;
            <mtext>otherwise</mtext>&#13;
          </mtd>&#13;
        </mtr>&#13;
      </mtable>&#13;
    </mfenced>&#13;
    <mo>=</mo>&#13;
    <mfenced close="]" open="[" separators="">&#13;
      <mtable>&#13;
        <mtr>&#13;
          <mtd>&#13;
            <mn>0</mn>&#13;
          </mtd>&#13;
        </mtr>&#13;
        <mtr>&#13;
          <mtd>&#13;
            <mo>â‹®</mo>&#13;
          </mtd>&#13;
        </mtr>&#13;
        <mtr>&#13;
          <mtd>&#13;
            <mn>1</mn>&#13;
          </mtd>&#13;
        </mtr>&#13;
        <mtr>&#13;
          <mtd>&#13;
            <mo>â‹®</mo>&#13;
          </mtd>&#13;
        </mtr>&#13;
        <mtr>&#13;
          <mtd>&#13;
            <mn>0</mn>&#13;
          </mtd>&#13;
        </mtr>&#13;
      </mtable>&#13;
    </mfenced>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p class="less_space pagebreak-before">Then we can consider maxâ€”or even softmaxâ€”of the preceding dot products:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper C Subscript script upper I Baseline equals script upper I Superscript upper T Baseline dot script upper I asterisk q Subscript x Sub Subscript i" display="block">&#13;
  <mrow>&#13;
    <msub><mi>C</mi> <mi>â„</mi> </msub>&#13;
    <mo>=</mo>&#13;
    <msup><mi>â„</mi> <mi>T</mi> </msup>&#13;
    <mo>Â·</mo>&#13;
    <mi>â„</mi>&#13;
    <mo>*</mo>&#13;
    <msub><mi>q</mi> <msub><mi>x</mi> <mi>i</mi> </msub> </msub>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>This yields the vector of co-occurrence counts between <math alttext="x Subscript i">&#13;
  <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
</math> and each other item. Here we frequently will call <math alttext="q Subscript x Sub Subscript i">&#13;
  <msub><mi>q</mi> <msub><mi>x</mi> <mi>i</mi> </msub> </msub>&#13;
</math> a <em>query</em> to indicate that itâ€™s the input to our co-occurrence recommendation model.</p>&#13;
<div data-type="warning" epub:type="warning"><h1>How Do You Store This Data?</h1>&#13;
<p>We<a data-primary="co-occurrence" data-secondary="representing sparse matrices" data-type="indexterm" id="id848"/> can think about co-occurrence data in a <em>lot</em> of ways. The main reason is because we expect that co-occurrences for recommendation systems are incredibly sparse. This means that the preceding method of matrix multiplicationâ€”which is approximately <math alttext="upper O left-parenthesis n cubed right-parenthesis">&#13;
  <mrow>&#13;
    <mi>O</mi>&#13;
    <mo>(</mo>&#13;
    <msup><mi>n</mi> <mn>3</mn> </msup>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>â€”is going to be relatively slow to compute fewer nonzero entries. Because of this and concerns about storing huge matrices full of zeros, computer scientists have taken seriously the problem of representing sparse matrices.</p>&#13;
&#13;
<p><a href="https://oreil.ly/c3Gif">Max Grossman</a> claims there are 101 ways, but in practice there are only a few. JAX supports <a href="https://oreil.ly/AB5vk">BCOO</a>, or<a data-primary="batched coordinate format" data-type="indexterm" id="id849"/> <em>batched coordinate format</em>, which is essentially a list of coordinates for nonzero elements, and then what those elements are.</p>&#13;
&#13;
<p>In our binary case of interactions, those are 1s, and for the co-occurrence matrix, those are the counts. The structure of these<a data-primary="" data-startref="cormin09" data-type="indexterm" id="id850"/><a data-primary="" data-startref="CRcorrelation09" data-type="indexterm" id="id851"/> matrices can be written as follows:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="p">{</code>&#13;
  <code class="s1">'indices'</code><code class="p">:</code> <code class="n">indices</code><code class="p">,</code>&#13;
  <code class="s1">'values'</code><code class="p">:</code> <code class="n">values</code><code class="p">,</code>&#13;
  <code class="s1">'shape'</code><code class="p">:</code> <code class="p">[</code><code class="n">user_dim</code><code class="p">,</code> <code class="n">items_dim</code><code class="p">]</code>&#13;
<code class="p">}</code></pre>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Pointwise Mutual Information via Co-occurrences" data-type="sect2"><div class="sect2" id="id104">&#13;
<h2>Pointwise Mutual Information via Co-occurrences</h2>&#13;
&#13;
<p>An<a data-primary="counting recommenders" data-secondary="pointwise mutual information via co-occurrences" data-type="indexterm" id="id852"/><a data-primary="pointwise mutual information (PMI)" data-type="indexterm" id="id853"/><a data-primary="co-occurrence" data-secondary="pointwise mutual information via" data-type="indexterm" id="id854"/> early recommendation system for articles used <em>pointwise mutual information</em>, or PMI, which is closely related to co-occurrences. In the context of NLP, PMI attempts to express how much more frequent co-occurrence is than random chance. Given what weâ€™ve seen before, you can think of this as a normalized co-occurrences model. Computational linguists frequently use PMI as an estimator for word similarity or word meaning following from the distributional hypothesis:</p>&#13;
<blockquote>&#13;
<p>You shall know a word by the company it keeps.</p>&#13;
<p data-type="attribution">John R. Firth, <cite>British linguist</cite></p>&#13;
</blockquote>&#13;
&#13;
<p>In the context of recommendation ranking, items with very high PMI are said to have a highly meaningful co-occurrence. This can thus be used as an estimator for<a data-primary="complementary items, estimating" data-type="indexterm" id="id855"/> <em>complementary</em> items: given youâ€™ve interacted with one of them, you should interact with the other.</p>&#13;
&#13;
<p>PMI is computed for two items, <math alttext="x Subscript i Baseline comma x Subscript j Baseline">&#13;
  <mrow>&#13;
    <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
    <mo>,</mo>&#13;
    <msub><mi>x</mi> <mi>j</mi> </msub>&#13;
  </mrow>&#13;
</math>, via the following:</p>&#13;
<div data-type="equation">&#13;
<math alttext="StartFraction p left-parenthesis x Subscript i Baseline comma x Subscript j Baseline right-parenthesis Over p left-parenthesis x Subscript i Baseline right-parenthesis asterisk p left-parenthesis x Subscript j Baseline right-parenthesis EndFraction equals StartFraction left-parenthesis upper C Subscript script upper I Baseline right-parenthesis Subscript x Sub Subscript i Subscript comma x Sub Subscript j Subscript Baseline asterisk number-sign left-parenthesis normal t normal o normal t normal a normal l normal i normal n normal t normal e normal r normal a normal c normal t normal i normal o normal n normal s right-parenthesis Over number-sign left-parenthesis x Subscript i Baseline right-parenthesis asterisk number-sign left-parenthesis x Subscript j Baseline right-parenthesis EndFraction" display="block">&#13;
  <mrow>&#13;
    <mstyle displaystyle="true" scriptlevel="0">&#13;
      <mfrac><mrow><mi>p</mi><mo>(</mo><msub><mi>x</mi> <mi>i</mi> </msub><mo>,</mo><msub><mi>x</mi> <mi>j</mi> </msub><mo>)</mo></mrow> <mrow><mi>p</mi><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi> </msub><mo>)</mo></mrow><mo>*</mo><mi>p</mi><mrow><mo>(</mo><msub><mi>x</mi> <mi>j</mi> </msub><mo>)</mo></mrow></mrow></mfrac>&#13;
    </mstyle>&#13;
    <mo>=</mo>&#13;
    <mstyle displaystyle="true" scriptlevel="0">&#13;
      <mfrac><mrow><msub><mfenced close=")" open="(" separators=""><msub><mi>C</mi> <mi>â„</mi> </msub></mfenced> <mrow><msub><mi>x</mi> <mi>i</mi> </msub><mo>,</mo><msub><mi>x</mi> <mi>j</mi> </msub></mrow> </msub><mo>*</mo><mo>#</mo><mrow><mo>(</mo><mi> total </mi><mspace width="0.277778em"/><mi> interactions </mi><mo>)</mo></mrow></mrow> <mrow><mo>#</mo><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi> </msub><mo>)</mo></mrow><mo>*</mo><mo>#</mo><mrow><mo>(</mo><msub><mi>x</mi> <mi>j</mi> </msub><mo>)</mo></mrow></mrow></mfrac>&#13;
    </mstyle>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>The PMI calculation allows us to modify all our work on co-occurrence to a more normalized computation, and thus is a bit more meaningful. This process is related to the GloVE model we learned in <a data-type="xref" href="ch08.html#Glove">â€œGloVE Model Definitionâ€</a>. The negative PMI values allow us to understand when two things are not often  witnessed together.</p>&#13;
&#13;
<p>These PMI calculations can be used to recommend <em>another item in a cart</em> when an item has been added and you find those with very high PMI. It can be used as a retrieval method by looking at the set of items a user has already interacted with and finding items that have high PMI with several of them.</p>&#13;
&#13;
<p>Letâ€™s look at how to turn co-occurrences into other similarity measures.</p>&#13;
<div data-type="warning" epub:type="warning"><h1>Is PMI a Distance Measurement?</h1>&#13;
<p>A good question to consider at this point is â€œIs PMI between two objects a measurement of distance? Can I define similarity directly as the PMI between two items, and thus yield a convenient geometry in which to  consider distances?â€ The answer is no. Recall that one of the axioms of a distance function is the triangle inequality; a useful exercise is to consider why the triangle inequality would not be true for PMI.</p>&#13;
&#13;
<p>But all is not lost. In the next section, weâ€™ll show you how to formulate some important similarity measurements from co-occurrence structures. Further, in the next chapter, weâ€™ll discuss Wasserstein distance, which allows you to turn the co-occurrence counts into a distance metric directly. The key difference will be considering the co-occurrence counts of all other items simultaneously as a distribution.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Similarity from Co-occurrence" data-type="sect2"><div class="sect2" id="sim_measures">&#13;
<h2>Similarity from Co-occurrence</h2>&#13;
&#13;
<p>Earlier, we<a data-primary="co-occurrence" data-secondary="similarity from" data-type="indexterm" id="COsimilar09"/><a data-primary="similarity" data-secondary="from co-occurrence" data-secondary-sortas="co-occurrence" data-type="indexterm" id="Scooc09"/> discussed similarity measures and how they come from the Pearson correlation. The Pearson correlation is a special case of similarity when we have explicit ratings, so letâ€™s instead look at when we donâ€™t.</p>&#13;
&#13;
<p>Consider incidence sets associated to users, <math alttext="StartSet y Subscript u Baseline EndSet Subscript u element-of upper U">&#13;
  <msub><mfenced close="}" open="{" separators=""><msub><mi>y</mi> <mi>u</mi> </msub></mfenced> <mrow><mi>u</mi><mo>âˆˆ</mo><mi>U</mi></mrow> </msub>&#13;
</math>, as we define three distance metrics:</p>&#13;
<dl>&#13;
<dt>Jaccard similarity, <math alttext="upper J a c left-parenthesis minus right-parenthesis">&#13;
  <mrow>&#13;
    <mi>J</mi>&#13;
    <mi>a</mi>&#13;
    <mi>c</mi>&#13;
    <mo>(</mo>&#13;
    <mo>-</mo>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math></dt>&#13;
<dd>&#13;
<p>The<a data-primary="Jaccard similarity" data-type="indexterm" id="id856"/> ratio of shared items by two users to the total items those users have interacted with</p>&#13;
</dd>&#13;
<dt>SÃ¸rensen-Dice similarity, <math alttext="upper D upper S upper C left-parenthesis minus right-parenthesis">&#13;
  <mrow>&#13;
    <mi>D</mi>&#13;
    <mi>S</mi>&#13;
    <mi>C</mi>&#13;
    <mo>(</mo>&#13;
    <mo>-</mo>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math></dt>&#13;
<dd>&#13;
<p>Twice<a data-primary="SÃ¸rensen-Dice similarity" data-type="indexterm" id="id857"/> the ratio of shared items by two users to the sum of total items each user has interacted with</p>&#13;
</dd>&#13;
<dt>Cosine similarity, <math alttext="upper C o s i m left-parenthesis minus right-parenthesis">&#13;
  <mrow>&#13;
    <mi>C</mi>&#13;
    <mi>o</mi>&#13;
    <mi>s</mi>&#13;
    <mi>i</mi>&#13;
    <mi>m</mi>&#13;
    <mo>(</mo>&#13;
    <mo>-</mo>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math></dt>&#13;
<dd>&#13;
<p>The ratio<a data-primary="cosine similarity" data-type="indexterm" id="id858"/><a data-primary="similarity" data-secondary="cosine similarity" data-type="indexterm" id="id859"/> of shared items by two users to the product of total items each user has interacted with</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>These are all very related metrics with slightly different strengths. Here are some points to consider:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Jaccard similarity is a real distance metric that has some nice properties for geometry; neither of the other two is.</p>&#13;
</li>&#13;
<li>&#13;
<p>All three are on the interval <math alttext="left-parenthesis 0 comma 1 right-parenthesis">&#13;
  <mfenced close=")" open="(" separators="">&#13;
    <mn>0</mn>&#13;
    <mo>,</mo>&#13;
    <mn>1</mn>&#13;
  </mfenced>&#13;
</math>, but youâ€™ll often see cosine extended to <math alttext="left-parenthesis negative 1 comma 1 right-parenthesis">&#13;
  <mfenced close=")" open="(" separators="">&#13;
    <mo>-</mo>&#13;
    <mn>1</mn>&#13;
    <mo>,</mo>&#13;
    <mn>1</mn>&#13;
  </mfenced>&#13;
</math> by including negative ratings.</p>&#13;
</li>&#13;
<li>&#13;
<p>Cosine can accommodate â€œthumbs-up/thumbs-downâ€ by merely extending all interactions to have a polarity of <math alttext="plus-or-minus 1">&#13;
  <mrow>&#13;
    <mo>Â±</mo>&#13;
    <mn>1</mn>&#13;
  </mrow>&#13;
</math>.</p>&#13;
</li>&#13;
<li>&#13;
<p>Cosine can accommodate â€œmultiple interactionsâ€ if you allow the vectors to be nonbinary and count the number of times a user interacts with an item.</p>&#13;
</li>&#13;
<li>&#13;
<p>Jaccard and Dice are related by the simple equation <math alttext="upper S equals 2 upper J slash left-parenthesis 1 plus upper J right-parenthesis">&#13;
  <mrow>&#13;
    <mi>S</mi>&#13;
    <mo>=</mo>&#13;
    <mn>2</mn>&#13;
    <mi>J</mi>&#13;
    <mo>/</mo>&#13;
    <mo>(</mo>&#13;
    <mn>1</mn>&#13;
    <mo>+</mo>&#13;
    <mi>J</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>, and you can easily compute one from the other.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Notice that weâ€™ve defined all these similarity measures between users. Weâ€™ll show in the next section how to extend these definitions to items and how to turn these into recommendations.<a data-primary="" data-startref="COsimilar09" data-type="indexterm" id="id860"/><a data-primary="" data-startref="Scooc09" data-type="indexterm" id="id861"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Similarity-Based Recommendations" data-type="sect2"><div class="sect2" id="id106">&#13;
<h2>Similarity-Based Recommendations</h2>&#13;
&#13;
<p>In<a data-primary="similarity" data-secondary="similarity-based recommendations" data-type="indexterm" id="id862"/> each of the preceding distance metrics, weâ€™ve defined a similarity measure, but we havenâ€™t yet discussed how similarity measures turn into recommendations. As we discussed in <a data-type="xref" href="ch03.html#nearest_neighbors">â€œNearest Neighborsâ€</a>, we utilize similarity measures in our retrieval step; we wish to find a space in that items that are <em>close</em> to one another are good recommendations. In the context of ranking, our similarity measure can be used directly to order the recommendations in terms of how likely the recommendation is relevant. In the next chapter, weâ€™ll talk more about metrics of relevance.</p>&#13;
&#13;
<p>In the preceding section, we looked at three similarity scores, but we need to expand our notion of the relevant sets for these measures. Letâ€™s consider Jaccard similarity as a prototype.</p>&#13;
&#13;
<p>Given a user <math alttext="y Subscript u">&#13;
  <msub><mi>y</mi> <mi>u</mi> </msub>&#13;
</math> and an unseen item <math alttext="x Subscript i">&#13;
  <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
</math>, letâ€™s  ask, â€œWhat is the Jaccard similarity between this user and item?â€ Letâ€™s remember that Jaccard similarity is the similarity between two sets, and in the definition those sets were both <em>incidence sets of usersâ€™ interactions</em>. Here are three ways to use this approach for recommendations:</p>&#13;
<dl>&#13;
<dt>User-user</dt>&#13;
<dd>&#13;
<p>Using<a data-primary="user similarity" data-secondary="user-user recommendations" data-type="indexterm" id="id863"/> our preceding definition, find the <math alttext="k">&#13;
  <mi>k</mi>&#13;
</math> users with maximum Jaccard similarity. Compute the percentage of these users who have interacted with <math alttext="x Subscript i">&#13;
  <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
</math>. You may also wish to normalize this by popularity of the item <math alttext="x Subscript i">&#13;
  <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
</math>.</p>&#13;
</dd>&#13;
<dt>Item-item</dt>&#13;
<dd>&#13;
<p>Compute<a data-primary="item-item recommendations" data-type="indexterm" id="id864"/> the set of users that each item has interacted with, and compute the <math alttext="k">&#13;
  <mi>k</mi>&#13;
</math> items most similar to <math alttext="x Subscript i">&#13;
  <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
</math> with respect to Jaccard similarity of these item-user incidence sets. Compute the percentage of these items that are in <math alttext="y Subscript u">&#13;
  <msub><mi>y</mi> <mi>u</mi> </msub>&#13;
</math>â€™s set of interactions. You may also wish to normalize this by total interactions of <math alttext="y Subscript u">&#13;
  <msub><mi>y</mi> <mi>u</mi> </msub>&#13;
</math> or the popularity of the similar items.</p>&#13;
</dd>&#13;
<dt>User-item</dt>&#13;
<dd>&#13;
<p>Compute<a data-primary="user-item ratings" data-secondary="user-item recommendations" data-type="indexterm" id="id865"/> the user <math alttext="y Subscript u">&#13;
  <msub><mi>y</mi> <mi>u</mi> </msub>&#13;
</math>â€™s set of items theyâ€™ve interacted with, and the set of items co-occurring with <math alttext="x Subscript i">&#13;
  <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
</math> in any userâ€™s incidence set of interaction. Compute the Jaccard similarity between these two sets.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Frequently in designing ranking systems, we specify the <em>query</em>, which refers to which nearest neighbors youâ€™re looking for. We then specify how you use those neighbors to yield a recommendation. The items that may become the recommendation are the candidates, but as you saw in the preceding example, the neighbors may not be the candidates themselves. An additional complication is that you usually need to compute many candidate scores simultaneously, which requires optimized computations that weâ€™ll see in <a data-type="xref" href="ch16.html#acceleration_structures">ChapterÂ 16</a>.<a data-primary="" data-startref="RScounting09" data-type="indexterm" id="id866"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id312">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>In this chapter, weâ€™ve begun to dig deeper into notions of similarityâ€”building on our intuition from retrieval that usersâ€™ preferences might be captured by the interactions theyâ€™ve already demonstrated.</p>&#13;
&#13;
<p>We started out with simple models based on features about users and built linear models relating them to our target outcomes. We then combined those simple models with other aspects of feature modeling and hybrid systems.</p>&#13;
&#13;
<p class="less_space pagebreak-before">Next, we moved into discussing countingâ€”in particular, counting the co-occurrence of items, users, or baskets. By looking at frequent co-occurrence, we can build models that capture â€œIf you liked <em>a</em>, you may like <em>b</em>.â€ These models are simple to understand, but we can use these basic correlation structures to build similarity measures, and thus latent spaces where ANN-based retrieval can yield good candidates for recommendations.</p>&#13;
&#13;
<p>One point that you may have noticed about the featurization of all the items and the building of our co-occurrence matrices is that the number of features is astronomically largeâ€”one dimension for each item! This is the area of investigation weâ€™ll tackle in the next chapter: how to reduce the dimensionality of your <em>latent space</em>.</p>&#13;
</div></section>&#13;
</div></section></body></html>