["```py\n\t'page_view_id': 'd15220a8e9a8e488162af3120b4396a9ca1',\n\t'anonymous_id': 'e455d516-3c08-4b6f-ab12-77f930e2661f',\n\t'view_tstamp': 2020-10-29 17:44:41+00:00,\n\t'page_url': 'https://bookshop.org/lists/best-sellers-of-the-week',\n\t'page_url_host': 'bookshop.org',\n\t'page_url_path': '/lists/bookshop-org-best-sellers-of-the-week',\n\t'page_title': 'Best Sellers of the Week',\n\t'page_url_query': None,\n\t'authenticated_user_id': 15822493.0,\n\t'url_report_id': 511629659.0,\n\t'is_profile_page': False,\n\t'product_viewed': 'list',\n```", "```py\nuser_item_view_counts_qry = \"\"\"\nSELECT\n page_views.authenticated_user_id\n , page_views.page_url_path\n , COUNT(DISTINCT page_views.page_view_id) AS count_views\n\nFROM prod.page_views\nJOIN prod.dim_users\n ON page_views.authenticated_user_id = dim_users.authenticated_user_id\n\nWHERE DATE page_views.view_tstamp >= '2017-01-01'\n AND dim_users.country_code = 'US'\n\nGROUP BY\n page_views.authenticated_user_id\n , page_views.page_url_path\n\nORDER BY 3, page_views.authenticated_user_id\n\"\"\"\n\nuser_item_view_counts_sdf = spark.sql(user_item_view_counts_qry)\n```", "```py\nitem_popularity_qry = \"\"\"\nSELECT\n page_views.page_url_path\n , COUNT(DISTINCT page_views.authenticated_user_id) AS count_viewers\n\nFROM prod.page_views\nJOIN prod.dim_users\n ON page_views.authenticated_user_id = dim_users.authenticated_user_id\n\nWHERE DATE page_views.view_tstamp >= '2017-01-01'\n AND dim_users.country_code = 'US'\n\nGROUP BY\n page_views.page_url_path\n\nORDER BY 2\n\"\"\"\n\nitem_view_counts_sdf = spark.sql(item_popularity_qry)\n```", "```py\nuser_item_ratings_qry = \"\"\"\nSELECT\n book_ratings.book_id\n book_ratings.user_id\n , book_ratings.rating_value\n , book_ratings.rating_tstamp\n\nFROM prod.book_ratings\nJOIN prod.dim_users\n ON book_ratings.user_id = dim_users.user_id\nJOIN prod.dim_books\n ON book_ratings.book_id = dim_books.dim_books\n\nWHERE\n DATE book_ratings.rating_tstamp\n BETWEEN (DATE '2017-01-01')\n AND (CAST(current_timestamp() as DATE)\n AND book_ratings.rating_value IS NOT NULL\n AND dim_users.country_code = 'US'\n AND dim_books.book_active\n\"\"\"\n\nuser_item_ratings_sdf = spark.sql(user_item_ratings_qry)\n```", "```py\nfrom pyspark.sql.window import Window\n\nwindows = Window().partitionBy(\n\t['book_id', 'user_id']\n).orderBy(\n\tcol(\"rating_tstamp\").desc()\n)\n\nuser_item_ratings_sdf.withColumn(\n\t\"current_rating\",\n\tfirst(\n\t\tuser_item_ratings_sdf(\"rating_tstamp\")\n\t).over(windows).as(\"max_rating_tstamp\")\n).filter(\"rating_tstamp = max_rating_tstamp\")\n```", "```py\nfrom pyspark.sql.window import Window\nfrom pyspark.sql import functions as F\n\nuser_partition = Window.partitionBy('user_id')\n\nuser_item_ratings_sdf = user_item_ratings_sdf.withColumn(\n\t\"user_average_rating\",\n\tF.avg(\"current_rating\").over(user_partition)\n)\n\nuser_item_ratings_sdf = user_item_ratings_sdf.withColumn(\n\t\"rating_deviation_from_user_mean\",\n\tF.col(\"current_rating\") - F.col(\"user_average_rating\")\n)\n```", "```py\n+-------+-------+------------+-------------+\n|book_id|user_id|rating_value|rating_tstamp|\n+-------+-------+------------+-------------+\n+-------------+-------------------+-------------------------------+\ncurrent_rating|user_average_rating|rating_deviation_from_user_mean|\n+-------------+-------------------+-------------------------------+\n```", "```py\nuser_pair_item_rating_deviations = user_item_ratings_sdf.alias(\"left_ratings\")\n.join(user_item_ratings_sdf.alias(\"right_ratings\"),\n  (\nF.col(\"left_ratings.book_id\") == F.col(\"right_ratings.book_id\") &\\\nF.col(\"left_ratings.user_id\") != F.col(\"right_ratings.user_id\")\n\t),\n\t\"inner\"\n).select(\n\tF.col(\"left_ratings.book_id\"),\n\tF.col(\"left_ratings.user_id\").alias(\"user_id_1\"),\n\tF.col(\"right_ratings.user_id\").alias(\"user_id_2\"),\n  F.col(\"left_ratings.rating_deviation_from_user_mean\").alias(\"dev_1\"),\n  F.col(\"right_ratings.rating_deviation_from_user_mean\").alias(\"dev_2\")\n).withColumn(\n\t'dev_product',\n\tF.col(\"dev_1\")*F.col(\"dev_2\")\n)\n\nuser_similarities_sdf = user_pair_item_rating_deviations.groupBy(\n\t\"user_id_1\", \"user_id_2\"\n).agg(\n\tsum('dev_product').alias(\"dev_product_sum\"),\n\tsum(F.pow(F.col(\"dev_1\"),2)).alias(\"sum_of_sqrd_devs_1\"),\n\tsum(F.pow(F.col(\"dev_2\"),2)).alias(\"sum_of_sqrd_devs_2\")\n).withColumn(\n\t\"user_similarity\",\n\t(\n\t\tF.col(\"dev_product_sum\") / (\n\t\t\tF.sqrt(F.col(\"sum_of_sqrd_devs_2\")) *\n\t\t\tF.sqrt(F.col(\"sum_of_sqrd_devs_2\"))\n\t\t)\n\t)\n)\n```", "```py\nparams = {\n         'batch_size': _,\n         'shuffle': _,\n         'num_workers': _\n}\n\ntraining_generator = torch.utils.data.DataLoader(training_set, params)\n\nvalidation_generator = torch.utils.data.DataLoader(validation_set, params)\n\n// Loop over epochs\nfor epoch in range(max_epochs):\n    // Training\n    for local_batch, local_labels in training_generator:\n\n        // Model computations\n        [...]\n\n    // Validation\n    with torch.set_grad_enabled(False):\n        for local_batch, local_labels in validation_generator:\n\n            // Model computations\n            [...]\n```"]