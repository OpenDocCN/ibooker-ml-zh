<html><head></head><body><section data-pdf-bookmark="Chapter 1. Unsupervised Learning in the Machine Learning Ecosystem" data-type="chapter" epub:type="chapter"><div class="chapter" id="Chapter_1">&#13;
<h1><span class="label">Chapter 1. </span>Unsupervised Learning in the Machine Learning Ecosystem</h1>&#13;
&#13;
<blockquote>&#13;
<p>Most of human and animal learning is unsupervised learning. If intelligence was a cake, unsupervised learning would be the cake, supervised learning would be the icing on the cake, and reinforcement learning would be the cherry on the cake. We know how to make the icing and the cherry, but we don’t know how to make the cake. We need to solve the unsupervised learning problem before we can even think of getting to true AI.</p>&#13;
<p data-type="attribution">Yann LeCun</p>&#13;
</blockquote>&#13;
&#13;
<p>In<a data-primary="machine learning" data-secondary="ecosystem for" data-type="indexterm" id="MLeco01"/><a data-primary="machine learning" data-see="also machine learning example project" data-type="indexterm" id="idm140637565391600"/> this chapter, we will explore the difference between a rules-based system and machine learning, the difference between supervised learning and unsupervised learning, and the relative strengths and weaknesses of each.</p>&#13;
&#13;
<p>We will also cover many popular supervised learning algorithms and unsupervised learning algorithms and briefly examine how semisupervised learning and reinforcement learning fit into the mix.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Basic Machine Learning Terminology" data-type="sect1"><div class="sect1" id="idm140637565389504">&#13;
<h1>Basic Machine Learning Terminology</h1>&#13;
&#13;
<p>Before<a data-primary="terminology" data-type="indexterm" id="idm140637565387968"/> we delve into the different types of machine learning, let’s take a look at a simple and commonly used machine learning example to help make the concepts we introduce tangible: the email spam filter. We need to build a simple program that takes in emails and correctly classifies them as either “spam” or “not spam.” This is a straightforward classification problem.</p>&#13;
&#13;
<p>Here’s<a data-primary="input variables" data-type="indexterm" id="idm140637565386320"/><a data-primary="output variables" data-type="indexterm" id="idm140637565385264"/><a data-primary="features" data-type="indexterm" id="idm140637565384592"/><a data-primary="predictors" data-type="indexterm" id="idm140637565383920"/><a data-primary="independent variables" data-type="indexterm" id="idm140637565383248"/> a bit of machine learning terminology as a refresher: the <em>input variables</em> into this problem are the text of the emails. These input variables are also known as <em>features</em> or <em>predictors</em> or <em>independent variables</em>. The <em>output variable</em>—what we are trying to predict—is the <em>label</em> “spam” or “not spam.” This<a data-primary="target variables" data-type="indexterm" id="idm140637565379888"/><a data-primary="response variables" data-type="indexterm" id="idm140637565379184"/><a data-primary="dependent variables" data-type="indexterm" id="idm140637565378512"/> is also known as the <em>target variable</em>, <em>dependent variable</em>, or <em>response variable</em> (or <em>class</em> since this is a classification problem).</p>&#13;
&#13;
<p>The<a data-primary="training sets" data-type="indexterm" id="idm140637565375536"/><a data-primary="training instances or samples" data-type="indexterm" id="idm140637565374800"/><a data-primary="error rate" data-type="indexterm" id="idm140637565374160"/><a data-primary="cost function" data-type="indexterm" id="idm140637565373488"/><a data-primary="value function" data-type="indexterm" id="idm140637565372816"/> set of examples the AI trains on is known as the <em>training set</em>, and each individual example is called a training <em>instance</em> or <em>sample</em>. During the training, the AI is attempting to minimize its <em>cost function</em> or <em>error rate</em>, or framed more positively, to maximize its <em>value function</em>—in this case, the ratio of correctly classified emails. The AI actively optimizes for a minimal error rate during training. Its error rate is calculated by comparing the AI’s predicted label with the true label.</p>&#13;
&#13;
<p>However, what we care about most is how well the AI generalizes its training to never-before-seen emails. This will be the true test for the AI: can it correctly classify emails that it has never seen before using what it has learned by training on the examples in the training set? This<a data-primary="generalization error" data-type="indexterm" id="idm140637565041168"/><a data-primary="out-of-sample error" data-type="indexterm" id="idm140637565040464"/> <em>generalization error</em> or <em>out-of-sample error</em> is the main thing we use to evaluate machine learning solutions.</p>&#13;
&#13;
<p>This<a data-primary="test set" data-type="indexterm" id="idm140637565038080"/><a data-primary="holdout set" data-type="indexterm" id="idm140637565037344"/><a data-primary="validation sets" data-type="indexterm" id="idm140637565036672"/> set of never-before-seen examples is known as the <em>test set</em> or <em>holdout set</em> (because the data is held out from the training). If we choose to have multiple holdout sets (perhaps to gauge our generalization error as we train, which is advisable), we may have intermediate holdout sets that we use to evaluate our progress before the final test set; these intermediate holdout sets are called <em>validation sets</em>.</p>&#13;
&#13;
<p>To<a data-primary="experience" data-type="indexterm" id="idm140637565033680"/><a data-primary="performance" data-type="indexterm" id="idm140637565032944"/><a data-primary="task" data-type="indexterm" id="idm140637565032272"/> put all of this together, the AI trains on the training data (<em>experience</em>) to improve its error rate (<em>performance</em>) in flagging spam (<em>task</em>), and the ultimate success criterion is how well its experience generalizes to new, never-before-seen data (<em>generalization error</em>).</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Rules-Based vs. Machine Learning" data-type="sect1"><div class="sect1" id="idm140637565029600">&#13;
<h1>Rules-Based vs. Machine Learning</h1>&#13;
&#13;
<p>Using<a data-primary="rules-based approach" data-type="indexterm" id="idm140637565028032"/> a rules-based approach, we can design a spam filter with explicit rules to catch spam such as flag emails with “u” instead of “you,” “4” instead of “for,” “BUY NOW,” etc. But this system would be difficult to maintain over time as bad guys change their spam behavior to evade the rules. If we used a rules-based system, we would have to frequently adjust the rules manually just to stay up-to-date. Also, it would be very expensive to set up—think of all the rules we would need to create to make this a well-functioning system.</p>&#13;
&#13;
<p>Instead of a rules-based approach, we can use machine learning to train on the email data and automatically engineer rules to correctly flag malicious email as spam. This machine learning-based system could be automatically adjusted over time as well. This system would be much cheaper to train and maintain.</p>&#13;
&#13;
<p>In this simple email problem, it may be possible for us to handcraft rules, but, for many problems, handcrafting rules is not feasible at all. For example, consider designing a self-driving car—imagine drafting rules for how the car should behave in each and every single instance it ever encounters. This is an intractable problem unless the car can learn and adapt on its own based on its experience.</p>&#13;
&#13;
<p>We could also use machine learning systems as an exploration or data discovery tool to gain deeper insight into the problem we are trying to solve. For example, in the email spam filter example, we can learn which words or phrases are most predictive of spam and recognize newly emerging malicious spam patterns.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Supervised vs. Unsupervised" data-type="sect1"><div class="sect1" id="idm140637565024192">&#13;
<h1>Supervised vs. Unsupervised</h1>&#13;
&#13;
<p>The<a data-primary="supervised learning" data-secondary="definition of" data-type="indexterm" id="idm140637565022816"/><a data-primary="unsupervised learning" data-secondary="definition of" data-type="indexterm" id="idm140637565021808"/> field of machine learning has two major branches—<em>supervised learning</em> and <em>unsupervised learning</em>—and plenty of sub-branches that bridge the two.</p>&#13;
&#13;
<p>In supervised learning, the AI agent has access to labels, which it can use to improve its performance on some task. In the email spam filter problem, we have a dataset of emails with all the text within each and every email. We also know which of these emails are spam or not (the so-called <em>labels</em>). These labels are very valuable in helping the supervised learning AI separate the spam emails from the rest.</p>&#13;
&#13;
<p>In unsupervised learning, labels are not available. Therefore, the task of the AI agent is not well-defined, and performance cannot be so clearly measured. Consider the email spam filter problem—this time without labels. Now, the AI agent will attempt to understand the underlying structure of emails, separating the database of emails into different groups such that emails within a group are similar to each other but different from emails in other groups.</p>&#13;
&#13;
<p>This unsupervised learning problem is less clearly defined than the supervised learning problem and harder for the AI agent to solve. But, if handled well, the solution is more powerful.</p>&#13;
&#13;
<p>Here’s why: the unsupervised learning AI may find several groups that it later tags as being “spam”—but the AI may also find groups that it later tags as being “important” or categorize as “family,” “professional,” “news,” “shopping,” etc. In other words, because the problem does not have a strictly defined task, the AI agent may find interesting patterns above and beyond what we initially were looking for.</p>&#13;
&#13;
<p>Moreover, this unsupervised system is better than the supervised system at finding new patterns in future data, making the unsupervised solution more nimble on a go-forward basis. This is the power of unsupervised learning.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Strengths and Weaknesses of Supervised Learning" data-type="sect2"><div class="sect2" id="idm140637565015344">&#13;
<h2>The Strengths and Weaknesses of Supervised Learning</h2>&#13;
&#13;
<p>Supervised learning<a data-primary="supervised learning" data-secondary="strengths and weaknesses of" data-type="indexterm" id="idm140637565013808"/> excels at optimizing performance in well-defined tasks with plenty of labels. For example, consider a very large dataset of images of objects, where each image is labeled. If the dataset is sufficiently large enough and we train using the right machine learning algorithms (i.e., convolutional neural networks) and with powerful enough computers, we can build a very good supervised learning-based image classification system.</p>&#13;
&#13;
<p>As the supervised learning AI trains on the data, it will be able to measure its performance (via a cost function) by comparing its predicted image label with the true image label that we have on file. The AI will explicitly try to minimize this cost function such that its error on never-before-seen images (from a holdout set) is as low as possible.</p>&#13;
&#13;
<p>This is why labels are so powerful—they help guide the AI agent by providing it with an error measure. The AI uses the error measure to improve its performance over time. Without such labels, the AI does not know how successful it is (or isn’t) in correctly classifying images.</p>&#13;
&#13;
<p>However, the costs of manually labeling an image dataset are high. And, even the best curated image datasets have only thousands of labels. This is a problem because supervised learning systems will be very good at classifying images of objects for which it has labels but poor at classifying images of objects for which it has no labels.</p>&#13;
&#13;
<p>As powerful as supervised learning systems are, they are also limited at generalizing knowledge beyond the labeled items they have trained on. Since the majority of the world’s data is unlabeled, with supervised learning, the ability of AI to expand its performance to never-before-seen instances is quite limited.</p>&#13;
&#13;
<p>In other words, supervised learning is great at solving narrow AI problems but not so good at solving more ambitious, less clearly defined problems of the strong AI type.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Strengths and Weaknesses of Unsupervised Learning" data-type="sect2"><div class="sect2" id="idm140637565008528">&#13;
<h2>The Strengths and Weaknesses of Unsupervised Learning</h2>&#13;
&#13;
<p>Supervised learning<a data-primary="unsupervised learning" data-secondary="strengths and weaknesses of" data-type="indexterm" id="idm140637565006832"/> will trounce unsupervised learning at narrowly defined tasks for which we have well-defined patterns that do not change much over time and sufficiently large, readily available labeled datasets.</p>&#13;
&#13;
<p>However, for problems where patterns are unknown or constantly changing or for which we do not have sufficiently large labeled datasets, unsupervised learning truly shines.</p>&#13;
&#13;
<p>Instead of being guided by labels, unsupervised learning works by learning the underlying structure of the data it has trained on. It does this by trying to represent the data it trains on with a set of parameters that is significantly smaller than the number of examples available in the dataset. By performing this representation learning, unsupervised learning is able to identify distinct patterns in the dataset.</p>&#13;
&#13;
<p>In the image dataset example (this time without labels), the unsupervised learning AI may be able to identify and group images based on how similar they are to each other and how different they are from the rest. For example, all the images that look like chairs will be grouped together, all the images that look like dogs will be grouped together, etc.</p>&#13;
&#13;
<p>Of course, the unsupervised learning AI itself cannot label these groups as “chairs” or “dogs” but now that similar images are grouped together, humans have a much <span class="keep-together">simpler</span> labeling task. Instead of labeling millions of images by hand, humans can manually label all the distinct groups, and the labels will apply to all the members within each group.</p>&#13;
&#13;
<p>After the initial training, if the unsupervised learning AI finds images that do not belong to any of the labeled groups, the AI will create separate groups for the unclassified images, triggering a human to label the new, yet-to-be-labeled groups of images.</p>&#13;
&#13;
<p>Unsupervised learning makes previously intractable problems more solvable and is much more nimble at finding hidden patterns both in the historical data that is available for training and in future data. Moreover, we now have an AI approach for the huge troves of unlabeled data that exist in the world.</p>&#13;
&#13;
<p>Even though unsupervised learning is less adept than supervised learning at solving specific, narrowly defined problems, it is better at tackling more open-ended problems of the strong AI type and at generalizing this knowledge.</p>&#13;
&#13;
<p>Just as importantly, unsupervised learning can address many of the common problems data scientists encounter when building machine learning solutions.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Using Unsupervised Learning to Improve Machine Learning Solutions" data-type="sect1"><div class="sect1" id="idm140637565023856">&#13;
<h1>Using Unsupervised Learning to Improve Machine Learning Solutions</h1>&#13;
&#13;
<p>Recent<a data-primary="unsupervised learning" data-secondary="challenges faced by" data-type="indexterm" id="ULchalleng01"/> successes in machine learning have been driven by the availability of lots of data, advances in computer hardware and cloud-based resources, and breakthroughs in machine learning algorithms. But these successes have been in mostly narrow AI problems such as image classification, computer vision, speech recognition, natural language processing, and machine translation.</p>&#13;
&#13;
<p>To solve more ambitious AI problems, we need to unlock the value of unsupervised learning. Let’s explore the most common challenges data scientists face when building solutions and how unsupervised learning can help.<a data-primary="labeled versus unlabeled data" data-type="indexterm" id="idm140637564304112"/><a data-primary="unlabeled versus labeled data" data-type="indexterm" id="idm140637564303440"/></p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Insufficient labeled data" data-type="sect3"><div class="sect3" id="idm140637564302672">&#13;
<h3>Insufficient labeled data</h3>&#13;
<blockquote>&#13;
<p>I think AI is akin to building a rocket ship. You need a huge engine and a lot of fuel. If you have a large engine and a tiny amount of fuel, you won’t make it to orbit. If you have a tiny engine and a ton of fuel, you can’t even lift off. To build a rocket you need a huge engine and a lot of fuel.</p>&#13;
<p data-type="attribution">Andrew Ng</p>&#13;
</blockquote>&#13;
&#13;
<p>If machine learning were a rocket ship, data would be the fuel—without lots and lots of data, the rocket ship cannot fly. But not all data is created equal. To use supervised algorithms, we need lots of labeled data, which is hard and costly to generate.<sup><a data-type="noteref" href="ch01.html#idm140637564299232" id="idm140637564299232-marker">1</a></sup></p>&#13;
&#13;
<p>With unsupervised learning, we can automatically label unlabeled examples. Here is how it would work: we would cluster all the examples and then apply the labels from labeled examples to the unlabeled ones within the same cluster. Unlabeled examples would receive the label of the labeled ones they are most similar to. We will explore clustering in <a data-type="xref" href="ch05.html#Chapter_5">Chapter 5</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Overfitting" data-type="sect3"><div class="sect3" id="idm140637564296304">&#13;
<h3>Overfitting</h3>&#13;
&#13;
<p>If<a data-primary="overfitting" data-type="indexterm" id="idm140637564294976"/> the machine learning algorithm learns an overly complex function based on the training data, it may perform very poorly on never-before-seen instances from holdout sets such as the validation set or test set. In this case, the algorithm has overfit the training data—by extracting too much from the noise in the data—and has very poor generalization error. In other words, the algorithm is memorizing the training data rather than learning how to generalize knowledge based off of it.<sup><a data-type="noteref" href="ch01.html#idm140637564293600" id="idm140637564293600-marker">2</a></sup></p>&#13;
&#13;
<p>To<a data-primary="regularization" data-type="indexterm" id="idm140637564291360"/> address this, we can introduce unsupervised learning as a <em>regularizer</em>. <em>Regularization</em> is a process used to reduce the complexity of a machine learning algorithm, <span class="keep-together">helping</span> it capture the signal in the data without adjusting too much to the noise. Unsupervised pretraining is one such form of regularization. Instead of feeding the original input data directly into a supervised learning algorithm, we can feed a new representation of the original input data that we generate.</p>&#13;
&#13;
<p>This new representation captures the essence of the original data—the true underlying structure—while losing some of the less representative noise along the way. When we feed this new representation into the supervised learning algorithm, it has less noise to wade through and captures more of the signal, improving its generalization error. We will explore feature extraction in <a data-type="xref" href="ch07.html#Chapter_7">Chapter 7</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Curse of dimensionality" data-type="sect3"><div class="sect3" id="idm140637564286704">&#13;
<h3>Curse of dimensionality</h3>&#13;
&#13;
<p>Even<a data-primary="curse of dimensionality" data-type="indexterm" id="idm140637564285376"/> with the advances in computational power, big data is hard for machine learning algorithms to manage. In general, adding more instances is not too problematic because we can parallelize operations using modern map-reduce solutions such as Spark. However, the more features we have, the more difficult training becomes.</p>&#13;
&#13;
<p>In a very high-dimensional space, supervised algorithms need to learn how to separate points and build a function approximation to make good decisions. When the features are very numerous, this search becomes very expensive, both from a time and compute perspective. In some cases, it may be impossible to find a good solution fast enough.</p>&#13;
&#13;
<p>This problem is known as the <em>curse of dimensionality</em>, and unsupervised learning is well suited to help manage this. With dimensionality reduction, we can find the most salient features in the original feature set, reduce the number of dimensions to a more manageable number while losing very little important information in the process, and then apply supervised algorithms to more efficiently perform the search for a good function approximation. We will cover dimensionality reduction in <a data-type="xref" href="ch03.html#Chapter_3">Chapter 3</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Feature engineering" data-type="sect3"><div class="sect3" id="idm140637564281024">&#13;
<h3>Feature engineering</h3>&#13;
&#13;
<p>Feature engineering<a data-primary="feature engineering" data-type="indexterm" id="idm140637564279696"/> is one of the most vital tasks data scientists perform. Without the right features, the machine learning algorithm will not be able to separate points in space well enough to make good decisions on never-before-seen examples. However, feature engineering is typically very labor-intensive; it requires humans to creatively hand-engineer the right types of features. Instead, we can use representation learning from unsupervised learning algorithms to automatically learn the right types of feature representations to help solve the task at hand. We will explore automatic feature extraction in <a data-type="xref" href="ch07.html#Chapter_7">Chapter 7</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Outliers" data-type="sect3"><div class="sect3" id="idm140637564277168">&#13;
<h3>Outliers</h3>&#13;
&#13;
<p>The<a data-primary="outliers" data-type="indexterm" id="idm140637564275840"/> quality of data is also very important. If machine learning algorithms train on rare, distortive outliers, their generalization error will be lower than if they ignored or addressed the outliers separately. With unsupervised learning, we can perform outlier detection using dimensionality reduction and create a solution specifically for the outliers and, separately, a solution for the normal data. We will build an anomaly detection system in <a data-type="xref" href="ch04.html#Chapter_4">Chapter 4</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Data drift" data-type="sect3"><div class="sect3" id="idm140637564273456">&#13;
<h3>Data drift</h3>&#13;
&#13;
<p>Machine<a data-primary="data drift" data-type="indexterm" id="idm140637564272128"/> learning models also need to be aware of drift in the data. If the data the model is making predictions on differs statistically from the data the model trained on, the model may need to retrain on data that is more representative of the current data. If the model does not retrain or does not recognize the drift, the model’s prediction quality on current data will suffer.</p>&#13;
&#13;
<p>By building probability distributions using unsupervised learning, we can assess how different the current data is from the training set data—if the two are different enough, we can automatically trigger a retraining. We will explore how to build these types of data discriminators in <a data-type="xref" href="ch12.html#Chapter_12">Chapter 12</a>.<a data-primary="" data-startref="ULchalleng01" data-type="indexterm" id="idm140637564269344"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="A Closer Look at Supervised Algorithms" data-type="sect1"><div class="sect1" id="idm140637564268112">&#13;
<h1>A Closer Look at Supervised Algorithms</h1>&#13;
&#13;
<p>Before<a data-primary="algorithms" data-secondary="supervised" data-type="indexterm" id="Asuper01"/> we delve into unsupervised learning systems, let’s take a look at supervised learning algorithms and how they work. This will help frame where unsupervised learning fits within the machine learning ecosystem.</p>&#13;
&#13;
<p>In<a data-primary="supervised learning algorithms" data-secondary="overview of" data-type="indexterm" id="idm140637564264208"/><a data-primary="classification problems" data-type="indexterm" id="idm140637564263184"/><a data-primary="regression problems" data-type="indexterm" id="idm140637564262512"/><a data-primary="binary classification" data-type="indexterm" id="idm140637564261840"/><a data-primary="multi-class classification" data-type="indexterm" id="idm140637564261168"/> supervised learning, there are two major types of problems: <em>classification</em> and <em>regression</em>. In classification, the AI must correctly classify items into one of two or more classes. If there are just two classes, the problem is called <em>binary classification</em>. If there are three or more classes, the problem is classed <em>multiclass classification</em>.</p>&#13;
&#13;
<p>Classification<a data-primary="discrete prediction problems" data-type="indexterm" id="idm140637564257856"/><a data-primary="qualitative problems" data-type="indexterm" id="idm140637564257152"/><a data-primary="categorical problems" data-type="indexterm" id="idm140637564256480"/> problems are also known as <em>discrete</em> prediction problems because each class is a discrete group. Classification problems also may be referred to as <em>qualitative</em> or <em>categorical</em> problems.</p>&#13;
&#13;
<p>In<a data-primary="continuous variables" data-type="indexterm" id="idm140637564253984"/><a data-primary="quantitative problems" data-type="indexterm" id="idm140637564253248"/> regression, the AI must predict a <em>continuous</em> variable rather than a discrete one. Regression problems also may be referred to as <em>quantitative</em> problems.</p>&#13;
&#13;
<p>Supervised machine learning algorithms span the gamut, from very simple to very complex, but they are all aimed at minimizing some cost function or error rate (or maximizing a value function) that is associated with the labels we have for the dataset.</p>&#13;
&#13;
<p>As mentioned before, what we care about most is how well the machine learning solution generalizes to never-before-seen cases. The choice of the supervised learning algorithm is very important at minimizing this generalization error.</p>&#13;
&#13;
<p>To achieve the lowest possible generalization error, the complexity of the algorithmic model should match the complexity of the true function underlying the data. We do not know what this true function really is. If we did, we would not need to use machine learning to create a model—we would just solve the function to find the right answer. But since we do not know what this true function is, we choose a machine learning algorithm to test hypotheses and find the model that best approximates this true function (i.e., has the lowest possible generalization error).</p>&#13;
&#13;
<p>If<a data-primary="underfitting" data-type="indexterm" id="idm140637564248832"/> what the algorithm models is less complex than the true function, we have <em>underfit</em> the data. In this case, we could improve the generalization error by choosing an algorithm that can model a more complex function. However, if<a data-primary="overfitting" data-type="indexterm" id="idm140637564247424"/> the algorithm designs an overly complex model, we have <em>overfit</em> the training data and will have poor performance on never-before-seen cases, increasing our generalization error.</p>&#13;
&#13;
<p>In other words, choosing more complex algorithms over simpler ones is not always the right choice—sometimes simpler is better. Each algorithm comes with its set of strengths, weaknesses, and assumptions, and knowing what to use when given the data you have and the problem you are trying to solve is very important to mastering machine learning.</p>&#13;
&#13;
<p>In the rest of this chapter, we will describe some of the most common supervised algorithms (including some real-world applications) before doing the same for unsupervised algorithms.<sup><a data-type="noteref" href="ch01.html#idm140637564244608" id="idm140637564244608-marker">3</a></sup></p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Linear Methods" data-type="sect2"><div class="sect2" id="idm140637564243792">&#13;
<h2>Linear Methods</h2>&#13;
&#13;
<p>The<a data-primary="supervised learning algorithms" data-secondary="linear methods" data-type="indexterm" id="idm140637564242496"/> most basic supervised learning algorithms model a simple linear relationship between the input features and the output variable that we wish to predict.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Linear regression" data-type="sect3"><div class="sect3" id="idm140637564240992">&#13;
<h3>Linear regression</h3>&#13;
&#13;
<p>The<a data-primary="linear methods" data-secondary="linear regression algorithm" data-type="indexterm" id="idm140637564239472"/><a data-primary="algorithms" data-secondary="linear regression" data-type="indexterm" id="idm140637564238496"/> simplest of all the algorithms is <em>linear regression</em>, which uses a model that assumes a linear relationship between the input variables (x) and the single output variable (y). If the true relationship between the inputs and the output is linear and the input variables are not highly correlated (a situation<a data-primary="collinearity" data-type="indexterm" id="idm140637564236768"/> known as <em>collinearity</em>), linear regression may be an appropriate choice. If the true relationship is more complex or nonlinear, linear regression will underfit the data.<sup><a data-type="noteref" href="ch01.html#idm140637564235360" id="idm140637564235360-marker">4</a></sup></p>&#13;
&#13;
<p>Because<a data-primary="interpretability" data-type="indexterm" id="idm140637564234352"/> it is so simple, interpreting the relationship modeled by the algorithm is also very straightforward. <em>Interpretability</em> is a very important consideration for applied machine learning because solutions need to be understood and enacted by both technical and nontechnical people in industry. Without interpretability, the solutions become inscrutable black boxes.</p>&#13;
<dl>&#13;
<dt>Strengths</dt>&#13;
<dd>&#13;
<p>Linear regression is simple, intrepretable, and hard to overfit because it cannot model overly complex relationships. It is an excellent choice when the underlying relationship between the input and output variables is linear.</p>&#13;
</dd>&#13;
<dt>Weaknesses</dt>&#13;
<dd>&#13;
<p>Linear regression will underfit the data when the relationship between the input and output variables is nonlinear.</p>&#13;
</dd>&#13;
<dt>Applications</dt>&#13;
<dd>&#13;
<p>Since the true underlying relationship between human weight and human height is linear, linear regression is great for predicting weight using height as the input variable or, vice versa, for predicting height using weight as the input variable.</p>&#13;
</dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Logistic regression" data-type="sect3"><div class="sect3" id="idm140637564227280">&#13;
<h3>Logistic regression</h3>&#13;
&#13;
<p>The<a data-primary="linear methods" data-secondary="logistic regression algorithm" data-type="indexterm" id="idm140637564225728"/><a data-primary="logistic regression algorithm" data-type="indexterm" id="idm140637564224752"/><a data-primary="algorithms" data-secondary="logistic regression" data-type="indexterm" id="idm140637564224064"/> simplest classification algorithm is <em>logistic regression</em>, which is also a linear method but the predictions are transformed using the logistic function. The<a data-primary="class probabilities" data-type="indexterm" id="idm140637564222496"/> outputs of this transformation are <em>class probabilities</em>—in other words, the probabilities that the instance belongs to the various classes, where the sum of the probabilities for each instance adds up to one. Each instance is then assigned to the class for which it has the highest probability of belonging in.</p>&#13;
<dl>&#13;
<dt>Strengths</dt>&#13;
<dd>&#13;
<p>Like linear regression, logistic regression is simple and interpretable. When the classes we are trying to predict are nonoverlapping and linearly separable, logistic regression is an excellent choice.</p>&#13;
</dd>&#13;
<dt>Weaknesses</dt>&#13;
<dd>&#13;
<p>When classes are not linearly separable, logistic regression will fail.</p>&#13;
</dd>&#13;
<dt>Applications</dt>&#13;
<dd>&#13;
<p>When classes are mostly nonoverlapping—for example, the heights of young children versus the heights of adults—logistic regression will work well.</p>&#13;
</dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Neighborhood-Based Methods" data-type="sect2"><div class="sect2" id="idm140637564215760">&#13;
<h2>Neighborhood-Based Methods</h2>&#13;
&#13;
<p>Another<a data-primary="supervised learning algorithms" data-secondary="neighborhood-based methods" data-type="indexterm" id="idm140637564478368"/><a data-primary="neighborhood-based methods" data-type="indexterm" id="idm140637564477328"/><a data-primary="lazy learners" data-type="indexterm" id="idm140637564476640"/> group of very simple algorithms are neighborhood-based methods. Neighborhood-based methods are <em>lazy learners</em> since they learn how to label new points based on the proximity of the new points to existing labeled points. Unlike linear regression or logistic regression, neighborhood-based models do not learn a set model to predict labels for new points; rather, these models predict labels for new points based purely on distance of new points to preexisting labeled points. Lazy learning<a data-primary="instance-based learning" data-type="indexterm" id="idm140637564474944"/><a data-primary="nonparametric methods" data-type="indexterm" id="idm140637564474240"/> is also referred to as <em>instance-based learning</em> or <em>nonparametric methods</em>.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="k-nearest neighbors" data-type="sect3"><div class="sect3" id="idm140637564472608">&#13;
<h3>k-nearest neighbors</h3>&#13;
&#13;
<p>The<a data-primary="k-nearest neighbors (KNN)" data-type="indexterm" id="idm140637564471248"/> most common neighborhood-based method is <em>k-nearest neighbors (KNN)</em>. To label each new point, KNN looks at a <em>k</em> number (where <em>k</em> is an integer value) of nearest labeled points and has these already labeled neighbors vote on how to label the new point. By default, KNN uses Euclidean distance to measure what is closest.</p>&#13;
&#13;
<p>The choice of <em>k</em> is very important. If <em>k</em> is set to a very low value, KNN becomes very flexible, drawing highly nuanced boundaries and potentially overfitting the data. If <em>k</em> is set to a very high value, KNN becomes inflexible, drawing a too rigid boundary and potentially underfitting the data.</p>&#13;
<dl>&#13;
<dt>Strengths</dt>&#13;
<dd>&#13;
<p>Unlike linear methods, KNN is highly flexible and adept at learning more complex, nonlinear relationships. Yet, KNN remains simple and interpretable.</p>&#13;
</dd>&#13;
<dt>Weaknesses</dt>&#13;
<dd>&#13;
<p>KNN does poorly when the number of observations and features grow. KNN becomes computationally inefficient in this highly populated, high-dimensional space since it needs to calculate distances from the new point to many nearby labeled points in order to predict labels. It cannot rely on an efficient model with a reduced number of parameters to make the necessary prediction. Also, KNN is very sensitive to the choice of <em>k</em>. When <em>k</em> is set too low, KNN can overfit, and when <em>k</em> is set too high, KNN can underfit.</p>&#13;
</dd>&#13;
<dt>Applications</dt>&#13;
<dd>&#13;
<p>KNN is regularly used in recommender systems, such as those used to predict taste in movies (Netflix), music (Spotify), friends (Facebook), photos (Instagram), search (Google), and shopping (Amazon). For example, KNN can help predict<a data-primary="collaborative filtering" data-type="indexterm" id="idm140637564460208"/><a data-primary="content-based filtering" data-type="indexterm" id="idm140637564459504"/> what a user will like given what similar users like (known as <em>collaborative filtering</em>) or what the user has liked in the past (known as <em>content-based filtering</em>).</p>&#13;
</dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Tree-Based Methods" data-type="sect2"><div class="sect2" id="idm140637564457248">&#13;
<h2>Tree-Based Methods</h2>&#13;
&#13;
<p>Instead<a data-primary="decision trees" data-see="also tree-based methods" data-type="indexterm" id="idm140637564455424"/><a data-primary="supervised learning algorithms" data-secondary="tree-based methods" data-type="indexterm" id="idm140637564454416"/><a data-primary="tree-based methods" data-secondary="goal of" data-type="indexterm" id="idm140637564453504"/><a data-primary="decision trees" data-type="indexterm" id="idm140637564452560"/><a data-primary="segmented instances" data-type="indexterm" id="idm140637564451888"/><a data-primary="stratified instances" data-type="indexterm" id="idm140637564451216"/> of using a linear method, we can have the AI build a <em>decision tree</em> where all the instances are <em>segmented</em> or <em>stratified</em> into many regions, guided by the labels we have. Once this segmentation is complete, each region corresponds to a particular class of label (for classification problems) or a range of predicted values (for regression problems). This process is similar to having the AI build rules automatically with the explicit goal of making better decisions or predictions.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Single decision tree" data-type="sect3"><div class="sect3" id="idm140637564448784">&#13;
<h3>Single decision tree</h3>&#13;
&#13;
<p>The<a data-primary="single decision tree" data-see="also tree-based methods" data-type="indexterm" id="idm140637564447456"/><a data-primary="tree-based methods" data-secondary="single decision tree" data-type="indexterm" id="idm140637564446448"/><a data-primary="single decision tree" data-type="indexterm" id="idm140637564445504"/> simplest tree-based method is a <em>single decision tree</em>, in which the AI goes once through the training data, creates rules for segmenting the data guided by the labels, and uses this tree to make predictions on the never-before-seen validation or test set. However, a single decision tree is usually poor at generalizing what it has learned during training to never-before-seen cases because it usually overfits the training data during its one and only training iteration.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Bagging" data-type="sect3"><div class="sect3" id="idm140637564443632">&#13;
<h3>Bagging</h3>&#13;
&#13;
<p>To<a data-primary="tree-based methods" data-secondary="bagging" data-type="indexterm" id="idm140637564442080"/><a data-primary="bagging" data-type="indexterm" id="idm140637564441072"/><a data-primary="bootstrap aggregation" data-type="indexterm" id="idm140637564440400"/><a data-primary="randomization" data-type="indexterm" id="idm140637564439728"/><a data-primary="ensemble method" data-type="indexterm" id="idm140637564439056"/> improve the single decision tree, we can introduce <em>bootstrap aggregation</em> (more commonly known as <em>bagging</em>), in which we take <em>multiple random samples of instances</em> from the training data, create a decision tree for each sample, and then predict the output for each instance by averaging the predictions of each of these trees. By using <em>randomization</em> of samples and averaging results from multiple trees—an approach that is also known as the <em>ensemble method</em>—bagging will address some of the overfitting that results from a single decision tree.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Random forests" data-type="sect3"><div class="sect3" id="idm140637564435552">&#13;
<h3>Random forests</h3>&#13;
&#13;
<p>We<a data-primary="tree-based methods" data-secondary="random forests" data-type="indexterm" id="idm140637564434224"/><a data-primary="random forests" data-type="indexterm" id="idm140637564433216"/> can improve overfitting further by sampling not only the instances but also the predictors. With <em>random forests</em>, we take multiple random samples of instances from the training data like we do in bagging, but, for each split in each decision tree, we make the split based not on all the predictors but rather a <em>random sample of the predictors</em>. The number of predictors we consider for each split is usually the square root of the total number of predictors.</p>&#13;
&#13;
<p>By sampling the predictors in this way, the random forests algorithm creates trees that are even less correlated with each other (compared to the trees in bagging), reducing overfitting and improving the generalization error.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Boosting" data-type="sect3"><div class="sect3" id="idm140637564430288">&#13;
<h3>Boosting</h3>&#13;
&#13;
<p>Another<a data-primary="tree-based methods" data-secondary="boosting" data-type="indexterm" id="idm140637564428736"/><a data-primary="boosting" data-type="indexterm" id="idm140637564427728"/> approach, known as <em>boosting</em>, is used to create multiple trees like in bagging but to <em>build the trees sequentially</em>, using what the AI learned from the previous tree to improve results on the subsequent tree. Each tree is kept pretty shallow, with only a few decision splits, and the learning occurs slowly, tree by tree. Of<a data-primary="gradient boosting machines (GBMs)" data-type="indexterm" id="idm140637564425792"/> all the tree-based methods, <em>gradient boosting machines</em> are among the best-performing and are commonly used to win machine learning competitions.<sup><a data-type="noteref" href="ch01.html#idm140637564424384" id="idm140637564424384-marker">5</a></sup></p>&#13;
<dl>&#13;
<dt>Strengths</dt>&#13;
<dd>&#13;
<p>Tree-based methods are among the best-performing supervised-learning algorithms for prediction problems. These methods are able to capture complex <span class="keep-together">relationships</span> in the data by learning many simple rules, one rule at a time. They are also capable of handling missing data and categorical features.</p>&#13;
</dd>&#13;
<dt>Weaknesses</dt>&#13;
<dd>&#13;
<p>Tree-based methods are difficult to interpret, especially if many rules are needed to make a good prediction. Performance also becomes an issue as the number of features increase.</p>&#13;
</dd>&#13;
<dt>Applications</dt>&#13;
<dd>&#13;
<p>Gradient boosting and random forests are excellent for prediction problems.</p>&#13;
</dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Support Vector Machines" data-type="sect2"><div class="sect2" id="idm140637564417360">&#13;
<h2>Support Vector Machines</h2>&#13;
&#13;
<p>Instead<a data-primary="support vector machines (SVMs)" data-type="indexterm" id="idm140637564416032"/><a data-primary="supervised learning algorithms" data-secondary="support vector machines (SVMs)" data-type="indexterm" id="idm140637564415328"/> of building trees to separate data, we can use algorithms to create hyperplanes in space that separate the data, guided by the labels that we have. The approach is known as <em>support vector machines (SVMs)</em>. SVMs allow some violations to this separation—not all the points within an area in hyperspace need to have the same label—but the distance between boundary-defining points of a certain label and the boundary-defining points of another label should be maximized as much as possible. Also, the boundaries do not have to be linear—we can use nonlinear kernels to more flexibly separate the data.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Neural Networks" data-type="sect2"><div class="sect2" id="idm140637564412976">&#13;
<h2>Neural Networks</h2>&#13;
&#13;
<p>We<a data-primary="neural networks" data-secondary="overview of" data-type="indexterm" id="idm140637564411680"/><a data-primary="supervised learning algorithms" data-secondary="neural networks" data-type="indexterm" id="idm140637564410672"/> can learn representations of the data using neural networks, which are composed of an input layer, several hidden layers, and an output layer.<sup><a data-type="noteref" href="ch01.html#idm140637564409376" id="idm140637564409376-marker">6</a></sup> The input layer uses the features, and the output layer tries to match the response variable. The hidden layers are a nested hierarchy of concepts—each layer (or concept) is trying to understand how the previous layer relates to the output layer.</p>&#13;
&#13;
<p>Using this hierarchy of concepts, the neural network is able to learn complicated concepts by building them out of simpler ones. Neural networks are one of the most powerful approaches to function approximation but are prone to overfitting and are hard to interpret, shortcomings that we will explore in greater detail later in the book.<a data-primary="" data-startref="Asuper01" data-type="indexterm" id="idm140637564406704"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="A Closer Look at Unsupervised Algorithms" data-type="sect1"><div class="sect1" id="idm140637564267520">&#13;
<h1>A Closer Look at Unsupervised Algorithms</h1>&#13;
&#13;
<p>We<a data-primary="algorithms" data-secondary="unsupervised" data-type="indexterm" id="Aunsup01"/><a data-primary="unsupervised learning algorithms" data-secondary="goal of" data-type="indexterm" id="idm140637564402848"/> will now turn our attention to problems where we do not have labels. Instead of trying to make predictions, unsupervised learning algorithms will try to learn the underlying structure of the data.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Dimensionality Reduction" data-type="sect2"><div class="sect2" id="idm140637564401424">&#13;
<h2>Dimensionality Reduction</h2>&#13;
&#13;
<p>One<a data-primary="curse of dimensionality" data-see="also dimensionality reduction" data-type="indexterm" id="idm140637564399920"/><a data-primary="unsupervised learning algorithms" data-secondary="dimensionality reduction" data-type="indexterm" id="ULAdim01"/><a data-primary="dimensionality reduction" data-secondary="goal of" data-type="indexterm" id="idm140637564397600"/><a data-primary="algorithms" data-secondary="for dimensionality reduction" data-type="indexterm" id="idm140637564396640"/> family of algorithms—known as <em>dimensionality reduction algorithms</em>—projects the original high-dimensional input data to a low-dimensional space, filtering out the not-so-relevant features and keeping as much of the interesting ones as possible. Dimensionality reduction allows unsupervised learning AI to more effectively identify patterns and more efficiently solve large-scale, computationally expensive <span class="keep-together">problems</span> (often involving images, video, speech, and text).</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Linear projection" data-type="sect3"><div class="sect3" id="idm140637564393984">&#13;
<h3>Linear projection</h3>&#13;
&#13;
<p>There<a data-primary="dimensionality reduction" data-secondary="algorithms for" data-type="indexterm" id="DRover01"/><a data-primary="linear projection dimensionality reduction" data-type="indexterm" id="idm140637564391072"/> are two major branches of dimensionality—linear projection and nonlinear dimensionality reduction. We will start with linear projection first.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Principal component analysis (PCA)" data-type="sect4"><div class="sect4" id="idm140637564389952">&#13;
<h4>Principal component analysis (PCA)</h4>&#13;
&#13;
<p>One<a data-primary="principal component analysis (PCA)" data-secondary="overview of" data-type="indexterm" id="idm140637564388688"/> approach to learning the underlying structure of data is to identify which features out of the full set of features are most important in explaining the variability among the instances in the data. Not all features are equal—for some features, the values in the dataset do not vary much, and these features are less useful in explaining the dataset. For other features, the values might vary considerably—these features are worth exploring in greater detail since they will be better at helping the model we design separate the data.</p>&#13;
&#13;
<p>In <em>PCA</em>, the algorithm finds a low-dimensional representation of the data while retaining as much of the variation as possible. The number of dimensions we are left with is considerably smaller than the number of dimensions of the full dataset (i.e., the number of total features). We lose some of the variance by moving to this low-dimensional space, but the underlying structure of the data is easier to identify, <span class="keep-together">allowing</span> us to perform tasks like clustering more efficiently.</p>&#13;
&#13;
<p>There<a data-primary="incremental PCA" data-type="indexterm" id="idm140637564384336"/><a data-primary="kernel PCA" data-type="indexterm" id="idm140637564383600"/><a data-primary="sparse PCA" data-type="indexterm" id="idm140637564382928"/> are several variants of PCA, which we will explore later in the book. These include mini-batch variants such as <em>incremental PCA</em>, nonlinear variants such as <em>kernel PCA</em>, and sparse variants such as <em>sparse PCA</em>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Singular value decomposition (SVD)" data-type="sect4"><div class="sect4" id="idm140637564380656">&#13;
<h4>Singular value decomposition (SVD)</h4>&#13;
&#13;
<p>Another<a data-primary="singular value decomposition (SVD)" data-type="indexterm" id="idm140637564379328"/> approach to learning the underlying structure of the data is to reduce the rank of the original matrix of features to a smaller rank such that the original matrix can be recreated using a linear combination of some of the vectors in the smaller rank matrix. This is known as <em>SVD</em>. To generate the smaller rank matrix, SVD keeps the vectors of the original matrix that have the most information (i.e., the highest singular value). The smaller rank matrix captures the most important elements of the original feature space.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Random projection" data-type="sect4"><div class="sect4" id="idm140637564377216">&#13;
<h4>Random projection</h4>&#13;
&#13;
<p>A<a data-primary="random projection dimensionality reduction" data-type="indexterm" id="idm140637564375888"/><a data-primary="random Gaussian matrix" data-type="indexterm" id="idm140637564375056"/><a data-primary="random sparse matrix" data-type="indexterm" id="idm140637564374384"/> similar dimensionality reduction algorithm involves projecting points from a high-dimensional space to a space of much lower dimensions in such a way that the scale of distances between the points is preserved. We can use either a <em>random Gaussian matrix</em> or a <em>random sparse matrix</em> to accomplish this.</p>&#13;
</div></section>&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Manifold learning" data-type="sect3"><div class="sect3" id="idm140637564372240">&#13;
<h3>Manifold learning</h3>&#13;
&#13;
<p>Both<a data-primary="nonlinear dimensionality reduction" data-type="indexterm" id="idm140637564370640"/><a data-primary="manifold learning" data-type="indexterm" id="idm140637564369936"/> PCA and random projection rely on projecting the data linearly from a high-dimensional space to a low-dimensional space. Instead of a linear projection, it may be better to perform a nonlinear transformation of the data—this is known as <em>manifold learning</em> or <em>nonlinear dimensionality reduction</em>.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Isomap" data-type="sect4"><div class="sect4" id="idm140637564367920">&#13;
<h4>Isomap</h4>&#13;
&#13;
<p><em>Isomap</em> is<a data-primary="isometric mapping (Isomap)" data-type="indexterm" id="idm140637564366080"/><a data-primary="geodesic distance" data-type="indexterm" id="idm140637564365328"/> one type of manifold learning approach. This algorithm learns the intrinsic geometry of the data manifold by estimating the <em>geodesic</em> or <em>curved distance</em> between each point and its neighbors rather than the<a data-primary="Euclidean distance" data-type="indexterm" id="idm140637564363552"/> Euclidean distance. Isomap uses this to then embed the original high-dimensional space to a low-dimensional one.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="t-distributed stochastic neighbor embedding (t-SNE)" data-type="sect4"><div class="sect4" id="idm140637564362464">&#13;
<h4>t-distributed stochastic neighbor embedding (t-SNE)</h4>&#13;
&#13;
<p>Another<a data-primary="t-distributed stochastic neighbor embedding" data-type="indexterm" id="idm140637564361200"/> nonlinear dimensionality reduction—known as <em>t-SNE</em>—embeds high-dimensional data into a space of just two or three dimensions, allowing the transformed data to be visualized. In this two- or three-dimensional space, similar instances are modeled closer together and dissimilar instances are modeled further away.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Dictionary learning" data-type="sect4"><div class="sect4" id="idm140637564359328">&#13;
<h4>Dictionary learning</h4>&#13;
&#13;
<p>An<a data-primary="dictionary learning" data-type="indexterm" id="idm140637564358000"/> approach known as <em>dictionary learning</em> involves learning the sparse representation of the underlying data. These representative elements are <span class="keep-together">simple</span>, binary vectors (zeros and ones), and each instance in the dataset can be reconstructed as a weighted sum of the representative elements. The matrix (known as the <em>dictionary</em>) that this unsupervised learning generates is mostly populated by zeros with only a few nonzero weights.</p>&#13;
&#13;
<p>By creating such a dictionary, this algorithm is able to efficiently identify the most salient representative elements of the original feature space—these are the ones that have the most nonzero weights. The representative elements that are less important will have few nonzero weights. As with PCA, dictionary learning is excellent for learning the underlying structure of the data, which will be helpful in separating the data and in identifying interesting patterns.</p>&#13;
</div></section>&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Independent component analysis" data-type="sect3"><div class="sect3" id="idm140637564354048">&#13;
<h3>Independent component analysis</h3>&#13;
&#13;
<p>One<a data-primary="independent component analysis (ICA)" data-type="indexterm" id="idm140637564352752"/> common problem with unlabeled data is that there are many independent signals embedded together into the features we are given. Using <em>independent component analysis (ICA)</em>, we can separate these blended signals into their individual components. After the separation is complete, we can reconstruct any of the original features by adding together some combination of the individual components we generate. ICA is commonly used in signal processing tasks (for example, to identify the individual voices in an audio clip of a busy coffeehouse).</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Latent Dirichlet allocation" data-type="sect3"><div class="sect3" id="idm140637564350752">&#13;
<h3>Latent Dirichlet allocation</h3>&#13;
&#13;
<p>Unsupervised<a data-primary="latent Dirichlet allocation (LDA)" data-type="indexterm" id="idm140637564349440"/> learning can also explain a dataset by learning why some parts of the dataset are similar to each other. This requires learning unobserved elements within the dataset—an approach known as <em>latent Dirichlet allocation (LDA)</em>. For example, consider a document of text with many, many words. These words within a document are not purely random; rather, they exhibit some structure.</p>&#13;
&#13;
<p>This structure can be modeled as unobserved elements known as topics. After training, LDA is able to explain a given document with a small set of topics, where for each topic there is a small set of frequently used words. This is the hidden structure the LDA is able to capture, helping us better explain a previously unstructured corpus of text.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Dimensionality reduction<a data-primary="dimensionality reduction" data-secondary="goal of" data-type="indexterm" id="idm140637564345936"/> reduces the original set of features to a smaller set of just the most important features. From here, we can run other unsupervised learning algorithms on this smaller set of features to find interesting patterns in the data (see the next section on clustering), or, if we have labels, we can speed up the training cycle of supervised learning algorithms by feeding in this smaller matrix of features instead of using the original feature matrix.<a data-primary="" data-startref="ULAdim01" data-type="indexterm" id="idm140637564344304"/><a data-primary="" data-startref="DRover01" data-type="indexterm" id="idm140637564343360"/></p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Clustering" data-type="sect2"><div class="sect2" id="idm140637564342032">&#13;
<h2>Clustering</h2>&#13;
&#13;
<p>Once<a data-primary="unsupervised learning algorithms" data-secondary="clustering" data-type="indexterm" id="idm140637564340496"/><a data-primary="clustering" data-secondary="goal of" data-type="indexterm" id="idm140637564339520"/> we have reduced the set of original features to a smaller, more manageable set, we can find interesting patterns by grouping similar instances of data together. This is known as clustering and can be accomplished with a variety of unsupervised learning algorithms and be used for real-world applications such as market segmentation.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="k-means" data-type="sect3"><div class="sect3" id="idm140637564337968">&#13;
<h3>k-means</h3>&#13;
&#13;
<p>To<a data-primary="clustering" data-secondary="algorithms for" data-type="indexterm" id="Cover01"/><a data-primary="within-cluster variation" data-type="indexterm" id="idm140637564335168"/><a data-primary="inertia" data-type="indexterm" id="idm140637564334528"/><a data-primary="clustering" data-secondary="k-means" data-type="indexterm" id="idm140637564333856"/><a data-primary="k-means clustering" data-secondary="overview of" data-type="indexterm" id="idm140637564332912"/> cluster well, we need to identify distinct groups such that the instances within a group are similar to each other but different from instances in other groups. One such algorithm is <em>k-means clustering</em>. With this algorithm, we specify the number of desired clusters <em>k</em>, and the algorithm will assign each instance to exactly one of these <em>k</em> clusters. It optimizes the grouping by minimizing the <em>within-cluster variation</em> (also known as <em>inertia</em>) such that the sum of the within-cluster variations across all <em>k</em> clusters is as small as possible.</p>&#13;
&#13;
<p>To speed up this clustering process, <em>k</em>-means randomly assigns each observation to one of the <em>k</em> clusters and then begins to reassign these observations to minimize the Euclidean distance between each observation and its cluster’s center point, or<a data-primary="centroids" data-type="indexterm" id="idm140637564327376"/> <em>centroid</em>. As a result, different runs of <em>k</em>-means—each with a randomized start—will result in slightly different clustering assignments of the observations. From these different runs, we can choose the one that has the best separation, defined as the lowest total sum of within-cluster variations across all <em>k</em> clusters.<sup><a data-type="noteref" href="ch01.html#idm140637564325008" id="idm140637564325008-marker">7</a></sup></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Hierarchical clustering" data-type="sect3"><div class="sect3" id="idm140637564323296">&#13;
<h3>Hierarchical clustering</h3>&#13;
&#13;
<p>An<a data-primary="clustering" data-secondary="hierarchical" data-type="indexterm" id="idm140637564321776"/><a data-primary="hierarchical clustering" data-secondary="overview of" data-type="indexterm" id="idm140637564320768"/><a data-primary="agglomerative clustering" data-type="indexterm" id="idm140637564319824"/><a data-primary="dendrograms" data-type="indexterm" id="idm140637564319184"/> alternative clustering approach—one that does not require us to precommit to a particular number of clusters—is known as <em>hierarchical clustering</em>. One version of hierarchical clustering called <em>agglomerative clustering</em> uses a tree-based clustering method, and builds what is called a <em>dendrogram</em>. A dendrogram can be depicted graphically as an upside-down tree, where the leaves are at the bottom and the tree trunk is at the top.</p>&#13;
&#13;
<p>The leaves at the very bottom are individual instances in the dataset. Hierarchical clustering then joins the leaves together—as we move vertically up the upside-down tree—based on how similar they are to each other. The instances (or groups of instances) that are most similar to each other are joined sooner, while the instances that are not as similar are joined later. With this iterative process, all the instances are eventually linked together forming the single trunk of the tree.</p>&#13;
&#13;
<p>This vertical depiction is very helpful. Once the hierarchical clustering algorithm has finished running, we can view the dendrogram and determine where we want to cut the tree—the lower we cut, the more individual branches we are left with (i.e., more clusters). If we want fewer clusters, we can cut higher on the dendrogram, closer to the single trunk at the very top of this upside-down tree. The placement of this vertical cut is similar to choosing the number of <em>k</em> clusters in the <em>k</em>-means clustering <span class="keep-together">algorithm</span>.<sup><a data-type="noteref" href="ch01.html#idm140637564993824" id="idm140637564993824-marker">8</a></sup></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="DBSCAN" data-type="sect3"><div class="sect3" id="idm140637564992768">&#13;
<h3>DBSCAN</h3>&#13;
&#13;
<p>An<a data-primary="clustering" data-secondary="DBSCAN (density-based spatial clustering of applications with noise)" data-type="indexterm" id="idm140637564991472"/><a data-primary="DBSCAN (density-based spatial clustering of applications with noise)" data-type="indexterm" id="idm140637564990416"/> even more powerful clustering algorithm (based on the density of points) is known as <em>DBSCAN</em> (density-based spatial clustering of applications with noise). Given all the instances we have in space, DBSCAN will group together those that are packed closely together, where close together is defined as a minimum number of instances that must exist within a certain distance. We specify both the minimum number of instances required and the distance.</p>&#13;
&#13;
<p>If an instance is within this specified distance of multiple clusters, it will be grouped with the cluster to which it is most densely located. Any instance that is not within this specified distance of another cluster is labeled an outlier.</p>&#13;
&#13;
<p>Unlike <em>k</em>-means, we do not need to prespecify the number of clusters. We can also have arbitrarily shaped clusters. DBSCAN is much less prone to the distortion typically caused by outliers in the data.<a data-primary="" data-startref="Cover01" data-type="indexterm" id="idm140637564987072"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Feature Extraction" data-type="sect2"><div class="sect2" id="idm140637564985840">&#13;
<h2>Feature Extraction</h2>&#13;
&#13;
<p>With<a data-primary="feature extraction" data-type="indexterm" id="idm140637564984544"/><a data-primary="unsupervised learning algorithms" data-secondary="feature extraction" data-type="indexterm" id="idm140637564983808"/> unsupervised learning, we can learn new representations of the original features of data—a field known as <em>feature extraction</em>. Feature extraction can be used to reduce the number of original features to a smaller subset, effectively performing dimensionality reduction. But feature extraction can also generate new feature representations to help improve performance on supervised learning problems.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Autoencoders" data-type="sect3"><div class="sect3" id="idm140637564981840">&#13;
<h3>Autoencoders</h3>&#13;
&#13;
<p>To<a data-primary="autoencoders" data-secondary="overview of" data-type="indexterm" id="idm140637564980512"/> generate new feature representations, we can use a feedforward, nonrecurrent neural network to perform representation learning, where the number of nodes in the output layer matches the number of nodes in the input layer. This neural network is known as an <em>autoencoder</em> and effectively reconstructs the original features, learning a new representation using the hidden layers in between.<sup><a data-type="noteref" href="ch01.html#idm140637564978592" id="idm140637564978592-marker">9</a></sup></p>&#13;
&#13;
<p>Each hidden layer of the autoencoder learns a representation of the original features, and subsequent layers build on the representation learned by the preceding layers. Layer by layer, the autoencoder learns increasingly complicated representations from simpler ones.</p>&#13;
&#13;
<p>The output layer is the final newly learned representation of the original features. This learned representation can then be used as an input into a supervised learning model with the objective of improving the generalization error.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Feature extraction using supervised training of feedforward networks" data-type="sect3"><div class="sect3" id="idm140637564976192">&#13;
<h3>Feature extraction using supervised training of feedforward networks</h3>&#13;
&#13;
<p>If<a data-primary="feedforward networks" data-type="indexterm" id="idm140637564974816"/> we have labels, an alternate feature extraction approach is to use a feedforward, nonrecurrent neural network where the output layer attempts to predict the correct label. Just like with autoencoders, each hidden layer learns a representation of the original features.</p>&#13;
&#13;
<p>However, when generating the new representations, this network is explicitly <em>guided by the labels</em>. To extract the final newly learned representation of the original features in this network, we extract the penultimate layer—the hidden layer just before the output layer. This penultimate layer can then be used as an input into any supervised learning model.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Unsupervised Deep Learning" data-type="sect2"><div class="sect2" id="idm140637564972208">&#13;
<h2>Unsupervised Deep Learning</h2>&#13;
&#13;
<p>Unsupervised learning<a data-primary="unsupervised deep learning" data-secondary="overview of" data-type="indexterm" id="UDLover01"/><a data-primary="unsupervised learning algorithms" data-secondary="unsupervised deep learning" data-type="indexterm" id="ULAunsupdeep01"/> performs many important functions in the field of deep learning, some of which we will explore in this book. This field is known as <em>unsupervised deep learning</em>.</p>&#13;
&#13;
<p>Until very recently, the training of<a data-primary="deep neural networks" data-type="indexterm" id="idm140637564966960"/><a data-primary="gradient of the error function" data-type="indexterm" id="idm140637564966256"/> deep neural networks was computationally intractable. In these neural networks, the hidden layers learn internal representations to help solve the problem at hand. The representations improve over time based on how the neural network uses the <em>gradient of the error function</em> in each training iteration to update the weights of the various nodes.</p>&#13;
&#13;
<p>These updates are computationally expensive, and two major types of problems may occur in the process. First, the gradient of the error function may become very small, and, since<a data-primary="backpropagation" data-type="indexterm" id="idm140637564963872"/> <em>backpropagation</em> relies on multiplying these small weights together, the weights of the network may update very slowly or not at all, preventing proper training of the network.<sup><a data-type="noteref" href="ch01.html#idm140637564962448" id="idm140637564962448-marker">10</a></sup> This<a data-primary="vanishing gradient problem" data-type="indexterm" id="idm140637564960992"/> is known as the <em>vanishing gradient problem</em>.</p>&#13;
&#13;
<p>Conversely, the other issue is that the gradient of the error function might become very large; with backprop, the weights throughout the network may update in huge increments, making the training of the network very unstable. This<a data-primary="exploding gradient problem" data-type="indexterm" id="idm140637564959184"/> is known as the <em>exploding gradient problem</em>.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Unsupervised pretraining" data-type="sect3"><div class="sect3" id="idm140637564957792">&#13;
<h3>Unsupervised pretraining</h3>&#13;
&#13;
<p>To<a data-primary="greedy layer-wise unsupervised pretraining" data-type="indexterm" id="idm140637564956208"/><a data-primary="unsupervised pretraining" data-type="indexterm" id="idm140637564955504"/> address these difficulties in training very deep, multilayered neural networks, machine learning researchers train neural networks in multiple, successive stages, where each stage involves a shallow neural network. The output of one shallow network is then used as the input of the next neural network. Typically, the first shallow neural network in this pipeline involves an unsupervised neural network, but the later networks are supervised.</p>&#13;
&#13;
<p>This unsupervised portion is known as <em>greedy layer-wise unsupervised pretraining</em>. In 2006, Geoffrey Hinton demonstrated the successful application of unsupervised pretraining to initialize the training of deeper neural network pipelines, kicking off the current deep learning revolution. Unsupervised pretaining allows the AI to capture an improved representation of the original input data, which the supervised portion then takes advantage of to solve the specific task at hand.</p>&#13;
&#13;
<p>This approach is called “greedy” because each portion of the neural network is trained independently, not jointly. “Layer-wise” refers to the layers of the network. In most modern neural networks, pretraining is usually not necessary. Instead, all the layers are trained jointly using backpropagation. Major computer advances have made the vanishing gradient problem and the exploding gradient problem much more <span class="keep-together">manageable</span>.</p>&#13;
&#13;
<p>Unsupervised pretraining<a data-primary="transfer learning" data-type="indexterm" id="idm140637564950960"/> not only makes supervised problems easier to solve but also facilitates <em>transfer learning</em>. Transfer learning involves using machine learning algorithms to store knowledge gained from solving one task to solve another related task much more quickly and with considerably less data.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Restricted Boltzmann machines" data-type="sect3"><div class="sect3" id="idm140637564949408">&#13;
<h3>Restricted Boltzmann machines</h3>&#13;
&#13;
<p>One<a data-primary="restricted Boltzmann machines (RBMs)" data-secondary="overview of" data-type="indexterm" id="idm140637564948112"/> applied example of unsupervised pretraining is the <em>restricted Boltzmann machine (RBM)</em>, a shallow, two-layer neural network. The first layer is the input layer, and the second layer is the hidden layer. Each node is connected to every node in the other layer, but nodes are not connected to nodes of the same layer—this is where the restriction occurs.</p>&#13;
&#13;
<p>RBMs can perform unsupervised tasks such as dimensionality reduction and feature extraction and provide helpful unsupervised pretraining as part of supervised learning solutions. RBMs are similar to autoencoders but differ in some important ways. For example, autoencoders have an output layer, while RBMs do not. We will explore these and other differences in detail later in the book.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Deep belief networks" data-type="sect3"><div class="sect3" id="idm140637564945136">&#13;
<h3>Deep belief networks</h3>&#13;
&#13;
<p>RBMs<a data-primary="deep belief networks (DBNs)" data-secondary="overview of" data-type="indexterm" id="idm140637564943808"/><a data-primary="feature detection" data-type="indexterm" id="idm140637564942736"/> can be linked together to form a multistage neural network pipeline known as a <em>deep belief network (DBN)</em>. The hidden layer of each RBM is used as the input for the next RBM. In other words, each RBM generates a representation of the data that the next RBM then builds upon. By successively linking this type of representation learning, the deep belief network is able to learn more complicated representations that are often used as <em>feature detectors</em>.<sup><a data-type="noteref" href="ch01.html#idm140637564940720" id="idm140637564940720-marker">11</a></sup></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Generative adversarial networks" data-type="sect3"><div class="sect3" id="idm140637564939648">&#13;
<h3>Generative adversarial networks</h3>&#13;
&#13;
<p>One<a data-primary="generative adversarial networks (GANs)" data-secondary="overview of" data-type="indexterm" id="idm140637564938096"/> major advance in unsupervised deep learning has been the advent of <em>generative adversarial networks (GANs)</em>, introduced by Ian Goodfellow and his fellow researchers at the University of Montreal in 2014. GANs have many applications; for example, we can use GANs to create near-realistic synthetic data, such as images and speech, or perform anomaly detection.</p>&#13;
&#13;
<p>In GANs, we have two neural networks. One network—known as the generator—generates data based on a model data distribution it has created using samples of real data it has received. The other network—known as the discriminator—discriminates between the data created by the generator and data from the true data distribution.</p>&#13;
&#13;
<p>As a simple analogy, the generator is the counterfeiter, and the discriminator is the police trying to identify the forgery. The two networks are locked in a zero-sum game. The generator is trying to fool the discriminator into thinking the synthetic data comes from the true data distribution, and the discriminator is trying to call out the synthetic data as fake.</p>&#13;
&#13;
<p>GANs are unsupervised learning algorithms because the generator can learn the underlying structure of the true data distribution even when there are no labels. GANs learn the underlying structure in the data through the training process and efficiently capture the structure using a small, manageable number of parameters.</p>&#13;
&#13;
<p>This process is similar to the representation learning that occurs in deep learning. Each hidden layer in the neutral network of a generator captures a representation of the underlying data—starting very simply—and subsequent layers pick up more complicated representations by building on the simpler preceding layers.</p>&#13;
&#13;
<p>Using all these layers together, the generator learns the underlying structure of the data and, using what it has learned, the generator attempts to create synthetic data that is nearly identical to the true data distribution. If the generator has captured the essence of the true data distribution, the synthetic data will appear real.<a data-primary="" data-startref="ULAunsupdeep01" data-type="indexterm" id="idm140637564932352"/><a data-primary="" data-startref="UDLover01" data-type="indexterm" id="idm140637564931376"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Sequential Data Problems Using Unsupervised Learning" data-type="sect2"><div class="sect2" id="idm140637564930304">&#13;
<h2>Sequential Data Problems Using Unsupervised Learning</h2>&#13;
&#13;
<p>Unsupervised learning <a data-primary="unsupervised learning algorithms" data-secondary="sequential data problems" data-type="indexterm" id="idm140637564929088"/>can also handle sequential data such as time series data. One<a data-primary="Markov models" data-type="indexterm" id="idm140637564927920"/><a data-primary="simple Markov models" data-type="indexterm" id="idm140637564927168"/> such approach involves learning the hidden states of a <em>Markov model</em>. In the <em>simple Markov model</em>, states are fully observed and change stochastically (in other words, randomly). Future states depend only on the current state and are not dependent on previous states.</p>&#13;
&#13;
<p>In<a data-primary="hidden Markov models" data-type="indexterm" id="idm140637564924848"/> a <em>hidden Markov model</em>, the states are only partially observable, but, like with simple Markov models, the outputs of these partially observable states are fully observable. Since the observations that we have are insufficient to determine the state completely, we need unsupervised learning to help discover these hidden states more fully.</p>&#13;
&#13;
<p>Hidden Markov model algorithms involve learning the probable next state given what we know about the sequence of previously occurring, partially observable states and fully observable outputs. These algorithms have had major commercial applications in sequential data problems involving speech, text, and time series.<a data-primary="" data-startref="Aunsup01" data-type="indexterm" id="idm140637564922480"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Reinforcement Learning Using Unsupervised Learning" data-type="sect1"><div class="sect1" id="idm140637564921248">&#13;
<h1>Reinforcement Learning Using Unsupervised Learning</h1>&#13;
&#13;
<p>Reinforcement learning<a data-primary="reinforcement learning" data-type="indexterm" id="idm140637564919520"/><a data-primary="machine learning" data-secondary="reinforcement learning" data-type="indexterm" id="idm140637564918784"/> is the third major branch of machine learning, in which an <em>agent</em> determines its optimal behavior (<em>actions</em>) in an <em>environment</em> based on feedback (<em>reward</em>) that it receives. This feedback is known as the <em>reinforcement signal</em>. The agent’s goal is to maximize its cumulative reward over time.</p>&#13;
&#13;
<p>While reinforcement learning has been around since the 1950s, it has made mainstream headline news only in recent years. In 2013, DeepMind—now owned by Google—applied reinforcement learning to achieve superhuman-level performance at playing many different Atari games. DeepMind’s system achieved this with just raw sensory data as input and no prior knowledge of the rules of the games.</p>&#13;
&#13;
<p>In 2016, DeepMind again captured the imagination of the machine learning community—this time the DeepMind reinforcement learning-based AI agent AlphaGo beat Lee Sedol, one of the world’s best Go players. These successes have cemented reinforcement learning as a mainstream AI topic.</p>&#13;
&#13;
<p>Today, machine learning researchers are applying reinforcement learning to solve many different types of problems including:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Stock market trading, in which the agent buys and sells (actions) and receives profits or losses (rewards) in return</p>&#13;
</li>&#13;
<li>&#13;
<p>Video games and board games, in which the agent makes game decisions (actions) and wins or loses (rewards)</p>&#13;
</li>&#13;
<li>&#13;
<p>Self-driving cars, in which the agent directs the vehicle (actions) and either stays on course or crashes (rewards)</p>&#13;
</li>&#13;
<li>&#13;
<p>Machine control, in which the agent moves about its environment (actions) and either completes the course or fails (rewards)</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>In the simplest reinforcement learning problems, we have a finite problem—with a finite number of states of the environment, a finite number of actions that are possible at any given state of the environment, and a finite number of rewards. The action taken by the agent given the current state of the environment determines the next state, and the agent’s goal is to maximize its long-term reward. This family of problems is known as finite <em>Markov decision processes</em>.</p>&#13;
&#13;
<p>However, in the real world, things are not so simple—the reward is unknown and dynamic rather than known and static. To help discover this unknown reward function and approximate it as best as possible, we can apply unsupervised learning. Using this approximated reward function, we can apply reinforcement learning solutions to increase the cumulative reward over time.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Semisupervised Learning" data-type="sect1"><div class="sect1" id="idm140637564906736">&#13;
<h1>Semisupervised Learning</h1>&#13;
&#13;
<p>Even<a data-primary="semisupervised learning" data-secondary="overview of" data-type="indexterm" id="idm140637564905408"/> though supervised learning and unsupervised learning are two distinct major branches of machine learning, the algorithms from each branch can be mixed together as part of a machine learning pipeline.<sup><a data-type="noteref" href="ch01.html#idm140637564904048" id="idm140637564904048-marker">12</a></sup> Typically, this mix of supervised and unsupervised is used when we want to take full advantage of the few labels that we have or when we want to find new, yet unknown patterns from unlabeled data in addition to the known patterns from the labeled data. These types of problems are solved using a hybrid of supervised and unsupervised learning known as semisupervised learning. We will explore this area in greater detail later in the book.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Successful Applications of Unsupervised Learning" data-type="sect1"><div class="sect1" id="idm140637564902544">&#13;
<h1>Successful Applications of Unsupervised Learning</h1>&#13;
&#13;
<p>In<a data-primary="unsupervised learning" data-secondary="successful applications of" data-type="indexterm" id="idm140637564901216"/> the last ten years, most successful commercial applications of machine learning have come from the supervised learning space, but this is changing. Unsupervised learning applications have become more commonplace. Sometimes, unsupervised learning is just a means to make supervised applications better. Other times, unsupervised learning achieves the commercial application itself. Here is a closer look at two of the biggest applications of unsupervised learning to date: anomaly detection and group segmentation.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Anomaly Detection" data-type="sect2"><div class="sect2" id="idm140637564899408">&#13;
<h2>Anomaly Detection</h2>&#13;
&#13;
<p>Performing<a data-primary="anomaly detection" data-secondary="overview of" data-type="indexterm" id="idm140637564897648"/> dimensionality reduction can reduce the original high-dimensional feature space into a transformed lower-dimensional space. In this lower-dimensional space, we find where the majority of points densely lie. This portion is the <em>normal space</em>. Points<a data-primary="outliers" data-type="indexterm" id="idm140637564895888"/> that lie much farther away are called <em>outliers</em>—or <em>anomalies</em>—and are worth investigating in greater detail.</p>&#13;
&#13;
<p>Anomaly detection systems are commonly used for fraud detection such as credit card fraud, wire fraud, cyber fraud, and insurance fraud. Anomaly detection is also used to identify rare, malicious events such as hacking of internet-connected devices, maintenance failures in mission-critical equipment such as airplanes and trains, and cybersecurity breaches due to malware and other pernicious agents.</p>&#13;
&#13;
<p>We can use these systems for spam detection, such as the email spam filter example we used earlier in the chapter. Other applications include finding bad actors to stop activity such as terrorist financing, money laundering, human and narcotics <span class="keep-together">trafficking</span>, and arms dealing, identifying high risk events in financial trading, and discovering diseases such as cancer.</p>&#13;
&#13;
<p>To make the analysis of anomalies more manageable, we can use a clustering algorithm to group similar anomalies together and then hand-label these clusters based on the types of behavior they represent. With such a system, we can have an unsupervised learning AI that is able to identify anomalies, cluster them into appropriate groups, and, using the cluster labels provided by humans, recommend to business analysts the appropriate course of action.</p>&#13;
&#13;
<p>With anomaly detection systems, we can take an unsupervised problem and eventually create a semisupervised one with this cluster-and-label approach. Over time, we can run supervised algorithms on the labeled data alongside the unsupervised <span class="keep-together">algorithms</span>. For successful machine learning applications, unsupervised systems and supervised systems should be used in conjunction, complementing one another.</p>&#13;
&#13;
<p>The supervised system finds the known patterns with a high level of accuracy, while the unsupervised system discovers new patterns that may be of interest. Once these patterns are uncovered by the unsupervised AI, the patterns are labeled by humans, transitioning more of the data from unlabeled to labeled.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Group segmentation" data-type="sect3"><div class="sect3" id="idm140637564888736">&#13;
<h3>Group segmentation</h3>&#13;
&#13;
<p>With<a data-primary="group segmentation" data-secondary="overview of" data-type="indexterm" id="idm140637564887408"/> clustering, we can segment groups based on similarity in behavior  in areas such as marketing, customer retention, disease diagnosis, online shopping, music listening, video watching, online dating, social media activity, and document classification. The amount of data that is generated in each of these areas is massive, and the data is only partially labeled.</p>&#13;
&#13;
<p>For patterns that we already know and want to reinforce, we can use supervised learning algorithms. But often we want to discover new patterns and groups of interest—for this discovery process, unsupervised learning is a natural fit. Again, it is all about synergy. We should use supervised and unsupervised learning systems in conjunction to build a stronger machine learning solution.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="idm140637564884720">&#13;
<h1>Conclusion</h1>&#13;
&#13;
<p>In this chapter, we explored the following:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The difference between a rules-based system and machine learning</p>&#13;
</li>&#13;
<li>&#13;
<p>The difference between supervised and unsupervised learning</p>&#13;
</li>&#13;
<li>&#13;
<p>How unsupervised learning can help address common problems in training machine learning models</p>&#13;
</li>&#13;
<li>&#13;
<p>Common algorithms for supervised, unsupervised, reinforcement, and semisupervised learning</p>&#13;
</li>&#13;
<li>&#13;
<p>Two major applications of unsupervised learning—anomaly detection and group segmentation</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>In <a data-type="xref" href="ch02.html#Chapter_2">Chapter 2</a>, we’ll explore how to build machine learning applications. Then, we will cover dimensionality reduction and clustering in detail, building an anomaly detection system and a group segmentation system in the process.<a data-primary="" data-startref="MLeco01" data-type="indexterm" id="idm140637564876448"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm140637564299232"><sup><a href="ch01.html#idm140637564299232-marker">1</a></sup> There are startups such as Figure Eight that explicitly provide this <em>human in the loop</em> service.</p><p data-type="footnote" id="idm140637564293600"><sup><a href="ch01.html#idm140637564293600-marker">2</a></sup> Underfitting<a data-primary="underfitting" data-type="indexterm" id="idm140637564293072"/> is another problem that may occur in building machine learning applications, but this is easier to solve. Underfitting occurs because the model is too simple—the algorithm cannot build a complex enough function approximation to make good decisions for the task at hand. To solve this, we can allow the algorithm to grow in size (have more parameters, perform more training iterations, etc.) or apply a more complicated machine learning algorithm.</p><p data-type="footnote" id="idm140637564244608"><sup><a href="ch01.html#idm140637564244608-marker">3</a></sup> This list is by no means exhaustive but does include the most commonly used machine learning algorithms.</p><p data-type="footnote" id="idm140637564235360"><sup><a href="ch01.html#idm140637564235360-marker">4</a></sup> There may be other potential issues that might make linear regression a poor choice, including outliers, correlation of error terms, and nonconstant variance of error terms.</p><p data-type="footnote" id="idm140637564424384"><sup><a href="ch01.html#idm140637564424384-marker">5</a></sup> For more on gradient boosting in machine learning competitions, consult Ben Gorman’s <a href="http://bit.ly/2S1C8Qy">blog post</a>.</p><p data-type="footnote" id="idm140637564409376"><sup><a href="ch01.html#idm140637564409376-marker">6</a></sup> For more on neutral networks, check out <a href="http://www.deeplearningbook.org/"><em>Deep Learning</em></a> by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (MIT Press).</p><p data-type="footnote" id="idm140637564325008"><sup><a href="ch01.html#idm140637564325008-marker">7</a></sup> There are faster variants of <em>k</em>-means clustering such as mini-batch <em>k</em>-means, which we cover later in the book.</p><p data-type="footnote" id="idm140637564993824"><sup><a href="ch01.html#idm140637564993824-marker">8</a></sup> Hierarchical clustering uses Euclidean distance by default, but it can also use other similarity metrics such as correlation-based distance, which we will explore in greater detail later in the book.</p><p data-type="footnote" id="idm140637564978592"><sup><a href="ch01.html#idm140637564978592-marker">9</a></sup> There are several types of autoencoders, and each learns a different set of representations. These include denoising autoencoders, sparse autoencoders, and variational autoencoders, all of which we will explore later in the book.</p><p data-type="footnote" id="idm140637564962448"><sup><a href="ch01.html#idm140637564962448-marker">10</a></sup> Backpropagation (also known as <em>backward propagation of errors</em>) is a gradient descent-based algorithm used by neural networks to update weights. In backprop, the weights of the final layer are calculated first and then used to update the weights of the preceding layers. This process continues until the weights of the very first layer are updated.</p><p data-type="footnote" id="idm140637564940720"><sup><a href="ch01.html#idm140637564940720-marker">11</a></sup> Feature detectors learn good representations of the original data, helping separate distinct elements. For example, in images, feature detectors help separate elements such as noses, eyes, mouths, etc.</p><p data-type="footnote" id="idm140637564904048"><sup><a href="ch01.html#idm140637564904048-marker">12</a></sup> Pipeline refers to a system of machine learning solutions that are applied in succession to achieve a larger objective.</p></div></div></section></body></html>