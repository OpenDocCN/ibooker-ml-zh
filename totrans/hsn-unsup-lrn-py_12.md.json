["```py\n'''Main'''\nimport numpy as np\nimport pandas as pd\nimport os, time, re\nimport pickle, gzip\n\n'''Data Viz'''\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nimport matplotlib as mpl\n\n%matplotlib inline\n\n'''Data Prep and Model Evaluation'''\nfrom sklearn import preprocessing as pp\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\n\n'''Algos'''\nimport lightgbm as lgb\n\n'''TensorFlow and Keras'''\nimport tensorflow as tf\nimport keras\nfrom keras import backend as K\nfrom keras.models import Sequential, Model\nfrom keras.layers import Activation, Dense, Dropout\nfrom keras.layers import BatchNormalization, Input, Lambda\nfrom keras import regularizers\nfrom keras.losses import mse, binary_crossentropy\n```", "```py\n# Load the data\ncurrent_path = os.getcwd()\nfile = '\\\\datasets\\\\credit_card_data\\\\credit_card.csv'\ndata = pd.read_csv(current_path + file)\n\ndataX = data.copy().drop(['Class','Time'],axis=1)\ndataY = data['Class'].copy()\n\n# Scale data\nfeaturesToScale = dataX.columns\nsX = pp.StandardScaler(copy=True, with_mean=True, with_std=True)\ndataX.loc[:,featuresToScale] = sX.fit_transform(dataX[featuresToScale])\n\n# Split into train and test\nX_train, X_test, y_train, y_test = \\\n    train_test_split(dataX, dataY, test_size=0.33, \\\n                     random_state=2018, stratify=dataY)\n\n# Drop 95% of the labels from the training set\ntoDrop = y_train[y_train==1].sample(frac=0.90,random_state=2018)\nX_train.drop(labels=toDrop.index,inplace=True)\ny_train.drop(labels=toDrop.index,inplace=True)\n```", "```py\ndef anomalyScores(originalDF, reducedDF):\n    loss = np.sum((np.array(originalDF) - \\\n                   np.array(reducedDF))**2, axis=1)\n    loss = pd.Series(data=loss,index=originalDF.index)\n    loss = (loss-np.min(loss))/(np.max(loss)-np.min(loss))\n    return loss\n```", "```py\ndef plotResults(trueLabels, anomalyScores, returnPreds = False):\n    preds = pd.concat([trueLabels, anomalyScores], axis=1)\n    preds.columns = ['trueLabel', 'anomalyScore']\n    precision, recall, thresholds = \\\n        precision_recall_curve(preds['trueLabel'], \\\n                               preds['anomalyScore'])\n    average_precision = average_precision_score( \\\n                        preds['trueLabel'], preds['anomalyScore'])\n\n    plt.step(recall, precision, color='k', alpha=0.7, where='post')\n    plt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.ylim([0.0, 1.05])\n    plt.xlim([0.0, 1.0])\n\n    plt.title('Precision-Recall curve: Average Precision = \\\n {0:0.2f}'.format(average_precision))\n\n    fpr, tpr, thresholds = roc_curve(preds['trueLabel'], \\\n                                     preds['anomalyScore'])\n    areaUnderROC = auc(fpr, tpr)\n\n    plt.figure()\n    plt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\n    plt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic: Area under the \\\n curve = {0:0.2f}'.format(areaUnderROC))\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    if returnPreds==True:\n        return preds, average_precision\n```", "```py\ndef precisionAnalysis(df, column, threshold):\n    df.sort_values(by=column, ascending=False, inplace=True)\n    threshold_value = threshold*df.trueLabel.sum()\n    i = 0\n    j = 0\n    while i < threshold_value+1:\n        if df.iloc[j][\"trueLabel\"]==1:\n            i += 1\n        j += 1\n    return df, i/j\n```", "```py\nk_fold = StratifiedKFold(n_splits=5,shuffle=True,random_state=2018)\n```", "```py\nparams_lightGB = {\n    'task': 'train',\n    'application':'binary',\n    'num_class':1,\n    'boosting': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'metric_freq':50,\n    'is_training_metric':False,\n    'max_depth':4,\n    'num_leaves': 31,\n    'learning_rate': 0.01,\n    'feature_fraction': 1.0,\n    'bagging_fraction': 1.0,\n    'bagging_freq': 0,\n    'bagging_seed': 2018,\n    'verbose': 0,\n    'num_threads':16\n}\n```", "```py\ntrainingScores = []\ncvScores = []\npredictionsBasedOnKFolds = pd.DataFrame(data=[], index=y_train.index, \\\n                                        columns=['prediction'])\n\nfor train_index, cv_index in k_fold.split(np.zeros(len(X_train)), \\\n                                          y_train.ravel()):\n    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], \\\n        X_train.iloc[cv_index,:]\n    y_train_fold, y_cv_fold = y_train.iloc[train_index], \\\n        y_train.iloc[cv_index]\n\n    lgb_train = lgb.Dataset(X_train_fold, y_train_fold)\n    lgb_eval = lgb.Dataset(X_cv_fold, y_cv_fold, reference=lgb_train)\n    gbm = lgb.train(params_lightGB, lgb_train, num_boost_round=2000,\n                   valid_sets=lgb_eval, early_stopping_rounds=200)\n\n    loglossTraining = log_loss(y_train_fold, gbm.predict(X_train_fold, \\\n                                num_iteration=gbm.best_iteration))\n    trainingScores.append(loglossTraining)\n\n    predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'] = \\\n        gbm.predict(X_cv_fold, num_iteration=gbm.best_iteration)\n    loglossCV = log_loss(y_cv_fold, \\\n        predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'])\n    cvScores.append(loglossCV)\n\n    print('Training Log Loss: ', loglossTraining)\n    print('CV Log Loss: ', loglossCV)\n\nloglossLightGBMGradientBoosting = log_loss(y_train, \\\n        predictionsBasedOnKFolds.loc[:,'prediction'])\nprint('LightGBM Gradient Boosting Log Loss: ', \\\n        loglossLightGBMGradientBoosting)\n```", "```py\noversample_multiplier = 100\n\nX_train_original = X_train.copy()\ny_train_original = y_train.copy()\nX_test_original = X_test.copy()\ny_test_original = y_test.copy()\n\nX_train_oversampled = X_train.copy()\ny_train_oversampled = y_train.copy()\nX_train_oversampled = X_train_oversampled.append( \\\n        [X_train_oversampled[y_train==1]]*oversample_multiplier, \\\n        ignore_index=False)\ny_train_oversampled = y_train_oversampled.append( \\\n        [y_train_oversampled[y_train==1]]*oversample_multiplier, \\\n        ignore_index=False)\n\nX_train = X_train_oversampled.copy()\ny_train = y_train_oversampled.copy()\n```", "```py\nmodel = Sequential()\nmodel.add(Dense(units=40, activation='linear', \\\n                activity_regularizer=regularizers.l1(10e-5), \\\n                input_dim=29,name='hidden_layer'))\nmodel.add(Dropout(0.02))\nmodel.add(Dense(units=29, activation='linear'))\n\nmodel.compile(optimizer='adam',\n              loss='mean_squared_error',\n              metrics=['accuracy'])\n\nnum_epochs = 5\nbatch_size = 32\n\nhistory = model.fit(x=X_train, y=X_train,\n                    epochs=num_epochs,\n                    batch_size=batch_size,\n                    shuffle=True,\n                    validation_split=0.20,\n                    verbose=1)\n\npredictions = model.predict(X_test, verbose=1)\nanomalyScoresAE = anomalyScores(X_test, predictions)\npreds, average_precision = plotResults(y_test, anomalyScoresAE, True)\n```", "```py\nlayer_name = 'hidden_layer'\n\nintermediate_layer_model = Model(inputs=model.input, \\\n                                 outputs=model.get_layer(layer_name).output)\nintermediate_output_train = intermediate_layer_model.predict(X_train_original)\nintermediate_output_test = intermediate_layer_model.predict(X_test_original)\n```", "```py\nintermediate_output_trainDF = \\\n    pd.DataFrame(data=intermediate_output_train,index=X_train_original.index)\nintermediate_output_testDF = \\\n    pd.DataFrame(data=intermediate_output_test,index=X_test_original.index)\n\nX_train = X_train_original.merge(intermediate_output_trainDF, \\\n                                 left_index=True,right_index=True)\nX_test = X_test_original.merge(intermediate_output_testDF, \\\n                               left_index=True,right_index=True)\ny_train = y_train_original.copy()\n```", "```py\ntrainingScores = []\ncvScores = []\npredictionsBasedOnKFolds = pd.DataFrame(data=[],index=y_train.index, \\\n                                        columns=['prediction'])\n\nfor train_index, cv_index in k_fold.split(np.zeros(len(X_train)), \\\n                                          y_train.ravel()):\n    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], \\\n        X_train.iloc[cv_index,:]\n    y_train_fold, y_cv_fold = y_train.iloc[train_index], \\\n        y_train.iloc[cv_index]\n\n    lgb_train = lgb.Dataset(X_train_fold, y_train_fold)\n    lgb_eval = lgb.Dataset(X_cv_fold, y_cv_fold, reference=lgb_train)\n    gbm = lgb.train(params_lightGB, lgb_train, num_boost_round=5000,\n                   valid_sets=lgb_eval, early_stopping_rounds=200)\n\n    loglossTraining = log_loss(y_train_fold,\n                                gbm.predict(X_train_fold, \\\n                                num_iteration=gbm.best_iteration))\n    trainingScores.append(loglossTraining)\n\n    predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'] = \\\n        gbm.predict(X_cv_fold, num_iteration=gbm.best_iteration)\n    loglossCV = log_loss(y_cv_fold, \\\n            predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'])\n    cvScores.append(loglossCV)\n\n    print('Training Log Loss: ', loglossTraining)\n    print('CV Log Loss: ', loglossCV)\n\nloglossLightGBMGradientBoosting = log_loss(y_train, \\\n                        predictionsBasedOnKFolds.loc[:,'prediction'])\nprint('LightGBM Gradient Boosting Log Loss: ', \\\n                        loglossLightGBMGradientBoosting)\n```", "```py\nfeaturesImportance = pd.DataFrame(data=list(gbm.feature_importance()), \\\n                        index=X_train.columns,columns=['featImportance'])\nfeaturesImportance = featuresImportance/featuresImportance.sum()\nfeaturesImportance.sort_values(by='featImportance', \\\n                               ascending=False,inplace=True)\nfeaturesImportance\n```"]