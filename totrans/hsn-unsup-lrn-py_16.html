<html><head></head><body><section data-pdf-bookmark="Chapter 12. Generative Adversarial Networks" data-type="chapter" epub:type="chapter"><div class="chapter" id="Chapter_12">&#13;
<h1><span class="label">Chapter 12. </span>Generative Adversarial Networks</h1>&#13;
&#13;
&#13;
<p>We<a data-primary="unsupervised deep learning" data-secondary="generative adversarial networks (GANs)" data-type="indexterm" id="UDLgener12"/> have already explored two types of generative models: RBMs and DBNs. In this chapter, we will explore <em>generative adversarial networks (GANs)</em>, one of the latest and most promising areas of unsupervised learning and generative modeling.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="GANs, the Concept" data-type="sect1"><div class="sect1" id="idm140637527578544">&#13;
<h1>GANs, the Concept</h1>&#13;
&#13;
<p>GANs<a data-primary="generative adversarial networks (GANs)" data-secondary="concept of" data-type="indexterm" id="idm140637527576784"/><a data-primary="Goodfellow, Ian" data-type="indexterm" id="idm140637527575760"/><a data-primary="discriminative models" data-type="indexterm" id="idm140637527575088"/><a data-primary="data discrimination" data-type="indexterm" id="idm140637527574416"/><a data-primary="generative unsupervised models" data-type="indexterm" id="idm140637527573744"/> were introduced by Ian Goodfellow and his fellow researchers at the University of Montreal in 2014. In GANs, we have two neural networks. One network known as the <em>generator</em> generates data based on a model it has created using samples of real data it has received as input. The other network known as the <em>discriminator</em> discriminates between the data created by the generator and data from the true distribution.</p>&#13;
&#13;
<p>As a simple analogy, the generator is the counterfeiter, and the discriminator is the police trying to identify the forgery. The two networks are locked in a zero-sum game. The generator is trying to fool the discriminator into thinking the synthetic data comes from the true distribution, and the discriminator is trying to call out the synthetic data as fake.</p>&#13;
&#13;
<p>GANs are unsupervised learning algorithms because the generator can learn the underlying structure of the true distribution even when there are no labels. The generator learns the underlying structure by using a number of parameters significantly smaller than the amount of data it has trained on—a core concept of unsupervised learning that we have explored many times in previous chapters. This constraint forces the generator to efficiently capture the most salient aspects of the true data distribution. This is similar to the representation learning that occurs in deep learning. Each hidden layer in the neutral network of a generator captures a representation of the underlying data—starting very simply—and subsequent layers pick up more complicated representations by building on the simpler preceding layers.</p>&#13;
&#13;
<p>Using all these layers together, the generator learns the underlying structure of the data and attempts to create synthetic data that is nearly identical to the true data. If the generator has captured the essence of the true data, the synthetic data will appear real.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Power of GANs" data-type="sect2"><div class="sect2" id="idm140637527568928">&#13;
<h2>The Power of GANs</h2>&#13;
&#13;
<p>In <a data-type="xref" href="ch11.html#Chapter_11">Chapter 11</a>, we<a data-primary="generative adversarial networks (GANs)" data-secondary="benefits of" data-type="indexterm" id="idm140637527566736"/> explored the ability to use synthetic data from an unsupervised learning model (such as a deep belief network) to improve the performance of a supervised learning model. Like<a data-primary="synthetic data" data-type="indexterm" id="idm140637527565472"/> DBNs, GANs are very good at generating synthetic data.</p>&#13;
&#13;
<p>If the objective is to generate a lot of new training examples to help supplement existing training data—for example, to improve accuracy on an image recognition task—we can use the generator to create a lot of synthetic data, add the new synthetic data to the original training data, and then run a supervised machine learning model on the now much larger dataset.</p>&#13;
&#13;
<p>GANs can also excel at anomaly detection. If the objective is to identify anomalies—for example, to detect fraud, hacking, or other suspicious behavior—we can use the discriminator to score each instance in the real data. The instances that the discriminator ranks as “likely synthetic” will be the most anomalous instances and also the ones most likely to represent malicious behavior.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Deep Convolutional GANs" data-type="sect1"><div class="sect1" id="idm140637527562816">&#13;
<h1>Deep Convolutional GANs</h1>&#13;
&#13;
<p>In<a data-primary="generative adversarial networks (GANs)" data-secondary="deep convolutional GANs (DCGANs)" data-type="indexterm" id="idm140637527561408"/><a data-primary="deep convolutional GANs (DCGANs)" data-type="indexterm" id="idm140637527560368"/><a data-primary="MNIST digits database" data-type="indexterm" id="idm140637527559680"/> this chapter, we will return to the MNIST dataset we used in previous chapters and apply a version of GANs to generate synthetic data to supplement the existing MNIST dataset. We will then apply a supervised learning model to perform image classification. This is yet another version of semisupervised learning.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>As<a data-primary="semisupervised learning" data-secondary="advantages of" data-type="indexterm" id="idm140637527557568"/> a side note, you should now have a much deeper appreciation for semisupervised learning. Because much of the world’s data is unlabeled, the ability of unsupervised learning to efficiently help label data by itself is very powerful. As part of such semisupervised machine learning systems, unsupervised learning enhances the potential of all successful commercial applications of supervised learning to date.</p>&#13;
&#13;
<p>Even outside of applications in semisupervised systems, unsupervised learning has potential on a standalone basis because it learns from data without any labels and is one of the fields of AI that has the greatest potential to help the machine learning community move from narrow AI to more AGI applications.</p>&#13;
</div>&#13;
&#13;
<p>The<a data-primary="Radford, Alec" data-type="indexterm" id="idm140637527554752"/><a data-primary="Metz, Luke" data-type="indexterm" id="idm140637527554016"/><a data-primary="Chintala, Soumith" data-type="indexterm" id="idm140637527553344"/> version of GANs we will use is called <em>deep convolutional generative adversarial networks (DCGANs)</em>, which were first introduced in late 2015 by Alec Radford, Luke Metz, and Soumith Chintala.<sup><a data-type="noteref" href="ch12.html#idm140637527552048" id="idm140637527552048-marker">1</a></sup></p>&#13;
&#13;
<p>DCGANs<a data-primary="convolutional neural networks (CNNs)" data-type="indexterm" id="CNN12"/><a data-primary="generative adversarial networks (GANs)" data-secondary="convolutional neural networks (CNNs)" data-type="indexterm" id="GANcnn12"/> are an unsupervised learning form of <em>convolution neural networks (CNNs)</em>, which are commonly used—and with great success—in supervised learning systems for computer vision and image classification. Before we delve into DCGANs, let’s explore CNNs first, especially how they are used for image classification in supervised learning systems.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Convolutional Neural Networks" data-type="sect1"><div class="sect1" id="idm140637527547040">&#13;
<h1>Convolutional Neural Networks</h1>&#13;
&#13;
<p>Compared to numerical and text data, images and video are considerably more computationally expensive to work with. For instance, a 4K Ultra HD image has dimensions of 4096 x 2160 x 3 (26,542,080) in total. Training a neural network on images of this resolution directly would require tens of millions of neurons and result in very long training times.</p>&#13;
&#13;
<p>Instead of building a neural network directly on the raw images, we can take advantage of certain properties of images, namely that pixels are related to other pixels that are close by but not usually related to other pixels that are far away.</p>&#13;
&#13;
<p><em>Convolution</em> (from which convolutional neural networks derive their name) is the process of filtering the image to decrease the size of the image without losing the relationships among pixels.<sup><a data-type="noteref" href="ch12.html#idm140637527543376" id="idm140637527543376-marker">2</a></sup></p>&#13;
&#13;
<p>On<a data-primary="kernel size" data-type="indexterm" id="idm140637527541376"/><a data-primary="stride" data-type="indexterm" id="idm140637527540640"/> the original image, we apply several filters of a certain size, known as the <em>kernel size</em>, and move these filters with a small step, known as the <em>stride</em>, to derive the new reduced pixel output. After<a data-primary="max pooling" data-type="indexterm" id="idm140637527539040"/> the convolution, we reduce the size of the representation further by taking the max of the pixels in the reduced pixel output, one small area at a time. This is known as <em>max pooling</em>.</p>&#13;
&#13;
<p>We perform this convolution and max pooling several times to reduce the complexity of the images. Then, we flatten the images and use a normal fully connected layer to perform image classification.</p>&#13;
&#13;
<p>Let’s now build a CNN and use it to perform image classification on the MNIST dataset. First, we will load the necessary libraries:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="sd">'''Main'''</code>&#13;
<code class="kn">import</code> <code class="nn">numpy</code> <code class="kn">as</code> <code class="nn">np</code>&#13;
<code class="kn">import</code> <code class="nn">pandas</code> <code class="kn">as</code> <code class="nn">pd</code>&#13;
<code class="kn">import</code> <code class="nn">os</code><code class="o">,</code> <code class="nn">time</code><code class="o">,</code> <code class="nn">re</code>&#13;
<code class="kn">import</code> <code class="nn">pickle</code><code class="o">,</code> <code class="nn">gzip</code><code class="o">,</code> <code class="nn">datetime</code>&#13;
&#13;
<code class="sd">'''Data Viz'''</code>&#13;
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="kn">as</code> <code class="nn">plt</code>&#13;
<code class="kn">import</code> <code class="nn">seaborn</code> <code class="kn">as</code> <code class="nn">sns</code>&#13;
<code class="n">color</code> <code class="o">=</code> <code class="n">sns</code><code class="o">.</code><code class="n">color_palette</code><code class="p">()</code>&#13;
<code class="kn">import</code> <code class="nn">matplotlib</code> <code class="kn">as</code> <code class="nn">mpl</code>&#13;
<code class="kn">from</code> <code class="nn">mpl_toolkits.axes_grid1</code> <code class="kn">import</code> <code class="n">Grid</code>&#13;
&#13;
<code class="o">%</code><code class="n">matplotlib</code> <code class="n">inline</code>&#13;
&#13;
<code class="sd">'''Data Prep and Model Evaluation'''</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">preprocessing</code> <code class="k">as</code> <code class="n">pp</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">StratifiedKFold</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">log_loss</code><code class="p">,</code> <code class="n">accuracy_score</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">precision_recall_curve</code><code class="p">,</code> <code class="n">average_precision_score</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">roc_curve</code><code class="p">,</code> <code class="n">auc</code><code class="p">,</code> <code class="n">roc_auc_score</code><code class="p">,</code> <code class="n">mean_squared_error</code>&#13;
&#13;
<code class="sd">'''Algos'''</code>&#13;
<code class="kn">import</code> <code class="nn">lightgbm</code> <code class="kn">as</code> <code class="nn">lgb</code>&#13;
&#13;
<code class="sd">'''TensorFlow and Keras'''</code>&#13;
<code class="kn">import</code> <code class="nn">tensorflow</code> <code class="kn">as</code> <code class="nn">tf</code>&#13;
<code class="kn">import</code> <code class="nn">keras</code>&#13;
<code class="kn">from</code> <code class="nn">keras</code> <code class="kn">import</code> <code class="n">backend</code> <code class="k">as</code> <code class="n">K</code>&#13;
<code class="kn">from</code> <code class="nn">keras.models</code> <code class="kn">import</code> <code class="n">Sequential</code><code class="p">,</code> <code class="n">Model</code>&#13;
<code class="kn">from</code> <code class="nn">keras.layers</code> <code class="kn">import</code> <code class="n">Activation</code><code class="p">,</code> <code class="n">Dense</code><code class="p">,</code> <code class="n">Dropout</code><code class="p">,</code> <code class="n">Flatten</code><code class="p">,</code> <code class="n">Conv2D</code><code class="p">,</code> <code class="n">MaxPool2D</code>&#13;
<code class="kn">from</code> <code class="nn">keras.layers</code> <code class="kn">import</code> <code class="n">LeakyReLU</code><code class="p">,</code> <code class="n">Reshape</code><code class="p">,</code> <code class="n">UpSampling2D</code><code class="p">,</code> <code class="n">Conv2DTranspose</code>&#13;
<code class="kn">from</code> <code class="nn">keras.layers</code> <code class="kn">import</code> <code class="n">BatchNormalization</code><code class="p">,</code> <code class="n">Input</code><code class="p">,</code> <code class="n">Lambda</code>&#13;
<code class="kn">from</code> <code class="nn">keras.layers</code> <code class="kn">import</code> <code class="n">Embedding</code><code class="p">,</code> <code class="n">Flatten</code><code class="p">,</code> <code class="n">dot</code>&#13;
<code class="kn">from</code> <code class="nn">keras</code> <code class="kn">import</code> <code class="n">regularizers</code>&#13;
<code class="kn">from</code> <code class="nn">keras.losses</code> <code class="kn">import</code> <code class="n">mse</code><code class="p">,</code> <code class="n">binary_crossentropy</code>&#13;
<code class="kn">from</code> <code class="nn">IPython.display</code> <code class="kn">import</code> <code class="n">SVG</code>&#13;
<code class="kn">from</code> <code class="nn">keras.utils.vis_utils</code> <code class="kn">import</code> <code class="n">model_to_dot</code>&#13;
<code class="kn">from</code> <code class="nn">keras.optimizers</code> <code class="kn">import</code> <code class="n">Adam</code><code class="p">,</code> <code class="n">RMSprop</code>&#13;
<code class="kn">from</code> <code class="nn">tensorflow.examples.tutorials.mnist</code> <code class="kn">import</code> <code class="n">input_data</code></pre>&#13;
&#13;
<p>Next, we will load the MNIST datasets and store the image data in a 4D tensor since Keras requires image data in this format. We will also create one-hot vectors from the labels using<a data-primary="to_categorical function" data-type="indexterm" id="idm140637527535552"/><a data-primary="Keras" data-secondary="to_categorical function" data-type="indexterm" id="idm140637527534944"/> the <code>to_categorical</code> function in Keras.</p>&#13;
&#13;
<p>For<a data-primary="view_digit function" data-type="indexterm" id="idm140637527533040"/> use later, we will create Pandas DataFrames from the data, too. And, let’s reuse the <code>view_digit</code> function from earlier in the book to view the images:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load the datasets</code>&#13;
<code class="n">current_path</code> <code class="o">=</code> <code class="n">os</code><code class="o">.</code><code class="n">getcwd</code><code class="p">()</code>&#13;
<code class="nb">file</code> <code class="o">=</code> <code class="s1">'</code><code class="se">\\</code><code class="s1">datasets</code><code class="se">\\</code><code class="s1">mnist_data</code><code class="se">\\</code><code class="s1">mnist.pkl.gz'</code>&#13;
<code class="n">f</code> <code class="o">=</code> <code class="n">gzip</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">current_path</code><code class="o">+</code><code class="nb">file</code><code class="p">,</code> <code class="s1">'rb'</code><code class="p">)</code>&#13;
<code class="n">train_set</code><code class="p">,</code> <code class="n">validation_set</code><code class="p">,</code> <code class="n">test_set</code> <code class="o">=</code> <code class="n">pickle</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="n">f</code><code class="p">,</code> <code class="n">encoding</code><code class="o">=</code><code class="s1">'latin1'</code><code class="p">)</code>&#13;
<code class="n">f</code><code class="o">.</code><code class="n">close</code><code class="p">()</code>&#13;
&#13;
<code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code> <code class="o">=</code> <code class="n">train_set</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">train_set</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>&#13;
<code class="n">X_validation</code><code class="p">,</code> <code class="n">y_validation</code> <code class="o">=</code> <code class="n">validation_set</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">validation_set</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>&#13;
<code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">test_set</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">test_set</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>&#13;
&#13;
<code class="n">X_train_keras</code> <code class="o">=</code> <code class="n">X_train</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="mi">50000</code><code class="p">,</code><code class="mi">28</code><code class="p">,</code><code class="mi">28</code><code class="p">,</code><code class="mi">1</code><code class="p">)</code>&#13;
<code class="n">X_validation_keras</code> <code class="o">=</code> <code class="n">X_validation</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="mi">10000</code><code class="p">,</code><code class="mi">28</code><code class="p">,</code><code class="mi">28</code><code class="p">,</code><code class="mi">1</code><code class="p">)</code>&#13;
<code class="n">X_test_keras</code> <code class="o">=</code> <code class="n">X_test</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="mi">10000</code><code class="p">,</code><code class="mi">28</code><code class="p">,</code><code class="mi">28</code><code class="p">,</code><code class="mi">1</code><code class="p">)</code>&#13;
&#13;
<code class="n">y_train_keras</code> <code class="o">=</code> <code class="n">to_categorical</code><code class="p">(</code><code class="n">y_train</code><code class="p">)</code>&#13;
<code class="n">y_validation_keras</code> <code class="o">=</code> <code class="n">to_categorical</code><code class="p">(</code><code class="n">y_validation</code><code class="p">)</code>&#13;
<code class="n">y_test_keras</code> <code class="o">=</code> <code class="n">to_categorical</code><code class="p">(</code><code class="n">y_test</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Create Pandas DataFrames from the datasets</code>&#13;
<code class="n">train_index</code> <code class="o">=</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code><code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">))</code>&#13;
<code class="n">validation_index</code> <code class="o">=</code> <code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">),</code><code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code><code class="o">+</code><code class="nb">len</code><code class="p">(</code><code class="n">X_validation</code><code class="p">))</code>&#13;
<code class="n">test_index</code> <code class="o">=</code> <code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code><code class="o">+</code><code class="nb">len</code><code class="p">(</code><code class="n">X_validation</code><code class="p">),</code><code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code><code class="o">+</code> \&#13;
                   <code class="nb">len</code><code class="p">(</code><code class="n">X_validation</code><code class="p">)</code><code class="o">+</code><code class="nb">len</code><code class="p">(</code><code class="n">X_test</code><code class="p">))</code>&#13;
&#13;
<code class="n">X_train</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">train_index</code><code class="p">)</code>&#13;
<code class="n">y_train</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">y_train</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">train_index</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_validation</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_validation</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">validation_index</code><code class="p">)</code>&#13;
<code class="n">y_validation</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">y_validation</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">validation_index</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_test</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_test</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">test_index</code><code class="p">)</code>&#13;
<code class="n">y_test</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">y_test</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">test_index</code><code class="p">)</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">view_digit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">example</code><code class="p">):</code>&#13;
    <code class="n">label</code> <code class="o">=</code> <code class="n">y</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">example</code><code class="p">]</code>&#13;
    <code class="n">image</code> <code class="o">=</code> <code class="n">X</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">example</code><code class="p">,:]</code><code class="o">.</code><code class="n">values</code><code class="o">.</code><code class="n">reshape</code><code class="p">([</code><code class="mi">28</code><code class="p">,</code><code class="mi">28</code><code class="p">])</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s1">'Example: </code><code class="si">%d</code><code class="s1">  Label: </code><code class="si">%d</code><code class="s1">'</code> <code class="o">%</code> <code class="p">(</code><code class="n">example</code><code class="p">,</code> <code class="n">label</code><code class="p">))</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="n">plt</code><code class="o">.</code><code class="n">get_cmap</code><code class="p">(</code><code class="s1">'gray'</code><code class="p">))</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>&#13;
&#13;
<p>Now let’s build the CNN.</p>&#13;
&#13;
<p>We<a data-primary="Sequential()" data-type="indexterm" id="idm140637527336432"/><a data-primary="Keras" data-secondary="Sequential()" data-type="indexterm" id="idm140637527335824"/> will call <code>Sequential()</code> in Keras to begin the model creation. Then, we will add two convolution layers, each with 32 filters of a kernel size of 5 x 5, a default stride of 1, and a ReLU activation. Then, we perform max pooling with a pooling window of 2 x 2 and a stride of 1. We also perform dropout, which you may recall is a form of regularization to reduce overfitting of the neural network. Specifically, we will drop 25% of the input units.</p>&#13;
&#13;
<p>In the next stage, we add two convolution layers again, this time with 64 filters of a kernel size of 3 x 3. Then, we perform max pooling with a pooling window of 2 x 2 and a stride of 2. And, we follow this up with a dropout layer, with a dropout percentage of 25%.</p>&#13;
&#13;
<p>Finally, we flatten the images, add a regular neural network with 256 hidden units, perform dropout with a dropout percentage of 50%, and perform 10-class classification using the <code>softmax</code> function:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">model</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">()</code>&#13;
&#13;
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Conv2D</code><code class="p">(</code><code class="n">filters</code> <code class="o">=</code> <code class="mi">32</code><code class="p">,</code> <code class="n">kernel_size</code> <code class="o">=</code> <code class="p">(</code><code class="mi">5</code><code class="p">,</code><code class="mi">5</code><code class="p">),</code> <code class="n">padding</code> <code class="o">=</code> <code class="s1">'Same'</code><code class="p">,</code>&#13;
                 <code class="n">activation</code> <code class="o">=</code><code class="s1">'relu'</code><code class="p">,</code> <code class="n">input_shape</code> <code class="o">=</code> <code class="p">(</code><code class="mi">28</code><code class="p">,</code><code class="mi">28</code><code class="p">,</code><code class="mi">1</code><code class="p">)))</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Conv2D</code><code class="p">(</code><code class="n">filters</code> <code class="o">=</code> <code class="mi">32</code><code class="p">,</code> <code class="n">kernel_size</code> <code class="o">=</code> <code class="p">(</code><code class="mi">5</code><code class="p">,</code><code class="mi">5</code><code class="p">),</code> <code class="n">padding</code> <code class="o">=</code> <code class="s1">'Same'</code><code class="p">,</code>&#13;
                 <code class="n">activation</code> <code class="o">=</code><code class="s1">'relu'</code><code class="p">))</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">MaxPooling2D</code><code class="p">(</code><code class="n">pool_size</code><code class="o">=</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code><code class="mi">2</code><code class="p">)))</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dropout</code><code class="p">(</code><code class="mf">0.25</code><code class="p">))</code>&#13;
&#13;
&#13;
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Conv2D</code><code class="p">(</code><code class="n">filters</code> <code class="o">=</code> <code class="mi">64</code><code class="p">,</code> <code class="n">kernel_size</code> <code class="o">=</code> <code class="p">(</code><code class="mi">3</code><code class="p">,</code><code class="mi">3</code><code class="p">),</code> <code class="n">padding</code> <code class="o">=</code> <code class="s1">'Same'</code><code class="p">,</code>&#13;
                 <code class="n">activation</code> <code class="o">=</code><code class="s1">'relu'</code><code class="p">))</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Conv2D</code><code class="p">(</code><code class="n">filters</code> <code class="o">=</code> <code class="mi">64</code><code class="p">,</code> <code class="n">kernel_size</code> <code class="o">=</code> <code class="p">(</code><code class="mi">3</code><code class="p">,</code><code class="mi">3</code><code class="p">),</code> <code class="n">padding</code> <code class="o">=</code> <code class="s1">'Same'</code><code class="p">,</code>&#13;
                 <code class="n">activation</code> <code class="o">=</code><code class="s1">'relu'</code><code class="p">))</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">MaxPooling2D</code><code class="p">(</code><code class="n">pool_size</code><code class="o">=</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code><code class="mi">2</code><code class="p">),</code> <code class="n">strides</code><code class="o">=</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code><code class="mi">2</code><code class="p">)))</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dropout</code><code class="p">(</code><code class="mf">0.25</code><code class="p">))</code>&#13;
&#13;
&#13;
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Flatten</code><code class="p">())</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="mi">256</code><code class="p">,</code> <code class="n">activation</code> <code class="o">=</code> <code class="s2">"relu"</code><code class="p">))</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dropout</code><code class="p">(</code><code class="mf">0.5</code><code class="p">))</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="n">activation</code> <code class="o">=</code> <code class="s2">"softmax"</code><code class="p">))</code></pre>&#13;
&#13;
<p>For<a data-primary="Adam optimization algorithm" data-type="indexterm" id="idm140637526973488"/><a data-primary="algorithms" data-secondary="Adam optimization algorithm" data-type="indexterm" id="idm140637526972880"/> this CNN training, we will use the <em>Adam optimizer</em> and minimize the cross-entropy. We will also store the accuracy of the image classification as the evaluation metric.</p>&#13;
&#13;
<p>Now let’s train the model for one hundred epochs and evaluate the results on the validation set:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Train CNN</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">optimizer</code><code class="o">=</code><code class="s1">'adam'</code><code class="p">,</code>&#13;
              <code class="n">loss</code><code class="o">=</code><code class="s1">'categorical_crossentropy'</code><code class="p">,</code>&#13;
              <code class="n">metrics</code><code class="o">=</code><code class="p">[</code><code class="s1">'accuracy'</code><code class="p">])</code>&#13;
&#13;
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_keras</code><code class="p">,</code> <code class="n">y_train_keras</code><code class="p">,</code>&#13;
          <code class="n">validation_data</code><code class="o">=</code><code class="p">(</code><code class="n">X_validation_keras</code><code class="p">,</code> <code class="n">y_validation_keras</code><code class="p">),</code> \&#13;
          <code class="n">epochs</code><code class="o">=</code><code class="mi">100</code><code class="p">)</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#cnn_results">Figure 12-1</a> displays the accuracy over the one hundred epochs of training.</p>&#13;
&#13;
<figure><div class="figure" id="cnn_results">&#13;
<img alt="CNN Results" src="assets/hulp_1201.png"/>&#13;
<h6><span class="label">Figure 12-1. </span>CNN results</h6>&#13;
</div></figure>&#13;
&#13;
<p>As you can see, the CNN we just trained has a final accuracy of 99.55%, better than any of the MNIST image classification solutions we have trained so far throughout this book.<a data-primary="" data-startref="CNN12" data-type="indexterm" id="idm140637526739488"/><a data-primary="" data-startref="GANcnn12" data-type="indexterm" id="idm140637526738512"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="DCGANs Revisited" data-type="sect1"><div class="sect1" id="idm140637527546128">&#13;
<h1>DCGANs Revisited</h1>&#13;
&#13;
<p>Let’s<a data-primary="generative adversarial networks (GANs)" data-secondary="deep convolutional GANs (DCGANs)" data-type="indexterm" id="GANdeepgan12"/><a data-primary="deep convolutional GANs (DCGANs)" data-type="indexterm" id="deepgan12"/> now turn back to deep convolutional generative adversarial networks once again. We will build a generative model to produce synthetic MNIST images that are very similar to the original MNIST ones.</p>&#13;
&#13;
<p>To produce near-realistic yet synthetic images, we need to train a generator that generates new images from the original MNIST images and a discriminator that judges whether those images are believably similar to the original ones or not (essentially performing a bullshit test).</p>&#13;
&#13;
<p>Here is another way to think about this. The original MNIST dataset represents the original data distribution. The generator learns from this original distribution and generates new images based off what it has learned, and the discriminator attempts to determine whether the newly generated images are virtually indistinguishable from the original distribution or not.</p>&#13;
&#13;
<p>For the generator, we will use the architecture presented in the Radford, Metz, and Chintala paper presented at the ICLR 2016 conference, which we referenced earlier (<a data-type="xref" href="#dcgan_generator">Figure 12-2</a>).</p>&#13;
&#13;
<figure><div class="figure" id="dcgan_generator">&#13;
<img alt="DCGAN Generator" src="assets/hulp_1202.png"/>&#13;
<h6><span class="label">Figure 12-2. </span>DCGAN generator</h6>&#13;
</div></figure>&#13;
&#13;
<p>The generator takes in an initial<a data-primary="noise vector" data-type="indexterm" id="idm140637526728208"/> <em>noise vector</em>, shown as a 100 x 1 noise vector here denoted as <em>z</em>, and then projects and reshapes it into a 1024 x 4 x 4 tensor. This<a data-primary="project and reshape action" data-type="indexterm" id="idm140637526726544"/> <em>project and reshape</em> action is the opposite of convolution and is known<a data-primary="transposed convolution" data-type="indexterm" id="idm140637526725328"/><a data-primary="deconvolution" data-type="indexterm" id="idm140637526724624"/> as <em>transposed convolution</em> (or <em>deconvolution</em> in some cases). In transposed convolution, the original process of convolution is reversed, mapping a reduced tensor to a larger one.<sup><a data-type="noteref" href="ch12.html#idm140637526722816" id="idm140637526722816-marker">3</a></sup></p>&#13;
&#13;
<p>After the initial transposed convolution, the generator applies four additional deconvolution layers to map to a final 64 x 3 x 3 tensor.</p>&#13;
&#13;
<p>Here are the various stages:</p>&#13;
&#13;
<p>100 x 1 → 1024 x 4 x 4 → 512 x 8 x 8 → 256 x 16 x 16 → 128 x 32 x 32 → 64 x 64 x 3</p>&#13;
&#13;
<p>We will apply a similar (but not exact) architecture when designing a DCGAN on the MNIST dataset.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Generator of the DCGAN" data-type="sect2"><div class="sect2" id="idm140637526719456">&#13;
<h2>Generator of the DCGAN</h2>&#13;
&#13;
<p>For the DCGAN we design, we will leverage work done by Rowel Atienza and build on top of it.<sup><a data-type="noteref" href="ch12.html#idm140637526717616" id="idm140637526717616-marker">4</a></sup> We will first create a class called <em>DCGAN</em>, which we will use to build the generator, discriminator, discriminator model, and adversarial model.</p>&#13;
&#13;
<p>Let’s start with the generator. We will set several parameters for the generator, including the dropout percentage (default value of 0.3), the depth of the tensor (default value of 256), and the other dimensions (default value of 7 x 7). We will also use batch normalization with a default momentum value of 0.8. The initial input dimensions are one hundred, and the final output dimensions are 28 x 28 x 1.</p>&#13;
&#13;
<p>Recall that both dropout and batch normalization are regularizers to help the neural network we design avoid overfitting.</p>&#13;
&#13;
<p>To<a data-primary="Sequential()" data-type="indexterm" id="idm140637526713776"/><a data-primary="Keras" data-secondary="Sequential()" data-type="indexterm" id="idm140637526713040"/><a data-primary="Dense()" data-type="indexterm" id="idm140637526712096"/><a data-primary="Keras" data-secondary="Dense()" data-type="indexterm" id="idm140637526711424"/> build the generator, we call the <code>Sequential()</code> function from Keras. Then, we will add a dense, fully connected neural network layer by calling the <code>Dense()</code> function. This will have an input dimension of 100 and an output dimension of 7 x 7 x 256. We will perform batch normalization, use the ReLU activation function, and perform dropout:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">generator</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">depth</code><code class="o">=</code><code class="mi">256</code><code class="p">,</code> <code class="n">dim</code><code class="o">=</code><code class="mi">7</code><code class="p">,</code> <code class="n">dropout</code><code class="o">=</code><code class="mf">0.3</code><code class="p">,</code> <code class="n">momentum</code><code class="o">=</code><code class="mf">0.8</code><code class="p">,</code> \&#13;
              <code class="n">window</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">input_dim</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">output_depth</code><code class="o">=</code><code class="mi">1</code><code class="p">):</code>&#13;
    <code class="k">if</code> <code class="bp">self</code><code class="o">.</code><code class="n">G</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">G</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">G</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">()</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">G</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="n">dim</code><code class="o">*</code><code class="n">dim</code><code class="o">*</code><code class="n">depth</code><code class="p">,</code> <code class="n">input_dim</code><code class="o">=</code><code class="n">input_dim</code><code class="p">))</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">G</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">BatchNormalization</code><code class="p">(</code><code class="n">momentum</code><code class="o">=</code><code class="n">momentum</code><code class="p">))</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">G</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Activation</code><code class="p">(</code><code class="s1">'relu'</code><code class="p">))</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">G</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Reshape</code><code class="p">((</code><code class="n">dim</code><code class="p">,</code> <code class="n">dim</code><code class="p">,</code> <code class="n">depth</code><code class="p">)))</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">G</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dropout</code><code class="p">(</code><code class="n">dropout</code><code class="p">))</code></pre>&#13;
&#13;
<p>Next, we<a data-primary="upsampling" data-type="indexterm" id="idm140637526707216"/> will perform <em>upsampling</em> and <em>transposed convolution</em> three times. Each time, we will halve the depth of the output space from 256 to 128 to 64 to 32 while increasing the other dimensions. We will maintain a convolution window of 5 x 5 and the default stride of one. During each transposed convolution, we will perform batch normalization and use the ReLU activation function.</p>&#13;
&#13;
<p>Here is what this looks like:</p>&#13;
&#13;
<p>100 → 7 x 7 x 256 → 14 x 14 x 128 → 28 x 28 x 64 → 28 x 28 x 32 → 28 x 28 x 1</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">    <code class="bp">self</code><code class="o">.</code><code class="n">G</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">UpSampling2D</code><code class="p">())</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">G</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Conv2DTranspose</code><code class="p">(</code><code class="nb">int</code><code class="p">(</code><code class="n">depth</code><code class="o">/</code><code class="mi">2</code><code class="p">),</code> <code class="n">window</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s1">'same'</code><code class="p">))</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">G</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">BatchNormalization</code><code class="p">(</code><code class="n">momentum</code><code class="o">=</code><code class="n">momentum</code><code class="p">))</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">G</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Activation</code><code class="p">(</code><code class="s1">'relu'</code><code class="p">))</code>&#13;
&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">G</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">UpSampling2D</code><code class="p">())</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">G</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Conv2DTranspose</code><code class="p">(</code><code class="nb">int</code><code class="p">(</code><code class="n">depth</code><code class="o">/</code><code class="mi">4</code><code class="p">),</code> <code class="n">window</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s1">'same'</code><code class="p">))</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">G</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">BatchNormalization</code><code class="p">(</code><code class="n">momentum</code><code class="o">=</code><code class="n">momentum</code><code class="p">))</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">G</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Activation</code><code class="p">(</code><code class="s1">'relu'</code><code class="p">))</code>&#13;
&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">G</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Conv2DTranspose</code><code class="p">(</code><code class="nb">int</code><code class="p">(</code><code class="n">depth</code><code class="o">/</code><code class="mi">8</code><code class="p">),</code> <code class="n">window</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s1">'same'</code><code class="p">))</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">G</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">BatchNormalization</code><code class="p">(</code><code class="n">momentum</code><code class="o">=</code><code class="n">momentum</code><code class="p">))</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">G</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Activation</code><code class="p">(</code><code class="s1">'relu'</code><code class="p">))</code></pre>&#13;
&#13;
<p>Finally, the generator will output a 28 x 28 image, which has the same dimensions as the original MNIST image:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">    <code class="bp">self</code><code class="o">.</code><code class="n">G</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Conv2DTranspose</code><code class="p">(</code><code class="n">output_depth</code><code class="p">,</code> <code class="n">window</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s1">'same'</code><code class="p">))</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">G</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Activation</code><code class="p">(</code><code class="s1">'sigmoid'</code><code class="p">))</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">G</code><code class="o">.</code><code class="n">summary</code><code class="p">()</code>&#13;
    <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">G</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Discriminator of the DCGAN" data-type="sect2"><div class="sect2" id="idm140637526718832">&#13;
<h2>Discriminator of the DCGAN</h2>&#13;
&#13;
<p>For the discriminator, we will set the default dropout percentage to 0.3, the depth as 64, and the alpha for the <code>LeakyReLU</code> function as 0.3.<sup><a data-type="noteref" href="ch12.html#idm140637525902512" id="idm140637525902512-marker">5</a></sup></p>&#13;
&#13;
<p>First, we will load a 28 x 28 x 1 image and perform convolution using 64 channels, a filter of 5 x 5, and a stride of two. We will use <code>LeakyReLU</code> as the activation function and perform dropout. We will continue this process three more times, doubling the depth of the output space each time while decreasing the other dimensions. For each convolution, we will use the <code>LeakyReLU</code> activation function and dropout.</p>&#13;
&#13;
<p>Finally, we will flatten the images and use the sigmoid function to output a probability. This probability designates the discriminator’s confidence in calling the input image a fake (where 0.0 is fake and 1.0 is real).</p>&#13;
&#13;
<p>Here is what this looks like:</p>&#13;
&#13;
<p>28 x 28 x 1 → 14 x 14 x 64 → 7 x 7 x 128 → 4 x 4 x 256 → 4 x 4 x 512 → 1</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">discriminator</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">depth</code><code class="o">=</code><code class="mi">64</code><code class="p">,</code> <code class="n">dropout</code><code class="o">=</code><code class="mf">0.3</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.3</code><code class="p">):</code>&#13;
    <code class="k">if</code> <code class="bp">self</code><code class="o">.</code><code class="n">D</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">D</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">D</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">()</code>&#13;
    <code class="n">input_shape</code> <code class="o">=</code> <code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">img_rows</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">img_cols</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">channel</code><code class="p">)</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">D</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Conv2D</code><code class="p">(</code><code class="n">depth</code><code class="o">*</code><code class="mi">1</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="n">strides</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">input_shape</code><code class="o">=</code><code class="n">input_shape</code><code class="p">,</code>&#13;
        <code class="n">padding</code><code class="o">=</code><code class="s1">'same'</code><code class="p">))</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">D</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">LeakyReLU</code><code class="p">(</code><code class="n">alpha</code><code class="o">=</code><code class="n">alpha</code><code class="p">))</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">D</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dropout</code><code class="p">(</code><code class="n">dropout</code><code class="p">))</code>&#13;
&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">D</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Conv2D</code><code class="p">(</code><code class="n">depth</code><code class="o">*</code><code class="mi">2</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="n">strides</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s1">'same'</code><code class="p">))</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">D</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">LeakyReLU</code><code class="p">(</code><code class="n">alpha</code><code class="o">=</code><code class="n">alpha</code><code class="p">))</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">D</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dropout</code><code class="p">(</code><code class="n">dropout</code><code class="p">))</code>&#13;
&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">D</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Conv2D</code><code class="p">(</code><code class="n">depth</code><code class="o">*</code><code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="n">strides</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s1">'same'</code><code class="p">))</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">D</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">LeakyReLU</code><code class="p">(</code><code class="n">alpha</code><code class="o">=</code><code class="n">alpha</code><code class="p">))</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">D</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dropout</code><code class="p">(</code><code class="n">dropout</code><code class="p">))</code>&#13;
&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">D</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Conv2D</code><code class="p">(</code><code class="n">depth</code><code class="o">*</code><code class="mi">8</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="n">strides</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s1">'same'</code><code class="p">))</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">D</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">LeakyReLU</code><code class="p">(</code><code class="n">alpha</code><code class="o">=</code><code class="n">alpha</code><code class="p">))</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">D</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dropout</code><code class="p">(</code><code class="n">dropout</code><code class="p">))</code>&#13;
&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">D</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Flatten</code><code class="p">())</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">D</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">))</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">D</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">Activation</code><code class="p">(</code><code class="s1">'sigmoid'</code><code class="p">))</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">D</code><code class="o">.</code><code class="n">summary</code><code class="p">()</code>&#13;
    <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">D</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Discriminator and Adversarial Models" data-type="sect2"><div class="sect2" id="idm140637525895120">&#13;
<h2>Discriminator and Adversarial Models</h2>&#13;
&#13;
<p>Next, let’s define the discriminator model (i.e., the police detecting the fakes) and the adversarial model (i.e., the counterfeiter learning from the police). For both the adversarial and the discriminator model, we will use the RMSprop optimizer, define the loss function as binary cross-entropy, and use accuracy as our reported metric.</p>&#13;
&#13;
<p>For the adversarial model, we use the generator and discriminator networks we defined earlier. For the discriminator model, we use just the discriminator network:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">discriminator_model</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>&#13;
    <code class="k">if</code> <code class="bp">self</code><code class="o">.</code><code class="n">DM</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">DM</code>&#13;
    <code class="n">optimizer</code> <code class="o">=</code> <code class="n">RMSprop</code><code class="p">(</code><code class="n">lr</code><code class="o">=</code><code class="mf">0.0002</code><code class="p">,</code> <code class="n">decay</code><code class="o">=</code><code class="mf">6e-8</code><code class="p">)</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">DM</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">()</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">DM</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">discriminator</code><code class="p">())</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">DM</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s1">'binary_crossentropy'</code><code class="p">,</code> \&#13;
                    <code class="n">optimizer</code><code class="o">=</code><code class="n">optimizer</code><code class="p">,</code> <code class="n">metrics</code><code class="o">=</code><code class="p">[</code><code class="s1">'accuracy'</code><code class="p">])</code>&#13;
    <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">DM</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">adversarial_model</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>&#13;
    <code class="k">if</code> <code class="bp">self</code><code class="o">.</code><code class="n">AM</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">AM</code>&#13;
    <code class="n">optimizer</code> <code class="o">=</code> <code class="n">RMSprop</code><code class="p">(</code><code class="n">lr</code><code class="o">=</code><code class="mf">0.0001</code><code class="p">,</code> <code class="n">decay</code><code class="o">=</code><code class="mf">3e-8</code><code class="p">)</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">AM</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">()</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">AM</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">generator</code><code class="p">())</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">AM</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">discriminator</code><code class="p">())</code>&#13;
    <code class="bp">self</code><code class="o">.</code><code class="n">AM</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">loss</code><code class="o">=</code><code class="s1">'binary_crossentropy'</code><code class="p">,</code> \&#13;
                    <code class="n">optimizer</code><code class="o">=</code><code class="n">optimizer</code><code class="p">,</code> <code class="n">metrics</code><code class="o">=</code><code class="p">[</code><code class="s1">'accuracy'</code><code class="p">])</code>&#13;
    <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">AM</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="DCGAN for the MNIST Dataset" data-type="sect2"><div class="sect2" id="idm140637525729872">&#13;
<h2>DCGAN for the MNIST Dataset</h2>&#13;
&#13;
<p>Now let’s define the DCGAN for the MNIST dataset. First, we will initialize the <code>MNIST_DCGAN</code> class for the 28 x 28 x 1 MNIST images and use the generator, discriminator model, and adversarial model from earlier:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">class</code> <code class="nc">MNIST_DCGAN</code><code class="p">(</code><code class="nb">object</code><code class="p">):</code>&#13;
    <code class="k">def</code> <code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x_train</code><code class="p">):</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">img_rows</code> <code class="o">=</code> <code class="mi">28</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">img_cols</code> <code class="o">=</code> <code class="mi">28</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">channel</code> <code class="o">=</code> <code class="mi">1</code>&#13;
&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">x_train</code> <code class="o">=</code> <code class="n">x_train</code>&#13;
&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">DCGAN</code> <code class="o">=</code> <code class="n">DCGAN</code><code class="p">()</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">discriminator</code> <code class="o">=</code>  <code class="bp">self</code><code class="o">.</code><code class="n">DCGAN</code><code class="o">.</code><code class="n">discriminator_model</code><code class="p">()</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">adversarial</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">DCGAN</code><code class="o">.</code><code class="n">adversarial_model</code><code class="p">()</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">generator</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">DCGAN</code><code class="o">.</code><code class="n">generator</code><code class="p">()</code></pre>&#13;
&#13;
<p>The <code>train</code> function will train for a default two thousand training epochs and use a batch size of 256. In this function, we will feed batches of images into the DCGAN architecture we just defined. The generator will generate images, and the discriminator will call out images as real or fake. As the generator and discriminator duke it out in this adversarial model, the synthetic images become more and more similar to the original MNIST images:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">train</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">train_steps</code><code class="o">=</code><code class="mi">2000</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">256</code><code class="p">,</code> <code class="n">save_interval</code><code class="o">=</code><code class="mi">0</code><code class="p">):</code>&#13;
    <code class="n">noise_input</code> <code class="o">=</code> <code class="bp">None</code>&#13;
    <code class="k">if</code> <code class="n">save_interval</code><code class="o">&gt;</code><code class="mi">0</code><code class="p">:</code>&#13;
        <code class="n">noise_input</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="o">-</code><code class="mf">1.0</code><code class="p">,</code> <code class="mf">1.0</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="p">[</code><code class="mi">16</code><code class="p">,</code> <code class="mi">100</code><code class="p">])</code>&#13;
    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">train_steps</code><code class="p">):</code>&#13;
        <code class="n">images_train</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">x_train</code><code class="p">[</code><code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">randint</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code>&#13;
            <code class="bp">self</code><code class="o">.</code><code class="n">x_train</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">size</code><code class="o">=</code><code class="n">batch_size</code><code class="p">),</code> <code class="p">:,</code> <code class="p">:,</code> <code class="p">:]</code>&#13;
        <code class="n">noise</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="o">-</code><code class="mf">1.0</code><code class="p">,</code> <code class="mf">1.0</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="p">[</code><code class="n">batch_size</code><code class="p">,</code> <code class="mi">100</code><code class="p">])</code>&#13;
        <code class="n">images_fake</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">generator</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">noise</code><code class="p">)</code>&#13;
        <code class="n">x</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">concatenate</code><code class="p">((</code><code class="n">images_train</code><code class="p">,</code> <code class="n">images_fake</code><code class="p">))</code>&#13;
        <code class="n">y</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">ones</code><code class="p">([</code><code class="mi">2</code><code class="o">*</code><code class="n">batch_size</code><code class="p">,</code> <code class="mi">1</code><code class="p">])</code>&#13;
        <code class="n">y</code><code class="p">[</code><code class="n">batch_size</code><code class="p">:,</code> <code class="p">:]</code> <code class="o">=</code> <code class="mi">0</code>&#13;
&#13;
        <code class="n">d_loss</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">discriminator</code><code class="o">.</code><code class="n">train_on_batch</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>&#13;
&#13;
        <code class="n">y</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">ones</code><code class="p">([</code><code class="n">batch_size</code><code class="p">,</code> <code class="mi">1</code><code class="p">])</code>&#13;
        <code class="n">noise</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="o">-</code><code class="mf">1.0</code><code class="p">,</code> <code class="mf">1.0</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="p">[</code><code class="n">batch_size</code><code class="p">,</code> <code class="mi">100</code><code class="p">])</code>&#13;
        <code class="n">a_loss</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">adversarial</code><code class="o">.</code><code class="n">train_on_batch</code><code class="p">(</code><code class="n">noise</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>&#13;
        <code class="n">log_mesg</code> <code class="o">=</code> <code class="s2">"</code><code class="si">%d</code><code class="s2">: [D loss: </code><code class="si">%f</code><code class="s2">, acc: </code><code class="si">%f</code><code class="s2">]"</code> <code class="o">%</code> <code class="p">(</code><code class="n">i</code><code class="p">,</code> <code class="n">d_loss</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">d_loss</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code>&#13;
        <code class="n">log_mesg</code> <code class="o">=</code> <code class="s2">"</code><code class="si">%s</code><code class="s2">  [A loss: </code><code class="si">%f</code><code class="s2">, acc: </code><code class="si">%f</code><code class="s2">]"</code> <code class="o">%</code> <code class="p">(</code><code class="n">log_mesg</code><code class="p">,</code> <code class="n">a_loss</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> \&#13;
                                                  <code class="n">a_loss</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code>&#13;
        <code class="k">print</code><code class="p">(</code><code class="n">log_mesg</code><code class="p">)</code>&#13;
        <code class="k">if</code> <code class="n">save_interval</code><code class="o">&gt;</code><code class="mi">0</code><code class="p">:</code>&#13;
            <code class="k">if</code> <code class="p">(</code><code class="n">i</code><code class="o">+</code><code class="mi">1</code><code class="p">)</code><code class="o">%</code><code class="n">save_interval</code><code class="o">==</code><code class="mi">0</code><code class="p">:</code>&#13;
                <code class="bp">self</code><code class="o">.</code><code class="n">plot_images</code><code class="p">(</code><code class="n">save2file</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> \&#13;
                    <code class="n">samples</code><code class="o">=</code><code class="n">noise_input</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code>\&#13;
                    <code class="n">noise</code><code class="o">=</code><code class="n">noise_input</code><code class="p">,</code> <code class="n">step</code><code class="o">=</code><code class="p">(</code><code class="n">i</code><code class="o">+</code><code class="mi">1</code><code class="p">))</code></pre>&#13;
&#13;
<p>Let’s<a data-primary="" data-startref="GANdeepgan12" data-type="indexterm" id="idm140637525619232"/><a data-primary="" data-startref="deepgan12" data-type="indexterm" id="idm140637525618384"/> also define a function to plot the images generated by this DCGAN model:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">plot_images</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">save2file</code><code class="o">=</code><code class="bp">False</code><code class="p">,</code> <code class="n">fake</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">samples</code><code class="o">=</code><code class="mi">16</code><code class="p">,</code> \&#13;
                <code class="n">noise</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code> <code class="n">step</code><code class="o">=</code><code class="mi">0</code><code class="p">):</code>&#13;
    <code class="n">filename</code> <code class="o">=</code> <code class="s1">'mnist.png'</code>&#13;
    <code class="k">if</code> <code class="n">fake</code><code class="p">:</code>&#13;
        <code class="k">if</code> <code class="n">noise</code> <code class="ow">is</code> <code class="bp">None</code><code class="p">:</code>&#13;
            <code class="n">noise</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="o">-</code><code class="mf">1.0</code><code class="p">,</code> <code class="mf">1.0</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="p">[</code><code class="n">samples</code><code class="p">,</code> <code class="mi">100</code><code class="p">])</code>&#13;
        <code class="k">else</code><code class="p">:</code>&#13;
            <code class="n">filename</code> <code class="o">=</code> <code class="s2">"mnist_</code><code class="si">%d</code><code class="s2">.png"</code> <code class="o">%</code> <code class="n">step</code>&#13;
        <code class="n">images</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">generator</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">noise</code><code class="p">)</code>&#13;
    <code class="k">else</code><code class="p">:</code>&#13;
        <code class="n">i</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">randint</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">x_train</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">samples</code><code class="p">)</code>&#13;
        <code class="n">images</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">x_train</code><code class="p">[</code><code class="n">i</code><code class="p">,</code> <code class="p">:,</code> <code class="p">:,</code> <code class="p">:]</code>&#13;
&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code><code class="mi">10</code><code class="p">))</code>&#13;
    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">images</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]):</code>&#13;
        <code class="n">plt</code><code class="o">.</code><code class="n">subplot</code><code class="p">(</code><code class="mi">4</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="n">i</code><code class="o">+</code><code class="mi">1</code><code class="p">)</code>&#13;
        <code class="n">image</code> <code class="o">=</code> <code class="n">images</code><code class="p">[</code><code class="n">i</code><code class="p">,</code> <code class="p">:,</code> <code class="p">:,</code> <code class="p">:]</code>&#13;
        <code class="n">image</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">image</code><code class="p">,</code> <code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">img_rows</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">img_cols</code><code class="p">])</code>&#13;
        <code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'gray'</code><code class="p">)</code>&#13;
        <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s1">'off'</code><code class="p">)</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">tight_layout</code><code class="p">()</code>&#13;
    <code class="k">if</code> <code class="n">save2file</code><code class="p">:</code>&#13;
        <code class="n">plt</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="n">filename</code><code class="p">)</code>&#13;
        <code class="n">plt</code><code class="o">.</code><code class="n">close</code><code class="p">(</code><code class="s1">'all'</code><code class="p">)</code>&#13;
    <code class="k">else</code><code class="p">:</code>&#13;
        <code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="MNIST DCGAN in Action" data-type="sect1"><div class="sect1" id="idm140637526737312">&#13;
<h1>MNIST DCGAN in Action</h1>&#13;
&#13;
<p>Now<a data-primary="generative adversarial networks (GANs)" data-secondary="MNIST DCGAN operation" data-type="indexterm" id="GANmnist12"/> that we have defined the <code>MNIST_DCGAN</code> call, let’s call it and begin the training process. We will train for 10,000 epochs with a batch size of 256:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Initialize MNIST_DCGAN and train</code>&#13;
<code class="n">mnist_dcgan</code> <code class="o">=</code> <code class="n">MNIST_DCGAN</code><code class="p">(</code><code class="n">X_train_keras</code><code class="p">)</code>&#13;
<code class="n">timer</code> <code class="o">=</code> <code class="n">ElapsedTimer</code><code class="p">()</code>&#13;
<code class="n">mnist_dcgan</code><code class="o">.</code><code class="n">train</code><code class="p">(</code><code class="n">train_steps</code><code class="o">=</code><code class="mi">10000</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">256</code><code class="p">,</code> <code class="n">save_interval</code><code class="o">=</code><code class="mi">500</code><code class="p">)</code></pre>&#13;
&#13;
<p>The following code displays the loss and the accuracy of the discriminator and the adversarial model:</p>&#13;
&#13;
<pre data-type="programlisting">0:  [D loss: 0.692640, acc: 0.527344] [A loss: 1.297974, acc: 0.000000]&#13;
1:  [D loss: 0.651119, acc: 0.500000] [A loss: 0.920461, acc: 0.000000]&#13;
2:  [D loss: 0.735192, acc: 0.500000] [A loss: 1.289153, acc: 0.000000]&#13;
3:  [D loss: 0.556142, acc: 0.947266] [A loss: 1.218020, acc: 0.000000]&#13;
4:  [D loss: 0.492492, acc: 0.994141] [A loss: 1.306247, acc: 0.000000]&#13;
5:  [D loss: 0.491894, acc: 0.916016] [A loss: 1.722399, acc: 0.000000]&#13;
6:  [D loss: 0.607124, acc: 0.527344] [A loss: 1.698651, acc: 0.000000]&#13;
7:  [D loss: 0.578594, acc: 0.921875] [A loss: 1.042844, acc: 0.000000]&#13;
8:  [D loss: 0.509973, acc: 0.587891] [A loss: 1.957741, acc: 0.000000]&#13;
9:  [D loss: 0.538314, acc: 0.896484] [A loss: 1.133667, acc: 0.000000]&#13;
10: [D loss: 0.510218, acc: 0.572266] [A loss: 1.855000, acc: 0.000000]&#13;
11: [D loss: 0.501239, acc: 0.923828] [A loss: 1.098140, acc: 0.000000]&#13;
12: [D loss: 0.509211, acc: 0.519531] [A loss: 1.911793, acc: 0.000000]&#13;
13: [D loss: 0.482305, acc: 0.923828] [A loss: 1.187290, acc: 0.000000]&#13;
14: [D loss: 0.395886, acc: 0.900391] [A loss: 1.465053, acc: 0.000000]&#13;
15: [D loss: 0.346876, acc: 0.992188] [A loss: 1.443823, acc: 0.000000]</pre>&#13;
&#13;
<p>The initial loss of the discriminator fluctuates wildly but remains considerably above 0.50. In other words, the discriminator is initially very good at catching the poorly constructed counterfeits from the generator. Then, as the generator becomes better at creating counterfeits, the discriminator struggles; its accuracy drops close to 0.50:</p>&#13;
&#13;
<pre data-type="programlisting">9985: [D loss: 0.696480, acc: 0.521484] [A loss: 0.955954, acc: 0.125000]&#13;
9986: [D loss: 0.716583, acc: 0.472656] [A loss: 0.761385, acc: 0.363281]&#13;
9987: [D loss: 0.710941, acc: 0.533203] [A loss: 0.981265, acc: 0.074219]&#13;
9988: [D loss: 0.703731, acc: 0.515625] [A loss: 0.679451, acc: 0.558594]&#13;
9989: [D loss: 0.722460, acc: 0.492188] [A loss: 0.899768, acc: 0.125000]&#13;
9990: [D loss: 0.691914, acc: 0.539062] [A loss: 0.726867, acc: 0.464844]&#13;
9991: [D loss: 0.716197, acc: 0.500000] [A loss: 0.932500, acc: 0.144531]&#13;
9992: [D loss: 0.689704, acc: 0.548828] [A loss: 0.734389, acc: 0.414062]&#13;
9993: [D loss: 0.714405, acc: 0.517578] [A loss: 0.850408, acc: 0.218750]&#13;
9994: [D loss: 0.690414, acc: 0.550781] [A loss: 0.766320, acc: 0.355469]&#13;
9995: [D loss: 0.709792, acc: 0.511719] [A loss: 0.960070, acc: 0.105469]&#13;
9996: [D loss: 0.695851, acc: 0.500000] [A loss: 0.774395, acc: 0.324219]&#13;
9997: [D loss: 0.712254, acc: 0.521484] [A loss: 0.853828, acc: 0.183594]&#13;
9998: [D loss: 0.702689, acc: 0.529297] [A loss: 0.802785, acc: 0.308594]&#13;
9999: [D loss: 0.698032, acc: 0.517578] [A loss: 0.810278, acc: 0.304688]</pre>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Synthetic Image Generation" data-type="sect2"><div class="sect2" id="idm140637524886272">&#13;
<h2>Synthetic Image Generation</h2>&#13;
&#13;
<p>Now<a data-primary="synthetic image generation" data-type="indexterm" id="idm140637524884736"/> that the MNIST DCGAN has been trained, let’s use it to generate a sample of synthetic images (<a data-type="xref" href="#synthetic_images_generated_by_the_mnist_dcgan">Figure 12-3</a>).</p>&#13;
&#13;
<figure><div class="figure" id="synthetic_images_generated_by_the_mnist_dcgan">&#13;
<img alt="Synthetic Images Generated by the MNIST DCGAN" src="assets/hulp_1203.png"/>&#13;
<h6><span class="label">Figure 12-3. </span>Synthetic images generated by the MNIST DCGAN</h6>&#13;
</div></figure>&#13;
&#13;
<p>These synthetic images—while not entirely indistinguishable from the real MNIST dataset—are eerily similar to real digits. With more training time, the MNIST DCGAN should be capable of generating synthetic images that more closely resemble those of the real MNIST dataset and could be used to supplement the size of that dataset.</p>&#13;
&#13;
<p>While our solution is reasonably good, there are many ways to make the MNIST DCGAN perform better. The paper <a href="https://arxiv.org/pdf/1606.03498.pdf">“Improved Techniques for Training GANs”</a> and the accompanying <a href="https://github.com/openai/improved-gan">code</a> delves into more advanced methods to improve GAN <span class="keep-together">performance</span>.<a data-primary="" data-startref="GANmnist12" data-type="indexterm" id="idm140637524877440"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="idm140637524876304">&#13;
<h1>Conclusion</h1>&#13;
&#13;
<p>In this chapter, we explored deep convolutional generative adversarial networks, a specialized form of generative adversarial networks that perform well on image and computer vision datasets.</p>&#13;
&#13;
<p>GANs are a generative model with two neural networks locked in a zero-sum game. One of the networks, the generator (i.e., the counterfeiter), is generating synthetic data from real data, while the other network, the discriminator (i.e, the police), is calling the counterfeits fake or real.<sup><a data-type="noteref" href="ch12.html#idm140637524873840" id="idm140637524873840-marker">6</a></sup> This zero-sum game in which the generator learns from the discriminator leads to an overall generative model that generates pretty realistic synthetic data and generally gets better over time (i.e., as we train for more <span class="keep-together">training</span> epochs).</p>&#13;
&#13;
<p>GANs are relatively new—they were first introduced by Ian Goodfellow et al. in 2014.<sup><a data-type="noteref" href="ch12.html#idm140637524871088" id="idm140637524871088-marker">7</a></sup> GANs are currently mainly used to perform anomaly detection and generate synthetic data, but they could have many other applications in the near future. The machine learning community is barely scratching the surface with what is possible, and, if you decide to use GANs in applied machine learning systems, be ready to experiment a lot.<sup><a data-type="noteref" href="ch12.html#idm140637524869328" id="idm140637524869328-marker">8</a></sup></p>&#13;
&#13;
<p>In <a data-type="xref" href="ch13.html#Chapter_13">Chapter 13</a>, we will conclude this part of the book by exploring temporal clustering, which is a form of unsupervised learning for use with time series data.<a data-primary="" data-startref="UDLgener12" data-type="indexterm" id="idm140637524865840"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm140637527552048"><sup><a href="ch12.html#idm140637527552048-marker">1</a></sup> For more on DCGANs, take a look at the <a href="https://arxiv.org/abs/1511.06434">official paper on the topic</a>.</p><p data-type="footnote" id="idm140637527543376"><sup><a href="ch12.html#idm140637527543376-marker">2</a></sup> For more on convolution layers, read <a href="http://bit.ly/2GeMQfu">“An Introduction to Different Types of Convolutions in Deep Learning”</a>.</p><p data-type="footnote" id="idm140637526722816"><sup><a href="ch12.html#idm140637526722816-marker">3</a></sup> For more on convolution layers, check out <a href="http://bit.ly/2GeMQfu">“An Introduction to Different Types of Convolutions in Deep Learning”</a>, also referenced earlier in the chapter.</p><p data-type="footnote" id="idm140637526717616"><sup><a href="ch12.html#idm140637526717616-marker">4</a></sup> For the original code base, visit <a href="http://bit.ly/2DLp4G1">Rowel Atienza’s GitHub page</a>.</p><p data-type="footnote" id="idm140637525902512"><sup><a href="ch12.html#idm140637525902512-marker">5</a></sup> <code>LeakyReLU</code> (<a href="https://keras.io/layers/advanced-activations/"><em class="hyperlink">https://keras.io/layers/advanced-activations/</em></a>) is an advanced activation function that is similar to the normal ReLU but allows a small gradient when the unit is not active. It is becoming a preferred activation function for image machine learning problems.</p><p data-type="footnote" id="idm140637524873840"><sup><a href="ch12.html#idm140637524873840-marker">6</a></sup> For additional information, check out <a href="https://blog.openai.com/generative-models/">the OpenAI blog’s generative models post</a>.</p><p data-type="footnote" id="idm140637524871088"><sup><a href="ch12.html#idm140637524871088-marker">7</a></sup> For more on this, take a look at this seminal <a href="https://arxiv.org/abs/1406.2661">paper</a>.</p><p data-type="footnote" id="idm140637524869328"><sup><a href="ch12.html#idm140637524869328-marker">8</a></sup> For some tips and tricks, read this post on how to <a href="https://github.com/soumith/ganhacks">refine GANs</a> and <a href="http://bit.ly/2G2FJHq">improve performance</a>.</p></div></div></section></body></html>