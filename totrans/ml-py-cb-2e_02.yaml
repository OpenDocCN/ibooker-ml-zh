- en: Chapter 2\. Loading Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 2.0 Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step in any machine learning endeavor is to get the raw data into
    our system. The raw data might be a logfile, dataset file, database, or cloud
    blob store such as Amazon S3\. Furthermore, often we will want to retrieve data
    from multiple sources.
  prefs: []
  type: TYPE_NORMAL
- en: The recipes in this chapter look at methods of loading data from a variety of
    sources, including CSV files and SQL databases. We also cover methods of generating
    simulated data with desirable properties for experimentation. Finally, while there
    are many ways to load data in the Python ecosystem, we will focus on using the
    pandas library’s extensive set of methods for loading external data, and using
    scikit-learn—​an open source machine learning library in Python—​for generating
    simulated data.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Loading a Sample Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to load a preexisting sample dataset from the scikit-learn library.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'scikit-learn comes with a number of popular datasets for you to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Often we do not want to go through the work of loading, transforming, and cleaning
    a real-world dataset before we can explore some machine learning algorithm or
    method. Luckily, scikit-learn comes with some common datasets we can quickly load.
    These datasets are often called “toy” datasets because they are far smaller and
    cleaner than a dataset we would see in the real world. Some popular sample datasets
    in scikit-learn are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`load_iris`'
  prefs: []
  type: TYPE_NORMAL
- en: Contains 150 observations on the measurements of iris flowers. It is a good
    dataset for exploring classification algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '`load_digits`'
  prefs: []
  type: TYPE_NORMAL
- en: Contains 1,797 observations from images of handwritten digits. It is a good
    dataset for teaching image classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see more details on any of these datasets, you can print the `DESCR` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[scikit-learn toy datasets](https://oreil.ly/WS1gc)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Digit Dataset](https://oreil.ly/0hukv)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2.2 Creating a Simulated Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to generate a dataset of simulated data.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'scikit-learn offers many methods for creating simulated data. Of those, three
    methods are particularly useful: `make_regression`, `make_classification`, and
    `make_blobs`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When we want a dataset designed to be used with linear regression, `make_regression`
    is a good choice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If we are interested in creating a simulated dataset for classification, we
    can use `make_classification`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, if we want a dataset designed to work well with clustering techniques,
    scikit-learn offers `make_blobs`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As might be apparent from the solutions, `make_regression` returns a feature
    matrix of float values and a target vector of float values, while `make_classification`
    and `make_blobs` return a feature matrix of float values and a target vector of
    integers representing membership in a class.
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn’s simulated datasets offer extensive options to control the type
    of data generated. scikit-learn’s documentation contains a full description of
    all the parameters, but a few are worth noting.
  prefs: []
  type: TYPE_NORMAL
- en: In `make_regression` and `make_classification`, `n_informative` determines the
    number of features that are used to generate the target vector. If `n_informative`
    is less than the total number of features (`n_features`), the resulting dataset
    will have redundant features that can be identified through feature selection
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, `make_classification` contains a `weights` parameter that allows
    us to simulate datasets with imbalanced classes. For example, `weights = [.25,
    .75]` would return a dataset with 25% of observations belonging to one class and
    75% of observations belonging to a second class.
  prefs: []
  type: TYPE_NORMAL
- en: 'For `make_blobs`, the `centers` parameter determines the number of clusters
    generated. Using the `matplotlib` visualization library, we can visualize the
    clusters generated by `make_blobs`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 02in01](assets/mpc2_02in01.png)'
  prefs: []
  type: TYPE_IMG
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[`make_regression` documentation](https://oreil.ly/VrtN3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`make_classification` documentation](https://oreil.ly/rehc-)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`make_blobs` documentation](https://oreil.ly/1LZAI)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2.3 Loading a CSV File
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to import a comma-separated value (CSV) file.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the pandas library’s `read_csv` to load a local or hosted CSV file into
    a pandas DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '|  | integer | datetime | category |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 5 | 2015-01-01 00:00:00 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 5 | 2015-01-01 00:00:01 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are two things to note about loading CSV files. First, it is often useful
    to take a quick look at the contents of the file before loading. It can be very
    helpful to see how a dataset is structured beforehand and what parameters we need
    to set to load in the file. Second, `read_csv` has over 30 parameters and therefore
    the documentation can be daunting. Fortunately, those parameters are mostly there
    to allow it to handle a wide variety of CSV formats.
  prefs: []
  type: TYPE_NORMAL
- en: CSV files get their names from the fact that the values are literally separated
    by commas (e.g., one row might be `2,"2015-01-01 00:00:00",0`); however, it is
    common for CSV files to use other separators, such as tabs (which are referred
    to as TSV files). The pandas `sep` parameter allows us to define the delimiter
    used in the file. Although it is not always the case, a common formatting issue
    with CSV files is that the first line of the file is used to define column headers
    (e.g., `integer, datetime, category` in our solution). The `header` parameter
    allows us to specify if or where a header row exists. If a header row does not
    exist, we set `header=None`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `read_csv` function returns a pandas DataFrame: a common and useful object
    for working with tabular data that we’ll cover in more depth throughout this book.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Loading an Excel File
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to import an Excel spreadsheet.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the pandas library’s `read_excel` to load an Excel spreadsheet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '|  | integer | datetime | category |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | 5 | 2015-01-01 00:00:00 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 5 | 2015-01-01 00:00:01 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 9 | 2015-01-01 00:00:02 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This solution is similar to our solution for reading CSV files. The main difference
    is the additional parameter, `sheet_name`, that specifies which sheet in the Excel
    file we wish to load. `sheet_name` can accept both strings, containing the name
    of the sheet, and integers, pointing to sheet positions (zero-indexed). If we
    need to load multiple sheets, we include them as a list. For example, `sheet_name=[0,1,2,
    "Monthly Sales"]` will return a dictionary of pandas DataFrames containing the
    first, second, and third sheets, and the sheet named `Monthly Sales`.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Loading a JSON File
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to load a JSON file for data preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The pandas library provides `read_json` to convert a JSON file into a pandas
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '|  | category | datetime | integer |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 2015-01-01 00:00:00 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 2015-01-01 00:00:01 | 5 |'
  prefs: []
  type: TYPE_TB
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Importing JSON files into pandas is similar to the last few recipes we have
    seen. The key difference is the `orient` parameter, which indicates to pandas
    how the JSON file is structured. However, it might take some experimenting to
    figure out which argument (`split`, `records`, `index`, `columns`, or `values`)
    is the right one. Another helpful tool pandas offers is `json_normalize`, which
    can help convert semistructured JSON data into a pandas DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[`json_normalize` documentation](https://oreil.ly/nuvIB)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2.6 Loading a Parquet File
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to load a Parquet file.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The pandas `read_parquet` function allows us to read in Parquet files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '|  | category | datetime | integer |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 2015-01-01 00:00:00 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 2015-01-01 00:00:01 | 5 |'
  prefs: []
  type: TYPE_TB
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Parquet is a popular data storage format in the large data space. It is often
    used with big data tools such as Hadoop and Spark. While PySpark is outside the
    focus of this book, it’s highly likely companies operating on a large scale will
    use an efficient data storage format such as Parquet, and it’s valuable to know
    how to read it into a dataframe and manipulate it.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Apache Parquet documentation](https://oreil.ly/M5bRq)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2.7 Loading an Avro File
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to load an Avro file into a pandas DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The use the `pandavro` library’s `read_avro` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '|  | category | datetime | integer |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 2015-01-01 00:00:00 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 2015-01-01 00:00:01 | 5 |'
  prefs: []
  type: TYPE_TB
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apache Avro is an open source, binary data format that relies on schemas for
    the data structure. At the time of writing, it is not as common as Parquet. However,
    large binary data formats such as Avro, thrift, and Protocol Buffers are growing
    in popularity due to their efficient nature. If you work with large data systems,
    you’re likely to run into one of these formats in the near future.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Apache Avro documentation](https://oreil.ly/Y1TJA)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2.8 Querying a SQLite Database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to load data from a database using structured query language (SQL).
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'pandas’ `read_sql_query` allows us to make an SQL query to a database and load
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '|  | first_name | last_name | age | preTestScore | postTestScore |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | Jason | Miller | 42 | 4 | 25 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Molly | Jacobson | 52 | 24 | 94 |'
  prefs: []
  type: TYPE_TB
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SQL is the lingua franca for pulling data from databases. In this recipe we
    first use `create_engine` to define a connection to an SQL database engine called
    SQLite. Next we use pandas’ `read_sql_query` to query that database using SQL
    and put the results in a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: SQL is a language in its own right and, while beyond the scope of this book,
    it is certainly worth knowing for anyone wanting to learn about machine learning.
    Our SQL query, `SELECT * FROM data`, asks the database to give us all columns
    (`*`) from the table called `data`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this is one of a few recipes in this book that will not run without
    extra code. Specifically, `create_engine('sqlite:///sample.db')` assumes that
    an SQLite database already exists.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[SQLite](https://oreil.ly/8Y91T)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[W3Schools SQL Tutorial](https://oreil.ly/A7H1m)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2.9 Querying a Remote SQL Database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to connect to, and read from, a remote SQL database.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Create a connection with `pymysql` and read it into a dataframe with pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '|  | integer | datetime | category |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 5 | 2015-01-01 00:00:00 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 5 | 2015-01-01 00:00:01 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Of all of the recipes presented in this chapter, this is probably the one we
    will use most in the real world. While connecting and reading from an example
    `sqlite` database is useful, it’s likely not representative of tables you’ll need
    to connect to in an enterprise environment. Most SQL instances that you’ll connect
    to will require you to connect to the host and port of a remote machine, specifying
    a username and password for authentication. This example requires you [to start
    a running SQL instance locally](https://oreil.ly/Sxjqz) that mimics a remote server
    on localhost so that you can get a sense of the workflow.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PyMySQL documentation](https://oreil.ly/8zSnj)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[pandas Read SQL documentation](https://oreil.ly/Yb7sH)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2.10 Loading Data from a Google Sheet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to read in data directly from a Google Sheet.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use pandas `read_CSV` and pass a URL that exports the Google Sheet as a CSV:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '|  | integer | datetime | category |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 5 | 2015-01-01 00:00:00 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 5 | 2015-01-01 00:00:01 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While Google Sheets can easily be downloaded, it’s sometimes helpful to be able
    to read them directly into Python without any intermediate steps. The `/export?format=csv`
    query parameter at the end of the URL above creates an endpoint from which we
    can either download the file or read it into pandas.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Google Sheets API](https://oreil.ly/GRLzg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2.11 Loading Data from an S3 Bucket
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to read a CSV file from an S3 bucket you have access to.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Add storage options to pandas giving it access to the S3 object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '|  | integer | datetime | category |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 5 | 2015-01-01 00:00:00 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 5 | 2015-01-01 00:00:01 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many enterprises now keep data in cloud provider blob stores such as Amazon
    S3 or Google Cloud Storage (GCS). It’s common for machine learning practitioners
    to connect to these sources to retrieve data. Although the S3 URI (`s3://machine-learning-python-cookbook/data.csv`)
    is public, it still requires you to provide your own AWS access credentials to
    access it. It’s worth noting that public objects also have HTTP URLs from which
    they can download files, [such as this one for the CSV file](https://oreil.ly/byelc).
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Amazon S3](https://oreil.ly/E-CZX)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[AWS Security Credentials](https://oreil.ly/aHBBb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2.12 Loading Unstructured Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to load unstructured data like text or images.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the base Python `open` function to load the information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While structured data can easily be read in from CSV, JSON, or various databases,
    unstructured data can be more challenging and may require custom processing down
    the line. Sometimes it’s helpful to open and read in files using Python’s basic
    `open` function. This allows us to open files and then read the content of that
    file.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Python’s open function](https://oreil.ly/Xuuom)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Context managers in Python](https://oreil.ly/UyZnL)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
