- en: 3 Preliminary research
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Applying use cases from various domains to a given problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facing and solving the “build-or-buy” dilemma in choosing a suitable solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problem decompositioning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the right degree of innovation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In chapter 2, we discovered that identifying a problem is the key element to
    developing a successful machine learning (ML) system. The better and more precisely
    you describe the problem, the higher the probability of building a product that
    will efficiently meet business goals.
  prefs: []
  type: TYPE_NORMAL
- en: Now we will delve into several key aspects that mark the next important stage
    of designing a comprehensive and efficient ML system—the solution space. This
    chapter will tell you more about finding solutions that helped solve similar problems
    in the past, the always tough choice between building our components and buying
    third-party products, a proper approach to decompositioning the problem, and picking
    the optimal degree of innovation, depending on the main objectives of our future
    system.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 What problems can inspire you?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If I have seen further, it is by standing on the shoulders of Giants.—Isaac
    Newton
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Imagine you work for a taxi service like Uber or Lyft, and there is a worked-out
    fraud pattern: a legitimate driver starts working for the company, but later they
    pass their account to a person who can’t be a driver (they even may have no active
    driver’s license at all). Your goal is to do personal reidentification by taking
    a driver’s photo in the document they uploaded when signing up, prompting the
    driver to take a selfie from their car, and verifying it’s the same person as
    displayed on the driver’s license. At the same time, there are very reasonable
    nonfunctional requirements: for the sake of privacy, you would prefer to avoid
    uploading a driver’s photo from their device to your servers. One more aspect
    is the verification should be fast enough and resistant to various adversarial
    attacks (fraudsters can be so tricky!).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s summarize this case based on this information:'
  prefs: []
  type: TYPE_NORMAL
- en: The problem is based on face recognition. Thus, as a system designer, you need
    to familiarize yourself with the domain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The solution should be mobile-first. Thus, knowledge about ML on mobile devices
    is crucial.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The solution should be resistant to fraud attempts (a dishonest driver can try
    to show a photo of a legitimate driver instead of their own face). Thus, experience
    in liveness detection will be useful.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these problems are commonly solved in the industry, but they are rarely
    dealt with in a single solution. The following are some examples. Big surveillance
    systems (like those used for airport security) do a lot of face recognition, but
    they are rarely limited in computing power, and their inference does not have
    to be squeezed into a phone. Many consumer entertainment apps, on the other hand,
    run inference on mobile phones, and their developers are very proficient in running
    models with limited resources. Finally, liveness detection is usually applied
    to biometric systems used for authentication (FaceID on the iPhone is the most
    common example).
  prefs: []
  type: TYPE_NORMAL
- en: Nothing beats experience, so if you’re lucky enough to have successfully coped
    with all three problems, go right ahead. If not, we recommend you dedicate time
    to looking through use cases in various ML domains, because breadth of mind is
    your best friend here. You usually can’t work with tens of production ML systems
    during a single year of your career, but studying this number of use cases is
    achievable and can compensate for the lack of experience.
  prefs: []
  type: TYPE_NORMAL
- en: While designing a system, it is useful to recall similar systems and use them
    as a reference. You’re not obliged to copy certain patterns directly, but they
    can serve as an inspiration. We also advise that you not neglect failure stories,
    as they can become a hint of what to avoid in your case. This approach somewhat
    overlaps with the antigoals concept that we will touch on in chapter 4\.
  prefs: []
  type: TYPE_NORMAL
- en: 'As often happens in the software world, there are at least two aspects of similarity:
    the domain aspect and the technical aspect (as shown in figure 3.1).'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F01_Babushkin.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 Both aspects are equally important in looking for solutions that
    will help you build your system.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The former is about finding systems that are as close as possible in terms of
    a business problem; with the latter, we should recall systems with close technical
    requirements (e.g., platform, latency, data model, volume, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: Campfire story from Arseny
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'I used to work on an image segmentation problem in a manufacturing optimization
    company. My job was to find specific components in images from the assembly line.
    The problem was about the accuracy I needed: it was subpixel. In other words,
    my system needed to give highly detailed outputs as it searched for extremely
    small objects.'
  prefs: []
  type: TYPE_NORMAL
- en: Image segmentation for manufacturing data is not a common problem; you can’t
    just Google it and grab the first recipe from the internet. But fine-grained segmentation
    is popular in other domains, such as medical image analysis and photo/video editing,
    where it's often referred to as image matting.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you ever tried to change your background in the Zoom app, you must have
    noticed artifacts around your hair, and that’s exactly because the related algorithm
    is far from perfect (most likely, it optimizes for computation efficiency, not
    fine-grained precision). Hair segmentation is a classic example of image matting:
    telling the hair from the background is complicated and requires specific tricks
    like avoiding image downsampling when possible and using “soft labels”—pixels
    labeled as both foreground and background with specific weights.'
  prefs: []
  type: TYPE_NORMAL
- en: With this reasoning in mind, I learned more about the most advanced approaches
    to image matting and adapted them for my manufacturing data, which eventually
    reduced my test error significantly.
  prefs: []
  type: TYPE_NORMAL
- en: We also encourage you to ask yourself why certain decisions have been made in
    system designs and solutions you find relevant. Such exercises are valuable when
    developing and eventually applying your own intuition while designing a complicated
    ML system, including the ability to solve such dilemmas, such as whether to build
    from scratch or look for ready-to-go offers.
  prefs: []
  type: TYPE_NORMAL
- en: '3.2 Build or buy: Open source-based or proprietary tech'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Imagine you work for Slack, a team messenger with support for audio and video
    conversations. It has a feature: close to real-time speech recognition of audio
    conversations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But Slack was initially designed as a text-first messenger, and probably the
    share of users who utilize it for voice conversations is considerably smaller.
    Text captions are used even less often, as this feature is not enabled by default,
    and its application is somewhat limited. At the same time, the requirements for
    speech recognition accuracy are high: such a feature will be useless or even harmful
    if the quality does not meet expectations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The need for noncore functionality with proper quality may encourage your feature
    team to lurk through the market in search of ready-made solutions. We can’t ignore
    Slack’s scale: before the peak of the pandemic in 2019, it had 12 million daily
    average users. The currently claimed numbers have declined to 10 million, but
    it is still an impressive number. It means using a third-party tech provided by
    a vendor may cost too much, and kicking off the development of an internal, ideally
    tailored solution will be the optimal scenario. Which way will you choose?'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Build or buy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is a big dilemma related to complicated tech systems, including ML systems;
    it’s often called “build or buy.” When the problem is familiar, there is a good
    chance of finding a vendor already selling a solution as a service. Let’s capture
    the main angles of how to look at this dilemma.
  prefs: []
  type: TYPE_NORMAL
- en: Is the problem related to the core part of the business? It is a common practice
    to focus on key competitive advantages and use third-party services for commodities
    like infrastructure. Fifteen years ago, most companies had dedicated system administrators
    who managed massive servers in data centers; these days, most companies rent virtual
    machines from a cloud server provider. That’s an example of using a third-party
    service for a critical piece of infrastructure, which is, however, not crucial
    for winning over the market. Although there is an exception for companies where
    server infrastructure matters a lot (e.g., high-frequency trading, adtech, or
    cloud gaming), this area is subject to significant investments in R&D.
  prefs: []
  type: TYPE_NORMAL
- en: Many companies use third-party services for ML-related problems like machine
    translation, speech recognition, antifraud, and many more. Validating drivers’
    selfies with their license photos is a popular example of something to be delegated
    to a vendor.
  prefs: []
  type: TYPE_NORMAL
- en: Another aspect of the dilemma is economic. Say there is a vendor for this problem,
    and its service is good enough in terms of metrics, but the reasonable-price criteria
    are not met. Maybe your company is great at hiring talents in low-cost living
    areas (with a respective salary range), and thus building a system from scratch
    is cheaper compared to using a third-party solution. If a vendor provides reasonable
    pricing for a California-based VC-backed startup, it doesn’t mean the very same
    price is still reasonable for a company bootstrapped in Eastern Europe or Asia.
  prefs: []
  type: TYPE_NORMAL
- en: You can switch to an open source solution, but the choice between that and a
    purchased option may not be obvious. You can’t say the cost of an open source
    solution is zero, as its maintenance is often associated with hidden costs related
    to infrastructural work and potential problem-solving. On the other hand, using
    a purchased solution allows the delegation of many of these problems to the vendor,
    which means you will need a preliminary estimate of potential spending before
    sticking to a certain option.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also an aspect that is often not disclosed publicly but is still very
    relevant to this dilemma: careerism. Not every decision is made in the interests
    of the business, and the bigger the company, the more common the pattern. Consequently,
    some employees may be interested in pushing the idea of building, not buying,
    to deliver a big-impact project and thus justify their way to promotion or add
    a fancy achievement to their resume. Of course, we do not support this way of
    solving the build-or-buy dilemma, but since these cases are not a rare thing in
    the business, we can’t but mention them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, the build-or-buy dilemma boils down to several key factors that form
    the context in which you’re working. Buying a ready-to-go solution means saving
    time on development (which may be a factor if you’re a startup and release deadlines
    are tight and strict) and avoiding recruiting extra specialists who may be indispensable
    at the production stage but will be hard to find work for after the software is
    released. It also means that you get a tested, time-proven platform. However,
    you’re tied to a vendor’s schedule when it comes to patches or new releases. Building
    your own solution guarantees you’re in control of the feature set, scalability,
    and release calendar and can fix critical bugs on the go without depending on
    the vendor. But having more control comes with a higher price in other aspects:
    you will need in-house support, and you will definitely require a solid team of
    experienced developers.'
  prefs: []
  type: TYPE_NORMAL
- en: '*We recommend going for “buy” if*'
  prefs: []
  type: TYPE_NORMAL
- en: You opt for a faster release.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You don't have a dedicated team to develop/maintain the solution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is high demand for this solution from many companies across various domains.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be sure to look for a stable platform with a proven reputation in the market.
    *We recommend going for “build” if*
  prefs: []
  type: TYPE_NORMAL
- en: You have enough time to spend on development.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You prefer scalability and flexible on-demand updates over a scheduled release
    calendar.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can afford in-house support.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You need not a good-enough but a cutting-edge solution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have a vast legacy that requires smooth integration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You are dealing with highly sensitive data and cannot afford to rely on outsourced
    information security or simply are restricted by legal regulations regarding sharing
    data with third parties.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is also an extremely important budgetary component, which you can’t neglect,
    but at the same time, it cannot be attributed to any of the previous lists. That’s
    because the budget can affect your decision in either direction. You want to choose
    the buy option if developing your own solution may lead to overspending. On the
    other hand, the build option is your pick if none of the off-the-shelf solutions
    fit within your budget. Whatever your case is, budget is a crucial element that
    always needs to be considered.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get back to the opening example with Slack. One of the ways to resolve
    the dilemma would be to start out with a vendor, make sure the functionality is
    appreciated by customers, highlight main usage scenarios, and kickstart an internal
    solution based on the gathered information.
  prefs: []
  type: TYPE_NORMAL
- en: '*Reminder: we have no idea how this feature was actually implemented. That’s
    just how we would approach it.*'
  prefs: []
  type: TYPE_NORMAL
- en: The ratio of build versus buy decisions tends to shift over time. For example,
    at least 9 out of 10 natural language problems that would have required a very
    custom solution in the 2010s are solvable by a simple large language model (LLM)
    API call in the 2020s, making building such models from scratch far less attractive.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Open source-based or proprietary tech
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another dilemma may arise on the lower level of consideration, and that is
    open source tech versus enterprise-grade proprietary paid tech. At some point,
    you need to decide what database is used for storage or what inference server
    is preferable. It’s important to have extensive knowledge of nonfunctional requirements
    (like required uptime, latency, load tolerance, etc.) to answer this question.
    For an initial approximation, the logic is as follows: when you’re sure there
    is no need for urgent help from experts, the safe choice would be to use an open
    source solution. An opposite case would be when building a high-load, mission-critical
    system; it often makes sense to stand on the shoulders of a giant, such as a specific
    vendor. There are mixed scenarios as well—it is possible to buy enterprise-level
    support for open source solutions, and sometimes it can be a proper middle way.'
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting that the principles listed here are not ML specific—in fact,
    almost the same reasoning is applicable when we’re designing “regular” ML-free
    software.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Problem decompositioning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most useful tools in a software engineer’s toolbox is a “divide and
    conquer” approach, which is very applicable for ML, both on low-level algorithm
    implementation and the high-level system design level. That’s the first thing
    you can apply when facing a complicated problem that seems unsolvable at its existing
    scale.
  prefs: []
  type: TYPE_NORMAL
- en: A canonical example of problem decomposition is a search engine design. A user
    can query any wild set of words, including those that were never queried before
    (around 15% of Google search queries are new), and get a relevant result in a
    few hundred milliseconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, a search engine effectively does one thing, which is to provide
    relevant results from a database quickly. Let’s focus on two properties here:
    relevance and quickness. Would it be easier to fetch a somewhat relevant result
    quickly? We think so: just drop the sophisticated ranking algorithms and replace
    them with a simple “a document contains some of the queried words” heuristic.
    Scanning the whole database with such predicates is very doable. Would it be easier
    to find relevant results from a small subset of documents—thousands, not billions?
    Of course, on a small scale, we can apply sophisticated ML algorithms and big,
    although slow at inference, models.'
  prefs: []
  type: TYPE_NORMAL
- en: We bet you’ve already guessed what we are leading to—it’s time to combine those
    steps and make a two-stage system. The first stage is fast candidate filtering,
    and the second stage is a more sophisticated ranking across the identified candidates.
    Such an approach has been used in many search engines for decades.
  prefs: []
  type: TYPE_NORMAL
- en: 'This example can be developed further: instead of one iteration of candidate
    filtering, there may be a cascade of them. So, based on the query language and
    user location, documents in other languages can be filtered out even before the
    candidate filtering, reducing the number of documents that need to be processed
    downstream, as seen in figure 3.2.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F02_Babushkin.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 The process of problem decompositioning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Similar multistep pipelines are very popular in the computer vision domain:
    a deep learning model is applied first, with postprocessing responsible for the
    final answer. Another bucket of applications is related to texts and other semistructured
    data: one step extracts structured data, and these structs are processed downstream
    with more constrained models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We know six reasons for decomposition:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Computation complexity*—Decomposition is applied to reduce the amount of required
    computation (just like in the search engine example earlier).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Algorithm imperfection*—A following step is used to adjust an error made in
    the previous step. Those readers who are strong in ML theory may recall some parallels
    to boosting algorithms families.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Using an algorithm’s strengths while avoiding weaknesses*—For example, we
    need to count objects on an image. One of the approaches would be to train a convolutional
    neural network for a regression problem, but classic convolutional neural networks
    are not perfect for these needs by design (e.g., pooling layers tend to lose information
    of this kind; see figure 3.3). An alternative approach would use a model that
    would detect objects in the image and a classic computer vision algorithm on top
    of it to count contours from the previous step. The pure detector model can generalize
    better than the end-to-end regression model because of inductive bias, and the
    postprocessing step is deterministic and accurate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F03_Babushkin.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 Two activation areas were merged together after the pooling layer,
    thus losing information critical for the object counting problem.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Data fusion needs*—ML solutions can’t directly fetch data from other sources,
    so it is a popular pattern to run a model, fetch additional data based on the
    result, and process the fused data downstream. The recent interest in LLM applications
    is a good example here: many domain-specific LLM solutions follow the retrieval
    augmented generation pattern, which is essentially just pulling relevant data
    from a vector database and providing the LLM with this input as part of the prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Handling corner cases*—ML solutions can fail, and decomposition helps address
    problems early. For example, a simple model (or just a bunch of conditional checks)
    can score input and raise an error if the input is likely to be invalid.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Applying different models or logic to different subsets of data***—**There
    is a chance the model works well for a wide segment of users and is hard to generalize
    for the whole user base. It leads to a simple idea of routing users to different
    models or system paths based on a simple heuristic (e.g., separate models for
    various geographies). We’ll share more details on that in chapter 14\.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We know this list may be incomplete, but these are the six most obvious reasons
    we’ve stumbled upon throughout our careers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes pipelines are not designed in that sequential manner from the very
    beginning, and the idea of adding a step may appear later as part of further improvement.
    It may not be the best pattern, though: stacking up pieces one by one trying to
    cover the problems of a recently revealed previous step leads to a non-robust
    design, which is error-prone and hard to maintain because it doesn’t follow a
    single idea. On the other hand, it is absolutely acceptable to leave dummy stubs
    in the initial design and even first implementations (“later there will be model-based
    candidate fetching, but for now we use random samples as proof of concept”).'
  prefs: []
  type: TYPE_NORMAL
- en: Campfire story from Arseny
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: I worked in an augmented reality company building virtual try-on solutions.
    One of the products was a shoe try-on, an app that detected feet in video streams
    and rendered chosen footwear. It required multiple algorithms combined to build,
    including an occlusion algorithm responsible for determining which part of a shoe
    should be visible and rendered and which part was hidden by objects in the frame.
  prefs: []
  type: TYPE_NORMAL
- en: This part of the solution brought many troubles before the initial release;
    the team had no good ideas on how to implement it in a proper, reliable way. At
    some point, the company’s CTO took the lead and suggested an algorithm of his
    own that solved the problem for most cases. This algorithm had many disadvantages;
    it was not fast enough, not very generalizable, hard for the rest of the team
    to understand, and so on. But there was one big advantage that outweighed all
    of those—the algorithm worked in most cases!
  prefs: []
  type: TYPE_NORMAL
- en: '![sidebar figure](../Images/CH03_F04_Babushkin.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of the ML-based solution rendering a shoe only in the areas where a
    real shoe could be captured by a camera
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The CTO’s algorithm was part of the early design and implementation, and it
    became a valuable part of early product releases. Later, the team hacked together
    a completely different approach that mostly addressed the disadvantages of the
    old one, which, thanks to its proper design, didn’t require significant changes.
    Just one step in the whole pipeline was replaced by one with a more advanced approach,
    which boosted overall experience for later versions.
  prefs: []
  type: TYPE_NORMAL
- en: The design principles of ML systems are being influenced by recent trends in
    the field. In the past, it was common to build pipelines featuring many small,
    sequential components. However, with the rise of deep learning models, the trend
    shifted toward an end-to-end single-model approach. It could potentially capture
    more complex relationships in the data, as they are not limited by assumptions
    and limitations of manual design, require less domain knowledge, and reduce the
    accumulation of errors between steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Speech processing is a good example of how an end-to-end approach changed the
    design. Before end-to-end, text-to-speech (TTS) models typically included two
    main components: one processed text input and converted it into linguistic atoms
    such as phonemes, stress marks, and intonation patterns, and another synthesized
    human speech with a predefined set of rules or a statistical model to map the
    linguistic information to the sound waves.'
  prefs: []
  type: TYPE_NORMAL
- en: End-to-end TTS models, on the other hand, do not rely on explicit linguistic
    information as an intermediate representation. Instead, they directly map text
    input to an audio waveform using a single neural network model.
  prefs: []
  type: TYPE_NORMAL
- en: While end-to-end models were successful, they were not capable of containing
    knowledge on their own and often required the use of databases for many applications.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, LLMs such as GPT-4 have achieved impressive zero-shot performance,
    meaning they can answer questions directly without any additional input or training.
    However, these LLMs are computationally expensive and are prone to hallucination
    (i.e., presenting false information as true; see “Survey of Hallucination in Natural
    Language Generation,” [https://arxiv.org/abs/2202.03629](https://arxiv.org/abs/2202.03629),
    for wider context), and their knowledge is implicit and not directly accessible
    for modification.
  prefs: []
  type: TYPE_NORMAL
- en: There is ongoing research in finding ways to combine the benefits of LLMs with
    the ability to use maintainable external sources of information. For example,
    the Bing AI and ChatGPT plugins ([https://openai.com/blog/chatgpt-plugins](https://openai.com/blog/chatgpt-plugins))
    use additional online sources in a way similar to how people use search engines,
    and Galactica ([https://galactica.org/](https://galactica.org/)) by Meta AI was
    among the first to introduce the concept of a working memory token, which allows
    the model to generate a snippet of Python code that can be executed by an interpreter
    to provide a precise answer. These ideas are developed even further in Toolformer
    ([https://arxiv.org/abs/2302.04761v1](https://arxiv.org/abs/2302.04761v1)), a
    model specifically trained to use various third-party APIs. Similar ideas are
    reflected in the quickly growing open source framework LangChain ([https://python.langchain.com/api_reference](https://python.langchain.com/api_reference)).
    While these approaches are not yet widely used in production systems, they have
    the potential to change the way ML systems are decomposed.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on their complexity and degree of novelty, ML systems may imply various
    levels of innovation. Some competitive areas require huge investments in research;
    in other domains, you can use a very basic ML solution. Let's find out how to
    define the level of innovation you need for your system.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Choosing the right degree of innovation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ask any stakeholder of any ML system this straightforward question: how good
    (aka accurate) should the final product be? The most common answers are usually
    “perfect,” “100%,” and “as good as possible.” But let''s try to figure out what
    lies behind these straightforward yet ambiguous answers.'
  prefs: []
  type: TYPE_NORMAL
- en: The answer “as good as possible” implicitly means “as soon as we meet other
    constraints.” The most obvious constraints are time and budget. Would they want
    a perfect ML system in 10 years? Most likely not. Is the “acceptable good” system
    shipped by the end of next quarter better? Most likely, yes.
  prefs: []
  type: TYPE_NORMAL
- en: We will elaborate on the topic of precise understanding of the difference between
    “good enough” and “perfect” later, in chapter 5\. But even in the earliest stage,
    when the design process has just started, the exact metric is not important yet.
    It’s a rough understanding that is critical.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the experience we’ve gained creating, maintaining, and improving ML systems
    with multiple scales and objectives, we’ve identified three different buckets
    of required perfection that all systems can be distributed between. Terms may
    vary, but to our mind, these are the most fitting:'
  prefs: []
  type: TYPE_NORMAL
- en: Minimum viable ML system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average human-level ML system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best-in-class ML system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A minimum viable system can be a very spartan solution with duct tape as the
    key bonding element. Aligned expectations from such a system would be “it mostly
    works,” and an observer will be able to detect various failure modes. Such systems
    are considered baselines and prototypes; no innovation is expected.
  prefs: []
  type: TYPE_NORMAL
- en: Human-level performance adds a certain bar. Many successful existing ML systems
    don’t even match human-level performance yet are valuable for companies. Thus,
    we can say that reaching this kind of performance requires a fair amount of research
    and innovation.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, there is the best-in-class bucket. Some systems are hardly useful when
    they don’t beat a significant share of competitors—this is often the case in super-competitive
    domains like trading or adtech or global products like search engines. A tiny
    shift in accuracy may cost millions in profits or losses, and in such cases, ML
    systems are designed with the idea of reaching the best result possible.
  prefs: []
  type: TYPE_NORMAL
- en: Why do we even talk about innovation here? The bridge between the problem space
    and the solution space strongly depends on the level of innovation we assume from
    the very beginning. With the “minimum viable system” bucket, we have exactly zero
    innovation—we just use the simplest and fastest solution we know and move forward.
    On the other side of the spectrum, we get endless innovation, where a system is
    never ready, and the team is always looking for new improvements to implement
    in the next release.
  prefs: []
  type: TYPE_NORMAL
- en: 'Distributing problems between these three buckets would be a very powerful
    technique, but there’s one important factor we can’t ignore: the level of required
    innovation is not static. In many cases—especially in startups—things are built
    as minimalistic as possible to be upgraded later. And it makes sense: the company
    first evaluates if the functionality is required by customers (or internal users)
    and then addresses customer feedback to improve the system. If a shipped feature
    is unique to the market, even its minimalistic implementation brings so much value
    that competitors immediately get on to improving their own products. It moves
    the initial system from the first bucket to the second bucket or even closer to
    the state-of-the-art league. Many startups face problems with such transition,
    and cases of designing a system that can evolve from prototype to a world-class
    gem (which is the art of engineering) are extremely rare. The lite version of
    such art is designing a system that can be rebuilt while keeping as many existing
    building blocks as possible, and that’s a fairly high bar to aim for.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.1 What solutions can be useful?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Knowing the level of innovation you need and some high-level structure of the
    system, you can look for implementation ideas on a lower level. When this chapter
    was being prepared, there were five popular sources of information to dive into.
  prefs: []
  type: TYPE_NORMAL
- en: arXiv
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: arXiv ([https://arxiv.org/](https://arxiv.org/)) is a website distributing academic
    papers mostly in science, technology, engineering, and mathematics disciplines.
    Math and computer science, including its subdisciplines, make up a solid share
    of over 2 million papers published there.
  prefs: []
  type: TYPE_NORMAL
- en: 'arXiv is a good place to get familiar with academic perspectives on your problem.
    Other than just reading everything related to your keywords, we encourage you
    to use the citations and links mechanism: once you find a relevant paper, it’s
    likely you may be interested in getting familiar with older papers it mentions
    and newer papers citing it. arXiv is an ecosystem of its own kind—there are browser
    extensions and additional websites that can assist your search. A good start is
    to look for overview papers (often containing “survey” in their titles): usually
    they feature properly distilled wisdom on the topic.'
  prefs: []
  type: TYPE_NORMAL
- en: 'arXiv on its own may seem a little too raw as a source of knowledge: it’s barely
    possible to read all new papers, and its search mechanism is somewhat primitive
    from a modern perspective. There are multiple popular tools on top of arXiv that
    simplify exploration. Currently, we recommend [https://arxivxplorer.com/](https://arxivxplorer.com/),
    a modern search engine on top of paper abstracts, although it is very possible
    that there will be another fancy tool by the time the book is published (previously,
    the most popular add-on was [https://arxiv-sanity-lite.com/](https://arxiv-sanity-lite.com/)).'
  prefs: []
  type: TYPE_NORMAL
- en: Papers with code
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As it is easy to guess, Papers with Code ([https://paperswithcode.com/](https://paperswithcode.com/))
    is a compilation of academic ML-related papers that are accompanied by implementations
    in the code form. Papers are grouped by topics and ranked by performance when
    possible.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the closest problem from the academic world and see the top N papers
    solving this problem, their metrics, some meta information (e.g., does this approach
    require additional data?), and—what’s very important—links to public implementations.
    This website is a real game changer for those who prefer repositories to formal
    academic writing.
  prefs: []
  type: TYPE_NORMAL
- en: GitHub
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Once we’ve mentioned code implementations, we can’t avoid GitHub ([https://github.com](https://github.com)).
    The most popular platform for open source software, GitHub has repositories for
    any occasion. The downside derives from its scale: if you’re there to find something
    uncommon, you are effectively looking for a needle in a haystack.'
  prefs: []
  type: TYPE_NORMAL
- en: GitHub is not specialized to the ML domain, but at the same time, most open-source
    ML projects are located there.
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Hugging Face model hub ([https://huggingface.co/models](https://huggingface.co/models))
    is a major platform sharing numerous models and datasets. At the time of writing,
    the hub contained more than 560,000 publicly available ML models. Categorization
    and tags work quite precisely, with a huge portion of the models offering small
    interactive web-based demos to display their capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face as a company started with a focus on natural language processing
    (NLP), and the hub has been the main platform for sharing NLP-oriented models.
    We recommend going there for research-related models if an ML problem you’re solving
    includes text processing.
  prefs: []
  type: TYPE_NORMAL
- en: Kaggle
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Kaggle ([https://kaggle.com](https://kaggle.com)) is the most popular platform
    for competitive ML. Organizations use the platform to host challenges and lure
    the world’s best ML practitioners to fight for monetary prizes and, of course,
    glory. During competitions, participants share their ideas and code snippets related
    to a given challenge. At the end of a competition, winners and leaders usually
    reveal their secrets. Along with competitions, Kaggle serves as a hosting site
    for multiple datasets, so there is a good chance of finding a public dataset related
    to your problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kaggle is the most exceptional piece on this list for several reasons. If a
    competition is organized poorly, the problem is somewhat ill-posed: instead of
    solving the real problem, competitors may try to look for shortcuts like data
    leakage. Also, final solutions are usually not applicable in practice: the models
    are gargantuan because latency limits can be off the table. Finally, the code
    snippets are rarely clean: contestants aim for rapid iterations, not for long-time
    maintenance.'
  prefs: []
  type: TYPE_NORMAL
- en: Yet with all the mentioned disadvantages, Kaggle forums can be a source of great
    overviews for your problem, including both academic papers and hacker-style code
    that may become academic mainstream later. It’s also worth mentioning that there
    are websites aggregating the best Kaggle solutions, such as [https://farid.one/kaggle-solutions/](https://farid.one/kaggle-solutions/).
  prefs: []
  type: TYPE_NORMAL
- en: We would like to highlight the fact that the current stage still doesn’t require
    choosing solutions based on this research. It should give you more details on
    the landscape to make your decision-making process more reliable.
  prefs: []
  type: TYPE_NORMAL
- en: '3.4.2 Working on the solution space: Practical example'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s reiterate the points mentioned in the previous section using a single
    detailed example. Imagine you’ve joined a stock photo company. The business is
    effectively a marketplace: photographers join the platform and upload their shots,
    and customers who are looking for specific images for illustrative purposes (editors,
    designers, ad professionals) purchase rights for these photos. The marketplace
    makes money through commissions from sales. The company is highly interested in
    creating an effective search system on its website.'
  prefs: []
  type: TYPE_NORMAL
- en: From one perspective, the photo stock is huge, featuring millions of images.
    When customers look for photos, they are often interested in something specific,
    which is hard to find with simple categorization or other naive taxonomy. So you’ve
    been hired to build a modern search tool that will be able to find the most relevant
    shots upon text queries from customers. How should you understand the landscape
    for the problem?
  prefs: []
  type: TYPE_NORMAL
- en: The build-or-buy question arises first. Let’s assume you’re guessing that companies
    of scale like yours usually design their own solutions, but that’s not always
    the case. Some reconnaissance would be suitable. You can easily reveal the fact
    that many vendors—both huge enterprises and young startups—provide search engines
    as a service. When you try to prospect those, it’s very likely that many solutions
    can turn irrelevant—your company needs a search engine for images based on text
    queries, which is not the most popular paradigm. There may be several tech providers
    suggesting something relevant, though, so let’s keep them in mind.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s consider the similar problems other companies solve:'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, there are other photo banks, and some of them may have built nice
    search engines. There is little chance that they expose many details on how their
    engines are built (you could dig up some blog posts or conference talks), but
    it’s not zero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are generic-purpose search engines like Google and Bing. Obviously, the
    system you need is of a very different scale—you need to operate with millions
    of images while they do billions. Here, you might say, “How can I replicate such
    a juggernaut if my team is N times smaller?” Of course, it’s unlikely anyone can
    compete with “the big guys” in terms of capacity, but you won’t need to, as your
    main objective will be to find ideas that will meet the needs of your solution
    and not a line of code more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Very opposite of the previous point, you can find consumer-oriented projects
    that help categorize personal photo collections. They’re not ready for millions
    but more like thousands of images. The good side is that of them are open source,
    so you can dive into the code directly to fetch ideas.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, there are some nonvirtual goods marketplaces—for example, selling clothes,
    furniture, and so on. Some of them are giants like Amazon, and some are niche-oriented
    and even smaller than your company. Their business is very much dependent on search
    quality, but their goods are not just images, and they usually have way more attributes
    (they may be text descriptions or seller information). Such search engines use
    more information about items, not only visual information; in the ML world, we
    call them multimodal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Search engines are one of the most popular applications within the information
    retrieval discipline. Its practitioners were among the early adopters of many
    ML methods but didn’t limit themselves to ML-only approaches. Familiarizing yourself
    with the discipline (or refreshing your memories) on a high level, starting from
    Wikipedia, can be suitable for those who don’t feel confident in the domain. After
    learning more about information retrieval, you can dive deeper by reading more
    about image retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: 'While reading documents on building search engines, you definitely see a pattern
    for decomposition: as in many search engines, not every document should be ranked
    in your scenario. From the very start, a user can specify requirements: for example,
    the photo should be provided as a raw file (as opposed to compressed JPEGs), be
    at least 5,000 pixels wide, and cost not more than $50\. Such conditions can narrow
    the search candidates from millions to tens of thousands very quickly, while we
    didn’t touch upon image and query semantics at all. This optimization would be
    very valuable and may become a cornerstone of your future design.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another thing you could find is the fact that under the hood, most search engines
    effectively do one thing. They calculate a relevancy score for the pair of user
    queries and potentially related items (documents) and rank items based on this
    score (see figure 3.4):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH03_F05_Babushkin.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 The breakdown of how a search engine ranks images based on their
    relevancy regarding a given query
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In the case of our scenario, this brings up multiple open questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What family of algorithms can the function `f` be?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do you encode a query (the text) and an item (the image)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do you measure relevancy?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do you include user feedback in the system?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these questions is wide and deserves a separate book (or at least multiple
    chapters), so, to be succinct, we only suggest that those questions should be
    kept in mind when you dive deeper into sources of information we mentioned before,
    from arXiv to Kaggle.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next question is the degree of innovation you’re looking for. There are
    several thoughts here:'
  prefs: []
  type: TYPE_NORMAL
- en: The company already has a very basic search tech. It’s outdated and often yields
    irrelevant results, but it’s still better than no search at all.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The company’s business can benefit from good search. Currently, many users
    can’t find the shot they’re looking for because of poor search results, and thus
    they leave the website and go to competitors. Proper search quality blocks other
    initiatives in the company: what’s the point of launching a massive marketing
    campaign if new users will likely churn away, not being able to find what they
    need?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The budgets are very limited. At the same time, after the first successful case
    of this project, there is a good chance that money will flow for new R&D initiatives.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first point clearly shows that the very basic minimum viable product is
    not applicable here because you already have one. At the same time, limited budgets
    suggest you can’t aim for a state-of-the-art solution at first. Thus, you need
    to design a solid system with a limited budget and the option to improve it further.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we have covered the most important elements of your preparation
    for writing a design document. You now know how to decompose the problem, what
    external and internal factors will influence your approach to the build-or-buy
    dilemma, what online sources are most helpful, and how to decide on the degree
    of innovativeness your solution should carry.
  prefs: []
  type: TYPE_NORMAL
- en: All this knowledge will be the basis not only for writing a design document
    for your system but also for using it to understand whether you need an ML system
    in the first place. The last point might sound intriguing and even controversial,
    so we will try to elaborate on it (as well as many others) in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Look for solutions on the market that can fully or partially satisfy your needs.
    If those are found, ask yourself what parts of the system can use such solutions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the high-level view of the system. Try to draw it as three to five blocks
    that would be clear to a nontech executive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The build-or-buy dilemma can become a decisive factor in choosing between building
    your components and buying third-party products. To handle it properly, consider
    various internal and external factors, including available time and the capacity
    of all the involved teams.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check the list of potential factors pointing to the necessity of decompositioning.
    If the problem you’re solving meets any of them, it most probably needs to be
    decomposed, too.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There might be a strong temptation to deliver a state-of-the-art product; still,
    you need to ask yourself how ready you are to invest in the system for the sake
    of innovation and whether the investment will pay off in the first place.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don’t hesitate to lurk through popular online aggregators to find use cases
    you can utilize as references to your solution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
