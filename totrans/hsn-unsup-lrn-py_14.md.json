["```py\n'''Main'''\nimport numpy as np\nimport pandas as pd\nimport os, time, re\nimport pickle, gzip, datetime\n\n'''Data Viz'''\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nimport matplotlib as mpl\n\n%matplotlib inline\n\n'''Data Prep and Model Evaluation'''\nfrom sklearn import preprocessing as pp\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score, mean_squared_error\n\n'''Algos'''\nimport lightgbm as lgb\n\n'''TensorFlow and Keras'''\nimport tensorflow as tf\nimport keras\nfrom keras import backend as K\nfrom keras.models import Sequential, Model\nfrom keras.layers import Activation, Dense, Dropout\nfrom keras.layers import BatchNormalization, Input, Lambda\nfrom keras import regularizers\nfrom keras.losses import mse, binary_crossentropy\n```", "```py\n# Load the data\ncurrent_path = os.getcwd()\nfile = '\\\\datasets\\\\movielens_data\\\\ratings.csv'\nratingDF = pd.read_csv(current_path + file)\n\n# Convert fields into appropriate data types\nratingDF.userId = ratingDF.userId.astype(str).astype(int)\nratingDF.movieId = ratingDF.movieId.astype(str).astype(int)\nratingDF.rating = ratingDF.rating.astype(str).astype(float)\nratingDF.timestamp = ratingDF.timestamp.apply(lambda x: \\\n                datetime.utcfromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n```", "```py\nn_users = ratingDF.userId.unique().shape[0]\nn_movies = ratingDF.movieId.unique().shape[0]\nn_ratings = len(ratingDF)\navg_ratings_per_user = n_ratings/n_users\n\nprint('Number of unique users: ', n_users)\nprint('Number of unique movies: ', n_movies)\nprint('Number of total ratings: ', n_ratings)\nprint('Average number of ratings per user: ', avg_ratings_per_user)\n```", "```py\nNumber of unique users: 138493\nNumber of unique movies: 26744\nNumber of total ratings: 20000263\nAverage number of ratings per user: 144.4135299257002\n```", "```py\nmovieIndex = ratingDF.groupby(\"movieId\").count().sort_values(by= \\\n                \"rating\",ascending=False)[0:1000].index\nratingDFX2 = ratingDF[ratingDF.movieId.isin(movieIndex)]\nratingDFX2.count()\n```", "```py\nuserIndex = ratingDFX2.groupby(\"userId\").count().sort_values(by= \\\n    \"rating\",ascending=False).sample(n=1000, random_state=2018).index\nratingDFX3 = ratingDFX2[ratingDFX2.userId.isin(userIndex)]\nratingDFX3.count()\n```", "```py\nmovies = ratingDFX3.movieId.unique()\nmoviesDF = pd.DataFrame(data=movies,columns=['originalMovieId'])\nmoviesDF['newMovieId'] = moviesDF.index+1\n\nusers = ratingDFX3.userId.unique()\nusersDF = pd.DataFrame(data=users,columns=['originalUserId'])\nusersDF['newUserId'] = usersDF.index+1\n\nratingDFX3 = ratingDFX3.merge(moviesDF,left_on='movieId', \\\n                              right_on='originalMovieId')\nratingDFX3.drop(labels='originalMovieId', axis=1, inplace=True)\n\nratingDFX3 = ratingDFX3.merge(usersDF,left_on='userId', \\\n                              right_on='originalUserId')\nratingDFX3.drop(labels='originalUserId', axis=1, inplace=True)\n```", "```py\nn_users = ratingDFX3.userId.unique().shape[0]\nn_movies = ratingDFX3.movieId.unique().shape[0]\nn_ratings = len(ratingDFX3)\navg_ratings_per_user = n_ratings/n_users\n\nprint('Number of unique users: ', n_users)\nprint('Number of unique movies: ', n_movies)\nprint('Number of total ratings: ', n_ratings)\nprint('Average number of ratings per user: ', avg_ratings_per_user)\n```", "```py\nNumber of unique users: 1000\nNumber of unique movies: 1000\nNumber of total ratings: 90213\nAverage number of ratings per user: 90.213\n```", "```py\nX_train, X_test = train_test_split(ratingDFX3,\n test_size=0.10, shuffle=True, random_state=2018)\n\nX_validation, X_test = train_test_split(X_test,\n test_size=0.50, shuffle=True, random_state=2018)\n```", "```py\nSize of train set: 81191\nSize of validation set: 4511\nSize of test set: 4511\n```", "```py\n# Generate ratings matrix for train\nratings_train = np.zeros((n_users, n_movies))\nfor row in X_train.itertuples():\n    ratings_train[row[6]-1, row[5]-1] = row[3]\n\n# Calculate sparsity of the train ratings matrix\nsparsity = float(len(ratings_train.nonzero()[0]))\nsparsity /= (ratings_train.shape[0] * ratings_train.shape[1])\nsparsity *= 100\nprint('Sparsity: {:4.2f}%'.format(sparsity))\n```", "```py\n# Generate ratings matrix for validation\nratings_validation = np.zeros((n_users, n_movies))\nfor row in X_validation.itertuples():\n    ratings_validation[row[6]-1, row[5]-1] = row[3]\n\n# Generate ratings matrix for test\nratings_test = np.zeros((n_users, n_movies))\nfor row in X_test.itertuples():\n    ratings_test[row[6]-1, row[5]-1] = row[3]\n```", "```py\nactual_validation = ratings_validation[ratings_validation.nonzero()].flatten()\n```", "```py\npred_validation = np.zeros((len(X_validation),1))\npred_validation[pred_validation==0] = 3.5\npred_validation\n\nmean_squared_error(pred_validation, actual_validation)\n```", "```py\nMean squared error using naive prediction: 1.055420084238528\n```", "```py\nratings_validation_prediction = np.zeros((n_users, n_movies))\ni = 0\nfor row in ratings_train:\n    ratings_validation_prediction[i][ratings_validation_prediction[i]==0] \\\n        = np.mean(row[row>0])\n    i += 1\n\npred_validation = ratings_validation_prediction \\\n    [ratings_validation.nonzero()].flatten()\nuser_average = mean_squared_error(pred_validation, actual_validation)\nprint('Mean squared error using user average:', user_average)\n```", "```py\nMean squared error using user average: 0.9090717929472647\n```", "```py\nratings_validation_prediction = np.zeros((n_users, n_movies)).T\ni = 0\nfor row in ratings_train.T:\n    ratings_validation_prediction[i][ratings_validation_prediction[i]==0] \\\n        = np.mean(row[row>0])\n    i += 1\n\nratings_validation_prediction = ratings_validation_prediction.T\npred_validation = ratings_validation_prediction \\\n    [ratings_validation.nonzero()].flatten()\nmovie_average = mean_squared_error(pred_validation, actual_validation)\nprint('Mean squared error using movie average:', movie_average)\n```", "```py\nMean squared error using movie average: 0.9136057106858655\n```", "```py\nn_latent_factors = 1\n\nuser_input = Input(shape=[1], name='user')\nuser_embedding = Embedding(input_dim=n_users + 1, output_dim=n_latent_factors,\n name='user_embedding')(user_input)\nuser_vec = Flatten(name='flatten_users')(user_embedding)\n\nmovie_input = Input(shape=[1], name='movie')\nmovie_embedding = Embedding(input_dim=n_movies + 1, output_dim=n_latent_factors,\n name='movie_embedding')(movie_input)\nmovie_vec = Flatten(name='flatten_movies')(movie_embedding)\n\nproduct = dot([movie_vec, user_vec], axes=1)\nmodel = Model(inputs=[user_input, movie_input], outputs=product)\nmodel.compile('adam', 'mean_squared_error')\n```", "```py\nhistory = model.fit(x=[X_train.newUserId, X_train.newMovieId], \\\n                    y=X_train.rating, epochs=100, \\\n                    validation_data=([X_validation.newUserId, \\\n                    X_validation.newMovieId], X_validation.rating), \\\n                    verbose=1)\n\npd.Series(history.history['val_loss'][10:]).plot(logy=False)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Validation Error\")\nprint('Minimum MSE: ', min(history.history['val_loss']))\n```", "```py\n# Define RBM class\nclass RBM(object):\n\n    def __init__(self, input_size, output_size,\n                 learning_rate, epochs, batchsize):\n        # Define hyperparameters\n        self._input_size = input_size\n        self._output_size = output_size\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.batchsize = batchsize\n\n        # Initialize weights and biases using zero matrices\n        self.w = np.zeros([input_size, output_size], \"float\")\n        self.hb = np.zeros([output_size], \"float\")\n        self.vb = np.zeros([input_size], \"float\")\n```", "```py\ndef prob_h_given_v(self, visible, w, hb):\n    return tf.nn.sigmoid(tf.matmul(visible, w) + hb)\n```", "```py\ndef prob_v_given_h(self, hidden, w, vb):\n    return tf.nn.sigmoid(tf.matmul(hidden, tf.transpose(w)) + vb)\n```", "```py\ndef sample_prob(self, probs):\n    return tf.nn.relu(tf.sign(probs - tf.random_uniform(tf.shape(probs))))\n```", "```py\n_w = tf.placeholder(\"float\", [self._input_size, self._output_size])\n_hb = tf.placeholder(\"float\", [self._output_size])\n_vb = tf.placeholder(\"float\", [self._input_size])\n\nprv_w = np.zeros([self._input_size, self._output_size], \"float\")\nprv_hb = np.zeros([self._output_size], \"float\")\nprv_vb = np.zeros([self._input_size], \"float\")\n\ncur_w = np.zeros([self._input_size, self._output_size], \"float\")\ncur_hb = np.zeros([self._output_size], \"float\")\ncur_vb = np.zeros([self._input_size], \"float\")\n```", "```py\nv0 = tf.placeholder(\"float\", [None, self._input_size])\nh0 = self.sample_prob(self.prob_h_given_v(v0, _w, _hb))\n```", "```py\nv1 = self.sample_prob(self.prob_v_given_h(h0, _w, _vb))\nh1 = self.prob_h_given_v(v1, _w, _hb)\n```", "```py\npositive_grad = tf.matmul(tf.transpose(v0), h0)\nnegative_grad = tf.matmul(tf.transpose(v1), h1)\n\nupdate_w = _w + self.learning_rate * \\\n    (positive_grad - negative_grad) / tf.to_float(tf.shape(v0)[0])\nupdate_vb = _vb +  self.learning_rate * tf.reduce_mean(v0 - v1, 0)\nupdate_hb = _hb +  self.learning_rate * tf.reduce_mean(h0 - h1, 0)\n\nerr = tf.reduce_mean(tf.square(v0 - v1))\n```", "```py\nwith tf.Session() as sess:\n sess.run(tf.global_variables_initializer())\n\n for epoch in range(self.epochs):\n     for start, end in zip(range(0, len(X),\n      self.batchsize),range(self.batchsize,len(X), self.batchsize)):\n         batch = X[start:end]\n         cur_w = sess.run(update_w, feed_dict={v0: batch,\n          _w: prv_w, _hb: prv_hb, _vb: prv_vb})\n         cur_hb = sess.run(update_hb, feed_dict={v0: batch,\n          _w: prv_w, _hb: prv_hb, _vb: prv_vb})\n         cur_vb = sess.run(update_vb, feed_dict={v0: batch,\n          _w: prv_w, _hb: prv_hb, _vb: prv_vb})\n         prv_w = cur_w\n         prv_hb = cur_hb\n         prv_vb = cur_vb\n     error = sess.run(err, feed_dict={v0: X,\n      _w: cur_w, _vb: cur_vb, _hb: cur_hb})\n     print ('Epoch: %d' % epoch,'reconstruction error: %f' % error)\n self.w = prv_w\n self.hb = prv_hb\n self.vb = prv_vb\n```", "```py\n# Begin the training cycle\n\n# Convert inputX into float32\ninputX = ratings_train\ninputX = inputX.astype(np.float32)\n\n# Define the parameters of the RBMs we will train\nrbm=RBM(1000,1000,0.3,500,200)\n```", "```py\nrbm.train(inputX)\noutputX, reconstructedX, hiddenX = rbm.rbm_output(inputX)\n```", "```py\n# Predict ratings for validation set\ninputValidation = ratings_validation\ninputValidation = inputValidation.astype(np.float32)\n\nfinalOutput_validation, reconstructedOutput_validation, _ = \\\n    rbm.rbm_output(inputValidation)\n```", "```py\npredictionsArray = reconstructedOutput_validation\npred_validation = \\\n    predictionsArray[ratings_validation.nonzero()].flatten()\nactual_validation = \\\n    ratings_validation[ratings_validation.nonzero()].flatten()\n\nrbm_prediction = mean_squared_error(pred_validation, actual_validation)\nprint('Mean squared error using RBM prediction:', rbm_prediction)\n```", "```py\nMean squared error using RBM prediction: 9.331135003325205\n```"]