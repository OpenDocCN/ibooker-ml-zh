<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 10. Dimensionality Reduction Using Feature Selection" data-type="chapter" epub:type="chapter"><div class="chapter" id="dimensionality-reduction-using-feature-selection">
<h1><span class="label">Chapter 10. </span>Dimensionality Reduction <span class="keep-together">Using Feature Selection</span></h1>
<section data-pdf-bookmark="10.0 Introduction" data-type="sect1"><div class="sect1" id="id244">
<h1>10.0 Introduction</h1>
<p>In <a data-type="xref" href="ch09.xhtml#dimensionality-reduction-using-feature-extraction">Chapter 9</a>, we discussed how to reduce the dimensionality of
our feature matrix by creating new features with (ideally) similar
abilities to train quality models but with significantly fewer dimensions.
This is called <em>feature extraction</em>. In this chapter we will cover an
alternative approach: selecting high-quality, informative features and
dropping less useful features. This is <a data-primary="feature selection" data-type="indexterm" id="ix_feat_sel_ch10"/>called <em>feature selection</em>.</p>
<p>There are three types of feature selection methods: filter, wrapper, and embedded. <em>Filter methods</em> select the best <a data-primary="filter methods" data-secondary="feature selection" data-type="indexterm" id="ix_filter_feat_sel"/>features by examining their
statistical properties. Methods where we explicitly set a threshold for a statistic or manually select the number of features we want to keep are examples of feature selection by filtering. Wrapper methods use trial and error to find the
subset of features that produces models with the highest quality
predictions. <em>Wrapper methods</em> are <a data-primary="wrapper methods, feature selection" data-type="indexterm" id="id1497"/>often the most effective, as they find the best result through actual experimentation as opposed to naive assumptions. <a data-primary="embedded methods, feature selection" data-type="indexterm" id="id1498"/>Finally, <em>embedded methods</em> select the best feature subset as
part of, as an extension of, a learning algorithm’s training process.</p>
<p>Ideally, we’d describe all three methods in this chapter. However, since
embedded methods are closely intertwined with specific learning
algorithms, they are difficult to explain prior to a deeper dive into
the algorithms themselves. Therefore, in this chapter we cover only
filter and wrapper feature selection methods, leaving the discussion of
particular embedded methods until the chapters where those learning
algorithms are discussed in depth.</p>
</div></section>
<section data-pdf-bookmark="10.1 Thresholding Numerical Feature Variance" data-type="sect1"><div class="sect1" id="thresholding-numerical-feature-variance">
<h1>10.1 Thresholding Numerical Feature Variance</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id500">
<h2>Problem</h2>
<p>You have a set of <a data-primary="feature selection" data-secondary="thresholding numerical feature variance" data-type="indexterm" id="ix_feat_sel_num_feat_var"/>numerical features and want to filter out those with low
variance (i.e., likely containing little information).</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id501">
<h2>Solution</h2>
<p>Select a subset of features with variances above a given <a data-primary="thresholding" data-secondary="feature selection" data-type="indexterm" id="ix_thresh_feat_sel"/>threshold:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.feature_selection</code> <code class="kn">import</code> <code class="n">VarianceThreshold</code>

<code class="c1"># Import some data to play with</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>

<code class="c1"># Create features and target</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create thresholder</code>
<code class="n">thresholder</code> <code class="o">=</code> <code class="n">VarianceThreshold</code><code class="p">(</code><code class="n">threshold</code><code class="o">=</code><code class="mf">.5</code><code class="p">)</code>

<code class="c1"># Create high variance feature matrix</code>
<code class="n">features_high_variance</code> <code class="o">=</code> <code class="n">thresholder</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># View high variance feature matrix</code>
<code class="n">features_high_variance</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="mi">3</code><code class="p">]</code></pre>
<pre data-type="programlisting">array([[ 5.1,  1.4,  0.2],
       [ 4.9,  1.4,  0.2],
       [ 4.7,  1.3,  0.2]])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id245">
<h2>Discussion</h2>
<p><em>Variance thresholding</em> (VT) is an <a data-primary="variance thresholding (VT)" data-type="indexterm" id="id1499"/><a data-primary="VT (variance thresholding)" data-type="indexterm" id="id1500"/>example of feature selection by filtering, and one of the most basic approaches to
feature selection. It is motivated by the idea that features with low
variance are likely less interesting (and less useful) than features with
high variance. VT first calculates the variance of each <span class="keep-together">feature:</span></p>
<div data-type="equation">
<math display="block">
<mrow>
<mrow>
<mi>V</mi>
<mi>a</mi>
<mi>r</mi>
</mrow>
<mrow>
<mo>(</mo>
<mi>x</mi>
<mo>)</mo>
</mrow>
<mo>=</mo>
<mfrac><mn>1</mn> <mi>n</mi></mfrac>
<munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </munderover>
<msup><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi> </msub><mo>-</mo><mi>μ</mi><mo>)</mo></mrow> <mn>2</mn> </msup>
</mrow>
</math>
</div>
<p>where <math display="inline"><mi>x</mi></math> is the feature vector, <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math> is an
individual feature value, and <math display="inline"><mi>μ</mi></math> is that feature’s mean value. Next, it drops all features whose variance does not meet that threshold.</p>
<p>Keep two things in mind when employing VT. First, the
variance is not centered; that is, it is in the squared unit of the
feature itself. Therefore, VT will not work when feature sets
contain different units (e.g., one feature is in years while another is in dollars). Second, the variance threshold is
selected manually, so we have to use our own judgment for a good value
to select (or use a model selection technique described in <a data-type="xref" href="ch12.xhtml#model-selection">Chapter 12</a>). We can see the variance for each feature using <code>variances_</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View variances</code>
<code class="n">thresholder</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">)</code><code class="o">.</code><code class="n">variances_</code></pre>
<pre data-type="programlisting">array([0.68112222, 0.18871289, 3.09550267, 0.57713289])</pre>
<p>Finally, if the features have been standardized (to mean zero and unit
variance), then for obvious reasons VT will not work
correctly:<a data-primary="" data-startref="ix_feat_sel_num_feat_var" data-type="indexterm" id="id1501"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>

<code class="c1"># Standardize feature matrix</code>
<code class="n">scaler</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>
<code class="n">features_std</code> <code class="o">=</code> <code class="n">scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Caculate variance of each feature</code>
<code class="n">selector</code> <code class="o">=</code> <code class="n">VarianceThreshold</code><code class="p">()</code>
<code class="n">selector</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_std</code><code class="p">)</code><code class="o">.</code><code class="n">variances_</code></pre>
<pre data-type="programlisting">array([1., 1., 1., 1.])</pre>
</div></section>
</div></section>
<section data-pdf-bookmark="10.2 Thresholding Binary Feature Variance" data-type="sect1"><div class="sect1" id="thresholding-binary-feature-variance">
<h1>10.2 Thresholding Binary Feature Variance</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id502">
<h2>Problem</h2>
<p>You have a set of <a data-primary="binary feature data" data-secondary="thresholding variance" data-type="indexterm" id="id1502"/><a data-primary="feature selection" data-secondary="thresholding binary feature variance" data-type="indexterm" id="id1503"/>binary categorical features and want to filter out those
with low variance (i.e., likely containing little information).</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id1504">
<h2>Solution</h2>
<p>Select a subset of features with a Bernoulli random variable variance
above a given threshold:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">from</code> <code class="nn">sklearn.feature_selection</code> <code class="kn">import</code> <code class="n">VarianceThreshold</code>

<code class="c1"># Create feature matrix with:</code>
<code class="c1"># Feature 0: 80% class 0</code>
<code class="c1"># Feature 1: 80% class 1</code>
<code class="c1"># Feature 2: 60% class 0, 40% class 1</code>
<code class="n">features</code> <code class="o">=</code> <code class="p">[[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code>
            <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code>
            <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code>
            <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code>
            <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]]</code>

<code class="c1"># Run threshold by variance</code>
<code class="n">thresholder</code> <code class="o">=</code> <code class="n">VarianceThreshold</code><code class="p">(</code><code class="n">threshold</code><code class="o">=</code><code class="p">(</code><code class="mf">.75</code> <code class="o">*</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="mf">.75</code><code class="p">)))</code>
<code class="n">thresholder</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([[0],
       [1],
       [0],
       [1],
       [0]])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id246">
<h2>Discussion</h2>
<p>As with numerical features, one strategy for selecting highly
informative categorical features and filtering out less informative ones is to examine their variances. In
binary features (i.e., Bernoulli random variables), variance is
calculated as:</p>
<div data-type="equation">
<math display="block">
<mrow>
<mo form="prefix">Var</mo>
<mo>(</mo>
<mi>x</mi>
<mo>)</mo>
<mo>=</mo>
<mi>p</mi>
<mo>(</mo>
<mn>1</mn>
<mo>-</mo>
<mi>p</mi>
<mo>)</mo>
</mrow>
</math>
</div>
<p>where <math display="inline"><mi>p</mi></math> is the proportion of observations of class <code>1</code>.
Therefore, by setting <math display="inline"><mi>p</mi></math>, we can remove features where the
vast majority of observations are one class.<a data-primary="" data-startref="ix_thresh_feat_sel" data-type="indexterm" id="id1505"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="10.3 Handling Highly Correlated Features" data-type="sect1"><div class="sect1" id="handling-highly-correlated-features">
<h1>10.3 Handling Highly Correlated Features</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id247">
<h2>Problem</h2>
<p>You have a <a data-primary="feature selection" data-secondary="highly correlated features in matrix" data-type="indexterm" id="ix_feat_sel_correl_matrix"/><a data-primary="highly correlated features in matrix" data-type="indexterm" id="ix_hi_correl_feat_matrix"/><a data-primary="correlation matrix, feature selection" data-type="indexterm" id="ix_correl_matrix_feat_sel"/>feature matrix and suspect some features are highly
correlated.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id248">
<h2>Solution</h2>
<p>Use a <a data-primary="matrix factorization" data-type="indexterm" id="ix_matrix_factor2"/>correlation matrix to check for highly correlated features. If
highly correlated features exist, consider dropping one of the
correlated features:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>

<code class="c1"># Create feature matrix with two highly correlated features</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code>
                     <code class="p">[</code><code class="mi">2</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code>
                     <code class="p">[</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code>
                     <code class="p">[</code><code class="mi">4</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code>
                     <code class="p">[</code><code class="mi">5</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code>
                     <code class="p">[</code><code class="mi">6</code><code class="p">,</code> <code class="mi">6</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code>
                     <code class="p">[</code><code class="mi">7</code><code class="p">,</code> <code class="mi">7</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code>
                     <code class="p">[</code><code class="mi">8</code><code class="p">,</code> <code class="mi">7</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code>
                     <code class="p">[</code><code class="mi">9</code><code class="p">,</code> <code class="mi">7</code><code class="p">,</code> <code class="mi">1</code><code class="p">]])</code>

<code class="c1"># Convert feature matrix into DataFrame</code>
<code class="n">dataframe</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Create correlation matrix</code>
<code class="n">corr_matrix</code> <code class="o">=</code> <code class="n">dataframe</code><code class="o">.</code><code class="n">corr</code><code class="p">()</code><code class="o">.</code><code class="n">abs</code><code class="p">()</code>

<code class="c1"># Select upper triangle of correlation matrix</code>
<code class="n">upper</code> <code class="o">=</code> <code class="n">corr_matrix</code><code class="o">.</code><code class="n">where</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">triu</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">ones</code><code class="p">(</code><code class="n">corr_matrix</code><code class="o">.</code><code class="n">shape</code><code class="p">),</code>
                          <code class="n">k</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="nb">bool</code><code class="p">))</code>

<code class="c1"># Find index of feature columns with correlation greater than 0.95</code>
<code class="n">to_drop</code> <code class="o">=</code> <code class="p">[</code><code class="n">column</code> <code class="k">for</code> <code class="n">column</code> <code class="ow">in</code> <code class="n">upper</code><code class="o">.</code><code class="n">columns</code> <code class="k">if</code> <code class="nb">any</code><code class="p">(</code><code class="n">upper</code><code class="p">[</code><code class="n">column</code><code class="p">]</code> <code class="o">&gt;</code> <code class="mf">0.95</code><code class="p">)]</code>

<code class="c1"># Drop features</code>
<code class="n">dataframe</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="n">dataframe</code><code class="o">.</code><code class="n">columns</code><code class="p">[</code><code class="n">to_drop</code><code class="p">],</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">3</code><code class="p">)</code></pre>
<table>
<thead>
<tr>
<th/>
<th>0</th>
<th>2</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>1</td>
<td>1</td>
</tr>
<tr>
<th>1</th>
<td>2</td>
<td>0</td>
</tr>
<tr>
<th>2</th>
<td>3</td>
<td>1</td>
</tr>
</tbody>
</table>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id249">
<h2>Discussion</h2>
<p>One problem we often run into in machine learning is highly correlated
features. If two features are highly correlated, then the information
they contain is very similar, and it is likely redundant to include both
features. In the case of simple models like linear regression, failing to remove such features violates the assumptions of linear regression and can result in an artificially inflated R-squared value. The solution to highly correlated features is simple: remove
one of them from the feature set. Removing highly correlated features by setting a correlation threshold is another example of filtering.</p>
<p>In our solution, first we create a correlation matrix of all features:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Correlation matrix</code>
<code class="n">dataframe</code><code class="o">.</code><code class="n">corr</code><code class="p">()</code></pre>
<table>
<thead>
<tr>
<th/>
<th>0</th>
<th>1</th>
<th>2</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>1.000000</td>
<td>0.976103</td>
<td>0.000000</td>
</tr>
<tr>
<th>1</th>
<td>0.976103</td>
<td>1.000000</td>
<td>-0.034503</td>
</tr>
<tr>
<th>2</th>
<td>0.000000</td>
<td>-0.034503</td>
<td>1.000000</td>
</tr>
</tbody>
</table>
<p>Second, we look at the upper triangle of the correlation matrix to
identify pairs of highly correlated features:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Upper triangle of correlation matrix</code>
<code class="n">upper</code></pre>
<table>
<thead>
<tr>
<th/>
<th>0</th>
<th>1</th>
<th>2</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>NaN</td>
<td>0.976103</td>
<td>0.000000</td>
</tr>
<tr>
<th>1</th>
<td>NaN</td>
<td>NaN</td>
<td>0.034503</td>
</tr>
<tr>
<th>2</th>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
</tr>
</tbody>
</table>
<p>Third, we remove one feature from each of those pairs.<a data-primary="" data-startref="ix_correl_matrix_feat_sel" data-type="indexterm" id="id1506"/><a data-primary="" data-startref="ix_feat_sel_correl_matrix" data-type="indexterm" id="id1507"/><a data-primary="" data-startref="ix_hi_correl_feat_matrix" data-type="indexterm" id="id1508"/><a data-primary="" data-startref="ix_matrix_factor2" data-type="indexterm" id="id1509"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="10.4 Removing Irrelevant Features for Classification" data-type="sect1"><div class="sect1" id="removing-irrelevant-features-for-classification">
<h1>10.4 Removing Irrelevant Features for Classification</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id503">
<h2>Problem</h2>
<p>You have a <a data-primary="feature selection" data-secondary="removing irrelevant features for classification" data-type="indexterm" id="ix_feat_sel_rem_irr_feat"/><a data-primary="classification and classifiers" data-secondary="removing irrelevant features" data-type="indexterm" id="ix_classif_rem_irr_feat"/>categorical target vector and want to remove uninformative
features.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id250">
<h2>Solution</h2>
<p>If the features are categorical, calculate a <a data-primary="χ² (chi-square) statistic, feature selection" data-type="indexterm" id="id1510"/><a data-primary="chi-square (χ²) statistic, feature selection" data-type="indexterm" id="id1511"/>chi-square (<math display="inline"><msup><mi>χ</mi> <mn>2</mn> </msup></math>) statistic between each feature and the target vector:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_iris</code>
<code class="kn">from</code> <code class="nn">sklearn.feature_selection</code> <code class="kn">import</code> <code class="n">SelectKBest</code>
<code class="kn">from</code> <code class="nn">sklearn.feature_selection</code> <code class="kn">import</code> <code class="n">chi2</code><code class="p">,</code> <code class="n">f_classif</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Convert to categorical data by converting data to integers</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">features</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="nb">int</code><code class="p">)</code>

<code class="c1"># Select two features with highest chi-squared statistics</code>
<code class="n">chi2_selector</code> <code class="o">=</code> <code class="n">SelectKBest</code><code class="p">(</code><code class="n">chi2</code><code class="p">,</code> <code class="n">k</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
<code class="n">features_kbest</code> <code class="o">=</code> <code class="n">chi2_selector</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>

<code class="c1"># Show results</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Original number of features:"</code><code class="p">,</code> <code class="n">features</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Reduced number of features:"</code><code class="p">,</code> <code class="n">features_kbest</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code></pre>
<pre data-type="programlisting">Original number of features: 4
Reduced number of features: 2</pre>
<p>If the features are quantitative, compute the <a data-primary="ANOVA F-value, feature selection" data-type="indexterm" id="id1512"/>ANOVA F-value between each
feature and the target vector:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Select two features with highest F-values</code>
<code class="n">fvalue_selector</code> <code class="o">=</code> <code class="n">SelectKBest</code><code class="p">(</code><code class="n">f_classif</code><code class="p">,</code> <code class="n">k</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
<code class="n">features_kbest</code> <code class="o">=</code> <code class="n">fvalue_selector</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>

<code class="c1"># Show results</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Original number of features:"</code><code class="p">,</code> <code class="n">features</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Reduced number of features:"</code><code class="p">,</code> <code class="n">features_kbest</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code></pre>
<pre data-type="programlisting">Original number of features: 4
Reduced number of features: 2</pre>
<p>Instead of <a data-primary="SelectPercentile" data-type="indexterm" id="id1513"/>selecting a specific number of features, we can use
<code>SelectPercentile</code> to select the top <em>n</em> percent of features:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">from</code> <code class="nn">sklearn.feature_selection</code> <code class="kn">import</code> <code class="n">SelectPercentile</code>

<code class="c1"># Select top 75% of features with highest F-values</code>
<code class="n">fvalue_selector</code> <code class="o">=</code> <code class="n">SelectPercentile</code><code class="p">(</code><code class="n">f_classif</code><code class="p">,</code> <code class="n">percentile</code><code class="o">=</code><code class="mi">75</code><code class="p">)</code>
<code class="n">features_kbest</code> <code class="o">=</code> <code class="n">fvalue_selector</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>

<code class="c1"># Show results</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Original number of features:"</code><code class="p">,</code> <code class="n">features</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Reduced number of features:"</code><code class="p">,</code> <code class="n">features_kbest</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code></pre>
<pre data-type="programlisting">Original number of features: 4
Reduced number of features: 3</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id251">
<h2>Discussion</h2>
<p>Chi-square<a data-primary="chi-square (χ²) statistic, feature selection" data-type="indexterm" id="id1514"/><a data-primary="χ² (chi-square) statistic, feature selection" data-type="indexterm" id="id1515"/> statistics examine the independence of two categorical
vectors. That is, the statistic is the difference between the observed
number of observations in each class of a categorical feature and what
we would expect if that feature were independent (i.e., no relationship)
of the target vector:</p>
<div data-type="equation">
<math display="block">
<mrow>
<msup><mi>χ</mi> <mn>2</mn> </msup>
<mo>=</mo>
<munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </munderover>
<mfrac><msup><mrow><mo>(</mo><msub><mi>O</mi> <mi>i</mi> </msub><mo>-</mo><msub><mi>E</mi> <mi>i</mi> </msub><mo>)</mo></mrow> <mn>2</mn> </msup> <msub><mi>E</mi> <mi>i</mi> </msub></mfrac>
</mrow>
</math>
</div>
<p>where <math display="inline"><msub><mi>O</mi><mi>i</mi></msub></math> is the number of observed observations in class <math display="inline"><mi>i</mi></math>, and <math display="inline"><msub><mi>E</mi><mi>i</mi></msub></math> is the number of expected observations in
class <math display="inline"><mi>i</mi></math>.</p>
<p>A chi-squared statistic is a single number that tells you how much
difference exists between your observed counts and the counts you would
expect if there were no relationship at all in the population. By
calculating the chi-squared statistic between a feature and the target
vector, we obtain a measurement of the independence between the two. If
the target is independent of the feature variable, then it is irrelevant
for our purposes because it contains no information we can use for
classification. On the other hand, if the two features are highly
dependent, they likely are very informative for training our model.</p>
<p>To use chi-squared in feature selection, we calculate the chi-squared
statistic between each feature and the target vector, then select the features with the best chi-square statistics. In <a data-primary="SelectKBest" data-type="indexterm" id="id1516"/>scikit-learn, we can use <code>SelectKBest</code> to select them. The
parameter <code>k</code> determines the number of features we want to keep and filters out the least informative features.</p>
<p>It is important to note that chi-square statistics can be
calculated only between two categorical vectors. For this reason, chi-squared
for feature selection requires that both the target vector and the features are categorical. However, if we have a numerical feature we can use the chi-squared technique by first transforming the quantitative feature into a categorical feature. Finally, to use our chi-squared approach, all values need to be nonnegative.</p>
<p>Alternatively, if we have a numerical feature, we can use <code>f_classif</code> to
calculate the <a data-primary="ANOVA F-value, feature selection" data-type="indexterm" id="id1517"/>ANOVA F-value statistic with each feature and the target
vector. F-value scores examine if, when we group the numerical feature
by the target vector, the means for each group are significantly
different. For example, if we had a binary target vector, gender, and a
quantitative feature, test scores, the F-value score would tell us if
the mean test score for men is different than the mean test score for
women. If it is not, then test score doesn’t help us predict gender
and therefore the feature is irrelevant.<a data-primary="" data-startref="ix_filter_feat_sel" data-type="indexterm" id="id1518"/><a data-primary="" data-startref="ix_classif_rem_irr_feat" data-type="indexterm" id="id1519"/><a data-primary="" data-startref="ix_feat_sel_rem_irr_feat" data-type="indexterm" id="id1520"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="10.5 Recursively Eliminating Features" data-type="sect1"><div class="sect1" id="recursively-eliminating-features">
<h1>10.5 Recursively Eliminating Features</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id504">
<h2>Problem</h2>
<p>You want to <a data-primary="feature selection" data-secondary="recursively eliminating features" data-type="indexterm" id="ix_feat_sel_recurs_elim"/>automatically select the best features to keep.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id252">
<h2>Solution</h2>
<p>Use <a data-primary="RFE (recursive feature elimination)" data-type="indexterm" id="ix_rfe_recurs_elim"/><a data-primary="RFECV" data-type="indexterm" id="ix_rfe_cv"/><a data-primary="recursive feature elimination (RFE)" data-type="indexterm" id="ix_recurs_elim"/><a data-primary="cross-validation (CV) of ML models" data-secondary="RFECV" data-type="indexterm" id="ix_cross_val_cv_rfecv"/>scikit-learn’s <code>RFECV</code> to conduct <em>recursive feature elimination</em>
(RFE) using cross-validation (CV). That is, use the wrapper feature selection method and repeatedly train a model,
each time removing a feature until model performance (e.g., accuracy)
becomes worse. The remaining features are the best:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">warnings</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_regression</code>
<code class="kn">from</code> <code class="nn">sklearn.feature_selection</code> <code class="kn">import</code> <code class="n">RFECV</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code><code class="p">,</code> <code class="n">linear_model</code>

<code class="c1"># Suppress an annoying but harmless warning</code>
<code class="n">warnings</code><code class="o">.</code><code class="n">filterwarnings</code><code class="p">(</code><code class="n">action</code><code class="o">=</code><code class="s2">"ignore"</code><code class="p">,</code> <code class="n">module</code><code class="o">=</code><code class="s2">"scipy"</code><code class="p">,</code>
                        <code class="n">message</code><code class="o">=</code><code class="s2">"^internal gelsd"</code><code class="p">)</code>

<code class="c1"># Generate features matrix, target vector, and the true coefficients</code>
<code class="n">features</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="n">make_regression</code><code class="p">(</code><code class="n">n_samples</code> <code class="o">=</code> <code class="mi">10000</code><code class="p">,</code>
                                   <code class="n">n_features</code> <code class="o">=</code> <code class="mi">100</code><code class="p">,</code>
                                   <code class="n">n_informative</code> <code class="o">=</code> <code class="mi">2</code><code class="p">,</code>
                                   <code class="n">random_state</code> <code class="o">=</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Create a linear regression</code>
<code class="n">ols</code> <code class="o">=</code> <code class="n">linear_model</code><code class="o">.</code><code class="n">LinearRegression</code><code class="p">()</code>

<code class="c1"># Recursively eliminate features</code>
<code class="n">rfecv</code> <code class="o">=</code> <code class="n">RFECV</code><code class="p">(</code><code class="n">estimator</code><code class="o">=</code><code class="n">ols</code><code class="p">,</code> <code class="n">step</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">scoring</code><code class="o">=</code><code class="s2">"neg_mean_squared_error"</code><code class="p">)</code>
<code class="n">rfecv</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>
<code class="n">rfecv</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([[ 0.00850799,  0.7031277 ,  1.52821875],
       [-1.07500204,  2.56148527, -0.44567768],
       [ 1.37940721, -1.77039484, -0.74675125],
       ...,
       [-0.80331656, -1.60648007,  0.52231601],
       [ 0.39508844, -1.34564911,  0.4228057 ],
       [-0.55383035,  0.82880112,  1.73232647]])</pre>
<p>Once we have conducted RFE, we can see the number of features we should
keep:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Number of best features</code>
<code class="n">rfecv</code><code class="o">.</code><code class="n">n_features_</code></pre>
<pre data-type="programlisting">3</pre>
<p>We can also see which of those features we should keep:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Which categories are best</code>
<code class="n">rfecv</code><code class="o">.</code><code class="n">support_</code></pre>
<pre data-type="programlisting">array([False, False, False, False, False,  True, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False,  True, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False,  True, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False])</pre>
<p>We can even view the rankings of the features:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Rank features best (1) to worst</code>
<code class="n">rfecv</code><code class="o">.</code><code class="n">ranking_</code></pre>
<pre data-type="programlisting">array([11, 92, 96, 87, 46,  1, 48, 23, 16,  2, 66, 83, 33, 27, 70, 75, 29,
       84, 54, 88, 37, 42, 85, 62, 74, 50, 80, 10, 38, 59, 79, 57, 44,  8,
       82, 45, 89, 69, 94,  1, 35, 47, 39,  1, 34, 72, 19,  4, 17, 91, 90,
       24, 32, 13, 49, 26, 12, 71, 68, 40,  1, 43, 63, 28, 73, 58, 21, 67,
        1, 95, 77, 93, 22, 52, 30, 60, 81, 14, 86, 18, 15, 41,  7, 53, 65,
       51, 64,  6,  9, 20,  5, 55, 56, 25, 36, 61, 78, 31,  3, 76])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id253">
<h2>Discussion</h2>
<p>This is likely the most advanced recipe in this book up to this point,
combining a number of topics we have yet to address in detail. However,
the intuition is straightforward enough that we can address it here
rather than holding off until a later chapter. The idea behind RFE is to
train a model repeatedly, updating the <em>weights</em> or
<em>coefficients</em> of that model each time. The first time we train the model, we include all the
features. Then, we find the feature with the smallest parameter (notice
that this assumes the features are either rescaled or standardized),
meaning it is less important, and remove that feature from the feature
set.</p>
<p>The obvious question then is: how many features should we keep? We can
(hypothetically) repeat this loop until we only have one feature left. A
better approach requires that we include a new concept <a data-primary="cross-validation (CV) of ML models" data-type="indexterm" id="id1521"/>called
<em>cross-validation</em>. We will discuss CV in detail in the
next chapter, but here is the general idea.</p>
<p>Given data containing (1) a target we want to predict, and (2) a feature matrix, first we split the data into two groups: a training set and a test set. Second, we train our model using the training set. Third, we pretend that we do not know the target of the test set and apply our model to its features to predict the values of the test set. Finally, we compare our predicted target values with the true target values to evaluate our model.</p>
<p>We can use CV to find the optimum number of features to keep during RFE.
Specifically, in RFE with CV, after every iteration we use
cross-validation to evaluate our model. If CV shows that our model
improved after we eliminated a feature, then we continue on to the next
loop. However, if CV shows that our model got worse after we eliminated a feature, we put that feature back into the feature set and select those features as the best.</p>
<p>In scikit-learn, RFE with CV is implemented using <code>RFECV</code>, which contains a
number of important parameters. The <code>estimator</code> parameter determines the
type of model we want to train (e.g., linear regression), the <code>step</code>
parameter sets the number or proportion of features to drop during each
loop, and the <code>scoring</code> parameter sets the metric of quality we use to
evaluate our model during cross-validation.<a data-primary="" data-startref="ix_feat_sel_ch10" data-type="indexterm" id="id1522"/><a data-primary="" data-startref="ix_cross_val_cv_rfecv" data-type="indexterm" id="id1523"/><a data-primary="" data-startref="ix_feat_sel_recurs_elim" data-type="indexterm" id="id1524"/><a data-primary="" data-startref="ix_recurs_elim" data-type="indexterm" id="id1525"/><a data-primary="" data-startref="ix_rfe_cv" data-type="indexterm" id="id1526"/><a data-primary="" data-startref="ix_rfe_recurs_elim" data-type="indexterm" id="id1527"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1528">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/aV-Fz">scikit-learn documentation: Recursive feature elimination with cross-validation</a></p>
</li>
</ul>
</div></section>
</div></section>
</div></section></div></body></html>