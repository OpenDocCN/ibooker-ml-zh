<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 14. Trees and Forests" data-type="chapter" epub:type="chapter"><div class="chapter" id="trees-and-forests">
<h1><span class="label">Chapter 14. </span>Trees and Forests</h1>
<section data-pdf-bookmark="14.0 Introduction" data-type="sect1"><div class="sect1" id="id311">
<h1>14.0 Introduction</h1>
<p>Tree-based <a data-primary="trees and forests" data-type="indexterm" id="ix_trees_forests_ch14"/>learning algorithms are a broad and popular family of related non-parametric, supervised methods for both classification and regression. The basis of <a data-primary="decision trees" data-seealso="trees and forests" data-type="indexterm" id="id1713"/>tree-based learners is the <em>decision tree</em>,
wherein a series of decision rules (e.g., “If a person’s credit score is greater than 720…​”)
are chained. The result looks vaguely like an upside-down tree, with the
first decision rule at the top and subsequent decision rules spreading
out below. In a decision tree, every decision rule occurs at a decision
node, with the rule creating branches leading to new nodes. A branch
without a decision rule at the end is <a data-primary="leaf, decision tree" data-type="indexterm" id="id1714"/>called a <em>leaf</em>.</p>
<p>One reason for the popularity of tree-based models is their
interpretability. In fact, decision trees can literally be drawn out in
their complete form (see <a data-type="xref" href="#visualizing-a-decision-tree-model">Recipe 14.3</a>) to create a highly intuitive model. From this basic tree system comes a wide variety of extensions from random forests to stacking. In this chapter we will cover how to train, handle, adjust, visualize, and evaluate a number of tree-based models.</p>
</div></section>
<section data-pdf-bookmark="14.1 Training a Decision Tree Classifier" data-type="sect1"><div class="sect1" id="training-a-decision-tree-classifier">
<h1>14.1 Training a Decision Tree Classifier</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id522">
<h2>Problem</h2>
<p>You need to <a data-primary="trees and forests" data-secondary="decision tree training" data-type="indexterm" id="ix_trees_for_dec_tree"/><a data-primary="classification and classifiers" data-secondary="training decision tree classifiers" data-type="indexterm" id="ix_classif_dec_tree_class"/>train a classifier using a decision tree.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id312">
<h2>Solution</h2>
<p>Use <a data-primary="DecisionTreeClassifier" data-type="indexterm" id="id1715"/>scikit-learn’s <code>DecisionTreeClassifier</code>:</p>
<pre class="less_space pagebreak-before" data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="kn">import</code> <code class="n">DecisionTreeClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create decision tree classifier object</code>
<code class="n">decisiontree</code> <code class="o">=</code> <code class="n">DecisionTreeClassifier</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Train model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">decisiontree</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id313">
<h2>Discussion</h2>
<p>Decision <a data-primary="Gini impurity" data-type="indexterm" id="id1716"/>tree learners attempt to find a decision rule that produces the
greatest decrease in impurity at a node. While there are a number of
measurements of impurity, by default <code>DecisionTreeClassifier</code> uses Gini
impurity:</p>
<div data-type="equation">
<math display="block">
<mrow>
<mi>G</mi>
<mrow>
<mo>(</mo>
<mi>t</mi>
<mo>)</mo>
</mrow>
<mo>=</mo>
<mn>1</mn>
<mo>-</mo>
<munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>c</mi> </munderover>
<msup><mrow><msub><mi>p</mi> <mi>i</mi> </msub></mrow> <mn>2</mn> </msup>
</mrow>
</math>
</div>
<p>where <math display="inline"><mi>G</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo>
</mrow></math> is the Gini impurity at node <math display="inline"><mi>t</mi></math>, and <math display="inline"><msub><mi>p</mi><mi>i</mi></msub></math> is the proportion of observations of class
<math display="inline"><mi>c</mi></math> at node <math display="inline"><mi>t</mi></math>. This process of finding the decision rules that create splits to decrease impurity is repeated recursively until all leaf nodes are pure (i.e., contain only one class) or some arbitrary cutoff is reached.</p>
<p>In scikit-learn, <code>DecisionTreeClassifier</code> operates like other learning
methods; after the model is trained using <code>fit</code>, we can use the
model to predict the class of an observation:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Make new observation</code>
<code class="n">observation</code> <code class="o">=</code> <code class="p">[[</code> <code class="mi">5</code><code class="p">,</code>  <code class="mi">4</code><code class="p">,</code>  <code class="mi">3</code><code class="p">,</code>  <code class="mi">2</code><code class="p">]]</code>

<code class="c1"># Predict observation's class</code>
<code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">observation</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([1])</pre>
<p>We can also see the predicted class probabilities of the observation:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View predicted class probabilities for the three classes</code>
<code class="n">model</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">observation</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([[0., 1., 0.]])</pre>
<p class="less_space pagebreak-before">Finally, if we want to use a different impurity measurement we can use
the <code>criterion</code> parameter:<a data-primary="" data-startref="ix_classif_dec_tree_class" data-type="indexterm" id="id1717"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create decision tree classifier object using entropy</code>
<code class="n">decisiontree_entropy</code> <code class="o">=</code> <code class="n">DecisionTreeClassifier</code><code class="p">(</code>
    <code class="n">criterion</code><code class="o">=</code><code class="s1">'entropy'</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Train model</code>
<code class="n">model_entropy</code> <code class="o">=</code> <code class="n">decisiontree_entropy</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1718">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/lCPBG">Decision Tree Learning, Princeton</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="14.2 Training a Decision Tree Regressor" data-type="sect1"><div class="sect1" id="training-a-decision-tree-regressor">
<h1>14.2 Training a Decision Tree Regressor</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id314">
<h2>Problem</h2>
<p>You need to <a data-primary="regression and regressors" data-type="indexterm" id="ix_regress_dec_tree"/>train a regression model using a decision tree.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id315">
<h2>Solution</h2>
<p>Use <a data-primary="DecisionTreeRegressor" data-type="indexterm" id="id1719"/>scikit-learn’s <code>DecisionTreeRegressor</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="kn">import</code> <code class="n">DecisionTreeRegressor</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>

<code class="c1"># Load data with only two features</code>
<code class="n">diabetes</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_diabetes</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">diabetes</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">diabetes</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create decision tree regressor object</code>
<code class="n">decisiontree</code> <code class="o">=</code> <code class="n">DecisionTreeRegressor</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Train model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">decisiontree</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id316">
<h2>Discussion</h2>
<p>Decision <a data-primary="mean squared error (MSE)" data-type="indexterm" id="id1720"/><a data-primary="MSE (mean squared error)" data-type="indexterm" id="id1721"/>tree regression works similarly to decision tree classification; however, instead of reducing Gini impurity or entropy, potential splits are by default measured on how much they reduce mean squared error (MSE):</p>
<div data-type="equation">
<math display="block">
<mrow>
<mtext>MSE</mtext>
<mo>=</mo>
<mfrac><mn>1</mn> <mi>n</mi></mfrac>
<munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </munderover>
<msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi> </msub><mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>¯</mo></mover> <mi>i</mi> </msub><mo>)</mo></mrow> <mn>2</mn> </msup>
</mrow>
</math>
</div>
<p>where <math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math> is the true value of the target and
<math display="inline"> <msub><mover accent="true"><mi>y</mi> <mo>¯</mo></mover> <mi>i</mi></msub></math> is the mean value. In scikit-learn,
decision tree regression can be conducted using <code>DecisionTreeRegressor</code>.
Once we have trained a decision tree, we can use it to predict the
target value for an observation:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Make new observation</code>
<code class="n">observation</code> <code class="o">=</code> <code class="p">[</code><code class="n">features</code><code class="p">[</code><code class="mi">0</code><code class="p">]]</code>

<code class="c1"># Predict observation's value</code>
<code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">observation</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([151.])</pre>
<p>Just like with <code>DecisionTreeClassifier</code> we can use the <code>criterion</code>
parameter to select the desired measurement of split quality. For
example, we can construct a tree whose splits reduce mean absolute error
(MAE):<a data-primary="" data-startref="ix_trees_for_dec_tree" data-type="indexterm" id="id1722"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create decision tree classifier object using MAE</code>
<code class="n">decisiontree_mae</code> <code class="o">=</code> <code class="n">DecisionTreeRegressor</code><code class="p">(</code><code class="n">criterion</code><code class="o">=</code><code class="s2">"absolute_error"</code><code class="p">,</code>
  <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Train model</code>
<code class="n">model_mae</code> <code class="o">=</code> <code class="n">decisiontree_mae</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1723">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/EGkU_">scikit-learn documentation: Decision Tree Regression</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="14.3 Visualizing a Decision Tree Model" data-type="sect1"><div class="sect1" id="visualizing-a-decision-tree-model">
<h1>14.3 Visualizing a Decision Tree Model</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id523">
<h2>Problem</h2>
<p>You need to <a data-primary="trees and forests" data-secondary="visualizing decision tree model" data-type="indexterm" id="ix_trees_for_vis_mod"/><a data-primary="visualization" data-secondary="decision tree model" data-type="indexterm" id="ix_visual_dec_tree"/>visualize a model created by a decision tree learning
algorithm.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id317">
<h2>Solution</h2>
<p>Export the <a data-primary="DOT format, visualizing decision tree model" data-type="indexterm" id="ix_dot_form_vis"/>decision tree model into DOT format, then visualize:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">pydotplus</code>
<code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="kn">import</code> <code class="n">DecisionTreeClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">IPython.display</code> <code class="kn">import</code> <code class="n">Image</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">tree</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create decision tree classifier object</code>
<code class="n">decisiontree</code> <code class="o">=</code> <code class="n">DecisionTreeClassifier</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Train model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">decisiontree</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>

<code class="c1"># Create DOT data</code>
<code class="n">dot_data</code> <code class="o">=</code> <code class="n">tree</code><code class="o">.</code><code class="n">export_graphviz</code><code class="p">(</code><code class="n">decisiontree</code><code class="p">,</code>
                                <code class="n">out_file</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code>
                                <code class="n">feature_names</code><code class="o">=</code><code class="n">iris</code><code class="o">.</code><code class="n">feature_names</code><code class="p">,</code>
                                <code class="n">class_names</code><code class="o">=</code><code class="n">iris</code><code class="o">.</code><code class="n">target_names</code><code class="p">)</code>

<code class="c1"># Draw graph</code>
<code class="n">graph</code> <code class="o">=</code> <code class="n">pydotplus</code><code class="o">.</code><code class="n">graph_from_dot_data</code><code class="p">(</code><code class="n">dot_data</code><code class="p">)</code>

<code class="c1"># Show graph</code>
<code class="n">Image</code><code class="p">(</code><code class="n">graph</code><code class="o">.</code><code class="n">create_png</code><code class="p">())</code></pre>
<figure><div class="figure">
<img alt="mpc2 14in01" height="400" src="assets/mpc2_14in01.png" width="600"/>
<h6/>
</div></figure>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id318">
<h2>Discussion</h2>
<p>One of the advantages of decision tree classifiers is that we can
visualize the entire trained model, making decision trees one of the
most interpretable models in machine learning. In our solution, we
exported our trained model in DOT format (a graph description language)
and then used that to draw the graph.</p>
<p>If we look at the root node, we can see the decision rule is that if petal widths are less than or equal to 0.8 cm, then go to the left branch;
if not, go to the right branch. We can also see the Gini impurity index
(0.667), the number of observations (150), the <span class="keep-together">number</span> of observations in each class ([50,50,50]), and the class the observations would be predicted to be if we stopped at that node (<em>setosa</em>). We can also see that at that node the learner found that a single decision rule (<code>petal width (cm) &lt;= 0.8</code>) was able to perfectly identify all of the <em>setosa</em> class observations. Furthermore, with one more decision rule with the same feature (<code>petal width (cm) &lt;= 1.75</code>) the decision tree is able to correctly classify 144 of 150 observations. This makes petal width a very important feature!</p>
<p>If we want to use the decision tree in other applications or reports, we
can easily export the visualization into PDF or a PNG image:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create PDF</code>
<code class="n">graph</code><code class="o">.</code><code class="n">write_pdf</code><code class="p">(</code><code class="s2">"iris.pdf"</code><code class="p">)</code></pre>
<pre data-type="programlisting">True</pre>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create PNG</code>
<code class="n">graph</code><code class="o">.</code><code class="n">write_png</code><code class="p">(</code><code class="s2">"iris.png"</code><code class="p">)</code></pre>
<pre data-type="programlisting">True</pre>
<p>While this solution visualized a decision tree classifier, it can just
as easily be used to visualize a decision tree regressor.</p>
<p>Note: macOS users might have to install Graphviz’s executable to run the
preceding code. This can be done with the Homebrew command <code>brew install graphviz</code>. For Homebrew installation instructions, visit Homebrew’s website.<a data-primary="" data-startref="ix_dot_form_vis" data-type="indexterm" id="id1724"/><a data-primary="" data-startref="ix_trees_for_vis_mod" data-type="indexterm" id="id1725"/><a data-primary="" data-startref="ix_visual_dec_tree" data-type="indexterm" id="id1726"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1727">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/GgeNI">Homebrew</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="14.4 Training a Random Forest Classifier" data-type="sect1"><div class="sect1" id="training-a-random-forest-classifier">
<h1>14.4 Training a Random Forest Classifier</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id524">
<h2>Problem</h2>
<p>You want to train a <a data-primary="classification and classifiers" data-secondary="random forest classifier training" data-type="indexterm" id="ix_classif_rand_for_class"/>classification model using a “forest” of randomized decision trees.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id319">
<h2>Solution</h2>
<p>Use scikit-learn’s 
<code>RandomForestClassifier</code> to train a random forest classification<a data-primary="RandomForestClassifier" data-type="indexterm" id="ix_rand_for_class"/><a data-primary="random forests" data-secondary="classifier training" data-type="indexterm" id="ix_rand_for_class_train"/> model.</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">RandomForestClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create random forest classifier object</code>
<code class="n">randomforest</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Train model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">randomforest</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id320">
<h2>Discussion</h2>
<p>A common problem with decision trees is that they tend to fit the
training data too closely (i.e., overfitting). This has motivated the
widespread use of an ensemble learning method called <em>random forest</em>. In a random forest, many decision trees are trained, but each tree
receives only a bootstrapped sample of observations (i.e., a random sample of
observations with replacement that matches the original number of
observations), and each node considers only a subset of features when
determining the best split. This forest of randomized decision trees
(hence the name) votes to determine the predicted class.</p>
<p>As we can see by comparing this solution to <a data-type="xref" href="#training-a-decision-tree-classifier">Recipe 14.1</a>, scikit-learn’s

<span class="keep-together"><code>RandomForestClassifier</code></span> works similarly to <code>DecisionTreeClassifier</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Make new observation</code>
<code class="n">observation</code> <code class="o">=</code> <code class="p">[[</code> <code class="mi">5</code><code class="p">,</code>  <code class="mi">4</code><code class="p">,</code>  <code class="mi">3</code><code class="p">,</code>  <code class="mi">2</code><code class="p">]]</code>

<code class="c1"># Predict observation's class</code>
<code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">observation</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([1])</pre>
<p><code>RandomForestClassifier</code> also uses many of the same parameters as

<span class="keep-together"><code>DecisionTreeClassifier</code>.</span> For example, we can change the measure of split
quality used:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create random forest classifier object using entropy</code>
<code class="n">randomforest_entropy</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">(</code>
    <code class="n">criterion</code><code class="o">=</code><code class="s2">"entropy"</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Train model</code>
<code class="n">model_entropy</code> <code class="o">=</code> <code class="n">randomforest_entropy</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
<p>However, being a forest rather than an individual decision tree,

<span class="keep-together"><code>RandomForestClassifier</code></span> has certain parameters that are either unique
to random forests or particularly important. First, the <code>max_features</code> parameter determines the maximum number of features to be considered at each node and takes a number of arguments including integers (number of features), floats (percentage of features), and <code>sqrt</code> (square root of the number of features). By default, <code>max_features</code> is set to <code>auto</code>, which acts the same as <code>sqrt</code>. Second, the <code>bootstrap</code> parameter allows us to set whether the subset of observations considered for a tree is created using sampling with replacement (the default setting) or without replacement. Third, <code>n_estimators</code> sets the number of decision trees to include in the forest. Finally, while not specific to random forest classifiers, because we are effectively training many decision tree models, it is often useful to use all available cores by setting <code>n_jobs=-1</code>.<a data-primary="" data-startref="ix_classif_rand_for_class" data-type="indexterm" id="id1728"/><a data-primary="" data-startref="ix_rand_for_class" data-type="indexterm" id="id1729"/><a data-primary="" data-startref="ix_rand_for_class_train" data-type="indexterm" id="id1730"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1731">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/h-LQL">Random Forests, Berkeley Statistics</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="14.5 Training a Random Forest Regressor" data-type="sect1"><div class="sect1" id="training-a-random-forest-regressor">
<h1>14.5 Training a Random Forest Regressor</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id525">
<h2>Problem</h2>
<p>You want to train a <a data-primary="random forests" data-secondary="regressor training" data-type="indexterm" id="id1732"/><a data-primary="regression and regressors" data-secondary="random forest training" data-type="indexterm" id="id1733"/>regression model using a “forest” of randomized decision trees.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id321">
<h2>Solution</h2>
<p>Train a <a data-primary="RandomForestRegressor" data-type="indexterm" id="id1734"/>random forest regression model using scikit-learn’s
<code>RandomForestRegressor</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">RandomForestRegressor</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>

<code class="c1"># Load data with only two features</code>
<code class="n">diabetes</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_diabetes</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">diabetes</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">diabetes</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create random forest regressor object</code>
<code class="n">randomforest</code> <code class="o">=</code> <code class="n">RandomForestRegressor</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Train model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">randomforest</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id1735">
<h2>Discussion</h2>
<p>Just as we can make a forest of decision tree classifiers, we can
make a forest of decision tree regressors, where each tree uses a
bootstrapped subset of observations and at each node the decision rule
considers only a subset of features. As with <code>RandomForestClassifier</code> we
have certain important parameters:</p>
<dl>
<dt><code>max_features</code></dt>
<dd>
<p>Sets the maximum number of features to consider at each node. Defaults to <em>p</em> features, where <em>p</em> is the total number of features.</p>
</dd>
<dt><code>bootstrap</code></dt>
<dd>
<p>Sets whether or not to sample with replacement. Defaults to <code>True</code>.</p>
</dd>
<dt><code>n_estimators</code></dt>
<dd>
<p>Sets the number of decision trees to construct. Defaults to <code>10</code>.</p>
</dd>
</dl>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1736">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/ksa9Z">scikit-learn documentation: RandomForestRegressor</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="14.6 Evaluating Random Forests with Out-of-Bag Errors" data-type="sect1"><div class="sect1" id="evaluating-random-forests-with-out-of-bag-errors">
<h1>14.6 Evaluating Random Forests with Out-of-Bag Errors</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id526">
<h2>Problem</h2>
<p>You need to <a data-primary="random forests" data-secondary="evaluating out-of-bag errors" data-type="indexterm" id="id1737"/>evaluate a random forest model without using
cross-validation.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id322">
<h2>Solution</h2>
<p>Calculate the <a data-primary="OOB (out-of-bag) observations" data-type="indexterm" id="id1738"/><a data-primary="out-of-bag (OOB) observations" data-type="indexterm" id="id1739"/>model’s out-of-bag score:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">RandomForestClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create random forest classifier object</code>
<code class="n">randomforest</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">(</code>
    <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">n_estimators</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code> <code class="n">oob_score</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Train model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">randomforest</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>

<code class="c1"># View out-of-bag-error</code>
<code class="n">randomforest</code><code class="o">.</code><code class="n">oob_score_</code></pre>
<pre data-type="programlisting">0.9533333333333334</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id1740">
<h2>Discussion</h2>
<p>In random forests, each decision tree is trained using a bootstrapped
subset of observations. This means that for every tree there is a
separate subset of observations not being used to train that tree. These
are called out-of-bag (OOB) observations. We can use OOB observations as
a test set to evaluate the performance of our random forest.</p>
<p>For every observation, the learning algorithm compares the observation’s
true value with the prediction from a subset of trees not trained using
that observation. The overall score is calculated and provides a single
measure of a random forest’s performance. OOB score estimation is an
alternative to cross-validation.</p>
<p>In scikit-learn, we can compute OOB scores of a random forest by setting
<code>oob_score=True</code> in the random forest object (i.e.,
<code>RandomForestClassifier</code>). The score can be retrieved using
<code>oob_score_</code>.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="14.7 Identifying Important Features in Random Forests" data-type="sect1"><div class="sect1" id="identifying-important-features-in-random-forests">
<h1>14.7 Identifying Important Features in Random Forests</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id527">
<h2>Problem</h2>
<p>You need to <a data-primary="random forests" data-secondary="identifying important features" data-type="indexterm" id="ix_rand_for_feat_imp"/><a data-primary="visualization" data-secondary="decision tree model" data-type="indexterm" id="ix_visual_dec_tree_mod"/>know which features are most important in a random forest
model.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id323">
<h2>Solution</h2>
<p>Calculate and <a data-primary="feature_importances_ method" data-type="indexterm" id="ix_feat_imp_meth"/>visualize the importance of each feature by inspecting the model’s <code>feature_importances_</code> attribute:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>
<code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">RandomForestClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create random forest classifier object</code>
<code class="n">randomforest</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Train model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">randomforest</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>

<code class="c1"># Calculate feature importances</code>
<code class="n">importances</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">feature_importances_</code>

<code class="c1"># Sort feature importances in descending order</code>
<code class="n">indices</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">argsort</code><code class="p">(</code><code class="n">importances</code><code class="p">)[::</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code>

<code class="c1"># Rearrange feature names so they match the sorted feature importances</code>
<code class="n">names</code> <code class="o">=</code> <code class="p">[</code><code class="n">iris</code><code class="o">.</code><code class="n">feature_names</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="n">indices</code><code class="p">]</code>

<code class="c1"># Create plot</code>
<code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">()</code>

<code class="c1"># Create plot title</code>
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s2">"Feature Importance"</code><code class="p">)</code>

<code class="c1"># Add bars</code>
<code class="n">plt</code><code class="o">.</code><code class="n">bar</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="n">features</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">]),</code> <code class="n">importances</code><code class="p">[</code><code class="n">indices</code><code class="p">])</code>

<code class="c1"># Add feature names as x-axis labels</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xticks</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="n">features</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">]),</code> <code class="n">names</code><code class="p">,</code> <code class="n">rotation</code><code class="o">=</code><code class="mi">90</code><code class="p">)</code>

<code class="c1"># Show plot</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<figure><div class="figure">
<img alt="mpc2 14in02" height="524" src="assets/mpc2_14in02.png" width="529"/>
<h6/>
</div></figure>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id324">
<h2>Discussion</h2>
<p>One of the major benefits of decision trees is interpretability.
Specifically, we can visualize the entire model (see <a data-type="xref" href="#visualizing-a-decision-tree-model">Recipe 14.3</a>).
However, a random forest model is composed of tens, hundreds, or even
thousands of decision trees. This makes a simple, intuitive
visualization of a random forest model impractical. That said, there is
another option: we can compare (and visualize) the relative importance
of each feature.</p>
<p>In <a data-type="xref" href="#visualizing-a-decision-tree-model">Recipe 14.3</a>, we visualized a decision tree classifier model and saw
that decision rules based only on petal width were able to classify many
observations correctly. Intuitively, we can say this means that
petal width is an important feature in our classifier. More formally,
features with splits that have the greater mean decrease in impurity
(e.g., Gini impurity or entropy in classifiers and variance in
regressors) are considered more important.</p>
<p>However, there are two things to keep in mind regarding feature
importance. First, scikit-learn requires
that we break up nominal categorical features into multiple binary
features. This has the effect of spreading the importance of that
feature across all of the binary features and can make each
feature appear to be unimportant even when the original nominal
categorical feature is highly important. Second, if two features are
highly correlated, one feature will claim much of the importance,
making the other feature appear to be far less important, which has
implications for interpretation if not considered.</p>
<p>In scikit-learn, classification and regression decision trees and random
forests can report the relative importance of each feature using the
<code>feature_importances_</code> method:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># View feature importances</code>
<code class="n">model</code><code class="o">.</code><code class="n">feature_importances_</code></pre>
<pre data-type="programlisting">array([0.09090795, 0.02453104, 0.46044474, 0.42411627])</pre>
<p>The higher the number, the more important the feature (all importance
scores sum to 1). By plotting these values, we can add interpretability
to our random forest models.<a data-primary="" data-startref="ix_feat_imp_meth" data-type="indexterm" id="id1741"/><a data-primary="" data-startref="ix_rand_for_feat_imp" data-type="indexterm" id="id1742"/><a data-primary="" data-startref="ix_visual_dec_tree_mod" data-type="indexterm" id="id1743"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="14.8 Selecting Important Features in Random Forests" data-type="sect1"><div class="sect1" id="selecting-important-features-in-random-forests">
<h1>14.8 Selecting Important Features in Random Forests</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id528">
<h2>Problem</h2>
<p>You need to <a data-primary="feature selection" data-secondary="on random forest" data-secondary-sortas="random forest" data-type="indexterm" id="id1744"/><a data-primary="random forests" data-secondary="selecting important features" data-type="indexterm" id="id1745"/>conduct feature selection on a random forest.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id1746">
<h2>Solution</h2>
<p>Identify the importance features and retrain the model using only the
most important features:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">RandomForestClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.feature_selection</code> <code class="kn">import</code> <code class="n">SelectFromModel</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create random forest classifier</code>
<code class="n">randomforest</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Create object that selects features with importance greater</code>
<code class="c1"># than or equal to a threshold</code>
<code class="n">selector</code> <code class="o">=</code> <code class="n">SelectFromModel</code><code class="p">(</code><code class="n">randomforest</code><code class="p">,</code> <code class="n">threshold</code><code class="o">=</code><code class="mf">0.3</code><code class="p">)</code>

<code class="c1"># Create new feature matrix using selector</code>
<code class="n">features_important</code> <code class="o">=</code> <code class="n">selector</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>

<code class="c1"># Train random forest using most important features</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">randomforest</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_important</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id325">
<h2>Discussion</h2>
<p>There are situations where we might want to reduce the number of
features in our model. For example, we might want to reduce the model’s
variance, or we might want to improve interpretability by including only
the most important features.</p>
<p>In scikit-learn we can use a simple two-stage workflow to create a model
with reduced features. First, we train a random forest model using all
features. Then, we use this model to identify the most important
features. Next, we create a new feature matrix that includes only these
features. In our <a data-primary="SelectFromModel" data-type="indexterm" id="id1747"/>solution, we used the 
<span class="keep-together"><code>SelectFromModel</code></span> method to create
a feature matrix containing only features with an importance greater
than or equal to some <code>threshold</code> value. Finally, we created a new model using
only those features.</p>
<p>We must note two caveats to this approach. First, nominal
categorical features that have been one-hot encoded will see the
feature importance diluted across the binary features. Second, the
feature importance of highly correlated features will be effectively
assigned to one feature and not evenly distributed across both features.</p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1748">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/y9k2U">Variable Selection Using Random Forests, Robin Genuer, Jean-Michel Poggi, and Christine Tuleau-Malot</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="14.9 Handling Imbalanced Classes" data-type="sect1"><div class="sect1" id="id803">
<h1>14.9 Handling Imbalanced Classes</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id529">
<h2>Problem</h2>
<p>You have a <a data-primary="imbalanced classes, handling" data-secondary="trees and forests" data-type="indexterm" id="ix_imb_class_tree_for"/><a data-primary="trees and forests" data-secondary="and imbalanced classes" data-secondary-sortas="imbalanced classes" data-type="indexterm" id="ix_tree_for_imb_class"/>target vector with highly imbalanced classes and want to train a random forest model.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id1749">
<h2>Solution</h2>
<p>Train a decision tree or random forest model with
<code>class_weight="balanced"</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">RandomForestClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Make class highly imbalanced by removing first 40 observations</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">features</code><code class="p">[</code><code class="mi">40</code><code class="p">:,:]</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">target</code><code class="p">[</code><code class="mi">40</code><code class="p">:]</code>

<code class="c1"># Create target vector indicating if class 0, otherwise 1</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">where</code><code class="p">((</code><code class="n">target</code> <code class="o">==</code> <code class="mi">0</code><code class="p">),</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Create random forest classifier object</code>
<code class="n">randomforest</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">(</code>
    <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">,</code> <code class="n">class_weight</code><code class="o">=</code><code class="s2">"balanced"</code><code class="p">)</code>

<code class="c1"># Train model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">randomforest</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id326">
<h2>Discussion</h2>
<p>Imbalanced classes are a common problem when we are doing machine learning in the real world. Left unaddressed, the presence of imbalanced classes can reduce the performance of our model. We will discuss handling imbalanced classes during preprocessing in
<a data-type="xref" href="ch17.xhtml#handling-imbalanced-classes-ch17">Recipe 17.5</a>. However, many learning algorithms in scikit-learn come with built-in methods for correcting for imbalanced classes. We can set 
<span class="keep-together"><code>RandomForestClassifier</code></span> to correct for imbalanced classes using the <code>class_weight</code> parameter. If supplied with a dictionary in the form of class names and their desired weights (e.g., <code>{"male": 0.2, "female": 0.8}</code>), <code>RandomForestClassifier</code> will weight the classes accordingly. However, often a more useful argument is
<code>balanced</code>, wherein classes are automatically weighted inversely
proportional to how frequently they appear in the data:</p>
<div data-type="equation">
<math display="block">
<mrow>
<msub><mi>w</mi> <mi>j</mi> </msub>
<mo>=</mo>
<mfrac><mi>n</mi> <mrow><mi>k</mi><msub><mi>n</mi> <mi>j</mi> </msub></mrow></mfrac>
</mrow>
</math>
</div>
<p>where <math display="inline"><msub><mi>w</mi><mi>j</mi></msub></math> is the weight of class <math display="inline"><mi>j</mi></math>,
<math display="inline"><mi>n</mi></math> is the number of observations, <math display="inline"><msub><mi>n</mi><mi>j</mi></msub></math> is the
number of observations in class <math display="inline"><mi>j</mi></math>, and <math display="inline"><mi>k</mi></math> is the total number of classes. For example, in our solution we have 2 classes (<math display="inline"><mi>k</mi></math>), 110 observations (<math display="inline"><mi>n</mi></math>), and 10 and 100 observations in each class, respectively (<math display="inline"><msub><mi>n</mi><mi>j</mi></msub></math>). If we weight the classes using <code>class_weight="balanced"</code>, then the smaller
class is weighted more:</p>
<pre class="less_space pagebreak-before" data-code-language="python" data-type="programlisting"><code class="c1"># Calculate weight for small class</code>
<code class="mi">110</code><code class="o">/</code><code class="p">(</code><code class="mi">2</code><code class="o">*</code><code class="mi">10</code><code class="p">)</code></pre>
<pre data-type="programlisting">5.5</pre>
<p>while the larger class is weighted less:<a data-primary="" data-startref="ix_imb_class_tree_for" data-type="indexterm" id="id1750"/><a data-primary="" data-startref="ix_tree_for_imb_class" data-type="indexterm" id="id1751"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Calculate weight for large class</code>
<code class="mi">110</code><code class="o">/</code><code class="p">(</code><code class="mi">2</code><code class="o">*</code><code class="mi">100</code><code class="p">)</code></pre>
<pre data-type="programlisting">0.55</pre>
</div></section>
</div></section>
<section data-pdf-bookmark="14.10 Controlling Tree Size" data-type="sect1"><div class="sect1" id="controlling-tree-size">
<h1>14.10 Controlling Tree Size</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id530">
<h2>Problem</h2>
<p>You want to <a data-primary="trees and forests" data-secondary="controlling tree size" data-type="indexterm" id="ix_tree_for_size"/>manually determine the structure and size of a decision
tree.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id531">
<h2>Solution</h2>
<p>Use the <a data-primary="parameters" data-secondary="tree structure" data-type="indexterm" id="ix_param_tree"/>tree structure parameters in scikit-learn tree-based learning
algorithms:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="kn">import</code> <code class="n">DecisionTreeClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create decision tree classifier object</code>
<code class="n">decisiontree</code> <code class="o">=</code> <code class="n">DecisionTreeClassifier</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code>
                                      <code class="n">max_depth</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code>
                                      <code class="n">min_samples_split</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>
                                      <code class="n">min_samples_leaf</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>
                                      <code class="n">min_weight_fraction_leaf</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code>
                                      <code class="n">max_leaf_nodes</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code>
                                      <code class="n">min_impurity_decrease</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Train model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">decisiontree</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id327">
<h2>Discussion</h2>
<p>scikit-learn’s tree-based learning algorithms have a variety of
techniques for controlling the size of decision trees. These are
accessed through parameters:</p>
<dl>
<dt><code>max_depth</code></dt>
<dd>
<p>  Maximum depth of the tree. If <code>None</code>, the tree is grown
until all leaves are pure. If an integer, the tree is effectively
“pruned” to that depth.</p>
</dd>
<dt><code>min_samples_split</code></dt>
<dd>
<p>  Minimum number of observations at a node before
that node is split. If an integer is supplied as an argument, it
determines the raw minimum, while if a float is supplied, the minimum
is the percent of total observations.</p>
</dd>
<dt><code>min_samples_leaf</code></dt>
<dd>
<p>Minimum number of observations required to be at a leaf. Uses the same arguments as <code>min_samples_split</code>.</p>
</dd>
<dt><code>max_leaf_nodes</code></dt>
<dd>
<p>Maximum number of leaves.</p>
</dd>
<dt><code>min_impurity_split</code></dt>
<dd>
<p>  Minimum impurity decrease required before a
split is performed.</p>
</dd>
</dl>
<p>While it is useful to know these parameters exist, most likely we will
only be using <code>max_depth</code> and <code>min_impurity_split</code> because shallower
trees (sometimes called <em>stumps</em>) are simpler models and thus have lower
variance.<a data-primary="" data-startref="ix_param_tree" data-type="indexterm" id="id1752"/><a data-primary="" data-startref="ix_tree_for_size" data-type="indexterm" id="id1753"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="14.11 Improving Performance Through Boosting" data-type="sect1"><div class="sect1" id="improving-performance-through-boosting">
<h1>14.11 Improving Performance Through Boosting</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id328">
<h2>Problem</h2>
<p>You need a <a data-primary="performance" data-secondary="boosting to improve trees and forests" data-type="indexterm" id="ix_perf_boost_tree_for"/><a data-primary="boosting performance, trees and forests" data-type="indexterm" id="ix_boost_tree_for"/><a data-primary="trees and forests" data-secondary="boosting to improve performance" data-type="indexterm" id="ix_tree_for_boost_perf"/>model with better performance than decision trees or random
forests.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id329">
<h2>Solution</h2>
<p>Train a <a data-primary="AdaBoostRegressor" data-type="indexterm" id="ix_ada_boost_regress"/><a data-primary="AdaBoostClassifier" data-type="indexterm" id="ix_ada_boost_class"/>boosted model using <code>AdaBoostClassifier</code> or <code>AdaBoostRegressor</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">AdaBoostClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create adaboost tree classifier object</code>
<code class="n">adaboost</code> <code class="o">=</code> <code class="n">AdaBoostClassifier</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Train model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">adaboost</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
</div></section>
<section class="less_space pagebreak-before" data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id330">
<h2>Discussion</h2>
<p>In a random forest, an ensemble (group) of randomized decision trees
predicts the target vector. An alternative, and often more powerful,
approach is called <em>boosting</em>. In one form of boosting called AdaBoost, we
iteratively train a series of weak models (most often a shallow decision
tree, sometimes called a stump), each iteration giving higher priority
to observations the previous model predicted incorrectly. More specifically,
in AdaBoost:</p>
<ol>
<li>
<p>Assign every observation, <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math>, an initial weight value, <math display="inline"><mrow><msub><mi>w</mi> <mi>i</mi> </msub><mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac></mrow></math>, where <math display="inline"><mi>n</mi></math> is the total number of observations in the data.</p>
</li>
<li>
<p>Train a “weak” model on the data.</p>
</li>
<li>
<p>For each observation:</p>
<ol>
<li>
<p>If weak model predicts <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math> correctly,
<math display="inline"><msub><mi>w</mi><mi>i</mi></msub></math> is decreased.</p>
</li>
<li>
<p>If weak model predicts <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math> incorrectly,
<math display="inline"><msub><mi>w</mi><mi>i</mi></msub></math> is increased.</p>
</li>
</ol>
</li>
<li>
<p>Train a new weak model where observations with greater
<math display="inline"><msub><mi>w</mi><mi>i</mi></msub></math> are given greater priority.</p>
</li>
<li>
<p>Repeat steps 4 and 5 until the data is perfectly predicted or a
preset number of weak models has been trained.</p>
</li>
</ol>
<p>The result is an aggregated model where individual weak models focus
on more difficult (from a prediction perspective) observations. In
scikit-learn, we can implement AdaBoost using <code>AdaBoostClassifier</code> or
<code>AdaBoostRegressor</code>. The most important parameters are <code>base_estimator</code>, <code>n_estimators</code>, <code>learning_rate</code>, and <code>loss</code>:</p>
<dl>
<dt><code>base_estimator</code></dt>
<dd>
<p><code>base_estimator</code> is the learning algorithm to use to train the weak
models. The most common learner to use with AdaBoost is a decision tree, the
parameter’s default argument.</p>
</dd>
<dt><code>n_estimators</code></dt>
<dd>
<p><code>n_estimators</code> is the number of models to iteratively train.</p>
</dd>
<dt><code>learning_rate</code></dt>
<dd>
<p><code>learning_rate</code> is the contribution of each model to the weights, and it
defaults to <code>1</code>. Reducing the learning rate will mean the weights will
be increased or decreased to a small degree, forcing the model to train
slower (but sometimes resulting in better performance scores).</p>
</dd>
<dt><code>loss</code></dt>
<dd>
<p><code>loss</code> is exclusive to <code>AdaBoostRegressor</code> and sets the loss function
to use when updating weights. This defaults to a linear loss function
but can be changed to <code>square</code> or <code>exponential</code>.<a data-primary="" data-startref="ix_ada_boost_class" data-type="indexterm" id="id1754"/><a data-primary="" data-startref="ix_ada_boost_regress" data-type="indexterm" id="id1755"/><a data-primary="" data-startref="ix_tree_for_boost_perf" data-type="indexterm" id="id1756"/></p>
</dd>
</dl>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1757">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/5E1v4">Explaining AdaBoost, Robert E. Schapire</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="14.12 Training an XGBoost Model" data-type="sect1"><div class="sect1" id="training-an-xgboost-model">
<h1>14.12 Training an XGBoost Model</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id532">
<h2>Problem</h2>
<p>You need to <a data-primary="predictions and predicting" data-secondary="XGBoost model" data-type="indexterm" id="id1758"/>train a tree-based model with high predictive power.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id331">
<h2>Solution</h2>
<p>Use the <code>xgboost</code> Python <a data-primary="xgboost library" data-type="indexterm" id="id1759"/>library:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">xgboost</code> <code class="k">as</code> <code class="nn">xgb</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code><code class="p">,</code> <code class="n">preprocessing</code>
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">classification_report</code>
<code class="kn">from</code> <code class="nn">numpy</code> <code class="kn">import</code> <code class="n">argmax</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create dataset</code>
<code class="n">xgb_train</code> <code class="o">=</code> <code class="n">xgb</code><code class="o">.</code><code class="n">DMatrix</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="n">target</code><code class="p">)</code>

<code class="c1"># Define parameters</code>
<code class="n">param</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s1">'objective'</code><code class="p">:</code> <code class="s1">'multi:softprob'</code><code class="p">,</code>
    <code class="s1">'num_class'</code><code class="p">:</code> <code class="mi">3</code>
<code class="p">}</code>

<code class="c1"># Train model</code>
<code class="n">gbm</code> <code class="o">=</code> <code class="n">xgb</code><code class="o">.</code><code class="n">train</code><code class="p">(</code><code class="n">param</code><code class="p">,</code> <code class="n">xgb_train</code><code class="p">)</code>

<code class="c1"># Get predictions</code>
<code class="n">predictions</code> <code class="o">=</code> <code class="n">argmax</code><code class="p">(</code><code class="n">gbm</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">xgb_train</code><code class="p">),</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Get a classification report</code>
<code class="nb">print</code><code class="p">(</code><code class="n">classification_report</code><code class="p">(</code><code class="n">target</code><code class="p">,</code> <code class="n">predictions</code><code class="p">))</code></pre>
<pre data-type="programlisting">              precision    recall  f1-score   support

           0       1.00      1.00      1.00        50
           1       1.00      0.96      0.98        50
           2       0.96      1.00      0.98        50

    accuracy                           0.99       150
   macro avg       0.99      0.99      0.99       150
weighted avg       0.99      0.99      0.99       150</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id332">
<h2>Discussion</h2>
<p>XGBoost <a data-primary="Extreme Gradient Boosting (XGBoost)" data-type="indexterm" id="id1760"/><a data-primary="XGBoost algorithm" data-type="indexterm" id="id1761"/>(which stands for Extreme Gradient Boosting) is a very popular gradient boosting algorithm in the machine learning space. Though it is not always a tree-based model, it is frequently applied to ensembles of decision trees. It gained much of its popularity due to widespread success on the machine learning competition website Kaggle and has since been a reliable algorithm for improving performance beyond that of typical random forests or gradient boosted machines.</p>
<p>Although XGBoost is known for being computationally intensive, computational performance optimizations (such as GPU support) over the last few years have made iterating quickly with XGBoost significantly easier, and it remains a common choice of algorithm when statistical performance is a requirement.</p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1762">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/cAuGX">XGBoost documentation</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="14.13 Improving Real-Time Performance with LightGBM" data-type="sect1"><div class="sect1" id="improving-real-time-performance-with-lightgbm">
<h1>14.13 Improving Real-Time Performance with LightGBM</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id533">
<h2>Problem</h2>
<p>You need to <a data-primary="trees and forests" data-secondary="LightGBM to improve performance" data-type="indexterm" id="ix_tree_for_light_gbm"/>train a gradient boosted tree-based model that is computationally 
<span class="keep-together">optimized.</span></p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id333">
<h2>Solution</h2>
<p>Use the <a data-primary="lightgbm library" data-type="indexterm" id="ix_light_gbm_lib"/>gradient boosted machine library <code>lightgbm</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">import</code> <code class="nn">lightgbm</code> <code class="k">as</code> <code class="nn">lgb</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code><code class="p">,</code> <code class="n">preprocessing</code>
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">classification_report</code>
<code class="kn">from</code> <code class="nn">numpy</code> <code class="kn">import</code> <code class="n">argmax</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Create dataset</code>
<code class="n">lgb_train</code> <code class="o">=</code> <code class="n">lgb</code><code class="o">.</code><code class="n">Dataset</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>

<code class="c1"># Define parameters</code>
<code class="n">params</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s1">'objective'</code><code class="p">:</code> <code class="s1">'multiclass'</code><code class="p">,</code>
    <code class="s1">'num_class'</code><code class="p">:</code> <code class="mi">3</code><code class="p">,</code>
    <code class="s1">'verbose'</code><code class="p">:</code> <code class="o">-</code><code class="mi">1</code><code class="p">,</code>
<code class="p">}</code>

<code class="c1"># Train model</code>
<code class="n">gbm</code> <code class="o">=</code> <code class="n">lgb</code><code class="o">.</code><code class="n">train</code><code class="p">(</code><code class="n">params</code><code class="p">,</code> <code class="n">lgb_train</code><code class="p">)</code>

<code class="c1"># Get predictions</code>
<code class="n">predictions</code> <code class="o">=</code> <code class="n">argmax</code><code class="p">(</code><code class="n">gbm</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">features</code><code class="p">),</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Get a classification report</code>
<code class="nb">print</code><code class="p">(</code><code class="n">classification_report</code><code class="p">(</code><code class="n">target</code><code class="p">,</code> <code class="n">predictions</code><code class="p">))</code></pre>
<pre data-type="programlisting">              precision    recall  f1-score   support

           0       1.00      1.00      1.00        50
           1       1.00      1.00      1.00        50
           2       1.00      1.00      1.00        50

    accuracy                           1.00       150
   macro avg       1.00      1.00      1.00       150
weighted avg       1.00      1.00      1.00       150</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id334">
<h2>Discussion</h2>
<p>The <code>lightgbm</code> library is used for gradient boosted machines and is highly optimized for training time, inference, and GPU support. As a result of its computational efficiency, it’s often used in production and in large scale settings. Although scikit-learn models are typically easier to use, some libraries, such as <code>lightgbm</code>, can be handy when you’re limited by large data or strict model training/serving times.<a data-primary="" data-startref="ix_trees_forests_ch14" data-type="indexterm" id="id1763"/><a data-primary="" data-startref="ix_light_gbm_lib" data-type="indexterm" id="id1764"/><a data-primary="" data-startref="ix_tree_for_light_gbm" data-type="indexterm" id="id1765"/><a data-primary="" data-startref="ix_boost_tree_for" data-type="indexterm" id="id1766"/><a data-primary="" data-startref="ix_perf_boost_tree_for" data-type="indexterm" id="id1767"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1768">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/XDcpG">LightGBM documentation</a></p>
</li>
<li>
<p><a href="https://oreil.ly/4Bb8g">CatBoost documentation (another optimized library for GBMs)</a></p>
</li>
</ul>
</div></section>
</div></section>
</div></section></div></body></html>