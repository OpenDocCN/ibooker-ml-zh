<html><head></head><body><section data-pdf-bookmark="Chapter 7. Serving Models and Architectures" data-type="chapter" epub:type="chapter"><div class="chapter" id="serving-and-architecture">&#13;
<h1><span class="label">Chapter 7. </span>Serving Models and Architectures</h1>&#13;
&#13;
&#13;
<p>As<a data-primary="architectures" data-secondary="definition of term" data-type="indexterm" id="id571"/> we think about how recommendation systems utilize the available data to learn and eventually serve recommendations, it’s crucial to describe how the pieces fit together. The combination of the data flow and the jointly available data for learning is called the <em>architecture</em>. More formally, the architecture is the connections and interactions of the system or network of services; for data applications, the architecture also includes the available features and objective functions for each subsystem. Defining the architecture typically involves identifying components or individual services, defining the relationships and dependencies among those components, and specifying the protocols or interfaces through which they will communicate.</p>&#13;
&#13;
<p>In this chapter, we’ll spell out some of the most popular and important architectures for recommendation systems.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Architectures by Recommendation Structure" data-type="sect1"><div class="sect1" id="id203">&#13;
<h1>Architectures by Recommendation Structure</h1>&#13;
&#13;
<p>We<a data-primary="architectures" data-secondary="by recommendation structure" data-secondary-sortas="recommendation structure" data-type="indexterm" id="Arecstruct07"/> have returned several times to the concept of collector, ranker, and server, and we’ve seen that they may be regarded via two paradigms: the online and the offline modes. Further, we’ve seen how many of the components in <a data-type="xref" href="ch06.html#data-processing">Chapter 6</a> satisfy some of the core requirements of these functions.</p>&#13;
&#13;
<p>Designing large systems like these requires several architectural considerations. In this section, we will demonstrate how these concepts are adapted based on the type of recommendation system you are building. We’ll compare a mostly standard item-to-user recommendation system, a query-based recommendation system, context-based recommendations, and sequence-based recommendations.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Item-to-User Recommendations" data-type="sect2"><div class="sect2" id="id60">&#13;
<h2>Item-to-User Recommendations</h2>&#13;
&#13;
<p>We’ll<a data-primary="item-to-user recommendations" data-type="indexterm" id="id572"/><a data-primary="recommendation systems" data-secondary="item-to-user recommendations" data-type="indexterm" id="id573"/> start by describing the architecture of the system we’ve been building in the book thus far. As proposed in <a data-type="xref" href="ch04.html#ch:system_design">Chapter 4</a>, we built the collector offline to ingest and process our recommendations. We utilize representations to encode relationships between items, users, or user-item pairs.</p>&#13;
&#13;
<p>The online collector takes the request, usually in the form of a user ID, and finds a neighborhood of items in this representation space to pass along to the ranker. Those items are filtered when appropriate and sent for scoring.</p>&#13;
&#13;
<p>The offline ranker learns the relevant features for scoring and ranking, training on the historical data. It then uses this model and, in some cases, item features as well for inference.</p>&#13;
&#13;
<p>In the case of recommendation systems, this inference computes the scores associated to each item in the set of potential recommendations. We usually sort by this score, which you’ll learn more about in <a data-type="xref" href="part03.html#ranking">Part III</a>. Finally, we integrate a final round of ordering based on some business logic (described in <a data-type="xref" href="ch14.html#HardRanking">Chapter 14</a>). This last step is part of the serving, where we impose requirements like test criteria or recommendation diversity requirements.</p>&#13;
&#13;
<p><a data-type="xref" href="#fig:four-stage-recommender">Figure 7-1</a> is an excellent overview of the retrieval, ranking, and serving structure, although it depicts four stages and uses slightly different terminology. In this book, we combine the filtering stage shown here into retrieval.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Query-Based Recommendations" data-type="sect2"><div class="sect2" id="id61">&#13;
<h2>Query-Based Recommendations</h2>&#13;
&#13;
<p>To<a data-primary="queries" data-secondary="query-based recommendations" data-type="indexterm" id="id574"/><a data-primary="recommendation systems" data-secondary="query-based recommendations" data-type="indexterm" id="id575"/> start off our process, we want to make a query. The most obvious example of a query is a text query as in text-based search engines; however, queries may be more general! For example, you may wish to allow search-by-image or search-by-tag options. Note that an important type of query-based recommender uses an<a data-primary="implicit queries" data-type="indexterm" id="id576"/><a data-primary="queries" data-secondary="implicit queries" data-type="indexterm" id="id577"/> <em>implicit</em> query: the user is providing a search query via UI choices or by behaviors. While these systems are quite similar in overall structure to the item-to-user systems, let’s discover how to modify them to fit our use case.</p>&#13;
&#13;
<figure><div class="figure" id="fig:four-stage-recommender">&#13;
<img alt="Four Stage Recommender" src="assets/brpj_0701.png"/>&#13;
<h6><span class="label">Figure 7-1. </span>A four-stage recommendation system (adapted from an image by Karl Higley and Even Oldridge)</h6>&#13;
</div></figure>&#13;
&#13;
<p>We want to integrate more context about the query into the first step of the request. Note that we don’t want to throw out the user-item matching components of this system. Even though the user is performing a search, personalizing the recommendations based on their taste is useful. Instead, we need to utilize the query as well; later we will discuss various technical strategies, but a simple summary for now is to also generate an embedding for the query. Note that the query is like an item or a user but is sufficiently different.</p>&#13;
&#13;
<p>Some strategies might include similarity between the query and items, or co-occurrence of the query and items. Either way, we now have a query representation and user representation, and we want to utilize both for our recommendation. One simple approach is to use the query representation for retrieval, but during the scoring, score via both query-item and user-item, combining them via a multiobjective loss. Another approach is to use the user for retrieval and then the query for filtering.</p>&#13;
<div data-type="warning" epub:type="warning"><h1>Different Embeddings</h1>&#13;
<p>Unfortunately, while we’d love the same embedding space<a data-primary="embedding models" data-secondary="out of distribution queries" data-type="indexterm" id="id578"/> (for nearest-neighbors lookup) to work well for our queries and our documents (items, etc.), this is often not the case. The simplest example is something like asking questions and hoping to find relevant Wikipedia articles. This problem is often referred to as the queries being<a data-primary="out of distribution queries" data-type="indexterm" id="id579"/><a data-primary="queries" data-secondary="out of distribution queries" data-type="indexterm" id="id580"/> “out of distribution” from the documents.</p>&#13;
&#13;
<p>Wikipedia articles are written in a declarative informative article style, whereas questions are often brief and casual. If you were to use an embedding model focused on capturing semantic meaning, you’d naively expect the queries to be located in significantly different subspaces than the articles. This means that your distance computations will be affected. This is often <em>not</em> a huge problem because you retrieve via relative distances, and you can hope that the shared subspaces are enough to provide a good retrieval. However, it can be hard to predict when these perform poorly.</p>&#13;
&#13;
<p>The best practice is to carefully examine the embeddings on common queries and on target results. These problems can be <em>especially</em> bad on implicit queries like a series of actions taken at a particular time of day to look up food recommendations. In this case, we expect the queries to be wildly different from the documents.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Context-Based Recommendations" data-type="sect2"><div class="sect2" id="id204">&#13;
<h2>Context-Based Recommendations</h2>&#13;
&#13;
<p>A<a data-primary="context" data-secondary="context-based recommendations" data-type="indexterm" id="id581"/><a data-primary="recommendation systems" data-secondary="context-based" data-type="indexterm" id="id582"/> context is quite similar to a query but tends to be more obviously feature based and frequently less similar to the items/users distributions. <em>Context</em> <a data-primary="context" data-secondary="definition of term" data-type="indexterm" id="id583"/>is usually the term used to represent exogenous features to the system that may have an effect on the system—i.e., auxiliary information such as time, weather, or location. Context-based recommendation is similar to query based in that context is an additional signal that the system needs to consider during recommendation, but more often than not, the query should dominate the signal for recommendation, whereas the context <span class="keep-together">should not.</span></p>&#13;
&#13;
<p>Let’s take a simple example of ordering food. A query for a food-delivery recommendation system would look like <em>Mexican food</em>; this is an extremely important signal from the user looking for burritos or quesadillas of how the recommendations should look. A context for a food-delivery recommendation system would look like <em>it’s almost lunchtime</em>. This signal is useful but may not outweigh user personalization. Putting hard-and-fast rules on this weighting can be difficult, so usually we don’t, and instead we learn parameters via experimentation.</p>&#13;
&#13;
<p class="less_space pagebreak-before">Context features fit into the architecture similar to the way queries do, via learned weightings as part of the objective function. Your model will learn a representation between context features and items, and then add that affinity into the rest of the pipeline. Again, you can make use of this early in the retrieval, later in the ranking, or even during the serving step.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Sequence-Based Recommendations" data-type="sect2"><div class="sect2" id="id62">&#13;
<h2>Sequence-Based Recommendations</h2>&#13;
&#13;
<p><em>Sequence-based recommendations</em> build<a data-primary="sequential recommenders" data-secondary="influence of recent exposures on" data-type="indexterm" id="id584"/><a data-primary="recommendation systems" data-secondary="sequence-based recommendations" data-type="indexterm" id="id585"/> on context-based recommendations but with a specific type of context. Sequential recommendations are based on the idea that the recent items the user has been exposed to should have a significant influence on the recommendations. A common example here is a music-streaming service, as the last few songs that have been played can significantly inform what the user might want to hear next. To ensure that this<a data-primary="autoregression" data-type="indexterm" id="id586"/> <em>autoregressive</em>, or sequentially predictive, set of features has an influence on recommendations, we can treat each item in the sequence as a weighted context for the recommendation.</p>&#13;
&#13;
<p>Usually, the item-item representation similarities are weighted to provide a collection of recommendations, and various strategies are used for combining these. In this case, we normally expect the user to be of high importance in the recommendations, but the sequence is also of high importance. One simple model is to think of the sequence of items as a sequence of tokens, and form a single embedding for that sequence—as in NLP applications. This embedding can be used as the context in a context-based recommendation architecture.</p>&#13;
<div data-type="warning" epub:type="warning"><h1>Naive Sequence Embeddings</h1>&#13;
<p>The<a data-primary="naive sequence embeddings" data-type="indexterm" id="id587"/><a data-primary="embedding models" data-secondary="naive sequence embeddings" data-type="indexterm" id="id588"/> combinatorics of one-embedding-per-sequence explode in cardinality; the number of potential items in each sequential slot is very large, and each item in the sequence multiplies those possibilities together. Imagine, for example, five-word sequences, where the number of possibilities for each item is close to the size of the English lexicon, and thus it would be that size to the fifth power. We provide simple strategies for dealing with this in <a data-type="xref" href="ch17.html#Attention">Chapter 17</a>.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Why Bother with Extra Features?" data-type="sect2"><div class="sect2" id="id63">&#13;
<h2>Why Bother with Extra Features?</h2>&#13;
&#13;
<p>Sometimes it is useful to step back and ask if a new technology is actually worth caring about. So far in this section, we’ve introduced four new paradigms for thinking about a recommender problem. That level of detail may seem surprising and potentially even unnecessary.</p>&#13;
&#13;
<p>One of the core reasons that things like context- and query-based recommendations become relevant is to deal with some of the issues mentioned before around <a data-primary="data collection and user logging" data-secondary="data sparsity" data-type="indexterm" id="id589"/><a data-primary="sparsity" data-type="indexterm" id="id590"/><a data-primary="recommendation systems" data-secondary="data sparsity" data-type="indexterm" id="id591"/>sparsity and <a data-primary="cold starting" data-type="indexterm" id="coldstart07"/>cold starting. Sparsity makes things that aren’t cold seem cold via the learner’s underexposure to them, but true cold starting also exists because of new items being added to catalogs with high frequency in most applications. We will address cold starting in detail, but for now, suffice it to say that one strategy for warm starting is to use other features that <em>are</em> available even in this regime.</p>&#13;
&#13;
<p>In applications of ML that are explicitly feature based, we rarely battle the cold-start problem to such a degree, because at inference time we’re confident that the model parameters useful for prediction are well aligned with those features that are available. In this way, feature-included recommendation systems are bootstrapping from a potentially weaker learner that has more guaranteed performance via always-available features.</p>&#13;
&#13;
<p>The<a data-primary="boosting" data-type="indexterm" id="id592"/> second analogy that the previous architectures are reflecting is that of boosting. Boosted models<a data-primary="embedding models" data-secondary="boosted models" data-type="indexterm" id="id593"/> operate via the observation that ensembles of weaker learners can reach better performance. Here we are asking for some additional features to help these networks ensemble with weak learners, to boost their performance.<a data-primary="" data-startref="Arecstruct07" data-type="indexterm" id="id594"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Encoder Architectures and Cold Starting" data-type="sect1"><div class="sect1" id="id64">&#13;
<h1>Encoder Architectures and Cold Starting</h1>&#13;
&#13;
<p>The<a data-primary="architectures" data-secondary="encoder architectures and cold starting" data-type="indexterm" id="Aencod07"/> previous problem framings of various types of recommendation problems point out four model architectures, each fitting into our general framework of collector, ranker, and server. With this understanding, let’s discuss in a bit more detail how model architecture can become intertwined with serving architecture. In particular, we also need to discuss feature encoders.</p>&#13;
&#13;
<p>The key opportunity from encoder-augmented systems is that for users, items, or contexts without much data, we can still form embeddings on the fly. Recall from before that our embeddings make the rest of our system possible, but cold-starting recommendations is a huge challenge.</p>&#13;
&#13;
<p>The<a data-primary="two-towers architecture" data-type="indexterm" id="id595"/><a data-primary="dual-encoder networks" data-type="indexterm" id="id596"/> <em>two-towers architecture</em>—or dual-encoder networks—introduced in <a href="https://oreil.ly/gQHfo">“Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations”</a> by Xinyang Yi et al. is shown in <a data-type="xref" href="#fig:two-towers-arch">Figure 7-2</a> explicit model architecture is aimed at prioritizing features of both the user and items when building a scoring model for a recommendation system. We’ll see a lot more discussion of<a data-primary="matrix factorization (MF)" data-secondary="explanation of" data-type="indexterm" id="id597"/> matrix factorization (MF), which is a kind of latent collaborative filtering (CF) derived from the user-item matrix and some linear algebraic algorithms. In the preceding section, we explained why additional features matter. Adding these<a data-primary="side-car features" data-type="indexterm" id="id598"/> <em>side-car</em> features into an MF paradigm is possible and has shown to be successful—for example,<a data-primary="implicit feedback" data-type="indexterm" id="id599"/><a data-primary="factorization" data-type="indexterm" id="id600"/> applications <a href="https://oreil.ly/cbePb">CF for implicit feedback</a>, <a href="http://libfm.org/">factorization machines</a>, and <a href="https://oreil.ly/cN7fP">SVDFeature</a>. However, in this model we will take a more direct approach.</p>&#13;
&#13;
<figure><div class="figure" id="fig:two-towers-arch">&#13;
<img alt="Two Towers Architecture" src="assets/brpj_0702.png"/>&#13;
<h6><span class="label">Figure 7-2. </span>The two towers responsible for the two embeddings</h6>&#13;
</div></figure>&#13;
&#13;
<p>In this architecture, we take the left tower to be responsible for items and the right tower to be responsible for the user and, when appropriate, context. These two tower architectures are inspired by the NLP literature and, in particular, <a href="https://oreil.ly/m7IrK">“Learning Text Similarity with Siamese Recurrent Networks”</a>  by Paul Neculoiu et al.</p>&#13;
&#13;
<p>Let’s detail how this model architecture is applied to recommending videos on YouTube. For a full overview of where this architecture was first introduced, see <a href="https://oreil.ly/aXekc">“Deep Neural Networks for YouTube Recommendations”</a> by Paul Covington et al. Training labels will be given by clicks, but with an additional regression feature <math alttext="r Subscript i Baseline element-of StartSet 0 comma 1 EndSet">&#13;
  <mrow>&#13;
    <msub><mi>r</mi> <mi>i</mi> </msub>&#13;
    <mo>∈</mo>&#13;
    <mfenced close="}" open="{" separators="">&#13;
      <mn>0</mn>&#13;
      <mo>,</mo>&#13;
      <mn>1</mn>&#13;
    </mfenced>&#13;
  </mrow>&#13;
</math>, where the minimum value corresponds to a click but trivial watch time, and the maximum of the range corresponds to a full watch.</p>&#13;
&#13;
<p>As we’ve mentioned, this model architecture will explicitly include features from both user and items. The video features will consist of categorical and continuous features, like <code>VideoId</code>, <code>ChannelId</code>, <code>VideoTopic</code>, and so on. An embedding layer is used for many of the categorical features to move to dense representations. The user features include watch histories via bag of words and standard user features.</p>&#13;
&#13;
<p>This model structure combines many of the ideas you’ve seen before but has relevant takeaways for our system architecture. First is the idea of<a data-primary="sequential training" data-type="indexterm" id="id601"/><a data-primary="training" data-secondary="sequential training" data-type="indexterm" id="id602"/><a data-primary="temporal batches" data-type="indexterm" id="id603"/> sequential training. Each <em>temporal batch</em> of samples should be trained in sequence to ensure that model drift is shown to the model; we will discuss prequential datasets in <a data-type="xref" href="ch10.html#prequential">“Prequential validation”</a>. Next, we present an important idea for the productionizing of these kinds of models: encoders.</p>&#13;
&#13;
<p class="less_space pagebreak-before">In these models, we have feature encoders as the early layers in both towers, and when we move to inference, we will still need these encoders. When performing the online recommendations, we will be given <code>UserId</code> and <code>VideoId</code> and will first need to collect their features. As discussed in <a data-type="xref" href="ch06.html#feature-stores">“Feature Stores”</a>, the feature store will be useful in getting these raw features, but we need to also encode the features into the dense representations necessary for inference. This is something that can be stored in the feature store for known entities, but for unknown entities we will need to do the feature embedding at inference time.</p>&#13;
&#13;
<p>Encoding layers serve as a simple model for mapping a collection of features to a dense representation. When fitting encoding layers as the first step in a neural network, the common strategy is to take the first <math alttext="k">&#13;
  <mi>k</mi>&#13;
</math> layers and reuse them as an encoder model. More specifically, if <math alttext="script upper L Superscript i Baseline comma 0 less-than-or-equal-to i less-than-or-equal-to k">&#13;
  <mrow>&#13;
    <msup><mi>ℒ</mi> <mi>i</mi> </msup>&#13;
    <mo>,</mo>&#13;
    <mn>0</mn>&#13;
    <mo>≤</mo>&#13;
    <mi>i</mi>&#13;
    <mo>≤</mo>&#13;
    <mi>k</mi>&#13;
  </mrow>&#13;
</math> are the layers responsible for feature encoding, call <math alttext="upper E m b left-parenthesis ModifyingAbove upper V With caret right-parenthesis equals script upper L Superscript k Baseline left-parenthesis script upper L Superscript k minus 1 Baseline left-parenthesis ellipsis script upper L Superscript 0 Baseline left-parenthesis ModifyingAbove upper V With caret right-parenthesis right-parenthesis right-parenthesis">&#13;
  <mrow>&#13;
    <mi>E</mi>&#13;
    <mi>m</mi>&#13;
    <mi>b</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mover accent="true"><mi>V</mi> <mo>^</mo></mover>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <msup><mi>ℒ</mi> <mi>k</mi> </msup>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msup><mi>ℒ</mi> <mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow> </msup>&#13;
      <mrow>&#13;
        <mo>(</mo>&#13;
        <mo>...</mo>&#13;
        <msup><mi>ℒ</mi> <mn>0</mn> </msup>&#13;
        <mrow>&#13;
          <mo>(</mo>&#13;
          <mover accent="true"><mi>V</mi> <mo>^</mo></mover>&#13;
          <mo>)</mo>&#13;
        </mrow>&#13;
        <mo>)</mo>&#13;
      </mrow>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math> the function that maps a feature vector <math alttext="ModifyingAbove upper V With caret">&#13;
  <mover accent="true"><mi>V</mi> <mo>^</mo></mover>&#13;
</math> to its dense representation.</p>&#13;
&#13;
<p>In our previous system architecture, we would include this encoder as part of the fast layer, after receiving features from the feature store. It’s also important to note that we would still want to utilize vector search; these feature embedding layers are used upstream of the vector search and nearest neighbor searches.</p>&#13;
<div data-type="warning" epub:type="warning"><h1>Encoder as a Service</h1>&#13;
<p>Encoders<a data-primary="encoders" data-secondary="definition of term" data-type="indexterm" id="id604"/> and retrieval are a key part of the multistage recommendation pipeline. We’ve spoken briefly about the latent spaces in question (for more details, see <a data-type="xref" href="ch10.html#latent-spaces">“Latent Spaces”</a>), and we’ve alluded to an <em>encoder</em>. Briefly, an encoder is the model that converts users, items, queries, etc., into the latent space in which you’ll perform nearest-neighbors search. These models can be trained via a variety of processes, many of which will be discussed later, but it’s important to discuss where they live once trained.</p>&#13;
&#13;
<p>Encoders<a data-primary="encoders" data-secondary="encoder as a service" data-type="indexterm" id="id605"/> are often simple API endpoints that take the content to be embedded and return a vector (a list of floats). Encoders often work at the batch layer to encode all the documents/items that will be retrieved, but they must <em>also</em> be connected to the real-time layer to encode the queries as they come in. A common pattern is to set up a batch endpoint and a single query endpoint to facilitate optimization for both modalities. These endpoints should be fast and highly available.</p>&#13;
&#13;
<p>If you’re working with text data, a good starting place is to use BERT or GPT-based embeddings. The easiest at this time are provided as a hosted service from OpenAI.<a data-primary="" data-startref="Aencod07" data-type="indexterm" id="id606"/><a data-primary="" data-startref="coldstart07" data-type="indexterm" id="id607"/></p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Deployment" data-type="sect1"><div class="sect1" id="id205">&#13;
<h1>Deployment</h1>&#13;
&#13;
<p>Like<a data-primary="architectures" data-secondary="model deployment" data-type="indexterm" id="Adep07"/><a data-primary="models" data-secondary="deploying" data-type="indexterm" id="Mdeploy07"/> many ML applications, the final output of a recommendation system is itself a small program that runs continuously and exposes an API to interact with it; batch recommendations are often a powerful place to start, performing all the necessary recommendations ahead of time. Throughout this chapter, we’ve seen the pieces embedded in our backend system, but now we will discuss the components closer to the user.</p>&#13;
&#13;
<p>In our relatively general architecture, the server is responsible for handing over the recommendations, after all the work that comes before, and should adhere to a preset schema. But what does this deployment look like?</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Models as APIs" data-type="sect2"><div class="sect2" id="id65">&#13;
<h2>Models as APIs</h2>&#13;
&#13;
<p>Let’s<a data-primary="deployment" data-secondary="models as APIs" data-type="indexterm" id="id608"/> discuss two systems architectures that might be appropriate for serving your models in production: microservice and monolith.</p>&#13;
&#13;
<p>In web applications, this dichotomy is well covered from many perspectives and special use cases. As ML engineers, data scientists, and potentially data platform engineers, it’s not necessary to dig deep into this area, but it’s essential to know the basics:</p>&#13;
<dl>&#13;
<dt>Microservice architectures</dt>&#13;
<dd>&#13;
<p>Each<a data-primary="microservice architectures" data-type="indexterm" id="id609"/> component of the pipeline should be its own small program with a clear API and output schema. Composing these API calls allows for flexible and predictable pipelines.</p>&#13;
</dd>&#13;
<dt>Monolithic architectures</dt>&#13;
<dd>&#13;
<p>One<a data-primary="monolithic architectures" data-type="indexterm" id="id610"/> application should contain all the necessary logic and components for model predictions. Keeping the application self-contained means fewer interfaces that need to be kept aligned and fewer rabbit holes to hunt around in when a location in your pipeline is being starved.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Whatever you choose as your strategy, you’ll need to make a few decisions:</p>&#13;
<dl>&#13;
<dt><em>How large is the necessary application?</em></dt>&#13;
<dd>&#13;
<p>If your application will need fast access to large datasets at inference time, you’ll need to think carefully about memory requirements.</p>&#13;
</dd>&#13;
<dt><em>What access does your application need?</em></dt>&#13;
<dd>&#13;
<p>We’ve previously discussed using technologies like bloom filters and feature stores. These resources may be tightly coupled to your application (by building them in memory in the application) or may be an API call away. Make sure your deployment accounts for these relationships.</p>&#13;
</dd>&#13;
<dt><em>Should your model be deployed to a single node or a cluster?</em></dt>&#13;
<dd>&#13;
<p>For some model types, even at the inference step we wish to utilize<a data-primary="distributed computing" data-type="indexterm" id="id611"/> distributed computing. This will require additional configuration to allow for fast<a data-primary="parallelization" data-type="indexterm" id="id612"/> <span class="keep-together">parallelization</span>.</p>&#13;
</dd>&#13;
<dt><em>How much replication do you need?</em></dt>&#13;
<dd>&#13;
<p>Horizontal scaling<a data-primary="horizontal scaling" data-type="indexterm" id="id613"/><a data-primary="scaling, horizontal" data-type="indexterm" id="id614"/> allows you to have multiple copies of the same service running simultaneously to reduce the demand on any particular instance. This is important for ensuring availability and performance. As we horizontally scale, each service can operate independently, and various strategies exist for coordinating these services and an API request. Each replica is usually its own containerized application, and these APIs like CoreOS and Kubernetes are used to manage these. The requests themselves must also be balanced to the different replicas via something like nginx.</p>&#13;
</dd>&#13;
<dt><em>What are the relevant APIs that are exposed?</em></dt>&#13;
<dd>&#13;
<p>Each application in the stack should have a clear set of exposed schemas and an explicit communication about the types of other applications that may call to the APIs.</p>&#13;
</dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Spinning Up a Model Service" data-type="sect2"><div class="sect2" id="id66">&#13;
<h2>Spinning Up a Model Service</h2>&#13;
&#13;
<p>So<a data-primary="deployment" data-secondary="spinning up a model service" data-type="indexterm" id="id615"/> what can you use to get your model into an application? A variety of frameworks for application development are useful; some of the most popular in Python are Flask, FastAPI, and Django. Each has different advantages, but we’ll discuss FastAPI here.</p>&#13;
&#13;
<p>FastAPI is a targeted framework for API applications, making it especially well fit for serving ML models. It calls itself an<a data-primary="asynchronous server gateway interface (ASGI)" data-type="indexterm" id="id616"/> asynchronous server gateway interface (ASGI) framework, and its specificity grants a ton of simplicity.</p>&#13;
&#13;
<p>Let’s take a simple example of turning a fit torch model into a service with the FastAPI framework. First, let’s utilize an artifact store to pull down our fit model. Here we are using the Weights &amp; Biases artifact store:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">wandb</code><code class="o">,</code> <code class="nn">torch</code>&#13;
<code class="n">run</code> <code class="o">=</code> <code class="n">wandb</code><code class="o">.</code><code class="n">init</code><code class="p">(</code><code class="n">project</code><code class="o">=</code><code class="n">Prod_model</code><code class="p">,</code> <code class="n">job_type</code><code class="o">=</code><code class="s2">"inference"</code><code class="p">)</code>&#13;
&#13;
<code class="n">model_dir</code> <code class="o">=</code> <code class="n">run</code><code class="o">.</code><code class="n">use_artifact</code><code class="p">(</code>&#13;
		<code class="s1">'bryan-wandb/recsys-torch/model:latest'</code><code class="p">,</code>&#13;
		<code class="nb">type</code><code class="o">=</code><code class="s1">'model'</code>&#13;
<code class="p">)</code><code class="o">.</code><code class="n">download</code><code class="p">()</code>&#13;
&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="n">model_dir</code><code class="p">)</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">eval</code><code class="p">(</code><code class="n">user_id</code><code class="p">)</code></pre>&#13;
&#13;
<p>This looks just like your notebook workflow, so let’s see how easy it is to integrate this with FastAPI:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="n">FastAPI</code> <code class="c1"># FastAPI code</code>&#13;
&#13;
<code class="kn">import</code> <code class="nn">wandb</code><code class="o">,</code> <code class="nn">torch</code>&#13;
&#13;
<code class="n">app</code> <code class="o">=</code> <code class="n">FastAPI</code><code class="p">()</code> <code class="c1"># FastAPI code</code>&#13;
&#13;
<code class="n">run</code> <code class="o">=</code> <code class="n">wandb</code><code class="o">.</code><code class="n">init</code><code class="p">(</code><code class="n">project</code><code class="o">=</code><code class="n">Prod_model</code><code class="p">,</code> <code class="n">job_type</code><code class="o">=</code><code class="s2">"inference"</code><code class="p">)</code>&#13;
&#13;
<code class="n">model_dir</code> <code class="o">=</code> <code class="n">run</code><code class="o">.</code><code class="n">use_artifact</code><code class="p">(</code>&#13;
	<code class="s1">'bryan-wandb/recsys-torch/model:latest'</code><code class="p">,</code>&#13;
	<code class="nb">type</code><code class="o">=</code><code class="s1">'model'</code>&#13;
<code class="p">)</code><code class="o">.</code><code class="n">download</code><code class="p">()</code>&#13;
&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="n">model_dir</code><code class="p">)</code>&#13;
&#13;
<code class="nd">@app</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"/recommendations/</code><code class="si">{user_id}</code><code class="s2">"</code><code class="p">)</code> <code class="c1"># FastAPI code</code>&#13;
<code class="k">def</code> <code class="nf">make_recs_for_user</code><code class="p">(</code><code class="n">user_id</code><code class="p">:</code> <code class="nb">int</code><code class="p">):</code> <code class="c1"># FastAPI code</code>&#13;
		<code class="n">endpoint_name</code> <code class="o">=</code> <code class="s1">'make_recs_for_user_v0'</code>&#13;
		<code class="n">logger</code><code class="o">.</code><code class="n">info</code><code class="p">(</code>&#13;
			<code class="s2">"{'type': 'recommendation_request',"</code>&#13;
			<code class="sa">f</code><code class="s2">"'arguments': </code><code class="si">{</code><code class="s1">'user_id'</code><code class="si">:</code><code class="s2"> </code><code class="si">{</code><code class="n">user_id</code><code class="si">}}</code><code class="s2">,"</code>&#13;
			<code class="sa">f</code><code class="s2">"'response': </code><code class="si">{</code><code class="kc">None</code><code class="si">}}</code><code class="s2">,"</code><code class="p">,</code>&#13;
			<code class="sa">f</code><code class="s2">"'endpoint_name': </code><code class="si">{</code><code class="n">endpoint_name</code><code class="si">}</code><code class="s2">"</code>&#13;
		<code class="p">)</code>&#13;
		<code class="n">recommendation</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">eval</code><code class="p">(</code><code class="n">user_id</code><code class="p">)</code>&#13;
		<code class="n">logger</code><code class="o">.</code><code class="n">log</code><code class="p">(</code>&#13;
			<code class="s2">"{'type': 'model_inference',"</code>&#13;
			<code class="sa">f</code><code class="s2">"'arguments': </code><code class="si">{</code><code class="s1">'user_id'</code><code class="si">:</code><code class="s2"> </code><code class="si">{</code><code class="n">user_id</code><code class="si">}}</code><code class="s2">,"</code>&#13;
			<code class="sa">f</code><code class="s2">"'response': </code><code class="si">{</code><code class="n">recommendation</code><code class="si">}}</code><code class="s2">,"</code>&#13;
			<code class="sa">f</code><code class="s2">"'endpoint_name': </code><code class="si">{</code><code class="n">endpoint_name</code><code class="si">}</code><code class="s2">"</code>&#13;
		<code class="p">)</code>&#13;
    <code class="k">return</code> <code class="p">{</code> <code class="c1"># FastAPI code</code>&#13;
			<code class="s2">"user_id"</code><code class="p">:</code> <code class="n">user_id</code><code class="p">,</code>&#13;
			<code class="s2">"endpoint_name"</code><code class="p">:</code> <code class="n">endpoint_name</code><code class="p">,</code>&#13;
			<code class="s2">"recommendation"</code><code class="p">:</code> <code class="n">recommendation</code>&#13;
		<code class="p">}</code></pre>&#13;
&#13;
<p>I hope you share my enthusiasm that we now have a model as a service in five additional lines of code. While this scenario includes simple examples of logging, we’ll discuss logging in greater detail later in this chapter to help you improve observability in your applications.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Workflow Orchestration" data-type="sect2"><div class="sect2" id="id206">&#13;
<h2>Workflow Orchestration</h2>&#13;
&#13;
<p>The<a data-primary="deployment" data-secondary="workflow orchestration" data-type="indexterm" id="id617"/> other component necessary for your deployed system is workflow orchestration. The model service is responsible for receiving requests and serving results, but many system components need to be in place for this service to do anything of use. These workflows have several components, so we will discuss them in sequence: containerization, scheduling, and CI/CD.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Containerization" data-type="sect3"><div class="sect3" id="id67">&#13;
<h3>Containerization</h3>&#13;
&#13;
<p>We’ve<a data-primary="workflow orchestration" data-secondary="containerization" data-type="indexterm" id="id618"/><a data-primary="containerization" data-type="indexterm" id="id619"/> discussed how to put together a simple service that can return the results, and we suggested using FastAPI; however, the question of environments is now relevant. When executing Python code, it is important to keep the environment consistent if not identical. FastAPI<a data-primary="FastAPI" data-type="indexterm" id="id620"/> is a library for designing the interfaces; Docker<a data-primary="Docker" data-type="indexterm" id="id621"/> is the software that manages the environment that code runs in. It’s common to hear Docker described as a container or containerization tool: this is because you load a bunch of apps—or executable components of code—into one shared environment.</p>&#13;
&#13;
<p>We have a few subtle things to note at this point. The meaning of<a data-primary="environments" data-secondary="containerization and" data-type="indexterm" id="id622"/> <em>environment</em> encapsulates both the Python environment of package dependencies and the larger environment, including the operating system or GPU drivers. The environment is usually initialized from a predetermined<a data-primary="image" data-type="indexterm" id="id623"/> <em>image</em> that installs the most basic aspects of what you’ll need access to and in many cases is less variable across services to promote consistency and standardization. Finally, the container is usually equipped with a list of infrastructure code necessary to work wherever it is to be deployed.</p>&#13;
&#13;
<p>In practice, you specify details of the Python environment via your <em>requirements</em> file, which consists of a list of Python packages. Note that some library dependencies are outside Python and will require additional configuration mechanisms. The operating system and drivers are usually built as part of a base image; you can find these on DockerHub or similar. Finally, <em>infrastructure as code</em> is<a data-primary="infrastructure as code" data-type="indexterm" id="id624"/> a paradigm wherein you write code to orchestrate the necessary steps in getting your container configured to run in the infrastructure it will be deployed into. Dockerfile and Docker Compose are specific to the Docker container interfacing with infrastructure, but you can further generalize these concepts to include other details of the infrastructure. This infrastructure as code begins to encapsulate provisioning of resources in your cloud, setting up open ports for network communication, access control via security roles, and more. A common way to write this code is in Terraform. This book doesn’t dive into infrastructure specification, but infrastructure as code is becoming a more important tool to the ML practitioner. Many companies are beginning to attempt to simplify these aspects of training and deploying systems including Weights &amp; Biases or Modal.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Scheduling" data-type="sect3"><div class="sect3" id="id68">&#13;
<h3>Scheduling</h3>&#13;
&#13;
<p>Two<a data-primary="workflow orchestration" data-secondary="scheduling" data-type="indexterm" id="id625"/><a data-primary="scheduling jobs" data-type="indexterm" id="id626"/><a data-primary="jobs, scheduling" data-type="indexterm" id="id627"/> paradigms exist for scheduling jobs: cron and triggers. Later we’ll talk more about the continuous training loop and active learning processes, but upstream of those is your ML workflow. ML workflows are a set of ordered steps necessary to prepare your model for inference. We’ve introduced our notion of collector, ranker, and server, which are organized into a sequence of stages for recommendation systems—but these are the three coarsest elements of the system topology.</p>&#13;
&#13;
<p>In ML systems, we frequently assume that there’s an upstream stage of the workflow that corresponds to data transformations, as discussed in <a data-type="xref" href="ch06.html#data-processing">Chapter 6</a>. Wherever that stage takes place, the output of those transformations results in our vector store—and potentially the additional feature stores. The handoff between those steps and the next steps in your workflow are the result of a job scheduler. As mentioned previously, tools like Dagster<a data-primary="Dagster" data-type="indexterm" id="id628"/> and Airflow<a data-primary="Airflow" data-type="indexterm" id="id629"/> can run sequences of jobs with dependent assets. These kinds of tools are needed to orchestrate the transitions and to ensure that they’re timely.</p>&#13;
&#13;
<p><em>Cron</em> refers<a data-primary="Cron" data-type="indexterm" id="id630"/> to a time schedule where a workflow should begin—for example, hourly at the top of the hour or four times a day. <em>Triggers</em> refers<a data-primary="trigger scheduling" data-type="indexterm" id="id631"/> to the instigation of a job run when another event has taken place—for example, if an endpoint receives a request, or a set of data gets a new version, or a limit of responses is exceeded. These are meant to capture more ad hoc relationships between the next job stage and the trigger. Both paradigms are very important.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="CI/CD" data-type="sect3"><div class="sect3" id="id69">&#13;
<h3>CI/CD</h3>&#13;
&#13;
<p>Your<a data-primary="workflow orchestration" data-secondary="CI/CD" data-type="indexterm" id="id632"/> workflow execution system is the backbone of your ML systems, often the bridge between the data collection process, the training process, and the deployment process. Modern workflow execution systems also include automatic validation and tracking so that you can audit the steps on the way to production.</p>&#13;
&#13;
<p><em>Continuous integration</em> (CI)<a data-primary="continuous integration (CI)" data-type="indexterm" id="id633"/><a data-primary="CI (continuous integration)" data-type="indexterm" id="id634"/> is a term taken from software engineering to enforce a set of checks on new code in order to accelerate the development process. In traditional software engineering, this comprises automating unit and integration testing, usually run after checking the code into version control. For ML systems, CI may mean running test scripts against the model, checking the typed output of data transformations, or running validation sets through the model and benchmarking the performance against previous models.</p>&#13;
&#13;
<p><em>Continuous deployment</em> (CD) is<a data-primary="continuous deployment (CD)" data-type="indexterm" id="id635"/><a data-primary="CD (continuous deployment)" data-type="indexterm" id="id636"/> also a term popularized in software engineering to refer to automating the process of pushing new packaged code into an existing system. In software engineering, deploying code when it has passed the relevant checks speeds development and reduces the risk of stale systems. In ML, CD can involve strategies like automatically deploying your new model behind a service endpoint in shadow (which we’ll discuss in <a data-type="xref" href="#shadowing-sect">“Shadowing”</a>) to test that it works as expected under live traffic. It could also mean deploying a model behind a very small allocation of an A/B test or multiarm bandit treatment to begin to measure effects on target outcomes. CD usually requires effective triggering by the requirements it has to satisfy before being pushed. It’s common to hear CD utilizing a model registry, where you house and index variations on your model.<a data-primary="" data-startref="Mdeploy07" data-type="indexterm" id="id637"/><a data-primary="" data-startref="Adep07" data-type="indexterm" id="id638"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Alerting and Monitoring" data-type="sect1"><div class="sect1" id="id207">&#13;
<h1>Alerting and Monitoring</h1>&#13;
&#13;
<p>Alerting and monitoring<a data-primary="architectures" data-secondary="alerting and monitoring" data-type="indexterm" id="Aalert07"/> take a lot of their inspiration from the DevOps world for software engineering. Here are some high-level principles that will guide our thinking:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Clearly defined schemas and priors</p>&#13;
</li>&#13;
<li>&#13;
<p>Observability</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Schemas and Priors" data-type="sect2"><div class="sect2" id="id70">&#13;
<h2>Schemas and Priors</h2>&#13;
&#13;
<p>When<a data-primary="alerting and monitoring" data-secondary="schemas and priors" data-type="indexterm" id="id639"/><a data-primary="schemas" data-type="indexterm" id="id640"/><a data-primary="priors" data-type="indexterm" id="id641"/> designing software systems, you almost always have expectations about how the components fit together. Just as you anticipate the input and output to functions when writing code, in software systems you anticipate these at each interface. This is relevant not only for microservice architectures; even in a monolith architecture, components of the system need to work together and often have boundaries between their defining responsibilities.</p>&#13;
&#13;
<p>Let’s make this more concrete via an example. You’ve built a user-item latent space, a feature store for user features, a bloom filter for client avoids  (things the client specifically tells you they don’t want), and an experiment index that defines which of two models should be used for scoring. First let’s examine the latent space; when provided a <code>user_id</code>, we need to look up its representation, and we already have some assumptions:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The <code>user_id</code> provided will be of the correct type.</p>&#13;
</li>&#13;
<li>&#13;
<p>The <code>user_id</code> will have a representation in our space.</p>&#13;
</li>&#13;
<li>&#13;
<p>The representation returned will be of the correct type and shape.</p>&#13;
</li>&#13;
<li>&#13;
<p>The component values of the representation vector will be in the appropriate domain. (<em>The support of representations in your latent space may vary day to day</em>.)</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>From here, we need look up<a data-primary="approximate nearest neighbors (ANN)" data-type="indexterm" id="id642"/> the <math alttext="k">&#13;
  <mi>k</mi>&#13;
</math> ANN, which incurs more assumptions:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>There are <math alttext="greater-than-or-equal-to k">&#13;
  <mrow>&#13;
    <mo>≥</mo>&#13;
    <mi>k</mi>&#13;
  </mrow>&#13;
</math> vectors in our latent space.</p>&#13;
</li>&#13;
<li>&#13;
<p>Those vectors adhere to the expected distributional behavior of the latent space.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>While these seem like relatively straightforward applications of<a data-primary="testing" data-secondary="unit tests" data-type="indexterm" id="id643"/><a data-primary="unit tests" data-type="indexterm" id="id644"/> unit tests, canonizing these assumptions is important. Take the last assumption in both of the two services: how can you know the appropriate domain for the representation vectors? As part of your training procedure, you’ll need to calculate this and then store it for access during the inference pipeline.</p>&#13;
&#13;
<p>In the second case, when finding nearest neighbors in high-dimensional spaces, well-discussed difficulties arise in distributional uniformity, but this can mean particularly poor performance for recommendations. In practice, we have observed a spiky nature to the behavior of <math alttext="k">&#13;
  <mi>k</mi>&#13;
</math>-nearest neighbors in latent spaces, leading to difficult challenges downstream in ensuring diversity of recommendations. These distributions can be estimated as priors, and simple checks like KL divergence can be used online; we can estimate the average behavior of the embeddings and the difference between local geometries.</p>&#13;
&#13;
<p>In both cases, collecting and logging the output of this information can provide a rich history of what is going on with your system. This can shorten debugging loops later if model performance is low in production.</p>&#13;
&#13;
<p>Returning to the possibility of <code>user_id</code> lacking a representation in our space: this is precisely the cold-start problem! In that case, we need to transition over to a different prediction pipeline: perhaps user-feature-based, explore-exploit, or even hardcoded recommendations. In this setting, we need to understand next steps when a schema condition is not met and then gracefully move forward.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Integration Tests" data-type="sect2"><div class="sect2" id="id71">&#13;
<h2>Integration Tests</h2>&#13;
&#13;
<p>Let’s consider<a data-primary="alerting and monitoring" data-secondary="integration tests" data-type="indexterm" id="id645"/><a data-primary="integration tests" data-type="indexterm" id="id646"/><a data-primary="testing" data-secondary="integration tests" data-type="indexterm" id="id647"/> one higher-level challenge that might emerge in a system like this at the level of integration. Some refer to these issues as <em>entanglement.</em></p>&#13;
&#13;
<p>You’ve learned through experimentation that you should find <math alttext="k equals 20">&#13;
  <mrow>&#13;
    <mi>k</mi>&#13;
    <mo>=</mo>&#13;
    <mn>20</mn>&#13;
  </mrow>&#13;
</math> ANNs in the item space for a user to get good recommendations. You make a call to your representation space, get your 20 items, and pass them onto the filtering step. However, this user is quite picky; they have previously made many restrictions on their account about the kind of recommendations they allow: no shoes, no dresses, no jeans, no hats, no handbags—what’s a struggling recommendation system to do?</p>&#13;
&#13;
<p>Naively, if you take the 20 neighbors and pass them into the bloom, you’re likely to be left with nothing! You can approach this challenge in two ways:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Allow for a callback from the filter step to the retrieval (see <a data-type="xref" href="ch15.html#PredicatePushdown">“Predicate Pushdown”</a>)</p>&#13;
</li>&#13;
<li>&#13;
<p>Build a user distribution and store that for access during retrieval</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>In the first approach, you give access to your filter step to call the retrieval step with a larger <math alttext="k">&#13;
  <mi>k</mi>&#13;
</math> until the requirements are satisfied after the bloom. Of course, this incurs significant slowdown as it requires multiple passes and ever-growing queries with redundancy! While this approach is simple, it requires building defensively and knowing ahead of time what may go wrong.</p>&#13;
&#13;
<p>In the second approach, during training, you can sample from the user space to build estimates of the appropriate <math alttext="k">&#13;
  <mi>k</mi>&#13;
</math> for varying numbers of avoids by user. Then, giving access to a lookup of total avoids by user to the collector can help defend against this behavior.</p>&#13;
<div data-type="tip"><h1>Over-Retrieval</h1>&#13;
<p>Sometimes<a data-primary="over-retrieval" data-type="indexterm" id="id648"/><a data-primary="retrieval" data-secondary="over-retrieval" data-type="indexterm" id="id649"/> people in information retrieval perform <em>over-retrieval</em> to mitigate issues of conflicting requirements from the search request, which can arise if the user makes a search and applies many filters simultaneously. This is applicable in recommendation systems as well.</p>&#13;
&#13;
<p>If you retrieve only exactly the number of potential recommendations you need to serve to the user, downstream rules or poor personalization scores can sometimes cause a serious issue for serving up recommendations. This is why it is common to retrieve more items than you anticipate showing to the user.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Observability" data-type="sect2"><div class="sect2" id="id72">&#13;
<h2>Observability</h2>&#13;
&#13;
<p>Many<a data-primary="alerting and monitoring" data-secondary="observability" data-type="indexterm" id="id650"/><a data-primary="observability" data-type="indexterm" id="id651"/> tools in software engineering can assist with observability—understanding the <em>whys</em> of what’s going on in the software stack. Because the systems we are building become quite distributed, the interfaces become critical monitoring points, but the paths also become complex.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Spans and traces" data-type="sect3"><div class="sect3" id="id73">&#13;
<h3>Spans and traces</h3>&#13;
&#13;
<p>Common terms<a data-primary="spans" data-type="indexterm" id="id652"/><a data-primary="traces" data-type="indexterm" id="id653"/> in this area are <em>spans</em> and <em>traces,</em> which refer to two dimensions of a call stack, illustrated in <a data-type="xref" href="#fig:trace-spans">Figure 7-3</a>. Given a collection of connected services, as in our preceding examples, an individual inference request will pass through some or all of those services in a sequence. The sequence of service requests is the <em>trace</em>. The potentially parallel time delays of each of these services is the <em>span</em>.</p>&#13;
&#13;
<figure><div class="figure" id="fig:trace-spans">&#13;
<img alt="A Trace-Span diagram for a request" src="assets/brpj_0703.png"/>&#13;
<h6><span class="label">Figure 7-3. </span>The spans of a trace</h6>&#13;
</div></figure>&#13;
&#13;
<p>The graphical representation of spans usually demonstrates how the time for one service to respond comprises several other delays from other calls.</p>&#13;
&#13;
<p><em>Observability</em> enables you to see traces, spans, and logs in conjunction to appropriately diagnose the behavior of your system. In our example of utilizing a callback from the filter step to get more neighbors from the collector, we might see a slow response and wonder, “What has happened?” By viewing the spans and traces, we’d be able to see that the first call to the collector was as expected, then the filter step made a call to the collector, then another call to the collector, and so on, which built up a huge span for the filter step. Combining that view with logging would help us rapidly diagnose what might be happening.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Timeouts" data-type="sect3"><div class="sect3" id="id74">&#13;
<h3>Timeouts</h3>&#13;
&#13;
<p>In<a data-primary="timeouts" data-type="indexterm" id="id654"/> the preceding example, we had a long process that could lead to a very bad user experience. In most cases, we impose hard restrictions on how bad we let things get; these are called <em>timeouts</em>.</p>&#13;
&#13;
<p>Usually, we have an upper bound on how long we’re willing to wait for our inference response, so implementing timeouts aligns our system with these restrictions. It’s important in these cases to have a<a data-primary="fallback" data-type="indexterm" id="id655"/> <em>fallback.</em> In the setting of recommendation systems, a fallback usually comprises things like the MPIR prepared such that it incurs minimal additional delay.<a data-primary="" data-startref="Aalert07" data-type="indexterm" id="id656"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Evaluation in Production" data-type="sect1"><div class="sect1" id="id208">&#13;
<h1>Evaluation in Production</h1>&#13;
&#13;
<p>If<a data-primary="architectures" data-secondary="evaluation in production" data-type="indexterm" id="Aeval07"/><a data-primary="models" data-secondary="evaluating in production" data-type="indexterm" id="Meval07"/> the previous section was about understanding what’s coming into your model in production, this one might be summarized as what’s coming out of your model in production. At a high level, evaluation in production can be thought of as extending all your model-validation techniques to the inference time. In particular, you are looking at <em>what the model actually is doing</em>!</p>&#13;
&#13;
<p>On one hand, we already have tools to do this evaluation. You can use the same methods to evaluate performance as you do for training, but now on real observations streaming in. However, this process is not as obvious as we might first guess. Let’s discuss some of the challenges.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Slow Feedback" data-type="sect2"><div class="sect2" id="id75">&#13;
<h2>Slow Feedback</h2>&#13;
&#13;
<p>Recommendation systems<a data-primary="slow feedback" data-type="indexterm" id="id657"/><a data-primary="feedback" data-type="indexterm" id="id658"/><a data-primary="evaluation in production" data-secondary="slow feedback" data-type="indexterm" id="id659"/> fundamentally are trying to lead to item selection, and in many cases, purchases. But if we step back and think more holistically about the purpose of integrating recommendation systems into businesses, it’s to drive revenue. If you’re an ecommerce shop, item selection and revenue may seem easily associated: a purchase leads to revenue, so good item recommendation leads to revenue. However, what about returns? Or even a harder question: is this revenue incremental? One challenge with recommendation systems is that it can be difficult to draw a causal arrow between any metric used to measure the performance of your models to the business-oriented KPIs.</p>&#13;
&#13;
<p>We call this <em>slow feedback</em> because sometimes the loop from a recommendation to a meaningful metric and back to the recommender can take weeks or even longer. This is especially challenging when you want to run experiments to understand whether a new model should be rolled out. The length of the test may need to stretch quite a bit more to get meaningful results.</p>&#13;
&#13;
<p>Usually, the team aligns on a proxy metric that the data scientists believe is a good estimator for the KPI, and that proxy metric is measured live. This approach has a huge variety of challenges, but it often suffices and provides motivation for more testing. Well-correlated proxies are often a great start to get directional information indicating where to take further iterations.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Model Metrics" data-type="sect2"><div class="sect2" id="id76">&#13;
<h2>Model Metrics</h2>&#13;
&#13;
<p>So, what<a data-primary="evaluation in production" data-secondary="model metrics" data-type="indexterm" id="id660"/><a data-primary="metrics" data-secondary="model metrics" data-type="indexterm" id="id661"/> are the key metrics to track for your model in production? Given that we’re looking at recommendation systems at inference time, we should seek to understand the following:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Distribution of recommendation across categorical features</p>&#13;
</li>&#13;
<li>&#13;
<p>Distribution of affinity scores</p>&#13;
</li>&#13;
<li>&#13;
<p>Number of candidates</p>&#13;
</li>&#13;
<li>&#13;
<p>Distribution of other ranking scores</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>As we discussed before, during the training process, we should be calculating broadly the ranges of our similarity scores in our latent space. Whether we are looking at high-level estimations or finer ones, we can use these distributions to get warning signals that something might be strange. Simply comparing the output of our model during inference, or over a set of inference requests, to these precompute distributions can be extremely helpful.</p>&#13;
&#13;
<p>Comparing distributions<a data-primary="Kullback–Leibler (KL) divergence" data-type="indexterm" id="id662"/><a data-primary="distributions, comparing" data-type="indexterm" id="id663"/> can be a long topic, but one standard approach is computing <em>KL-divergence</em> between the observed distribution and the expected distribution from training. By computing KL divergence between these, we can understand how <em>surprising</em> the model’s predictions are on a given day.</p>&#13;
&#13;
<p>What we’d really like is to understand the<a data-primary="receiver operating characteristic curve (ROC)" data-type="indexterm" id="id664"/><a data-primary="ROC (receiver operating characteristic curve)" data-type="indexterm" id="id665"/> receiver operating characteristic curve (ROC) of our model predictions with respect to one of our conversion types. However, this involves yet another integration to tie back to logging. Since our model API produces only the recommendation, we’ll still need to tie into logging from the web application to understand outcomes! To tie back in outcomes, we must join the model predictions with the logging output to get the evaluation labels, which can be done via log-parsing technologies (like Grafana, ELK, or Prometheus). We’ll see more of this in <a data-type="xref" href="ch08.html#ch:wikipedia-e2e">Chapter 8</a>.</p>&#13;
<div data-type="note" epub:type="note"><h1>Receiver Operating Characteristic Curve</h1>&#13;
<p>If we assume that the relevance scores are estimating whether the item will be relevant to the user, this forms a binary classification problem. Utilizing these (normalized) scores, we can build an ROC to estimate over the distributions of queries when the relevance score begins to accurately predict a relevant item via retrieval history. This curve can thus be used to estimate parameters like necessary retrieval depth or even problematic queries.<a data-primary="" data-startref="Aeval07" data-type="indexterm" id="id666"/><a data-primary="" data-startref="Meval07" data-type="indexterm" id="id667"/></p>&#13;
</div>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Continuous Training and Deployment" data-type="sect1"><div class="sect1" id="id209">&#13;
<h1>Continuous Training and Deployment</h1>&#13;
&#13;
<p>It<a data-primary="architectures" data-secondary="continuous training and deployment" data-type="indexterm" id="Acontin07"/><a data-primary="deployment" data-secondary="continuous training and deployment" data-type="indexterm" id="Dcont07"/> may feel like we’re done with this story since we have models tracked and production monitoring in place, but rarely are we satisfied with set-it-and-forget-it model development. One important characteristic of ML products is that models frequently need to be updated to even be useful. Previously, we discussed model metrics and that sometimes performance in production might look different from our expectations based on the trained models’ performance. This can be further exacerbated by model drift.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Model Drift" data-type="sect2"><div class="sect2" id="id77">&#13;
<h2>Model Drift</h2>&#13;
&#13;
<p><em>Model drift</em> is<a data-primary="continuous training and deployment" data-secondary="model drift" data-type="indexterm" id="id668"/><a data-primary="models" data-secondary="model drift" data-type="indexterm" id="id669"/> the notion that the same model may exhibit different prediction behavior over time, merely due to changes in the data-generating process. A simple example is a time-series forecasting model. When you build a time-series forecasting model, the especially unique property that is essential for good performance is<a data-primary="autoregression" data-type="indexterm" id="id670"/> <em>autoregression</em>: the value of the function covaries with previous values of the function. We won’t go into detail on time-series forecasting, but suffice it to say: your best hope of making a good forecast is to use up-to-date data! If you want to forecast stock prices, you should always use the most recent prices as part of your predictions.</p>&#13;
&#13;
<p>This simple example demonstrates how models may drift, and forecasting models are not so different from recommendation models—especially when considering the seasonal realities of many recommendation problems. A model that did well two weeks ago needs to be retrained with recent data to be expected to continue to perform well.</p>&#13;
&#13;
<p class="less_space pagebreak-before">One criticism of a model that drifts is “that’s the smoking gun of an <a data-primary="overfitting" data-type="indexterm" id="id671"/>overfit model,” but in reality these models require a certain amount of over-parameterization to be useful. In the context of recommendation systems, we’ve already seen that quirks like the Matthew effect<a data-primary="Matthew effect" data-type="indexterm" id="id672"/> have disastrous effects on the expected performance of a recommender model. If we don’t consider things like new items in our recommender, we are doomed to fail. Models can drift for a variety of reasons, often coming down to exogenous factors in the generating process that may not be captured by the model.</p>&#13;
&#13;
<p>One approach to dealing with and predicting<a data-primary="models" data-secondary="stale models" data-type="indexterm" id="id673"/><a data-primary="stale models" data-type="indexterm" id="id674"/> stale models is to simulate these scenarios during training. If you suspect that the model goes stale mostly because of the distribution changing over time, you can employ sequential cross-validation—training on a contiguous period and testing on a subsequent period—but with a specified block of time delay. For example, if you think your model performance is going to decrease after two weeks because it’s being trained on out-of-date observations, then during training you can purposely build your evaluation to incorporate a two-week delay before measuring performance. This is called<a data-primary="two-phase prediction comparison" data-type="indexterm" id="id675"/> <em>two-phase prediction comparison</em>, and by comparing the performances, you can estimate drift magnitudes to keep an eye out in production.</p>&#13;
&#13;
<p>A wealth of statistical approaches can be used to rein in these differences. In lieu of a deep dive into variational modeling for variability and reliability for your predictions, we’ll discuss continuous training and deployment and open this peanut with a sledge hammer.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Deployment Topologies" data-type="sect2"><div class="sect2" id="id210">&#13;
<h2>Deployment Topologies</h2>&#13;
&#13;
<p>Let’s<a data-primary="continuous training and deployment" data-secondary="deployment topologies" data-type="indexterm" id="CTDtopol07"/> consider a few structures for deploying models that will not only keep your models well in tune but also accommodate iteration, experimentation, and <span class="keep-together">optimization.</span></p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Ensembles" data-type="sect3"><div class="sect3" id="id78">&#13;
<h3>Ensembles</h3>&#13;
&#13;
<p><em>Ensembles</em> are<a data-primary="models" data-secondary="ensemble structure" data-type="indexterm" id="id676"/><a data-primary="ensemble modeling" data-type="indexterm" id="id677"/> a type of model structure in which multiple models are built, and the predictions from those models are pooled together in one of a variety of ways. While this notion of an ensemble is usually packaged into the model called for inference, you can generalize the idea to your deployment topology.</p>&#13;
&#13;
<p>Let’s take an example that builds on our previous discussion of prediction priors. If we have a collection of models with comparable performance on a task, we can deploy them in an ensemble, weighted by their deviation from the prior distributions of prediction that we’ve set before. This way, instead of having a simple yes/no filter on the output of your model’s range, you can more smoothly transition potentially problematic predictions into more expected ones.</p>&#13;
&#13;
<p>Another benefit of treating the ensemble as a deployment topology instead of only a model architecture is that you can <em>hot-swap</em> components of an ensemble as you make improvements in specific subdomains of your observation feature space. Take, for example, a life-time-value (LTV) model comprising three components: one that predicts well for new clients, another for activated clients, and a third for super-users. You may find that pooling via a voting mechanism performs the best on average, so you decide to implement a bagging approach. This works well, but later you find a better model for the new clients. By using the deployment topology for your ensemble, you can swap in the new model for the new clients and start comparing performance in your ensemble in production. This brings us to the next strategy, model comparison.</p>&#13;
<div data-type="note" epub:type="note"><h1>Ensemble Modeling</h1>&#13;
<p><em>Ensemble modeling</em> is popular in all kinds of ML, built upon the simple notion that the mixture of expert opinions is strictly more effective than a single estimator. In fact, assume for a moment that you have <math alttext="upper M">&#13;
  <mi>M</mi>&#13;
</math> classifiers with error rate <math alttext="epsilon">&#13;
  <mi>ϵ</mi>&#13;
</math>; then for an <math alttext="upper N">&#13;
  <mi>N</mi>&#13;
</math> class classification problem, your error would be <math alttext="upper P left-parenthesis y greater-than-or-equal-to k right-parenthesis equals sigma-summation Underscript k Overscript n Endscripts asterisk StartBinomialOrMatrix n Choose k EndBinomialOrMatrix epsilon Superscript k Baseline asterisk left-parenthesis 1 minus epsilon right-parenthesis Superscript n minus k">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>y</mi>&#13;
      <mo>≥</mo>&#13;
      <mi>k</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <msubsup><mo>∑</mo> <mrow><mi>k</mi></mrow> <mi>n</mi> </msubsup>&#13;
    <mo>*</mo>&#13;
    <mfenced close=")" open="(" separators="">&#13;
      <mfrac linethickness="0pt"><mi>n</mi> <mi>k</mi></mfrac>&#13;
    </mfenced>&#13;
    <msup><mi>ϵ</mi> <mi>k</mi> </msup>&#13;
    <mo>*</mo>&#13;
    <msup><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mi>ϵ</mi><mo>)</mo></mrow> <mrow><mi>n</mi><mo>-</mo><mi>k</mi></mrow> </msup>&#13;
  </mrow>&#13;
</math>, and the exciting part is that this is smaller than <math alttext="epsilon">&#13;
  <mi>ϵ</mi>&#13;
</math> for all values less than 0.5!</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Shadowing" data-type="sect3"><div class="sect3" id="shadowing-sect">&#13;
<h3>Shadowing</h3>&#13;
&#13;
<p>Deploying two models, even for the same task, can be enormously informative. We call this<a data-primary="shadowing" data-type="indexterm" id="id678"/><a data-primary="models" data-secondary="shadowing" data-type="indexterm" id="id679"/> <em>shadowing</em> when one model is “live” and the other is secretly also receiving all the requests and doing inference, and logging the results, of course. By shadowing traffic to the other model, you get the best expectations possible about how the model behaves before making your model live. This is especially useful when wanting to ensure that the prediction ranges align with expectation.</p>&#13;
&#13;
<p>In software engineering and DevOps, there’s a notion of<a data-primary="staging" data-type="indexterm" id="id680"/> <em>staging</em> for software. It’s a hotly contested question of “how much of the real infrastructure should staging see,” but shadowing is the staging of ML models. You can basically build a parallel pipeline for your entire infrastructure to connect for shadow models, or you can just put them both in the line of fire and have the request sent to both but use only one response. Shadowing is also crucial for implementing experimentation.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="less_space pagebreak-before" data-pdf-bookmark="Experimentation" data-type="sect3"><div class="sect3" id="id80">&#13;
<h3>Experimentation</h3>&#13;
&#13;
<p>As<a data-primary="experimentation" data-secondary="A/B testing" data-type="indexterm" id="id681"/><a data-primary="A/B testing" data-type="indexterm" id="id682"/><a data-primary="testing" data-secondary="A/B testing" data-type="indexterm" id="id683"/> good data scientists, we know that without a proper experimental framework, it’s risky to advertise much about the performance of a feature or, in this case, model. Experimentation can be handled with shadowing by having a controller layer that is taking the incoming requests and orchestrating which of the deployed models to curry the response along. A simple A/B experimentation framework might ask for a randomization at every request, whereas something like a multiarmed bandit will require the controller layer to have notions of the reward function.</p>&#13;
&#13;
<p>Experimentation is a deep topic that we don’t have the knowledge or space to do adequate justice, but it’s useful to know that this is where experimentation can fit into the larger deployment pipeline.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id684">&#13;
<h1>Model Cascades</h1>&#13;
<p>A<a data-primary="models" data-secondary="model cascades" data-type="indexterm" id="id685"/> really nice extension of the concepts of ensembling and shadowing is <a href="https://oreil.ly/Ao4t_">model cascading</a>, illustrated in <a data-type="xref" href="#fig:model-cascades">Figure 7-4</a>. The simplified idea of a model cascade is that we use model confidence to create a conditional ensemble. In particular, given an inference request, the model provides a prediction with a confidence estimate; when the model confidence is high, that prediction is returned, but if the confidence is below a certain threshold, a downstream model is called and the ensemble is started. There’s no reason to stop at two models; this method can be used to iteratively expand the number of ensemble layers for any number of models that in training show improved performance in an ensemble.</p>&#13;
&#13;
<figure><div class="figure" id="fig:model-cascades">&#13;
<img alt="Model cascades architecture" src="assets/brpj_0704.png"/>&#13;
<h6><span class="label">Figure 7-4. </span>Ensembles versus cascades</h6>&#13;
</div></figure>&#13;
&#13;
<p>Here are a few advantages of this approach:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Better expected performance overall, as ensembles usually increase performance</p>&#13;
</li>&#13;
<li>&#13;
<p>Ensemble performance with lower average computation time</p>&#13;
</li>&#13;
<li>&#13;
<p>Especially better performance in out-of-sample scenarios</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>This method scales to larger pools of models and, while it may incur significant training efforts, finding the right ordering of the models can have significant effects on model accuracy and latency.<a data-primary="" data-startref="Acontin07" data-type="indexterm" id="id686"/><a data-primary="" data-startref="Dcont07" data-type="indexterm" id="id687"/><a data-primary="" data-startref="CTDtopol07" data-type="indexterm" id="id688"/></p>&#13;
</div></aside>&#13;
</div></section>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Evaluation Flywheel" data-type="sect1"><div class="sect1" id="sec:eval_flywheel">&#13;
<h1>The Evaluation Flywheel</h1>&#13;
&#13;
<p>By now, it’s likely obvious<a data-primary="architectures" data-secondary="evaluation flywheel" data-type="indexterm" id="Aevalfly07"/> that a production ML model is far from a static object. Production ML systems of any kind are subject to as many deployment concerns as a traditional software stack, in addition to the added challenge of dataset shift and new users/items. In this section, we’ll look closely at the feedback loops introduced and understand how the components fit together to continuously improve our system—even with little input from a data scientist or ML engineer.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Daily Warm Starts" data-type="sect2"><div class="sect2" id="id81">&#13;
<h2>Daily Warm Starts</h2>&#13;
&#13;
<p>As<a data-primary="evaluation flywheel" data-secondary="daily warm starts" data-type="indexterm" id="id689"/><a data-primary="daily warm starts" data-type="indexterm" id="id690"/><a data-primary="warm starts" data-type="indexterm" id="id691"/> we’ve now discussed several times, we need a connection between the continuous output of our model and retraining. The first simplest example of this is daily warm starts, which essentially ask us to utilize the new data seen each day in our system.</p>&#13;
&#13;
<p>As might already be obvious, some of the recommendation models that show great success are quite large. Retraining some of them can be a massive undertaking, and simply <em>rerunning everything</em> each day is often infeasible. So, what can be done?</p>&#13;
&#13;
<p>Let’s ground this conversation in the user-user CF example that we’ve been sketching out; the first step was to build an embedding via our similarity definition. Let’s recall:</p>&#13;
<div data-type="equation">&#13;
<math alttext="normal upper U normal upper S normal i normal m Subscript upper A comma upper B Baseline equals StartFraction sigma-summation Underscript x element-of script upper R Subscript upper A comma upper B Baseline Endscripts left-parenthesis r Subscript upper A comma x Baseline minus r overbar Subscript upper A Baseline right-parenthesis left-parenthesis r Subscript upper B comma x Baseline minus r overbar Subscript upper B Baseline right-parenthesis Over StartRoot sigma-summation Underscript x element-of script upper R Subscript upper A comma upper B Baseline Endscripts left-parenthesis r Subscript upper A comma x Baseline minus r overbar Subscript upper A Baseline right-parenthesis squared EndRoot StartRoot sigma-summation Underscript x element-of script upper R Subscript upper A comma upper B Baseline Endscripts left-parenthesis r Subscript upper B comma x Baseline minus r overbar Subscript upper B Baseline right-parenthesis squared EndRoot EndFraction" display="block">&#13;
  <mrow>&#13;
    <msub><mi> USim </mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow> </msub>&#13;
    <mo>=</mo>&#13;
    <mfrac><mrow><msub><mo>∑</mo> <mrow><mi>x</mi><mo>∈</mo><msub><mi>ℛ</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow> </msub></mrow> </msub><mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>A</mi><mo>,</mo><mi>x</mi></mrow> </msub><mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>A</mi> </msub><mo>)</mo></mrow><mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>B</mi><mo>,</mo><mi>x</mi></mrow> </msub><mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>B</mi> </msub><mo>)</mo></mrow></mrow> <mrow><msqrt><mrow><msub><mo>∑</mo> <mrow><mi>x</mi><mo>∈</mo><msub><mi>ℛ</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow> </msub></mrow> </msub><msup><mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>A</mi><mo>,</mo><mi>x</mi></mrow> </msub><mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>A</mi> </msub><mo>)</mo></mrow> <mn>2</mn> </msup></mrow></msqrt><msqrt><mrow><msub><mo>∑</mo> <mrow><mi>x</mi><mo>∈</mo><msub><mi>ℛ</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow> </msub></mrow> </msub><msup><mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>B</mi><mo>,</mo><mi>x</mi></mrow> </msub><mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>B</mi> </msub><mo>)</mo></mrow> <mn>2</mn> </msup></mrow></msqrt></mrow></mfrac>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Here we remember that the similarity between two users is dependent on the shared ratings and on each user’s average rating.</p>&#13;
&#13;
<p>On a given day, let’s say <math alttext="upper X overTilde equals StartSet x overTilde bar x was rated since yesterday by a user EndSet">&#13;
  <mrow>&#13;
    <mover accent="true"><mi>X</mi> <mo>˜</mo></mover>&#13;
    <mo>=</mo>&#13;
    <mfenced close="}" open="{" separators="">&#13;
      <mover accent="true"><mi>x</mi> <mo>˜</mo></mover>&#13;
      <mo>∣</mo>&#13;
      <mi>x</mi>&#13;
      <mspace width="4.pt"/>&#13;
      <mtext>was</mtext>&#13;
      <mspace width="4.pt"/>&#13;
      <mtext>rated</mtext>&#13;
      <mspace width="4.pt"/>&#13;
      <mtext>since</mtext>&#13;
      <mspace width="4.pt"/>&#13;
      <mtext>yesterday</mtext>&#13;
      <mspace width="4.pt"/>&#13;
      <mtext>by</mtext>&#13;
      <mspace width="4.pt"/>&#13;
      <mtext>a</mtext>&#13;
      <mspace width="4.pt"/>&#13;
      <mtext>user</mtext>&#13;
      <mspace width="4.pt"/>&#13;
    </mfenced>&#13;
  </mrow>&#13;
</math>. Then we’d need to update our user similarities, but ideally we’d leave everything else the same. To update the user’s data, we see that all <math alttext="x overTilde">&#13;
  <mover accent="true"><mi>x</mi> <mo>˜</mo></mover>&#13;
</math> rated by two users, <math alttext="upper A">&#13;
  <mi>A</mi>&#13;
</math> and <math alttext="upper B dollar-sign comma w i l l c o n t r i b u t e c h a n g e s period upper O n e a l s o m i g h t n o t i c e t h a t t h e l a t e x m a t h colon left-bracket dollar-sign r overbar Subscript upper A Baseline">&#13;
  <mi>B</mi>&#13;
</math><math>&#13;
  <msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>A</mi> </msub>&#13;
</math> and <math alttext="r overbar Subscript upper B">&#13;
  <msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>B</mi> </msub>&#13;
</math>, would need to change, but we could probably skip these updates in many cases where the number of ratings by those users was large. All in all, this means for each <math alttext="x overTilde">&#13;
  <mover accent="true"><mi>x</mi> <mo>˜</mo></mover>&#13;
</math>, we should look up which users previously rated <math alttext="x">&#13;
  <mi>x</mi>&#13;
</math> and update the user similarity between them and the new rater.</p>&#13;
&#13;
<p>This is a bit ad hoc, but for many methods you can utilize these tricks to reduce a full retraining. This would avoid a full batch retraining, via a fast layer. Other approaches exist, like building a separate model that can approximate recommendations for low-signal items. This can be done via feature models and can significantly reduce the complexity of these quick retrainings.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Lambda Architecture and Orchestration" data-type="sect2"><div class="sect2" id="id82">&#13;
<h2>Lambda Architecture and Orchestration</h2>&#13;
&#13;
<p>On<a data-primary="evaluation flywheel" data-secondary="lambda architecture and orchestration" data-type="indexterm" id="id692"/><a data-primary="Lambda architecture" data-type="indexterm" id="id693"/> the more extreme end of the spectrum of these strategies is the lambda architecture; as discussed in <a data-type="xref" href="ch06.html#data-processing">Chapter 6</a>, the lambda architecture seeks to have a much more frequent pipeline for adding new data into the system. The<a data-primary="speed layer" data-type="indexterm" id="id694"/> <em>speed</em> layer is responsible for working on small batches to perform the data transformations, and on model fitting to combine with the core model. As a reminder, many other aspects of the pipeline should also be updated during these fast layers, like the nearest neighbors graph, the feature store, and the filters.</p>&#13;
&#13;
<p>Different components of the pipeline can require different investments to keep updated, so their schedules are an important consideration. You might be starting to notice that keeping all of these aspects in sync can be a bit challenging. If you have model training, model updating, feature store updates, redeployment, and new items/users all coming in on potentially different schedules, a <em>lot</em> of coordination may be necessary. This is where<a data-primary="orchestration tools" data-type="indexterm" id="id695"/> an <em>orchestration tool</em> can become relevant. A variety of approaches exist, but a few useful technologies here are GoCD, MetaFlow, and KubeFlow; the latter is more oriented at Kubernetes infrastructures. Another pipeline orchestration tool that can handle both batch and streaming pipelines is Apache Beam.</p>&#13;
&#13;
<p>Generally, for ML deployment pipelines, we need to have a reliable core pipeline and the ability to keep the systems up to date as more data pours in. Orchestration systems usually define the topology of the systems, the relevant infrastructure configurations, and the mapping of the code artifacts needing to be run—not to mention the CRON schedules of when all these jobs need to run. Code as infrastructure is a popular paradigm that captures these goals as a mantra, so that even all this configuration itself is reproducible and automatable.</p>&#13;
&#13;
<p>In all these orchestration considerations, there’s a heavy overlap with containerization and <em>how</em> these steps may be deployed. Unfortunately, most of this discussion is beyond the scope of this book, but a simple overview is that containerized deployment with something like Docker is extremely helpful for ML services, and managing those deployments with various container management systems, like Kubernetes, is also popular.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Logging" data-type="sect2"><div class="sect2" id="id83">&#13;
<h2>Logging</h2>&#13;
&#13;
<p>Logging<a data-primary="evaluation flywheel" data-secondary="logging" data-type="indexterm" id="EFlog07"/><a data-primary="logging" data-secondary="best practices for" data-type="indexterm" id="id696"/> has come up several times already. Previously in this chapter, you saw that logging was important for ensuring that our system was behaving as expected. Let’s discuss some best practices for logging and how they fit into our plans.</p>&#13;
&#13;
<p>When we discussed traces and spans earlier, we were able to get a snapshot of the entire call stack of the services involved in responding to a request. Linking the services together to see the larger picture is incredibly useful, and when it comes to logging, gives us a hint as to how we should be orienting our thinking. Returning to our favorite RecSys architecture, we have the following:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Collector receiving the request and looking up the embedding relevant to the user</p>&#13;
</li>&#13;
<li>&#13;
<p>Computing ANN on items for that vector</p>&#13;
</li>&#13;
<li>&#13;
<p>Applying filters via blooms to eliminate potential bad recommendations</p>&#13;
</li>&#13;
<li>&#13;
<p>Augmenting features of the candidate items and user via the feature stores</p>&#13;
</li>&#13;
<li>&#13;
<p>Scoring of candidates via the ranking model and estimating potential confidence</p>&#13;
</li>&#13;
<li>&#13;
<p>Ordering and application of business logic or experimentation</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Each of these elements has potential applications of logging, but let’s now think about how to link them together. The relevant concept from microservices is<a data-primary="correlation" data-secondary="correlation identifiers" data-type="indexterm" id="id697"/> correlation IDs; a <em>correlation ID</em> is simply an identifier that’s passed along the call stack to ensure the ability to link everything later. As is likely obvious at this point, each of these services will be responsible for its own logging, but the services are almost always more useful in aggregate.</p>&#13;
&#13;
<p>These days, Kafka<a data-primary="Kafka" data-type="indexterm" id="id698"/> is often used as the log-stream processor to listen for logs from all the services in your pipeline and to manage their processing and storing. Kafka relies on a message-based architecture; each service is a producer, and Kafka helps manage those messages to consumer channels. In terms of log management, the Kafka cluster receives all the logs in the relevant formats, hopefully augmented with correlation IDs, and sends them off to an ELK stack. The<a data-primary="ELK stack (Elasticsearch, Logstash, Kibana)" data-type="indexterm" id="id699"/> <em>ELK stack</em>—Elasticsearch, Logstash, Kibana—consists of a Logstash component to handle incoming log streams and apply structured processing, Elasticsearch to build search indices to the log store, and Kibana to add a UI and high-level dashboarding to the logging.</p>&#13;
&#13;
<p>This stack of technologies is focused on ensuring that you have access and observability from your logs. Other technologies focus on other aspects, but what should you be logging?</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="less_space pagebreak-before" data-pdf-bookmark="Collector logs" data-type="sect3"><div class="sect3" id="id212">&#13;
<h3>Collector logs</h3>&#13;
&#13;
<p>Again, we wish<a data-primary="logging" data-secondary="collector logs" data-type="indexterm" id="id700"/><a data-primary="collector" data-secondary="collector logs" data-type="indexterm" id="id701"/> to log during the following:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Collector receiving the request and looking up the embedding relevant to the user</p>&#13;
</li>&#13;
<li>&#13;
<p>Computing ANN on items for that vector</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The collector receives a request, consisting in our simplest example of <code>user_id</code>, <code>requesting_timestamp</code>, and any augmenting keyword elements (kwargs) that might be required. A <code>correlation_id</code> should be passed along from the requester or generated at this step. A log with these basic keys should be fired, along with the timestamp of request received. A call is made to the embedding store, and the collector should log this request. Then the embedding store should log this request when received, along with the embedding store’s response. Finally, the collector should log the response as it returns. This may feel like a lot of redundant information, but the explicit parameters included in the API calls become extremely useful when <span class="keep-together">troubleshooting.</span></p>&#13;
&#13;
<p>The collector now has the vector it will need to perform a vector search, so it will make a call to the ANN service. Logging this call, and any relevant logic in choosing the <math alttext="k">&#13;
  <mi>k</mi>&#13;
</math> for number of neighbors will be important, along with the ANN’s received API request, the relevant state for computing ANN, and ANN’s response. Back in the collector, logging that response and any potential data augmentation for downstream service requirements are the next steps.</p>&#13;
&#13;
<p>At this point, at least six logs have been emitted—only reinforcing the need for a way to link these all together. In practice, you often have other relevant steps in your service that should be logged (e.g., checking that the distribution of distances in returned neighbors is appropriate for downstream ranking).</p>&#13;
&#13;
<p>Note that if the embedding lookup was a miss, logging that miss is obviously important, as well as logging the subsequent request to the cold-start recommendation pipeline. The cold-start pipeline will incur additional logs.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Filtering and scoring" data-type="sect3"><div class="sect3" id="id84">&#13;
<h3>Filtering and scoring</h3>&#13;
&#13;
<p>Now<a data-primary="logging" data-secondary="filtering and scoring" data-type="indexterm" id="id702"/><a data-primary="filtering" data-type="indexterm" id="id703"/><a data-primary="scoring" data-type="indexterm" id="id704"/> we need to monitor the following steps:</p>&#13;
<ol>&#13;
<li>&#13;
<p>Applying filters via blooms to eliminate potential bad recommendations</p>&#13;
</li>&#13;
<li>&#13;
<p>Augmenting features to the candidate items and user via the feature stores</p>&#13;
</li>&#13;
<li>&#13;
<p>Scoring candidates via the ranking model, and potential confidence estimation</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p class="less_space pagebreak-before">We should log the incoming request to the filtering service as well as the collection of filters we wish to apply. Additionally, as we search the blooms for each item and rule them in or out of the bloom, we should build up some structured logging of which items are caught in which filters and then log all this as a blob for later inspection. Responses and requests should be logged as part of feature augmentation—where we should log requests and responses to the feature store.</p>&#13;
&#13;
<p>Also log the augmented features that end up attached to the item entities. This may seem redundant with the feature store itself, but understanding which features were added during a recommendation pipeline is <em>crucial</em> when looking back later to figure out why the pipeline might have behaved differently than anticipated.</p>&#13;
&#13;
<p>At the time of scoring, the entire set of candidates should be logged with the features necessary for scoring and the output scores. It’s extremely powerful to log this entire dataset, because training later can use these to get a better sense for real ranking sets. Finally, the response is passed to the next step with the ranked candidates and all their features.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Ordering" data-type="sect3"><div class="sect3" id="id85">&#13;
<h3>Ordering</h3>&#13;
&#13;
<p>We<a data-primary="logging" data-secondary="ordering" data-type="indexterm" id="id705"/><a data-primary="business logic" data-secondary="ordering and application of" data-type="indexterm" id="id706"/> have one more step to go, but it’s an essential one: <em>ordering and application of business logic or experimentation</em>. This step is probably the most important logging step, because of how complicated and ad hoc the logic in this step can get.</p>&#13;
&#13;
<p>If you have multiple intersecting business requirements implemented via filters at this step, while also integrating with experimentation, you can find yourself seriously struggling to unpack how reasonable expectations coming out of the ranker have turned into a mess by response time. Techniques like logging the incoming candidates, keyed to why they’re eliminated, and the order of business rules applied will make reconstructing the behavior much more tractable.</p>&#13;
&#13;
<p>Additionally, experimentation routing<a data-primary="experimentation" data-secondary="experimentation routing" data-type="indexterm" id="id707"/> will likely be handled by another service, but the experiment ID seen in this step and the way that experiment assignment was utilized are the responsibility of the server. As we ship off the final recommendations, or decide to go another round, one last log of the state of the recommendation will ensure that app logs can be validated with responses.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id708">&#13;
<h1>Notes on Formatting</h1>&#13;
<p>Structured logs<a data-primary="logging" data-secondary="formatting of" data-type="indexterm" id="id709"/><a data-primary="structured logs" data-type="indexterm" id="id710"/><a data-primary="formatting, of logs" data-type="indexterm" id="id711"/> are your friend. Implementing a data structure to hold the relevant data for your logs and then utilizing a <a href="https://oreil.ly/5Nu5N">log-formatter object</a> will significantly reduce the difficulty in parsing and writing these logs. One often underappreciated feature of building message objects in code, and utilizing them as a running data structure throughout your call stack, is tight coupling between logs and app logic.</p>&#13;
&#13;
<p>Tight coupling<a data-primary="tight coupling" data-type="indexterm" id="id712"/> is often  bemoaned in service-architecture discussions, but when that coupling is between your logs and your actual objects of execution, this saves you a lot of headaches. When changing the objects used for your service, instead of having an additional step to ensure the logs reflect that, you can propagate those changes through automatically by using the same objects in tandem with a log formatter.</p>&#13;
&#13;
<p>These processes can also make good use of testing, to ensure that the objects your code cares about are visible in the logs, and these log-formatter objects can have enforced matching via unit tests. Finally, because we want to connect to downstream log parsing and log searching, it will be invaluable to have a clear relationship between the log stack and the application stack via object parameters and keys in the log data structure.<a data-primary="" data-startref="EFlog07" data-type="indexterm" id="id713"/></p>&#13;
</div></aside>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Active Learning" data-type="sect2"><div class="sect2" id="id86">&#13;
<h2>Active Learning</h2>&#13;
&#13;
<p>So far, we have discussed<a data-primary="evaluation flywheel" data-secondary="active learning" data-type="indexterm" id="EFactlearn07"/> using updating data to train on a much more frequent schedule, and we’ve discussed how to provide good recommendations, even when the model hasn’t seen enough data for those entities. An additional opportunity for the feedback loop of recommendation and rating is active learning.</p>&#13;
&#13;
<p>We<a data-primary="active learning" data-secondary="core ideas of" data-type="indexterm" id="id714"/> won’t be able to go deep into the topic, which is a large and active field of research, but we will discuss the core ideas in relation to recommendation systems. <em>Active learning</em> changes the learning paradigm a bit by suggesting that the learner should not only be passively collecting labeled (maybe implicit) observations but also attempting to mine relations and preferences from them. Active learning determines which data and observations would be most useful in improving model performance and then seeks out those labels. In the context of RecSys, we know that the Matthew effect<a data-primary="Matthew effect" data-type="indexterm" id="id715"/> is one of our biggest challenges, in that many potentially good matches for a user may be lacking enough or appropriate ratings to bubble to the top during the recommendations.</p>&#13;
&#13;
<p>What<a data-primary="active learning" data-secondary="advantages of" data-type="indexterm" id="id716"/> if we employed a simple policy: every new item to the store gets recommended as a second option to the first 100 customers. Two outcomes would result:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>We would quickly establish data for our new item to help cold-start it.</p>&#13;
</li>&#13;
<li>&#13;
<p>We would likely decrease the performance of our recommender.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>In many cases, the second outcome is worth enduring to achieve the first, but when? And is this the right way to approach this problem? Active learning provides a methodical approach to these problems.</p>&#13;
&#13;
<p>Another more specific advantage of active learning schemes is that you can broaden the distribution of observed data. In addition, to just cold-start items, we can use active learning to target broadening users’ interests. This is usually framed as an uncertainty-reduction technique, as it can be used to improve the confidence in recommendations in a broader range of item categories. Here’s a simple example: a user shops for only sci-fi books, so one day you show them a few extremely well-liked Westerns to see whether that user might be open to occasionally getting recommendations for Westerns. See <a data-type="xref" href="ch10.html#propensity">“Propensity Weighting for Recommendation System Evaluation”</a> for more details.</p>&#13;
&#13;
<p>An<a data-primary="active learning" data-secondary="implementing" data-type="indexterm" id="id717"/> active learning system is instrumented as a loss function inherited from the model it’s trying to enhance—usually tied to uncertainty in some capacity—and it’s attempting to minimize that loss. Given a model <math alttext="script upper M">&#13;
  <mi>ℳ</mi>&#13;
</math> trained on a set of observations and labels <math alttext="StartSet x Subscript i Baseline comma y Subscript i Baseline EndSet">&#13;
  <mfenced close="}" open="{" separators="">&#13;
    <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
    <mo>,</mo>&#13;
    <msub><mi>y</mi> <mi>i</mi> </msub>&#13;
  </mfenced>&#13;
</math>, with loss <math alttext="script upper L">&#13;
  <mi>ℒ</mi>&#13;
</math>, an active learner seeks to find a new observation, <math alttext="x overbar">&#13;
  <mover accent="true"><mi>x</mi> <mo>¯</mo></mover>&#13;
</math>, such that if a label was obtained, <math alttext="y overbar">&#13;
  <mover accent="true"><mi>y</mi> <mo>¯</mo></mover>&#13;
</math>, the loss would decrease via the model’s training including this new pair. In particular, the goal is to approximate the marginal reduction in loss due to each possible new observation and find the observation that maximizes that reduction in the loss function:</p>&#13;
<div data-type="equation">&#13;
<math alttext="Argmax Subscript x overbar Baseline left-parenthesis script upper L left-parenthesis script upper M Subscript StartSet x Sub Subscript i Subscript comma y Sub Subscript i Subscript EndSet Baseline right-parenthesis minus script upper L left-parenthesis script upper M Subscript StartSet x Sub Subscript i Subscript comma y Sub Subscript i Subscript EndSet union StartSet x overbar EndSet Baseline right-parenthesis right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <msub><mi> Argmax </mi> <mover accent="true"><mi>x</mi> <mo>¯</mo></mover> </msub>&#13;
    <mfenced close=")" open="(" separators="">&#13;
      <mi>ℒ</mi>&#13;
      <mfenced close=")" open="(" separators="">&#13;
        <msub><mi>ℳ</mi> <mfenced close="}" open="{" separators=""><msub><mi>x</mi> <mi>i</mi> </msub><mo>,</mo><msub><mi>y</mi> <mi>i</mi> </msub></mfenced> </msub>&#13;
      </mfenced>&#13;
      <mo>-</mo>&#13;
      <mi>ℒ</mi>&#13;
      <mfenced close=")" open="(" separators="">&#13;
        <msub><mi>ℳ</mi> <mrow><mfenced close="}" open="{" separators=""><msub><mi>x</mi> <mi>i</mi> </msub><mo>,</mo><msub><mi>y</mi> <mi>i</mi> </msub></mfenced><mo>∪</mo><mfenced close="}" open="{" separators=""><mover accent="true"><mi>x</mi> <mo>¯</mo></mover></mfenced></mrow> </msub>&#13;
      </mfenced>&#13;
    </mfenced>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>The<a data-primary="active learning" data-secondary="structure of" data-type="indexterm" id="id718"/> structure of an active learning system roughly follows these steps:</p>&#13;
<ol>&#13;
<li>&#13;
<p>Estimate marginal decrease in loss due to obtaining one of a set of observations.</p>&#13;
</li>&#13;
<li>&#13;
<p>Select the observation with the largest effect.</p>&#13;
</li>&#13;
<li>&#13;
<p><em>Query</em> the user; i.e., provide the recommendation to obtain a label.</p>&#13;
</li>&#13;
<li>&#13;
<p>Update the model.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>It’s probably clear that this paradigm requires a much faster training loop than our previous fast retraining schemes. Active learning can be instrumented in the same infrastructure as our other setups, or it can have its own mechanisms for integration into the pipeline.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Types of optimization" data-type="sect3"><div class="sect3" id="id87">&#13;
<h3>Types of optimization</h3>&#13;
&#13;
<p>The<a data-primary="active learning" data-secondary="optimization" data-type="indexterm" id="ALopt07"/><a data-primary="optimization" data-secondary="active learning" data-type="indexterm" id="Oactlearn07"/> optimization procedure carried out by an active learner in a recommendation system has two approaches: personalized and nonpersonalized. Because RecSys is all about personalization, it’s no surprise that we would, in time, want to push the utility of our active learning further by integrating the great details we already know about users.</p>&#13;
&#13;
<p>We can think of these two approaches as global loss minimization and local loss minimization. Active learning that isn’t personalized tends to be about minimizing the loss over the entire system, not for only one user. (This split doesn’t perfectly capture the ontology, but it’s a useful mnemonic). In practice, optimization methods are nuanced and sometimes utilize complicated algorithms and training procedures.</p>&#13;
&#13;
<p>Let’s talk through some factors to optimize for nonpersonalized active learning:</p>&#13;
<dl>&#13;
<dt><em>User rating variance</em></dt>&#13;
<dd>&#13;
<p>Consider<a data-primary="user rating variance" data-type="indexterm" id="id719"/> which items have the largest variance in user ratings to try to get more data on those we find the most complicated in our observations.</p>&#13;
</dd>&#13;
<dt><em>Entropy</em></dt>&#13;
<dd>&#13;
<p>Consider<a data-primary="entropy" data-type="indexterm" id="id720"/> the dispersion of ratings of a particular item across an ordinal feature. This is useful for understanding whether our set of ratings for an item is distributed uniformly at random.</p>&#13;
</dd>&#13;
<dt><em>Greedy extend</em></dt>&#13;
<dd>&#13;
<p>Measure<a data-primary="greedy extend" data-type="indexterm" id="id721"/> which items seem to yield the worst performance in our current model; this attempts to improve our performance overall by collecting more data on the hardest items to recommend well.</p>&#13;
</dd>&#13;
<dt><em>Representatives or exemplars</em></dt>&#13;
<dd>&#13;
<p>Pick out items that are extremely representative of large groups of items; we can think of this as “If we have good labels for this, we have good labels for everything like this.”</p>&#13;
</dd>&#13;
<dt><em>Popularity</em></dt>&#13;
<dd>&#13;
<p>Select<a data-primary="popularity" data-type="indexterm" id="id722"/> items that the user is most likely to have experience with to maximize the likelihood that they’ll give an opinion or rating.</p>&#13;
</dd>&#13;
<dt><em>Co-coverage</em></dt>&#13;
<dd>&#13;
<p>Attempt<a data-primary="co-coverage" data-type="indexterm" id="id723"/> to amplify the ratings for frequently occurring pairs in the dataset; this strikes directly at the CF structure to maximize the utility of observations.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>On the personalized side:</p>&#13;
<dl>&#13;
<dt><em>Binary prediction</em></dt>&#13;
<dd>&#13;
<p>To<a data-primary="binary prediction" data-type="indexterm" id="id724"/> maximize the chances that the user can provide the requested rating, choose the items that the user is more likely to have experienced. This can be achieved via an MF on the binary ratings matrix.</p>&#13;
</dd>&#13;
<dt><em>Influence based</em></dt>&#13;
<dd>&#13;
<p>Estimate<a data-primary="influenced-based optimization" data-type="indexterm" id="id725"/><a data-primary="optimization" data-secondary="influenced-based optimization" data-type="indexterm" id="id726"/> the influence of item ratings on the rating prediction of other items, and select the items with the largest influence. This attempts to directly measure the impact of a new item rating on the system.</p>&#13;
</dd>&#13;
<dt><em>Rating optimized</em></dt>&#13;
<dd>&#13;
<p>Obviously, there’s an opportunity to simply use the best rating or best rating within a class to perform active learning queries, but this is precisely the standard strategy in recommendation systems to serve good recommendations.</p>&#13;
</dd>&#13;
<dt><em>User segmented</em></dt>&#13;
<dd>&#13;
<p>When available, use user segmentation<a data-primary="user segmentation" data-type="indexterm" id="id727"/> and feature clusters within users to anticipate when users have opinions and preferences on an item by virtue of the user-similarity structure.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>In general, a soft trade-off exists between active learning that’s useful for maximally improving your model globally and active learning that’s useful for maximizing the likelihood that a user can and will rate a particular item. Let’s look at one particular example that uses both.<a data-primary="" data-startref="ALopt07" data-type="indexterm" id="id728"/><a data-primary="" data-startref="Oactlearn07" data-type="indexterm" id="id729"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Application: User sign-up" data-type="sect3"><div class="sect3" id="id88">&#13;
<h3>Application: User sign-up</h3>&#13;
&#13;
<p>One<a data-primary="user sign-up" data-type="indexterm" id="id730"/><a data-primary="new users, onboarding" data-type="indexterm" id="id731"/><a data-primary="onboarding new users" data-type="indexterm" id="id732"/> common hurdle to overcome in building recommendation systems is on-boarding new users. By definition, new users will be cold-starting with no ratings of any kind and will likely not expect great recommendations from the start.</p>&#13;
&#13;
<p>We may begin with the MPIR for all new users—simply show them <em>something</em> to get them started and then learn as you go. But is there something better?</p>&#13;
&#13;
<p>One approach you’ve probably experienced is the user onboarding flow: a simple set of questions employed by many websites to quickly ascertain basic information about the user, to help guide early recommendation. If discussing our book recommender, this might be asking what genres the user likes, or in the case of a coffee recommender, how the user brews coffee in the morning. It’s probably clear that these questions are building up knowledge-based recommender systems and don’t directly feed into our previous pipelines but can still provide some help in early recommendations.</p>&#13;
&#13;
<p>If instead we looked at all our previous data and asked, “Which books in particular are most useful for determining a user’s taste?,” this would be an active learning approach. We could even have a decision tree of possibilities as the user answered each question, wherein the answer determines which next question is most useful <span class="keep-together">to ask.</span><a data-primary="" data-startref="EFactlearn07" data-type="indexterm" id="id733"/><a data-primary="" data-startref="Aevalfly07" data-type="indexterm" id="id734"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id309">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>Now we have the confidence that we can serve up our recommendations, and even better, we have instrumented our system to gather feedback. We’ve shown how you can gain confidence before you deploy and how you can experiment with new models or solutions. Ensembles and cascades allow you to combine testing with iteration, and the data flywheel provides a powerful mechanism for improving your product.</p>&#13;
&#13;
<p>You may be wondering how to put all this new knowledge into practice, to which the next chapter will speak. Let’s understand how data processing and simple counting can lead to an effective—and useful!—recommendation system.</p>&#13;
</div></section>&#13;
</div></section></body></html>