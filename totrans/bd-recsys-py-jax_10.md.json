["```py\n// Generic text document.\nmessage TextDocument {\n  // Primary entity, in wikipedia it is the title.\n  string primary = 1;\n  // Secondary entity, in wikipedia it is other titles.\n  repeated string secondary = 2;\n  // Raw body tokens.\n  repeated string tokens = 3;\n  // URL. Only visible documents have urls, some e.g. redirect shouldn't.\n  string url = 4;\n}\n```", "```py\nbin/spark-submit\n--master=local[4]\n--conf=\"spark.files.ignoreCorruptFiles=true\"\ntokenize_wiki_pyspark.py\n--input_file=data/enwiki-latest-parsed --output_file=data/enwiki-latest-tokenized\n```", "```py\ndef lower_rdd(input_file: str,\n              output_file: str):\n  \"\"\"Takes a text file and converts it to lowercase..\"\"\"\n  sc = SparkContext()\n  input_rdd = sc.textFile(input_file)\n  input_rdd.map(lambda line: line.lower()).saveAsTextFile(output_file)\n```", "```py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n#\n\n\"\"\"\n This reads a doc.pb.b64.bz2 file and generates a dictionary.\n\"\"\"\nimport base64\nimport bz2\nimport nlp_pb2 as nlp_pb\nimport re\nfrom absl import app\nfrom absl import flags\nfrom pyspark import SparkContext\nfrom token_dictionary import TokenDictionary\n\nFLAGS = flags.FLAGS\nflags.DEFINE_string(\"input_file\", None, \"Input doc.pb.b64.bz2 file.\")\nflags.DEFINE_string(\"title_output\", None,\n                    \"The title dictionary output file.\")\nflags.DEFINE_string(\"token_output\", None,\n                    \"The token dictionary output file.\")\nflags.DEFINE_integer(\"min_token_frequency\", 20,\n                     \"Minimum token frequency\")\nflags.DEFINE_integer(\"max_token_dictionary_size\", 500000,\n                     \"Maximum size of the token dictionary.\")\nflags.DEFINE_integer(\"max_title_dictionary_size\", 500000,\n                     \"Maximum size of the title dictionary.\")\nflags.DEFINE_integer(\"min_title_frequency\", 5,\n                     \"Titles must occur this often.\")\n\n# Required flag.\nflags.mark_flag_as_required(\"input_file\")\nflags.mark_flag_as_required(\"token_output\")\nflags.mark_flag_as_required(\"title_output\")\n```", "```py\ndef main(argv):\n  \"\"\"Main function.\"\"\"\n  del argv  # Unused.\n  sc = SparkContext()\n  input_rdd = sc.textFile(FLAGS.input_file)\n  text_doc = parse_document(input_rdd)\n  make_token_dictionary(\n    text_doc,\n    FLAGS.token_output,\n    FLAGS.min_token_frequency,\n    FLAGS.max_token_dictionary_size\n  )\n  make_title_dictionary(\n    text_doc,\n    FLAGS.title_output,\n    FLAGS.min_title_frequency,\n    FLAGS.max_title_dictionary_size\n  )\n\nif __name__ == \"__main__\":\n    app.run(main)\n```", "```py\ndef update_dict_term(term, dictionary):\n    \"\"\"Updates a dictionary with a term.\"\"\"\n    if term in dictionary:\n        x = dictionary[term]\n    else:\n        x = nlp_pb.TokenStat()\n        x.token = term\n        dictionary[term] = x\n    x.frequency += 1\n\ndef update_dict_doc(term, dictionary):\n    \"\"\"Updates a dictionary with the doc frequency.\"\"\"\n    dictionary[term].doc_frequency += 1\n\ndef count_titles(doc, title_dict):\n    \"\"\"Counts the titles.\"\"\"\n    # Handle the titles.\n    all_titles = [doc.primary]\n    all_titles.extend(doc.secondary)\n    for title in all_titles:\n        update_dict_term(title, title_dict)\n    title_set = set(all_titles)\n    for title in title_set:\n        update_dict_doc(title, title_dict)\n\ndef count_tokens(doc, token_dict):\n    \"\"\"Counts the tokens.\"\"\"\n    # Handle the tokens.\n    for term in doc.tokens:\n        update_dict_term(term, token_dict)\n    term_set = set(doc.tokens)\n    for term in term_set:\n        update_dict_doc(term, token_dict)\n\ndef parse_document(rdd):\n    \"\"\"Parses documents.\"\"\"\n    def parser(x):\n        result = nlp_pb.TextDocument()\n        try:\n            result.ParseFromString(x)\n        except google.protobuf.message.DecodeError:\n            result = None\n        return result\n    output = rdd.map(base64.b64decode)\\\n        .map(parser)\\\n        .filter(lambda x: x is not None)\n    return output\n\ndef process_partition_for_tokens(doc_iterator):\n    \"\"\"Processes a document partition for tokens.\"\"\"\n    token_dict = {}\n    for doc in doc_iterator:\n        count_tokens(doc, token_dict)\n    for token_stat in token_dict.values():\n        yield (token_stat.token, token_stat)\n\ndef tokenstat_reducer(x, y):\n    \"\"\"Combines two token stats together.\"\"\"\n    x.frequency += y.frequency\n    x.doc_frequency += y.doc_frequency\n    return x\n\ndef make_token_dictionary(\n    text_doc,\n    token_output,\n    min_term_frequency,\n    max_token_dictionary_size\n):\n    \"\"\"Makes the token dictionary.\"\"\"\n    tokens = text_doc.mapPartitions(process_partition_for_tokens)\n        .reduceByKey(tokenstat_reducer).values()\n    filtered_tokens = tokens.filter(\n        lambda x: x.frequency >= min_term_frequency)\n    all_tokens = filtered_tokens.collect()\n    sorted_token_dict = sorted(\n        all_tokens, key=lambda x: x.frequency, reverse=True)\n    count = min(max_token_dictionary_size, len(sorted_token_dict))\n    for i in range(count):\n        sorted_token_dict[i].index = i\n    TokenDictionary.save(sorted_token_dict[:count], token_output)\n\ndef process_partition_for_titles(doc_iterator):\n    \"\"\"Processes a document partition for titles.\"\"\"\n    title_dict = {}\n    for doc in doc_iterator:\n        count_titles(doc, title_dict)\n    for token_stat in title_dict.values():\n        yield (token_stat.token, token_stat)\n\ndef make_title_dictionary(\n    text_doc,\n    title_output,\n    min_title_frequency,\n    max_title_dictionary_size\n):\n    \"\"\"Makes the title dictionary.\"\"\"\n    titles = text_doc\n      .mapPartitions(process_partition_for_titles)\n      .reduceByKey(tokenstat_reducer).values()\n    filtered_titles = titles.filter(\n      lambda x: x.frequency >= min_title_frequency)\n    all_titles = filtered_titles.collect()\n    sorted_title_dict = sorted(\n      all_titles, key=lambda x: x.frequency, reverse=True)\n    count = min(max_title_dictionary_size, len(sorted_title_dict))\n    for i in range(count):\n        sorted_title_dict[i].index = i\n    TokenDictionary.save(sorted_title_dict[:count], title_output)\n```", "```py\n// Co-occurrence matrix row.\nmessage CooccurrenceRow {\n    uint64 index = 1;\n    repeated uint64 other_index = 2;\n    repeated float count = 3;\n}\n```", "```py\nimport flax\nfrom flax import linen as nn\nfrom flax.training import train_state\nimport jax\nimport jax.numpy as jnp\n\nclass Glove(nn.Module):\n    \"\"\"A simple embedding model based on gloVe.\n https://nlp.stanford.edu/projects/glove/\n \"\"\"\n    num_embeddings: int = 1024\n    features: int = 64\n\n    def setup(self):\n        self._token_embedding = nn.Embed(self.num_embeddings,\n                                         self.features)\n        self._bias = nn.Embed(\n            self.num_embeddings, 1, embedding_init=flax.linen.initializers.zeros)\n\n    def __call__(self, inputs):\n        \"\"\"Calculates the approximate log count between tokens 1 and 2.\n Args:\n A batch of (token1, token2) integers representing co-occurence.\n Returns:\n Approximate log count between x and y.\n \"\"\"\n        token1, token2 = inputs\n        embed1 = self._token_embedding(token1)\n        bias1 = self._bias(token1)\n        embed2 = self._token_embedding(token2)\n        bias2 = self._bias(token2)\n        dot_vmap = jax.vmap(jnp.dot, in_axes=[0, 0], out_axes=0)\n        dot = dot_vmap(embed1, embed2)\n        output = dot + bias1 + bias2\n        return output\n\n    def score_all(self, token):\n        \"\"\"Finds the score of token vs all tokens.\n Args:\n max_count: The maximum count of tokens to return.\n token: Integer index of token to find neighbors of.\n Returns:\n Scores of nearest tokens.\n \"\"\"\n        embed1 = self._token_embedding(token)\n        all_tokens = jnp.arange(0, self.num_embeddings, 1, dtype=jnp.int32)\n        all_embeds = self._token_embedding(all_tokens)\n        dot_vmap = jax.vmap(jnp.dot, in_axes=[None, 0], out_axes=0)\n        scores = dot_vmap(embed1, all_embeds)\n        return scores\n```", "```py\n@jax.jit\ndef apply_model(state, inputs, target):\n    \"\"\"Computes the gradients and loss for a single batch.\"\"\"\n\n    # Define glove loss.\n    def glove_loss(params):\n        \"\"\"The GloVe weighted loss.\"\"\"\n        predicted = state.apply_fn({'params': params}, inputs)\n        ones = jnp.ones_like(target)\n        weight = jnp.minimum(ones, target / 100.0)\n        weight = jnp.power(weight, 0.75)\n        log_target = jnp.log10(1.0 + target)\n        loss = jnp.mean(jnp.square(log_target - predicted) * weight)\n        return loss\n\n    grad_fn = jax.value_and_grad(glove_loss)\n    loss, grads = grad_fn(state.params)\n\n    return grads, loss\n```"]