<html><head></head><body><section data-pdf-bookmark="Chapter 10. Low-Rank Methods" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch10">&#13;
<h1><span class="label">Chapter 10. </span>Low-Rank Methods</h1>&#13;
&#13;
&#13;
<p>In the preceding chapter, we lamented at the challenge of working with so many features. By letting each item be its own feature, we were able to express a lot of information about user preference and item-affinity correlations, but we were in big trouble in terms of the<a data-primary="curse of dimensionality" data-type="indexterm" id="id867"/><a data-primary="dimensionality" data-type="indexterm" id="id868"/> curse of dimensionality. Combine this with the reality of very sparse features, and you’re in danger. In this chapter, we’ll turn to smaller feature spaces. By representing users and items as low-dimensional vectors, we can capture the complex relationships between them in a more efficient and effective way. This allows us to generate more personalized and relevant recommendations for users while also reducing the computational complexity of the recommendation process.</p>&#13;
&#13;
<p>We will explore the use of low-dimensional embeddings and discuss the benefits and some of the implementation details of this approach. We will also look at code in JAX that uses modern gradient-based optimization to reduce the dimension of your item or user representations.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Latent Spaces" data-type="sect1"><div class="sect1" id="latent-spaces">&#13;
<h1>Latent Spaces</h1>&#13;
&#13;
<p>You<a data-primary="low-rank methods" data-secondary="low-rank methods" data-type="indexterm" id="LRMlatent10"/><a data-primary="latent space" data-secondary="low-rank methods" data-type="indexterm" id="latspace10"/> are already familiar with feature spaces, which are usually categorical or vector-valued direct representations of the data. This can be the raw red, green, and blue values of an image, counts of items in a histogram, or attributes of an object like length, width, and height. Latent features, on the other hand, do not represent any specific real value feature of the items but are initialized randomly and then learned to suit a task. The GloVe embeddings we discussed in <a data-type="xref" href="ch08.html#ch:wikipedia-e2e">Chapter 8</a> are one such example of a latent vector that was learned to represent the log count of words. Here we will cover more ways to generate these latent features or embeddings.</p>&#13;
<div data-type="tip"><h1>Focus on Your “Strangths”</h1>&#13;
<p>This chapter relies heavily on linear algebra, so it’s good to read up on vectors, dot products, and norms of vectors before proceeding. Having an understanding of matrices and the rank of matrices will also be useful. Consider <a href="https://oreil.ly/8MBN8"><em>Linear Algebra and Its Applications</em></a> by Gilbert Strang.</p>&#13;
</div>&#13;
&#13;
<p>One of the reasons latent spaces are so popular is that they are usually lower in dimension than the features they represent. For example, if the user-item rating matrix or interaction matrix (where the matrix entries are 1 if a user has interacted with an item) is <span><em>N</em> × <em>M</em></span> dimensional, then factorizing the matrix into latent factors of <span><em>N</em> × <em>K</em></span> and <span><em>K</em> × <em>M</em></span>, where <em>K</em> is much smaller than <em>N</em> or <em>M</em> is an approximation of the missing entries because we’re relaxing the factorization. <em>K</em> being smaller than <em>N</em> or <em>M</em> is usually called an<a data-primary="information bottleneck" data-type="indexterm" id="id869"/> <em>information bottleneck</em>—that is, we are forcing the matrix to be made up of a much smaller matrix. This means the ML model has to make up missing entries, which might be good for recommender systems. As long as users have interacted with enough similar items, by forcing the system to have a lot less capacity in terms of degrees of freedom, then factorization can completely reconstruct the matrix, and the missing entries tend to get filled by similar items.</p>&#13;
&#13;
<p>Let’s see what happens, for example, when we factor a user-item matrix of <span>4 × 4</span> into a <span>4 × 2</span> and a <span>2 × 4</span> vector using SVD.</p>&#13;
&#13;
<p>We are supplying a matrix whose rows are users and whose columns are items. For example, row 0 is <code>[1, 0, 0, 1]</code>, which means user 0 has selected item 0 and item 3. These can be ratings or purchases. Now let’s look at some code:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>&#13;
&#13;
<code class="n">a</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code>&#13;
    <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code> <code class="p">,</code><code class="mi">1</code><code class="p">],</code>&#13;
    <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code> <code class="p">,</code><code class="mi">0</code><code class="p">],</code>&#13;
    <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code>&#13;
    <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]]</code>&#13;
<code class="p">)</code>&#13;
&#13;
<code class="n">u</code><code class="p">,</code> <code class="n">s</code><code class="p">,</code> <code class="n">v</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linalg</code><code class="o">.</code><code class="n">svd</code><code class="p">(</code><code class="n">a</code><code class="p">,</code> <code class="n">full_matrices</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Set the last two eigenvalues to 0.</code>&#13;
<code class="n">s</code><code class="p">[</code><code class="mi">2</code><code class="p">:</code><code class="mi">4</code><code class="p">]</code> <code class="o">=</code> <code class="mi">0</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="n">s</code><code class="p">)</code>&#13;
<code class="n">b</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">u</code> <code class="o">*</code> <code class="n">s</code><code class="p">,</code> <code class="n">v</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="n">b</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># These are the eigenvalues with the smallest two set to 0.</code>&#13;
<code class="n">s</code> <code class="o">=</code> <code class="p">[</code><code class="mf">1.61803399</code> <code class="mf">1.61803399</code> <code class="mf">0.</code>         <code class="mf">0.</code>        <code class="p">]</code>&#13;
&#13;
<code class="c1"># This is the newly reconstructed matrix.</code>&#13;
<code class="n">b</code> <code class="o">=</code> <code class="p">[[</code><code class="mf">1.17082039</code> <code class="mf">0.</code>         <code class="mf">0.</code>         <code class="mf">0.7236068</code> <code class="p">]</code>&#13;
 <code class="p">[</code><code class="mf">0.7236068</code>  <code class="mf">0.</code>         <code class="mf">0.</code>         <code class="mf">0.4472136</code> <code class="p">]</code>&#13;
 <code class="p">[</code><code class="mf">0.</code>         <code class="mf">1.17082039</code> <code class="mf">0.7236068</code>  <code class="mf">0.</code>        <code class="p">]</code>&#13;
 <code class="p">[</code><code class="mf">0.</code>         <code class="mf">0.7236068</code>  <code class="mf">0.4472136</code>  <code class="mf">0.</code>        <code class="p">]]</code></pre>&#13;
&#13;
<p>Notice that the user in row 1 now has a score for an item in column 3, and the user in row 3 now has a positive score for the item in column 2. This phenomenon is generally known as<a data-primary="matrix completion" data-type="indexterm" id="id870"/> <em>matrix completion</em> and is a good property for recommender systems to have because now we get to recommend new items to users. The general method of forcing the ML to go through a bottleneck that is smaller than the size of the matrix that it is trying to reconstruct is known as a<a data-primary="low-rank methods" data-secondary="low-rank approximation" data-type="indexterm" id="id871"/> <em>low-rank approximation</em> because the rank of the approximation is 2 but the rank of the original user-item matrix is 4.</p>&#13;
<div data-type="note" epub:type="note"><h1>What Is the Rank of a Matrix?</h1>&#13;
<p>An <span><em>N</em> × <em>M</em></span> matrix may be considered as <em>N</em> row vectors (corresponding to users) and <em>M</em> column vectors (corresponding to items). When you consider the <em>N</em> vectors of dimension <em>M</em>, the <em>rank of the matrix</em> is the volume of the polyhedron defined by the <em>N</em> vectors in <em>M</em> dimensions. This is often different from the way we talk about the rank of matrices, however. While it’s the most natural and precise definition, we instead say it’s the “minimum number of dimensions necessary to represent the vectors of the matrix.”<a data-primary="" data-startref="LRMlatent10" data-type="indexterm" id="id872"/><a data-primary="" data-startref="latspace10" data-type="indexterm" id="id873"/></p>&#13;
</div>&#13;
&#13;
<p>We will cover SVD in more detail later in the chapter. This was just to whet your appetite to understand how latent spaces are related to recommender systems.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Dot Product Similarity" data-type="sect1"><div class="sect1" id="dot-product-similarity">&#13;
<h1>Dot Product Similarity</h1>&#13;
&#13;
<p>In <a data-type="xref" href="ch03.html#ch:math">Chapter 3</a> we introduced<a data-primary="low-rank methods" data-secondary="dot-product similarity" data-type="indexterm" id="id874"/><a data-primary="dot-product similarity" data-type="indexterm" id="id875"/><a data-primary="similarity" data-secondary="dot-product similarity" data-type="indexterm" id="id876"/> similarity measures, but now we return to the dot product in the context of similarity because of their increased importance in latent spaces. After all, latent spaces are built on the assumption that distance is similarity.</p>&#13;
&#13;
<p>The dot-product similarity is meaningful in recommendation systems because it provides a geometric interpretation of the relationship between users and items in the latent space (or potentially items and items, users and users, etc.). In the context of recommendation systems, the dot product can be seen as a projection of one vector onto another, indicating the degree of similarity or alignment between the user’s preferences and the item’s characteristics.</p>&#13;
&#13;
<p>To understand the geometric significance of the dot product, consider two vectors, <em>u</em> and <em>p</em>, representing the user and the product in the latent space, respectively. The dot product of these two vectors can be defined as follows:</p>&#13;
<div data-type="equation">&#13;
<math alttext="u times p equals StartAbsoluteValue EndAbsoluteValue u StartAbsoluteValue EndAbsoluteValue StartAbsoluteValue EndAbsoluteValue p StartAbsoluteValue EndAbsoluteValue c o s left-parenthesis theta right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <mi>u</mi>&#13;
    <mo>×</mo>&#13;
    <mi>p</mi>&#13;
    <mo>=</mo>&#13;
    <mo>|</mo>&#13;
    <mo>|</mo>&#13;
    <mi>u</mi>&#13;
    <mo>|</mo>&#13;
    <mo>|</mo>&#13;
    <mo>|</mo>&#13;
    <mo>|</mo>&#13;
    <mi>p</mi>&#13;
    <mo>|</mo>&#13;
    <mo>|</mo>&#13;
    <mi>c</mi>&#13;
    <mi>o</mi>&#13;
    <mi>s</mi>&#13;
    <mo>(</mo>&#13;
    <mi>θ</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Here, ||u|| and ||p|| represent the magnitudes of the user and product vectors, and θ is the angle between them. The dot product is thus a measure of the projection of one vector onto another, which is scaled by the magnitudes of both vectors.</p>&#13;
&#13;
<p>The<a data-primary="cosine similarity" data-type="indexterm" id="id877"/><a data-primary="similarity" data-secondary="cosine similarity" data-type="indexterm" id="id878"/> cosine similarity, which is another popular similarity measure in recommendation systems, is derived directly from the dot product:</p>&#13;
<div data-type="equation">&#13;
<math alttext="c o s i n e s i m i l a r i t y left-parenthesis u comma p right-parenthesis equals StartFraction left-parenthesis u times p right-parenthesis Over left-parenthesis StartAbsoluteValue EndAbsoluteValue u StartAbsoluteValue EndAbsoluteValue StartAbsoluteValue EndAbsoluteValue p StartAbsoluteValue EndAbsoluteValue right-parenthesis EndFraction" display="block">&#13;
  <mrow>&#13;
    <mi>c</mi>&#13;
    <mi>o</mi>&#13;
    <mi>s</mi>&#13;
    <mi>i</mi>&#13;
    <mi>n</mi>&#13;
    <mi>e</mi>&#13;
    <mspace width="0.277778em"/>&#13;
    <mi>s</mi>&#13;
    <mi>i</mi>&#13;
    <mi>m</mi>&#13;
    <mi>i</mi>&#13;
    <mi>l</mi>&#13;
    <mi>a</mi>&#13;
    <mi>r</mi>&#13;
    <mi>i</mi>&#13;
    <mi>t</mi>&#13;
    <mi>y</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>u</mi>&#13;
      <mo>,</mo>&#13;
      <mi>p</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mstyle displaystyle="true" scriptlevel="0">&#13;
      <mfrac><mrow><mo>(</mo><mi>u</mi><mo>×</mo><mi>p</mi><mo>)</mo></mrow> <mrow><mo>(</mo><mo>|</mo><mo>|</mo><mi>u</mi><mo>|</mo><mo>|</mo><mo>|</mo><mo>|</mo><mi>p</mi><mo>|</mo><mo>|</mo><mo>)</mo></mrow></mfrac>&#13;
    </mstyle>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>The cosine similarity ranges from –1 to 1, where –1 indicates completely dissimilar preferences and characteristics. A 0 indicates no similarity, and 1 indicates perfect alignment between the user’s preferences and the product’s characteristics. In the context of recommendation systems, cosine similarity provides a normalized measure of similarity that is invariant to the magnitudes of the user and product vectors. Note that the choice of using cosine similarity versus L2 distance depends on the type of embeddings you’re using and the way you optimize the computations. In practice, the only important feature is often the relative values.</p>&#13;
&#13;
<p>The geometric interpretation of the dot product (and cosine similarity) in recommendation systems is that it captures the alignment between user preferences and product characteristics. If the angle between the user and product vectors is small, the user’s preferences align well with the product’s characteristics, leading to a higher similarity score. Conversely, if the angle is large, the user’s preferences and product’s characteristics are dissimilar, resulting in a lower similarity score. By projecting user and item vectors onto each other, the dot-product similarity can capture the degree of alignment between user preferences and item characteristics, allowing the recommendation system to identify items that are most likely to be relevant and appealing to the user.</p>&#13;
&#13;
<p>Anecdotally, the dot product seems to capture<a data-primary="popularity" data-type="indexterm" id="id879"/> popularity, as very long vectors tend to be easy to project on anything that isn’t completely perpendicular or pointing away from them. As a result, a trade-off exists between frequently recommending popular items with a large vector length and longer-tail items that have smaller angular difference with cosine distance.</p>&#13;
&#13;
<p><a data-type="xref" href="#cosine_dot">Figure 10-1</a> considers two vectors, <em>a</em> and <em>b</em>. With cosine similarity, the vectors are unit length, so the angle is just the measure of similarity. However, with dot product, a very long vector like <em>c</em> might be considered more similar to <em>a</em> than <em>b</em> even though the angle between <em>a</em> and <em>b</em> is smaller because of the longer length of <em>c</em>. These long vectors tend to be very popular items that co-occur with many other items.</p>&#13;
&#13;
<figure><div class="figure" id="cosine_dot">&#13;
<img alt="Cosine vs Dot product similarity" src="assets/brpj_1001.png"/>&#13;
<h6><span class="label">Figure 10-1. </span>Cosine versus dot-product similarity</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Co-occurrence Models" data-type="sect1"><div class="sect1" id="id110">&#13;
<h1>Co-occurrence Models</h1>&#13;
&#13;
<p>In<a data-primary="low-rank methods" data-secondary="co-occurrence models" data-type="indexterm" id="id880"/><a data-primary="co-occurrence" data-secondary="co-occurrence models" data-type="indexterm" id="id881"/> our Wikipedia co-occurrences examples, we determined that the co-occurrence structure between two items could be used to generate measures of similarity. We covered how PMI can take the counts of co-occurrence and make recommendations based on very high mutual information between an item in the cart and others.</p>&#13;
&#13;
<p>As we’ve discussed, PMI is not a distance metric but still has important similarity measures based on co-occurrence. Let’s return to this topic.</p>&#13;
&#13;
<p>Recall from earlier that PMI is defined as follows:</p>&#13;
<div data-type="equation">&#13;
<math alttext="StartFraction p left-parenthesis x Subscript i Baseline comma x Subscript j Baseline right-parenthesis Over p left-parenthesis x Subscript i Baseline right-parenthesis asterisk p left-parenthesis x Subscript j Baseline right-parenthesis EndFraction equals StartFraction left-parenthesis upper C Subscript script upper I Baseline right-parenthesis Subscript x Sub Subscript i Subscript comma x Sub Subscript j Subscript Baseline asterisk number-sign left-parenthesis total interactions right-parenthesis Over number-sign left-parenthesis x Subscript i Baseline right-parenthesis asterisk number-sign left-parenthesis x Subscript j Baseline right-parenthesis EndFraction" display="block">&#13;
  <mrow>&#13;
    <mstyle displaystyle="true" scriptlevel="0">&#13;
      <mfrac><mrow><mi>p</mi><mo>(</mo><msub><mi>x</mi> <mi>i</mi> </msub><mo>,</mo><msub><mi>x</mi> <mi>j</mi> </msub><mo>)</mo></mrow> <mrow><mi>p</mi><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi> </msub><mo>)</mo></mrow><mo>*</mo><mi>p</mi><mrow><mo>(</mo><msub><mi>x</mi> <mi>j</mi> </msub><mo>)</mo></mrow></mrow></mfrac>&#13;
    </mstyle>&#13;
    <mo>=</mo>&#13;
    <mstyle displaystyle="true" scriptlevel="0">&#13;
      <mfrac><mrow><msub><mfenced close=")" open="(" separators=""><msub><mi>C</mi> <mi>ℐ</mi> </msub></mfenced> <mrow><msub><mi>x</mi> <mi>i</mi> </msub><mo>,</mo><msub><mi>x</mi> <mi>j</mi> </msub></mrow> </msub><mo>*</mo><mo>#</mo><mfenced close=")" open="(" separators=""><mtext>total</mtext><mspace width="4.pt"/><mtext>interactions</mtext></mfenced></mrow> <mrow><mo>#</mo><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi> </msub><mo>)</mo></mrow><mo>*</mo><mo>#</mo><mrow><mo>(</mo><msub><mi>x</mi> <mi>j</mi> </msub><mo>)</mo></mrow></mrow></mfrac>&#13;
    </mstyle>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Now let’s consider the<a data-primary="discrete co-occurrence distribution" data-type="indexterm" id="id882"/> <em>discrete co-occurrence distribution</em>, <math alttext="upper C upper D Subscript x Sub Subscript i">&#13;
  <mrow>&#13;
    <mi>C</mi>&#13;
    <msub><mi>D</mi> <msub><mi>x</mi> <mi>i</mi> </msub> </msub>&#13;
  </mrow>&#13;
</math>, defined as the collection of co-occurrences over all other <math alttext="x Subscript j">&#13;
  <msub><mi>x</mi> <mi>j</mi> </msub>&#13;
</math>:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper C upper D Subscript x Sub Subscript i Baseline equals left-parenthesis upper C Subscript script upper I Baseline right-parenthesis Subscript x Sub Subscript i Subscript comma x 1 Baseline comma ellipsis comma left-parenthesis upper C Subscript script upper I Baseline right-parenthesis Subscript x Sub Subscript i Subscript comma x Sub Subscript i Subscript Baseline comma ellipsis comma left-parenthesis upper C Subscript script upper I Baseline right-parenthesis Subscript x Sub Subscript i Subscript comma x Sub Subscript upper N Subscript Baseline" display="block">&#13;
  <mrow>&#13;
    <mi>C</mi>&#13;
    <msub><mi>D</mi> <msub><mi>x</mi> <mi>i</mi> </msub> </msub>&#13;
    <mo>=</mo>&#13;
    <mrow>&#13;
      <msub><mfenced close=")" open="(" separators=""><msub><mi>C</mi> <mi>ℐ</mi> </msub></mfenced> <mrow><msub><mi>x</mi> <mi>i</mi> </msub><mo>,</mo><msub><mi>x</mi> <mn>1</mn> </msub></mrow> </msub>&#13;
      <mo>,</mo>&#13;
      <mo>...</mo>&#13;
      <mo>,</mo>&#13;
      <msub><mfenced close=")" open="(" separators=""><msub><mi>C</mi> <mi>ℐ</mi> </msub></mfenced> <mrow><msub><mi>x</mi> <mi>i</mi> </msub><mo>,</mo><msub><mi>x</mi> <mi>i</mi> </msub></mrow> </msub>&#13;
      <mo>,</mo>&#13;
      <mo>...</mo>&#13;
      <mo>,</mo>&#13;
      <msub><mfenced close=")" open="(" separators=""><msub><mi>C</mi> <mi>ℐ</mi> </msub></mfenced> <mrow><msub><mi>x</mi> <mi>i</mi> </msub><mo>,</mo><msub><mi>x</mi> <mi>N</mi> </msub></mrow> </msub>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Here, <math alttext="j element-of 1 ellipsis upper N">&#13;
  <mrow>&#13;
    <mi>j</mi>&#13;
    <mo>∈</mo>&#13;
    <mn>1</mn>&#13;
    <mo>...</mo>&#13;
    <mi>N</mi>&#13;
  </mrow>&#13;
</math>, and <math alttext="upper N">&#13;
  <mi>N</mi>&#13;
</math> is the total number of items. This represents the co-occurrence histogram between <math alttext="x Subscript i">&#13;
  <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
</math> and all other items. By introducing this discrete distribution, we can utilize another tool: the Hellinger distance.</p>&#13;
&#13;
<p>We can measure distributional distance in a few ways, each with different advantages. For our discussion, we will not go deeply into the differences and will stick to the simplest but most appropriate. Hellinger distance is defined as follows:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper H left-parenthesis upper P comma upper Q right-parenthesis equals NestedStartRoot 1 minus sigma-summation Underscript i Overscript n Endscripts StartRoot p Subscript i Baseline q Subscript i Baseline EndRoot NestedEndRoot equals StartFraction 1 Over StartRoot 2 EndRoot EndFraction double-vertical-bar StartRoot upper P EndRoot minus StartRoot upper Q EndRoot double-vertical-bar Subscript 2" display="block">&#13;
  <mrow>&#13;
    <mi>H</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>P</mi>&#13;
      <mo>,</mo>&#13;
      <mi>Q</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <msqrt>&#13;
      <mrow>&#13;
        <mn>1</mn>&#13;
        <mo>-</mo>&#13;
        <msubsup><mo>∑</mo> <mi>i</mi> <mi>n</mi> </msubsup>&#13;
        <msqrt>&#13;
          <mrow>&#13;
            <msub><mi>p</mi> <mi>i</mi> </msub>&#13;
            <msub><mi>q</mi> <mi>i</mi> </msub>&#13;
          </mrow>&#13;
        </msqrt>&#13;
      </mrow>&#13;
    </msqrt>&#13;
    <mo>=</mo>&#13;
    <mstyle displaystyle="true" scriptlevel="0">&#13;
      <mfrac><mn>1</mn> <msqrt><mn>2</mn></msqrt></mfrac>&#13;
    </mstyle>&#13;
    <msub><mfenced close="∥" open="∥" separators=""><msqrt><mi>P</mi></msqrt><mo>-</mo><msqrt><mi>Q</mi></msqrt></mfenced> <mn>2</mn> </msub>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p><math alttext="upper P equals mathematical left-angle p Subscript i Baseline mathematical right-angle">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mo>=</mo>&#13;
    <mfenced close="〉" open="〈" separators="">&#13;
      <msub><mi>p</mi> <mi>i</mi> </msub>&#13;
    </mfenced>&#13;
  </mrow>&#13;
</math> and <math alttext="upper Q equals mathematical left-angle q Subscript i Baseline mathematical right-angle">&#13;
  <mrow>&#13;
    <mi>Q</mi>&#13;
    <mo>=</mo>&#13;
    <mfenced close="〉" open="〈" separators="">&#13;
      <msub><mi>q</mi> <mi>i</mi> </msub>&#13;
    </mfenced>&#13;
  </mrow>&#13;
</math> are two probability density vectors. In our setting, <math alttext="upper P">&#13;
  <mi>P</mi>&#13;
</math> and <math alttext="upper Q">&#13;
  <mi>Q</mi>&#13;
</math> can be <math alttext="upper C upper D Subscript x Sub Subscript i">&#13;
  <mrow>&#13;
    <mi>C</mi>&#13;
    <msub><mi>D</mi> <msub><mi>x</mi> <mi>i</mi> </msub> </msub>&#13;
  </mrow>&#13;
</math> and <math alttext="upper C upper D Subscript x Sub Subscript j">&#13;
  <mrow>&#13;
    <mi>C</mi>&#13;
    <msub><mi>D</mi> <msub><mi>x</mi> <mi>j</mi> </msub> </msub>&#13;
  </mrow>&#13;
</math>.</p>&#13;
&#13;
<p>The motivation behind this process is that we now have a proper distance between items purely based on co-occurrences. We can use any dimension transformation or reduction on this geometry. Later we will show dimension-reduction techniques that can use an arbitrary distance matrix and reduce the space to a lower-dimensional embedding that approximates it.</p>&#13;
<div data-type="warning" epub:type="warning"><h1>What About Measure Spaces and Information Theory?</h1>&#13;
<p>While we’re discussing distributions, you may find yourself wondering, “Is there a distance between distributions such that distributions are points in a latent space?” Oh, you weren’t wondering that? Well, OK. We’ll address it anyway.</p>&#13;
&#13;
<p>The short answer is that we can measure the differences between distributions. The most popular is<a data-primary="Kullback–Leibler (KL) divergence" data-type="indexterm" id="id883"/> Kullback–Leibler (KL) divergence, which is usually described in a Bayesian sense as the amount of surprise in seeing the distribution P, when expecting the distribution Q. However, KL is not a proper distance metric because it is asymmetric.</p>&#13;
&#13;
<p>Another <a data-primary="Hellinger distance" data-type="indexterm" id="id884"/> symmetric distance metric that has some nice properties is the Hellinger distance. Hellinger distance is effectively the <span class="keep-together">2-norm</span> measure theoretic distance. Additionally, Hellinger distance naturally generalizes to discrete distributions.</p>&#13;
&#13;
<p>If this still hasn’t scratched your itch for abstraction, we can also consider the total variation distance, which is the limit in the space of Fisher’s exact distance measures, which really means that it has all the nice properties of a distance of two distributions and no measure would ever consider them more dissimilar. Well, all the nice properties except for one: it’s not smooth. If you also want smoothness for differentiability, you’ll need to approximate it via an offset.</p>&#13;
&#13;
<p>If you ever need a distance between distributions, just use Hellinger.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Reducing the Rank of a Recommender Problem" data-type="sect1"><div class="sect1" id="id111">&#13;
<h1>Reducing the Rank of a Recommender Problem</h1>&#13;
&#13;
<p>We’ve<a data-primary="low-rank methods" data-secondary="reducing rank of recommender problems" data-type="indexterm" id="LRMreducing10"/> shown that as the number of items and users grows, we rapidly increase the dimensionality of our recommender problem. Because we’re representing each item and user as a column or vector, this scales like <math alttext="n squared">&#13;
  <msup><mi>n</mi> <mn>2</mn> </msup>&#13;
</math>. One way to push back against this difficulty is by rank reduction; recall our previous discussions about rank reduction<a data-primary="dimensionality reduction" data-secondary="factorization" data-type="indexterm" id="id885"/><a data-primary="factorization" data-type="indexterm" id="id886"/> via factorization.</p>&#13;
&#13;
<p>Like many integers, many matrices can be <em>factored</em> into smaller matrices; for integers, <em>smaller</em> means of smaller value, and for matrices, <em>smaller</em> means of smaller dimensions. When we factor an <math alttext="upper N times upper M">&#13;
  <mrow>&#13;
    <mi>N</mi>&#13;
    <mo>×</mo>&#13;
    <mi>M</mi>&#13;
  </mrow>&#13;
</math> matrix, we will be looking for two matrices <math alttext="upper U Subscript upper N times d">&#13;
  <msub><mi>U</mi> <mrow><mi>N</mi><mo>×</mo><mi>d</mi></mrow> </msub>&#13;
</math> and <math alttext="upper V Subscript d times upper M">&#13;
  <msub><mi>V</mi> <mrow><mi>d</mi><mo>×</mo><mi>M</mi></mrow> </msub>&#13;
</math>; note that when you multiply matrices together, they must share a dimension, and that dimension is eliminated, leaving the other two. Here, we’ll consider MFs when <math alttext="d less-than-or-equal-to upper N">&#13;
  <mrow>&#13;
    <mi>d</mi>&#13;
    <mo>≤</mo>&#13;
    <mi>N</mi>&#13;
  </mrow>&#13;
</math> and <math alttext="d less-than-or-equal-to upper M">&#13;
  <mrow>&#13;
    <mi>d</mi>&#13;
    <mo>≤</mo>&#13;
    <mi>M</mi>&#13;
  </mrow>&#13;
</math>. By factorizing a matrix, we ask for two matrices that together equal, or approximate, the original matrix:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper A Subscript i comma j Baseline asymptotically-equals mathematical left-angle upper U Subscript i Baseline comma upper V Subscript j Baseline mathematical right-angle" display="block">&#13;
  <mrow>&#13;
    <msub><mi>A</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow> </msub>&#13;
    <mo>≃</mo>&#13;
    <mfenced close="〉" open="〈" separators="">&#13;
      <msub><mi>U</mi> <mi>i</mi> </msub>&#13;
      <mo>,</mo>&#13;
      <msub><mi>V</mi> <mi>j</mi> </msub>&#13;
    </mfenced>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>We seek a small value for <math alttext="d">&#13;
  <mi>d</mi>&#13;
</math> to reduce the number of latent dimensions. As you may have already noticed, each of the matrices <math alttext="upper U Subscript upper N times d">&#13;
  <msub><mi>U</mi> <mrow><mi>N</mi><mo>×</mo><mi>d</mi></mrow> </msub>&#13;
</math> and <math alttext="upper V Subscript d times upper M">&#13;
  <msub><mi>V</mi> <mrow><mi>d</mi><mo>×</mo><mi>M</mi></mrow> </msub>&#13;
</math> will correspond to rows or columns of the original ratings matrix. However, they’re expressed in fewer dimensions. This<a data-primary="low-dimensional latent space" data-type="indexterm" id="id887"/> utilizes the idea of a <em>low-dimensional latent space</em>. Intuitively, a latent space seeks to represent the same relationships as the full <math alttext="upper N times upper M">&#13;
  <mrow>&#13;
    <mi>N</mi>&#13;
    <mo>×</mo>&#13;
    <mi>M</mi>&#13;
  </mrow>&#13;
</math> dimensional relationships in two sets of relationships: items versus latent features, and users versus latent features.</p>&#13;
&#13;
<p>These methods are also popular in other kinds of ML, but for our case, we’ll primarily be looking at factorizing ratings or interaction matrices.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id888">&#13;
<h1>MF via SVD</h1>&#13;
<p>SVD<a data-primary="matrix factorization (MF)" data-secondary="via SVD" data-secondary-sortas="SVD" data-type="indexterm" id="id889"/><a data-primary="singular value decomposition (SVD)" data-type="indexterm" id="id890"/> and MF are closely related, but SVD is an important special case. The key difference is in how the factorization is done and the types of matrices that each can be applied to.</p>&#13;
&#13;
<p><a data-type="xref" href="#svd">Figure 10-2</a> shows how SVD works. The eigenvectors <math alttext="e Baseline 1">&#13;
  <mrow>&#13;
    <mi>e</mi>&#13;
    <mn>1</mn>&#13;
  </mrow>&#13;
</math> and <math alttext="e Baseline 2">&#13;
  <mrow>&#13;
    <mi>e</mi>&#13;
    <mn>2</mn>&#13;
  </mrow>&#13;
</math> correspond to the largest two eigenvalues. <math alttext="e Baseline 1">&#13;
  <mrow>&#13;
    <mi>e</mi>&#13;
    <mn>1</mn>&#13;
  </mrow>&#13;
</math> explains more of the data than <math alttext="e Baseline 2">&#13;
  <mrow>&#13;
    <mi>e</mi>&#13;
    <mn>2</mn>&#13;
  </mrow>&#13;
</math> and lies along the direction of the largest spread of points. Eigenvectors are always perpendicular to each other, so their dot product is always 0.</p>&#13;
&#13;
<figure><div class="figure" id="svd">&#13;
<img alt="Singular Value Decomposition" src="assets/brpj_1002.png"/>&#13;
<h6><span class="label">Figure 10-2. </span>Singular value decomposition</h6>&#13;
</div></figure>&#13;
&#13;
<p>SVD is a specific type of MF that decomposes a matrix into three separate matrices: a left singular matrix, a diagonal matrix, and a right singular matrix. SVD can be applied to any real-valued matrix, but it is particularly well suited to dense matrices with many nonzero entries; additionally, SVD matrices have properties useful for extracting particular kinds of relationships between latent features. The columns and rows of the singular matrices are the eigenvectors, and the values in the diagonal matrix are the eigenvalues. This decomposition is handy in seeing how much information in the original space is explained by the eigenvectors and corresponds to the magnitude of the eigenvalue, so an eigenvector with a larger eigenvalue explains more of the original data than an eigenvector with a correspondingly smaller eigenvalue.</p>&#13;
&#13;
<p>MF decomposes a user-item matrix into two matrices that represent the preferences of users and the characteristics of items. This allows the recommendation system to generate personalized recommendations by matching the preferences of users with the characteristics of items.</p>&#13;
</div></aside>&#13;
&#13;
<p>Frequently, you must overcome a few<a data-primary="matrix factorization (MF)" data-secondary="challenges of" data-type="indexterm" id="id891"/> challenges when considering MF:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The matrix you wish to factor is sparse and often non-negative and/or binary.</p>&#13;
</li>&#13;
<li>&#13;
<p>The number of nonzero elements in each item vector can vary wildly, as we saw in the Matthew effect<a data-primary="Matthew effect" data-type="indexterm" id="id892"/>.</p>&#13;
</li>&#13;
<li>&#13;
<p>Factorizing matrices is cubic in complexity.</p>&#13;
</li>&#13;
<li>&#13;
<p>SVD and other full-rank methods don’t work without imputation, which itself is complicated.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>We’ll address these with some alternative optimization methods.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Optimizing for MF with ALS" data-type="sect2"><div class="sect2" id="id112">&#13;
<h2>Optimizing for MF with ALS</h2>&#13;
&#13;
<p>The<a data-primary="dimensionality reduction" data-secondary="optimizing for MF with ALS" data-type="indexterm" id="RRoptim10"/><a data-primary="optimization" data-secondary="dimensionality reduction for MF with ALS" data-type="indexterm" id="Orank10"/><a data-primary="matrix factorization (MF)" data-secondary="optimizing with ALS" data-type="indexterm" id="MFoptim10"/><a data-primary="alternating least squares (ALS)" data-type="indexterm" id="id893"/> basic optimization we wish to execute is to approximate as follows:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper A Subscript i comma j Baseline asymptotically-equals mathematical left-angle upper U Subscript i Baseline comma upper V Subscript j Baseline mathematical right-angle" display="block">&#13;
  <mrow>&#13;
    <msub><mi>A</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow> </msub>&#13;
    <mo>≃</mo>&#13;
    <mfenced close="〉" open="〈" separators="">&#13;
      <msub><mi>U</mi> <mi>i</mi> </msub>&#13;
      <mo>,</mo>&#13;
      <msub><mi>V</mi> <mi>j</mi> </msub>&#13;
    </mfenced>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Notably, if you wish to optimize matrix entries directly, you’ll need to optimize <math alttext="d squared asterisk upper N asterisk upper M">&#13;
  <mrow>&#13;
    <msup><mi>d</mi> <mn>2</mn> </msup>&#13;
    <mo>*</mo>&#13;
    <mi>N</mi>&#13;
    <mo>*</mo>&#13;
    <mi>M</mi>&#13;
  </mrow>&#13;
</math> elements simultaneously corresponding to the numbers of parameters in these factorizations. We can easily achieve a significant speedup, however, by alternating between tuning one matrix or the other. This is called <em>alternating least squares</em>, commonly <em>ALS</em>, and it is a common approach to this problem. Instead of back-propagating updates to all terms in both matrices on each pass, you may update only one of the two matrices, which dramatically reduces the number of computations that need to take place.</p>&#13;
&#13;
<p>ALS seeks to switch back and forth between <math alttext="upper U">&#13;
  <mi>U</mi>&#13;
</math> and <math alttext="upper V">&#13;
  <mi>V</mi>&#13;
</math>, evaluating on the same loss function but updating the weights in only one matrix at a time:</p>&#13;
<div data-type="equation">&#13;
<math alttext="StartLayout 1st Row 1st Column Blank 2nd Column upper U left-arrow upper U minus eta asterisk upper U asterisk nabla upper U asterisk script upper D left-parenthesis upper A comma upper U upper V right-parenthesis 2nd Row 1st Column Blank 2nd Column upper V left-arrow upper V minus eta asterisk upper V asterisk nabla upper V asterisk script upper D left-parenthesis upper A comma upper U upper V right-parenthesis EndLayout" display="block">&#13;
  <mtable displaystyle="true">&#13;
    <mtr>&#13;
      <mtd/>&#13;
      <mtd columnalign="left">&#13;
        <mrow>&#13;
          <mi>U</mi>&#13;
          <mo>←</mo>&#13;
          <mi>U</mi>&#13;
          <mo>-</mo>&#13;
          <mi>η</mi>&#13;
          <mo>*</mo>&#13;
          <mi>U</mi>&#13;
          <mo>*</mo>&#13;
          <mi>∇</mi>&#13;
          <mi>U</mi>&#13;
          <mo>*</mo>&#13;
          <mi>𝒟</mi>&#13;
          <mfenced close=")" open="(" separators="">&#13;
            <mi>A</mi>&#13;
            <mo>,</mo>&#13;
            <mi>U</mi>&#13;
            <mi>V</mi>&#13;
          </mfenced>&#13;
        </mrow>&#13;
      </mtd>&#13;
    </mtr>&#13;
    <mtr>&#13;
      <mtd/>&#13;
      <mtd columnalign="left">&#13;
        <mrow>&#13;
          <mi>V</mi>&#13;
          <mo>←</mo>&#13;
          <mi>V</mi>&#13;
          <mo>-</mo>&#13;
          <mi>η</mi>&#13;
          <mo>*</mo>&#13;
          <mi>V</mi>&#13;
          <mo>*</mo>&#13;
          <mi>∇</mi>&#13;
          <mi>V</mi>&#13;
          <mo>*</mo>&#13;
          <mi>𝒟</mi>&#13;
          <mfenced close=")" open="(" separators="">&#13;
            <mi>A</mi>&#13;
            <mo>,</mo>&#13;
            <mi>U</mi>&#13;
            <mi>V</mi>&#13;
          </mfenced>&#13;
        </mrow>&#13;
      </mtd>&#13;
    </mtr>&#13;
  </mtable>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Here, <math alttext="eta">&#13;
  <mi>η</mi>&#13;
</math> is the learning rate and <math alttext="script upper D">&#13;
  <mi>𝒟</mi>&#13;
</math> is our chosen distance function. We’ll present more details of this distance function momentarily. Before we move on, let’s consider a few of the intricacies here:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Each of these update rules requires the gradient with respect to the relevant factor matrix.</p>&#13;
</li>&#13;
<li>&#13;
<p>We update an entire factor matrix at a time, but we evaluate loss on the product of the factor matrices versus the original matrix.</p>&#13;
</li>&#13;
<li>&#13;
<p>We have a mysterious distance function.</p>&#13;
</li>&#13;
<li>&#13;
<p>By the way that we’ve constructed this optimization, we’re implicitly assuming that we’ll use this process to converge two well-approximating matrices (we often also impose a limit on the number of iterations).</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>In JAX, these optimizations will be straightforward to implement, and we’ll see how similar the equational forms and the JAX code look.</p>&#13;
<div data-type="tip"><h1>Distance Between Matrices</h1>&#13;
<p>We can determine the distance between two matrices in a variety of ways. As we’ve seen before, different distance measurements for vectors yield different interpretations from the underlying space. We won’t have as many complications for these computations, but it’s worth a short observation. The most obvious approach is one you’ve already seen,<a data-primary="observed mean square error (OMSE)" data-type="indexterm" id="id894"/> <em>observed mean squared error</em>:</p>&#13;
&#13;
<p><math alttext="StartFraction sigma-summation Underscript normal upper Omega Endscripts left-parenthesis upper A Subscript i comma j Baseline minus mathematical left-angle upper U Subscript i Baseline comma upper V Subscript j Baseline mathematical right-angle right-parenthesis squared Over 1 normal upper Omega vertical-bar EndFraction">&#13;
  <mstyle displaystyle="true" scriptlevel="0">&#13;
    <mfrac><mrow><msub><mo>∑</mo> <mi>Ω</mi> </msub><msup><mfenced close=")" open="(" separators=""><msub><mi>A</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow> </msub><mo>-</mo><mfenced close="〉" open="〈" separators=""><msub><mi>U</mi> <mi>i</mi> </msub><mo>,</mo><msub><mi>V</mi> <mi>j</mi> </msub></mfenced></mfenced> <mn>2</mn> </msup></mrow> <mrow><mn>1</mn><mi>Ω</mi><mo>|</mo></mrow></mfrac>&#13;
  </mstyle>&#13;
</math></p>&#13;
&#13;
<p>One useful alternative to the observed mean squared error can be used when you have a single nonzero entry for a user vector (alternatively, a max rating). In that case, you could instead use a cross-entropy loss, which provides a <em>logistic MF</em>, and thus a probability estimate. For more details on how to implement this, see the <a href="https://oreil.ly/7qWy6">“Matrix Factorization for Recommender Systems” tutorial</a> by Kyle Chung.</p>&#13;
</div>&#13;
&#13;
<p class="less_space pagebreak-before">In our observed ratings, we expect (and see!) a large number of missing values and some item vectors with an overrepresented number of ratings. This suggests that we should consider nonuniformly weighted matrices. Next we’ll discuss how to account for this and other variants with regularization.<a data-primary="" data-startref="RRoptim10" data-type="indexterm" id="id895"/><a data-primary="" data-startref="MFoptim10" data-type="indexterm" id="id896"/><a data-primary="" data-startref="Orank10" data-type="indexterm" id="id897"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Regularization for MF" data-type="sect2"><div class="sect2" id="id113">&#13;
<h2>Regularization for MF</h2>&#13;
&#13;
<p><em>Weighted alternating least squares</em> (WALS) is<a data-primary="WALS (weighted alternating least squares)" data-type="indexterm" id="id898"/><a data-primary="weighted alternating least squares (WALS)" data-type="indexterm" id="id899"/><a data-primary="matrix factorization (MF)" data-secondary="regularization for" data-type="indexterm" id="id900"/><a data-primary="regularization" data-type="indexterm" id="id901"/><a data-primary="dimensionality reduction" data-secondary="regularization for MF" data-type="indexterm" id="id902"/> similar to ALS but attempts to resolve these two data issues more gracefully. In WALS, the weight assigned to each observed rating is inversely proportional to the number of observed ratings for the user or item. Therefore, observed ratings for users or items with few ratings are given more weight in the optimization process.</p>&#13;
&#13;
<p>We can apply these weights as a regularization parameter in our eventual loss <span class="keep-together">function:</span></p>&#13;
<div data-type="equation">&#13;
<math alttext="StartFraction sigma-summation Underscript normal upper Omega Endscripts left-parenthesis upper A Subscript i comma j Baseline minus less-than upper U Subscript i Baseline comma upper V Subscript j Baseline greater-than right-parenthesis squared Over StartAbsoluteValue normal upper Omega EndAbsoluteValue EndFraction plus StartFraction 1 Over upper N EndFraction sigma-summation StartAbsoluteValue upper U EndAbsoluteValue" display="block">&#13;
  <mrow>&#13;
    <mstyle displaystyle="true" scriptlevel="0">&#13;
      <mfrac><mrow><msub><mo>∑</mo> <mi>Ω</mi> </msub><msup><mfenced close=")" open="(" separators=""><msub><mi>A</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow> </msub><mo>-</mo><mo>&lt;</mo><msub><mi>U</mi> <mi>i</mi> </msub><mo>,</mo><msub><mi>V</mi> <mi>j</mi> </msub><mo>&gt;</mo></mfenced> <mn>2</mn> </msup></mrow> <mrow><mo>|</mo><mi>Ω</mi><mo>|</mo></mrow></mfrac>&#13;
    </mstyle>&#13;
    <mo>+</mo>&#13;
    <mstyle displaystyle="true" scriptlevel="0">&#13;
      <mfrac><mn>1</mn> <mi>N</mi></mfrac>&#13;
    </mstyle>&#13;
    <mo>∑</mo>&#13;
    <mrow>&#13;
      <mo>|</mo>&#13;
      <mi>U</mi>&#13;
      <mo>|</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Other regularization methods are important, and also popular, for MF. We’ll discuss these two powerful regularization techniques:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Weight decay</p>&#13;
</li>&#13;
<li>&#13;
<p>Gramian regularization</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>As is often the case,<a data-primary="weight decay" data-type="indexterm" id="id903"/> <em>weight decay</em> is our <math alttext="l squared">&#13;
  <msup><mi>l</mi> <mn>2</mn> </msup>&#13;
</math> regularization, which in this case is at the level of the Frobenius norm, i.e., the magnitude of the weight matrix. An elegant way to view this weight decay is that it’s minimizing the magnitude of the <em>singular values</em>.</p>&#13;
&#13;
<p>Similarly, MF has another regularization technique that looks very standard but is quite different in calculation. This is via the<a data-primary="Gramian regularization" data-type="indexterm" id="id904"/> <em>Gramians</em>—essentially regularizing the size of the individual matrix entries, but there’s an elegant trick for the optimization. In particular, a Gramian of a matrix <math alttext="upper U">&#13;
  <mi>U</mi>&#13;
</math> is the product <math alttext="upper U Superscript upper T Baseline upper U">&#13;
  <mrow>&#13;
    <msup><mi>U</mi> <mi>T</mi> </msup>&#13;
    <mi>U</mi>&#13;
  </mrow>&#13;
</math>. The eagle-eyed among you may recognize this term as the same term we previously used to calculate co-occurrences for binary matrices. The connection is that both are simply trying to find efficient representations of dot products between a matrix’s rows and columns.</p>&#13;
&#13;
<p>These regularizations are the<a data-primary="Frobenius terms" data-type="indexterm" id="id905"/> Frobenius terms:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper R left-parenthesis upper U comma upper V right-parenthesis equals StartFraction 1 Over upper N EndFraction sigma-summation Underscript i Overscript upper N Endscripts StartAbsoluteValue upper U Subscript i Baseline EndAbsoluteValue Subscript 2 Superscript 2 Baseline plus StartFraction 1 Over upper M EndFraction sigma-summation Underscript j Overscript upper M Endscripts StartAbsoluteValue upper V Subscript j Baseline EndAbsoluteValue Subscript 2 Superscript 2" display="block">&#13;
  <mstyle displaystyle="true" scriptlevel="0">&#13;
    <mrow>&#13;
      <mi>R</mi>&#13;
      <mrow>&#13;
        <mo>(</mo>&#13;
        <mi>U</mi>&#13;
        <mo>,</mo>&#13;
        <mi>V</mi>&#13;
        <mo>)</mo>&#13;
      </mrow>&#13;
      <mo>=</mo>&#13;
      <mfrac><mn>1</mn> <mi>N</mi></mfrac>&#13;
      <munderover><mo>∑</mo> <mi>i</mi> <mi>N</mi> </munderover>&#13;
      <mrow>&#13;
        <mo>|</mo>&#13;
      </mrow>&#13;
      <msub><mi>U</mi> <mi>i</mi> </msub>&#13;
      <msubsup><mrow><mo>|</mo></mrow> <mn>2</mn> <mn>2</mn> </msubsup>&#13;
      <mo>+</mo>&#13;
      <mfrac><mn>1</mn> <mi>M</mi></mfrac>&#13;
      <munderover><mo>∑</mo> <mi>j</mi> <mi>M</mi> </munderover>&#13;
      <msubsup><mrow><mo>|</mo><msub><mi>V</mi> <mi>j</mi> </msub><mo>|</mo></mrow> <mn>2</mn> <mn>2</mn> </msubsup>&#13;
    </mrow>&#13;
  </mstyle>&#13;
</math>&#13;
</div>&#13;
&#13;
<p class="less_space pagebreak-before">Or expanded, the equation looks like this:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper R left-parenthesis upper U comma upper V right-parenthesis equals StartFraction 1 Over upper N EndFraction sigma-summation Underscript i Overscript upper N Endscripts sigma-summation Underscript k Overscript d Endscripts upper U Subscript i comma k Superscript 2 Baseline plus StartFraction 1 Over upper M EndFraction sigma-summation Underscript j Overscript upper M Endscripts sigma-summation Underscript l Overscript d Endscripts upper V Subscript j comma l Superscript 2" display="block">&#13;
  <mstyle displaystyle="true" scriptlevel="0">&#13;
    <mrow>&#13;
      <mi>R</mi>&#13;
      <mrow>&#13;
        <mo>(</mo>&#13;
        <mi>U</mi>&#13;
        <mo>,</mo>&#13;
        <mi>V</mi>&#13;
        <mo>)</mo>&#13;
      </mrow>&#13;
      <mo>=</mo>&#13;
      <mfrac><mn>1</mn> <mi>N</mi></mfrac>&#13;
      <munderover><mo>∑</mo> <mi>i</mi> <mi>N</mi> </munderover>&#13;
      <munderover><mo>∑</mo> <mi>k</mi> <mi>d</mi> </munderover>&#13;
      <msubsup><mi>U</mi> <mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow> <mn>2</mn> </msubsup>&#13;
      <mo>+</mo>&#13;
      <mfrac><mn>1</mn> <mi>M</mi></mfrac>&#13;
      <munderover><mo>∑</mo> <mi>j</mi> <mi>M</mi> </munderover>&#13;
      <munderover><mo>∑</mo> <mi>l</mi> <mi>d</mi> </munderover>&#13;
      <msubsup><mi>V</mi> <mrow><mi>j</mi><mo>,</mo><mi>l</mi></mrow> <mn>2</mn> </msubsup>&#13;
    </mrow>&#13;
  </mstyle>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>And here are the Gramian terms:</p>&#13;
<div data-type="equation">&#13;
<math alttext="StartLayout 1st Row 1st Column upper G left-parenthesis upper U comma upper V right-parenthesis 2nd Column colon equals StartFraction 1 Over upper N dot upper M EndFraction sigma-summation Underscript i Overscript upper N Endscripts sigma-summation Underscript j Overscript upper M Endscripts mathematical left-angle upper U Subscript i Baseline comma upper V Subscript j Baseline mathematical right-angle squared 2nd Row 1st Column Blank 3rd Row 1st Column Blank 2nd Column equals StartFraction 1 Over upper N dot upper M EndFraction asterisk sigma-summation Underscript k comma l Overscript d Endscripts left-parenthesis upper U Superscript upper T Baseline upper U asterisk upper V Superscript upper T Baseline upper V right-parenthesis Subscript k comma l Baseline EndLayout period" display="block">&#13;
  <mrow>&#13;
    <mtable>&#13;
      <mtr>&#13;
        <mtd columnalign="left">&#13;
          <mrow>&#13;
            <mi>G</mi>&#13;
            <mo>(</mo>&#13;
            <mi>U</mi>&#13;
            <mo>,</mo>&#13;
            <mi>V</mi>&#13;
            <mo>)</mo>&#13;
          </mrow>&#13;
        </mtd>&#13;
        <mtd columnalign="left">&#13;
          <mstyle displaystyle="true" scriptlevel="0">&#13;
            <mrow>&#13;
              <mo>:</mo>&#13;
              <mo>=</mo>&#13;
              <mfrac><mn>1</mn> <mrow><mi>N</mi><mo>·</mo><mi>M</mi></mrow></mfrac>&#13;
              <munderover><mo>∑</mo> <mi>i</mi> <mi>N</mi> </munderover>&#13;
              <munderover><mo>∑</mo> <mi>j</mi> <mi>M</mi> </munderover>&#13;
              <msup><mfenced close="〉" open="〈" separators=""><msub><mi>U</mi> <mi>i</mi> </msub><mo>,</mo><msub><mi>V</mi> <mi>j</mi> </msub></mfenced> <mn>2</mn> </msup>&#13;
            </mrow>&#13;
          </mstyle>&#13;
        </mtd>&#13;
      </mtr>&#13;
      <mtr>&#13;
        <mtd/>&#13;
      </mtr>&#13;
      <mtr>&#13;
        <mtd/>&#13;
        <mtd columnalign="left">&#13;
          <mstyle displaystyle="true" scriptlevel="0">&#13;
            <mrow>&#13;
              <mo>=</mo>&#13;
              <mfrac><mn>1</mn> <mrow><mi>N</mi><mo>·</mo><mi>M</mi></mrow></mfrac>&#13;
              <mo>*</mo>&#13;
              <munderover><mo>∑</mo> <mrow><mi>k</mi><mo>,</mo><mi>l</mi></mrow> <mi>d</mi> </munderover>&#13;
              <msub><mfenced close=")" open="(" separators=""><msup><mi>U</mi> <mi>T</mi> </msup><mi>U</mi><mo>*</mo><msup><mi>V</mi> <mi>T</mi> </msup><mi>V</mi></mfenced> <mrow><mi>k</mi><mo>,</mo><mi>l</mi></mrow> </msub>&#13;
            </mrow>&#13;
          </mstyle>&#13;
        </mtd>&#13;
      </mtr>&#13;
    </mtable>&#13;
    <mo>.</mo>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Finally, we have our loss function:</p>&#13;
<div data-type="equation">&#13;
<math alttext="StartFraction 1 Over StartAbsoluteValue normal upper Omega EndAbsoluteValue EndFraction sigma-summation Underscript left-parenthesis i comma j right-parenthesis element-of normal upper Omega Endscripts left-parenthesis upper A Subscript i j Baseline minus mathematical left-angle upper U Subscript i Baseline comma upper V Subscript j Baseline mathematical right-angle right-parenthesis squared plus lamda Subscript upper R Baseline left-parenthesis StartFraction 1 Over upper N EndFraction sigma-summation Underscript i Overscript upper N Endscripts sigma-summation Underscript k Overscript d Endscripts upper U Subscript i comma k Superscript 2 Baseline plus StartFraction 1 Over upper M EndFraction sigma-summation Underscript j Overscript upper M Endscripts sigma-summation Underscript l Overscript d Endscripts upper V Subscript j comma l Superscript 2 Baseline right-parenthesis plus lamda Subscript upper G Baseline left-parenthesis StartFraction 1 Over upper N dot upper M EndFraction asterisk sigma-summation Underscript k comma l Overscript d Endscripts left-parenthesis upper U Superscript upper T Baseline upper U asterisk upper V Superscript upper T Baseline upper V right-parenthesis Subscript k comma l Baseline right-parenthesis" display="block">&#13;
  <mstyle displaystyle="true" scriptlevel="0">&#13;
    <mrow>&#13;
      <mfrac><mn>1</mn> <mrow><mo>|</mo><mi>Ω</mi><mo>|</mo></mrow></mfrac>&#13;
      <munder><mo>∑</mo> <mrow><mo>(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo>)</mo><mo>∈</mo><mi>Ω</mi></mrow> </munder>&#13;
      <msup><mrow><mo>(</mo><msub><mi>A</mi> <mrow><mi>i</mi><mi>j</mi></mrow> </msub><mo>-</mo><mrow><mo>〈</mo><msub><mi>U</mi> <mi>i</mi> </msub><mo>,</mo><msub><mi>V</mi> <mi>j</mi> </msub><mo>〉</mo></mrow><mo>)</mo></mrow> <mn>2</mn> </msup>&#13;
      <mo>+</mo>&#13;
      <msub><mi>λ</mi> <mi>R</mi> </msub>&#13;
      <mfenced close=")" open="(" separators="">&#13;
        <mfrac><mn>1</mn> <mi>N</mi></mfrac>&#13;
        <munderover><mo>∑</mo> <mi>i</mi> <mi>N</mi> </munderover>&#13;
        <munderover><mo>∑</mo> <mi>k</mi> <mi>d</mi> </munderover>&#13;
        <msubsup><mi>U</mi> <mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow> <mn>2</mn> </msubsup>&#13;
        <mo>+</mo>&#13;
        <mfrac><mn>1</mn> <mi>M</mi></mfrac>&#13;
        <munderover><mo>∑</mo> <mi>j</mi> <mi>M</mi> </munderover>&#13;
        <munderover><mo>∑</mo> <mi>l</mi> <mi>d</mi> </munderover>&#13;
        <msubsup><mi>V</mi> <mrow><mi>j</mi><mo>,</mo><mi>l</mi></mrow> <mn>2</mn> </msubsup>&#13;
      </mfenced>&#13;
      <mo>+</mo>&#13;
      <msub><mi>λ</mi> <mi>G</mi> </msub>&#13;
      <mfenced close=")" open="(" separators="">&#13;
        <mfrac><mn>1</mn> <mrow><mi>N</mi><mo>·</mo><mi>M</mi></mrow></mfrac>&#13;
        <mo>*</mo>&#13;
        <munderover><mo>∑</mo> <mrow><mi>k</mi><mo>,</mo><mi>l</mi></mrow> <mi>d</mi> </munderover>&#13;
        <msub><mfenced close=")" open="(" separators=""><msup><mi>U</mi> <mi>T</mi> </msup><mi>U</mi><mo>*</mo><msup><mi>V</mi> <mi>T</mi> </msup><mi>V</mi></mfenced> <mrow><mi>k</mi><mo>,</mo><mi>l</mi></mrow> </msub>&#13;
      </mfenced>&#13;
    </mrow>&#13;
  </mstyle>&#13;
</math>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Regularized MF Implementation" data-type="sect2"><div class="sect2" id="id114">&#13;
<h2>Regularized MF Implementation</h2>&#13;
&#13;
<p>So<a data-primary="regularized matrix factorization" data-type="indexterm" id="regmatfacto10"/><a data-primary="dimensionality reduction" data-secondary="regularized MF implementation" data-type="indexterm" id="RRimpl10"/><a data-primary="JAX framework" data-secondary="regularized MF implementation" data-type="indexterm" id="JAXregMF10"/><a data-primary="matrix factorization (MF)" data-secondary="regularized MF implementation" data-type="indexterm" id="MRimple10"/> far, we’ve written a <em>lot</em> of math symbols, but all of those symbols have allowed us to arrive at a model that is extremely powerful. <em>Regularized matrix factorization</em> is an effective model for medium-sized recommender problems. This model type is still in production for many serious businesses. One classic issue with MF implementations is performance, but because we’re working in JAX, which has extremely native GPU support, our implementation can actually be much more compact than what you may find in something like a <a href="https://oreil.ly/U-K-V">PyTorch example</a>.</p>&#13;
&#13;
<p>Let’s work through how this model would look to predict ratings for a user-item matrix via this doubly regularized model with Gramians.</p>&#13;
&#13;
<p>First we’ll do the simple setup. This will assume your ratings matrix is already on wandb:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">jax</code>&#13;
<code class="kn">import</code> <code class="nn">jax.numpy</code> <code class="k">as</code> <code class="nn">jnp</code>&#13;
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>&#13;
<code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>&#13;
<code class="kn">import</code> <code class="nn">os</code><code class="o">,</code> <code class="nn">json</code><code class="o">,</code> <code class="nn">wandb</code><code class="o">,</code> <code class="nn">math</code>&#13;
&#13;
<code class="kn">from</code> <code class="nn">jax</code> <code class="kn">import</code> <code class="n">grad</code><code class="p">,</code> <code class="n">jit</code>&#13;
<code class="kn">from</code> <code class="nn">jax</code> <code class="kn">import</code> <code class="n">random</code>&#13;
<code class="kn">from</code> <code class="nn">jax.experimental</code> <code class="kn">import</code> <code class="n">sparse</code>&#13;
&#13;
<code class="n">key</code> <code class="o">=</code> <code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>&#13;
&#13;
<code class="n">wandb</code><code class="o">.</code><code class="n">login</code><code class="p">()</code>&#13;
<code class="n">run</code> <code class="o">=</code> <code class="n">wandb</code><code class="o">.</code><code class="n">init</code><code class="p">(</code>&#13;
    <code class="c1"># Set entity to specify your username or team name</code>&#13;
    <code class="n">entity</code><code class="o">=</code><code class="s2">"wandb-un"</code><code class="p">,</code>&#13;
    <code class="c1"># Set the project where this run will be logged</code>&#13;
    <code class="n">project</code><code class="o">=</code><code class="s2">"jax-mf"</code><code class="p">,</code>&#13;
    <code class="c1"># associate the runs to the right dataset</code>&#13;
    <code class="n">config</code><code class="o">=</code><code class="p">{</code>&#13;
      <code class="s2">"dataset"</code><code class="p">:</code> <code class="s2">"MF-Dataset"</code><code class="p">,</code>&#13;
    <code class="p">}</code>&#13;
<code class="p">)</code>&#13;
&#13;
<code class="c1"># note that we assume the dataset is a ratings table stored in wandb</code>&#13;
<code class="n">artifact</code> <code class="o">=</code> <code class="n">run</code><code class="o">.</code><code class="n">use_artifact</code><code class="p">(</code><code class="s1">'stored-dataset:latest'</code><code class="p">)</code>&#13;
<code class="n">ratings_artifact</code> <code class="o">=</code> <code class="n">artifact</code><code class="o">.</code><code class="n">download</code><code class="p">()</code>&#13;
<code class="n">ratings_artifact_blob</code> <code class="o">=</code> <code class="n">json</code><code class="o">.</code><code class="n">load</code><code class="p">(</code>&#13;
    <code class="nb">open</code><code class="p">(</code>&#13;
        <code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">join</code><code class="p">(</code>&#13;
            <code class="n">ratings_artifact</code><code class="p">,</code>&#13;
            <code class="s1">'ratings.table.json'</code>&#13;
        <code class="p">)</code>&#13;
    <code class="p">)</code>&#13;
<code class="p">)</code>&#13;
&#13;
<code class="n">ratings_artifact_blob</code><code class="o">.</code><code class="n">keys</code><code class="p">()</code>&#13;
<code class="c1"># ['_type', 'column_types', 'columns', 'data', 'ncols', 'nrows']</code>&#13;
&#13;
<code class="n">ratings</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code> <code class="c1"># user_id, item_id, rating, unix_timestamp</code>&#13;
    <code class="n">data</code><code class="o">=</code><code class="n">ratings_artifact_blob</code><code class="p">[</code><code class="s1">'data'</code><code class="p">],</code>&#13;
    <code class="n">columns</code><code class="o">=</code><code class="n">ratings_artifact_blob</code><code class="p">[</code><code class="s1">'columns'</code><code class="p">]</code>&#13;
<code class="p">)</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">start_pipeline</code><code class="p">(</code><code class="n">df</code><code class="p">):</code>&#13;
    <code class="k">return</code> <code class="n">df</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">column_as_type</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="n">column</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code> <code class="n">cast_type</code><code class="p">):</code>&#13;
    <code class="n">df</code><code class="p">[</code><code class="n">column</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="n">column</code><code class="p">]</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="n">cast_type</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="n">df</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">rename_column_value</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="n">target_column</code><code class="p">,</code> <code class="n">prior_val</code><code class="p">,</code> <code class="n">post_val</code><code class="p">):</code>&#13;
    <code class="n">df</code><code class="p">[</code><code class="n">target_column</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="n">target_column</code><code class="p">]</code><code class="o">.</code><code class="n">replace</code><code class="p">({</code><code class="n">prior_val</code><code class="p">:</code> <code class="n">post_val</code><code class="p">})</code>&#13;
    <code class="k">return</code> <code class="n">df</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">split_dataframe</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="n">holdout_fraction</code><code class="o">=</code><code class="mf">0.1</code><code class="p">):</code>&#13;
    <code class="sd">"""Splits a DataFrame into training and test sets.</code>&#13;
<code class="sd">    Args:</code>&#13;
<code class="sd">      df: a dataframe.</code>&#13;
<code class="sd">      holdout_fraction: fraction of dataframe rows to use in the test set.</code>&#13;
<code class="sd">    Returns:</code>&#13;
<code class="sd">      train: dataframe for training</code>&#13;
<code class="sd">      test: dataframe for testing</code>&#13;
<code class="sd">    """</code>&#13;
    <code class="n">test</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="n">frac</code><code class="o">=</code><code class="n">holdout_fraction</code><code class="p">,</code> <code class="n">replace</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>&#13;
    <code class="n">train</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="o">~</code><code class="n">df</code><code class="o">.</code><code class="n">index</code><code class="o">.</code><code class="n">isin</code><code class="p">(</code><code class="n">test</code><code class="o">.</code><code class="n">index</code><code class="p">)]</code>&#13;
    <code class="k">return</code> <code class="n">train</code><code class="p">,</code> <code class="n">test</code>&#13;
&#13;
<code class="n">all_rat</code> <code class="o">=</code> <code class="p">(</code><code class="n">ratings</code>&#13;
    <code class="o">.</code><code class="n">pipe</code><code class="p">(</code><code class="n">start_pipeline</code><code class="p">)</code>&#13;
    <code class="o">.</code><code class="n">pipe</code><code class="p">(</code><code class="n">column_as_type</code><code class="p">,</code> <code class="n">column</code><code class="o">=</code><code class="s1">'user_id'</code><code class="p">,</code> <code class="n">cast_type</code><code class="o">=</code><code class="nb">int</code><code class="p">)</code>&#13;
    <code class="o">.</code><code class="n">pipe</code><code class="p">(</code><code class="n">column_as_type</code><code class="p">,</code> <code class="n">column</code><code class="o">=</code><code class="s1">'item_id'</code><code class="p">,</code> <code class="n">cast_type</code><code class="o">=</code><code class="nb">int</code><code class="p">)</code>&#13;
<code class="p">)</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">ratings_to_sparse_array</code><code class="p">(</code><code class="n">ratings_df</code><code class="p">,</code> <code class="n">user_dim</code><code class="p">,</code> <code class="n">item_dim</code><code class="p">):</code>&#13;
    <code class="n">indices</code> <code class="o">=</code> <code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">ratings_df</code><code class="p">[</code><code class="s1">'user_id'</code><code class="p">]),</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">ratings_df</code><code class="p">[</code><code class="s1">'item_id'</code><code class="p">]))</code>&#13;
    <code class="n">values</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">ratings_df</code><code class="p">[</code><code class="s1">'rating'</code><code class="p">])</code>&#13;
&#13;
    <code class="k">return</code> <code class="p">{</code>&#13;
        <code class="s1">'indices'</code><code class="p">:</code> <code class="n">indices</code><code class="p">,</code>&#13;
        <code class="s1">'values'</code><code class="p">:</code> <code class="n">values</code><code class="p">,</code>&#13;
        <code class="s1">'shape'</code><code class="p">:</code> <code class="p">[</code><code class="n">user_dim</code><code class="p">,</code> <code class="n">item_dim</code><code class="p">]</code>&#13;
    <code class="p">}</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">random_normal</code><code class="p">(</code><code class="n">pr_key</code><code class="p">,</code> <code class="n">shape</code><code class="p">,</code> <code class="n">mu</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">sigma</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="p">):</code>&#13;
    <code class="k">return</code> <code class="p">(</code><code class="n">mu</code> <code class="o">+</code> <code class="n">sigma</code> <code class="o">*</code> <code class="n">random</code><code class="o">.</code><code class="n">normal</code><code class="p">(</code><code class="n">pr_key</code><code class="p">,</code> <code class="n">shape</code><code class="o">=</code><code class="n">shape</code><code class="p">))</code>&#13;
&#13;
<code class="n">x</code> <code class="o">=</code> <code class="n">random_normal</code><code class="p">(</code>&#13;
    <code class="n">pr_key</code> <code class="o">=</code> <code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="mi">1701</code><code class="p">),</code>&#13;
    <code class="n">shape</code><code class="o">=</code><code class="p">(</code><code class="mi">10000</code><code class="p">,),</code>&#13;
    <code class="n">mu</code> <code class="o">=</code> <code class="mf">1.0</code><code class="p">,</code>&#13;
    <code class="n">sigma</code> <code class="o">=</code> <code class="mf">3.0</code><code class="p">,</code>&#13;
<code class="p">)</code> <code class="c1"># these hyperparameters are pretty meaningless</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">sp_mse_loss</code><code class="p">(</code><code class="n">A</code><code class="p">,</code> <code class="n">params</code><code class="p">):</code>&#13;
    <code class="n">U</code><code class="p">,</code> <code class="n">V</code> <code class="o">=</code> <code class="n">params</code><code class="p">[</code><code class="s1">'users'</code><code class="p">],</code> <code class="n">params</code><code class="p">[</code><code class="s1">'items'</code><code class="p">]</code>&#13;
    <code class="n">rows</code><code class="p">,</code> <code class="n">columns</code> <code class="o">=</code> <code class="n">A</code><code class="p">[</code><code class="s1">'indices'</code><code class="p">]</code>&#13;
    <code class="n">estimator</code> <code class="o">=</code> <code class="o">-</code><code class="p">(</code><code class="n">U</code> <code class="o">@</code> <code class="n">V</code><code class="o">.</code><code class="n">T</code><code class="p">)[(</code><code class="n">rows</code><code class="p">,</code> <code class="n">columns</code><code class="p">)]</code>&#13;
    <code class="n">square_err</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">tree_map</code><code class="p">(</code>&#13;
        <code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code><code class="o">**</code><code class="mi">2</code><code class="p">,</code>&#13;
        <code class="n">A</code><code class="p">[</code><code class="s1">'values'</code><code class="p">]</code><code class="o">+</code><code class="n">estimator</code>&#13;
    <code class="p">)</code>&#13;
    <code class="k">return</code> <code class="n">jnp</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">square_err</code><code class="p">)</code>&#13;
&#13;
<code class="n">omse_loss</code> <code class="o">=</code> <code class="n">jit</code><code class="p">(</code><code class="n">sp_mse_loss</code><code class="p">)</code></pre>&#13;
&#13;
<p>Note that we’ve had to implement our own loss function here. This is a relatively straightforward<a data-primary="mean square error (MSE) loss" data-type="indexterm" id="id906"/><a data-primary="MSE (mean square error) loss" data-type="indexterm" id="id907"/> mean square error (MSE) loss, but it’s taking advantage of the sparse nature of our matrix. You may notice in the code that we’ve converted the matrix to a sparse representation, so it’s important that our loss function cannot only take advantage of that representation, but also be written to utilize the JAX device arrays and mapping/jitting.</p>&#13;
<div data-type="tip"><h1>Is That Loss Function Really Right?</h1>&#13;
<p>If you’re curious about this loss function that appeared like magic, we understand. While writing this book, we were extremely uncertain about what the best implementation of this loss function that leverages JAX would look like. There are actually many reasonable approaches to this kind of optimization. To that end, we wrote a public experiment to benchmark several approaches <a href="https://oreil.ly/6zwEX">on Colab</a>.</p>&#13;
</div>&#13;
&#13;
<p>Next, we need to build model objects to handle our MF state as we train. This code, while essentially mostly template code, will set us up well to feed the model into a training loop in a relatively memory-efficient way. This model was trained on 100 million entries for a few thousand epochs on a MacBook Pro in less than a day:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">class</code> <code class="nc">CFModel</code><code class="p">(</code><code class="nb">object</code><code class="p">):</code>&#13;
    <code class="sd">"""Simple class that represents a collaborative filtering model"""</code>&#13;
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code>&#13;
          <code class="bp">self</code><code class="p">,</code>&#13;
          <code class="n">metrics</code><code class="p">:</code> <code class="nb">dict</code><code class="p">,</code>&#13;
          <code class="n">embeddings</code><code class="p">:</code> <code class="nb">dict</code><code class="p">,</code>&#13;
          <code class="n">ground_truth</code><code class="p">:</code> <code class="nb">dict</code><code class="p">,</code>&#13;
          <code class="n">embeddings_parameters</code><code class="p">:</code> <code class="nb">dict</code><code class="p">,</code>&#13;
          <code class="n">prng_key</code><code class="o">=</code><code class="kc">None</code>&#13;
    <code class="p">):</code>&#13;
        <code class="sd">"""Initializes a CFModel.</code>&#13;
<code class="sd">        Args:</code>&#13;
<code class="sd">        """</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">_metrics</code> <code class="o">=</code> <code class="n">metrics</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">_embeddings</code> <code class="o">=</code> <code class="n">embeddings</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">_ground_truth</code> <code class="o">=</code> <code class="n">ground_truth</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">_embeddings_parameters</code> <code class="o">=</code> <code class="n">embeddings_parameters</code>&#13;
&#13;
        <code class="k">if</code> <code class="n">prng_key</code> <code class="ow">is</code> <code class="kc">None</code><code class="p">:</code>&#13;
            <code class="n">prng_key</code> <code class="o">=</code> <code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">_prng_key</code> <code class="o">=</code> <code class="n">prng_key</code>&#13;
&#13;
&#13;
    <code class="nd">@property</code>&#13;
    <code class="k">def</code> <code class="nf">embeddings</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>&#13;
        <code class="sd">"""The embeddings dictionary."""</code>&#13;
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">_embeddings</code>&#13;
&#13;
    <code class="nd">@embeddings</code><code class="o">.</code><code class="n">setter</code>&#13;
    <code class="k">def</code> <code class="nf">embeddings</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">value</code><code class="p">):</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">_embeddings</code> <code class="o">=</code> <code class="n">value</code>&#13;
&#13;
    <code class="nd">@property</code>&#13;
    <code class="k">def</code> <code class="nf">metrics</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>&#13;
        <code class="sd">"""The metrics dictionary."""</code>&#13;
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">_metrics</code>&#13;
&#13;
    <code class="nd">@property</code>&#13;
    <code class="k">def</code> <code class="nf">ground_truth</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>&#13;
        <code class="sd">"""The train/test dictionary."""</code>&#13;
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">_ground_truth</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">reset_embeddings</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>&#13;
        <code class="sd">"""Clear out embeddings state."""</code>&#13;
&#13;
        <code class="n">prng_key1</code><code class="p">,</code> <code class="n">prng_key2</code> <code class="o">=</code> <code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">_prng_key</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>&#13;
&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">_embeddings</code><code class="p">[</code><code class="s1">'users'</code><code class="p">]</code> <code class="o">=</code> <code class="n">random_normal</code><code class="p">(</code>&#13;
            <code class="n">prng_key1</code><code class="p">,</code>&#13;
            <code class="p">[</code>&#13;
              <code class="bp">self</code><code class="o">.</code><code class="n">_embeddings_parameters</code><code class="p">[</code><code class="s1">'user_dim'</code><code class="p">],</code>&#13;
              <code class="bp">self</code><code class="o">.</code><code class="n">_embeddings_parameters</code><code class="p">[</code><code class="s1">'embedding_dim'</code><code class="p">]</code>&#13;
            <code class="p">],</code>&#13;
            <code class="n">mu</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code>&#13;
            <code class="n">sigma</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">_embeddings_parameters</code><code class="p">[</code><code class="s1">'init_stddev'</code><code class="p">],</code>&#13;
        <code class="p">)</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">_embeddings</code><code class="p">[</code><code class="s1">'items'</code><code class="p">]</code> <code class="o">=</code> <code class="n">random_normal</code><code class="p">(</code>&#13;
            <code class="n">prng_key2</code><code class="p">,</code>&#13;
            <code class="p">[</code>&#13;
              <code class="bp">self</code><code class="o">.</code><code class="n">_embeddings_parameters</code><code class="p">[</code><code class="s1">'item_dim'</code><code class="p">],</code>&#13;
              <code class="bp">self</code><code class="o">.</code><code class="n">_embeddings_parameters</code><code class="p">[</code><code class="s1">'embedding_dim'</code><code class="p">]],</code>&#13;
            <code class="n">mu</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code>&#13;
            <code class="n">sigma</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">_embeddings_parameters</code><code class="p">[</code><code class="s1">'init_stddev'</code><code class="p">],</code>&#13;
        <code class="p">)</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">model_constructor</code><code class="p">(</code>&#13;
    <code class="n">ratings_df</code><code class="p">,</code>&#13;
    <code class="n">user_dim</code><code class="p">,</code>&#13;
    <code class="n">item_dim</code><code class="p">,</code>&#13;
    <code class="n">embedding_dim</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code>&#13;
    <code class="n">init_stddev</code><code class="o">=</code><code class="mf">1.</code><code class="p">,</code>&#13;
    <code class="n">holdout_fraction</code><code class="o">=</code><code class="mf">0.2</code><code class="p">,</code>&#13;
    <code class="n">prng_key</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code>&#13;
    <code class="n">train_set</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code>&#13;
    <code class="n">test_set</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code>&#13;
<code class="p">):</code>&#13;
    <code class="k">if</code> <code class="n">prng_key</code> <code class="ow">is</code> <code class="kc">None</code><code class="p">:</code>&#13;
      <code class="n">prng_key</code> <code class="o">=</code> <code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>&#13;
&#13;
    <code class="n">prng_key1</code><code class="p">,</code> <code class="n">prng_key2</code> <code class="o">=</code> <code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">prng_key</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>&#13;
&#13;
    <code class="k">if</code> <code class="p">(</code><code class="n">train_set</code> <code class="ow">is</code> <code class="kc">None</code><code class="p">)</code> <code class="ow">and</code> <code class="p">(</code><code class="n">test_set</code> <code class="ow">is</code> <code class="kc">None</code><code class="p">):</code>&#13;
        <code class="n">train</code><code class="p">,</code> <code class="n">test</code> <code class="o">=</code> <code class="p">(</code><code class="n">ratings_df</code>&#13;
            <code class="o">.</code><code class="n">pipe</code><code class="p">(</code><code class="n">start_pipeline</code><code class="p">)</code>&#13;
            <code class="o">.</code><code class="n">pipe</code><code class="p">(</code><code class="n">split_dataframe</code><code class="p">,</code> <code class="n">holdout_fraction</code><code class="o">=</code><code class="n">holdout_fraction</code><code class="p">)</code>&#13;
        <code class="p">)</code>&#13;
&#13;
        <code class="n">A_train</code> <code class="o">=</code> <code class="p">(</code><code class="n">train</code>&#13;
            <code class="o">.</code><code class="n">pipe</code><code class="p">(</code><code class="n">start_pipeline</code><code class="p">)</code>&#13;
            <code class="o">.</code><code class="n">pipe</code><code class="p">(</code><code class="n">ratings_to_sparse_array</code><code class="p">,</code> <code class="n">user_dim</code><code class="o">=</code><code class="n">user_dim</code><code class="p">,</code> <code class="n">item_dim</code><code class="o">=</code><code class="n">item_dim</code><code class="p">)</code>&#13;
        <code class="p">)</code>&#13;
        <code class="n">A_test</code> <code class="o">=</code> <code class="p">(</code><code class="n">test</code>&#13;
            <code class="o">.</code><code class="n">pipe</code><code class="p">(</code><code class="n">start_pipeline</code><code class="p">)</code>&#13;
            <code class="o">.</code><code class="n">pipe</code><code class="p">(</code><code class="n">ratings_to_sparse_array</code><code class="p">,</code> <code class="n">user_dim</code><code class="o">=</code><code class="n">user_dim</code><code class="p">,</code> <code class="n">item_dim</code><code class="o">=</code><code class="n">item_dim</code><code class="p">)</code>&#13;
        <code class="p">)</code>&#13;
    <code class="k">elif</code> <code class="p">(</code><code class="n">train_set</code> <code class="ow">is</code> <code class="kc">None</code><code class="p">)</code> <code class="o">^</code> <code class="p">(</code><code class="n">test_set</code> <code class="ow">is</code> <code class="kc">None</code><code class="p">):</code>&#13;
        <code class="k">raise</code><code class="p">(</code><code class="s1">'Must send train and test if sending one'</code><code class="p">)</code>&#13;
    <code class="k">else</code><code class="p">:</code>&#13;
        <code class="n">A_train</code><code class="p">,</code> <code class="n">A_test</code> <code class="o">=</code> <code class="n">train_set</code><code class="p">,</code> <code class="n">test_set</code>&#13;
&#13;
    <code class="n">U</code> <code class="o">=</code> <code class="n">random_normal</code><code class="p">(</code>&#13;
        <code class="n">prng_key1</code><code class="p">,</code>&#13;
        <code class="p">[</code><code class="n">user_dim</code><code class="p">,</code> <code class="n">embedding_dim</code><code class="p">],</code>&#13;
        <code class="n">mu</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code>&#13;
        <code class="n">sigma</code><code class="o">=</code><code class="n">init_stddev</code><code class="p">,</code>&#13;
    <code class="p">)</code>&#13;
    <code class="n">V</code> <code class="o">=</code> <code class="n">random_normal</code><code class="p">(</code>&#13;
        <code class="n">prng_key2</code><code class="p">,</code>&#13;
        <code class="p">[</code><code class="n">item_dim</code><code class="p">,</code> <code class="n">embedding_dim</code><code class="p">],</code>&#13;
        <code class="n">mu</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code>&#13;
        <code class="n">sigma</code><code class="o">=</code><code class="n">init_stddev</code><code class="p">,</code>&#13;
    <code class="p">)</code>&#13;
&#13;
&#13;
    <code class="n">train_loss</code> <code class="o">=</code> <code class="n">omse_loss</code><code class="p">(</code><code class="n">A_train</code><code class="p">,</code> <code class="p">{</code><code class="s1">'users'</code><code class="p">:</code> <code class="n">U</code><code class="p">,</code> <code class="s1">'items'</code><code class="p">:</code> <code class="n">V</code><code class="p">})</code>&#13;
    <code class="n">test_loss</code> <code class="o">=</code> <code class="n">omse_loss</code><code class="p">(</code><code class="n">A_test</code><code class="p">,</code> <code class="p">{</code><code class="s1">'users'</code><code class="p">:</code> <code class="n">U</code><code class="p">,</code> <code class="s1">'items'</code><code class="p">:</code> <code class="n">V</code><code class="p">})</code>&#13;
&#13;
    <code class="n">metrics</code> <code class="o">=</code> <code class="p">{</code>&#13;
        <code class="s1">'train_error'</code><code class="p">:</code> <code class="n">train_loss</code><code class="p">,</code>&#13;
        <code class="s1">'test_error'</code><code class="p">:</code> <code class="n">test_loss</code>&#13;
    <code class="p">}</code>&#13;
    <code class="n">embeddings</code> <code class="o">=</code> <code class="p">{</code><code class="s1">'users'</code><code class="p">:</code> <code class="n">U</code><code class="p">,</code> <code class="s1">'items'</code><code class="p">:</code> <code class="n">V</code><code class="p">}</code>&#13;
    <code class="n">ground_truth</code> <code class="o">=</code> <code class="p">{</code>&#13;
        <code class="s2">"A_train"</code><code class="p">:</code> <code class="n">A_train</code><code class="p">,</code>&#13;
        <code class="s2">"A_test"</code><code class="p">:</code> <code class="n">A_test</code>&#13;
    <code class="p">}</code>&#13;
    <code class="k">return</code> <code class="n">CFModel</code><code class="p">(</code>&#13;
        <code class="n">metrics</code><code class="o">=</code><code class="n">metrics</code><code class="p">,</code>&#13;
        <code class="n">embeddings</code><code class="o">=</code><code class="n">embeddings</code><code class="p">,</code>&#13;
        <code class="n">ground_truth</code><code class="o">=</code><code class="n">ground_truth</code><code class="p">,</code>&#13;
        <code class="n">embeddings_parameters</code><code class="o">=</code><code class="p">{</code>&#13;
            <code class="s1">'user_dim'</code><code class="p">:</code> <code class="n">user_dim</code><code class="p">,</code>&#13;
            <code class="s1">'item_dim'</code><code class="p">:</code> <code class="n">item_dim</code><code class="p">,</code>&#13;
            <code class="s1">'embedding_dim'</code><code class="p">:</code> <code class="n">embedding_dim</code><code class="p">,</code>&#13;
            <code class="s1">'init_stddev'</code><code class="p">:</code> <code class="n">init_stddev</code><code class="p">,</code>&#13;
        <code class="p">},</code>&#13;
        <code class="n">prng_key</code><code class="o">=</code><code class="n">prng_key</code><code class="p">,</code>&#13;
    <code class="p">)</code>&#13;
&#13;
<code class="n">mf_model</code> <code class="o">=</code> <code class="n">model_constructor</code><code class="p">(</code><code class="n">all_rat</code><code class="p">,</code> <code class="n">user_count</code><code class="p">,</code> <code class="n">item_count</code><code class="p">)</code></pre>&#13;
&#13;
<p>We should also set this up to log nicely to wandb so it’s easy to understand what is happening during training:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">train</code><code class="p">():</code>&#13;
  <code class="n">run_config</code> <code class="o">=</code> <code class="p">{</code> <code class="c1"># These will be hyperparameters we will tune via wandb</code>&#13;
      <code class="s1">'emb_dim'</code><code class="p">:</code> <code class="mi">10</code><code class="p">,</code> <code class="c1"># Latent dimension</code>&#13;
      <code class="s1">'prior_std'</code><code class="p">:</code> <code class="mf">0.1</code><code class="p">,</code> <code class="c1"># Std dev around 0 for weights initialization</code>&#13;
      <code class="s1">'alpha'</code><code class="p">:</code> <code class="mf">1.0</code><code class="p">,</code> <code class="c1"># Learning rate</code>&#13;
      <code class="s1">'steps'</code><code class="p">:</code> <code class="mi">1500</code><code class="p">,</code> <code class="c1"># Number of training steps</code>&#13;
  <code class="p">}</code>&#13;
&#13;
  <code class="k">with</code> <code class="n">wandb</code><code class="o">.</code><code class="n">init</code><code class="p">()</code> <code class="k">as</code> <code class="n">run</code><code class="p">:</code>&#13;
    <code class="n">run_config</code><code class="o">.</code><code class="n">update</code><code class="p">(</code><code class="n">run</code><code class="o">.</code><code class="n">config</code><code class="p">)</code>&#13;
    <code class="n">model_object</code> <code class="o">=</code> <code class="n">model_constructor</code><code class="p">(</code>&#13;
        <code class="n">ratings_df</code><code class="o">=</code><code class="n">all_rat</code><code class="p">,</code>&#13;
        <code class="n">user_dim</code><code class="o">=</code><code class="n">user_count</code><code class="p">,</code>&#13;
        <code class="n">item_dim</code><code class="o">=</code><code class="n">item_count</code><code class="p">,</code>&#13;
        <code class="n">embedding_dim</code><code class="o">=</code><code class="n">run_config</code><code class="p">[</code><code class="s1">'emb_dim'</code><code class="p">],</code>&#13;
        <code class="n">init_stddev</code><code class="o">=</code><code class="n">run_config</code><code class="p">[</code><code class="s1">'prior_std'</code><code class="p">],</code>&#13;
        <code class="n">prng_key</code><code class="o">=</code><code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="mi">0</code><code class="p">),</code>&#13;
        <code class="n">train_set</code><code class="o">=</code><code class="n">mf_model</code><code class="o">.</code><code class="n">ground_truth</code><code class="p">[</code><code class="s1">'A_train'</code><code class="p">],</code>&#13;
        <code class="n">test_set</code><code class="o">=</code><code class="n">mf_model</code><code class="o">.</code><code class="n">ground_truth</code><code class="p">[</code><code class="s1">'A_test'</code><code class="p">]</code>&#13;
    <code class="p">)</code>&#13;
    <code class="n">model_object</code><code class="o">.</code><code class="n">reset_embeddings</code><code class="p">()</code> <code class="c1"># Ensure we are starting from priors</code>&#13;
    <code class="n">alpha</code><code class="p">,</code> <code class="n">steps</code> <code class="o">=</code> <code class="n">run_config</code><code class="p">[</code><code class="s1">'alpha'</code><code class="p">],</code> <code class="n">run_config</code><code class="p">[</code><code class="s1">'steps'</code><code class="p">]</code>&#13;
    <code class="nb">print</code><code class="p">(</code><code class="n">run_config</code><code class="p">)</code>&#13;
    <code class="n">grad_fn</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">value_and_grad</code><code class="p">(</code><code class="n">omse_loss</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>&#13;
    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">steps</code><code class="p">):</code>&#13;
      <code class="c1"># We perform one gradient update</code>&#13;
      <code class="n">loss_val</code><code class="p">,</code> <code class="n">grads</code> <code class="o">=</code> <code class="n">grad_fn</code><code class="p">(</code>&#13;
          <code class="n">model_object</code><code class="o">.</code><code class="n">ground_truth</code><code class="p">[</code><code class="s1">'A_train'</code><code class="p">],</code>&#13;
          <code class="n">model_object</code><code class="o">.</code><code class="n">embeddings</code>&#13;
      <code class="p">)</code>&#13;
      <code class="n">model_object</code><code class="o">.</code><code class="n">embeddings</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">tree_multimap</code><code class="p">(</code>&#13;
          <code class="k">lambda</code> <code class="n">p</code><code class="p">,</code> <code class="n">g</code><code class="p">:</code> <code class="n">p</code> <code class="o">-</code> <code class="n">alpha</code> <code class="o">*</code> <code class="n">g</code><code class="p">,</code>&#13;
          <code class="c1"># Basic update rule; JAX handles broadcasting for us</code>&#13;
          <code class="n">model_object</code><code class="o">.</code><code class="n">embeddings</code><code class="p">,</code>&#13;
          <code class="n">grads</code>&#13;
      <code class="p">)</code>&#13;
      <code class="k">if</code> <code class="n">i</code> <code class="o">%</code> <code class="mi">1000</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code> <code class="c1"># Most output in wandb; little bit of logging</code>&#13;
        <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Loss step </code><code class="si">{</code><code class="n">i</code><code class="si">}</code><code class="s1">: '</code><code class="p">,</code> <code class="n">loss_val</code><code class="p">)</code>&#13;
        <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"""Test loss: </code><code class="si">{</code>&#13;
            <code class="n">omse_loss</code><code class="p">(</code>&#13;
                <code class="n">model_object</code><code class="o">.</code><code class="n">ground_truth</code><code class="p">[</code><code class="s1">'A_train'</code><code class="p">],</code>&#13;
                <code class="n">model_object</code><code class="o">.</code><code class="n">embeddings</code>&#13;
            <code class="p">)</code><code class="si">}</code><code class="s2">"""</code><code class="p">)</code>&#13;
&#13;
      <code class="n">wandb</code><code class="o">.</code><code class="n">log</code><code class="p">({</code>&#13;
          <code class="s2">"Train omse"</code><code class="p">:</code> <code class="n">loss_val</code><code class="p">,</code>&#13;
          <code class="s2">"Test omse"</code><code class="p">:</code> <code class="n">omse_loss</code><code class="p">(</code>&#13;
              <code class="n">model_object</code><code class="o">.</code><code class="n">ground_truth</code><code class="p">[</code><code class="s1">'A_test'</code><code class="p">],</code>&#13;
              <code class="n">model_object</code><code class="o">.</code><code class="n">embeddings</code>&#13;
           <code class="p">)</code>&#13;
      <code class="p">})</code></pre>&#13;
&#13;
<p>Note that this code is using <code>tree_multimap</code> to handle broadcasting our update rule, and we’re using the jitted loss from before in the <code>omse_loss</code> call. Also, we’re calling <code>value_and_grad</code> so we can log the loss to wandb as we go. This is a common trick you’ll see for efficiently doing both without a callback.</p>&#13;
&#13;
<p>You can finish this off and start the training with a sweep:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">sweep_config</code> <code class="o">=</code> <code class="p">{</code>&#13;
    <code class="s2">"name"</code> <code class="p">:</code> <code class="s2">"mf-test-sweep"</code><code class="p">,</code>&#13;
    <code class="s2">"method"</code> <code class="p">:</code> <code class="s2">"random"</code><code class="p">,</code>&#13;
    <code class="s2">"parameters"</code> <code class="p">:</code> <code class="p">{</code>&#13;
        <code class="s2">"steps"</code> <code class="p">:</code> <code class="p">{</code>&#13;
            <code class="s2">"min"</code><code class="p">:</code> <code class="mi">1000</code><code class="p">,</code>&#13;
            <code class="s2">"max"</code><code class="p">:</code> <code class="mi">3000</code><code class="p">,</code>&#13;
        <code class="p">},</code>&#13;
        <code class="s2">"alpha"</code> <code class="p">:{</code>&#13;
            <code class="s2">"min"</code><code class="p">:</code> <code class="mf">0.6</code><code class="p">,</code>&#13;
            <code class="s2">"max"</code><code class="p">:</code> <code class="mf">1.75</code>&#13;
        <code class="p">},</code>&#13;
        <code class="s2">"emb_dim"</code> <code class="p">:{</code>&#13;
            <code class="s2">"min"</code><code class="p">:</code> <code class="mi">3</code><code class="p">,</code>&#13;
            <code class="s2">"max"</code><code class="p">:</code> <code class="mi">10</code>&#13;
        <code class="p">},</code>&#13;
        <code class="s2">"prior_std"</code> <code class="p">:{</code>&#13;
            <code class="s2">"min"</code><code class="p">:</code> <code class="mf">.5</code><code class="p">,</code>&#13;
            <code class="s2">"max"</code><code class="p">:</code> <code class="mf">2.0</code>&#13;
        <code class="p">},</code>&#13;
    <code class="p">},</code>&#13;
    <code class="s2">"metric"</code> <code class="p">:</code> <code class="p">{</code>&#13;
        <code class="s1">'name'</code><code class="p">:</code> <code class="s1">'Test omse'</code><code class="p">,</code>&#13;
        <code class="s1">'goal'</code><code class="p">:</code> <code class="s1">'minimize'</code>&#13;
    <code class="p">}</code>&#13;
<code class="p">}</code>&#13;
&#13;
<code class="n">sweep_id</code> <code class="o">=</code> <code class="n">wandb</code><code class="o">.</code><code class="n">sweep</code><code class="p">(</code><code class="n">sweep_config</code><code class="p">,</code> <code class="n">project</code><code class="o">=</code><code class="s2">"jax-mf"</code><code class="p">,</code> <code class="n">entity</code><code class="o">=</code><code class="s2">"wandb-un"</code><code class="p">)</code>&#13;
&#13;
<code class="n">wandb</code><code class="o">.</code><code class="n">init</code><code class="p">()</code>&#13;
<code class="n">train</code><code class="p">()</code>&#13;
&#13;
<code class="n">count</code> <code class="o">=</code> <code class="mi">50</code>&#13;
<code class="n">wandb</code><code class="o">.</code><code class="n">agent</code><code class="p">(</code><code class="n">sweep_id</code><code class="p">,</code> <code class="n">function</code><code class="o">=</code><code class="n">train</code><code class="p">,</code> <code class="n">count</code><code class="o">=</code><code class="n">count</code><code class="p">)</code></pre>&#13;
&#13;
<p class="less_space pagebreak-before">In this case, the<a data-primary="hyperparameter optimization (HPO)" data-type="indexterm" id="id908"/><a data-primary="HPO (hyperparameter optimization)" data-type="indexterm" id="id909"/> hyperparameter optimization (HPO) is over our hyperparameters like embedding dimension and the priors (randomized matrices). Up until now, we have trained some MF models on our ratings matrix. Let’s now add regularization and cross-validation.</p>&#13;
&#13;
<p>Let’s translate the preceding math equations directly into code:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">ell_two_regularization_term</code><code class="p">(</code><code class="n">params</code><code class="p">,</code> <code class="n">dimensions</code><code class="p">):</code>&#13;
    <code class="n">U</code><code class="p">,</code> <code class="n">V</code> <code class="o">=</code> <code class="n">params</code><code class="p">[</code><code class="s1">'users'</code><code class="p">],</code> <code class="n">params</code><code class="p">[</code><code class="s1">'items'</code><code class="p">]</code>&#13;
    <code class="n">N</code><code class="p">,</code> <code class="n">M</code> <code class="o">=</code> <code class="n">dimensions</code><code class="p">[</code><code class="s1">'users'</code><code class="p">],</code> <code class="n">dimensions</code><code class="p">[</code><code class="s1">'items'</code><code class="p">]</code>&#13;
    <code class="n">user_sq</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">multiply</code><code class="p">(</code><code class="n">U</code><code class="p">,</code> <code class="n">U</code><code class="p">)</code>&#13;
    <code class="n">item_sq</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">multiply</code><code class="p">(</code><code class="n">V</code><code class="p">,</code> <code class="n">V</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="p">(</code><code class="n">jnp</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">user_sq</code><code class="p">)</code><code class="o">/</code><code class="n">N</code> <code class="o">+</code> <code class="n">jnp</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">item_sq</code><code class="p">)</code><code class="o">/</code><code class="n">M</code><code class="p">)</code>&#13;
&#13;
<code class="n">l2_loss</code> <code class="o">=</code> <code class="n">jit</code><code class="p">(</code><code class="n">ell_two_regularization_term</code><code class="p">)</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">gramian_regularization_term</code><code class="p">(</code><code class="n">params</code><code class="p">,</code> <code class="n">dimensions</code><code class="p">):</code>&#13;
    <code class="n">U</code><code class="p">,</code> <code class="n">V</code> <code class="o">=</code> <code class="n">params</code><code class="p">[</code><code class="s1">'users'</code><code class="p">],</code> <code class="n">params</code><code class="p">[</code><code class="s1">'items'</code><code class="p">]</code>&#13;
    <code class="n">N</code><code class="p">,</code> <code class="n">M</code> <code class="o">=</code> <code class="n">dimensions</code><code class="p">[</code><code class="s1">'users'</code><code class="p">],</code> <code class="n">dimensions</code><code class="p">[</code><code class="s1">'items'</code><code class="p">]</code>&#13;
    <code class="n">gr_user</code> <code class="o">=</code> <code class="n">U</code><code class="o">.</code><code class="n">T</code> <code class="o">@</code> <code class="n">U</code>&#13;
    <code class="n">gr_item</code> <code class="o">=</code> <code class="n">V</code><code class="o">.</code><code class="n">T</code> <code class="o">@</code> <code class="n">V</code>&#13;
    <code class="n">gr_square</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">multiply</code><code class="p">(</code><code class="n">gr_user</code><code class="p">,</code> <code class="n">gr_item</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="p">(</code><code class="n">jnp</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">gr_square</code><code class="p">)</code><code class="o">/</code><code class="p">(</code><code class="n">N</code><code class="o">*</code><code class="n">M</code><code class="p">))</code>&#13;
&#13;
<code class="n">gr_loss</code> <code class="o">=</code> <code class="n">jit</code><code class="p">(</code><code class="n">gramian_regularization_term</code><code class="p">)</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">regularized_omse</code><code class="p">(</code><code class="n">A</code><code class="p">,</code> <code class="n">params</code><code class="p">,</code> <code class="n">dimensions</code><code class="p">,</code> <code class="n">hyperparams</code><code class="p">):</code>&#13;
  <code class="n">lr</code><code class="p">,</code> <code class="n">lg</code> <code class="o">=</code> <code class="n">hyperparams</code><code class="p">[</code><code class="s1">'ell_2'</code><code class="p">],</code> <code class="n">hyperparams</code><code class="p">[</code><code class="s1">'gram'</code><code class="p">]</code>&#13;
  <code class="n">losses</code> <code class="o">=</code> <code class="p">{</code>&#13;
      <code class="s1">'omse'</code><code class="p">:</code> <code class="n">sp_mse_loss</code><code class="p">(</code><code class="n">A</code><code class="p">,</code> <code class="n">params</code><code class="p">),</code>&#13;
      <code class="s1">'l2_loss'</code><code class="p">:</code> <code class="n">l2_loss</code><code class="p">(</code><code class="n">params</code><code class="p">,</code> <code class="n">dimensions</code><code class="p">),</code>&#13;
      <code class="s1">'gr_loss'</code><code class="p">:</code> <code class="n">gr_loss</code><code class="p">(</code><code class="n">params</code><code class="p">,</code> <code class="n">dimensions</code><code class="p">),</code>&#13;
  <code class="p">}</code>&#13;
  <code class="n">losses</code><code class="o">.</code><code class="n">update</code><code class="p">({</code>&#13;
      <code class="s1">'total_loss'</code><code class="p">:</code> <code class="n">losses</code><code class="p">[</code><code class="s1">'omse'</code><code class="p">]</code> <code class="o">+</code> <code class="n">lr</code><code class="o">*</code><code class="n">losses</code><code class="p">[</code><code class="s1">'l2_loss'</code><code class="p">]</code> <code class="o">+</code> <code class="n">lg</code><code class="o">*</code><code class="n">losses</code><code class="p">[</code><code class="s1">'gr_loss'</code><code class="p">]</code>&#13;
  <code class="p">})</code>&#13;
  <code class="k">return</code> <code class="n">losses</code><code class="p">[</code><code class="s1">'total_loss'</code><code class="p">],</code> <code class="n">losses</code>&#13;
&#13;
<code class="n">reg_loss_observed</code> <code class="o">=</code> <code class="n">jit</code><code class="p">(</code><code class="n">regularized_omse</code><code class="p">)</code></pre>&#13;
&#13;
<p>We won’t dive super deep into learning rate schedulers, but we will do a simple decay:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">lr_decay</code><code class="p">(</code>&#13;
    <code class="n">step_num</code><code class="p">,</code>&#13;
    <code class="n">base_learning_rate</code><code class="p">,</code>&#13;
    <code class="n">decay_pct</code> <code class="o">=</code> <code class="mf">0.5</code><code class="p">,</code>&#13;
    <code class="n">period_length</code> <code class="o">=</code> <code class="mf">100.0</code>&#13;
<code class="p">):</code>&#13;
    <code class="k">return</code> <code class="n">base_learning_rate</code> <code class="o">*</code> <code class="n">math</code><code class="o">.</code><code class="n">pow</code><code class="p">(</code>&#13;
        <code class="n">decay_pct</code><code class="p">,</code>&#13;
        <code class="n">math</code><code class="o">.</code><code class="n">floor</code><code class="p">((</code><code class="mi">1</code><code class="o">+</code><code class="n">step_num</code><code class="p">)</code><code class="o">/</code><code class="n">period_length</code><code class="p">)</code>&#13;
    <code class="p">)</code></pre>&#13;
&#13;
<p>Our updated train function will incorporate our new regularizations—which come with some hyperparameters—and a bit of additional logging setup. This code makes it easy to log our experiment as it trains and configures the hyperparameters to work with regularization:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">train_with_reg_loss</code><code class="p">():</code>&#13;
    <code class="n">run_config</code> <code class="o">=</code> <code class="p">{</code> <code class="c1"># These will be hyperparameters we will tune via wandb</code>&#13;
        <code class="s1">'emb_dim'</code><code class="p">:</code> <code class="kc">None</code><code class="p">,</code>&#13;
        <code class="s1">'prior_std'</code><code class="p">:</code> <code class="kc">None</code><code class="p">,</code>&#13;
        <code class="s1">'alpha'</code><code class="p">:</code> <code class="kc">None</code><code class="p">,</code> <code class="c1"># Learning rate</code>&#13;
        <code class="s1">'steps'</code><code class="p">:</code> <code class="kc">None</code><code class="p">,</code>&#13;
        <code class="s1">'ell_2'</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="c1">#l2 regularization penalization weight</code>&#13;
        <code class="s1">'gram'</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="c1">#gramian regularization penalization weight</code>&#13;
        <code class="s1">'decay_pct'</code><code class="p">:</code> <code class="mf">0.5</code><code class="p">,</code>&#13;
        <code class="s1">'period_length'</code><code class="p">:</code> <code class="mf">100.0</code>&#13;
    <code class="p">}</code>&#13;
&#13;
    <code class="k">with</code> <code class="n">wandb</code><code class="o">.</code><code class="n">init</code><code class="p">()</code> <code class="k">as</code> <code class="n">run</code><code class="p">:</code>&#13;
        <code class="n">run_config</code><code class="o">.</code><code class="n">update</code><code class="p">(</code><code class="n">run</code><code class="o">.</code><code class="n">config</code><code class="p">)</code>&#13;
        <code class="n">model_object</code> <code class="o">=</code> <code class="n">model_constructor</code><code class="p">(</code>&#13;
            <code class="n">ratings_df</code><code class="o">=</code><code class="n">all_rat</code><code class="p">,</code>&#13;
            <code class="n">user_dim</code><code class="o">=</code><code class="mi">942</code><code class="p">,</code>&#13;
            <code class="n">item_dim</code><code class="o">=</code><code class="mi">1681</code><code class="p">,</code>&#13;
            <code class="n">embedding_dim</code><code class="o">=</code><code class="n">run_config</code><code class="p">[</code><code class="s1">'emb_dim'</code><code class="p">],</code>&#13;
            <code class="n">init_stddev</code><code class="o">=</code><code class="n">run_config</code><code class="p">[</code><code class="s1">'prior_std'</code><code class="p">],</code>&#13;
            <code class="n">prng_key</code><code class="o">=</code><code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="mi">0</code><code class="p">),</code>&#13;
            <code class="n">train_set</code><code class="o">=</code><code class="n">mf_model</code><code class="o">.</code><code class="n">ground_truth</code><code class="p">[</code><code class="s1">'A_train'</code><code class="p">],</code>&#13;
            <code class="n">test_set</code><code class="o">=</code><code class="n">mf_model</code><code class="o">.</code><code class="n">ground_truth</code><code class="p">[</code><code class="s1">'A_test'</code><code class="p">]</code>&#13;
        <code class="p">)</code>&#13;
        <code class="n">model_object</code><code class="o">.</code><code class="n">reset_embeddings</code><code class="p">()</code> <code class="c1"># Ensure we start from priors</code>&#13;
&#13;
        <code class="n">alpha</code><code class="p">,</code> <code class="n">steps</code> <code class="o">=</code> <code class="n">run_config</code><code class="p">[</code><code class="s1">'alpha'</code><code class="p">],</code> <code class="n">run_config</code><code class="p">[</code><code class="s1">'steps'</code><code class="p">]</code>&#13;
        <code class="nb">print</code><code class="p">(</code><code class="n">run_config</code><code class="p">)</code>&#13;
&#13;
        <code class="n">grad_fn</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">value_and_grad</code><code class="p">(</code>&#13;
            <code class="n">reg_loss_observed</code><code class="p">,</code>&#13;
            <code class="mi">1</code><code class="p">,</code>&#13;
            <code class="n">has_aux</code><code class="o">=</code><code class="kc">True</code>&#13;
        <code class="p">)</code> <code class="c1"># Tell JAX to expect an aux dict as output</code>&#13;
&#13;
        <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">steps</code><code class="p">):</code>&#13;
            <code class="p">(</code><code class="n">total_loss_val</code><code class="p">,</code> <code class="n">loss_dict</code><code class="p">),</code> <code class="n">grads</code> <code class="o">=</code> <code class="n">grad_fn</code><code class="p">(</code>&#13;
                <code class="n">model_object</code><code class="o">.</code><code class="n">ground_truth</code><code class="p">[</code><code class="s1">'A_train'</code><code class="p">],</code>&#13;
                <code class="n">model_object</code><code class="o">.</code><code class="n">embeddings</code><code class="p">,</code>&#13;
                <code class="n">dimensions</code><code class="o">=</code><code class="p">{</code><code class="s1">'users'</code><code class="p">:</code> <code class="n">user_count</code><code class="p">,</code> <code class="s1">'items'</code><code class="p">:</code> <code class="n">item_count</code><code class="p">},</code>&#13;
                <code class="n">hyperparams</code><code class="o">=</code><code class="p">{</code>&#13;
                    <code class="s1">'ell_2'</code><code class="p">:</code> <code class="n">run_config</code><code class="p">[</code><code class="s1">'ell_2'</code><code class="p">],</code>&#13;
                    <code class="s1">'gram'</code><code class="p">:</code> <code class="n">run_config</code><code class="p">[</code><code class="s1">'gram'</code><code class="p">]</code>&#13;
                <code class="p">}</code> <code class="c1"># JAX carries our loss dict along for logging</code>&#13;
            <code class="p">)</code>&#13;
&#13;
&#13;
            <code class="n">model_object</code><code class="o">.</code><code class="n">embeddings</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">tree_multimap</code><code class="p">(</code>&#13;
                <code class="k">lambda</code> <code class="n">p</code><code class="p">,</code> <code class="n">g</code><code class="p">:</code> <code class="n">p</code> <code class="o">-</code> <code class="n">lr_decay</code><code class="p">(</code>&#13;
                    <code class="n">i</code><code class="p">,</code>&#13;
                    <code class="n">alpha</code><code class="p">,</code>&#13;
                    <code class="n">run_config</code><code class="p">[</code><code class="s1">'decay_pct'</code><code class="p">],</code>&#13;
                    <code class="n">run_config</code><code class="p">[</code><code class="s1">'period_length'</code><code class="p">]</code>&#13;
                <code class="p">)</code> <code class="o">*</code> <code class="n">g</code><code class="p">,</code> <code class="c1"># update with decay</code>&#13;
                <code class="n">model_object</code><code class="o">.</code><code class="n">embeddings</code><code class="p">,</code>&#13;
                <code class="n">grads</code>&#13;
            <code class="p">)</code>&#13;
            <code class="k">if</code> <code class="n">i</code> <code class="o">%</code> <code class="mi">1000</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>&#13;
                <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Loss step </code><code class="si">{</code><code class="n">i</code><code class="si">}</code><code class="s1">:'</code><code class="p">)</code>&#13;
                <code class="nb">print</code><code class="p">(</code><code class="n">loss_dict</code><code class="p">)</code>&#13;
                <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"""Test loss: </code><code class="si">{</code>&#13;
                    <code class="n">omse_loss</code><code class="p">(</code><code class="n">model_object</code><code class="o">.</code><code class="n">ground_truth</code><code class="p">[</code><code class="s1">'A_test'</code><code class="p">],</code>&#13;
                    <code class="n">model_object</code><code class="o">.</code><code class="n">embeddings</code><code class="p">)</code><code class="si">}</code><code class="s2">"""</code><code class="p">)</code>&#13;
&#13;
            <code class="n">loss_dict</code><code class="o">.</code><code class="n">update</code><code class="p">(</code> <code class="c1"># wandb takes the entire loss dictionary</code>&#13;
                <code class="p">{</code>&#13;
                    <code class="s2">"Test omse"</code><code class="p">:</code> <code class="n">omse_loss</code><code class="p">(</code>&#13;
                        <code class="n">model_object</code><code class="o">.</code><code class="n">ground_truth</code><code class="p">[</code><code class="s1">'A_test'</code><code class="p">],</code>&#13;
                        <code class="n">model_object</code><code class="o">.</code><code class="n">embeddings</code>&#13;
                    <code class="p">),</code>&#13;
                    <code class="s2">"learning_rate"</code><code class="p">:</code> <code class="n">lr_decay</code><code class="p">(</code><code class="n">i</code><code class="p">,</code> <code class="n">alpha</code><code class="p">),</code>&#13;
                <code class="p">}</code>&#13;
            <code class="p">)</code>&#13;
            <code class="n">wandb</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="n">loss_dict</code><code class="p">)</code>&#13;
&#13;
 <code class="n">sweep_config</code> <code class="o">=</code> <code class="p">{</code>&#13;
    <code class="s2">"name"</code> <code class="p">:</code> <code class="s2">"mf-HPO-with-reg"</code><code class="p">,</code>&#13;
    <code class="s2">"method"</code> <code class="p">:</code> <code class="s2">"random"</code><code class="p">,</code>&#13;
    <code class="s2">"parameters"</code> <code class="p">:</code> <code class="p">{</code>&#13;
      <code class="s2">"steps"</code><code class="p">:</code> <code class="p">{</code>&#13;
        <code class="s2">"value"</code><code class="p">:</code> <code class="mi">2000</code>&#13;
      <code class="p">},</code>&#13;
      <code class="s2">"alpha"</code> <code class="p">:{</code>&#13;
        <code class="s2">"min"</code><code class="p">:</code> <code class="mf">0.6</code><code class="p">,</code>&#13;
        <code class="s2">"max"</code><code class="p">:</code> <code class="mf">2.25</code>&#13;
      <code class="p">},</code>&#13;
      <code class="s2">"emb_dim"</code> <code class="p">:{</code>&#13;
        <code class="s2">"min"</code><code class="p">:</code> <code class="mi">15</code><code class="p">,</code>&#13;
        <code class="s2">"max"</code><code class="p">:</code> <code class="mi">80</code>&#13;
      <code class="p">},</code>&#13;
      <code class="s2">"prior_std"</code> <code class="p">:{</code>&#13;
        <code class="s2">"min"</code><code class="p">:</code> <code class="mf">.5</code><code class="p">,</code>&#13;
        <code class="s2">"max"</code><code class="p">:</code> <code class="mf">2.0</code>&#13;
      <code class="p">},</code>&#13;
      <code class="s2">"ell_2"</code> <code class="p">:{</code>&#13;
        <code class="s2">"min"</code><code class="p">:</code> <code class="mf">.05</code><code class="p">,</code>&#13;
        <code class="s2">"max"</code><code class="p">:</code> <code class="mf">0.5</code>&#13;
      <code class="p">},</code>&#13;
      <code class="s2">"gram"</code> <code class="p">:{</code>&#13;
        <code class="s2">"min"</code><code class="p">:</code> <code class="mf">.1</code><code class="p">,</code>&#13;
        <code class="s2">"max"</code><code class="p">:</code> <code class="mf">.75</code>&#13;
      <code class="p">},</code>&#13;
      <code class="s2">"decay_pct"</code> <code class="p">:{</code>&#13;
        <code class="s2">"min"</code><code class="p">:</code> <code class="mf">.2</code><code class="p">,</code>&#13;
        <code class="s2">"max"</code><code class="p">:</code> <code class="mf">.8</code>&#13;
      <code class="p">},</code>&#13;
      <code class="s2">"period_length"</code> <code class="p">:{</code>&#13;
        <code class="s2">"min"</code><code class="p">:</code> <code class="mi">50</code><code class="p">,</code>&#13;
        <code class="s2">"max"</code><code class="p">:</code> <code class="mi">500</code>&#13;
      <code class="p">}</code>&#13;
    <code class="p">},</code>&#13;
    <code class="s2">"metric"</code> <code class="p">:</code> <code class="p">{</code>&#13;
      <code class="s1">'name'</code><code class="p">:</code> <code class="s1">'Test omse'</code><code class="p">,</code>&#13;
      <code class="s1">'goal'</code><code class="p">:</code> <code class="s1">'minimize'</code>&#13;
    <code class="p">}</code>&#13;
  <code class="p">}</code>&#13;
&#13;
  <code class="n">sweep_id</code> <code class="o">=</code> <code class="n">wandb</code><code class="o">.</code><code class="n">sweep</code><code class="p">(</code>&#13;
      <code class="n">sweep_config</code><code class="p">,</code>&#13;
      <code class="n">project</code><code class="o">=</code><code class="s2">"jax-mf"</code><code class="p">,</code>&#13;
      <code class="n">entity</code><code class="o">=</code><code class="s2">"wandb-un"</code>&#13;
  <code class="p">)</code>&#13;
&#13;
<code class="n">run_config</code> <code class="o">=</code> <code class="p">{</code> <code class="c1"># These will be hyperparameters we will tune via wandb</code>&#13;
      <code class="s1">'emb_dim'</code><code class="p">:</code> <code class="mi">10</code><code class="p">,</code> <code class="c1"># Latent dimension</code>&#13;
      <code class="s1">'prior_std'</code><code class="p">:</code> <code class="mf">0.1</code><code class="p">,</code>&#13;
      <code class="s1">'alpha'</code><code class="p">:</code> <code class="mf">1.0</code><code class="p">,</code> <code class="c1"># Learning rate</code>&#13;
      <code class="s1">'steps'</code><code class="p">:</code> <code class="mi">1000</code><code class="p">,</code> <code class="c1"># Number of training steps</code>&#13;
      <code class="s1">'ell_2'</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="c1">#l2 regularization penalization weight</code>&#13;
      <code class="s1">'gram'</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="c1">#gramian regularization penalization weight</code>&#13;
      <code class="s1">'decay_pct'</code><code class="p">:</code> <code class="mf">0.5</code><code class="p">,</code>&#13;
      <code class="s1">'period_length'</code><code class="p">:</code> <code class="mf">100.0</code>&#13;
  <code class="p">}</code>&#13;
&#13;
<code class="n">train_with_reg_loss</code><code class="p">()</code></pre>&#13;
&#13;
<p>The last step is to do this in a way that gives us confidence in the models we’re seeing. Unfortunately, setting up cross-validation for MF problems can be tricky, so we’ll need to make a few modifications to our data structures:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">sparse_array_concatenate</code><code class="p">(</code><code class="n">sparse_array_iterable</code><code class="p">):</code>&#13;
    <code class="k">return</code> <code class="p">{</code>&#13;
        <code class="s1">'indices'</code><code class="p">:</code> <code class="nb">tuple</code><code class="p">(</code>&#13;
            <code class="nb">map</code><code class="p">(</code>&#13;
                <code class="n">jnp</code><code class="o">.</code><code class="n">concatenate</code><code class="p">,</code>&#13;
                <code class="nb">zip</code><code class="p">(</code><code class="o">*</code><code class="p">(</code><code class="n">x</code><code class="p">[</code><code class="s1">'indices'</code><code class="p">]</code> <code class="k">for</code> <code class="n">x</code> <code class="ow">in</code> <code class="n">sparse_array_iterable</code><code class="p">)))</code>&#13;
            <code class="p">),</code>&#13;
        <code class="s1">'values'</code><code class="p">:</code> <code class="n">jnp</code><code class="o">.</code><code class="n">concatenate</code><code class="p">(</code>&#13;
            <code class="p">[</code><code class="n">x</code><code class="p">[</code><code class="s1">'values'</code><code class="p">]</code> <code class="k">for</code> <code class="n">x</code> <code class="ow">in</code> <code class="n">sparse_array_iterable</code><code class="p">]</code>&#13;
        <code class="p">),</code>&#13;
    <code class="p">}</code>&#13;
&#13;
<code class="k">class</code> <code class="nc">jax_df_Kfold</code><code class="p">(</code><code class="nb">object</code><code class="p">):</code>&#13;
    <code class="sd">"""Simple class that handles Kfold</code>&#13;
<code class="sd">    splitting of a matrix as a dataframe and stores as sparse jarrays"""</code>&#13;
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code>&#13;
        <code class="bp">self</code><code class="p">,</code>&#13;
        <code class="n">df</code><code class="p">:</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">,</code>&#13;
        <code class="n">user_dim</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code>&#13;
        <code class="n">item_dim</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code>&#13;
        <code class="n">k</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">5</code><code class="p">,</code>&#13;
        <code class="n">prng_key</code><code class="o">=</code><code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>&#13;
    <code class="p">):</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">_df</code> <code class="o">=</code> <code class="n">df</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">_num_folds</code> <code class="o">=</code> <code class="n">k</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">_split_idxes</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">array_split</code><code class="p">(</code>&#13;
            <code class="n">random</code><code class="o">.</code><code class="n">permutation</code><code class="p">(</code>&#13;
                <code class="n">prng_key</code><code class="p">,</code>&#13;
                <code class="n">df</code><code class="o">.</code><code class="n">index</code><code class="o">.</code><code class="n">to_numpy</code><code class="p">(),</code>&#13;
                <code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code>&#13;
                <code class="n">independent</code><code class="o">=</code><code class="kc">True</code>&#13;
            <code class="p">),</code>&#13;
            <code class="bp">self</code><code class="o">.</code><code class="n">_num_folds</code>&#13;
        <code class="p">)</code>&#13;
&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">_fold_arrays</code> <code class="o">=</code> <code class="nb">dict</code><code class="p">()</code>&#13;
&#13;
        <code class="k">for</code> <code class="n">fold_index</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">_num_folds</code><code class="p">):</code>&#13;
        <code class="c1"># let's create sparse jax arrays for each fold piece</code>&#13;
            <code class="bp">self</code><code class="o">.</code><code class="n">_fold_arrays</code><code class="p">[</code><code class="n">fold_index</code><code class="p">]</code> <code class="o">=</code> <code class="p">(</code>&#13;
                <code class="bp">self</code><code class="o">.</code><code class="n">_df</code><code class="p">[</code>&#13;
                    <code class="bp">self</code><code class="o">.</code><code class="n">_df</code><code class="o">.</code><code class="n">index</code><code class="o">.</code><code class="n">isin</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">_split_idxes</code><code class="p">[</code><code class="n">fold_index</code><code class="p">])</code>&#13;
                <code class="p">]</code><code class="o">.</code><code class="n">pipe</code><code class="p">(</code><code class="n">start_pipeline</code><code class="p">)</code>&#13;
                <code class="o">.</code><code class="n">pipe</code><code class="p">(</code>&#13;
                    <code class="n">ratings_to_sparse_array</code><code class="p">,</code>&#13;
                    <code class="n">user_dim</code><code class="o">=</code><code class="n">user_dim</code><code class="p">,</code>&#13;
                    <code class="n">item_dim</code><code class="o">=</code><code class="n">item_dim</code>&#13;
                <code class="p">)</code>&#13;
            <code class="p">)</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">get_fold</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">fold_index</code><code class="p">:</code> <code class="nb">int</code><code class="p">):</code>&#13;
        <code class="k">assert</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">_num_folds</code> <code class="o">&gt;</code> <code class="n">fold_index</code><code class="p">)</code>&#13;
        <code class="n">test</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">_fold_arrays</code><code class="p">[</code><code class="n">fold_index</code><code class="p">]</code>&#13;
        <code class="n">train</code> <code class="o">=</code> <code class="n">sparse_array_concatenate</code><code class="p">(</code>&#13;
            <code class="p">[</code><code class="n">v</code> <code class="k">for</code> <code class="n">k</code><code class="p">,</code><code class="n">v</code> <code class="ow">in</code> <code class="bp">self</code><code class="o">.</code><code class="n">_fold_arrays</code><code class="o">.</code><code class="n">items</code><code class="p">()</code> <code class="k">if</code> <code class="n">k</code> <code class="o">!=</code> <code class="n">fold_index</code><code class="p">]</code>&#13;
        <code class="p">)</code>&#13;
        <code class="k">return</code> <code class="n">train</code><code class="p">,</code> <code class="n">test</code></pre>&#13;
&#13;
<p>Each hyperparameter setup should yield loss for each fold, so within <em>wandb.init</em>, we build a model with each fold:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">for</code> <code class="n">j</code> <code class="ow">in</code> <code class="n">num_folds</code><code class="p">:</code>&#13;
  <code class="n">train</code><code class="p">,</code> <code class="n">test</code> <code class="o">=</code> <code class="n">folder</code><code class="o">.</code><code class="n">get_fold</code><code class="p">(</code><code class="n">j</code><code class="p">)</code>&#13;
  <code class="n">model_object_dict</code><code class="p">[</code><code class="n">j</code><code class="p">]</code> <code class="o">=</code> <code class="n">model_constructor</code><code class="p">(</code>&#13;
          <code class="n">ratings_df</code><code class="o">=</code><code class="n">all_rat</code><code class="p">,</code>&#13;
          <code class="n">user_dim</code><code class="o">=</code><code class="n">user_count</code><code class="p">,</code>&#13;
          <code class="n">item_dim</code><code class="o">=</code><code class="n">item_count</code><code class="p">,</code>&#13;
          <code class="n">embedding_dim</code><code class="o">=</code><code class="n">run_config</code><code class="p">[</code><code class="s1">'emb_dim'</code><code class="p">],</code>&#13;
          <code class="n">init_stddev</code><code class="o">=</code><code class="n">run_config</code><code class="p">[</code><code class="s1">'prior_std'</code><code class="p">],</code>&#13;
          <code class="n">prng_key</code><code class="o">=</code><code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="mi">0</code><code class="p">),</code>&#13;
          <code class="n">train_set</code><code class="o">=</code><code class="n">train</code><code class="p">,</code>&#13;
          <code class="n">test_set</code><code class="o">=</code><code class="n">test</code>&#13;
      <code class="p">)</code></pre>&#13;
&#13;
<p>At each step, we’d like to not only compute the gradient for the training and evaluate on the test but also compute gradients for all folds, evaluate on all the tests, and produce the relevant errors:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">steps</code><code class="p">):</code>&#13;
    <code class="n">loss_dict</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"learning_rate"</code><code class="p">:</code> <code class="n">step_decay</code><code class="p">(</code><code class="n">i</code><code class="p">)}</code>&#13;
    <code class="k">for</code> <code class="n">j</code><code class="p">,</code> <code class="n">M</code> <code class="ow">in</code> <code class="n">model_object_dict</code><code class="o">.</code><code class="n">items</code><code class="p">():</code>&#13;
        <code class="p">(</code><code class="n">total_loss_val</code><code class="p">,</code> <code class="n">fold_loss_dict</code><code class="p">),</code> <code class="n">grads</code> <code class="o">=</code> <code class="n">grad_fn</code><code class="p">(</code>&#13;
          <code class="n">M</code><code class="o">.</code><code class="n">ground_truth</code><code class="p">[</code><code class="s1">'A_train'</code><code class="p">],</code>&#13;
          <code class="n">M</code><code class="o">.</code><code class="n">embeddings</code><code class="p">,</code>&#13;
          <code class="n">dimensions</code><code class="o">=</code><code class="p">{</code><code class="s1">'users'</code><code class="p">:</code> <code class="mi">942</code><code class="p">,</code> <code class="s1">'items'</code><code class="p">:</code> <code class="mi">1681</code><code class="p">},</code>&#13;
          <code class="n">hyperparams</code><code class="o">=</code><code class="p">{</code><code class="s1">'ell_2'</code><code class="p">:</code> <code class="n">run_config</code><code class="p">[</code><code class="s1">'ell_2'</code><code class="p">],</code> <code class="s1">'gram'</code><code class="p">:</code> <code class="n">run_config</code><code class="p">[</code><code class="s1">'gram'</code><code class="p">]}</code>&#13;
        <code class="p">)</code>&#13;
&#13;
        <code class="n">M</code><code class="o">.</code><code class="n">embeddings</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">tree_multimap</code><code class="p">(</code>&#13;
            <code class="k">lambda</code> <code class="n">p</code><code class="p">,</code> <code class="n">g</code><code class="p">:</code> <code class="n">p</code> <code class="o">-</code> <code class="n">step_decay</code><code class="p">(</code><code class="n">i</code><code class="p">)</code> <code class="o">*</code> <code class="n">g</code><code class="p">,</code>&#13;
            <code class="n">M</code><code class="o">.</code><code class="n">embeddings</code><code class="p">,</code>&#13;
            <code class="n">grads</code>&#13;
        <code class="p">)</code></pre>&#13;
&#13;
<p>Logging should be losses per fold, and the aggregate loss should be the target metric. This is because each fold is an independent optimization of the model parameters; however, we wish to see aggregate behavior across the folds:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">        <code class="n">fold_loss_dict</code> <code class="o">=</code> <code class="p">{</code><code class="sa">f</code><code class="s1">'</code><code class="si">{</code><code class="n">k</code><code class="si">}</code><code class="s1">_fold-</code><code class="si">{</code><code class="n">j</code><code class="si">}</code><code class="s1">'</code><code class="p">:</code> <code class="n">v</code> <code class="k">for</code> <code class="n">k</code><code class="p">,</code> <code class="n">v</code> <code class="ow">in</code> <code class="n">fold_loss_dict</code><code class="o">.</code><code class="n">items</code><code class="p">()}</code>&#13;
        <code class="n">fold_loss_dict</code><code class="o">.</code><code class="n">update</code><code class="p">(</code>&#13;
                  <code class="p">{</code>&#13;
                      <code class="sa">f</code><code class="s2">"Test omse_fold-</code><code class="si">{</code><code class="n">j</code><code class="si">}</code><code class="s2">"</code><code class="p">:</code> <code class="n">omse_loss</code><code class="p">(</code>&#13;
                        <code class="n">M</code><code class="o">.</code><code class="n">ground_truth</code><code class="p">[</code><code class="s1">'A_test'</code><code class="p">],</code>&#13;
                        <code class="n">M</code><code class="o">.</code><code class="n">embeddings</code>&#13;
                      <code class="p">),</code>&#13;
                  <code class="p">}</code>&#13;
              <code class="p">)</code>&#13;
&#13;
        <code class="n">loss_dict</code><code class="o">.</code><code class="n">update</code><code class="p">(</code><code class="n">fold_loss_dict</code><code class="p">)</code>&#13;
&#13;
    <code class="n">loss_dict</code><code class="o">.</code><code class="n">update</code><code class="p">({</code>&#13;
      <code class="s2">"Test omse_mean"</code><code class="p">:</code> <code class="n">jnp</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code>&#13;
        <code class="p">[</code><code class="n">v</code> <code class="k">for</code> <code class="n">k</code><code class="p">,</code><code class="n">v</code> <code class="ow">in</code> <code class="n">loss_dict</code><code class="o">.</code><code class="n">items</code><code class="p">()</code> <code class="k">if</code> <code class="n">k</code><code class="o">.</code><code class="n">startswith</code><code class="p">(</code><code class="s1">'Test omse_fold-'</code><code class="p">)]</code>&#13;
      <code class="p">)</code>&#13;
    <code class="p">})</code>&#13;
    <code class="n">wandb</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="n">loss_dict</code><code class="p">)</code></pre>&#13;
&#13;
<p>We wrap up into one big training method:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">train_with_reg_loss_CV</code><code class="p">():</code>&#13;
    <code class="n">run_config</code> <code class="o">=</code> <code class="p">{</code> <code class="c1"># These will be hyperparameters we will tune via wandb</code>&#13;
        <code class="s1">'emb_dim'</code><code class="p">:</code> <code class="kc">None</code><code class="p">,</code> <code class="c1"># Latent dimension</code>&#13;
        <code class="s1">'prior_std'</code><code class="p">:</code> <code class="kc">None</code><code class="p">,</code>&#13;
        <code class="c1"># Standard deviation around 0 that our weights are initialized to</code>&#13;
        <code class="s1">'alpha'</code><code class="p">:</code> <code class="kc">None</code><code class="p">,</code> <code class="c1"># Learning rate</code>&#13;
        <code class="s1">'steps'</code><code class="p">:</code> <code class="kc">None</code><code class="p">,</code> <code class="c1"># Number of training steps</code>&#13;
        <code class="s1">'num_folds'</code><code class="p">:</code> <code class="kc">None</code><code class="p">,</code> <code class="c1"># Number of CV Folds</code>&#13;
        <code class="s1">'ell_2'</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="c1">#hyperparameter for l2 regularization penalization weight</code>&#13;
        <code class="s1">'gram'</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="c1">#hyperparameter for gramian regularization penalization weight</code>&#13;
    <code class="p">}</code>&#13;
&#13;
    <code class="k">with</code> <code class="n">wandb</code><code class="o">.</code><code class="n">init</code><code class="p">()</code> <code class="k">as</code> <code class="n">run</code><code class="p">:</code>&#13;
        <code class="n">run_config</code><code class="o">.</code><code class="n">update</code><code class="p">(</code><code class="n">run</code><code class="o">.</code><code class="n">config</code><code class="p">)</code> <code class="c1"># This is how the wandb agent passes params</code>&#13;
        <code class="n">model_object_dict</code> <code class="o">=</code> <code class="nb">dict</code><code class="p">()</code>&#13;
&#13;
        <code class="k">for</code> <code class="n">j</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">run_config</code><code class="p">[</code><code class="s1">'num_folds'</code><code class="p">]):</code>&#13;
            <code class="n">train</code><code class="p">,</code> <code class="n">test</code> <code class="o">=</code> <code class="n">folder</code><code class="o">.</code><code class="n">get_fold</code><code class="p">(</code><code class="n">j</code><code class="p">)</code>&#13;
            <code class="n">model_object_dict</code><code class="p">[</code><code class="n">j</code><code class="p">]</code> <code class="o">=</code> <code class="n">model_constructor</code><code class="p">(</code>&#13;
                <code class="n">ratings_df</code><code class="o">=</code><code class="n">all_rat</code><code class="p">,</code>&#13;
                <code class="n">user_dim</code><code class="o">=</code><code class="mi">942</code><code class="p">,</code>&#13;
                <code class="n">item_dim</code><code class="o">=</code><code class="mi">1681</code><code class="p">,</code>&#13;
                <code class="n">embedding_dim</code><code class="o">=</code><code class="n">run_config</code><code class="p">[</code><code class="s1">'emb_dim'</code><code class="p">],</code>&#13;
                <code class="n">init_stddev</code><code class="o">=</code><code class="n">run_config</code><code class="p">[</code><code class="s1">'prior_std'</code><code class="p">],</code>&#13;
                <code class="n">prng_key</code><code class="o">=</code><code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="mi">0</code><code class="p">),</code>&#13;
                <code class="n">train_set</code><code class="o">=</code><code class="n">train</code><code class="p">,</code>&#13;
                <code class="n">test_set</code><code class="o">=</code><code class="n">test</code>&#13;
            <code class="p">)</code>&#13;
            <code class="n">model_object_dict</code><code class="p">[</code><code class="n">j</code><code class="p">]</code><code class="o">.</code><code class="n">reset_embeddings</code><code class="p">()</code>&#13;
            <code class="c1"># Ensure we are starting from priors</code>&#13;
&#13;
        <code class="n">alpha</code><code class="p">,</code> <code class="n">steps</code> <code class="o">=</code> <code class="n">run_config</code><code class="p">[</code><code class="s1">'alpha'</code><code class="p">],</code> <code class="n">run_config</code><code class="p">[</code><code class="s1">'steps'</code><code class="p">]</code>&#13;
        <code class="nb">print</code><code class="p">(</code><code class="n">run_config</code><code class="p">)</code>&#13;
&#13;
        <code class="n">grad_fn</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">value_and_grad</code><code class="p">(</code><code class="n">reg_loss_observed</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="n">has_aux</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>&#13;
        <code class="c1"># Tell JAX to expect an aux dict as output</code>&#13;
&#13;
        <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">steps</code><code class="p">):</code>&#13;
            <code class="n">loss_dict</code> <code class="o">=</code> <code class="p">{</code>&#13;
              <code class="s2">"learning_rate"</code><code class="p">:</code> <code class="n">lr_decay</code><code class="p">(</code>&#13;
                <code class="n">i</code><code class="p">,</code>&#13;
                <code class="n">alpha</code><code class="p">,</code>&#13;
                <code class="n">decay_pct</code><code class="o">=</code><code class="mf">.75</code><code class="p">,</code>&#13;
                <code class="n">period_length</code><code class="o">=</code><code class="mi">250</code>&#13;
              <code class="p">)</code>&#13;
            <code class="p">}</code>&#13;
            <code class="k">for</code> <code class="n">j</code><code class="p">,</code> <code class="n">M</code> <code class="ow">in</code> <code class="n">model_object_dict</code><code class="o">.</code><code class="n">items</code><code class="p">():</code>&#13;
            <code class="c1"># Iterate through folds</code>&#13;
&#13;
                <code class="p">(</code><code class="n">total_loss_val</code><code class="p">,</code> <code class="n">fold_loss_dict</code><code class="p">),</code> <code class="n">grads</code> <code class="o">=</code> <code class="n">grad_fn</code><code class="p">(</code>&#13;
                <code class="c1"># compute gradients for one fold</code>&#13;
                    <code class="n">M</code><code class="o">.</code><code class="n">ground_truth</code><code class="p">[</code><code class="s1">'A_train'</code><code class="p">],</code>&#13;
                    <code class="n">M</code><code class="o">.</code><code class="n">embeddings</code><code class="p">,</code>&#13;
                    <code class="n">dimensions</code><code class="o">=</code><code class="p">{</code><code class="s1">'users'</code><code class="p">:</code> <code class="mi">942</code><code class="p">,</code> <code class="s1">'items'</code><code class="p">:</code> <code class="mi">1681</code><code class="p">},</code>&#13;
                    <code class="n">hyperparams</code><code class="o">=</code><code class="p">{</code>&#13;
                      <code class="s1">'ell_2'</code><code class="p">:</code> <code class="n">run_config</code><code class="p">[</code><code class="s1">'ell_2'</code><code class="p">],</code>&#13;
                      <code class="s1">'gram'</code><code class="p">:</code> <code class="n">run_config</code><code class="p">[</code><code class="s1">'gram'</code><code class="p">]</code>&#13;
                    <code class="p">}</code>&#13;
                <code class="p">)</code>&#13;
&#13;
                <code class="n">M</code><code class="o">.</code><code class="n">embeddings</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">tree_multimap</code><code class="p">(</code>&#13;
                <code class="c1"># update weights for one fold</code>&#13;
                    <code class="k">lambda</code> <code class="n">p</code><code class="p">,</code> <code class="n">g</code><code class="p">:</code> <code class="n">p</code> <code class="o">-</code> <code class="n">lr_decay</code><code class="p">(</code>&#13;
                      <code class="n">i</code><code class="p">,</code>&#13;
                      <code class="n">alpha</code><code class="p">,</code>&#13;
                      <code class="n">decay_pct</code><code class="o">=</code><code class="mf">.75</code><code class="p">,</code>&#13;
                      <code class="n">period_length</code><code class="o">=</code><code class="mi">250</code>&#13;
                    <code class="p">)</code> <code class="o">*</code> <code class="n">g</code><code class="p">,</code>&#13;
                    <code class="n">M</code><code class="o">.</code><code class="n">embeddings</code><code class="p">,</code>&#13;
                    <code class="n">grads</code>&#13;
                <code class="p">)</code>&#13;
&#13;
                <code class="n">fold_loss_dict</code> <code class="o">=</code> <code class="p">{</code>&#13;
                  <code class="sa">f</code><code class="s1">'</code><code class="si">{</code><code class="n">k</code><code class="si">}</code><code class="s1">_fold-</code><code class="si">{</code><code class="n">j</code><code class="si">}</code><code class="s1">'</code><code class="p">:</code>&#13;
                  <code class="n">v</code> <code class="k">for</code> <code class="n">k</code><code class="p">,</code> <code class="n">v</code> <code class="ow">in</code> <code class="n">fold_loss_dict</code><code class="o">.</code><code class="n">items</code><code class="p">()</code>&#13;
                <code class="p">}</code>&#13;
                <code class="n">fold_loss_dict</code><code class="o">.</code><code class="n">update</code><code class="p">(</code> <code class="c1"># loss calculation within fold</code>&#13;
                    <code class="p">{</code>&#13;
                        <code class="sa">f</code><code class="s2">"Test omse_fold-</code><code class="si">{</code><code class="n">j</code><code class="si">}</code><code class="s2">"</code><code class="p">:</code> <code class="n">omse_loss</code><code class="p">(</code>&#13;
                          <code class="n">M</code><code class="o">.</code><code class="n">ground_truth</code><code class="p">[</code><code class="s1">'A_test'</code><code class="p">],</code>&#13;
                          <code class="n">M</code><code class="o">.</code><code class="n">embeddings</code>&#13;
                        <code class="p">),</code>&#13;
                    <code class="p">}</code>&#13;
                <code class="p">)</code>&#13;
&#13;
                <code class="n">loss_dict</code><code class="o">.</code><code class="n">update</code><code class="p">(</code><code class="n">fold_loss_dict</code><code class="p">)</code>&#13;
&#13;
            <code class="n">loss_dict</code><code class="o">.</code><code class="n">update</code><code class="p">({</code> <code class="c1"># average loss over all folds</code>&#13;
                <code class="s2">"Test omse_mean"</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code>&#13;
                    <code class="p">[</code><code class="n">v</code> <code class="k">for</code> <code class="n">k</code><code class="p">,</code><code class="n">v</code> <code class="ow">in</code> <code class="n">loss_dict</code><code class="o">.</code><code class="n">items</code><code class="p">()</code>&#13;
                    <code class="k">if</code> <code class="n">k</code><code class="o">.</code><code class="n">startswith</code><code class="p">(</code><code class="s1">'Test omse_fold-'</code><code class="p">)]</code>&#13;
                <code class="p">),</code>&#13;
                <code class="s2">"test omse_max"</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">max</code><code class="p">(</code>&#13;
                    <code class="p">[</code><code class="n">v</code> <code class="k">for</code> <code class="n">k</code><code class="p">,</code><code class="n">v</code> <code class="ow">in</code> <code class="n">loss_dict</code><code class="o">.</code><code class="n">items</code><code class="p">()</code>&#13;
                    <code class="k">if</code> <code class="n">k</code><code class="o">.</code><code class="n">startswith</code><code class="p">(</code><code class="s1">'Test omse_fold-'</code><code class="p">)]</code>&#13;
                <code class="p">),</code>&#13;
                <code class="s2">"test omse_min"</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">min</code><code class="p">(</code>&#13;
                    <code class="p">[</code><code class="n">v</code> <code class="k">for</code> <code class="n">k</code><code class="p">,</code><code class="n">v</code> <code class="ow">in</code> <code class="n">loss_dict</code><code class="o">.</code><code class="n">items</code><code class="p">()</code>&#13;
                    <code class="k">if</code> <code class="n">k</code><code class="o">.</code><code class="n">startswith</code><code class="p">(</code><code class="s1">'Test omse_fold-'</code><code class="p">)]</code>&#13;
                <code class="p">)</code>&#13;
            <code class="p">})</code>&#13;
            <code class="n">wandb</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="n">loss_dict</code><code class="p">)</code>&#13;
&#13;
            <code class="k">if</code> <code class="n">i</code> <code class="o">%</code> <code class="mi">1000</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>&#13;
                <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'Loss step </code><code class="si">{</code><code class="n">i</code><code class="si">}</code><code class="s1">:'</code><code class="p">)</code>&#13;
                <code class="nb">print</code><code class="p">(</code><code class="n">loss_dict</code><code class="p">)</code></pre>&#13;
&#13;
<p>Here’s our final sweeps configuration:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">sweep_config</code> <code class="o">=</code> <code class="p">{</code>&#13;
    <code class="s2">"name"</code> <code class="p">:</code> <code class="s2">"mf-HPO-CV"</code><code class="p">,</code>&#13;
    <code class="s2">"method"</code> <code class="p">:</code> <code class="s2">"random"</code><code class="p">,</code>&#13;
    <code class="s2">"parameters"</code> <code class="p">:</code> <code class="p">{</code>&#13;
      <code class="s2">"steps"</code><code class="p">:</code> <code class="p">{</code>&#13;
        <code class="s2">"value"</code><code class="p">:</code> <code class="mi">2000</code>&#13;
      <code class="p">},</code>&#13;
      <code class="s2">"num_folds"</code><code class="p">:</code> <code class="p">{</code>&#13;
        <code class="s2">"value"</code><code class="p">:</code> <code class="mi">5</code>&#13;
      <code class="p">},</code>&#13;
      <code class="s2">"alpha"</code> <code class="p">:{</code>&#13;
        <code class="s2">"min"</code><code class="p">:</code> <code class="mf">2.0</code><code class="p">,</code>&#13;
        <code class="s2">"max"</code><code class="p">:</code> <code class="mf">3.0</code>&#13;
      <code class="p">},</code>&#13;
      <code class="s2">"emb_dim"</code> <code class="p">:{</code>&#13;
        <code class="s2">"min"</code><code class="p">:</code> <code class="mi">15</code><code class="p">,</code>&#13;
        <code class="s2">"max"</code><code class="p">:</code> <code class="mi">70</code>&#13;
      <code class="p">},</code>&#13;
      <code class="s2">"prior_std"</code> <code class="p">:{</code>&#13;
        <code class="s2">"min"</code><code class="p">:</code> <code class="mf">.75</code><code class="p">,</code>&#13;
        <code class="s2">"max"</code><code class="p">:</code> <code class="mf">1.0</code>&#13;
      <code class="p">},</code>&#13;
      <code class="s2">"ell_2"</code> <code class="p">:{</code>&#13;
        <code class="s2">"min"</code><code class="p">:</code> <code class="mf">.05</code><code class="p">,</code>&#13;
        <code class="s2">"max"</code><code class="p">:</code> <code class="mf">0.5</code>&#13;
      <code class="p">},</code>&#13;
      <code class="s2">"gram"</code> <code class="p">:{</code>&#13;
        <code class="s2">"min"</code><code class="p">:</code> <code class="mf">.1</code><code class="p">,</code>&#13;
        <code class="s2">"max"</code><code class="p">:</code> <code class="mf">.6</code>&#13;
      <code class="p">},</code>&#13;
    <code class="p">},</code>&#13;
    <code class="s2">"metric"</code> <code class="p">:</code> <code class="p">{</code>&#13;
      <code class="s1">'name'</code><code class="p">:</code> <code class="s1">'Test omse_mean'</code><code class="p">,</code>&#13;
      <code class="s1">'goal'</code><code class="p">:</code> <code class="s1">'minimize'</code>&#13;
    <code class="p">}</code>&#13;
  <code class="p">}</code>&#13;
&#13;
  <code class="n">sweep_id</code> <code class="o">=</code> <code class="n">wandb</code><code class="o">.</code><code class="n">sweep</code><code class="p">(</code><code class="n">sweep_config</code><code class="p">,</code> <code class="n">project</code><code class="o">=</code><code class="s2">"jax-mf"</code><code class="p">,</code> <code class="n">entity</code><code class="o">=</code><code class="s2">"wandb-un"</code><code class="p">)</code>&#13;
&#13;
&#13;
<code class="n">wandb</code><code class="o">.</code><code class="n">agent</code><code class="p">(</code><code class="n">sweep_id</code><code class="p">,</code> <code class="n">function</code><code class="o">=</code><code class="n">train_with_reg_loss_CV</code><code class="p">,</code> <code class="n">count</code><code class="o">=</code><code class="n">count</code><code class="p">)</code></pre>&#13;
&#13;
<p class="less_space pagebreak-before">That may seem like a lot of setup, but we’ve really achieved a lot here. We’ve initialized the model to optimize the two matrix factors while simultaneously keeping the matrix elements and the Gramians small.</p>&#13;
&#13;
<p>This brings us to our lovely images.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Output from HPO MF" data-type="sect3"><div class="sect3" id="id115">&#13;
<h3>Output from HPO MF</h3>&#13;
&#13;
<p>Let’s<a data-primary="matrix factorization (MF)" data-secondary="HPO output" data-type="indexterm" id="id910"/> have a quick look at what the prior work has produced. First, <a data-type="xref" href="#overallLoss">Figure 10-3</a> shows that our primary loss function,<a data-primary="observed mean square error (OMSE)" data-type="indexterm" id="id911"/> observed mean square error (OMSE), is rapidly decreasing. This is great, but we should take a deeper look.</p>&#13;
&#13;
<figure><div class="figure" id="overallLoss">&#13;
<img alt="The loss during training" src="assets/brpj_1003.png"/>&#13;
<h6><span class="label">Figure 10-3. </span>The loss during training</h6>&#13;
</div></figure>&#13;
&#13;
<p>Let’s also have a quick look to ensure that our regularization parameters (<a data-type="xref" href="#regparams">Figure 10-4</a>) are converging. We  can see that our L2 regularization could probably still decrease if we were to continue for more epochs.</p>&#13;
&#13;
<figure><div class="figure" id="regparams">&#13;
<img alt="Regularization Params" src="assets/brpj_1004.png"/>&#13;
<h6><span class="label">Figure 10-4. </span>Regularization parameters</h6>&#13;
</div></figure>&#13;
&#13;
<p>We’d like to see our cross-validation laid out by fold and corresponding loss (<a data-type="xref" href="#CrossVal">Figure 10-5</a>). This is a<a data-primary="parallel coordinates chart" data-type="indexterm" id="id912"/> <em>parallel coordinates chart</em>; its lines correspond to different runs that are in correspondence with different choices of parameters, and its vertical axes are different metrics. The far-right heatmap axis corresponds to the overall total loss that we’re trying to minimize. In this case, we alternate test loss on a fold and total loss on that fold. Lower numbers are better, and we hope to see individual lines consistent across their loss per fold (otherwise, we may have a skewed dataset). We see that choices of hyperparameters can interact with fold behavior, but in all the low-loss scenarios (at the bottom), we see a high correlation between performance on different folds (the vertical axes in the plot).</p>&#13;
&#13;
<figure><div class="figure" id="CrossVal">&#13;
<img alt="The loss during training" src="assets/brpj_1005.png"/>&#13;
<h6><span class="label">Figure 10-5. </span>The loss during training</h6>&#13;
</div></figure>&#13;
&#13;
<p>Next up, which choices of hyperparameters have a strong effect on performance? <a data-type="xref" href="#HPO">Figure 10-6</a> is another parallel coordinates plot with the vertical axes corresponding to different hyperparameters. Generally, we’re looking for which domains on the vertical axes correspond to low loss on the far-right heatmap. We see that some of our hyperparameters like priors distribution and, somewhat surprisingly, <code>ell_2</code> have virtually no effect. However, small embedding dimension and small Gramian weight definitely do. A larger alpha also seems to correlate well with good performance.</p>&#13;
&#13;
<figure><div class="figure" id="HPO">&#13;
<img alt="The loss during training" src="assets/brpj_1006.png"/>&#13;
<h6><span class="label">Figure 10-6. </span>The loss by hyperparameter</h6>&#13;
</div></figure>&#13;
&#13;
<p>Finally, we see that as we do a Bayesian hyperparameter search, we really do improve our performance over time. <a data-type="xref" href="#pareto-curve">Figure 10-7</a> is a Pareto plot in which each dot in the scatterplot represents one run, and left to right is a time axis. The vertical axis is overall total loss, so lower is better, and it means that generally we’re converging toward better performance. The<a data-primary="Pareto frontier" data-type="indexterm" id="id913"/> line inscribed along the bottom of the convex hull of the scatter points is the <em>Pareto frontier</em>, or the best performance at that x value. Since this is a time-series Pareto plot, it merely tracks the best performance in time.</p>&#13;
&#13;
<p>You may be wondering how and why we’re able to converge to better loss values in time. This is because we’ve conducted a Bayesian hyperparameter search, which means we selected our hyperparameters from independent Gaussians, and we updated our priors for each parameter based on performance of previous runs. For an introduction to this method, see  <a href="https://oreil.ly/4nd3D">“Bayesian Hyperparameter Optimization—A Primer”</a> by Robert Mitson. In a real setting, we’d see less monotonic behavior in this plot, but we’d always be hoping to improve.</p>&#13;
&#13;
<figure><div class="figure" id="pareto-curve">&#13;
<img alt="The loss during training" src="assets/brpj_1007.png"/>&#13;
<h6><span class="label">Figure 10-7. </span>The Pareto frontier of the loss values</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Prequential validation" data-type="sect3"><div class="sect3" id="prequential">&#13;
<h3>Prequential validation</h3>&#13;
&#13;
<p>If<a data-primary="prequential validation" data-type="indexterm" id="id914"/> we were to put the preceding approach into practice, we would need to capture our trained models in a model registry for use in production. Best practice is to establish a set of explicit evaluations against which to test a selection of models. In your basic ML training, you’ve likely been encouraged to think about  validation datasets; these may take many forms, testing particular subsets of instances or features or even distributed across covariates in a known way.</p>&#13;
&#13;
<p>One useful framing for recommendation systems is to remember that they’re a fundamentally sequential dataset. With this in mind, let’s take another look at our ratings data. Later we will talk more about sequential recommenders, but while we’re talking about validation, it’s useful to mention how to take proper care.</p>&#13;
&#13;
<p class="less_space pagebreak-before">Notice that all our ratings have an associated timestamp. To build a proper validation set, it’s a good idea to take that timestamp from the end of our data.</p>&#13;
&#13;
<p>However, you might be wondering, “When are different users active?” and “Is it possible that the later timestamps are a biased selection of the ratings?” These are important questions. To account for these questions, we should do a holdout by user.</p>&#13;
&#13;
<p>To create this <em>prequential dataset</em>, where the test set follows directly after the training set in a chronological sequence, start by deciding on a desired size for validation, like 10%. Next, group the data by user. Finally, employ rejection sampling, ensuring you don’t use the most recent timestamp as the rejection criterion.</p>&#13;
&#13;
<p>Here’s a simple implementation for pandas using rejection sampling. This is not the most computationally efficient implementation, but it will get the job done:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">prequential_validation_set</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="n">holdout_perc</code><code class="o">=</code><code class="mf">0.1</code><code class="p">):</code>&#13;
    <code class="sd">'''</code>&#13;
<code class="sd">    We utilize rejection sampling.</code>&#13;
&#13;
<code class="sd">    Assign a probability to all observations, if they lie below the</code>&#13;
<code class="sd">    sample percentage AND they're the most recent still in the set, include.</code>&#13;
&#13;
<code class="sd">    Otherwise return them and repeat.</code>&#13;
<code class="sd">    Each time, take no more than the remaining necessary to fill the count.</code>&#13;
<code class="sd">    '''</code>&#13;
    <code class="n">count</code> <code class="o">=</code> <code class="nb">int</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">df</code><code class="p">)</code><code class="o">*</code><code class="n">holdout_perc</code><code class="p">)</code>&#13;
    <code class="n">sample</code> <code class="o">=</code> <code class="p">[]</code>&#13;
    <code class="k">while</code> <code class="n">count</code> <code class="o">&gt;</code><code class="mi">0</code><code class="p">:</code>&#13;
      <code class="n">df</code><code class="p">[</code><code class="s1">'p'</code><code class="p">]</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">rand</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">df</code><code class="p">),</code><code class="mi">1</code><code class="p">)</code> <code class="c1">#generate probabilities</code>&#13;
      <code class="n">x</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code>&#13;
          <code class="n">df</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="o">~</code><code class="n">df</code><code class="o">.</code><code class="n">index</code><code class="o">.</code><code class="n">isin</code><code class="p">(</code><code class="n">sample</code><code class="p">)]</code> <code class="c1"># exclude already selected</code>&#13;
          <code class="o">.</code><code class="n">sort_values</code><code class="p">([</code><code class="s1">'unix_timestamp'</code><code class="p">],</code> <code class="n">ascending</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>&#13;
          <code class="o">.</code><code class="n">groupby</code><code class="p">(</code><code class="s1">'user_id'</code><code class="p">)</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code> <code class="c1"># only allow the first in each group</code>&#13;
          <code class="o">.</code><code class="n">query</code><code class="p">(</code><code class="s2">"p &lt; @holdout_perc"</code><code class="p">)</code><code class="o">.</code><code class="n">index</code> <code class="c1"># grab the indices</code>&#13;
      <code class="p">)</code>&#13;
      <code class="n">rnd</code><code class="o">.</code><code class="n">shuffle</code><code class="p">(</code><code class="n">x</code><code class="p">)</code> <code class="c1"># ensure our previous sorting doesn't bias the users subset</code>&#13;
      <code class="n">sample</code> <code class="o">+=</code> <code class="n">x</code><code class="p">[:</code><code class="n">count</code><code class="p">]</code> <code class="c1"># add observations up to the remaining needed</code>&#13;
      <code class="n">count</code> <code class="o">-=</code> <code class="nb">len</code><code class="p">(</code><code class="n">x</code><code class="p">[:</code><code class="n">count</code><code class="p">])</code> <code class="c1"># decrement the remaining needed</code>&#13;
&#13;
    <code class="n">df</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'p'</code><code class="p">],</code> <code class="n">inplace</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>&#13;
&#13;
    <code class="n">test</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">sample</code><code class="p">]</code>&#13;
    <code class="n">train</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="o">~</code><code class="n">df</code><code class="o">.</code><code class="n">index</code><code class="o">.</code><code class="n">isin</code><code class="p">(</code><code class="n">test</code><code class="o">.</code><code class="n">index</code><code class="p">)]</code>&#13;
    <code class="k">return</code> <code class="n">train</code><code class="p">,</code> <code class="n">test</code></pre>&#13;
&#13;
<p>This is an effective and important validation scheme for inherently sequential <span class="keep-together">datasets.</span><a data-primary="" data-startref="MRimple10" data-type="indexterm" id="id915"/><a data-primary="" data-startref="JAXregMF10" data-type="indexterm" id="id916"/><a data-primary="" data-startref="RRimpl10" data-type="indexterm" id="id917"/><a data-primary="" data-startref="regmatfacto10" data-type="indexterm" id="id918"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="less_space pagebreak-before" data-pdf-bookmark="WSABIE" data-type="sect2"><div class="sect2" id="id117">&#13;
<h2>WSABIE</h2>&#13;
&#13;
<p>Let’s<a data-primary="dimensionality reduction" data-secondary="WSABIE" data-type="indexterm" id="id919"/><a data-primary="WSABIE (web scale annotation by image embedding)" data-type="indexterm" id="id920"/><a data-primary="web scale annotation by image embedding (WSABIE)" data-type="indexterm" id="id921"/> focus again on optimizations and modifications. Another optimization is to treat the MF problem as a single optimization.</p>&#13;
&#13;
<p>The paper <a href="https://oreil.ly/1GB3x">“WSABIE: Scaling Up to Large Vocabulary Image Annotation”</a>  by Jason Weston et al. also contains a factorization for just the item matrix. In this scheme, we replace the user matrix with a weighted sum of the items a user has affinity to. We  cover web scale annotation by image embedding (WSABIE) and Warp loss in <a data-type="xref" href="ch12.html#warp">“WARP”</a>. Representing a user as the average of items they like is a way of saving space and not needing a separate user matrix if there are large numbers of users.</p>&#13;
<div data-type="note" epub:type="note"><h1>Latent Space HPO</h1>&#13;
<p>A<a data-primary="latent space" data-secondary="hyper-parameter optimization for" data-type="indexterm" id="id922"/><a data-primary="optimization" data-secondary="hyper-parameter optimization for latent space" data-type="indexterm" id="id923"/> completely alternative way to do HPO for RecSys is via the latent spaces themselves! <a href="https://oreil.ly/RLeEC">“Hyper-Parameter Optimization for Latent Spaces in Dynamic Recommender Systems”</a>  by Bruno Veloso et al. attempts to modify the relative embeddings during each step to optimize the embedding model.<a data-primary="" data-startref="LRMreducing10" data-type="indexterm" id="id924"/></p>&#13;
</div>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Dimension Reduction" data-type="sect1"><div class="sect1" id="id118">&#13;
<h1>Dimension Reduction</h1>&#13;
&#13;
<p>Dimension-reduction techniques<a data-primary="low-rank methods" data-secondary="dimensionality reduction" data-type="indexterm" id="LRMdimred10"/> are frequently employed in recommendation systems to decrease computational complexity and enhance the accuracy of recommendation algorithms. In this context, the<a data-primary="dimensionality reduction" data-secondary="primary concepts of" data-type="indexterm" id="DRprimcon10"/> primary concepts of dimension reduction for recommendation systems include MF and SVD.</p>&#13;
&#13;
<p>The<a data-primary="matrix factorization (MF)" data-secondary="matrix factorization method" data-type="indexterm" id="id925"/> <em>matrix factorization method</em> decomposes the user-item interaction matrix <math alttext="left-parenthesis upper A element-of double-struck upper R Superscript left-parenthesis m times n right-parenthesis Baseline right-parenthesis">&#13;
  <mrow>&#13;
    <mo>(</mo>&#13;
    <mi>A</mi>&#13;
    <mo>∈</mo>&#13;
    <msup><mi>ℝ</mi> <mrow><mo>(</mo><mi>m</mi><mo>×</mo><mi>n</mi><mo>)</mo></mrow> </msup>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math> into two lower-dimensional matrices, representing the user <math alttext="left-parenthesis upper U element-of double-struck upper R Superscript left-parenthesis m times r right-parenthesis Baseline right-parenthesis">&#13;
  <mrow>&#13;
    <mo>(</mo>&#13;
    <mi>U</mi>&#13;
    <mo>∈</mo>&#13;
    <msup><mi>ℝ</mi> <mrow><mo>(</mo><mi>m</mi><mo>×</mo><mi>r</mi><mo>)</mo></mrow> </msup>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math> and item <math alttext="left-parenthesis upper V element-of double-struck upper R Superscript left-parenthesis n times r right-parenthesis Baseline right-parenthesis">&#13;
  <mrow>&#13;
    <mo>(</mo>&#13;
    <mi>V</mi>&#13;
    <mo>∈</mo>&#13;
    <msup><mi>ℝ</mi> <mrow><mo>(</mo><mi>n</mi><mo>×</mo><mi>r</mi><mo>)</mo></mrow> </msup>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math> latent factors, respectively. This technique can reveal the underlying data structure and offer recommendations based on a user’s previous interactions. Mathematically, MF can be represented as follows:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper A tilde upper U times upper V Superscript left-parenthesis upper T right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <mi>A</mi>&#13;
    <mo>∼</mo>&#13;
    <mi>U</mi>&#13;
    <mo>×</mo>&#13;
    <msup><mi>V</mi> <mrow><mo>(</mo><mi>T</mi><mo>)</mo></mrow> </msup>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p><em>SVD</em> is<a data-primary="singular value decomposition (SVD)" data-type="indexterm" id="id926"/> a linear-algebra technique that decomposes a matrix (<em>A</em>) into three matrices—the left singular vectors (<em>U</em>), the singular values (<em>Σ</em>), and the right singular vectors (<em>V</em>). SVD can be utilized for MF in recommendation systems, where the user-item interaction matrix is decomposed into a smaller number of latent factors. The mathematical representation of SVD is as follows:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper A equals upper U times normal upper Sigma times upper V Superscript left-parenthesis upper T right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <mi>A</mi>&#13;
    <mo>=</mo>&#13;
    <mi>U</mi>&#13;
    <mo>×</mo>&#13;
    <mi>Σ</mi>&#13;
    <mo>×</mo>&#13;
    <msup><mi>V</mi> <mrow><mo>(</mo><mi>T</mi><mo>)</mo></mrow> </msup>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>In practice, though, rather than using a mathematical library to find the eigenvectors, folks might use the <a href="https://oreil.ly/DCsRs">power iteration method</a> to discover the<a data-primary="eigenvectors" data-type="indexterm" id="id927"/> eigenvectors approximately. This method is far more scalable than a full dense SVD solution that is optimized for correctness and dense vectors:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">jax</code>&#13;
<code class="kn">import</code> <code class="nn">jax.numpy</code> <code class="k">as</code> <code class="nn">jnp</code>&#13;
&#13;
&#13;
<code class="k">def</code> <code class="nf">power_iteration</code><code class="p">(</code><code class="n">a</code><code class="p">:</code> <code class="n">jnp</code><code class="o">.</code><code class="n">ndarray</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jnp</code><code class="o">.</code><code class="n">ndarray</code><code class="p">:</code>&#13;
  <code class="sd">"""Returns an eigenvector of the matrix a.</code>&#13;
<code class="sd">  Args:</code>&#13;
<code class="sd">    a: a n x m matrix</code>&#13;
<code class="sd">  """</code>&#13;
  <code class="n">key</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>&#13;
  <code class="n">x</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">normal</code><code class="p">(</code><code class="n">key</code><code class="p">,</code> <code class="n">shape</code><code class="o">=</code><code class="p">(</code><code class="n">a</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="mi">1</code><code class="p">))</code>&#13;
  <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">100</code><code class="p">):</code>&#13;
    <code class="n">x</code> <code class="o">=</code> <code class="n">a</code> <code class="o">@</code> <code class="n">x</code>&#13;
    <code class="n">x</code> <code class="o">=</code> <code class="n">x</code> <code class="o">/</code> <code class="n">jnp</code><code class="o">.</code><code class="n">linalg</code><code class="o">.</code><code class="n">norm</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>&#13;
  <code class="k">return</code> <code class="n">x</code><code class="o">.</code><code class="n">T</code>&#13;
&#13;
<code class="n">key</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="mi">123</code><code class="p">)</code>&#13;
<code class="n">A</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">normal</code><code class="p">(</code><code class="n">key</code><code class="p">,</code> <code class="n">shape</code><code class="o">=</code><code class="p">[</code><code class="mi">4</code><code class="p">,</code> <code class="mi">4</code><code class="p">])</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="n">A</code><code class="p">)</code>&#13;
<code class="p">[[</code> <code class="mf">0.52830553</code>  <code class="mf">0.3722206</code>  <code class="o">-</code><code class="mf">1.2219944</code>  <code class="o">-</code><code class="mf">0.10314374</code><code class="p">]</code>&#13;
 <code class="p">[</code> <code class="mf">1.4722222</code>   <code class="mf">0.47889313</code> <code class="o">-</code><code class="mf">1.2940298</code>   <code class="mf">1.0449569</code> <code class="p">]</code>&#13;
 <code class="p">[</code> <code class="mf">0.23724185</code>  <code class="mf">0.3545859</code>  <code class="o">-</code><code class="mf">0.172465</code>   <code class="o">-</code><code class="mf">1.8011322</code> <code class="p">]</code>&#13;
 <code class="p">[</code> <code class="mf">0.4864215</code>   <code class="mf">0.08039388</code> <code class="o">-</code><code class="mf">1.2540827</code>   <code class="mf">0.72071517</code><code class="p">]]</code>&#13;
<code class="n">S</code><code class="p">,</code> <code class="n">_</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">linalg</code><code class="o">.</code><code class="n">svd</code><code class="p">(</code><code class="n">A</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="n">S</code><code class="p">)</code>&#13;
<code class="p">[[</code><code class="o">-</code><code class="mf">0.375782</code>    <code class="mf">0.40269807</code>  <code class="mf">0.44086716</code> <code class="o">-</code><code class="mf">0.70870167</code><code class="p">]</code>&#13;
 <code class="p">[</code><code class="o">-</code><code class="mf">0.753597</code>    <code class="mf">0.0482972</code>  <code class="o">-</code><code class="mf">0.65527284</code>  <code class="mf">0.01940039</code><code class="p">]</code>&#13;
 <code class="p">[</code> <code class="mf">0.2040088</code>   <code class="mf">0.91405433</code> <code class="o">-</code><code class="mf">0.15798494</code>  <code class="mf">0.31293103</code><code class="p">]</code>&#13;
 <code class="p">[</code><code class="o">-</code><code class="mf">0.49925917</code> <code class="o">-</code><code class="mf">0.00250015</code>  <code class="mf">0.5927009</code>   <code class="mf">0.6320123</code> <code class="p">]]</code>&#13;
<code class="n">x1</code> <code class="o">=</code> <code class="n">power_iteration</code><code class="p">(</code><code class="n">A</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="n">x1</code><code class="o">.</code><code class="n">T</code><code class="p">)</code>&#13;
<code class="p">[[</code><code class="o">-</code><code class="mf">0.35423845</code><code class="p">]</code>&#13;
 <code class="p">[</code><code class="o">-</code><code class="mf">0.8332922</code> <code class="p">]</code>&#13;
 <code class="p">[</code> <code class="mf">0.16189891</code><code class="p">]</code>&#13;
 <code class="p">[</code><code class="o">-</code><code class="mf">0.39233655</code><code class="p">]]</code></pre>&#13;
&#13;
<p>Notice that the eigenvector returned by the power iteration is close to the first column of <math alttext="upper S">&#13;
  <mi>S</mi>&#13;
</math>, but not quite. This is because the method is approximate. It relies on the fact that an eigenvector doesn’t change in direction when multiplied by the matrix. So by repeatedly multiplying by the matrix, we eventually iterate onto an eigenvector. Also notice that we solved for column eigenvectors instead of the row eigenvectors. In this example, the columns are users, and the rows are items. It is important to play with transposed matrices because a lot of ML involves reshaping and transposing matrices, so getting used to them early is an important skill.</p>&#13;
<div data-type="tip"><h1>Eigenvector Examples</h1>&#13;
<p>Here’s<a data-primary="eigenvectors" data-type="indexterm" id="id928"/> a nice exercise for you: the second eigenvector is computed by subtracting out the first eigenvector after the matrix multiplication. This is telling the algorithm to ignore any component along the first eigenvector in order to compute the second eigenvector. As a fun exercise, hop over to <a href="https://oreil.ly/0zmWq">Colab</a> and try computing the second eigenvector. Extending this to sparse vector representations is another interesting exercise, as it allows you to start computing the eigenvectors of sparse matrices, which is usually the form of matrix that recommender systems use.</p>&#13;
</div>&#13;
&#13;
<p>Next, we construct a recommendation for a user by creating a column and then taking the dot product with all the eigenvectors and finding the closest. We then find all the highest-scoring entries in the eigenvector that the user hasn’t seen and return them as recommendations. So in the preceding example, if the eigenvector <math alttext="x 1">&#13;
  <msub><mi>x</mi> <mn>1</mn> </msub>&#13;
</math> was the closest to the user column, then the best item to recommend would be item 3 because it is the largest component in the eigenvector and thus rated most highly if the user is closest to the eigenvector <math alttext="x 1">&#13;
  <msub><mi>x</mi> <mn>1</mn> </msub>&#13;
</math>. Here’s what this looks like in code:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">jax</code>&#13;
<code class="kn">import</code> <code class="nn">jax.numpy</code> <code class="k">as</code> <code class="nn">jnp</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">recommend_items</code><code class="p">(</code><code class="n">eigenvectors</code><code class="p">:</code> <code class="n">jnp</code><code class="o">.</code><code class="n">ndarray</code><code class="p">,</code> <code class="n">user</code><code class="p">:</code><code class="n">jnp</code><code class="o">.</code><code class="n">ndarray</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jnp</code><code class="o">.</code><code class="n">ndarray</code><code class="p">:</code>&#13;
  <code class="sd">"""Returns an ordered list of recommend items for the user.</code>&#13;
<code class="sd">  Args:</code>&#13;
<code class="sd">    eigenvectors: a nxm eigenvector matrix</code>&#13;
<code class="sd">    user: a user vector of size m.</code>&#13;
<code class="sd">  """</code>&#13;
  <code class="n">score_eigenvectors</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">matmul</code><code class="p">(</code><code class="n">eigenvectors</code><code class="o">.</code><code class="n">T</code><code class="p">,</code> <code class="n">user</code><code class="p">)</code>&#13;
  <code class="n">which_eigenvector</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">score_eigenvectors</code><code class="p">)</code>&#13;
  <code class="n">closest_eigenvector</code> <code class="o">=</code> <code class="n">eigenvectors</code><code class="o">.</code><code class="n">T</code><code class="p">[</code><code class="n">which_eigenvector</code><code class="p">]</code>&#13;
  <code class="n">scores</code><code class="p">,</code> <code class="n">items</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">lax</code><code class="o">.</code><code class="n">top_k</code><code class="p">(</code><code class="n">closest_eigenvector</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>&#13;
  <code class="k">return</code> <code class="n">scores</code><code class="p">,</code> <code class="n">items</code>&#13;
&#13;
<code class="n">S</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">array</code><code class="p">(</code>&#13;
<code class="p">[[</code><code class="o">-</code><code class="mf">0.375782</code><code class="p">,</code>    <code class="mf">0.40269807</code><code class="p">],</code>&#13;
 <code class="p">[</code><code class="o">-</code><code class="mf">0.753597</code><code class="p">,</code>    <code class="mf">0.0482972</code><code class="p">],</code>&#13;
 <code class="p">[</code> <code class="mf">0.2040088</code><code class="p">,</code>   <code class="mf">0.91405433</code><code class="p">],</code>&#13;
 <code class="p">[</code><code class="o">-</code><code class="mf">0.49925917</code><code class="p">,</code> <code class="o">-</code><code class="mf">0.00250015</code><code class="p">]])</code>&#13;
<code class="n">u</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">])</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="mi">4</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>&#13;
<code class="n">scores</code><code class="p">,</code> <code class="n">items</code> <code class="o">=</code> <code class="n">recommend_items</code><code class="p">(</code><code class="n">S</code><code class="p">,</code> <code class="n">u</code><code class="p">)</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="n">scores</code><code class="p">)</code>&#13;
<code class="p">[</code> <code class="mf">0.2040088</code>  <code class="o">-</code><code class="mf">0.375782</code>   <code class="o">-</code><code class="mf">0.49925917</code><code class="p">]</code>&#13;
<code class="nb">print</code><code class="p">(</code><code class="n">items</code><code class="p">)</code>&#13;
<code class="p">[</code><code class="mi">2</code> <code class="mi">0</code> <code class="mi">3</code><code class="p">]</code></pre>&#13;
&#13;
<p class="less_space pagebreak-before">In this example, a user has downvoted item 0 and item 1. The closest column eigenvector is therefore column 0. We then select the closest eigenvector to the user, order the entries, and recommend item 2 to the user, which is the highest-scoring entry that the user has not seen.<a data-primary="" data-startref="DRprimcon10" data-type="indexterm" id="id929"/></p>&#13;
&#13;
<p>Two techniques<a data-primary="dimensionality reduction" data-secondary="extracting most relevant features" data-type="indexterm" id="id930"/> aim to extract the most relevant features from the user-item interaction matrix and reduce its dimensionality, which can improve performance:</p>&#13;
<dl>&#13;
<dt>Principal component analysis (PCA)</dt>&#13;
<dd>&#13;
<p>This<a data-primary="principal component analysis (PCA)" data-type="indexterm" id="id931"/><a data-primary="PCA (principal component analysis)" data-type="indexterm" id="id932"/> statistical technique transforms the original high-dimensional data into a lower-dimensional representation while retaining the most important information. PCA can be applied to the user-item interaction matrix to reduce the number of dimensions and improve the computational efficiency of the recommendation algorithm.</p>&#13;
</dd>&#13;
<dt>Nonnegative matrix factorization (NMF)</dt>&#13;
<dd>&#13;
<p>This<a data-primary="nonnegative matrix factorization (NMF)" data-type="indexterm" id="id933"/> technique decomposes the nonnegative user-item interaction matrix <math alttext="left-parenthesis upper A element-of double-struck upper R Superscript left-parenthesis m times n right-parenthesis asterisk plus Baseline right-parenthesis">&#13;
  <mrow>&#13;
    <mo>(</mo>&#13;
    <mi>A</mi>&#13;
    <mo>∈</mo>&#13;
    <msup><mi>ℝ</mi> <mrow><mo>(</mo><mi>m</mi><mo>×</mo><mi>n</mi><mo>)</mo><mo>*</mo><mo>+</mo></mrow> </msup>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math> into two nonnegative matrices <math alttext="left-parenthesis upper W element-of double-struck upper R Superscript left-parenthesis m times r right-parenthesis asterisk plus">&#13;
  <mrow>&#13;
    <mo>(</mo>&#13;
    <mi>W</mi>&#13;
    <mo>∈</mo>&#13;
    <msup><mi>ℝ</mi> <mrow><mo>(</mo><mi>m</mi><mo>×</mo><mi>r</mi><mo>)</mo><mo>*</mo><mo>+</mo></mrow> </msup>&#13;
  </mrow>&#13;
</math> and <math alttext="upper H element-of double-struck upper R Superscript left-parenthesis r times n right-parenthesis Super Subscript plus Superscript Baseline right-parenthesis">&#13;
  <mrow>&#13;
    <mi>H</mi>&#13;
    <mo>∈</mo>&#13;
    <msup><mi>ℝ</mi> <msub><mrow><mo>(</mo><mi>r</mi><mo>×</mo><mi>n</mi><mo>)</mo></mrow> <mo>+</mo> </msub> </msup>&#13;
    <mrow>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math>. NMF can be utilized for dimension reduction in recommendation systems, where the latent factors are nonnegative and interpretable. The mathematical representation of NMF is <math alttext="upper A asymptotically-equals upper W times upper H">&#13;
  <mrow>&#13;
    <mi>A</mi>&#13;
    <mo>≃</mo>&#13;
    <mi>W</mi>&#13;
    <mo>×</mo>&#13;
    <mi>H</mi>&#13;
  </mrow>&#13;
</math>.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>MF techniques can be further extended to incorporate additional information, such as item content or user demographic data, through the use of side information. Side information can be employed to augment the user-item interaction matrix, allowing for more accurate and personalized recommendations.</p>&#13;
&#13;
<p>Furthermore, MF models can be extended to handle implicit feedback data, where the absence of interaction data is not equivalent to the lack of interest. By incorporating additional regularization terms into the objective function, MF models can learn a more robust representation of the user-item interaction matrix, leading to better recommendations for implicit feedback scenarios.</p>&#13;
&#13;
<p>Consider a recommendation system that employs MF to model the user-item interaction matrix. If the system comprises many users and items, the resulting factor matrices can be high-dimensional and computationally expensive to process. However, by using dimension-reduction techniques like SVD or PCA, the algorithm can reduce the dimensionality of the factor matrices while preserving the most important information about the user-item interactions. This enables the algorithm to generate more efficient and accurate recommendations, even for new users or items with limited interaction data.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="less_space pagebreak-before" data-pdf-bookmark="Isometric Embeddings" data-type="sect2"><div class="sect2" id="id119">&#13;
<h2>Isometric Embeddings</h2>&#13;
&#13;
<p><em>Isometric embeddings</em> are<a data-primary="dimensionality reduction" data-secondary="isometric embeddings" data-type="indexterm" id="DRisometric10"/><a data-primary="isometric embeddings" data-type="indexterm" id="isoembed10"/><a data-primary="embedding models" data-secondary="isometric embeddings" data-type="indexterm" id="EMiso10"/> a specific type of embedding that maintains distances between points in high-dimensional space when mapping them onto a lower-dimensional space. The term <em>isometric</em> signifies that the distances between points in the high-dimensional space are preserved precisely in the lower-dimensional space, up to a scaling factor.</p>&#13;
&#13;
<p>In contrast to other types of embeddings, such as linear or nonlinear embeddings, which may distort the distances between points, isometric embeddings are preferable in numerous applications where distance preservation is essential. For example, in ML, isometric embeddings can be employed to visualize high-dimensional data in two or three dimensions while preserving the relative distances between the data points. In NLP, isometric embeddings can be utilized to represent the semantic similarities between words or documents while maintaining their relative distances in the embedding space.</p>&#13;
&#13;
<p>One popular technique for generating isometric embeddings is<a data-primary="MDS (multidimensional scaling)" data-type="indexterm" id="id934"/><a data-primary="multidimensional scaling (MDS)" data-type="indexterm" id="id935"/> <em>multidimensional scaling</em> (<em>MDS</em>). MDS operates by computing pairwise distances between the data points in the high-dimensional space and then determining a lower-dimensional embedding that preserves these distances. The optimization problem is generally formulated as a constrained optimization problem, where the objective is to minimize the difference between the pairwise distances in the high-dimensional space and the corresponding distances in the lower-dimensional embedding. Mathematically, we write: <math alttext="m i n Subscript left-parenthesis upper X right-parenthesis Baseline sigma-summation Underscript left-parenthesis i comma j right-parenthesis Endscripts left-parenthesis d Subscript i j Baseline minus StartAbsoluteValue EndAbsoluteValue x Subscript i Baseline minus x Subscript j Baseline StartAbsoluteValue EndAbsoluteValue right-parenthesis squared">&#13;
  <mrow>&#13;
    <mi>m</mi>&#13;
    <mi>i</mi>&#13;
    <msub><mi>n</mi> <mrow><mo>(</mo><mi>X</mi><mo>)</mo></mrow> </msub>&#13;
    <msub><mo>∑</mo> <mrow><mo>(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo>)</mo></mrow> </msub>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
    </mrow>&#13;
    <msub><mi>d</mi> <mrow><mi>i</mi><mi>j</mi></mrow> </msub>&#13;
    <mo>-</mo>&#13;
    <mrow>&#13;
      <mo>|</mo>&#13;
      <mo>|</mo>&#13;
    </mrow>&#13;
    <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
    <mo>-</mo>&#13;
    <msub><mi>x</mi> <mi>j</mi> </msub>&#13;
    <msup><mrow><mo>|</mo><mo>|</mo><mo>)</mo></mrow> <mn>2</mn> </msup>&#13;
  </mrow>&#13;
</math>.</p>&#13;
&#13;
<p>Here, <math alttext="d Subscript i j">&#13;
  <msub><mi>d</mi> <mrow><mi>i</mi><mi>j</mi></mrow> </msub>&#13;
</math> denotes the pairwise distances in the high-dimensional space, and <math alttext="x Subscript i">&#13;
  <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
</math> and <math alttext="x Subscript j">&#13;
  <msub><mi>x</mi> <mi>j</mi> </msub>&#13;
</math> represent points in the lower-dimensional embedding.</p>&#13;
&#13;
<p>Another<a data-primary="kernel methods" data-type="indexterm" id="id936"/> approach for generating isometric embeddings is through the use of kernel methods, such as kernel PCA or kernel MDS. Kernel methods work by implicitly mapping the data points into a higher-dimensional feature space, where the distances between the points are easier to compute. The isometric embedding is then calculated in the feature space, and the resulting embedding is mapped back to the original space.</p>&#13;
&#13;
<p>Isometric embeddings have been employed in recommendation systems to represent the user-item interaction matrix in a lower-dimensional space where the distances between the items are preserved. By preserving the distances between items in the embedding space, the recommendation algorithm can better capture the underlying structure of the data and provide more accurate and diverse recommendations.</p>&#13;
&#13;
<p>Isometric embeddings can also be employed to incorporate additional information into the recommendation algorithm, such as item content or user demographic data. By using isometric embeddings to represent the items and the additional <span class="keep-together">information,</span> the algorithm can capture the similarities between items based on both the user-item interaction data and the item content or user demographics, leading to more accurate and diverse recommendations.</p>&#13;
&#13;
<p>Moreover, isometric embeddings can also be used to address the cold-start problem in recommendation systems. By using the isometric embeddings to represent the items, the algorithm can make recommendations for new items based on their similarities to the existing items in the embedding space, even in the absence of user interactions.</p>&#13;
&#13;
<p>In summary, isometric embeddings are a valuable technique in recommendation systems for representing the user-item interaction matrix in a lower-dimensional space where the distances between the items are preserved. Isometric embeddings can be generated using MF techniques and can be employed to incorporate additional information, address the cold-start problem, and improve the accuracy and diversity of recommendations.<a data-primary="" data-startref="DRisometric10" data-type="indexterm" id="id937"/><a data-primary="" data-startref="isoembed10" data-type="indexterm" id="id938"/><a data-primary="" data-startref="EMiso10" data-type="indexterm" id="id939"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Nonlinear Locally Metrizable Embeddings" data-type="sect2"><div class="sect2" id="id120">&#13;
<h2>Nonlinear Locally Metrizable Embeddings</h2>&#13;
&#13;
<p><em>Nonlinear locally metrizable embeddings</em> are<a data-primary="nonlinear locally metrizable embeddings" data-type="indexterm" id="id940"/><a data-primary="embedding models" data-secondary="nonlinear locally metrizable embeddings" data-type="indexterm" id="id941"/><a data-primary="dimensionality reduction" data-secondary="nonlinear locally metrizable embeddings" data-type="indexterm" id="id942"/> yet another method to represent the user-item interaction matrix in a lower-dimensional space where the local distances between nearby items are preserved. By preserving the local distances between items in the embedding space, the recommendation algorithm can better capture the local structure of the data and provide more accurate and diverse recommendations.</p>&#13;
&#13;
<p>Mathematically, let <math alttext="upper X equals x 1 comma x 2 comma period period period comma x Subscript n Baseline">&#13;
  <mrow>&#13;
    <mi>X</mi>&#13;
    <mo>=</mo>&#13;
    <mrow>&#13;
      <msub><mi>x</mi> <mn>1</mn> </msub>&#13;
      <mo>,</mo>&#13;
      <msub><mi>x</mi> <mn>2</mn> </msub>&#13;
      <mo>,</mo>&#13;
      <mo>.</mo>&#13;
      <mo>.</mo>&#13;
      <mo>.</mo>&#13;
      <mo>,</mo>&#13;
      <msub><mi>x</mi> <mi>n</mi> </msub>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math> be the set of items in the high-dimensional space, and <math alttext="upper Y equals y 1 comma y 2 comma period period period comma y Subscript n Baseline">&#13;
  <mrow>&#13;
    <mi>Y</mi>&#13;
    <mo>=</mo>&#13;
    <mrow>&#13;
      <msub><mi>y</mi> <mn>1</mn> </msub>&#13;
      <mo>,</mo>&#13;
      <msub><mi>y</mi> <mn>2</mn> </msub>&#13;
      <mo>,</mo>&#13;
      <mo>.</mo>&#13;
      <mo>.</mo>&#13;
      <mo>.</mo>&#13;
      <mo>,</mo>&#13;
      <msub><mi>y</mi> <mi>n</mi> </msub>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math> be the set of items in the lower-dimensional space. The goal of nonlinear locally metrizable embeddings is to find a mapping <math alttext="f colon upper X right-arrow upper Y">&#13;
  <mrow>&#13;
    <mi>f</mi>&#13;
    <mo>:</mo>&#13;
    <mi>X</mi>&#13;
    <mo>→</mo>&#13;
    <mi>Y</mi>&#13;
  </mrow>&#13;
</math> that preserves the local distances, i.e., for any <math alttext="x Subscript i Baseline comma x Subscript j Baseline element-of upper X">&#13;
  <mrow>&#13;
    <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
    <mo>,</mo>&#13;
    <msub><mi>x</mi> <mi>j</mi> </msub>&#13;
    <mo>∈</mo>&#13;
    <mi>X</mi>&#13;
  </mrow>&#13;
</math>, we have this:</p>&#13;
<div data-type="equation">&#13;
<math alttext="d Subscript upper Y Baseline left-parenthesis f left-parenthesis x Subscript i Baseline right-parenthesis comma f left-parenthesis x Subscript j Baseline right-parenthesis right-parenthesis asymptotically-equals d Subscript upper X Baseline left-parenthesis x Subscript i Baseline comma x Subscript j Baseline right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <msub><mi>d</mi> <mi>Y</mi> </msub>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>f</mi>&#13;
      <mrow>&#13;
        <mo>(</mo>&#13;
        <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
        <mo>)</mo>&#13;
      </mrow>&#13;
      <mo>,</mo>&#13;
      <mi>f</mi>&#13;
      <mrow>&#13;
        <mo>(</mo>&#13;
        <msub><mi>x</mi> <mi>j</mi> </msub>&#13;
        <mo>)</mo>&#13;
      </mrow>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>≃</mo>&#13;
    <msub><mi>d</mi> <mi>X</mi> </msub>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
      <mo>,</mo>&#13;
      <msub><mi>x</mi> <mi>j</mi> </msub>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>One popular approach to generating nonlinear locally metrizable embeddings in recommendation systems is via autoencoder neural networks. Autoencoders work by mapping the high-dimensional user-item interaction matrix onto a lower-dimensional space through an encoder network, and then reconstructing the matrix back in the high-dimensional space through a decoder network. The encoder and decoder networks are trained jointly to minimize the difference between the input data and the reconstructed data, with the objective of capturing the underlying structure of the data in the embedding space:</p>&#13;
<div data-type="equation">&#13;
<math alttext="m i n Subscript left-parenthesis theta comma phi right-parenthesis Baseline sigma-summation Underscript left-parenthesis i equals 1 right-parenthesis Overscript n Endscripts StartAbsoluteValue EndAbsoluteValue x Subscript i minus g Subscript phi Baseline left-parenthesis f Subscript theta Baseline left-parenthesis x Subscript i Baseline right-parenthesis right-parenthesis StartAbsoluteValue EndAbsoluteValue squared" display="block">&#13;
  <mrow>&#13;
    <mi>m</mi>&#13;
    <mi>i</mi>&#13;
    <msub><mi>n</mi> <mrow><mo>(</mo><mi>θ</mi><mo>,</mo><mi>φ</mi><mo>)</mo></mrow> </msub>&#13;
    <munderover><mo>∑</mo> <mrow><mo>(</mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>)</mo></mrow> <mi>n</mi> </munderover>&#13;
    <mrow>&#13;
      <mo>|</mo>&#13;
      <mo>|</mo>&#13;
    </mrow>&#13;
    <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
    <mo>-</mo>&#13;
    <msub><mi>g</mi> <mi>φ</mi> </msub>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>f</mi> <mi>θ</mi> </msub>&#13;
      <mrow>&#13;
        <mo>(</mo>&#13;
        <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
        <mo>)</mo>&#13;
      </mrow>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <msup><mrow><mo>|</mo><mo>|</mo></mrow> <mn>2</mn> </msup>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Here, <math alttext="f Subscript theta">&#13;
  <msub><mi>f</mi> <mi>θ</mi> </msub>&#13;
</math> denotes the encoder network with parameters <math alttext="theta comma g Subscript theta Baseline">&#13;
  <mrow>&#13;
    <mi>θ</mi>&#13;
    <mo>,</mo>&#13;
    <msub><mi>g</mi> <mi>θ</mi> </msub>&#13;
  </mrow>&#13;
</math> denotes the decoder network with parameters <math alttext="theta">&#13;
  <mi>θ</mi>&#13;
</math>, and <math alttext="StartAbsoluteValue EndAbsoluteValue dot StartAbsoluteValue EndAbsoluteValue">&#13;
  <mrow>&#13;
    <mo>|</mo>&#13;
    <mo>|</mo>&#13;
    <mo>·</mo>&#13;
    <mo>|</mo>&#13;
    <mo>|</mo>&#13;
  </mrow>&#13;
</math> represents the Euclidean norm.</p>&#13;
&#13;
<p>Another approach for generating nonlinear locally metrizable embeddings in recommendation systems is through the use of<a data-primary="t-distributed stochastic neighbor embedding (t-SNE)" data-type="indexterm" id="id943"/> t-distributed stochastic neighbor embedding (t-SNE). t-SNE works by modeling the pairwise similarities between the items in the high-dimensional space, and then finding a lower-dimensional embedding that preserves these similarities.</p>&#13;
&#13;
<p>A more popular approach in modern times is<a data-primary="Uniform Manifold Approximation and Projection (UMAP)" data-type="indexterm" id="id944"/> UMAP, which instead attempts to fit a minimal manifold that preserves density in local neighborhoods. UMAP is an essential technique for finding low-dimensional representations in complex and high-dimensional latent spaces; find it’s documentation at <a class="bare" href="https://oreil.ly/NLqDg"><em class="hyperlink">https://oreil.ly/NLqDg</em></a>. The optimization problem is typically formulated as a cost function <em>C</em> that measures <span class="keep-together">the difference</span> between the pairwise similarities in the high-dimensional space <span class="keep-together">and the</span> corresponding similarities in the lower-dimensional embedding:</p>&#13;
<div data-type="equation">&#13;
<math alttext="dollar-sign upper C left-parenthesis upper Y right-parenthesis equals sigma-summation Underscript left-parenthesis i comma j right-parenthesis Endscripts p Subscript i j Baseline asterisk l o g left-parenthesis StartFraction p Subscript i j Baseline Over q Subscript i j Baseline EndFraction right-parenthesis dollar-sign">&#13;
  <mrow>&#13;
    <mi>C</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>Y</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <msub><mo>∑</mo> <mrow><mo>(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo>)</mo></mrow> </msub>&#13;
    <msub><mi>p</mi> <mrow><mi>i</mi><mi>j</mi></mrow> </msub>&#13;
    <mo>*</mo>&#13;
    <mi>l</mi>&#13;
    <mi>o</mi>&#13;
    <mi>g</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mstyle displaystyle="true" scriptlevel="0">&#13;
        <mfrac><msub><mi>p</mi> <mrow><mi>i</mi><mi>j</mi></mrow> </msub> <msub><mi>q</mi> <mrow><mi>i</mi><mi>j</mi></mrow> </msub></mfrac>&#13;
      </mstyle>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Here, <math alttext="p Subscript i j">&#13;
  <msub><mi>p</mi> <mrow><mi>i</mi><mi>j</mi></mrow> </msub>&#13;
</math> denotes the pairwise similarities in the high-dimensional space, <math alttext="q Subscript i j">&#13;
  <msub><mi>q</mi> <mrow><mi>i</mi><mi>j</mi></mrow> </msub>&#13;
</math> denotes the pairwise similarities in the lower-dimensional space, and the sum is over all pairs of items <math alttext="left-parenthesis i comma j right-parenthesis">&#13;
  <mrow>&#13;
    <mo>(</mo>&#13;
    <mi>i</mi>&#13;
    <mo>,</mo>&#13;
    <mi>j</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>.</p>&#13;
&#13;
<p>Nonlinear locally metrizable embeddings can also be used to incorporate additional information into the recommendation algorithm, such as item content or user demographic data. By using nonlinear locally metrizable embeddings to represent the items and the additional information, the algorithm can capture the similarities between items based on both the user-item interaction data and the item content or user demographics, leading to more accurate and diverse recommendations.</p>&#13;
&#13;
<p>Moreover, nonlinear locally metrizable embeddings can also be used to address the cold-start problem in recommendation systems. By using the nonlinear locally metrizable embeddings to represent the items, the algorithm can make recommendations for new items based on their similarities to the existing items in the embedding space, even in the absence of user interactions.</p>&#13;
&#13;
<p>In summary, nonlinear locally metrizable embeddings are a useful technique in recommendation systems for representing the user-item interaction matrix in a lower-dimensional space where the local distances between nearby items are preserved. Nonlinear locally metrizable embeddings can be generated using techniques such as autoencoder neural networks or t-SNE and can be used to incorporate additional information, address the cold-start problem, and improve the accuracy and diversity of recommendations.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Centered Kernel Alignment" data-type="sect2"><div class="sect2" id="id121">&#13;
<h2>Centered Kernel Alignment</h2>&#13;
&#13;
<p>When<a data-primary="dimensionality reduction" data-secondary="centered kernel alignment" data-type="indexterm" id="id945"/><a data-primary="centered kernel alignment" data-type="indexterm" id="id946"/> training neural networks, the latent space representations at each layer are expected to express correlation structures between the incoming signals. Frequently, these interstitial representations comprise a sequence of states transitioning from the initial layer to the final layer. You may naturally wonder, “How do these representations change throughout the layers of the network” and “How similar are these layers?” Interestingly, for some architectures, this question may yield deep insight into the network’s behavior.</p>&#13;
&#13;
<p>This process of comparing layer representations is called<a data-primary="correlation" data-secondary="correlation analysis" data-type="indexterm" id="id947"/> <em>correlation analysis</em>. For an MLP with layers <math alttext="1 comma ellipsis comma upper N">&#13;
  <mrow>&#13;
    <mn>1</mn>&#13;
    <mo>,</mo>&#13;
    <mo>...</mo>&#13;
    <mo>,</mo>&#13;
    <mi>N</mi>&#13;
  </mrow>&#13;
</math>, the correlations may be represented by an <math alttext="upper N times upper N">&#13;
  <mrow>&#13;
    <mi>N</mi>&#13;
    <mo>×</mo>&#13;
    <mi>N</mi>&#13;
  </mrow>&#13;
</math> matrix of pairwise relationships. The idea is that each layer comprises a series of latent factors, and similar to correlation analysis for other features of a dataset, these latent features’ relationships may be simply summarized by their covariance.<a data-primary="" data-startref="LRMdimred10" data-type="indexterm" id="id948"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Affinity and p-sale" data-type="sect1"><div class="sect1" id="id122">&#13;
<h1>Affinity and p-sale</h1>&#13;
&#13;
<p>As<a data-primary="low-rank methods" data-secondary="affinity and p-sale" data-type="indexterm" id="id949"/><a data-primary="affinity" data-type="indexterm" id="id950"/><a data-primary="probability of a sale (p-sale)" data-type="indexterm" id="id951"/><a data-primary="p-sale (probability of a sale)" data-type="indexterm" id="id952"/> you’ve seen, MF is a powerful dimension-reduction technique that can yield an estimator for the probability of a sale (often shorted to <em>p-sale</em>). In MF, the goal has been to decompose this historical data on user behavior and the product sales matrix into two lower-dimensional matrices: one that represents user preferences and another that represents product characteristics. Now, let’s convert this MF model into a sale estimator.</p>&#13;
&#13;
<p>Let <math alttext="upper R element-of double-struck upper R Superscript left-parenthesis upper M times upper N right-parenthesis">&#13;
  <mrow>&#13;
    <mi>R</mi>&#13;
    <mo>∈</mo>&#13;
    <msup><mi>ℝ</mi> <mrow><mo>(</mo><mi>M</mi><mo>×</mo><mi>N</mi><mo>)</mo></mrow> </msup>&#13;
  </mrow>&#13;
</math> be the historical data matrix, where <em>M</em> is the number of users and <em>N</em> is the number of products. The MF aims to find two matrices <math alttext="upper U element-of double-struck upper R Superscript left-parenthesis upper M times d right-parenthesis">&#13;
  <mrow>&#13;
    <mi>U</mi>&#13;
    <mo>∈</mo>&#13;
    <msup><mi>ℝ</mi> <mrow><mo>(</mo><mi>M</mi><mo>×</mo><mi>d</mi><mo>)</mo></mrow> </msup>&#13;
  </mrow>&#13;
</math> and <math alttext="upper V element-of double-struck upper R Superscript left-parenthesis upper N times d right-parenthesis">&#13;
  <mrow>&#13;
    <mi>V</mi>&#13;
    <mo>∈</mo>&#13;
    <msup><mi>ℝ</mi> <mrow><mo>(</mo><mi>N</mi><mo>×</mo><mi>d</mi><mo>)</mo></mrow> </msup>&#13;
  </mrow>&#13;
</math>, where <math alttext="d">&#13;
  <mi>d</mi>&#13;
</math> is the dimensionality of the latent space, such that:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper R asymptotically-equals upper U asterisk upper V Superscript upper T" display="block">&#13;
  <mrow>&#13;
    <mi>R</mi>&#13;
    <mo>≃</mo>&#13;
    <mi>U</mi>&#13;
    <mo>*</mo>&#13;
    <msup><mi>V</mi> <mi>T</mi> </msup>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>The <em>probability of a sale</em>, or equivalently a read, watch, eat, or click, can be predicted using MF by first decomposing the historical data matrix into user and product matrices, and then calculating a score that represents the likelihood of a user purchasing a given product. This score can be calculated using the dot product of the corresponding row in the user matrix and the column in the product matrix, followed by a logistic function to transform the dot product into a probability score.</p>&#13;
&#13;
<p>Mathematically, the probability of a sale for a user <em>u</em> and a product <em>p</em> can be represented as follows:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper P left-parenthesis u comma p right-parenthesis equals normal s normal i normal g normal m normal o normal i normal d left-parenthesis u asterisk p Superscript upper T Baseline right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>u</mi>&#13;
      <mo>,</mo>&#13;
      <mi>p</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mi> sigmoid </mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>u</mi>&#13;
      <mo>*</mo>&#13;
      <msup><mi>p</mi> <mi>T</mi> </msup>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Here, sigmoid is the logistic function that maps the dot product of the user and product vectors to a probability score between 0 and 1:</p>&#13;
<div data-type="equation">&#13;
<math alttext="s i g m o i d left-parenthesis x right-parenthesis equals 1 slash left-parenthesis 1 plus e x p left-parenthesis negative x right-parenthesis right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <mi>s</mi>&#13;
    <mi>i</mi>&#13;
    <mi>g</mi>&#13;
    <mi>m</mi>&#13;
    <mi>o</mi>&#13;
    <mi>i</mi>&#13;
    <mi>d</mi>&#13;
    <mo>(</mo>&#13;
    <mi>x</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mn>1</mn>&#13;
    <mo>/</mo>&#13;
    <mo>(</mo>&#13;
    <mn>1</mn>&#13;
    <mo>+</mo>&#13;
    <mi>e</mi>&#13;
    <mi>x</mi>&#13;
    <mi>p</mi>&#13;
    <mo>(</mo>&#13;
    <mo>-</mo>&#13;
    <mi>x</mi>&#13;
    <mo>)</mo>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>The <math alttext="p Superscript upper T">&#13;
  <msup><mi>p</mi> <mi>T</mi> </msup>&#13;
</math> represents the transpose of the product vector. The dot product of the user and product vectors is a measure of the similarity between the user’s preferences and the product’s characteristics, and the logistic function maps this similarity score to a probability score.</p>&#13;
&#13;
<p>The user and product matrices can be trained on the historical data by using various MF algorithms, such as SVD, NMF, or ALS. Once the matrices are trained, the dot product and logistic function can be applied to new user-product pairs to predict the probability of a sale. The predicted probabilities can then be used to rank and recommend products to the user.</p>&#13;
&#13;
<p>It’s worth highlighting that, since the loss function for ALS is convex (meaning there is a single global minimum), the convergence can be fast when we fix either the user or item matrix. In this method, the user matrix is fixed and the item matrix is solved for. Then the item matrix is fixed and the user matrix is solved for. The method alternates between the two solutions, and because the loss is convex in this regime, the method converges quickly.</p>&#13;
&#13;
<p>The dot product of the corresponding row in the user matrix and column in the product matrix represents the affinity score between the user and the product, or how well the user’s preferences match the product’s characteristics. However, this score alone may not be a sufficient predictor of whether the user will actually purchase the product.</p>&#13;
&#13;
<p>The logistic function applied to the dot product in the MF model transforms the affinity score into a probability score, which represents the likelihood of a sale. This transformation takes into account additional factors beyond just the user’s preferences and the product’s characteristics, such as the overall popularity of the product, the user’s purchasing behavior, and any other relevant external factors. By incorporating these additional factors, MF is able to better predict the probability of a sale, rather than just an affinity score.</p>&#13;
&#13;
<p>A comparison library (however, not in JAX) for computing latent embeddings linearly is  <a href="http://libfm.org">libFM</a>. The formulation for a factorization machine is similar to a GloVe embedding in that it also models the interaction between two vectors, but the dot product can be used for regression or binary classification tasks. The method can also be extended to recommend more than two kinds of items beyond user and item.</p>&#13;
&#13;
<p>In summary, MF produces probabilities of sale instead of just affinity scores by incorporating additional factors beyond the user’s preferences and the product’s characteristics, and transforming the affinity score into a probability score by using a logistic function.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Propensity Weighting for Recommendation System Evaluation" data-type="sect1"><div class="sect1" id="propensity">&#13;
<h1>Propensity Weighting for Recommendation System Evaluation</h1>&#13;
&#13;
<p>As you’ve seen, recommendation systems<a data-primary="low-rank methods" data-secondary="propensity weighting for evaluation" data-type="indexterm" id="LRMpropen10"/><a data-primary="propensity weighting" data-type="indexterm" id="propweight10"/><a data-primary="recommendation systems" data-secondary="evaluating" data-type="indexterm" id="RSeval10"/><a data-primary="feedback" data-type="indexterm" id="id953"/> are evaluated based on user feedback, which is collected from the deployed recommendation system. However, this data is causally influenced by the deployed system, creating a feedback loop that may bias the evaluation of new models. This feedback loop can lead to confounding variables, making it difficult to distinguish between user preferences and the influence of the deployed system.</p>&#13;
&#13;
<p>If this surprises you, let’s consider for a moment what would have to be true for a recommendation system to <em>not</em> causally influence the actions users take and/or the outcomes that result from those actions. That would require assumptions like “the recommendations are completely ignored by the user” and “the system makes recommendations at random.” Propensity weighting can mitigate some of the worst effects of this problem.</p>&#13;
&#13;
<p>The performance of a recommender system depends on many factors, including user-item characteristics, contextual information, and trends, which can affect the quality of the recommendations and the user engagement. However, the influence can be mutual: the user interactions influence the recommender, and vice versa. Evaluating the causal effect of a recommender system on user behavior and satisfaction is therefore a challenging task, as it requires controlling for potential confounding factors—those that may affect both the treatment assignment (the recommendation strategy) and the outcome of interest (the user’s response to the recommendations).</p>&#13;
&#13;
<p>Causal inference provides a framework for addressing these challenges. In the context of recommender systems, causal inference can help answer questions such as these:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>How does the choice of recommendation strategy affect user engagement, such as CTRs, purchase rates, and satisfaction ratings?</p>&#13;
</li>&#13;
<li>&#13;
<p>What is the optimal recommendation strategy for a given user segment, item category, or context?</p>&#13;
</li>&#13;
<li>&#13;
<p>What are the long-term effects of a recommendation strategy on user retention, loyalty, and lifetime value?</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p class="less_space pagebreak-before">We’ll round out this chapter by introducing one aspect of causal inference important to recommender systems, based on the concept of propensity score. We’ll introduce propensity to quantify the adjusted likelihood of some items being shown to the user. We’ll then see how this interacts with the famous Simpson’s paradox.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Propensity" data-type="sect2"><div class="sect2" id="id124">&#13;
<h2>Propensity</h2>&#13;
&#13;
<p>In many data science problems, we are forced to contend with confounders and, notably, the correlation between those confounders and a target outcome. Depending on the setting, the confounder may be of a variety of forms. Interestingly, in recommendation systems, that confounder can be the system itself! Offline evaluation of recommendation systems is subject to confounders derived from the item selection behavior of users and the deployed recommendation system.</p>&#13;
&#13;
<p>If this issue seems a bit circular, it kind of is. This<a data-primary="closed-loop feedback" data-type="indexterm" id="id954"/> is sometimes called <em>closed-loop feedback</em>. One approach to mitigation is propensity weighting, which aims to address this problem by considering each feedback in the corresponding stratum based on the estimated propensities. You may recall that <em>propensity</em> refers to the likelihood of a user seeing an item; by inversely weighting by this, we can offset the selection bias. Compared to the standard offline holdout evaluation, this method attempts to represent the actual utility of the examined recommendation models.</p>&#13;
<div data-type="note" epub:type="note"><h1>Utilizing Counterfactuals</h1>&#13;
<p>One<a data-primary="counterfactual evaluation" data-type="indexterm" id="id955"/> other approach to mitigating selection bias that we won’t dive into is <em>counterfactual evaluation</em>, which estimates the actual utility of a recommendation model with propensity-weighting techniques more similar to off-policy evaluation approaches in reinforcement learning (RL). However, counterfactual evaluation often relies on accurate logging propensities in an open-loop setting where some random items are exposed to the user, which is not practical for most recommendation problems. If you have the option to include randomized recommendations to users for rating, this can help de-bias as well. One such setting where these methods may be combined is in RL-based recommenders that use explore-exploit methods like a multiarmed bandit or other structured <span class="keep-together">randomization.</span></p>&#13;
</div>&#13;
&#13;
<p><em>Inverse propensity scoring</em> (<em>IPS</em>) is<a data-primary="inverse propensity scoring (IPS)" data-type="indexterm" id="id956"/> a propensity-based evaluation method that leverages importance sampling to account for the fact that the feedback collected from the deployed recommendation system is not uniformly random. The propensity score is a balancing factor that adjusts the observed feedback distribution conditioned on the propensity score. The IPS evaluation method is theoretically unbiased if open-loop feedback can be sampled from all possible items uniformly at random. In <a data-type="xref" href="ch03.html#ch:math">Chapter 3</a>, we discussed the Matthew effect,<a data-primary="Matthew effect" data-type="indexterm" id="id957"/> or “the rich get richer” for recommendation systems; IPS is one way to combat this effect. Note the relationship here between the two ideas of the Matthew effect and Simpson’s paradox, when within different strata, selection effects create significant biasing.</p>&#13;
&#13;
<p>Propensity weighting is based on the idea that the probability of an item being exposed to a user by the deployed recommendation system (the propensity score) affects the feedback that is collected from that user. By reweighting the feedback based on the propensity scores, we can adjust for the bias introduced by the deployed system and obtain a more accurate evaluation of the new recommendation model.</p>&#13;
&#13;
<p>To apply IPS, we need to estimate the propensity scores for each item-user interaction in the collected feedback dataset. This can be done by modeling the probability that the deployed system would have exposed the item to the user at the time of the interaction. One simple approach is to use the popularity of the item as a proxy for its propensity score. However, more sophisticated methods can be used to model the propensity scores based on user and item features, as well as the context of the interaction.</p>&#13;
&#13;
<p>Once the propensity scores are estimated, we can reweight the feedback by using importance sampling. Specifically, each feedback is weighted by the inverse of its propensity score so that items that are more likely to be exposed by the deployed system are downweighted, while items that are less likely to be exposed are upweighted. This reweighting process approximates a counterfactual distribution of feedback expected from surfacing recommendations from a uniform distribution of popularity.</p>&#13;
&#13;
<p>Finally, we can use the reweighted feedback to evaluate the new recommendation model via standard metrics for evaluation, as we’ve seen in this chapter. The effectiveness of the new model is then compared to that of the deployed system by using the reweighted feedback, providing a fairer and more accurate evaluation of the new model’s performance.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Simpson’s and Mitigating Confounding" data-type="sect2"><div class="sect2" id="id125">&#13;
<h2>Simpson’s and Mitigating Confounding</h2>&#13;
&#13;
<p><em>Simpson’s paradox</em> is<a data-primary="Simpson's paradox" data-type="indexterm" id="id958"/><a data-primary="confounding variables" data-type="indexterm" id="id959"/> predicated on the idea of a confounding variable that establishes strata within which we see (potentially misleading) covariation. This paradox arises when the association between two variables is investigated but these variables are strongly influenced by a confounding variable.</p>&#13;
&#13;
<p>In the case of recommendation systems, this confounding variable is the deployed model’s characteristics and tendencies of selection. The propensity score is introduced as a measure of a system’s deviation from an unbiased open-loop exposure scenario. This score allows for the design and analysis of offline evaluation of recommendation models based on the observed closed-loop feedback, mimicking some of the particular characteristics of the open-loop scenario.</p>&#13;
&#13;
<p class="less_space pagebreak-before">Traditional descriptions of Simpson’s paradox often suggest stratification, a well-known approach to identify and estimate causal effects by first identifying the underlying strata before investigating causal effects in each stratum. This approach enables the measurement of the potential outcome irrespective of the confounding variable. For recommendation systems, this involves stratifying the observed outcome based on the possible values of the confounding variable, which is the deployed model’s characteristics.</p>&#13;
&#13;
<p>The<a data-primary="user-independent propensity score" data-type="indexterm" id="id960"/> user-independent propensity score is estimated via a two-step generative process using the prior probability that an item is recommended by the deployed model and the conditional probability that the user interacts with the item, given that it is recommended. Based on a set of mild assumptions (but too mathematically technical to cover here), the user-independent propensity score can be estimated using maximum likelihood for each dataset.</p>&#13;
&#13;
<p>We need to define the user-propensity score <math alttext="p Subscript u comma i">&#13;
  <msub><mi>p</mi> <mrow><mi>u</mi><mo>,</mo><mi>i</mi></mrow> </msub>&#13;
</math>, which indicates the tendency—or frequency—of the deployed model to expose item <math alttext="i element-of upper I">&#13;
  <mrow>&#13;
    <mi>i</mi>&#13;
    <mo>∈</mo>&#13;
    <mi>I</mi>&#13;
  </mrow>&#13;
</math> to user <math alttext="u element-of upper U">&#13;
  <mrow>&#13;
    <mi>u</mi>&#13;
    <mo>∈</mo>&#13;
    <mi>U</mi>&#13;
  </mrow>&#13;
</math>. In practice, we marginalize over users to get the user-independent propensity score <math alttext="p Subscript asterisk comma i">&#13;
  <msub><mi>p</mi> <mrow><mo>*</mo><mo>,</mo><mi>i</mi></mrow> </msub>&#13;
</math>. As described in <a href="https://oreil.ly/mpM87">“Unbiased Offline Recommender Evaluation for Missing-Not-at-Random Implicit Feedback”</a> by Longqi Yang et al., the equation is as follows:</p>&#13;
<div data-type="equation">&#13;
<math alttext="p Subscript asterisk comma i Baseline alpha left-parenthesis n Subscript i Superscript asterisk Baseline right-parenthesis Superscript StartFraction gamma plus 1 Over 2 EndFraction" display="block">&#13;
  <mrow>&#13;
    <msub><mi>p</mi> <mrow><mo>*</mo><mo>,</mo><mi>i</mi></mrow> </msub>&#13;
    <mi>α</mi>&#13;
    <msup><mfenced close=")" open="(" separators=""><msubsup><mi>n</mi> <mi>i</mi> <mo>*</mo> </msubsup></mfenced> <mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>γ</mi><mo>+</mo><mn>1</mn></mrow> <mn>2</mn></mfrac></mstyle> </msup>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Here, <math alttext="n Subscript i Superscript asterisk">&#13;
  <msubsup><mi>n</mi> <mi>i</mi> <mo>*</mo> </msubsup>&#13;
</math> is the total number of times item  <math alttext="i">&#13;
  <mi>i</mi>&#13;
</math> interacted with, and <math alttext="gamma">&#13;
  <mi>γ</mi>&#13;
</math> is a parameter that affects the propensity distributions over items with different observed popularity. The power-law parameter <math alttext="gamma">&#13;
  <mi>γ</mi>&#13;
</math> affects the propensity distributions over items and depends on the examined dataset; we estimate the <math alttext="gamma">&#13;
  <mi>γ</mi>&#13;
</math> parameter by using maximum likelihood for each dataset.</p>&#13;
&#13;
<p>With these estimates for propensity, we can then apply a simple inverse weighting <math alttext="w Subscript i Baseline equals StartFraction 1 Over p Subscript i Baseline EndFraction">&#13;
  <mrow>&#13;
    <msub><mi>w</mi> <mi>i</mi> </msub>&#13;
    <mo>=</mo>&#13;
    <mstyle displaystyle="true" scriptlevel="0">&#13;
      <mfrac><mn>1</mn> <msub><mi>p</mi> <mi>i</mi> </msub></mfrac>&#13;
    </mstyle>&#13;
  </mrow>&#13;
</math> when calculating the effect of feedback. Finally, we can combine these weightings with propensity matching, to generate counterfactual recommendations; by collecting approximately equal propensity items into strata, we can then use these strata as our confounding variable.<a data-primary="" data-startref="LRMpropen10" data-type="indexterm" id="id961"/><a data-primary="" data-startref="propweight10" data-type="indexterm" id="id962"/><a data-primary="" data-startref="RSeval10" data-type="indexterm" id="id963"/></p>&#13;
<div data-type="warning" epub:type="warning"><h1>Doubly Robust Estimation</h1>&#13;
<p>Doubly robust estimation (DRE) is<a data-primary="doubly-robust estimation (DRE)" data-type="indexterm" id="id964"/> a method that combines two models: one that models the probability of receiving the treatment (being recommended an item by the deployed model) and one that models the outcome of interest (the user’s feedback on the item). The weights used in DRE depend on the predicted probabilities from both models. This method has the advantage that it can still provide unbiased estimates even if one of the models is <span class="keep-together">misspecified.</span></p>&#13;
&#13;
<p>The structural equations for a doubly robust estimator with propensity score weighting and outcome model is as follows:</p>&#13;
<div data-type="equation">&#13;
<math alttext="normal upper Theta equals StartFraction sigma-summation w Subscript i Baseline left-parenthesis upper Y Subscript i Baseline minus f left-parenthesis upper X Subscript i Baseline right-parenthesis right-parenthesis Over sigma-summation w Subscript i Baseline left-parenthesis upper T Subscript i Baseline minus p Subscript i Baseline right-parenthesis plus sigma-summation w Subscript i Baseline left-parenthesis p Subscript i Baseline left-parenthesis 1 minus p Subscript i Baseline right-parenthesis squared left-parenthesis f left-parenthesis upper X Subscript i Baseline right-parenthesis minus f Superscript asterisk Baseline left-parenthesis upper X Subscript i Baseline right-parenthesis right-parenthesis right-parenthesis EndFraction" display="block">&#13;
  <mrow>&#13;
    <mi>Θ</mi>&#13;
    <mo>=</mo>&#13;
    <mstyle displaystyle="true" scriptlevel="0">&#13;
      <mfrac><mrow><mo>∑</mo><msub><mi>w</mi> <mi>i</mi> </msub><mfenced close=")" open="(" separators=""><msub><mi>Y</mi> <mi>i</mi> </msub><mo>-</mo><mi>f</mi><mrow><mo>(</mo><msub><mi>X</mi> <mi>i</mi> </msub><mo>)</mo></mrow></mfenced></mrow> <mrow><mo>∑</mo><msub><mi>w</mi> <mi>i</mi> </msub><mfenced close=")" open="(" separators=""><msub><mi>T</mi> <mi>i</mi> </msub><mo>-</mo><msub><mi>p</mi> <mi>i</mi> </msub></mfenced><mo>+</mo><mo>∑</mo><msub><mi>w</mi> <mi>i</mi> </msub><mfenced close=")" open="(" separators=""><msub><mi>p</mi> <mi>i</mi> </msub><msup><mrow><mo>(</mo><mn>1</mn><mo>-</mo><msub><mi>p</mi> <mi>i</mi> </msub><mo>)</mo></mrow> <mn>2</mn> </msup><mfenced close=")" open="(" separators=""><mi>f</mi><mrow><mo>(</mo><msub><mi>X</mi> <mi>i</mi> </msub><mo>)</mo></mrow><mo>-</mo><msup><mi>f</mi> <mo>*</mo> </msup><mrow><mo>(</mo><msub><mi>X</mi> <mi>i</mi> </msub><mo>)</mo></mrow></mfenced></mfenced></mrow></mfrac>&#13;
    </mstyle>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Here, <math alttext="upper Y Subscript i">&#13;
  <msub><mi>Y</mi> <mi>i</mi> </msub>&#13;
</math> is the outcome, <math alttext="upper X Subscript i">&#13;
  <msub><mi>X</mi> <mi>i</mi> </msub>&#13;
</math> are covariates, <math alttext="upper T Subscript i">&#13;
  <msub><mi>T</mi> <mi>i</mi> </msub>&#13;
</math> is the treatment, <math alttext="p Subscript i">&#13;
  <msub><mi>p</mi> <mi>i</mi> </msub>&#13;
</math> is the propensity score, <math alttext="w Subscript i">&#13;
  <msub><mi>w</mi> <mi>i</mi> </msub>&#13;
</math> is the weight, <math alttext="f left-parenthesis upper X Subscript i Baseline right-parenthesis">&#13;
  <mrow>&#13;
    <mi>f</mi>&#13;
    <mo>(</mo>&#13;
    <msub><mi>X</mi> <mi>i</mi> </msub>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math> is the outcome model, and <math alttext="f Superscript asterisk Baseline left-parenthesis upper X Subscript i Baseline right-parenthesis">&#13;
  <mrow>&#13;
    <msup><mi>f</mi> <mo>*</mo> </msup>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>X</mi> <mi>i</mi> </msub>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math> is the estimated outcome model.</p>&#13;
&#13;
<p>For a great introduction to these considerations, check out <a href="https://oreil.ly/sNhvF">“Give Me a Robust Estimator—and Make It a Double!</a>.</p>&#13;
</div>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id313">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>What a whirlwind! Latent spaces are one of the <em>most</em> important aspects of recommendation systems. They are the representations that we use to encode our users and items. Ultimately, latent spaces are about more than dimension reduction; they are about understanding a geometry in which measures of distance encode the meaning relevant to your ML task.</p>&#13;
&#13;
<p>The world of embeddings and encoders runs deep. We haven’t had time to discuss CLIP embeddings (image + text) or the Poincaré disk (naturally hierarchical distance measures). We didn’t dive deep into UMAP (a nonlinear density-aware dimension-reduction technique) or HNSW (a method for retrieval in latent spaces that respects local geometry well). Instead, we point you to the (contemporaneously published) <a href="https://oreil.ly/Rpu23">article by Vicki Boykis</a> on embeddings, the <a href="https://oreil.ly/UDZ1_">essay and guide to constructing embeddings</a> by Karel Minařík, or the <a href="https://oreil.ly/t1N48">beautiful visual guide to text embeddings</a> by Meor Amer from Cohere.</p>&#13;
&#13;
<p>We’re now equipped with representations, but next we need to optimize. We’re building <em>personalized</em> recommendation systems, so let’s define the metrics that measure our performance on our task.</p>&#13;
</div></section>&#13;
</div></section></body></html>