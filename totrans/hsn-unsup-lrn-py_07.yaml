- en: Chapter 5\. Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 3](ch03.html#Chapter_3), we introduced the most important dimensionality
    reduction algorithms in unsupervised learning and highlighted their ability to
    densely capture information. In [Chapter 4](ch04.html#Chapter_4), we used the
    dimensionality reduction algorithms to build an anomaly detection system. Specifically,
    we applied these algorithms to detect credit card fraud without using any labels.
    These algorithms learned the underlying structure in the credit card transactions.
    Then, we separated the normal transactions from the rare, potentially fraudulent
    ones based on the reconstruction error.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will build on these unsupervised learning concepts by introducing
    *clustering*, which attempts to group objects together based on similarity. Clustering
    achieves this without using any labels, comparing how similar the data for one
    observation is to data for other observations and groups.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering has many applications. For example, in credit card fraud detection,
    clustering can group fraudulent transactions together, separating them from normal
    transactions. Or, if we had only a few labels for the observations in our dataset,
    we could use clustering to group the observations first (without using labels).
    Then, we could transfer the labels of the few labeled observations to the rest
    of the observations within the same group. This is a form of *transfer learning*,
    a rapidly growing field in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: In areas such as online and retail shopping, marketing, social media, recommender
    systems for movies, music, books, dating, etc., clustering can group similar people
    together based on their behavior. Once these groups are established, business
    users will have better insight into their user base and can craft targeted business
    strategies for each of the distinct groups.
  prefs: []
  type: TYPE_NORMAL
- en: As we did with dimensionality reduction, let’s introduce the concepts first
    in this chapter, and then we will build an applied unsupervised learning solution
    in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: MNIST Digits Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To keep things simple, we will continue to work with the MNIST image dataset
    of digits that we introduced in [Chapter 3](ch03.html#Chapter_3).
  prefs: []
  type: TYPE_NORMAL
- en: Data Preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s first load the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s load the dataset and create Pandas DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Clustering Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we perform clustering, we will reduce the dimensionality of the data
    using PCA. As shown in [Chapter 3](ch03.html#Chapter_3), dimensionality reduction
    algorithms capture the salient information in the original data while reducing
    the size of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: As we move from a high number of dimensions to a lower number, the noise in
    the dataset is minimized because the dimensionality reduction algorithm (PCA,
    in this case) needs to capture the most important aspects of the original data
    and cannot devote attention to infrequently occurring elements (such as the noise
    in the dataset).
  prefs: []
  type: TYPE_NORMAL
- en: Recall that dimensionality reduction algorithms are very powerful in learning
    the underlying structure in data. In [Chapter 3](ch03.html#Chapter_3), we showed
    that it was possible to meaningfully separate the MNIST images based on the digits
    they displayed using just two dimensions after dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s apply PCA to the MNIST dataset again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Although we did not reduce the dimensionality, we will designate the number
    of principal components we will use during the clustering stage, effectively reducing
    the dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s move to clustering. The three major clustering algorithms are *k-means*,
    *hierarchical clustering*, and *DBSCAN*. We will introduce and explore each now.
  prefs: []
  type: TYPE_NORMAL
- en: k-Means
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The objective of clustering is to identify distinct groups in a dataset such
    that the observations within a group are similar to each other but different from
    observations in other groups. In *k*-means clustering, we specify the number of
    desired clusters *k*, and the algorithm will assign each observation to exactly
    one of these *k* clusters. The algorithm optimizes the groups by minimizing the
    *within-cluster variation* (also known as *inertia*) such that the sum of the
    within-cluster variations across all *k* clusters is as small as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Different runs of *k*-means will result in slightly different cluster assignments
    because *k*-means randomly assigns each observation to one of the *k* clusters
    to kick off the clustering process. *k*-means does this random initialization
    to speed up the clustering process. After this random initialization, *k*-means
    reassigns the observations to different clusters as it attempts to minimize the
    Euclidean distance between each observation and its cluster’s center point, or
    *centroid*. This random initialization is a source of randomness, resulting in
    slightly different clustering assignments, from one *k*-means run to another.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, the *k*-means algorithm does several runs and chooses the run that
    has the best separation, defined as the lowest total sum of within-cluster variations
    across all *k* clusters.
  prefs: []
  type: TYPE_NORMAL
- en: k-Means Inertia
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s introduce the algorithm. We need to set the number of clusters we would
    like (`n_clusters`), the number of initializations we would like to perform (`n_init`),
    the maximum number of iterations the algorithm will run to reassign observations
    to minimize inertia (`max_iter`), and the tolerance to declare convergence (`tol`).
  prefs: []
  type: TYPE_NORMAL
- en: We will keep the default values for number of initializations (10), maximum
    number of iterations (300), and tolerance (0.0001). Also, for now, we will use
    the first 100 principal components from PCA (`cutoff`). To test how the number
    of clusters we designate affects the inertia measure, let’s run *k*-means for
    cluster sizes 2 through 20 and record the inertia for each.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As [Figure 5-1](#k_means_inertia_for_cluster_sizes_2_through_20) shows, the
    inertia decreases as the number of clusters increases. This makes sense. The more
    clusters we have, the greater the homogeneity among observations within each cluster.
    However, fewer clusters are easier to work with than more, so finding the right
    number of clusters to generate is an important consideration when running *k*-means.
  prefs: []
  type: TYPE_NORMAL
- en: '![k-Means Inertia for Cluster Sizes 2 through 20](assets/hulp_0501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. k-means inertia for cluster sizes 2 through 20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Evaluating the Clustering Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To demonstrate how *k*-means works and how increasing the number of clusters
    results in more homogeneous clusters, let’s define a function to analyze the results
    of each experiment we do. The cluster assignments—generated by the clustering
    algorithm—will be stored in a Pandas DataFrame called `clusterDF`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s count the number of observations in each cluster and store these in a
    Pandas DataFrame called `countByCluster`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s join the `clusterDF` with the true labels array, which we will
    call `labelsDF`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s also count the number of observations for each true label in the training
    set (this won’t change but is good for us to know):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now, for each cluster, we will count the number of observations for each distinct
    label within a cluster. For example, if a given cluster has three thousand observations,
    two thousand may represent the number two, five hundred may represent the number
    one, three hundred may represent the number zero, and the remaining two hundred
    may represent the number nine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we calculate these, we will store the count for the most frequently occurring
    number for each cluster. In the example above, we would store a count of two thousand
    for this cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we will judge the success of each clustering run based on how tightly
    grouped the observations are within each cluster. For example, in the example
    above, the cluster has two thousand observations that have the same label out
    of a total of three thousand observations in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: This cluster is not great since we ideally want to group similar observations
    together in the same cluster and exclude dissimilar ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s define the overall accuracy of the clustering as the sum of the counts
    of the most frequently occuring observations across all the clusters divided by
    the total number of observations in the training set (i.e., 50,000):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also assess the accuracy by cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: For the sake of conciseness, we have all this code in a single function, available
    on [GitHub](http://bit.ly/2Gd4v7e).
  prefs: []
  type: TYPE_NORMAL
- en: k-Means Accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s now perform the experiments we did earlier, but instead of calculating
    inertia, we will calculate the overall homogeneity of the clusters based on the
    accuracy measure we’ve defined for this MNIST digits dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 5-2](#k_means_accuracy_for_cluster_sizes_2_through_20) shows the plot
    of the overall accuracy for different cluster sizes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![k-Means Accuracy for Cluster Sizes 2 through 20](assets/hulp_0502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. k-means accuracy for cluster sizes 2 through 20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As [Figure 5-2](#k_means_accuracy_for_cluster_sizes_2_through_20) shows, the
    accuracy improves as the number of clusters increases. In other words, clusters
    become more homogeneous as we increase the number of clusters because each cluster
    becomes smaller and more tightly formed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Accuracy by cluster varies quite a bit, with some clusters exhibiting a high
    degree of homogeneity and others exhibiting less. For example, in some clusters,
    over 90% of the images have the same digit; in other clusters, less than 50% of
    the images have the same digit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: k-Means and the Number of Principal Components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s perform yet another experiment—this time, let’s assess how varying the
    number of principal components we use in the clustering algorithm impacts the
    homogeneity of the clusters (defined as *accuracy*).
  prefs: []
  type: TYPE_NORMAL
- en: In the experiments earlier, we used one hundred principal components, derived
    from normal PCA. Recall that the original number of dimensions for the MNIST digits
    dataset is 784\. If PCA does a good job of capturing the underlying structure
    in the data as compactly as possible, the clustering algorithm will have an easy
    time grouping similar images together, regardless of whether the clustering happens
    on just a fraction of the principal components or many more. In other words, clustering
    should perform just as well using 10 or 50 principal components as it does using
    one hundred or several hundred principal components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s test this hypothesis. We will pass along 10, 50, 100, 200, 300, 400,
    500, 600, 700, and 784 principal components and gauge the accuracy of each clustering
    experiment. We will then plot these results to see how varying the number of principal
    components affects the clustering accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 5-3](#k_means_clustering_accuracy_as_number_of_principal_components_varies)
    shows the plot of the clustering accuracy for the different number of principal
    components.'
  prefs: []
  type: TYPE_NORMAL
- en: '![k-means Clustering Accuracy As Number of Principal Components Varies](assets/hulp_0503.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-3\. k-means clustering accuracy with varying number of principal components
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This plot supports our hypothesis. As the number of principal components varies
    from 10 to 784, the clustering accuracy remains stable and consistent around 70%.
    This is one reason why clustering should be performed on dimensionality-reduced
    datasets—the clustering algorithms generally perform better, both in terms of
    time and clustering accuracy, on dimensionality-reduced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, for the MNIST dataset, the original 784 dimensions are manageable
    for a clustering algorithm, but imagine if the original dataset were thousands
    or millions of dimensions large. The case for reducing the dimensionality before
    performing clustering is even stronger in such a scenario.
  prefs: []
  type: TYPE_NORMAL
- en: k-Means on the Original Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To make this point clearer, let’s perform clustering on the original dataset
    and measure how varying the number of dimensions we pass into the clustering algorithm
    affects clustering accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: For the PCA-reduced dataset in the previous section, varying the number of principal
    components that we passed into the clustering algorithm did not affect the clustering
    accuracy, which remained stable and consistent at approximately 70%. Is this true
    for the original dataset, too?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 5-4](#k_means_clustering_accuracy_as_number_of_original_dimension_varies)
    plots the clustering accuracy at the different original dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![k-means Clustering Accuracy As Number of Original Dimensions Varies](assets/hulp_0504.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-4\. k-means clustering accuracy with varying number of original dimensions
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As the plot shows, clustering accuracy is very poor at lower dimensions but
    improves to nearly 70% only as the number of dimensions climbs to six hundred
    dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: In the PCA case, clustering accuracy was approximately 70% even at 10 dimensions,
    demonstrating the power of dimensionality reduction to densely capture salient
    information in the original dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s move to a second clustering approach called *hierarchical clustering*.
    This approach does not require us to precommit to a particular number of clusters.
    Instead, we can choose how many clusters we would like after hierarchical clustering
    has finished running.
  prefs: []
  type: TYPE_NORMAL
- en: Using the observations in our dataset, the hierarchical clustering algorithm
    will build a *dendrogram*, which can be depicted as an upside-down tree where
    the leaves are at the bottom and the tree trunk is at the top.
  prefs: []
  type: TYPE_NORMAL
- en: The leaves at the very bottom are individual instances in the dataset. Hierarchical
    clustering then joins the leaves together—as we move vertically up the upside-down
    tree—based on how similar they are to each other. The instances (or groups of
    instances) that are most similar to each other are joined sooner, while the instances
    that are not as similar are joined later.
  prefs: []
  type: TYPE_NORMAL
- en: With this iterative process, all the instances are eventually linked together
    forming the single trunk of the tree.
  prefs: []
  type: TYPE_NORMAL
- en: This vertical depiction is very helpful. Once the hierarchical clustering algorithm
    has finished running, we can view the dendrogram and determine where we want to
    cut the tree—the lower we cut, the more individual branches we are left with (i.e.,
    more clusters). If we want fewer clusters, we can cut higher on the dendrogram,
    closer to the single trunk at the very top of this upside-down tree.
  prefs: []
  type: TYPE_NORMAL
- en: The placement of this vertical cut is similar to choosing the number of *k*
    clusters in the *k*-means clustering algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Agglomerative Hierarchical Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The version of hierarchical clustering we will explore is called *agglomerative
    clustering*. Although Scikit-Learn has a library for this, it performs very slowly.
    Instead, we will choose to use another version of hierarchical clustering called
    *fastcluster*. This package is a C++ library with an interface in Python/SciPy.^([1](ch05.html#idm140637545889136))
  prefs: []
  type: TYPE_NORMAL
- en: The main function that we will use in this package is `fastcluster.linkage_vector`.
    This requires several arguments, including the training matrix *X*, the *method*,
    and the *metric*. The method—which can be set to `single`, `centroid`, `median`,
    or `ward`—specifies which clustering scheme to use to determine the distance from
    a new node in the dendrogram to the other nodes. The metric should be set to `euclidean`
    in most cases, and it is required to be `euclidean` if the method is `centroid`,
    `median`, or `ward`. For more on these arguments, refer to the fastcluster documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s set up the hierarchical clustering algorithm for our data. As before,
    we will train the algorithm on the first one hundred principal components from
    the PCA-reduced MNIST image dataset. We will set the method to `ward` (which performed
    the best, by far, in the experimentation), and the metric to `euclidean`.
  prefs: []
  type: TYPE_NORMAL
- en: Ward stands for *Ward’s minimum variance method*. You can learn more about this
    method [online](http://bit.ly/2WwOJK5). Ward is a good default choice to use in
    hierarchical clustering, but, as always, it is best to experiment on your specific
    datasets in practice.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The hierarchical clustering algorithm will return a matrix *Z*. The algorithm
    treats each observation in our 50,000 MNIST digits dataset as a single-point cluster,
    and, in each iteration of training, the algorithm will merge the two clusters
    that have the smallest distance between them.
  prefs: []
  type: TYPE_NORMAL
- en: Initially, the algorithm is just merging single-point clusters together, but
    as it proceeds, it will merge multipoint clusters with either single-point or
    multipoint clusters. Eventually, through this iterative process, all the clusters
    are merged together, forming the trunk in the upside-down tree (dendrogram).
  prefs: []
  type: TYPE_NORMAL
- en: The Dendrogram
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Table 5-1](#first_few_rows_of_z_matrix_of_hierarchical_clustering) shows the
    Z matrix that was generated by the clustering algorithm, showing what the algorithm
    can accomplish.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 5-1\. First few rows of Z matrix of hierarchical clustering
  prefs: []
  type: TYPE_NORMAL
- en: '|  | clusterOne | clusterTwo | distance | newClusterSize |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 42194.0 | 43025.0 | 0.562682 | 2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 28350.0 | 37674.0 | 0.590866 | 2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 26696.0 | 44705.0 | 0.621506 | 2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 12634.0 | 32823.0 | 0.627762 | 2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 24707.0 | 43151.0 | 0.637668 | 2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 20465.0 | 24483.0 | 0.662557 | 2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 466.0 | 42098.0 | 0.664189 | 2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 46542.0 | 49961.0 | 0.665520 | 2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 2301.0 | 5732.0 | 0.671215 | 2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 37564.0 | 47668.0 | 0.675121 | 2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 3375.0 | 26243.0 | 0.685797 | 2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 15722.0 | 30368.0 | 0.686356 | 2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 21247.0 | 21575.0 | 0.694412 | 2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | 14900.0 | 42486.0 | 0.696769 | 2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | 30100.0 | 41908.0 | 0.699261 | 2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 12040.0 | 13254.0 | 0.701134 | 2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | 10508.0 | 25434.0 | 0.708872 | 2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 17 | 30695.0 | 30757.0 | 0.710023 | 2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 18 | 31019.0 | 31033.0 | 0.712052 | 2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 19 | 36264.0 | 37285.0 | 0.713130 | 2.0 |'
  prefs: []
  type: TYPE_TB
- en: The first two columns in this table, `clusterOne` and `clusterTwo`, list which
    two clusters—could be single-point clusters (i.e., the original observations)
    or multipoint clusters—are being merged given their distance relative to each
    other. The third column, `distance`, displays this distance, which was determined
    by the Ward method and `euclidean` metric that we passed into the clustering algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the distance is monotonically increasing. In other words, the
    shortest-distance clusters are merged first, and the algorithm iteratively merges
    the next shortest-distance clusters until all the points have been joined into
    a single cluster at the top of the dendrogram.
  prefs: []
  type: TYPE_NORMAL
- en: Initially, the algorithm merges single-point clusters together, forming new
    clusters with a size of two, as shown in the fourth column, `newClusterSize`.
    However, as we get much further along, the algorithm joins large multipoint clusters
    with other large multipoint clusters, as shown in [Table 5-2](#last_few_rows_of_z_matrix_of_hierarchical_clustering).
    At the very last iteration (49,998), two large clusters are joined together, forming
    a single cluster—the top tree trunk—with all 50,000 original observations.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5-2\. Last few rows of Z matrix of hierarchical clustering
  prefs: []
  type: TYPE_NORMAL
- en: '|  | clusterOne | clusterTwo | distance | newClusterSize |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 49980 | 99965.0 | 99972.0 | 161.106998 | 5197.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 49981 | 99932.0 | 99980.0 | 172.070003 | 6505.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 49982 | 99945.0 | 99960.0 | 182.840860 | 3245.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 49983 | 99964.0 | 99976.0 | 184.475761 | 3683.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 49984 | 99974.0 | 99979.0 | 185.027847 | 7744.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 49985 | 99940.0 | 99975.0 | 185.345207 | 5596.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 49986 | 99957.0 | 99967.0 | 211.854714 | 5957.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 49987 | 99938.0 | 99983.0 | 215.494857 | 4846.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 49988 | 99978.0 | 99984.0 | 216.760365 | 11072.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 49989 | 99970.0 | 99973.0 | 217.355871 | 4899.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 49990 | 99969.0 | 99986.0 | 225.468298 | 8270.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 49991 | 99981.0 | 99982.0 | 238.845135 | 9750.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 49992 | 99968.0 | 99977.0 | 266.146782 | 5567.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 49993 | 99985.0 | 99989.0 | 270.929453 | 10495.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 49994 | 99990.0 | 99991.0 | 346.840948 | 18020.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 49995 | 99988.0 | 99993.0 | 394.365194 | 21567.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 49996 | 99987.0 | 99995.0 | 425.142387 | 26413.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 49997 | 99992.0 | 99994.0 | 440.148301 | 23587.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 49998 | 99996.0 | 99997.0 | 494.383855 | 50000.0 |'
  prefs: []
  type: TYPE_TB
- en: You may be a bit confused by the `clusterOne` and `clusterTwo` entries in this
    table. For example, in the last row—49,998—cluster 99,996 is joined with cluster
    99,997\. But as you know, there are only 50,000 observations in the MNIST digits
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '`clusterOne` and `clusterTwo` refer to the original observations for numbers
    0 through 49,999\. For numbers above 49,999, the cluster numbers refer to previously
    clustered points. For example, 50,000 refers to the newly formed cluster in row
    0, 50,001 refers to the newly formed cluster in row 1, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: In row 49,998, `clusterOne`, 99,996 refers to the cluster formed in row 49,996,
    and `clusterTwo`, 99,997, refers to the cluster formed in row 49,997\. You can
    continue to work your way through this table using this formula to see how the
    clusters are being joined.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the Clustering Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have the dendrogram in place, let’s determine where to cut off the
    dendrogram to make the number of clusters we desire. To more easily compare hierarchical
    clustering results with those of *k*-means, let’s cut the dendrogram to have exactly
    20 clusters. We will then use the clustering accuracy metric—defined in the `k-means
    section`—to judge how homogenous the hierarchical clustering clusters are.
  prefs: []
  type: TYPE_NORMAL
- en: To create the clusters we desire from the dendrogram, let’s pull in the *fcluster*
    library from SciPy. We need to specify the *distance threshold* of the dendrogram
    to determine how many distinct clusters we are left with. The larger the distance
    threshold, the fewer clusters we will have. Data points within the distance threshold
    we set will belong to the same cluster. A large distance threshold is akin to
    cutting the upside-down tree at a very high vertical point. Since more and more
    of the points are grouped together the higher up the tree we go, the fewer clusters
    we will have.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get exactly 20 clusters, we need to experiment with the distance threshold,
    as done here. The *fcluster* library will take our dendrogram and cut it with
    the distance threshold we specify. Each observation in the 50,000 observations
    MNIST digits dataset will get a cluster label, and we will store these in a Pandas
    DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s verify that there are exactly 20 distinct clusters, given our choice
    of distance threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, this confirms the 20 clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s evaluate the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We find that the overall accuracy is approximately 77%, even better than the
    approximately 70% accuracy from *k*-means:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Let’s also assess the accuracy by cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown here, the accuracy varies quite a bit. For some clusters, the accuracy
    is remarkably high, nearly 100%. For some, the accuracy is shy of 50%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Overall, hierarchical clustering performs well on the MNIST digits dataset.
    Remember that we accomplished this without using any labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is how it would work on real-world examples: we would apply dimensionality
    reduction first (such as PCA), then we would perform clustering (such as hierarchical
    clustering), and finally we would hand-label a few points per cluster. For example,
    for this MNIST digits dataset, if we did not have any labels, we would look at
    a few images per cluster and label those images based on the digits they displayed.
    So long as the clusters were homogeneous enough, the few hand labels we generated
    could be applied automatically to all the other images in the cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: All of a sudden, without much effort, we could have labeled all the images in
    our 50,000 dataset with a near 77% accuracy. This is impressive and highlights
    the power of unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let’s turn to the third and final major clustering algorithm, *DBSCAN*,
    which stands for *density-based spatial clustering of applications with noise*.
    As the name implies, this clustering algorithm groups based on the density of
    points.
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN will group together closely packed points, where close together is defined
    as a minimum number of points that must exist within a certian distance. If the
    point is within a certain distance of multiple clusters, it will be grouped with
    the cluster to which it is most densely located. Any instance that is not within
    this certain distance of another cluster is labeled an outlier.
  prefs: []
  type: TYPE_NORMAL
- en: In *k*-means and hierarchical clustering, all points had to be clustered, and
    outliers were poorly dealt with. In DBSCAN, we can explicitly label points as
    outliers and avoid having to cluster them. This is powerful. Compared to the other
    clustering algorithms, DBSCAN is much less prone to the distortion typically caused
    by outliers in the data. Also, like hierarchical clustering—and unlike *k*-means—we
    do not need to prespecify the number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN Algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s first use the DBSCAN library from Scikit-Learn. We need to specify the
    *maximum distance* (called `eps`) between two points for them to be considered
    in the same neighborhood and the *minimum samples* (called `min_samples`) for
    a group to be called a cluster. The default value for `eps` is 0.5, and the default
    value for `min_samples` is 5\. If `eps` is set too low, no points may be close
    enough to other points for them to be considered in the same neighborhood. Hence,
    all the points would remain unclustered. If `eps` is set too high, many points
    may be clustered and only a handful of points would remain unclustered, effectively
    being labeled as outliers in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We need to search for the optimal `eps` for our MNIST digits dataset. `min_samples`
    designates how many points need to be within the `eps` distance in order for the
    points to be called a cluster. Once there are `min_samples` number of closely
    located points, any other point that is within the `eps` distance of any of these
    so-called *core points* is part of that cluster, even if those other points do
    not have the `min_samples` number of points within `eps` distance around them.
    These other points—if they do not have the *min_samples* number of points within
    `eps` distance around them—are called the *border points* of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, as the `min_samples` increases, the number of clusters decreases.
    As with `eps`, we need to search for the optimal `min_samples` for our MNIST digits
    dataset. As you can see, the clusters have core points and border points, but
    for all intents and purposes, they belong to the same group. All points that do
    not get grouped—either as the core or border points of a cluster—are labeled as
    outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Applying DBSCAN to Our Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s now move to our specific problem. As before, we will apply DBSCAN to
    the first one hundred principal components of the PCA-reduced MNIST digits dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We will keep the `min_samples` at the default value of five, but we will adjust
    the `eps` to three to avoid having too few points clustered.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the overall accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the accuracy is very poor compared to *k*-means and hierarchical
    clustering. We can fidget with the parameters `eps` and `min_samples` to improve
    the results, but it appears that DBSCAN is poorly suited to cluster the observations
    for this particular dataset.
  prefs: []
  type: TYPE_NORMAL
- en: To explore why, let’s look at the clusters ([Table 5-3](#cluster_results_for_dbscan)).
  prefs: []
  type: TYPE_NORMAL
- en: Table 5-3\. Cluster results for DBSCAN
  prefs: []
  type: TYPE_NORMAL
- en: '|  | cluster | clusterCount |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | –1 | 39575 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 8885 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 8 | 720 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 5 | 92 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 18 | 51 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 38 | 38 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 41 | 22 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 39 | 22 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 4 | 16 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 20 | 16 |'
  prefs: []
  type: TYPE_TB
- en: Most of the points are unclustered. You can see this in the plot. 39,651 points—out
    of the 50,000 observations in the training set—are in cluster -1, which means
    that they do not belong to any cluster. They are labeled as outliers—noise, in
    other words.
  prefs: []
  type: TYPE_NORMAL
- en: 8,885 points belong in cluster 0\. Then, there is a long tail of smaller-sized
    clusters. It appears that DBSCAN has a hard time finding distinct dense groups
    of points, and, therefore, does a poor job of clustering the MNIST images based
    on the digits they display.
  prefs: []
  type: TYPE_NORMAL
- en: HDBSCAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s try another version of DBSCAN and see if the results improve. This one
    is known as *HDBSCAN*, or *hierarchical DBSCAN*. The takes the DBSCAN algorithm
    we introduced and converts it into a hierarchical clustering algorithm. In other
    words, it groups based on density and then links the density-based clusters based
    on distance iteratively, like in the hierarchical clustering algorithm we introduced
    in an earlier section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two main parameters for this algorithm are `min_cluster_size` and `min_samples`,
    which defaults to `min_cluster_size` when set to `None`. Let’s use the out-of-the-box
    parameter selections and gauge if HDBSCAN performs better than DBSCAN did for
    our MNIST digits dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the overall accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: At 25%, this is only marginally better than that of DBSCAN and well short of
    the 70%-plus achieved by *k*-means and hierarchical clustering. [Table 5-4](#cluster_results_for_hdbscan)
    displays the accuracy of the various clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5-4\. Cluster results for HDBSCAN
  prefs: []
  type: TYPE_NORMAL
- en: '|  | cluster | clusterCount |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | –1 | 42570 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 4 | 5140 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 7 | 942 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0 | 605 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 6 | 295 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 3 | 252 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 1 | 119 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 5 | 45 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 2 | 32 |'
  prefs: []
  type: TYPE_TB
- en: We see a similar phenomenon as we did for DBSCAN. Most points are unclustered,
    and then there is a long tail of small-sized clusters. The results do not improve
    much.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced three major types of clustering algorithms—*k*-means,
    hierarchical clustering, and DBSCAN—and applied them to a dimensionality-reduced
    version of the MNIST digits dataset. The first two clustering algorithms performed
    very well on the dataset, grouping the images well enough to have a 70%-plus consistency
    in labels across the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN did not perform quite so well for this dataset but remains a viable clustering
    algorithm. Now that we’ve introduced the clustering algorithms, let’s build an
    applied unsupervised learning solution using these algorithms in [Chapter 6](ch06.html#Chapter_6).
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch05.html#idm140637545889136-marker)) For more on [fastcluster](https://pypi.org/project/fastcluster/),
    check out the project’s web page.
  prefs: []
  type: TYPE_NORMAL
