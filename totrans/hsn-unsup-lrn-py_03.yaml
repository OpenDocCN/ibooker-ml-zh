- en: Chapter 2\. End-to-End Machine Learning Project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we begin exploring unsupervised learning algorithms in detail, we will
    review how to set up and manage machine learning projects, covering everything
    from acquiring data to building and evaluating a model and implementing a solution.
    We will work with supervised learning models in this chapter—an area most readers
    should have some experience in—before jumping into unsupervised learning models
    in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Environment Setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s set up the data science environment before going further. This environment
    is the same for both supervised and unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: These instructions are optimized for the Windows operating system but installation
    packages are available for Mac and Linux, too.
  prefs: []
  type: TYPE_NORMAL
- en: 'Version Control: Git'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have not already, you will need to install [Git](https://git-scm.com/).
    Git is a version control system for code, and all the coding examples in this
    book are available as Jupyter notebooks from [the GitHub repository](http://bit.ly/2Gd4v7e).
    Review Roger Dudler’s [Git guide](http://rogerdudler.github.io/git-guide/) to
    learn how to clone repositories; add, commit, and push changes; and maintain version
    control with branches.
  prefs: []
  type: TYPE_NORMAL
- en: Clone the Hands-On Unsupervised Learning Git Repository
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Open the command-line interface (i.e., command prompt on Windows, terminal
    on Mac, etc.). Navigate to the directory where you will store your unsupervised
    learning projects. Use the following prompt to clone the repository associated
    with this book from GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, you can visit [the repository](http://bit.ly/2Gd4v7e) on the
    GitHub website and manually download the repository for your use. You can *watch*
    or *star* the repository to stay updated on changes.
  prefs: []
  type: TYPE_NORMAL
- en: Once the repository has been pulled or manually downloaded, use the command-line
    interface to navigate into the *handson-unsupervised-learning* repository.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: For the rest of the installations, we will continue to use the command-line
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scientific Libraries: Anaconda Distribution of Python'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To install Python and the scientific libraries necessary for machine learning,
    download the [Anaconda distribution](https://www.anaconda.com/download/) of Python
    (version 3.6 is recommended because version 3.7 is relatively new as of the writing
    of this book and not supported by all the machine libraries we will use).
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an isolated Python environment so that you can import different libraries
    for each project separately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This creates an isolated Python 3.6 environment—with all of the scientific libraries
    that come with the Anaconda distribution—called `unsupervisedLearning`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, activate this for use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Neural Networks: TensorFlow and Keras'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once unsupervisedLearning is activated, you will need to install TensorFlow
    and Keras to build neutral networks. TensorFlow is an open source project by Google
    and is not part of the Anaconda distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Keras is an open source netural network library that offers a higher-level
    API for us to use the lower-level functions in TensorFlow. In other words, we
    will use Keras on top of TensorFlow (the backend) to have a more intuitive set
    of API calls to develop our deep learning models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Gradient Boosting, Version One: XGBoost'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, install one version of gradient boosting known as XGBoost. To make this
    simple (for Windows users, at least), you can navigate into the *xgboost* folder
    in the *handson-unsupervised-learning* repository and find the package there.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the package, use `pip install`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, download the correct version of [XGBoost](http://bit.ly/2G1jBxs)
    based on your system—either the 32-bit or the 64-bit version.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the command-line interface, navigate to the folder with this newly downloaded
    file. Use `pip install`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Your XGBoost WHL filename may be slightly different as newer versions of the
    software are released publicly.
  prefs: []
  type: TYPE_NORMAL
- en: Once XGBoost has been successfully installed, navigate back to the *handson-unsupervised-learning*
    folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Gradient Boosting, Version Two: LightGBM'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Install another version of gradient boosting, Microsoft’s LightGBM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Clustering Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s install a few clustering algorithms we will use later in the book. One
    clustering package, *fastcluster*, is a C++ library with an interface in Python/SciPy.^([1](ch02.html#idm140637564820880))
  prefs: []
  type: TYPE_NORMAL
- en: 'This fastcluster package can be installed with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Another clustering algorithm is *hdbscan*, which can also be installed via
    pip:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'And, for time series clustering, let’s install *tslearn*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Interactive Computing Environment: Jupyter Notebook'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Jupyter notebook is part of the Anaconda distribution, so we will now activate
    it to launch the environment we just set up. Make sure you are in the *handson-unsupervised-learning*
    repository before you enter the following command (for ease of use):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: You should see your browser open up and launch the *[*http://localhost:8888/*](http://localhost:8888/)*
    page. Cookies must be enabled for proper access.
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to build our first machine learning project.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will use a real dataset of anonymized credit card transactions
    made by European cardholders from September 2013.^([2](ch02.html#idm140637564803888))
    These transactions are labeled as fraudulent or genuine, and we will build a fraud
    detection solution using machine learning to predict the correct labels for never-before-seen
    instances.
  prefs: []
  type: TYPE_NORMAL
- en: This dataset is highly imbalanced. Of the 284,807 transactions, only 492 are
    fraudulent (0.172%). This low percentage of fraud is pretty typical for credit
    card transactions.
  prefs: []
  type: TYPE_NORMAL
- en: There are 28 features, all of which are numerical, and there are no categorical
    variables.^([3](ch02.html#idm140637564800304)) These features are not the original
    features but rather the output of principal component analysis, which we will
    explore in [Chapter 3](ch03.html#Chapter_3). The original features were distilled
    to 28 principal components using this form of dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the 28 principal components, we have three other variables—the
    time of the transaction, the amount of the transaction, and the true class of
    the transaction (one if fraud, zero if genuine).
  prefs: []
  type: TYPE_NORMAL
- en: Data Preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we can use machine learning to train on the data and develop a fraud
    detection solution, we need to prepare the data for the algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Data Acquisition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step in any machine learning project is data acquisition.
  prefs: []
  type: TYPE_NORMAL
- en: Download the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Download the dataset and, within the *handson-unsupervised-learning* directory,
    place the CSV file in a folder called */datasets/credit_card_data/*. If you downloaded
    the GitHub repository earlier, you already have this file in this folder in the
    repository.
  prefs: []
  type: TYPE_NORMAL
- en: Import the necessary libraries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Import the Python libraries that we will need to build our fraud detection
    solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Read the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Preview the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Table 2-1](#preview_of_the_data) shows the first five rows of the dataset.
    As you can see, the data has been properly loaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Table 2-1\. Preview of the data
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Time | V1 | V2 | V3 | V4 | V5 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0.0 | –1.359807 | –0.072781 | 2.536347 | 1.378155 | –0.338321 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.0 | 1.191857 | 0.266151 | 0.166480 | 0.448154 | 0.060018 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1.0 | –1.358354 | –1.340163 | 1.773209 | 0.379780 | –0.503198 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1.0 | –0.966272 | –0.185226 | 1.792993 | –0.863291 | –0.010309 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 2.0 | –1.158233 | 0.877737 | 1.548718 | 0.403034 | –0.407193 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 rows x 31 columns |'
  prefs: []
  type: TYPE_TB
- en: Data Exploration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, let’s get a deeper understanding of the data. We will generate summary
    statistics for the data, identify any missing values or categorical features,
    and count the number of distinct values by feature.
  prefs: []
  type: TYPE_NORMAL
- en: Generate summary statistics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Table 2-2](#simple_summary_statistics) describes the data, column by column.
    The block of code that follows lists all the column names for easy reference.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Table 2-2\. Simple summary statistics
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Time | V1 | V2 | V3 | V4 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| count | 284807.000000 | 2.848070e+05 | 2.848070e+05 | 2.848070e+05 | 2.848070e+05
    |'
  prefs: []
  type: TYPE_TB
- en: '| mean | 94813.859575 | 3.919560e–15 | 5.688174e–16 | –8.769071e–15 | 2.782312e–15
    |'
  prefs: []
  type: TYPE_TB
- en: '| std | 47488.145955 | 1.958696e+00 | 1.651309e+00 | 1.516255e+00 | 1.415869e+00
    |'
  prefs: []
  type: TYPE_TB
- en: '| min | 0.000000 | –5.640751e+01 | –7.271573e+01 | –4.832559e+01 | –5.683171e+00
    |'
  prefs: []
  type: TYPE_TB
- en: '| 25% | 54201.500000 | –9.203734e–01 | –5.985499e–01 | –8.903648e–01 | –8.486401e–01
    |'
  prefs: []
  type: TYPE_TB
- en: '| 50% | 84692.000000 | 1.810880e–02 | 6.548556e–02 | 1.798463e–01 | –1.984653e–02
    |'
  prefs: []
  type: TYPE_TB
- en: '| 75% | 139320.500000 | 1.315642e+00 | 8.037239e–01 | 1.027196e+00 | 7.433413e–01
    |'
  prefs: []
  type: TYPE_TB
- en: '| max | 172792.000000 | 2.454930e+00 | 2.205773e+01 | 9.382558e+00 | 1.687534e+01
    |'
  prefs: []
  type: TYPE_TB
- en: '| 8 rows x 31 columns |'
  prefs: []
  type: TYPE_TB
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The total number of positive labels, or fraudulent transactions, is 492\. There
    are 284,807 instances and 31 columns as expected—28 numerical features (V1 through
    V28), Time, Amount, and Class.
  prefs: []
  type: TYPE_NORMAL
- en: The timestamps range from 0 to 172,792, the amounts range from 0 to 25,691.16,
    and there are 492 fraudulent transactions. These fraudulent transactions are also
    referred to as positive cases or positive labels (labeled as one); the normal
    transactions are negative cases or negative labels (labeled as zero).
  prefs: []
  type: TYPE_NORMAL
- en: The 28 numerical features are not standardized yet, but we will standardize
    the data soon. *Standardization* rescales the data to have a mean of zero and
    standard deviation of one.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Some machine learning solutions are very sensitive to the scale of the data,
    so having all the data on the same relative scale—via standardization—is a good
    machine learning practice.
  prefs: []
  type: TYPE_NORMAL
- en: Another common method to scale data is *normalization*, which rescales the data
    to a zero to one range. Unlike the standardized data, all the normalized data
    is on a positive scale.
  prefs: []
  type: TYPE_NORMAL
- en: Identify nonnumerical values by feature
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some machine learning algorithms cannot handle nonnumerical values or missing
    values. Therefore, it is best practice to identify nonnumerical values (also known
    as *not a number*, or *NaNs*).
  prefs: []
  type: TYPE_NORMAL
- en: In the case of missing values, we can impute the value—for example, by replacing
    the missing points with the mean, median, or mode of the feature—or substitute
    with some user-defined value. In the case of categorical values, we can encode
    the data such that all the categorical values are represented with a sparse matrix.
    This sparse matrix is then combined with the numerical features. The machine learning
    algorithm trains on this combined feature set.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows that none of the observations have NaNs, so we will
    not need to impute or encode any of the values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Identify distinct values by feature
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To develop a better understanding of the credit card transactions dataset, let’s
    count the number of distinct values by feature.
  prefs: []
  type: TYPE_NORMAL
- en: The following code shows that we have 124,592 distinct timestamps. But we know
    from earlier that we have 284,807 observations in total. That means that there
    are multiple transactions at some timestamps.
  prefs: []
  type: TYPE_NORMAL
- en: 'And, as expected, there are just two classes—one for fraud, zero for not fraud:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Generate Feature Matrix and Labels Array
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s create and standardize the feature matrix X and isolate the labels array
    y (one for fraud, zero for not fraud). Later on we will feed these into the machine
    learning algorithms during training.
  prefs: []
  type: TYPE_NORMAL
- en: Create the feature matrix X and the labels array Y
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Standardize the feature matrix X
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s rescale the feature matrix so that each feature, except for time, has
    a mean of zero and standard deviation of one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As shown in [Table 2-3](#summary_of_scaled_features), the standardized features
    now have a mean of zero and a standard deviation of one.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-3\. Summary of scaled features
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Time | V1 | V2 | V3 | V4 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| count | 284807.000000 | 2.848070e+05 | 2.848070e+05 | 2.848070e+05 | 2.848070e+05
    |'
  prefs: []
  type: TYPE_TB
- en: '| mean | 94813.859575 | –8.157366e–16 | 3.154853e–17 | –4.409878e–15 | –6.734811e–16
    |'
  prefs: []
  type: TYPE_TB
- en: '| std | 47488.145955 | 1.000002e+00 | 1.000002e+00 | 1.000002e+00 | 1.000002e+00
    |'
  prefs: []
  type: TYPE_TB
- en: '| min | 0.000000 | –2.879855e+01 | –4.403529e+01 | –3.187173e+01 | –4.013919e+00
    |'
  prefs: []
  type: TYPE_TB
- en: '| 25% | 54201.500000 | –4.698918e–01 | –3.624707e–01 | –5.872142e–01 | –5.993788e–01
    |'
  prefs: []
  type: TYPE_TB
- en: '| 50% | 84692.000000 | 9.245351e–03 | 3.965683e–02 | 1.186124e–02 | –1.401724e–01
    |'
  prefs: []
  type: TYPE_TB
- en: '| 75% | 139320.500000 | 6.716939e–01 | 4.867202e–01 | 6.774569e–01 | 5.250082e–01
    |'
  prefs: []
  type: TYPE_TB
- en: '| max | 172792.000000 | 1.253351e+00 | 1.335775e+01 | 6.187993e+00 | 1.191874e+01
    |'
  prefs: []
  type: TYPE_TB
- en: '| 8 rows x 30 columns |'
  prefs: []
  type: TYPE_TB
- en: Feature Engineering and Feature Selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In most machine learning projects, we should consider *feature engineering*
    and *feature selection* as part of the solution. Feature engineering involves
    creating new features—for example, calculating ratios or counts or sums from the
    original features—to help the machine learning algorithm extract a stronger signal
    from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection involves selecting a subset of the features for training,
    effectively removing some of the less relevant features from consideration. This
    may help prevent the machine learning algorithm from overfitting to the noise
    in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: For this credit card fraud dataset, we do not have the original features. We
    have only the principal components, which were derived from PCA, a form of dimensionality
    reduction that we will explore in [Chapter 3](ch03.html#Chapter_3). Since we do
    not know what any of the features represent, we cannot perform any intelligent
    feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection is not necessary either since the number of observations (284,807)
    vastly outnumbers the number of features (30), which dramatically reduces the
    chances of overfitting. And, as [Figure 2-1](#correlation_matrix) shows, the features
    are only slightly correlated to each other. In other words, we do not have redundant
    features. If we did, we could remove or reduce the redundancy via dimensionality
    reduction. Of course, this is not a surprise. PCA was already performed on this
    credit card dataset, removing the redundancy for us.
  prefs: []
  type: TYPE_NORMAL
- en: Check correlation of features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![Correlation matrix](assets/hulp_0201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-1\. Correlation matrix
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Data Visualization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a final step, let’s visualize the data to appreciate just how imbalanced
    the dataset is ([Figure 2-2](#frequency_percentage_of_labels)). Since there are
    so few cases of fraud to learn from, this is a difficult problem to solve; fortunately,
    we have labels for the entire dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![Frequency percentage of labels](assets/hulp_0202.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-2\. Frequency percentage of labels
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Model Preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that the data is ready, let’s prepare for the model. We need to split the
    data into a training and a test set, select a cost function, and prepare for *k*-fold
    cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: Split into Training and Test Sets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you may recall from [Chapter 1](ch01.html#Chapter_1), machine learning algorithms
    learn from data (i.e., train on the data) to have good performance (i.e., accurately
    predict) on never-before-seen cases. The performance on these never-before-seen
    cases is known as the generalization error—this is the most important metric in
    determining the goodness of a machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: We need to set up our machine learning project so that we have a training set
    from which the machine learning algorithm learns. We also need a test set (the
    never-before-seen cases) the machine learning algorithm can make predictions on.
    The performance on this test set will be the ultimate gauge of success.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go ahead and split our credit card transactions dataset into a training
    set and a test set.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We now have a training set with 190,280 instances (67% of the original dataset)
    and a test set with 93,987 instances (the remaining 33%). To preserve the percentage
    of fraud (~0.17%) for both the training and the test set, we have set the stratify
    parameter. We also fixed the random state to 2018 to make it easier to reproduce
    results.^([4](ch02.html#idm140637556238384))
  prefs: []
  type: TYPE_NORMAL
- en: We will use the test set for a final evaluation of our generalization error
    (also known as out-of-sample error).
  prefs: []
  type: TYPE_NORMAL
- en: Select Cost Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we train on the training set, we need a cost function (also referred
    to as the error rate or value function) to pass into the machine learning algorithm.
    The machine learning algorithm will try to minimize this cost function by learning
    from the training examples.
  prefs: []
  type: TYPE_NORMAL
- en: Since this is a supervised classification problem—with two classes—let’s use
    *binary classification log loss* (as shown in [Equation 2-1](#log_loss_function)),
    which will calculate the cross-entropy between the true labels and the model-based
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Equation 2-1\. Log loss function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math class="center" display="block"><mi>log loss</mi><mo>=</mo> <mrow><mo>–</mo>
    <mfrac><mn>1</mn><mi>N</mi></mfrac></mrow> <munderover><mo mathsize="200%">Σ</mo>
    <mrow><mi>i</mi><mo>=</mo><mi>1</mi></mrow> <mi>N</mi></munderover> <munderover><mo
    mathsize="200%">Σ</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>M</mi></munderover>
    <msub><mi>y</mi> <mrow><mi>i</mi><mtext>,</mtext><mi>j</mi></mrow></msub> <mi>log</mi>
    <mo stretchy="false">(</mo> <msub><mi>p</mi> <mrow><mi>i</mi><mtext>,</mtext><mi>j</mi></mrow></msub>
    <mo stretchy="false">)</mo></math>
  prefs: []
  type: TYPE_NORMAL
- en: Where *N* is the number of observations; *M* is the number of class labels (in
    this case, two); log is the natural logarithm; [*yi,j*] is 1 if observation *i*
    is in class *j* and 0 otherwise; and [*pi,j*] is the predicted probability that
    observation *i* is in class *j*.
  prefs: []
  type: TYPE_NORMAL
- en: The machine learning model will generate the fraud probability for each credit
    card transaction. The closer the fraud probabilities are to the true labels (i.e.,
    one for fraud or zero for not fraud), the lower the value of the log loss function.
    This is what the machine learning algorithm will try to minimize.
  prefs: []
  type: TYPE_NORMAL
- en: Create k-Fold Cross-Validation Sets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To help the machine learning algorithm estimate what its performance will be
    on the never-before-seen examples (the test set), it is best practice to further
    split the training set into a training set and a validation set.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we split the training set into fifths, we can train on four-fifths
    of the original training set and evalulate the newly training model by making
    predictions on the fifth slice of the original training set, known as the validation
    set.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to train and evaluate like this five times—leaving aside a different
    fifth slice as the validation set each time. This is known as *k-fold cross-validation*,
    where *k* in this case is five. With this approach, we will have not one estimate
    but five estimates for the generalization error.
  prefs: []
  type: TYPE_NORMAL
- en: We will store the training score and the cross-validation score for each of
    the five runs, and we will store the cross-validation predictions each time. After
    all five runs are complete, we will have cross-validation predictions for the
    entire dataset. This will be the best all-in estimate of the performance the test
    set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how to set up for the *k*-fold validation, where *k* is five:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Machine Learning Models (Part I)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we’re ready to build the machine learning models. For each machine algorithm
    we consider, we will set hyperparameters, train the model, and evaluate the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Model #1: Logistic Regression'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start with the most basic classification algorithm, logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Set hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We will set the penalty to the default value L2 instead of L1\. Compared to
    L1, L2 is less sensitive to outliers and will assign nonzero weights to nearly
    all the features, resulting in a stable solution. L1 will assign high weights
    to the most important features and near-zero weights to the rest, essentially
    performing feature selection as the algorithm trains. However, because the weights
    vary so much feature to feature, the L1 solution is not as stable to changes in
    data points as the L2 solution.^([5](ch02.html#idm140637556129056))
  prefs: []
  type: TYPE_NORMAL
- en: C is the regularization strength. As you may recall from [Chapter 1](ch01.html#Chapter_1),
    regularization helps address overfitting by penalizing complexity. In other words,
    the stronger the regularization, the greater the penalty the machine learning
    algorithm applies to complexity. Regularization nudges the machine learning algorithm
    to prefer simpler models to more complex ones, all else equal.
  prefs: []
  type: TYPE_NORMAL
- en: This regularization constant, C, must be a positive floating number. The smaller
    the value, the stronger the regularization. We will keep the default 1.0.
  prefs: []
  type: TYPE_NORMAL
- en: Our credit card transactions dataset is very imbalanced—out of all the 284,807
    cases, only 492 are fraudulent. As the machine learning algorithm trains, we want
    the algorithm to focus more attention on learning from the positive labeled transactions—in
    other words, the fraudulent transactions—because there are so few of them in the
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: For this logistic regression model, we will set the `class_weight` to balanced.
    This signals to the logistic regression algorithm that we have an imbalanced class
    problem; the algorithm will need to weigh the positive labels more heavily as
    it trains. In this case, the weights will be inversely proportional to the class
    frequencies; the algorithm will assign higher weights to the rare positive labels
    (i.e., fraud) and lower weights to the more frequent negative labels (i.e., not
    fraud).
  prefs: []
  type: TYPE_NORMAL
- en: The random state is fixed to 2018 to help others—such as you, the reader—reproduce
    results. We will keep the default solver liblinear.
  prefs: []
  type: TYPE_NORMAL
- en: Train the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that the hyperparameters are set, we will train the logistic regression
    model on each of the five *k*-fold cross-validation splits, training on four-fifths
    of the training set and evaulating the performance on the fifth slice that is
    held aside.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we train and evaluate like this five times, we will calculate the cost function—log
    loss for our credit card transactions problem—for the training (i.e., the four-fifths
    slice of the original training set) and for the validation (i.e., the one-fifth
    slice of the original training set). We will also store the predictions for each
    of the five cross-validation sets; by the end of the fifth run, we will have predictions
    for the entire training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Evaluate the results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The training log loss and cross-validation log loss are shown for each of the
    five runs in the following code. Generally (but not always) the training log loss
    will be lower than the cross-validation log loss. Because the machine learning
    algorithm has learned directly from the training data, its performance (i.e.,
    log loss) should be better on the training set than on the cross-validation set.
    Remember, the cross-validation set has the transactions that were explicitly held
    out from the training exercise.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For our credit card transactions dataset, it is important to keep in mind that
    we are building a fraud detection solution. When we refer to the *performance*
    of the machine learning model, we mean how good the model is at predicting fraud
    among the transactions in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The machine learning model outputs a prediction probability for each transaction,
    where one is fraud and zero is not fraud. The closer the probability is to one,
    the more likely the transaction is fraudulent; the closer the probability is to
    zero, the more likely the transaction is normal. By comparing the model’s probabilities
    with the true labels, we can assess the goodness of the model.
  prefs: []
  type: TYPE_NORMAL
- en: For each of the five runs, their training and cross-validation log losses are
    similar. The logistic regression model does not exhibit severe overfitting; if
    it did, we would have a low training log loss and comparably high cross-validation
    log loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we stored the predictions for each of the five cross-validation sets,
    we can combine the predictions into a single set. This single set is the same
    as the original training set, and we can now calculate the overall log loss for
    this entire training set. This is the best estimate for the logistic regression
    model’s log loss on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Evaluation Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although the log loss is a great way to estimate the performance of the machine
    learning model, we may want a more intuitive way to understand the results. For
    example, of the fraudulent transactions in the training set, how many did we catch?
    This is known as the *recall*. Or, the transactions that were flagged as fraudulent
    by the logistic regression model, how many were truly fraudulent? This is known
    as the *precision* of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at these and other similar evaluation metrics to help us more
    intuitively grasp the results.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: These evaluation metrics are very important because they empower data scientists
    to intuitively explain results to business people, who may be less familiar with
    log loss, cross-entropy, and other cost functions. The ability to convey complex
    results as simply as possible to nondata scientists is one of the essential skills
    for applied data scientists to master.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion Matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a typical classification problem (without class imbalance) we can evaluate
    the results using a confusion matrix, which is a table that summarizes the number
    of true positives, true negatives, false positives, and false negatives ([Figure 2-3](#confusion_matrix)).^([6](ch02.html#idm140637555940304))
  prefs: []
  type: TYPE_NORMAL
- en: '![Confusion matrix](assets/hulp_0203.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-3\. Confusion matrix
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Given that our credit card transactions dataset is highly imbalanced, using
    the confusion matrix would be meaningful. For example, if we predict that every
    transaction is not fraudulent, we would have 284,315 true negatives, 492 false
    negatives, zero true positives, and zero false positives. We would have a 0% accuracy
    in identifying the truly fraudulent transactions. The confusion matrix does a
    poor job of capturing this suboptimal outcome given this imbalanced class problem.
  prefs: []
  type: TYPE_NORMAL
- en: For problems involving more balanced classes (i.e., the number of true positives
    is roughly similar to the number of true negatives), the confusion matrix may
    be a good, straightforward evaluation metric. We need to find a more appropriate
    evaluation metric given our imbalanced dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Precision-Recall Curve
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For our imbalanced credit card transactions dataset, a better way to evaluate
    the results is to use precision and recall. *Precision* is the number of true
    positives over the number of total positive predictions. In other words, how many
    of the fraudulent transactions does the model catch?
  prefs: []
  type: TYPE_NORMAL
- en: <math><mi>Precision</mi> <mo>=</mo> <mi>True Positives</mi> <mo>∕</mo> <mo>(</mo><mi>True
    Positives</mi> <mo>+</mo> <mi>False Positives</mi><mo>)</mo></math>
  prefs: []
  type: TYPE_NORMAL
- en: A high precision means that—of all our positive predictions—many are true positives
    (in other words, it has a low false positive rate).
  prefs: []
  type: TYPE_NORMAL
- en: '*Recall* is the number of true positives over the number of total actual positives
    in the dataset. In other words how many of the fraudulent transactions does the
    model catch?^([7](ch02.html#idm140637555924656))'
  prefs: []
  type: TYPE_NORMAL
- en: <math><mi>Recall</mi> <mo>=</mo> <mi>True Positives</mi> <mo>∕</mo> <mo>(</mo><mi>True
    Positives</mi> <mo>+</mo> <mi>False Positives</mi><mo>)</mo></math>
  prefs: []
  type: TYPE_NORMAL
- en: A high recall means that the model has captured most of the true positives (in
    other words, it has a low false negative rate).
  prefs: []
  type: TYPE_NORMAL
- en: A solution with high recall but low precision returns many results—capturing
    many of the positives—but with many false alarms. A solution with high precision
    but low recall is the exact opposite; it returns few results—capturing a fraction
    of all the positives in the dataset—but most of its predictions are correct.
  prefs: []
  type: TYPE_NORMAL
- en: To put this into context, if our solution had high precision but low recall,
    there would be a very small number of fraudulent transactions found but most would
    be truly fraudulent.
  prefs: []
  type: TYPE_NORMAL
- en: However, if the solution had low precision but high recall it would flag many
    of the transactions as fraudulent, thus catching a lot of the fraud, but most
    of the flagged transactions would not be fraudulent.
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, both solutions have major problems. In the high precision–low recall
    case, the credit card company would lose a lot of money due to fraud, but it would
    not antagonize customers by unnecessarily rejecting transactions. In the low precision-high
    recall case, the credit card company would catch a lot of the fraud, but it would
    most certainly anger customers by unnecessarily rejecting a lot of normal, non-fraudulent
    transactions.
  prefs: []
  type: TYPE_NORMAL
- en: An optimal solution needs to have high precision and high recall, rejecting
    only those transactions that are truly fraudulent (i.e., high precision) and catching
    most of the fraudulent cases in the dataset (high recall).
  prefs: []
  type: TYPE_NORMAL
- en: There is generally a trade-off between precision and recall, which is usually
    determined by the threshold set by the algorithm to separate the positive cases
    from the negative cases; in our example, positive is fraud and negative is not
    fraud. If the threshold is set too high, very few cases are predicted as positive,
    resulting in high precision but low recall. As the threshold is lowered, more
    cases are predicted as positive, generally decreasing the precision and increasing
    the recall.
  prefs: []
  type: TYPE_NORMAL
- en: For our credit card transactions dataset, think of the threshold as the sensitivity
    of the machine learning model in rejecting transactions. If the threshold is too
    high/strict, the model will reject few transactions, but the ones it does reject
    will be very likely to be fraudulent.
  prefs: []
  type: TYPE_NORMAL
- en: As the threshold moves lower (i.e., becomes less strict), the model will reject
    more transactions, catching more of the fraudulent cases but also unnecessarily
    rejecting more of the normal cases as well.
  prefs: []
  type: TYPE_NORMAL
- en: A graph of the trade-off between precision and recall is known as the precision-recall
    curve. To evaluate the precision-recall curve, we can calculate the average precision,
    which is the weighted mean of the precision achieved at each threshold. The higher
    the average precision, the better the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The choice of the threshold is a very important one and usually involves the
    input of business decision makers. Data scientists can present the precision-recall
    curve to these business decision makers to figure out where the threshold should
    be.
  prefs: []
  type: TYPE_NORMAL
- en: For our credit card transactions dataset, the key question is how do we balance
    customer experience (i.e., avoid rejecting normal transactions) with fraud detection
    (i.e., catch the fraudulent transactions)? We cannot answer this without business
    input, but we can find the model with the best precision-recall curve. Then, we
    can present this model to business decision makers to set the appropriate threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Receiver Operating Characteristic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another good evaluation metric is the area under the receiver operating characteristic
    (auROC). The receiver operating characteristic (ROC) curve plots the true positive
    rate on the Y axis and the false positive rate on the X axis. The true positive
    rate can also be referred to as the sensitivity, and the false positive rate can
    also be referred to as the 1-specificity. The closer the curve is to the top-left
    corner of the plot, the better the solution—with a value of (0.0, 1.0) as the
    absolute optimal point, signifying a 0% false positive rate and a 100% true positive
    rate.
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate the solution, we can compute the area under this curve. The larger
    the auROC, the better the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the logistic regression model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we understand some of the evaluation metrics used, let’s use them to
    better understand the logistic regression model’s results.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s plot the precision-recall curve and calculate the average precision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 2-4](#precision_recall_curve_of_logistic_regression) shows the plot
    of the precision-recall curve. Putting together what we discussed earlier, you
    can see that we can achieve approximately 80% recall (i.e., catch 80% of the fraudulent
    transactions) with approximately 70% precision (i.e., of the transactions the
    model flags as fraudulent, 70% are truly fraudulent while the remaining 30% were
    incorrectly flagged as fraudulent).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Precision-recall curve of logistic regression](assets/hulp_0204.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-4\. Precision-recall curve of logistic regression
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can distill this precision-recall curve into a single number by calculating
    the average precision, which is 0.73 for this logistic regression model. We cannot
    yet tell whether this is good or bad average precision yet since we have no other
    models to compare our logistic regression against.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s measure the auROC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: As shown in [Figure 2-5](#area_under_the_roc_curve_of_logistic_regression),
    the auROC curve is 0.97\. This metric is just another way to evaluate the goodness
    of the logistic regression model, allowing you to determine how much of the fraud
    you can catch while keeping the false positive rate as low as possible. As with
    the average precision, we do not know whether this auROC curve of 0.97 is good
    or not, but we will once we compare it with those of other models.
  prefs: []
  type: TYPE_NORMAL
- en: '![Area under the ROC curve of logistic regression](assets/hulp_0205.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-5\. auROC curve of logistic regression
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Machine Learning Models (Part II)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To compare the goodness of the logistic regression model, let’s build a few
    more models using other supervised learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Model #2: Random Forests'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start with random forests.
  prefs: []
  type: TYPE_NORMAL
- en: As with logistic regression, we will set the hyperparameters, train the model,
    and evaluate the results using the precision-recall curve and the auROC.
  prefs: []
  type: TYPE_NORMAL
- en: Set the hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Let’s start with the default hyperparameters. The number of estimators is set
    at 10; in other words, we will build 10 trees and average the results across these
    10 trees. For each tree, the model will consider the square root of the total
    number of features (in this case, the square root of 30 total features, which
    is 5 features, rounded down).
  prefs: []
  type: TYPE_NORMAL
- en: By setting the `max_depth` to none, the tree will grow as deep as possible,
    splitting as much as possible given the subset of features. Similar to what we
    did for logistic regression, we set the random state to 2018 for reproducibility
    of results and class weight to balanced given our imbalanced dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Train the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will run *k*-fold cross-validation five times, training on four-fifths of
    the training data and predicting on the fifth slice. We will store the predictions
    as we go:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Evaluate the results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The training and cross-validation log loss results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the training log losses are considerably lower than the cross-validation
    log losses, suggesting that the random forests classifier—with the mostly default
    hyperparameters—overfits the data during the training somewhat.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows the log loss over the entire training set (using cross-validation
    predictions):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Even though it overfits the training data somewhat, the random forests has a
    validation log loss that is about one-tenth that of the logistic regression—significant
    improvement over the previous machine learning solution. The random forests model
    is better at correctly flagging the fraud among credit card transactions.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-6](#precision_recall_curve_of_random_forests) shows the precision-recall
    curve of random forests. As you can see from the curve, the model can catch approximately
    80% of all the fraud with approximately 80% precision. This is more impressive
    than the approximately 80% of all the fraud the logistic regression model caught
    with 70% precision.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Precision-recall curve of random forests](assets/hulp_0206.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-6\. Precision-recall curve of random fores"ts
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The average precision of 0.79 of the random forests model is a clear improvement
    over the 0.73 average precision of the logistic regression model. However, the
    auROC, shown in [Figure 2-7](#area_under_the_roc_curve_of_random_forests), is
    somewhat worse—0.93 for random forests versus 0.97 for logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: '![Area under the ROC curve of random forests](assets/hulp_0207.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-7\. auROC curve of random forests
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Model #3: Gradient Boosting Machine (XGBoost)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now let’s train using gradient boosting and evaluate the results. There are
    two popular versions of gradient boosting—one known as XGBoost and another, much
    faster version by Microsoft called LightGBM. Let’s build a model using each one,
    starting with XGBoost.^([8](ch02.html#idm140637555072000))
  prefs: []
  type: TYPE_NORMAL
- en: Set the hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will set this up as a binary classification problem and use log loss as
    the cost function. We will set the max depth of each tree to the default six and
    a default learning rate of 0.3\. For each tree, we will use all the observations
    and all the features; these are the default settings. We will set a random state
    of 2018 to ensure the reproducibility of the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Train the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As before, we will use *k*-fold cross-validation, training on a different four-fifths
    of the training data and predicting on the fifth slice for a total of five runs.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each of the five runs, the gradient boosting model will train for as many
    as two thousand rounds, evaluating whether the cross-validation log loss is decreasing
    as it goes. If the cross-validation log loss stops improving (over the previous
    two hundred rounds), the training process will stop to avoid overfitting. The
    results of the training process are verbose, so we will not print them here, but
    they can be found via the [code on GitHub](http://bit.ly/2Gd4v7e):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Evaluate the results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As shown in the following results, the log loss over the entire training set
    (using the cross-validation predictions) is one-fifth that of the random forests
    and one-fiftieth that of logistic regression. This is a substantial improvement
    over the previous two models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: As shown in [Figure 2-8](#precision_recall_curve_of_xgboost_gradient_boosting),
    the average precision is 0.82, just shy of that of random forests (0.79) and considerably
    better than that of logistic regression (0.73).
  prefs: []
  type: TYPE_NORMAL
- en: '![Precision-recall curve of XGBoost Gradient Boosting](assets/hulp_0208.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-8\. Precision-recall curve of XGBoost gradient boosting
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As shown in [Figure 2-9](#area_under_the_roc_curve_of_xgboost_gradient_boosting),
    the auROC curve is 0.97, the same as that of logistic regression (0.97) and an
    improvement over random forests (0.93). So far, gradient boosting is the best
    of the three models based on the log loss, the precision-recall curve, and the
    auROC.
  prefs: []
  type: TYPE_NORMAL
- en: '![Area under the ROC curve of XGBoost Gradient Boosting](assets/hulp_0209.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-9\. auROC curve of XGBoost gradient boosting
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Model #4: Gradient Boosting Machine (LightGBM)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s now train using another version of gradient boosting known as LightGBM.^([9](ch02.html#idm140637554683376))
  prefs: []
  type: TYPE_NORMAL
- en: Set the hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will set this up as a binary classification problem and use log loss as
    the cost function. We will set the max depth of each tree to 4 and use a learning
    rate of 0.1\. For each tree, we will use all the samples and all the features;
    these are the default settings. We will use the default number of leaves for one
    tree (31) and set a random state to ensure reproducibility of the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Train the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As before, we will use *k*-fold cross-validation and cycle through this five
    times, storing the predictions on the validation sets as we go:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: For each of the five runs, the gradient boosting model will train for as many
    as two thousand rounds, evaluating whether the cross-validation log loss is decreasing
    as it goes. If the cross-validation log loss stops improving (over the previous
    two hundred rounds), the training process will stop to avoid overfitting. The
    results of the training process are verbose, so we will not print them here, but
    they can be found via the [code on GitHub](http://bit.ly/2Gd4v7e).
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate the results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following results show that the log loss over the entire training set (using
    the cross-validation predictions) is similar to that of XGBoost, one-fifth that
    of the random forests and one-fiftieth that of logistic regression. But compared
    to XGBoost, LightGBM is considerably faster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: As shown in [Figure 2-10](#precision_recall_curve_of_lightgbm_gradient_boosting),
    the average precision is 0.82, the same as that of XGboost (0.82), better than
    that of random forests (0.79), and considerably better than that of logistic regression
    (0.73).
  prefs: []
  type: TYPE_NORMAL
- en: '![Precision-recall curve of LightGBM gadient boosting](assets/hulp_0210.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-10\. Precision-recall curve of LightGBM gradient boosting
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As shown in [Figure 2-11](#area_under_the_roc_curve_of_lightgbm_gradient_boosting),
    the auROC curve is 0.98, an improvement over that of XGBoost (0.97), logistic
    regression (0.97), and random forests (0.93).
  prefs: []
  type: TYPE_NORMAL
- en: '![Area under the ROC curve of LightGBM gradient boosting](assets/hulp_0211.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-11\. auROC curve of LightGBM gradient boosting
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Evaluation of the Four Models Using the Test Set
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far in this chapter, we have learned how to:'
  prefs: []
  type: TYPE_NORMAL
- en: Set up the environment for machine learning projects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acquire, load, explore, clean, and visualize data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split the dataset into training and test sets and set up *k*-fold cross-validation
    sets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose the appropriate cost function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set the hyperparameters and perform training and cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have not explored how to adjust the hyperparameters (a process known as hyperparameter
    fine-tuning) to improve the results of each machine learning solution and address
    underfitting/overfitting, but the [code on GitHub](http://bit.ly/2Gd4v7e) will
    allow you to conduct these experiments very easily.
  prefs: []
  type: TYPE_NORMAL
- en: Even without such fine-tuning, the results are pretty clear. Based on our training
    and *k*-fold cross-validation, LightGBM gradient boosting is the best solution,
    closely followed by XGBoost. Random forests and logistic regression are worse.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use the test set as a final evaluation of each of the four models.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each model, we will use the trained model to predict the fraud probabilities
    for the test set transactions. Then, we will calculate the log loss for each model
    by comparing the fraud probabilities predicted by the model against the true fraud
    labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: There are no surprises in the following log loss block. LightGBM gradient boosting
    has the lowest log loss on the test set, followed by the rest.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Figures [2-12](#test_set_precision_recall_curve_of_logistic_regression) through
    [2-19](#test_set_area_under_the_roc_curve_of_lightgbm_gradient_boosting) are the
    precision-recall curves, average precisions, and auROC curve for all four models,
    corroborating our findings above.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Test set precision-recall curve of logistic regression](assets/hulp_0212.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-12\. Test set precision-recall curve of logistic regression
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Test set area under the ROC curve of logistic regression](assets/hulp_0213.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-13\. Test set auROC curve of logistic regression
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Random forests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Test set precision-recall curve of random forests](assets/hulp_0214.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-14\. Test set precision-recall curve of random forests
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Test set area under the ROC curve of random forests](assets/hulp_0215.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-15\. Test set auROC curve of logistic regression
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: XGBoost gradient boosting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Test set precision-recall curve of XGBoost gradient boosting](assets/hulp_0216.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-16\. Test set precision-recall curve of XGBoost gradient boosting
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Test set area under the ROC curve of XGBoost gradient boosting](assets/hulp_0217.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-17\. Test set auROC curve of XGBoost gradient boosting
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: LightGBM gradient boosting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Test set precision-recall curve of LightGBM gradient boosting](assets/hulp_0218.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-18\. Test set precision-recall curve of LightGBM gradient boosting
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Test set area under the ROC curve of LightGBM gradient boosting](assets/hulp_0219.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-19\. Test set auROC curve of LightGBM gradient boosting
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The results of LightGBM gradient boosting are impressive—we can catch over 80%
    of the fraudulent transactions with nearly 90% precision (in other words, in catching
    80% of the total fraud the LightGBM model gets only 10% of the cases wrong).
  prefs: []
  type: TYPE_NORMAL
- en: Considering how few cases of fraud our dataset has, this is a great accomplishment.
  prefs: []
  type: TYPE_NORMAL
- en: Ensembles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Instead of picking just one of the machine learning solutions we have developed
    for use in production, we can evaluate whether an ensemble of the models leads
    to an improved fraud detection rate.^([10](ch02.html#idm140637554057392))
  prefs: []
  type: TYPE_NORMAL
- en: Generally, if we include similarly strong solutions from different machine learning
    families (such as one from random forests and one from neural networks), the ensemble
    of the solutions will lead to a better result than any of the standalone solutions.
    This is because each of the standalone solutions has different strengths and weaknesses.
    By including the standalone solutions together in an ensemble, the strengths of
    some of the models compensate for the weaknesses of the others, and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: There are important caveats, though. If the standalone solutions are similarly
    strong, the ensemble will have better performance than any of the standalone solutions.
    But if one of the solutions is much better than the others, the ensemble’s performance
    will equal the performance of the best standalone solution; the subpar solutions
    will contribute nothing to the ensemble’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: Also, the standalone solutions need to be relatively uncorrelated. If they are
    very correlated, the strengths of one will mirror those of the rest, and the same
    will be true with the weaknesses. We will see little benefit from diversifying
    via an ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: Stacking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our problem here, two of the models (LightGBM gradient boosting and XGBoost
    gradient boosting) are much stronger than the others (random forests and logistic
    regression). But the two strongest models are from the same family, which means
    their strengths and weaknesses will be highly correlated.
  prefs: []
  type: TYPE_NORMAL
- en: We can use stacking (which is a form of ensembling) to determine whether we
    can get an improvement in performance compared to the standalone models from earlier.
    In stacking, we take the predictions from the *k*-fold cross-validation from each
    of the four standalone models (known as *layer one predictions*) and append them
    to the original training dataset. We then train on this original features plus
    layer one predictions dataset using *k*-fold cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: This will result in a new set of *k*-fold cross-validation predictions, known
    as layer two predictions, which we will evaluate to see if we have an improvement
    in performance over any of the standalone models.
  prefs: []
  type: TYPE_NORMAL
- en: Combine layer one predictions with the original training dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, let’s combine the predictions from each of the four machine learning
    models that we have built with the original training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Set the hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now we will use LightGBM gradient boosting—the best machine learning algorithm
    from the earlier exercise—to train on this original features plus layer one predictions
    dataset. The hyperparameters will remain the same as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Train the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As before, we will use *k*-fold cross-validation and generate fraud probabilities
    for the five different cross-validation sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Evaluate the results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the following results, we do not see an improvement. The ensemble log loss
    is very similar to the standalone gradient boosting log loss. Since the best standalone
    solutions are from the same family (gradient boosting), we do not see an improvement
    in the results. They have highly correlated strengths and weaknesses in detecting
    fraud. There is no benefit in diversifying across models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: As shown in Figures [2-20](#precision_recall_curve_of_the_ensemble) and [2-21](#area_under_the_roc_curve_of_the_ensemble),
    the precision-recall curve, the average precision, and the auROC also corroborate
    the lack of improvement.
  prefs: []
  type: TYPE_NORMAL
- en: '![Precision-recall curve of ensemble](assets/hulp_0220.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-20\. Precision-recall curve of the ensemble
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Area under the ROC curve of the ensemble](assets/hulp_0221.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-21\. auROC curve of the ensemble
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Final Model Selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since the ensemble does not improve performance, we favor the simplicity of
    the standalone LightGBM gradient boosting model and will use it in production.
  prefs: []
  type: TYPE_NORMAL
- en: Before we create a pipeline for new, incoming transactions, let’s visualize
    how well the LightGBM model separates the fraudulent transactions from the normal
    transactions for the test set.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-22](#plot_of_prediction_probabilities_and_the_true_label) displays
    the predicted probabilities on the x-axis. Based on this plot, the model does
    a reasonably good job of assigning a high probability of fraud to the transactions
    that are actually fraudulent. Vice versa, the model generally assigns a low probability
    to the transactions that are not fraudulent. Occasionally, the model is wrong,
    and assigns a low probability to a case of actual fraud and a high probability
    to a case of not fraud.'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the results are pretty impressive.
  prefs: []
  type: TYPE_NORMAL
- en: '![Plot of prediction probabilities and the true label](assets/hulp_0222.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-22\. Plot of prediction probabilities and the true label
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Production Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have selected a model for production, let’s design a simple pipeline
    that performs three simple steps on new, incoming data: load the data, scale the
    features, and generate predictions using the LightGBM model we have already trained
    and selected for use in production:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Once these predictions are generated, analysts can act on (i.e., investigate
    further) the ones with the highest predicted probability of being fraudulent and
    work through the list. Or, if automation is the goal, analysts can use a system
    that automatically rejects transactions that have a predicted probability of being
    fraudulent above a certain threshold.
  prefs: []
  type: TYPE_NORMAL
- en: For example, based on [Figure 2-13](#test_set_area_under_the_roc_curve_of_logistic_regression),
    if we automatically reject transactions with a predicted probability above 0.90,
    we will reject cases that are almost certain to be fraudulent without accidentally
    rejecting a case of not fraud.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations! You have built a credit card fraud detection system using supervised
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Together, we set up a machine learning environment, acquired and prepared the
    data, trained and evaluated multiple models, selected the final model for production,
    and designed a pipeline for new, incoming transactions. You have successfully
    created an applied machine learning solution.
  prefs: []
  type: TYPE_NORMAL
- en: Now we will use this same hands-on approach to develop applied machine learning
    solutions using unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The solution above will need to be retrained over time as the patterns of fraud
    change. Also, we should find other machine learning algorithms—from different
    machine learning families—that perform just as well as gradient boosting and include
    them in an ensemble to improve fraud detection performance overall.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, interpretability is very important for real-world applications of machine
    learning. Because the features in this credit card transactions dataset are the
    output of PCA (a form of dimensionality reduction that we will explore in [Chapter 3](ch03.html#Chapter_3))
    we cannot explain in plain English why certain transactions are being flagged
    as potentially fraudulent. For greater interpretability of the results, we need
    access to the original pre-PCA features, which we do not have for this sample
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch02.html#idm140637564820880-marker)) For more on fastcluster, consult
    the [documentation](https://pypi.org/project/fastcluster/).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch02.html#idm140637564803888-marker)) This dataset is available via [Kaggle](https://www.kaggle.com/dalpozz/creditcardfraud)
    and was collected during a research collaboration by Worldline and the Machine
    Learning Group of Universite Libre de Bruxelles. For more information, see Andrea
    Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi, “Calibrating
    Probability with Undersampling for Unbalanced Classification” in Symposium on
    Computational Intelligence and Data Mining (CIDM), IEEE, 2015.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch02.html#idm140637564800304-marker)) Categorical variables take on one
    of a limited number of possible qualitative values and often have to be encoded
    for use in machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch02.html#idm140637556238384-marker)) For more on how the stratify parameter
    preserves the ratio of positive labels, visit [the official website](http://bit.ly/2NiKWfi).
    To reproduce the same split in your experiments, set the random state to 2018\.
    If you set this to another number or don’t set it at all, the results will be
    different.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch02.html#idm140637556129056-marker)) For more on L1 versus L2, refer
    to the blog post [“Differences Between L1 and L2 as Loss Function and Regularization.”](http://bit.ly/2Bcx413)
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch02.html#idm140637555940304-marker)) True positives are instances where
    the prediction and the actual label are both true. True negatives are instances
    where the prediction and the actual label are both false. False positives are
    instances where the prediction is true but the actual label is false (also known
    as a false alarm or Type I error). False negatives are instances where the prediction
    is false but the actual label is true (also known as a miss or Type II error).
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch02.html#idm140637555924656-marker)) Recall is also known as sensitivity
    or true positive rate. Related to sensitivity is a concept called specificity,
    or the true negative rate. This is defined as the number of true negatives over
    the total number of total actual negatives in the dataset. Specificity = true
    negative rate = true negatives / (true negatives + false positives).
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch02.html#idm140637555072000-marker)) For more on XGBoost gradient boosting,
    consult the [GitHub repository](https://github.com/dmlc/xgboost).
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch02.html#idm140637554683376-marker)) For more on Microsoft’s LightGBM
    gradient boosting, consult the [GitHub repository](https://github.com/Microsoft/LightGBM).
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch02.html#idm140637554057392-marker)) For more on ensemble learning,
    refer to the [“Kaggle Ensembling Guide,”](https://mlwave.com/kaggle-ensembling-guide/)
    [“Introduction to Ensembling/Stacking in Python,”](http://bit.ly/2RYV4iF) and
    [“A Kaggler’s Guide to Model Stacking in Practice”](http://bit.ly/2Rrs1iI).
  prefs: []
  type: TYPE_NORMAL
