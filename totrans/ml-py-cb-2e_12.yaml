- en: Chapter 12\. Model Selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 12.0 Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In machine learning, we use training algorithms to learn the parameters of a
    model by minimizing some loss function. However, many learning algorithms (e.g.,
    support vector classifier and random forests) have additional *hyperparameters*
    that are defined by the user and affect how the model will learn its parameters.
    As we mentioned earlier in the book, *parameters* (also sometimes called model
    weights) are what models learn during the training process, whereas hyperparameters
    are provided manually by us (the users).
  prefs: []
  type: TYPE_NORMAL
- en: For example, random forests are collections of decision trees (hence the word
    *forest*); however, the number of decision trees in the forest is not learned
    by the algorithm and must be set prior to fitting. This is often referred to as
    *hyperparameter tuning*, *hyperparameter optimization*, or *model selection*.
    Additionally, we might want to try multiple learning algorithms (for example,
    trying both support vector classifier and random forests to see which learning
    method produces the best model).
  prefs: []
  type: TYPE_NORMAL
- en: 'While there is widespread terminology variation in this area, in this book
    we refer to selecting both the best learning algorithm and its best hyperparameters
    as model selection. The reason is straightforward: imagine we have data and want
    to train a support vector classifier with 10 candidate hyperparameter values and
    a random forest classifier with 10 candidate hyperparameter values. The result
    is that we are trying to select the best model from a set of 20 candidate models.
    In this chapter, we will cover techniques to efficiently select the best model
    from the set of candidates.'
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter we will refer to specific hyperparameters, such as C
    (the inverse of regularization strength). Don’t worry if you don’t know what the
    hyperparameters are. We will cover them in later chapters. Instead, just treat
    hyperparameters like the settings for the learning algorithm that we must choose
    before starting training. In general, finding the model and associated hyperparameters
    that yield the best performance is the result of experimentation—trying a bunch
    of things out and seeing what works best.
  prefs: []
  type: TYPE_NORMAL
- en: 12.1 Selecting the Best Models Using Exhaustive Search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to select the best model by searching over a range of hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use scikit-learn’s `GridSearchCV`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`GridSearchCV` is a brute-force approach to model selection using cross-validation.
    Specifically, a user defines sets of possible values for one or multiple hyperparameters,
    and then `GridSearchCV` trains a model using every value and/or combination of
    values. The model with the best performance score is selected as the best model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in our solution we used logistic regression as our learning algorithm
    and tuned two hyperparameters: C and the regularization penalty. We also specified
    two other parameters, the solver and max iterations. Don’t worry if you don’t
    know what these terms mean; we cover them in the next few chapters. Just realize
    that C and the regularization penalty can take a range of values, which have to
    be specified prior to training. For C, we define 10 possible values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we define two possible values for the regularization penalty: `[''l1'',
    ''l2'']`. For each combination of C and regularization penalty values, we train
    the model and evaluate it using k-fold cross-validation. In our solution, we have
    10 possible values of C, 2 possible values of regularization penalty, and 5 folds.
    They create 10 × 2 × 5 = 100 candidate models, from which the best is selected.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once `GridSearchCV` is complete, we can see the hyperparameters of the best
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, after identifying the best hyperparameters, `GridSearchCV` will
    retrain a model using the best hyperparameters on the entire dataset (rather than
    leaving a fold out for cross-validation). We can use this model to predict values
    like any other scikit-learn model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'One `GridSearchCV` parameter is worth noting: `verbose`. While mostly unnecessary,
    it can be reassuring during long searching processes to receive an indication
    that the search is progressing. The `verbose` parameter determines the number
    of messages outputted during the search, with `0` showing no output, and `1` to
    `3` outputting additional messages.'
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[scikit-learn documentation: GridSearchCV](https://oreil.ly/XlMPG)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 12.2 Selecting the Best Models Using Randomized Search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want a computationally cheaper method than exhaustive search to select the
    best model.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use scikit-learn’s `RandomizedSearchCV`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Recipe 12.1](#selecting-best-models-using-exhaustive-search), we used `GridSearchCV`
    on a user-defined set of hyperparameter values to search for the best model according
    to a score function. A more efficient method than `GridSearchCV`’s brute-force
    search is to search over a specific number of random combinations of hyperparameter
    values from user-supplied distributions (e.g., normal, uniform). scikit-learn
    implements this randomized search technique with `RandomizedSearchCV`.
  prefs: []
  type: TYPE_NORMAL
- en: 'With `RandomizedSearchCV`, if we specify a distribution, scikit-learn will
    randomly sample without replacement hyperparameter values from that distribution.
    As an example of the general concept, here we randomly sample 10 values from a
    uniform distribution ranging from 0 to 4:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, if we specify a list of values, such as two regularization penalty
    hyperparameter values `['l1', 'l2']`, `RandomizedSearchCV` will randomly sample
    with replacement from the list.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like with `GridSearchCV`, we can see the hyperparameter values of the
    best model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'And just like with `GridSearchCV`, after the search is complete `RandomizedSearchCV`
    fits a new model using the best hyperparameters on the entire dataset. We can
    use this model like any other in scikit-learn; for example, to make predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The number of sampled combinations of hyperparameters (i.e., the number of candidate
    models trained) is specified with the `n_iter` (number of iterations) setting.
    It’s worth noting that `RandomizedSearchCV` isn’t inherently faster than `GridSearchCV`,
    but it often achieves comparable performance to `GridSearchCV` in less time just
    by testing fewer combinations.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[scikit-learn documentation: RandomizedSearchCV](https://oreil.ly/rpiSs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Random Search for Hyper-Parameter Optimization](https://oreil.ly/iBcbo)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 12.3 Selecting the Best Models from Multiple Learning Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to select the best model by searching over a range of learning algorithms
    and their respective hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Create a dictionary of candidate learning algorithms and their hyperparameters
    to use as the search space for `GridSearchCV`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous two recipes, we found the best model by searching over possible
    hyperparameter values of a learning algorithm. However, what if we are not certain
    which learning algorithm to use? scikit-learn allows us to include learning algorithms
    as part of the search space. In our solution we define a search space that includes
    two learning algorithms: logistic regression and random forest classifier. Each
    learning algorithm has its own hyperparameters, and we define their candidate
    values using the format `classifier__[*hyperparameter name*]`. For example, for
    our logistic regression, to define the set of possible values for regularization
    hyperparameter space, `C`, and potential types of regularization penalties, `penalty`,
    we create a dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also create a similar dictionary for the random forest hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'After the search is complete, we can use `best_estimator_` to view the best
    model’s learning algorithm and hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Just like with the last two recipes, once we have fit the model selection search,
    we can use this best model just like any other scikit-learn model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 12.4 Selecting the Best Models When Preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to include a preprocessing step during model selection.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Create a pipeline that includes the preprocessing step and any of its parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Very often we will need to preprocess our data before using it to train a model.
    We have to be careful to properly handle preprocessing when conducting model selection.
    First, `GridSearchCV` uses cross-validation to determine which model has the highest
    performance. However, in cross-validation, we are in effect pretending that the
    fold held out as the test set is not seen, and thus not part of fitting any preprocessing
    steps (e.g., scaling or standardization). For this reason, we cannot preprocess
    the data and then run `GridSearchCV`. Rather, the preprocessing steps must be
    a part of the set of actions taken by `GridSearchCV`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This might appear complex, but scikit-learn makes it simple. `FeatureUnion`
    allows us to combine multiple preprocessing actions properly. In our solution,
    we use `FeatureUnion` to combine two preprocessing steps: standardize the feature
    values (`StandardScaler`) and principal component analysis (`PCA`). This object
    is called `preprocess` and contains both of our preprocessing steps. We then include
    `preprocess` in a pipeline with our learning algorithm. The result is that this
    allows us to outsource the proper (and confusing) handling of fitting, transforming,
    and training the models with combinations of hyperparameters to scikit-learn.'
  prefs: []
  type: TYPE_NORMAL
- en: Second, some preprocessing methods have their own parameters, which often have
    to be supplied by the user. For example, dimensionality reduction using PCA requires
    the user to define the number of principal components to use to produce the transformed
    feature set. Ideally, we would choose the number of components that produces a
    model with the greatest performance for some evaluation test metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily, scikit-learn makes this easy. When we include candidate component
    values in the search space, they are treated like any other hyperparameter to
    be searched over. In our solution, we defined `features__pca__n_components'':
    [1, 2, 3]` in the search space to indicate that we want to discover if one, two,
    or three principal components produce the best model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After model selection is complete, we can view the preprocessing values that
    produced the best model. For example, we can see the best number of principal
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 12.5 Speeding Up Model Selection with Parallelization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to speed up model selection.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use all the cores in your machine by setting `n_jobs=-1`, which enables you
    to train multiple models simultaneously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the recipes in this chapter, we have kept the number of candidate models
    small to make the code complete quickly. However, in the real world we may have
    many thousands or tens of thousands of models to train. As a result, it can take
    many hours to find the best model.
  prefs: []
  type: TYPE_NORMAL
- en: To speed up the process, scikit-learn lets us train multiple models simultaneously.
    Without going into too much technical detail, scikit-learn can simultaneously
    train models up to the number of cores on the machine. Most modern laptops have
    at least four cores, so (assuming you’re currently on a laptop) we can potentially
    train four models at the same time. This will dramatically increase the speed
    of our model selection process. The parameter `n_jobs` defines the number of models
    to train in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our solution, we set `n_jobs` to `-1`, which tells scikit-learn to use *all*
    cores. However, by default `n_jobs` is set to `1`, meaning it uses only one core.
    To demonstrate this, if we run the same `GridSearchCV` as in the solution, but
    with `n_jobs=1`, we can see it takes significantly longer to find the best model
    (note that exact time will depend on your computer):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 12.6 Speeding Up Model Selection Using Algorithm-Specific Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to speed up model selection without using additional compute power.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you are using a select number of learning algorithms, use scikit-learn’s
    model-specific cross-validation hyperparameter tuning, `LogisticRegressionCV`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes the characteristics of a learning algorithm allow us to search for
    the best hyperparameters significantly faster than either brute-force or randomized
    model search methods. In scikit-learn, many learning algorithms (e.g., ridge,
    lasso, and elastic net regression) have an algorithm-specific cross-validation
    method to take advantage of this. For example, `LogisticRegression` is used to
    conduct a standard logistic regression classifier, while `LogisticRegressionCV`
    implements an efficient cross-validated logistic regression classifier that can
    identify the optimum value of the hyperparameter C.
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn’s `LogisticRegressionCV` method includes a parameter `Cs`. If supplied
    a list, `Cs` contains the candidate hyperparameter values to select from. If supplied
    an integer, the parameter `Cs` generates a list of that number of candidate values.
    The candidate values are drawn logarithmically from a range between 0.0001 and
    10,0000 (a range of reasonable values for C).
  prefs: []
  type: TYPE_NORMAL
- en: However, a major downside to `LogisticRegressionCV` is that it can only search
    a range of values for C. In [Recipe 12.1](#selecting-best-models-using-exhaustive-search)
    our possible hyperparameter space included both C and another hyperparameter (the
    regularization penalty norm). This limitation is common to many of scikit-learn’s
    model-specific cross-validated approaches.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[scikit-learn documentation: LogisticRegressionCV](https://oreil.ly/uguJi)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[scikit-learn documentation: Model specific cross-validation](https://oreil.ly/6xfn6)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 12.7 Evaluating Performance After Model Selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to evaluate the performance of a model found through model selection.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use nested cross-validation to avoid biased evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Nested cross-validation during model selection is a difficult concept for many
    people to grasp the first time. Remember that in k-fold cross-validation, we train
    our model on *k–1* folds of the data, use this model to make predictions on the
    remaining fold, and then evaluate our model on how well its predictions compare
    to the true values. We then repeat this process *k* times.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the model selection searches described in this chapter (i.e., `GridSearchCV`
    and `RandomizedSearchCV`), we used cross-validation to evaluate which hyperparameter
    values produced the best models. However, a nuanced and generally underappreciated
    problem arises: since we used the data to select the best hyperparameter values,
    we cannot use that same data to evaluate the model’s performance. The solution?
    Wrap the cross-validation used for model search in another cross-validation! In
    nested cross-validation, the “inner” cross-validation selects the best model,
    while the “outer” cross-validation provides an unbiased evaluation of the model’s
    performance. In our solution, the inner cross-validation is our `GridSearchCV`
    object, which we then wrap in an outer cross-validation using `cross_val_score`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are confused, try a simple experiment. First, set `verbose=1` so we
    can see what is happening:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, run `gridsearch.fit(features, target)`, which is our inner cross-validation
    used to find the best model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'From the output you can see the inner cross-validation trained 20 candidate
    models five times, totaling 100 models. Next, nest `clf` inside a new cross-validation,
    which defaults to five folds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The output shows that the inner cross-validation trained 20 models five times
    to find the best model, and this model was evaluated using an outer five-fold
    cross-validation, creating a total of 500 models trained.
  prefs: []
  type: TYPE_NORMAL
