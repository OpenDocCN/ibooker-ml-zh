["```py\ngit clone git@github.com:BBischof/ESRecsys.git\n```", "```py\nabsl-py==1.1.0\ntensorflow==2.9.1\ntyped-ast==1.5.4\ntyping_extensions==4.2.0\njax==0.3.25\nflax==0.5.2\noptax==0.1.2\nwandb==0.13.4\n```", "```py\npython -m venv pinterest_venv\nsource pinterest_venv/bin/activate\n```", "```py\npip install -r requirements.txt\n```", "```py\nFLAGS = flags.FLAGS\n_INPUT_FILE = flags.DEFINE_string(\n  \"input_file\", None, \"Input cat json file.\")\n_OUTPUT_HTML = flags.DEFINE_string(\n  \"output_html\", None, \"The output html file.\")\n_NUM_ITEMS = flags.DEFINE_integer(\n  \"num_items\", 10, \"Number of items to recommend.\")\n\n# Required flag.\nflags.mark_flag_as_required(\"input_file\")\nflags.mark_flag_as_required(\"output_html\")\n\ndef read_catalog(catalog: str) -> Dict[str, str]:\n    \"\"\"\n Reads in the product to category catalog.\n \"\"\"\n    with open(catalog, \"r\") as f:\n        data = f.read()\n    result = json.loads(data)\n    return result\n\ndef dump_html(subset, output_html:str) -> None:\n    \"\"\"\n Dumps a subset of items.\n \"\"\"\n    with open(output_html, \"w\") as f:\n        f.write(\"<HTML>\\n\")\n        f.write(\"\"\"\n <TABLE><tr>\n <th>Key</th>\n <th>Category</th>\n <th>Image</th>\n </tr>\"\"\")\n        for item in subset:\n            key, category = item\n            url = pin_util.key_to_url(key)\n            img_url = \"<img src=\\\"%s\\\">\" % url\n            out = \"<tr><td>%s</td><td>%s</td><td>%s</td></tr>\\n\" %\n            (key, category, img_url)\n            f.write(out)\n        f.write(\"</TABLE></HTML>\")\n\ndef main(argv):\n    \"\"\"\n Main function.\n \"\"\"\n    del argv  # Unused.\n\n    catalog = read_catalog(_INPUT_FILE.value)\n    catalog = list(catalog.items())\n    random.shuffle(catalog)\n    dump_html(catalog[:_NUM_ITEMS.value], _OUTPUT_HTML.value)\n```", "```py\npython3 random_item_recommender.py\n--input_file=STL-Dataset/fashion-cat.json --output_html=output.html\n```", "```py\nwandb artifact get building-recsys/recsys-pinterest/shop_the_look:latest\n```", "```py\nimport tensorflow as tf\n\ndef normalize_image(img):\n  img = tf.cast(img, dtype=tf.float32)\n  img = (img / 255.0) - 0.5\n  return img\n\ndef process_image(x):\n  x = tf.io.read_file(x)\n  x = tf.io.decode_jpeg(x, channels=3)\n  x = tf.image.resize_with_crop_or_pad(x, 512, 512)\n  x = normalize_image(x)\n  return x\n\ndef process_image_with_id(id):\n  image = process_image(id)\n  return id, image\n\ndef process_triplet(x):\n  x = (process_image(x[0]), process_image(x[1]), process_image(x[2]))\n  return x\n\ndef create_dataset(\n    triplet: Sequence[Tuple[str, str, str]]):\n    \"\"\"Creates a triplet dataset.\n Args:\n triplet: filenames of scene, positive product, negative product.\n \"\"\"\n    ds = tf.data.Dataset.from_tensor_slices(triplet)\n    ds = ds.map(process_triplet)\n    return ds\n```", "```py\nfrom flax import linen as nn\nimport jax.numpy as jnp\n\nclass CNN(nn.Module):\n    \"\"\"Simple CNN.\"\"\"\n    filters : Sequence[int]\n    output_size : int\n\n    @nn.compact\n    def __call__(self, x, train: bool = True):\n        for filter in self.filters:\n            # Stride 2 downsamples 2x.\n            residual = nn.Conv(filter, (3, 3), (2, 2))(x)\n            x = nn.Conv(filter, (3, 3), (2, 2))(x)\n            x = nn.BatchNorm(\n              use_running_average=not train, use_bias=False)(x)\n            x = nn.swish(x)\n            x = nn.Conv(filter, (1, 1), (1, 1))(x)\n            x = nn.BatchNorm(\n              use_running_average=not train, use_bias=False)(x)\n            x = nn.swish(x)\n            x = nn.Conv(filter, (1, 1), (1, 1))(x)\n            x = nn.BatchNorm(\n              use_running_average=not train, use_bias=False)(x)\n            x = x + residual\n            # Average pool downsamples 2x.\n            x = nn.avg_pool(x, (3, 3), strides=(2, 2), padding=\"SAME\")\n        x = jnp.mean(x, axis=(1, 2))\n        x = nn.Dense(self.output_size, dtype=jnp.float32)(x)\n        return x\n\nclass STLModel(nn.Module):\n    \"\"\"Shop the look model that takes in a scene\n and item and computes a score for them.\n \"\"\"\n    output_size : int\n\n    def setup(self):\n        default_filter = [16, 32, 64, 128]\n        self.scene_cnn = CNN(\n          filters=default_filter, output_size=self.output_size)\n        self.product_cnn = CNN(\n          filters=default_filter, output_size=self.output_size)\n\n    def get_scene_embed(self, scene):\n        return self.scene_cnn(scene, False)\n\n    def get_product_embed(self, product):\n        return self.product_cnn(product, False)\n\n    def __call__(self, scene, pos_product, neg_product,\n                 train: bool = True):\n        scene_embed = self.scene_cnn(scene, train)\n\n        pos_product_embed = self.product_cnn(pos_product, train)\n        pos_score = scene_embed * pos_product_embed\n        pos_score = jnp.sum(pos_score, axis=-1)\n\n        neg_product_embed = self.product_cnn(neg_product, train)\n        neg_score = scene_embed * neg_product_embed\n        neg_score = jnp.sum(neg_score, axis=-1)\n\n        return pos_score, neg_score, scene_embed,\n          pos_product_embed, neg_product_embed\n```", "```py\ndef generate_triplets(\n    scene_product: Sequence[Tuple[str, str]],\n    num_neg: int) -> Sequence[Tuple[str, str, str]]:\n    \"\"\"Generate positive and negative triplets.\"\"\"\n    count = len(scene_product)\n    train = []\n    test = []\n    key = jax.random.PRNGKey(0)\n    for i in range(count):\n        scene, pos = scene_product[i]\n        is_test = i % 10 == 0\n        key, subkey = jax.random.split(key)\n        neg_indices = jax.random.randint(subkey, [num_neg], 0, count - 1)\n        for neg_idx in neg_indices:\n            _, neg = scene_product[neg_idx]\n            if is_test:\n                test.append((scene, pos, neg))\n            else:\n                train.append((scene, pos, neg))\n    return train, test\n\n def shuffle_array(key, x):\n    \"\"\"Deterministic string shuffle.\"\"\"\n    num = len(x)\n    to_swap = jax.random.randint(key, [num], 0, num - 1)\n    return [x[t] for t in to_swap]\n```", "```py\ndef train_step(state, scene, pos_product,\n               neg_product, regularization, batch_size):\n    def loss_fn(params):\n        result, new_model_state = state.apply_fn(\n            params,\n            scene, pos_product, neg_product, True,\n            mutable=['batch_stats'])\n        triplet_loss = jnp.sum(nn.relu(1.0 + result[1] - result[0]))\n        def reg_fn(embed):\n            return nn.relu(\n              jnp.sqrt(jnp.sum(jnp.square(embed), axis=-1)) - 1.0)\n        reg_loss = reg_fn(result[2]) +\n                   reg_fn(result[3]) + reg_fn(result[4])\n        reg_loss = jnp.sum(reg_loss)\n        return (triplet_loss + regularization * reg_loss) / batch_size\n\n    grad_fn = jax.value_and_grad(loss_fn)\n    loss, grads = grad_fn(state.params)\n    new_state = state.apply_gradients(grads=grads)\n    return new_state, loss\n\ndef eval_step(state, scene, pos_product, neg_product):\n    def loss_fn(params):\n        result, new_model_state = state.apply_fn(\n            state.params,\n            scene, pos_product, neg_product, True,\n            mutable=['batch_stats'])\n        # Use a fixed margin for the eval.\n        triplet_loss = jnp.sum(nn.relu(1.0 + result[1] - result[0]))\n        return triplet_loss\n```", "```py\ndef main(argv):\n    \"\"\"Main function.\"\"\"\n    del argv  # Unused.\n    config = {\n        \"learning_rate\" : _LEARNING_RATE.value,\n        \"regularization\" : _REGULARIZATION.value,\n        \"output_size\" : _OUTPUT_SIZE.value\n    }\n\n    run = wandb.init(\n        config=config,\n        project=\"recsys-pinterest\"\n    )\n\n    tf.config.set_visible_devices([], 'GPU')\n    tf.compat.v1.enable_eager_execution()\n    logging.info(\"Image dir %s, input file %s\",\n      _IMAGE_DIRECTORY.value, _INPUT_FILE.value)\n    scene_product = pin_util.get_valid_scene_product(\n      _IMAGE_DIRECTORY.value, _INPUT_FILE.value)\n    logging.info(\"Found %d valid scene product pairs.\" % len(scene_product))\n\n    train, test = generate_triplets(scene_product, _NUM_NEG.value)\n    num_train = len(train)\n    num_test = len(test)\n    logging.info(\"Train triplets %d\", num_train)\n    logging.info(\"Test triplets %d\", num_test)\n\n     # Random shuffle the train.\n    key = jax.random.PRNGKey(0)\n    train = shuffle_array(key, train)\n    test = shuffle_array(key, test)\n    train = np.array(train)\n    test = np.array(test)\n\n    train_ds = input_pipeline.create_dataset(train).repeat()\n    train_ds = train_ds.batch(_BATCH_SIZE.value).prefetch(\n      tf.data.AUTOTUNE)\n\n    test_ds = input_pipeline.create_dataset(test).repeat()\n    test_ds = test_ds.batch(_BATCH_SIZE.value)\n\n    stl = models.STLModel(output_size=wandb.config.output_size)\n    train_it = train_ds.as_numpy_iterator()\n    test_it = test_ds.as_numpy_iterator()\n    x = next(train_it)\n    key, subkey = jax.random.split(key)\n    params = stl.init(subkey, x[0], x[1], x[2])\n    tx = optax.adam(learning_rate=wandb.config.learning_rate)\n    state = train_state.TrainState.create(\n        apply_fn=stl.apply, params=params, tx=tx)\n    if _RESTORE_CHECKPOINT.value:\n        state = checkpoints.restore_checkpoint(_WORKDIR.value, state)\n\n    train_step_fn = jax.jit(train_step)\n    eval_step_fn = jax.jit(eval_step)\n\n    losses = []\n    init_step = state.step\n    logging.info(\"Starting at step %d\", init_step)\n    regularization = wandb.config.regularization\n    batch_size = _BATCH_SIZE.value\n    eval_steps = int(num_test / batch_size)\n    for i in range(init_step, _MAX_STEPS.value + 1):\n        batch = next(train_it)\n        scene = batch[0]\n        pos_product = batch[1]\n        neg_product = batch[2]\n\n        state, loss = train_step_fn(\n            state, scene, pos_product, neg_product,\n            regularization, batch_size)\n        losses.append(loss)\n        if i % _CHECKPOINT_EVERY_STEPS.value == 0 and i > 0:\n            logging.info(\"Saving checkpoint\")\n            checkpoints.save_checkpoint(\n              _WORKDIR.value, state, state.step, keep=3)\n        metrics = {\n            \"step\" : state.step\n        }\n        if i % _EVAL_EVERY_STEPS.value == 0 and i > 0:\n            eval_loss = []\n            for j in range(eval_steps):\n                ebatch = next(test_it)\n                escene = ebatch[0]\n                epos_product = ebatch[1]\n                eneg_product = ebatch[2]\n                loss = eval_step_fn(\n                  state, escene, epos_product, eneg_product)\n                eval_loss.append(loss)\n            eval_loss = jnp.mean(jnp.array(eval_loss)) / batch_size\n            metrics.update({\"eval_loss\" : eval_loss})\n        if i % _LOG_EVERY_STEPS.value == 0 and i > 0:\n            mean_loss = jnp.mean(jnp.array(losses))\n            losses = []\n            metrics.update({\"train_loss\" : mean_loss})\n            wandb.log(metrics)\n            logging.info(metrics)\n\n    logging.info(\"Saving as %s\", _MODEL_NAME.value)\n    data = flax.serialization.to_bytes(state)\n    metadata = { \"output_size\" : wandb.config.output_size }\n    artifact = wandb.Artifact(\n        name=_MODEL_NAME.value,\n        metadata=metadata,\n        type=\"model\")\n    with artifact.new_file(\"pinterest_stl.model\", \"wb\") as f:\n        f.write(data)\n    run.log_artifact(artifact)\n\nif __name__ == \"__main__\":\n    app.run(main)\n```", "```py\n    model = models.STLModel(output_size=_OUTPUT_SIZE.value)\n    state = None\n    logging.info(\"Attempting to read model %s\", _MODEL_NAME.value)\n    with open(_MODEL_NAME.value, \"rb\") as f:\n        data = f.read()\n        state = flax.serialization.from_bytes(model, data)\n    assert(state != None)\n\n    @jax.jit\n    def get_scene_embed(x):\n      return model.apply(state[\"params\"], x, method=models.STLModel.get_scene_embed)\n    @jax.jit\n    def get_product_embed(x):\n      return model.apply(\n      state[\"params\"],\n      x,\n      method=models.STLModel.get_product_embed\n      )\n\n    ds = tf.data.Dataset\n      .from_tensor_slices(unique_scenes)\n      .map(input_pipeline.process_image_with_id)\n    ds = ds.batch(_BATCH_SIZE.value, drop_remainder=True)\n    it = ds.as_numpy_iterator()\n    scene_dict = {}\n    count = 0\n    for id, image in it:\n      count = count + 1\n      if count % 100 == 0:\n        logging.info(\"Created %d scene embeddings\", count * _BATCH_SIZE.value)\n      result = get_scene_embed(image)\n      for i in range(_BATCH_SIZE.value):\n        current_id = id[i].decode(\"utf-8\")\n        tmp = np.array(result[i])\n        current_result = [float(tmp[j]) for j in range(tmp.shape[0])]\n        scene_dict.update({current_id : current_result})\n    scene_filename = os.path.join(_OUTDIR.value, \"scene_embed.json\")\n    with open(scene_filename, \"w\") as scene_file:\n      json.dump(scene_dict, scene_file)\n```", "```py\ndef find_top_k(\n  scene_embedding,\n  product_embeddings,\n  k):\n  \"\"\"\n Finds the top K nearest product embeddings to the scene embedding.\n Args:\n scene_embedding: embedding vector for the scene\n product_embedding: embedding vectors for the products.\n k: number of top results to return.\n \"\"\"\n\n  scores = scene_embedding * product_embeddings\n  scores = jnp.sum(scores, axis=-1)\n  scores_and_indices = jax.lax.top_k(scores, k)\n  return scores_and_indices\n\ntop_k_finder = jax.jit(find_top_k, static_argnames=[\"k\"])\n```", "```py\nwandb artifact get building-recsys/recsys-pinterest/scene_product_results:v0\n```"]