- en: Chapter 3\. Dimensionality Reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will focus on one of the major challenges in building successful
    applied machine learning solutions: the curse of dimensionality. Unsupervised
    learning has a great counter—*dimensionality reduction*. In this chapter, we will
    introduce this concept and build from there so that you can develop an intuition
    for how it all works.'
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 4](ch04.html#Chapter_4), we will build our own unsupervised learning
    solution based on dimensionality reduction—specifically, an unsupervised learning-based
    credit card fraud detection system (as opposed to the supervised-based system
    we built in [Chapter 2](ch02.html#Chapter_2)). This type of unsupervised fraud
    detection is known as anomaly detection, a rapidly growing area in the field of
    applied unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: But before we build an anomaly detection system, let’s cover dimensionality
    reduction in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The Motivation for Dimensionality Reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in [Chapter 1](ch01.html#Chapter_1), dimensionality reduction helps
    counteract one of the most commonly occurring problems in machine learning—the
    curse of dimensionality—in which algorithms cannot effectively and efficiently
    train on the data because of the sheer size of the feature space.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction algorithms project high-dimensional data to a low-dimensional
    space, retaining as much of the salient information as possible while removing
    redundant information. Once the data is in the low-dimensional space, machine
    learning algorithms are able to identify interesting patterns more effectively
    and efficiently because a lot of the noise has been reduced.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, dimensionality reduction is the goal itself—for example, to build
    anomaly detection systems, as we will show in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Other times, dimensionality reduction is not an end in itself but rather a means
    to another end. For example, dimensionality reduction is commonly a part of the
    machine learning pipeline to help solve large-scale, computationally expensive
    problems involving images, video, speech, and text.
  prefs: []
  type: TYPE_NORMAL
- en: The MNIST Digits Database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we introduce the dimensionality reduction algorithms, let’s explore
    the dataset that we will use in this chapter. We will work with a simple computer
    vision dataset: the MNIST (Mixed National Institute of Standards and Technology)
    database of handwritten digits, one of the best known datasets in machine learning.
    We will use the version of the MNIST dataset publicly available on Yann LeCun’s
    website.^([1](ch03.html#idm140637553285552)) To make it easier, we will use the
    pickled version, courtesy of [deeplearning.net](http://deeplearning.net).^([2](ch03.html#idm140637553283488))'
  prefs: []
  type: TYPE_NORMAL
- en: This dataset has been divided into three sets—a training set with 50,000 examples,
    a validation set with 10,000 examples, and a test set with 10,000 examples. We
    have labels for all the examples.
  prefs: []
  type: TYPE_NORMAL
- en: This dataset consists of 28x28 pixel images of handwritten digits. Every single
    data point (i.e., every image) can be conveyed as an array of numbers, where each
    number describes how dark each pixel is. In other words, a 28x28 array of numbers
    corresponds to a 28x28 pixel image.
  prefs: []
  type: TYPE_NORMAL
- en: To make this simpler, we can flatten each array into a 28x28, or 784, dimensional
    vector. Each component of the vector is a float between zero and one—representing
    the intensity of each pixel in the image. Zero stands for black; one stands for
    white. The labels are numbers between zero and nine, and indicate which digit
    the image represents.
  prefs: []
  type: TYPE_NORMAL
- en: Data acquisition and exploration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we work with the dimensionality reduction algorithms, let’s load the
    libraries we will use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Load the MNIST datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s now load the MNIST datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Verify shape of datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s verify the shape of the datasets to make sure they loaded properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code confirms the shapes of the datasets are as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Create Pandas DataFrames from the datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s convert the numpy arrays into Pandas DataFrames so they are easier to
    explore and work with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Explore the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s generate a summary view of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[Table 3-1](#data_exploration) displays a summary view of the image data. Many
    of the values are zeros—in other words, most of the pixels in the images are black.
    This makes sense since the digits are in white and shown in the middle of the
    image on a black backdrop.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-1\. Data exploration
  prefs: []
  type: TYPE_NORMAL
- en: '|  | 0 | 1 | 2 | 3 | 4 | 5 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| count | 50000.0 | 50000.0 | 50000.0 | 50000.0 | 50000.0 | 50000.0 | 50000.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| mean | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| std | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| min | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 25% | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 50% | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 75% | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| max | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 rows x 784 columns |'
  prefs: []
  type: TYPE_TB
- en: 'The labels data is a one-dimensional vector representing the actual content
    in the image. Labels for the first few images are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Display the images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s define a function to view the image along with its label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: A view of the first image—once the 784-dimensional vector is reshaped into a
    28 x 28 pixel image—shows the number five ([Figure 3-1](#view_the_first_digit)).
  prefs: []
  type: TYPE_NORMAL
- en: '![View the first digit](assets/hulp_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. View of the first digit
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Dimensionality Reduction Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we’ve loaded and explored the MNIST digits dataset, let’s move to the
    dimensionality reduction algorithms. For each algorithm, we will introduce the
    concept first and then build a deeper understanding by applying the algorithm
    to the MNIST digits dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Projection vs. Manifold Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are two major branches of dimensionality reduction. The first is known
    as *linear projection*, which involves linearly projecting data from a high-dimensional
    space to a low-dimensional space. This includes techniques such as *principal
    component analysis, singular value decomposition*, and *random projection*.
  prefs: []
  type: TYPE_NORMAL
- en: The second is known as *manifold learning*, which is also referred to as *nonlinear
    dimensionality reduction*. This involves techniques such as *isomap*, which learns
    the *curved distance* (also called the *geodesic distance*) between points rather
    than the *Euclidean distance*. Other techniques include *multidimensional scaling
    (MDS), locally linear embedding (LLE), t-distributed stochastic neighbor embedding
    (t-SNE), dictionary learning, random trees embedding*, and *independent component
    analysis*.
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will explore several versions of PCA, including standard PCA, incremental
    PCA, sparse PCA, and kernel PCA.
  prefs: []
  type: TYPE_NORMAL
- en: PCA, the Concept
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start with standard PCA, one of the most common linear dimensionality
    reduction techniques. In PCA, the algorithm finds a low-dimensional representation
    of the data while retaining as much of the variation (i.e., salient information)
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: PCA does this by addressing the correlation among features. If the correlation
    is very high among a subset of the features, PCA will attempt to combine the highly
    correlated features and represent this data with a smaller number of linearly
    uncorrelated features. The algorithm keeps performing this correlation reduction,
    finding the directions of maximum variance in the original high-dimensional data
    and projecting them onto a smaller dimensional space. These newly derived components
    are known as principal components.
  prefs: []
  type: TYPE_NORMAL
- en: With these components, it is possible to reconstruct the original features—not
    exactly but generally close enough. The PCA algorithm actively attempts to minimize
    the reconstruction error during its search for the optimal components.
  prefs: []
  type: TYPE_NORMAL
- en: In our MNIST example, the original feature space has 784 dimensions, known as
    *d* dimensions. PCA will project the data onto a smaller subspace of *k* dimensions
    (where *k < d*) while retaining as much of the salient information as possible.
    These *k* dimensions are known as the principal components.
  prefs: []
  type: TYPE_NORMAL
- en: The number of meaningful principal components we are left with is considerably
    smaller than the number of dimensions in the original dataset. We lose some of
    the variance (i.e., information) by moving to this low-dimensional space, but
    the underlying structure of the data is easier to identify, allowing us to perform
    tasks like anomaly detection and clustering more effectively and efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, by reducing the dimensionality of the data, PCA will reduce the size
    of the data, improving the performance of machine learning algorithms further
    along in the machine learning pipeline (for example, for tasks such as image classification).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is essential to perform feature scaling before running PCA. PCA is very sensitive
    to the relative ranges of the original features. Generally we must scale the data
    to make sure the features are in the same relative range. However, for our MNIST
    digits dataset, the features are already scaled to a range of zero to one, so
    we can skip this step.
  prefs: []
  type: TYPE_NORMAL
- en: PCA in Practice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you have a better grasp of how PCA works, let’s apply PCA to the MNIST
    digits dataset and see how well PCA captures the most salient information about
    the digits as its projects the data from the original 784-dimensional space to
    a lower dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: Set the hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s set the hyperparameters for the PCA algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Apply PCA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will set the number of principal components to the original number of dimensions
    (i.e., 784). Then, PCA will capture the salient information from the original
    dimensions and start generating principal components. Once these components are
    generated, we will determine how many principal components we need to effectively
    capture most of the variance/information from the original feature set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s fit and transform our training data, generating these principal components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Evaluate PCA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Because we have not reduced the dimensionality at all (we’ve just transformed
    the data) the variance/information of the original data captured by the 784 principal
    components should be 100%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'However, it is important to note that the importance of the 784 principal components
    varies quite a bit. The importance of the first X principal components are summarized
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The first 10 components in total capture approximately 50% of the variance,
    the first one hundred components over 90%, and the first three hundred components
    almost 99% of the variance; the information in the rest of the principal components
    is of negligible value.
  prefs: []
  type: TYPE_NORMAL
- en: We can also plot the importance of each principal component, ranked from the
    first principal component to the last. For the sake of readability, just the first
    10 components are displayed in [Figure 3-2](#importance_of_pca_components).
  prefs: []
  type: TYPE_NORMAL
- en: The power of PCA should be more apparent now. With just the first two hundred
    principal components (far fewer than the original 784 dimensions), we capture
    over 96% of the variance/information.
  prefs: []
  type: TYPE_NORMAL
- en: PCA allows us to reduce the dimensionality of the original data substantially
    while retaining most of the salient information. On the PCA-reduced feature set,
    other machine learning algorithms—downstream in the machine learning pipeline—will
    have an easier time separating the data points in space (to perform tasks such
    as anomaly detection and clustering) and will require fewer computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: '![Importance of PCA components](assets/hulp_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. Importance of PCA components
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Visualize the separation of points in space
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To demonstrate the power of PCA to efficiently and compactly capture the variance/information
    in data, let’s plot the observations in two dimensions. Specifically, we will
    display a scatterplot of the first and second principal components and mark the
    observations by the true label. Let’s create a function for this called `scatterPlot`
    because we also need to present visualizations for the other dimensionality algorithms
    later on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As seen in [Figure 3-3](#separation_of_observations_using_pca), with just the
    top two principal components, PCA does a good job of separating the points in
    space such that similar points are generally closer to each other than they are
    to other, less similar points. In other words, images of the same digit are closer
    to each other than they are to images of other digits.
  prefs: []
  type: TYPE_NORMAL
- en: PCA accomplishes this without using any labels whatsoever. This demonstrates
    the power of unsupervised learning to capture the underlying structure of data,
    helping discover hidden patterns in the absence of labels.
  prefs: []
  type: TYPE_NORMAL
- en: '![Separation of Observations Using PCA](assets/hulp_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. Separation of observations using PCA
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If we run the same two-dimensional scatterplot using two of the most important
    features from the original 784 feature set—determined by training a supervised
    learning model—the separation is poor, at best ([Figure 3-4](#separation_of_observations_without_pca)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Separation of Observations Without PCA](assets/hulp_0304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-4\. Separation of observations without PCA
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Comparison of Figures [3-3](#separation_of_observations_using_pca) and [3-4](#separation_of_observations_without_pca)
    shows just how powerful PCA is in learning the underlying structure of the dataset
    without using any labels whatsoever—even with just two dimensions, we can start
    meaningfully separating the images by the digits they display.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Not only does PCA help separate data so that we can discover hidden patterns
    more readily, it also helps reduce the size of the feature set, making it less
    costly—both in time and in computational resources—to train machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: With the MNIST dataset, the reduction in training time will be modest at best
    since the dataset is very small—we have only 784 features and 50,000 observations.
    But if the dataset were millions of features and billions of observations, dimensionality
    reduction would dramatically reduce the training time of the machine learning
    algorithms further along in the machine learning pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, PCA usually throws away some of the information available in the original
    feature set but does so wisely, capturing the most important elements and tossing
    the less valuable ones. A model that is trained on a PCA-reduced feature set may
    not perform quite as well in terms of accuracy as a model that is trained on the
    full feature set, but both the training and prediction times will be much faster.
    This is one of the important trade-offs you must consider when choosing whether
    to use dimensionality reduction in your machine learning product.
  prefs: []
  type: TYPE_NORMAL
- en: Incremental PCA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For datasets that are very large and cannot fit in memory, we can perform PCA
    incrementally in small batches, where each batch is able to fit in memory. The
    batch size can be either set manually or determined automatically. This batch-based
    form of PCA is known as *incremental PCA*. The resulting principal components
    of PCA and incremental PCA are generally pretty similar ([Figure 3-5](#separation_of_observations_using_incremental_pca)).
    Here is the code for incremental PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![Separation of Observations Using Incremental PCA](assets/hulp_0305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-5\. Separation of observations using incremental PCA
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Sparse PCA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The normal PCA algorithm searches for linear combinations in all the input variables,
    reducing the original feature space as densely as possible. But for some machine
    learning problems, some degree of sparsity may be preferred. A version of PCA
    that retains some degree of sparsity—controlled by a hyperparameter called *alpha*—is
    known as *sparse PCA*. The sparse PCA algorithm searches for linear combinations
    in just some of the input variables, reducing the original feature space to some
    degree but not as compactly as normal PCA.
  prefs: []
  type: TYPE_NORMAL
- en: Because this algorithm trains a bit more slowly than normal PCA, we will train
    on just the first 10,000 examples in our training set (out of the total 50,000
    examples). We will continue this practice of training on fewer than the total
    number of observations when the algorithm training times are slow.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our purposes (i.e., developing some intuition of how these dimensionality
    reduction algorithms work), the reduced training process is fine. For a better
    solution, training on the complete training set is advised:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 3-6](#separation_of_observations_using_sparse_pca) shows a two-dimensional
    scatterplot using the first two principal components using sparse PCA.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Separation of Observations Using Sparse PCA](assets/hulp_0306.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-6\. Separation of observations using sparse PCA
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice that this scatterplot looks different from that of the normal PCA, as
    expected. Normal and sparse PCA generate principal components differently, and
    the separation of points is somewhat different, too.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel PCA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Normal PCA, incremental PCA, and sparse PCA linearly project the original data
    onto a lower dimensional space, but there is also a nonlinear form of PCA known
    as *kernel PCA*, which runs a similarity function over pairs of original data
    points in order to perform nonlinear dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: By learning this similarity function (known as the *kernel method*), kernel
    PCA maps the implicit feature space where the majority of data points lie and
    creates this implicit feature space in a much smaller number of dimensions than
    the dimensions in the original feature set. This method is especially effective
    when the original feature set is not linearly separable.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the kernel PCA algorithm, we need to set the number of components we desire,
    the type of kernel, and the kernel coefficient, which is known as the *gamma*.
    The most popular kernel is the *radial basis function kernel*, more commonly referred
    to as the *RBF kernel*. This is what we will use here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The two-dimensional scatterplot of the kernel PCA is nearly identical to the
    one of the linear PCA for our MNIST digits dataset ([Figure 3-7](#separation_of_observations_using_kernel_pca)).
    Learning the RBF kernel does not improve the dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: '![Separation of Observations Using Kernel PCA](assets/hulp_0307.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-7\. Separation of observations using kernel PCA
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Singular Value Decomposition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another approach to learning the underlying structure of the data is to reduce
    the rank of the original matrix of features to a smaller rank such that the original
    matrix can be recreated using a linear combination of some of the vectors in the
    smaller rank matrix. This is known as *singular value decomposition (SVD)*.
  prefs: []
  type: TYPE_NORMAL
- en: To generate the smaller rank matrix, SVD keeps the vectors of the original matrix
    that have the most information (i.e., the highest singular value). The smaller
    rank matrix captures the most important elements of the original feature space.
  prefs: []
  type: TYPE_NORMAL
- en: This is very similar to PCA. PCA, which uses the eigen-decomposition of the
    covariance matrix to perform dimensionality reduction. SVD uses singular value
    decomposition, as its name implies. In fact, PCA involves the use of SVD in its
    calculation, but much of this discussion is beyond the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how SVD works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 3-8](#separation_of_observations_using_svd) displays the separation
    of points that we achieve using the two most important vectors from SVD.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Separation of Observations Using SVD](assets/hulp_0308.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-8\. Separation of observations using SVD
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Random Projection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another linear dimensionality reduction technique is random projection, which
    relies on the *Johnson–Lindenstrauss lemma*. According to the Johnson–Lindenstrauss
    lemma, points in a high-dimensional space can be embedded into a much lower-dimensional
    space so that distances between the points are nearly preserved. In other words,
    even as we move from high-dimensional space to low-dimensional space, the relevant
    structure of the original feature set is preserved.
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian Random Projection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are two versions of random projection—the standard version known as *Gaussian
    random projection* and a sparse version known as *sparse random projection*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For Gaussian random projection, we can either specify the number of components
    we would like to have in the reduced feature space, or we can set the hyperparameter
    *eps*. The eps controls the quality of the embedding according to the Johnson–Lindenstrauss
    lemma, where smaller values generate a higher number of dimensions. In our case,
    we will set this hyperparameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 3-9](#separation_of_observations_using_gaussian_random_projection)
    shows the two-dimensional scatterplot using Gaussian random projection.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Separation of Observations Using Gaussian Random Projection](assets/hulp_0309.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-9\. Separation of observations using Gaussian random projection
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Although it is a form of linear projection like PCA, random projection is an
    entirely different family of dimensionality reduction. Thus the random projection
    scatterplot looks very different from the scatterplots of normal PCA, incremental
    PCA, sparse PCA, and kernel PCA.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse Random Projection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Just as there is a sparse version of PCA, there is a sparse version of random
    projection known as sparse random projection. It retains some degree of sparsity
    in the transformed feature set and is generally much more efficient, transforming
    the original data into the reduced space much faster than normal Gaussian random
    projection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 3-10](#separation_of_observations_using_sparse_random_projection) shows
    the two-dimensional scatterplot using sparse random projection.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Separation of Observations Using Sparse Random Projection](assets/hulp_0310.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-10\. Separation of observations using sparse random projection
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Isomap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Instead of linearly projecting the data from a high-dimensional space to a low-dimensional
    space, we can use nonlinear dimensionality reduction methods. These methods are
    collectively known as manifold learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most vanilla form of manifold learning is known as *isometric mapping*,
    or *Isomap* for short. Like kernel PCA, Isomap learns a new, low-dimensional embedding
    of the original feature set by calculating the pairwise distances of all the points,
    where distance is *curved* or *geodesic distance* rather than *Euclidean distance*.
    In other words, it learns the intrinsic geometry of the original data based on
    where each point lies relative to its neighbors on a manifold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 3-11](#separation_of_observations_using_isomap) shows the two-dimensional
    scatterplot using Isomap.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Separation of Observations Using Isomap](assets/hulp_0311.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-11\. Separation of observations using isomap
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Multidimensional Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Multidimensional scaling (MDS)* is a form of nonlinear dimensionality reduction
    that learns the similarity of points in the original dataset and, using this similarity
    learning, models this in a lower dimensional space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 3-12](#separation_of_observations_using_mds) displays the two-dimensional
    scatterplot using MDS.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Separation of Observations Using MDS](assets/hulp_0312.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-12\. Separation of observations using MDS
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Locally Linear Embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another popular nonlinear dimensionality reduction method is called *locally
    linear embedding (LLE)*. This method preserves distances within local neighborhoods
    as it projects the data from the original feature space to a reduced space. LLE
    discovers the nonlinear structure in the original, high-dimensional data by segmenting
    the data into smaller components (i.e., into neighborhoods of points) and modeling
    each component as a linear embedding.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this algorithm, we set the number of components we desire and the number
    of points to consider in a given neighborhood:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 3-13](#separation_of_observations_using_lle) shows the two-dimensional
    scatterplot using LLE.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Separation of Observations Using LLE](assets/hulp_0313.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-13\. Separation of observations using LLE
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: t-Distributed Stochastic Neighbor Embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*t-distributed stochastic neighbor embedding (t-SNE)* is a nonlinear dimensionality
    reduction technique for visualizing high-dimensional data. t-SNE accomplishes
    this by modeling each high-dimensional point into a two- or three-dimensional
    space, where similar points are modeled close to each other and dissimilar points
    are modeled farther away. It does this by constructing two probability distributions,
    one over pairs of points in the high-dimensional space and another over pairs
    of points in the low-dimensional space such that similar points have a high probability
    and dissimilar points have a lower probability. Specifically, t-SNE minimizes
    the *Kullback–Leibler divergence* between the two probability distributions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In real-world applications of t-SNE, it is best to use another dimensionality
    reduction technique (such as PCA, as we do here) to reduce the number of dimensions
    before applying t-SNE. By applying another form of dimensionality reduction first,
    we reduce the noise in the features that are fed into t-SNE and speed up the computation
    of the algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: t-SNE has a nonconvex cost function, which means that different initializations
    of the algorithm will generate different results. There is no stable solution.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-14](#separation_of_observations_using_t_sne) shows the two-dimensional
    scatterplot of t-SNE.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Separation of Observations Using t-SNE](assets/hulp_0314.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-14\. Separation of observations using t-SNE
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Other Dimensionality Reduction Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have covered both linear and nonlinear forms of dimensionality reduction.
    Now we will move to methods that do not rely on any sort of geometry or distance
    metric.
  prefs: []
  type: TYPE_NORMAL
- en: Dictionary Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One such method is *dictionary learning*, which learns the sparse representation
    of the original data. The resulting matrix is known as the *dictionary*, and the
    vectors in the dictionary are known as *atoms*. These atoms are simple, binary
    vectors, populated by zeros and ones. Each instance in the original data can be
    reconstructed as a weighted sum of these atoms.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming there are *d* features in the original data and *n* atoms in the dictionary,
    we can have a dictionary that is either *undercomplete*, where *n < d*, or *overcomplete*,
    where *n > d*. The undercomplete dictionary achieves dimensionality reduction,
    representing the original data with a fewer number of vectors, which is what we
    will focus on.^([3](ch03.html#idm140637550396848))
  prefs: []
  type: TYPE_NORMAL
- en: There is a mini-batch version of dictionary learning that we will apply to our
    dataset of digits. As with the other dimensionality reduction methods, we will
    set the number of components. We will also set the batch size and the number of
    iterations to perform the training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we want to visualize the images using a two-dimensional scatterplot,
    we will learn a very dense dictionary, but, in practice, we would use a much sparser
    version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 3-15](#separation_of_observations_using_dictionary_learning) shows
    the two-dimensional scatterplot using dictionary learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Separation of Observations Using Dictionary Learning](assets/hulp_0315.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-15\. Separation of observations using dictionary learning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Independent Component Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One common problem with unlabeled data is that there are many independent signals
    embedded together into the features we are given. Using *independent component
    analysis (ICA)*, we can separate these blended signals into their individual components.
    After the separation is complete, we can reconstruct any of the original features
    by adding together some combination of the individual components we generate.
    ICA is commonly used in signal processing tasks (for example, to identify the
    individual voices in an audio clip of a busy coffeehouse).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following shows how ICA works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 3-16](#separation_of_observations_using_independent_component_analysis)
    shows the two-dimensional scatterplot using ICA.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Separation of Observations Using Independent Component Analysis](assets/hulp_0316.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-16\. Separation of observations using independent component analysis
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced and explored a number of dimensionality reduction
    algorithms starting with linear methods such as PCA and random projection. Then,
    we switched to nonlinear methods—also known as manifold learning—such as Isomap,
    multidimensional scaling, LLE, and t-SNE. We also covered nondistance-based methods
    such as dictionary learning and ICA.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction captures the most salient information in a dataset
    in a small number of dimensions by learning the underlying structure of the data,
    and it does this without using any labels. By applying these algorithms to the
    MNIST digits dataset, we were able to meaningfully separate the images based on
    the digits they represented with just the top two dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: This highlights the power of dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 4](ch04.html#Chapter_4), we will build an applied unsupervised learning
    solution using these dimensionality reduction algorithms. Specifically, we will
    revist the fraud detection problem introduced in [Chapter 2](ch02.html#Chapter_2)
    and attempt to separate fraudulent transactions from normal ones without using
    labels.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch03.html#idm140637553285552-marker)) The [MNIST database of handwritten
    digits](http://yann.lecun.com/exdb/mnist/), courtesy of Yann Lecun.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch03.html#idm140637553283488-marker)) The [pickled version of the MNIST
    dataset](http://deeplearning.net/tutorial/gettingstarted.html), courtesy of deeplearning.net.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch03.html#idm140637550396848-marker)) The overcomplete dictionary serves
    a different purpose and has applications such as image compression.
  prefs: []
  type: TYPE_NORMAL
