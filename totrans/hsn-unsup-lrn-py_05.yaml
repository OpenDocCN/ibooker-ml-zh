- en: Chapter 3\. Dimensionality Reduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章。降维
- en: 'In this chapter, we will focus on one of the major challenges in building successful
    applied machine learning solutions: the curse of dimensionality. Unsupervised
    learning has a great counter—*dimensionality reduction*. In this chapter, we will
    introduce this concept and build from there so that you can develop an intuition
    for how it all works.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将关注构建成功应用机器学习解决方案的一个主要挑战：维度灾难。无监督学习有一个很好的对策——*降维*。在本章中，我们将介绍这个概念，并从那里开始，帮助你培养对其工作原理的直觉。
- en: In [Chapter 4](ch04.html#Chapter_4), we will build our own unsupervised learning
    solution based on dimensionality reduction—specifically, an unsupervised learning-based
    credit card fraud detection system (as opposed to the supervised-based system
    we built in [Chapter 2](ch02.html#Chapter_2)). This type of unsupervised fraud
    detection is known as anomaly detection, a rapidly growing area in the field of
    applied unsupervised learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第4章](ch04.html#Chapter_4)中，我们将基于降维构建我们自己的无监督学习解决方案——具体来说，是一个基于无监督学习的信用卡欺诈检测系统（与我们在[第2章](ch02.html#Chapter_2)中构建的基于有监督的系统不同）。这种无监督的欺诈检测被称为异常检测，是应用无监督学习领域一个迅速发展的领域。
- en: But before we build an anomaly detection system, let’s cover dimensionality
    reduction in this chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 但在构建异常检测系统之前，让我们在本章中介绍降维。
- en: The Motivation for Dimensionality Reduction
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维的动机
- en: As mentioned in [Chapter 1](ch01.html#Chapter_1), dimensionality reduction helps
    counteract one of the most commonly occurring problems in machine learning—the
    curse of dimensionality—in which algorithms cannot effectively and efficiently
    train on the data because of the sheer size of the feature space.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 正如[第1章](ch01.html#Chapter_1)中所提到的，降维有助于克服机器学习中最常见的问题之一——维度灾难，其中算法由于特征空间的巨大规模，无法有效和高效地在数据上训练。
- en: Dimensionality reduction algorithms project high-dimensional data to a low-dimensional
    space, retaining as much of the salient information as possible while removing
    redundant information. Once the data is in the low-dimensional space, machine
    learning algorithms are able to identify interesting patterns more effectively
    and efficiently because a lot of the noise has been reduced.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 降维算法将高维数据投影到低维空间，同时尽可能保留重要信息，去除冗余信息。一旦数据进入低维空间，机器学习算法能够更有效、更高效地识别有趣的模式，因为噪声已经被大大减少。
- en: Sometimes, dimensionality reduction is the goal itself—for example, to build
    anomaly detection systems, as we will show in the next chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，降维本身就是目标——例如，构建异常检测系统，我们将在下一章中展示。
- en: Other times, dimensionality reduction is not an end in itself but rather a means
    to another end. For example, dimensionality reduction is commonly a part of the
    machine learning pipeline to help solve large-scale, computationally expensive
    problems involving images, video, speech, and text.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 其他时候，降维不是一个终点，而是达到另一个终点的手段。例如，降维通常是机器学习管道的一部分，帮助解决涉及图像、视频、语音和文本的大规模、计算密集型问题。
- en: The MNIST Digits Database
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MNIST 手写数字数据库
- en: 'Before we introduce the dimensionality reduction algorithms, let’s explore
    the dataset that we will use in this chapter. We will work with a simple computer
    vision dataset: the MNIST (Mixed National Institute of Standards and Technology)
    database of handwritten digits, one of the best known datasets in machine learning.
    We will use the version of the MNIST dataset publicly available on Yann LeCun’s
    website.^([1](ch03.html#idm140637553285552)) To make it easier, we will use the
    pickled version, courtesy of [deeplearning.net](http://deeplearning.net).^([2](ch03.html#idm140637553283488))'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍降维算法之前，让我们先探索一下本章将使用的数据集。我们将使用一个简单的计算机视觉数据集：MNIST（美国国家标准与技术研究院）手写数字数据库，这是机器学习中最为人知的数据集之一。我们将使用Yann
    LeCun网站上公开的MNIST数据集版本。^([1](ch03.html#idm140637553285552)) 为了方便起见，我们将使用[deeplearning.net](http://deeplearning.net)提供的pickle版本。^([2](ch03.html#idm140637553283488))
- en: This dataset has been divided into three sets—a training set with 50,000 examples,
    a validation set with 10,000 examples, and a test set with 10,000 examples. We
    have labels for all the examples.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集已被分为三个部分——一个包含50,000个例子的训练集，一个包含10,000个例子的验证集和一个包含10,000个例子的测试集。我们为所有例子都有标签。
- en: This dataset consists of 28x28 pixel images of handwritten digits. Every single
    data point (i.e., every image) can be conveyed as an array of numbers, where each
    number describes how dark each pixel is. In other words, a 28x28 array of numbers
    corresponds to a 28x28 pixel image.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集由手写数字的 28x28 像素图像组成。每个数据点（即每个图像）可以表示为一组数字的数组，其中每个数字描述每个像素的暗度。换句话说，一个 28x28
    的数字数组对应于一个 28x28 像素的图像。
- en: To make this simpler, we can flatten each array into a 28x28, or 784, dimensional
    vector. Each component of the vector is a float between zero and one—representing
    the intensity of each pixel in the image. Zero stands for black; one stands for
    white. The labels are numbers between zero and nine, and indicate which digit
    the image represents.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，我们可以将每个数组展平为一个 28x28 或 784 维度的向量。向量的每个分量是介于零和一之间的浮点数——表示图像中每个像素的强度。零表示黑色，一表示白色。标签是介于零和九之间的数字，指示图像表示的数字。
- en: Data acquisition and exploration
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据获取和探索
- en: 'Before we work with the dimensionality reduction algorithms, let’s load the
    libraries we will use:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们使用降维算法之前，让我们加载将要使用的库：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Load the MNIST datasets
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载 MNIST 数据集
- en: 'Let’s now load the MNIST datasets:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们加载 MNIST 数据集：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Verify shape of datasets
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 验证数据集的形状
- en: 'Let’s verify the shape of the datasets to make sure they loaded properly:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们验证数据集的形状，以确保它们已正确加载：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following code confirms the shapes of the datasets are as expected:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码确认了数据集的形状与预期相符：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Create Pandas DataFrames from the datasets
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从数据集创建 Pandas DataFrames
- en: 'Let’s convert the numpy arrays into Pandas DataFrames so they are easier to
    explore and work with:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将 numpy 数组转换为 Pandas DataFrames，以便更容易进行探索和处理：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Explore the data
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索数据
- en: 'Let’s generate a summary view of the data:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们生成数据的摘要视图：
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[Table 3-1](#data_exploration) displays a summary view of the image data. Many
    of the values are zeros—in other words, most of the pixels in the images are black.
    This makes sense since the digits are in white and shown in the middle of the
    image on a black backdrop.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[表格 3-1](#data_exploration) 显示了图像数据的摘要视图。许多数值为零——换句话说，图像中的大多数像素是黑色的。这是有道理的，因为数字是白色的，显示在黑色背景的中央。'
- en: Table 3-1\. Data exploration
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 3-1\. 数据探索
- en: '|  | 0 | 1 | 2 | 3 | 4 | 5 | 6 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 | 2 | 3 | 4 | 5 | 6 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| count | 50000.0 | 50000.0 | 50000.0 | 50000.0 | 50000.0 | 50000.0 | 50000.0
    |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 计数 | 50000.0 | 50000.0 | 50000.0 | 50000.0 | 50000.0 | 50000.0 | 50000.0
    |'
- en: '| mean | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |'
- en: '| std | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 标准差 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |'
- en: '| min | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 最小值 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |'
- en: '| 25% | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 25% | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |'
- en: '| 50% | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 50% | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |'
- en: '| 75% | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 75% | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |'
- en: '| max | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 最大 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |'
- en: '| 8 rows x 784 columns |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 8 行 x 784 列 |'
- en: 'The labels data is a one-dimensional vector representing the actual content
    in the image. Labels for the first few images are as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 标签数据是一个表示图像中实际内容的一维向量。前几个图像的标签如下：
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Display the images
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 显示图像
- en: 'Let’s define a function to view the image along with its label:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个函数来查看图像及其标签：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: A view of the first image—once the 784-dimensional vector is reshaped into a
    28 x 28 pixel image—shows the number five ([Figure 3-1](#view_the_first_digit)).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个图像的视图——一旦将 784 维向量重塑为 28 x 28 像素图像——显示数字五（[图 3-1](#view_the_first_digit)）。
- en: '![View the first digit](assets/hulp_0301.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![查看第一个数字](assets/hulp_0301.png)'
- en: Figure 3-1\. View of the first digit
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-1\. 第一个数字的视图
- en: Dimensionality Reduction Algorithms
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维算法
- en: Now that we’ve loaded and explored the MNIST digits dataset, let’s move to the
    dimensionality reduction algorithms. For each algorithm, we will introduce the
    concept first and then build a deeper understanding by applying the algorithm
    to the MNIST digits dataset.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经加载并探索了 MNIST 数字数据集，让我们转向降维算法。对于每个算法，我们将首先介绍概念，然后通过将算法应用于 MNIST 数字数据集来深入理解。
- en: Linear Projection vs. Manifold Learning
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性投影 vs 流形学习
- en: There are two major branches of dimensionality reduction. The first is known
    as *linear projection*, which involves linearly projecting data from a high-dimensional
    space to a low-dimensional space. This includes techniques such as *principal
    component analysis, singular value decomposition*, and *random projection*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 降维有两大主要分支。第一种被称为*线性投影*，它涉及将数据从高维空间线性投影到低维空间。这包括*主成分分析，奇异值分解*和*随机投影*等技术。
- en: The second is known as *manifold learning*, which is also referred to as *nonlinear
    dimensionality reduction*. This involves techniques such as *isomap*, which learns
    the *curved distance* (also called the *geodesic distance*) between points rather
    than the *Euclidean distance*. Other techniques include *multidimensional scaling
    (MDS), locally linear embedding (LLE), t-distributed stochastic neighbor embedding
    (t-SNE), dictionary learning, random trees embedding*, and *independent component
    analysis*.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种被称为*流形学习*，也被称为*非线性降维*。这涉及技术，如*isomap*，它学习点之间的*曲线距离*（也称为*测地距离*），而不是*欧氏距离*。其他技术包括*多维缩放（MDS），局部线性嵌入（LLE），t分布随机近邻嵌入（t-SNE），字典学习，随机树嵌入*和*独立成分分析*。
- en: Principal Component Analysis
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分分析
- en: We will explore several versions of PCA, including standard PCA, incremental
    PCA, sparse PCA, and kernel PCA.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨几个PCA版本，包括标准PCA，增量PCA，稀疏PCA和核PCA。
- en: PCA, the Concept
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PCA，概念
- en: Let’s start with standard PCA, one of the most common linear dimensionality
    reduction techniques. In PCA, the algorithm finds a low-dimensional representation
    of the data while retaining as much of the variation (i.e., salient information)
    as possible.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从标准PCA开始，这是最常见的线性降维技术之一。在PCA中，算法找到数据的低维表示，同时尽可能保留尽可能多的变化（即显著信息）。
- en: PCA does this by addressing the correlation among features. If the correlation
    is very high among a subset of the features, PCA will attempt to combine the highly
    correlated features and represent this data with a smaller number of linearly
    uncorrelated features. The algorithm keeps performing this correlation reduction,
    finding the directions of maximum variance in the original high-dimensional data
    and projecting them onto a smaller dimensional space. These newly derived components
    are known as principal components.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: PCA通过处理特征之间的相关性来实现这一点。如果一组特征之间的相关性非常高，PCA将尝试合并高度相关的特征，并用较少数量的线性不相关特征表示这些数据。该算法持续执行这种相关性减少，找到原始高维数据中方差最大的方向，并将它们投影到较小维度的空间中。这些新导出的成分称为主成分。
- en: With these components, it is possible to reconstruct the original features—not
    exactly but generally close enough. The PCA algorithm actively attempts to minimize
    the reconstruction error during its search for the optimal components.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些成分，可以重构原始特征，虽然不完全准确但一般足够接近。PCA算法在寻找最优成分期间积极尝试最小化重构误差。
- en: In our MNIST example, the original feature space has 784 dimensions, known as
    *d* dimensions. PCA will project the data onto a smaller subspace of *k* dimensions
    (where *k < d*) while retaining as much of the salient information as possible.
    These *k* dimensions are known as the principal components.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的MNIST示例中，原始特征空间有784维，称为*d*维。PCA将数据投影到较小的*k*维子空间（其中*k < d*），同时尽可能保留关键信息。这*k*个维度称为主成分。
- en: The number of meaningful principal components we are left with is considerably
    smaller than the number of dimensions in the original dataset. We lose some of
    the variance (i.e., information) by moving to this low-dimensional space, but
    the underlying structure of the data is easier to identify, allowing us to perform
    tasks like anomaly detection and clustering more effectively and efficiently.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们留下的有意义的主成分数量远远小于原始数据集中的维数。通过转移到这个低维空间，我们会失去一些方差（即信息），但数据的基本结构更容易识别，使我们能够更有效地执行异常检测和聚类等任务。
- en: Moreover, by reducing the dimensionality of the data, PCA will reduce the size
    of the data, improving the performance of machine learning algorithms further
    along in the machine learning pipeline (for example, for tasks such as image classification).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过减少数据的维数，PCA将减少数据的大小，进一步提高机器学习管道中后续阶段（例如图像分类等任务）的性能。
- en: Note
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: It is essential to perform feature scaling before running PCA. PCA is very sensitive
    to the relative ranges of the original features. Generally we must scale the data
    to make sure the features are in the same relative range. However, for our MNIST
    digits dataset, the features are already scaled to a range of zero to one, so
    we can skip this step.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行 PCA 之前执行特征缩放非常重要。PCA 对原始特征的相对范围非常敏感。通常，我们必须缩放数据以确保特征处于相同的相对范围。然而，对于我们的 MNIST
    数字数据集，特征已经缩放到 0 到 1 的范围，因此我们可以跳过这一步。
- en: PCA in Practice
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PCA 实践
- en: Now that you have a better grasp of how PCA works, let’s apply PCA to the MNIST
    digits dataset and see how well PCA captures the most salient information about
    the digits as its projects the data from the original 784-dimensional space to
    a lower dimensional space.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您对 PCA 工作原理有了更好的掌握，让我们将 PCA 应用于 MNIST 数字数据集，并看看 PCA 如何在将数据从原始的 784 维空间投影到较低维空间时捕获数字的最显著信息。
- en: Set the hyperparameters
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置超参数
- en: 'Let’s set the hyperparameters for the PCA algorithm:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为 PCA 算法设置超参数：
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Apply PCA
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用 PCA
- en: We will set the number of principal components to the original number of dimensions
    (i.e., 784). Then, PCA will capture the salient information from the original
    dimensions and start generating principal components. Once these components are
    generated, we will determine how many principal components we need to effectively
    capture most of the variance/information from the original feature set.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将主成分的数量设置为原始维数（即 784）。然后，PCA 将从原始维度捕获显著信息并开始生成主成分。生成这些组件后，我们将确定需要多少个主成分才能有效地捕获原始特征集中大部分的方差/信息。
- en: 'Let’s fit and transform our training data, generating these principal components:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们拟合并转换我们的训练数据，生成这些主成分：
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Evaluate PCA
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估 PCA
- en: 'Because we have not reduced the dimensionality at all (we’ve just transformed
    the data) the variance/information of the original data captured by the 784 principal
    components should be 100%:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们完全没有降低维度（只是转换了数据），所以由 784 个主成分捕获的原始数据的方差/信息应为 100%：
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'However, it is important to note that the importance of the 784 principal components
    varies quite a bit. The importance of the first X principal components are summarized
    here:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，需要注意的是 784 个主成分的重要性差异相当大。这里总结了前 X 个主成分的重要性：
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The first 10 components in total capture approximately 50% of the variance,
    the first one hundred components over 90%, and the first three hundred components
    almost 99% of the variance; the information in the rest of the principal components
    is of negligible value.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 前 10 个组件总共捕获了大约 50% 的方差，前一百个组件超过了 90%，前三百个组件几乎捕获了 99% 的方差；其余主成分中的信息几乎可以忽略不计。
- en: We can also plot the importance of each principal component, ranked from the
    first principal component to the last. For the sake of readability, just the first
    10 components are displayed in [Figure 3-2](#importance_of_pca_components).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以绘制每个主成分的重要性，从第一个主成分到最后一个主成分进行排名。为了便于阅读，只显示了前 10 个组件在 [图 3-2](#importance_of_pca_components)
    中。
- en: The power of PCA should be more apparent now. With just the first two hundred
    principal components (far fewer than the original 784 dimensions), we capture
    over 96% of the variance/information.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 PCA 的力量应该更加明显了。仅使用前两百个主成分（远少于原始的 784 维度），我们就捕获了超过 96% 的方差/信息。
- en: PCA allows us to reduce the dimensionality of the original data substantially
    while retaining most of the salient information. On the PCA-reduced feature set,
    other machine learning algorithms—downstream in the machine learning pipeline—will
    have an easier time separating the data points in space (to perform tasks such
    as anomaly detection and clustering) and will require fewer computational resources.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 允许我们大幅减少原始数据的维度，同时保留大部分显著信息。在 PCA 减少的特征集上，其他机器学习算法——在机器学习流水线中的下游——将更容易在空间中分离数据点（执行异常检测和聚类等任务），并且需要更少的计算资源。
- en: '![Importance of PCA components](assets/hulp_0302.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![PCA 组件的重要性](assets/hulp_0302.png)'
- en: Figure 3-2\. Importance of PCA components
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-2\. PCA 组件的重要性
- en: Visualize the separation of points in space
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化空间中点的分离
- en: 'To demonstrate the power of PCA to efficiently and compactly capture the variance/information
    in data, let’s plot the observations in two dimensions. Specifically, we will
    display a scatterplot of the first and second principal components and mark the
    observations by the true label. Let’s create a function for this called `scatterPlot`
    because we also need to present visualizations for the other dimensionality algorithms
    later on:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示PCA高效、简洁地捕捉数据中的方差/信息的能力，让我们在二维空间中绘制这些观察结果。具体来说，我们将展示第一和第二主成分的散点图，并用真实标签标记这些观察结果。我们将称这个函数为`scatterPlot`，因为接下来我们还需要为其他维度算法呈现可视化效果。
- en: '[PRE15]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As seen in [Figure 3-3](#separation_of_observations_using_pca), with just the
    top two principal components, PCA does a good job of separating the points in
    space such that similar points are generally closer to each other than they are
    to other, less similar points. In other words, images of the same digit are closer
    to each other than they are to images of other digits.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 3-3](#separation_of_observations_using_pca)所示，仅使用前两个主成分，PCA能够有效地将空间中的点分离开来，使相似的点通常比其他不相似的点更靠近。换句话说，相同数字的图像彼此之间比与其他数字的图像更接近。
- en: PCA accomplishes this without using any labels whatsoever. This demonstrates
    the power of unsupervised learning to capture the underlying structure of data,
    helping discover hidden patterns in the absence of labels.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: PCA可以在完全不使用标签的情况下完成这一任务。这展示了无监督学习捕捉数据潜在结构的能力，帮助在没有标签的情况下发现隐藏的模式。
- en: '![Separation of Observations Using PCA](assets/hulp_0303.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![使用PCA分离观察结果](assets/hulp_0303.png)'
- en: Figure 3-3\. Separation of observations using PCA
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-3\. 使用PCA分离观察结果
- en: If we run the same two-dimensional scatterplot using two of the most important
    features from the original 784 feature set—determined by training a supervised
    learning model—the separation is poor, at best ([Figure 3-4](#separation_of_observations_without_pca)).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用原始的 784 个特征集中最重要的两个特征（通过训练监督学习模型确定），运行相同的二维散点图，最多也只能得到很差的分离效果（见[图 3-4](#separation_of_observations_without_pca)）。
- en: '![Separation of Observations Without PCA](assets/hulp_0304.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![不使用PCA分离观察结果](assets/hulp_0304.png)'
- en: Figure 3-4\. Separation of observations without PCA
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-4\. 不使用PCA分离观察结果
- en: Comparison of Figures [3-3](#separation_of_observations_using_pca) and [3-4](#separation_of_observations_without_pca)
    shows just how powerful PCA is in learning the underlying structure of the dataset
    without using any labels whatsoever—even with just two dimensions, we can start
    meaningfully separating the images by the digits they display.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 比较[图 3-3](#separation_of_observations_using_pca)和[图 3-4](#separation_of_observations_without_pca)，可以看出PCA在学习数据集潜在结构方面的强大能力，完全不使用任何标签——即使仅使用两个维度，我们也可以开始有意义地通过显示的数字分离图像。
- en: Note
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Not only does PCA help separate data so that we can discover hidden patterns
    more readily, it also helps reduce the size of the feature set, making it less
    costly—both in time and in computational resources—to train machine learning models.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: PCA不仅帮助分离数据以便更容易发现隐藏模式，还有助于减少特征集的大小，从而在训练机器学习模型时节省时间和计算资源。
- en: With the MNIST dataset, the reduction in training time will be modest at best
    since the dataset is very small—we have only 784 features and 50,000 observations.
    But if the dataset were millions of features and billions of observations, dimensionality
    reduction would dramatically reduce the training time of the machine learning
    algorithms further along in the machine learning pipeline.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对于MNIST数据集来说，由于数据集非常小——仅有 784 个特征和 50,000 个观察结果，因此减少训练时间的效果可能很有限。但如果数据集拥有数百万个特征和数十亿个观察结果，降维将大大减少后续机器学习管道中机器学习算法的训练时间。
- en: Lastly, PCA usually throws away some of the information available in the original
    feature set but does so wisely, capturing the most important elements and tossing
    the less valuable ones. A model that is trained on a PCA-reduced feature set may
    not perform quite as well in terms of accuracy as a model that is trained on the
    full feature set, but both the training and prediction times will be much faster.
    This is one of the important trade-offs you must consider when choosing whether
    to use dimensionality reduction in your machine learning product.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，PCA通常会丢弃原始特征集中的一些信息，但它会明智地捕获最重要的元素并丢弃不太有价值的元素。基于PCA减少特征集训练的模型在准确性上可能不如基于完整特征集训练的模型表现得好，但训练和预测时间会快得多。这是在选择是否在机器学习产品中使用降维时必须考虑的重要权衡之一。
- en: Incremental PCA
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增量PCA
- en: 'For datasets that are very large and cannot fit in memory, we can perform PCA
    incrementally in small batches, where each batch is able to fit in memory. The
    batch size can be either set manually or determined automatically. This batch-based
    form of PCA is known as *incremental PCA*. The resulting principal components
    of PCA and incremental PCA are generally pretty similar ([Figure 3-5](#separation_of_observations_using_incremental_pca)).
    Here is the code for incremental PCA:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对于无法全部存入内存的大型数据集，我们可以逐批次增量地执行PCA，其中每个批次都能放入内存中。批处理大小可以手动设置或自动确定。这种基于批处理的PCA形式称为*增量PCA*。PCA和增量PCA的生成主成分通常非常相似（[图 3-5](#separation_of_observations_using_incremental_pca)）。以下是增量PCA的代码：
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![Separation of Observations Using Incremental PCA](assets/hulp_0305.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![使用增量PCA分离观测结果](assets/hulp_0305.png)'
- en: Figure 3-5\. Separation of observations using incremental PCA
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-5\. 使用增量PCA分离观测结果
- en: Sparse PCA
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 稀疏PCA
- en: The normal PCA algorithm searches for linear combinations in all the input variables,
    reducing the original feature space as densely as possible. But for some machine
    learning problems, some degree of sparsity may be preferred. A version of PCA
    that retains some degree of sparsity—controlled by a hyperparameter called *alpha*—is
    known as *sparse PCA*. The sparse PCA algorithm searches for linear combinations
    in just some of the input variables, reducing the original feature space to some
    degree but not as compactly as normal PCA.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 普通PCA算法在所有输入变量中搜索线性组合，尽可能紧凑地减少原始特征空间。但对于某些机器学习问题，可能更倾向于一定程度的稀疏性。保留一定程度稀疏性的PCA版本，由名为*alpha*的超参数控制，称为*稀疏PCA*。稀疏PCA算法仅在部分输入变量中搜索线性组合，将原始特征空间减少到一定程度，但不像普通PCA那样紧凑。
- en: Because this algorithm trains a bit more slowly than normal PCA, we will train
    on just the first 10,000 examples in our training set (out of the total 50,000
    examples). We will continue this practice of training on fewer than the total
    number of observations when the algorithm training times are slow.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这种算法的训练速度比普通PCA稍慢，所以我们将仅在训练集中的前10,000个示例上进行训练（总共有50,000个示例）。当算法的训练时间较慢时，我们会继续采用在少于总观测数的情况下进行训练的做法。
- en: 'For our purposes (i.e., developing some intuition of how these dimensionality
    reduction algorithms work), the reduced training process is fine. For a better
    solution, training on the complete training set is advised:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的（即开发这些降维算法如何工作的直觉），减少训练过程是可以接受的。为了获得更好的解决方案，建议在完整的训练集上进行训练：
- en: '[PRE17]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[Figure 3-6](#separation_of_observations_using_sparse_pca) shows a two-dimensional
    scatterplot using the first two principal components using sparse PCA.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-6](#separation_of_observations_using_sparse_pca)展示了使用稀疏PCA的前两个主成分的二维散点图。'
- en: '![Separation of Observations Using Sparse PCA](assets/hulp_0306.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![使用稀疏PCA分离观测结果](assets/hulp_0306.png)'
- en: Figure 3-6\. Separation of observations using sparse PCA
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-6\. 使用稀疏PCA分离观测结果
- en: Notice that this scatterplot looks different from that of the normal PCA, as
    expected. Normal and sparse PCA generate principal components differently, and
    the separation of points is somewhat different, too.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个散点图看起来与普通PCA的不同，正如预期的那样。普通和稀疏PCA生成主成分的方式不同，点的分离也有所不同。
- en: Kernel PCA
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 核PCA
- en: Normal PCA, incremental PCA, and sparse PCA linearly project the original data
    onto a lower dimensional space, but there is also a nonlinear form of PCA known
    as *kernel PCA*, which runs a similarity function over pairs of original data
    points in order to perform nonlinear dimensionality reduction.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 正常PCA、增量PCA和稀疏PCA将原始数据线性投影到较低维度空间，但也有一种非线性形式的PCA称为*核PCA*，它在原始数据点对上运行相似度函数以执行非线性降维。
- en: By learning this similarity function (known as the *kernel method*), kernel
    PCA maps the implicit feature space where the majority of data points lie and
    creates this implicit feature space in a much smaller number of dimensions than
    the dimensions in the original feature set. This method is especially effective
    when the original feature set is not linearly separable.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 通过学习这个相似度函数（称为*核方法*），核PCA映射了大部分数据点所在的隐式特征空间，并在比原始特征集中的维度小得多的空间中创建了这个隐式特征空间。当原始特征集不是线性可分时，这种方法特别有效。
- en: 'For the kernel PCA algorithm, we need to set the number of components we desire,
    the type of kernel, and the kernel coefficient, which is known as the *gamma*.
    The most popular kernel is the *radial basis function kernel*, more commonly referred
    to as the *RBF kernel*. This is what we will use here:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 对于核PCA算法，我们需要设置所需的组件数、核类型和核系数，称为*gamma*。最流行的核是*径向基函数核*，更常被称为*RBF核*。这是我们将在这里使用的核心：
- en: '[PRE18]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The two-dimensional scatterplot of the kernel PCA is nearly identical to the
    one of the linear PCA for our MNIST digits dataset ([Figure 3-7](#separation_of_observations_using_kernel_pca)).
    Learning the RBF kernel does not improve the dimensionality reduction.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 核PCA的二维散点图与我们的MNIST数字数据集的线性PCA几乎相同（[图 3-7](#separation_of_observations_using_kernel_pca)）。学习RBF核并不改善降维。
- en: '![Separation of Observations Using Kernel PCA](assets/hulp_0307.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![使用核PCA进行观测分离](assets/hulp_0307.png)'
- en: Figure 3-7\. Separation of observations using kernel PCA
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-7\. 使用核PCA进行观测分离
- en: Singular Value Decomposition
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奇异值分解
- en: Another approach to learning the underlying structure of the data is to reduce
    the rank of the original matrix of features to a smaller rank such that the original
    matrix can be recreated using a linear combination of some of the vectors in the
    smaller rank matrix. This is known as *singular value decomposition (SVD)*.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 学习数据的潜在结构的另一种方法是将特征的原始矩阵的秩降低到一个较小的秩，以便可以使用较小秩矩阵中某些向量的线性组合重新创建原始矩阵。这被称为*奇异值分解（SVD）*。
- en: To generate the smaller rank matrix, SVD keeps the vectors of the original matrix
    that have the most information (i.e., the highest singular value). The smaller
    rank matrix captures the most important elements of the original feature space.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成较小秩矩阵，SVD保留原始矩阵中具有最多信息的向量（即最高的奇异值）。较小秩矩阵捕获了原始特征空间的最重要元素。
- en: This is very similar to PCA. PCA, which uses the eigen-decomposition of the
    covariance matrix to perform dimensionality reduction. SVD uses singular value
    decomposition, as its name implies. In fact, PCA involves the use of SVD in its
    calculation, but much of this discussion is beyond the scope of this book.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这与PCA非常相似。PCA使用协方差矩阵的特征值分解来进行降维。奇异值分解（SVD）使用奇异值分解，正如其名称所示。事实上，PCA在其计算中使用了SVD，但本书的大部分讨论超出了此范围。
- en: 'Here is how SVD works:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这是SVD的工作原理：
- en: '[PRE19]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[Figure 3-8](#separation_of_observations_using_svd) displays the separation
    of points that we achieve using the two most important vectors from SVD.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-8](#separation_of_observations_using_svd) 显示了我们使用SVD的两个最重要向量实现的点的分离。'
- en: '![Separation of Observations Using SVD](assets/hulp_0308.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![使用SVD进行观测分离](assets/hulp_0308.png)'
- en: Figure 3-8\. Separation of observations using SVD
  id: totrans-136
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-8\. 使用SVD进行观测分离
- en: Random Projection
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机投影
- en: Another linear dimensionality reduction technique is random projection, which
    relies on the *Johnson–Lindenstrauss lemma*. According to the Johnson–Lindenstrauss
    lemma, points in a high-dimensional space can be embedded into a much lower-dimensional
    space so that distances between the points are nearly preserved. In other words,
    even as we move from high-dimensional space to low-dimensional space, the relevant
    structure of the original feature set is preserved.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种线性降维技术是随机投影，它依赖于*Johnson–Lindenstrauss引理*。根据Johnson–Lindenstrauss引理，高维空间中的点可以嵌入到一个远低于其维度的空间中，以便点之间的距离几乎保持不变。换句话说，即使从高维空间移动到低维空间，原始特征集的相关结构也得到保留。
- en: Gaussian Random Projection
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高斯随机投影
- en: There are two versions of random projection—the standard version known as *Gaussian
    random projection* and a sparse version known as *sparse random projection*.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 随机投影有两个版本——标准版本称为*高斯随机投影*，稀疏版本称为*稀疏随机投影*。
- en: 'For Gaussian random projection, we can either specify the number of components
    we would like to have in the reduced feature space, or we can set the hyperparameter
    *eps*. The eps controls the quality of the embedding according to the Johnson–Lindenstrauss
    lemma, where smaller values generate a higher number of dimensions. In our case,
    we will set this hyperparameter:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 对于高斯随机投影，我们可以指定我们希望在降维特征空间中拥有的组件数量，或者我们可以设置超参数*eps*。eps控制嵌入的质量，根据Johnson–Lindenstrauss引理，较小的值会生成更多的维度。在我们的情况下，我们将设置这个超参数：
- en: '[PRE20]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[Figure 3-9](#separation_of_observations_using_gaussian_random_projection)
    shows the two-dimensional scatterplot using Gaussian random projection.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-9](#separation_of_observations_using_gaussian_random_projection) 显示了使用高斯随机投影的二维散点图。'
- en: '![Separation of Observations Using Gaussian Random Projection](assets/hulp_0309.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![使用高斯随机投影分离观测](assets/hulp_0309.png)'
- en: Figure 3-9\. Separation of observations using Gaussian random projection
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-9\. 使用高斯随机投影分离观测
- en: Although it is a form of linear projection like PCA, random projection is an
    entirely different family of dimensionality reduction. Thus the random projection
    scatterplot looks very different from the scatterplots of normal PCA, incremental
    PCA, sparse PCA, and kernel PCA.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管随机投影与PCA一样都是一种线性投影形式，但随机投影是一种完全不同的降维方法家族。因此，随机投影的散点图看起来与普通PCA、增量PCA、稀疏PCA和核PCA的散点图非常不同。
- en: Sparse Random Projection
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 稀疏随机投影
- en: 'Just as there is a sparse version of PCA, there is a sparse version of random
    projection known as sparse random projection. It retains some degree of sparsity
    in the transformed feature set and is generally much more efficient, transforming
    the original data into the reduced space much faster than normal Gaussian random
    projection:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 正如PCA有稀疏版本一样，随机投影也有稀疏版本，称为稀疏随机投影。它在转换后的特征集中保留了一定程度的稀疏性，并且通常比普通的高斯随机投影更高效，能够更快地将原始数据转换为降维空间：
- en: '[PRE21]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[Figure 3-10](#separation_of_observations_using_sparse_random_projection) shows
    the two-dimensional scatterplot using sparse random projection.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-10](#separation_of_observations_using_sparse_random_projection) 显示了使用稀疏随机投影的二维散点图。'
- en: '![Separation of Observations Using Sparse Random Projection](assets/hulp_0310.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![使用稀疏随机投影分离观测](assets/hulp_0310.png)'
- en: Figure 3-10\. Separation of observations using sparse random projection
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-10\. 使用稀疏随机投影分离观测
- en: Isomap
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Isomap
- en: Instead of linearly projecting the data from a high-dimensional space to a low-dimensional
    space, we can use nonlinear dimensionality reduction methods. These methods are
    collectively known as manifold learning.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 与其线性投影高维空间到低维空间的数据，我们可以使用非线性降维方法。这些方法统称为流形学习。
- en: 'The most vanilla form of manifold learning is known as *isometric mapping*,
    or *Isomap* for short. Like kernel PCA, Isomap learns a new, low-dimensional embedding
    of the original feature set by calculating the pairwise distances of all the points,
    where distance is *curved* or *geodesic distance* rather than *Euclidean distance*.
    In other words, it learns the intrinsic geometry of the original data based on
    where each point lies relative to its neighbors on a manifold:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 流形学习最基本的形式被称为*等距映射*，简称*Isomap*。与核PCA类似，Isomap通过计算所有点的成对距离来学习原始特征集的新的低维嵌入，其中距离是*曲线*或*测地距离*，而不是*欧氏距离*。换句话说，它基于每个点相对于流形上邻近点的位置学习原始数据的内在几何结构：
- en: '[PRE22]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[Figure 3-11](#separation_of_observations_using_isomap) shows the two-dimensional
    scatterplot using Isomap.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-11](#separation_of_observations_using_isomap) 显示了使用Isomap的二维散点图。'
- en: '![Separation of Observations Using Isomap](assets/hulp_0311.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![使用Isomap分离观察结果](assets/hulp_0311.png)'
- en: Figure 3-11\. Separation of observations using isomap
  id: totrans-159
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-11\. 使用isomap分离观察结果
- en: Multidimensional Scaling
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多维缩放
- en: '*Multidimensional scaling (MDS)* is a form of nonlinear dimensionality reduction
    that learns the similarity of points in the original dataset and, using this similarity
    learning, models this in a lower dimensional space:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '*多维缩放（MDS）*是一种非线性降维形式，它学习原始数据集中点的相似性，并利用这种相似性在较低维度空间中进行建模：'
- en: '[PRE23]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[Figure 3-12](#separation_of_observations_using_mds) displays the two-dimensional
    scatterplot using MDS.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-12](#separation_of_observations_using_mds) 显示了使用MDS的二维散点图。'
- en: '![Separation of Observations Using MDS](assets/hulp_0312.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![使用MDS分离观察结果](assets/hulp_0312.png)'
- en: Figure 3-12\. Separation of observations using MDS
  id: totrans-165
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-12\. 使用MDS分离观察结果
- en: Locally Linear Embedding
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 局部线性嵌入
- en: Another popular nonlinear dimensionality reduction method is called *locally
    linear embedding (LLE)*. This method preserves distances within local neighborhoods
    as it projects the data from the original feature space to a reduced space. LLE
    discovers the nonlinear structure in the original, high-dimensional data by segmenting
    the data into smaller components (i.e., into neighborhoods of points) and modeling
    each component as a linear embedding.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行的非线性降维方法称为*局部线性嵌入（LLE）*。该方法在将数据从原始特征空间投影到降维空间时保持了局部邻域内的距离。LLE通过将数据分段成较小的组件（即点的邻域）并将每个组件建模为线性嵌入，发现了原始高维数据中的非线性结构。
- en: 'For this algorithm, we set the number of components we desire and the number
    of points to consider in a given neighborhood:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 对于该算法，我们设置我们期望的组件数量和在给定邻域中考虑的点数：
- en: '[PRE24]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[Figure 3-13](#separation_of_observations_using_lle) shows the two-dimensional
    scatterplot using LLE.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-13](#separation_of_observations_using_lle) 显示了使用LLE的二维散点图。'
- en: '![Separation of Observations Using LLE](assets/hulp_0313.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![使用LLE分离观察结果](assets/hulp_0313.png)'
- en: Figure 3-13\. Separation of observations using LLE
  id: totrans-172
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-13\. 使用LLE分离观察结果
- en: t-Distributed Stochastic Neighbor Embedding
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: t-分布随机邻域嵌入
- en: '*t-distributed stochastic neighbor embedding (t-SNE)* is a nonlinear dimensionality
    reduction technique for visualizing high-dimensional data. t-SNE accomplishes
    this by modeling each high-dimensional point into a two- or three-dimensional
    space, where similar points are modeled close to each other and dissimilar points
    are modeled farther away. It does this by constructing two probability distributions,
    one over pairs of points in the high-dimensional space and another over pairs
    of points in the low-dimensional space such that similar points have a high probability
    and dissimilar points have a lower probability. Specifically, t-SNE minimizes
    the *Kullback–Leibler divergence* between the two probability distributions.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '*t-分布随机邻域嵌入（t-SNE）*是一种非线性降维技术，用于可视化高维数据。t-SNE通过将每个高维点建模到二维或三维空间中来实现这一目标，使得相似的点模型接近，而不相似的点则模型远离。它通过构建两个概率分布实现此目标，一个是在高维空间中点对的概率分布，另一个是在低维空间中点对的概率分布，使得相似的点具有较高的概率，而不相似的点具有较低的概率。具体来说，t-SNE最小化了两个概率分布之间的*Kullback–Leibler散度*。'
- en: 'In real-world applications of t-SNE, it is best to use another dimensionality
    reduction technique (such as PCA, as we do here) to reduce the number of dimensions
    before applying t-SNE. By applying another form of dimensionality reduction first,
    we reduce the noise in the features that are fed into t-SNE and speed up the computation
    of the algorithm:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在t-SNE的实际应用中，最好在应用t-SNE之前使用另一种降维技术（例如PCA，正如我们在此处所做的那样）来减少维数。通过先应用另一种降维方法，我们可以减少馈入t-SNE的特征中的噪音，并加快算法的计算速度：
- en: '[PRE25]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note
  id: totrans-177
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: t-SNE has a nonconvex cost function, which means that different initializations
    of the algorithm will generate different results. There is no stable solution.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE具有非凸成本函数，这意味着算法的不同初始化会生成不同的结果。不存在稳定的解决方案。
- en: '[Figure 3-14](#separation_of_observations_using_t_sne) shows the two-dimensional
    scatterplot of t-SNE.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-14](#separation_of_observations_using_t_sne) 显示了t-SNE的二维散点图。'
- en: '![Separation of Observations Using t-SNE](assets/hulp_0314.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![使用t-SNE分离观察结果](assets/hulp_0314.png)'
- en: Figure 3-14\. Separation of observations using t-SNE
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-14\. 使用 t-SNE 进行观察分离
- en: Other Dimensionality Reduction Methods
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他降维方法
- en: We have covered both linear and nonlinear forms of dimensionality reduction.
    Now we will move to methods that do not rely on any sort of geometry or distance
    metric.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了线性和非线性形式的降维。现在我们将转向不依赖任何几何或距离度量的方法。
- en: Dictionary Learning
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 字典学习
- en: One such method is *dictionary learning*, which learns the sparse representation
    of the original data. The resulting matrix is known as the *dictionary*, and the
    vectors in the dictionary are known as *atoms*. These atoms are simple, binary
    vectors, populated by zeros and ones. Each instance in the original data can be
    reconstructed as a weighted sum of these atoms.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一种方法是*字典学习*，它学习原始数据的稀疏表示。生成的矩阵称为*字典*，字典中的向量称为*原子*。这些原子是简单的二进制向量，由零和一填充。原始数据中的每个实例可以被重构为这些原子的加权和。
- en: Assuming there are *d* features in the original data and *n* atoms in the dictionary,
    we can have a dictionary that is either *undercomplete*, where *n < d*, or *overcomplete*,
    where *n > d*. The undercomplete dictionary achieves dimensionality reduction,
    representing the original data with a fewer number of vectors, which is what we
    will focus on.^([3](ch03.html#idm140637550396848))
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 假设原始数据中有*d*个特征和*n*个字典原子，我们可以有一个*欠完备*字典，其中*n < d*，或*过完备*字典，其中*n > d*。欠完备字典实现了降维，用较少的向量表示原始数据，这是我们将专注的内容。^([3](ch03.html#idm140637550396848))
- en: There is a mini-batch version of dictionary learning that we will apply to our
    dataset of digits. As with the other dimensionality reduction methods, we will
    set the number of components. We will also set the batch size and the number of
    iterations to perform the training.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在我们的数字数据集上应用字典学习的小批量版本。与其他降维方法一样，我们将设置成分的数量。我们还将设置批量大小和执行训练的迭代次数。
- en: 'Since we want to visualize the images using a two-dimensional scatterplot,
    we will learn a very dense dictionary, but, in practice, we would use a much sparser
    version:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们想要使用二维散点图来可视化图像，我们将学习一个非常密集的字典，但实际上，我们会使用一个更稀疏的版本：
- en: '[PRE26]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[Figure 3-15](#separation_of_observations_using_dictionary_learning) shows
    the two-dimensional scatterplot using dictionary learning.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-15](#separation_of_observations_using_dictionary_learning) 展示了使用字典学习的二维散点图。'
- en: '![Separation of Observations Using Dictionary Learning](assets/hulp_0315.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![使用字典学习进行观察分离](assets/hulp_0315.png)'
- en: Figure 3-15\. Separation of observations using dictionary learning
  id: totrans-192
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-15\. 使用字典学习进行观察分离
- en: Independent Component Analysis
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 独立成分分析
- en: One common problem with unlabeled data is that there are many independent signals
    embedded together into the features we are given. Using *independent component
    analysis (ICA)*, we can separate these blended signals into their individual components.
    After the separation is complete, we can reconstruct any of the original features
    by adding together some combination of the individual components we generate.
    ICA is commonly used in signal processing tasks (for example, to identify the
    individual voices in an audio clip of a busy coffeehouse).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 无标签数据的一个常见问题是，有许多独立信号嵌入到我们给定的特征中。使用*独立成分分析（ICA）*，我们可以将这些混合信号分离成它们的各个组成部分。分离完成后，我们可以通过组合生成的各个个体成分的某些组合来重建任何原始特征。ICA在信号处理任务中广泛应用（例如，在繁忙咖啡馆音频剪辑中识别各个声音）。
- en: 'The following shows how ICA works:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 以下展示了ICA的工作原理：
- en: '[PRE27]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[Figure 3-16](#separation_of_observations_using_independent_component_analysis)
    shows the two-dimensional scatterplot using ICA.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-16](#separation_of_observations_using_independent_component_analysis)
    展示了使用ICA的二维散点图。'
- en: '![Separation of Observations Using Independent Component Analysis](assets/hulp_0316.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![使用独立成分分析进行观察分离](assets/hulp_0316.png)'
- en: Figure 3-16\. Separation of observations using independent component analysis
  id: totrans-199
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-16\. 使用独立成分分析进行观察分离
- en: Conclusion
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, we introduced and explored a number of dimensionality reduction
    algorithms starting with linear methods such as PCA and random projection. Then,
    we switched to nonlinear methods—also known as manifold learning—such as Isomap,
    multidimensional scaling, LLE, and t-SNE. We also covered nondistance-based methods
    such as dictionary learning and ICA.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍并探讨了多种降维算法，从线性方法如PCA和随机投影开始。然后，我们转向非线性方法——也称为流形学习，例如Isomap、多维尺度分析、LLE和t-SNE。我们还涵盖了非基于距离的方法，如字典学习和ICA。
- en: Dimensionality reduction captures the most salient information in a dataset
    in a small number of dimensions by learning the underlying structure of the data,
    and it does this without using any labels. By applying these algorithms to the
    MNIST digits dataset, we were able to meaningfully separate the images based on
    the digits they represented with just the top two dimensions.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 降维捕捉数据集中最显著的信息，并通过学习数据的潜在结构将其压缩到少量维度，而无需使用任何标签。通过将这些算法应用于MNIST数字数据集，我们能够仅使用前两个维度基于其所代表的数字有效地分离图像。
- en: This highlights the power of dimensionality reduction.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这突显了降维的强大能力。
- en: In [Chapter 4](ch04.html#Chapter_4), we will build an applied unsupervised learning
    solution using these dimensionality reduction algorithms. Specifically, we will
    revist the fraud detection problem introduced in [Chapter 2](ch02.html#Chapter_2)
    and attempt to separate fraudulent transactions from normal ones without using
    labels.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第四章](ch04.html#Chapter_4)，我们将使用这些降维算法构建一个应用型的无监督学习解决方案。具体来说，我们将重新审视在 [第二章](ch02.html#Chapter_2)
    中介绍的欺诈检测问题，并尝试在不使用标签的情况下将欺诈交易与正常交易分离开来。
- en: ^([1](ch03.html#idm140637553285552-marker)) The [MNIST database of handwritten
    digits](http://yann.lecun.com/exdb/mnist/), courtesy of Yann Lecun.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch03.html#idm140637553285552-marker)) [手写数字MNIST数据库](http://yann.lecun.com/exdb/mnist/)，由Yann
    Lecun提供。
- en: ^([2](ch03.html#idm140637553283488-marker)) The [pickled version of the MNIST
    dataset](http://deeplearning.net/tutorial/gettingstarted.html), courtesy of deeplearning.net.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch03.html#idm140637553283488-marker)) [MNIST数据集的Pickled版本](http://deeplearning.net/tutorial/gettingstarted.html)，由deeplearning.net提供。
- en: ^([3](ch03.html#idm140637550396848-marker)) The overcomplete dictionary serves
    a different purpose and has applications such as image compression.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch03.html#idm140637550396848-marker)) 过完备字典有不同的用途，例如图像压缩。
