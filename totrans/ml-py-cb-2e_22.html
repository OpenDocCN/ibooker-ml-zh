<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 22. Neural Networks for Unstructured Data" data-type="chapter" epub:type="chapter"><div class="chapter" id="neural-networks-for-unstructured-data">
<h1><span class="label">Chapter 22. </span>Neural Networks for Unstructured Data</h1>
<section data-pdf-bookmark="22.0 Introduction" data-type="sect1"><div class="sect1" id="id433">
<h1>22.0 Introduction</h1>
<p>In the previous <a data-primary="unstructured data" data-type="indexterm" id="ix_unstruct_data_ch22"/><a data-primary="neural networks" data-secondary="unstructured data" data-type="indexterm" id="ix_nn_unstruct_data"/>chapter, we focused on neural network recipes for <em>structured</em> data, i.e., tabular data. Most of the largest advances in the past few years have actually involved using neural networks and deep learning for <em>unstructured</em> data, such as text or images. Working with these unstructured datasets is a bit different than working with structured sources of data.</p>
<p>Deep learning is particularly powerful in the unstructured data space, where “classic” machine learning techniques (such as boosted trees) typically fail to capture all the complexity and nuance present in text data, audio, images, videos, etc. In this chapter, we will explore using deep learning specifically for text and image data.</p>
<p>In a supervised learning space for text and images, there are many subtasks or “types” of learning. The following are a few examples (though this is not a comprehensive list):</p>
<ul>
<li>
<p>Text or image <a data-primary="supervised learning models" data-secondary="text and image classification" data-type="indexterm" id="id2051"/>classification (example: classifying whether or not an image is a picture of a hotdog)</p>
</li>
<li>
<p>Transfer <a data-primary="supervised learning models" data-secondary="transfer learning" data-type="indexterm" id="id2052"/>learning (example: using a pretrained contextual model like BERT and fine-tuning it on a task to predict whether or not an email is spam)</p>
</li>
<li>
<p>Object <a data-primary="supervised learning models" data-secondary="object detection" data-type="indexterm" id="id2053"/>detection (example: identifying and classifying specific objects within an image)</p>
</li>
<li>
<p>Generative <a data-primary="supervised learning models" data-secondary="generative models" data-type="indexterm" id="id2054"/>models (example: models that generate text based on a given input such as the GPT models)</p>
</li>
</ul>
<p>As deep learning has grown in popularity and become increasingly commoditized, both the open source and enterprise solutions for dealing with these use cases have become more easily accessible. In this chapter, we’ll leverage a few key libraries as our entry point into performing these deep learning tasks. In particular, we’ll use PyTorch, Torchvision, and the Transformers Python libraries to accomplish a set of tasks across both text and image ML data.</p>
</div></section>
<section data-pdf-bookmark="22.1 Training a Neural Network for Image Classification" data-type="sect1"><div class="sect1" id="id859">
<h1>22.1 Training a Neural Network for Image Classification</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id570">
<h2>Problem</h2>
<p>You need to train an <a data-primary="unstructured data" data-secondary="image classification training" data-type="indexterm" id="ix_unstruct_data_image"/><a data-primary="image classification" data-secondary="neural networks for" data-type="indexterm" id="ix_image_class_nn"/><a data-primary="classification and classifiers" data-secondary="image classification" data-type="indexterm" id="ix_classif_image_class_nn"/>image classification neural network.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id434">
<h2>Solution</h2>
<p>Use a <a data-primary="PyTorch" data-secondary="convolutional neural network" data-type="indexterm" id="ix_pyt_convo_nn"/><a data-primary="convolutional neural network" data-type="indexterm" id="ix_convo_nn"/>convolutional neural network in PyTorch:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import libraries</code>
<code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>
<code class="kn">import</code> <code class="nn">torch.optim</code> <code class="k">as</code> <code class="nn">optim</code>
<code class="kn">from</code> <code class="nn">torchvision</code> <code class="kn">import</code> <code class="n">datasets</code><code class="p">,</code> <code class="n">transforms</code>

<code class="c1"># Define the convolutional neural network architecture</code>
<code class="k">class</code> <code class="nc">Net</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">Net</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">conv1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv2d</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">32</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">conv2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv2d</code><code class="p">(</code><code class="mi">32</code><code class="p">,</code> <code class="mi">64</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">dropout1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dropout2d</code><code class="p">(</code><code class="mf">0.25</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">dropout2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dropout2d</code><code class="p">(</code><code class="mf">0.5</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">fc1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">64</code> <code class="o">*</code> <code class="mi">14</code> <code class="o">*</code> <code class="mi">14</code><code class="p">,</code> <code class="mi">128</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">fc2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="mi">10</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
        <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">functional</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">conv1</code><code class="p">(</code><code class="n">x</code><code class="p">))</code>
        <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">functional</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">conv2</code><code class="p">(</code><code class="n">x</code><code class="p">))</code>
        <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">functional</code><code class="o">.</code><code class="n">max_pool2d</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">dropout1</code><code class="p">(</code><code class="n">x</code><code class="p">),</code> <code class="mi">2</code><code class="p">)</code>
        <code class="n">x</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">flatten</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
        <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">functional</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">fc1</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">dropout2</code><code class="p">(</code><code class="n">x</code><code class="p">)))</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">fc2</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">nn</code><code class="o">.</code><code class="n">functional</code><code class="o">.</code><code class="n">log_softmax</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Set the device to run on</code>
<code class="n">device</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">device</code><code class="p">(</code><code class="s2">"cuda"</code> <code class="k">if</code> <code class="n">torch</code><code class="o">.</code><code class="n">cuda</code><code class="o">.</code><code class="n">is_available</code><code class="p">()</code> <code class="k">else</code> <code class="s2">"cpu"</code><code class="p">)</code>

<code class="c1"># Define the data preprocessing steps</code>
<code class="n">transform</code> <code class="o">=</code> <code class="n">transforms</code><code class="o">.</code><code class="n">Compose</code><code class="p">([</code>
    <code class="n">transforms</code><code class="o">.</code><code class="n">ToTensor</code><code class="p">(),</code>
    <code class="n">transforms</code><code class="o">.</code><code class="n">Normalize</code><code class="p">((</code><code class="mf">0.1307</code><code class="p">,),</code> <code class="p">(</code><code class="mf">0.3081</code><code class="p">,))</code>
<code class="p">])</code>

<code class="c1"># Load the MNIST dataset</code>
<code class="n">train_dataset</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">MNIST</code><code class="p">(</code><code class="s1">'./data'</code><code class="p">,</code> <code class="n">train</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">download</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
    <code class="n">transform</code><code class="o">=</code><code class="n">transform</code><code class="p">)</code>
<code class="n">test_dataset</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">MNIST</code><code class="p">(</code><code class="s1">'./data'</code><code class="p">,</code> <code class="n">train</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code> <code class="n">transform</code><code class="o">=</code><code class="n">transform</code><code class="p">)</code>

<code class="c1"># Create data loaders</code>
<code class="n">batch_size</code> <code class="o">=</code> <code class="mi">64</code>
<code class="n">train_loader</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">utils</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">DataLoader</code><code class="p">(</code><code class="n">train_dataset</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="n">batch_size</code><code class="p">,</code>
    <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">test_loader</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">utils</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">DataLoader</code><code class="p">(</code><code class="n">test_dataset</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="n">batch_size</code><code class="p">,</code>
    <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>

<code class="c1"># Initialize the model and optimizer</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">Net</code><code class="p">()</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">optim</code><code class="o">.</code><code class="n">Adam</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">())</code>

<code class="c1"># Compile the model using torch 2.0's optimizer</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">model</code><code class="p">)</code>

<code class="c1"># Define the training loop</code>
<code class="n">model</code><code class="o">.</code><code class="n">train</code><code class="p">()</code>
<code class="k">for</code> <code class="n">batch_idx</code><code class="p">,</code> <code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">train_loader</code><code class="p">):</code>
    <code class="n">data</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="n">data</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">),</code> <code class="n">target</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
    <code class="n">optimizer</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code>
    <code class="n">output</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>
    <code class="n">loss</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">functional</code><code class="o">.</code><code class="n">nll_loss</code><code class="p">(</code><code class="n">output</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>
    <code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
    <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">()</code>

<code class="c1"># Define the testing loop</code>
<code class="n">model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code>
<code class="n">test_loss</code> <code class="o">=</code> <code class="mi">0</code>
<code class="n">correct</code> <code class="o">=</code> <code class="mi">0</code>
<code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
    <code class="k">for</code> <code class="n">data</code><code class="p">,</code> <code class="n">target</code> <code class="ow">in</code> <code class="n">test_loader</code><code class="p">:</code>
        <code class="n">data</code><code class="p">,</code> <code class="n">target</code> <code class="o">=</code> <code class="n">data</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">),</code> <code class="n">target</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code>
        <code class="n">output</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>

        <code class="c1"># get the index of the max log-probability</code>
        <code class="n">test_loss</code> <code class="o">+=</code> <code class="n">nn</code><code class="o">.</code><code class="n">functional</code><code class="o">.</code><code class="n">nll_loss</code><code class="p">(</code>
            <code class="n">output</code><code class="p">,</code> <code class="n">target</code><code class="p">,</code> <code class="n">reduction</code><code class="o">=</code><code class="s1">'sum'</code>
        <code class="p">)</code><code class="o">.</code><code class="n">item</code><code class="p">()</code>  <code class="c1"># sum up batch loss</code>
        <code class="n">pred</code> <code class="o">=</code> <code class="n">output</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">keepdim</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
        <code class="n">correct</code> <code class="o">+=</code> <code class="n">pred</code><code class="o">.</code><code class="n">eq</code><code class="p">(</code><code class="n">target</code><code class="o">.</code><code class="n">view_as</code><code class="p">(</code><code class="n">pred</code><code class="p">))</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code><code class="o">.</code><code class="n">item</code><code class="p">()</code>

<code class="n">test_loss</code> <code class="o">/=</code> <code class="nb">len</code><code class="p">(</code><code class="n">test_loader</code><code class="o">.</code><code class="n">dataset</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id435">
<h2>Discussion</h2>
<p>Convolutional neural networks are typically used for tasks in image recognition and computer vision. They typically consist of convolutional layers, pooling layers, and a fully connected layer.</p>
<p>The purpose of the <em>convolutional layers</em> is to learn important image features that can be used for the task at hand. Convolutional layers work by applying a filter to a particular area of an image (the size of the convolution). The weights of this layer then learn to recognize specific image features critical in the classification task. For instance, if we’re training a model that recognizes a person’s hand, the filter may learn to recognize fingers.</p>
<p>The <a data-primary="average pooling" data-type="indexterm" id="id2055"/><a data-primary="dimensionality reduction" data-secondary="convolutional neural network" data-type="indexterm" id="id2056"/><a data-primary="max pooling" data-type="indexterm" id="id2057"/><a data-primary="pooling layer" data-type="indexterm" id="id2058"/>purpose of the <em>pooling layer</em> is typically to reduce the dimensionality of the inputs from the previous layer. This layer also uses a filter applied to a portion of the input, but it has no activation. Instead, it reduces dimensionality of the input by performing <em>max pooling</em> (where it selects the pixel in the filter with the highest value) or <em>average pooling</em> (where it takes an average of the input pixels to use instead).</p>
<p>Finally, the <em>fully connected layer</em> <a data-primary="fully connected layer" data-type="indexterm" id="id2059"/>can be used with something like a softmax activation function to create a binary classification task.<a data-primary="" data-startref="ix_classif_image_class_nn" data-type="indexterm" id="id2060"/><a data-primary="" data-startref="ix_convo_nn" data-type="indexterm" id="id2061"/><a data-primary="" data-startref="ix_image_class_nn" data-type="indexterm" id="id2062"/><a data-primary="" data-startref="ix_pyt_convo_nn" data-type="indexterm" id="id2063"/><a data-primary="" data-startref="ix_unstruct_data_image" data-type="indexterm" id="id2064"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id2065">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/HoO9g">Convolutional Neural Networks</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="22.2 Training a Neural Network for Text Classification" data-type="sect1"><div class="sect1" id="id860">
<h1>22.2 Training a Neural Network for Text Classification</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id571">
<h2>Problem</h2>
<p>You need to <a data-primary="text classification" data-secondary="neural networks for" data-type="indexterm" id="ix_text_classif_nn"/><a data-primary="unstructured data" data-secondary="text classification" data-type="indexterm" id="ix_unstruct_data_text"/><a data-primary="classification and classifiers" data-secondary="text classification" data-type="indexterm" id="ix_classif_text_class"/>train a neural network to classify text data.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id2066">
<h2>Solution</h2>
<p>Use a PyTorch neural network whose first layer is the size of your vocabulary:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import libraries</code>
<code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">import</code> <code class="nn">torch.nn</code> <code class="k">as</code> <code class="nn">nn</code>
<code class="kn">import</code> <code class="nn">torch.optim</code> <code class="k">as</code> <code class="nn">optim</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">fetch_20newsgroups</code>
<code class="kn">from</code> <code class="nn">sklearn.feature_extraction.text</code> <code class="kn">import</code> <code class="n">CountVectorizer</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">accuracy_score</code>

<code class="c1"># Load the 20 newsgroups dataset</code>
<code class="n">cats</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'alt.atheism'</code><code class="p">,</code> <code class="s1">'sci.space'</code><code class="p">]</code>
<code class="n">newsgroups_data</code> <code class="o">=</code> <code class="n">fetch_20newsgroups</code><code class="p">(</code><code class="n">subset</code><code class="o">=</code><code class="s1">'all'</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
    <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">,</code> <code class="n">categories</code><code class="o">=</code><code class="n">cats</code><code class="p">)</code>

<code class="c1"># Split the dataset into training and test sets</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">newsgroups_data</code><code class="o">.</code><code class="n">data</code><code class="p">,</code>
    <code class="n">newsgroups_data</code><code class="o">.</code><code class="n">target</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="mf">0.2</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>

<code class="c1"># Vectorize the text data using a bag-of-words approach</code>
<code class="n">vectorizer</code> <code class="o">=</code> <code class="n">CountVectorizer</code><code class="p">(</code><code class="n">stop_words</code><code class="o">=</code><code class="s1">'english'</code><code class="p">)</code>
<code class="n">X_train</code> <code class="o">=</code> <code class="n">vectorizer</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code><code class="o">.</code><code class="n">toarray</code><code class="p">()</code>
<code class="n">X_test</code> <code class="o">=</code> <code class="n">vectorizer</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code><code class="o">.</code><code class="n">toarray</code><code class="p">()</code>

<code class="c1"># Convert the data to PyTorch tensors</code>
<code class="n">X_train</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">float32</code><code class="p">)</code>
<code class="n">y_train</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">(</code><code class="n">y_train</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">long</code><code class="p">)</code>
<code class="n">X_test</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">float32</code><code class="p">)</code>
<code class="n">y_test</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">(</code><code class="n">y_test</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">long</code><code class="p">)</code>

<code class="c1"># Define the model</code>
<code class="k">class</code> <code class="nc">TextClassifier</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">num_classes</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">TextClassifier</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">fc1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">X_train</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="mi">128</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">fc2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="n">num_classes</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
        <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">functional</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">fc1</code><code class="p">(</code><code class="n">x</code><code class="p">))</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">fc2</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">nn</code><code class="o">.</code><code class="n">functional</code><code class="o">.</code><code class="n">log_softmax</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Instantiate the model and define the loss function and optimizer</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">TextClassifier</code><code class="p">(</code><code class="n">num_classes</code><code class="o">=</code><code class="nb">len</code><code class="p">(</code><code class="n">cats</code><code class="p">))</code>
<code class="n">loss_function</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">CrossEntropyLoss</code><code class="p">()</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">optim</code><code class="o">.</code><code class="n">Adam</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">lr</code><code class="o">=</code><code class="mf">0.01</code><code class="p">)</code>

<code class="c1"># Compile the model using torch 2.0's optimizer</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">model</code><code class="p">)</code>

<code class="c1"># Train the model</code>
<code class="n">num_epochs</code> <code class="o">=</code> <code class="mi">1</code>
<code class="n">batch_size</code> <code class="o">=</code> <code class="mi">10</code>
<code class="n">num_batches</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code> <code class="o">//</code> <code class="n">batch_size</code>
<code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">num_epochs</code><code class="p">):</code>
    <code class="n">total_loss</code> <code class="o">=</code> <code class="mf">0.0</code>
    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">num_batches</code><code class="p">):</code>
        <code class="c1"># Prepare the input and target data for the current batch</code>
        <code class="n">start_idx</code> <code class="o">=</code> <code class="n">i</code> <code class="o">*</code> <code class="n">batch_size</code>
        <code class="n">end_idx</code> <code class="o">=</code> <code class="p">(</code><code class="n">i</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)</code> <code class="o">*</code> <code class="n">batch_size</code>
        <code class="n">inputs</code> <code class="o">=</code> <code class="n">X_train</code><code class="p">[</code><code class="n">start_idx</code><code class="p">:</code><code class="n">end_idx</code><code class="p">]</code>
        <code class="n">targets</code> <code class="o">=</code> <code class="n">y_train</code><code class="p">[</code><code class="n">start_idx</code><code class="p">:</code><code class="n">end_idx</code><code class="p">]</code>

        <code class="c1"># Zero the gradients for the optimizer</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code>

        <code class="c1"># Forward pass through the model and compute the loss</code>
        <code class="n">outputs</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">inputs</code><code class="p">)</code>
        <code class="n">loss</code> <code class="o">=</code> <code class="n">loss_function</code><code class="p">(</code><code class="n">outputs</code><code class="p">,</code> <code class="n">targets</code><code class="p">)</code>

        <code class="c1"># Backward pass through the model and update the parameters</code>
        <code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">()</code>

        <code class="c1"># Update the total loss for the epoch</code>
        <code class="n">total_loss</code> <code class="o">+=</code> <code class="n">loss</code><code class="o">.</code><code class="n">item</code><code class="p">()</code>

    <code class="c1"># Compute the accuracy on the test set for the epoch</code>
    <code class="n">test_outputs</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>
    <code class="n">test_predictions</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">test_outputs</code><code class="p">,</code> <code class="n">dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
    <code class="n">test_accuracy</code> <code class="o">=</code> <code class="n">accuracy_score</code><code class="p">(</code><code class="n">y_test</code><code class="p">,</code> <code class="n">test_predictions</code><code class="p">)</code>

    <code class="c1"># Print the epoch number, average loss, and test accuracy</code>
    <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Epoch: </code><code class="si">{</code><code class="n">epoch</code><code class="o">+</code><code class="mi">1</code><code class="si">}</code><code class="s2">, Loss: </code><code class="si">{</code><code class="n">total_loss</code><code class="o">/</code><code class="n">num_batches</code><code class="si">}</code><code class="s2">, Test Accuracy:"</code>
        <code class="s2">"</code><code class="si">{test_accuracy}</code><code class="s2">"</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id436">
<h2>Discussion</h2>
<p>Unlike <a data-primary="CountVectorizer" data-type="indexterm" id="id2067"/>images, text data is inherently nonnumeric. Before training a model, we need to convert the text into a numeric representation that the model can use to learn which words and word combinations are important for the classification task at hand. In this example, we use scikit-learn’s <code>CountVectorizer</code> to encode the vocabulary as a vector the size of the entire vocabulary, where each word is assigned to a specific index in the vector, and the value at that location is the number of times that word appears in a given paragraph. In this case, we can see the vocabulary size by looking at our training set:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">X_train</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code></pre>
<pre data-type="programlisting">25150</pre>
<p>We use this <a data-primary="word embeddings" data-type="indexterm" id="id2068"/>same value in the first layer of our neural network to determine the size of the input layer: <code>self.fc1 = nn.Linear(X_train.shape[1], 128)</code>. This allows our network to learn what are called <em>word embeddings</em>, vector representations of individual words learned from a supervised learning task like the one in this recipe. This task will allow us to learn word embeddings of size 128, though these embeddings will primarily be useful for this specific task and vocabulary.<a data-primary="" data-startref="ix_classif_text_class" data-type="indexterm" id="id2069"/><a data-primary="" data-startref="ix_text_classif_nn" data-type="indexterm" id="id2070"/><a data-primary="" data-startref="ix_unstruct_data_text" data-type="indexterm" id="id2071"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="22.3 Fine-Tuning a Pretrained Model for Image Classification" data-type="sect1"><div class="sect1" id="id861">
<h1>22.3 Fine-Tuning a Pretrained Model for Image Classification</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id572">
<h2>Problem</h2>
<p>You want to <a data-primary="image classification" data-secondary="neural networks for" data-type="indexterm" id="ix_image_class_nn2"/><a data-primary="unstructured data" data-secondary="image classification training" data-type="indexterm" id="ix_unstruct_data_image2"/><a data-primary="classification and classifiers" data-secondary="image classification" data-type="indexterm" id="ix_classif_image_class2"/>train an image classification model using learnings from a pretrained model.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id437">
<h2>Solution</h2>
<p>Use the <code>transformers</code> <a data-primary="transformers library" data-type="indexterm" id="ix_transf_lib"/>library with <code>torchvision</code> to fine-tune a pretrained model on your data:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import libraries</code>
<code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">from</code> <code class="nn">torchvision.transforms</code> <code class="kn">import</code><code class="p">(</code>
    <code class="n">RandomResizedCrop</code><code class="p">,</code> <code class="n">Compose</code><code class="p">,</code> <code class="n">Normalize</code><code class="p">,</code> <code class="n">ToTensor</code>
    <code class="p">)</code>
<code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">Trainer</code><code class="p">,</code> <code class="n">TrainingArguments</code><code class="p">,</code> <code class="n">DefaultDataCollator</code>
<code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">ViTFeatureExtractor</code><code class="p">,</code> <code class="n">ViTForImageClassification</code>
<code class="kn">from</code> <code class="nn">datasets</code> <code class="kn">import</code> <code class="n">load_dataset</code><code class="p">,</code> <code class="n">load_metric</code><code class="p">,</code> <code class="n">Image</code>

<code class="c1"># Define a helper function to convert the images into RGB</code>
<code class="k">def</code> <code class="nf">transforms</code><code class="p">(</code><code class="n">examples</code><code class="p">):</code>
    <code class="n">examples</code><code class="p">[</code><code class="s2">"pixel_values"</code><code class="p">]</code> <code class="o">=</code> <code class="p">[</code><code class="n">_transforms</code><code class="p">(</code><code class="n">img</code><code class="o">.</code><code class="n">convert</code><code class="p">(</code><code class="s2">"RGB"</code><code class="p">))</code> <code class="k">for</code> <code class="n">img</code> <code class="ow">in</code>
        <code class="n">examples</code><code class="p">[</code><code class="s2">"image"</code><code class="p">]]</code>
    <code class="k">del</code> <code class="n">examples</code><code class="p">[</code><code class="s2">"image"</code><code class="p">]</code>
    <code class="k">return</code> <code class="n">examples</code>

<code class="c1"># Define a helper function to compute metrics</code>
<code class="k">def</code> <code class="nf">compute_metrics</code><code class="p">(</code><code class="n">p</code><code class="p">):</code>
    <code class="k">return</code> <code class="n">metric</code><code class="o">.</code><code class="n">compute</code><code class="p">(</code><code class="n">predictions</code><code class="o">=</code><code class="n">np</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">p</code><code class="o">.</code><code class="n">predictions</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">),</code>
        <code class="n">references</code><code class="o">=</code><code class="n">p</code><code class="o">.</code><code class="n">label_ids</code><code class="p">)</code>

<code class="c1"># Load the fashion mnist dataset</code>
<code class="n">dataset</code> <code class="o">=</code> <code class="n">load_dataset</code><code class="p">(</code><code class="s2">"fashion_mnist"</code><code class="p">)</code>

<code class="c1"># Load the processor from the VIT model</code>
<code class="n">image_processor</code> <code class="o">=</code> <code class="n">ViTFeatureExtractor</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code>
    <code class="s2">"google/vit-base-patch16-224-in21k"</code>
<code class="p">)</code>

<code class="c1"># Set the labels from the dataset</code>
<code class="n">labels</code> <code class="o">=</code> <code class="n">dataset</code><code class="p">[</code><code class="s1">'train'</code><code class="p">]</code><code class="o">.</code><code class="n">features</code><code class="p">[</code><code class="s1">'label'</code><code class="p">]</code><code class="o">.</code><code class="n">names</code>

<code class="c1"># Load the pretrained model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">ViTForImageClassification</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code>
    <code class="s2">"google/vit-base-patch16-224-in21k"</code><code class="p">,</code>
    <code class="n">num_labels</code><code class="o">=</code><code class="nb">len</code><code class="p">(</code><code class="n">labels</code><code class="p">),</code>
    <code class="n">id2label</code><code class="o">=</code><code class="p">{</code><code class="nb">str</code><code class="p">(</code><code class="n">i</code><code class="p">):</code> <code class="n">c</code> <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">c</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">labels</code><code class="p">)},</code>
    <code class="n">label2id</code><code class="o">=</code><code class="p">{</code><code class="n">c</code><code class="p">:</code> <code class="nb">str</code><code class="p">(</code><code class="n">i</code><code class="p">)</code> <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">c</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">labels</code><code class="p">)}</code>
<code class="p">)</code>

<code class="c1"># Define the collator, normalizer, and transforms</code>
<code class="n">collate_fn</code> <code class="o">=</code> <code class="n">DefaultDataCollator</code><code class="p">()</code>
<code class="n">normalize</code> <code class="o">=</code> <code class="n">Normalize</code><code class="p">(</code><code class="n">mean</code><code class="o">=</code><code class="n">image_processor</code><code class="o">.</code><code class="n">image_mean</code><code class="p">,</code>
    <code class="n">std</code><code class="o">=</code><code class="n">image_processor</code><code class="o">.</code><code class="n">image_std</code><code class="p">)</code>
<code class="n">size</code> <code class="o">=</code> <code class="p">(</code>
    <code class="n">image_processor</code><code class="o">.</code><code class="n">size</code><code class="p">[</code><code class="s2">"shortest_edge"</code><code class="p">]</code>
    <code class="k">if</code> <code class="s2">"shortest_edge"</code> <code class="ow">in</code> <code class="n">image_processor</code><code class="o">.</code><code class="n">size</code>
    <code class="k">else</code> <code class="p">(</code><code class="n">image_processor</code><code class="o">.</code><code class="n">size</code><code class="p">[</code><code class="s2">"height"</code><code class="p">],</code> <code class="n">image_processor</code><code class="o">.</code><code class="n">size</code><code class="p">[</code><code class="s2">"width"</code><code class="p">])</code>
<code class="p">)</code>
<code class="n">_transforms</code> <code class="o">=</code> <code class="n">Compose</code><code class="p">([</code><code class="n">RandomResizedCrop</code><code class="p">(</code><code class="n">size</code><code class="p">),</code> <code class="n">ToTensor</code><code class="p">(),</code> <code class="n">normalize</code><code class="p">])</code>

<code class="c1"># Load the dataset we'll use with transformations</code>
<code class="n">dataset</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">with_transform</code><code class="p">(</code><code class="n">transforms</code><code class="p">)</code>

<code class="c1"># Use accuracy as our metric</code>
<code class="n">metric</code> <code class="o">=</code> <code class="n">load_metric</code><code class="p">(</code><code class="s2">"accuracy"</code><code class="p">)</code>

<code class="c1"># Set the training args</code>
<code class="n">training_args</code> <code class="o">=</code> <code class="n">TrainingArguments</code><code class="p">(</code>
    <code class="n">output_dir</code><code class="o">=</code><code class="s2">"fashion_mnist_model"</code><code class="p">,</code>
    <code class="n">remove_unused_columns</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code>
    <code class="n">evaluation_strategy</code><code class="o">=</code><code class="s2">"epoch"</code><code class="p">,</code>
    <code class="n">save_strategy</code><code class="o">=</code><code class="s2">"epoch"</code><code class="p">,</code>
    <code class="n">learning_rate</code><code class="o">=</code><code class="mf">0.01</code><code class="p">,</code>
    <code class="n">per_device_train_batch_size</code><code class="o">=</code><code class="mi">16</code><code class="p">,</code>
    <code class="n">gradient_accumulation_steps</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code>
    <code class="n">per_device_eval_batch_size</code><code class="o">=</code><code class="mi">16</code><code class="p">,</code>
    <code class="n">num_train_epochs</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>
    <code class="n">warmup_ratio</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code>
    <code class="n">logging_steps</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code>
    <code class="n">load_best_model_at_end</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
    <code class="n">metric_for_best_model</code><code class="o">=</code><code class="s2">"accuracy"</code><code class="p">,</code>
    <code class="n">push_to_hub</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code>
<code class="p">)</code>

<code class="c1"># Instantiate a trainer</code>
<code class="n">trainer</code> <code class="o">=</code> <code class="n">Trainer</code><code class="p">(</code>
    <code class="n">model</code><code class="o">=</code><code class="n">model</code><code class="p">,</code>
    <code class="n">args</code><code class="o">=</code><code class="n">training_args</code><code class="p">,</code>
    <code class="n">data_collator</code><code class="o">=</code><code class="n">collate_fn</code><code class="p">,</code>
    <code class="n">compute_metrics</code><code class="o">=</code><code class="n">compute_metrics</code><code class="p">,</code>
    <code class="n">train_dataset</code><code class="o">=</code><code class="n">dataset</code><code class="p">[</code><code class="s2">"train"</code><code class="p">],</code>
    <code class="n">eval_dataset</code><code class="o">=</code><code class="n">dataset</code><code class="p">[</code><code class="s2">"test"</code><code class="p">],</code>
    <code class="n">tokenizer</code><code class="o">=</code><code class="n">image_processor</code><code class="p">,</code>
<code class="p">)</code>

<code class="c1"># Train the model, log and save metrics</code>
<code class="n">train_results</code> <code class="o">=</code> <code class="n">trainer</code><code class="o">.</code><code class="n">train</code><code class="p">()</code>
<code class="n">trainer</code><code class="o">.</code><code class="n">save_model</code><code class="p">()</code>
<code class="n">trainer</code><code class="o">.</code><code class="n">log_metrics</code><code class="p">(</code><code class="s2">"train"</code><code class="p">,</code> <code class="n">train_results</code><code class="o">.</code><code class="n">metrics</code><code class="p">)</code>
<code class="n">trainer</code><code class="o">.</code><code class="n">save_metrics</code><code class="p">(</code><code class="s2">"train"</code><code class="p">,</code> <code class="n">train_results</code><code class="o">.</code><code class="n">metrics</code><code class="p">)</code>
<code class="n">trainer</code><code class="o">.</code><code class="n">save_state</code><code class="p">()</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id438">
<h2>Discussion</h2>
<p>In the <a data-primary="transfer learning" data-type="indexterm" id="id2072"/><a data-primary="ViT (Vision Transformer) model" data-type="indexterm" id="id2073"/><a data-primary="pretrained models" data-secondary="with unstructured data" data-secondary-sortas="unstructured data" data-type="indexterm" id="id2074"/>realm of unstructured data like text and images, it is extremely common to start from pretrained models trained on large datasets, instead of starting from scratch, especially in cases where we don’t have access to as much labeled data. Using embeddings and other information from the larger model, we can then fine-tune our own model for a new task without the need for as much labeled information. In addition, the pretrained model may have information not captured at all in our training dataset, resulting in an overall performance improvement. This process is known as <em>transfer learning</em>.</p>
<p>In this <a data-primary="Google ViT (Vision Transformer) model" data-type="indexterm" id="id2075"/><a data-primary="MNIST dataset" data-type="indexterm" id="id2076"/>example, we load the weights from Google’s ViT (Vision Transformer) model. Then, we use the <code>transformers</code> library to fine-tune it for a classification task on the fashion MNIST dataset, a simple dataset of clothing items. This approach can be applied to increase performance on any computer vision dataset, and the 
<span class="keep-together"><code>transformers</code></span> library provides a high-level interface we can use to fine-tune our own model from larger, pretrained ones without writing an egregious amount of code.<a data-primary="" data-startref="ix_classif_image_class2" data-type="indexterm" id="id2077"/><a data-primary="" data-startref="ix_image_class_nn2" data-type="indexterm" id="id2078"/><a data-primary="" data-startref="ix_unstruct_data_image2" data-type="indexterm" id="id2079"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id2080">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/5F3Rf">Hugging Face website and documentation</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="22.4 Fine-Tuning a Pretrained Model for Text Classification" data-type="sect1"><div class="sect1" id="id862">
<h1>22.4 Fine-Tuning a Pretrained Model for Text Classification</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id573">
<h2>Problem</h2>
<p>You want to <a data-primary="unstructured data" data-secondary="text classification" data-type="indexterm" id="id2081"/><a data-primary="pretrained models" data-secondary="with unstructured data" data-secondary-sortas="unstructured data" data-type="indexterm" id="ix_pretrain_unstruct"/><a data-primary="text classification" data-secondary="fine-tuning pretrained model" data-type="indexterm" id="ix_text_class_pretrain"/><a data-primary="classification and classifiers" data-secondary="text classification" data-type="indexterm" id="ix_classif_text2"/>train a text classification model using learnings from a pretrained model.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id2082">
<h2>Solution</h2>
<p>Use the <code>transformers</code> library:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import libraries</code>
<code class="kn">from</code> <code class="nn">datasets</code> <code class="kn">import</code> <code class="n">load_dataset</code>
<code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">AutoTokenizer</code><code class="p">,</code> <code class="n">DataCollatorWithPadding</code>
<code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="p">(</code>
    <code class="n">AutoModelForSequenceClassification</code><code class="p">,</code> <code class="n">TrainingArguments</code><code class="p">,</code> <code class="n">Trainer</code>
    <code class="p">)</code>
<code class="kn">import</code> <code class="nn">evaluate</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>

<code class="c1"># Load the imdb dataset</code>
<code class="n">imdb</code> <code class="o">=</code> <code class="n">load_dataset</code><code class="p">(</code><code class="s2">"imdb"</code><code class="p">)</code>

<code class="c1"># Create a tokenizer and collator</code>
<code class="n">tokenizer</code> <code class="o">=</code> <code class="n">AutoTokenizer</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"distilbert-base-uncased"</code><code class="p">)</code>
<code class="n">data_collator</code> <code class="o">=</code> <code class="n">DataCollatorWithPadding</code><code class="p">(</code><code class="n">tokenizer</code><code class="o">=</code><code class="n">tokenizer</code><code class="p">)</code>

<code class="c1"># Tokenize the imdb dataset</code>
<code class="n">tokenized_imdb</code> <code class="o">=</code> <code class="n">imdb</code><code class="o">.</code><code class="n">map</code><code class="p">(</code>
    <code class="k">lambda</code> <code class="n">example</code><code class="p">:</code> <code class="n">tokenizer</code><code class="p">(</code>
        <code class="n">example</code><code class="p">[</code><code class="s2">"text"</code><code class="p">],</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"max_length"</code><code class="p">,</code> <code class="n">truncation</code><code class="o">=</code><code class="kc">True</code>
    <code class="p">),</code>
    <code class="n">batched</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
<code class="p">)</code>

<code class="c1"># User the accuracy metric</code>
<code class="n">accuracy</code> <code class="o">=</code> <code class="n">evaluate</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="s2">"accuracy"</code><code class="p">)</code>

<code class="c1"># Define a helper function to produce metrics</code>
<code class="k">def</code> <code class="nf">compute_metrics</code><code class="p">(</code><code class="n">eval_pred</code><code class="p">):</code>
    <code class="n">predictions</code><code class="p">,</code> <code class="n">labels</code> <code class="o">=</code> <code class="n">eval_pred</code>
    <code class="n">predictions</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">predictions</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">accuracy</code><code class="o">.</code><code class="n">compute</code><code class="p">(</code><code class="n">predictions</code><code class="o">=</code><code class="n">predictions</code><code class="p">,</code> <code class="n">references</code><code class="o">=</code><code class="n">labels</code><code class="p">)</code>

<code class="c1"># Create dictionaries to map indices to labels and vice versa</code>
<code class="n">id2label</code> <code class="o">=</code> <code class="p">{</code><code class="mi">0</code><code class="p">:</code> <code class="s2">"NEGATIVE"</code><code class="p">,</code> <code class="mi">1</code><code class="p">:</code> <code class="s2">"POSITIVE"</code><code class="p">}</code>
<code class="n">label2id</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"NEGATIVE"</code><code class="p">:</code> <code class="mi">0</code><code class="p">,</code> <code class="s2">"POSITIVE"</code><code class="p">:</code> <code class="mi">1</code><code class="p">}</code>

<code class="c1"># Load a pretrained model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">AutoModelForSequenceClassification</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code>
    <code class="s2">"distilbert-base-uncased"</code><code class="p">,</code> <code class="n">num_labels</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">id2label</code><code class="o">=</code><code class="n">id2label</code><code class="p">,</code>
        <code class="n">label2id</code><code class="o">=</code><code class="n">label2id</code>
<code class="p">)</code>

<code class="c1"># Specify the training arguments</code>
<code class="n">training_args</code> <code class="o">=</code> <code class="n">TrainingArguments</code><code class="p">(</code>
    <code class="n">output_dir</code><code class="o">=</code><code class="s2">"my_awesome_model"</code><code class="p">,</code>
    <code class="n">learning_rate</code><code class="o">=</code><code class="mf">2e-5</code><code class="p">,</code>
    <code class="n">per_device_train_batch_size</code><code class="o">=</code><code class="mi">16</code><code class="p">,</code>
    <code class="n">per_device_eval_batch_size</code><code class="o">=</code><code class="mi">16</code><code class="p">,</code>
    <code class="n">num_train_epochs</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>
    <code class="n">weight_decay</code><code class="o">=</code><code class="mf">0.01</code><code class="p">,</code>
    <code class="n">evaluation_strategy</code><code class="o">=</code><code class="s2">"epoch"</code><code class="p">,</code>
    <code class="n">save_strategy</code><code class="o">=</code><code class="s2">"epoch"</code><code class="p">,</code>
    <code class="n">load_best_model_at_end</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
<code class="p">)</code>

<code class="c1"># Instantiate a trainer</code>
<code class="n">trainer</code> <code class="o">=</code> <code class="n">Trainer</code><code class="p">(</code>
    <code class="n">model</code><code class="o">=</code><code class="n">model</code><code class="p">,</code>
    <code class="n">args</code><code class="o">=</code><code class="n">training_args</code><code class="p">,</code>
    <code class="n">train_dataset</code><code class="o">=</code><code class="n">tokenized_imdb</code><code class="p">[</code><code class="s2">"train"</code><code class="p">],</code>
    <code class="n">eval_dataset</code><code class="o">=</code><code class="n">tokenized_imdb</code><code class="p">[</code><code class="s2">"test"</code><code class="p">],</code>
    <code class="n">tokenizer</code><code class="o">=</code><code class="n">tokenizer</code><code class="p">,</code>
    <code class="n">data_collator</code><code class="o">=</code><code class="n">data_collator</code><code class="p">,</code>
    <code class="n">compute_metrics</code><code class="o">=</code><code class="n">compute_metrics</code><code class="p">,</code>
<code class="p">)</code>

<code class="c1"># Train the model</code>
<code class="n">trainer</code><code class="o">.</code><code class="n">train</code><code class="p">()</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id439">
<h2>Discussion</h2>
<p>Just like using pretrained image models, pretrained language models hold a massive amount of context about language, as they’re typically trained on a wide variety of open internet sources. When we start from a pretrained model base, what we’re typically doing is swapping out the classification layer of the existing network for one of our own. This allows us to alter the network weights already learned to fit our specific task.</p>
<p>In this example, we’re fine-tuning a <a data-primary="DistilBERT model" data-type="indexterm" id="id2083"/>DistilBERT model to recognize whether IMDB movie reviews were positive (1) or negative (0). The pretrained DistilBERT model provides a large corpus of words and context on each one, in addition to neural network weights learned from a previous training task. Transfer learning allows us to take advantage of all the initial work done training the DistilBERT model and repurpose it for our use case, which in this instance is classifying movie reviews.<a data-primary="" data-startref="ix_nn_unstruct_data" data-type="indexterm" id="id2084"/><a data-primary="" data-startref="ix_unstruct_data_ch22" data-type="indexterm" id="id2085"/><a data-primary="" data-startref="ix_transf_lib" data-type="indexterm" id="id2086"/><a data-primary="" data-startref="ix_classif_text2" data-type="indexterm" id="id2087"/><a data-primary="" data-startref="ix_pretrain_unstruct" data-type="indexterm" id="id2088"/><a data-primary="" data-startref="ix_text_class_pretrain" data-type="indexterm" id="id2089"/><a data-primary="" data-startref="ix_unstruct_text" data-type="indexterm" id="id2090"/></p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id2091">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/uhrjI">Text classification in transformers</a></p>
</li>
</ul>
</div></section>
</div></section>
</div></section></div></body></html>