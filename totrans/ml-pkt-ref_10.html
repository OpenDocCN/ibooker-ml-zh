<html><head></head><body><section data-pdf-bookmark="Chapter 10. Classification" data-type="chapter" epub:type="chapter"><div class="chapter" id="idm46066896091368">&#13;
<h1><span class="label">Chapter 10. </span>Classification</h1>&#13;
&#13;
&#13;
<p><a data-primary="classification" data-secondary="models" data-type="indexterm" id="ix_ch10-asciidoc0"/>Classification <a data-primary="supervised learning" data-secondary="classification and" data-type="indexterm" id="idm46066896088424"/>is a <em>supervised learning</em> mechanism for labeling a&#13;
sample based on the features. Supervised learning means that we have labels for classification&#13;
or numbers for regression that the algorithm should learn.</p>&#13;
&#13;
<p>We will look at various classification models in this chapter. Sklearn implements many common and useful models. We will also see some that are not in sklearn, but implement&#13;
the sklearn interface. Because they follow the same interface, it is&#13;
easy to try different families of models and see how well they perform.</p>&#13;
&#13;
<p>In sklearn, we create a model instance and call the <code>.fit</code> method on it with the training data and training labels. We can now call the <code>.predict</code> method (or the <code>.predict_proba</code> or the <span class="keep-together"><code>.predict_</code></span><code>log_proba</code> methods) with the fitted model. To evaluate the model, we use the <code>.score</code> with testing data and testing labels.</p>&#13;
&#13;
<p><a data-primary="sklearn" data-secondary="data format for" data-type="indexterm" id="idm46066896082328"/>The bigger challenge is usually arranging data in a form that will work with sklearn.&#13;
The data (<code>X</code>) should be an (m by n) numpy array (or pandas DataFrame) with m rows of sample data each with n features (columns). The&#13;
label (<code>y</code>) is a vector (or pandas series) of size m with a value (class) for each sample.</p>&#13;
&#13;
<p>The <code>.score</code> method returns the mean accuracy, which by itself might not&#13;
be sufficient to evaluate a classifier. We will see other evaluation metrics.</p>&#13;
&#13;
<p>We will look at many models and discuss their efficiency, the preprocessing techniques they require, how to prevent overfitting, and if the model supports intuitive interpretation of results.</p>&#13;
&#13;
<p><a data-primary="sklearn" data-secondary="methods implemented by type models" data-type="indexterm" id="idm46066896078232"/>The general methods that sklearn type models implement are:</p>&#13;
<dl>&#13;
<dt><code>fit(X, y[, sample_weight])</code></dt>&#13;
<dd>&#13;
<p>Fit a model</p>&#13;
</dd>&#13;
<dt><code>predict(X)</code></dt>&#13;
<dd>&#13;
<p>Predict classes</p>&#13;
</dd>&#13;
<dt><code>predict_log_proba(X)</code></dt>&#13;
<dd>&#13;
<p>Predict log probability</p>&#13;
</dd>&#13;
<dt><code>predict_proba(X)</code></dt>&#13;
<dd>&#13;
<p>Predict probability</p>&#13;
</dd>&#13;
<dt><code>score(X, y[, sample_weight])</code></dt>&#13;
<dd>&#13;
<p>Get accuracy</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Logistic Regression" data-type="sect1"><div class="sect1" id="idm46066896076392">&#13;
<h1>Logistic Regression</h1>&#13;
&#13;
<p><a data-primary="classification" data-secondary="logistic regression" data-type="indexterm" id="ix_ch10-asciidoc1"/><a data-primary="logistic regression" data-type="indexterm" id="ix_ch10-asciidoc2"/>Logistic regression estimates probabilities by using a logistic&#13;
function. (Careful; even though it has regression in the name, it is&#13;
used for classification.) This has been the standard classification&#13;
model for most sciences.</p>&#13;
&#13;
<p>The following are some model characteristics that we will include for each model:</p>&#13;
<dl>&#13;
<dt>Runtime efficiency</dt>&#13;
<dd>&#13;
<p>Can use <code>n_jobs</code> if not using <code>'liblinear'</code> solver.</p>&#13;
</dd>&#13;
<dt>Preprocess data</dt>&#13;
<dd>&#13;
<p>If <code>solver</code> is set to <code>'sag'</code> or <code>'saga'</code>, standardize so that convergence works. Can handle sparse input.</p>&#13;
</dd>&#13;
<dt>Prevent overfitting</dt>&#13;
<dd>&#13;
<p>The <code>C</code> parameter controls regularization. (Lower <code>C</code> is more regularization, higher means less.) Can specify <code>penalty</code> to <code>'l1'</code> or <code>'l2'</code> (the default).</p>&#13;
</dd>&#13;
<dt>Interpret results</dt>&#13;
<dd>&#13;
<p>The <code>.coef_</code> attribute of the fitted model shows the decision function coefficients. A change in x one unit changes the log odds ratio by the coefficient. The <code>.intercept_</code> attribute is the inverse log odds of the baseline condition.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Here is an example using this model:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">LogisticRegression</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lr</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lr</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
<code class="go">LogisticRegression(C=1.0, class_weight=None,</code>&#13;
<code class="go">    dual=False, fit_intercept=True,</code>&#13;
<code class="go">    intercept_scaling=1, max_iter=100,</code>&#13;
<code class="go">    multi_class='ovr', n_jobs=1, penalty='l2',</code>&#13;
<code class="go">    random_state=42, solver='liblinear',</code>&#13;
<code class="go">    tol=0.0001, verbose=0, warm_start=False)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lr</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>&#13;
<code class="go">0.8040712468193384</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lr</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[[</code><code class="mi">0</code><code class="p">]])</code>&#13;
<code class="go">array([1])</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lr</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[[</code><code class="mi">0</code><code class="p">]])</code>&#13;
<code class="go">array([[0.08698937, 0.91301063]])</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lr</code><code class="o">.</code><code class="n">predict_log_proba</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[[</code><code class="mi">0</code><code class="p">]])</code>&#13;
<code class="go">array([[-2.4419694 , -0.09100775]])</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lr</code><code class="o">.</code><code class="n">decision_function</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[[</code><code class="mi">0</code><code class="p">]])</code>&#13;
<code class="go">array([2.35096164])</code></pre>&#13;
&#13;
<p>Instance parameters:</p>&#13;
<dl>&#13;
<dt><code>penalty='l2'</code></dt>&#13;
<dd>&#13;
<p>Penalization norm, <code>'l1'</code> or <code>'l2'</code>.</p>&#13;
</dd>&#13;
<dt><code>dual=False</code></dt>&#13;
<dd>&#13;
<p>Use dual formulation (only with <code>'l2'</code> and <code>'liblinear'</code>).</p>&#13;
</dd>&#13;
<dt><code>C=1.0</code></dt>&#13;
<dd>&#13;
<p>Positive float. Inverse regularization strength. Smaller is stronger regularization.</p>&#13;
</dd>&#13;
<dt><code>fit_intercept=True</code></dt>&#13;
<dd>&#13;
<p>Add bias to the decision function.</p>&#13;
</dd>&#13;
<dt><code>intercept_scaling=1</code></dt>&#13;
<dd>&#13;
<p>If <code>fit_intercept</code> and <code>'liblinear'</code>, scale the intercept.</p>&#13;
</dd>&#13;
<dt><code>max_iter=100</code></dt>&#13;
<dd>&#13;
<p>Maximum number of iterations.</p>&#13;
</dd>&#13;
<dt><code>multi_class='ovr'</code></dt>&#13;
<dd>&#13;
<p>Use one versus rest for each class, or for <code>'multinomial'</code>, train one class.</p>&#13;
</dd>&#13;
<dt><code>class_weight=None</code></dt>&#13;
<dd>&#13;
<p>Dictionary or <code>'balanced'</code>.</p>&#13;
</dd>&#13;
<dt><code>solver='liblinear'</code></dt>&#13;
<dd>&#13;
<p><code>'liblinear'</code> is good for small data. <code>'newton-cg'</code>, <code>'sag'</code>, <code>'saga'</code>, and <code>'lbfgs'</code> are for multiclass data. <code>'liblinear'</code> and <code>'saga'</code> only work with <code>'l1'</code> penalty. The others work with <code>'l2'</code>.</p>&#13;
</dd>&#13;
<dt><code>tol=0.0001</code></dt>&#13;
<dd>&#13;
<p>Stopping tolerance.</p>&#13;
</dd>&#13;
<dt><code>verbose=0</code></dt>&#13;
<dd>&#13;
<p>Be verbose (if nonzero int).</p>&#13;
</dd>&#13;
<dt><code>warm_start=False</code></dt>&#13;
<dd>&#13;
<p>If <code>True</code>, remember previous fit.</p>&#13;
</dd>&#13;
<dt><code>njobs=1</code></dt>&#13;
<dd>&#13;
<p>Number of CPUs to use. <code>-1</code> is all. Only works with <code>multi_class='over'</code> and <code>solver</code> is not <code>'liblinear'</code>.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p class="pagebreak-before">Attributes after fitting:</p>&#13;
<dl>&#13;
<dt><code>coef_</code></dt>&#13;
<dd>&#13;
<p>Decision function coefficients</p>&#13;
</dd>&#13;
<dt><code>intercept_</code></dt>&#13;
<dd>&#13;
<p>Intercept of the decision function</p>&#13;
</dd>&#13;
<dt><code>n_iter_</code></dt>&#13;
<dd>&#13;
<p>Number of iterations</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>The intercept is the log odds of the baseline condition. We can convert it back to a percent accuracy (proportion):</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">lr</code><code class="o">.</code><code class="n">intercept_</code>&#13;
<code class="go">array([-0.62386001])</code></pre>&#13;
&#13;
<p>Using the inverse logit function, we see that the&#13;
baseline for survival is 34%:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="k">def</code> <code class="nf">inv_logit</code><code class="p">(</code><code class="n">p</code><code class="p">):</code>&#13;
<code class="gp">... </code>    <code class="k">return</code> <code class="n">np</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="n">p</code><code class="p">)</code> <code class="o">/</code> <code class="p">(</code><code class="mi">1</code> <code class="o">+</code> <code class="n">np</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="n">p</code><code class="p">))</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">inv_logit</code><code class="p">(</code><code class="n">lr</code><code class="o">.</code><code class="n">intercept_</code><code class="p">)</code>&#13;
<code class="go">array([0.34890406])</code></pre>&#13;
&#13;
<p>We can inspect the coefficients. The inverse logit of the coefficients gives the proportion of the positive cases. In this case, if fare goes up, we are more likely to survive. If sex is male, we are less likely to survive:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">cols</code> <code class="o">=</code> <code class="n">X</code><code class="o">.</code><code class="n">columns</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">col</code><code class="p">,</code> <code class="n">val</code> <code class="ow">in</code> <code class="nb">sorted</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="nb">zip</code><code class="p">(</code><code class="n">cols</code><code class="p">,</code> <code class="n">lr</code><code class="o">.</code><code class="n">coef_</code><code class="p">[</code><code class="mi">0</code><code class="p">]),</code>&#13;
<code class="gp">... </code>    <code class="n">key</code><code class="o">=</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code>&#13;
<code class="gp">... </code>    <code class="n">reverse</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">):</code>&#13;
<code class="gp">... </code>    <code class="k">print</code><code class="p">(</code>&#13;
<code class="gp">... </code>        <code class="n">f</code><code class="s">"{col:10}{val:10.3f} {inv_logit(val):10.3f}"</code>&#13;
<code class="gp">... </code>    <code class="p">)</code>&#13;
<code class="go">fare           0.104      0.526</code>&#13;
<code class="go">parch         -0.062      0.485</code>&#13;
<code class="go">sibsp         -0.274      0.432</code>&#13;
<code class="go">age           -0.296      0.427</code>&#13;
<code class="go">embarked_Q    -0.504      0.377</code>&#13;
<code class="go">embarked_S    -0.507      0.376</code>&#13;
<code class="go">pclass        -0.740      0.323</code>&#13;
<code class="go">sex_male      -2.400      0.083</code></pre>&#13;
&#13;
<p><a data-primary="Yellowbrick" data-secondary="coefficient visualization" data-type="indexterm" id="idm46066895748952"/>Yellowbrick can also visualize the coefficients. This visualizer has&#13;
a <code>relative=True</code> parameter that makes the largest value be 100&#13;
(or -100), and the others are the percentages of that (see <a data-type="xref" href="#idlr1_2">Figure 10-1</a>):<a data-startref="ix_ch10-asciidoc2" data-type="indexterm" id="idm46066895708920"/><a data-startref="ix_ch10-asciidoc1" data-type="indexterm" id="idm46066895708216"/></p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">yellowbrick.features.importances</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">FeatureImportances</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fi_viz</code> <code class="o">=</code> <code class="n">FeatureImportances</code><code class="p">(</code><code class="n">lr</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fi_viz</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fi_viz</code><code class="o">.</code><code class="n">poof</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1001.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="idlr1_2">&#13;
<img alt="Feature importance (relative to largest absolute regression coefficient)." src="assets/mlpr_1001.png"/>&#13;
<h6><span class="label">Figure 10-1. </span>Feature importance (relative to largest absolute regression coefficient).</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Naive Bayes" data-type="sect1"><div class="sect1" id="idm46066896068584">&#13;
<h1>Naive Bayes</h1>&#13;
&#13;
<p><a data-primary="classification" data-secondary="Naive Bayes classifier" data-type="indexterm" id="ix_ch10-asciidoc3"/><a data-primary="Naive Bayes classifier" data-type="indexterm" id="ix_ch10-asciidoc4"/>Naive Bayes is a probabilistic <span class="keep-together">classifier</span> that assumes independence&#13;
between the features of the data. It is popular for text <span class="keep-together">classification</span>&#13;
applications, such as catching spam. One advantage of this model is&#13;
that because it assumes feature independence, it can train a model&#13;
with a small number of samples. (A downside is that it can’t capture&#13;
the interactions between features.) This simple model can also&#13;
work with data that has many features. As such, it serves as a good&#13;
baseline model.</p>&#13;
&#13;
<p><a data-primary="sklearn" data-secondary="Naive Bayes classes" data-type="indexterm" id="idm46066895595384"/>There are three classes in sklearn: <code>GaussianNB</code>, <code>MultinomialNB</code>, and <code>BernoulliNB</code>.&#13;
The first assumes a Gaussian distribution (continuous features with a normal&#13;
distribution), the second is for discrete occurrence counts, and the&#13;
third is for discrete Boolean features.</p>&#13;
&#13;
<p>This model has the following properties:</p>&#13;
<dl>&#13;
<dt>Runtime efficiency</dt>&#13;
<dd>&#13;
<p>Training O(Nd), where N is the number of training examples and d is dimensionality. Testing O(cd), where c is the number of classes.</p>&#13;
</dd>&#13;
<dt>Preprocess data</dt>&#13;
<dd>&#13;
<p>Assumes that data is independent. Should perform better after removing colinear columns. For continuous numerical data, might be good to bin data. Gaussian assumes normal distribution, and you might need to transform data to convert to normal distribution.</p>&#13;
</dd>&#13;
<dt>Prevent overfitting</dt>&#13;
<dd>&#13;
<p>Exhibits high bias and low variance (ensembles won’t reduce variance).</p>&#13;
</dd>&#13;
<dt>Interpret results</dt>&#13;
<dd>&#13;
<p>Percentage is the likelihood that a sample belongs to a class based on priors.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p class="pagebreak-before">Here is an example using this model:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.naive_bayes</code> <code class="kn">import</code> <code class="n">GaussianNB</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">nb</code> <code class="o">=</code> <code class="n">GaussianNB</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">nb</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
<code class="go">GaussianNB(priors=None, var_smoothing=1e-09)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">nb</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>&#13;
<code class="go">0.7837150127226463</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">nb</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[[</code><code class="mi">0</code><code class="p">]])</code>&#13;
<code class="go">array([1])</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">nb</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[[</code><code class="mi">0</code><code class="p">]])</code>&#13;
<code class="go">array([[2.17472227e-08, 9.99999978e-01]])</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">nb</code><code class="o">.</code><code class="n">predict_log_proba</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[[</code><code class="mi">0</code><code class="p">]])</code>&#13;
<code class="go">array([[-1.76437798e+01, -2.17472227e-08]])</code></pre>&#13;
&#13;
<p>Instance parameters:</p>&#13;
<dl>&#13;
<dt><code>priors=None</code></dt>&#13;
<dd>&#13;
<p>Prior probabilities of classes.</p>&#13;
</dd>&#13;
<dt><code>var_smoothing=1e-9</code></dt>&#13;
<dd>&#13;
<p>Added to variance for stable calculations.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Attributes after fitting:</p>&#13;
<dl>&#13;
<dt><code>class_prior_</code></dt>&#13;
<dd>&#13;
<p>Probabilities of classes</p>&#13;
</dd>&#13;
<dt><code>class_count_</code></dt>&#13;
<dd>&#13;
<p>Counts of classes</p>&#13;
</dd>&#13;
<dt><code>theta_</code></dt>&#13;
<dd>&#13;
<p>Mean of each column per class</p>&#13;
</dd>&#13;
<dt><code>sigma_</code></dt>&#13;
<dd>&#13;
<p>Variance of each column per class</p>&#13;
</dd>&#13;
<dt><code>epsilon_</code></dt>&#13;
<dd>&#13;
<p>Additive value to each variance</p>&#13;
</dd>&#13;
</dl>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p><a data-primary="zero probability problem" data-type="indexterm" id="idm46066895464408"/>These models are susceptible to the <em>zero probability problem</em>. If you&#13;
try to classify a new sample that has no training data, it will have a zero&#13;
probability. <a data-primary="Laplace smoothing" data-type="indexterm" id="idm46066895462840"/><a data-primary="sklearn" data-secondary="Laplace smoothing with" data-type="indexterm" id="idm46066895462168"/>One solution is to use <em>Laplace smoothing</em>. Sklearn&#13;
controls this with the <code>alpha</code> parameter, which defaults to <code>1</code> and enables&#13;
smoothing on the <code>MultinomialNB</code> and <code>BernoulliNB</code> models.<a data-startref="ix_ch10-asciidoc4" data-type="indexterm" id="idm46066895458840"/><a data-startref="ix_ch10-asciidoc3" data-type="indexterm" id="idm46066895458104"/></p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Support Vector Machine" data-type="sect1"><div class="sect1" id="idm46066895600680">&#13;
<h1>Support Vector Machine</h1>&#13;
&#13;
<p><a data-primary="classification" data-secondary="support vector machine" data-type="indexterm" id="ix_ch10-asciidoc5"/><a data-primary="support vector machines (SVMs)" data-type="indexterm" id="ix_ch10-asciidoc6"/>A Support Vector Machine (SVM) is an algorithm that tries to fit a line (or plane&#13;
or hyperplane) between the different classes that maximizes the distance from&#13;
the line to the points of the classes. In this way it tries to find a robust&#13;
separation between the classes. The <em>support vectors</em> are the&#13;
points of the edge of the dividing hyperplane.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p><a data-primary="sklearn" data-secondary="SVM implementations in" data-type="indexterm" id="idm46066895451976"/>There are a few different SVM implementations in sklearn. <code>SVC</code> wraps the <code>libsvm</code>&#13;
library, while <code>LinearSVC</code> wraps the <code>liblinear</code> library.</p>&#13;
&#13;
<p>There is also the <code>linear_model.SGDClassifier</code>, which implements SVM when using the default&#13;
<code>loss</code> parameter. This chapter will describe the first implementation.</p>&#13;
</div>&#13;
&#13;
<p><a data-primary="kernel trick" data-type="indexterm" id="idm46066895447384"/>SVM generally performs well and&#13;
can support linear spaces or nonlinear spaces by using a <em>kernel trick</em>.&#13;
The kernel trick is the idea that we can create a decision boundary in a new&#13;
dimension by minimizing a formula that is easier to calculate than actually&#13;
mapping the points to the new dimension. The default kernel is the Radial Basis Function (<code>'rbf'</code>),&#13;
which is controlled by the <code>gamma</code> parameter and can map an input space&#13;
into a high dimensional space.</p>&#13;
&#13;
<p>SVMs have the following properties:</p>&#13;
<dl>&#13;
<dt>Runtime efficiency</dt>&#13;
<dd>&#13;
<p>The scikit-learn implementation is O(n⁴), so it can be hard to scale to large sizes. Using a linear kernel or the <code>LinearSVC</code> model can improve the runtime performance at perhaps the cost of accuracy. Upping the <code>cache_size</code> parameter can bring that down to O(n³).</p>&#13;
</dd>&#13;
<dt>Preprocess data</dt>&#13;
<dd>&#13;
<p>The algorithm is not scale invariant. Standardizing the data is highly recommended.</p>&#13;
</dd>&#13;
<dt>Prevent overfitting</dt>&#13;
<dd>&#13;
<p>The <code>C</code> (penalty parameter) controls regularization. A smaller value allows for a smaller margin in the hyperplane. A higher value for <code>gamma</code> will tend to overfit the training data. The <code>LinearSVC</code> model supports a <code>loss</code> and <code>penalty</code> parameter to support regularization.</p>&#13;
</dd>&#13;
<dt>Interpret results</dt>&#13;
<dd>&#13;
<p>Inspect <code>.support_vectors_</code>, though these are hard to explain. With linear kernels, you can inspect <code>.coef_</code>.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Here is an example using scikit-learn’s SVM implementation:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="kn">import</code> <code class="n">SVC</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">svc</code> <code class="o">=</code> <code class="n">SVC</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">,</code> <code class="n">probability</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">svc</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
<code class="go">SVC(C=1.0, cache_size=200, class_weight=None,</code>&#13;
<code class="go">  coef0=0.0, decision_function_shape='ovr',</code>&#13;
<code class="go">  degree=3, gamma='auto', kernel='rbf',</code>&#13;
<code class="go">  max_iter=-1, probability=True, random_state=42,</code>&#13;
<code class="go">  shrinking=True, tol=0.001, verbose=False)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">svc</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>&#13;
<code class="go">0.8015267175572519</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">svc</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[[</code><code class="mi">0</code><code class="p">]])</code>&#13;
<code class="go">array([1])</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">svc</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[[</code><code class="mi">0</code><code class="p">]])</code>&#13;
<code class="go">array([[0.15344656, 0.84655344]])</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">svc</code><code class="o">.</code><code class="n">predict_log_proba</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[[</code><code class="mi">0</code><code class="p">]])</code>&#13;
<code class="go">array([[-1.87440289, -0.16658195]])</code></pre>&#13;
&#13;
<p>To get probability, use <code>probability=True</code>, which will slow down fitting of the&#13;
model.</p>&#13;
&#13;
<p>This is similar to a perceptron, but will find the maximum margin. If the data is&#13;
not linearly separable, it will minimize the error. Alternatively, a&#13;
different kernel may be used.</p>&#13;
&#13;
<p>Instance parameters:</p>&#13;
<dl>&#13;
<dt><code>C=1.0</code></dt>&#13;
<dd>&#13;
<p>The penalty parameter. The smaller the value, the tighter the decision boundary (more overfitting).</p>&#13;
</dd>&#13;
<dt><code>cache_size=200</code></dt>&#13;
<dd>&#13;
<p>Cache size (MB). Bumping this up can improve training time on large datasets.</p>&#13;
</dd>&#13;
<dt><code>class_weight=None</code></dt>&#13;
<dd>&#13;
<p>Dictionary or <code>'balanced'</code>. Use dictionary to set <code>C</code> for each class.</p>&#13;
</dd>&#13;
<dt><code>coef0=0.0</code></dt>&#13;
<dd>&#13;
<p>Independent term for poly and sigmoid kernels.</p>&#13;
</dd>&#13;
<dt><code>decision_function_shape='ovr'</code></dt>&#13;
<dd>&#13;
<p>Use one versus rest (<code>'ovr'</code>) or one versus one.</p>&#13;
</dd>&#13;
<dt><code>degree=3</code></dt>&#13;
<dd>&#13;
<p>Degree for polynomial kernel.</p>&#13;
</dd>&#13;
<dt><code>gamma='auto'</code></dt>&#13;
<dd>&#13;
<p>Kernel coefficient. Can be a number, <code>'scale'</code> (default in 0.22, 1 / (<code>num features</code> * <code>X.std()</code> ) ), or <code>'auto'</code> (default prior, 1 / <code>num features</code>). A lower value leads to overfitting the training data.</p>&#13;
</dd>&#13;
<dt><code>kernel='rbf'</code></dt>&#13;
<dd>&#13;
<p>Kernel type: <code>'linear'</code>, <code>'poly'</code>, <code>'rbf'</code> (default), <code>'sigmoid'</code>, <code>'precomputed'</code>, or a function.</p>&#13;
</dd>&#13;
<dt><code>max_iter=-1</code></dt>&#13;
<dd>&#13;
<p>Maximum number of iterations for solver. -1 for no limit.</p>&#13;
</dd>&#13;
<dt><code>probability=False</code></dt>&#13;
<dd>&#13;
<p>Enable probability estimation. Slows down training.</p>&#13;
</dd>&#13;
<dt><code>random_state=None</code></dt>&#13;
<dd>&#13;
<p>Random seed.</p>&#13;
</dd>&#13;
<dt><code>shrinking=True</code></dt>&#13;
<dd>&#13;
<p>Use shrinking heuristic.</p>&#13;
</dd>&#13;
<dt><code>tol=0.001</code></dt>&#13;
<dd>&#13;
<p>Stopping tolerance.</p>&#13;
</dd>&#13;
<dt><code>verbose=False</code></dt>&#13;
<dd>&#13;
<p>Verbosity.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Attributes after fitting:</p>&#13;
<dl>&#13;
<dt><code>support_</code></dt>&#13;
<dd>&#13;
<p>Support vector indices</p>&#13;
</dd>&#13;
<dt><code>support_vectors_</code></dt>&#13;
<dd>&#13;
<p>Support vectors</p>&#13;
</dd>&#13;
<dt><code>n_support_vectors_</code></dt>&#13;
<dd>&#13;
<p>Count of per-class support vectors</p>&#13;
</dd>&#13;
<dt><code>coef_</code></dt>&#13;
<dd>&#13;
<p>Coefficients (for linear) kernel<a data-startref="ix_ch10-asciidoc6" data-type="indexterm" id="idm46066895266088"/><a data-startref="ix_ch10-asciidoc5" data-type="indexterm" id="idm46066895265384"/></p>&#13;
</dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="K-Nearest Neighbor" data-type="sect1"><div class="sect1" id="idm46066895456840">&#13;
<h1>K-Nearest Neighbor</h1>&#13;
&#13;
<p><a data-primary="classification" data-secondary="k-nearest neighbor" data-type="indexterm" id="ix_ch10-asciidoc7"/><a data-primary="k-nearest neighbor (KNN)" data-type="indexterm" id="ix_ch10-asciidoc8"/>The K-Nearest Neighbor (KNN) algorithm classifies based on distance to some number (k) of&#13;
training samples. <a data-primary="instance-based learning" data-type="indexterm" id="idm46066895260552"/>The algorithm family is called <em>instance-based</em> learning as there are no&#13;
parameters to learn. This model assumes that distance is sufficient for inference; otherwise&#13;
it makes no assumptions about the underlying data or&#13;
its distributions.</p>&#13;
&#13;
<p>The tricky part is selecting the appropriate k&#13;
value. Also, the curse of dimensionality can hamper distance metrics as there&#13;
is little difference in high dimensions between nearest and farthest neighbor.</p>&#13;
&#13;
<p>Nearest neighbor models have the following properties:</p>&#13;
<dl>&#13;
<dt>Runtime efficiency</dt>&#13;
<dd>&#13;
<p>Training O(1), but need to store data. Testing O(Nd) where N is the number of training examples and d is dimensionality.</p>&#13;
</dd>&#13;
<dt>Preprocess data</dt>&#13;
<dd>&#13;
<p>Yes, distance-based calculations perform better when standardized.</p>&#13;
</dd>&#13;
<dt>Prevent overfitting</dt>&#13;
<dd>&#13;
<p>Raise <code>n_neighbors</code>. Change <code>p</code> for L1 or L2 metric.</p>&#13;
</dd>&#13;
<dt>Interpret results</dt>&#13;
<dd>&#13;
<p>Interpret the k-nearest neighbors to the sample (using the <code>.kneighbors</code> method). Those neighbors (if you can explain them) explain your result.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Here is an example of using the model:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.neighbors</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">KNeighborsClassifier</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">knc</code> <code class="o">=</code> <code class="n">KNeighborsClassifier</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">knc</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
<code class="go">KNeighborsClassifier(algorithm='auto',</code>&#13;
<code class="go">  leaf_size=30, metric='minkowski',</code>&#13;
<code class="go">  metric_params=None, n_jobs=1, n_neighbors=5,</code>&#13;
<code class="go">  p=2, weights='uniform')</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">knc</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>&#13;
<code class="go">0.7837150127226463</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">knc</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[[</code><code class="mi">0</code><code class="p">]])</code>&#13;
<code class="go">array([1])</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">knc</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[[</code><code class="mi">0</code><code class="p">]])</code>&#13;
<code class="go">array([[0., 1.]])</code></pre>&#13;
&#13;
<p class="pagebreak-before">Attributes:</p>&#13;
<dl>&#13;
<dt><code>algorithm='auto'</code></dt>&#13;
<dd>&#13;
<p>Can be <code>'brute'</code>, <code>'ball_tree'</code>, or <code>'kd_tree'</code>.</p>&#13;
</dd>&#13;
<dt><code>leaf_size=30</code></dt>&#13;
<dd>&#13;
<p>Used for tree algorithms.</p>&#13;
</dd>&#13;
<dt><code>metric='minkowski'</code></dt>&#13;
<dd>&#13;
<p>Distance metric.</p>&#13;
</dd>&#13;
<dt><code>metric_params=None</code></dt>&#13;
<dd>&#13;
<p>Additional dictionary of parameters for custom metric function.</p>&#13;
</dd>&#13;
<dt><code>n_jobs=1</code></dt>&#13;
<dd>&#13;
<p>Number of CPUs.</p>&#13;
</dd>&#13;
<dt><code>n_neighbors=5</code></dt>&#13;
<dd>&#13;
<p>Number of neighbors.</p>&#13;
</dd>&#13;
<dt><code>p=2</code></dt>&#13;
<dd>&#13;
<p>Minkowski power parameter: 1 = manhattan (L1). 2 = Euclidean (L2).</p>&#13;
</dd>&#13;
<dt><code>weights='uniform'</code></dt>&#13;
<dd>&#13;
<p>Can be <code>'distance'</code>, in which case, closer points have more influence.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Distance metrics include: <code>'euclidean'</code>, <code>'manhattan'</code>, <span class="keep-together"><code>'chebyshev'</code></span>, <code>'minkowski'</code>, <code>'wminkowski'</code>, <code>'seuclidean'</code>, <span class="keep-together"><code>'mahalanobis'</code>,</span> <code>'haversine'</code>, <code>'hamming'</code>, <code>'canberra'</code>, <span class="keep-together"><code>'braycurtis'</code></span>, <code>'jaccard'</code>, <code>'matching'</code>, <code>'dice'</code>, <span class="keep-together"><code>'rogerstanimoto'</code></span>, <code>'russellrao'</code>, <code>'sokalmichener'</code>, <span class="keep-together"><code>'sokalsneath'</code></span>, or a callable (user defined).</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>If k is an even number and the neighbors are split, the result depends&#13;
on the order of the training data.<a data-startref="ix_ch10-asciidoc8" data-type="indexterm" id="idm46066895145080"/><a data-startref="ix_ch10-asciidoc7" data-type="indexterm" id="idm46066895144376"/></p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Decision Tree" data-type="sect1"><div class="sect1" id="idm46066895263992">&#13;
<h1>Decision Tree</h1>&#13;
&#13;
<p><a data-primary="classification" data-secondary="decision tree" data-type="indexterm" id="ix_ch10-asciidoc9"/><a data-primary="decision tree" data-type="indexterm" id="ix_ch10-asciidoc10"/>A decision tree is like going to a doctor who asks a series of questions&#13;
to determine the cause of your symptoms. We can use a process to create a&#13;
decision tree and have a series of questions to predict a target&#13;
class. The advantages of this model include support for nonnumeric data&#13;
(in some implementations), little data preparation (no need for scaling),&#13;
support for dealing with nonlinear relationships, feature importances are revealed,&#13;
and it is easy to explain.</p>&#13;
&#13;
<p><a data-primary="CART (classification and regression trees) algorithm" data-type="indexterm" id="idm46066895138632"/><a data-primary="classification and regression trees (CART) algorithm" data-type="indexterm" id="idm46066895137896"/>The default algorithm used for creation is called the classification and regression tree (CART). It uses the Gini impurity or index&#13;
measure to construct decisions.&#13;
This is done by looping over the features and finding the value&#13;
that gives the lowest probability of misclassifying.</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>The default values will lead to a fully grown (read overfit) tree. Use a mechanism&#13;
such as <code>max_depth</code> and cross-validation to control for this.</p>&#13;
</div>&#13;
&#13;
<p>Decision trees have the following properties:</p>&#13;
<dl>&#13;
<dt>Runtime efficiency</dt>&#13;
<dd>&#13;
<p>For creation, loop over each of the m features, and sort all n samples, O(mn log n). For predicting, you walk the tree, O( height).</p>&#13;
</dd>&#13;
<dt>Preprocess data</dt>&#13;
<dd>&#13;
<p>Scaling is not necessary. Need to get rid of missing values and convert to numeric.</p>&#13;
</dd>&#13;
<dt>Prevent overfitting</dt>&#13;
<dd>&#13;
<p>Set <code>max_depth</code> to a lower number, raise <code>min_</code><span class="keep-together"><code>impurity_decrease</code></span>.</p>&#13;
</dd>&#13;
<dt>Interpret results</dt>&#13;
<dd>&#13;
<p>Can step through the tree of choices. Because there are steps, a tree is bad at dealing with linear relationships (a small change in a number can go down a different path). The tree is also highly dependent on the training data. A small change can change the whole tree.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Here is an example using the scikit-learn library:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="kn">import</code> <code class="n">DecisionTreeClassifier</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">dt</code> <code class="o">=</code> <code class="n">DecisionTreeClassifier</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">,</code> <code class="n">max_depth</code><code class="o">=</code><code class="mi">3</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">dt</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
<code class="go">DecisionTreeClassifier(class_weight=None,</code>&#13;
<code class="go">  criterion='gini', max_depth=None,</code>&#13;
<code class="go">  max_features=None, max_leaf_nodes=None,</code>&#13;
<code class="go">  min_impurity_decrease=0.0,</code>&#13;
<code class="go">  min_impurity_split=None,</code>&#13;
<code class="go">  min_samples_leaf=1, min_samples_split=2,</code>&#13;
<code class="go">  min_weight_fraction_leaf=0.0, presort=False,</code>&#13;
<code class="go">  random_state=42, splitter='best')</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">dt</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>&#13;
<code class="go">0.8142493638676844</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">dt</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[[</code><code class="mi">0</code><code class="p">]])</code>&#13;
<code class="go">array([1])</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">dt</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[[</code><code class="mi">0</code><code class="p">]])</code>&#13;
<code class="go">array([[0.02040816, 0.97959184]])</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">dt</code><code class="o">.</code><code class="n">predict_log_proba</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[[</code><code class="mi">0</code><code class="p">]])</code>&#13;
<code class="go">array([[-3.8918203 , -0.02061929]])</code></pre>&#13;
&#13;
<p>Instance parameters:</p>&#13;
<dl>&#13;
<dt><code>class_weight=None</code></dt>&#13;
<dd>&#13;
<p>Weights for class in dictionary. <code>'balanced'</code> will set values to the inverse proportion of class frequencies. Default is a value of 1 for each class. For multiclass, need a list of dictionaries, one-versus-rest (OVR) for each class.</p>&#13;
</dd>&#13;
<dt><code>criterion='gini'</code></dt>&#13;
<dd>&#13;
<p>Splitting function, <code>'gini'</code> or <code>'entropy'</code>.</p>&#13;
</dd>&#13;
<dt><code>max_depth=None</code></dt>&#13;
<dd>&#13;
<p>Depth of tree. Default will build until the leaves contain less than <code>min_samples_split</code>.</p>&#13;
</dd>&#13;
<dt><code>max_features=None</code></dt>&#13;
<dd>&#13;
<p>Number of features to examine for split. Default is all.</p>&#13;
</dd>&#13;
<dt><code>max_leaf_nodes=None</code></dt>&#13;
<dd>&#13;
<p>Limit the number of leaves. Default is unlimited.</p>&#13;
</dd>&#13;
<dt><code>min_impurity_decrease=0.0</code></dt>&#13;
<dd>&#13;
<p>Split node if a split will decrease impurity &gt;= value.</p>&#13;
</dd>&#13;
<dt><code>min_impurity_split=None</code></dt>&#13;
<dd>&#13;
<p>Deprecated.</p>&#13;
</dd>&#13;
<dt><code>min_samples_leaf=1</code></dt>&#13;
<dd>&#13;
<p>Minimum number of samples at each leaf.</p>&#13;
</dd>&#13;
<dt><code>min_samples_split=2</code></dt>&#13;
<dd>&#13;
<p>Minimum number of samples required to split a node.</p>&#13;
</dd>&#13;
<dt><code>min_weight_fraction_leaf=0.0</code></dt>&#13;
<dd>&#13;
<p>Minimum sum total of weights required for leaf nodes.</p>&#13;
</dd>&#13;
<dt><code>presort=False</code></dt>&#13;
<dd>&#13;
<p>May speed up training with a small dataset or restricted depth if set to <code>True</code>.</p>&#13;
</dd>&#13;
<dt><code>random_state=None</code></dt>&#13;
<dd>&#13;
<p>Random seed.</p>&#13;
</dd>&#13;
<dt><code>splitter='best'</code></dt>&#13;
<dd>&#13;
<p>Use <code>'random'</code> or <code>'best'</code>.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Attributes after fitting:</p>&#13;
<dl>&#13;
<dt><code>classes_</code></dt>&#13;
<dd>&#13;
<p>Class labels</p>&#13;
</dd>&#13;
<dt><code>feature_importances_</code></dt>&#13;
<dd>&#13;
<p>Array of Gini importance</p>&#13;
</dd>&#13;
<dt><code>n_classes_</code></dt>&#13;
<dd>&#13;
<p>Number of classes</p>&#13;
</dd>&#13;
<dt><code>n_features_</code></dt>&#13;
<dd>&#13;
<p>Number of features</p>&#13;
</dd>&#13;
<dt><code>tree_</code></dt>&#13;
<dd>&#13;
<p>Underlying tree object</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>View the tree with this code (see <a data-type="xref" href="#id21">Figure 10-2</a>):</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">pydotplus</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">io</code> <code class="kn">import</code> <code class="n">StringIO</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="kn">import</code> <code class="n">export_graphviz</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">dot_data</code> <code class="o">=</code> <code class="n">StringIO</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">tree</code><code class="o">.</code><code class="n">export_graphviz</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">dt</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">out_file</code><code class="o">=</code><code class="n">dot_data</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">feature_names</code><code class="o">=</code><code class="n">X</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">class_names</code><code class="o">=</code><code class="p">[</code><code class="s">"Died"</code><code class="p">,</code> <code class="s">"Survived"</code><code class="p">],</code>&#13;
<code class="gp">... </code>    <code class="n">filled</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">g</code> <code class="o">=</code> <code class="n">pydotplus</code><code class="o">.</code><code class="n">graph_from_dot_data</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">dot_data</code><code class="o">.</code><code class="n">getvalue</code><code class="p">()</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">g</code><code class="o">.</code><code class="n">write_png</code><code class="p">(</code><code class="s">"images/mlpr_1002.png"</code><code class="p">)</code></pre>&#13;
&#13;
<p><a data-primary="Jupyter" data-secondary="decision tree creation" data-type="indexterm" id="idm46066894900344"/>For Jupyter, use:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="go">from IPython.display import Image</code>&#13;
<code class="go">Image(g.create_png())</code></pre>&#13;
<div class="landscape">&#13;
&#13;
<figure><div class="figure" id="id21">&#13;
<img alt="Decision Tree." src="assets/mlpr_1002.png"/>&#13;
<h6><span class="label">Figure 10-2. </span>Decision tree.</h6>&#13;
</div></figure>&#13;
</div>&#13;
&#13;
<p><a data-primary="dtreeviz" data-type="indexterm" id="idm46066894885832"/>The <a href="https://github.com/parrt/dtreeviz">dtreeviz package</a> can aid in understanding how the decision tree works.&#13;
It creates a tree with labeled histograms, which gives valuable insight (see <a data-type="xref" href="#iddtviz">Figure 10-3</a>).&#13;
Here is an example. In Jupyter we can just display the <code>viz</code> object directly.&#13;
If we are working from a script, we can call the <code>.save</code> method to create a PDF,&#13;
SVG, or PNG:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">viz</code> <code class="o">=</code> <code class="n">dtreeviz</code><code class="o">.</code><code class="n">trees</code><code class="o">.</code><code class="n">dtreeviz</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">dt</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">X</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">y</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">target_name</code><code class="o">=</code><code class="s">"survived"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">feature_names</code><code class="o">=</code><code class="n">X</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">class_names</code><code class="o">=</code><code class="p">[</code><code class="s">"died"</code><code class="p">,</code> <code class="s">"survived"</code><code class="p">],</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">viz</code></pre>&#13;
<div class="landscape">&#13;
&#13;
<figure><div class="figure" id="iddtviz">&#13;
<img alt="dtreeviz output." src="assets/mlpr_1003.png"/>&#13;
<h6><span class="label">Figure 10-3. </span>dtreeviz output.</h6>&#13;
</div></figure>&#13;
</div>&#13;
&#13;
<p>Feature importance<a data-primary="feature importance" data-secondary="decision trees" data-type="indexterm" id="idm46066894739304"/> showing Gini importance (reduction of error by using that feature):</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">col</code><code class="p">,</code> <code class="n">val</code> <code class="ow">in</code> <code class="nb">sorted</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="nb">zip</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code> <code class="n">dt</code><code class="o">.</code><code class="n">feature_importances_</code><code class="p">),</code>&#13;
<code class="gp">... </code>    <code class="n">key</code><code class="o">=</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code>&#13;
<code class="gp">... </code>    <code class="n">reverse</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)[:</code><code class="mi">5</code><code class="p">]:</code>&#13;
<code class="gp">... </code>    <code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s">"{col:10}{val:10.3f}"</code><code class="p">)</code>&#13;
<code class="go">sex_male       0.607</code>&#13;
<code class="go">pclass         0.248</code>&#13;
<code class="go">sibsp          0.052</code>&#13;
<code class="go">fare           0.050</code>&#13;
<code class="go">age            0.043</code></pre>&#13;
&#13;
<p><a data-primary="Yellowbrick" data-secondary="feature importance visualization" data-type="indexterm" id="idm46066894716776"/>You can also use Yellowbrick to visualize feature importance (see <a data-type="xref" href="#id22">Figure 10-4</a>):<a data-startref="ix_ch10-asciidoc10" data-type="indexterm" id="idm46066894670616"/><a data-startref="ix_ch10-asciidoc9" data-type="indexterm" id="idm46066894669912"/></p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">yellowbrick.features.importances</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">FeatureImportances</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fi_viz</code> <code class="o">=</code> <code class="n">FeatureImportances</code><code class="p">(</code><code class="n">dt</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fi_viz</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fi_viz</code><code class="o">.</code><code class="n">poof</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1004.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id22">&#13;
<img alt="Feature importance (Gini coefficient) for decision tree (normalized to male importance)." src="assets/mlpr_1004.png"/>&#13;
<h6><span class="label">Figure 10-4. </span>Feature importance (Gini coefficient) for decision tree (normalized to male importance).</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Random Forest" data-type="sect1"><div class="sect1" id="idm46066895142664">&#13;
<h1>Random Forest</h1>&#13;
&#13;
<p><a data-primary="classification" data-secondary="random forest" data-type="indexterm" id="ix_ch10-asciidoc11"/><a data-primary="decision tree" data-secondary="random forest and" data-type="indexterm" id="ix_ch10-asciidoc12"/><a data-primary="random forest" data-type="indexterm" id="ix_ch10-asciidoc13"/>A random forest is an ensemble of decision trees. <a data-primary="bagging" data-type="indexterm" id="idm46066894572088"/>It uses <em>bagging</em>&#13;
to correct the tendency of decision trees to overfit. By creating&#13;
many trees trained on random subsamples of the samples and random&#13;
features of the data, the variance is lowered.</p>&#13;
&#13;
<p><a data-primary="out-of-bag (OOB) error" data-type="indexterm" id="idm46066894570392"/>Because they train on subsamples of the data, random forests can evaluate OOB error and evaluate performance. They can also&#13;
track feature importance by averaging the feature importance over&#13;
all of the trees.</p>&#13;
&#13;
<p>The intuition for understanding bagging comes from a 1785 essay by Marquis de Condorcet. The essence is that if you are creating&#13;
a jury, you should add anyone who has a greater than 50% chance of delivering&#13;
the correct verdict and then average their decisions. Every time you&#13;
add another member (and their selection process is independent of&#13;
the others), you will get a better result.</p>&#13;
&#13;
<p>The idea with random forests is to create a “forest” of decision trees&#13;
trained on different columns of the training data. If each tree has a&#13;
better than 50% chance of correct classification, you should incorporate&#13;
its prediction. The random forest&#13;
has been an excellent tool for both classification and regression,&#13;
though it has recently fallen out of favor for gradient-boosted&#13;
trees.</p>&#13;
&#13;
<p>It has the following properties:</p>&#13;
<dl>&#13;
<dt>Runtime efficiency</dt>&#13;
<dd>&#13;
<p>Need to create j random trees. This can be done in parallel using <code>n_jobs</code>. Complexity for each tree is O(mn log n), where n is the number of samples and m is the number of features. For creation, loop over each of the m features, and sort all n samples, O(mn log n). For predicting, walk the tree O( height).</p>&#13;
</dd>&#13;
<dt>Preprocess data</dt>&#13;
<dd>&#13;
<p>Not necessary.</p>&#13;
</dd>&#13;
<dt>Prevent overfitting</dt>&#13;
<dd>&#13;
<p>Add more trees (<code>n_estimators</code>). Use lower <code>max_depth</code>.</p>&#13;
</dd>&#13;
<dt>Interpret results</dt>&#13;
<dd>&#13;
<p>Supports feature importance, but we don’t have a single decision tree that we can walk through. Can inspect single trees from the ensemble.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Here is an example:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">RandomForestClassifier</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rf</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
<code class="go">RandomForestClassifier(bootstrap=True,</code>&#13;
<code class="go">  class_weight=None, criterion='gini',</code>&#13;
<code class="go">  max_depth=None, max_features='auto',</code>&#13;
<code class="go">  max_leaf_nodes=None, min_impurity_decrease=0.0,</code>&#13;
<code class="go">  min_impurity_split=None, min_samples_leaf=1,</code>&#13;
<code class="go">  min_samples_split=2,</code>&#13;
<code class="go">  min_weight_fraction_leaf=0.0,</code>&#13;
<code class="go">  n_estimators=10, n_jobs=1, oob_score=False,</code>&#13;
<code class="go">  random_state=42, verbose=0, warm_start=False)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rf</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>&#13;
<code class="go">0.7862595419847328</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rf</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[[</code><code class="mi">0</code><code class="p">]])</code>&#13;
<code class="go">array([1])</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rf</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[[</code><code class="mi">0</code><code class="p">]])</code>&#13;
<code class="go">array([[0., 1.]])</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rf</code><code class="o">.</code><code class="n">predict_log_proba</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[[</code><code class="mi">0</code><code class="p">]])</code>&#13;
<code class="go">array([[-inf,   0.]])</code></pre>&#13;
&#13;
<p>Instance parameters (these options mirror the decision tree):</p>&#13;
<dl>&#13;
<dt><code>bootstrap=True</code></dt>&#13;
<dd>&#13;
<p>Bootstrap when building trees.</p>&#13;
</dd>&#13;
<dt><code>class_weight=None</code> </dt>&#13;
<dd>&#13;
<p>Weights for class in dictionary. <code>'balanced'</code> will set values to the inverse proportion of class frequencies. Default is a value of 1 for each class. For multiclass, need a list of dictionaries (OVR) for each class.</p>&#13;
</dd>&#13;
<dt><code>criterion='gini'</code></dt>&#13;
<dd>&#13;
<p>Splitting function, <code>'gini'</code> or <code>'entropy'</code>.</p>&#13;
</dd>&#13;
<dt><code>max_depth=None</code></dt>&#13;
<dd>&#13;
<p>Depth of tree. Default will build until leaves contain less than <code>min_samples_split</code>.</p>&#13;
</dd>&#13;
<dt><code>max_features='auto'</code></dt>&#13;
<dd>&#13;
<p>Number of features to examine for split. Default is all.</p>&#13;
</dd>&#13;
<dt><code>max_leaf_nodes=None</code></dt>&#13;
<dd>&#13;
<p>Limit the number of leaves. Default is unlimited.</p>&#13;
</dd>&#13;
<dt><code>min_impurity_decrease=0.0</code></dt>&#13;
<dd>&#13;
<p>Split node if a split will decrease impurity &gt;= value.</p>&#13;
</dd>&#13;
<dt><code>min_impurity_split=None</code></dt>&#13;
<dd>&#13;
<p>Deprecated.</p>&#13;
</dd>&#13;
<dt><code>min_samples_leaf=1</code></dt>&#13;
<dd>&#13;
<p>Minimum number of samples at each leaf.</p>&#13;
</dd>&#13;
<dt><code>min_samples_split=2</code></dt>&#13;
<dd>&#13;
<p>Minimum number of samples required to split a node.&#13;
<code>min_weight_fraction_leaf=0.0</code>-  Minimum sum total of weights required for leaf nodes.</p>&#13;
</dd>&#13;
<dt>* <code>n_estimators=10</code></dt>&#13;
<dd>&#13;
<p>Number of trees in the forest.</p>&#13;
</dd>&#13;
<dt><code>n_jobs=1</code></dt>&#13;
<dd>&#13;
<p>Number of jobs for fitting and predicting.</p>&#13;
</dd>&#13;
<dt><code>oob_score=False</code></dt>&#13;
<dd>&#13;
<p>Whether to estimate <code>oob_score</code>.</p>&#13;
</dd>&#13;
<dt><code>random_state=None</code></dt>&#13;
<dd>&#13;
<p>Random seed.</p>&#13;
</dd>&#13;
<dt><code>verbose=0</code></dt>&#13;
<dd>&#13;
<p>Verbosity.</p>&#13;
</dd>&#13;
<dt><code>warm_start=False</code></dt>&#13;
<dd>&#13;
<p>Fit a new forest or use the existing one.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Attributes after fitting:</p>&#13;
<dl>&#13;
<dt><code>classes_</code></dt>&#13;
<dd>&#13;
<p>Class labels.</p>&#13;
</dd>&#13;
<dt><code>feature_importances_</code></dt>&#13;
<dd>&#13;
<p>Array of Gini importance.</p>&#13;
</dd>&#13;
<dt><code>n_classes_</code></dt>&#13;
<dd>&#13;
<p>Number of classes.</p>&#13;
</dd>&#13;
<dt><code>n_features_</code></dt>&#13;
<dd>&#13;
<p>Number of features.</p>&#13;
</dd>&#13;
<dt><code>oob_score_</code></dt>&#13;
<dd>&#13;
<p>OOB score. Average accuracy for each observation not used in trees.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p class="pagebreak-before"><a data-primary="feature importance" data-secondary="random forests" data-type="indexterm" id="ix_ch10-asciidoc14"/><a data-primary="Gini importance" data-type="indexterm" id="ix_ch10-asciidoc15"/>Feature importance showing Gini importance (reduction of error by using that feature):</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">col</code><code class="p">,</code> <code class="n">val</code> <code class="ow">in</code> <code class="nb">sorted</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="nb">zip</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code> <code class="n">rf</code><code class="o">.</code><code class="n">feature_importances_</code><code class="p">),</code>&#13;
<code class="gp">... </code>    <code class="n">key</code><code class="o">=</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code>&#13;
<code class="gp">... </code>    <code class="n">reverse</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)[:</code><code class="mi">5</code><code class="p">]:</code>&#13;
<code class="gp">... </code>    <code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s">"{col:10}{val:10.3f}"</code><code class="p">)</code>&#13;
<code class="go">age            0.285</code>&#13;
<code class="go">fare           0.268</code>&#13;
<code class="go">sex_male       0.232</code>&#13;
<code class="go">pclass         0.077</code>&#13;
<code class="go">sibsp          0.059</code></pre>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>The random forest classifier computes the feature importance by determining the <em>mean decrease&#13;
in impurity</em> for each feature (also known as Gini importance). Features that reduce uncertainty in classification receive higher scores.</p>&#13;
&#13;
<p>These numbers might be off if features vary in scale or cardinality of categorical columns.&#13;
<a data-primary="permutation importance" data-type="indexterm" id="idm46066894304184"/>A more reliable score is <em>permutation importance</em> (where each column has its values permuted and the&#13;
drop in accuracy is measured). <a data-primary="drop column importance" data-type="indexterm" id="idm46066894303032"/>An even more reliable mechanism is <em>drop column importance</em> (where each&#13;
column is dropped and the model is re-evaluated), but sadly this requires creating a new model for&#13;
each column that is dropped. See the <code>importances</code> function in the <code>rfpimp</code> package<a data-startref="ix_ch10-asciidoc15" data-type="indexterm" id="idm46066894300792"/><a data-startref="ix_ch10-asciidoc14" data-type="indexterm" id="idm46066894300056"/>:<a data-startref="ix_ch10-asciidoc13" data-type="indexterm" id="idm46066894299256"/><a data-startref="ix_ch10-asciidoc12" data-type="indexterm" id="idm46066894298552"/><a data-startref="ix_ch10-asciidoc11" data-type="indexterm" id="idm46066894297880"/></p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">rfpimp</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rf</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rfpimp</code><code class="o">.</code><code class="n">importances</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">rf</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code>&#13;
<code class="gp">... </code><code class="p">)</code><code class="o">.</code><code class="n">Importance</code>&#13;
<code class="go">Feature</code>&#13;
<code class="go">sex_male      0.155216</code>&#13;
<code class="go">fare          0.043257</code>&#13;
<code class="go">age           0.033079</code>&#13;
<code class="go">pclass        0.027990</code>&#13;
<code class="go">parch         0.020356</code>&#13;
<code class="go">embarked_Q    0.005089</code>&#13;
<code class="go">sibsp         0.002545</code>&#13;
<code class="go">embarked_S    0.000000</code>&#13;
<code class="go">Name: Importance, dtype: float64</code></pre>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="XGBoost" data-type="sect1"><div class="sect1" id="idm46066894576664">&#13;
<h1>XGBoost</h1>&#13;
&#13;
<p><a data-primary="classification" data-secondary="XGBoost" data-type="indexterm" id="ix_ch10-asciidoc16"/><a data-primary="XGBoost" data-type="indexterm" id="ix_ch10-asciidoc17"/>Although sklearn has a <code>GradientBoostedClassifier</code>, it is better to&#13;
use a third-party implementation that uses extreme boosting. These&#13;
tend to provide better results.</p>&#13;
&#13;
<p><a href="https://oreil.ly/WBo0g">XGBoost</a> is a popular library outside of scikit-learn. It creates a weak&#13;
tree and then “boosts” the subsequent trees to reduce the residual errors.&#13;
It tries to capture and address any patterns in the errors until they appear&#13;
to be <span class="keep-together">random.</span></p>&#13;
&#13;
<p>XGBoost has the following properties:</p>&#13;
<dl>&#13;
<dt>Runtime efficiency</dt>&#13;
<dd>&#13;
<p>XGBoost is parallelizeable. Use the <code>n_jobs</code> option to indicate the number of CPUs. Use GPU for even better <span class="keep-together">performance.</span></p>&#13;
</dd>&#13;
<dt>Preprocess data</dt>&#13;
<dd>&#13;
<p>No scaling necessary with tree models. Need to encode categorical data.</p>&#13;
</dd>&#13;
<dt>Prevent overfitting</dt>&#13;
<dd>&#13;
<p>The <code>early_stopping_rounds=N</code> parameter can be set to stop training if there is no improvement after N rounds. L1 and L2 regularization are controlled by <code>reg_alpha</code> and <code>reg_lambda</code>, respectively. Higher numbers are more <span class="keep-together">conservative.</span></p>&#13;
</dd>&#13;
<dt>Interpret results</dt>&#13;
<dd>&#13;
<p>Has feature importance.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>XGBoost has an extra parameter for the <code>.fit</code> method. The <code>early_stopping_rounds</code> parameter can be combined with the <code>eval_set</code> parameter to tell XGBoost to stop creating trees if the evaluation metric has not improved after that many boosting rounds. The <code>eval_metric</code> can also be set to one of the following: <code>'rmse'</code>, <code>'mae'</code>, <code>'logloss'</code>, <code>'error'</code> (default), <code>'auc'</code>, <code>'aucpr'</code>, as well as a custom function.</p>&#13;
&#13;
<p class="pagebreak-before">Here is an example using the library:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">xgboost</code> <code class="kn">as</code> <code class="nn">xgb</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">xgb_class</code> <code class="o">=</code> <code class="n">xgb</code><code class="o">.</code><code class="n">XGBClassifier</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">xgb_class</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">X_train</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">y_train</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">early_stopping_rounds</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">eval_set</code><code class="o">=</code><code class="p">[(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)],</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="go">XGBClassifier(base_score=0.5, booster='gbtree',</code>&#13;
<code class="go">  colsample_bylevel=1, colsample_bytree=1, gamma=0,</code>&#13;
<code class="go">  learning_rate=0.1, max_delta_step=0, max_depth=3,</code>&#13;
<code class="go">  min_child_weight=1, missing=None,</code>&#13;
<code class="go">  n_estimators=100, n_jobs=1, nthread=None,</code>&#13;
<code class="go">  objective='binary:logistic', random_state=42,</code>&#13;
<code class="go">  reg_alpha=0, reg_lambda=1, scale_pos_weight=1,</code>&#13;
<code class="go">  seed=None, silent=True, subsample=1)</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">xgb_class</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>&#13;
<code class="go">0.7862595419847328</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">xgb_class</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[[</code><code class="mi">0</code><code class="p">]])</code>&#13;
<code class="go">array([1])</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">xgb_class</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[[</code><code class="mi">0</code><code class="p">]])</code>&#13;
<code class="go">array([[0.06732017, 0.93267983]], dtype=float32)</code></pre>&#13;
&#13;
<p>Instance parameters:</p>&#13;
<dl>&#13;
<dt><code>max_depth=3</code></dt>&#13;
<dd>&#13;
<p>Maximum depth.</p>&#13;
</dd>&#13;
<dt><code>learning_rate=0.1</code></dt>&#13;
<dd>&#13;
<p>Learning rate (also called eta) for boosting (between 0 and 1). After each boost step, the newly added weights are scaled by this factor. The lower the value, the more conservative, but will also need more trees to converge. In the call to <code>.train</code>, you can pass a <code>learning_rates</code> parameter, which is a list of rates at each round (i.e., <code>[.1]*100 + [.05]*100</code>).</p>&#13;
</dd>&#13;
<dt><code>n_estimators=100</code></dt>&#13;
<dd>&#13;
<p>Number of rounds or boosted trees.</p>&#13;
</dd>&#13;
<dt><code>silent=True</code></dt>&#13;
<dd>&#13;
<p>Opposite of verbose. Whether to print messages while running boosting.</p>&#13;
</dd>&#13;
<dt><code>objective='binary:logistic'</code> </dt>&#13;
<dd>&#13;
<p>Learning task or callable for classification.</p>&#13;
</dd>&#13;
<dt><code>booster='gbtree'</code></dt>&#13;
<dd>&#13;
<p>Can be <code>'gbtree'</code>, <code>'gblinear'</code>, or <code>'dart'</code>.</p>&#13;
</dd>&#13;
<dt><code>nthread=None</code></dt>&#13;
<dd>&#13;
<p>Deprecated.</p>&#13;
</dd>&#13;
<dt><code>n_jobs=1</code></dt>&#13;
<dd>&#13;
<p>Number of threads to use.</p>&#13;
</dd>&#13;
<dt><code>gamma=0</code></dt>&#13;
<dd>&#13;
<p>Controls pruning. Range is 0 to infinite. Minimum loss reduction needed to further split a leaf. Higher gamma is more conservative. If training and test scores are diverging, insert a higher number (around 10). If training and test scores are close, use a lower number.</p>&#13;
</dd>&#13;
<dt><code>min_child_weight=1</code></dt>&#13;
<dd>&#13;
<p>Minimum value for sum of hessian for a child.</p>&#13;
</dd>&#13;
<dt><code>max_delta_step=0</code></dt>&#13;
<dd>&#13;
<p>Make update more conservative. Set 1 to 10 for imbalanced classes.</p>&#13;
</dd>&#13;
<dt><code>subsample=1</code></dt>&#13;
<dd>&#13;
<p>Fraction of samples to use for next round.</p>&#13;
</dd>&#13;
<dt><code>colsample_bytree=1</code></dt>&#13;
<dd>&#13;
<p>Fraction of columns to use for round.</p>&#13;
</dd>&#13;
<dt><code>colsample_bylevel=1</code></dt>&#13;
<dd>&#13;
<p>Fraction of columns to use for level.</p>&#13;
</dd>&#13;
<dt><code>colsample_bynode=1</code></dt>&#13;
<dd>&#13;
<p>Fraction of columns to use for node.</p>&#13;
</dd>&#13;
<dt><code>reg_alpha=0</code></dt>&#13;
<dd>&#13;
<p>L1 regularization (mean of weights) encourages sparsity. Increase to be more conservative.</p>&#13;
</dd>&#13;
<dt><code>reg_lambda=1</code></dt>&#13;
<dd>&#13;
<p>L2 regularization (root of squared weights) encourages small weights. Increase to be more conservative.</p>&#13;
</dd>&#13;
<dt><code>scale_pos_weight=1</code></dt>&#13;
<dd>&#13;
<p>Ratio of negative/positive weight.</p>&#13;
</dd>&#13;
<dt><code>base_score=.5</code></dt>&#13;
<dd>&#13;
<p>Initial prediction.</p>&#13;
</dd>&#13;
<dt><code>seed=None</code></dt>&#13;
<dd>&#13;
<p>Deprecated.</p>&#13;
</dd>&#13;
<dt><code>random_state=0</code></dt>&#13;
<dd>&#13;
<p>Random seed.</p>&#13;
</dd>&#13;
<dt><code>missing=None</code></dt>&#13;
<dd>&#13;
<p>Value to interpret for <code>missing</code>. <code>None</code> means <code>np.nan</code>.</p>&#13;
</dd>&#13;
<dt><code>importance_type='gain'</code></dt>&#13;
<dd>&#13;
<p>The feature importance type: <code>'gain'</code>, <code>'weight'</code>, <code>'cover'</code>, <code>'total_gain'</code>, or <code>'total_cover'</code>.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Attributes:</p>&#13;
<dl>&#13;
<dt><code>coef_</code></dt>&#13;
<dd>&#13;
<p>Coefficients for gblinear learners</p>&#13;
</dd>&#13;
<dt><code>feature_importances_</code></dt>&#13;
<dd>&#13;
<p>Feature importances for gbtree learners</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p><a data-primary="feature importance" data-secondary="XGBoost" data-type="indexterm" id="idm46066894026744"/>Feature importance is the average gain across all the nodes where the feature is used:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">col</code><code class="p">,</code> <code class="n">val</code> <code class="ow">in</code> <code class="nb">sorted</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="nb">zip</code><code class="p">(</code>&#13;
<code class="gp">... </code>        <code class="n">X</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code>&#13;
<code class="gp">... </code>        <code class="n">xgb_class</code><code class="o">.</code><code class="n">feature_importances_</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="p">),</code>&#13;
<code class="gp">... </code>    <code class="n">key</code><code class="o">=</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code>&#13;
<code class="gp">... </code>    <code class="n">reverse</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)[:</code><code class="mi">5</code><code class="p">]:</code>&#13;
<code class="gp">... </code>    <code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s">"{col:10}{val:10.3f}"</code><code class="p">)</code>&#13;
<code class="go">fare           0.420</code>&#13;
<code class="go">age            0.309</code>&#13;
<code class="go">pclass         0.071</code>&#13;
<code class="go">sex_male       0.066</code>&#13;
<code class="go">sibsp          0.050</code></pre>&#13;
&#13;
<p>XGBoost can plot the feature importance (see <a data-type="xref" href="#id25">Figure 10-5</a>). It has an <code>importance_type</code> parameter. The default value is <code>"weight"</code>, which is the number of times a feature appears in a tree. It can also be <code>"gain"</code>, which shows the average gain when the feature is used, or <code>"cover"</code>, which is the number of samples affected by a split:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">xgb</code><code class="o">.</code><code class="n">plot_importance</code><code class="p">(</code><code class="n">xgb_class</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1005.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id25">&#13;
<img alt="Feature importance showing weight (how many times a feature appears in the trees)." src="assets/mlpr_1005.png"/>&#13;
<h6><span class="label">Figure 10-5. </span>Feature importance showing weight (how many times a feature appears in the trees).</h6>&#13;
</div></figure>&#13;
&#13;
<p class="pagebreak-before">We can plot this in <a data-primary="Yellowbrick" data-secondary="feature importance for XGBoost" data-type="indexterm" id="idm46066893847880"/>Yellowbrick, which normalizes the values (see <a data-type="xref" href="#id26">Figure 10-6</a>):</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fi_viz</code> <code class="o">=</code> <code class="n">FeatureImportances</code><code class="p">(</code><code class="n">xgb_class</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fi_viz</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fi_viz</code><code class="o">.</code><code class="n">poof</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1006.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id26">&#13;
<img alt="Yellowbrick feature importance for XGBoost (normalized to 100)." src="assets/mlpr_1006.png"/>&#13;
<h6><span class="label">Figure 10-6. </span>Yellowbrick feature importance for XGBoost (normalized to 100).</h6>&#13;
</div></figure>&#13;
&#13;
<p>XGBoost provides both a textual representation of the trees&#13;
and a graphical one. Here is the text representation:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">booster</code> <code class="o">=</code> <code class="n">xgb_class</code><code class="o">.</code><code class="n">get_booster</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="k">print</code><code class="p">(</code><code class="n">booster</code><code class="o">.</code><code class="n">get_dump</code><code class="p">()[</code><code class="mi">0</code><code class="p">])</code>&#13;
<code class="go">0:[sex_male&lt;0.5] yes=1,no=2,missing=1</code>&#13;
<code class="go">  1:[pclass&lt;0.23096] yes=3,no=4,missing=3</code>&#13;
<code class="go">    3:[fare&lt;-0.142866] yes=7,no=8,missing=7</code>&#13;
<code class="go">      7:leaf=0.132530</code>&#13;
<code class="go">      8:leaf=0.184</code>&#13;
<code class="go">    4:[fare&lt;-0.19542] yes=9,no=10,missing=9</code>&#13;
<code class="go">      9:leaf=0.024598</code>&#13;
<code class="go">      10:leaf=-0.1459</code>&#13;
<code class="go">  2:[age&lt;-1.4911] yes=5,no=6,missing=5</code>&#13;
<code class="go">    5:[sibsp&lt;1.81278] yes=11,no=12,missing=11</code>&#13;
<code class="go">      11:leaf=0.13548</code>&#13;
<code class="go">      12:leaf=-0.15000</code>&#13;
<code class="go">    6:[pclass&lt;-0.95759] yes=13,no=14,missing=13</code>&#13;
<code class="go">      13:leaf=-0.06666</code>&#13;
<code class="go">      14:leaf=-0.1487</code></pre>&#13;
&#13;
<p>The value in the leaf is the score for class 1. It can be converted into&#13;
a probability using the logistic function. If the decisions fell through&#13;
to leaf 7, the probability of class 1 is 53%. This is the score from a&#13;
single tree. If our model had 100 trees, you would sum up each leaf&#13;
value and get the probability with the logistic function:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="c"># score from first tree leaf 7</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="mi">1</code> <code class="o">/</code> <code class="p">(</code><code class="mi">1</code> <code class="o">+</code> <code class="n">np</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code> <code class="o">*</code> <code class="mf">0.1238</code><code class="p">))</code>&#13;
<code class="go">0.5309105310475829</code></pre>&#13;
&#13;
<p>Here is the graphical version of the first tree in the model (see <a data-type="xref" href="#id27">Figure 10-7</a>):</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">xgb</code><code class="o">.</code><code class="n">plot_tree</code><code class="p">(</code><code class="n">xgb_class</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">,</code> <code class="n">num_trees</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1007.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id27">&#13;
<img alt="Tree of XGBoost." src="assets/mlpr_1007.png"/>&#13;
<h6><span class="label">Figure 10-7. </span>Tree of XGBoost.</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-primary="feature importance" data-secondary="xgbfir package" data-type="indexterm" id="ix_ch10-asciidoc18"/><a data-primary="xgbfir" data-type="indexterm" id="ix_ch10-asciidoc19"/>The <a href="https://oreil.ly/kPnRv">xgbfir package</a> is a library built on top of XGBoost. This library gives various measures about feature importance.&#13;
What is unique is that it provides these measures about the&#13;
columns, and also pairs of columns, so you can see the interactions.&#13;
In addition, you can get information about triplets (three-column) interactions.</p>&#13;
&#13;
<p>The measures it provides are:</p>&#13;
<dl>&#13;
<dt><code>Gain</code></dt>&#13;
<dd>&#13;
<p>Total gain of each feature or feature interaction</p>&#13;
</dd>&#13;
<dt><code>FScore</code></dt>&#13;
<dd>&#13;
<p>Amount of possible splits taken on a feature or feature interaction</p>&#13;
</dd>&#13;
<dt><code>wFScore</code></dt>&#13;
<dd>&#13;
<p>Amount of possible splits taken on a feature or feature interaction, weighted by the probability of the splits to take place</p>&#13;
</dd>&#13;
<dt><code>Average wFScore</code></dt>&#13;
<dd>&#13;
<p><code>wFScore</code> divided by <code>FScore</code></p>&#13;
</dd>&#13;
<dt><code>Average Gain</code></dt>&#13;
<dd>&#13;
<p><code>Gain</code> divided by <code>FScore</code></p>&#13;
</dd>&#13;
<dt><code>Expected Gain</code></dt>&#13;
<dd>&#13;
<p>Total gain of each feature or feature interaction weighted by the probability to gather the gain</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>The interface is simply an export to a spreadsheet, so we will use pandas&#13;
to read them back in. Here is the column <span class="keep-together">importance:</span></p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">xgbfir</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">xgbfir</code><code class="o">.</code><code class="n">saveXgbFI</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">xgb_class</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">feature_names</code><code class="o">=</code><code class="n">X</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">OutputXlsxFile</code><code class="o">=</code><code class="s">"fir.xlsx"</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">pd</code><code class="o">.</code><code class="n">read_excel</code><code class="p">(</code><code class="s">"/tmp/surv-fir.xlsx"</code><code class="p">)</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">3</code><code class="p">)</code><code class="o">.</code><code class="n">T</code>&#13;
<code class="go">                           0         1         2</code>&#13;
<code class="go">Interaction         sex_male    pclass      fare</code>&#13;
<code class="go">Gain                 1311.44   585.794   544.884</code>&#13;
<code class="go">FScore                    42        45       267</code>&#13;
<code class="go">wFScore              39.2892   21.5038    128.33</code>&#13;
<code class="go">Average wFScore     0.935458  0.477861  0.480636</code>&#13;
<code class="go">Average Gain         31.2247   13.0177   2.04076</code>&#13;
<code class="go">Expected Gain        1307.43   229.565   236.738</code>&#13;
<code class="go">Gain Rank                  1         2         3</code>&#13;
<code class="go">FScore Rank                4         3         1</code>&#13;
<code class="go">wFScore Rank               3         4         1</code>&#13;
<code class="go">Avg wFScore Rank           1         5         4</code>&#13;
<code class="go">Avg Gain Rank              1         2         4</code>&#13;
<code class="go">Expected Gain Rank         1         3         2</code>&#13;
<code class="go">Average Rank         1.83333   3.16667       2.5</code>&#13;
<code class="go">Average Tree Index   32.2381   20.9778   51.9101</code>&#13;
<code class="go">Average Tree Depth  0.142857   1.13333   1.50562</code></pre>&#13;
&#13;
<p>From this table, we see sex_male ranks high in gain, average wFScore, average gain, and expected gain, whereas fare tops out in FScore and wFScore.</p>&#13;
&#13;
<p>Let’s look at pairs of column interactions:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">pd</code><code class="o">.</code><code class="n">read_excel</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="s">"fir.xlsx"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">sheet_name</code><code class="o">=</code><code class="s">"Interaction Depth 1"</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code><code class="o">.</code><code class="n">T</code>&#13;
<code class="go">Interaction         pclass|sex_male  age|sex_male</code>&#13;
<code class="go">Gain                        2090.27       964.046</code>&#13;
<code class="go">FScore                           35            18</code>&#13;
<code class="go">wFScore                     14.3608       9.65915</code>&#13;
<code class="go">Average wFScore            0.410308      0.536619</code>&#13;
<code class="go">Average Gain                 59.722       53.5581</code>&#13;
<code class="go">Expected Gain                827.49        616.17</code>&#13;
<code class="go">Gain Rank                         1             2</code>&#13;
<code class="go">FScore Rank                       5            10</code>&#13;
<code class="go">wFScore Rank                      4             8</code>&#13;
<code class="go">Avg wFScore Rank                  8             5</code>&#13;
<code class="go">Avg Gain Rank                     1             2</code>&#13;
<code class="go">Expected Gain Rank                1             2</code>&#13;
<code class="go">Average Rank                3.33333       4.83333</code>&#13;
<code class="go">Average Tree Index          18.9714       38.1111</code>&#13;
<code class="go">Average Tree Depth                1       1.11111</code></pre>&#13;
&#13;
<p>Here we see that the top two interactions involve the sex_male column in combination with pclass and age. If you were only able to make a model with two features, you would probably want to choose pclass and sex_male.</p>&#13;
&#13;
<p>Finally, let’s look at triplets:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">pd</code><code class="o">.</code><code class="n">read_excel</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="s">"fir.xlsx"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">sheet_name</code><code class="o">=</code><code class="s">"Interaction Depth 2"</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">T</code>&#13;
<code class="go">                                       0</code>&#13;
<code class="go">Interaction         fare|pclass|sex_male</code>&#13;
<code class="go">Gain                             2973.16</code>&#13;
<code class="go">FScore                                44</code>&#13;
<code class="go">wFScore                          8.92572</code>&#13;
<code class="go">Average wFScore                 0.202857</code>&#13;
<code class="go">Average Gain                     67.5719</code>&#13;
<code class="go">Expected Gain                    549.145</code>&#13;
<code class="go">Gain Rank                              1</code>&#13;
<code class="go">FScore Rank                            1</code>&#13;
<code class="go">wFScore Rank                           4</code>&#13;
<code class="go">Avg wFScore Rank                      21</code>&#13;
<code class="go">Avg Gain Rank                          3</code>&#13;
<code class="go">Expected Gain Rank                     2</code>&#13;
<code class="go">Average Rank                     5.33333</code>&#13;
<code class="go">Average Tree Index               16.6591</code>&#13;
<code class="go">Average Tree Depth                     2</code></pre>&#13;
&#13;
<p>This is only showing the first triplet due to space limitations, but the spreadsheet has many more<a data-startref="ix_ch10-asciidoc19" data-type="indexterm" id="idm46066893443080"/><a data-startref="ix_ch10-asciidoc18" data-type="indexterm" id="idm46066893334248"/>:<a data-startref="ix_ch10-asciidoc17" data-type="indexterm" id="idm46066893333512"/><a data-startref="ix_ch10-asciidoc16" data-type="indexterm" id="idm46066893332808"/></p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">pd</code><code class="o">.</code><code class="n">read_excel</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="s">"/tmp/surv-fir.xlsx"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">sheet_name</code><code class="o">=</code><code class="s">"Interaction Depth 2"</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)[[</code><code class="s">"Interaction"</code><code class="p">,</code> <code class="s">"Gain"</code><code class="p">]]</code><code class="o">.</code><code class="n">head</code><code class="p">()</code>&#13;
<code class="go">            Interaction         Gain</code>&#13;
<code class="go">0  fare|pclass|sex_male  2973.162529</code>&#13;
<code class="go">1   age|pclass|sex_male  1621.945151</code>&#13;
<code class="go">2    age|sex_male|sibsp  1042.320428</code>&#13;
<code class="go">3     age|fare|sex_male   366.860828</code>&#13;
<code class="go">4    fare|fare|sex_male   196.224791</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Gradient Boosted with LightGBM" data-type="sect1"><div class="sect1" id="idm46066893330232">&#13;
<h1>Gradient Boosted with LightGBM</h1>&#13;
&#13;
<p><a data-primary="classification" data-secondary="gradient boosted with LightGBM" data-type="indexterm" id="ix_ch10-asciidoc20"/><a data-primary="LightGBM" data-secondary="gradient boosted with" data-type="indexterm" id="ix_ch10-asciidoc21"/>LightGBM is an implementation by Microsoft. LightGBM uses&#13;
a sampling mechanism to deal with continuous values. This&#13;
allows quicker creation of trees (than say XGBoost), and reduces&#13;
memory usage.</p>&#13;
&#13;
<p>LightGBM also grows trees depth first (<em>leaf-wise</em> rather than&#13;
<em>level-wise</em>). Because of this, rather than using <code>max_depth</code> to <span class="keep-together">control</span> overfitting, use <code>num_leaves</code> (where this value is <span class="keep-together">&lt;&#13;
 2</span>^(<code>max_depth</code>)).</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Installation of this library currently requires having a compiler and is a little more involved than just a <code>pip install</code>.</p>&#13;
</div>&#13;
&#13;
<p>It has the following properties:</p>&#13;
<dl>&#13;
<dt>Runtime efficiency</dt>&#13;
<dd>&#13;
<p>Can take advantage of multiple CPUs. By using binning, can be 15 times faster than XGBoost.</p>&#13;
</dd>&#13;
<dt>Preprocess data</dt>&#13;
<dd>&#13;
<p>Has some support for encoding categorical columns as integers (or pandas <code>Categorical</code> type), but AUC appears to suffer compared to one-hot encoding.</p>&#13;
</dd>&#13;
<dt>Prevent overfitting</dt>&#13;
<dd>&#13;
<p>Lower <code>num_leaves</code>. Increase <code>min_data_in_leaf</code>. Use <code>min_gain_to_split</code> with <code>lambda_l1</code> or <code>lambda_l2</code>.</p>&#13;
</dd>&#13;
<dt>Interpret results</dt>&#13;
<dd>&#13;
<p>Feature importance is available. Individual trees are weak and tend to be hard to interpret.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Here is an example using the library:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">lightgbm</code> <code class="kn">as</code> <code class="nn">lgb</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lgbm_class</code> <code class="o">=</code> <code class="n">lgb</code><code class="o">.</code><code class="n">LGBMClassifier</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lgbm_class</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
<code class="go">LGBMClassifier(boosting_type='gbdt',</code>&#13;
<code class="go">  class_weight=None, colsample_bytree=1.0,</code>&#13;
<code class="go">  learning_rate=0.1, max_depth=-1,</code>&#13;
<code class="go">  min_child_samples=20, min_child_weight=0.001,</code>&#13;
<code class="go">  min_split_gain=0.0, n_estimators=100,</code>&#13;
<code class="go">  n_jobs=-1, num_leaves=31, objective=None,</code>&#13;
<code class="go">  random_state=42, reg_alpha=0.0, reg_lambda=0.0,</code>&#13;
<code class="go">  silent=True, subsample=1.0,</code>&#13;
<code class="go">  subsample_for_bin=200000, subsample_freq=0)</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lgbm_class</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>&#13;
<code class="go">0.7964376590330788</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lgbm_class</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[[</code><code class="mi">0</code><code class="p">]])</code>&#13;
<code class="go">array([1])</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lgbm_class</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[[</code><code class="mi">0</code><code class="p">]])</code>&#13;
<code class="go">array([[0.01637168, 0.98362832]])</code></pre>&#13;
&#13;
<p>Instance parameters:</p>&#13;
<dl>&#13;
<dt><code>boosting_type='gbdt'</code></dt>&#13;
<dd>&#13;
<p>Can be: <code>'gbdt'</code> (gradient boosting), <code>'rf'</code> (random forest), <code>'dart'</code> (dropouts meet multiple additive regression trees), or <code>'goss'</code> (gradient-based, one-sided sampling).</p>&#13;
</dd>&#13;
<dt><code>class_weight=None</code></dt>&#13;
<dd>&#13;
<p>Dictionary or <code>'balanced'</code>. Use dictionary to set weight for each class label when doing multiclass problems. For binary problems, use <code>is_unbalance</code> or <code>scale_pos_weight</code>.</p>&#13;
</dd>&#13;
<dt><code>colsample_bytree=1.0</code></dt>&#13;
<dd>&#13;
<p>Range (0, 1.0]. Select percent of features for each boosting round.</p>&#13;
</dd>&#13;
<dt><code>importance_type='split'</code></dt>&#13;
<dd>&#13;
<p>How to calculate feature importance. <code>'split'</code> means number of times a feature is used. <code>'gain'</code> is total gains of splits for a feature.</p>&#13;
</dd>&#13;
<dt><code>learning_rate=0.1</code></dt>&#13;
<dd>&#13;
<p>Range (0, 1.0]. Learning rate for boosting. A smaller value slows down overfitting as boosting rounds have less impact. A smaller number should give better performance but will require more <code>num_iterations</code>.</p>&#13;
</dd>&#13;
<dt><code>max_depth=-1</code></dt>&#13;
<dd>&#13;
<p>Maximum tree depth. -1 is unlimited. Larger depths tend to overfit more.</p>&#13;
</dd>&#13;
<dt><code>min_child_samples=20</code></dt>&#13;
<dd>&#13;
<p>Number of samples required for a leaf. Lower numbers mean more overfitting.</p>&#13;
</dd>&#13;
<dt><code>min_child_weight=0.001</code></dt>&#13;
<dd>&#13;
<p>Sum of hessian weight required for a leaf.</p>&#13;
</dd>&#13;
<dt><code>min_split_gain=0.0</code></dt>&#13;
<dd>&#13;
<p>Loss reduction required to partition leaf.</p>&#13;
</dd>&#13;
<dt><code>n_estimators=100</code></dt>&#13;
<dd>&#13;
<p>Number of trees or boosting rounds.</p>&#13;
</dd>&#13;
<dt><code>n_jobs=-1</code></dt>&#13;
<dd>&#13;
<p>Number of threads.</p>&#13;
</dd>&#13;
<dt><code>num_leaves=31</code></dt>&#13;
<dd>&#13;
<p>Maximum tree leaves.</p>&#13;
</dd>&#13;
<dt><code>objective=None</code></dt>&#13;
<dd>&#13;
<p><code>None</code> is <code>'binary'</code> or <code>'multiclass'</code> for classifier. Can be a function or string.</p>&#13;
</dd>&#13;
<dt><code>random_state=42</code></dt>&#13;
<dd>&#13;
<p>Random seed.</p>&#13;
</dd>&#13;
<dt><code>reg_alpha=0.0</code></dt>&#13;
<dd>&#13;
<p>L1 regularization (mean of weights). Increase to be more conservative.</p>&#13;
</dd>&#13;
<dt><code>reg_lambda=0.0</code></dt>&#13;
<dd>&#13;
<p>L2 regularization (root of squared weights). Increase to be more conservative.</p>&#13;
</dd>&#13;
<dt><code>silent=True</code></dt>&#13;
<dd>&#13;
<p>Verbose mode.</p>&#13;
</dd>&#13;
<dt><code>subsample=1.0</code></dt>&#13;
<dd>&#13;
<p>Fraction of samples to use for next round.</p>&#13;
</dd>&#13;
<dt><code>subsample_for_bin=200000</code></dt>&#13;
<dd>&#13;
<p>Samples required to create bins.</p>&#13;
</dd>&#13;
<dt><code>subsample_freq=0</code></dt>&#13;
<dd>&#13;
<p>Subsample frequency. Change to 1 to enable.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p><a data-primary="feature importance" data-secondary="LightGBM" data-type="indexterm" id="idm46066893145368"/><a data-primary="splits" data-type="indexterm" id="idm46066893144616"/>Feature importance based on <code>'splits'</code> (number of times a product is used):</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="k">for</code> <code class="n">col</code><code class="p">,</code> <code class="n">val</code> <code class="ow">in</code> <code class="nb">sorted</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="nb">zip</code><code class="p">(</code><code class="n">cols</code><code class="p">,</code> <code class="n">lgbm_class</code><code class="o">.</code><code class="n">feature_importances_</code><code class="p">),</code>&#13;
<code class="gp">... </code>    <code class="n">key</code><code class="o">=</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code>&#13;
<code class="gp">... </code>    <code class="n">reverse</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)[:</code><code class="mi">5</code><code class="p">]:</code>&#13;
<code class="gp">... </code>    <code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s">"{col:10}{val:10.3f}"</code><code class="p">)</code>&#13;
<code class="go">fare        1272.000</code>&#13;
<code class="go">age         1182.000</code>&#13;
<code class="go">sibsp        118.000</code>&#13;
<code class="go">pclass       115.000</code>&#13;
<code class="go">sex_male     110.000</code></pre>&#13;
&#13;
<p>The LightGBM library supports creating a feature importance plot (see <a data-type="xref" href="#id23">Figure 10-8</a>). The default is based on <code>'splits'</code>, the number of times a feature is used. You can specify <span class="keep-together"><code>'importance_type'</code></span> if you want to change it to <code>'gain'</code>:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lgb</code><code class="o">.</code><code class="n">plot_importance</code><code class="p">(</code><code class="n">lgbm_class</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">tight_layout</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1008.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id23">&#13;
<img alt="Feature Importance splits for LightGBM." src="assets/mlpr_1008.png"/>&#13;
<h6><span class="label">Figure 10-8. </span>Feature importance splits for LightGBM.</h6>&#13;
</div></figure>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>As of version 0.9, Yellowbrick doesn’t work with LightGBM for creating feature importance plots.</p>&#13;
</div>&#13;
&#13;
<p>We can create a tree of the decisions as well (see <a data-type="xref" href="#id24">Figure 10-9</a>):</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lgb</code><code class="o">.</code><code class="n">plot_tree</code><code class="p">(</code><code class="n">lgbm_class</code><code class="p">,</code> <code class="n">tree_index</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1009.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id24">&#13;
<img alt="LightGBM tree." src="assets/mlpr_1009.png"/>&#13;
<h6><span class="label">Figure 10-9. </span>LightGBM tree.</h6>&#13;
</div></figure>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>In Jupyter, use the following command to view a tree:<a data-startref="ix_ch10-asciidoc21" data-type="indexterm" id="idm46066892886664"/><a data-startref="ix_ch10-asciidoc20" data-type="indexterm" id="idm46066892885960"/></p>&#13;
&#13;
<pre data-type="programlisting">lgb.create_tree_digraph(lgbm_class)</pre>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="TPOT" data-type="sect1"><div class="sect1" id="idm46066893320136">&#13;
<h1>TPOT</h1>&#13;
&#13;
<p><a href="https://oreil.ly/NFJvl">TPOT</a> <a data-primary="classification" data-secondary="TPOT" data-type="indexterm" id="ix_ch10-asciidoc22"/><a data-primary="TPOT" data-type="indexterm" id="ix_ch10-asciidoc23"/>uses a genetic algorithm to try different models and ensembles.&#13;
This can take hours to days to run as it considers multiple models and&#13;
preprocessing steps, as well as the hyperparameters for said models, and&#13;
ensembling options. On a typical machine, a generation may take five or more minutes&#13;
to run.</p>&#13;
&#13;
<p>It has the following properties:</p>&#13;
<dl>&#13;
<dt>Runtime efficiency</dt>&#13;
<dd>&#13;
<p>Can take hours or days. Use <code>n_jobs=-1</code> to use all CPUs.</p>&#13;
</dd>&#13;
<dt>Preprocess data</dt>&#13;
<dd>&#13;
<p>You need to remove NaN and categorical data.</p>&#13;
</dd>&#13;
<dt>Prevent overfitting</dt>&#13;
<dd>&#13;
<p>Ideally, results should use cross-validation to minimize overfitting.</p>&#13;
</dd>&#13;
<dt>Interpret results</dt>&#13;
<dd>&#13;
<p>Depends on the results.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Here is an example of using the library:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">tpot</code> <code class="kn">import</code> <code class="n">TPOTClassifier</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">tc</code> <code class="o">=</code> <code class="n">TPOTClassifier</code><code class="p">(</code><code class="n">generations</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">tc</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">tc</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>&#13;
<code class="go">0.7888040712468194</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">tc</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[[</code><code class="mi">0</code><code class="p">]])</code>&#13;
<code class="go">array([1])</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">tc</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[[</code><code class="mi">0</code><code class="p">]])</code>&#13;
<code class="go">array([[0.07449919, 0.92550081]])</code></pre>&#13;
&#13;
<p>Instance parameters:</p>&#13;
<dl>&#13;
<dt><code>generations=100</code></dt>&#13;
<dd>&#13;
<p>Iterations to run.</p>&#13;
</dd>&#13;
<dt><code>population_size=100</code></dt>&#13;
<dd>&#13;
<p>Population size for genetic programming. Larger size usually performs better but takes more memory and time.</p>&#13;
</dd>&#13;
<dt><code>offspring_size=None</code></dt>&#13;
<dd>&#13;
<p>Offspring for each generation. Default is <code>population_size</code>.</p>&#13;
</dd>&#13;
<dt><code>mutation_rate=.9</code></dt>&#13;
<dd>&#13;
<p>Mutation rate for algorithm [0, 1]. Default is .9.</p>&#13;
</dd>&#13;
<dt><code>crossover_rate=.1</code></dt>&#13;
<dd>&#13;
<p>Cross-over rate (how many pipelines to breed in a generation). Range [0, 1]. Default is .1.</p>&#13;
</dd>&#13;
<dt><code>scoring='accuracy'</code></dt>&#13;
<dd>&#13;
<p>Scoring mechanism. Uses sklearn strings.</p>&#13;
</dd>&#13;
<dt><code>cv=5</code></dt>&#13;
<dd>&#13;
<p>Cross-validation folds.</p>&#13;
</dd>&#13;
<dt><code>subsample=1</code></dt>&#13;
<dd>&#13;
<p>Subsample training instances. Range [0, 1]. Default is 1.</p>&#13;
</dd>&#13;
<dt><code>n_jobs=1</code></dt>&#13;
<dd>&#13;
<p>Number of CPUs to use, -1 for all cores.</p>&#13;
</dd>&#13;
<dt><code>max_time_mins=None</code></dt>&#13;
<dd>&#13;
<p>Maximum amount of minutes to run.</p>&#13;
</dd>&#13;
<dt><code>max_eval_time_mins=5</code></dt>&#13;
<dd>&#13;
<p>Maximum amount of minutes to evaluate a single <span class="keep-together">pipeline.</span></p>&#13;
</dd>&#13;
<dt><code>random_state=None</code></dt>&#13;
<dd>&#13;
<p>Random seed.</p>&#13;
</dd>&#13;
<dt><code>config_dict</code></dt>&#13;
<dd>&#13;
<p>Configuration options for optimization.</p>&#13;
</dd>&#13;
<dt><code>warm_start=False</code></dt>&#13;
<dd>&#13;
<p>Reuse previous calls to <code>.fit</code>.</p>&#13;
</dd>&#13;
<dt><code>memory=None</code></dt>&#13;
<dd>&#13;
<p>Can cache pipelines. <code>'auto'</code> or a path will persist in a directory.</p>&#13;
</dd>&#13;
<dt><code>use_dask=False</code></dt>&#13;
<dd>&#13;
<p>Use dask.</p>&#13;
</dd>&#13;
<dt><code>periodic_checkpoint_folder=None</code></dt>&#13;
<dd>&#13;
<p>Path to a folder where the best pipeline will be persisted periodically.</p>&#13;
</dd>&#13;
<dt><code>early_stop=None</code></dt>&#13;
<dd>&#13;
<p>Stop after running this many generations with no improvement.</p>&#13;
</dd>&#13;
<dt><code>verbosity=0</code></dt>&#13;
<dd>&#13;
<p>0 = none, 1 = minimal, 2 = high, or 3 = all. 2 and higher shows a progress bar.</p>&#13;
</dd>&#13;
<dt><code>disable_update_check=False</code></dt>&#13;
<dd>&#13;
<p>Disable version check.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Attributes:</p>&#13;
<dl>&#13;
<dt><code>evaluated_individuals_</code></dt>&#13;
<dd>&#13;
<p>Dictionary with all pipelines that were evaluated.</p>&#13;
</dd>&#13;
<dt><code>fitted_pipeline_</code></dt>&#13;
<dd>&#13;
<p>Best pipeline.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>After you are done, you can export the pipeline:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">tc</code><code class="o">.</code><code class="n">export</code><code class="p">(</code><code class="s">"tpot_exported_pipeline.py"</code><code class="p">)</code></pre>&#13;
&#13;
<p>The result might look like this<a data-startref="ix_ch10-asciidoc23" data-type="indexterm" id="idm46066892741352"/><a data-startref="ix_ch10-asciidoc22" data-type="indexterm" id="idm46066892739080"/>:<a data-startref="ix_ch10-asciidoc0" data-type="indexterm" id="idm46066892738344"/></p>&#13;
&#13;
<pre data-type="programlisting">import numpy as np&#13;
import pandas as pd&#13;
from sklearn.ensemble import ExtraTreesClassifier&#13;
from sklearn.model_selection import \&#13;
    train_test_split&#13;
from sklearn.pipeline import make_pipeline, \&#13;
    make_union&#13;
from sklearn.preprocessing import Normalizer&#13;
from tpot.builtins import StackingEstimator&#13;
&#13;
# NOTE: Make sure that the class is labeled&#13;
# 'target' in the data file&#13;
tpot_data = pd.read_csv('PATH/TO/DATA/FILE',&#13;
    sep='COLUMN_SEPARATOR', dtype=np.float64)&#13;
features = tpot_data.drop('target', axis=1).values&#13;
training_features, testing_features, \&#13;
    training_target, testing_target = \&#13;
    train_test_split(features,&#13;
       tpot_data['target'].values, random_state=42)&#13;
&#13;
# Score on the training set was:0.8122535043953432&#13;
exported_pipeline = make_pipeline(&#13;
  Normalizer(norm="max"),&#13;
  StackingEstimator(&#13;
    estimator=ExtraTreesClassifier(bootstrap=True,&#13;
      criterion="gini", max_features=0.85,&#13;
      min_samples_leaf=2, min_samples_split=19,&#13;
      n_estimators=100)),&#13;
  ExtraTreesClassifier(bootstrap=False,&#13;
    criterion="entropy", max_features=0.3,&#13;
    min_samples_leaf=13, min_samples_split=9,&#13;
    n_estimators=100)&#13;
)&#13;
&#13;
exported_pipeline.fit(training_features,&#13;
  training_target)&#13;
results = exported_pipeline.predict(&#13;
  testing_features)</pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>