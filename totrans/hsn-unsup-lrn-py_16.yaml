- en: Chapter 12\. Generative Adversarial Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have already explored two types of generative models: RBMs and DBNs. In
    this chapter, we will explore *generative adversarial networks (GANs)*, one of
    the latest and most promising areas of unsupervised learning and generative modeling.'
  prefs: []
  type: TYPE_NORMAL
- en: GANs, the Concept
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GANs were introduced by Ian Goodfellow and his fellow researchers at the University
    of Montreal in 2014\. In GANs, we have two neural networks. One network known
    as the *generator* generates data based on a model it has created using samples
    of real data it has received as input. The other network known as the *discriminator*
    discriminates between the data created by the generator and data from the true
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: As a simple analogy, the generator is the counterfeiter, and the discriminator
    is the police trying to identify the forgery. The two networks are locked in a
    zero-sum game. The generator is trying to fool the discriminator into thinking
    the synthetic data comes from the true distribution, and the discriminator is
    trying to call out the synthetic data as fake.
  prefs: []
  type: TYPE_NORMAL
- en: GANs are unsupervised learning algorithms because the generator can learn the
    underlying structure of the true distribution even when there are no labels. The
    generator learns the underlying structure by using a number of parameters significantly
    smaller than the amount of data it has trained on—a core concept of unsupervised
    learning that we have explored many times in previous chapters. This constraint
    forces the generator to efficiently capture the most salient aspects of the true
    data distribution. This is similar to the representation learning that occurs
    in deep learning. Each hidden layer in the neutral network of a generator captures
    a representation of the underlying data—starting very simply—and subsequent layers
    pick up more complicated representations by building on the simpler preceding
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: Using all these layers together, the generator learns the underlying structure
    of the data and attempts to create synthetic data that is nearly identical to
    the true data. If the generator has captured the essence of the true data, the
    synthetic data will appear real.
  prefs: []
  type: TYPE_NORMAL
- en: The Power of GANs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Chapter 11](ch11.html#Chapter_11), we explored the ability to use synthetic
    data from an unsupervised learning model (such as a deep belief network) to improve
    the performance of a supervised learning model. Like DBNs, GANs are very good
    at generating synthetic data.
  prefs: []
  type: TYPE_NORMAL
- en: If the objective is to generate a lot of new training examples to help supplement
    existing training data—for example, to improve accuracy on an image recognition
    task—we can use the generator to create a lot of synthetic data, add the new synthetic
    data to the original training data, and then run a supervised machine learning
    model on the now much larger dataset.
  prefs: []
  type: TYPE_NORMAL
- en: GANs can also excel at anomaly detection. If the objective is to identify anomalies—for
    example, to detect fraud, hacking, or other suspicious behavior—we can use the
    discriminator to score each instance in the real data. The instances that the
    discriminator ranks as “likely synthetic” will be the most anomalous instances
    and also the ones most likely to represent malicious behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Convolutional GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will return to the MNIST dataset we used in previous chapters
    and apply a version of GANs to generate synthetic data to supplement the existing
    MNIST dataset. We will then apply a supervised learning model to perform image
    classification. This is yet another version of semisupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As a side note, you should now have a much deeper appreciation for semisupervised
    learning. Because much of the world’s data is unlabeled, the ability of unsupervised
    learning to efficiently help label data by itself is very powerful. As part of
    such semisupervised machine learning systems, unsupervised learning enhances the
    potential of all successful commercial applications of supervised learning to
    date.
  prefs: []
  type: TYPE_NORMAL
- en: Even outside of applications in semisupervised systems, unsupervised learning
    has potential on a standalone basis because it learns from data without any labels
    and is one of the fields of AI that has the greatest potential to help the machine
    learning community move from narrow AI to more AGI applications.
  prefs: []
  type: TYPE_NORMAL
- en: The version of GANs we will use is called *deep convolutional generative adversarial
    networks (DCGANs)*, which were first introduced in late 2015 by Alec Radford,
    Luke Metz, and Soumith Chintala.^([1](ch12.html#idm140637527552048))
  prefs: []
  type: TYPE_NORMAL
- en: DCGANs are an unsupervised learning form of *convolution neural networks (CNNs)*,
    which are commonly used—and with great success—in supervised learning systems
    for computer vision and image classification. Before we delve into DCGANs, let’s
    explore CNNs first, especially how they are used for image classification in supervised
    learning systems.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Compared to numerical and text data, images and video are considerably more
    computationally expensive to work with. For instance, a 4K Ultra HD image has
    dimensions of 4096 x 2160 x 3 (26,542,080) in total. Training a neural network
    on images of this resolution directly would require tens of millions of neurons
    and result in very long training times.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of building a neural network directly on the raw images, we can take
    advantage of certain properties of images, namely that pixels are related to other
    pixels that are close by but not usually related to other pixels that are far
    away.
  prefs: []
  type: TYPE_NORMAL
- en: '*Convolution* (from which convolutional neural networks derive their name)
    is the process of filtering the image to decrease the size of the image without
    losing the relationships among pixels.^([2](ch12.html#idm140637527543376))'
  prefs: []
  type: TYPE_NORMAL
- en: On the original image, we apply several filters of a certain size, known as
    the *kernel size*, and move these filters with a small step, known as the *stride*,
    to derive the new reduced pixel output. After the convolution, we reduce the size
    of the representation further by taking the max of the pixels in the reduced pixel
    output, one small area at a time. This is known as *max pooling*.
  prefs: []
  type: TYPE_NORMAL
- en: We perform this convolution and max pooling several times to reduce the complexity
    of the images. Then, we flatten the images and use a normal fully connected layer
    to perform image classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now build a CNN and use it to perform image classification on the MNIST
    dataset. First, we will load the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will load the MNIST datasets and store the image data in a 4D tensor
    since Keras requires image data in this format. We will also create one-hot vectors
    from the labels using the `to_categorical` function in Keras.
  prefs: []
  type: TYPE_NORMAL
- en: 'For use later, we will create Pandas DataFrames from the data, too. And, let’s
    reuse the `view_digit` function from earlier in the book to view the images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now let’s build the CNN.
  prefs: []
  type: TYPE_NORMAL
- en: We will call `Sequential()` in Keras to begin the model creation. Then, we will
    add two convolution layers, each with 32 filters of a kernel size of 5 x 5, a
    default stride of 1, and a ReLU activation. Then, we perform max pooling with
    a pooling window of 2 x 2 and a stride of 1\. We also perform dropout, which you
    may recall is a form of regularization to reduce overfitting of the neural network.
    Specifically, we will drop 25% of the input units.
  prefs: []
  type: TYPE_NORMAL
- en: In the next stage, we add two convolution layers again, this time with 64 filters
    of a kernel size of 3 x 3\. Then, we perform max pooling with a pooling window
    of 2 x 2 and a stride of 2\. And, we follow this up with a dropout layer, with
    a dropout percentage of 25%.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we flatten the images, add a regular neural network with 256 hidden
    units, perform dropout with a dropout percentage of 50%, and perform 10-class
    classification using the `softmax` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: For this CNN training, we will use the *Adam optimizer* and minimize the cross-entropy.
    We will also store the accuracy of the image classification as the evaluation
    metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s train the model for one hundred epochs and evaluate the results on
    the validation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 12-1](#cnn_results) displays the accuracy over the one hundred epochs
    of training.'
  prefs: []
  type: TYPE_NORMAL
- en: '![CNN Results](assets/hulp_1201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-1\. CNN results
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, the CNN we just trained has a final accuracy of 99.55%, better
    than any of the MNIST image classification solutions we have trained so far throughout
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: DCGANs Revisited
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s now turn back to deep convolutional generative adversarial networks once
    again. We will build a generative model to produce synthetic MNIST images that
    are very similar to the original MNIST ones.
  prefs: []
  type: TYPE_NORMAL
- en: To produce near-realistic yet synthetic images, we need to train a generator
    that generates new images from the original MNIST images and a discriminator that
    judges whether those images are believably similar to the original ones or not
    (essentially performing a bullshit test).
  prefs: []
  type: TYPE_NORMAL
- en: Here is another way to think about this. The original MNIST dataset represents
    the original data distribution. The generator learns from this original distribution
    and generates new images based off what it has learned, and the discriminator
    attempts to determine whether the newly generated images are virtually indistinguishable
    from the original distribution or not.
  prefs: []
  type: TYPE_NORMAL
- en: For the generator, we will use the architecture presented in the Radford, Metz,
    and Chintala paper presented at the ICLR 2016 conference, which we referenced
    earlier ([Figure 12-2](#dcgan_generator)).
  prefs: []
  type: TYPE_NORMAL
- en: '![DCGAN Generator](assets/hulp_1202.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-2\. DCGAN generator
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The generator takes in an initial *noise vector*, shown as a 100 x 1 noise vector
    here denoted as *z*, and then projects and reshapes it into a 1024 x 4 x 4 tensor.
    This *project and reshape* action is the opposite of convolution and is known
    as *transposed convolution* (or *deconvolution* in some cases). In transposed
    convolution, the original process of convolution is reversed, mapping a reduced
    tensor to a larger one.^([3](ch12.html#idm140637526722816))
  prefs: []
  type: TYPE_NORMAL
- en: After the initial transposed convolution, the generator applies four additional
    deconvolution layers to map to a final 64 x 3 x 3 tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the various stages:'
  prefs: []
  type: TYPE_NORMAL
- en: 100 x 1 → 1024 x 4 x 4 → 512 x 8 x 8 → 256 x 16 x 16 → 128 x 32 x 32 → 64 x
    64 x 3
  prefs: []
  type: TYPE_NORMAL
- en: We will apply a similar (but not exact) architecture when designing a DCGAN
    on the MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Generator of the DCGAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the DCGAN we design, we will leverage work done by Rowel Atienza and build
    on top of it.^([4](ch12.html#idm140637526717616)) We will first create a class
    called *DCGAN*, which we will use to build the generator, discriminator, discriminator
    model, and adversarial model.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with the generator. We will set several parameters for the generator,
    including the dropout percentage (default value of 0.3), the depth of the tensor
    (default value of 256), and the other dimensions (default value of 7 x 7). We
    will also use batch normalization with a default momentum value of 0.8\. The initial
    input dimensions are one hundred, and the final output dimensions are 28 x 28
    x 1.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that both dropout and batch normalization are regularizers to help the
    neural network we design avoid overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build the generator, we call the `Sequential()` function from Keras. Then,
    we will add a dense, fully connected neural network layer by calling the `Dense()`
    function. This will have an input dimension of 100 and an output dimension of
    7 x 7 x 256\. We will perform batch normalization, use the ReLU activation function,
    and perform dropout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will perform *upsampling* and *transposed convolution* three times.
    Each time, we will halve the depth of the output space from 256 to 128 to 64 to
    32 while increasing the other dimensions. We will maintain a convolution window
    of 5 x 5 and the default stride of one. During each transposed convolution, we
    will perform batch normalization and use the ReLU activation function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what this looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: 100 → 7 x 7 x 256 → 14 x 14 x 128 → 28 x 28 x 64 → 28 x 28 x 32 → 28 x 28 x
    1
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the generator will output a 28 x 28 image, which has the same dimensions
    as the original MNIST image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Discriminator of the DCGAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the discriminator, we will set the default dropout percentage to 0.3, the
    depth as 64, and the alpha for the `LeakyReLU` function as 0.3.^([5](ch12.html#idm140637525902512))
  prefs: []
  type: TYPE_NORMAL
- en: First, we will load a 28 x 28 x 1 image and perform convolution using 64 channels,
    a filter of 5 x 5, and a stride of two. We will use `LeakyReLU` as the activation
    function and perform dropout. We will continue this process three more times,
    doubling the depth of the output space each time while decreasing the other dimensions.
    For each convolution, we will use the `LeakyReLU` activation function and dropout.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will flatten the images and use the sigmoid function to output a
    probability. This probability designates the discriminator’s confidence in calling
    the input image a fake (where 0.0 is fake and 1.0 is real).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what this looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: 28 x 28 x 1 → 14 x 14 x 64 → 7 x 7 x 128 → 4 x 4 x 256 → 4 x 4 x 512 → 1
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Discriminator and Adversarial Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, let’s define the discriminator model (i.e., the police detecting the fakes)
    and the adversarial model (i.e., the counterfeiter learning from the police).
    For both the adversarial and the discriminator model, we will use the RMSprop
    optimizer, define the loss function as binary cross-entropy, and use accuracy
    as our reported metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the adversarial model, we use the generator and discriminator networks
    we defined earlier. For the discriminator model, we use just the discriminator
    network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: DCGAN for the MNIST Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now let’s define the DCGAN for the MNIST dataset. First, we will initialize
    the `MNIST_DCGAN` class for the 28 x 28 x 1 MNIST images and use the generator,
    discriminator model, and adversarial model from earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `train` function will train for a default two thousand training epochs
    and use a batch size of 256\. In this function, we will feed batches of images
    into the DCGAN architecture we just defined. The generator will generate images,
    and the discriminator will call out images as real or fake. As the generator and
    discriminator duke it out in this adversarial model, the synthetic images become
    more and more similar to the original MNIST images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s also define a function to plot the images generated by this DCGAN model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: MNIST DCGAN in Action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have defined the `MNIST_DCGAN` call, let’s call it and begin the
    training process. We will train for 10,000 epochs with a batch size of 256:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code displays the loss and the accuracy of the discriminator
    and the adversarial model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The initial loss of the discriminator fluctuates wildly but remains considerably
    above 0.50\. In other words, the discriminator is initially very good at catching
    the poorly constructed counterfeits from the generator. Then, as the generator
    becomes better at creating counterfeits, the discriminator struggles; its accuracy
    drops close to 0.50:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Synthetic Image Generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that the MNIST DCGAN has been trained, let’s use it to generate a sample
    of synthetic images ([Figure 12-3](#synthetic_images_generated_by_the_mnist_dcgan)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Synthetic Images Generated by the MNIST DCGAN](assets/hulp_1203.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-3\. Synthetic images generated by the MNIST DCGAN
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: These synthetic images—while not entirely indistinguishable from the real MNIST
    dataset—are eerily similar to real digits. With more training time, the MNIST
    DCGAN should be capable of generating synthetic images that more closely resemble
    those of the real MNIST dataset and could be used to supplement the size of that
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: While our solution is reasonably good, there are many ways to make the MNIST
    DCGAN perform better. The paper [“Improved Techniques for Training GANs”](https://arxiv.org/pdf/1606.03498.pdf)
    and the accompanying [code](https://github.com/openai/improved-gan) delves into
    more advanced methods to improve GAN performance.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored deep convolutional generative adversarial networks,
    a specialized form of generative adversarial networks that perform well on image
    and computer vision datasets.
  prefs: []
  type: TYPE_NORMAL
- en: GANs are a generative model with two neural networks locked in a zero-sum game.
    One of the networks, the generator (i.e., the counterfeiter), is generating synthetic
    data from real data, while the other network, the discriminator (i.e, the police),
    is calling the counterfeits fake or real.^([6](ch12.html#idm140637524873840))
    This zero-sum game in which the generator learns from the discriminator leads
    to an overall generative model that generates pretty realistic synthetic data
    and generally gets better over time (i.e., as we train for more training epochs).
  prefs: []
  type: TYPE_NORMAL
- en: GANs are relatively new—they were first introduced by Ian Goodfellow et al.
    in 2014.^([7](ch12.html#idm140637524871088)) GANs are currently mainly used to
    perform anomaly detection and generate synthetic data, but they could have many
    other applications in the near future. The machine learning community is barely
    scratching the surface with what is possible, and, if you decide to use GANs in
    applied machine learning systems, be ready to experiment a lot.^([8](ch12.html#idm140637524869328))
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 13](ch13.html#Chapter_13), we will conclude this part of the book
    by exploring temporal clustering, which is a form of unsupervised learning for
    use with time series data.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch12.html#idm140637527552048-marker)) For more on DCGANs, take a look
    at the [official paper on the topic](https://arxiv.org/abs/1511.06434).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch12.html#idm140637527543376-marker)) For more on convolution layers,
    read [“An Introduction to Different Types of Convolutions in Deep Learning”](http://bit.ly/2GeMQfu).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch12.html#idm140637526722816-marker)) For more on convolution layers,
    check out [“An Introduction to Different Types of Convolutions in Deep Learning”](http://bit.ly/2GeMQfu),
    also referenced earlier in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch12.html#idm140637526717616-marker)) For the original code base, visit
    [Rowel Atienza’s GitHub page](http://bit.ly/2DLp4G1).
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch12.html#idm140637525902512-marker)) `LeakyReLU` ([*https://keras.io/layers/advanced-activations/*](https://keras.io/layers/advanced-activations/))
    is an advanced activation function that is similar to the normal ReLU but allows
    a small gradient when the unit is not active. It is becoming a preferred activation
    function for image machine learning problems.
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch12.html#idm140637524873840-marker)) For additional information, check
    out [the OpenAI blog’s generative models post](https://blog.openai.com/generative-models/).
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch12.html#idm140637524871088-marker)) For more on this, take a look at
    this seminal [paper](https://arxiv.org/abs/1406.2661).
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch12.html#idm140637524869328-marker)) For some tips and tricks, read this
    post on how to [refine GANs](https://github.com/soumith/ganhacks) and [improve
    performance](http://bit.ly/2G2FJHq).
  prefs: []
  type: TYPE_NORMAL
