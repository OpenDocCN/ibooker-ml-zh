- en: Chapter 12\. Generative Adversarial Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章。生成对抗网络
- en: 'We have already explored two types of generative models: RBMs and DBNs. In
    this chapter, we will explore *generative adversarial networks (GANs)*, one of
    the latest and most promising areas of unsupervised learning and generative modeling.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探索了两种生成模型：RBM和DBN。在本章中，我们将探讨*生成对抗网络（GANs）*，这是无监督学习和生成建模中最新和最有前景的领域之一。
- en: GANs, the Concept
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GANs，概念
- en: GANs were introduced by Ian Goodfellow and his fellow researchers at the University
    of Montreal in 2014\. In GANs, we have two neural networks. One network known
    as the *generator* generates data based on a model it has created using samples
    of real data it has received as input. The other network known as the *discriminator*
    discriminates between the data created by the generator and data from the true
    distribution.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: GANs是由Ian Goodfellow及其蒙特利尔大学的同行研究人员在2014年引入的。在GANs中，我们有两个神经网络。一个网络称为*生成器*，根据其已创建的模型生成数据，该模型是使用其作为输入接收到的真实数据样本创建的。另一个网络称为*鉴别器*，用于区分生成器创建的数据和来自真实分布的数据。
- en: As a simple analogy, the generator is the counterfeiter, and the discriminator
    is the police trying to identify the forgery. The two networks are locked in a
    zero-sum game. The generator is trying to fool the discriminator into thinking
    the synthetic data comes from the true distribution, and the discriminator is
    trying to call out the synthetic data as fake.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 简单类比，生成器就像是伪造者，而鉴别器则是试图识别伪造品的警察。这两个网络处于零和博弈中。生成器试图欺骗鉴别器，使其认为合成数据来自真实分布，而鉴别器则试图揭露合成数据为假。
- en: GANs are unsupervised learning algorithms because the generator can learn the
    underlying structure of the true distribution even when there are no labels. The
    generator learns the underlying structure by using a number of parameters significantly
    smaller than the amount of data it has trained on—a core concept of unsupervised
    learning that we have explored many times in previous chapters. This constraint
    forces the generator to efficiently capture the most salient aspects of the true
    data distribution. This is similar to the representation learning that occurs
    in deep learning. Each hidden layer in the neutral network of a generator captures
    a representation of the underlying data—starting very simply—and subsequent layers
    pick up more complicated representations by building on the simpler preceding
    layers.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: GANs是无监督学习算法，因为即使没有标签，生成器也可以学习真实分布的基本结构。生成器通过使用比其训练的数据量明显较少的一些参数来学习基本结构——这是我们在前几章中多次探讨过的无监督学习的核心概念。这一约束迫使生成器有效地捕捉到真实数据分布的最显著方面。这类似于深度学习中发生的表示学习。生成器的每个隐藏层捕捉到数据的底层表示——从非常简单的开始——而后续层通过在简单前层基础上构建更复杂的表示来增强。
- en: Using all these layers together, the generator learns the underlying structure
    of the data and attempts to create synthetic data that is nearly identical to
    the true data. If the generator has captured the essence of the true data, the
    synthetic data will appear real.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 使用所有这些层次，生成器学习数据的基本结构，并尝试创建几乎与真实数据相同的合成数据。如果生成器捕捉到了真实数据的本质，那么合成数据看起来将会是真实的。
- en: The Power of GANs
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GANs的威力
- en: In [Chapter 11](ch11.html#Chapter_11), we explored the ability to use synthetic
    data from an unsupervised learning model (such as a deep belief network) to improve
    the performance of a supervised learning model. Like DBNs, GANs are very good
    at generating synthetic data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第11章](ch11.html#Chapter_11)中，我们探讨了利用无监督学习模型（如深度信念网络）生成的合成数据来提高监督学习模型性能的能力。像DBNs一样，GANs在生成合成数据方面非常擅长。
- en: If the objective is to generate a lot of new training examples to help supplement
    existing training data—for example, to improve accuracy on an image recognition
    task—we can use the generator to create a lot of synthetic data, add the new synthetic
    data to the original training data, and then run a supervised machine learning
    model on the now much larger dataset.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果目标是生成大量新的训练样本，以帮助补充现有的训练数据——例如，以提高图像识别任务的准确性——我们可以使用生成器创建大量合成数据，将新合成数据添加到原始训练数据中，然后在现在大得多的数据集上运行监督式机器学习模型。
- en: GANs can also excel at anomaly detection. If the objective is to identify anomalies—for
    example, to detect fraud, hacking, or other suspicious behavior—we can use the
    discriminator to score each instance in the real data. The instances that the
    discriminator ranks as “likely synthetic” will be the most anomalous instances
    and also the ones most likely to represent malicious behavior.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: GANs在异常检测方面也表现出色。如果目标是识别异常，例如检测欺诈、黑客攻击或其他可疑行为，我们可以使用判别器对真实数据中的每个实例进行评分。判别器排名为“可能合成”的实例将是最异常的实例，也是最有可能代表恶意行为的实例。
- en: Deep Convolutional GANs
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度卷积GANs
- en: In this chapter, we will return to the MNIST dataset we used in previous chapters
    and apply a version of GANs to generate synthetic data to supplement the existing
    MNIST dataset. We will then apply a supervised learning model to perform image
    classification. This is yet another version of semisupervised learning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将返回到我们在前几章中使用过的MNIST数据集，并应用一种GANs版本来生成合成数据以补充现有的MNIST数据集。然后我们将应用一个监督学习模型来进行图像分类。这是半监督学习的又一版本。
- en: Note
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: As a side note, you should now have a much deeper appreciation for semisupervised
    learning. Because much of the world’s data is unlabeled, the ability of unsupervised
    learning to efficiently help label data by itself is very powerful. As part of
    such semisupervised machine learning systems, unsupervised learning enhances the
    potential of all successful commercial applications of supervised learning to
    date.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，现在你应该对半监督学习有了更深的理解。因为世界上大部分数据都没有标签，无监督学习自身有效地帮助标记数据的能力非常强大。作为半监督机器学习系统的一部分，无监督学习增强了迄今为止所有成功商业应用的监督学习的潜力。
- en: Even outside of applications in semisupervised systems, unsupervised learning
    has potential on a standalone basis because it learns from data without any labels
    and is one of the fields of AI that has the greatest potential to help the machine
    learning community move from narrow AI to more AGI applications.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在半监督系统的应用之外，无监督学习也有独立运用的潜力，因为它能够从没有任何标签的数据中学习，并且是AI领域中从狭义AI向更广义AI应用迈进的最有潜力的领域之一。
- en: The version of GANs we will use is called *deep convolutional generative adversarial
    networks (DCGANs)*, which were first introduced in late 2015 by Alec Radford,
    Luke Metz, and Soumith Chintala.^([1](ch12.html#idm140637527552048))
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的 GANs 版本称为*深度卷积生成对抗网络（DCGANs）*，这是由Alec Radford、Luke Metz和Soumith Chintala于2015年底首次引入的^([1](ch12.html#idm140637527552048))。
- en: DCGANs are an unsupervised learning form of *convolution neural networks (CNNs)*,
    which are commonly used—and with great success—in supervised learning systems
    for computer vision and image classification. Before we delve into DCGANs, let’s
    explore CNNs first, especially how they are used for image classification in supervised
    learning systems.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: DCGANs是一种无监督学习的形式*卷积神经网络（CNNs）*，在监督学习系统中用于计算机视觉和图像分类方面被广泛使用并取得了巨大成功。在深入研究DCGANs之前，让我们首先探讨CNNs，特别是它们在监督学习系统中用于图像分类的方式。
- en: Convolutional Neural Networks
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: Compared to numerical and text data, images and video are considerably more
    computationally expensive to work with. For instance, a 4K Ultra HD image has
    dimensions of 4096 x 2160 x 3 (26,542,080) in total. Training a neural network
    on images of this resolution directly would require tens of millions of neurons
    and result in very long training times.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 与数值和文本数据相比，图像和视频的计算成本要高得多。例如，一个4K Ultra HD图像的尺寸总共为4096 x 2160 x 3（26,542,080）。直接在这种分辨率的图像上训练神经网络将需要数千万个神经元，并且导致非常长的训练时间。
- en: Instead of building a neural network directly on the raw images, we can take
    advantage of certain properties of images, namely that pixels are related to other
    pixels that are close by but not usually related to other pixels that are far
    away.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是直接在原始图像上构建神经网络，我们可以利用图像的某些属性，即像素与附近的像素相关联，但通常与远处的像素无关。
- en: '*Convolution* (from which convolutional neural networks derive their name)
    is the process of filtering the image to decrease the size of the image without
    losing the relationships among pixels.^([2](ch12.html#idm140637527543376))'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*卷积*（从中卷积神经网络得名）是将图像进行滤波处理以减小图像尺寸而不丢失像素之间关系的过程。^([2](ch12.html#idm140637527543376))'
- en: On the original image, we apply several filters of a certain size, known as
    the *kernel size*, and move these filters with a small step, known as the *stride*,
    to derive the new reduced pixel output. After the convolution, we reduce the size
    of the representation further by taking the max of the pixels in the reduced pixel
    output, one small area at a time. This is known as *max pooling*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始图像上，我们应用几个特定大小的滤波器，称为*核大小*，并以小步长移动这些滤波器，称为*步幅*，以得出新的减少像素输出。卷积后，我们通过逐个小区域获取减少像素输出中的像素的最大值来进一步减小表示的大小。这称为*最大池化*。
- en: We perform this convolution and max pooling several times to reduce the complexity
    of the images. Then, we flatten the images and use a normal fully connected layer
    to perform image classification.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们多次执行这种卷积和最大池化，以降低图像的复杂性。然后，我们展平图像并使用正常的全连接层进行图像分类。
- en: 'Let’s now build a CNN and use it to perform image classification on the MNIST
    dataset. First, we will load the necessary libraries:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们构建一个CNN，并在MNIST数据集上进行图像分类。首先，我们将加载必要的库：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, we will load the MNIST datasets and store the image data in a 4D tensor
    since Keras requires image data in this format. We will also create one-hot vectors
    from the labels using the `to_categorical` function in Keras.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将加载MNIST数据集，并将图像数据存储在4D张量中，因为Keras需要图像数据以这种格式。我们还将使用Keras中的`to_categorical`函数从标签创建独热向量。
- en: 'For use later, we will create Pandas DataFrames from the data, too. And, let’s
    reuse the `view_digit` function from earlier in the book to view the images:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以后使用，我们还将从数据中创建Pandas DataFrames。让我们再次使用本书早期的`view_digit`函数来查看这些图像：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now let’s build the CNN.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们构建CNN。
- en: We will call `Sequential()` in Keras to begin the model creation. Then, we will
    add two convolution layers, each with 32 filters of a kernel size of 5 x 5, a
    default stride of 1, and a ReLU activation. Then, we perform max pooling with
    a pooling window of 2 x 2 and a stride of 1\. We also perform dropout, which you
    may recall is a form of regularization to reduce overfitting of the neural network.
    Specifically, we will drop 25% of the input units.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在Keras中调用`Sequential()`开始模型创建。然后，我们将添加两个卷积层，每个层有32个大小为5 x 5的过滤器，默认步幅为1，并使用ReLU激活函数。然后，我们使用2
    x 2的池化窗口和1的步幅进行最大池化。我们还执行dropout，你可能记得这是一种正则化形式，用于减少神经网络的过拟合。具体来说，我们将丢弃输入单元的25%。
- en: In the next stage, we add two convolution layers again, this time with 64 filters
    of a kernel size of 3 x 3\. Then, we perform max pooling with a pooling window
    of 2 x 2 and a stride of 2\. And, we follow this up with a dropout layer, with
    a dropout percentage of 25%.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一阶段，我们再次添加两个卷积层，这次使用64个大小为3 x 3的过滤器。然后，我们使用2 x 2的池化窗口和2的步幅进行最大池化。接着，我们添加一个dropout层，dropout比例为25%。
- en: 'Finally, we flatten the images, add a regular neural network with 256 hidden
    units, perform dropout with a dropout percentage of 50%, and perform 10-class
    classification using the `softmax` function:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将图像展平，添加一个具有256个隐藏单元的常规神经网络，使用50%的dropout比例进行dropout，并使用`softmax`函数进行10类分类：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For this CNN training, we will use the *Adam optimizer* and minimize the cross-entropy.
    We will also store the accuracy of the image classification as the evaluation
    metric.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个CNN训练，我们将使用*Adam优化器*并最小化交叉熵。我们还将将图像分类的准确性作为评估指标存储。
- en: 'Now let’s train the model for one hundred epochs and evaluate the results on
    the validation set:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们对模型进行一百个epochs的训练，并在验证集上评估结果：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[Figure 12-1](#cnn_results) displays the accuracy over the one hundred epochs
    of training.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12-1](#cnn_results)显示了训练一百个epochs后的准确性。'
- en: '![CNN Results](assets/hulp_1201.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![CNN结果](assets/hulp_1201.png)'
- en: Figure 12-1\. CNN results
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-1\. CNN结果
- en: As you can see, the CNN we just trained has a final accuracy of 99.55%, better
    than any of the MNIST image classification solutions we have trained so far throughout
    this book.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们刚刚训练的CNN最终准确率达到了99.55%，优于本书中迄今为止训练过的任何MNIST图像分类解决方案。
- en: DCGANs Revisited
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新审视DCGANs
- en: Let’s now turn back to deep convolutional generative adversarial networks once
    again. We will build a generative model to produce synthetic MNIST images that
    are very similar to the original MNIST ones.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们再次转向深度卷积生成对抗网络。我们将建立一个生成模型，生成与原始MNIST图像非常相似的合成MNIST图像。
- en: To produce near-realistic yet synthetic images, we need to train a generator
    that generates new images from the original MNIST images and a discriminator that
    judges whether those images are believably similar to the original ones or not
    (essentially performing a bullshit test).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成接近真实但合成的图像，我们需要训练一个生成器，从原始的 MNIST 图像生成新的图像，以及一个判别器，判断这些图像是否与原始图像相似（基本上执行一种“胡说测试”）。
- en: Here is another way to think about this. The original MNIST dataset represents
    the original data distribution. The generator learns from this original distribution
    and generates new images based off what it has learned, and the discriminator
    attempts to determine whether the newly generated images are virtually indistinguishable
    from the original distribution or not.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还有另一种思考方式。原始的 MNIST 数据集代表了原始的数据分布。生成器从这个原始分布中学习，并基于所学内容生成新的图像，而判别器则试图确定新生成的图像是否与原始分布几乎无法区分。
- en: For the generator, we will use the architecture presented in the Radford, Metz,
    and Chintala paper presented at the ICLR 2016 conference, which we referenced
    earlier ([Figure 12-2](#dcgan_generator)).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生成器，我们将使用 Radford、Metz 和 Chintala 在 ICLR 2016 年会议上提出的架构，这是我们之前引用过的（见 [图 12-2](#dcgan_generator)）。
- en: '![DCGAN Generator](assets/hulp_1202.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![DCGAN 生成器](assets/hulp_1202.png)'
- en: Figure 12-2\. DCGAN generator
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-2。DCGAN 生成器
- en: The generator takes in an initial *noise vector*, shown as a 100 x 1 noise vector
    here denoted as *z*, and then projects and reshapes it into a 1024 x 4 x 4 tensor.
    This *project and reshape* action is the opposite of convolution and is known
    as *transposed convolution* (or *deconvolution* in some cases). In transposed
    convolution, the original process of convolution is reversed, mapping a reduced
    tensor to a larger one.^([3](ch12.html#idm140637526722816))
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器接受一个初始的 *噪声向量*，这里显示为 100 x 1 的噪声向量，表示为 *z*，然后将其投影和重塑成 1024 x 4 x 4 张量。这种
    *投影和重塑* 行为是卷积的反向过程，被称为 *转置卷积*（或在某些情况下称为 *反卷积*）。在转置卷积中，卷积的原始过程被反转，将一个缩小的张量映射到一个较大的张量^([3](ch12.html#idm140637526722816))。
- en: After the initial transposed convolution, the generator applies four additional
    deconvolution layers to map to a final 64 x 3 x 3 tensor.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始的转置卷积之后，生成器应用四个额外的反卷积层映射到最终的 64 x 3 x 3 张量。
- en: 'Here are the various stages:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是各个阶段：
- en: 100 x 1 → 1024 x 4 x 4 → 512 x 8 x 8 → 256 x 16 x 16 → 128 x 32 x 32 → 64 x
    64 x 3
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 100 x 1 → 1024 x 4 x 4 → 512 x 8 x 8 → 256 x 16 x 16 → 128 x 32 x 32 → 64 x
    64 x 3
- en: We will apply a similar (but not exact) architecture when designing a DCGAN
    on the MNIST dataset.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计 MNIST 数据集上的 DCGAN 时，我们将应用类似（但不完全相同）的架构。
- en: Generator of the DCGAN
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DCGAN 的生成器
- en: For the DCGAN we design, we will leverage work done by Rowel Atienza and build
    on top of it.^([4](ch12.html#idm140637526717616)) We will first create a class
    called *DCGAN*, which we will use to build the generator, discriminator, discriminator
    model, and adversarial model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们设计的 DCGAN，我们将利用 Rowel Atienza 的工作并在此基础上构建^([4](ch12.html#idm140637526717616))。我们首先会创建一个名为
    *DCGAN* 的类，用于构建生成器、判别器、判别器模型和对抗模型。
- en: Let’s start with the generator. We will set several parameters for the generator,
    including the dropout percentage (default value of 0.3), the depth of the tensor
    (default value of 256), and the other dimensions (default value of 7 x 7). We
    will also use batch normalization with a default momentum value of 0.8\. The initial
    input dimensions are one hundred, and the final output dimensions are 28 x 28
    x 1.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从生成器开始。我们将为生成器设置几个参数，包括辍学率（默认值为 0.3）、张量的深度（默认值为 256）以及其他维度（默认值为 7 x 7）。我们还将使用批归一化，其默认动量值为
    0.8。初始输入维度为一百，最终输出维度为 28 x 28 x 1。
- en: Recall that both dropout and batch normalization are regularizers to help the
    neural network we design avoid overfitting.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，辍学和批归一化都是正则化器，帮助我们设计的神经网络避免过拟合。
- en: 'To build the generator, we call the `Sequential()` function from Keras. Then,
    we will add a dense, fully connected neural network layer by calling the `Dense()`
    function. This will have an input dimension of 100 and an output dimension of
    7 x 7 x 256\. We will perform batch normalization, use the ReLU activation function,
    and perform dropout:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建生成器，我们从 Keras 中调用 `Sequential()` 函数。然后，我们通过调用 `Dense()` 函数添加一个全连接神经网络层。它的输入维度为
    100，输出维度为 7 x 7 x 256。我们将执行批归一化，使用 ReLU 激活函数，并执行辍学：
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, we will perform *upsampling* and *transposed convolution* three times.
    Each time, we will halve the depth of the output space from 256 to 128 to 64 to
    32 while increasing the other dimensions. We will maintain a convolution window
    of 5 x 5 and the default stride of one. During each transposed convolution, we
    will perform batch normalization and use the ReLU activation function.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将进行 *上采样* 和 *转置卷积* 三次。每次，我们将输出空间的深度从 256 逐渐减半至 128、64、32，并增加其他维度。我们将保持
    5 x 5 的卷积窗口和默认的步幅为一。在每次转置卷积期间，我们将执行批归一化，并使用 ReLU 激活函数。
- en: 'Here is what this looks like:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是它的样子：
- en: 100 → 7 x 7 x 256 → 14 x 14 x 128 → 28 x 28 x 64 → 28 x 28 x 32 → 28 x 28 x
    1
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 100 → 7 x 7 x 256 → 14 x 14 x 128 → 28 x 28 x 64 → 28 x 28 x 32 → 28 x 28 x
    1
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, the generator will output a 28 x 28 image, which has the same dimensions
    as the original MNIST image:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，生成器将输出一个 28 x 28 的图像，与原始 MNIST 图像具有相同的尺寸：
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Discriminator of the DCGAN
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DCGAN 的鉴别器
- en: For the discriminator, we will set the default dropout percentage to 0.3, the
    depth as 64, and the alpha for the `LeakyReLU` function as 0.3.^([5](ch12.html#idm140637525902512))
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于鉴别器，我们将将默认的 dropout 百分比设置为 0.3，深度为 64，并将 `LeakyReLU` 函数的 alpha 设置为 0.3。^([5](ch12.html#idm140637525902512))
- en: First, we will load a 28 x 28 x 1 image and perform convolution using 64 channels,
    a filter of 5 x 5, and a stride of two. We will use `LeakyReLU` as the activation
    function and perform dropout. We will continue this process three more times,
    doubling the depth of the output space each time while decreasing the other dimensions.
    For each convolution, we will use the `LeakyReLU` activation function and dropout.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将加载一个 28 x 28 x 1 的图像，并使用 64 个通道、5 x 5 的滤波器和步幅为二进行卷积。我们将使用 `LeakyReLU`
    作为激活函数，并执行 dropout。我们将继续这个过程三次，每次将输出空间的深度加倍，同时减少其他维度。对于每个卷积，我们将使用 `LeakyReLU`
    激活函数和 dropout。
- en: Finally, we will flatten the images and use the sigmoid function to output a
    probability. This probability designates the discriminator’s confidence in calling
    the input image a fake (where 0.0 is fake and 1.0 is real).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将展平图像，并使用 Sigmoid 函数输出一个概率。这个概率表示鉴别器对输入图像判断为伪造的信心程度（0.0 表示伪造，1.0 表示真实）。
- en: 'Here is what this looks like:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这是它的样子：
- en: 28 x 28 x 1 → 14 x 14 x 64 → 7 x 7 x 128 → 4 x 4 x 256 → 4 x 4 x 512 → 1
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 28 x 28 x 1 → 14 x 14 x 64 → 7 x 7 x 128 → 4 x 4 x 256 → 4 x 4 x 512 → 1
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Discriminator and Adversarial Models
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 鉴别器和对抗模型
- en: Next, let’s define the discriminator model (i.e., the police detecting the fakes)
    and the adversarial model (i.e., the counterfeiter learning from the police).
    For both the adversarial and the discriminator model, we will use the RMSprop
    optimizer, define the loss function as binary cross-entropy, and use accuracy
    as our reported metric.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义鉴别器模型（即检测伪造品的警察）和对抗模型（即从警察学习的伪造者）。对于对抗模型和鉴别器模型，我们将使用 RMSprop 优化器，将损失函数定义为二元交叉熵，并使用准确率作为我们的报告指标。
- en: 'For the adversarial model, we use the generator and discriminator networks
    we defined earlier. For the discriminator model, we use just the discriminator
    network:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于对抗模型，我们使用之前定义的生成器和鉴别器网络。对于鉴别器模型，我们仅使用鉴别器网络：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: DCGAN for the MNIST Dataset
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于 MNIST 数据集的 DCGAN
- en: 'Now let’s define the DCGAN for the MNIST dataset. First, we will initialize
    the `MNIST_DCGAN` class for the 28 x 28 x 1 MNIST images and use the generator,
    discriminator model, and adversarial model from earlier:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们为 MNIST 数据集定义 DCGAN。首先，我们将为 28 x 28 x 1 的 MNIST 图像初始化 `MNIST_DCGAN` 类，并使用之前定义的生成器、鉴别器模型和对抗模型：
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `train` function will train for a default two thousand training epochs
    and use a batch size of 256\. In this function, we will feed batches of images
    into the DCGAN architecture we just defined. The generator will generate images,
    and the discriminator will call out images as real or fake. As the generator and
    discriminator duke it out in this adversarial model, the synthetic images become
    more and more similar to the original MNIST images:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`train` 函数将默认进行两千次训练周期，并使用批大小为 256。在这个函数中，我们将批量的图像输入到刚刚定义的 DCGAN 架构中。生成器将生成图像，鉴别器将判断图像是真实的还是假的。在这个对抗模型中，随着生成器和鉴别器的较量，合成图像变得越来越接近原始的
    MNIST 图像：'
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let’s also define a function to plot the images generated by this DCGAN model:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也来定义一个函数来绘制由这个 DCGAN 模型生成的图像：
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: MNIST DCGAN in Action
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MNIST DCGAN 的实际应用
- en: 'Now that we have defined the `MNIST_DCGAN` call, let’s call it and begin the
    training process. We will train for 10,000 epochs with a batch size of 256:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了`MNIST_DCGAN`调用，让我们调用它并开始训练过程。我们将使用256的批次大小训练10,000个epochs：
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following code displays the loss and the accuracy of the discriminator
    and the adversarial model:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码显示了判别器和对抗模型的损失和准确率：
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The initial loss of the discriminator fluctuates wildly but remains considerably
    above 0.50\. In other words, the discriminator is initially very good at catching
    the poorly constructed counterfeits from the generator. Then, as the generator
    becomes better at creating counterfeits, the discriminator struggles; its accuracy
    drops close to 0.50:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器的初始损失波动很大，但始终保持在0.50以上。换句话说，判别器最初非常擅长捕捉生成器生成的低质量赝品。随着生成器变得越来越擅长创建赝品，判别器开始困难；其准确率接近0.50：
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Synthetic Image Generation
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合成图像生成
- en: Now that the MNIST DCGAN has been trained, let’s use it to generate a sample
    of synthetic images ([Figure 12-3](#synthetic_images_generated_by_the_mnist_dcgan)).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在MNIST DCGAN已经训练完毕，让我们使用它生成一些合成图像的样本（[图12-3](#synthetic_images_generated_by_the_mnist_dcgan)）。
- en: '![Synthetic Images Generated by the MNIST DCGAN](assets/hulp_1203.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![MNIST DCGAN 生成的合成图像](assets/hulp_1203.png)'
- en: Figure 12-3\. Synthetic images generated by the MNIST DCGAN
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-3\. MNIST DCGAN 生成的合成图像
- en: These synthetic images—while not entirely indistinguishable from the real MNIST
    dataset—are eerily similar to real digits. With more training time, the MNIST
    DCGAN should be capable of generating synthetic images that more closely resemble
    those of the real MNIST dataset and could be used to supplement the size of that
    dataset.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这些合成图像——虽然不能完全与真实的MNIST数据集区分开来——与真实数字非常相似。随着训练时间的增加，MNIST DCGAN应该能够生成更接近真实MNIST数据集的合成图像，并可用于扩充该数据集的规模。
- en: While our solution is reasonably good, there are many ways to make the MNIST
    DCGAN perform better. The paper [“Improved Techniques for Training GANs”](https://arxiv.org/pdf/1606.03498.pdf)
    and the accompanying [code](https://github.com/openai/improved-gan) delves into
    more advanced methods to improve GAN performance.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们的解决方案相对不错，但有许多方法可以使MNIST DCGAN表现更好。论文["Improved Techniques for Training
    GANs"](https://arxiv.org/pdf/1606.03498.pdf)和其附带的[代码](https://github.com/openai/improved-gan)深入探讨了改进GAN性能的更高级方法。
- en: Conclusion
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, we explored deep convolutional generative adversarial networks,
    a specialized form of generative adversarial networks that perform well on image
    and computer vision datasets.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了深度卷积生成对抗网络（DCGAN），这是一种专门用于图像和计算机视觉数据集的生成对抗网络形式。
- en: GANs are a generative model with two neural networks locked in a zero-sum game.
    One of the networks, the generator (i.e., the counterfeiter), is generating synthetic
    data from real data, while the other network, the discriminator (i.e, the police),
    is calling the counterfeits fake or real.^([6](ch12.html#idm140637524873840))
    This zero-sum game in which the generator learns from the discriminator leads
    to an overall generative model that generates pretty realistic synthetic data
    and generally gets better over time (i.e., as we train for more training epochs).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: GAN是一种具有两个神经网络的生成模型，它们被锁定在一个零和博弈中。其中一个网络是生成器（即伪造者），从真实数据中生成合成数据，而另一个网络是判别器（即警察），负责判断伪造品是真实还是假的。^([6](ch12.html#idm140637524873840))
    生成器从判别器中学习的这种零和博弈导致一个总体上生成相当逼真的合成数据的生成模型，并且通常随着训练时间的增加而变得更好。
- en: GANs are relatively new—they were first introduced by Ian Goodfellow et al.
    in 2014.^([7](ch12.html#idm140637524871088)) GANs are currently mainly used to
    perform anomaly detection and generate synthetic data, but they could have many
    other applications in the near future. The machine learning community is barely
    scratching the surface with what is possible, and, if you decide to use GANs in
    applied machine learning systems, be ready to experiment a lot.^([8](ch12.html#idm140637524869328))
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: GAN（生成对抗网络）相对较新 —— 首次由Ian Goodfellow等人于2014年提出。^([7](ch12.html#idm140637524871088))
    GAN目前主要用于异常检测和生成合成数据，但在不久的将来可能有许多其他应用。机器学习社区仅仅开始探索其可能性，如果你决定在应用的机器学习系统中使用GAN，一定要做好大量实验的准备。^([8](ch12.html#idm140637524869328))
- en: In [Chapter 13](ch13.html#Chapter_13), we will conclude this part of the book
    by exploring temporal clustering, which is a form of unsupervised learning for
    use with time series data.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 13 章](ch13.html#Chapter_13) 中，我们将通过探索时间聚类来结束本书的这一部分内容，这是一种用于处理时间序列数据的无监督学习方法。
- en: ^([1](ch12.html#idm140637527552048-marker)) For more on DCGANs, take a look
    at the [official paper on the topic](https://arxiv.org/abs/1511.06434).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch12.html#idm140637527552048-marker)) 想深入了解 DCGANs，可以参考该主题的[官方论文](https://arxiv.org/abs/1511.06434)。
- en: ^([2](ch12.html#idm140637527543376-marker)) For more on convolution layers,
    read [“An Introduction to Different Types of Convolutions in Deep Learning”](http://bit.ly/2GeMQfu).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch12.html#idm140637527543376-marker)) 想了解更多关于卷积层的内容，可以阅读《深度学习中不同类型卷积的介绍》[一文](http://bit.ly/2GeMQfu)。
- en: ^([3](ch12.html#idm140637526722816-marker)) For more on convolution layers,
    check out [“An Introduction to Different Types of Convolutions in Deep Learning”](http://bit.ly/2GeMQfu),
    also referenced earlier in the chapter.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch12.html#idm140637526722816-marker)) 想了解更多关于卷积层的内容，可以查看《深度学习中不同类型卷积的介绍》[一文](http://bit.ly/2GeMQfu)，这篇文章也在本章中有提及。
- en: ^([4](ch12.html#idm140637526717616-marker)) For the original code base, visit
    [Rowel Atienza’s GitHub page](http://bit.ly/2DLp4G1).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch12.html#idm140637526717616-marker)) 想获取原始代码基础，请访问[Rowel Atienza 的 GitHub
    页面](http://bit.ly/2DLp4G1)。
- en: ^([5](ch12.html#idm140637525902512-marker)) `LeakyReLU` ([*https://keras.io/layers/advanced-activations/*](https://keras.io/layers/advanced-activations/))
    is an advanced activation function that is similar to the normal ReLU but allows
    a small gradient when the unit is not active. It is becoming a preferred activation
    function for image machine learning problems.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch12.html#idm140637525902512-marker)) `LeakyReLU`（[*https://keras.io/layers/advanced-activations/*](https://keras.io/layers/advanced-activations/)）是一种先进的激活函数，类似于普通的
    ReLU，但在单元不活跃时允许一个小的梯度。它正在成为图像机器学习问题中首选的激活函数。
- en: ^([6](ch12.html#idm140637524873840-marker)) For additional information, check
    out [the OpenAI blog’s generative models post](https://blog.openai.com/generative-models/).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch12.html#idm140637524873840-marker)) 想获取更多信息，请查阅[OpenAI 博客上的生成模型文章](https://blog.openai.com/generative-models/)。
- en: ^([7](ch12.html#idm140637524871088-marker)) For more on this, take a look at
    this seminal [paper](https://arxiv.org/abs/1406.2661).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch12.html#idm140637524871088-marker)) 想了解更多相关内容，请参阅这篇重要的[论文](https://arxiv.org/abs/1406.2661)。
- en: ^([8](ch12.html#idm140637524869328-marker)) For some tips and tricks, read this
    post on how to [refine GANs](https://github.com/soumith/ganhacks) and [improve
    performance](http://bit.ly/2G2FJHq).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch12.html#idm140637524869328-marker)) 阅读这篇关于如何[优化 GANs](https://github.com/soumith/ganhacks)和[提升性能](http://bit.ly/2G2FJHq)的文章，可以了解一些技巧和窍门。
