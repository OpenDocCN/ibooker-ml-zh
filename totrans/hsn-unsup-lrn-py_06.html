<html><head></head><body><section data-pdf-bookmark="Chapter 4. Anomaly Detection" data-type="chapter" epub:type="chapter"><div class="chapter" id="Chapter_4">&#13;
<h1><span class="label">Chapter 4. </span>Anomaly Detection</h1>&#13;
&#13;
&#13;
<p>In <a data-type="xref" href="ch03.html#Chapter_3">Chapter 3</a>, we introduced the core dimensionality reduction algorithms and explored their ability to capture the most salient information in the MNIST digits database in significantly fewer dimensions than the original 784 dimensions. Even in just two dimensions, the algorithms meaningfully separated the digits, without using labels. This is the power of unsupervised learning algorithms—they can learn the underlying structure of data and help discover hidden patterns in the absence of labels.</p>&#13;
&#13;
<p>Let’s build an applied machine learning solution using these dimensionality reduction methods. We will turn to the problem we introduced in <a data-type="xref" href="ch02.html#Chapter_2">Chapter 2</a> and build a credit card fraud detection system without using labels.</p>&#13;
&#13;
<p>In the real world, fraud often goes undiscovered, and only the fraud that is caught provides any labels for the datasets. Moreover, fraud patterns change over time, so supervised systems that are built using fraud labels—like the one we built in <a data-type="xref" href="ch02.html#Chapter_2">Chapter 2</a>—become stale, capturing historical patterns of fraud but failing to adapt to newly emerging patterns.</p>&#13;
&#13;
<p>For these reasons (the lack of sufficient labels and the need to adapt to newly emerging patterns of fraud as quickly as possible), unsupervised learning fraud detection systems are in vogue.</p>&#13;
&#13;
<p>In this chapter, we will build such a solution using some of the dimensionality reduction algorithms we explored in the previous chapter.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before" data-pdf-bookmark="Credit Card Fraud Detection" data-type="sect1"><div class="sect1" id="idm140637549957040">&#13;
<h1>Credit Card Fraud Detection</h1>&#13;
&#13;
<p>Let’s<a data-primary="anomaly detection" data-secondary="credit card fraud detection" data-type="indexterm" id="ADcredit04"/> revisit the credit card transactions problem from <a data-type="xref" href="ch02.html#Chapter_2">Chapter 2</a>.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Prepare the Data" data-type="sect2"><div class="sect2" id="idm140637549952928">&#13;
<h2>Prepare the Data</h2>&#13;
&#13;
<p>Like<a data-primary="credit card fraud detection" data-secondary="data preparation" data-type="indexterm" id="idm140637549951360"/><a data-primary="credit card fraud detection" data-see="also machine learning example project" data-type="indexterm" id="idm140637549950384"/> we did in <a data-type="xref" href="ch02.html#Chapter_2">Chapter 2</a>, let’s load the credit card transactions dataset, generate the features matrix and labels array, and split the data into training and test sets. We will not use the labels to perform anomaly detection, but we will use the labels to help evaluate the fraud detection systems we build.</p>&#13;
&#13;
<p>As a reminder, we have 284,807 credit card transactions in total, of which 492 are fraudulent, with a positive (fraud) label of one. The rest are normal transactions, with a negative (not fraud) label of zero.</p>&#13;
&#13;
<p>We have 30 features to use for anomaly detection—time, amount, and 28 principal components. And, we will split the dataset into a training set (with 190,820 transactions and 330 cases of fraud) and a test set (with the remaining 93,987 transactions and 162 cases of fraud):</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load datasets</code>&#13;
<code class="n">current_path</code> <code class="o">=</code> <code class="n">os</code><code class="o">.</code><code class="n">getcwd</code><code class="p">()</code>&#13;
<code class="nb">file</code> <code class="o">=</code> <code class="s1">'</code><code class="se">\\</code><code class="s1">datasets</code><code class="se">\\</code><code class="s1">credit_card_data</code><code class="se">\\</code><code class="s1">credit_card.csv'</code>&#13;
<code class="n">data</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="n">current_path</code> <code class="o">+</code> <code class="nb">file</code><code class="p">)</code>&#13;
&#13;
<code class="n">dataX</code> <code class="o">=</code> <code class="n">data</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code><code class="o">.</code><code class="n">drop</code><code class="p">([</code><code class="s1">'Class'</code><code class="p">],</code><code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>&#13;
<code class="n">dataY</code> <code class="o">=</code> <code class="n">data</code><code class="p">[</code><code class="s1">'Class'</code><code class="p">]</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code>&#13;
&#13;
<code class="n">featuresToScale</code> <code class="o">=</code> <code class="n">dataX</code><code class="o">.</code><code class="n">columns</code>&#13;
<code class="n">sX</code> <code class="o">=</code> <code class="n">pp</code><code class="o">.</code><code class="n">StandardScaler</code><code class="p">(</code><code class="n">copy</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>&#13;
<code class="n">dataX</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code><code class="n">featuresToScale</code><code class="p">]</code> <code class="o">=</code> <code class="n">sX</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">dataX</code><code class="p">[</code><code class="n">featuresToScale</code><code class="p">])</code>&#13;
&#13;
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> \&#13;
    <code class="n">train_test_split</code><code class="p">(</code><code class="n">dataX</code><code class="p">,</code> <code class="n">dataY</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="mf">0.33</code><code class="p">,</code> \&#13;
                    <code class="n">random_state</code><code class="o">=</code><code class="mi">2018</code><code class="p">,</code> <code class="n">stratify</code><code class="o">=</code><code class="n">dataY</code><code class="p">)</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Define Anomaly Score Function" data-type="sect2"><div class="sect2" id="idm140637549944720">&#13;
<h2>Define Anomaly Score Function</h2>&#13;
&#13;
<p>Next, we<a data-primary="anomaly Scores function" data-type="indexterm" id="idm140637550070576"/><a data-primary="credit card fraud detection" data-secondary="defining anomaly score function" data-type="indexterm" id="idm140637550069872"/> need to define a function that calculates how anomalous each transaction is. The more anomalous the transaction is, the more likely it is to be fraudulent, assuming that fraud is rare and looks somewhat different than the majority of transactions, which are normal.</p>&#13;
&#13;
<p>As we discussed in the previous chapter, dimensionality reduction algorithms reduce the dimensionality of data while attempting to minimize the reconstruction error. In other words, these algorithms try to capture the most salient information of the original features in such a way that they can reconstruct the original feature set from the reduced feature set as well as possible. However, these dimensionality reduction algorithms cannot capture all the information of the original features as they move to a lower dimensional space; therefore, there will be some error as these algorithms reconstruct the reduced feature set back to the original number of dimensions.</p>&#13;
&#13;
<p>In the context of our credit card transactions dataset, the algorithms will have the largest reconstruction error on those transactions that are hardest to model—in other words, those that occur the least often and are the most anomalous. Since fraud is rare and presumably different than normal transactions, the fraudulent transactions should exhibit the largest reconstruction error. So let’s define the anomaly score as the reconstruction error. The reconstruction error for each transaction is the sum of the squared differences between the original feature matrix and the reconstructed matrix using the dimensionality reduction algorithm. We will scale the sum of the squared differences by the max-min range of the sum of the squared differences for the entire dataset, so that all the reconstruction errors are within a zero to one range.</p>&#13;
&#13;
<p>The transactions that have the largest sum of squared differences will have an error close to one, while those that have the smallest sum of squared differences will have an error close to zero.</p>&#13;
&#13;
<p>This should be familiar. Like the supervised fraud detection solution we built in <a data-type="xref" href="ch02.html#Chapter_2">Chapter 2</a>, the dimensionality reduction algorithm will effectively assign each transaction an anomaly score between zero and one. Zero is normal and one is anomalous (and most likely to be fraudulent).</p>&#13;
&#13;
<p>Here is the function:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">anomalyScores</code><code class="p">(</code><code class="n">originalDF</code><code class="p">,</code> <code class="n">reducedDF</code><code class="p">):</code>&#13;
    <code class="n">loss</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">sum</code><code class="p">((</code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">originalDF</code><code class="p">)</code><code class="o">-</code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">reducedDF</code><code class="p">))</code><code class="o">**</code><code class="mi">2</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>&#13;
    <code class="n">loss</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">loss</code><code class="p">,</code><code class="n">index</code><code class="o">=</code><code class="n">originalDF</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>&#13;
    <code class="n">loss</code> <code class="o">=</code> <code class="p">(</code><code class="n">loss</code><code class="o">-</code><code class="n">np</code><code class="o">.</code><code class="n">min</code><code class="p">(</code><code class="n">loss</code><code class="p">))</code><code class="o">/</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="n">loss</code><code class="p">)</code><code class="o">-</code><code class="n">np</code><code class="o">.</code><code class="n">min</code><code class="p">(</code><code class="n">loss</code><code class="p">))</code>&#13;
    <code class="k">return</code> <code class="n">loss</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Define Evaluation Metrics" data-type="sect2"><div class="sect2" id="idm140637549843664">&#13;
<h2>Define Evaluation Metrics</h2>&#13;
&#13;
<p>Although<a data-primary="credit card fraud detection" data-secondary="defining evaluation metrics" data-type="indexterm" id="idm140637549763536"/> we will not use the fraud labels to build the unsupervised fraud detection solutions, we will use the labels to evaluate the unsupervised solutions we develop. The labels will help us understand just how well these solutions are at catching known patterns of fraud.</p>&#13;
&#13;
<p>As<a data-primary="precision" data-type="indexterm" id="idm140637549761824"/><a data-primary="evaluation metrics" data-secondary="precision-recall curve" data-type="indexterm" id="idm140637549761088"/><a data-primary="precision-recall curve" data-type="indexterm" id="idm140637549760144"/> we did in <a data-type="xref" href="ch02.html#Chapter_2">Chapter 2</a>, we will use the precision-recall curve, the average precision, and the auROC as our evaluation metrics.</p>&#13;
&#13;
<p>Here is the function that will plot these results:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">plotResults</code><code class="p">(</code><code class="n">trueLabels</code><code class="p">,</code> <code class="n">anomalyScores</code><code class="p">,</code> <code class="n">returnPreds</code> <code class="o">=</code> <code class="bp">False</code><code class="p">):</code>&#13;
    <code class="n">preds</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">([</code><code class="n">trueLabels</code><code class="p">,</code> <code class="n">anomalyScores</code><code class="p">],</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>&#13;
    <code class="n">preds</code><code class="o">.</code><code class="n">columns</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'trueLabel'</code><code class="p">,</code> <code class="s1">'anomalyScore'</code><code class="p">]</code>&#13;
    <code class="n">precision</code><code class="p">,</code> <code class="n">recall</code><code class="p">,</code> <code class="n">thresholds</code> <code class="o">=</code> \&#13;
        <code class="n">precision_recall_curve</code><code class="p">(</code><code class="n">preds</code><code class="p">[</code><code class="s1">'trueLabel'</code><code class="p">],</code><code class="n">preds</code><code class="p">[</code><code class="s1">'anomalyScore'</code><code class="p">])</code>&#13;
    <code class="n">average_precision</code> <code class="o">=</code> \&#13;
        <code class="n">average_precision_score</code><code class="p">(</code><code class="n">preds</code><code class="p">[</code><code class="s1">'trueLabel'</code><code class="p">],</code><code class="n">preds</code><code class="p">[</code><code class="s1">'anomalyScore'</code><code class="p">])</code>&#13;
&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">recall</code><code class="p">,</code> <code class="n">precision</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s1">'k'</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.7</code><code class="p">,</code> <code class="n">where</code><code class="o">=</code><code class="s1">'post'</code><code class="p">)</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">fill_between</code><code class="p">(</code><code class="n">recall</code><code class="p">,</code> <code class="n">precision</code><code class="p">,</code> <code class="n">step</code><code class="o">=</code><code class="s1">'post'</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.3</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s1">'k'</code><code class="p">)</code>&#13;
&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'Recall'</code><code class="p">)</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'Precision'</code><code class="p">)</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">ylim</code><code class="p">([</code><code class="mf">0.0</code><code class="p">,</code> <code class="mf">1.05</code><code class="p">])</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">xlim</code><code class="p">([</code><code class="mf">0.0</code><code class="p">,</code> <code class="mf">1.0</code><code class="p">])</code>&#13;
&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s1">'Precision-Recall curve: Average Precision = </code><code class="se">\</code>&#13;
<code class="s1">    {0:0.2f}'</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">average_precision</code><code class="p">))</code>&#13;
&#13;
    <code class="n">fpr</code><code class="p">,</code> <code class="n">tpr</code><code class="p">,</code> <code class="n">thresholds</code> <code class="o">=</code> <code class="n">roc_curve</code><code class="p">(</code><code class="n">preds</code><code class="p">[</code><code class="s1">'trueLabel'</code><code class="p">],</code> \&#13;
                                     <code class="n">preds</code><code class="p">[</code><code class="s1">'anomalyScore'</code><code class="p">])</code>&#13;
    <code class="n">areaUnderROC</code> <code class="o">=</code> <code class="n">auc</code><code class="p">(</code><code class="n">fpr</code><code class="p">,</code> <code class="n">tpr</code><code class="p">)</code>&#13;
&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">()</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">fpr</code><code class="p">,</code> <code class="n">tpr</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s1">'r'</code><code class="p">,</code> <code class="n">lw</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'ROC curve'</code><code class="p">)</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">([</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">color</code><code class="o">=</code><code class="s1">'k'</code><code class="p">,</code> <code class="n">lw</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">linestyle</code><code class="o">=</code><code class="s1">'--'</code><code class="p">)</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">xlim</code><code class="p">([</code><code class="mf">0.0</code><code class="p">,</code> <code class="mf">1.0</code><code class="p">])</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">ylim</code><code class="p">([</code><code class="mf">0.0</code><code class="p">,</code> <code class="mf">1.05</code><code class="p">])</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'False Positive Rate'</code><code class="p">)</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'True Positive Rate'</code><code class="p">)</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s1">'Receiver operating characteristic: </code><code class="se">\</code>&#13;
<code class="s1">    Area under the curve = {0:0.2f}'</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">areaUnderROC</code><code class="p">))</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">(</code><code class="n">loc</code><code class="o">=</code><code class="s2">"lower right"</code><code class="p">)</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>&#13;
&#13;
    <code class="k">if</code> <code class="n">returnPreds</code><code class="o">==</code><code class="bp">True</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="n">preds</code></pre>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>The<a data-primary="credit card fraud detection" data-secondary="shortcomings of" data-type="indexterm" id="idm140637549749168"/><a data-primary="evaluation metrics" data-secondary="shortcomings of" data-type="indexterm" id="idm140637549748192"/> fraud labels and the evaluation metrics will help us assess just how good the unsupervised fraud detection systems are at catching known patterns of fraud—fraud that we have caught in the past and have labels for.</p>&#13;
&#13;
<p>However, we will not be able to assess how good the unsupervised fraud detection systems are at catching unknown patterns of fraud. In other words, there may be fraud in the dataset that is incorrectly labeled as not fraud because the financial company never <span class="keep-together">discovered</span> it.</p>&#13;
&#13;
<p>As you may see already, unsupervised learning systems are much harder to evaluate than supervised learning systems. Often, unsupervised learning systems are judged by their ability to catch known patterns of fraud. This is an incomplete assessment; a better evaluation metric would be to assess them on their ability to identify unknown patterns of fraud, both in the past and in the future.</p>&#13;
&#13;
<p>Since we cannot go back to the financial company and have them evaluate any unknown patterns of fraud we identify, we will have to evaluate these unsupervised systems solely based on how well they detect the known patterns of fraud. It’s important to be mindful of this limitation as we proceed in evaluating the results.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Define Plotting Function" data-type="sect2"><div class="sect2" id="idm140637549745088">&#13;
<h2>Define Plotting Function</h2>&#13;
&#13;
<p>We<a data-primary="credit card fraud detection" data-secondary="defining plotting function" data-type="indexterm" id="idm140637549461120"/> will reuse the scatterplot function from <a data-type="xref" href="ch03.html#Chapter_3">Chapter 3</a> to display the separation of points the dimensionality reduction algorithm achieves in just the first two<a data-primary="" data-startref="ADcredit04" data-type="indexterm" id="idm140637549459056"/> <span class="keep-together">dimensions</span>:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">scatterPlot</code><code class="p">(</code><code class="n">xDF</code><code class="p">,</code> <code class="n">yDF</code><code class="p">,</code> <code class="n">algoName</code><code class="p">):</code>&#13;
    <code class="n">tempDF</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">xDF</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code><code class="mi">0</code><code class="p">:</code><code class="mi">1</code><code class="p">],</code> <code class="n">index</code><code class="o">=</code><code class="n">xDF</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>&#13;
    <code class="n">tempDF</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">((</code><code class="n">tempDF</code><code class="p">,</code><code class="n">yDF</code><code class="p">),</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">join</code><code class="o">=</code><code class="s2">"inner"</code><code class="p">)</code>&#13;
    <code class="n">tempDF</code><code class="o">.</code><code class="n">columns</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"First Vector"</code><code class="p">,</code> <code class="s2">"Second Vector"</code><code class="p">,</code> <code class="s2">"Label"</code><code class="p">]</code>&#13;
    <code class="n">sns</code><code class="o">.</code><code class="n">lmplot</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="s2">"First Vector"</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s2">"Second Vector"</code><code class="p">,</code> <code class="n">hue</code><code class="o">=</code><code class="s2">"Label"</code><code class="p">,</code> \&#13;
               <code class="n">data</code><code class="o">=</code><code class="n">tempDF</code><code class="p">,</code> <code class="n">fit_reg</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code>&#13;
    <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">gca</code><code class="p">()</code>&#13;
    <code class="n">ax</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="s2">"Separation of Observations using "</code><code class="o">+</code><code class="n">algoName</code><code class="p">)</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Normal PCA Anomaly Detection" data-type="sect1"><div class="sect1" id="idm140637549455808">&#13;
<h1>Normal PCA Anomaly Detection</h1>&#13;
&#13;
<p>In <a data-type="xref" href="ch03.html#Chapter_3">Chapter 3</a>, we<a data-primary="anomaly detection" data-secondary="normal PCA" data-type="indexterm" id="ADnormal04"/><a data-primary="principal component analysis (PCA)" data-secondary="normal PCA anomaly detection" data-type="indexterm" id="PCAnormal04"/> demonstrated how PCA captured the majority of information in the MNIST digits dataset in just a few principal components, far fewer in number than the original dimensions. In fact, with just two dimensions, it was possible to visually separate the images into distinct groups based on the digits they displayed.</p>&#13;
&#13;
<p>Building on this concept, we will now use PCA to learn the underlying structure of the credit card transactions dataset. Once we learn this structure, we will use the learned model to reconstruct the credit card transactions and then calculate how <span class="keep-together">different</span> the reconstructed transactions are from the original transactions. Those transactions that PCA does the poorest job of reconstructing are the most anomalous (and most likely to be fraudulent).</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Remember that the features in the credit card transactions dataset we have are already the output of PCA—this is what we were given by the financial company. However, there is nothing unusual about performing PCA for anomaly detection on an already dimensionality-reduced dataset. We just treat the original principal components that we are given as the original features.</p>&#13;
&#13;
<p>Going forward, we will refer to the original principal components that we were given as the original features. Any future mention of principal components will refer to the principal components from the PCA process rather than the original features we were given.</p>&#13;
</div>&#13;
&#13;
<p>Let’s start by developing a deeper understanding of how PCA—and dimensionality reduction in general—helps perform anomaly detection. As we’ve defined it, anomaly detection relies on reconstruction error. We want the reconstruction error for rare transactions—the ones that are most likely to be fraudulent—to be as high as possible and the reconstruction error for the rest to be as low as possible.</p>&#13;
&#13;
<p>For PCA, the reconstruction error will depend largely on the number of principal components we keep and use to reconstruct the original transactions. The more principal components we keep, the better PCA will be at learning the underlying structure of the original transactions.</p>&#13;
&#13;
<p>However, there is a balance. If we keep too many principal components, PCA may too easily reconstruct the original transactions, so much so that the reconstruction error will be minimal for all of the transactions. If we keep too few principal components, PCA may not be able to reconstruct any of the original transactions well enough—not even the normal, nonfraudulent transactions.</p>&#13;
&#13;
<p>Let’s search for the right number of principal components to keep to build a good fraud detection system.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="PCA Components Equal Number of Original Dimensions" data-type="sect2"><div class="sect2" id="idm140637549407968">&#13;
<h2>PCA Components Equal Number of Original Dimensions</h2>&#13;
&#13;
<p>First, let’s think about something. If we use PCA to generate the same number of principal components as the number of original features, will we be able to perform anomaly detection?</p>&#13;
&#13;
<p>If you think through this, the answer should be obvious. Recall our PCA example from the previous chapter for the MNIST digits dataset.</p>&#13;
&#13;
<p>When the number of principal components equals the number of original dimensions, PCA captures nearly 100% of the variance/information in the data as it <span class="keep-together">generates</span> the principal components. Therefore, when PCA reconstructs the transactions from the principal components, it will have too little reconstruction error for all the transactions, fraudulent or otherwise. We will not be able to differentiate between rare transactions and normal ones—in other words, anomaly detection will be poor.</p>&#13;
&#13;
<p>To<a data-primary="Scikit-learn" data-secondary="anomaly detection using" data-type="indexterm" id="SKLanomaly04"/> highlight this, let’s apply PCA to generate the same number of principal components as the number of original features (30 for our credit card transactions dataset). This is accomplished with the <code>fit_transform</code> function from Scikit-Learn.</p>&#13;
&#13;
<p>To reconstruct the original transactions from the principal components we generate, we will use the <code>inverse_transform</code> function from Scikit-Learn:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># 30 principal components</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">PCA</code>&#13;
&#13;
<code class="n">n_components</code> <code class="o">=</code> <code class="mi">30</code>&#13;
<code class="n">whiten</code> <code class="o">=</code> <code class="bp">False</code>&#13;
<code class="n">random_state</code> <code class="o">=</code> <code class="mi">2018</code>&#13;
&#13;
<code class="n">pca</code> <code class="o">=</code> <code class="n">PCA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="n">n_components</code><code class="p">,</code> <code class="n">whiten</code><code class="o">=</code><code class="n">whiten</code><code class="p">,</code> \&#13;
          <code class="n">random_state</code><code class="o">=</code><code class="n">random_state</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_train_PCA</code> <code class="o">=</code> <code class="n">pca</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>&#13;
<code class="n">X_train_PCA</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train_PCA</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">X_train</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_train_PCA_inverse</code> <code class="o">=</code> <code class="n">pca</code><code class="o">.</code><code class="n">inverse_transform</code><code class="p">(</code><code class="n">X_train_PCA</code><code class="p">)</code>&#13;
<code class="n">X_train_PCA_inverse</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train_PCA_inverse</code><code class="p">,</code> \&#13;
                                   <code class="n">index</code><code class="o">=</code><code class="n">X_train</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>&#13;
&#13;
<code class="n">scatterPlot</code><code class="p">(</code><code class="n">X_train_PCA</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="s2">"PCA"</code><code class="p">)</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#separation_of_observations_using_normal_pca_and_30_principal_components">Figure 4-1</a> shows the plot of the separation of transactions using the first two principal components of PCA.</p>&#13;
&#13;
<figure><div class="figure" id="separation_of_observations_using_normal_pca_and_30_principal_components">&#13;
<img alt="Separation of Obversations Using Normal PCA and 30 Principal Components" src="assets/hulp_0401.png"/>&#13;
<h6><span class="label">Figure 4-1. </span>Separation of observations using normal PCA and 30 principal components</h6>&#13;
</div></figure>&#13;
&#13;
<p>Let’s calculate the precision-recall curve and the ROC curve:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">anomalyScoresPCA</code> <code class="o">=</code> <code class="n">anomalyScores</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">X_train_PCA_inverse</code><code class="p">)</code>&#13;
<code class="n">preds</code> <code class="o">=</code> <code class="n">plotResults</code><code class="p">(</code><code class="n">y_train</code><code class="p">,</code> <code class="n">anomalyScoresPCA</code><code class="p">,</code> <code class="bp">True</code><code class="p">)</code></pre>&#13;
&#13;
<p>With an average precision of 0.11, this is a poor fraud detection solution (see <a data-type="xref" href="#results_using_30_principal_components">Figure 4-2</a>). It catches very little of the fraud.</p>&#13;
&#13;
<figure><div class="figure" id="results_using_30_principal_components">&#13;
<img alt="Results Using Normal PCA and 30 Principal Components" src="assets/hulp_0402.png"/>&#13;
<h6><span class="label">Figure 4-2. </span>Results using 30 principal components</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Search for the Optimal Number of Principal Components" data-type="sect2"><div class="sect2" id="idm140637549407344">&#13;
<h2>Search for the Optimal Number of Principal Components</h2>&#13;
&#13;
<p>Now, let’s perform a few experiments by reducing the number of principal components PCA generates and evaluate the fraud detection results. We need the PCA-based fraud detection solution to have enough error on the rare cases that it can meaningfully separate fraud cases from the normal ones. But the error cannot be so low or so high for all the transactions that the rare and normal transactions are virtually indistinguishable.</p>&#13;
&#13;
<p>After some experimentation, which you can perform using the <a href="http://bit.ly/2Gd4v7e">GitHub code</a>, we find that 27 principal components is the optimal number for this credit card transactions dataset.</p>&#13;
&#13;
<p><a data-type="xref" href="#separation_of_observations_using_normal_pca_and_27_principal_components">Figure 4-3</a> shows the plot of the separation of transactions using the first two principal components of PCA.</p>&#13;
&#13;
<figure><div class="figure" id="separation_of_observations_using_normal_pca_and_27_principal_components">&#13;
<img alt="Separation of Obversations Using Normal PCA and 27 Principal Components" src="assets/hulp_0403.png"/>&#13;
<h6><span class="label">Figure 4-3. </span>Separation of observations using normal PCA and 27 principal components</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-type="xref" href="#results_using_normal_pca_and_27_principal_components">Figure 4-4</a> shows the precision-recall curve, average precision, and auROC curve.</p>&#13;
&#13;
<figure><div class="figure" id="results_using_normal_pca_and_27_principal_components">&#13;
<img alt="Results Using Normal PCA and 27 Principal Components" src="assets/hulp_0404.png"/>&#13;
<h6><span class="label">Figure 4-4. </span>Results using normal PCA and 27 principal components</h6>&#13;
</div></figure>&#13;
&#13;
<p>As you can see, we are able to catch 80% of the fraud with 75% precision. This is very impressive considering that we did not use any labels. To make these results more tangible, consider that there are 190,820 transactions in the training set and only 330 are fraudulent.</p>&#13;
&#13;
<p>Using PCA, we calculated the reconstruction error for each of these 190,820 transactions. If we sort these transactions by highest reconstruction error (also referred to as anomaly score) in descending order and extract the top 350 transactions from the list, we can see that 264 of these transactions are fraudulent.</p>&#13;
&#13;
<p>That is a precision of 75%. Moreover, the 264 transactions we caught from the 350 we picked represent 80% of the total fraud in the training set (264 out of 330 fraudulent cases). And, remember that we accomplished this without using labels. This is a truly unsupervised fraud detection solution.</p>&#13;
&#13;
<p>Here is the code to highlight this:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">preds</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="n">by</code><code class="o">=</code><code class="s2">"anomalyScore"</code><code class="p">,</code><code class="n">ascending</code><code class="o">=</code><code class="bp">False</code><code class="p">,</code><code class="n">inplace</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>&#13;
<code class="n">cutoff</code> <code class="o">=</code> <code class="mi">350</code>&#13;
<code class="n">predsTop</code> <code class="o">=</code> <code class="n">preds</code><code class="p">[:</code><code class="n">cutoff</code><code class="p">]</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s2">"Precision: "</code><code class="p">,</code><code class="n">np</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="n">predsTop</code><code class="o">.</code> \&#13;
            <code class="n">anomalyScore</code><code class="p">[</code><code class="n">predsTop</code><code class="o">.</code><code class="n">trueLabel</code><code class="o">==</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">count</code><code class="p">()</code><code class="o">/</code><code class="n">cutoff</code><code class="p">,</code><code class="mi">2</code><code class="p">))</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s2">"Recall: "</code><code class="p">,</code><code class="n">np</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="n">predsTop</code><code class="o">.</code> \&#13;
            <code class="n">anomalyScore</code><code class="p">[</code><code class="n">predsTop</code><code class="o">.</code><code class="n">trueLabel</code><code class="o">==</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">count</code><code class="p">()</code><code class="o">/</code><code class="n">y_train</code><code class="o">.</code><code class="n">sum</code><code class="p">(),</code><code class="mi">2</code><code class="p">))</code></pre>&#13;
&#13;
<p>The following code summarizes the results:</p>&#13;
&#13;
<pre data-type="programlisting">Precision: 0.75&#13;
Recall: 0.8&#13;
Fraud Caught out of 330 Cases: 264</pre>&#13;
&#13;
<p>Although this is a pretty good solution already, let’s try to develop fraud detection systems using some of the other dimensionality reduction methods.<a data-primary="" data-startref="PCAnormal04" data-type="indexterm" id="idm140637549331152"/><a data-primary="" data-startref="ADnormal04" data-type="indexterm" id="idm140637549330272"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Sparse PCA Anomaly Detection" data-type="sect1"><div class="sect1" id="idm140637549455344">&#13;
<h1>Sparse PCA Anomaly Detection</h1>&#13;
&#13;
<p>Let’s<a data-primary="anomaly detection" data-secondary="sparse PCA" data-type="indexterm" id="ADsparse04"/><a data-primary="sparse PCA" data-type="indexterm" id="sppca04"/><a data-primary="principal component analysis (PCA)" data-secondary="sparse PCA" data-type="indexterm" id="PCAspapca04"/> try to use sparse PCA to design a fraud detection solution. Recall that sparse PCA is similar to normal PCA but delivers a less dense version; in other words, sparse PCA provides a sparse representation of the principal components.</p>&#13;
&#13;
<p>We still need to specify the number of principal components we desire, but we must also set the alpha parameter, which controls the degree of sparsity. We will experiment with different values for the principal components and the alpha parameter as we search for the optimal sparse PCA fraud detection solution.</p>&#13;
&#13;
<p>Note<a data-primary="Scikit-learn" data-secondary="fit_transform function" data-type="indexterm" id="idm140637548877136"/><a data-primary="Scikit-learn" data-secondary="inverse_transform function" data-type="indexterm" id="idm140637548876128"/><a data-primary="fit_transform function" data-type="indexterm" id="idm140637548875216"/><a data-primary="inverse_transform function" data-type="indexterm" id="idm140637548874544"/> that for normal PCA Scikit-Learn used a <code>fit_transform</code> function to generate the principal components and an <code>inverse_transform</code> function to reconstruct the original dimensions from the principal components. Using these two functions, we were able to calculate the reconstruction error between the original feature set and the reconstructed feature set derived from the PCA.</p>&#13;
&#13;
<p>Unfortunately, Scikit-Learn does not provide an <code>inverse_transform</code> function for sparse PCA. Therefore, we must reconstruct the original dimensions after we perform sparse PCA ourselves.</p>&#13;
&#13;
<p>Let’s begin by generating the sparse PCA matrix with 27 principal components and the default alpha parameter of 0.0001:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Sparse PCA</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">SparsePCA</code>&#13;
&#13;
<code class="n">n_components</code> <code class="o">=</code> <code class="mi">27</code>&#13;
<code class="n">alpha</code> <code class="o">=</code> <code class="mf">0.0001</code>&#13;
<code class="n">random_state</code> <code class="o">=</code> <code class="mi">2018</code>&#13;
<code class="n">n_jobs</code> <code class="o">=</code> <code class="o">-</code><code class="mi">1</code>&#13;
&#13;
<code class="n">sparsePCA</code> <code class="o">=</code> <code class="n">SparsePCA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="n">n_components</code><code class="p">,</code> \&#13;
                <code class="n">alpha</code><code class="o">=</code><code class="n">alpha</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="n">random_state</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=</code><code class="n">n_jobs</code><code class="p">)</code>&#13;
&#13;
<code class="n">sparsePCA</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,:])</code>&#13;
<code class="n">X_train_sparsePCA</code> <code class="o">=</code> <code class="n">sparsePCA</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>&#13;
<code class="n">X_train_sparsePCA</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train_sparsePCA</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">X_train</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>&#13;
&#13;
<code class="n">scatterPlot</code><code class="p">(</code><code class="n">X_train_sparsePCA</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="s2">"Sparse PCA"</code><code class="p">)</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#separation_of_observations_using_sparse_pca_and_27_principal_components">Figure 4-5</a> shows the scatterplot for sparse PCA.</p>&#13;
&#13;
<figure><div class="figure" id="separation_of_observations_using_sparse_pca_and_27_principal_components">&#13;
<img alt="Separation of Obversations Using Sparse PCA and 27 Principal Components" src="assets/hulp_0405.png"/>&#13;
<h6><span class="label">Figure 4-5. </span>Separation of observations using sparse PCA and 27 principal components</h6>&#13;
</div></figure>&#13;
&#13;
<p>Now let’s generate the original dimensions from the sparse PCA matrix by simple matrix multiplication of the sparse PCA matrix (with 190,820 samples and 27 dimensions) and the sparse PCA components (a 27 x 30 matrix), provided by Scikit-Learn library. This creates a matrix that is the original size (a 190,820 x 30 matrix). We also need to add the mean of each original feature to this new matrix, but then we are done.</p>&#13;
&#13;
<p>From this newly derived inverse matrix, we can calculate the reconstruction errors (anomaly scores) as we did with normal PCA:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">X_train_sparsePCA_inverse</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">X_train_sparsePCA</code><code class="p">)</code><code class="o">.</code> \&#13;
    <code class="n">dot</code><code class="p">(</code><code class="n">sparsePCA</code><code class="o">.</code><code class="n">components_</code><code class="p">)</code> <code class="o">+</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">X_train</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">))</code>&#13;
<code class="n">X_train_sparsePCA_inverse</code> <code class="o">=</code> \&#13;
    <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train_sparsePCA_inverse</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">X_train</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>&#13;
&#13;
<code class="n">anomalyScoresSparsePCA</code> <code class="o">=</code> <code class="n">anomalyScores</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">X_train_sparsePCA_inverse</code><code class="p">)</code>&#13;
<code class="n">preds</code> <code class="o">=</code> <code class="n">plotResults</code><code class="p">(</code><code class="n">y_train</code><code class="p">,</code> <code class="n">anomalyScoresSparsePCA</code><code class="p">,</code> <code class="bp">True</code><code class="p">)</code></pre>&#13;
&#13;
<p>Now, let’s generate the precision-recall curve and ROC curve.</p>&#13;
&#13;
<figure><div class="figure" id="results_using_sparse_pca_and_27_principal_components">&#13;
<img alt="Results Using Sparse PCA and 27 Principal Components" src="assets/hulp_0406.png"/>&#13;
<h6><span class="label">Figure 4-6. </span>Results using sparse PCA and 27 principal components</h6>&#13;
</div></figure>&#13;
&#13;
<p>As <a data-type="xref" href="#results_using_sparse_pca_and_27_principal_components">Figure 4-6</a> shows, the results are identical to those of normal PCA. This is expected since normal and sparse PCA are very similar—the latter is just a sparse representaion of the former.</p>&#13;
&#13;
<p>Using the <a href="http://bit.ly/2Gd4v7e">GitHub code</a>, you can experiment by changing the number of principal components generated and the alpha parameter, but, based on our experimentation, this is the best sparse PCA-based fraud detection solution.<a data-primary="" data-startref="ADsparse04" data-type="indexterm" id="idm140637548542944"/><a data-primary="" data-startref="sppca04" data-type="indexterm" id="idm140637548541968"/><a data-primary="" data-startref="PCAspapca04" data-type="indexterm" id="idm140637548541024"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Kernel PCA Anomaly Detection" data-type="sect1"><div class="sect1" id="idm140637549328736">&#13;
<h1>Kernel PCA Anomaly Detection</h1>&#13;
&#13;
<p>Now<a data-primary="anomaly detection" data-secondary="kernel PCA" data-type="indexterm" id="ADkernel04"/><a data-primary="kernel PCA" data-type="indexterm" id="kernpca04"/><a data-primary="principal component analysis (PCA)" data-secondary="kernel PCA" data-type="indexterm" id="PCAkernel04"/> let’s design a fraud detection solution using kernel PCA, which is a nonlinear form of PCA and is useful if the fraud transactions are not linearly separable from the nonfraud transactions.</p>&#13;
&#13;
<p>We<a data-primary="radial basis function (RBF) kernel" data-type="indexterm" id="idm140637548533968"/> need to specify the number of components we would like to generate, the kernel (we will use the RBF kernel as we did in the previous chapter), and the gamma (which is set to 1/n_features by default, so 1/30 in our case). We<a data-primary="fit_inverse_transform function" data-type="indexterm" id="idm140637548532896"/><a data-primary="inverse_transform function" data-type="indexterm" id="idm140637548532208"/><a data-primary="Scikit-learn" data-secondary="fit_transform function" data-type="indexterm" id="idm140637548531520"/><a data-primary="Scikit-learn" data-secondary="inverse_transform function" data-type="indexterm" id="idm140637548530576"/> also need to set the <code>fit_inverse_transform</code> to <code>true</code> to apply the built-in <code>inverse_transform</code> function provided by Scikit-Learn.</p>&#13;
&#13;
<p>Finally, because kernel PCA is so expensive to train with, we will train on just the first two thousand samples in the transactions dataset. This is not ideal but it is necessary to perform experiments quickly.</p>&#13;
&#13;
<p>We will use this training to transform the entire training set and generate the principal components. Then, we will use the <code>inverse_transform</code> function to recreate the original dimension from the principal components derived by kernel PCA:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Kernel PCA</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">KernelPCA</code>&#13;
&#13;
<code class="n">n_components</code> <code class="o">=</code> <code class="mi">27</code>&#13;
<code class="n">kernel</code> <code class="o">=</code> <code class="s1">'rbf'</code>&#13;
<code class="n">gamma</code> <code class="o">=</code> <code class="bp">None</code>&#13;
<code class="n">fit_inverse_transform</code> <code class="o">=</code> <code class="bp">True</code>&#13;
<code class="n">random_state</code> <code class="o">=</code> <code class="mi">2018</code>&#13;
<code class="n">n_jobs</code> <code class="o">=</code> <code class="mi">1</code>&#13;
&#13;
<code class="n">kernelPCA</code> <code class="o">=</code> <code class="n">KernelPCA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="n">n_components</code><code class="p">,</code> <code class="n">kernel</code><code class="o">=</code><code class="n">kernel</code><code class="p">,</code> \&#13;
                <code class="n">gamma</code><code class="o">=</code><code class="n">gamma</code><code class="p">,</code> <code class="n">fit_inverse_transform</code><code class="o">=</code> \&#13;
                <code class="n">fit_inverse_transform</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=</code><code class="n">n_jobs</code><code class="p">,</code> \&#13;
                <code class="n">random_state</code><code class="o">=</code><code class="n">random_state</code><code class="p">)</code>&#13;
&#13;
<code class="n">kernelPCA</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="o">.</code><code class="n">iloc</code><code class="p">[:</code><code class="mi">2000</code><code class="p">])</code>&#13;
<code class="n">X_train_kernelPCA</code> <code class="o">=</code> <code class="n">kernelPCA</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>&#13;
<code class="n">X_train_kernelPCA</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train_kernelPCA</code><code class="p">,</code> \&#13;
                                 <code class="n">index</code><code class="o">=</code><code class="n">X_train</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_train_kernelPCA_inverse</code> <code class="o">=</code> <code class="n">kernelPCA</code><code class="o">.</code><code class="n">inverse_transform</code><code class="p">(</code><code class="n">X_train_kernelPCA</code><code class="p">)</code>&#13;
<code class="n">X_train_kernelPCA_inverse</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train_kernelPCA_inverse</code><code class="p">,</code> \&#13;
                                         <code class="n">index</code><code class="o">=</code><code class="n">X_train</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>&#13;
&#13;
<code class="n">scatterPlot</code><code class="p">(</code><code class="n">X_train_kernelPCA</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="s2">"Kernel PCA"</code><code class="p">)</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#separation_of_obversations_using_kernel_pca_and_27_principal_components">Figure 4-7</a> shows the scatterplot for kernel PCA.</p>&#13;
&#13;
<figure><div class="figure" id="separation_of_obversations_using_kernel_pca_and_27_principal_components">&#13;
<img alt="Separation of Obversations Using Kernel PCA and 27 Principal Components" src="assets/hulp_0407.png"/>&#13;
<h6><span class="label">Figure 4-7. </span>Separation of observations using kernel PCA and 27 principal components</h6>&#13;
</div></figure>&#13;
&#13;
<p>Now, let’s calculate the anomaly scores and print the results.</p>&#13;
&#13;
<figure><div class="figure" id="results_using_kernel_pca_and_27_principal_components">&#13;
<img alt="Results Using Kernel PCA and 27 Principal Components" src="assets/hulp_0408.png"/>&#13;
<h6><span class="label">Figure 4-8. </span>Results using kernel PCA and 27 principal components</h6>&#13;
</div></figure>&#13;
&#13;
<p>As <a data-type="xref" href="#results_using_kernel_pca_and_27_principal_components">Figure 4-8</a> shows, the results are far worse than those for normal PCA and sparse PCA. While it was worth experimenting with kernel PCA, we will not use this solution for fraud detection given that we have better performing solutions from earlier.<a data-primary="" data-startref="ADkernel04" data-type="indexterm" id="idm140637548476112"/><a data-primary="" data-startref="kernpca04" data-type="indexterm" id="idm140637548475168"/><a data-primary="" data-startref="PCAkernel04" data-type="indexterm" id="idm140637548474224"/></p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>We will not build an anomaly detection solution using SVD because the solution is very similar to that of normal PCA. This is expected—PCA and SVD are closely related.</p>&#13;
&#13;
<p>Instead, let’s move to random projection-based anomaly detection.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Gaussian Random Projection Anomaly Detection" data-type="sect1"><div class="sect1" id="idm140637548539488">&#13;
<h1>Gaussian Random Projection Anomaly Detection</h1>&#13;
&#13;
<p>Now, let’s try<a data-primary="anomaly detection" data-secondary="Gaussian random projection" data-type="indexterm" id="ADgauss04"/><a data-primary="Gaussian random projection" data-type="indexterm" id="gaussian04"/> to develop a fraud detection solution using Gaussian random projection. Remember that we can set either the number of components we want or the <em>eps</em> parameter, which controls the quality of the embedding derived based on the Johnson–Lindenstrauss lemma.</p>&#13;
&#13;
<p>We will choose to explicitly set the number of components. Gaussian random projection trains very quickly, so we can train on the entire training set.</p>&#13;
&#13;
<p>As with sparse PCA, we will need to derive our own <code>inverse_transform</code> function because none is provided by Scikit-Learn:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Gaussian Random Projection</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.random_projection</code> <code class="kn">import</code> <code class="n">GaussianRandomProjection</code>&#13;
&#13;
<code class="n">n_components</code> <code class="o">=</code> <code class="mi">27</code>&#13;
<code class="n">eps</code> <code class="o">=</code> <code class="bp">None</code>&#13;
<code class="n">random_state</code> <code class="o">=</code> <code class="mi">2018</code>&#13;
&#13;
<code class="n">GRP</code> <code class="o">=</code> <code class="n">GaussianRandomProjection</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="n">n_components</code><code class="p">,</code> \&#13;
                               <code class="n">eps</code><code class="o">=</code><code class="n">eps</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="n">random_state</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_train_GRP</code> <code class="o">=</code> <code class="n">GRP</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>&#13;
<code class="n">X_train_GRP</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train_GRP</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">X_train</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>&#13;
&#13;
<code class="n">scatterPlot</code><code class="p">(</code><code class="n">X_train_GRP</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="s2">"Gaussian Random Projection"</code><code class="p">)</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#separation_of_observations_using_gaussian_random_projection_and_27_components">Figure 4-9</a> shows the scatterplot for Gaussian random projection. <a data-type="xref" href="#results_using_gaussian_random_projection_and_27_components">Figure 4-10</a> displays the results for Gaussian random projection.</p>&#13;
&#13;
<figure><div class="figure" id="separation_of_observations_using_gaussian_random_projection_and_27_components">&#13;
<img alt="Separation of Obversations Using Gaussian Random Projection and 27 Components" src="assets/hulp_0409.png"/>&#13;
<h6><span class="label">Figure 4-9. </span>Separation of observations using Gaussian random projection and 27 <span class="keep-together">components</span></h6>&#13;
</div></figure>&#13;
&#13;
<figure><div class="figure" id="results_using_gaussian_random_projection_and_27_components">&#13;
<img alt="Results Using Gaussian Random Projection and 27 Components" src="assets/hulp_0410.png"/>&#13;
<h6><span class="label">Figure 4-10. </span>Results using Gaussian random projection and 27 components</h6>&#13;
</div></figure>&#13;
&#13;
<p>These results are poor, so we won’t use Gaussian random projection for fraud detection.<a data-primary="" data-startref="gaussian04" data-type="indexterm" id="idm140637548227632"/><a data-primary="" data-startref="ADgauss04" data-type="indexterm" id="idm140637548226656"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Sparse Random Projection Anomaly Detection" data-type="sect1"><div class="sect1" id="idm140637548225584">&#13;
<h1>Sparse Random Projection Anomaly Detection</h1>&#13;
&#13;
<p>Let’s try<a data-primary="anomaly detection" data-secondary="sparse random projection" data-type="indexterm" id="idm140637548224080"/><a data-primary="sparse random projection" data-type="indexterm" id="idm140637548223056"/> to design a fraud detection solution using sparse random projection.</p>&#13;
&#13;
<p>We will designate the number of components we want (instead of setting the <em>eps</em> parameter). And, like with Gaussian random projection, we will use our own <code>inverse_transform</code> function to create the original dimensions from the sparse random projection-derived components:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Sparse Random Projection</code>&#13;
&#13;
<code class="kn">from</code> <code class="nn">sklearn.random_projection</code> <code class="kn">import</code> <code class="n">SparseRandomProjection</code>&#13;
&#13;
<code class="n">n_components</code> <code class="o">=</code> <code class="mi">27</code>&#13;
<code class="n">density</code> <code class="o">=</code> <code class="s1">'auto'</code>&#13;
<code class="n">eps</code> <code class="o">=</code> <code class="o">.</code><code class="mo">01</code>&#13;
<code class="n">dense_output</code> <code class="o">=</code> <code class="bp">True</code>&#13;
<code class="n">random_state</code> <code class="o">=</code> <code class="mi">2018</code>&#13;
&#13;
<code class="n">SRP</code> <code class="o">=</code> <code class="n">SparseRandomProjection</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="n">n_components</code><code class="p">,</code> \&#13;
        <code class="n">density</code><code class="o">=</code><code class="n">density</code><code class="p">,</code> <code class="n">eps</code><code class="o">=</code><code class="n">eps</code><code class="p">,</code> <code class="n">dense_output</code><code class="o">=</code><code class="n">dense_output</code><code class="p">,</code> \&#13;
                                <code class="n">random_state</code><code class="o">=</code><code class="n">random_state</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_train_SRP</code> <code class="o">=</code> <code class="n">SRP</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>&#13;
<code class="n">X_train_SRP</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train_SRP</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">X_train</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>&#13;
&#13;
<code class="n">scatterPlot</code><code class="p">(</code><code class="n">X_train_SRP</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="s2">"Sparse Random Projection"</code><code class="p">)</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#separation_of_obversations_using_sparse_random_projection_and_27_components">Figure 4-11</a> shows the scatterplot for sparse random projection. <a data-type="xref" href="#results_using_sparse_random_projection_and_27_components">Figure 4-12</a> displays the results for sparse random projection.</p>&#13;
&#13;
<figure><div class="figure" id="separation_of_obversations_using_sparse_random_projection_and_27_components">&#13;
<img alt="Separation of Obversations Using Sparse Random Projection and 27 Components" src="assets/hulp_0411.png"/>&#13;
<h6><span class="label">Figure 4-11. </span>Separation of observations using sparse random projection and 27 <span class="keep-together">components</span></h6>&#13;
</div></figure>&#13;
&#13;
<figure><div class="figure" id="results_using_sparse_random_projection_and_27_components">&#13;
<img alt="Results Using Sparse Random Projection and 27 Components" src="assets/hulp_0412.png"/>&#13;
<h6><span class="label">Figure 4-12. </span>Results using sparse random projection and 27 components</h6>&#13;
</div></figure>&#13;
&#13;
<p>As with Gaussian random projection, these results are poor. Let’s continue to build anomaly detection systems using other dimensionality reduction methods.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Nonlinear Anomaly Detection" data-type="sect1"><div class="sect1" id="idm140637548360288">&#13;
<h1>Nonlinear Anomaly Detection</h1>&#13;
&#13;
<p>So far, we<a data-primary="anomaly detection" data-secondary="nonlinear" data-type="indexterm" id="idm140637548358992"/><a data-primary="nonlinear anomaly detection" data-type="indexterm" id="idm140637548357984"/> have developed fraud detection solutions using linear dimensionality reduction methods such as normal PCA, sparse PCA, Gaussian random projection, and sparse random projection. We also developed a solution using the nonlinear version of PCA—kernel PCA.</p>&#13;
&#13;
<p>At this point, PCA is by far the best solution.</p>&#13;
&#13;
<p>We could turn to nonlinear dimensionality reduction algorithms, but the open source versions of these algorithms run very slowly and are not viable for fast fraud detection. Therefore, we will skip this and go directly to nondistance-based dimensionality reduction methods: dictionary learning and independent component analysis.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Dictionary Learning Anomaly Detection" data-type="sect1"><div class="sect1" id="idm140637548355424">&#13;
<h1>Dictionary Learning Anomaly Detection</h1>&#13;
&#13;
<p>Let’s use dictionary learning<a data-primary="dictionary learning" data-type="indexterm" id="diction04"/><a data-primary="anomaly detection" data-secondary="dictionary learning" data-type="indexterm" id="ADdictionary04"/> to develop a fraud detection solution. Recall that, in dictionary learning, the algorithm learns the sparse representation of the original data. Using the vectors in the learned dictionary, each instance in the original data can be reconstructed as a weighted sum of these learned vectors.</p>&#13;
&#13;
<p>For anomaly detection, we want to learn an undercomplete dictionary so that the vectors in the dictionary are fewer in number than the original dimensions. With this constraint, it will be easier to reconstruct the more frequently occurring normal transactions and much more difficult to construct the rarer fraud transactions.</p>&#13;
&#13;
<p>In our case, we will generate 28 vectors (or components). To learn the dictionary, we will feed in 10 batches, where each batch has 200 samples.</p>&#13;
&#13;
<p>We<a data-primary="inverse_transform function" data-type="indexterm" id="idm140637548349344"/><a data-primary="Scikit-learn" data-secondary="inverse_transform function" data-type="indexterm" id="idm140637548348544"/> will need to use our own <code>inverse_transform</code> function, too:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Mini-batch dictionary learning</code>&#13;
<code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">MiniBatchDictionaryLearning</code>&#13;
&#13;
<code class="n">n_components</code> <code class="o">=</code> <code class="mi">28</code>&#13;
<code class="n">alpha</code> <code class="o">=</code> <code class="mi">1</code>&#13;
<code class="n">batch_size</code> <code class="o">=</code> <code class="mi">200</code>&#13;
<code class="n">n_iter</code> <code class="o">=</code> <code class="mi">10</code>&#13;
<code class="n">random_state</code> <code class="o">=</code> <code class="mi">2018</code>&#13;
&#13;
<code class="n">miniBatchDictLearning</code> <code class="o">=</code> <code class="n">MiniBatchDictionaryLearning</code><code class="p">(</code> \&#13;
    <code class="n">n_components</code><code class="o">=</code><code class="n">n_components</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="n">alpha</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="n">batch_size</code><code class="p">,</code> \&#13;
    <code class="n">n_iter</code><code class="o">=</code><code class="n">n_iter</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="n">random_state</code><code class="p">)</code>&#13;
&#13;
<code class="n">miniBatchDictLearning</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>&#13;
<code class="n">X_train_miniBatchDictLearning</code> <code class="o">=</code> \&#13;
    <code class="n">miniBatchDictLearning</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>&#13;
<code class="n">X_train_miniBatchDictLearning</code> <code class="o">=</code> \&#13;
    <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train_miniBatchDictLearning</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">X_train</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>&#13;
&#13;
<code class="n">scatterPlot</code><code class="p">(</code><code class="n">X_train_miniBatchDictLearning</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> \&#13;
            <code class="s2">"Mini-batch Dictionary Learning"</code><code class="p">)</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#separation_of_obversations_using_dictionary_learning_and_28_components">Figure 4-13</a> shows the scatterplot for dictionary learning. <a data-type="xref" href="#results_using_dictionary_learning_and_28_components">Figure 4-14</a> shows the results for dictionary learning.</p>&#13;
&#13;
<figure><div class="figure" id="separation_of_obversations_using_dictionary_learning_and_28_components">&#13;
<img alt="Separation of Obversations Using Dictionary Learning and 28 Components" src="assets/hulp_0413.png"/>&#13;
<h6><span class="label">Figure 4-13. </span>Separation of observations using dictionary learning and 28 components</h6>&#13;
</div></figure>&#13;
&#13;
<figure><div class="figure" id="results_using_dictionary_learning_and_28_components">&#13;
<img alt="Results Using Dictionary Learning and 28 Components" src="assets/hulp_0414.png"/>&#13;
<h6><span class="label">Figure 4-14. </span>Results using dictionary learning and 28 components</h6>&#13;
</div></figure>&#13;
&#13;
<p>These results are much better than those for kernal PCA, Gaussian random projection, and sparse random projection but are no match for those of normal PCA.</p>&#13;
&#13;
<p>You can experiment with the code on GitHub to see if you could improve on this solution, but, for now, PCA remains the best fraud detection solution for this credit card transactions dataset.<a data-primary="" data-startref="ADdictionary04" data-type="indexterm" id="idm140637548097792"/><a data-primary="" data-startref="diction04" data-type="indexterm" id="idm140637548096816"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="ICA Anomaly Detection" data-type="sect1"><div class="sect1" id="idm140637548354800">&#13;
<h1>ICA Anomaly Detection</h1>&#13;
&#13;
<p>Let’s<a data-primary="anomaly detection" data-secondary="independent component analysis (ICA)" data-type="indexterm" id="ADica04"/><a data-primary="independent component analysis (ICA)" data-type="indexterm" id="ica04"/> use ICA to design our last fraud detection solution.</p>&#13;
&#13;
<p>We<a data-primary="inverse_transform function" data-type="indexterm" id="idm140637548091568"/><a data-primary="Scikit-learn" data-secondary="inverse_transform function" data-type="indexterm" id="idm140637548090816"/> need to specify the number of components, which we will set to 27. Scikit-Learn provides an <code>inverse_transform</code> function so we do not need to use our own:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Independent Component Analysis</code>&#13;
&#13;
<code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">FastICA</code>&#13;
&#13;
<code class="n">n_components</code> <code class="o">=</code> <code class="mi">27</code>&#13;
<code class="n">algorithm</code> <code class="o">=</code> <code class="s1">'parallel'</code>&#13;
<code class="n">whiten</code> <code class="o">=</code> <code class="bp">True</code>&#13;
<code class="n">max_iter</code> <code class="o">=</code> <code class="mi">200</code>&#13;
<code class="n">random_state</code> <code class="o">=</code> <code class="mi">2018</code>&#13;
&#13;
<code class="n">fastICA</code> <code class="o">=</code> <code class="n">FastICA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="n">n_components</code><code class="p">,</code> \&#13;
    <code class="n">algorithm</code><code class="o">=</code><code class="n">algorithm</code><code class="p">,</code> <code class="n">whiten</code><code class="o">=</code><code class="n">whiten</code><code class="p">,</code> <code class="n">max_iter</code><code class="o">=</code><code class="n">max_iter</code><code class="p">,</code> \&#13;
    <code class="n">random_state</code><code class="o">=</code><code class="n">random_state</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_train_fastICA</code> <code class="o">=</code> <code class="n">fastICA</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>&#13;
<code class="n">X_train_fastICA</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train_fastICA</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">X_train</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_train_fastICA_inverse</code> <code class="o">=</code> <code class="n">fastICA</code><code class="o">.</code><code class="n">inverse_transform</code><code class="p">(</code><code class="n">X_train_fastICA</code><code class="p">)</code>&#13;
<code class="n">X_train_fastICA_inverse</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_train_fastICA_inverse</code><code class="p">,</code> \&#13;
                                       <code class="n">index</code><code class="o">=</code><code class="n">X_train</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>&#13;
&#13;
<code class="n">scatterPlot</code><code class="p">(</code><code class="n">X_train_fastICA</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="s2">"Independent Component Analysis"</code><code class="p">)</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#separation_of_obversations_using_independent_component_analysis_and_27_components">Figure 4-15</a> shows the scatterplot for ICA. <a data-type="xref" href="#results_using_independent_component_analysis_and_27_components">Figure 4-16</a> shows the results for ICA.</p>&#13;
&#13;
<figure><div class="figure" id="separation_of_obversations_using_independent_component_analysis_and_27_components">&#13;
<img alt="Separation of Obversations Using Dictionary Learning and 27 Components" src="assets/hulp_0415.png"/>&#13;
<h6><span class="label">Figure 4-15. </span>Separation of observations using ICA and 27 components</h6>&#13;
</div></figure>&#13;
&#13;
<figure><div class="figure" id="results_using_independent_component_analysis_and_27_components">&#13;
<img alt="Results Using Independent Component Analsysis and 27 Components" src="assets/hulp_0416.png"/>&#13;
<h6><span class="label">Figure 4-16. </span>Results using ICA and 27 components</h6>&#13;
</div></figure>&#13;
&#13;
<p>These results are identical to those of normal PCA. The fraud detection solution using ICA matches the best solution we’ve developed so far.<a data-primary="" data-startref="ADica04" data-type="indexterm" id="idm140637547877776"/><a data-primary="" data-startref="ica04" data-type="indexterm" id="idm140637547876800"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Fraud Detection on the Test Set" data-type="sect1"><div class="sect1" id="idm140637547875728">&#13;
<h1>Fraud Detection on the Test Set</h1>&#13;
&#13;
<p>Now, to<a data-primary="anomaly detection" data-secondary="fraud detection on test set" data-type="indexterm" id="ADtest04"/> evaluate our fraud detection solutions, let’s apply them to the never-before-seen test set. We will do this for the top three solutions we’ve developed: normal PCA, ICA, and dictionary learning. We will not use sparse PCA because it is very similar to the normal PCA solution.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Normal PCA Anomaly Detection on the Test Set" data-type="sect2"><div class="sect2" id="idm140637547872320">&#13;
<h2>Normal PCA Anomaly Detection on the Test Set</h2>&#13;
&#13;
<p>Let’s start with<a data-primary="principal component analysis (PCA)" data-secondary="normal PCA anomaly detection" data-type="indexterm" id="idm140637547870816"/> normal PCA. We will use the PCA embedding that the PCA algorithm learned from the training set and use this to transform the test set. We will then use the Scikit-Learn <code>inverse_transform</code> function to recreate the original dimensions from the principal components matrix of the test set.</p>&#13;
&#13;
<p>By comparing the original test set matrix with the newly reconstructed one, we can calculate the anomaly scores (as we’ve done many times before in this chapter):</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># PCA on Test Set</code>&#13;
<code class="n">X_test_PCA</code> <code class="o">=</code> <code class="n">pca</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>&#13;
<code class="n">X_test_PCA</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_test_PCA</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">X_test</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_test_PCA_inverse</code> <code class="o">=</code> <code class="n">pca</code><code class="o">.</code><code class="n">inverse_transform</code><code class="p">(</code><code class="n">X_test_PCA</code><code class="p">)</code>&#13;
<code class="n">X_test_PCA_inverse</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_test_PCA_inverse</code><code class="p">,</code> \&#13;
                                  <code class="n">index</code><code class="o">=</code><code class="n">X_test</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>&#13;
&#13;
<code class="n">scatterPlot</code><code class="p">(</code><code class="n">X_test_PCA</code><code class="p">,</code> <code class="n">y_test</code><code class="p">,</code> <code class="s2">"PCA"</code><code class="p">)</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#separation_of_obversations_using_pca_and_27_components_on_the_test_set">Figure 4-17</a> shows the scatterplot for PCA on the test set. <a data-type="xref" href="#results_using_pca_and_27_components_on_the_test_set">Figure 4-18</a> displays the results for PCA on the test set.</p>&#13;
&#13;
<figure><div class="figure" id="separation_of_obversations_using_pca_and_27_components_on_the_test_set">&#13;
<img alt="Separation of Obversations Using PCA and 27 Components on the Test Set" src="assets/hulp_0417.png"/>&#13;
<h6><span class="label">Figure 4-17. </span>Separation of observations using PCA and 27 components on the test set</h6>&#13;
</div></figure>&#13;
&#13;
<figure><div class="figure" id="results_using_pca_and_27_components_on_the_test_set">&#13;
<img alt="Results Using PCA and 27 Components on the Test Set" src="assets/hulp_0418.png"/>&#13;
<h6><span class="label">Figure 4-18. </span>Results using PCA and 27 components on the test set</h6>&#13;
</div></figure>&#13;
&#13;
<p>These are impressive results. We are able to catch 80% of the known fraud in the test set with an 80% precision—all without using any labels.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="ICA Anomaly Detection on the Test Set" data-type="sect2"><div class="sect2" id="idm140637547679504">&#13;
<h2>ICA Anomaly Detection on the Test Set</h2>&#13;
&#13;
<p>Let’s<a data-primary="independent component analysis (ICA)" data-type="indexterm" id="idm140637547677968"/> now move to ICA and perform fraud detection on the test set:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Independent Component Analysis on Test Set</code>&#13;
<code class="n">X_test_fastICA</code> <code class="o">=</code> <code class="n">fastICA</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>&#13;
<code class="n">X_test_fastICA</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_test_fastICA</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">X_test</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>&#13;
&#13;
<code class="n">X_test_fastICA_inverse</code> <code class="o">=</code> <code class="n">fastICA</code><code class="o">.</code><code class="n">inverse_transform</code><code class="p">(</code><code class="n">X_test_fastICA</code><code class="p">)</code>&#13;
<code class="n">X_test_fastICA_inverse</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_test_fastICA_inverse</code><code class="p">,</code> \&#13;
                                      <code class="n">index</code><code class="o">=</code><code class="n">X_test</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>&#13;
&#13;
<code class="n">scatterPlot</code><code class="p">(</code><code class="n">X_test_fastICA</code><code class="p">,</code> <code class="n">y_test</code><code class="p">,</code> <code class="s2">"Independent Component Analysis"</code><code class="p">)</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#separation_of_obversations_using_independent_component_analysis_and_27_components_on_the_test_set">Figure 4-19</a> shows the scatterplot for ICA on the test set. <a data-type="xref" href="#results_using_independent_component_analysis_and_27_components_on_the_test_set">Figure 4-20</a> shows the results for ICA on the test set.</p>&#13;
&#13;
<figure><div class="figure" id="separation_of_obversations_using_independent_component_analysis_and_27_components_on_the_test_set">&#13;
<img alt="Separation of Obversations Using Independent Component Analysis and 27 Components on the Test Set" src="assets/hulp_0419.png"/>&#13;
<h6><span class="label">Figure 4-19. </span>Separation of observations using ICA and 27 components on the test set</h6>&#13;
</div></figure>&#13;
&#13;
<figure><div class="figure" id="results_using_independent_component_analysis_and_27_components_on_the_test_set">&#13;
<img alt="Results Using Independent Component Analysis and 27 Components on the Test Set" src="assets/hulp_0420.png"/>&#13;
<h6><span class="label">Figure 4-20. </span>Results using ICA and 27 components on the test set</h6>&#13;
</div></figure>&#13;
&#13;
<p>The results are identical to normal PCA and thus quite impressive.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Dictionary Learning Anomaly Detection on the Test Set" data-type="sect2"><div class="sect2" id="idm140637547799056">&#13;
<h2>Dictionary Learning Anomaly Detection on the Test Set</h2>&#13;
&#13;
<p>Let’s<a data-primary="dictionary learning" data-type="indexterm" id="idm140637547797520"/> now turn to dictionary learning, which did not perform as well as normal PCA and ICA but is worth a final look:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">X_test_miniBatchDictLearning</code> <code class="o">=</code> <code class="n">miniBatchDictLearning</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>&#13;
<code class="n">X_test_miniBatchDictLearning</code> <code class="o">=</code> \&#13;
    <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">X_test_miniBatchDictLearning</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">X_test</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>&#13;
&#13;
<code class="n">scatterPlot</code><code class="p">(</code><code class="n">X_test_miniBatchDictLearning</code><code class="p">,</code> <code class="n">y_test</code><code class="p">,</code> \&#13;
            <code class="s2">"Mini-batch Dictionary Learning"</code><code class="p">)</code></pre>&#13;
&#13;
<p><a data-type="xref" href="#separation_of_obversations_using_dictionary_learning_and_28_components_on_the_test_set">Figure 4-21</a> shows the scatterplot for dictionary learning on the test set. <a data-type="xref" href="#results_using_the_dictionary_learning_and_28_components_on_the_test_set">Figure 4-22</a> displays the results for dictionary learning on the test set.</p>&#13;
&#13;
<figure><div class="figure" id="separation_of_obversations_using_dictionary_learning_and_28_components_on_the_test_set">&#13;
<img alt="Separation of Obversations Using Dictionary Learning and 28 Components on the Test Set" src="assets/hulp_0421.png"/>&#13;
<h6><span class="label">Figure 4-21. </span>Separation of observations using dictionary learning and 28 components on the test set</h6>&#13;
</div></figure>&#13;
&#13;
<figure><div class="figure" id="results_using_the_dictionary_learning_and_28_components_on_the_test_set">&#13;
<img alt="Results Using Dictionary Learning and 28 Components on the Test Set" src="assets/hulp_0422.png"/>&#13;
<h6><span class="label">Figure 4-22. </span>Results using dictionary learning and 28 components on the test set</h6>&#13;
</div></figure>&#13;
&#13;
<p>While the results are not terrible—we can catch 80% of the fraud with a 20% precision—they fall far short of the results from normal PCA and ICA.<a data-primary="" data-startref="ADtest04" data-type="indexterm" id="idm140637547524576"/><a data-primary="" data-startref="SKLanomaly04" data-type="indexterm" id="idm140637547523600"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="idm140637547522528">&#13;
<h1>Conclusion</h1>&#13;
&#13;
<p>In this chapter, we used the core dimensionality reduction algorithms from the previous chapter to develop fraud detection solutions for the credit card transactions dataset from <a data-type="xref" href="ch02.html#Chapter_2">Chapter 2</a>.</p>&#13;
&#13;
<p>In <a data-type="xref" href="ch02.html#Chapter_2">Chapter 2</a> we used labels to build a fraud detection solution, but we did not use any labels during the training process in this chapter. In other words, we built an applied fraud detection system using unsupervised learning.</p>&#13;
&#13;
<p>While not all the dimensionality reduction algorithms performed well on this credit card transactions dataset, two performed remarkably well—normal PCA and ICA.</p>&#13;
&#13;
<p>Normal PCA and ICA caught over 80% of the known fraud with an 80% precision. By comparison, the best-performing supervised learning-based fraud detection system from <a data-type="xref" href="ch02.html#Chapter_2">Chapter 2</a> caught nearly 90% of the known fraud with an 80% precision. The unsupervised fraud detection system is only marginally worse than the supervised system at catching known patterns of fraud.</p>&#13;
&#13;
<p>Recall that unsupervised fraud detection systems require no labels for training, adapt well to changing fraud patterns, and can catch fraud that had gone previously undiscovered. Given these additional advantages, the unsupervised learning-based solution will generally perform better than the supervised learning-based solution at catching known and unknown or newly emerging patterns of fraud in the future, although using both in tandem is best.</p>&#13;
&#13;
<p>Now that we’ve covered dimensionality reduction and anomaly detection, let’s explore clustering, another major concept in the field of unsupervised learning.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>