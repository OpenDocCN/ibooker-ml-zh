- en: Chapter 15\. Metrics and Regression Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter will evaluate the results of a random forest regressor trained
    on the Boston housing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `sklearn.metrics` module includes metrics to evaluate regression models.
    Metric functions ending in `loss` or `error` should be minimized. Functions ending
    in `score` should be maximized.
  prefs: []
  type: TYPE_NORMAL
- en: The *coefficient of determination* (r²) is a common regression metric. This
    value is typically between 0 and 1\. It represents the percent of the variance
    of the target that the features contribute. Higher values are better, but in general
    it is difficult to evaluate the model from this metric alone. Does a .7 mean it
    is a good score? It depends. For a given dataset, .5 might be a good score, while
    for another dataset, a .9 may be a bad score. Typically we use this number in
    combination with other metrics or visualizations to evaluate a model.
  prefs: []
  type: TYPE_NORMAL
- en: For example, it is easy to make a model that predicts stock prices for the next
    day with an r² of .99\. But I wouldn’t trade my own money with that model. It
    might be slightly low or high, which can wreak havoc on trades.
  prefs: []
  type: TYPE_NORMAL
- en: The r² metric is the default metric used during grid search. You can specify
    other metrics using the `scoring` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `.score` method calculates this for regression models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'There is also an *explained variance* metric (`''explained_variance''` in grid
    search). If the mean of the *residuals* (errors in predictions) is 0 (in ordinary
    least squares (OLS) models), then the variance explained is the same as the coefficient
    of determination:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '*Mean absolute error* (`''neg_mean_absolute_error''` when used in grid search)
    expresses the average absolute model prediction error. A perfect model would score
    0, but this metric has no upper bounds, unlike the coefficient of determination.
    However, since it is in units of the target, it is more interpretable. If you
    want to ignore outliers, this is a good metric to use.'
  prefs: []
  type: TYPE_NORMAL
- en: This measure cannot indicate how bad a model is, but can be used to compare
    two models. If you have two models, the model with a lower score is better.
  prefs: []
  type: TYPE_NORMAL
- en: 'This number tells us that the average error is about two above or below the
    real value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '*Root mean squared error* (`''neg_mean_squared_error''` in grid search) also
    measures model error in terms of the target. However, because it averages the
    square of errors before taking the square root, it penalizes large errors. If
    you want to penalize large errors, this is a good metric to use. For example,
    if being off by eight is more than two times worse than being off by four.'
  prefs: []
  type: TYPE_NORMAL
- en: As with mean absolute error, this measure cannot indicate how bad a model is,
    but can be used to compare two models. If you assume that errors are normally
    distributed, this is a good choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result tells us if we square the errors and average them, the result will
    be around 9.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The *mean squared logarithmic error* (in grid search, `'neg_mean_squared_log_error'`)
    penalizes underprediction more than overprediction. If you have targets that experience
    exponential growth (population, stock, etc.), this is a good metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you take the log of the error and then square it, the average of these results
    will be 0.021:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Residuals Plot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Good models (with appropriate R2 scores) will exhibit *homoscedasticity*. This
    means the variance is the same for all values of targets regardless of the input.
    Plotted, this looks like randomly distributed values in a residuals plot. If there
    are patterns, the model or the data are problematic.
  prefs: []
  type: TYPE_NORMAL
- en: Residuals plots also show outliers, which can have a big impact on model fitting
    (see [Figure 15-1](#id46)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Yellowbrick can make residuals plots to visualize this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![Residuals plot. Further testing will show these to be heteroscedastic.](assets/mlpr_1501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-1\. Residuals plot. Further testing will show these to be heteroscedastic.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Heteroscedasticity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The [statsmodel library](https://oreil.ly/HtIi5) includes the *Breusch-Pagan
    test* for heteroscedasticity. This means that variance of the residuals varies
    over the predicted values. In the Breusch-Pagan test, if the p-values are significant
    (`p-value` less than 0.05), the null hypothesis of homoscedasticity is rejected.
    This indicates that residuals are heteroscedastic, and the predictions are biased.
  prefs: []
  type: TYPE_NORMAL
- en: 'The test confirms heteroscedasticity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Normal Residuals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The scipy library includes a *probability plot* and the *Kolmogorov-Smirnov
    test*, both of which measure whether the residuals are normal.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can plot a histogram (see [Figure 15-2](#idres1)) to visualize the residuals
    and check for normality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![Histogram of residuals.](assets/mlpr_1502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-2\. Histogram of residuals.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 15-3](#idres2) shows a probability plot. If the samples plotted against
    the quantiles line up, the residuals are normal. We can see that this fails in
    this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![Probability plot of residuals.](assets/mlpr_1503.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-3\. Probability plot of residuals.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Kolmogorov-Smirnov test can evaluate whether a distribution is normal. If
    the p-value is significant (< 0.05), then the values are not normal.
  prefs: []
  type: TYPE_NORMAL
- en: 'This fails as well, which tells us the residuals are not normal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Prediction Error Plot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A prediction error plot shows the real targets against the predicted values.
    For a perfect model these points would line up in a 45-degree line.
  prefs: []
  type: TYPE_NORMAL
- en: As our model seems to predict lower values for the high end of y, the model
    has some performance issues. This is also evident in the residuals plot (see [Figure 15-4](#id47)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the Yellowbrick version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![Prediction error. Plots predicted y (y-hat) versus actual y.](assets/mlpr_1504.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-4\. Prediction error. Plots predicted y (y-hat) versus actual y.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
