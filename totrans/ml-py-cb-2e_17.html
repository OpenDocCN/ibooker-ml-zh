<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 17. Support Vector Machines" data-type="chapter" epub:type="chapter"><div class="chapter" id="support-vector-machines">
<h1><span class="label">Chapter 17. </span>Support Vector Machines</h1>
<section data-pdf-bookmark="17.0 Introduction" data-type="sect1"><div class="sect1" id="id358">
<h1>17.0 Introduction</h1>
<p>To understand <a data-primary="support vector machines" data-type="indexterm" id="ix_supp_vector_mach_ch17"/><a data-primary="hyperplanes" data-type="indexterm" id="id1832"/>support vector machines, we must understand hyperplanes.
Formally, a <em>hyperplane</em> is an <em>n – 1</em> subspace in an
<em>n</em>-dimensional space. While that sounds complex, it
actually is pretty simple. For example, if we wanted to divide a two-dimensional space, we’d use a one-dimensional hyperplane (i.e., a line). If we wanted to divide a three-dimensional space, we’d use a two-dimensional hyperplane (i.e., a flat piece of paper or a bed sheet). A hyperplane is simply a generalization of that concept into <em>n</em> dimensions.</p>
<p><em>Support vector machines</em> classify data by finding the hyperplane that
maximizes the margin between the classes in the training data. In a
two-dimensional example with two classes, we can think of a hyperplane as the widest straight “band” (i.e., line with margins) that separates the two classes.</p>
<p>In this chapter, we cover training support vector machines in a variety
of situations and dive under the hood to look at how we can extend the
approach to tackle common problems.</p>
</div></section>
<section data-pdf-bookmark="17.1 Training a Linear Classifier" data-type="sect1"><div class="sect1" id="training-a-linear-classifier">
<h1>17.1 Training a Linear Classifier</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id543">
<h2>Problem</h2>
<p>You need to <a data-primary="support vector machines" data-secondary="linear classifier training" data-type="indexterm" id="ix_svm_classif_train"/><a data-primary="classification and classifiers" data-secondary="support vector machines" data-type="indexterm" id="ix_classif_svm"/>train a model to classify observations.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id359">
<h2>Solution</h2>
<p>Use a<a data-primary="support vector classifier (SVC)" data-type="indexterm" id="id1833"/><a data-primary="SVC (support vector classifier)" data-type="indexterm" id="id1834"/> <em>support vector classifier</em> (SVC) to find the hyperplane that
maximizes the margins between the classes:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="kn">import</code> <code class="n">LinearSVC</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>

<code class="c1"># Load data with only two classes and two features</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code><code class="p">[:</code><code class="mi">100</code><code class="p">,:</code><code class="mi">2</code><code class="p">]</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code><code class="p">[:</code><code class="mi">100</code><code class="p">]</code>

<code class="c1"># Standardize features</code>
<code class="n">scaler</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>
<code class="n">features_standardized</code> <code class="o">=</code> <code class="n">scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Create support vector classifier</code>
<code class="n">svc</code> <code class="o">=</code> <code class="n">LinearSVC</code><code class="p">(</code><code class="n">C</code><code class="o">=</code><code class="mf">1.0</code><code class="p">)</code>

<code class="c1"># Train model</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">svc</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_standardized</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id360">
<h2>Discussion</h2>
<p>scikit-learn’s <code>LinearSVC</code> <a data-primary="LinearSVC" data-type="indexterm" id="id1835"/>implements a simple SVC. To get an intuition
behind what an SVC is doing, let’s plot out the data and hyperplane.
While SVCs work well in high dimensions, in our solution we loaded
only two features and took a subset of observations so that the data contains
only two classes. This will let us visualize the model. Recall that SVC
attempts to find the <a data-primary="hyperplanes" data-type="indexterm" id="ix_hyper_plane"/>hyperplane—​a line when we have only two
dimensions—​with the maximum margin between the classes. In the following code we plot the two classes on a two-dimensional space, then draw the
hyperplane:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">from</code> <code class="nn">matplotlib</code> <code class="kn">import</code> <code class="n">pyplot</code> <code class="k">as</code> <code class="n">plt</code>

<code class="c1"># Plot data points and color using their class</code>
<code class="n">color</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"black"</code> <code class="k">if</code> <code class="n">c</code> <code class="o">==</code> <code class="mi">0</code> <code class="k">else</code> <code class="s2">"lightgrey"</code> <code class="k">for</code> <code class="n">c</code> <code class="ow">in</code> <code class="n">target</code><code class="p">]</code>
<code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">features_standardized</code><code class="p">[:,</code><code class="mi">0</code><code class="p">],</code> <code class="n">features_standardized</code><code class="p">[:,</code><code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">color</code><code class="p">)</code>

<code class="c1"># Create the hyperplane</code>
<code class="n">w</code> <code class="o">=</code> <code class="n">svc</code><code class="o">.</code><code class="n">coef_</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
<code class="n">a</code> <code class="o">=</code> <code class="o">-</code><code class="n">w</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">/</code> <code class="n">w</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>
<code class="n">xx</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="o">-</code><code class="mf">2.5</code><code class="p">,</code> <code class="mf">2.5</code><code class="p">)</code>
<code class="n">yy</code> <code class="o">=</code> <code class="n">a</code> <code class="o">*</code> <code class="n">xx</code> <code class="o">-</code> <code class="p">(</code><code class="n">svc</code><code class="o">.</code><code class="n">intercept_</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code> <code class="o">/</code> <code class="n">w</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>

<code class="c1"># Plot the hyperplane</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">xx</code><code class="p">,</code> <code class="n">yy</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">();</code></pre>
<figure><div class="figure">
<img alt="mpc2 17in01" height="340" src="assets/mpc2_17in01.png" width="454"/>
<h6/>
</div></figure>
<p>In this visualization, all observations of class 0 are black and
observations of class 1 are light gray. The hyperplane is the
decision boundary deciding how new observations are classified.
Specifically, any observation above the line will by classified as class
0, while any observation below the line will be classified as class 1. We
can prove this by creating a new observation in the top-left corner of
our visualization, meaning it should be predicted to be class 0:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create new observation</code>
<code class="n">new_observation</code> <code class="o">=</code> <code class="p">[[</code> <code class="o">-</code><code class="mi">2</code><code class="p">,</code>  <code class="mi">3</code><code class="p">]]</code>

<code class="c1"># Predict class of new observation</code>
<code class="n">svc</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">new_observation</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([0])</pre>
<p>There are a few things to note about SVCs. First, for the sake of
visualization, we limited our example to a binary example (i.e., only two
classes); however, SVCs can work well with multiple classes. Second, as
our visualization shows, the hyperplane is by definition linear (i.e.,
not curved). This was okay in this example because the data was linearly
separable, meaning there was a hyperplane that could perfectly separate
the two classes. Unfortunately, in the real world this is rarely
the case.<a data-primary="" data-startref="ix_hyper_plane" data-type="indexterm" id="id1836"/></p>
<p>More <a data-primary="C hyperparameter" data-secondary="support vector classifier" data-type="indexterm" id="id1837"/>typically, we will not be able to perfectly separate classes. In
these situations there is a balance between SVC maximizing the margin of
the hyperplane and minimizing the misclassification. In SVC, the latter
is controlled with the hyperparameter <em>C</em>. <em>C</em> is a parameter of the SVC learner and is the
penalty for misclassifying a data point. When <em>C</em> is small, the classifier is okay with misclassified data points (high bias but low variance). When <em>C</em> is large, the classifier is heavily penalized for misclassified data and therefore bends over backward to avoid any misclassified data points (low bias but high variance).</p>
<p>In scikit-learn, <em>C</em> is determined by the parameter <code>C</code> and
defaults to <code>C=1.0</code>. We should treat <em>C</em> has a
hyperparameter of our learning algorithm, which we tune using model
selection techniques  in <a data-type="xref" href="ch12.xhtml#model-selection">Chapter 12</a>.<a data-primary="" data-startref="ix_classif_svm" data-type="indexterm" id="id1838"/><a data-primary="" data-startref="ix_svm_classif_train" data-type="indexterm" id="id1839"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="17.2 Handling Linearly Inseparable Classes Using Kernels" data-type="sect1"><div class="sect1" id="handling-linearly-inseparable-classes-using-kernels">
<h1>17.2 Handling Linearly Inseparable Classes Using Kernels</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id544">
<h2>Problem</h2>
<p>You need to <a data-primary="support vector machines" data-secondary="kernel functions for linearly inseparable classes" data-type="indexterm" id="ix_svm_kernel_func2"/>train a support vector classifier, but your classes are
linearly inseparable.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id361">
<h2>Solution</h2>
<p>Train an extension of a <a data-primary="kernel functions" data-type="indexterm" id="ix_kernel_func2"/>support vector machine using kernel functions to
create nonlinear decision boundaries:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="kn">import</code> <code class="n">SVC</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>

<code class="c1"># Set randomization seed</code>
<code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Generate two features</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="mi">200</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>

<code class="c1"># Use an XOR gate (you don't need to know what this is) to generate</code>
<code class="c1"># linearly inseparable classes</code>
<code class="n">target_xor</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">logical_xor</code><code class="p">(</code><code class="n">features</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">]</code> <code class="o">&gt;</code> <code class="mi">0</code><code class="p">,</code> <code class="n">features</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">]</code> <code class="o">&gt;</code> <code class="mi">0</code><code class="p">)</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">where</code><code class="p">(</code><code class="n">target_xor</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Create a support vector machine with a radial basis function kernel</code>
<code class="n">svc</code> <code class="o">=</code> <code class="n">SVC</code><code class="p">(</code><code class="n">kernel</code><code class="o">=</code><code class="s2">"rbf"</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">gamma</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">C</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Train the classifier</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">svc</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id362">
<h2>Discussion</h2>
<p>A full explanation of support vector machines is outside the scope of
this book. However, a short explanation is likely beneficial for
understanding support vector machines and kernels. For reasons best
learned elsewhere, a support vector classifier can be represented as:</p>
<div data-type="equation">
<math display="block">
<mrow>
<mi>f</mi>
<mrow>
<mo>(</mo>
<mi>x</mi>
<mo>)</mo>
</mrow>
<mo>=</mo>
<msub><mi>β</mi> <mn>0</mn> </msub>
<mo>+</mo>
<munder><mo>∑</mo> <mrow><mi>i</mi><mi>ϵ</mi><mi>S</mi></mrow> </munder>
<msub><mi>α</mi> <mi>i</mi> </msub>
<mi>K</mi>
<mrow>
<mo>(</mo>
<msub><mi>x</mi> <mi>i</mi> </msub>
<mo>,</mo>
<msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo> </mrow> </msub>
<mo>)</mo>
</mrow>
</mrow>
</math>
</div>
<p>where <math display="inline"><msub><mi>β</mi><mn>0</mn></msub></math> is the bias, <math display="inline"><mi>S</mi></math> is the set of all support vector observations, <math display="inline"><mi>α</mi></math> is the model parameters to be learned, and <math display="inline"><mo>(</mo>
<msub><mi>x</mi> <mi>i</mi> </msub>
<mo>,</mo>
<msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo> </mrow> </msub>
<mo>)</mo></math> are pairs of two support vector observations, <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math> and <math display="inline"> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo> </mrow> </msub></math>. Most importantly, <math display="inline"><mi>K</mi></math> is a kernel function that compares the similarity between <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math> and <math display="inline"> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo> </mrow> </msub></math>. Don’t worry if you don’t understand kernel functions. For our purposes, just realize that (1) <math display="inline"><mi>K</mi></math> determines the type of hyperplane used to separate our classes, and (2) we create different hyperplanes by using different kernels. For example, if we want a basic linear hyperplane like the one we created in <a data-type="xref" href="#training-a-linear-classifier">Recipe 17.1</a>, we can use the linear kernel:</p>
<div data-type="equation">
<math display="block">
<mrow>
<mi>K</mi>
<mrow>
<mo>(</mo>
<msub><mi>x</mi> <mi>i</mi> </msub>
<mo>,</mo>
<msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo> </mrow> </msub>
<mo>)</mo>
</mrow>
<mo>=</mo>
<munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>p</mi> </munderover>
<msub><mi>x</mi> <mrow><mi>i</mi><mi>j</mi></mrow> </msub>
<msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo> <mi>j</mi></mrow> </msub>
</mrow>
</math>
</div>
<p>where <math display="inline"><mi>p</mi></math> is the number of features. However, if we want a
nonlinear decision boundary, we swap the linear kernel with a
polynomial kernel:</p>
<div data-type="equation">
<math display="block">
<mrow>
<mi>K</mi>
<mrow>
<mo>(</mo>
<msub><mi>x</mi> <mi>i</mi> </msub>
<mo>,</mo>
<msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo> </mrow> </msub>
<mo>)</mo>
</mrow>
<mo>=</mo>
<msup><mrow><mo>(</mo><mi>r</mi><mo>+</mo><mi>γ</mi><msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>p</mi> </msubsup><msub><mi>x</mi> <mrow><mi>i</mi><mi>j</mi></mrow> </msub><msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo> <mi>j</mi></mrow> </msub><mo>)</mo></mrow> <mi>d</mi> </msup>
</mrow>
</math>
</div>
<p>where <math display="inline"><mi>d</mi></math> is the degree of the polynomial kernel function.
Alternatively, we can use one of the most common kernels in support
vectors machines, the <em>radial basis function kernel</em>:</p>
<div data-type="equation">
<math display="block">
<mrow>
<mi>K</mi>
<mrow>
<mo>(</mo>
<msub><mi>x</mi> <mi>i</mi> </msub>
<mo>,</mo>
<msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo> </mrow> </msub>
<mo>)</mo>
</mrow>
<mo>=</mo>
<msup><mi>e</mi> <mrow><mo>(</mo><mo>-</mo><mi>γ</mi><msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>p</mi> </msubsup><msup><mrow><mo>(</mo><msub><mi>x</mi> <mrow><mi>i</mi><mi>j</mi></mrow> </msub><msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo> <mi>j</mi></mrow> </msub><mo>)</mo></mrow> <mn>2</mn> </msup><mo>)</mo></mrow> </msup>
</mrow>
</math>
</div>
<p>where <math display="inline"><mi>γ</mi></math> is a hyperparameter and must be greater than
zero. <a data-primary="linearly inseparable data, reducing features" data-type="indexterm" id="id1840"/>The main point of the preceding explanation is that if we have
linearly inseparable data, we can swap out a linear kernel with an
alternative kernel to create a nonlinear hyperplane decision boundary.</p>
<p>We can understand the intuition behind kernels by visualizing a simple
example. This function, based on one by Sebastian Raschka, plots the
observations and decision boundary hyperplane of a two-dimensional
space. You do not need to understand how this function works; I have
included it here so you can experiment on your own:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Plot observations and decision boundary hyperplane</code>
<code class="kn">from</code> <code class="nn">matplotlib.colors</code> <code class="kn">import</code> <code class="n">ListedColormap</code>
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>

<code class="k">def</code> <code class="nf">plot_decision_regions</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">classifier</code><code class="p">):</code>
    <code class="n">cmap</code> <code class="o">=</code> <code class="n">ListedColormap</code><code class="p">((</code><code class="s2">"red"</code><code class="p">,</code> <code class="s2">"blue"</code><code class="p">))</code>
    <code class="n">xx1</code><code class="p">,</code> <code class="n">xx2</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">meshgrid</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="o">-</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mf">0.02</code><code class="p">),</code> <code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="o">-</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mf">0.02</code><code class="p">))</code>
    <code class="n">Z</code> <code class="o">=</code> <code class="n">classifier</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="n">xx1</code><code class="o">.</code><code class="n">ravel</code><code class="p">(),</code> <code class="n">xx2</code><code class="o">.</code><code class="n">ravel</code><code class="p">()])</code><code class="o">.</code><code class="n">T</code><code class="p">)</code>
    <code class="n">Z</code> <code class="o">=</code> <code class="n">Z</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">xx1</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">contourf</code><code class="p">(</code><code class="n">xx1</code><code class="p">,</code> <code class="n">xx2</code><code class="p">,</code> <code class="n">Z</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="n">cmap</code><code class="p">)</code>

    <code class="k">for</code> <code class="n">idx</code><code class="p">,</code> <code class="n">cl</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">unique</code><code class="p">(</code><code class="n">y</code><code class="p">)):</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="n">X</code><code class="p">[</code><code class="n">y</code> <code class="o">==</code> <code class="n">cl</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">y</code><code class="o">=</code><code class="n">X</code><code class="p">[</code><code class="n">y</code> <code class="o">==</code> <code class="n">cl</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code>
                    <code class="n">alpha</code><code class="o">=</code><code class="mf">0.8</code><code class="p">,</code> <code class="n">c</code><code class="o">=</code><code class="n">cmap</code><code class="p">(</code><code class="n">idx</code><code class="p">),</code>
                    <code class="n">marker</code><code class="o">=</code><code class="s2">"+"</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="n">cl</code><code class="p">)</code></pre>
<p>In our solution, we have data containing two features (i.e., two dimensions)
and a target vector with the class of each observation. Importantly, the
classes are assigned such that they are <em>linearly inseparable</em>. That is,
there is no straight line we can draw that will divide the two classes.
First, let’s create a support vector machine classifier with a linear
kernel:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create support vector classifier with a linear kernel</code>
<code class="n">svc_linear</code> <code class="o">=</code> <code class="n">SVC</code><code class="p">(</code><code class="n">kernel</code><code class="o">=</code><code class="s2">"linear"</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">C</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Train model</code>
<code class="n">svc_linear</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
<pre data-type="programlisting">SVC(C=1, kernel='linear', random_state=0)</pre>
<p>Next, since we have only two features, we are working in a
two-dimensional space and can visualize the observations, their classes,
and our model’s linear hyperplane:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Plot observations and hyperplane</code>
<code class="n">plot_decision_regions</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">,</code> <code class="n">classifier</code><code class="o">=</code><code class="n">svc_linear</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">();</code></pre>
<figure><div class="figure">
<img alt="mpc2 17in02" height="370" src="assets/mpc2_17in02.png" width="496"/>
<h6/>
</div></figure>
<p>As we can see, our linear hyperplane did very poorly at dividing the
two classes! Now, let’s swap out the linear kernel with a radial basis
function kernel and use it to train a new model:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create a support vector machine with a radial basis function kernel</code>
<code class="n">svc</code> <code class="o">=</code> <code class="n">SVC</code><code class="p">(</code><code class="n">kernel</code><code class="o">=</code><code class="s2">"rbf"</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">gamma</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">C</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># Train the classifier</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">svc</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
<p>And then visualize the observations and hyperplane:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Plot observations and hyperplane</code>
<code class="n">plot_decision_regions</code><code class="p">(</code><code class="n">features</code><code class="p">,</code> <code class="n">target</code><code class="p">,</code> <code class="n">classifier</code><code class="o">=</code><code class="n">svc</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">),</code> <code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">();</code></pre>
<figure><div class="figure">
<img alt="mpc2 17in03" height="370" src="assets/mpc2_17in03.png" width="496"/>
<h6/>
</div></figure>
<p>By using the radial basis function kernel we can create a
decision boundary that is able to do a much better job of separating the two
classes than the linear kernel. This is the motivation behind using
kernels in support vector machines.</p>
<p>In scikit-learn, we can select the kernel we want to use by using the
<code>kernel</code> parameter. Once we select a kernel, we need to specify the
appropriate kernel options, such as the value of <em>d</em> (using
the <code>degree</code> parameter) in polynomial kernels, and the value of γ
(using the <code>gamma</code> parameter) in radial basis function kernels. We will
also need to set the penalty parameter, <code>C</code>. When training the model, in
most cases we should treat all of these as hyperparameters and use model
selection techniques to identify the combination of their values that
produces the model with the best performance.<a data-primary="" data-startref="ix_kernel_func2" data-type="indexterm" id="id1841"/><a data-primary="" data-startref="ix_svm_kernel_func2" data-type="indexterm" id="id1842"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="17.3 Creating Predicted Probabilities" data-type="sect1"><div class="sect1" id="creating-predicted-probabilities">
<h1>17.3 Creating Predicted Probabilities</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id545">
<h2>Problem</h2>
<p>You need to know the <a data-primary="support vector machines" data-secondary="predicted probabilities" data-type="indexterm" id="ix_svm_predict_prob"/><a data-primary="predictions and predicting" data-secondary="SVC predicted probabilities" data-type="indexterm" id="ix_predict_svc_prob"/>predicted class probabilities for an observation.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id1843">
<h2>Solution</h2>
<p>When using scikit-learn’s <code>SVC</code>, set <code>probability=True</code>, train the
model, then use 
<span class="keep-together"><code>predict_proba</code></span> to see the calibrated probabilities:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="kn">import</code> <code class="n">SVC</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>

<code class="c1"># Load data</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="c1"># Standardize features</code>
<code class="n">scaler</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>
<code class="n">features_standardized</code> <code class="o">=</code> <code class="n">scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Create support vector classifier object</code>
<code class="n">svc</code> <code class="o">=</code> <code class="n">SVC</code><code class="p">(</code><code class="n">kernel</code><code class="o">=</code><code class="s2">"linear"</code><code class="p">,</code> <code class="n">probability</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Train classifier</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">svc</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_standardized</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>

<code class="c1"># Create new observation</code>
<code class="n">new_observation</code> <code class="o">=</code> <code class="p">[[</code><code class="mf">.4</code><code class="p">,</code> <code class="mf">.4</code><code class="p">,</code> <code class="mf">.4</code><code class="p">,</code> <code class="mf">.4</code><code class="p">]]</code>

<code class="c1"># View predicted probabilities</code>
<code class="n">model</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">new_observation</code><code class="p">)</code></pre>
<pre data-type="programlisting">array([[0.00541761, 0.97348825, 0.02109414]])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id363">
<h2>Discussion</h2>
<p>Many of the supervised learning algorithms we have covered use
probability estimates to predict classes. For example, in k-nearest
neighbors, an observation’s <em>k</em> neighbor’s classes were treated as votes to create a probability that an observation was of that class. Then the class with the highest probability was predicted. SVC’s use of a hyperplane to create decision regions does not naturally output a probability estimate that an observation is a member of a certain class. However, we can in fact output calibrated class probabilities with a few caveats. In an <a data-primary="Platt scaling" data-type="indexterm" id="id1844"/>SVC with two classes, <em>Platt scaling</em> can be used, wherein first the SVC is trained, and then a separate cross-validated logistic regression is trained to map the SVC outputs into probabilities:</p>
<div data-type="equation">
<math display="block">
<mrow>
<mi>P</mi>
<mrow>
<mo>(</mo>
<mi>y</mi>
<mo>=</mo>
<mn>1</mn>
<mo>∣</mo>
<mi>x</mi>
<mo>)</mo>
</mrow>
<mo>=</mo>
<mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>(</mo><mi>A</mi><mo>×</mo><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>+</mo><mi>B</mi><mo>)</mo></mrow> </msup></mrow></mfrac>
</mrow>
</math>
</div>
<p>where <math display="inline"><mi>A</mi></math> and <math display="inline"><mi>B</mi></math> are parameter vectors, and <math display="inline"><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo></math> is the <math display="inline"><mi>i</mi></math>th observation’s signed distance from the hyperplane. When we have more than two classes, an extension of Platt scaling is used.</p>
<p>In more practical terms, creating predicted probabilities has two major
issues. First, because we are training a second model with
cross-validation, generating predicted probabilities can significantly
increase the time it takes to train our model. Second, because the
predicted probabilities are created using cross-validation, they might
not always match the predicted classes. That is, an observation might be
predicted to be class 1 but have a predicted probability of being class
1 of less than 0.5.</p>
<p>In scikit-learn, the predicted probabilities must be generated when the
model is being trained. We can do this by setting <code>SVC</code>’s <code>probability</code> to <code>True</code>. After the model is trained, we can output the estimated probabilities for each class using <code>predict_proba</code>.<a data-primary="" data-startref="ix_predict_svc_prob" data-type="indexterm" id="id1845"/><a data-primary="" data-startref="ix_svm_predict_prob" data-type="indexterm" id="id1846"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="17.4 Identifying Support Vectors" data-type="sect1"><div class="sect1" id="identifying-support-vectors">
<h1>17.4 Identifying Support Vectors</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id546">
<h2>Problem</h2>
<p>You need to <a data-primary="support vector machines" data-secondary="identifying support vectors" data-type="indexterm" id="id1847"/>identify which observations are the support vectors of the
decision hyperplane.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id364">
<h2>Solution</h2>
<p>Train the <a data-primary="support_vectors_ method" data-type="indexterm" id="id1848"/>model, then use <code>support_vectors_</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="kn">import</code> <code class="n">SVC</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>

<code class="c1"># Load data with only two classes</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code><code class="p">[:</code><code class="mi">100</code><code class="p">,:]</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code><code class="p">[:</code><code class="mi">100</code><code class="p">]</code>

<code class="c1"># Standardize features</code>
<code class="n">scaler</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>
<code class="n">features_standardized</code> <code class="o">=</code> <code class="n">scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Create support vector classifier object</code>
<code class="n">svc</code> <code class="o">=</code> <code class="n">SVC</code><code class="p">(</code><code class="n">kernel</code><code class="o">=</code><code class="s2">"linear"</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Train classifier</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">svc</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_standardized</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>

<code class="c1"># View support vectors</code>
<code class="n">model</code><code class="o">.</code><code class="n">support_vectors_</code></pre>
<pre data-type="programlisting">array([[-0.5810659 ,  0.42196824, -0.80497402, -0.50860702],
       [-1.52079513, -1.67737625, -1.08231219, -0.86427627],
       [-0.89430898, -1.4674418 ,  0.30437864,  0.38056609],
       [-0.5810659 , -1.25750735,  0.09637501,  0.55840072]])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id1849">
<h2>Discussion</h2>
<p>Support vector machines get their name from the fact that the hyperplane
is being determined by a relatively small number of observations, called
the <em>support vectors</em>. Intuitively, think of the hyperplane as being
“carried” by these support vectors. These support vectors are therefore
very important to our model. For example, if we remove an observation
that is not a support vector from the data, the model does not change;
however, if we remove a support vector, the hyperplane will not have
the maximum margin.</p>
<p>After we have trained an SVC, scikit-learn offers a number of options
for identifying the support vector. In our solution, we used
<code>support_vectors_</code> to output the actual observations’ features of the
four support vectors in our model. Alternatively, we can view the
indices of the support vectors using <code>support_</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">model</code><code class="o">.</code><code class="n">support_</code></pre>
<pre data-type="programlisting">array([23, 41, 57, 98], dtype=int32)</pre>
<p>Finally, we can use <code>n_support_</code> to find the number of support vectors belonging
to each class:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">model</code><code class="o">.</code><code class="n">n_support_</code></pre>
<pre data-type="programlisting">array([2, 2], dtype=int32)</pre>
</div></section>
</div></section>
<section data-pdf-bookmark="17.5 Handling Imbalanced Classes" data-type="sect1"><div class="sect1" id="handling-imbalanced-classes-ch17">
<h1>17.5 Handling Imbalanced Classes</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id1850">
<h2>Problem</h2>
<p>You need to train a support vector machine classifier in the presence of
imbalanced classes.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id365">
<h2>Solution</h2>
<p>Increase the <a data-primary="class_weight method" data-type="indexterm" id="id1851"/><a data-primary="support vector machines" data-secondary="and imbalanced classes" data-secondary-sortas="imbalanced classes" data-type="indexterm" id="id1852"/>penalty for misclassifying the smaller class using
<code>class_weight</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load libraries</code>
<code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="kn">import</code> <code class="n">SVC</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>

<code class="c1"># Load data with only two classes</code>
<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code><code class="p">[:</code><code class="mi">100</code><code class="p">,:]</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code><code class="p">[:</code><code class="mi">100</code><code class="p">]</code>

<code class="c1"># Make class highly imbalanced by removing first 40 observations</code>
<code class="n">features</code> <code class="o">=</code> <code class="n">features</code><code class="p">[</code><code class="mi">40</code><code class="p">:,:]</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">target</code><code class="p">[</code><code class="mi">40</code><code class="p">:]</code>

<code class="c1"># Create target vector indicating if class 0, otherwise 1</code>
<code class="n">target</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">where</code><code class="p">((</code><code class="n">target</code> <code class="o">==</code> <code class="mi">0</code><code class="p">),</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>

<code class="c1"># Standardize features</code>
<code class="n">scaler</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code>
<code class="n">features_standardized</code> <code class="o">=</code> <code class="n">scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">features</code><code class="p">)</code>

<code class="c1"># Create support vector classifier</code>
<code class="n">svc</code> <code class="o">=</code> <code class="n">SVC</code><code class="p">(</code><code class="n">kernel</code><code class="o">=</code><code class="s2">"linear"</code><code class="p">,</code> <code class="n">class_weight</code><code class="o">=</code><code class="s2">"balanced"</code><code class="p">,</code> <code class="n">C</code><code class="o">=</code><code class="mf">1.0</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># Train classifier</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">svc</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">features_standardized</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id366">
<h2>Discussion</h2>
<p>In support vector machines, <math display="inline"><mi>C</mi></math> is a <a data-primary="C hyperparameter" data-secondary="support vector classifier" data-type="indexterm" id="id1853"/>hyperparameter that determines the penalty for misclassifying an observation. One method for handling imbalanced classes in support vector machines is to weight <math display="inline"><mi>C</mi></math> by classes, so that:</p>
<div data-type="equation">
<math display="block">
<mrow>
<msub><mi>C</mi> <mi>k</mi> </msub>
<mo>=</mo>
<mi>C</mi>
<mo>×</mo>
<msub><mi>w</mi> <mi>j</mi> </msub>
</mrow>
</math>
</div>
<p>where <math display="inline"><mi>C</mi></math> is the penalty for misclassification,
<math display="inline"><msub><mi>w</mi><mi>j</mi></msub></math> is a weight inversely proportional to class <math display="inline"><mi>j</mi></math>’s frequency, and <math display="inline"><msub><mi>C</mi><mi>k</mi></msub></math> is the
<math display="inline"><mi>C</mi></math> value for class <math display="inline"><mi>k</mi></math>. The general idea is to increase the penalty for misclassifying minority classes to prevent them from being “overwhelmed” by the majority class.</p>
<p>In scikit-learn, when using <code>SVC</code> we can set the values for <math display="inline"><msub><mi>C</mi><mi>k</mi></msub></math> automatically by setting <code>class_weight="balanced"</code>. The <code>balanced</code> argument automatically weighs classes such that:<a data-primary="" data-startref="ix_supp_vector_mach_ch17" data-type="indexterm" id="id1854"/></p>
<div data-type="equation">
<math display="block">
<mrow>
<msub><mi>w</mi> <mi>j</mi> </msub>
<mo>=</mo>
<mfrac><mi>n</mi> <mrow><mi>k</mi><msub><mi>n</mi> <mi>j</mi> </msub></mrow></mfrac>
</mrow>
</math>
</div>
<p>where <math display="inline"><msub><mi>w</mi><mi>j</mi></msub></math> is the weight to class <math display="inline"><mi>j</mi></math>,
<math display="inline"><mi>n</mi></math> is the number of observations, <math display="inline"><msub><mi>n</mi><mi>j</mi></msub></math> is the
number of observations in class <math display="inline"><mi>j</mi></math>, and <math display="inline"><mi>k</mi></math> is the total number of classes.</p>
</div></section>
</div></section>
</div></section></div></body></html>