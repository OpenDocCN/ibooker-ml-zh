- en: Chapter 13\. Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 13.0 Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Linear regression* is one of the simplest supervised learning algorithms in
    our toolkit. If you have ever taken an introductory statistics course in college,
    likely the final topic you covered was linear regression. Linear regression and
    its extensions continue to be a common and useful method of making predictions
    when the target vector is a quantitative value (e.g., home price, age). In this
    chapter we will cover a variety of linear regression methods (and some extensions)
    for creating well-performing prediction models.'
  prefs: []
  type: TYPE_NORMAL
- en: 13.1 Fitting a Line
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to train a model that represents a linear relationship between the
    feature and target vector.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use a linear regression (in scikit-learn, `LinearRegression`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Linear regression assumes that the relationship between the features and the
    target vector is approximately linear. That is, the *effect* (also called *coefficient*,
    *weight*, or *parameter*) of the features on the target vector is constant. In
    our solution, for the sake of explanation, we have trained our model using only
    three features. This means our linear model will be:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>=</mo> <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>0</mn></msub>
    <mo>+</mo> <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mover accent="true"><mi>β</mi>
    <mo>^</mo></mover> <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo>
    <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>3</mn></msub> <msub><mi>x</mi>
    <mn>3</mn></msub> <mo>+</mo> <mi>ϵ</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'where <math display="inline"><mover accent="true"><mi>y</mi> <mo>^</mo></mover></math>
    is our target, <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math>
    is the data for a single feature, <math display="inline"><msub><mover accent="true"><mi>β</mi>
    <mo>^</mo></mover> <mn>1</mn></msub></math> , <math display="inline"><msub><mover
    accent="true"><mi>β</mi><mo>^</mo></mover> <mn>2</mn></msub></math> , and <math
    display="inline"><msub><mover accent="true"><mi>β</mi><mo>^</mo></mover> <mn>3</mn></msub></math>
    are the coefficients identified by fitting the model, and <math display="inline"><mi>ϵ</mi></math>
    is the error. After we have fit our model, we can view the value of each parameter.
    For example, <math display="inline"><msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover>
    <mn>0</mn></msub></math>, also called the *bias* or *intercept*, can be viewed
    using `intercept_`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'And <math display="inline"><msub><mover accent="true"><mi>β</mi><mo>^</mo></mover>
    <mn>1</mn></msub></math> and <math display="inline"><msub><mover accent="true"><mi>β</mi><mo>^</mo></mover>
    <mn>2</mn></msub></math> are shown using `coef_`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In our dataset, the target value is a randomly generated continuous variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `predict` method, we can predict the output based on the input features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Not bad! Our model was off only by about 0.01!
  prefs: []
  type: TYPE_NORMAL
- en: The major advantage of linear regression is its interpretability, in large part
    because the coefficients of the model are the effect of a one-unit change on the
    target vector. Our model’s coefficient of the first feature was ~–0.02, meaning
    that we have the change in target for each additional unit change in the first
    feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `score` function, we can also see how well our model performed on
    the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The default score for linear regression in scikit learn is R², which ranges
    from 0.0 (worst) to 1.0 (best). As we can see in this example, we are very close
    to the perfect value of 1.0\. However it’s worth noting that we are evaluating
    this model on data it has already seen (the training data), where typically we’d
    evaluate on a held-out test set of data instead. Nonetheless, such a high score
    would bode well for our model in a real setting.
  prefs: []
  type: TYPE_NORMAL
- en: 13.2 Handling Interactive Effects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have a feature whose effect on the target variable depends on another feature.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Create an interaction term to capture that dependence using scikit-learn’s
    `PolynomialFeatures`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes a feature’s effect on our target variable is at least partially dependent
    on another feature. For example, imagine a simple coffee-based example where we
    have two binary features—​the presence of sugar (`sugar`) and whether or not we
    have stirred (`stirred`)—and we want to predict if the coffee tastes sweet. Just
    putting sugar in the coffee (`sugar=1, stirred=0`) won’t make the coffee taste
    sweet (all the sugar is at the bottom!) and just stirring the coffee without adding
    sugar (`sugar=0, stirred=1`) won’t make it sweet either. Instead it is the interaction
    of putting sugar in the coffee *and* stirring the coffee (`sugar=1, stirred=1`)
    that will make a coffee taste sweet. The effects of `sugar` and `stirred` on sweetness
    are dependent on each other. In this case we say there is an *interaction effect*
    between the features `sugar` and `stirred`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can account for interaction effects by including a new feature comprising
    the product of corresponding values from the interacting features:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>=</mo> <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>0</mn></msub>
    <mo>+</mo> <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mover accent="true"><mi>β</mi>
    <mo>^</mo></mover> <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo>
    <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>3</mn></msub> <msub><mi>x</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <mi>ϵ</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math display="inline"><msub><mi>x</mi><mn>1</mn></msub></math> and <math
    display="inline"><msub><mi>x</mi><mn>2</mn></msub></math> are the values of the
    `sugar` and `stirred`, respectively, and <math display="inline"><msub><mi>x</mi><mn>1</mn></msub><msub><mi>x</mi><mn>2</mn></msub></math>
    represents the interaction between the two.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our solution, we used a dataset containing only two features. Here is the
    first observation’s values for each of those features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'To create an interaction term, we simply multiply those two values together
    for every observation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then view the interaction term for the first observation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: However, while often we will have a substantive reason for believing there is
    an interaction between two features, sometimes we will not. In those cases it
    can be useful to use scikit-learn’s `PolynomialFeatures` to create interaction
    terms for all combinations of features. We can then use model selection strategies
    to identify the combination of features and interaction terms that produces the
    best model.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create interaction terms using `PolynomialFeatures`, there are three important
    parameters we must set. Most important, `interaction_only=True` tells `PolynomialFeatures`
    to return only interaction terms (and not polynomial features, which we will discuss
    in [Recipe 13.3](#fitting-a-non-linear-relationship)). By default, `PolynomialFeatures`
    will add a feature containing 1s called a *bias*. We can prevent that with `include_bias=False`.
    Finally, the `degree` parameter determines the maximum number of features to create
    interaction terms from (in case we wanted to create an interaction term that is
    the combination of three features). We can see the output of `PolynomialFeatures`
    from our solution by checking to see if the first observation’s feature values
    and interaction term value match our manually calculated version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 13.3 Fitting a Nonlinear Relationship
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to model a nonlinear relationship.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Create a polynomial regression by including polynomial features in a linear
    regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far we have discussed modeling only linear relationships. An example of a
    linear relationship would be the number of stories a building has and the building’s
    height. In linear regression, we assume the effect of number of stories and building
    height is approximately constant, meaning a 20-story building will be roughly
    twice as high as a 10-story building, which will be roughly twice as high as a
    5-story building. Many relationships of interest, however, are not strictly linear.
  prefs: []
  type: TYPE_NORMAL
- en: Often we want to model a nonlinear relationship—for example, the relationship
    between the number of hours a student studies and the score she gets on a test.
    Intuitively, we can imagine there is a big difference in test scores between students
    who study for one hour compared to students who did not study at all. However,
    there is a much smaller difference in test scores between a student who studied
    for 99 hours and a student who studied for 100 hours. The effect that one hour
    of studying has on a student’s test score decreases as the number of hours increases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Polynomial regression is an extension of linear regression that allows us to
    model nonlinear relationships. To create a polynomial regression, convert the
    linear function we used in [Recipe 13.1](#fitting-a-line):'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>=</mo> <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>0</mn></msub>
    <mo>+</mo> <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <mi>ϵ</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'into a polynomial function by adding polynomial features:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>=</mo> <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>0</mn></msub>
    <mo>+</mo> <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mover accent="true"><mi>β</mi>
    <mo>^</mo></mover> <mn>2</mn></msub> <msup><msub><mi>x</mi> <mn>1</mn></msub>
    <mn>2</mn></msup> <mo>+</mo> <mo>.</mo> <mo>.</mo> <mo>.</mo> <mo>+</mo> <msub><mover
    accent="true"><mi>β</mi> <mo>^</mo></mover> <mi>d</mi></msub> <msup><msub><mi>x</mi>
    <mn>1</mn></msub> <mi>d</mi></msup> <mo>+</mo> <mi>ϵ</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math display="inline"><mi>d</mi></math> is the degree of the polynomial.
    How are we able to use a linear regression for a nonlinear function? The answer
    is that we do not change how the linear regression fits the model but rather only
    add polynomial features. That is, the linear regression does not “know” that the
    <math display="inline"><msup><mi>x</mi><mn>2</mn></msup></math> is a quadratic
    transformation of <math display="inline"><mi>x</mi></math>. It just considers
    it one more variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'A more practical description might be in order. To model nonlinear relationships,
    we can create new features that raise an existing feature, <math display="inline"><mi>x</mi></math>,
    up to some power: <math display="inline"><msup><mi>x</mi><mn>2</mn></msup></math>,
    <math display="inline"><msup><mi>x</mi><mn>3</mn></msup></math>, and so on. The
    more of these new features we add, the more flexible the “line” created by our
    model. To make this more explicit, imagine we want to create a polynomial to the
    third degree. For the sake of simplicity, we will focus on only one observation
    (the first observation in the dataset), <math display="inline"><mi>x</mi></math>[0]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'To create a polynomial feature, we would raise the first observation’s value
    to the second degree, <math display="inline"><msup><msub><mi>x</mi> <mn>1</mn></msub>
    <mn>2</mn></msup></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This would be our new feature. We would then also raise the first observation’s
    value to the third degree, <math display="inline"><msup><msub><mi>x</mi> <mn>1</mn></msub>
    <mn>3</mn></msup></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'By including all three features (<math display="inline"><mi>x</mi></math>,
    <math display="inline"><msup><mi>x</mi><mn>2</mn></msup></math>, and <math display="inline"><msup><mi>x</mi><mn>3</mn></msup></math>)
    in our feature matrix and then running a linear regression, we have conducted
    a polynomial regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '`PolynomialFeatures` has two important parameters. First, `degree` determines
    the maximum number of degrees for the polynomial features. For example, `degree=3`
    will generate <math display="inline"><msup><mi>x</mi><mn>2</mn></msup></math>
    and <math display="inline"><msup><mi>x</mi><mn>3</mn></msup></math>. Second, by
    default `PolynomialFeatures` includes a feature containing only 1s (called a bias).
    We can remove that by setting `include_bias=False`.'
  prefs: []
  type: TYPE_NORMAL
- en: 13.4 Reducing Variance with Regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to reduce the variance of your linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use a learning algorithm that includes a *shrinkage penalty* (also called *regularization*)
    like ridge regression and lasso regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In standard linear regression the model trains to minimize the sum of squared
    error between the true (<math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math>)
    and prediction (<math display="inline"><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover>
    <mi>i</mi></msub></math> ) target values, or residual sum of squares (RSS):'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mstyle displaystyle="true" scriptlevel="0"><mrow><mi>R</mi>
    <mi>S</mi> <mi>S</mi> <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></mstyle></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Regularized regression learners are similar, except they attempt to minimize
    RSS *and* some penalty for the total size of the coefficient values, called a
    *shrinkage penalty* because it attempts to “shrink” the model. There are two common
    types of regularized learners for linear regression: ridge regression and the
    lasso. The only formal difference is the type of shrinkage penalty used. In *ridge
    regression*, the shrinkage penalty is a tuning hyperparameter multiplied by the
    squared sum of all coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>RSS</mtext> <mo>+</mo> <mi>α</mi> <munderover><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>p</mi></munderover> <msup><msub><mover
    accent="true"><mi>β</mi> <mo>^</mo></mover> <mi>j</mi></msub> <mn>2</mn></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'where <math display="inline"><msub><mover accent="true"><mi>β</mi><mo>^</mo></mover>
    <mi>j</mi></msub></math> is the coefficient of the <math display="inline"><mi>j</mi></math>th
    of <math display="inline"><mi>p</mi></math> features and <math display="inline"><mi>α</mi></math>
    is a hyperparameter (discussed next). The *lasso* is similar, except the shrinkage
    penalty is a tuning hyperparameter multiplied by the sum of the absolute value
    of all coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mfrac><mn>1</mn> <mrow><mn>2</mn><mi>n</mi></mrow></mfrac>
    <mtext>RSS</mtext> <mo>+</mo> <mi>α</mi> <munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>p</mi></munderover> <mfenced close="|" open="|" separators=""><msub><mover
    accent="true"><mi>β</mi> <mo>^</mo></mover> <mi>j</mi></msub></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math display="inline"><mi>n</mi></math> is the number of observations.
    So which one should we use? As a very general rule of thumb, ridge regression
    often produces slightly better predictions than lasso, but lasso (for reasons
    we will discuss in [Recipe 13.5](#reducing-features-with-lasso-regression)) produces
    more interpretable models. If we want a balance between ridge and lasso’s penalty
    functions we can use *elastic net*, which is simply a regression model with both
    penalties included. Regardless of which one we use, both ridge and lasso regressions
    can penalize large or complex models by including coefficient values in the loss
    function we are trying to minimize.
  prefs: []
  type: TYPE_NORMAL
- en: The hyperparameter, <math display="inline"><mi>α</mi></math>, lets us control
    how much we penalize the coefficients, with higher values of <math display="inline"><mi>α</mi></math>
    creating simpler models. The ideal value of <math display="inline"><mi>α</mi></math>
    should be tuned like any other hyperparameter. In scikit-learn, <math display="inline"><mi>α</mi></math>
    is set using the `alpha` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'scikit-learn includes a `RidgeCV` method that allows us to select the ideal
    value for <math display="inline"><mi>α</mi></math>:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then easily view the best model’s <math display="inline"><mi>α</mi></math>
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'One final note: because in linear regression the value of the coefficients
    is partially determined by the scale of the feature, and in regularized models
    all coefficients are summed together, we must make sure to standardize the feature
    prior to training.'
  prefs: []
  type: TYPE_NORMAL
- en: 13.5 Reducing Features with Lasso Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to simplify your linear regression model by reducing the number of
    features.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use a lasso regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One interesting characteristic of lasso regression’s penalty is that it can
    shrink the coefficients of a model to zero, effectively reducing the number of
    features in the model. For example, in our solution we set `alpha` to `0.5`, and
    we can see that many of the coefficients are 0, meaning their corresponding features
    are not used in the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'However, if we increase α to a much higher value, we see that literally none
    of the features are being used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The practical benefit of this effect is that it means we could include 100 features
    in our feature matrix and then, through adjusting lasso’s α hyperparameter, produce
    a model that uses only 10 (for instance) of the most important features. This
    lets us reduce variance while improving the interpretability of our model (since
    fewer features are easier to explain).
  prefs: []
  type: TYPE_NORMAL
