<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 20. Tensors with PyTorch" data-type="chapter" epub:type="chapter"><div class="chapter" id="tensors-with-pytorch">
<h1><span class="label">Chapter 20. </span>Tensors with PyTorch</h1>
<section data-pdf-bookmark="20.0 Introduction" data-type="sect1"><div class="sect1" id="id390">
<h1>20.0 Introduction</h1>
<p>Just as NumPy is a foundational tool for data manipulation in the machine learning stack, <a data-primary="PyTorch" data-type="indexterm" id="ix_pytorch_ch20"/>PyTorch is a foundational tool for working with tensors in the deep learning stack. Before moving on to deep learning itself, we should familiarize ourselves with PyTorch tensors and create many operations analogous to those performed with NumPy in <a data-type="xref" href="ch01.xhtml#vectors-matrices-and-arrays">Chapter 1</a>.</p>
<p>Although <a data-primary="NumPy arrays" data-secondary="versus PyTorch tensors" data-secondary-sortas="PyTorch tensors" data-type="indexterm" id="id1905"/>PyTorch is just one of multiple deep learning libraries, it is significantly popular both within academia and industry. PyTorch tensors are <em>very</em> similar to NumPy arrays. However, they also allow us to perform tensor operations on GPUs (hardware specialized for deep learning). In this chapter, we’ll familiarize ourselves with the basics of PyTorch tensors and many common low-level operations.</p>
</div></section>
<section data-pdf-bookmark="20.1 Creating a Tensor" data-type="sect1"><div class="sect1" id="creating-a-tensor">
<h1>20.1 Creating a Tensor</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id550">
<h2>Problem</h2>
<p>You <a data-primary="PyTorch" data-secondary="creating tensors" data-type="indexterm" id="id1906"/>need to create a tensor.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id1907">
<h2>Solution</h2>
<p>Use Pytorch to create a tensor:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">import</code> <code class="nn">torch</code>

<code class="c1"># Create a vector as a row</code>
<code class="n">tensor_row</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">])</code>

<code class="c1"># Create a vector as a column</code>
<code class="n">tensor_column</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">(</code>
    <code class="p">[</code>
        <code class="p">[</code><code class="mi">1</code><code class="p">],</code>
        <code class="p">[</code><code class="mi">2</code><code class="p">],</code>
        <code class="p">[</code><code class="mi">3</code><code class="p">]</code>
    <code class="p">]</code>
<code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id1908">
<h2>Discussion</h2>
<p>The main data structure within PyTorch is a tensor, and in many ways tensors are exactly like the multidimensional NumPy arrays used in <a data-type="xref" href="ch01.xhtml#vectors-matrices-and-arrays">Chapter 1</a>. Just like vectors and arrays, these tensors can be represented horizontally (i.e., rows) or vertically (i.e., columns).</p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1909">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/utaTD">PyTorch documentation: Tensors</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="20.2 Creating a Tensor from NumPy" data-type="sect1"><div class="sect1" id="creating-a-tensor-from-numpy">
<h1>20.2 Creating a Tensor from NumPy</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id551">
<h2>Problem</h2>
<p>You need to <a data-primary="NumPy arrays" data-secondary="creating PyTorch tensors from" data-type="indexterm" id="id1910"/>create PyTorch tensors from NumPy arrays.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id391">
<h2>Solution</h2>
<p>Use the <a data-primary="from_numpy function" data-type="indexterm" id="id1911"/>PyTorch <code>from_numpy</code> function:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import libraries</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
<code class="kn">import</code> <code class="nn">torch</code>

<code class="c1"># Create a NumPy array</code>
<code class="n">vector_row</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">])</code>

<code class="c1"># Create a tensor from a NumPy array</code>
<code class="n">tensor_row</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">from_numpy</code><code class="p">(</code><code class="n">vector_row</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id1912">
<h2>Discussion</h2>
<p>As we can see, PyTorch is very similar to NumPy syntactically. In addition, it easily allows us to convert NumPy arrays to PyTorch tensors that we can use on GPUs and other accelerated hardware. At the time of writing, NumPy is mentioned frequently in the PyTorch documentation, and PyTorch itself even offers a way that PyTorch tensors and NumPy arrays can share the same memory to reduce overhead.</p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1913">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/zEJo6">PyTorch documentation: Bridge with NumPy</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="20.3 Creating a Sparse Tensor" data-type="sect1"><div class="sect1" id="creating-a-sparse-tensor">
<h1>20.3 Creating a Sparse Tensor</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id392">
<h2>Problem</h2>
<p>Given <a data-primary="PyTorch" data-secondary="sparse tensor" data-type="indexterm" id="id1914"/><a data-primary="sparse tensor" data-type="indexterm" id="id1915"/>data with very few nonzero values, you want to efficiently represent it with a tensor.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id393">
<h2>Solution</h2>
<p>Use the <a data-primary="to_sparse function" data-type="indexterm" id="id1916"/>PyTorch <code>to_sparse</code> function:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Import libraries</code>
<code class="kn">import</code> <code class="nn">torch</code>

<code class="c1"># Create a tensor</code>
<code class="n">tensor</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">(</code>
<code class="p">[</code>
<code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code>
<code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code>
<code class="p">[</code><code class="mi">3</code><code class="p">,</code> <code class="mi">0</code><code class="p">]</code>
<code class="p">]</code>
<code class="p">)</code>

<code class="c1"># Create a sparse tensor from a regular tensor</code>
<code class="n">sparse_tensor</code> <code class="o">=</code> <code class="n">tensor</code><code class="o">.</code><code class="n">to_sparse</code><code class="p">()</code></pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id1917">
<h2>Discussion</h2>
<p>Sparse tensors are memory-efficient ways to represent data composed of mostly 0s. In <a data-type="xref" href="ch01.xhtml#vectors-matrices-and-arrays">Chapter 1</a> we used <code>scipy</code> to create a  compressed sparse row (CSR) matrix that was no longer a NumPy array.</p>
<p>The <code>torch.Tensor</code> class allows us to create both regular and sparse matrices using the same object. If we inspect the types of the two tensors we just created, we can see they’re actually both of the same class:</p>
<pre data-code-language="python" data-type="programlisting"><code class="nb">print</code><code class="p">(</code><code class="nb">type</code><code class="p">(</code><code class="n">tensor</code><code class="p">))</code>
<code class="nb">print</code><code class="p">(</code><code class="nb">type</code><code class="p">(</code><code class="n">sparse_tensor</code><code class="p">))</code></pre>
<pre data-type="programlisting">&lt;class 'torch.Tensor'&gt;
&lt;class 'torch.Tensor'&gt;</pre>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1918">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/8J3IO">PyTorch documentation: Sparse Tensor</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="20.4 Selecting Elements in a Tensor" data-type="sect1"><div class="sect1" id="selecting-elements-in-a-tensor">
<h1>20.4 Selecting Elements in a Tensor</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id552">
<h2>Problem</h2>
<p>We need to <a data-primary="selecting elements" data-secondary="PyTorch elements" data-type="indexterm" id="ix_sel_elem_pyt"/><a data-primary="PyTorch" data-secondary="selecting elements" data-type="indexterm" id="ix_pyt_sel_elem"/>select specific elements of a tensor.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id394">
<h2>Solution</h2>
<p>Use NumPy-like <a data-primary="slicing" data-secondary="PyTorch" data-type="indexterm" id="ix_slice_pyt"/><a data-primary="indexing, in PyTorch" data-type="indexterm" id="id1919"/>indexing and slicing to return elements:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">import</code> <code class="nn">torch</code>

<code class="c1"># Create vector tensor</code>
<code class="n">vector</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="mi">6</code><code class="p">])</code>

<code class="c1"># Create matrix tensor</code>
<code class="n">matrix</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">(</code>
    <code class="p">[</code>
        <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">],</code>
        <code class="p">[</code><code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="mi">6</code><code class="p">],</code>
        <code class="p">[</code><code class="mi">7</code><code class="p">,</code> <code class="mi">8</code><code class="p">,</code> <code class="mi">9</code><code class="p">]</code>
    <code class="p">]</code>
<code class="p">)</code>

<code class="c1"># Select third element of vector</code>
<code class="n">vector</code><code class="p">[</code><code class="mi">2</code><code class="p">]</code></pre>
<pre data-type="programlisting">tensor(3)</pre>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Select second row, second column</code>
<code class="n">matrix</code><code class="p">[</code><code class="mi">1</code><code class="p">,</code><code class="mi">1</code><code class="p">]</code></pre>
<pre data-type="programlisting">tensor(5)</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id395">
<h2>Discussion</h2>
<p>Like NumPy arrays and most everything in Python, PyTorch tensors are zero-indexed. Both indexing and slicing are supported as well. One key difference is that indexing a PyTorch tensor to return a single element still returns a tensor as opposed to the value of the object itself (which would be in the form of an integer or float). Slicing syntax also has parity with NumPy and will return objects of type tensor in PyTorch:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Select all elements of a vector</code>
<code class="n">vector</code><code class="p">[:]</code></pre>
<pre data-type="programlisting">array([1, 2, 3, 4, 5, 6])</pre>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Select everything up to and including the third element</code>
<code class="n">vector</code><code class="p">[:</code><code class="mi">3</code><code class="p">]</code></pre>
<pre data-type="programlisting">tensor([1, 2, 3])</pre>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Select everything after the third element</code>
<code class="n">vector</code><code class="p">[</code><code class="mi">3</code><code class="p">:]</code></pre>
<pre data-type="programlisting">tensor([4, 5, 6])</pre>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Select the last element</code>
<code class="n">vector</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code></pre>
<pre data-type="programlisting">tensor(6)</pre>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Select the first two rows and all columns of a matrix</code>
<code class="n">matrix</code><code class="p">[:</code><code class="mi">2</code><code class="p">,:]</code></pre>
<pre data-type="programlisting">tensor([[1, 2, 3],
       [4, 5, 6]])</pre>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Select all rows and the second column</code>
<code class="n">matrix</code><code class="p">[:,</code><code class="mi">1</code><code class="p">:</code><code class="mi">2</code><code class="p">]</code></pre>
<pre data-type="programlisting">tensor([[2],
       [5],
       [8]])</pre>
<p>One key difference is that PyTorch tensors do not yet support negative steps when slicing. Therefore, attempting to reverse a tensor using slicing yields an error:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Reverse the vector</code>
<code class="n">vector</code><code class="p">[::</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code></pre>
<pre data-type="programlisting">ValueError: step must be greater than zero</pre>
<p>Instead, if we wish to reverse a tensor we can use the <code>flip</code> method:<a data-primary="" data-startref="ix_pyt_sel_elem" data-type="indexterm" id="id1920"/><a data-primary="" data-startref="ix_sel_elem_pyt" data-type="indexterm" id="id1921"/><a data-primary="" data-startref="ix_slice_pyt" data-type="indexterm" id="id1922"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="n">vector</code><code class="o">.</code><code class="n">flip</code><code class="p">(</code><code class="n">dims</code><code class="o">=</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,))</code></pre>
<pre data-type="programlisting">tensor([6, 5, 4, 3, 2, 1])</pre>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1923">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/8-xj7">PyTorch documentation: Operations on Tensors</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="20.5 Describing a Tensor" data-type="sect1"><div class="sect1" id="describing-a-tensor">
<h1>20.5 Describing a Tensor</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id553">
<h2>Problem</h2>
<p>You want to <a data-primary="PyTorch" data-secondary="attributes for tensors" data-type="indexterm" id="id1924"/>describe the shape, data type, and format of a tensor along with the hardware it’s using.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id1925">
<h2>Solution</h2>
<p>Inpect the <code>shape</code>, <code>dtype</code>, <code>layout</code>, and <code>device</code> attributes of the tensor:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">import</code> <code class="nn">torch</code>

<code class="c1"># Create a tensor</code>
<code class="n">tensor</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([[</code><code class="mi">1</code><code class="p">,</code><code class="mi">2</code><code class="p">,</code><code class="mi">3</code><code class="p">],</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code><code class="mi">2</code><code class="p">,</code><code class="mi">3</code><code class="p">]])</code>

<code class="c1"># Get the shape of the tensor</code>
<code class="n">tensor</code><code class="o">.</code><code class="n">shape</code></pre>
<pre data-type="programlisting">torch.Size([2, 3])</pre>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Get the data type of items in the tensor</code>
<code class="n">tensor</code><code class="o">.</code><code class="n">dtype</code></pre>
<pre data-type="programlisting">torch.int64</pre>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Get the layout of the tensor</code>
<code class="n">tensor</code><code class="o">.</code><code class="n">layout</code></pre>
<pre data-type="programlisting">torch.strided</pre>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Get the device being used by the tensor</code>
<code class="n">tensor</code><code class="o">.</code><code class="n">device</code></pre>
<pre data-type="programlisting">device(type='cpu')</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id1926">
<h2>Discussion</h2>
<p>PyTorch tensors provide a number of helpful attributes for gathering information about a given tensor, including:</p>
<dl>
<dt>Shape</dt>
<dd>
<p>Returns the dimensions of the tensor</p>
</dd>
<dt>Dtype</dt>
<dd>
<p>Returns the data type of objects within the tensor</p>
</dd>
<dt>Layout</dt>
<dd>
<p>Returns the memory layout (most common is <code>strided</code> used for dense tensors)</p>
</dd>
<dt>Device</dt>
<dd>
<p>Returns the hardware the tensor is being stored on (CPU/GPU)</p>
</dd>
</dl>
<p>Again, the key differentiator between tensors and arrays is an attribute like <em>device</em>, because tensors provide us with hardware-accelerated options like GPUs.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="20.6 Applying Operations to Elements" data-type="sect1"><div class="sect1" id="applying-operations-to-elements">
<h1>20.6 Applying Operations to Elements</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id396">
<h2>Problem</h2>
<p>You want to apply an <a data-primary="operations, applying to elements" data-type="indexterm" id="id1927"/><a data-primary="PyTorch" data-secondary="applying operations to elements" data-type="indexterm" id="id1928"/>operation to all elements in a tensor.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id397">
<h2>Solution</h2>
<p>Take <a data-primary="broadcasting" data-type="indexterm" id="id1929"/>advantage of <em>broadcasting</em> with PyTorch:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">import</code> <code class="nn">torch</code>

<code class="c1"># Create a tensor</code>
<code class="n">tensor</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">])</code>

<code class="c1"># Broadcast an arithmetic operation to all elements in a tensor</code>
<code class="n">tensor</code> <code class="o">*</code> <code class="mi">100</code></pre>
<pre data-type="programlisting">tensor([100, 200, 300])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id1930">
<h2>Discussion</h2>
<p class="fix_tracking2">Basic operations in PyTorch will take advantage of broadcasting to parallelize them using accelerated hardware such as GPUs. This is true for supported mathematical operators in Python (+, -, ×, /) and other functions inherent to PyTorch. Unlike NumPy, PyTorch doesn’t include a <code>vectorize</code> method for applying a function over all elements in a tensor. However, PyTorch comes equipped with all of the mathematical tools necessary to distribute and accelerate the usual operations required for deep learning workflows. </p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1931">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/NsPpa">PyTorch documentation: Broadcasting Semantics</a></p>
</li>
<li>
<p><a href="https://oreil.ly/dfzIJ">Vectorization and Broadcasting with PyTorch</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="20.7 Finding the Maximum and Minimum Values" data-type="sect1"><div class="sect1" id="finding-the-maximum-and-minimum-values20">
<h1>20.7 Finding the Maximum and Minimum Values</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id1932">
<h2>Problem</h2>
<p>You need to find the maximum or minimum value in a tensor.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id554">
<h2>Solution</h2>
<p>Use the <a data-primary="max method" data-secondary="PyTorch" data-type="indexterm" id="id1933"/><a data-primary="min method" data-secondary="PyTorch" data-type="indexterm" id="id1934"/><a data-primary="PyTorch" data-secondary="maximum and minimum values" data-type="indexterm" id="id1935"/>PyTorch <code>max</code> and <code>min</code> methods:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">import</code> <code class="nn">torch</code>

<code class="c1"># Create a tensor</code>
<code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code><code class="mi">2</code><code class="p">,</code><code class="mi">3</code><code class="p">])</code>

<code class="c1"># Find the largest value</code>
<code class="n">tensor</code><code class="o">.</code><code class="n">max</code><code class="p">()</code></pre>
<pre data-type="programlisting">tensor(3)</pre>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Find the smallest value</code>
<code class="n">tensor</code><code class="o">.</code><code class="n">min</code><code class="p">()</code></pre>
<pre data-type="programlisting">tensor(1)</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id1936">
<h2>Discussion</h2>
<p>The <code>max</code> and <code>min</code> methods of a tensor help us find the largest or smallest values in that tensor. These methods work the same across multidimensional tensors as well:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Create a multidimensional tensor</code>
<code class="n">tensor</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([[</code><code class="mi">1</code><code class="p">,</code><code class="mi">2</code><code class="p">,</code><code class="mi">3</code><code class="p">],[</code><code class="mi">1</code><code class="p">,</code><code class="mi">2</code><code class="p">,</code><code class="mi">5</code><code class="p">]])</code>

<code class="c1"># Find the largest value</code>
<code class="n">tensor</code><code class="o">.</code><code class="n">max</code><code class="p">()</code></pre>
<pre data-type="programlisting">tensor(5)</pre>
</div></section>
</div></section>
<section data-pdf-bookmark="20.8 Reshaping Tensors" data-type="sect1"><div class="sect1" id="reshaping-tensors">
<h1>20.8 Reshaping Tensors</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id555">
<h2>Problem</h2>
<p>You want to change the <a data-primary="PyTorch" data-secondary="reshaping tensors" data-type="indexterm" id="id1937"/>shape (number of rows and columns) of a tensor without changing the element values.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id556">
<h2>Solution</h2>
<p>Use the <a data-primary="reshape method" data-secondary="PyTorch" data-type="indexterm" id="id1938"/>PyTorch <code>reshape</code> method:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">import</code> <code class="nn">torch</code>

<code class="c1"># Create 4x3 tensor</code>
<code class="n">tensor</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">],</code>
                       <code class="p">[</code><code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="mi">6</code><code class="p">],</code>
                       <code class="p">[</code><code class="mi">7</code><code class="p">,</code> <code class="mi">8</code><code class="p">,</code> <code class="mi">9</code><code class="p">],</code>
                       <code class="p">[</code><code class="mi">10</code><code class="p">,</code> <code class="mi">11</code><code class="p">,</code> <code class="mi">12</code><code class="p">]])</code>

<code class="c1"># Reshape tensor into 2x6 tensor</code>
<code class="n">tensor</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">6</code><code class="p">)</code></pre>
<pre data-type="programlisting">tensor([[ 1,  2,  3,  4,  5,  6],
        [ 7,  8,  9, 10, 11, 12]])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id1939">
<h2>Discussion</h2>
<p>Manipulating the shape of a tensor can be common in the field of deep learning, as neurons in a neural network often require tensors of a very specific shape. Since the required shape of a tensor can change between neurons in a given neural network, it is good to have a low-level understanding of our inputs and outputs in deep learning.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="20.9 Transposing a Tensor" data-type="sect1"><div class="sect1" id="transposing-a-tensor">
<h1>20.9 Transposing a Tensor</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id557">
<h2>Problem</h2>
<p>You need to <a data-primary="PyTorch" data-secondary="transposing tensors" data-type="indexterm" id="id1940"/><a data-primary="transposing" data-secondary="tensors" data-type="indexterm" id="id1941"/>transpose a tensor.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id398">
<h2>Solution</h2>
<p>Use <a data-primary="mT method" data-type="indexterm" id="id1942"/>the <code>mT</code> method:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">import</code> <code class="nn">torch</code>

<code class="c1"># Create a two-dimensional tensor</code>
<code class="n">tensor</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([[[</code><code class="mi">1</code><code class="p">,</code><code class="mi">2</code><code class="p">,</code><code class="mi">3</code><code class="p">]]])</code>

<code class="c1"># Transpose it</code>
<code class="n">tensor</code><code class="o">.</code><code class="n">mT</code></pre>
<pre data-type="programlisting">tensor([[1],
        [2],
        [3]])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id399">
<h2>Discussion</h2>
<p>Transposing with PyTorch is slightly different from NumPy. The <code>T</code> method used for NumPy arrays is supported in PyTorch only with tensors of two dimensions and at the time of writing is deprecated for tensors of other shapes. The <code>mT</code> method used to transpose batches of tensors is preferred, as it scales to greater than two dimensions.</p>
<p>An <a data-primary="permute method" data-type="indexterm" id="id1943"/>additional way to transpose PyTorch tensors of any shape is to use the <code>permute</code> method:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">tensor</code><code class="o">.</code><code class="n">permute</code><code class="p">(</code><code class="o">*</code><code class="n">torch</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="n">tensor</code><code class="o">.</code><code class="n">ndim</code> <code class="o">-</code> <code class="mi">1</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">))</code></pre>
<pre data-type="programlisting">tensor([[1],
        [2],
        [3]])</pre>
<p>This method also works for one-dimensional tensors (for which the value of the tranposed tensor is the same as the original tensor).</p>
</div></section>
</div></section>
<section data-pdf-bookmark="20.10 Flattening a Tensor" data-type="sect1"><div class="sect1" id="flattening-a-tensor">
<h1>20.10 Flattening a Tensor</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id558">
<h2>Problem</h2>
<p>You <a data-primary="PyTorch" data-secondary="flattening tensors" data-type="indexterm" id="id1944"/>need to transform a tensor into one dimension.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id559">
<h2>Solution</h2>
<p>Use <a data-primary="flatten method" data-secondary="PyTorch" data-type="indexterm" id="id1945"/>the <code>flatten</code> method:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">import</code> <code class="nn">torch</code>

<code class="c1"># Create tensor</code>
<code class="n">tensor</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">],</code>
                       <code class="p">[</code><code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="mi">6</code><code class="p">],</code>
                       <code class="p">[</code><code class="mi">7</code><code class="p">,</code> <code class="mi">8</code><code class="p">,</code> <code class="mi">9</code><code class="p">]])</code>

<code class="c1"># Flatten tensor</code>
<code class="n">tensor</code><code class="o">.</code><code class="n">flatten</code><code class="p">()</code></pre>
<pre data-type="programlisting">tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id1946">
<h2>Discussion</h2>
<p>Flattening a tensor is a useful technique for reducing a multidimensional tensor into one dimension.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="20.11 Calculating Dot Products" data-type="sect1"><div class="sect1" id="calculating-dot-products20">
<h1>20.11 Calculating Dot Products</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id400">
<h2>Problem</h2>
<p>You need to calculate the <a data-primary="PyTorch" data-secondary="dot product calculation" data-type="indexterm" id="id1947"/><a data-primary="dot product calculation" data-type="indexterm" id="id1948"/>dot product of two tensors.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id401">
<h2>Solution</h2>
<p>Use <a data-primary="dot method (PyTorch)" data-type="indexterm" id="id1949"/>the <code>dot</code> method:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">import</code> <code class="nn">torch</code>

<code class="c1"># Create one tensor</code>
<code class="n">tensor_1</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">])</code>

<code class="c1"># Create another tensor</code>
<code class="n">tensor_2</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([</code><code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="mi">6</code><code class="p">])</code>

<code class="c1"># Calculate the dot product of the two tensors</code>
<code class="n">tensor_1</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">tensor_2</code><code class="p">)</code></pre>
<pre data-type="programlisting">tensor(32)</pre>
</div></section>
<section class="less_space pagebreak-before" data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id1950">
<h2>Discussion</h2>
<p>Calculating the dot product of two tensors is a common operation useful in the deep learning space as well as the information retrieval space. You may remember earlier in the book where we used the dot product of two vectors to perform a cosine similarity-based search. Doing this in PyTorch on GPU (instead of with NumPy or scikit-learn on CPU) can yield impressive performance benefits on information retrieval problems.</p>
</div></section>
<section data-pdf-bookmark="See Also" data-type="sect2"><div class="sect2" id="id1951">
<h2>See Also</h2>
<ul>
<li>
<p><a href="https://oreil.ly/lIjtB">Vectorization and Broadcasting with PyTorch</a></p>
</li>
</ul>
</div></section>
</div></section>
<section data-pdf-bookmark="20.12 Multiplying Tensors" data-type="sect1"><div class="sect1" id="multiplying-tensors">
<h1>20.12 Multiplying Tensors</h1>
<section data-pdf-bookmark="Problem" data-type="sect2"><div class="sect2" id="id402">
<h2>Problem</h2>
<p>You need to <a data-primary="PyTorch" data-secondary="multiplying tensors" data-type="indexterm" id="id1952"/><a data-primary="multiplying tensors" data-type="indexterm" id="id1953"/>multiply two tensors.</p>
</div></section>
<section data-pdf-bookmark="Solution" data-type="sect2"><div class="sect2" id="id1954">
<h2>Solution</h2>
<p>Use basic Python arithmetic operators:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># Load library</code>
<code class="kn">import</code> <code class="nn">torch</code>

<code class="c1"># Create one tensor</code>
<code class="n">tensor_1</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">])</code>

<code class="c1"># Create another tensor</code>
<code class="n">tensor_2</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">([</code><code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="mi">6</code><code class="p">])</code>

<code class="c1"># Multiply the two tensors</code>
<code class="n">tensor_1</code> <code class="o">*</code> <code class="n">tensor_2</code></pre>
<pre data-type="programlisting">tensor([ 4, 10, 18])</pre>
</div></section>
<section data-pdf-bookmark="Discussion" data-type="sect2"><div class="sect2" id="id403">
<h2>Discussion</h2>
<p>PyTorch supports basic <a data-primary="arithmetic operators, for multiplying tensors" data-type="indexterm" id="id1955"/>arithmetic operators such as ×, +, - and /. Although multiplying tensors is probably one of the most common operations used in deep learning, it’s useful to know tensors can also be added, subtracted, and divided.<a data-primary="" data-startref="ix_pytorch_ch20" data-type="indexterm" id="id1956"/></p>
<p>Add one tensor to another:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">tensor_1</code><code class="o">+</code><code class="n">tensor_2</code></pre>
<pre data-type="programlisting">tensor([5, 7, 9])</pre>
<p>Subtract one tensor from another:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">tensor_1</code><code class="o">-</code><code class="n">tensor_2</code></pre>
<pre data-type="programlisting">tensor([-3, -3, -3])</pre>
<p>Divide one tensor by another:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">tensor_1</code><code class="o">/</code><code class="n">tensor_2</code></pre>
<pre data-type="programlisting">tensor([0.2500, 0.4000, 0.5000])</pre>
</div></section>
</div></section>
</div></section></div></body></html>