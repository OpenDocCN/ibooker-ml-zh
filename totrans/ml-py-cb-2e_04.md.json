["```py\n# Load libraries\nimport numpy as np\nfrom sklearn import preprocessing\n\n# Create feature\nfeature = np.array([[-500.5],\n                    [-100.1],\n                    [0],\n                    [100.1],\n                    [900.9]])\n\n# Create scaler\nminmax_scale = preprocessing.MinMaxScaler(feature_range=(0, 1))\n\n# Scale feature\nscaled_feature = minmax_scale.fit_transform(feature)\n\n# Show feature\nscaled_feature\n```", "```py\narray([[ 0\\.        ],\n       [ 0.28571429],\n       [ 0.35714286],\n       [ 0.42857143],\n       [ 1\\.        ]])\n```", "```py\narray([[ 0\\.        ],\n      [ 0.28571429],\n      [ 0.35714286],\n      [ 0.42857143],\n      [ 1\\.        ]])\n```", "```py\n# Load libraries\nimport numpy as np\nfrom sklearn import preprocessing\n\n# Create feature\nx = np.array([[-1000.1],\n              [-200.2],\n              [500.5],\n              [600.6],\n              [9000.9]])\n\n# Create scaler\nscaler = preprocessing.StandardScaler()\n\n# Transform the feature\nstandardized = scaler.fit_transform(x)\n\n# Show feature\nstandardized\n```", "```py\narray([[-0.76058269],\n       [-0.54177196],\n       [-0.35009716],\n       [-0.32271504],\n       [ 1.97516685]])\n```", "```py\n# Print mean and standard deviation\nprint(\"Mean:\", round(standardized.mean()))\nprint(\"Standard deviation:\", standardized.std())\n```", "```py\nMean: 0.0\nStandard deviation: 1.0\n```", "```py\n# Create scaler\nrobust_scaler = preprocessing.RobustScaler()\n\n# Transform feature\nrobust_scaler.fit_transform(x)\n```", "```py\narray([[ -1.87387612],\n       [ -0.875     ],\n       [  0\\.        ],\n       [  0.125     ],\n       [ 10.61488511]])\n```", "```py\n# Load libraries\nimport numpy as np\nfrom sklearn.preprocessing import Normalizer\n\n# Create feature matrix\nfeatures = np.array([[0.5, 0.5],\n                     [1.1, 3.4],\n                     [1.5, 20.2],\n                     [1.63, 34.4],\n                     [10.9, 3.3]])\n\n# Create normalizer\nnormalizer = Normalizer(norm=\"l2\")\n\n# Transform feature matrix\nnormalizer.transform(features)\n```", "```py\narray([[ 0.70710678,  0.70710678],\n       [ 0.30782029,  0.95144452],\n       [ 0.07405353,  0.99725427],\n       [ 0.04733062,  0.99887928],\n       [ 0.95709822,  0.28976368]])\n```", "```py\n# Transform feature matrix\nfeatures_l2_norm = Normalizer(norm=\"l2\").transform(features)\n\n# Show feature matrix\nfeatures_l2_norm\n```", "```py\narray([[ 0.70710678,  0.70710678],\n       [ 0.30782029,  0.95144452],\n       [ 0.07405353,  0.99725427],\n       [ 0.04733062,  0.99887928],\n       [ 0.95709822,  0.28976368]])\n```", "```py\n# Transform feature matrix\nfeatures_l1_norm = Normalizer(norm=\"l1\").transform(features)\n\n# Show feature matrix\nfeatures_l1_norm\n```", "```py\narray([[ 0.5       ,  0.5       ],\n       [ 0.24444444,  0.75555556],\n       [ 0.06912442,  0.93087558],\n       [ 0.04524008,  0.95475992],\n       [ 0.76760563,  0.23239437]])\n```", "```py\n# Print sum\nprint(\"Sum of the first observation\\'s values:\",\n   features_l1_norm[0, 0] + features_l1_norm[0, 1])\n```", "```py\nSum of the first observation's values: 1.0\n```", "```py\n# Load libraries\nimport numpy as np\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Create feature matrix\nfeatures = np.array([[2, 3],\n                     [2, 3],\n                     [2, 3]])\n\n# Create PolynomialFeatures object\npolynomial_interaction = PolynomialFeatures(degree=2, include_bias=False)\n\n# Create polynomial features\npolynomial_interaction.fit_transform(features)\n```", "```py\narray([[ 2.,  3.,  4.,  6.,  9.],\n       [ 2.,  3.,  4.,  6.,  9.],\n       [ 2.,  3.,  4.,  6.,  9.]])\n```", "```py\ninteraction = PolynomialFeatures(degree=2,\n              interaction_only=True, include_bias=False)\n\ninteraction.fit_transform(features)\n```", "```py\narray([[ 2.,  3.,  6.],\n       [ 2.,  3.,  6.],\n       [ 2.,  3.,  6.]])\n```", "```py\n# Load libraries\nimport numpy as np\nfrom sklearn.preprocessing import FunctionTransformer\n\n# Create feature matrix\nfeatures = np.array([[2, 3],\n                     [2, 3],\n                     [2, 3]])\n\n# Define a simple function\ndef add_ten(x: int) -> int:\n    return x + 10\n\n# Create transformer\nten_transformer = FunctionTransformer(add_ten)\n\n# Transform feature matrix\nten_transformer.transform(features)\n```", "```py\narray([[12, 13],\n       [12, 13],\n       [12, 13]])\n```", "```py\n# Load library\nimport pandas as pd\n\n# Create DataFrame\ndf = pd.DataFrame(features, columns=[\"feature_1\", \"feature_2\"])\n\n# Apply function\ndf.apply(add_ten)\n```", "```py\n# Load libraries\nimport numpy as np\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.datasets import make_blobs\n\n# Create simulated data\nfeatures, _ = make_blobs(n_samples = 10,\n                         n_features = 2,\n                         centers = 1,\n                         random_state = 1)\n\n# Replace the first observation's values with extreme values\nfeatures[0,0] = 10000\nfeatures[0,1] = 10000\n\n# Create detector\noutlier_detector = EllipticEnvelope(contamination=.1)\n\n# Fit detector\noutlier_detector.fit(features)\n\n# Predict outliers\noutlier_detector.predict(features)\n```", "```py\narray([-1,  1,  1,  1,  1,  1,  1,  1,  1,  1])\n```", "```py\n# Create one feature\nfeature = features[:,0]\n\n# Create a function to return index of outliers\ndef indicies_of_outliers(x: int) -> np.array(int):\n    q1, q3 = np.percentile(x, [25, 75])\n    iqr = q3 - q1\n    lower_bound = q1 - (iqr * 1.5)\n    upper_bound = q3 + (iqr * 1.5)\n    return np.where((x > upper_bound) | (x < lower_bound))\n\n# Run function\nindicies_of_outliers(feature)\n```", "```py\n(array([0]),)\n```", "```py\n# Load library\nimport pandas as pd\n\n# Create DataFrame\nhouses = pd.DataFrame()\nhouses['Price'] = [534433, 392333, 293222, 4322032]\nhouses['Bathrooms'] = [2, 3.5, 2, 116]\nhouses['Square_Feet'] = [1500, 2500, 1500, 48000]\n\n# Filter observations\nhouses[houses['Bathrooms'] < 20]\n```", "```py\n# Load library\nimport numpy as np\n\n# Create feature based on boolean condition\nhouses[\"Outlier\"] = np.where(houses[\"Bathrooms\"] < 20, 0, 1)\n\n# Show data\nhouses\n```", "```py\n# Log feature\nhouses[\"Log_Of_Square_Feet\"] = [np.log(x) for x in houses[\"Square_Feet\"]]\n\n# Show data\nhouses\n```", "```py\n# Load libraries\nimport numpy as np\nfrom sklearn.preprocessing import Binarizer\n\n# Create feature\nage = np.array([[6],\n                [12],\n                [20],\n                [36],\n                [65]])\n\n# Create binarizer\nbinarizer = Binarizer(threshold=18)\n\n# Transform feature\nbinarizer.fit_transform(age)\n```", "```py\narray([[0],\n       [0],\n       [1],\n       [1],\n       [1]])\n```", "```py\n# Bin feature\nnp.digitize(age, bins=[20,30,64])\n```", "```py\narray([[0],\n       [0],\n       [1],\n       [2],\n       [3]])\n```", "```py\n# Bin feature\nnp.digitize(age, bins=[20,30,64], right=True)\n```", "```py\narray([[0],\n       [0],\n       [0],\n       [2],\n       [3]])\n```", "```py\n# Bin feature\nnp.digitize(age, bins=[18])\n```", "```py\narray([[0],\n       [0],\n       [1],\n       [1],\n       [1]])\n```", "```py\n# Load libraries\nimport pandas as pd\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\n\n# Make simulated feature matrix\nfeatures, _ = make_blobs(n_samples = 50,\n                         n_features = 2,\n                         centers = 3,\n                         random_state = 1)\n\n# Create DataFrame\ndataframe = pd.DataFrame(features, columns=[\"feature_1\", \"feature_2\"])\n\n# Make k-means clusterer\nclusterer = KMeans(3, random_state=0)\n\n# Fit clusterer\nclusterer.fit(features)\n\n# Predict values\ndataframe[\"group\"] = clusterer.predict(features)\n\n# View first few observations\ndataframe.head(5)\n```", "```py\n# Load library\nimport numpy as np\n\n# Create feature matrix\nfeatures = np.array([[1.1, 11.1],\n                     [2.2, 22.2],\n                     [3.3, 33.3],\n                     [4.4, 44.4],\n                     [np.nan, 55]])\n\n# Keep only observations that are not (denoted by ~) missing\nfeatures[~np.isnan(features).any(axis=1)]\n```", "```py\narray([[  1.1,  11.1],\n       [  2.2,  22.2],\n       [  3.3,  33.3],\n       [  4.4,  44.4]])\n```", "```py\n# Load library\nimport pandas as pd\n\n# Load data\ndataframe = pd.DataFrame(features, columns=[\"feature_1\", \"feature_2\"])\n\n# Remove observations with missing values\ndataframe.dropna()\n```", "```py\n# Load libraries\nimport numpy as np\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_blobs\n\n# Make a simulated feature matrix\nfeatures, _ = make_blobs(n_samples = 1000,\n                         n_features = 2,\n                         random_state = 1)\n\n# Standardize the features\nscaler = StandardScaler()\nstandardized_features = scaler.fit_transform(features)\n\n# Replace the first feature's first value with a missing value\ntrue_value = standardized_features[0,0]\nstandardized_features[0,0] = np.nan\n\n# Predict the missing values in the feature matrix\nknn_imputer = KNNImputer(n_neighbors=5)\nfeatures_knn_imputed = knn_imputer.fit_transform(standardized_features)\n\n# Compare true and imputed values\nprint(\"True Value:\", true_value)\nprint(\"Imputed Value:\", features_knn_imputed[0,0])\n```", "```py\nTrue Value: 0.8730186114\nImputed Value: 1.09553327131\n```", "```py\n# Load libraries\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_blobs\n\n# Make a simulated feature matrix\nfeatures, _ = make_blobs(n_samples = 1000,\n                         n_features = 2,\n                         random_state = 1)\n\n# Standardize the features\nscaler = StandardScaler()\nstandardized_features = scaler.fit_transform(features)\n\n# Replace the first feature's first value with a missing value\ntrue_value = standardized_features[0,0]\nstandardized_features[0,0] = np.nan\n\n# Create imputer using the \"mean\" strategy\nmean_imputer = SimpleImputer(strategy=\"mean\")\n\n# Impute values\nfeatures_mean_imputed = mean_imputer.fit_transform(features)\n\n# Compare true and imputed values\nprint(\"True Value:\", true_value)\nprint(\"Imputed Value:\", features_mean_imputed[0,0])\n```", "```py\nTrue Value: 0.8730186114\nImputed Value: -3.05837272461\n```"]