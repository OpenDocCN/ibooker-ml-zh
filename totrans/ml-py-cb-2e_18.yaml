- en: Chapter 18\. Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 18.0 Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Bayes’ theorem* is the premier method for understanding the probability of
    some event, <math display="inline"><mi>P</mi> <mrow><mo>(</mo> <mi>A</mi> <mo>∣</mo>
    <mi>B</mi> <mo>)</mo></mrow></math> , given some new information, <math display="inline"><mi>P</mi>
    <mrow><mo>(</mo> <mi>B</mi> <mo>∣</mo> <mi>A</mi> <mo>)</mo></mrow></math> , and
    a prior belief in the probability of the event, <math display="inline"><mi>P</mi>
    <mrow><mo>(</mo> <mi>A</mi> <mo>)</mo></mrow></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <mi>A</mi> <mo>∣</mo>
    <mi>B</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>P</mi><mo>(</mo><mi>B</mi><mo>∣</mo><mi>A</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow>
    <mrow><mi>P</mi><mo>(</mo><mi>B</mi><mo>)</mo></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The Bayesian method’s popularity has skyrocketed in the last decade, increasingly
    rivaling traditional frequentist applications in academia, government, and business.
    In machine learning, one application of Bayes’ theorem to classification comes
    in the form of the *naive Bayes classifier*. Naive Bayes classifiers combine a
    number of desirable qualities in practical machine learning into a single classifier.
    These include:'
  prefs: []
  type: TYPE_NORMAL
- en: An intuitive approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to work with small data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low computation costs for training and prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Often solid results in a variety of settings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Specifically, a naive Bayes classifier is based on:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <mi>y</mi> <mo>∣</mo>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <mo>…</mo> <mo>,</mo> <msub><mi>x</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>P</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>x</mi> <mi>j</mi></msub>
    <mo>∣</mo><mi>y</mi><mo>)</mo></mrow><mi>P</mi><mrow><mo>(</mo><mi>y</mi><mo>)</mo></mrow></mrow>
    <mrow><mi>P</mi><mo>(</mo><msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>x</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="inline"><mi>P</mi> <mrow><mo>(</mo> <mi>y</mi> <mo>∣</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <mo>…</mo> <mo>,</mo> <msub><mi>x</mi> <mi>j</mi></msub>
    <mo>)</mo></mrow></math> is called the *posterior* and is the probability that
    an observation is class <math display="inline"><mi>y</mi></math> given the observation’s
    values for the <math display="inline"><mi>j</mi></math> features, <math display="inline"><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <mo>…</mo> <mo>,</mo> <msub><mi>x</mi> <mi>j</mi></msub></math>
    .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math display="inline"><mi>P</mi><mrow><mo>(</mo><msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>x</mi> <mi>j</mi></msub> <mo>∣</mo><mi>y</mi><mo>)</mo></mrow></math>
    is called *likelihood* and is the likelihood of an observation’s values for features
    <math display="inline"><msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <mo>…</mo>
    <mo>,</mo> <msub><mi>x</mi> <mi>j</mi></msub></math> given their class, <math
    display="inline"><mi>y</mi></math>.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math display="inline"><mi>P</mi><mrow><mo>(</mo><mi>y</mi><mo>)</mo></mrow></math>
    is called the *prior* and is our belief for the probability of class <math display="inline"><mi>y</mi></math>
    before looking at the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math display="inline"><mi>P</mi><mo>(</mo><msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>x</mi> <mi>j</mi></msub> <mo>)</mo></math>
    is called the *marginal probability*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In naive Bayes, we compare an observation’s posterior values for each possible
    class. Specifically, because the marginal probability is constant across these
    comparisons, we compare the numerators of the posterior for each class. For each
    observation, the class with the greatest posterior numerator becomes the predicted
    class, <math display="inline"><mover accent="true"><mi>y</mi><mo>^</mo></mover></math>.
  prefs: []
  type: TYPE_NORMAL
- en: There are two important things to note about naive Bayes classifiers. First,
    for each feature in the data, we have to assume the statistical distribution of
    the likelihood, <math display="inline"><mrow><mi>P</mi><mo>(</mo><msub><mi>x</mi>
    <mi>j</mi></msub> <mo>∣</mo><mi>y</mi><mo>)</mo></mrow></math>. The common distributions
    are the normal (Gaussian), multinomial, and Bernoulli distributions. The distribution
    chosen is often determined by the nature of features (continuous, binary, etc.).
    Second, naive Bayes gets its name because we assume that each feature, and its
    resulting likelihood, is independent. This “naive” assumption is frequently wrong
    yet in practice does little to prevent building high-quality classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we will cover using scikit-learn to train three types of naive
    Bayes classifiers using three different likelihood distributions. Afterwards,
    we will learn to calibrate the predictions from naive Bayes models to make them
    interpretable.
  prefs: []
  type: TYPE_NORMAL
- en: 18.1 Training a Classifier for Continuous Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have only continuous features and you want to train a naive Bayes classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use a Gaussian naive Bayes classifier in scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most common type of naive Bayes classifier is the *Gaussian naive Bayes*.
    In Gaussian naive Bayes, we assume that the likelihood of the feature values <math
    display="inline"><mi>x</mi></math>, given an observation is of class <math display="inline"><mi>y</mi></math>,
    follows a normal distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>p</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>j</mi></msub>
    <mo>∣</mo> <mi>y</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <msqrt><mrow><mn>2</mn><mi>π</mi><msup><msub><mi>σ</mi>
    <mi>y</mi></msub> <mn>2</mn></msup></mrow></msqrt></mfrac> <msup><mi>e</mi> <mrow><mo>-</mo><mfrac><msup><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>j</mi></msub> <mo>-</mo><msub><mi>μ</mi> <mi>y</mi></msub> <mo>)</mo></mrow>
    <mn>2</mn></msup> <mrow><mn>2</mn><msup><msub><mi>σ</mi> <mi>y</mi></msub> <mn>2</mn></msup></mrow></mfrac></mrow></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math display="inline"><msup><msub><mi>σ</mi> <mi>y</mi></msub> <mn>2</mn></msup></math>
    and <math display="inline"><msub><mi>μ</mi> <mi>y</mi></msub></math> are the variance
    and mean values of feature <math display="inline"><msub><mi>x</mi> <mi>j</mi></msub></math>
    for class <math display="inline"><mi>y</mi></math>. Because of the assumption
    of the normal distribution, Gaussian naive Bayes is best used in cases where all
    our features are continuous.
  prefs: []
  type: TYPE_NORMAL
- en: 'In scikit-learn, we train a Gaussian naive Bayes like any other model using
    `fit`, and in turn can then make predictions about the class of an observation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'One of the interesting aspects of naive Bayes classifiers is that they allow
    us to assign a prior belief over the respected target classes. We can do this
    using the `GaussianNB priors` parameter, which takes in a list of the probabilities
    assigned to each class of the target vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: If we do not add any argument to the `priors` parameter, the prior is adjusted
    based on the data.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, note that the raw predicted probabilities from Gaussian naive Bayes
    (outputted using `predict_proba`) are not calibrated. That is, they should not
    be believed. If we want to create useful predicted probabilities, we will need
    to calibrate them using an isotonic regression or a related method.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[How the Naive Bayes Classifier Works in Machine Learning](https://oreil.ly/9yqSw)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 18.2 Training a Classifier for Discrete and Count Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given discrete or count data, you need to train a naive Bayes classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use a multinomial naive Bayes classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Multinomial naive Bayes* works similarly to Gaussian naive Bayes, but the
    features are assumed to be multinomially distributed. In practice, this means
    that this classifier is commonly used when we have discrete data (e.g., movie
    ratings ranging from 1 to 5). One of the most common uses of multinomial naive
    Bayes is text classification using bags of words or <math display="inline"><mtext
    fontstyle="italic">tf-idf</mtext></math> approaches (see Recipes [6.9](ch06.xhtml#encoding-text-as-a-bag-of-words)
    and [6.10](ch06.xhtml#weighting-word-importance)).'
  prefs: []
  type: TYPE_NORMAL
- en: In our solution, we created a toy text dataset of three observations and converted
    the text strings into a bag-of-words feature matrix and an accompanying target
    vector. We then used `MultinomialNB` to train a model while defining the prior
    probabilities for the two classes (pro-`brazil` and pro-`germany`).
  prefs: []
  type: TYPE_NORMAL
- en: '`MultinomialNB` works similarly to `GaussianNB`; models are trained using `fit`,
    and observations can be predicted using `predict`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If `class_prior` is not specified, prior probabilities are learned using the
    data. However, if we want a uniform distribution to be used as the prior, we can
    set `fit_prior=False`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, `MultinomialNB` contains an additive smoothing hyperparameter, `alpha`,
    that should be tuned. The default value is `1.0`, with `0.0` meaning no smoothing
    takes place.
  prefs: []
  type: TYPE_NORMAL
- en: 18.3 Training a Naive Bayes Classifier for Binary Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have binary feature data and need to train a naive Bayes classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use a Bernoulli naive Bayes classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The *Bernoulli naive Bayes* classifier assumes that all our features are binary,
    such that they take only two values (e.g., a nominal categorical feature that
    has been one-hot encoded). Like its multinomial cousin, Bernoulli naive Bayes
    is often used in text classification, when our feature matrix is simply the presence
    or absence of a word in a document. Furthermore, like `MultinomialNB`, `BernoulliNB`
    has an additive smoothing hyperparameter, `alpha`, we will want to tune using
    model selection techniques. Finally, if we want to use priors, we can use the
    `class_prior` parameter with a list containing the prior probabilities for each
    class. If we want to specify a uniform prior, we can set `fit_prior=False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 18.4 Calibrating Predicted Probabilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to calibrate the predicted probabilities from naive Bayes classifiers
    so they are interpretable.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use `CalibratedClassifierCV`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Class probabilities are a common and useful part of machine learning models.
    In scikit-learn, most learning algorithms allow us to see the predicted probabilities
    of class membership using `predict_proba`. This can be extremely useful if, for
    instance, we want to predict a certain class only if the model predicts the probability
    that the class is over 90%. However, some models, including naive Bayes classifiers,
    output probabilities that are not based on the real world. That is, `predict_proba`
    might predict an observation has a 0.70 chance of being a certain class, when
    the reality is that it is 0.10 or 0.99. Specifically in naive Bayes, while the
    ranking of predicted probabilities for the different target classes is valid,
    the raw predicted probabilities tend to take on extreme values close to 0 and
    1.
  prefs: []
  type: TYPE_NORMAL
- en: To obtain meaningful predicted probabilities we need conduct what is called
    *calibration*. In scikit-learn we can use the `CalibratedClassifierCV` class to
    create well-calibrated predicted probabilities using k-fold cross-validation.
    In `CalibratedClassifierCV`, the training sets are used to train the model, and
    the test set is used to calibrate the predicted probabilities. The returned predicted
    probabilities are the average of the k-folds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using our solution we can see the difference between raw and well-calibrated
    predicted probabilities. In our solution, we created a Gaussian naive Bayes classifier.
    If we train that classifier and then predict the class probabilities for a new
    observation, we can see very extreme probability estimates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'However if, after we calibrate the predicted probabilities (which we did in
    our solution), we get very different results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '`CalibratedClassifierCV` offers two calibration methods—​Platt’s sigmoid model
    and isotonic regression—​defined by the `method` parameter. While we don’t have
    the space to go into the specifics, because isotonic regression is nonparametric
    it tends to overfit when sample sizes are very small (e.g., 100 observations).
    In our solution we used the Iris dataset with 150 observations and therefore used
    the Platt’s sigmoid model.'
  prefs: []
  type: TYPE_NORMAL
