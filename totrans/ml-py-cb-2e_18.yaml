- en: Chapter 18\. Naive Bayes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第18章 朴素贝叶斯
- en: 18.0 Introduction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 18.0 引言
- en: '*Bayes’ theorem* is the premier method for understanding the probability of
    some event, <math display="inline"><mi>P</mi> <mrow><mo>(</mo> <mi>A</mi> <mo>∣</mo>
    <mi>B</mi> <mo>)</mo></mrow></math> , given some new information, <math display="inline"><mi>P</mi>
    <mrow><mo>(</mo> <mi>B</mi> <mo>∣</mo> <mi>A</mi> <mo>)</mo></mrow></math> , and
    a prior belief in the probability of the event, <math display="inline"><mi>P</mi>
    <mrow><mo>(</mo> <mi>A</mi> <mo>)</mo></mrow></math> :'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*贝叶斯定理*是理解某些事件概率的首选方法，如在给定一些新信息 <math display="inline"><mi>P</mi> <mrow><mo>(</mo>
    <mi>A</mi> <mo>∣</mo> <mi>B</mi> <mo>)</mo></mrow></math> 和对事件概率的先验信念 <math display="inline"><mi>P</mi>
    <mrow><mo>(</mo> <mi>A</mi> <mo>)</mo></mrow></math> 的情况下，事件 <math display="inline"><mi>P</mi>
    <mrow><mo>(</mo> <mi>B</mi> <mo>∣</mo> <mi>A</mi> <mo>)</mo></mrow></math> 的概率。'
- en: <math display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <mi>A</mi> <mo>∣</mo>
    <mi>B</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>P</mi><mo>(</mo><mi>B</mi><mo>∣</mo><mi>A</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow>
    <mrow><mi>P</mi><mo>(</mo><mi>B</mi><mo>)</mo></mrow></mfrac></mrow></math>
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <mi>A</mi> <mo>∣</mo>
    <mi>B</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>P</mi><mo>(</mo><mi>B</mi><mo>∣</mo><mi>A</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow>
    <mrow><mi>P</mi><mo>(</mo><mi>B</mi><mo>)</mo></mrow></mfrac></mrow></math>
- en: 'The Bayesian method’s popularity has skyrocketed in the last decade, increasingly
    rivaling traditional frequentist applications in academia, government, and business.
    In machine learning, one application of Bayes’ theorem to classification comes
    in the form of the *naive Bayes classifier*. Naive Bayes classifiers combine a
    number of desirable qualities in practical machine learning into a single classifier.
    These include:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯方法在过去十年中的流行度急剧上升，越来越多地在学术界、政府和企业中与传统的频率学应用竞争。在机器学习中，贝叶斯定理在分类问题上的一种应用是*朴素贝叶斯分类器*。朴素贝叶斯分类器将多种实用的机器学习优点结合到一个单一的分类器中。这些优点包括：
- en: An intuitive approach
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种直观的方法
- en: The ability to work with small data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够处理少量数据
- en: Low computation costs for training and prediction
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和预测的低计算成本
- en: Often solid results in a variety of settings
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在各种设置中通常能够产生可靠的结果
- en: 'Specifically, a naive Bayes classifier is based on:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，朴素贝叶斯分类器基于：
- en: <math display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <mi>y</mi> <mo>∣</mo>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <mo>…</mo> <mo>,</mo> <msub><mi>x</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>P</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>x</mi> <mi>j</mi></msub>
    <mo>∣</mo><mi>y</mi><mo>)</mo></mrow><mi>P</mi><mrow><mo>(</mo><mi>y</mi><mo>)</mo></mrow></mrow>
    <mrow><mi>P</mi><mo>(</mo><msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>x</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow></mfrac></mrow></math>
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <mi>y</mi> <mo>∣</mo>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <mo>…</mo> <mo>,</mo> <msub><mi>x</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>P</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>x</mi> <mi>j</mi></msub>
    <mo>∣</mo><mi>y</mi><mo>)</mo></mrow><mi>P</mi><mrow><mo>(</mo><mi>y</mi><mo>)</mo></mrow></mrow>
    <mrow><mi>P</mi><mo>(</mo><msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>x</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow></mfrac></mrow></math>
- en: 'where:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: <math display="inline"><mi>P</mi> <mrow><mo>(</mo> <mi>y</mi> <mo>∣</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <mo>…</mo> <mo>,</mo> <msub><mi>x</mi> <mi>j</mi></msub>
    <mo>)</mo></mrow></math> is called the *posterior* and is the probability that
    an observation is class <math display="inline"><mi>y</mi></math> given the observation’s
    values for the <math display="inline"><mi>j</mi></math> features, <math display="inline"><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <mo>…</mo> <mo>,</mo> <msub><mi>x</mi> <mi>j</mi></msub></math>
    .
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math display="inline"><mi>P</mi> <mrow><mo>(</mo> <mi>y</mi> <mo>∣</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <mo>…</mo> <mo>,</mo> <msub><mi>x</mi> <mi>j</mi></msub>
    <mo>)</mo></mrow></math> 被称为*后验概率*，表示观察值为 <math display="inline"><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <mo>…</mo> <mo>,</mo> <msub><mi>x</mi> <mi>j</mi></msub></math>
    特征时类别 <math display="inline"><mi>y</mi></math> 的概率。
- en: <math display="inline"><mi>P</mi><mrow><mo>(</mo><msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>x</mi> <mi>j</mi></msub> <mo>∣</mo><mi>y</mi><mo>)</mo></mrow></math>
    is called *likelihood* and is the likelihood of an observation’s values for features
    <math display="inline"><msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <mo>…</mo>
    <mo>,</mo> <msub><mi>x</mi> <mi>j</mi></msub></math> given their class, <math
    display="inline"><mi>y</mi></math>.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math display="inline"><mi>P</mi><mrow><mo>(</mo><msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>x</mi> <mi>j</mi></msub> <mo>∣</mo><mi>y</mi><mo>)</mo></mrow></math>
    被称为*似然*，表示在给定类别 <math display="inline"><mi>y</mi></math> 时，特征 <math display="inline"><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <mo>…</mo> <mo>,</mo> <msub><mi>x</mi> <mi>j</mi></msub></math>
    的观察值的可能性。
- en: <math display="inline"><mi>P</mi><mrow><mo>(</mo><mi>y</mi><mo>)</mo></mrow></math>
    is called the *prior* and is our belief for the probability of class <math display="inline"><mi>y</mi></math>
    before looking at the data.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math display="inline"><mi>P</mi><mrow><mo>(</mo><mi>y</mi><mo>)</mo></mrow></math>
    被称为*先验概率*，表示在观察数据之前，类别 <math display="inline"><mi>y</mi></math> 的概率信念。
- en: <math display="inline"><mi>P</mi><mo>(</mo><msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>x</mi> <mi>j</mi></msub> <mo>)</mo></math>
    is called the *marginal probability*.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math display="inline"><mi>P</mi><mo>(</mo><msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>x</mi> <mi>j</mi></msub> <mo>)</mo></math>
    被称为*边缘概率*。
- en: In naive Bayes, we compare an observation’s posterior values for each possible
    class. Specifically, because the marginal probability is constant across these
    comparisons, we compare the numerators of the posterior for each class. For each
    observation, the class with the greatest posterior numerator becomes the predicted
    class, <math display="inline"><mover accent="true"><mi>y</mi><mo>^</mo></mover></math>.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在朴素贝叶斯中，我们比较每个可能类别的观测后验概率值。具体来说，因为边际概率在这些比较中是恒定的，我们比较每个类别后验的分子部分。对于每个观测，具有最大后验分子的类别成为预测类别，<math
    display="inline"><mover accent="true"><mi>y</mi><mo>^</mo></mover></math>。
- en: There are two important things to note about naive Bayes classifiers. First,
    for each feature in the data, we have to assume the statistical distribution of
    the likelihood, <math display="inline"><mrow><mi>P</mi><mo>(</mo><msub><mi>x</mi>
    <mi>j</mi></msub> <mo>∣</mo><mi>y</mi><mo>)</mo></mrow></math>. The common distributions
    are the normal (Gaussian), multinomial, and Bernoulli distributions. The distribution
    chosen is often determined by the nature of features (continuous, binary, etc.).
    Second, naive Bayes gets its name because we assume that each feature, and its
    resulting likelihood, is independent. This “naive” assumption is frequently wrong
    yet in practice does little to prevent building high-quality classifiers.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个关于朴素贝叶斯分类器需要注意的重要事项。首先，对于数据中的每个特征，我们必须假设似然的统计分布，<math display="inline"><mrow><mi>P</mi><mo>(</mo><msub><mi>x</mi>
    <mi>j</mi></msub> <mo>∣</mo><mi>y</mi><mo>)</mo></mrow></math>。常见的分布包括正态（高斯）、多项式和伯努利分布。选择的分布通常由特征的性质（连续、二进制等）决定。其次，朴素贝叶斯之所以得名，是因为我们假设每个特征及其结果的似然是独立的。这种“朴素”的假设在实践中往往是错误的，但并不会阻止构建高质量的分类器。
- en: In this chapter we will cover using scikit-learn to train three types of naive
    Bayes classifiers using three different likelihood distributions. Afterwards,
    we will learn to calibrate the predictions from naive Bayes models to make them
    interpretable.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍使用scikit-learn训练三种类型的朴素贝叶斯分类器，使用三种不同的似然分布。此后，我们将学习如何校准朴素贝叶斯模型的预测，使其可解释。
- en: 18.1 Training a Classifier for Continuous Features
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 18.1 训练连续特征的分类器
- en: Problem
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You have only continuous features and you want to train a naive Bayes classifier.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 您只有连续特征，并且希望训练朴素贝叶斯分类器。
- en: Solution
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use a Gaussian naive Bayes classifier in scikit-learn:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中使用高斯朴素贝叶斯分类器：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Discussion
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'The most common type of naive Bayes classifier is the *Gaussian naive Bayes*.
    In Gaussian naive Bayes, we assume that the likelihood of the feature values <math
    display="inline"><mi>x</mi></math>, given an observation is of class <math display="inline"><mi>y</mi></math>,
    follows a normal distribution:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的朴素贝叶斯分类器类型是*高斯朴素贝叶斯*。在高斯朴素贝叶斯中，我们假设给定观测的特征值的似然，<math display="inline"><mi>x</mi></math>，属于类别<math
    display="inline"><mi>y</mi></math>，遵循正态分布：
- en: <math display="block"><mrow><mi>p</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>j</mi></msub>
    <mo>∣</mo> <mi>y</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <msqrt><mrow><mn>2</mn><mi>π</mi><msup><msub><mi>σ</mi>
    <mi>y</mi></msub> <mn>2</mn></msup></mrow></msqrt></mfrac> <msup><mi>e</mi> <mrow><mo>-</mo><mfrac><msup><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>j</mi></msub> <mo>-</mo><msub><mi>μ</mi> <mi>y</mi></msub> <mo>)</mo></mrow>
    <mn>2</mn></msup> <mrow><mn>2</mn><msup><msub><mi>σ</mi> <mi>y</mi></msub> <mn>2</mn></msup></mrow></mfrac></mrow></msup></mrow></math>
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>p</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>j</mi></msub>
    <mo>∣</mo> <mi>y</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <msqrt><mrow><mn>2</mn><mi>π</mi><msup><msub><mi>σ</mi>
    <mi>y</mi></msub> <mn>2</mn></msup></mrow></msqrt></mfrac> <msup><mi>e</mi> <mrow><mo>-</mo><mfrac><msup><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>j</mi></msub> <mo>-</mo><msub><mi>μ</mi> <mi>y</mi></msub> <mo>)</mo></mrow>
    <mn>2</mn></msup> <mrow><mn>2</mn><msup><msub><mi>σ</mi> <mi>y</mi></msub> <mn>2</mn></msup></mrow></mfrac></mrow></msup></mrow></math>
- en: where <math display="inline"><msup><msub><mi>σ</mi> <mi>y</mi></msub> <mn>2</mn></msup></math>
    and <math display="inline"><msub><mi>μ</mi> <mi>y</mi></msub></math> are the variance
    and mean values of feature <math display="inline"><msub><mi>x</mi> <mi>j</mi></msub></math>
    for class <math display="inline"><mi>y</mi></math>. Because of the assumption
    of the normal distribution, Gaussian naive Bayes is best used in cases where all
    our features are continuous.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math display="inline"><msup><msub><mi>σ</mi> <mi>y</mi></msub> <mn>2</mn></msup></math>和<math
    display="inline"><msub><mi>μ</mi> <mi>y</mi></msub></math>分别是特征<math display="inline"><msub><mi>x</mi>
    <mi>j</mi></msub></math>对类别<math display="inline"><mi>y</mi></math>的方差和均值。由于正态分布的假设，高斯朴素贝叶斯最适合于所有特征均为连续的情况。
- en: 'In scikit-learn, we train a Gaussian naive Bayes like any other model using
    `fit`, and in turn can then make predictions about the class of an observation:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，我们像训练其他模型一样训练高斯朴素贝叶斯，使用`fit`，然后可以对观测的类别进行预测：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'One of the interesting aspects of naive Bayes classifiers is that they allow
    us to assign a prior belief over the respected target classes. We can do this
    using the `GaussianNB priors` parameter, which takes in a list of the probabilities
    assigned to each class of the target vector:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器的一个有趣方面之一是，它们允许我们对目标类别分配先验信念。我们可以使用`GaussianNB priors`参数来实现这一点，该参数接受目标向量每个类别的概率列表：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If we do not add any argument to the `priors` parameter, the prior is adjusted
    based on the data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不向`priors`参数添加任何参数，则根据数据调整先验。
- en: Finally, note that the raw predicted probabilities from Gaussian naive Bayes
    (outputted using `predict_proba`) are not calibrated. That is, they should not
    be believed. If we want to create useful predicted probabilities, we will need
    to calibrate them using an isotonic regression or a related method.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意，从高斯朴素贝叶斯获得的原始预测概率（使用`predict_proba`输出）未经校准。也就是说，它们不应被信任。如果我们想要创建有用的预测概率，我们需要使用等渗回归或相关方法进行校准。
- en: See Also
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '[How the Naive Bayes Classifier Works in Machine Learning](https://oreil.ly/9yqSw)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习中朴素贝叶斯分类器的工作原理](https://oreil.ly/9yqSw)'
- en: 18.2 Training a Classifier for Discrete and Count Features
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 18.2 训练离散和计数特征的分类器
- en: Problem
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Given discrete or count data, you need to train a naive Bayes classifier.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 给定离散或计数数据，您需要训练一个朴素贝叶斯分类器。
- en: Solution
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use a multinomial naive Bayes classifier:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多项式朴素贝叶斯分类器：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Discussion
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: '*Multinomial naive Bayes* works similarly to Gaussian naive Bayes, but the
    features are assumed to be multinomially distributed. In practice, this means
    that this classifier is commonly used when we have discrete data (e.g., movie
    ratings ranging from 1 to 5). One of the most common uses of multinomial naive
    Bayes is text classification using bags of words or <math display="inline"><mtext
    fontstyle="italic">tf-idf</mtext></math> approaches (see Recipes [6.9](ch06.xhtml#encoding-text-as-a-bag-of-words)
    and [6.10](ch06.xhtml#weighting-word-importance)).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*多项式朴素贝叶斯*的工作方式与高斯朴素贝叶斯类似，但特征被假定为多项式分布。实际上，这意味着当我们有离散数据时（例如，电影评分从1到5），这种分类器通常被使用。多项式朴素贝叶斯最常见的用途之一是使用词袋或<math
    display="inline"><mtext fontstyle="italic">tf-idf</mtext></math>方法进行文本分类（参见 Recipes
    [6.9](ch06.xhtml#encoding-text-as-a-bag-of-words) 和 [6.10](ch06.xhtml#weighting-word-importance))。'
- en: In our solution, we created a toy text dataset of three observations and converted
    the text strings into a bag-of-words feature matrix and an accompanying target
    vector. We then used `MultinomialNB` to train a model while defining the prior
    probabilities for the two classes (pro-`brazil` and pro-`germany`).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的解决方案中，我们创建了一个包含三个观察结果的玩具文本数据集，并将文本字符串转换为词袋特征矩阵和相应的目标向量。然后，我们使用`MultinomialNB`来训练一个模型，同时为两个类别（支持巴西和支持德国）定义了先验概率。
- en: '`MultinomialNB` works similarly to `GaussianNB`; models are trained using `fit`,
    and observations can be predicted using `predict`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`MultinomialNB`的工作方式类似于`GaussianNB`；模型使用`fit`进行训练，并且可以使用`predict`进行预测：'
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If `class_prior` is not specified, prior probabilities are learned using the
    data. However, if we want a uniform distribution to be used as the prior, we can
    set `fit_prior=False`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果未指定`class_prior`，则使用数据学习先验概率。但是，如果我们想要使用均匀分布作为先验，可以设置`fit_prior=False`。
- en: Finally, `MultinomialNB` contains an additive smoothing hyperparameter, `alpha`,
    that should be tuned. The default value is `1.0`, with `0.0` meaning no smoothing
    takes place.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`MultinomialNB`包含一个添加平滑的超参数`alpha`，应该进行调节。默认值为`1.0`，`0.0`表示不进行平滑。
- en: 18.3 Training a Naive Bayes Classifier for Binary Features
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 18.3 训练二元特征的朴素贝叶斯分类器
- en: Problem
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You have binary feature data and need to train a naive Bayes classifier.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 您有二元特征数据，并需要训练一个朴素贝叶斯分类器。
- en: Solution
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use a Bernoulli naive Bayes classifier:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用伯努利朴素贝叶斯分类器：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Discussion
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'The *Bernoulli naive Bayes* classifier assumes that all our features are binary,
    such that they take only two values (e.g., a nominal categorical feature that
    has been one-hot encoded). Like its multinomial cousin, Bernoulli naive Bayes
    is often used in text classification, when our feature matrix is simply the presence
    or absence of a word in a document. Furthermore, like `MultinomialNB`, `BernoulliNB`
    has an additive smoothing hyperparameter, `alpha`, we will want to tune using
    model selection techniques. Finally, if we want to use priors, we can use the
    `class_prior` parameter with a list containing the prior probabilities for each
    class. If we want to specify a uniform prior, we can set `fit_prior=False`:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*伯努利朴素贝叶斯*分类器假设所有特征都是二元的，即它们只能取两个值（例如，已经进行了独热编码的名义分类特征）。与其多项式兄弟一样，伯努利朴素贝叶斯在文本分类中经常被使用，当我们的特征矩阵仅是文档中单词的存在或不存在时。此外，像`MultinomialNB`一样，`BernoulliNB`也有一个添加平滑的超参数`alpha`，我们可以使用模型选择技术来调节。最后，如果我们想使用先验概率，可以使用`class_prior`参数并将其设置为包含每个类的先验概率的列表。如果我们想指定均匀先验，可以设置`fit_prior=False`：'
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 18.4 Calibrating Predicted Probabilities
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 18.4 校准预测概率
- en: Problem
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to calibrate the predicted probabilities from naive Bayes classifiers
    so they are interpretable.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望校准朴素贝叶斯分类器的预测概率，以便能够解释它们。
- en: Solution
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use `CalibratedClassifierCV`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `CalibratedClassifierCV`：
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Discussion
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Class probabilities are a common and useful part of machine learning models.
    In scikit-learn, most learning algorithms allow us to see the predicted probabilities
    of class membership using `predict_proba`. This can be extremely useful if, for
    instance, we want to predict a certain class only if the model predicts the probability
    that the class is over 90%. However, some models, including naive Bayes classifiers,
    output probabilities that are not based on the real world. That is, `predict_proba`
    might predict an observation has a 0.70 chance of being a certain class, when
    the reality is that it is 0.10 or 0.99. Specifically in naive Bayes, while the
    ranking of predicted probabilities for the different target classes is valid,
    the raw predicted probabilities tend to take on extreme values close to 0 and
    1.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 类概率是机器学习模型中常见且有用的一部分。在scikit-learn中，大多数学习算法允许我们使用 `predict_proba` 来查看类成员的预测概率。例如，如果我们只想在模型预测某个类的概率超过90%时预测该类，这将非常有用。然而，一些模型，包括朴素贝叶斯分类器，输出的概率不是基于现实世界的。也就是说，`predict_proba`
    可能会预测一个观测属于某一类的概率是0.70，而实际上可能是0.10或0.99。具体来说，在朴素贝叶斯中，虽然对不同目标类的预测概率排序是有效的，但原始预测概率往往会取极端值，接近0或1。
- en: To obtain meaningful predicted probabilities we need conduct what is called
    *calibration*. In scikit-learn we can use the `CalibratedClassifierCV` class to
    create well-calibrated predicted probabilities using k-fold cross-validation.
    In `CalibratedClassifierCV`, the training sets are used to train the model, and
    the test set is used to calibrate the predicted probabilities. The returned predicted
    probabilities are the average of the k-folds.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得有意义的预测概率，我们需要进行所谓的*校准*。在scikit-learn中，我们可以使用 `CalibratedClassifierCV` 类通过k折交叉验证创建良好校准的预测概率。在
    `CalibratedClassifierCV` 中，训练集用于训练模型，测试集用于校准预测概率。返回的预测概率是k折交叉验证的平均值。
- en: 'Using our solution we can see the difference between raw and well-calibrated
    predicted probabilities. In our solution, we created a Gaussian naive Bayes classifier.
    If we train that classifier and then predict the class probabilities for a new
    observation, we can see very extreme probability estimates:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的解决方案，我们可以看到原始和良好校准的预测概率之间的差异。在我们的解决方案中，我们创建了一个高斯朴素贝叶斯分类器。如果我们训练该分类器，然后预测新观测的类概率，我们可以看到非常极端的概率估计：
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'However if, after we calibrate the predicted probabilities (which we did in
    our solution), we get very different results:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果在我们校准预测的概率之后（我们在我们的解决方案中完成了这一步），我们得到非常不同的结果：
- en: '[PRE13]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`CalibratedClassifierCV` offers two calibration methods—​Platt’s sigmoid model
    and isotonic regression—​defined by the `method` parameter. While we don’t have
    the space to go into the specifics, because isotonic regression is nonparametric
    it tends to overfit when sample sizes are very small (e.g., 100 observations).
    In our solution we used the Iris dataset with 150 observations and therefore used
    the Platt’s sigmoid model.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`CalibratedClassifierCV` 提供两种校准方法——Platt的sigmoid模型和等温回归——由 `method` 参数定义。虽然我们没有空间详细讨论，但由于等温回归是非参数的，当样本量非常小时（例如100个观测），它往往会过拟合。在我们的解决方案中，我们使用了包含150个观测的鸢尾花数据集，因此使用了Platt的sigmoid模型。'
