- en: Chapter 14\. Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regression is a supervised machine learning process. It is similar to classification,
    but rather than predicting a label, we try to predict a continuous value. If you
    are trying to predict a number, then use regression.
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that sklearn supports many of the same classification models for
    regression problems. In fact, the API is the same, calling `.fit`, `.score`, and
    `.predict`. This is also true for the next-generation boosting libraries, XGBoost
    and LightGBM.
  prefs: []
  type: TYPE_NORMAL
- en: Though there are similarities with the classification models and hyperparameters,
    the evaluation metrics are different for regression. This chapter will review
    many of the types of regression models. We will use the [Boston housing dataset](https://oreil.ly/b2bKQ)
    to explore them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we load the data, create a split version for training and testing, and
    create another split version with standardized data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are descriptions of the features of the housing dataset taken from the
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: CRIM
  prefs: []
  type: TYPE_NORMAL
- en: Per capita crime rate by town
  prefs: []
  type: TYPE_NORMAL
- en: ZN
  prefs: []
  type: TYPE_NORMAL
- en: Proportion of residential land zoned for lots over 25,000 square feet
  prefs: []
  type: TYPE_NORMAL
- en: INDUS
  prefs: []
  type: TYPE_NORMAL
- en: Proportion of nonretail business acres per town
  prefs: []
  type: TYPE_NORMAL
- en: CHAS
  prefs: []
  type: TYPE_NORMAL
- en: Charles River dummy variable (1 if tract bounds river; 0 otherwise)
  prefs: []
  type: TYPE_NORMAL
- en: NOX
  prefs: []
  type: TYPE_NORMAL
- en: Nitric oxides concentration (parts per 10 million)
  prefs: []
  type: TYPE_NORMAL
- en: RM
  prefs: []
  type: TYPE_NORMAL
- en: Average number of rooms per dwelling
  prefs: []
  type: TYPE_NORMAL
- en: AGE
  prefs: []
  type: TYPE_NORMAL
- en: Proportion of owner-occupied units built prior to 1940
  prefs: []
  type: TYPE_NORMAL
- en: DIS
  prefs: []
  type: TYPE_NORMAL
- en: Weighted distances to five Boston employment centers
  prefs: []
  type: TYPE_NORMAL
- en: RAD
  prefs: []
  type: TYPE_NORMAL
- en: Index of accessibility to radial highways
  prefs: []
  type: TYPE_NORMAL
- en: TAX
  prefs: []
  type: TYPE_NORMAL
- en: Full-value property tax rate per $10,000
  prefs: []
  type: TYPE_NORMAL
- en: PTRATIO
  prefs: []
  type: TYPE_NORMAL
- en: Pupil-teacher ratio by town
  prefs: []
  type: TYPE_NORMAL
- en: B
  prefs: []
  type: TYPE_NORMAL
- en: 1000(Bk - 0.63)^2, where Bk is the proportion of black people by town (this
    dataset is from 1978)
  prefs: []
  type: TYPE_NORMAL
- en: LSTAT
  prefs: []
  type: TYPE_NORMAL
- en: Percent lower status of the population
  prefs: []
  type: TYPE_NORMAL
- en: MEDV
  prefs: []
  type: TYPE_NORMAL
- en: Median value of owner-occupied homes in increments of $1000
  prefs: []
  type: TYPE_NORMAL
- en: Baseline Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A baseline regression model will give us something to compare our other models
    to. In sklearn, the default result of the `.score` method is the *coefficient
    of determination* (r² or R²). This number explains the percent of variation of
    the input data that the prediction captures. The value is typically between 0
    and 1, but it can be negative in the case of particulary bad models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The default strategy of the `DummyRegressor` is to predict the mean value of
    the training set. We can see that this model does not perform very well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simple linear regression is taught in math and beginning statistics courses.
    It tries to fit a form of the formula y = mx + b while minimizing the square of
    the errors. When solved, we have an intercept and coefficient. The intercept gives
    a base value for a prediction modified by adding the product of the coefficient
    and the input.
  prefs: []
  type: TYPE_NORMAL
- en: This form can be generalized to higher dimensions. In that case each feature
    has a coefficient. The larger the absolute value of the coefficient, the more
    impact the feature has on the target.
  prefs: []
  type: TYPE_NORMAL
- en: This model assumes that the prediction is a linear combination of the inputs.
    For some datasets, this is not flexible enough. Complexity can be added by transforming
    the features (the sklearn `preprocessing.PolynomialFeatures` transformer can create
    polynomial combinations of the features). If this leads to overfitting, ridge
    and lasso regression may be used to regularize the estimator.
  prefs: []
  type: TYPE_NORMAL
- en: This model is also susceptible to *heteroscedasticity*. This is the idea that
    as the input values change in size, the error of the prediction (or the residuals)
    often changes as well. If you plot the input against the residuals, you will see
    a fan or cone shape. We will see examples of that later.
  prefs: []
  type: TYPE_NORMAL
- en: Another issue to be aware of is *multicollinearity*. If columns have high correlation,
    it can hinder interpretation of the coefficients. This usually does not impact
    the model, only coefficient meaning.
  prefs: []
  type: TYPE_NORMAL
- en: 'A linear regression model has the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Runtime efficiency
  prefs: []
  type: TYPE_NORMAL
- en: Use `n_jobs` to speed up performance.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocess data
  prefs: []
  type: TYPE_NORMAL
- en: Standardize data before training the model.
  prefs: []
  type: TYPE_NORMAL
- en: Prevent overfitting
  prefs: []
  type: TYPE_NORMAL
- en: You can simplify the model by not using or adding polynomial features.
  prefs: []
  type: TYPE_NORMAL
- en: Interpret results
  prefs: []
  type: TYPE_NORMAL
- en: Can interpret results as weights for feature contribution, but assumes normality
    and independence of features. You might want to remove colinear features to improve
    interpretability. R² will tell you how much of the total variance of the outcome
    is explained by the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a sample run with the default data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Instance parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_jobs=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of CPUs to use. `-1` is all.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attributes after fitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '`coef_`'
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression coefficients
  prefs: []
  type: TYPE_NORMAL
- en: '`intercept_`'
  prefs: []
  type: TYPE_NORMAL
- en: Intercept of the linear model
  prefs: []
  type: TYPE_NORMAL
- en: 'The `.intercept_` value is the expected mean value. You can see how scaling
    the data affects the coefficients. The sign of the coefficients explains the direction
    of the relation between the feature and the target. A positive sign indicates
    that as the feature increases, the label increases. A negative sign indicates
    that as the feature increases, the label decreases. The larger the absolute value
    of the coefficient, the more impact it has:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You can use Yellowbrick to visualize coefficients (see [Figure 14-1](#id45)).
    Because the scaled Boston data is a numpy array rather than a pandas DataFrame,
    we need to pass the `labels` parameter if we want to use the column names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![Feature importance. This indicates that RM (number of rooms) increases the
    price, age doesn''t really matter, and LSTAT (percent of low status in population)
    lowers the price.](assets/mlpr_1401.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-1\. Feature importance. This indicates that RM (number of rooms) increases
    the price, age doesn’t really matter, and LSTAT (percent of low status in population)
    lowers the price.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: SVMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Support vector machines can perform regression as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'SVMs have the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Runtime efficiency
  prefs: []
  type: TYPE_NORMAL
- en: The scikit-learn implementation is O(n⁴), so it can be hard to scale to large
    sizes. Using a linear kernel or the `LinearSVR` model can improve the runtime
    performance at perhaps the cost of accuracy. Upping the `cache_size` parameter
    can bring that down to O(n³).
  prefs: []
  type: TYPE_NORMAL
- en: Preprocess data
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm is not scale invariant, so standardizing the data is highly recommended.
  prefs: []
  type: TYPE_NORMAL
- en: Prevent overfitting
  prefs: []
  type: TYPE_NORMAL
- en: The `C` (penalty parameter) controls regularization. A smaller value allows
    for a smaller margin in the hyperplane. A higher value for `gamma` will tend to
    overfit the training data. The `LinearSVR` model supports a `loss` and `penalty`
    parameter for regularization. The `epsilon` parameter can be raised (with 0 you
    should expect overfitting).
  prefs: []
  type: TYPE_NORMAL
- en: Interpret results
  prefs: []
  type: TYPE_NORMAL
- en: Inspect `.support_vectors_`, though these are hard to interpret. With linear
    kernels, you can inspect `.coef_`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of using the library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Instance parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`C=1.0`'
  prefs: []
  type: TYPE_NORMAL
- en: The penalty parameter. The smaller the value, the tighter the decision boundary
    (more overfitting).
  prefs: []
  type: TYPE_NORMAL
- en: '`cache_size=200`'
  prefs: []
  type: TYPE_NORMAL
- en: Cache size (MB). Bumping this up can improve training time on large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '`coef0=0.0`'
  prefs: []
  type: TYPE_NORMAL
- en: Independent term for poly and sigmoid kernels.
  prefs: []
  type: TYPE_NORMAL
- en: '`epsilon=0.1`'
  prefs: []
  type: TYPE_NORMAL
- en: Defines a margin of tolerance where no penalty is given to errors. Should be
    smaller for larger datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '`degree=3`'
  prefs: []
  type: TYPE_NORMAL
- en: Degree for polynomial kernel.
  prefs: []
  type: TYPE_NORMAL
- en: '`gamma=''auto''`'
  prefs: []
  type: TYPE_NORMAL
- en: Kernel coefficient. Can be a number, `'scale'` (default in 0.22, 1 / (`num features`
    * `X.std()`) ), or `'auto'` (default prior, 1 / `num_features`). A lower value
    leads to overfitting the training data.
  prefs: []
  type: TYPE_NORMAL
- en: '`kernel=''rbf''`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kernel type: `''linear''`, `''poly''`, `''rbf''` (default), `''sigmoid''`,
    `''precomputed''`, or a function.'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_iter=-1`'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum number of iterations for solver. -1 for no limit.
  prefs: []
  type: TYPE_NORMAL
- en: '`probability=False`'
  prefs: []
  type: TYPE_NORMAL
- en: Enable probability estimation. Slows down training.
  prefs: []
  type: TYPE_NORMAL
- en: '`random_state=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Random seed.
  prefs: []
  type: TYPE_NORMAL
- en: '`shrinking=True`'
  prefs: []
  type: TYPE_NORMAL
- en: Use shrinking heuristic.
  prefs: []
  type: TYPE_NORMAL
- en: '`tol=0.001`'
  prefs: []
  type: TYPE_NORMAL
- en: Stopping tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: '`verbose=False`'
  prefs: []
  type: TYPE_NORMAL
- en: Verbosity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attributes after fitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '`support_`'
  prefs: []
  type: TYPE_NORMAL
- en: Support vector indices
  prefs: []
  type: TYPE_NORMAL
- en: '`support_vectors_`'
  prefs: []
  type: TYPE_NORMAL
- en: Support vectors
  prefs: []
  type: TYPE_NORMAL
- en: '`coef_`'
  prefs: []
  type: TYPE_NORMAL
- en: Coefficients (for linear) kernel
  prefs: []
  type: TYPE_NORMAL
- en: '`intercept_`'
  prefs: []
  type: TYPE_NORMAL
- en: Constant for decision function
  prefs: []
  type: TYPE_NORMAL
- en: K-Nearest Neighbor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The KNN model also supports regression by finding k neighbor targets to the
    sample for which you want to predict. For regression, this model averages the
    targets together to determine a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nearest neighbor models have the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Runtime efficiency
  prefs: []
  type: TYPE_NORMAL
- en: Training runtime is O(1), but there is a trade-off as the sample data needs
    to be stored. Testing runtime is O(Nd), where N is the number of training examples
    and d is dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocess data
  prefs: []
  type: TYPE_NORMAL
- en: Yes, distance-based calculations perform better when standardized.
  prefs: []
  type: TYPE_NORMAL
- en: Prevent overfitting
  prefs: []
  type: TYPE_NORMAL
- en: Raise `n_neighbors`. Change `p` for L1 or L2 metric.
  prefs: []
  type: TYPE_NORMAL
- en: Interpret results
  prefs: []
  type: TYPE_NORMAL
- en: Interpret the k-nearest neighbors to the sample (using the `.kneighbors` method).
    Those neighbors (if you can explain them) explain your result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of using the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`algorithm=''auto''`'
  prefs: []
  type: TYPE_NORMAL
- en: Can be `'brute'`, `'ball_tree'`, or `'kd_tree'`.
  prefs: []
  type: TYPE_NORMAL
- en: '`leaf_size=30`'
  prefs: []
  type: TYPE_NORMAL
- en: Used for tree algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '`metric=''minkowski''`'
  prefs: []
  type: TYPE_NORMAL
- en: Distance metric.
  prefs: []
  type: TYPE_NORMAL
- en: '`metric_params=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Additional dictionary of parameters for custom metric function.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_jobs=1`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_neighbors=5`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: '`p=2`'
  prefs: []
  type: TYPE_NORMAL
- en: Minkowski power parameter. 1 = manhattan (L1). 2 = euclidean (L2).
  prefs: []
  type: TYPE_NORMAL
- en: '`weights=''uniform''`'
  prefs: []
  type: TYPE_NORMAL
- en: Can be `'distance'`, in which case, closer points have more influence.
  prefs: []
  type: TYPE_NORMAL
- en: Decision Tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees support classification and regression. At each level of the tree,
    various splits on features are evaluated. The split that will produce the lowest
    error (impurity) is chosen. The `criterion` parameter can be adjusted to determine
    the metric for impurity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Decision trees have the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Runtime efficiency
  prefs: []
  type: TYPE_NORMAL
- en: 'For creation, loop over each of the m features we have to sort all n samples:
    O(mn log n). For predicting, you walk the tree: O(height).'
  prefs: []
  type: TYPE_NORMAL
- en: Preprocess data
  prefs: []
  type: TYPE_NORMAL
- en: Scaling not necessary. Need to get rid of missing values and convert to numeric.
  prefs: []
  type: TYPE_NORMAL
- en: Prevent overfitting
  prefs: []
  type: TYPE_NORMAL
- en: Set `max_depth` to a lower number, raise `min_impurity_decrease`.
  prefs: []
  type: TYPE_NORMAL
- en: Interpret results
  prefs: []
  type: TYPE_NORMAL
- en: Can step through the tree of choices. Because there are steps, a tree is bad
    at dealing with linear relationships (a small change in the values of a feature
    can cause a completely different tree to be formed). The tree is also highly dependent
    on the training data. A small change can change the whole tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example using the scikit-learn library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Instance parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`criterion=''mse''`'
  prefs: []
  type: TYPE_NORMAL
- en: Splitting function. Default is mean squared error (L2 loss). `'friedman_mse'`
    or `'mae'` (L1 loss).
  prefs: []
  type: TYPE_NORMAL
- en: '`max_depth=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Depth of tree. Default will build until leaves contain less than `min_samples_split`.
  prefs: []
  type: TYPE_NORMAL
- en: '`max_features=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of features to examine for split. Default is all.
  prefs: []
  type: TYPE_NORMAL
- en: '`max_leaf_nodes=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Limit number of leaves. Default is unlimited.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_impurity_decrease=0.0`'
  prefs: []
  type: TYPE_NORMAL
- en: Split node if a split will decrease impurity >= value.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_impurity_split=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Deprecated.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_samples_leaf=1`'
  prefs: []
  type: TYPE_NORMAL
- en: Minimum number of samples at each leaf.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_samples_split=2`'
  prefs: []
  type: TYPE_NORMAL
- en: Minimum number of samples required to split a node.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_weight_fraction_leaf=0.0`'
  prefs: []
  type: TYPE_NORMAL
- en: Minimum sum of weights required for leaf nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '`presort=False`'
  prefs: []
  type: TYPE_NORMAL
- en: May speed up training with small dataset or restricted depth if set to `True`.
  prefs: []
  type: TYPE_NORMAL
- en: '`random_state=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Random seed.
  prefs: []
  type: TYPE_NORMAL
- en: '`splitter=''best''`'
  prefs: []
  type: TYPE_NORMAL
- en: Use `'random'` or `'best'`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attributes after fitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '`feature_importances_`'
  prefs: []
  type: TYPE_NORMAL
- en: Array of Gini importance
  prefs: []
  type: TYPE_NORMAL
- en: '`max_features_`'
  prefs: []
  type: TYPE_NORMAL
- en: Computed value of `max_features`
  prefs: []
  type: TYPE_NORMAL
- en: '`n_outputs_`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of outputs
  prefs: []
  type: TYPE_NORMAL
- en: '`n_features_`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of features
  prefs: []
  type: TYPE_NORMAL
- en: '`tree_`'
  prefs: []
  type: TYPE_NORMAL
- en: Underlying tree object
  prefs: []
  type: TYPE_NORMAL
- en: 'View the tree (see [Figure 14-2](#tree2)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'For Jupyter, use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![Decision tree.](assets/mlpr_1402.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-2\. Decision tree.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This plot was a little wide. On a computer you can zoom in on portions of it.
    You can also limit the depth of the chart (see [Figure 14-3](#tree3)). (It turns
    out that the most important features are typically near the top of the tree.)
    We will use the `max_depth` parameter to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![The first two layers of a decision tree.](assets/mlpr_1403.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-3\. The first two layers of a decision tree.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can also use the dtreeviz package to view a scatter plot at each of the
    nodes of the tree (see [Figure 14-4](#treedtviz2)). We will use a tree limited
    to a depth of two so we can see the details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![Regression with dtviz.](assets/mlpr_1404.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-4\. Regression with dtviz.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Feature importance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Random Forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees are good because they are explainable, but they have a tendency
    to overfit. A random forest trades some of the explainability for a model that
    tends to generalize better. This model can also be used for regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Random forests have the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Runtime efficiency
  prefs: []
  type: TYPE_NORMAL
- en: 'Need to create j random trees. This can be done in parallel using `n_jobs`.
    Complexity for each tree is O(mn log n), where n is the number of samples and
    m is the number of features. For creation, loop over each of the m features, and
    sort all n samples: O(mn log n). For predicting, you walk the tree: O(height).'
  prefs: []
  type: TYPE_NORMAL
- en: Preprocess data
  prefs: []
  type: TYPE_NORMAL
- en: Not necessary as long as the input is numeric and not missing values.
  prefs: []
  type: TYPE_NORMAL
- en: Prevent overfitting
  prefs: []
  type: TYPE_NORMAL
- en: Add more trees (`n_estimators`). Use lower `max_depth`.
  prefs: []
  type: TYPE_NORMAL
- en: Interpret results
  prefs: []
  type: TYPE_NORMAL
- en: Supports feature importance, but we don’t have a single decision tree that we
    can walk through. Can inspect single trees from the ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of using the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Instance parameters (these options mirror the decision tree):'
  prefs: []
  type: TYPE_NORMAL
- en: '`bootstrap=True`'
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrap when building trees.
  prefs: []
  type: TYPE_NORMAL
- en: '`criterion=''mse''`'
  prefs: []
  type: TYPE_NORMAL
- en: Splitting function, `'mae'`.
  prefs: []
  type: TYPE_NORMAL
- en: '`max_depth=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Depth of tree. Default will build until leaves contain less than `min_samples_split`.
  prefs: []
  type: TYPE_NORMAL
- en: '`max_features=''auto''`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of features to examine for split. Default is all.
  prefs: []
  type: TYPE_NORMAL
- en: '`max_leaf_nodes=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Limit number of leaves. Default is unlimited.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_impurity_decrease=0.0`'
  prefs: []
  type: TYPE_NORMAL
- en: Split node if a split will decrease impurity by this value or more.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_impurity_split=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Deprecated.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_samples_leaf=1`'
  prefs: []
  type: TYPE_NORMAL
- en: Minimum number of samples at each leaf.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_samples_split=2`'
  prefs: []
  type: TYPE_NORMAL
- en: Minimum number of samples required to split a node.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_weight_fraction_leaf=0.0`'
  prefs: []
  type: TYPE_NORMAL
- en: Minimum sum total of weights required for leaf nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators=10`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of trees in the forest.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_jobs=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of jobs for fitting and predicting. (`None` means 1.)
  prefs: []
  type: TYPE_NORMAL
- en: '`oob_score=False`'
  prefs: []
  type: TYPE_NORMAL
- en: Whether to use OOB samples to estimate score on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: '`random_state=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Random seed.
  prefs: []
  type: TYPE_NORMAL
- en: '`verbose=0`'
  prefs: []
  type: TYPE_NORMAL
- en: Verbosity.
  prefs: []
  type: TYPE_NORMAL
- en: '`warm_start=False`'
  prefs: []
  type: TYPE_NORMAL
- en: Fit a new forest or use existing one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attributes after fitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '`estimators_`'
  prefs: []
  type: TYPE_NORMAL
- en: Collection of trees
  prefs: []
  type: TYPE_NORMAL
- en: '`feature_importances_`'
  prefs: []
  type: TYPE_NORMAL
- en: Array of Gini importance
  prefs: []
  type: TYPE_NORMAL
- en: '`n_classes_`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of classes
  prefs: []
  type: TYPE_NORMAL
- en: '`n_features_`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of features
  prefs: []
  type: TYPE_NORMAL
- en: '`oob_score_`'
  prefs: []
  type: TYPE_NORMAL
- en: Score of the training dataset using OOB estimate
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature importance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: XGBoost Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The XGBoost library also supports regression. It builds a simple decision tree,
    then “boosts” it by adding subsequent trees. Each tree tries to correct the residuals
    of the previous output. In practice, this works quite well on structured data.
  prefs: []
  type: TYPE_NORMAL
- en: 'It has the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Runtime efficiency
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost is parallelizeable. Use the `n_jobs` option to indicate the number of
    CPUs. Use GPU for even better performance.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocess data
  prefs: []
  type: TYPE_NORMAL
- en: No scaling necessary with tree models. Need to encode categorical data. Supports
    missing data!
  prefs: []
  type: TYPE_NORMAL
- en: Prevent overfitting
  prefs: []
  type: TYPE_NORMAL
- en: The `early_stopping_rounds=N` parameter can be set to stop training if there
    is no improvement after N rounds. L1 and L2 regularization are controlled by `reg_alpha`
    and `reg_lambda`, respectively. Higher numbers mean more conservative.
  prefs: []
  type: TYPE_NORMAL
- en: Interpret results
  prefs: []
  type: TYPE_NORMAL
- en: Has feature importance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example using the library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Instance parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_depth=3`'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum depth.
  prefs: []
  type: TYPE_NORMAL
- en: '`learning_rate=0.1`'
  prefs: []
  type: TYPE_NORMAL
- en: Learning rate (eta) for boosting (between 0 and 1). After each boost step, the
    newly added weights are scaled by this factor. The lower the value, the more conservative,
    but will also need more trees to converge. In the call to `.train`, you can pass
    a `learning_rates` parameter, which is a list of rates at each round (i.e., `[.1]*100
    + [.05]*100`).
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators=100`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of rounds or boosted trees.
  prefs: []
  type: TYPE_NORMAL
- en: '`silent=True`'
  prefs: []
  type: TYPE_NORMAL
- en: Whether to print messages while running boosting.
  prefs: []
  type: TYPE_NORMAL
- en: '`objective="reg:linear"`'
  prefs: []
  type: TYPE_NORMAL
- en: Learning task or callable for classification.
  prefs: []
  type: TYPE_NORMAL
- en: '`booster="gbtree"`'
  prefs: []
  type: TYPE_NORMAL
- en: Can be `'gbtree'`, `'gblinear'`, or `'dart'`. The `'dart'` option adds dropout
    (drops random trees to prevent overfitting). The `'gblinear'` option creates a
    regularized linear model (read not a tree but similar to lasso regression).
  prefs: []
  type: TYPE_NORMAL
- en: '`nthread=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Deprecated.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_jobs=1`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of threads to use.
  prefs: []
  type: TYPE_NORMAL
- en: '`gamma=0`'
  prefs: []
  type: TYPE_NORMAL
- en: Minimum loss reduction needed to further split a leaf.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_child_weight=1`'
  prefs: []
  type: TYPE_NORMAL
- en: Minimum value for sum of hessian for a child.
  prefs: []
  type: TYPE_NORMAL
- en: '`max_delta_step=0`'
  prefs: []
  type: TYPE_NORMAL
- en: Make update more conservative. Set 1 to 10 for imbalanced classes.
  prefs: []
  type: TYPE_NORMAL
- en: '`subsample=1`'
  prefs: []
  type: TYPE_NORMAL
- en: Fraction of samples to use for next boosting round.
  prefs: []
  type: TYPE_NORMAL
- en: '`colsample_bytree=1`'
  prefs: []
  type: TYPE_NORMAL
- en: Fraction of columns to use for boosting round.
  prefs: []
  type: TYPE_NORMAL
- en: '`colsample_bylevel=1`'
  prefs: []
  type: TYPE_NORMAL
- en: Fraction of columns to use for level in tree.
  prefs: []
  type: TYPE_NORMAL
- en: '`colsample_bynode=1`'
  prefs: []
  type: TYPE_NORMAL
- en: Fraction of columns to use for split (node in tree).
  prefs: []
  type: TYPE_NORMAL
- en: '`reg_alpha=0`'
  prefs: []
  type: TYPE_NORMAL
- en: L1 regularization (mean of weights). Increase to be more conservative.
  prefs: []
  type: TYPE_NORMAL
- en: '`reg_lambda=1`'
  prefs: []
  type: TYPE_NORMAL
- en: L2 regularization (root of squared weights). Increase to be more conservative.
  prefs: []
  type: TYPE_NORMAL
- en: '`base_score=.5`'
  prefs: []
  type: TYPE_NORMAL
- en: Initial prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '`seed=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Deprecated.
  prefs: []
  type: TYPE_NORMAL
- en: '`random_state=0`'
  prefs: []
  type: TYPE_NORMAL
- en: Random seed.
  prefs: []
  type: TYPE_NORMAL
- en: '`missing=None`'
  prefs: []
  type: TYPE_NORMAL
- en: Value to interpret for missing. `None` means `np.nan`.
  prefs: []
  type: TYPE_NORMAL
- en: '`importance_type=''gain''`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The feature importance type: `''gain''`, `''weight''`, `''cover''`, `''total_gain''`,
    or `''total_cover''`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`coef_`'
  prefs: []
  type: TYPE_NORMAL
- en: Coefficients for gblinear learners (`booster = 'gblinear'`)
  prefs: []
  type: TYPE_NORMAL
- en: '`intercept_`'
  prefs: []
  type: TYPE_NORMAL
- en: Intercept for gblinear learners
  prefs: []
  type: TYPE_NORMAL
- en: '`feature_importances_`'
  prefs: []
  type: TYPE_NORMAL
- en: Feature importances for gbtree learners
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature importance is the average gain across all the nodes where the feature
    is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'XGBoost includes plotting facilities for feature importance. Note that the
    `importance_type` parameter changes the values in this plot (see [Figure 14-5](#idfir2)).
    The default is using weight to determine feature importance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![Feature importance using weight (how many times a feature is split on in
    the trees).](assets/mlpr_1405.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-5\. Feature importance using weight (how many times a feature is split
    on in the trees).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Using Yellowbrick to plot feature importances (it will normalize the `feature_importances_`
    attribute) (see [Figure 14-6](#idfir1)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![Feature importance using relative importance of gain (percent importance
    of the most important feature).](assets/mlpr_1406.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-6\. Feature importance using relative importance of gain (percent
    importance of the most important feature).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'XGBoost provides both a textual representation of the trees and a graphical
    one. Here is the text representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The leaf values can be interpreted as the sum of the `base_score` and the leaf.
    (To validate this, call `.predict` with the `ntree_limit=1` parameter to limit
    the model to using the result of the first tree.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a graphical version of the tree (see [Figure 14-7](#idr27)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![XGBoost tree.](assets/mlpr_1407.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-7\. XGBoost tree.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: LightGBM Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The gradient boosting tree library, LightGBM, also supports regression. As mentioned
    in the classification chapter, it can be faster than XGBoost for creating trees
    due to the sampling mechanism used to determine node splits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, remember that it grows trees depth first, so limiting depth may harm
    the model. It has the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Runtime efficiency
  prefs: []
  type: TYPE_NORMAL
- en: Can take advantage of multiple CPUs. By using binning, can be 15 times faster
    than XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocess data
  prefs: []
  type: TYPE_NORMAL
- en: Has some support for encoding categorical columns as integers (or pandas `Categorical`
    type), but AUC appears to suffer compared to one-hot encoding.
  prefs: []
  type: TYPE_NORMAL
- en: Prevent overfitting
  prefs: []
  type: TYPE_NORMAL
- en: Lower `num_leaves`. Increase `min_data_in_leaf`. Use `min_gain_to_split` with
    `lambda_l1` or `lambda_l2`.
  prefs: []
  type: TYPE_NORMAL
- en: Interpret results
  prefs: []
  type: TYPE_NORMAL
- en: Feature importance is available. Individual trees are weak and tend to be hard
    to interpret.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of using the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Instance parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`boosting_type=''gbdt''`'
  prefs: []
  type: TYPE_NORMAL
- en: Can be `'gbdt'` (gradient boosting), `'rf'` (random forest), `'dart'` (dropouts
    meet multiple additive regression trees), or `'goss'` (gradient-based, one-sided
    sampling).
  prefs: []
  type: TYPE_NORMAL
- en: '`num_leaves=31`'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum tree leaves.
  prefs: []
  type: TYPE_NORMAL
- en: '`max_depth=-1`'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum tree depth. -1 is unlimited. Larger depths tend to overfit more.
  prefs: []
  type: TYPE_NORMAL
- en: '`learning_rate=0.1`'
  prefs: []
  type: TYPE_NORMAL
- en: Range (0, 1.0]. Learning rate for boosting. A smaller value slows down overfitting
    as the boosting rounds have less impact. A smaller number should give better performance
    but will require more `num_iterations`.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators=100`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of trees or boosting rounds.
  prefs: []
  type: TYPE_NORMAL
- en: '`subsample_for_bin=200000`'
  prefs: []
  type: TYPE_NORMAL
- en: Samples required to create bins.
  prefs: []
  type: TYPE_NORMAL
- en: '`objective=None`'
  prefs: []
  type: TYPE_NORMAL
- en: '`None` - Does regression by default. Can be a function or string.'
  prefs: []
  type: TYPE_NORMAL
- en: '`min_split_gain=0.0`'
  prefs: []
  type: TYPE_NORMAL
- en: Loss reduction required to partition leaf.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_child_weight=0.001`'
  prefs: []
  type: TYPE_NORMAL
- en: Sum of hessian weight required for a leaf. Larger will be more conservative.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_child_samples=20`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of samples required for a leaf. Lower numbers mean more overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '`subsample=1.0`'
  prefs: []
  type: TYPE_NORMAL
- en: Fraction of samples to use for the next round.
  prefs: []
  type: TYPE_NORMAL
- en: '`subsample_freq=0`'
  prefs: []
  type: TYPE_NORMAL
- en: Subsample frequency. Change to 1 to enable.
  prefs: []
  type: TYPE_NORMAL
- en: '`colsample_bytree=1.0`'
  prefs: []
  type: TYPE_NORMAL
- en: Range (0, 1.0]. Select percent of features for each boosting round.
  prefs: []
  type: TYPE_NORMAL
- en: '`reg_alpha=0.0`'
  prefs: []
  type: TYPE_NORMAL
- en: L1 regularization (mean of weights). Increase to be more conservative.
  prefs: []
  type: TYPE_NORMAL
- en: '`reg_lambda=0.0`'
  prefs: []
  type: TYPE_NORMAL
- en: L2 regularization (root of squared weights). Increase to be more conservative.
  prefs: []
  type: TYPE_NORMAL
- en: '`random_state=42`'
  prefs: []
  type: TYPE_NORMAL
- en: Random seed.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_jobs=-1`'
  prefs: []
  type: TYPE_NORMAL
- en: Number of threads.
  prefs: []
  type: TYPE_NORMAL
- en: '`silent=True`'
  prefs: []
  type: TYPE_NORMAL
- en: Verbose mode.
  prefs: []
  type: TYPE_NORMAL
- en: '`importance_type=''split''`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Determines how importance is calculated: `*split*` (times a feature was used)
    or `*gain*` (total gains of splits when a feature was used).'
  prefs: []
  type: TYPE_NORMAL
- en: 'LightGBM supports feature importance. The `importance_type` parameter determines
    how this is calculated (the default is based on how many times a feature was used):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Feature importance plot showing how many times a feature is used (see [Figure 14-8](#id23_2)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![Feature importance showing how many times a feature is used.](assets/mlpr_1408.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-8\. Feature importance showing how many times a feature is used.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In Jupyter, use the following command to view a tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
