- en: Chapter 22\. Neural Networks for Unstructured Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 22.0 Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we focused on neural network recipes for *structured*
    data, i.e., tabular data. Most of the largest advances in the past few years have
    actually involved using neural networks and deep learning for *unstructured* data,
    such as text or images. Working with these unstructured datasets is a bit different
    than working with structured sources of data.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning is particularly powerful in the unstructured data space, where
    “classic” machine learning techniques (such as boosted trees) typically fail to
    capture all the complexity and nuance present in text data, audio, images, videos,
    etc. In this chapter, we will explore using deep learning specifically for text
    and image data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a supervised learning space for text and images, there are many subtasks
    or “types” of learning. The following are a few examples (though this is not a
    comprehensive list):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Text or image classification (example: classifying whether or not an image
    is a picture of a hotdog)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Transfer learning (example: using a pretrained contextual model like BERT and
    fine-tuning it on a task to predict whether or not an email is spam)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Object detection (example: identifying and classifying specific objects within
    an image)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Generative models (example: models that generate text based on a given input
    such as the GPT models)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As deep learning has grown in popularity and become increasingly commoditized,
    both the open source and enterprise solutions for dealing with these use cases
    have become more easily accessible. In this chapter, we’ll leverage a few key
    libraries as our entry point into performing these deep learning tasks. In particular,
    we’ll use PyTorch, Torchvision, and the Transformers Python libraries to accomplish
    a set of tasks across both text and image ML data.
  prefs: []
  type: TYPE_NORMAL
- en: 22.1 Training a Neural Network for Image Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to train an image classification neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use a convolutional neural network in PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Convolutional neural networks are typically used for tasks in image recognition
    and computer vision. They typically consist of convolutional layers, pooling layers,
    and a fully connected layer.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of the *convolutional layers* is to learn important image features
    that can be used for the task at hand. Convolutional layers work by applying a
    filter to a particular area of an image (the size of the convolution). The weights
    of this layer then learn to recognize specific image features critical in the
    classification task. For instance, if we’re training a model that recognizes a
    person’s hand, the filter may learn to recognize fingers.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of the *pooling layer* is typically to reduce the dimensionality
    of the inputs from the previous layer. This layer also uses a filter applied to
    a portion of the input, but it has no activation. Instead, it reduces dimensionality
    of the input by performing *max pooling* (where it selects the pixel in the filter
    with the highest value) or *average pooling* (where it takes an average of the
    input pixels to use instead).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the *fully connected layer* can be used with something like a softmax
    activation function to create a binary classification task.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Convolutional Neural Networks](https://oreil.ly/HoO9g)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 22.2 Training a Neural Network for Text Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to train a neural network to classify text data.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use a PyTorch neural network whose first layer is the size of your vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unlike images, text data is inherently nonnumeric. Before training a model,
    we need to convert the text into a numeric representation that the model can use
    to learn which words and word combinations are important for the classification
    task at hand. In this example, we use scikit-learn’s `CountVectorizer` to encode
    the vocabulary as a vector the size of the entire vocabulary, where each word
    is assigned to a specific index in the vector, and the value at that location
    is the number of times that word appears in a given paragraph. In this case, we
    can see the vocabulary size by looking at our training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We use this same value in the first layer of our neural network to determine
    the size of the input layer: `self.fc1 = nn.Linear(X_train.shape[1], 128)`. This
    allows our network to learn what are called *word embeddings*, vector representations
    of individual words learned from a supervised learning task like the one in this
    recipe. This task will allow us to learn word embeddings of size 128, though these
    embeddings will primarily be useful for this specific task and vocabulary.'
  prefs: []
  type: TYPE_NORMAL
- en: 22.3 Fine-Tuning a Pretrained Model for Image Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to train an image classification model using learnings from a pretrained
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the `transformers` library with `torchvision` to fine-tune a pretrained
    model on your data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the realm of unstructured data like text and images, it is extremely common
    to start from pretrained models trained on large datasets, instead of starting
    from scratch, especially in cases where we don’t have access to as much labeled
    data. Using embeddings and other information from the larger model, we can then
    fine-tune our own model for a new task without the need for as much labeled information.
    In addition, the pretrained model may have information not captured at all in
    our training dataset, resulting in an overall performance improvement. This process
    is known as *transfer learning*.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we load the weights from Google’s ViT (Vision Transformer)
    model. Then, we use the `transformers` library to fine-tune it for a classification
    task on the fashion MNIST dataset, a simple dataset of clothing items. This approach
    can be applied to increase performance on any computer vision dataset, and the
    `transformers` library provides a high-level interface we can use to fine-tune
    our own model from larger, pretrained ones without writing an egregious amount
    of code.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Hugging Face website and documentation](https://oreil.ly/5F3Rf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 22.4 Fine-Tuning a Pretrained Model for Text Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to train a text classification model using learnings from a pretrained
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the `transformers` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just like using pretrained image models, pretrained language models hold a massive
    amount of context about language, as they’re typically trained on a wide variety
    of open internet sources. When we start from a pretrained model base, what we’re
    typically doing is swapping out the classification layer of the existing network
    for one of our own. This allows us to alter the network weights already learned
    to fit our specific task.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we’re fine-tuning a DistilBERT model to recognize whether IMDB
    movie reviews were positive (1) or negative (0). The pretrained DistilBERT model
    provides a large corpus of words and context on each one, in addition to neural
    network weights learned from a previous training task. Transfer learning allows
    us to take advantage of all the initial work done training the DistilBERT model
    and repurpose it for our use case, which in this instance is classifying movie
    reviews.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Text classification in transformers](https://oreil.ly/uhrjI)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
