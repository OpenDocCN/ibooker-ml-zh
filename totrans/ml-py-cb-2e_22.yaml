- en: Chapter 22\. Neural Networks for Unstructured Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第22章 神经网络用于非结构化数据
- en: 22.0 Introduction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 22.0 介绍
- en: In the previous chapter, we focused on neural network recipes for *structured*
    data, i.e., tabular data. Most of the largest advances in the past few years have
    actually involved using neural networks and deep learning for *unstructured* data,
    such as text or images. Working with these unstructured datasets is a bit different
    than working with structured sources of data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们专注于适用于*结构化*数据的神经网络配方，即表格数据。实际上，过去几年中最大的进展大部分涉及使用神经网络和深度学习处理*非结构化*数据，例如文本或图像。与处理结构化数据源不同，处理这些非结构化数据集有所不同。
- en: Deep learning is particularly powerful in the unstructured data space, where
    “classic” machine learning techniques (such as boosted trees) typically fail to
    capture all the complexity and nuance present in text data, audio, images, videos,
    etc. In this chapter, we will explore using deep learning specifically for text
    and image data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在非结构化数据领域尤为强大，而“经典”的机器学习技术（如提升树）通常无法捕捉文本数据、音频、图像、视频等中存在的所有复杂性和细微差别。在本章中，我们将专门探讨将深度学习用于文本和图像数据。
- en: 'In a supervised learning space for text and images, there are many subtasks
    or “types” of learning. The following are a few examples (though this is not a
    comprehensive list):'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本和图像的监督学习空间中，存在许多子任务或“类型”学习。以下是一些示例（尽管这不是一个全面的列表）：
- en: 'Text or image classification (example: classifying whether or not an image
    is a picture of a hotdog)'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本或图像分类（例如：分类一张照片是否是热狗的图像）
- en: 'Transfer learning (example: using a pretrained contextual model like BERT and
    fine-tuning it on a task to predict whether or not an email is spam)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移学习（例如：使用预训练的上下文模型如BERT，并在一个任务上进行微调以预测电子邮件是否为垃圾邮件）
- en: 'Object detection (example: identifying and classifying specific objects within
    an image)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标检测（例如：识别和分类图像中的特定对象）
- en: 'Generative models (example: models that generate text based on a given input
    such as the GPT models)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成模型（例如：基于给定输入生成文本的模型，如GPT模型）
- en: As deep learning has grown in popularity and become increasingly commoditized,
    both the open source and enterprise solutions for dealing with these use cases
    have become more easily accessible. In this chapter, we’ll leverage a few key
    libraries as our entry point into performing these deep learning tasks. In particular,
    we’ll use PyTorch, Torchvision, and the Transformers Python libraries to accomplish
    a set of tasks across both text and image ML data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习的普及和越来越普遍，处理这些用例的开源和企业解决方案变得更加易于访问。在本章中，我们将利用几个关键库作为我们进入执行这些深度学习任务的起点。特别是，我们将使用
    PyTorch、Torchvision 和 Transformers Python 库来完成跨文本和图像ML数据的一系列任务。
- en: 22.1 Training a Neural Network for Image Classification
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 22.1 训练神经网络进行图像分类
- en: Problem
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You need to train an image classification neural network.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要训练一个图像分类神经网络。
- en: Solution
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use a convolutional neural network in PyTorch:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中使用卷积神经网络：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Discussion
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Convolutional neural networks are typically used for tasks in image recognition
    and computer vision. They typically consist of convolutional layers, pooling layers,
    and a fully connected layer.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络通常用于图像识别和计算机视觉任务。它们通常包括卷积层、池化层和全连接层。
- en: The purpose of the *convolutional layers* is to learn important image features
    that can be used for the task at hand. Convolutional layers work by applying a
    filter to a particular area of an image (the size of the convolution). The weights
    of this layer then learn to recognize specific image features critical in the
    classification task. For instance, if we’re training a model that recognizes a
    person’s hand, the filter may learn to recognize fingers.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*卷积层*的目的是学习可以用于当前任务的重要图像特征。卷积层通过对图像的特定区域（卷积的大小）应用滤波器来工作。这一层的权重然后学习识别在分类任务中关键的特定图像特征。例如，如果我们在训练一个识别人手的模型，滤波器可能学会识别手指。'
- en: The purpose of the *pooling layer* is typically to reduce the dimensionality
    of the inputs from the previous layer. This layer also uses a filter applied to
    a portion of the input, but it has no activation. Instead, it reduces dimensionality
    of the input by performing *max pooling* (where it selects the pixel in the filter
    with the highest value) or *average pooling* (where it takes an average of the
    input pixels to use instead).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*池化层*的目的通常是从前一层的输入中减少维度。该层还使用应用于输入部分的滤波器，但没有激活。相反，它通过执行*最大池化*（选择具有最高值的滤波器中的像素）或*平均池化*（取输入像素的平均值来代替）来减少输入的维度。'
- en: Finally, the *fully connected layer* can be used with something like a softmax
    activation function to create a binary classification task.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，*全连接层*可以与类似softmax的激活函数一起用于创建一个二元分类任务。
- en: See Also
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Convolutional Neural Networks](https://oreil.ly/HoO9g)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[卷积神经网络](https://oreil.ly/HoO9g)'
- en: 22.2 Training a Neural Network for Text Classification
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 22.2 训练用于文本分类的神经网络
- en: Problem
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You need to train a neural network to classify text data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要训练一个神经网络来对文本数据进行分类。
- en: Solution
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use a PyTorch neural network whose first layer is the size of your vocabulary:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一个PyTorch神经网络，其第一层是您的词汇表的大小：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Discussion
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Unlike images, text data is inherently nonnumeric. Before training a model,
    we need to convert the text into a numeric representation that the model can use
    to learn which words and word combinations are important for the classification
    task at hand. In this example, we use scikit-learn’s `CountVectorizer` to encode
    the vocabulary as a vector the size of the entire vocabulary, where each word
    is assigned to a specific index in the vector, and the value at that location
    is the number of times that word appears in a given paragraph. In this case, we
    can see the vocabulary size by looking at our training set:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 不像图像，文本数据本质上是非数值的。在训练模型之前，我们需要将文本转换为模型可以使用的数值表示，以便学习哪些单词和单词组合对于当前分类任务是重要的。在这个例子中，我们使用scikit-learn的`CountVectorizer`将词汇表编码为一个大小为整个词汇表的向量，其中每个单词被分配到向量中的特定索引，该位置的值是该单词在给定段落中出现的次数。在这种情况下，我们可以通过查看我们的训练集来看到词汇表的大小：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We use this same value in the first layer of our neural network to determine
    the size of the input layer: `self.fc1 = nn.Linear(X_train.shape[1], 128)`. This
    allows our network to learn what are called *word embeddings*, vector representations
    of individual words learned from a supervised learning task like the one in this
    recipe. This task will allow us to learn word embeddings of size 128, though these
    embeddings will primarily be useful for this specific task and vocabulary.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在神经网络的第一层使用相同的值来确定输入层的大小：`self.fc1 = nn.Linear(X_train.shape[1], 128)`。这允许我们的网络学习所谓的*词嵌入*，即从像本配方中的监督学习任务学习到的单词的向量表示。这个任务将允许我们学习大小为128的词嵌入，尽管这些嵌入主要对这个特定的任务和词汇表有用。
- en: 22.3 Fine-Tuning a Pretrained Model for Image Classification
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 22.3 对图像分类进行微调预训练模型
- en: Problem
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to train an image classification model using learnings from a pretrained
    model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望使用从预训练模型中学到的知识来训练图像分类模型。
- en: Solution
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use the `transformers` library with `torchvision` to fine-tune a pretrained
    model on your data:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`transformers`库和`torchvision`在您的数据上对预训练模型进行微调：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Discussion
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: In the realm of unstructured data like text and images, it is extremely common
    to start from pretrained models trained on large datasets, instead of starting
    from scratch, especially in cases where we don’t have access to as much labeled
    data. Using embeddings and other information from the larger model, we can then
    fine-tune our own model for a new task without the need for as much labeled information.
    In addition, the pretrained model may have information not captured at all in
    our training dataset, resulting in an overall performance improvement. This process
    is known as *transfer learning*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在像文本和图像这样的非结构化数据领域，通常会使用在大型数据集上训练过的预训练模型作为起点，而不是从头开始，尤其是在我们没有太多标记数据的情况下。利用来自更大模型的嵌入和其他信息，我们可以调整我们自己的模型以适应新任务，而不需要大量标记信息。此外，预训练模型可能具有我们训练数据中未完全捕获的信息，从而导致整体性能的提升。这个过程被称为*迁移学习*。
- en: In this example, we load the weights from Google’s ViT (Vision Transformer)
    model. Then, we use the `transformers` library to fine-tune it for a classification
    task on the fashion MNIST dataset, a simple dataset of clothing items. This approach
    can be applied to increase performance on any computer vision dataset, and the
    `transformers` library provides a high-level interface we can use to fine-tune
    our own model from larger, pretrained ones without writing an egregious amount
    of code.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们加载了来自Google的ViT（Vision Transformer）模型的权重。然后，我们使用`transformers`库对其进行微调，以在时尚MNIST数据集上进行分类任务，这是一个简单的服装项目数据集。这种方法可以应用于增加任何计算机视觉数据集的性能，并且`transformers`库提供了一个高级接口，我们可以使用它来微调我们自己的模型，而无需编写大量代码。
- en: See Also
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Hugging Face website and documentation](https://oreil.ly/5F3Rf)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hugging Face网站和文档](https://oreil.ly/5F3Rf)'
- en: 22.4 Fine-Tuning a Pretrained Model for Text Classification
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 22.4 对预训练模型进行文本分类的微调
- en: Problem
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to train a text classification model using learnings from a pretrained
    model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 你想使用预训练模型的学习成果来训练一个文本分类模型。
- en: Solution
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use the `transformers` library:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`transformers`库：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Discussion
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Just like using pretrained image models, pretrained language models hold a massive
    amount of context about language, as they’re typically trained on a wide variety
    of open internet sources. When we start from a pretrained model base, what we’re
    typically doing is swapping out the classification layer of the existing network
    for one of our own. This allows us to alter the network weights already learned
    to fit our specific task.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 就像使用预训练图像模型一样，预训练语言模型包含了大量关于语言的上下文信息，因为它们通常是在各种开放互联网来源上进行训练的。当我们从一个预训练模型基础开始时，我们通常做的是将现有网络的分类层替换为我们自己的分类层。这使我们能够修改已经学习的网络权重以适应我们的特定任务。
- en: In this example, we’re fine-tuning a DistilBERT model to recognize whether IMDB
    movie reviews were positive (1) or negative (0). The pretrained DistilBERT model
    provides a large corpus of words and context on each one, in addition to neural
    network weights learned from a previous training task. Transfer learning allows
    us to take advantage of all the initial work done training the DistilBERT model
    and repurpose it for our use case, which in this instance is classifying movie
    reviews.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们正在对一个DistilBERT模型进行微调，以识别IMDB电影评论是积极的（1）还是消极的（0）。预训练的DistilBERT模型为每个单词提供了大量的语境信息，以及从先前的训练任务中学到的神经网络权重。迁移学习使我们能够利用所有用于训练DistilBERT模型的初始工作，并将其重新用于我们的用例，即对电影评论进行分类。
- en: See Also
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Text classification in transformers](https://oreil.ly/uhrjI)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[transformers中的文本分类](https://oreil.ly/uhrjI)'
