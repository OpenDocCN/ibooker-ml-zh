<html><head></head><body><section data-pdf-bookmark="Chapter 11. Model Selection" data-type="chapter" epub:type="chapter"><div class="chapter" id="idm46066892884248">&#13;
<h1><span class="label">Chapter 11. </span>Model Selection</h1>&#13;
&#13;
&#13;
<p><a data-primary="hyperparameters" data-secondary="model selection and" data-type="indexterm" id="ix_ch11-asciidoc0"/><a data-primary="model selection" data-type="indexterm" id="ix_ch11-asciidoc1"/>This chapter will discuss optimizing hyperparameters. It will also&#13;
explore the issue of whether the model requires more data to perform better.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Validation Curve" data-type="sect1"><div class="sect1" id="idm46066892704440">&#13;
<h1>Validation Curve</h1>&#13;
&#13;
<p><a data-primary="hyperparameters" data-secondary="validation curve for determining values" data-type="indexterm" id="ix_ch11-asciidoc2"/><a data-primary="validation curve" data-type="indexterm" id="ix_ch11-asciidoc3"/>Creating a validation curve is one way to determine an appropriate value for a hyperparameter.&#13;
A validation curve is a plot that shows how the model performance responds to changes in the hyperparameter’s value (see <a data-type="xref" href="#id28">Figure 11-1</a>). The chart shows both the training data and the validation data. The validation scores allow us to infer how the model would respond to unseen data. Typically, we would choose a hyperparameter that maximizes the validation score.</p>&#13;
&#13;
<p><a data-primary="Yellowbrick" data-secondary="validation curve report" data-type="indexterm" id="ix_ch11-asciidoc4"/>In the following example, we will use Yellowbrick to see if changing the value of the <code>max_depth</code>&#13;
hyperparameter changes the model performance of a random forest. You can provide a <code>scoring</code> parameter&#13;
set to a scikit-learn model metric (the default for classification is <code>'accuracy'</code>):</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>Use the <code>n_jobs</code> parameter to take advantage of the CPUs and run this faster. If you set it to <code>-1</code>, it will use all of the CPUs.</p>&#13;
</div>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">yellowbrick.model_selection</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">ValidationCurve</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">vc_viz</code> <code class="o">=</code> <code class="n">ValidationCurve</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">n_estimators</code><code class="o">=</code><code class="mi">100</code><code class="p">),</code>&#13;
<code class="gp">... </code>    <code class="n">param_name</code><code class="o">=</code><code class="s">"max_depth"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">param_range</code><code class="o">=</code><code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">11</code><code class="p">),</code>&#13;
<code class="gp">... </code>    <code class="n">cv</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">n_jobs</code><code class="o">=-</code><code class="mi">1</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">vc_viz</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">vc_viz</code><code class="o">.</code><code class="n">poof</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1101.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id28">&#13;
<img alt="Validation curve report." src="assets/mlpr_1101.png"/>&#13;
<h6><span class="label">Figure 11-1. </span>Validation curve report.</h6>&#13;
</div></figure>&#13;
&#13;
<p>The <code>ValidationCurve</code> class supports a <code>scoring</code> parameter. The parameter can be a custom function or one of the following options, depending on the task.</p>&#13;
&#13;
<p>Classification <code>scoring</code> options include: <code>'accuracy'</code>, <code>'average_precision'</code>,&#13;
<code>'f1'</code>, <code>'f1_micro'</code>, <code>'f1_macro'</code>,&#13;
<code>'f1_weighted'</code>, <code>'f1_samples'</code>, <code>'neg_log_loss'</code>, <code>'precision'</code>, <code>'recall'</code>, and&#13;
<code>'roc_auc'</code>.</p>&#13;
&#13;
<p>Clustering <code>scoring</code> options: <code>'adjusted_mutual_info_score'</code>,&#13;
<code>'adjusted_rand_score'</code>, <code>'completeness_score'</code>, <code>'fowlkes</code><span class="keep-together"><code>mallows_score'</code></span>,&#13;
<code>'homogeneity_score'</code>, <code>'mutual_info_score'</code>,&#13;
<code>'normalized_mutual_info_score'</code>, and <code>'v_measure_score'</code>.</p>&#13;
&#13;
<p>Regression <code>scoring</code> options<a data-startref="ix_ch11-asciidoc4" data-type="indexterm" id="idm46066892569096"/>:<a data-startref="ix_ch11-asciidoc3" data-type="indexterm" id="idm46066892568232"/><a data-startref="ix_ch11-asciidoc2" data-type="indexterm" id="idm46066892567528"/> <code>'explained_variance'</code>,&#13;
<code>'neg_mean_absolute_error'</code>, <code>'neg_mean_squared_error'</code>,&#13;
<code>'neg_mean_squared_log_error'</code>, <code>'neg_median_absolute_error'</code>, and <code>'r2'</code>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Learning Curve" data-type="sect1"><div class="sect1" id="idm46066892703848">&#13;
<h1>Learning Curve</h1>&#13;
&#13;
<p><a data-primary="learning curve" data-type="indexterm" id="ix_ch11-asciidoc5"/>To select the best model for your project, how much data do you need?&#13;
A learning curve can help us answer that question.&#13;
This curve plots the training and cross-validation score as we&#13;
create models with more samples. If the cross-validation score continues to rise, for example, that could indicate that&#13;
more data would help the model perform better.</p>&#13;
&#13;
<p><a data-primary="bias" data-type="indexterm" id="idm46066892561112"/>The following visualization shows a validation curve and also&#13;
helps us explore bias and variance in our model (see <a data-type="xref" href="#id29">Figure 11-2</a>).&#13;
<a data-primary="underfitting" data-type="indexterm" id="idm46066892559352"/>If there is variability (a large shaded area) in the&#13;
training score, then the model suffers from bias error and is too simple (underfit). <a data-primary="overfitting" data-type="indexterm" id="idm46066892558360"/>If&#13;
there is variability in the cross-validated score, then the model&#13;
suffers from variance error and is too complicated (overfit). Another&#13;
indication that the model is overfit is that the performance of the&#13;
validation set is much worse than the training set.</p>&#13;
&#13;
<p><a data-primary="Yellowbrick" data-secondary="learning curve plot" data-type="indexterm" id="idm46066892557032"/>Here is an example of creating a learning curve using <span class="keep-together">Yellowbrick:</span></p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">yellowbrick.model_selection</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">LearningCurve</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lc3_viz</code> <code class="o">=</code> <code class="n">LearningCurve</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">n_estimators</code><code class="o">=</code><code class="mi">100</code><code class="p">),</code>&#13;
<code class="gp">... </code>    <code class="n">cv</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lc3_viz</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">lc3_viz</code><code class="o">.</code><code class="n">poof</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1102.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id29">&#13;
<img alt="Learining curve plot. The plateau in the validation score indicates that adding more data would not improve this model." src="assets/mlpr_1102.png"/>&#13;
<h6><span class="label">Figure 11-2. </span>Learning curve plot. The plateau in the validation score indicates that adding more data would not improve this model.</h6>&#13;
</div></figure>&#13;
&#13;
<p>This visualization can also be used for regression or clustering by changing the scoring&#13;
options<a data-startref="ix_ch11-asciidoc5" data-type="indexterm" id="idm46066892456904"/>.<a data-startref="ix_ch11-asciidoc1" data-type="indexterm" id="idm46066892456072"/><a data-startref="ix_ch11-asciidoc0" data-type="indexterm" id="idm46066892455368"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>