- en: Chapter 14\. Trees and Forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 14.0 Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tree-based learning algorithms are a broad and popular family of related non-parametric,
    supervised methods for both classification and regression. The basis of tree-based
    learners is the *decision tree*, wherein a series of decision rules (e.g., “If
    a person’s credit score is greater than 720…​”) are chained. The result looks
    vaguely like an upside-down tree, with the first decision rule at the top and
    subsequent decision rules spreading out below. In a decision tree, every decision
    rule occurs at a decision node, with the rule creating branches leading to new
    nodes. A branch without a decision rule at the end is called a *leaf*.
  prefs: []
  type: TYPE_NORMAL
- en: One reason for the popularity of tree-based models is their interpretability.
    In fact, decision trees can literally be drawn out in their complete form (see
    [Recipe 14.3](#visualizing-a-decision-tree-model)) to create a highly intuitive
    model. From this basic tree system comes a wide variety of extensions from random
    forests to stacking. In this chapter we will cover how to train, handle, adjust,
    visualize, and evaluate a number of tree-based models.
  prefs: []
  type: TYPE_NORMAL
- en: 14.1 Training a Decision Tree Classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to train a classifier using a decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use scikit-learn’s `DecisionTreeClassifier`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Decision tree learners attempt to find a decision rule that produces the greatest
    decrease in impurity at a node. While there are a number of measurements of impurity,
    by default `DecisionTreeClassifier` uses Gini impurity:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>G</mi> <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mn>1</mn> <mo>-</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>c</mi></munderover> <msup><mrow><msub><mi>p</mi> <mi>i</mi></msub></mrow>
    <mn>2</mn></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math display="inline"><mi>G</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></math>
    is the Gini impurity at node <math display="inline"><mi>t</mi></math>, and <math
    display="inline"><msub><mi>p</mi><mi>i</mi></msub></math> is the proportion of
    observations of class <math display="inline"><mi>c</mi></math> at node <math display="inline"><mi>t</mi></math>.
    This process of finding the decision rules that create splits to decrease impurity
    is repeated recursively until all leaf nodes are pure (i.e., contain only one
    class) or some arbitrary cutoff is reached.
  prefs: []
  type: TYPE_NORMAL
- en: 'In scikit-learn, `DecisionTreeClassifier` operates like other learning methods;
    after the model is trained using `fit`, we can use the model to predict the class
    of an observation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also see the predicted class probabilities of the observation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, if we want to use a different impurity measurement we can use the
    `criterion` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Decision Tree Learning, Princeton](https://oreil.ly/lCPBG)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 14.2 Training a Decision Tree Regressor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to train a regression model using a decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use scikit-learn’s `DecisionTreeRegressor`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Decision tree regression works similarly to decision tree classification; however,
    instead of reducing Gini impurity or entropy, potential splits are by default
    measured on how much they reduce mean squared error (MSE):'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>MSE</mtext> <mo>=</mo> <mfrac><mn>1</mn>
    <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>¯</mo></mover> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'where <math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math> is the
    true value of the target and <math display="inline"><msub><mover accent="true"><mi>y</mi>
    <mo>¯</mo></mover> <mi>i</mi></msub></math> is the mean value. In scikit-learn,
    decision tree regression can be conducted using `DecisionTreeRegressor`. Once
    we have trained a decision tree, we can use it to predict the target value for
    an observation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Just like with `DecisionTreeClassifier` we can use the `criterion` parameter
    to select the desired measurement of split quality. For example, we can construct
    a tree whose splits reduce mean absolute error (MAE):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[scikit-learn documentation: Decision Tree Regression](https://oreil.ly/EGkU_)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 14.3 Visualizing a Decision Tree Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to visualize a model created by a decision tree learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Export the decision tree model into DOT format, then visualize:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 14in01](assets/mpc2_14in01.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the advantages of decision tree classifiers is that we can visualize
    the entire trained model, making decision trees one of the most interpretable
    models in machine learning. In our solution, we exported our trained model in
    DOT format (a graph description language) and then used that to draw the graph.
  prefs: []
  type: TYPE_NORMAL
- en: If we look at the root node, we can see the decision rule is that if petal widths
    are less than or equal to 0.8 cm, then go to the left branch; if not, go to the
    right branch. We can also see the Gini impurity index (0.667), the number of observations
    (150), the number of observations in each class ([50,50,50]), and the class the
    observations would be predicted to be if we stopped at that node (*setosa*). We
    can also see that at that node the learner found that a single decision rule (`petal
    width (cm) <= 0.8`) was able to perfectly identify all of the *setosa* class observations.
    Furthermore, with one more decision rule with the same feature (`petal width (cm)
    <= 1.75`) the decision tree is able to correctly classify 144 of 150 observations.
    This makes petal width a very important feature!
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to use the decision tree in other applications or reports, we can
    easily export the visualization into PDF or a PNG image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: While this solution visualized a decision tree classifier, it can just as easily
    be used to visualize a decision tree regressor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: macOS users might have to install Graphviz’s executable to run the preceding
    code. This can be done with the Homebrew command `brew install graphviz`. For
    Homebrew installation instructions, visit Homebrew’s website.'
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Homebrew](https://oreil.ly/GgeNI)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 14.4 Training a Random Forest Classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to train a classification model using a “forest” of randomized decision
    trees.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use scikit-learn’s `RandomForestClassifier` to train a random forest classification
    model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A common problem with decision trees is that they tend to fit the training data
    too closely (i.e., overfitting). This has motivated the widespread use of an ensemble
    learning method called *random forest*. In a random forest, many decision trees
    are trained, but each tree receives only a bootstrapped sample of observations
    (i.e., a random sample of observations with replacement that matches the original
    number of observations), and each node considers only a subset of features when
    determining the best split. This forest of randomized decision trees (hence the
    name) votes to determine the predicted class.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see by comparing this solution to [Recipe 14.1](#training-a-decision-tree-classifier),
    scikit-learn’s `RandomForestClassifier` works similarly to `DecisionTreeClassifier`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '`RandomForestClassifier` also uses many of the same parameters as `DecisionTreeClassifier`.
    For example, we can change the measure of split quality used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: However, being a forest rather than an individual decision tree, `RandomForestClassifier`
    has certain parameters that are either unique to random forests or particularly
    important. First, the `max_features` parameter determines the maximum number of
    features to be considered at each node and takes a number of arguments including
    integers (number of features), floats (percentage of features), and `sqrt` (square
    root of the number of features). By default, `max_features` is set to `auto`,
    which acts the same as `sqrt`. Second, the `bootstrap` parameter allows us to
    set whether the subset of observations considered for a tree is created using
    sampling with replacement (the default setting) or without replacement. Third,
    `n_estimators` sets the number of decision trees to include in the forest. Finally,
    while not specific to random forest classifiers, because we are effectively training
    many decision tree models, it is often useful to use all available cores by setting
    `n_jobs=-1`.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Random Forests, Berkeley Statistics](https://oreil.ly/h-LQL)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 14.5 Training a Random Forest Regressor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to train a regression model using a “forest” of randomized decision
    trees.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Train a random forest regression model using scikit-learn’s `RandomForestRegressor`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Just as we can make a forest of decision tree classifiers, we can make a forest
    of decision tree regressors, where each tree uses a bootstrapped subset of observations
    and at each node the decision rule considers only a subset of features. As with
    `RandomForestClassifier` we have certain important parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_features`'
  prefs: []
  type: TYPE_NORMAL
- en: Sets the maximum number of features to consider at each node. Defaults to *p*
    features, where *p* is the total number of features.
  prefs: []
  type: TYPE_NORMAL
- en: '`bootstrap`'
  prefs: []
  type: TYPE_NORMAL
- en: Sets whether or not to sample with replacement. Defaults to `True`.
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators`'
  prefs: []
  type: TYPE_NORMAL
- en: Sets the number of decision trees to construct. Defaults to `10`.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[scikit-learn documentation: RandomForestRegressor](https://oreil.ly/ksa9Z)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 14.6 Evaluating Random Forests with Out-of-Bag Errors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to evaluate a random forest model without using cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Calculate the model’s out-of-bag score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In random forests, each decision tree is trained using a bootstrapped subset
    of observations. This means that for every tree there is a separate subset of
    observations not being used to train that tree. These are called out-of-bag (OOB)
    observations. We can use OOB observations as a test set to evaluate the performance
    of our random forest.
  prefs: []
  type: TYPE_NORMAL
- en: For every observation, the learning algorithm compares the observation’s true
    value with the prediction from a subset of trees not trained using that observation.
    The overall score is calculated and provides a single measure of a random forest’s
    performance. OOB score estimation is an alternative to cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: In scikit-learn, we can compute OOB scores of a random forest by setting `oob_score=True`
    in the random forest object (i.e., `RandomForestClassifier`). The score can be
    retrieved using `oob_score_`.
  prefs: []
  type: TYPE_NORMAL
- en: 14.7 Identifying Important Features in Random Forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to know which features are most important in a random forest model.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Calculate and visualize the importance of each feature by inspecting the model’s
    `feature_importances_` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![mpc2 14in02](assets/mpc2_14in02.png)'
  prefs: []
  type: TYPE_IMG
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the major benefits of decision trees is interpretability. Specifically,
    we can visualize the entire model (see [Recipe 14.3](#visualizing-a-decision-tree-model)).
    However, a random forest model is composed of tens, hundreds, or even thousands
    of decision trees. This makes a simple, intuitive visualization of a random forest
    model impractical. That said, there is another option: we can compare (and visualize)
    the relative importance of each feature.'
  prefs: []
  type: TYPE_NORMAL
- en: In [Recipe 14.3](#visualizing-a-decision-tree-model), we visualized a decision
    tree classifier model and saw that decision rules based only on petal width were
    able to classify many observations correctly. Intuitively, we can say this means
    that petal width is an important feature in our classifier. More formally, features
    with splits that have the greater mean decrease in impurity (e.g., Gini impurity
    or entropy in classifiers and variance in regressors) are considered more important.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are two things to keep in mind regarding feature importance.
    First, scikit-learn requires that we break up nominal categorical features into
    multiple binary features. This has the effect of spreading the importance of that
    feature across all of the binary features and can make each feature appear to
    be unimportant even when the original nominal categorical feature is highly important.
    Second, if two features are highly correlated, one feature will claim much of
    the importance, making the other feature appear to be far less important, which
    has implications for interpretation if not considered.
  prefs: []
  type: TYPE_NORMAL
- en: 'In scikit-learn, classification and regression decision trees and random forests
    can report the relative importance of each feature using the `feature_importances_`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The higher the number, the more important the feature (all importance scores
    sum to 1). By plotting these values, we can add interpretability to our random
    forest models.
  prefs: []
  type: TYPE_NORMAL
- en: 14.8 Selecting Important Features in Random Forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to conduct feature selection on a random forest.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Identify the importance features and retrain the model using only the most
    important features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are situations where we might want to reduce the number of features in
    our model. For example, we might want to reduce the model’s variance, or we might
    want to improve interpretability by including only the most important features.
  prefs: []
  type: TYPE_NORMAL
- en: In scikit-learn we can use a simple two-stage workflow to create a model with
    reduced features. First, we train a random forest model using all features. Then,
    we use this model to identify the most important features. Next, we create a new
    feature matrix that includes only these features. In our solution, we used the
    `SelectFromModel` method to create a feature matrix containing only features with
    an importance greater than or equal to some `threshold` value. Finally, we created
    a new model using only those features.
  prefs: []
  type: TYPE_NORMAL
- en: We must note two caveats to this approach. First, nominal categorical features
    that have been one-hot encoded will see the feature importance diluted across
    the binary features. Second, the feature importance of highly correlated features
    will be effectively assigned to one feature and not evenly distributed across
    both features.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Variable Selection Using Random Forests, Robin Genuer, Jean-Michel Poggi,
    and Christine Tuleau-Malot](https://oreil.ly/y9k2U)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 14.9 Handling Imbalanced Classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have a target vector with highly imbalanced classes and want to train a
    random forest model.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Train a decision tree or random forest model with `class_weight="balanced"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Imbalanced classes are a common problem when we are doing machine learning
    in the real world. Left unaddressed, the presence of imbalanced classes can reduce
    the performance of our model. We will discuss handling imbalanced classes during
    preprocessing in [Recipe 17.5](ch17.xhtml#handling-imbalanced-classes-ch17). However,
    many learning algorithms in scikit-learn come with built-in methods for correcting
    for imbalanced classes. We can set `RandomForestClassifier` to correct for imbalanced
    classes using the `class_weight` parameter. If supplied with a dictionary in the
    form of class names and their desired weights (e.g., `{"male": 0.2, "female":
    0.8}`), `RandomForestClassifier` will weight the classes accordingly. However,
    often a more useful argument is `balanced`, wherein classes are automatically
    weighted inversely proportional to how frequently they appear in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>w</mi> <mi>j</mi></msub> <mo>=</mo> <mfrac><mi>n</mi>
    <mrow><mi>k</mi><msub><mi>n</mi> <mi>j</mi></msub></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'where <math display="inline"><msub><mi>w</mi><mi>j</mi></msub></math> is the
    weight of class <math display="inline"><mi>j</mi></math>, <math display="inline"><mi>n</mi></math>
    is the number of observations, <math display="inline"><msub><mi>n</mi><mi>j</mi></msub></math>
    is the number of observations in class <math display="inline"><mi>j</mi></math>,
    and <math display="inline"><mi>k</mi></math> is the total number of classes. For
    example, in our solution we have 2 classes (<math display="inline"><mi>k</mi></math>),
    110 observations (<math display="inline"><mi>n</mi></math>), and 10 and 100 observations
    in each class, respectively (<math display="inline"><msub><mi>n</mi><mi>j</mi></msub></math>).
    If we weight the classes using `class_weight="balanced"`, then the smaller class
    is weighted more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'while the larger class is weighted less:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 14.10 Controlling Tree Size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to manually determine the structure and size of a decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the tree structure parameters in scikit-learn tree-based learning algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'scikit-learn’s tree-based learning algorithms have a variety of techniques
    for controlling the size of decision trees. These are accessed through parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_depth`'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum depth of the tree. If `None`, the tree is grown until all leaves are
    pure. If an integer, the tree is effectively “pruned” to that depth.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_samples_split`'
  prefs: []
  type: TYPE_NORMAL
- en: Minimum number of observations at a node before that node is split. If an integer
    is supplied as an argument, it determines the raw minimum, while if a float is
    supplied, the minimum is the percent of total observations.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_samples_leaf`'
  prefs: []
  type: TYPE_NORMAL
- en: Minimum number of observations required to be at a leaf. Uses the same arguments
    as `min_samples_split`.
  prefs: []
  type: TYPE_NORMAL
- en: '`max_leaf_nodes`'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum number of leaves.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_impurity_split`'
  prefs: []
  type: TYPE_NORMAL
- en: Minimum impurity decrease required before a split is performed.
  prefs: []
  type: TYPE_NORMAL
- en: While it is useful to know these parameters exist, most likely we will only
    be using `max_depth` and `min_impurity_split` because shallower trees (sometimes
    called *stumps*) are simpler models and thus have lower variance.
  prefs: []
  type: TYPE_NORMAL
- en: 14.11 Improving Performance Through Boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need a model with better performance than decision trees or random forests.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Train a boosted model using `AdaBoostClassifier` or `AdaBoostRegressor`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In a random forest, an ensemble (group) of randomized decision trees predicts
    the target vector. An alternative, and often more powerful, approach is called
    *boosting*. In one form of boosting called AdaBoost, we iteratively train a series
    of weak models (most often a shallow decision tree, sometimes called a stump),
    each iteration giving higher priority to observations the previous model predicted
    incorrectly. More specifically, in AdaBoost:'
  prefs: []
  type: TYPE_NORMAL
- en: Assign every observation, <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math>,
    an initial weight value, <math display="inline"><mrow><msub><mi>w</mi> <mi>i</mi></msub>
    <mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac></mrow></math>, where <math display="inline"><mi>n</mi></math>
    is the total number of observations in the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a “weak” model on the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each observation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If weak model predicts <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math>
    correctly, <math display="inline"><msub><mi>w</mi><mi>i</mi></msub></math> is
    decreased.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If weak model predicts <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math>
    incorrectly, <math display="inline"><msub><mi>w</mi><mi>i</mi></msub></math> is
    increased.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a new weak model where observations with greater <math display="inline"><msub><mi>w</mi><mi>i</mi></msub></math>
    are given greater priority.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 4 and 5 until the data is perfectly predicted or a preset number
    of weak models has been trained.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The result is an aggregated model where individual weak models focus on more
    difficult (from a prediction perspective) observations. In scikit-learn, we can
    implement AdaBoost using `AdaBoostClassifier` or `AdaBoostRegressor`. The most
    important parameters are `base_estimator`, `n_estimators`, `learning_rate`, and
    `loss`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`base_estimator`'
  prefs: []
  type: TYPE_NORMAL
- en: '`base_estimator` is the learning algorithm to use to train the weak models.
    The most common learner to use with AdaBoost is a decision tree, the parameter’s
    default argument.'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators`'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators` is the number of models to iteratively train.'
  prefs: []
  type: TYPE_NORMAL
- en: '`learning_rate`'
  prefs: []
  type: TYPE_NORMAL
- en: '`learning_rate` is the contribution of each model to the weights, and it defaults
    to `1`. Reducing the learning rate will mean the weights will be increased or
    decreased to a small degree, forcing the model to train slower (but sometimes
    resulting in better performance scores).'
  prefs: []
  type: TYPE_NORMAL
- en: '`loss`'
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` is exclusive to `AdaBoostRegressor` and sets the loss function to use
    when updating weights. This defaults to a linear loss function but can be changed
    to `square` or `exponential`.'
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Explaining AdaBoost, Robert E. Schapire](https://oreil.ly/5E1v4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 14.12 Training an XGBoost Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to train a tree-based model with high predictive power.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the `xgboost` Python library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: XGBoost (which stands for Extreme Gradient Boosting) is a very popular gradient
    boosting algorithm in the machine learning space. Though it is not always a tree-based
    model, it is frequently applied to ensembles of decision trees. It gained much
    of its popularity due to widespread success on the machine learning competition
    website Kaggle and has since been a reliable algorithm for improving performance
    beyond that of typical random forests or gradient boosted machines.
  prefs: []
  type: TYPE_NORMAL
- en: Although XGBoost is known for being computationally intensive, computational
    performance optimizations (such as GPU support) over the last few years have made
    iterating quickly with XGBoost significantly easier, and it remains a common choice
    of algorithm when statistical performance is a requirement.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[XGBoost documentation](https://oreil.ly/cAuGX)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 14.13 Improving Real-Time Performance with LightGBM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to train a gradient boosted tree-based model that is computationally
    optimized.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the gradient boosted machine library `lightgbm`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `lightgbm` library is used for gradient boosted machines and is highly optimized
    for training time, inference, and GPU support. As a result of its computational
    efficiency, it’s often used in production and in large scale settings. Although
    scikit-learn models are typically easier to use, some libraries, such as `lightgbm`,
    can be handy when you’re limited by large data or strict model training/serving
    times.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[LightGBM documentation](https://oreil.ly/XDcpG)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[CatBoost documentation (another optimized library for GBMs)](https://oreil.ly/4Bb8g)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
