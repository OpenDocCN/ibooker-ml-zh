- en: Chapter 14\. Trees and Forests
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第14章 树和森林
- en: 14.0 Introduction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 14.0 引言
- en: Tree-based learning algorithms are a broad and popular family of related non-parametric,
    supervised methods for both classification and regression. The basis of tree-based
    learners is the *decision tree*, wherein a series of decision rules (e.g., “If
    a person’s credit score is greater than 720…​”) are chained. The result looks
    vaguely like an upside-down tree, with the first decision rule at the top and
    subsequent decision rules spreading out below. In a decision tree, every decision
    rule occurs at a decision node, with the rule creating branches leading to new
    nodes. A branch without a decision rule at the end is called a *leaf*.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的学习算法是一类广泛且流行的非参数化监督方法，既适用于分类又适用于回归。基于树的学习器的基础是*决策树*，其中一系列决策规则（例如，“如果一个人的信用评分大于720…​”）被链接起来。结果看起来略像一个倒置的树形，顶部是第一个决策规则，下面是后续的决策规则分支开展。在决策树中，每个决策规则出现在一个决策节点，规则创建通向新节点的分支。末端没有决策规则的分支称为*叶节点*。
- en: One reason for the popularity of tree-based models is their interpretability.
    In fact, decision trees can literally be drawn out in their complete form (see
    [Recipe 14.3](#visualizing-a-decision-tree-model)) to create a highly intuitive
    model. From this basic tree system comes a wide variety of extensions from random
    forests to stacking. In this chapter we will cover how to train, handle, adjust,
    visualize, and evaluate a number of tree-based models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 树模型之所以受欢迎的一个原因是它们的可解释性。事实上，决策树可以以完整形式绘制出来（参见[配方14.3](#visualizing-a-decision-tree-model)），以创建一个高度直观的模型。从这个基本的树系统中产生了多种扩展，从随机森林到堆叠。在本章中，我们将讨论如何训练、处理、调整、可视化和评估多种基于树的模型。
- en: 14.1 Training a Decision Tree Classifier
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 14.1 训练决策树分类器
- en: Problem
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You need to train a classifier using a decision tree.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要使用决策树训练一个分类器。
- en: Solution
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use scikit-learn’s `DecisionTreeClassifier`:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 的 `DecisionTreeClassifier`：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Discussion
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Decision tree learners attempt to find a decision rule that produces the greatest
    decrease in impurity at a node. While there are a number of measurements of impurity,
    by default `DecisionTreeClassifier` uses Gini impurity:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树学习器试图找到一个决策规则，在节点处产生最大的不纯度减少。虽然有多种不纯度的测量方法，但默认情况下，`DecisionTreeClassifier`
    使用基尼不纯度：
- en: <math display="block"><mrow><mi>G</mi> <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mn>1</mn> <mo>-</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>c</mi></munderover> <msup><mrow><msub><mi>p</mi> <mi>i</mi></msub></mrow>
    <mn>2</mn></msup></mrow></math>
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>G</mi> <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mn>1</mn> <mo>-</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>c</mi></munderover> <msup><mrow><msub><mi>p</mi> <mi>i</mi></msub></mrow>
    <mn>2</mn></msup></mrow></math>
- en: where <math display="inline"><mi>G</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></math>
    is the Gini impurity at node <math display="inline"><mi>t</mi></math>, and <math
    display="inline"><msub><mi>p</mi><mi>i</mi></msub></math> is the proportion of
    observations of class <math display="inline"><mi>c</mi></math> at node <math display="inline"><mi>t</mi></math>.
    This process of finding the decision rules that create splits to decrease impurity
    is repeated recursively until all leaf nodes are pure (i.e., contain only one
    class) or some arbitrary cutoff is reached.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math display="inline"><mi>G</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></math>
    是节点 <math display="inline"><mi>t</mi></math> 处的基尼不纯度，<math display="inline"><msub><mi>p</mi><mi>i</mi></msub></math>
    是节点 <math display="inline"><mi>t</mi></math> 处类别 <math display="inline"><mi>c</mi></math>
    的观察比例。这个找到减少不纯度的决策规则并创建分裂的过程会递归重复，直到所有叶节点是纯净的（即只包含一个类别）或达到某个任意的截止点。
- en: 'In scikit-learn, `DecisionTreeClassifier` operates like other learning methods;
    after the model is trained using `fit`, we can use the model to predict the class
    of an observation:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中，`DecisionTreeClassifier` 的操作类似于其他学习方法；在使用 `fit` 训练模型之后，我们可以使用模型预测观察的类别：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can also see the predicted class probabilities of the observation:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看到观察的预测类别概率：
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, if we want to use a different impurity measurement we can use the
    `criterion` parameter:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果我们想使用不同的不纯度测量，我们可以使用 `criterion` 参数：
- en: '[PRE5]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: See Also
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Decision Tree Learning, Princeton](https://oreil.ly/lCPBG)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[决策树学习，普林斯顿](https://oreil.ly/lCPBG)'
- en: 14.2 Training a Decision Tree Regressor
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 14.2 训练决策树回归器
- en: Problem
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You need to train a regression model using a decision tree.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要使用决策树训练一个回归模型。
- en: Solution
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use scikit-learn’s `DecisionTreeRegressor`:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 的 `DecisionTreeRegressor`：
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Discussion
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Decision tree regression works similarly to decision tree classification; however,
    instead of reducing Gini impurity or entropy, potential splits are by default
    measured on how much they reduce mean squared error (MSE):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树回归与决策树分类类似；但是，它不是减少基尼不纯度或熵，而是默认情况下测量潜在分裂如何减少均方误差（MSE）：
- en: <math display="block"><mrow><mtext>MSE</mtext> <mo>=</mo> <mfrac><mn>1</mn>
    <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>¯</mo></mover> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>MSE</mtext> <mo>=</mo> <mfrac><mn>1</mn>
    <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>¯</mo></mover> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
- en: 'where <math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math> is the
    true value of the target and <math display="inline"><msub><mover accent="true"><mi>y</mi>
    <mo>¯</mo></mover> <mi>i</mi></msub></math> is the mean value. In scikit-learn,
    decision tree regression can be conducted using `DecisionTreeRegressor`. Once
    we have trained a decision tree, we can use it to predict the target value for
    an observation:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math> 是目标的真实值，而
    <math display="inline"><msub><mover accent="true"><mi>y</mi> <mo>¯</mo></mover>
    <mi>i</mi></msub></math> 是平均值。在 scikit-learn 中，可以使用 `DecisionTreeRegressor` 进行决策树回归。一旦我们训练好一个决策树，就可以用它来预测观测值的目标值：
- en: '[PRE7]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Just like with `DecisionTreeClassifier` we can use the `criterion` parameter
    to select the desired measurement of split quality. For example, we can construct
    a tree whose splits reduce mean absolute error (MAE):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 就像使用 `DecisionTreeClassifier` 一样，我们可以使用 `criterion` 参数来选择所需的分裂质量测量。例如，我们可以构建一个树，其分裂减少平均绝对误差（MAE）：
- en: '[PRE9]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: See Also
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[scikit-learn documentation: Decision Tree Regression](https://oreil.ly/EGkU_)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[scikit-learn 文档：决策树回归](https://oreil.ly/EGkU_)'
- en: 14.3 Visualizing a Decision Tree Model
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 14.3 可视化决策树模型
- en: Problem
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You need to visualize a model created by a decision tree learning algorithm.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 需要可视化由决策树学习算法创建的模型。
- en: Solution
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Export the decision tree model into DOT format, then visualize:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 将决策树模型导出为 DOT 格式，然后进行可视化：
- en: '[PRE10]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![mpc2 14in01](assets/mpc2_14in01.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![mpc2 14in01](assets/mpc2_14in01.png)'
- en: Discussion
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: One of the advantages of decision tree classifiers is that we can visualize
    the entire trained model, making decision trees one of the most interpretable
    models in machine learning. In our solution, we exported our trained model in
    DOT format (a graph description language) and then used that to draw the graph.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树分类器的一个优点是，我们可以可视化整个训练好的模型，使决策树成为机器学习中最具可解释性的模型之一。在我们的解决方案中，我们将训练好的模型导出为 DOT
    格式（一种图形描述语言），然后用它来绘制图形。
- en: If we look at the root node, we can see the decision rule is that if petal widths
    are less than or equal to 0.8 cm, then go to the left branch; if not, go to the
    right branch. We can also see the Gini impurity index (0.667), the number of observations
    (150), the number of observations in each class ([50,50,50]), and the class the
    observations would be predicted to be if we stopped at that node (*setosa*). We
    can also see that at that node the learner found that a single decision rule (`petal
    width (cm) <= 0.8`) was able to perfectly identify all of the *setosa* class observations.
    Furthermore, with one more decision rule with the same feature (`petal width (cm)
    <= 1.75`) the decision tree is able to correctly classify 144 of 150 observations.
    This makes petal width a very important feature!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看根节点，我们可以看到决策规则是，如果花瓣宽度小于或等于 0.8 厘米，则进入左分支；否则，进入右分支。我们还可以看到基尼不纯度指数（0.667）、观测数量（150）、每个类中的观测数量（[50,50,50]）以及如果我们在该节点停止，观测将被预测为的类别（*setosa*）。我们还可以看到在该节点，学习者发现单个决策规则（`花瓣宽度（厘米）<=
    0.8`）能够完美识别所有 *setosa* 类观测。此外，再增加一个相同特征的决策规则（`花瓣宽度（厘米）<= 1.75`），决策树能够正确分类 150
    个观测中的 144 个。这使得花瓣宽度成为非常重要的特征！
- en: 'If we want to use the decision tree in other applications or reports, we can
    easily export the visualization into PDF or a PNG image:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想在其他应用程序或报告中使用决策树，可以将可视化导出为 PDF 或 PNG 图像：
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: While this solution visualized a decision tree classifier, it can just as easily
    be used to visualize a decision tree regressor.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个解决方案可视化了决策树分类器，但同样可以轻松用于可视化决策树回归器。
- en: 'Note: macOS users might have to install Graphviz’s executable to run the preceding
    code. This can be done with the Homebrew command `brew install graphviz`. For
    Homebrew installation instructions, visit Homebrew’s website.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：macOS 用户可能需要安装 Graphviz 的可执行文件才能运行上述代码。可以使用 Homebrew 命令 `brew install graphviz`
    完成安装。有关 Homebrew 安装说明，请访问 Homebrew 的网站。
- en: See Also
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Homebrew](https://oreil.ly/GgeNI)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Homebrew](https://oreil.ly/GgeNI)'
- en: 14.4 Training a Random Forest Classifier
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 14.4 训练随机森林分类器
- en: Problem
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to train a classification model using a “forest” of randomized decision
    trees.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用随机决策树“森林”训练分类模型。
- en: Solution
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use scikit-learn’s `RandomForestClassifier` to train a random forest classification
    model.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 的 `RandomForestClassifier` 训练随机森林分类模型。
- en: '[PRE15]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Discussion
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: A common problem with decision trees is that they tend to fit the training data
    too closely (i.e., overfitting). This has motivated the widespread use of an ensemble
    learning method called *random forest*. In a random forest, many decision trees
    are trained, but each tree receives only a bootstrapped sample of observations
    (i.e., a random sample of observations with replacement that matches the original
    number of observations), and each node considers only a subset of features when
    determining the best split. This forest of randomized decision trees (hence the
    name) votes to determine the predicted class.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的一个常见问题是它们往往过度拟合训练数据。这促使了一种称为*随机森林*的集成学习方法的广泛使用。在随机森林中，训练许多决策树，但每棵树只接收一个自举样本的观测集（即使用替换的原始观测数量的随机样本），并且在确定最佳分裂时，每个节点只考虑特征的一个子集。这些随机化决策树的森林（因此得名）投票以确定预测类别。
- en: 'As we can see by comparing this solution to [Recipe 14.1](#training-a-decision-tree-classifier),
    scikit-learn’s `RandomForestClassifier` works similarly to `DecisionTreeClassifier`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将这个解决方案与[配方 14.1](#training-a-decision-tree-classifier)进行比较，我们可以看到scikit-learn的`RandomForestClassifier`与`DecisionTreeClassifier`类似：
- en: '[PRE16]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '`RandomForestClassifier` also uses many of the same parameters as `DecisionTreeClassifier`.
    For example, we can change the measure of split quality used:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`RandomForestClassifier`也使用与`DecisionTreeClassifier`许多相同的参数。例如，我们可以改变用于分裂质量的测量：'
- en: '[PRE18]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: However, being a forest rather than an individual decision tree, `RandomForestClassifier`
    has certain parameters that are either unique to random forests or particularly
    important. First, the `max_features` parameter determines the maximum number of
    features to be considered at each node and takes a number of arguments including
    integers (number of features), floats (percentage of features), and `sqrt` (square
    root of the number of features). By default, `max_features` is set to `auto`,
    which acts the same as `sqrt`. Second, the `bootstrap` parameter allows us to
    set whether the subset of observations considered for a tree is created using
    sampling with replacement (the default setting) or without replacement. Third,
    `n_estimators` sets the number of decision trees to include in the forest. Finally,
    while not specific to random forest classifiers, because we are effectively training
    many decision tree models, it is often useful to use all available cores by setting
    `n_jobs=-1`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，作为一组森林而不是单个决策树，`RandomForestClassifier`有一些参数是随机森林特有的或特别重要的。首先，`max_features`参数确定在每个节点考虑的最大特征数，并接受多个参数，包括整数（特征数量）、浮点数（特征百分比）和`sqrt`（特征数量的平方根）。默认情况下，`max_features`设置为`auto`，与`sqrt`相同。其次，`bootstrap`参数允许我们设置是否使用替换创建考虑树的观测子集（默认设置）或不使用替换。第三，`n_estimators`设置森林中包含的决策树数量。最后，虽然不特定于随机森林分类器，因为我们实际上在训练许多决策树模型，通常通过设置`n_jobs=-1`来使用所有可用核心是很有用的。
- en: See Also
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Random Forests, Berkeley Statistics](https://oreil.ly/h-LQL)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[随机森林，伯克利统计学](https://oreil.ly/h-LQL)'
- en: 14.5 Training a Random Forest Regressor
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 14.5 训练随机森林回归器
- en: Problem
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to train a regression model using a “forest” of randomized decision
    trees.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望使用“森林”中的随机决策树来训练回归模型。
- en: Solution
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Train a random forest regression model using scikit-learn’s `RandomForestRegressor`:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用scikit-learn的`RandomForestRegressor`训练随机森林回归模型：
- en: '[PRE19]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Discussion
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Just as we can make a forest of decision tree classifiers, we can make a forest
    of decision tree regressors, where each tree uses a bootstrapped subset of observations
    and at each node the decision rule considers only a subset of features. As with
    `RandomForestClassifier` we have certain important parameters:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们可以制作一组决策树分类器的森林一样，我们也可以制作一组决策树回归器，其中每棵树都使用一个自举样本集合，并且在每个节点处，决策规则只考虑特征的一个子集。与`RandomForestClassifier`一样，我们有一些重要的参数：
- en: '`max_features`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_features`'
- en: Sets the maximum number of features to consider at each node. Defaults to *p*
    features, where *p* is the total number of features.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 设置在每个节点考虑的最大特征数的最大值。默认为*p*个特征，其中*p*是总特征数。
- en: '`bootstrap`'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`bootstrap`'
- en: Sets whether or not to sample with replacement. Defaults to `True`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 设置是否使用替换采样。默认为`True`。
- en: '`n_estimators`'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_estimators`'
- en: Sets the number of decision trees to construct. Defaults to `10`.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 设置要构建的决策树数量。默认为`10`。
- en: See Also
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[scikit-learn documentation: RandomForestRegressor](https://oreil.ly/ksa9Z)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[scikit-learn文档：RandomForestRegressor](https://oreil.ly/ksa9Z)'
- en: 14.6 Evaluating Random Forests with Out-of-Bag Errors
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to evaluate a random forest model without using cross-validation.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Calculate the model’s out-of-bag score:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Discussion
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In random forests, each decision tree is trained using a bootstrapped subset
    of observations. This means that for every tree there is a separate subset of
    observations not being used to train that tree. These are called out-of-bag (OOB)
    observations. We can use OOB observations as a test set to evaluate the performance
    of our random forest.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: For every observation, the learning algorithm compares the observation’s true
    value with the prediction from a subset of trees not trained using that observation.
    The overall score is calculated and provides a single measure of a random forest’s
    performance. OOB score estimation is an alternative to cross-validation.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: In scikit-learn, we can compute OOB scores of a random forest by setting `oob_score=True`
    in the random forest object (i.e., `RandomForestClassifier`). The score can be
    retrieved using `oob_score_`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 14.7 Identifying Important Features in Random Forests
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to know which features are most important in a random forest model.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Calculate and visualize the importance of each feature by inspecting the model’s
    `feature_importances_` attribute:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![mpc2 14in02](assets/mpc2_14in02.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: Discussion
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the major benefits of decision trees is interpretability. Specifically,
    we can visualize the entire model (see [Recipe 14.3](#visualizing-a-decision-tree-model)).
    However, a random forest model is composed of tens, hundreds, or even thousands
    of decision trees. This makes a simple, intuitive visualization of a random forest
    model impractical. That said, there is another option: we can compare (and visualize)
    the relative importance of each feature.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: In [Recipe 14.3](#visualizing-a-decision-tree-model), we visualized a decision
    tree classifier model and saw that decision rules based only on petal width were
    able to classify many observations correctly. Intuitively, we can say this means
    that petal width is an important feature in our classifier. More formally, features
    with splits that have the greater mean decrease in impurity (e.g., Gini impurity
    or entropy in classifiers and variance in regressors) are considered more important.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: However, there are two things to keep in mind regarding feature importance.
    First, scikit-learn requires that we break up nominal categorical features into
    multiple binary features. This has the effect of spreading the importance of that
    feature across all of the binary features and can make each feature appear to
    be unimportant even when the original nominal categorical feature is highly important.
    Second, if two features are highly correlated, one feature will claim much of
    the importance, making the other feature appear to be far less important, which
    has implications for interpretation if not considered.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'In scikit-learn, classification and regression decision trees and random forests
    can report the relative importance of each feature using the `feature_importances_`
    method:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中，分类和回归决策树以及随机森林可以使用 `feature_importances_` 方法报告每个特征的相对重要性：
- en: '[PRE23]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The higher the number, the more important the feature (all importance scores
    sum to 1). By plotting these values, we can add interpretability to our random
    forest models.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 数值越高，特征越重要（所有重要性得分总和为1）。通过绘制这些值，我们可以为我们的随机森林模型增加可解释性。
- en: 14.8 Selecting Important Features in Random Forests
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 14.8 在随机森林中选择重要特征
- en: Problem
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You need to conduct feature selection on a random forest.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要对随机森林进行特征选择。
- en: Solution
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Identify the importance features and retrain the model using only the most
    important features:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 确定重要特征并仅使用最重要的特征重新训练模型：
- en: '[PRE25]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Discussion
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: There are situations where we might want to reduce the number of features in
    our model. For example, we might want to reduce the model’s variance, or we might
    want to improve interpretability by including only the most important features.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 有些情况下，我们可能希望减少模型中特征的数量。例如，我们可能希望减少模型的方差，或者我们可能希望通过只包含最重要的特征来提高可解释性。
- en: In scikit-learn we can use a simple two-stage workflow to create a model with
    reduced features. First, we train a random forest model using all features. Then,
    we use this model to identify the most important features. Next, we create a new
    feature matrix that includes only these features. In our solution, we used the
    `SelectFromModel` method to create a feature matrix containing only features with
    an importance greater than or equal to some `threshold` value. Finally, we created
    a new model using only those features.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中，我们可以使用一个简单的两阶段工作流程来创建一个具有减少特征的模型。首先，我们使用所有特征训练一个随机森林模型。然后，我们使用这个模型来识别最重要的特征。接下来，我们创建一个只包含这些特征的新特征矩阵。在我们的解决方案中，我们使用
    `SelectFromModel` 方法创建一个包含重要性大于或等于某个 `threshold` 值的特征的特征矩阵。最后，我们使用这些特征创建一个新模型。
- en: We must note two caveats to this approach. First, nominal categorical features
    that have been one-hot encoded will see the feature importance diluted across
    the binary features. Second, the feature importance of highly correlated features
    will be effectively assigned to one feature and not evenly distributed across
    both features.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须注意这种方法的两个限制。首先，已经进行了一次独热编码的名义分类特征会导致特征重要性在二元特征中被稀释。其次，高度相关特征的特征重要性将有效地分配给一个特征，而不是均匀分布在两个特征之间。
- en: See Also
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '[Variable Selection Using Random Forests, Robin Genuer, Jean-Michel Poggi,
    and Christine Tuleau-Malot](https://oreil.ly/y9k2U)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用随机森林进行变量选择，Robin Genuer, Jean-Michel Poggi, 和 Christine Tuleau-Malot](https://oreil.ly/y9k2U)'
- en: 14.9 Handling Imbalanced Classes
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 14.9 处理不平衡类别
- en: Problem
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You have a target vector with highly imbalanced classes and want to train a
    random forest model.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 您有一个目标向量，其中包含高度不平衡的类别，并希望训练一个随机森林模型。
- en: Solution
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Train a decision tree or random forest model with `class_weight="balanced"`:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `class_weight="balanced"` 训练决策树或随机森林模型：
- en: '[PRE26]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Discussion
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Imbalanced classes are a common problem when we are doing machine learning
    in the real world. Left unaddressed, the presence of imbalanced classes can reduce
    the performance of our model. We will discuss handling imbalanced classes during
    preprocessing in [Recipe 17.5](ch17.xhtml#handling-imbalanced-classes-ch17). However,
    many learning algorithms in scikit-learn come with built-in methods for correcting
    for imbalanced classes. We can set `RandomForestClassifier` to correct for imbalanced
    classes using the `class_weight` parameter. If supplied with a dictionary in the
    form of class names and their desired weights (e.g., `{"male": 0.2, "female":
    0.8}`), `RandomForestClassifier` will weight the classes accordingly. However,
    often a more useful argument is `balanced`, wherein classes are automatically
    weighted inversely proportional to how frequently they appear in the data:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '在实际进行机器学习时，不平衡类别是一个常见问题。如果不加以解决，不平衡类别的存在会降低模型的性能。我们将在预处理过程中讨论如何处理不平衡类别 [Recipe
    17.5](ch17.xhtml#handling-imbalanced-classes-ch17)。然而，在 scikit-learn 中的许多学习算法都具有用于纠正不平衡类别的内置方法。我们可以使用
    `class_weight` 参数将 `RandomForestClassifier` 设置为纠正不平衡类别。如果提供了一个字典，其形式为类别名称和所需权重（例如
    `{"male": 0.2, "female": 0.8}`），`RandomForestClassifier` 将相应地加权类别。然而，更常见的参数是 `balanced`，其中类别的权重自动与其在数据中出现的频率成反比：'
- en: <math display="block"><mrow><msub><mi>w</mi> <mi>j</mi></msub> <mo>=</mo> <mfrac><mi>n</mi>
    <mrow><mi>k</mi><msub><mi>n</mi> <mi>j</mi></msub></mrow></mfrac></mrow></math>
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>w</mi> <mi>j</mi></msub> <mo>=</mo> <mfrac><mi>n</mi>
    <mrow><mi>k</mi><msub><mi>n</mi> <mi>j</mi></msub></mrow></mfrac></mrow></math>
- en: 'where <math display="inline"><msub><mi>w</mi><mi>j</mi></msub></math> is the
    weight of class <math display="inline"><mi>j</mi></math>, <math display="inline"><mi>n</mi></math>
    is the number of observations, <math display="inline"><msub><mi>n</mi><mi>j</mi></msub></math>
    is the number of observations in class <math display="inline"><mi>j</mi></math>,
    and <math display="inline"><mi>k</mi></math> is the total number of classes. For
    example, in our solution we have 2 classes (<math display="inline"><mi>k</mi></math>),
    110 observations (<math display="inline"><mi>n</mi></math>), and 10 and 100 observations
    in each class, respectively (<math display="inline"><msub><mi>n</mi><mi>j</mi></msub></math>).
    If we weight the classes using `class_weight="balanced"`, then the smaller class
    is weighted more:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math display="inline"><msub><mi>w</mi><mi>j</mi></msub></math>是类<math display="inline"><mi>j</mi></math>的权重，<math
    display="inline"><mi>n</mi></math>是观察次数，<math display="inline"><msub><mi>n</mi><mi>j</mi></msub></math>是类<math
    display="inline"><mi>j</mi></math>中的观察次数，<math display="inline"><mi>k</mi></math>是总类数。例如，在我们的解决方案中，有2个类别（<math
    display="inline"><mi>k</mi></math>），110次观察（<math display="inline"><mi>n</mi></math>），分别有10和100次观察在每个类中（<math
    display="inline"><msub><mi>n</mi><mi>j</mi></msub></math>）。如果我们使用`class_weight="balanced"`来加权类别，则较小的类别被加权更多：
- en: '[PRE27]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'while the larger class is weighted less:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 而较大的类别被加权更少：
- en: '[PRE29]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 14.10 Controlling Tree Size
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 14.10 控制树的大小
- en: Problem
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to manually determine the structure and size of a decision tree.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望手动确定决策树的结构和大小。
- en: Solution
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use the tree structure parameters in scikit-learn tree-based learning algorithms:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn基于树的学习算法中使用树结构参数：
- en: '[PRE31]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Discussion
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'scikit-learn’s tree-based learning algorithms have a variety of techniques
    for controlling the size of decision trees. These are accessed through parameters:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn的基于树的学习算法有多种控制决策树大小的技术。这些通过参数访问：
- en: '`max_depth`'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_depth`'
- en: Maximum depth of the tree. If `None`, the tree is grown until all leaves are
    pure. If an integer, the tree is effectively “pruned” to that depth.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 树的最大深度。如果为`None`，则树会生长直到所有叶子节点都是纯净的。如果是整数，则树被“修剪”到该深度。
- en: '`min_samples_split`'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`min_samples_split`'
- en: Minimum number of observations at a node before that node is split. If an integer
    is supplied as an argument, it determines the raw minimum, while if a float is
    supplied, the minimum is the percent of total observations.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个节点上分裂之前所需的最小观察次数。如果提供整数作为参数，则确定原始最小值，如果提供浮点数，则最小值是总观察次数的百分比。
- en: '`min_samples_leaf`'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`min_samples_leaf`'
- en: Minimum number of observations required to be at a leaf. Uses the same arguments
    as `min_samples_split`.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在叶子节点上所需的最小观察次数。与`min_samples_split`相同的参数。
- en: '`max_leaf_nodes`'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_leaf_nodes`'
- en: Maximum number of leaves.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 最大叶子节点数。
- en: '`min_impurity_split`'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`min_impurity_split`'
- en: Minimum impurity decrease required before a split is performed.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行分割之前所需的最小不纯度减少。
- en: While it is useful to know these parameters exist, most likely we will only
    be using `max_depth` and `min_impurity_split` because shallower trees (sometimes
    called *stumps*) are simpler models and thus have lower variance.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然知道这些参数存在是有用的，但最可能我们只会使用`max_depth`和`min_impurity_split`，因为更浅的树（有时称为*树桩*）是更简单的模型，因此具有较低的方差。
- en: 14.11 Improving Performance Through Boosting
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 14.11 通过增强提高性能
- en: Problem
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You need a model with better performance than decision trees or random forests.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要比决策树或随机森林性能更好的模型。
- en: Solution
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Train a boosted model using `AdaBoostClassifier` or `AdaBoostRegressor`:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`AdaBoostClassifier`或`AdaBoostRegressor`训练增强模型：
- en: '[PRE32]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Discussion
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'In a random forest, an ensemble (group) of randomized decision trees predicts
    the target vector. An alternative, and often more powerful, approach is called
    *boosting*. In one form of boosting called AdaBoost, we iteratively train a series
    of weak models (most often a shallow decision tree, sometimes called a stump),
    each iteration giving higher priority to observations the previous model predicted
    incorrectly. More specifically, in AdaBoost:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机森林中，随机化决策树的集合预测目标向量。另一种常用且通常更强大的方法称为*增强*。在增强的一种形式中称为AdaBoost，我们迭代地训练一系列弱模型（通常是浅决策树，有时称为树桩），每次迭代都优先考虑前一个模型预测错误的观察结果。更具体地，在AdaBoost中：
- en: Assign every observation, <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math>,
    an initial weight value, <math display="inline"><mrow><msub><mi>w</mi> <mi>i</mi></msub>
    <mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac></mrow></math>, where <math display="inline"><mi>n</mi></math>
    is the total number of observations in the data.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个观察值 <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math> 分配初始权重值
    <math display="inline"><mrow><msub><mi>w</mi> <mi>i</mi></msub> <mo>=</mo> <mfrac><mn>1</mn>
    <mi>n</mi></mfrac></mrow></math>，其中 <math display="inline"><mi>n</mi></math> 是数据中观察总数。
- en: Train a “weak” model on the data.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在数据上训练一个“弱”模型。
- en: 'For each observation:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个观察：
- en: If weak model predicts <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math>
    correctly, <math display="inline"><msub><mi>w</mi><mi>i</mi></msub></math> is
    decreased.
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果弱模型正确预测 <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math>，则
    <math display="inline"><msub><mi>w</mi><mi>i</mi></msub></math> 减少。
- en: If weak model predicts <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math>
    incorrectly, <math display="inline"><msub><mi>w</mi><mi>i</mi></msub></math> is
    increased.
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果弱模型错误预测 <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math>，则
    <math display="inline"><msub><mi>w</mi><mi>i</mi></msub></math> 增加。
- en: Train a new weak model where observations with greater <math display="inline"><msub><mi>w</mi><mi>i</mi></msub></math>
    are given greater priority.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个新的弱模型，其中具有较大 <math display="inline"><msub><mi>w</mi><mi>i</mi></msub></math>
    的观察值优先考虑。
- en: Repeat steps 4 and 5 until the data is perfectly predicted or a preset number
    of weak models has been trained.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤4和5，直到数据完全预测或训练了预设数量的弱模型。
- en: 'The result is an aggregated model where individual weak models focus on more
    difficult (from a prediction perspective) observations. In scikit-learn, we can
    implement AdaBoost using `AdaBoostClassifier` or `AdaBoostRegressor`. The most
    important parameters are `base_estimator`, `n_estimators`, `learning_rate`, and
    `loss`:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个聚合模型，个体弱模型专注于更难（从预测角度）的观察。在scikit-learn中，我们可以使用 `AdaBoostClassifier` 或
    `AdaBoostRegressor` 实现AdaBoost。最重要的参数是 `base_estimator`、`n_estimators`、`learning_rate`
    和 `loss`：
- en: '`base_estimator`'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`base_estimator`'
- en: '`base_estimator` is the learning algorithm to use to train the weak models.
    The most common learner to use with AdaBoost is a decision tree, the parameter’s
    default argument.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '`base_estimator` 是用于训练弱模型的学习算法。与AdaBoost一起使用的最常见的学习器是决策树，默认参数。'
- en: '`n_estimators`'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_estimators`'
- en: '`n_estimators` is the number of models to iteratively train.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_estimators` 是要迭代训练的模型数量。'
- en: '`learning_rate`'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`learning_rate`'
- en: '`learning_rate` is the contribution of each model to the weights, and it defaults
    to `1`. Reducing the learning rate will mean the weights will be increased or
    decreased to a small degree, forcing the model to train slower (but sometimes
    resulting in better performance scores).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`learning_rate` 是每个模型对权重的贡献，默认为 `1`。减小学习率意味着权重将略微增加或减少，迫使模型训练速度变慢（但有时会导致更好的性能分数）。'
- en: '`loss`'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '`loss`'
- en: '`loss` is exclusive to `AdaBoostRegressor` and sets the loss function to use
    when updating weights. This defaults to a linear loss function but can be changed
    to `square` or `exponential`.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`loss` 仅适用于 `AdaBoostRegressor`，设置更新权重时使用的损失函数。默认为线性损失函数，但可以更改为 `square` 或
    `exponential`。'
- en: See Also
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Explaining AdaBoost, Robert E. Schapire](https://oreil.ly/5E1v4)'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[解释AdaBoost, Robert E. Schapire](https://oreil.ly/5E1v4)'
- en: 14.12 Training an XGBoost Model
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 14.12 训练XGBoost模型
- en: Problem
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You need to train a tree-based model with high predictive power.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要训练具有高预测能力的基于树的模型。
- en: Solution
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use the `xgboost` Python library:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `xgboost` Python库：
- en: '[PRE33]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Discussion
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: XGBoost (which stands for Extreme Gradient Boosting) is a very popular gradient
    boosting algorithm in the machine learning space. Though it is not always a tree-based
    model, it is frequently applied to ensembles of decision trees. It gained much
    of its popularity due to widespread success on the machine learning competition
    website Kaggle and has since been a reliable algorithm for improving performance
    beyond that of typical random forests or gradient boosted machines.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost（即极端梯度提升）是机器学习领域中非常流行的梯度提升算法。尽管它不总是基于树的模型，但经常应用于决策树集成中。由于在机器学习竞赛网站Kaggle上取得了广泛成功，XGBoost因其卓越的性能提升而备受青睐，已成为提高性能的可靠算法，超越了典型随机森林或梯度增强机器的性能。
- en: Although XGBoost is known for being computationally intensive, computational
    performance optimizations (such as GPU support) over the last few years have made
    iterating quickly with XGBoost significantly easier, and it remains a common choice
    of algorithm when statistical performance is a requirement.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 XGBoost 因计算密集而闻名，但过去几年的计算性能优化（例如 GPU 支持）显著简化了与 XGBoost 的快速迭代，它仍然是在统计性能要求时的常见选择算法。
- en: See Also
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '[XGBoost documentation](https://oreil.ly/cAuGX)'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XGBoost 文档](https://oreil.ly/cAuGX)'
- en: 14.13 Improving Real-Time Performance with LightGBM
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 14.13 使用 LightGBM 提升实时性能
- en: Problem
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You need to train a gradient boosted tree-based model that is computationally
    optimized.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要训练一个在计算上优化的基于梯度提升树的模型。
- en: Solution
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use the gradient boosted machine library `lightgbm`:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 使用梯度提升机库 `lightgbm`：
- en: '[PRE35]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Discussion
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: The `lightgbm` library is used for gradient boosted machines and is highly optimized
    for training time, inference, and GPU support. As a result of its computational
    efficiency, it’s often used in production and in large scale settings. Although
    scikit-learn models are typically easier to use, some libraries, such as `lightgbm`,
    can be handy when you’re limited by large data or strict model training/serving
    times.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '`lightgbm` 库用于梯度提升机，在训练时间、推断和 GPU 支持方面都经过高度优化。由于其计算效率高，常用于生产环境和大规模设置中。尽管 scikit-learn
    模型通常更易于使用，但某些库如 `lightgbm` 在受限于大数据或严格的模型训练/服务时间时会更实用。'
- en: See Also
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '[LightGBM documentation](https://oreil.ly/XDcpG)'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LightGBM 文档](https://oreil.ly/XDcpG)'
- en: '[CatBoost documentation (another optimized library for GBMs)](https://oreil.ly/4Bb8g)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CatBoost 文档（另一个优化的梯度提升库）](https://oreil.ly/4Bb8g)'
