- en: 'Chapter 3\. Classification Walkthrough: Titanic Dataset'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will walk through a common classification problem using the [Titanic
    dataset](https://oreil.ly/PjceO). Later chapters will dive into and expand on
    the common steps performed during an analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Project Layout Suggestion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An excellent tool for performing exploratory data analysis is [Jupyter](https://jupyter.org).
    Jupyter is an open-source notebook environment that supports Python and other
    languages. It allows you to create *cells* of code or Markdown content.
  prefs: []
  type: TYPE_NORMAL
- en: I tend to use Jupyter in two modes. One is for exploratory data analysis and
    quickly trying things out. The other is more of a deliverable style where I format
    a report using Markdown cells and insert code cells to illustrate important points
    or discoveries. If you aren’t careful, your notebooks might need some refactoring
    and application of software engineering practices (remove globals, use functions
    and classes, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: The [cookiecutter data science package](https://oreil.ly/86jL3) suggests a layout
    to create an analysis that allows for easy reproduction and sharing code.
  prefs: []
  type: TYPE_NORMAL
- en: Imports
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This example is based mostly on [pandas](http://pandas.pydata.org/), [scikit-learn](https://scikit-learn.org/),
    and [Yellowbrick](http://www.scikit-yb.org/). The pandas library gives us tooling
    for easy data munging. The scikit-learn library has great predictive modeling,
    and Yellowbrick is a visualization library for evaluating models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You might find documentation and examples online that include star imports
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Refrain from using star imports. Being explicit makes your code easier to understand.
  prefs: []
  type: TYPE_NORMAL
- en: Ask a Question
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we want to create a predictive model to answer a question.
    It will classify whether an individual survives the Titanic ship catastrophe based
    on individual and trip characteristics. This is a toy example, but it serves as
    a pedagogical tool for showing many steps of modeling. Our model should be able
    to take passenger information and predict whether that passenger would survive
    on the Titanic.
  prefs: []
  type: TYPE_NORMAL
- en: This is a classification question, as we are predicting a label for survival;
    either they survived or they died.
  prefs: []
  type: TYPE_NORMAL
- en: Terms for Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We typically train a model with a matrix of data. (I prefer to use pandas DataFrames
    because it is very nice to have column labels, but numpy arrays work as well.)
  prefs: []
  type: TYPE_NORMAL
- en: 'For supervised learning, such as regression or classification, our intent is
    to have a fuction that transforms features into a label. If we were to write this
    as an algebra formula, it would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: X is a matrix. Each row represents a *sample* of data or information about an
    individual. Every column in X is a *feature*. The output of our function, y, is
    a vector that contains labels (for classification) or values (for regression)
    (see [Figure 3-1](#idx1)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Structured data layout.](assets/mlpr_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. Structured data layout.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is standard naming procedure for naming the data and the output. If you
    read academic papers or even look at the documentation for libraries, they follow
    this convention. In Python, we use the variable name `X` to hold the sample data
    even though capitalization of variables is a violation of standard naming conventions
    (PEP 8). Don’t worry, everyone does it, and if you were to name your variable
    `x`, they might look at you funny. The variable `y` stores the labels or targets.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 3-1](#table_3_1) shows a basic dataset with two samples and three features
    for each sample.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-1\. Samples (rows) and features (columns)
  prefs: []
  type: TYPE_NORMAL
- en: '| pclass | age | sibsp |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 29 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2 | 1 |'
  prefs: []
  type: TYPE_TB
- en: Gather Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are going to load an Excel file (make sure you have pandas and xlrd^([1](ch03.html#idm46066905711928))
    installed) with the Titanic features. It has many columns, including a survived
    column that contains the label of what happened to an individual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following columns are included in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: pclass - Passenger class (1 = 1st, 2 = 2nd, 3 = 3rd)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: survival - Survival (0 = No, 1 = Yes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: name - Name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sex - Sex
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: age - Age
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sibsp - Number of siblings/spouses aboard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: parch - Number of parents/children aboard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ticket - Ticket number
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: fare - Passenger fare
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cabin - Cabin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: embarked - Point of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: boat - Lifeboat
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: body - Body identification number
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: home.dest - Home/destination
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pandas can read this spreadsheet and convert it into a DataFrame for us. We
    will need to spot-check the data and ensure that it is OK for performing analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Clean Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we have the data, we need to ensure that it is in a format that we can
    use to create a model. Most scikit-learn models require that our features be numeric
    (integer or float). In addition, many models fail if they are passed missing values
    (`NaN` in pandas or numpy). Some models perform better if the data is *standardized*
    (given a mean value of 0 and a standard deviation of 1). We will deal with these
    issues using pandas or scikit-learn. In addition, the Titanic dataset has *leaky*
    features.
  prefs: []
  type: TYPE_NORMAL
- en: Leaky features are variables that contain information about the future or target.
    There’s nothing bad in having data about the target, and we often have that data
    during model creation time. However, if those variables are not available when
    we perform a prediction on a new sample, we should remove them from the model
    as they are leaking data from the future.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning the data can take a bit of time. It helps to have access to a subject
    matter expert (SME) who can provide guidance on dealing with outliers or missing
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We typically see `int64`, `float64`, `datetime64[ns]`, or `object`. These are
    the types that pandas uses to store a column of data. `int64` and `float64` are
    numeric types. `datetime64[ns]` holds date and time data. `object` typically means
    that it is holding string data, though it could be a combination of string and
    other types.
  prefs: []
  type: TYPE_NORMAL
- en: When reading from CSV files, pandas will try to coerce data into the appropriate
    type, but will fall back to `object`. Reading data from spreadsheets, databases,
    or other systems may provide better types in the DataFrame. In any case, it is
    worthwhile to look through the data and ensure that the types make sense.
  prefs: []
  type: TYPE_NORMAL
- en: Integer types are typically fine. Float types might have some missing values.
    Date and string types will need to be converted or used to feature engineer numeric
    types. String types that have low cardinality are called categorical columns,
    and it might be worthwhile to create dummy columns from them (the `pd.get_dummies`
    function takes care of this).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Up to pandas 0.23, if the type is `int64`, we are guaranteed that there are
    no missing values. If the type is `float64`, the values might be all floats, but
    also could be integer-like numbers with missing values. The pandas library converts
    integer values that have missing numbers to floats, as this type supports missing
    values. The `object` typically means string types (or both string and numeric).
  prefs: []
  type: TYPE_NORMAL
- en: As of pandas 0.24, there is a new `Int64` type (notice the capitalization).
    This is not the default integer type, but you can coerce to this type and have
    support for missing numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pandas-profiling library includes a profile report. You can generate this
    report in a notebook. It will summarize the types of the columns and allow you
    to view details of quantile statistics, descriptive statistics, a histogram, common
    values, and extreme values (see Figures [3-2](#pp1) and [3-3](#pp2)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![Pandas-profiling summary.](assets/mlpr_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. Pandas-profiling summary.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Pandas-profiling variable details.](assets/mlpr_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. Pandas-profiling variable details.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Use the `.shape` attribute of the DataFrame to inspect the number of rows and
    columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `.describe` method to get summary stats as well as see the count of
    nonnull data. The default behavior of this method is to only report on numeric
    columns. Here the output is truncated to only show the first two columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The count statistic only includes values that are not NaN, so it is useful for
    checking whether a column is missing data. It is also a good idea to spot-check
    the minimum and maximum values to see if there are outliers. Summary statistics
    are one way to do this. Plotting a histogram or a box plot is a visual representation
    that we will see later.
  prefs: []
  type: TYPE_NORMAL
- en: We will need to deal with missing data. Use the `.isnull` method to find columns
    or rows with missing values. Calling `.isnull` on a DataFrame returns a new DataFrame
    with every cell containing a `True` or `False` value. In Python, these values
    evaluate to `1` and `0`, respectively. This allows us to sum them up or even calculate
    the percent missing (by calculating the mean).
  prefs: []
  type: TYPE_NORMAL
- en: 'The code indicates the count of missing data in each column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Replace `.sum` with `.mean` to get the percentage of null values. By default,
    calling these methods will apply the operation along axis 0, which is along the
    index. If you want to get the counts of missing features for each sample, you
    can apply this along axis 1 (along the columns):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: A SME can help in determining what to do with missing data. The age column might
    be useful, so keeping it and interpolating values could provide some signal to
    the model. Columns where most of the values are missing (cabin, boat, and body)
    tend to not provide value and can be dropped.
  prefs: []
  type: TYPE_NORMAL
- en: The body column (body identification number) is missing for many rows. We should
    drop this column at any rate because it leaks data. This column indicates that
    the passenger did not survive; by necessity our model could use that to cheat.
    We will pull it out. (If we are creating a model to predict if a passenger would
    die, knowing that they had a body identification number a priori would let us
    know they were already dead. We want our model to not know that information and
    make the prediction based on the other columns.) Likewise, the boat column leaks
    the reverse information (that a passenger survived).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at some of the rows with missing data. We can create a boolean array
    (a series with `True` or `False` to indicate if the row has missing data) and
    use it to inspect rows that are missing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We will impute (or derive values for) the missing values for the age column
    later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Columns with type of `object` tend to be categorical (but they may also be
    high cardinality string data, or a mix of column types). For `object` columns
    that we believe to be categorical, use the `.value_counts` method to examine the
    counts of the values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember that pandas typically ignores null or NaN values. If you want to include
    those, use `dropna=False` to also show counts for NaN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We have a couple of options for dealing with missing embarked values. Using
    S might seem logical as that is the most common value. We could dig into the data
    and try and determine if another option is better. We could also drop those two
    values. Or, because this is categorical, we can ignore them and use pandas to
    create dummy columns if these two samples will just have 0 entries for every option.
    We will use this latter choice for this feature.
  prefs: []
  type: TYPE_NORMAL
- en: Create Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can drop columns that have no variance or no signal. There aren’t features
    like that in this dataset, but if there was a column called “is human” that had
    1 for every sample this column would not be providing any information.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, unless we are using NLP or extracting data out of text columns
    where every value is different, a model will not be able to take advantage of
    this column. The name column is an example of this. Some have pulled out the title
    t from the name and treated it as categorical.
  prefs: []
  type: TYPE_NORMAL
- en: We also want to drop columns that leak information. Both boat and body columns
    leak whether a passenger survived.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pandas `.drop` method can drop either rows or columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to create dummy columns from string columns. This will create new columns
    for sex and embarked. Pandas has a convenient `get_dummies` function for that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point the sex_male and sex_female columns are perfectly inverse correlated.
    Typically we remove any columns with perfect or very high positive or negative
    correlation. Multicollinearity can impact interpretation of feature importance
    and coefficients in some models. Here is code to remove the sex_male column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can add a `drop_first=True` parameter to the `get_dummies`
    call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a DataFrame (`X`) with the features and a series (`y`) with the labels.
    We could also use numpy arrays, but then we don’t have column names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can use the [pyjanitor library](https://oreil.ly/_IWbA) to replace the last
    two lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Sample Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We always want to train and test on different data. Otherwise you don’t really
    know how well your model generalizes to data that it hasn’t seen before. We’ll
    use scikit-learn to pull out 30% for testing (using `random_state=42` to remove
    an element of randomness if we start comparing different models):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Impute Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The age column has missing values. We need to impute age from the numeric values.
    We only want to impute on the training set and then use that imputer to fill in
    the date for the test set. Otherwise we are leaking data (cheating by giving future
    information to the model).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have test and train data, we can impute missing values on the training
    set, and use the trained imputers to fill in the test dataset. The [fancyimpute
    library](https://oreil.ly/Vlf9e) has many algorithms that it implements. Sadly,
    most of these algorithms are not implemented in an *inductive* manner. This means
    that you cannot call `.fit` and then `.transform`, which means you cannot impute
    for new data based on how the model was trained.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `IterativeImputer` class (which was in fancyimpute but has been migrated
    to scikit-learn) does support inductive mode. To use it we need to add a special
    experimental import (as of scikit-learn version 0.21.2):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'If we wanted to impute with the median, we can use pandas to do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Normalize Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Normalizing or preprocessing the data will help many models perform better after
    this is done. Particularly those that depend on a distance metric to determine
    similarity. (Note that tree models, which treat each feature on its own, don’t
    have this requirement.)
  prefs: []
  type: TYPE_NORMAL
- en: We are going to standardize the data for the preprocessing. Standardizing is
    translating the data so that it has a mean value of zero and a standard deviation
    of one. This way models don’t treat variables with larger scales as more important
    than smaller scaled variables. I’m going to stick the result (numpy array) back
    into a pandas DataFrame for easier manipulation (and to keep column names).
  prefs: []
  type: TYPE_NORMAL
- en: 'I also normally don’t standardize dummy columns, so I will ignore those:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Refactor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this point I like to refactor my code. I typically make two functions. One
    for general cleaning, and another for dividing up into a training and testing
    set and to perform mutations that need to happen differently on those sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Baseline Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating a baseline model that does something really simple can give us something
    to compare our model to. Note that using the default `.score` result gives us
    the accuracy which can be misleading. A problem where a positive case is 1 in
    10,000 can easily get over 99% accuracy by always predicting negative.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Various Families
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This code tries a variety of algorithm families. The “No Free Lunch” theorem
    states that no algorithm performs well on all data. However, for some finite set
    of data, there may be an algorithm that does well on that set. (A popular choice
    for structured learning these days is a tree-boosted algorithm such as XGBoost.)
  prefs: []
  type: TYPE_NORMAL
- en: Here we use a few different families and compare the AUC score and standard
    deviation using k-fold cross-validation. An algorithm that has a slightly smaller
    average score but tighter standard deviation might be a better choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we are using k-fold cross-validation, we will feed the model all of
    `X` and `y`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Stacking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you were going down the Kaggle route (or want maximum performance at the
    cost of interpretability), *stacking* is an option. A stacking classifier takes
    other models and uses their output to predict a target or label. We will use the
    previous models’ outputs and combine them to see if a stacking classifier can
    do better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In this case it looks like performance went down a bit, as well as standard
    deviation.
  prefs: []
  type: TYPE_NORMAL
- en: Create Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I’m going to use a random forest classifier to create a model. It is a flexible
    model that tends to give decent out-of-the-box results. Remember to train it (calling
    `.fit`) with the training data from the data that we split earlier into a training
    and testing set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Evaluate Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have a model, we can use the test data to see how well the model
    generalizes to data that it hasn’t seen before. The `.score` method of a classifier
    returns the average of the prediction accuracy. We want to make sure that we call
    the `.score` method with the test data (presumably it should perform better with
    the training data):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also look at other metrics, such as precision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'A nice benefit of tree-based models is that you can inspect the feature importance.
    The feature importance tells you how much a feature contributes to the model.
    Note that removing a feature doesn’t mean that the score will go down accordingly,
    as other features might be colinear (in this case we could remove either the sex_male
    or sex_female column as they have a perfect negative correlation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The feature importance is calculated by looking at the error increase. If removing
    a feature increases the error in the model, the feature is more important.
  prefs: []
  type: TYPE_NORMAL
- en: I really like the SHAP library for exploring what features a model deems important,
    and for explaining predictions. This library works with black-box models, and
    we will show it later.
  prefs: []
  type: TYPE_NORMAL
- en: Optimize Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Models have *hyperparameters* that control how they behave. By varying the
    values for these parameters, we change their performance. Sklearn has a grid search
    class to evaluate a model with different combinations of parameters and return
    the best result. We can use those parameters to instantiate the model class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We can pass in a `scoring` parameter to `GridSearchCV` to optimize for different
    metrics. See [Chapter 12](ch12.html#metrics1) for a list of metrics and their
    meanings.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion Matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A confusion matrix allows us to see the correct classifications as well as
    false positives and false negatives. It may be that we want to optimize toward
    false positives or false negatives, and different models or parameters can alter
    that. We can use sklearn to get a text version, or Yellowbrick for a plot (see
    [Figure 3-4](#id0)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '![Yellowbrick confusion matrix. This is a useful evaluation tool that presents
    the predicted class along the bottom and the true class along the side. A good
    classifier would have all of the values along the diagonal, and zeros in the other
    cells.](assets/mlpr_0304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-4\. Yellowbrick confusion matrix. This is a useful evaluation tool
    that presents the predicted class along the bottom and the true class along the
    side. A good classifier would have all of the values along the diagonal, and zeros
    in the other cells.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: ROC Curve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A receiver operating characteristic (ROC) plot is a common tool used to evaluate
    classifiers. By measuring the area under the curve (AUC), we can get a metric
    to compare different classifiers (see [Figure 3-5](#id0a)). It plots the true
    positive rate against the false positive rate. We can use sklearn to calculate
    the AUC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Or Yellowbrick to visualize the plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '![ROC curve. This shows the true positive rate against the false positive rate.
    In general, the further it bulges out the better. Measuring the AUC gives a single
    number to evaluate. Closer to one is better. Below .5 is a poor model.](assets/mlpr_0305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-5\. ROC curve. This shows the true positive rate against the false
    positive rate. In general, the further it bulges out the better. Measuring the
    AUC gives a single number to evaluate. Closer to one is better. Below .5 is a
    poor model.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Learning Curve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A learning curve is used to tell us if we have enough training data. It trains
    the model with increasing portions of the data and measures the score (see [Figure 3-6](#id1a)).
    If the cross-validation score continues to climb, then we might need to invest
    in gathering more data. Here is a Yellowbrick example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![This learning curve shows that as we add more training samples, our cross-validation
    (testing) scores appear to improve.](assets/mlpr_0306.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-6\. This learning curve shows that as we add more training samples,
    our cross-validation (testing) scores appear to improve.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deploy Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using Python’s `pickle` module, we can persist models and load them. Once we
    have a model, we call the `.predict` method to get a classification or regression
    result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Using [Flask](https://palletsprojects.com/p/flask) to deploy a web service for
    prediction is very common. There are now other commercial and open source products
    coming out that support deployment. Among them are [Clipper](http://clipper.ai/),
    [Pipeline](https://oreil.ly/UfHdP), and [Google’s Cloud Machine Learning Engine](https://oreil.ly/1qYkH).
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch03.html#idm46066905711928-marker)) Even though we don’t directly call
    this library, when we load an Excel file, pandas leverages it behind the scenes.
  prefs: []
  type: TYPE_NORMAL
