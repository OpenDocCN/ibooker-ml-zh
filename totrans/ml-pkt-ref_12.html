<html><head></head><body><section data-pdf-bookmark="Chapter 12. Metrics and Classification Evaluation" data-type="chapter" epub:type="chapter"><div class="chapter" id="metrics1">&#13;
<h1><span class="label">Chapter 12. </span>Metrics and Classification Evaluation</h1>&#13;
&#13;
&#13;
<p><a data-primary="classification evaluation" data-type="indexterm" id="ix_ch12-asciidoc0"/>We’ll cover the following metrics and evaluation tools in this chapter:&#13;
confusion matrices, various metrics, a classification report, and some&#13;
visualizations.</p>&#13;
&#13;
<p>This will be evaluated as a decision tree model that predicts Titanic survival.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Confusion Matrix" data-type="sect1"><div class="sect1" id="idm46066892451144">&#13;
<h1>Confusion Matrix</h1>&#13;
&#13;
<p><a data-primary="binary classifiers" data-secondary="and confusion matrix" data-type="indexterm" id="ix_ch12-asciidoc1"/><a data-primary="classification evaluation" data-secondary="confusion matrix" data-type="indexterm" id="ix_ch12-asciidoc2"/><a data-primary="confusion matrix" data-type="indexterm" id="ix_ch12-asciidoc3"/>A confusion matrix can aid in understanding how a classifier performs.</p>&#13;
&#13;
<p><a data-primary="binary classifiers" data-secondary="possible classification results" data-type="indexterm" id="idm46066892445784"/>A binary classifier can have four classification results: true positives&#13;
(TP), true negatives (TN), false positives (FP), and false negatives&#13;
(FN). The first two are correct classifications.</p>&#13;
&#13;
<p>Here is a common example for&#13;
remembering the other results. <a data-primary="false positives" data-type="indexterm" id="idm46066892443848"/>Assuming positive means pregnant and negative is&#13;
not pregnant, a false positive is like claiming a man is&#13;
pregnant. <a data-primary="false negatives" data-type="indexterm" id="idm46066892442888"/>A false negative is claiming that a pregnant woman is not&#13;
(when she is clearly showing) (see <a data-type="xref" href="#iderr3">Figure 12-1</a>). <a data-primary="type 1/type 2 errors" data-type="indexterm" id="idm46066892441176"/>These last two types of errors are&#13;
referred to as <em>type 1</em> and <em>type 2</em> errors, respectively (see <a data-type="xref" href="#table_12_1">Table 12-1</a>).</p>&#13;
&#13;
<p>Another way to&#13;
remember these is that P (for false positive) has one straight line in&#13;
it (type 1 error), and N (for false negative) has two vertical lines in it.</p>&#13;
&#13;
<figure><div class="figure" id="iderr3">&#13;
<img alt="Classification errors." src="assets/mlpr_1201.png"/>&#13;
<h6><span class="label">Figure 12-1. </span>Classification errors.</h6>&#13;
</div></figure>&#13;
<table id="table_12_1">&#13;
<caption class="width-full"><span class="label">Table 12-1. </span>Binary classification results from a confusion matrix</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Actual</th>&#13;
<th>Predicted negative</th>&#13;
<th>Predicted positive</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>Actual negative</p></td>&#13;
<td><p>True negative</p></td>&#13;
<td><p>False positive (type 1)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Actual positive</p></td>&#13;
<td><p>False negative (type 2)</p></td>&#13;
<td><p>True positive</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p><a data-primary="pandas" data-secondary="classification calculations" data-type="indexterm" id="idm46066892427480"/>Here is the pandas code to calculate the classification results.&#13;
The comments show the results. We will use these variables to&#13;
calculate other metrics:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">y_predict</code> <code class="o">=</code> <code class="n">dt</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">tp</code> <code class="o">=</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="p">(</code><code class="n">y_test</code> <code class="o">==</code> <code class="mi">1</code><code class="p">)</code> <code class="o">&amp;</code> <code class="p">(</code><code class="n">y_test</code> <code class="o">==</code> <code class="n">y_predict</code><code class="p">)</code>&#13;
<code class="gp">... </code><code class="p">)</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code>  <code class="c"># 123</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">tn</code> <code class="o">=</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="p">(</code><code class="n">y_test</code> <code class="o">==</code> <code class="mi">0</code><code class="p">)</code> <code class="o">&amp;</code> <code class="p">(</code><code class="n">y_test</code> <code class="o">==</code> <code class="n">y_predict</code><code class="p">)</code>&#13;
<code class="gp">... </code><code class="p">)</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code>  <code class="c"># 199</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fp</code> <code class="o">=</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="p">(</code><code class="n">y_test</code> <code class="o">==</code> <code class="mi">0</code><code class="p">)</code> <code class="o">&amp;</code> <code class="p">(</code><code class="n">y_test</code> <code class="o">!=</code> <code class="n">y_predict</code><code class="p">)</code>&#13;
<code class="gp">... </code><code class="p">)</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code>  <code class="c"># 25</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fn</code> <code class="o">=</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="p">(</code><code class="n">y_test</code> <code class="o">==</code> <code class="mi">1</code><code class="p">)</code> <code class="o">&amp;</code> <code class="p">(</code><code class="n">y_test</code> <code class="o">!=</code> <code class="n">y_predict</code><code class="p">)</code>&#13;
<code class="gp">... </code><code class="p">)</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code>  <code class="c"># 46</code></pre>&#13;
&#13;
<p>Well-behaving classifiers ideally have high counts in the true diagonal.&#13;
<a data-primary="sklearn" data-secondary="DataFrame from confusion_matrix function" data-type="indexterm" id="idm46066892424024"/>We can create a DataFrame using the sklearn <code>confusion_matrix</code> function:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">confusion_matrix</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_predict</code> <code class="o">=</code> <code class="n">dt</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">confusion_matrix</code><code class="p">(</code><code class="n">y_test</code><code class="p">,</code> <code class="n">y_predict</code><code class="p">),</code>&#13;
<code class="gp">... </code>    <code class="n">columns</code><code class="o">=</code><code class="p">[</code>&#13;
<code class="gp">... </code>        <code class="s">"Predict died"</code><code class="p">,</code>&#13;
<code class="gp">... </code>        <code class="s">"Predict Survive"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="p">],</code>&#13;
<code class="gp">... </code>    <code class="n">index</code><code class="o">=</code><code class="p">[</code><code class="s">"True Death"</code><code class="p">,</code> <code class="s">"True Survive"</code><code class="p">],</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="go">              Predict died  Predict Survive</code>&#13;
<code class="go">True Death             199               25</code>&#13;
<code class="go">True Survive            46              123</code></pre>&#13;
&#13;
<p><a data-primary="Yellowbrick" data-secondary="confusion matrix" data-type="indexterm" id="idm46066892266680"/>Yellowbrick has a plot for the confusion matrix (see <a data-type="xref" href="#id30">Figure 12-2</a>):<a data-startref="ix_ch12-asciidoc3" data-type="indexterm" id="idm46066892136280"/><a data-startref="ix_ch12-asciidoc2" data-type="indexterm" id="idm46066892135576"/><a data-startref="ix_ch12-asciidoc1" data-type="indexterm" id="idm46066892134904"/></p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="kn">as</code> <code class="nn">plt</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">yellowbrick.classifier</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">ConfusionMatrix</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">mapping</code> <code class="o">=</code> <code class="p">{</code><code class="mi">0</code><code class="p">:</code> <code class="s">"died"</code><code class="p">,</code> <code class="mi">1</code><code class="p">:</code> <code class="s">"survived"</code><code class="p">}</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">6</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">cm_viz</code> <code class="o">=</code> <code class="n">ConfusionMatrix</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">dt</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">classes</code><code class="o">=</code><code class="p">[</code><code class="s">"died"</code><code class="p">,</code> <code class="s">"survived"</code><code class="p">],</code>&#13;
<code class="gp">... </code>    <code class="n">label_encoder</code><code class="o">=</code><code class="n">mapping</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">cm_viz</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">cm_viz</code><code class="o">.</code><code class="n">poof</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1202.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id30">&#13;
<img alt="Confusion matrix. The upper left and lower right are correct classifications. The lower left is false negative. The upper right is false positive." src="assets/mlpr_1202.png"/>&#13;
<h6><span class="label">Figure 12-2. </span>Confusion matrix. The upper left and lower right are correct classifications. The lower left is false negative. The upper right is false positive.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Metrics" data-type="sect1"><div class="sect1" id="idm46066892450520">&#13;
<h1>Metrics</h1>&#13;
&#13;
<p><a data-primary="classification evaluation" data-secondary="metrics" data-type="indexterm" id="idm46066892035112"/><a data-primary="metrics" data-secondary="classification evaluation" data-type="indexterm" id="idm46066892034168"/><a data-primary="sklearn" data-secondary="classification metrics implementation" data-type="indexterm" id="idm46066892033256"/>The <code>sklearn.metrics</code> module implements many common classification&#13;
metrics, including:</p>&#13;
<dl>&#13;
<dt><code>'accuracy'</code></dt>&#13;
<dd>&#13;
<p>Percent of correct predictions</p>&#13;
</dd>&#13;
<dt><code>'average_precision'</code></dt>&#13;
<dd>&#13;
<p>Precision recall curve summary</p>&#13;
</dd>&#13;
<dt><code>'f1'</code></dt>&#13;
<dd>&#13;
<p>Harmonic mean of precision and recall</p>&#13;
</dd>&#13;
<dt><code>'neg_log_loss'</code></dt>&#13;
<dd>&#13;
<p>Logistic or cross-entropy loss (model must support <span class="keep-together"><code>predict_proba</code></span>)</p>&#13;
</dd>&#13;
<dt><code>'precision'</code></dt>&#13;
<dd>&#13;
<p>Ability to find only relevant samples (not label a negative as a positive)</p>&#13;
</dd>&#13;
<dt><code>'recall'</code></dt>&#13;
<dd>&#13;
<p>Ability to find all positive samples</p>&#13;
</dd>&#13;
<dt><code>'roc_auc'</code></dt>&#13;
<dd>&#13;
<p>Area under the receiver operator characteristic curve</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>These strings can be used as the <code>scoring</code> parameter when doing grid search,&#13;
or you can use functions from the <code>sklearn.metrics</code> module that have the same&#13;
names as the strings but end in <code>_score</code>. See the following note for examples.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p><code>'f1'</code>, <code>'precision'</code>, and <code>'recall'</code> all support the following suffixes for&#13;
multiclass classifers:</p>&#13;
<dl>&#13;
<dt><code>'_micro'</code></dt>&#13;
<dd>&#13;
<p>Global weighted average of metric</p>&#13;
</dd>&#13;
<dt><code>'_macro'</code></dt>&#13;
<dd>&#13;
<p>Unweighted average of metric</p>&#13;
</dd>&#13;
<dt><code>'_weighted'</code></dt>&#13;
<dd>&#13;
<p>Multiclass weighted average of metric</p>&#13;
</dd>&#13;
<dt><code>'_samples'</code></dt>&#13;
<dd>&#13;
<p>Per sample metric</p>&#13;
</dd>&#13;
</dl>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Accuracy" data-type="sect1"><div class="sect1" id="idm46066892009928">&#13;
<h1>Accuracy</h1>&#13;
&#13;
<p><a data-primary="accuracy, of classifications" data-type="indexterm" id="idm46066892008328"/><a data-primary="classification evaluation" data-secondary="accuracy" data-type="indexterm" id="idm46066892007608"/>Accuracy is the percentage of correct classifications:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="p">(</code><code class="n">tp</code> <code class="o">+</code> <code class="n">tn</code><code class="p">)</code> <code class="o">/</code> <code class="p">(</code><code class="n">tp</code> <code class="o">+</code> <code class="n">tn</code> <code class="o">+</code> <code class="n">fp</code> <code class="o">+</code> <code class="n">fn</code><code class="p">)</code>&#13;
<code class="go">0.8142493638676844</code></pre>&#13;
&#13;
<p>What is good accuracy? It depends. If I’m predicting fraud (which usually is a rare event, say 1 in 10,000), I can get very high accuracy by always predicting not fraud. But this model is not very useful. Looking at other metrics and the cost of predicting a false positive and a false negative can help us determine if a model is decent.</p>&#13;
&#13;
<p>We can use sklearn to calculate it for us:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">accuracy_score</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_predict</code> <code class="o">=</code> <code class="n">dt</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">accuracy_score</code><code class="p">(</code><code class="n">y_test</code><code class="p">,</code> <code class="n">y_predict</code><code class="p">)</code>&#13;
<code class="go">0.8142493638676844</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Recall" data-type="sect1"><div class="sect1" id="idm46066891973368">&#13;
<h1>Recall</h1>&#13;
&#13;
<p><a data-primary="classification evaluation" data-secondary="recall" data-type="indexterm" id="idm46066891975992"/><a data-primary="recall (sensitivity)" data-secondary="of classifications" data-type="indexterm" id="idm46066891975048"/>Recall (also called <em>sensitivity</em>) is the percentage of positive values&#13;
correctly classified. (How many relevant results are returned?)</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">tp</code> <code class="o">/</code> <code class="p">(</code><code class="n">tp</code> <code class="o">+</code> <code class="n">fn</code><code class="p">)</code>&#13;
<code class="go">0.7159763313609467</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">recall_score</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_predict</code> <code class="o">=</code> <code class="n">dt</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">recall_score</code><code class="p">(</code><code class="n">y_test</code><code class="p">,</code> <code class="n">y_predict</code><code class="p">)</code>&#13;
<code class="go">0.7159763313609467</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Precision" data-type="sect1"><div class="sect1" id="idm46066891791624">&#13;
<h1>Precision</h1>&#13;
&#13;
<p><a data-primary="classification evaluation" data-secondary="precision" data-type="indexterm" id="idm46066891831304"/><a data-primary="precision" data-secondary="of classifications" data-type="indexterm" id="idm46066891830392"/>Precision is the percent of positive predictions that were correct (TP&#13;
divided by (TP + FP)). (How relevant are the results?)</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">tp</code> <code class="o">/</code> <code class="p">(</code><code class="n">tp</code> <code class="o">+</code> <code class="n">fp</code><code class="p">)</code>&#13;
<code class="go">0.8287671232876712</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">precision_score</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_predict</code> <code class="o">=</code> <code class="n">dt</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">precision_score</code><code class="p">(</code><code class="n">y_test</code><code class="p">,</code> <code class="n">y_predict</code><code class="p">)</code>&#13;
<code class="go">0.8287671232876712</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="F1" data-type="sect1"><div class="sect1" id="idm46066891807848">&#13;
<h1>F1</h1>&#13;
&#13;
<p><a data-primary="classification evaluation" data-secondary="F1" data-type="indexterm" id="idm46066891768936"/><a data-primary="F1" data-type="indexterm" id="idm46066891768024"/><a data-primary="precision" data-secondary="F1 and" data-type="indexterm" id="idm46066891767352"/><a data-primary="recall (sensitivity)" data-secondary="F1 and" data-type="indexterm" id="idm46066891766408"/>F1 is the harmonic mean of recall and precision:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">pre</code> <code class="o">=</code> <code class="n">tp</code> <code class="o">/</code> <code class="p">(</code><code class="n">tp</code> <code class="o">+</code> <code class="n">fp</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">rec</code> <code class="o">=</code> <code class="n">tp</code> <code class="o">/</code> <code class="p">(</code><code class="n">tp</code> <code class="o">+</code> <code class="n">fn</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="p">(</code><code class="mi">2</code> <code class="o">*</code> <code class="n">pre</code> <code class="o">*</code> <code class="n">rec</code><code class="p">)</code> <code class="o">/</code> <code class="p">(</code><code class="n">pre</code> <code class="o">+</code> <code class="n">rec</code><code class="p">)</code>&#13;
<code class="go">0.7682539682539683</code>&#13;
&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">f1_score</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_predict</code> <code class="o">=</code> <code class="n">dt</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">f1_score</code><code class="p">(</code><code class="n">y_test</code><code class="p">,</code> <code class="n">y_predict</code><code class="p">)</code>&#13;
<code class="go">0.7682539682539683</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Classification Report" data-type="sect1"><div class="sect1" id="idm46066891763784">&#13;
<h1>Classification Report</h1>&#13;
&#13;
<p><a data-primary="classification evaluation" data-secondary="classification report" data-type="indexterm" id="idm46066891762776"/><a data-primary="classification report" data-type="indexterm" id="idm46066891679384"/><a data-primary="Yellowbrick" data-secondary="classification report" data-type="indexterm" id="idm46066891678712"/>Yellowbrick has a classification report showing precision, recall, and&#13;
f1 scores for both positive and negative values (see <a data-type="xref" href="#id31">Figure 12-3</a>). This is colored,&#13;
and the redder the cell (closer to one), the better the score:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="kn">as</code> <code class="nn">plt</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">yellowbrick.classifier</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">ClassificationReport</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">3</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">cm_viz</code> <code class="o">=</code> <code class="n">ClassificationReport</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">dt</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">classes</code><code class="o">=</code><code class="p">[</code><code class="s">"died"</code><code class="p">,</code> <code class="s">"survived"</code><code class="p">],</code>&#13;
<code class="gp">... </code>    <code class="n">label_encoder</code><code class="o">=</code><code class="n">mapping</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">cm_viz</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">cm_viz</code><code class="o">.</code><code class="n">poof</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1203.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id31">&#13;
<img alt="Classification report." src="assets/mlpr_1203.png"/>&#13;
<h6><span class="label">Figure 12-3. </span>Classification report.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="ROC" data-type="sect1"><div class="sect1" id="idm46066891559336">&#13;
<h1>ROC</h1>&#13;
&#13;
<p><a data-primary="classification evaluation" data-secondary="ROC" data-type="indexterm" id="idm46066891557864"/><a data-primary="receiver operating characteristic (ROC) curve" data-type="indexterm" id="idm46066891556920"/><a data-primary="ROC (receiver operating characteristic) curve" data-type="indexterm" id="idm46066891556216"/>A ROC curve illustrates how the classifier&#13;
performs by tracking the <a data-primary="false positives" data-type="indexterm" id="idm46066891555384"/><a data-primary="true positives" data-type="indexterm" id="idm46066891554712"/>true positive rate (recall/sensitivity) as the&#13;
false positive rate (inverted specificity) changes (see <a data-type="xref" href="#id32">Figure 12-4</a>).</p>&#13;
&#13;
<p>A rule of thumb is&#13;
that the plot should bulge out toward the top-left corner. A plot that&#13;
is to the left and above another plot indicates better performance. The&#13;
diagonal in this plot indicates the behavior of a random guessing classifier.&#13;
<a data-primary="area under the curve (AUC)" data-type="indexterm" id="idm46066891552328"/>By taking the AUC, you get a metric for evaluating&#13;
the performance:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">roc_auc_score</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_predict</code> <code class="o">=</code> <code class="n">dt</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">roc_auc_score</code><code class="p">(</code><code class="n">y_test</code><code class="p">,</code> <code class="n">y_predict</code><code class="p">)</code>&#13;
<code class="go">0.8706304346418559</code></pre>&#13;
&#13;
<p><a data-primary="Yellowbrick" data-secondary="ROC curve" data-type="indexterm" id="idm46066891532312"/>Yellowbrick can plot this for us:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">yellowbrick.classifier</code> <code class="kn">import</code> <code class="n">ROCAUC</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">6</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">roc_viz</code> <code class="o">=</code> <code class="n">ROCAUC</code><code class="p">(</code><code class="n">dt</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">roc_viz</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>&#13;
<code class="go">0.8706304346418559</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">roc_viz</code><code class="o">.</code><code class="n">poof</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1204.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id32">&#13;
<img alt="ROC curve." src="assets/mlpr_1204.png"/>&#13;
<h6><span class="label">Figure 12-4. </span>ROC curve.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Precision-Recall Curve" data-type="sect1"><div class="sect1" id="idm46066891401672">&#13;
<h1>Precision-Recall Curve</h1>&#13;
&#13;
<p><a data-primary="classification evaluation" data-secondary="precision-recall curve" data-type="indexterm" id="idm46066891400200"/><a data-primary="precision-recall curve" data-type="indexterm" id="idm46066891399256"/>The ROC curve may be overly optimistic for imbalanced classes. Another option for&#13;
evaluating classifiers is using a precision-recall curve (see <a data-type="xref" href="#id33">Figure 12-5</a>). Classification&#13;
is a balancing act of finding everything you need (recall) while limiting the&#13;
junk results (precision). This is typically a trade-off. As recall goes&#13;
up, precision usually goes down and vice versa.</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">average_precision_score</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_predict</code> <code class="o">=</code> <code class="n">dt</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">average_precision_score</code><code class="p">(</code><code class="n">y_test</code><code class="p">,</code> <code class="n">y_predict</code><code class="p">)</code>&#13;
<code class="go">0.7155150490642249</code></pre>&#13;
&#13;
<p>Here is a  Yellowbrick precision-recall curve:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">yellowbrick.classifier</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">PrecisionRecallCurve</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">viz</code> <code class="o">=</code> <code class="n">PrecisionRecallCurve</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">DecisionTreeClassifier</code><code class="p">(</code><code class="n">max_depth</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">viz</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="k">print</code><code class="p">(</code><code class="n">viz</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">viz</code><code class="o">.</code><code class="n">poof</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1205.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id33">&#13;
<img alt="Precision-recall curve." src="assets/mlpr_1205.png"/>&#13;
<h6><span class="label">Figure 12-5. </span>Precision-recall curve.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Cumulative Gains Plot" data-type="sect1"><div class="sect1" id="idm46066891217512">&#13;
<h1>Cumulative Gains Plot</h1>&#13;
&#13;
<p><a data-primary="binary classifiers" data-secondary="cumulative gains plot for evaluation" data-type="indexterm" id="ix_ch12-asciidoc4"/><a data-primary="classification evaluation" data-secondary="cumulative gains plot" data-type="indexterm" id="ix_ch12-asciidoc5"/><a data-primary="cumulative gains plot" data-type="indexterm" id="ix_ch12-asciidoc6"/>A cumulative gains plot can be used to evaluate a binary classifier.&#13;
It models the true positive rate (sensitivity) against the support rate (fraction of positive predictions). The intuition behind this plot is to sort&#13;
all classifications by predicted probability. Ideally there would be a clean&#13;
cut that divides positive from negative samples. If the first 10% of the predictions&#13;
has 30% of the positive samples, you would plot a point from (0,0) to (.1, .3).&#13;
You continue this process through all of the samples (see <a data-type="xref" href="#idcgc1">Figure 12-6</a>).</p>&#13;
&#13;
<p>A common use for this is determining customer response. The cumulative gains curve plots the support or predicted positive rate along the x-axis. Our chart labels this as “Percentage of sample”. It plots the sensitivity or true positive rate along the y-axis. This is labeled as “Gain” in our plot.</p>&#13;
&#13;
<p>If you wanted to contact 90% of the customers that would respond (sensitivity), you can trace from .9 on the y-axis to the right until you hit that curve. The x-axis at that point will indicate how many total customers you need to contact (support) to get to 90%.</p>&#13;
&#13;
<p>In this case we aren’t contacting customers that would respond to a survey but predicting survival on the Titanic. If we ordered all passengers on the Titanic according to our model by how likely they are to survive, if you took the first 65% of them, you would have 90% of the survivors. If you have an associated cost per contact and revenue per response, you can calculate what the best number is.</p>&#13;
&#13;
<p>In general, a model that is to the left and above another model is a better model. The best models are lines that go up to the top (if 10% of the samples are positive, it would hit at (.1, 1)) and then directly to the right. If the plot is below the baseline, we would do better to randomly assign labels that use our model.</p>&#13;
&#13;
<p>The <a href="https://oreil.ly/dg0iQ">scikit-plot library</a>&#13;
can create a cumulative gains plot:<a data-startref="ix_ch12-asciidoc6" data-type="indexterm" id="idm46066891206968"/><a data-startref="ix_ch12-asciidoc5" data-type="indexterm" id="idm46066891206264"/><a data-startref="ix_ch12-asciidoc4" data-type="indexterm" id="idm46066891205592"/></p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">6</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_probas</code> <code class="o">=</code> <code class="n">dt</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">scikitplot</code><code class="o">.</code><code class="n">metrics</code><code class="o">.</code><code class="n">plot_cumulative_gain</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">y_test</code><code class="p">,</code> <code class="n">y_probas</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="s">"images/mlpr_1206.png"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">bbox_inches</code><code class="o">=</code><code class="s">"tight"</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="idcgc1">&#13;
<img alt="Cumulative gains plot. If we ordered people on the Titanic according to our model, looking at 20% of them we would get 40% of the survivors." src="assets/mlpr_1206.png"/>&#13;
<h6><span class="label">Figure 12-6. </span>Cumulative gains plot. If we ordered people on the Titanic according to our model, looking at 20% of them we would get 40% of the survivors.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Lift Curve" data-type="sect1"><div class="sect1" id="idm46066891217016">&#13;
<h1>Lift Curve</h1>&#13;
&#13;
<p><a data-primary="classification evaluation" data-secondary="lift curve" data-type="indexterm" id="idm46066891181176"/><a data-primary="cumulative gains plot" data-type="indexterm" id="idm46066891129208"/><a data-primary="lift curve" data-type="indexterm" id="idm46066891128536"/>A lift curve is another way of looking at the information in a cumulative&#13;
gains plot. <a data-primary="lift (term)" data-type="indexterm" id="idm46066891127736"/>The <em>lift</em> is how much better we are doing than the baseline model.&#13;
In our plot below, we can see that if we sorted our Titanic passengers by the survival probability&#13;
and took the first 20% of them, our lift&#13;
would be about 2.2 times (the gain divided by sample percent) better than randomly choosing survivors (see <a data-type="xref" href="#idlc1">Figure 12-7</a>). (We would&#13;
get 2.2 times as many survivors.)</p>&#13;
&#13;
<p>The scikit-plot library&#13;
can create a lift curve:</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">6</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_probas</code> <code class="o">=</code> <code class="n">dt</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">scikitplot</code><code class="o">.</code><code class="n">metrics</code><code class="o">.</code><code class="n">plot_lift_curve</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">y_test</code><code class="p">,</code> <code class="n">y_probas</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="s">"images/mlpr_1207.png"</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">,</code>&#13;
<code class="gp">... </code>    <code class="n">bbox_inches</code><code class="o">=</code><code class="s">"tight"</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="idlc1">&#13;
<img alt="lift curve." src="assets/mlpr_1207.png"/>&#13;
<h6><span class="label">Figure 12-7. </span>Lift curve.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Class Balance" data-type="sect1"><div class="sect1" id="idm46066891026408">&#13;
<h1>Class Balance</h1>&#13;
&#13;
<p><a data-primary="class balance" data-type="indexterm" id="idm46066891025176"/><a data-primary="classification evaluation" data-secondary="class balance" data-type="indexterm" id="idm46066891024248"/><a data-primary="Yellowbrick" data-secondary="class size bar plot" data-type="indexterm" id="idm46066891023336"/>Yellowbrick has a simple bar plot to view the class sizes. When the&#13;
relative class sizes are different, accuracy is not a good evaluation&#13;
metric (see <a data-type="xref" href="#id34">Figure 12-8</a>). <a data-primary="stratified sampling" data-type="indexterm" id="idm46066891021304"/>When splitting up the data into training and test sets, use <em>stratified sampling</em> so the sets keep a relative proportion of the classes. (The <code>test_train_split</code> function does this when you set the <code>stratify</code> parameter to the labels.)</p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">yellowbrick.classifier</code> <code class="kn">import</code> <code class="n">ClassBalance</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">6</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">cb_viz</code> <code class="o">=</code> <code class="n">ClassBalance</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">labels</code><code class="o">=</code><code class="p">[</code><code class="s">"Died"</code><code class="p">,</code> <code class="s">"Survived"</code><code class="p">]</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">cb_viz</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">y_test</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">cb_viz</code><code class="o">.</code><code class="n">poof</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1208.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id34">&#13;
<img alt="A slight class imbalance." src="assets/mlpr_1208.png"/>&#13;
<h6><span class="label">Figure 12-8. </span>A slight class imbalance.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Class Prediction Error" data-type="sect1"><div class="sect1" id="idm46066890922472">&#13;
<h1>Class Prediction Error</h1>&#13;
&#13;
<p><a data-primary="class prediction error" data-type="indexterm" id="idm46066890921304"/><a data-primary="classification evaluation" data-secondary="class prediction error" data-type="indexterm" id="idm46066890920600"/><a data-primary="Yellowbrick" data-secondary="class prediction error plot" data-type="indexterm" id="idm46066890919688"/>The class prediction error plot from Yellowbrick is a bar chart that&#13;
visualizes a confusion matrix (see <a data-type="xref" href="#id35">Figure 12-9</a>):</p>&#13;
&#13;
<pre class="pagebreak-before" data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">yellowbrick.classifier</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">ClassPredictionError</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">3</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">cpe_viz</code> <code class="o">=</code> <code class="n">ClassPredictionError</code><code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">dt</code><code class="p">,</code> <code class="n">classes</code><code class="o">=</code><code class="p">[</code><code class="s">"died"</code><code class="p">,</code> <code class="s">"survived"</code><code class="p">]</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">cpe_viz</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">cpe_viz</code><code class="o">.</code><code class="n">poof</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1209.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id35">&#13;
<img alt="Class prediction error. At the top of the left bar are people who died, but we predicted that they survived (false positive). At the bottom of the right bar are people who survived, but the model predicted death (false negative)." src="assets/mlpr_1209.png"/>&#13;
<h6><span class="label">Figure 12-9. </span>Class prediction error. At the top of the left bar are people who died, but we predicted that they survived (false positive). At the bottom of the right bar are people who survived, but the model predicted death (false negative).</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Discrimination Threshold" data-type="sect1"><div class="sect1" id="idm46066890797800">&#13;
<h1>Discrimination Threshold</h1>&#13;
&#13;
<p><a data-primary="binary classifiers" data-secondary="and discrimination threshold" data-type="indexterm" id="idm46066890796664"/><a data-primary="classification evaluation" data-secondary="discrimination threshold" data-type="indexterm" id="idm46066890795624"/><a data-primary="discrimination threshold" data-type="indexterm" id="idm46066890794648"/>Most binary classifiers that predict probability have a <em>discrimination threshold</em>&#13;
of 50%. If the predicted probability is above 50%, the classifier assigns a positive&#13;
label. <a data-type="xref" href="#id36">Figure 12-10</a> moves that threshold value between 0 and 100 and&#13;
shows the impact to precision, recall, f1, and queue rate.</p>&#13;
&#13;
<p><a data-primary="precision" data-secondary="discrimination threshold and" data-type="indexterm" id="idm46066890792152"/><a data-primary="recall (sensitivity)" data-secondary="discrimination threshold and" data-type="indexterm" id="idm46066890791112"/>This plot can be useful to view the trade-off between precision and recall. Assume we are looking for fraud (and considering fraud to be the positive classification). To get high recall (catch all of the fraud), we can just classify everything as fraud. But in a bank situation, this would not be profitable and would require an army of workers. To get high precision (only catch fraud if it is fraud), we could have a model that only triggers on cases of extreme fraud. But this would miss much of the fraud that might not be as obvious. There is a trade-off here.</p>&#13;
&#13;
<p><a data-primary="queue rate" data-type="indexterm" id="idm46066890789192"/>The <em>queue rate</em> is the&#13;
percent of predictions above the threshold.  You can consider this to be the percent of&#13;
cases to review if you are dealing with fraud.</p>&#13;
&#13;
<p>If you have the cost for positive, negative, and erroneous calculations, you can determine what threshold you are comfortable with.</p>&#13;
&#13;
<p>The following plot is useful to see what discrimination threshold will maximize the f1 score&#13;
or adjust precision or recall to an acceptable number when coupled with the queue rate.</p>&#13;
&#13;
<p class="pagebreak-before"><a data-primary="Yellowbrick" data-secondary="discrimination threshold visualization" data-type="indexterm" id="idm46066890786056"/>Yellowbrick provides this visualizer. This visualizer shuffles the data and runs 50 trials by default, splitting out 10% for <span class="keep-together">validation</span>:<a data-startref="ix_ch12-asciidoc0" data-type="indexterm" id="idm46066890784136"/></p>&#13;
&#13;
<pre data-code-language="pycon" data-type="programlisting"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">yellowbrick.classifier</code> <code class="kn">import</code> <code class="p">(</code>&#13;
<code class="gp">... </code>    <code class="n">DiscriminationThreshold</code><code class="p">,</code>&#13;
<code class="gp">... </code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">5</code><code class="p">))</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">dt_viz</code> <code class="o">=</code> <code class="n">DiscriminationThreshold</code><code class="p">(</code><code class="n">dt</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">dt_viz</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">dt_viz</code><code class="o">.</code><code class="n">poof</code><code class="p">()</code>&#13;
<code class="gp">&gt;&gt;&gt; </code><code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s">"images/mlpr_1210.png"</code><code class="p">,</code> <code class="n">dpi</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre>&#13;
&#13;
<figure><div class="figure" id="id36">&#13;
<img alt="Discrimination threshold." src="assets/mlpr_1210.png"/>&#13;
<h6><span class="label">Figure 12-10. </span>Discrimination threshold.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>