- en: Chapter 19\. Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 19.0 Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In much of this book we have looked at supervised machine learning—​where we
    have access to both the features and the target. This is, unfortunately, not always
    the case. Frequently, we run into situations where we only know the features.
    For example, imagine we have records of sales from a grocery store and we want
    to break up sales by whether the shopper is a member of a discount club. This
    would be impossible using supervised learning because we don’t have a target to
    train and evaluate our models. However, there is another option: unsupervised
    learning. If the behavior of discount club members and nonmembers in the grocery
    store is actually disparate, then the average difference in behavior between two
    members will be smaller than the average difference in behavior between a member
    and nonmember shopper. Put another way, there will be two clusters of observations.'
  prefs: []
  type: TYPE_NORMAL
- en: The goal of clustering algorithms is to identify those latent groupings of observations,
    which, if done well, allows us to predict the class of observations even without
    a target vector. There are many clustering algorithms, and they have a wide variety
    of approaches to identifying the clusters in data. In this chapter, we will cover
    a selection of clustering algorithms using scikit-learn and how to use them in
    practice.
  prefs: []
  type: TYPE_NORMAL
- en: 19.1 Clustering Using K-Means
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to group observations into *k* groups.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use *k-means clustering*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'K-means clustering is one of the most common clustering techniques. In k-means
    clustering, the algorithm attempts to group observations into *k* groups, with
    each group having roughly equal variance. The number of groups, *k*, is specified
    by the user as a hyperparameter. Specifically, in k-means:'
  prefs: []
  type: TYPE_NORMAL
- en: '*k* cluster “center” points are created at random locations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each observation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The distance between each observation and the *k* center points is calculated.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The observation is assigned to the cluster of the nearest center point.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The center points are moved to the means (i.e., centers) of their respective
    clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Steps 2 and 3 are repeated until no observation changes in cluster membership.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this point the algorithm is considered converged and stops.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note three things about k-means. First, k-means clustering
    assumes the clusters are convex shaped (e.g., a circle, a sphere). Second, all
    features are equally scaled. In our solution, we standardized the features to
    meet this assumption. Third, the groups are balanced (i.e., have roughly the same
    number of observations). If we suspect that we cannot meet these assumptions,
    we might try other clustering approaches.
  prefs: []
  type: TYPE_NORMAL
- en: In scikit-learn, k-means clustering is implemented in the `KMeans` class. The
    most important parameter is `n_clusters`, which sets the number of clusters *k*.
    In some situations, the nature of the data will determine the value for *k* (e.g.,
    data on a school’s students will have one cluster per grade), but often we don’t
    know the number of clusters. In these cases, we will want to select *k* based
    on using some criteria. For example, silhouette coefficients (see [Recipe 11.9](ch11.xhtml#evaluating-clustering-models))
    measure the similarity within clusters compared with the similarity between clusters.
    Furthermore, because k-means clustering is computationally expensive, we might
    want to take advantage of all the cores on our computer. We can do this by setting
    `n_jobs=-1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our solution, we cheated a little and used the iris flower data, which we
    know contains three classes. Therefore, we set *k = 3*. We can use `labels_` to
    see the predicted classes of each observation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If we compare this to the observation’s true class, we can see that, despite
    the difference in class labels (i.e., `0`, `1`, and `2`), k-means did reasonably
    well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: However, as you might imagine, the performance of k-means drops considerably,
    even critically, if we select the wrong number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, as with other scikit-learn models, we can use the trained cluster
    to predict the value of new observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The observation is predicted to belong to the cluster whose center point is
    closest. We can even use `cluster_centers_` to see those center points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Introduction to K-means Clustering](https://oreil.ly/HDfUz)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 19.2 Speeding Up K-Means Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to group observations into *k* groups, but k-means takes too long.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use mini-batch k-means:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Mini-batch k-means* works similarly to the k-means algorithm discussed in
    [Recipe 19.1](#clustering-using-k-means). Without going into too much detail,
    the difference is that in mini-batch k-means the most computationally costly step
    is conducted on only a random sample of observations as opposed to all observations.
    This approach can significantly reduce the time required for the algorithm to
    find convergence (i.e., fit the data) with only a small cost in quality.'
  prefs: []
  type: TYPE_NORMAL
- en: '`MiniBatchKMeans` works similarly to `KMeans`, with one significant difference:
    the `batch_size` parameter. `batch_size` controls the number of randomly selected
    observations in each batch. The larger the size of the batch, the more computationally
    costly the training process.'
  prefs: []
  type: TYPE_NORMAL
- en: 19.3 Clustering Using Mean Shift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to group observations without assuming the number of clusters or their
    shape.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use mean shift clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the disadvantages of k-means clustering we discussed previously is that
    we needed to set the number of clusters, *k*, prior to training, and the method
    made assumptions about the shape of the clusters. One clustering algorithm without
    these limitations is mean shift.
  prefs: []
  type: TYPE_NORMAL
- en: '*Mean shift* is a simple concept, but it’s somewhat difficult to explain. Therefore,
    an analogy might be the best approach. Imagine a very foggy football field (i.e.,
    a two-dimensional feature space) with 100 people standing on it (i.e., our observations).
    Because it is foggy, a person can see only a short distance. Every minute each
    person looks around and takes a step in the direction of the most people they
    can see. As time goes on, people start to group together as they repeatedly take
    steps toward larger and larger crowds. The end result is clusters of people around
    the field. People are assigned to the clusters in which they end up.'
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn’s actual implementation of mean shift, `MeanShift`, is more complex
    but follows the same basic logic. `MeanShift` has two important parameters we
    should be aware of. First, `bandwidth` sets the radius of the area (i.e., kernel)
    an observation uses to determine the direction to shift. In our analogy, bandwidth
    is how far a person can see through the fog. We can set this parameter manually,
    but by default a reasonable bandwidth is estimated automatically (with a significant
    increase in computational cost). Second, sometimes in mean shift there are no
    other observations within an observation’s kernel. That is, a person on our football
    field cannot see a single other person. By default, `MeanShift` assigns all these
    “orphan” observations to the kernel of the nearest observation. However, if we
    want to leave out these orphans, we can set `cluster_all=False`, wherein orphan
    observations are given the label of `-1`.
  prefs: []
  type: TYPE_NORMAL
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[The mean shift clustering algorithm, EFAVDB](https://oreil.ly/Gb3VG)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 19.4 Clustering Using DBSCAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to group observations into clusters of high density.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use DBSCAN clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*DBSCAN* is motivated by the idea that clusters will be areas where many observations
    are densely packed together and makes no assumptions of cluster shape. Specifically,
    in DBSCAN:'
  prefs: []
  type: TYPE_NORMAL
- en: A random observation, *x[i]*, is chosen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If *x[i]* has a minimum number of close neighbors, we consider it to be part
    of a cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Step 2 is repeated recursively for all of *x[i]*’s neighbors, then neighbor’s
    neighbor, and so on. These are the cluster’s core observations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once step 3 runs out of nearby observations, a new random point is chosen (i.e.,
    restart at step 1).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once this is complete, we have a set of core observations for a number of clusters.
    Finally, any observation close to a cluster but not a core sample is considered
    part of a cluster, while any observation not close to the cluster is labeled an
    outlier.
  prefs: []
  type: TYPE_NORMAL
- en: '`DBSCAN` has three main parameters to set:'
  prefs: []
  type: TYPE_NORMAL
- en: '`eps`'
  prefs: []
  type: TYPE_NORMAL
- en: The maximum distance from an observation for another observation to be considered
    its neighbor.
  prefs: []
  type: TYPE_NORMAL
- en: '`min_samples`'
  prefs: []
  type: TYPE_NORMAL
- en: The minimum number of observations less than `eps` distance from an observation
    for it to be considered a core observation.
  prefs: []
  type: TYPE_NORMAL
- en: '`metric`'
  prefs: []
  type: TYPE_NORMAL
- en: The distance metric used by `eps`—for example, `minkowski` or `euclidean` (note
    that if Minkowski distance is used, the parameter `p` can be used to set the power
    of the Minkowski metric).
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at the clusters in our training data we can see two clusters have
    been identified, `0` and `1`, while outlier observations are labeled `-1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: See Also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[DBSCAN, Wikipedia](https://oreil.ly/QBx3a)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 19.5 Clustering Using Hierarchical Merging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You want to group observations using a hierarchy of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use agglomerative clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Agglomerative clustering* is a powerful, flexible hierarchical clustering
    algorithm. In agglomerative clustering, all observations start as their own clusters.
    Next, clusters meeting some criteria are merged. This process is repeated, growing
    clusters until some end point is reached. In scikit-learn, `AgglomerativeClustering`
    uses the `linkage` parameter to determine the merging strategy to minimize:'
  prefs: []
  type: TYPE_NORMAL
- en: Variance of merged clusters (`ward`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average distance between observations from pairs of clusters (`average`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum distance between observations from pairs of clusters (`complete`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two other parameters are useful to know. First, the `affinity` parameter determines
    the distance metric used for `linkage` (`minkowski`, `euclidean`, etc.). Second,
    `n_clusters` sets the number of clusters the clustering algorithm will attempt
    to find. That is, clusters are successively merged until only `n_clusters` remain.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with other clustering algorithms we have covered, we can use `labels_` to
    see the cluster in which every observation is assigned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
