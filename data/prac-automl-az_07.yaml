- en: Chapter 4\. Feature Engineering and Automated Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章\. 特征工程与自动化机器学习
- en: Feature engineering is one of the most important parts of the data science process.
    If you ask data scientists to break down the time spent in each stage of the data
    science process, you’ll often hear that they spend a significant amount of time
    understanding and exploring the data, and doing *feature engineering*. Most experienced
    data scientists do not jump into model building. Rather, they first spend time
    doing feature engineering.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是数据科学过程中最重要的部分之一。如果你让数据科学家分析数据科学过程中每个阶段花费的时间，你通常会听到他们花大量时间理解和探索数据，并进行*特征工程*。大多数经验丰富的数据科学家不会直接开始建模，而是先花时间进行特征工程。
- en: But what is feature engineering? With feature engineering, you can transform
    your original data into a form that is more easily understood by the machine learning
    algorithms. For example, you might perform data processing, add new features (e.g.,
    additional data columns that combine values from existing columns), or you might
    transform the features from their original domain to a different domain. You might
    also remove features that are not useful or relevant to the model. When doing
    feature engineering, you will generate new features, transform existing features,
    or select a subset of features.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 但是什么是特征工程？通过特征工程，你可以将原始数据转换为机器学习算法更容易理解的形式。例如，你可以进行数据处理、添加新特征（例如，将现有列的值组合为新的数据列）或者将特征从其原始领域转换到不同的领域。你还可以移除对模型无用或无关的特征。在进行特征工程时，你将生成新特征、转换现有特征或选择特征子集。
- en: 'To illustrate how you can transform features, let’s consider a simple example
    of working with *categorical features* (otherwise known as *categorical variables*).
    Suppose that you have a dataset for an airline customer program with a feature
    called Status, which determines the status of the customers (e.g., based on how
    often the customer flies, total miles traveled, and others). Status contains the
    following five unique values: New, Silver, Gold, Platinum, and Diamond. Because
    some of the machine learning algorithms can work only with numerical variables,
    you will need to transform the feature. A common approach is to use *one-hot encoding*,
    as shown in [Table 4-1](#one-hot_coding).'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明如何转换特征，让我们考虑一个简单的例子，即处理*分类特征*（又称*分类变量*）。假设你有一个航空公司客户计划的数据集，其中有一个名为Status的特征，该特征确定客户的状态（例如，根据客户飞行频率、总里程等）。Status包含以下五个唯一值：New、Silver、Gold、Platinum和Diamond。因为一些机器学习算法只能处理数值变量，所以你需要对这个特征进行转换。常见的方法是使用*独热编码*，如[表格4-1](#one-hot_coding)所示。
- en: Table 4-1\. One-hot encoding
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 表格4-1\. 独热编码
- en: '| Status |  | New | Silver | Gold | Platinum | Diamond |'
  id: totrans-5
  prefs: []
  type: TYPE_TB
  zh: '| Status |  | New | Silver | Gold | Platinum | Diamond |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-6
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Gold |  | 0 | 0 | 1 | 0 | 0 |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| Gold |  | 0 | 0 | 1 | 0 | 0 |'
- en: '| Silver |  | 0 | 1 | 0 | 0 | 0 |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| Silver |  | 0 | 1 | 0 | 0 | 0 |'
- en: '| New | ➪ | 1 | 0 | 0 | 0 | 0 |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| New | ➪ | 1 | 0 | 0 | 0 | 0 |'
- en: '| Platinum |  | 0 | 0 | 0 | 1 | 0 |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| Platinum |  | 0 | 0 | 0 | 1 | 0 |'
- en: '| Silver |  | 0 | 1 | 0 | 0 | 0 |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| Silver |  | 0 | 1 | 0 | 0 | 0 |'
- en: '| Gold |  | 0 | 0 | 1 | 0 | 0 |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| Gold |  | 0 | 0 | 1 | 0 | 0 |'
- en: Another important aspect of feature engineering is taking advantage of domain
    expertise. You might consider working with a person who has relevant domain expertise
    when doing feature engineering—the inputs from the domain expert will be invaluable
    when working toward the goal of delivering a high-quality model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程的另一个重要方面是利用领域专业知识。在进行特征工程时，你可能考虑与具有相关领域专业知识的人合作——领域专家的输入在努力实现高质量模型的目标时将是非常宝贵的。
- en: '*Transparency* and *explainability* are important considerations when training
    machine learning models. Hence, doing feature engineering properly will contribute
    toward having high-performing models that can be explained.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练机器学习模型时，*透明性*和*可解释性*是重要考虑因素。因此，正确进行特征工程将有助于生成性能优越且能够解释的模型。
- en: Note
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '[Chapter 7](ch07.html#model_interpretability_and_transparency) provides a detailed
    discussion on how Azure Machine Learning gives you the tools to understand the
    models generated, the relative importance of features, and more.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[第7章](ch07.html#model_interpretability_and_transparency)详细讨论了Azure机器学习如何提供工具来理解生成的模型、特征的相对重要性等。'
- en: 'When performing feature engineering, data scientists often ask themselves the
    following questions:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行特征工程时，数据科学家经常问自己以下问题：
- en: Which features in the dataset are irrelevant to the model? For example, your
    data might contain an Identifier (ID) column. Though this column is useful when
    combining data from several datasets (e.g., joining two datasets based on an `employee_id`),
    the ID column is not used in any way when training the model.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集中哪些特征对模型是不相关的？例如，您的数据可能包含一个标识符（ID）列。尽管此列在合并来自多个数据集的数据时很有用（例如，根据`employee_id`连接两个数据集），但在训练模型时不会以任何方式使用
    ID 列。
- en: Can we combine one or more features to create new features that will be even
    more useful?
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们能否组合一个或多个特征以创建更加有用的新特征？
- en: For some of the classes that are sparse (i.e., those that contain significantly
    fewer observations), can we group them to create more meaningful classes?
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于一些稀疏的类别（即包含显著较少观察结果的类别），我们能否将它们分组以创建更有意义的类别？
- en: In this chapter, we focus on how to use the auto-featurization capabilities
    provided in the automated ML tool that is a part of Microsoft Azure Machine Learning.
    You will learn how auto-featurization works for classification, regression, and
    forecasting tasks. In addition, we share pointers and resources that enable you
    to go more in-depth with feature engineering. Before we dive into the auto-featurization
    performed by automated ML, let’s look at the data preprocessing methods that are
    available.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们关注如何使用 Microsoft Azure 机器学习中的自动化 ML 工具提供的自动特征化功能。您将学习自动特征化在分类、回归和预测任务中的工作原理。此外，我们还分享了指导和资源，帮助您更深入地进行特征工程。在深入探讨自动化
    ML 执行的自动特征化之前，让我们先看看可用的数据预处理方法。
- en: Data Preprocessing Methods Available in Automated ML
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化 ML 中可用的数据预处理方法
- en: Depending on the type of machine learning task (e.g., classification, regression,
    forecasting), different types of data preprocessing are performed. When you use
    automated ML and submit an experiment, you will notice that each iteration performs
    a different type of data preprocessing.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 根据机器学习任务的类型（例如分类、回归、预测），执行不同类型的数据预处理。当您使用自动化 ML 并提交实验时，您会注意到每次迭代执行不同类型的数据预处理。
- en: For example, you will notice that your data is scaled, or normalized, when it
    is relevant. When features have different ranges, scaling and normalization helps.
    [Table 4-2](#data_preprocessing_performed_by_automate) shows the scaling and normalization
    steps performed by automated ML.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当数据相关时，您会注意到数据被缩放或标准化。当特征具有不同的范围时，缩放和标准化是有帮助的。[表 4-2](#data_preprocessing_performed_by_automate)
    展示了自动化 ML 执行的缩放和标准化步骤。
- en: Table 4-2\. Data preprocessing performed by automated ML
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-2\. 自动化 ML 执行的数据预处理
- en: '| Scaling and normalization | Description |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 缩放和标准化 | 描述 |'
- en: '| --- | --- |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| [MinMaxScalar](https://oreil.ly/FBzE4) | Each feature is transformed by scaling
    to the minimum and maximum for that column. |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| [MinMaxScalar](https://oreil.ly/FBzE4) | 对每列进行缩放，使得特征在最小和最大值之间。 |'
- en: '| [MaxAbsScaler](https://oreil.ly/pGvd2) | Each feature is scaled by using
    the maximum absolute value. |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| [MaxAbsScaler](https://oreil.ly/pGvd2) | 使用最大绝对值对每个特征进行缩放。 |'
- en: '| [RobustScalar](https://oreil.ly/AUlqU) | Each feature is scaled by using
    the values from quantile range. |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| [RobustScalar](https://oreil.ly/AUlqU) | 使用分位数范围的值对每个特征进行缩放。 |'
- en: '| [PCA](https://oreil.ly/6wb1B) | Linear dimensionality reduction using singular
    value decomposition (SVD) of the data to project it to a lower dimensional space.
    |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| [PCA](https://oreil.ly/6wb1B) | 使用数据的奇异值分解（SVD）进行线性降维，将其投影到更低维度空间。 |'
- en: '| [TruncatedSVDWrapper](https://oreil.ly/mNQ86) | Uses truncated SVD to do
    linear dimensionality reduction.Unlike principal component analysis (PCA), the
    data is not centered before SVD is computed. Note: This enables it to work efficiently
    with `scipy.sparse` matrices. |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| [TruncatedSVDWrapper](https://oreil.ly/mNQ86) | 使用截断的 SVD 进行线性降维。与主成分分析（PCA）不同，数据在计算
    SVD 前不会居中。注意：这使得它能够有效地处理`scipy.sparse`矩阵。 |'
- en: '| [SparseNormalizer](https://oreil.ly/qmKyn) | Each sample that contains one
    or more nonzero components is independently rescaled, enabling the norm (L1 or
    L2) to equal one. |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| [SparseNormalizer](https://oreil.ly/qmKyn) | 对包含一个或多个非零分量的每个样本进行独立重新缩放，使得范数（L1
    或 L2）等于一。 |'
- en: Tip
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: For more details on how data is preprocessed in Azure Machine Learning, see
    [this section](https://oreil.ly/vEGJu) of the Microsoft Azure documentation.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 若要详细了解 Azure 机器学习中数据预处理的更多细节，请参阅[此部分](https://oreil.ly/vEGJu) Microsoft Azure
    文档。
- en: Auto-Featurization for Automated ML
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化 ML 的自动特征化
- en: 'Let’s get started with using auto-featurization. By now, you should be familiar
    with how to set up the automated ML configuration object. Let’s recap how you
    set up the automated ML experiment. In the code example that follows, you first
    define the `AutoMLConfig` object. Next, specify the name of the experiment, number
    of iterations to run, the logging granularity, and more. After you have defined
    the `AutoMLConfig` object, submit the experiment by using `experiment.submit(…)`:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始使用自动特征生成。到目前为止，您应该熟悉如何设置自动化 ML 配置对象。让我们回顾一下如何设置自动化 ML 实验。在接下来的代码示例中，首先定义
    `AutoMLConfig` 对象。接下来，指定实验的名称、运行的迭代次数、日志记录的粒度等。在定义了 `AutoMLConfig` 对象之后，通过使用 `experiment.submit(…)`
    提交实验：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: After you submit the experiment, notice the data processing that has been performed
    in each iteration (see the output in [Figure 4-1](#data_preprocessing_using_automated_ml)).
    From iteration 0 to 7, you can see that each iteration shows what type of data
    preprocessing has been performed. For example, in iteration 0, we can see that
    the `StandardScalerWrapper` is used. In iteration 3, the `RobustScaler` is used.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在提交实验后，请注意每次迭代中执行的数据处理（请参见 [Figure 4-1](#data_preprocessing_using_automated_ml)
    中的输出）。从迭代 0 到 7，您可以看到每次迭代显示的数据预处理类型。例如，在迭代 0 中，我们可以看到使用了 `StandardScalerWrapper`。在迭代
    3 中，使用了 `RobustScaler`。
- en: In the code shown earlier, in which you defined the `AutoMLConfig` object, notice
    that one of the properties, `preprocess`, is set to `False`. You can also set
    `preprocess = True` to turn on advanced data preprocessing. This makes it possible
    for you to use both data preprocessing and auto-featurization.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面展示的代码中，您定义了 `AutoMLConfig` 对象，在其中注意到其中一个属性 `preprocess` 被设置为 `False`。您还可以设置
    `preprocess = True` 来启用高级数据预处理。这使您可以同时使用数据预处理和自动特征生成。
- en: Note
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The type of auto-featurization performed depends on the machine learning task
    you’re planning. For example, if you use automated ML for classification and regression,
    auto-featurization might include dropping features with a high cardinality, or
    low variance. If you use automated ML for forecasting, additional features might
    be generated for DateTime, or relation of the DateTime to holidays in various
    countries.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 执行的自动特征生成类型取决于您计划的机器学习任务。例如，如果您使用自动化 ML 进行分类和回归，自动特征生成可能包括删除高基数或低方差的特征。如果您用于预测，可能会生成与日期时间相关的额外特征，或日期时间与各国节假日的关系。
- en: '![paml 0401](assets/paml_0401.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0401](assets/paml_0401.png)'
- en: Figure 4-1\. Data preprocessing using automated ML
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 4-1\. 使用自动化 ML 进行数据预处理
- en: '[Table 4-3](#auto-featurization_performed_by_automate) presents the auto-featurization
    features used by automated ML.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[Table 4-3](#auto-featurization_performed_by_automate) 展示了自动化 ML 使用的自动特征生成功能。'
- en: Table 4-3\. Auto-featurization performed by automated ML
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Table 4-3\. 由自动化 ML 执行的自动特征生成
- en: '| Preprocessing and auto-featurization | Description |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 预处理和自动特征生成 | 描述 |'
- en: '| --- | --- |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Drop high-cardinality or no variance features | Drop these from training
    and validation sets, including features with all values missing, same value across
    all rows, or with extremely high-cardinality (e.g., hashes, IDs, or GUIDs). |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 删除高基数或无方差特征 | 从训练和验证集中删除这些特征，包括所有值缺失、所有行中具有相同值或极高基数（例如哈希值、ID 或 GUID）的特征。
    |'
- en: '| Impute missing values | For numerical features, impute with average of values
    in the column.For categorical features, impute with most frequent value. |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 补充缺失值 | 对于数值特征，用该列值的平均值进行填补。对于分类特征，用最频繁出现的值进行填补。 |'
- en: '| Generate additional features | For DateTime features: Year, Month, Day, Day
    of week, Day of year, Quarter, Week of the year, Hour, Minute, Second.For Text
    features: Term frequency based on unigrams, bi-grams, and tri-character-grams.
    |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 生成额外特征 | 对于日期时间特征：年、月、日、星期几、年的天数、季度、年的第几周、小时、分钟、秒。对于文本特征：基于单个词、双字母和三字符组的词频。
    |'
- en: '| Transform and encode | Numeric features with few unique values are transformed
    into categorical features.One-hot encoding is performed for low cardinality categorical;
    for high cardinality, one-hot-hash encoding. |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 转换和编码 | 具有少量唯一值的数值特征被转换为分类特征。低基数分类特征执行独热编码；高基数分类特征执行独热哈希编码。 |'
- en: '| Word embeddings | Text featurizer that converts vectors of text tokens into
    sentence vectors using a pretrained model. In a given document, each word’s embedding
    vector is aggregated to produce a document feature vector. |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 词嵌入 | 文本特征化器，使用预训练模型将文本标记的向量转换为句向量。在给定文档中，聚合每个词的嵌入向量以生成一个文档特征向量。 |'
- en: '| Target encodings | For categorical features, maps each category to averaged
    target value for regression problems. For classification problems, maps each category
    to the class probability for each class. Frequency-based weighting and *k*-fold
    cross-validation is applied to reduce overfitting of the mapping and noise caused
    by sparse data categories. |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 目标编码 | 对于分类特征，将每个类别映射到回归问题的平均目标值。对于分类问题，将每个类别映射到每个类的类概率。应用基于频率的加权和 *k* 折交叉验证以减少映射过拟合和稀疏数据类别引起的噪声。
    |'
- en: '| Text target encoding | For text input, a stacked linear model with bag-of-words
    is used to generate the probability of each class. |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 文本目标编码 | 对于文本输入，使用带有词袋的堆叠线性模型生成每个类的概率。 |'
- en: '| Weight of Evidence (WoE) | Calculates WoE as a measure of correlation of
    categorical columns to the target column. It is calculated as the log of the ratio
    of in-class versus out-of-class probabilities. This step outputs one numerical
    feature column per class and removes the need to explicitly impute missing values
    and outlier treatment. |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 权重证据（WoE） | 计算 WoE 作为衡量分类列与目标列相关性的指标。它被计算为类内与类外概率比值的对数。此步骤为每个类输出一个数值特征列，并且无需显式填补缺失值和处理异常值。
    |'
- en: '| Cluster distance | Trains a *k*-means clustering model on all numerical columns.
    Outputs *k* new features, one new numerical feature per cluster, containing the
    distance of each sample to the centroid of each cluster. |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 聚类距离 | 在所有数值列上训练 *k*-均值聚类模型。输出 *k* 个新特征，每个聚类的样本到聚类中心的距离。 |'
- en: Auto-Featurization for Classification and Regression
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类和回归的自动特征化
- en: To show auto-featurization in action, let’s work through a predictive maintenance
    model using the NASA Turbofan Engine Degradation Simulation dataset. In this example,
    even though we show how regression is used to predict the remaining useful lifetime
    (RUL) value for the turbofan engine, we can apply the same approach to classification
    problems as well.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示自动特征化的实际应用，让我们通过使用 NASA 涡轮风扇发动机退化模拟数据集来处理预测维护模型。在此示例中，尽管我们展示了如何使用回归来预测涡轮风扇发动机的剩余使用寿命（RUL）值，但我们也可以将相同的方法应用于分类问题。
- en: 'To do this, let’s first download the dataset using the code block that follows.
    After you download the dataset, you extract the file into the data folder, and
    read the training data file, *data/train_FD004.txt*. Then, you add the column
    names for the 26 features. Use the following code to do this:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，让我们首先使用接下来的代码块下载数据集。在下载数据集之后，将文件解压缩到数据文件夹，并读取训练数据文件 *data/train_FD004.txt*。然后，为这26个特征添加列名。使用以下代码执行此操作：
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: An important part of the data science process is to explore the dataset. Since
    we use this dataset in other chapters, we won’t explore it here. In addition,
    we’ll omit the steps needed to create the Azure Machine Learning experiment and
    set up the `AutoMLConfig` object (shown earlier) and proceed directly to exploring
    the differences and quality of results when `preprocess` is set to different values
    (i.e., `True` or `False`).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学流程的一个重要部分是探索数据集。由于我们在其他章节中使用了此数据集，因此我们在这里不会探索它。此外，我们将省略创建 Azure 机器学习实验和设置
    `AutoMLConfig` 对象（如前所示）所需的步骤，并直接进行探索当 `preprocess` 设置为不同值（即 `True` 或 `False`）时的差异和结果质量。
- en: 'Before we do that, let’s define the utility functions that will be useful in
    the exploration. We will create two utility functions: `print_model()` ([Example 4-1](#EX1)),
    and `print_engineered_features()` ([Example 4-2](#EX2)). These two utility functions
    are used to print the pipelines for a model, and the features that are generated
    during auto-featurization, respectively, as shown in the following examples.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们执行此操作之前，让我们定义将在探索中有用的实用函数。我们将创建两个实用函数：`print_model()`（[示例 4-1](#EX1)）和 `print_engineered_features()`（[示例 4-2](#EX2)）。这两个实用函数用于打印模型的管道和自动特征化期间生成的特征，如以下示例所示。
- en: Example 4-1\. *print_model*
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-1\. *print_model*
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Example 4-2\. *print_engineered_features*
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-2\. *print_engineered_features*
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that we have defined the two utility functions, let’s explore two iterations
    for an experiment in which `preprocess` is set to `False`, and the data preprocessing
    shown in the outputs are similar. ([Figure 4-1](#data_preprocessing_using_automated_ml)
    shows the output after the experiment is submitted.) Iterations 4 and 5 of the
    experiment use the same data processing technique (`StandardScalerWrapper`) and
    the same machine learning algorithm (`LightGBM`). What’s the difference between
    the two iterations, and why do they show two different R2 score values? Iteration
    5 (R2 score of 0.6220) seems to have performed better than iteration 4 (R2 score
    of 0.4834).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了这两个实用函数，让我们探索一个实验的两次迭代，其中 `preprocess` 被设置为 `False`，并且输出中显示的数据预处理类似。
    （[图 4-1](#data_preprocessing_using_automated_ml) 显示了实验提交后的输出。）实验的第 4 和第 5 次迭代使用了相同的数据处理技术（`StandardScalerWrapper`）和相同的机器学习算法（`LightGBM`）。两次迭代之间有什么区别，以及它们为什么显示出两个不同的
    R2 分数值？第 5 次迭代（R2 分数为 0.6220）似乎比第 4 次迭代（R2 分数为 0.4834）表现更好。
- en: 'Using `local_run.get_output()`, we extracted the run and models that have been
    trained for iterations 4 and 5\. The run information is stored in *explore_run1*
    and *explore_run2*, and the model details are stored in *explore_model1* and *explore_model2*:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `local_run.get_output()`，我们提取了经过训练的第 4 和第 5 次迭代的运行和模型。运行信息存储在 *explore_run1*
    和 *explore_run2* 中，而模型细节则存储在 *explore_model1* 和 *explore_model2* 中：
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: After you have extracted the run information and model details, let’s look at
    them closely. From the output for iterations 4 and 5 shown, you will notice the
    hyperparameter values are different (e.g., `max_bin`, `max_depth`, `learning_rate`,
    `reg_alpha`, `reg_lambda`, and others). These hyperparameter values are determined
    by the automated ML meta-model that has been trained to decide which machine learning
    pipeline will be most relevant to the dataset (see Examples [4-3](#EX3) and [4-4](#EX4).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在提取了运行信息和模型细节后，让我们仔细查看它们。从所示的第 4 和第 5 次迭代的输出中，您会注意到超参数值不同（例如，`max_bin`、`max_depth`、`learning_rate`、`reg_alpha`、`reg_lambda`
    等）。这些超参数值由经过训练的自动 ML 元模型确定，该模型用于决定哪个机器学习流水线对数据集最相关（参见示例 [4-3](#EX3) 和 [4-4](#EX4)）。
- en: Note
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: See [Chapter 2](ch02.html#how_automated_machine_learning_works) for more on
    how Automated Machine Learning works.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 查看更多有关自动化机器学习工作原理的信息，请参阅 [第 2 章](ch02.html#how_automated_machine_learning_works)。
- en: Example 4-3\. *Iteration 4 run and model information*
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-3\. *迭代 4 的运行和模型信息*
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Example 4-4\. *Iteration 5 run and model information*
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-4\. *迭代 5 的运行和模型信息*
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, let’s look at the names of the engineered features. To do this, you can
    use the function `get_engineered_feature_names ()`. The code shows how you retrieve
    the best run and model by using `local_run.get_output()` and then extract the
    names of the engineered features:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看工程特征的名称。为此，您可以使用函数 `get_engineered_feature_names()`。代码显示了如何通过使用 `local_run.get_output()`
    检索最佳运行和模型，然后提取工程特征的名称：
- en: '[PRE7]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[Figure 4-2](#names_of_engineered_features) shows the output. In this example,
    you will see that the engineered features are derived from using the `MeanImputer`
    transform on the existing features. No additional features have been added.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-2](#names_of_engineered_features) 显示了输出。在这个例子中，您将看到工程特征是通过在现有特征上使用 `MeanImputer`
    转换来导出的。没有添加额外的特征。'
- en: '![paml 0402](assets/paml_0402.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0402](assets/paml_0402.png)'
- en: Figure 4-2\. Names of engineered features
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-2\. 工程特征的名称
- en: Let’s dive deeper and look at more details about the engineered features. To
    do this, use the `get_featurization_summary()` function. The utility function
    `print_engineered_features()` that we defined earlier will enable us to pretty-print
    the output and make it easier to read.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解更多关于工程特征的细节。为此，使用 `get_featurization_summary()` 函数。我们之前定义的 `print_engineered_features()`
    实用函数将帮助我们漂亮地打印输出并使其更易于阅读。
- en: '[Figure 4-3](#summary_of_engineered_features) shows the summary of the engineered
    features. For each original feature, you will see that the `MeanImputer` transform
    is applied to it and that the count for new engineered features is `1`. You will
    also observe that no features were dropped when data preprocessing and auto-featurization
    are performed:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-3](#summary_of_engineered_features) 显示了工程特征的摘要。对于每个原始特征，您会看到应用了 `MeanImputer`
    转换，并且新工程特征的计数为 `1`。您还会观察到在数据预处理和自动特征化时未丢弃任何特征：'
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![paml 0403](assets/paml_0403.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0403](assets/paml_0403.png)'
- en: Figure 4-3\. Summary of engineered features
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-3\. 工程特征总结
- en: Auto-Featurization for Time-Series Forecasting
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于时间序列预测的自动特征化
- en: In this next example, we show how data preprocessing and auto-featurization
    is performed for a time-series dataset, in which the data type for some of the
    features is `DateTime`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个示例中，我们展示了如何为时间序列数据集执行数据预处理和自动特征工程，其中某些特征的数据类型为 `DateTime`。
- en: 'Let’s begin by downloading the sample Energy Demand dataset ([Figure 4-4](#exploring_the_energy_demand_time-series)
    shows the output from running the code):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始下载样本能源需求数据集（[图 4-4](#exploring_the_energy_demand_time-series) 显示了运行代码的输出）：
- en: '[PRE9]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In [Figure 4-4](#exploring_the_energy_demand_time-series), you can see that
    the Energy Demand time-series dataset consists of these five columns: `ID` (leftmost
    column), `timestamp`, `demand`, `precip`, and `temp`.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 4-4](#exploring_the_energy_demand_time-series) 中，您可以看到能源需求时间序列数据集由以下五列组成：`ID`（最左列）、`timestamp`、`demand`、`precip`
    和 `temp`。
- en: '![paml 0404](assets/paml_0404.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0404](assets/paml_0404.png)'
- en: Figure 4-4\. Exploring the Energy Demand time-series dataset
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-4\. 探索能源需求时间序列数据集
- en: 'Let’s do a simple plot of the data by using the following code ([Figure 4-5](#visualization_of_the_energy_demand_time)
    shows the output):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下代码对数据进行简单的绘图（[图 4-5](#visualization_of_the_energy_demand_time) 显示输出结果）：
- en: '[PRE10]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![paml 0405](assets/paml_0405.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0405](assets/paml_0405.png)'
- en: Figure 4-5\. Visualization of the Energy Demand time-series dataset
  id: totrans-98
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-5\. 能源需求时间序列数据集的可视化
- en: 'Next, let’s split the data into training and testing datasets, into observations
    before 2017-02-01 (training dataset), and observations after 2017-02-01 (testing
    dataset). We extract the target column (the column for the demand values) into
    `y_train` and `y_test`:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将数据拆分为训练集和测试集，将 2017-02-01 之前的观察（训练集）和 2017-02-01 之后的观察（测试集）分开。我们将目标列（需求值列）提取为
    `y_train` 和 `y_test`：
- en: '[PRE11]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Let’s specify the automated ML configuration that we will use for forecasting.
    In the code that follows, notice that we specify the evaluation metrics for the
    `AutoMLConfig` object as the normalized root-mean-square error (RMSE). We also
    specify the `DateTime` column using `time_column_name`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们指定用于预测的自动化 ML 配置。在随后的代码中，请注意，我们将评估指标指定为 `AutoMLConfig` 对象的标准化均方根误差（RMSE）。我们还使用
    `time_column_name` 指定了 `DateTime` 列。
- en: 'As each row of the data denotes hourly observations, it is important to specify
    the time horizon for prediction by using the property `max_horizon`. Suppose that
    you want to predict for the next one day (i.e., 24 hours); the value of `max_horizon`
    is set to `24`. The property `country_or_region` is commented out in this example.
    This property is useful if you want to take into consideration autogenerated features
    that capture details about the holidays in the country specified. In this specific
    example, we do not need it; thus, we comment it out:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据的每行表示每小时的观测值，因此通过使用 `max_horizon` 属性指定预测的时间范围非常重要。假设您想预测未来一天（即 24 小时），则
    `max_horizon` 的值设置为 `24`。在本示例中，我们不需要考虑 `country_or_region` 属性；因此，我们对其进行了注释。
- en: '[PRE12]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now that you have defined the `AutoMLConfig` object, you are ready to submit
    the experiment. [Figure 4-6](#running_the_automated_ml_experiment) presents the
    output of running the experiment. When the automated ML experiment is run, you
    will see that the experiment starts by performing auto-featurization on the time-series
    dataset. This is captured in the steps “Current status: DatasetFeaturization.
    Beginning to featurize the dataset.” and “Current status: DatasetFeaturizationCompleted.
    Completed featurizing the dataset.” After featurization is completed, model selection
    using automated ML begins.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经定义了 `AutoMLConfig` 对象，可以准备提交实验。[图 4-6](#running_the_automated_ml_experiment)
    展示了运行实验的输出。运行自动化 ML 实验时，您会看到实验从对时间序列数据集执行自动特征工程开始。这些步骤包括“当前状态：DatasetFeaturization.
    开始对数据集进行特征工程。”和“当前状态：DatasetFeaturizationCompleted. 完成对数据集的特征工程。”特征工程完成后，将开始使用自动化
    ML 进行模型选择。
- en: '![paml 0406](assets/paml_0406.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0406](assets/paml_0406.png)'
- en: Figure 4-6\. Running the automated ML experiment
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-6\. 运行自动化 ML 实验
- en: 'During model selection, automated ML runs several iterations. Each iteration
    uses different data preprocessing methods (e.g., `RobustScaler`, `StandardScalerWrapper`,
    `MinMaxScaler`, `MaxAbsScaler`) and forecasting algorithms (`ElasticNet`, `LightGB`,
    `LassoLars`, `DecisionTree`, and `RandomForest`). The last two iterations use
    different ensemble methods (e.g., `VotingEnsemble` and `StackEnsemble`). For this
    specific example, the best result is achieved in iteration 9, which uses `StackEnsemble`:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型选择过程中，自动化ML运行了多个迭代。每个迭代使用不同的数据预处理方法（例如`RobustScaler`、`StandardScalerWrapper`、`MinMaxScaler`、`MaxAbsScaler`）和预测算法（`ElasticNet`、`LightGB`、`LassoLars`、`DecisionTree`和`RandomForest`）。最后两个迭代使用了不同的集成方法（例如`VotingEnsemble`和`StackEnsemble`）。在此特定示例中，第9次迭代达到了最佳结果，使用了`StackEnsemble`：
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now, let’s retrieve detailed information about the best run and the model. [Figure 4-7](#retrieving_information_for_the_best_run)
    shows the summary of the engineered features. As this is a time-series dataset,
    you’ll notice that for the feature timestamp, 11 additional features are autogenerated
    (i.e., `EngineeredFeatureCount` is shown as 11), all of data type `DateTime`.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们检索有关最佳运行和模型的详细信息。[图 4-7](#retrieving_information_for_the_best_run)显示了工程特征的摘要。由于这是一个时间序列数据集，你会注意到针对时间戳的11个额外特征是自动生成的（即`EngineeredFeatureCount`显示为11），全部为`DateTime`数据类型。
- en: '[PRE14]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![paml 0407](assets/paml_0407.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0407](assets/paml_0407.png)'
- en: Figure 4-7\. Retrieving information for the best run
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-7\. 检索最佳运行的信息
- en: 'Let’s now examine the features autogenerated for the `DateTime` column. To
    do this, we’ll use `fitted_model` for performing forecasting, using the test data
    we defined earlier. From the following code, we invoke the `forecast` function,
    and the results are stored in the variables `y_fcst` and `X_trans`:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们检查为`DateTime`列自动生成的特征。为此，我们将使用`fitted_model`执行预测，使用我们之前定义的测试数据。从以下代码中，我们调用`forecast`函数，并将结果存储在变量`y_fcst`和`X_trans`中：
- en: '[PRE15]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Next we turn to `X_trans`. In [Figure 4-8](#engineered_features_for_time-series_fore),
    you can see the 11 engineered features, which took the `DateTime` column and divided
    it into the time parts (e.g., year, half, quarter, month, day, hour, am_pm, hour12,
    wday, qday, and week). Changing it from a `DateTime` to a numerical value makes
    it more meaningful and easier to use by the machine learning algorithms during
    training and scoring.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们转向`X_trans`。在[图 4-8](#engineered_features_for_time-series_fore)中，您可以看到这11个工程特征，它们将`DateTime`列分成了时间部分（例如年、半年、季度、月份、日、小时、上午/下午、12小时制、周日和周数）。将其从`DateTime`转换为数值使其更有意义，并且在训练和评分过程中更容易被机器学习算法使用。
- en: '![paml 0408](assets/paml_0408.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0408](assets/paml_0408.png)'
- en: Figure 4-8\. Engineered features for time-series forecasting
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-8\. 用于时间序列预测的工程特征
- en: Conclusion
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'In this chapter, you learned about the importance of feature engineering, and
    how it affects the quality of the machine learning models produced. Feature engineering
    is an art: to do it well, it’s important to understand its foundations, to receive
    on-the-job training, and to build your toolbox for doing feature engineering as
    you work through various machine learning projects. In recent years, the machine
    learning community has been innovating on Python libraries that enable auto-featurization.
    For example, you can use the Python package `featuretools` to perform deep feature
    synthesis by taking advantage of the relationships between entities, and more.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您了解了特征工程的重要性，以及它如何影响生成的机器学习模型的质量。特征工程是一门艺术：要做好特征工程，重要的是理解其基础知识，接受在职培训，并在通过各种机器学习项目时建立特征工程工具箱。近年来，机器学习社区一直在创新Python库，以实现自动特征工程。例如，您可以使用Python包`featuretools`利用实体之间的关系执行深度特征合成等操作。
- en: We focused in this chapter on how to use the auto-featurization capabilities
    provided by automated ML in the Azure Machine Learning service. Using examples
    of regression and forecasting, we explored how to enable auto-featurization in
    automated ML and how to understand the engineered features.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点讨论了如何在Azure机器学习服务中利用自动化ML提供的自动特征工程能力。通过回归和预测的示例，我们探讨了如何在自动化ML中启用自动特征工程，以及如何理解工程特征。
- en: Though automated ML provides auto-featurization capabilities (that are continuously
    improving and evolving), note that it doesn’t exhaustively cover all aspects of
    feature engineering. It’s important for data scientists to perform feature engineering,
    taking advantage of domain expertise, before using the dataset as input to automated
    ML.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然自动化机器学习提供了自动特征工程的能力（这些能力不断改进和演变），但请注意它并不详尽涵盖所有特征工程的方面。在使用数据集作为自动化机器学习的输入之前，数据科学家有必要进行特征工程，利用领域专业知识。
