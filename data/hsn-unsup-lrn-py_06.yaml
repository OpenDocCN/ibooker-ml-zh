- en: Chapter 4\. Anomaly Detection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章 异常检测
- en: In [Chapter 3](ch03.html#Chapter_3), we introduced the core dimensionality reduction
    algorithms and explored their ability to capture the most salient information
    in the MNIST digits database in significantly fewer dimensions than the original
    784 dimensions. Even in just two dimensions, the algorithms meaningfully separated
    the digits, without using labels. This is the power of unsupervised learning algorithms—they
    can learn the underlying structure of data and help discover hidden patterns in
    the absence of labels.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第三章](ch03.html#Chapter_3) 中，我们介绍了核心的降维算法，并探讨了它们在 MNIST 数字数据库中以显著较少的维度捕获最显著信息的能力。即使在仅两个维度下，这些算法也能有意义地分离数字，而无需使用标签。这就是无监督学习算法的力量
    — 它们能够学习数据的潜在结构，并在缺乏标签的情况下帮助发现隐藏的模式。
- en: Let’s build an applied machine learning solution using these dimensionality
    reduction methods. We will turn to the problem we introduced in [Chapter 2](ch02.html#Chapter_2)
    and build a credit card fraud detection system without using labels.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这些降维方法构建一个应用的机器学习解决方案。我们将回顾在 [第二章](ch02.html#Chapter_2) 中介绍的问题，并构建一个无需使用标签的信用卡欺诈检测系统。
- en: In the real world, fraud often goes undiscovered, and only the fraud that is
    caught provides any labels for the datasets. Moreover, fraud patterns change over
    time, so supervised systems that are built using fraud labels—like the one we
    built in [Chapter 2](ch02.html#Chapter_2)—become stale, capturing historical patterns
    of fraud but failing to adapt to newly emerging patterns.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，欺诈通常不会被发现，只有被捕获的欺诈行为提供了数据集的标签。此外，欺诈模式随时间变化，因此使用欺诈标签构建的监督系统（如我们在 [第二章](ch02.html#Chapter_2)
    中构建的系统）变得过时，捕捉到的是历史上的欺诈模式，而不能适应新出现的欺诈模式。
- en: For these reasons (the lack of sufficient labels and the need to adapt to newly
    emerging patterns of fraud as quickly as possible), unsupervised learning fraud
    detection systems are in vogue.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 出于这些原因（标签不足和尽快适应新出现的欺诈模式的需求），无监督学习欺诈检测系统备受青睐。
- en: In this chapter, we will build such a solution using some of the dimensionality
    reduction algorithms we explored in the previous chapter.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用前一章探索的一些降维算法来构建这样一个解决方案。
- en: Credit Card Fraud Detection
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信用卡欺诈检测
- en: Let’s revisit the credit card transactions problem from [Chapter 2](ch02.html#Chapter_2).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新访问来自 [第二章](ch02.html#Chapter_2) 的信用卡交易问题。
- en: Prepare the Data
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备数据
- en: Like we did in [Chapter 2](ch02.html#Chapter_2), let’s load the credit card
    transactions dataset, generate the features matrix and labels array, and split
    the data into training and test sets. We will not use the labels to perform anomaly
    detection, but we will use the labels to help evaluate the fraud detection systems
    we build.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在 [第二章](ch02.html#Chapter_2) 中所做的那样，让我们加载信用卡交易数据集，生成特征矩阵和标签数组，并将数据拆分为训练集和测试集。我们不会使用标签来执行异常检测，但我们将使用标签来帮助评估我们构建的欺诈检测系统。
- en: As a reminder, we have 284,807 credit card transactions in total, of which 492
    are fraudulent, with a positive (fraud) label of one. The rest are normal transactions,
    with a negative (not fraud) label of zero.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下，总共有 284,807 笔信用卡交易，其中 492 笔是欺诈交易，具有正面（欺诈）标签为一。其余的是正常交易，具有负面（非欺诈）标签为零。
- en: 'We have 30 features to use for anomaly detection—time, amount, and 28 principal
    components. And, we will split the dataset into a training set (with 190,820 transactions
    and 330 cases of fraud) and a test set (with the remaining 93,987 transactions
    and 162 cases of fraud):'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有 30 个特征用于异常检测 — 时间、金额和 28 个主成分。然后，我们将数据集分为一个训练集（包含 190,820 笔交易和 330 笔欺诈案例）和一个测试集（剩余
    93,987 笔交易和 162 笔欺诈案例）：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Define Anomaly Score Function
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义异常分数函数
- en: Next, we need to define a function that calculates how anomalous each transaction
    is. The more anomalous the transaction is, the more likely it is to be fraudulent,
    assuming that fraud is rare and looks somewhat different than the majority of
    transactions, which are normal.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义一个函数来计算每笔交易的异常程度。交易越异常，它被识别为欺诈的可能性就越大，假设欺诈很少且看起来与大多数正常交易不同。
- en: As we discussed in the previous chapter, dimensionality reduction algorithms
    reduce the dimensionality of data while attempting to minimize the reconstruction
    error. In other words, these algorithms try to capture the most salient information
    of the original features in such a way that they can reconstruct the original
    feature set from the reduced feature set as well as possible. However, these dimensionality
    reduction algorithms cannot capture all the information of the original features
    as they move to a lower dimensional space; therefore, there will be some error
    as these algorithms reconstruct the reduced feature set back to the original number
    of dimensions.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一章讨论的那样，降维算法在试图最小化重建误差的同时减少数据的维度。换句话说，这些算法试图以尽可能好的方式捕捉原始特征的最显著信息，以便从降维特征集重建原始特征集。然而，这些降维算法不能捕捉所有原始特征的信息，因为它们移动到较低维空间；因此，在将这些算法从降维特征集重建回原始维数时会存在一些误差。
- en: In the context of our credit card transactions dataset, the algorithms will
    have the largest reconstruction error on those transactions that are hardest to
    model—in other words, those that occur the least often and are the most anomalous.
    Since fraud is rare and presumably different than normal transactions, the fraudulent
    transactions should exhibit the largest reconstruction error. So let’s define
    the anomaly score as the reconstruction error. The reconstruction error for each
    transaction is the sum of the squared differences between the original feature
    matrix and the reconstructed matrix using the dimensionality reduction algorithm.
    We will scale the sum of the squared differences by the max-min range of the sum
    of the squared differences for the entire dataset, so that all the reconstruction
    errors are within a zero to one range.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的信用卡交易数据集的背景下，算法将在最难建模的交易中产生最大的重建误差——换句话说，那些发生最少且最异常的交易。由于欺诈很少且可能与正常交易不同，欺诈交易应该表现出最大的重建误差。因此，让我们将异常分数定义为重建误差。每笔交易的重建误差是原始特征矩阵与使用降维算法重建的矩阵之间差异平方和。我们将通过整个数据集的差异平方和的最大-最小范围来缩放差异平方和的总和，以使所有重建误差都在零到一的范围内。
- en: The transactions that have the largest sum of squared differences will have
    an error close to one, while those that have the smallest sum of squared differences
    will have an error close to zero.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 具有最大差异平方和的交易将具有接近一的误差，而具有最小差异平方和的交易将具有接近零的误差。
- en: This should be familiar. Like the supervised fraud detection solution we built
    in [Chapter 2](ch02.html#Chapter_2), the dimensionality reduction algorithm will
    effectively assign each transaction an anomaly score between zero and one. Zero
    is normal and one is anomalous (and most likely to be fraudulent).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该是熟悉的。就像我们在[第2章](ch02.html#Chapter_2)中构建的监督式欺诈检测解决方案一样，降维算法将有效地为每笔交易分配一个介于零和一之间的异常分数。零表示正常，一表示异常（最有可能是欺诈）。
- en: 'Here is the function:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是函数：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Define Evaluation Metrics
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义评估指标
- en: Although we will not use the fraud labels to build the unsupervised fraud detection
    solutions, we will use the labels to evaluate the unsupervised solutions we develop.
    The labels will help us understand just how well these solutions are at catching
    known patterns of fraud.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们不会使用欺诈标签来构建无监督的欺诈检测解决方案，但我们将使用这些标签来评估我们开发的无监督解决方案。这些标签将帮助我们了解这些解决方案捕捉已知欺诈模式的效果如何。
- en: As we did in [Chapter 2](ch02.html#Chapter_2), we will use the precision-recall
    curve, the average precision, and the auROC as our evaluation metrics.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在[第2章](ch02.html#Chapter_2)中所做的那样，我们将使用精确-召回曲线、平均精度和auROC作为我们的评估指标。
- en: 'Here is the function that will plot these results:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是将绘制这些结果的函数：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The fraud labels and the evaluation metrics will help us assess just how good
    the unsupervised fraud detection systems are at catching known patterns of fraud—fraud
    that we have caught in the past and have labels for.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 欺诈标签和评估指标将帮助我们评估无监督欺诈检测系统在捕捉已知欺诈模式（我们过去已经捕捉到并有标签的欺诈）方面的表现如何。
- en: However, we will not be able to assess how good the unsupervised fraud detection
    systems are at catching unknown patterns of fraud. In other words, there may be
    fraud in the dataset that is incorrectly labeled as not fraud because the financial
    company never discovered it.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们将无法评估无监督欺诈检测系统在捕捉未知欺诈模式方面的表现如何。换句话说，数据集中可能存在被错误标记为非欺诈的欺诈行为，因为金融公司从未发现它们。
- en: As you may see already, unsupervised learning systems are much harder to evaluate
    than supervised learning systems. Often, unsupervised learning systems are judged
    by their ability to catch known patterns of fraud. This is an incomplete assessment;
    a better evaluation metric would be to assess them on their ability to identify
    unknown patterns of fraud, both in the past and in the future.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经看到的那样，无监督学习系统比监督学习系统更难评估。通常，无监督学习系统是通过其捕捉已知欺诈模式的能力来评判的。这是一个不完整的评估；更好的评估指标应该是评估它们在识别未知欺诈模式方面的能力，无论是在过去还是在未来。
- en: Since we cannot go back to the financial company and have them evaluate any
    unknown patterns of fraud we identify, we will have to evaluate these unsupervised
    systems solely based on how well they detect the known patterns of fraud. It’s
    important to be mindful of this limitation as we proceed in evaluating the results.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们无法返回金融公司，并要求他们评估我们识别出的任何未知欺诈模式，因此我们将仅基于它们如何检测已知欺诈模式来评估这些无监督系统。在评估结果时，牢记这一限制非常重要。
- en: Define Plotting Function
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义绘图函数
- en: 'We will reuse the scatterplot function from [Chapter 3](ch03.html#Chapter_3)
    to display the separation of points the dimensionality reduction algorithm achieves
    in just the first two dimensions:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重用第 3 章中的散点图函数来展示降维算法在前两个维度上实现的点的分离情况：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Normal PCA Anomaly Detection
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 普通 PCA 异常检测
- en: In [Chapter 3](ch03.html#Chapter_3), we demonstrated how PCA captured the majority
    of information in the MNIST digits dataset in just a few principal components,
    far fewer in number than the original dimensions. In fact, with just two dimensions,
    it was possible to visually separate the images into distinct groups based on
    the digits they displayed.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 3 章中，我们演示了 PCA 如何仅通过少数几个主成分就捕获了 MNIST 数字数据集中的大部分信息，远少于原始维度。事实上，仅通过两个维度，就可以根据它们展示的数字将图像明显地分成不同的组。
- en: Building on this concept, we will now use PCA to learn the underlying structure
    of the credit card transactions dataset. Once we learn this structure, we will
    use the learned model to reconstruct the credit card transactions and then calculate
    how different the reconstructed transactions are from the original transactions.
    Those transactions that PCA does the poorest job of reconstructing are the most
    anomalous (and most likely to be fraudulent).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这一概念，我们现在将使用 PCA 来学习信用卡交易数据集的潜在结构。一旦学习了这种结构，我们将使用学习模型来重构信用卡交易，然后计算重构交易与原始交易的差异。那些
    PCA 重构效果最差的交易是最异常的（也最可能是欺诈性的）。
- en: Note
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Remember that the features in the credit card transactions dataset we have are
    already the output of PCA—this is what we were given by the financial company.
    However, there is nothing unusual about performing PCA for anomaly detection on
    an already dimensionality-reduced dataset. We just treat the original principal
    components that we are given as the original features.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我们拥有的信用卡交易数据集中的特征已经是 PCA 的输出结果 — 这是金融公司提供给我们的。然而，对于已经降维的数据集进行 PCA 异常检测并没有什么特别的。我们只需将给定的原始主成分视为原始特征即可。
- en: Going forward, we will refer to the original principal components that we were
    given as the original features. Any future mention of principal components will
    refer to the principal components from the PCA process rather than the original
    features we were given.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 今后，我们将称我们得到的原始主成分为原始特征。未来任何对主成分的提及都将指的是 PCA 过程中的主成分，而不是我们最初获得的原始特征。
- en: Let’s start by developing a deeper understanding of how PCA—and dimensionality
    reduction in general—helps perform anomaly detection. As we’ve defined it, anomaly
    detection relies on reconstruction error. We want the reconstruction error for
    rare transactions—the ones that are most likely to be fraudulent—to be as high
    as possible and the reconstruction error for the rest to be as low as possible.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从更深入地理解PCA及其在异常检测中的作用开始。正如我们所定义的，异常检测依赖于重构误差。我们希望罕见交易（最有可能是欺诈的交易）的重构误差尽可能高，而其余交易的重构误差尽可能低。
- en: For PCA, the reconstruction error will depend largely on the number of principal
    components we keep and use to reconstruct the original transactions. The more
    principal components we keep, the better PCA will be at learning the underlying
    structure of the original transactions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于PCA，重构误差主要取决于我们保留和用于重构原始交易的主要成分数量。我们保留的主要成分越多，PCA在学习原始交易的潜在结构方面表现越好。
- en: However, there is a balance. If we keep too many principal components, PCA may
    too easily reconstruct the original transactions, so much so that the reconstruction
    error will be minimal for all of the transactions. If we keep too few principal
    components, PCA may not be able to reconstruct any of the original transactions
    well enough—not even the normal, nonfraudulent transactions.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，需要平衡。如果我们保留太多主要成分，PCA可能会过于容易地重构原始交易，以至于所有交易的重构误差都将最小化。如果我们保留的主要成分太少，PCA可能无法充分重构任何原始交易，甚至是正常的非欺诈性交易。
- en: Let’s search for the right number of principal components to keep to build a
    good fraud detection system.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们寻找保留以构建良好的欺诈检测系统的正确主成分数。
- en: PCA Components Equal Number of Original Dimensions
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PCA成分等于原始维度数
- en: First, let’s think about something. If we use PCA to generate the same number
    of principal components as the number of original features, will we be able to
    perform anomaly detection?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们考虑一些事情。如果我们使用PCA生成与原始特征数相同数量的主要成分，我们能执行异常检测吗？
- en: If you think through this, the answer should be obvious. Recall our PCA example
    from the previous chapter for the MNIST digits dataset.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细思考，答案应该是显而易见的。回顾我们在前一章对MNIST数字数据集的PCA示例。
- en: When the number of principal components equals the number of original dimensions,
    PCA captures nearly 100% of the variance/information in the data as it generates
    the principal components. Therefore, when PCA reconstructs the transactions from
    the principal components, it will have too little reconstruction error for all
    the transactions, fraudulent or otherwise. We will not be able to differentiate
    between rare transactions and normal ones—in other words, anomaly detection will
    be poor.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当主要成分的数量等于原始维度的数量时，PCA会捕获数据中近乎100%的方差/信息，因为它生成主要成分。因此，当PCA从主要成分重构交易时，所有交易（无论是欺诈还是正常的）的重构误差都将太小。我们将无法区分罕见交易和正常交易，换句话说，异常检测将效果不佳。
- en: To highlight this, let’s apply PCA to generate the same number of principal
    components as the number of original features (30 for our credit card transactions
    dataset). This is accomplished with the `fit_transform` function from Scikit-Learn.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了突出这一点，让我们应用PCA生成与原始特征数相同数量的主要成分（对于我们的信用卡交易数据集为30个）。这是通过Scikit-Learn中的`fit_transform`函数实现的。
- en: 'To reconstruct the original transactions from the principal components we generate,
    we will use the `inverse_transform` function from Scikit-Learn:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从我们生成的主要成分中重构原始交易，我们将使用Scikit-Learn中的`inverse_transform`函数：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[Figure 4-1](#separation_of_observations_using_normal_pca_and_30_principal_components)
    shows the plot of the separation of transactions using the first two principal
    components of PCA.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-1](#separation_of_observations_using_normal_pca_and_30_principal_components)展示了使用PCA的前两个主要成分对交易进行分离的图表。'
- en: '![Separation of Obversations Using Normal PCA and 30 Principal Components](assets/hulp_0401.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![使用正常PCA和30个主要成分分离观察](assets/hulp_0401.png)'
- en: Figure 4-1\. Separation of observations using normal PCA and 30 principal components
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-1\. 使用正常PCA和30个主要成分分离观察
- en: 'Let’s calculate the precision-recall curve and the ROC curve:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算精确率-召回率曲线和ROC曲线：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: With an average precision of 0.11, this is a poor fraud detection solution (see
    [Figure 4-2](#results_using_30_principal_components)). It catches very little
    of the fraud.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 具有平均精度为0.11，这是一个较差的欺诈检测解决方案（参见[图 4-2](#results_using_30_principal_components)）。它几乎无法捕捉到欺诈行为。
- en: '![Results Using Normal PCA and 30 Principal Components](assets/hulp_0402.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![使用正常PCA和30个主成分的结果](assets/hulp_0402.png)'
- en: Figure 4-2\. Results using 30 principal components
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-2\. 使用 30 个主成分的结果
- en: Search for the Optimal Number of Principal Components
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 搜索最佳主成分数量
- en: Now, let’s perform a few experiments by reducing the number of principal components
    PCA generates and evaluate the fraud detection results. We need the PCA-based
    fraud detection solution to have enough error on the rare cases that it can meaningfully
    separate fraud cases from the normal ones. But the error cannot be so low or so
    high for all the transactions that the rare and normal transactions are virtually
    indistinguishable.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过减少PCA生成的主成分数量来执行一些实验，并评估欺诈检测结果。我们需要基于PCA的欺诈检测解决方案在罕见情况下有足够的误差，以便能够有效地区分欺诈案例和正常案例。但是误差不能对所有交易的罕见和正常交易都过低或过高，以至于它们几乎无法区分。
- en: After some experimentation, which you can perform using the [GitHub code](http://bit.ly/2Gd4v7e),
    we find that 27 principal components is the optimal number for this credit card
    transactions dataset.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 经过一些实验（可以使用[GitHub 代码](http://bit.ly/2Gd4v7e)执行），我们发现27个主成分是此信用卡交易数据集的最佳数量。
- en: '[Figure 4-3](#separation_of_observations_using_normal_pca_and_27_principal_components)
    shows the plot of the separation of transactions using the first two principal
    components of PCA.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-3](#separation_of_observations_using_normal_pca_and_27_principal_components)
    展示了使用PCA的前两个主成分分离交易的图表。'
- en: '![Separation of Obversations Using Normal PCA and 27 Principal Components](assets/hulp_0403.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![使用正常PCA和27个主成分分离观察](assets/hulp_0403.png)'
- en: Figure 4-3\. Separation of observations using normal PCA and 27 principal components
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-3\. 使用正常PCA和27个主成分分离观察
- en: '[Figure 4-4](#results_using_normal_pca_and_27_principal_components) shows the
    precision-recall curve, average precision, and auROC curve.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-4](#results_using_normal_pca_and_27_principal_components) 展示了精度-召回曲线、平均精度和auROC曲线。'
- en: '![Results Using Normal PCA and 27 Principal Components](assets/hulp_0404.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![使用正常PCA和27个主成分的结果](assets/hulp_0404.png)'
- en: Figure 4-4\. Results using normal PCA and 27 principal components
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-4\. 使用正常PCA和27个主成分的结果
- en: As you can see, we are able to catch 80% of the fraud with 75% precision. This
    is very impressive considering that we did not use any labels. To make these results
    more tangible, consider that there are 190,820 transactions in the training set
    and only 330 are fraudulent.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们能够以75%的精度捕捉到80%的欺诈行为。考虑到训练集中有190,820笔交易，其中只有330笔是欺诈交易，这是非常令人印象深刻的结果。
- en: Using PCA, we calculated the reconstruction error for each of these 190,820
    transactions. If we sort these transactions by highest reconstruction error (also
    referred to as anomaly score) in descending order and extract the top 350 transactions
    from the list, we can see that 264 of these transactions are fraudulent.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PCA，我们计算了这190,820笔交易中每笔交易的重建误差。如果我们按照重建误差（也称为异常分数）的降序对这些交易进行排序，并从列表中提取前350笔交易，我们可以看到其中有264笔交易是欺诈的。
- en: That is a precision of 75%. Moreover, the 264 transactions we caught from the
    350 we picked represent 80% of the total fraud in the training set (264 out of
    330 fraudulent cases). And, remember that we accomplished this without using labels.
    This is a truly unsupervised fraud detection solution.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这是75%的精度。此外，我们从我们选择的350笔交易中捕捉到的264笔交易代表了训练集中80%的总欺诈行为（330笔欺诈案例中的264笔）。而且，请记住，这是一个真正的无监督欺诈检测解决方案，没有使用标签。
- en: 'Here is the code to highlight this:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是突出显示此问题的代码：
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following code summarizes the results:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码总结了结果：
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Although this is a pretty good solution already, let’s try to develop fraud
    detection systems using some of the other dimensionality reduction methods.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这已经是一个相当好的解决方案，但让我们尝试使用其他降维方法开发欺诈检测系统。
- en: Sparse PCA Anomaly Detection
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稀疏PCA异常检测
- en: Let’s try to use sparse PCA to design a fraud detection solution. Recall that
    sparse PCA is similar to normal PCA but delivers a less dense version; in other
    words, sparse PCA provides a sparse representation of the principal components.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用稀疏 PCA 设计一个欺诈检测解决方案。回想一下，稀疏 PCA 类似于普通 PCA，但提供一个更稀疏的版本；换句话说，稀疏 PCA 提供了主成分的稀疏表示。
- en: We still need to specify the number of principal components we desire, but we
    must also set the alpha parameter, which controls the degree of sparsity. We will
    experiment with different values for the principal components and the alpha parameter
    as we search for the optimal sparse PCA fraud detection solution.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然需要指定所需的主成分数量，但我们还必须设置控制稀疏程度的 alpha 参数。在搜索最佳稀疏 PCA 欺诈检测解决方案时，我们将尝试不同的主成分值和
    alpha 参数值。
- en: Note that for normal PCA Scikit-Learn used a `fit_transform` function to generate
    the principal components and an `inverse_transform` function to reconstruct the
    original dimensions from the principal components. Using these two functions,
    we were able to calculate the reconstruction error between the original feature
    set and the reconstructed feature set derived from the PCA.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于普通 PCA，Scikit-Learn 使用 `fit_transform` 函数生成主成分，并使用 `inverse_transform`
    函数从主成分重构原始维度。利用这两个函数，我们能够计算原始特征集和从 PCA 派生的重构特征集之间的重构误差。
- en: Unfortunately, Scikit-Learn does not provide an `inverse_transform` function
    for sparse PCA. Therefore, we must reconstruct the original dimensions after we
    perform sparse PCA ourselves.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，Scikit-Learn 并未为稀疏 PCA 提供 `inverse_transform` 函数。因此，在执行稀疏 PCA 后，我们必须自行重构原始维度。
- en: 'Let’s begin by generating the sparse PCA matrix with 27 principal components
    and the default alpha parameter of 0.0001:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先生成具有 27 个主成分和默认 alpha 参数 0.0001 的稀疏 PCA 矩阵：
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[Figure 4-5](#separation_of_observations_using_sparse_pca_and_27_principal_components)
    shows the scatterplot for sparse PCA.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [4-5](#separation_of_observations_using_sparse_pca_and_27_principal_components)
    显示了稀疏 PCA 的散点图。
- en: '![Separation of Obversations Using Sparse PCA and 27 Principal Components](assets/hulp_0405.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![使用稀疏 PCA 和 27 个主成分的观察分离](assets/hulp_0405.png)'
- en: Figure 4-5\. Separation of observations using sparse PCA and 27 principal components
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-5\. 使用稀疏 PCA 和 27 个主成分的观察分离
- en: Now let’s generate the original dimensions from the sparse PCA matrix by simple
    matrix multiplication of the sparse PCA matrix (with 190,820 samples and 27 dimensions)
    and the sparse PCA components (a 27 x 30 matrix), provided by Scikit-Learn library.
    This creates a matrix that is the original size (a 190,820 x 30 matrix). We also
    need to add the mean of each original feature to this new matrix, but then we
    are done.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过稀疏 PCA 矩阵的简单矩阵乘法（包含 190,820 个样本和 27 个维度）和 Scikit-Learn 库提供的稀疏 PCA 成分（一个
    27 x 30 矩阵）生成稀疏 PCA 矩阵的原始维度。这样可以创建一个原始尺寸的矩阵（一个 190,820 x 30 矩阵）。我们还需要将每个原始特征的均值添加到这个新矩阵中，然后就完成了。
- en: 'From this newly derived inverse matrix, we can calculate the reconstruction
    errors (anomaly scores) as we did with normal PCA:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这个新推导出的逆矩阵，我们可以像对待普通 PCA 那样计算重构误差（异常分数）：
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now, let’s generate the precision-recall curve and ROC curve.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们生成精确率-召回率曲线和 ROC 曲线。
- en: '![Results Using Sparse PCA and 27 Principal Components](assets/hulp_0406.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![使用稀疏 PCA 和 27 个主成分的结果](assets/hulp_0406.png)'
- en: Figure 4-6\. Results using sparse PCA and 27 principal components
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-6\. 使用稀疏 PCA 和 27 个主成分的结果
- en: As [Figure 4-6](#results_using_sparse_pca_and_27_principal_components) shows,
    the results are identical to those of normal PCA. This is expected since normal
    and sparse PCA are very similar—the latter is just a sparse representaion of the
    former.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [4-6](#results_using_sparse_pca_and_27_principal_components) 所示，结果与普通 PCA
    的结果完全相同。这是预期的，因为普通 PCA 和稀疏 PCA 非常相似——后者只是前者的稀疏表示。
- en: Using the [GitHub code](http://bit.ly/2Gd4v7e), you can experiment by changing
    the number of principal components generated and the alpha parameter, but, based
    on our experimentation, this is the best sparse PCA-based fraud detection solution.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 [GitHub 代码](http://bit.ly/2Gd4v7e)，您可以通过更改生成的主成分数量和 alpha 参数来进行实验，但根据我们的实验，这是最佳的基于稀疏
    PCA 的欺诈检测解决方案。
- en: Kernel PCA Anomaly Detection
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kernel PCA 异常检测
- en: Now let’s design a fraud detection solution using kernel PCA, which is a nonlinear
    form of PCA and is useful if the fraud transactions are not linearly separable
    from the nonfraud transactions.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们设计一个欺诈检测解决方案，使用核PCA，这是PCA的非线性形式，如果欺诈交易与非欺诈交易不是线性可分的，它将非常有用。
- en: We need to specify the number of components we would like to generate, the kernel
    (we will use the RBF kernel as we did in the previous chapter), and the gamma
    (which is set to 1/n_features by default, so 1/30 in our case). We also need to
    set the `fit_inverse_transform` to `true` to apply the built-in `inverse_transform`
    function provided by Scikit-Learn.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要指定要生成的组件数量，内核（我们将使用RBF内核，就像我们在上一章中做的那样），以及gamma（默认情况下设置为1/n_features，因此在我们的情况下为1/30）。我们还需要将`fit_inverse_transform`设置为`true`，以应用Scikit-Learn提供的内置`inverse_transform`函数。
- en: Finally, because kernel PCA is so expensive to train with, we will train on
    just the first two thousand samples in the transactions dataset. This is not ideal
    but it is necessary to perform experiments quickly.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，由于核PCA在训练中非常昂贵，我们将仅在交易数据集的前两千个样本上进行训练。这并非理想选择，但为了快速进行实验，这是必要的。
- en: 'We will use this training to transform the entire training set and generate
    the principal components. Then, we will use the `inverse_transform` function to
    recreate the original dimension from the principal components derived by kernel
    PCA:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这个训练来转换整个训练集并生成主成分。然后，我们将使用`inverse_transform`函数从由核PCA导出的主成分重新创建原始维度：
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[Figure 4-7](#separation_of_obversations_using_kernel_pca_and_27_principal_components)
    shows the scatterplot for kernel PCA.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-7](#separation_of_obversations_using_kernel_pca_and_27_principal_components)显示了核PCA的散点图。'
- en: '![Separation of Obversations Using Kernel PCA and 27 Principal Components](assets/hulp_0407.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![使用核PCA和27个主成分分离观察](assets/hulp_0407.png)'
- en: Figure 4-7\. Separation of observations using kernel PCA and 27 principal components
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-7\. 使用核PCA和27个主成分分离观察
- en: Now, let’s calculate the anomaly scores and print the results.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们计算异常分数并打印结果。
- en: '![Results Using Kernel PCA and 27 Principal Components](assets/hulp_0408.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![使用核PCA和27个主成分的结果](assets/hulp_0408.png)'
- en: Figure 4-8\. Results using kernel PCA and 27 principal components
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-8\. 使用核PCA和27个主成分的结果
- en: As [Figure 4-8](#results_using_kernel_pca_and_27_principal_components) shows,
    the results are far worse than those for normal PCA and sparse PCA. While it was
    worth experimenting with kernel PCA, we will not use this solution for fraud detection
    given that we have better performing solutions from earlier.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 4-8](#results_using_kernel_pca_and_27_principal_components)所示，其结果远不如普通PCA和稀疏PCA。虽然进行核PCA实验是值得的，但考虑到我们有更好的性能解决方案，我们不会将其用于欺诈检测。
- en: Note
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: We will not build an anomaly detection solution using SVD because the solution
    is very similar to that of normal PCA. This is expected—PCA and SVD are closely
    related.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会使用SVD构建异常检测解决方案，因为其解决方案与普通PCA非常相似。这是预期的——PCA和SVD密切相关。
- en: Instead, let’s move to random projection-based anomaly detection.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 反而，让我们转向基于随机投影的异常检测。
- en: Gaussian Random Projection Anomaly Detection
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高斯随机投影异常检测
- en: Now, let’s try to develop a fraud detection solution using Gaussian random projection.
    Remember that we can set either the number of components we want or the *eps*
    parameter, which controls the quality of the embedding derived based on the Johnson–Lindenstrauss
    lemma.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试使用高斯随机投影开发欺诈检测解决方案。请记住，我们可以设置我们想要的组件数量或*eps*参数，后者控制基于Johnson-Lindenstrauss引理导出的嵌入质量。
- en: We will choose to explicitly set the number of components. Gaussian random projection
    trains very quickly, so we can train on the entire training set.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将选择显式设置组件的数量。高斯随机投影训练非常快，因此我们可以在整个训练集上进行训练。
- en: 'As with sparse PCA, we will need to derive our own `inverse_transform` function
    because none is provided by Scikit-Learn:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 与稀疏PCA一样，我们需要推导出自己的`inverse_transform`函数，因为Scikit-Learn没有提供这样的函数：
- en: '[PRE11]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[Figure 4-9](#separation_of_observations_using_gaussian_random_projection_and_27_components)
    shows the scatterplot for Gaussian random projection. [Figure 4-10](#results_using_gaussian_random_projection_and_27_components)
    displays the results for Gaussian random projection.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-9](#separation_of_observations_using_gaussian_random_projection_and_27_components)显示了高斯随机投影的散点图。[图 4-10](#results_using_gaussian_random_projection_and_27_components)显示了高斯随机投影的结果。'
- en: '![Separation of Obversations Using Gaussian Random Projection and 27 Components](assets/hulp_0409.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![使用高斯随机投影和 27 个分量分离观察结果](assets/hulp_0409.png)'
- en: Figure 4-9\. Separation of observations using Gaussian random projection and
    27 components
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-9\. 使用高斯随机投影和 27 个分量分离观察结果
- en: '![Results Using Gaussian Random Projection and 27 Components](assets/hulp_0410.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![使用高斯随机投影和 27 个分量的结果](assets/hulp_0410.png)'
- en: Figure 4-10\. Results using Gaussian random projection and 27 components
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-10\. 使用高斯随机投影和 27 个分量的结果
- en: These results are poor, so we won’t use Gaussian random projection for fraud
    detection.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果很差，因此我们不会使用高斯随机投影进行欺诈检测。
- en: Sparse Random Projection Anomaly Detection
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稀疏随机投影异常检测
- en: Let’s try to design a fraud detection solution using sparse random projection.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用稀疏随机投影设计一个欺诈检测解决方案。
- en: 'We will designate the number of components we want (instead of setting the
    *eps* parameter). And, like with Gaussian random projection, we will use our own
    `inverse_transform` function to create the original dimensions from the sparse
    random projection-derived components:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将指定我们需要的分量数量（而不是设置 *eps* 参数）。而且，就像使用高斯随机投影一样，我们将使用我们自己的 `inverse_transform`
    函数从稀疏随机投影派生的分量中创建原始维度：
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[Figure 4-11](#separation_of_obversations_using_sparse_random_projection_and_27_components)
    shows the scatterplot for sparse random projection. [Figure 4-12](#results_using_sparse_random_projection_and_27_components)
    displays the results for sparse random projection.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-11](#separation_of_obversations_using_sparse_random_projection_and_27_components)
    显示了稀疏随机投影的散点图。 [图 4-12](#results_using_sparse_random_projection_and_27_components)
    展示了稀疏随机投影的结果。'
- en: '![Separation of Obversations Using Sparse Random Projection and 27 Components](assets/hulp_0411.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![使用稀疏随机投影和 27 个分量分离观察结果](assets/hulp_0411.png)'
- en: Figure 4-11\. Separation of observations using sparse random projection and
    27 components
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-11\. 使用稀疏随机投影和 27 个分量分离观察结果
- en: '![Results Using Sparse Random Projection and 27 Components](assets/hulp_0412.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![使用稀疏随机投影和 27 个分量的结果](assets/hulp_0412.png)'
- en: Figure 4-12\. Results using sparse random projection and 27 components
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-12\. 使用稀疏随机投影和 27 个分量的结果
- en: As with Gaussian random projection, these results are poor. Let’s continue to
    build anomaly detection systems using other dimensionality reduction methods.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 和高斯随机投影一样，这些结果很差。让我们继续使用其他降维方法构建异常检测系统。
- en: Nonlinear Anomaly Detection
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非线性异常检测
- en: So far, we have developed fraud detection solutions using linear dimensionality
    reduction methods such as normal PCA, sparse PCA, Gaussian random projection,
    and sparse random projection. We also developed a solution using the nonlinear
    version of PCA—kernel PCA.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经使用了线性降维方法开发了欺诈检测解决方案，如常规 PCA、稀疏 PCA、高斯随机投影和稀疏随机投影。我们还使用了非线性版本的 PCA——核
    PCA。
- en: At this point, PCA is by far the best solution.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，PCA 是迄今为止最好的解决方案。
- en: 'We could turn to nonlinear dimensionality reduction algorithms, but the open
    source versions of these algorithms run very slowly and are not viable for fast
    fraud detection. Therefore, we will skip this and go directly to nondistance-based
    dimensionality reduction methods: dictionary learning and independent component
    analysis.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以转向非线性降维算法，但这些算法的开源版本运行非常缓慢，不适合快速欺诈检测。因此，我们将跳过这一步，直接转向非距离度量的降维方法：字典学习和独立分量分析。
- en: Dictionary Learning Anomaly Detection
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 字典学习异常检测
- en: Let’s use dictionary learning to develop a fraud detection solution. Recall
    that, in dictionary learning, the algorithm learns the sparse representation of
    the original data. Using the vectors in the learned dictionary, each instance
    in the original data can be reconstructed as a weighted sum of these learned vectors.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用字典学习来开发一个欺诈检测解决方案。回想一下，在字典学习中，算法学习原始数据的稀疏表示。使用学习字典中的向量，可以将原始数据中的每个实例重构为这些学习向量的加权和。
- en: For anomaly detection, we want to learn an undercomplete dictionary so that
    the vectors in the dictionary are fewer in number than the original dimensions.
    With this constraint, it will be easier to reconstruct the more frequently occurring
    normal transactions and much more difficult to construct the rarer fraud transactions.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 对于异常检测，我们希望学习一个欠完备字典，使得字典中的向量数量少于原始维度。在这个约束条件下，更容易重构出频繁发生的正常交易，但更难构建出罕见的欺诈交易。
- en: In our case, we will generate 28 vectors (or components). To learn the dictionary,
    we will feed in 10 batches, where each batch has 200 samples.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们将生成28个向量（或成分）。为了学习字典，我们将提供10个批次，每个批次包含200个样本。
- en: 'We will need to use our own `inverse_transform` function, too:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也需要使用我们自己的`inverse_transform`函数：
- en: '[PRE13]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[Figure 4-13](#separation_of_obversations_using_dictionary_learning_and_28_components)
    shows the scatterplot for dictionary learning. [Figure 4-14](#results_using_dictionary_learning_and_28_components)
    shows the results for dictionary learning.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-13](#separation_of_obversations_using_dictionary_learning_and_28_components)展示了字典学习的散点图。[图 4-14](#results_using_dictionary_learning_and_28_components)展示了字典学习的结果。'
- en: '![Separation of Obversations Using Dictionary Learning and 28 Components](assets/hulp_0413.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![使用字典学习和28个成分的观测分离](assets/hulp_0413.png)'
- en: Figure 4-13\. Separation of observations using dictionary learning and 28 components
  id: totrans-143
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-13\. 使用字典学习和28个成分的观测分离
- en: '![Results Using Dictionary Learning and 28 Components](assets/hulp_0414.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![使用字典学习和28个成分的结果](assets/hulp_0414.png)'
- en: Figure 4-14\. Results using dictionary learning and 28 components
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-14\. 使用字典学习和28个成分的结果
- en: These results are much better than those for kernal PCA, Gaussian random projection,
    and sparse random projection but are no match for those of normal PCA.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果远优于核PCA、高斯随机投影和稀疏随机投影的结果，但与普通PCA的结果不相上下。
- en: You can experiment with the code on GitHub to see if you could improve on this
    solution, but, for now, PCA remains the best fraud detection solution for this
    credit card transactions dataset.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub上尝试代码，看看是否能改进这个解决方案，但目前来看，PCA仍然是这个信用卡交易数据集的最佳欺诈检测解决方案。
- en: ICA Anomaly Detection
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ICA异常检测
- en: Let’s use ICA to design our last fraud detection solution.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用ICA设计我们的最后一个欺诈检测解决方案。
- en: 'We need to specify the number of components, which we will set to 27\. Scikit-Learn
    provides an `inverse_transform` function so we do not need to use our own:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要指定成分的数量，我们将设置为27。Scikit-Learn提供了一个`inverse_transform`函数，因此我们不需要使用自己的函数：
- en: '[PRE14]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[Figure 4-15](#separation_of_obversations_using_independent_component_analysis_and_27_components)
    shows the scatterplot for ICA. [Figure 4-16](#results_using_independent_component_analysis_and_27_components)
    shows the results for ICA.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-15](#separation_of_obversations_using_independent_component_analysis_and_27_components)展示了ICA的散点图。[图 4-16](#results_using_independent_component_analysis_and_27_components)展示了ICA的结果。'
- en: '![Separation of Obversations Using Dictionary Learning and 27 Components](assets/hulp_0415.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![使用字典学习和28个成分的观测分离](assets/hulp_0415.png)'
- en: Figure 4-15\. Separation of observations using ICA and 27 components
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-15\. 使用ICA和27个成分的观测分离
- en: '![Results Using Independent Component Analsysis and 27 Components](assets/hulp_0416.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![独立成分分析和27个成分的结果](assets/hulp_0416.png)'
- en: Figure 4-16\. Results using ICA and 27 components
  id: totrans-156
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-16\. 使用ICA和27个成分的结果
- en: These results are identical to those of normal PCA. The fraud detection solution
    using ICA matches the best solution we’ve developed so far.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果与普通PCA的结果相同。ICA的欺诈检测解决方案与我们迄今为止开发的最佳解决方案相匹配。
- en: Fraud Detection on the Test Set
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试集上的欺诈检测
- en: 'Now, to evaluate our fraud detection solutions, let’s apply them to the never-before-seen
    test set. We will do this for the top three solutions we’ve developed: normal
    PCA, ICA, and dictionary learning. We will not use sparse PCA because it is very
    similar to the normal PCA solution.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了评估我们的欺诈检测解决方案，让我们将其应用于前所未见的测试集。我们将对我们开发的前三种解决方案进行评估：普通PCA、ICA和字典学习。我们不会使用稀疏PCA，因为它与普通PCA解决方案非常相似。
- en: Normal PCA Anomaly Detection on the Test Set
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 普通PCA在测试集上的异常检测
- en: Let’s start with normal PCA. We will use the PCA embedding that the PCA algorithm
    learned from the training set and use this to transform the test set. We will
    then use the Scikit-Learn `inverse_transform` function to recreate the original
    dimensions from the principal components matrix of the test set.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从普通PCA开始。我们将使用PCA算法从训练集学习到的PCA嵌入，并用此转换测试集。然后，我们将使用Scikit-Learn的`inverse_transform`函数从测试集的主成分矩阵重新创建原始维度。
- en: 'By comparing the original test set matrix with the newly reconstructed one,
    we can calculate the anomaly scores (as we’ve done many times before in this chapter):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较原始测试集矩阵和新重建的矩阵，我们可以计算异常分数（正如我们在本章中多次做过的）：
- en: '[PRE15]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[Figure 4-17](#separation_of_obversations_using_pca_and_27_components_on_the_test_set)
    shows the scatterplot for PCA on the test set. [Figure 4-18](#results_using_pca_and_27_components_on_the_test_set)
    displays the results for PCA on the test set.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-17](#separation_of_obversations_using_pca_and_27_components_on_the_test_set)
    显示了在测试集上使用 PCA 的散点图。[图 4-18](#results_using_pca_and_27_components_on_the_test_set)
    显示了在测试集上使用 PCA 的结果。'
- en: '![Separation of Obversations Using PCA and 27 Components on the Test Set](assets/hulp_0417.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![在测试集上使用 PCA 和 27 个分量的观察结果分离](assets/hulp_0417.png)'
- en: Figure 4-17\. Separation of observations using PCA and 27 components on the
    test set
  id: totrans-166
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-17\. 在测试集上使用 PCA 和 27 个分量进行观察结果分离
- en: '![Results Using PCA and 27 Components on the Test Set](assets/hulp_0418.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![在测试集上使用 PCA 和 27 个分量的结果](assets/hulp_0418.png)'
- en: Figure 4-18\. Results using PCA and 27 components on the test set
  id: totrans-168
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-18\. 在测试集上使用 PCA 和 27 个分量的结果
- en: These are impressive results. We are able to catch 80% of the known fraud in
    the test set with an 80% precision—all without using any labels.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是令人印象深刻的结果。我们能够在测试集中捕捉到 80% 的已知欺诈，精度为 80%——而且全部不使用任何标签。
- en: ICA Anomaly Detection on the Test Set
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在测试集上的 ICA 异常检测
- en: 'Let’s now move to ICA and perform fraud detection on the test set:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们转向 ICA，并在测试集上进行欺诈检测：
- en: '[PRE16]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[Figure 4-19](#separation_of_obversations_using_independent_component_analysis_and_27_components_on_the_test_set)
    shows the scatterplot for ICA on the test set. [Figure 4-20](#results_using_independent_component_analysis_and_27_components_on_the_test_set)
    shows the results for ICA on the test set.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-19](#separation_of_obversations_using_independent_component_analysis_and_27_components_on_the_test_set)
    显示了在测试集上使用 ICA 的散点图。[图 4-20](#results_using_independent_component_analysis_and_27_components_on_the_test_set)
    显示了在测试集上使用 ICA 的结果。'
- en: '![Separation of Obversations Using Independent Component Analysis and 27 Components
    on the Test Set](assets/hulp_0419.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![在测试集上使用独立分量分析和 27 个分量的观察结果分离](assets/hulp_0419.png)'
- en: Figure 4-19\. Separation of observations using ICA and 27 components on the
    test set
  id: totrans-175
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-19\. 在测试集上使用 ICA 和 27 个分量进行观察结果分离
- en: '![Results Using Independent Component Analysis and 27 Components on the Test
    Set](assets/hulp_0420.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![在测试集上使用独立分量分析和 27 个分量的结果](assets/hulp_0420.png)'
- en: Figure 4-20\. Results using ICA and 27 components on the test set
  id: totrans-177
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-20\. 在测试集上使用 ICA 和 27 个分量的结果
- en: The results are identical to normal PCA and thus quite impressive.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 结果与常规 PCA 完全相同，因此令人印象深刻。
- en: Dictionary Learning Anomaly Detection on the Test Set
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在测试集上的字典学习异常检测
- en: 'Let’s now turn to dictionary learning, which did not perform as well as normal
    PCA and ICA but is worth a final look:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们转向字典学习，虽然它的表现不如常规 PCA 和 ICA，但仍值得最后一看：
- en: '[PRE17]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[Figure 4-21](#separation_of_obversations_using_dictionary_learning_and_28_components_on_the_test_set)
    shows the scatterplot for dictionary learning on the test set. [Figure 4-22](#results_using_the_dictionary_learning_and_28_components_on_the_test_set)
    displays the results for dictionary learning on the test set.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-21](#separation_of_obversations_using_dictionary_learning_and_28_components_on_the_test_set)
    显示了在测试集上使用字典学习的散点图。[图 4-22](#results_using_the_dictionary_learning_and_28_components_on_the_test_set)
    显示了在测试集上使用字典学习的结果。'
- en: '![Separation of Obversations Using Dictionary Learning and 28 Components on
    the Test Set](assets/hulp_0421.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![在测试集上使用字典学习和 28 个分量的观察结果分离](assets/hulp_0421.png)'
- en: Figure 4-21\. Separation of observations using dictionary learning and 28 components
    on the test set
  id: totrans-184
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-21\. 在测试集上使用字典学习和 28 个分量进行观察结果分离
- en: '![Results Using Dictionary Learning and 28 Components on the Test Set](assets/hulp_0422.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![在测试集上使用字典学习和 28 个分量的结果](assets/hulp_0422.png)'
- en: Figure 4-22\. Results using dictionary learning and 28 components on the test
    set
  id: totrans-186
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-22\. 在测试集上使用字典学习和 28 个分量的结果
- en: While the results are not terrible—we can catch 80% of the fraud with a 20%
    precision—they fall far short of the results from normal PCA and ICA.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管结果并不糟糕——我们可以用 20% 的精度捕捉到 80% 的欺诈——但与常规 PCA 和 ICA 的结果相比差距很大。
- en: Conclusion
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, we used the core dimensionality reduction algorithms from the
    previous chapter to develop fraud detection solutions for the credit card transactions
    dataset from [Chapter 2](ch02.html#Chapter_2).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用了上一章的核心降维算法来针对[第 2 章](ch02.html#Chapter_2)的信用卡交易数据集开发欺诈检测解决方案。
- en: In [Chapter 2](ch02.html#Chapter_2) we used labels to build a fraud detection
    solution, but we did not use any labels during the training process in this chapter.
    In other words, we built an applied fraud detection system using unsupervised
    learning.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二章中，我们使用标签构建了一个欺诈检测解决方案，但是在本章的训练过程中我们没有使用任何标签。换句话说，我们使用无监督学习构建了一个应用型欺诈检测系统。
- en: While not all the dimensionality reduction algorithms performed well on this
    credit card transactions dataset, two performed remarkably well—normal PCA and
    ICA.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然并非所有降维算法在这个信用卡交易数据集上表现良好，但是有两个表现非常出色——普通PCA和ICA。
- en: Normal PCA and ICA caught over 80% of the known fraud with an 80% precision.
    By comparison, the best-performing supervised learning-based fraud detection system
    from [Chapter 2](ch02.html#Chapter_2) caught nearly 90% of the known fraud with
    an 80% precision. The unsupervised fraud detection system is only marginally worse
    than the supervised system at catching known patterns of fraud.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 普通的PCA和ICA可以捕捉到80%以上的已知欺诈，并且精度达到80%。相比之下，第二章中表现最佳的基于有监督学习的欺诈检测系统几乎可以捕捉到90%的已知欺诈，并且精度达到80%。无监督欺诈检测系统在捕捉已知欺诈模式方面只比有监督系统稍微差一点。
- en: Recall that unsupervised fraud detection systems require no labels for training,
    adapt well to changing fraud patterns, and can catch fraud that had gone previously
    undiscovered. Given these additional advantages, the unsupervised learning-based
    solution will generally perform better than the supervised learning-based solution
    at catching known and unknown or newly emerging patterns of fraud in the future,
    although using both in tandem is best.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，无监督的欺诈检测系统在训练过程中不需要标签，能够很好地适应不断变化的欺诈模式，并且可以发现以前未被发现的欺诈行为。考虑到这些额外的优势，无监督学习的解决方案通常会比有监督学习的解决方案更好地捕捉到未来已知和未知或新出现的欺诈模式，尽管将两者结合使用效果最佳。
- en: Now that we’ve covered dimensionality reduction and anomaly detection, let’s
    explore clustering, another major concept in the field of unsupervised learning.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了降维和异常检测，让我们来探讨聚类，这是无监督学习领域的另一个重要概念。
