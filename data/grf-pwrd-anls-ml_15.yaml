- en: Chapter 12\. Improving Fraud Detection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章 改进诈骗检测
- en: In an earlier chapter, we took on the problem of fraud detection by designing
    graph queries that looked for certain patterns of behavior that could be suspicious.
    This chapter will apply machine learning methods to improve fraud detection. Machine
    learning can help us via anomaly detection or by training the software to recognize
    fraud based on examples of known fraud cases. In both cases, graph-structured
    data is a valuable asset for sensing the unusual (anomalies) or for supplying
    data features (to build predictive models). No method is perfect, but machine
    learning can often detect patterns and anomalies that humans would miss. Conventional
    approaches only follow the rules that experts dictate. Using machine learning
    on graphs, we can detect patterns within the data that were not explicitly flagged
    as fraud cases, which makes it more adaptive to changing fraud tactics.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期的一章中，我们通过设计图查询来处理诈骗检测问题，这些查询寻找某些行为模式，这些模式可能是可疑的。本章将应用机器学习方法来改进诈骗检测。机器学习可以通过异常检测或者通过训练软件识别基于已知诈骗案例的方法来帮助我们。无论哪种情况，图结构化数据都是感知异常（异常）或提供数据特征（用于构建预测模型）的宝贵资产。没有绝对完美的方法，但机器学习通常可以检测到人类会忽略的模式和异常。传统方法只遵循专家规定的规则。通过在图上应用机器学习，我们可以检测到数据中未明确标记为诈骗案例的模式，这使其更适应于变化中的诈骗策略。
- en: 'After completing this chapter, you should be able to:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章后，您应能够：
- en: Deploy and use the TigerGraph Machine Learning Workbench
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署并使用TigerGraph机器学习工作台
- en: Use graph-based features to enrich the feature vector of a dataset and then
    compare the model accuracies with and without the graph features
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于图的特征来丰富数据集的特征向量，然后将带有图特征和不带图特征的模型精度进行比较。
- en: Prepare data for and train a graph neural network for node prediction—in this
    case, fraud prediction
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为节点预测准备数据并训练图神经网络——在本例中是诈骗预测
- en: 'Goal: Improve Fraud Detection'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标：改进诈骗检测
- en: Fraud is the use of deception for personal enrichment. Fraudsters might sabotage
    a system and its users, but in the end it is for personal gain. Examples of fraudulent
    activities are identity theft, false or exaggerated insurance claims, and money
    laundering. Fraud detection is a set of activities to prevent fraudsters from
    carrying out such activities successfully. In many cases, fraudsters want to gain
    money from their effort. Therefore fraud detection is a common practice among
    financial institutions, but it is also prevalent across organizations that hold
    valuable assets and properties, such as insurance, medical, governmental, and
    major retail organizations.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 诈骗是为了个人利益而使用欺骗手段。诈骗者可能会破坏系统及其用户，但最终目的是为了个人利益。欺诈活动的例子包括身份盗窃、虚假或夸大的保险索赔以及洗钱。诈骗检测是一组活动，旨在防止诈骗者成功地进行此类活动。在许多情况下，诈骗者希望从他们的努力中获取金钱。因此，诈骗检测是金融机构中常见的实践，但在持有有价值资产和财产的组织中，如保险、医疗、政府和主要零售组织中也很普遍。
- en: Fraud is a major business risk and is increasingly difficult to combat. According
    to a study from LexisNexis, every $1 of fraud costs $3.75 for companies within
    the ecommerce and retail sectors in the US, which is an increase of 19.8% since
    2019.^([1](ch12.html#ch01fn48)) These fraud costs come from fraudulent transactions
    due to identity fraud, which includes misuse of stolen identity or personal information.
    Fraud detection is becoming more challenging because of the growing channels through
    which fraudsters can operate. For example, an alarming trend is that fraud costs
    are surging via mobile smartphones. During the COVID-19 pandemic, consumers were
    pushed to do more digital transactions. Many of those transactions rely on smartphones,
    which have opened up new ways for fraudsters to mislead people.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 诈骗是一个重大的商业风险，越来越难以应对。根据LexisNexis的一项研究，对于美国电子商务和零售部门的公司，每1美元的诈骗成本为公司带来3.75美元的损失，这是自2019年以来增加了19.8%。[^1]
    这些诈骗成本来自于因身份欺诈而产生的欺诈交易，其中包括对被盗身份或个人信息的误用。由于欺诈者可以操作的渠道越来越多，诈骗检测变得更加具有挑战性。例如，一个令人担忧的趋势是，通过移动智能手机进行的诈骗成本激增。在COVID-19大流行期间，消费者被推动进行更多的数字交易。许多这类交易依赖于智能手机，这为诈骗者提供了欺骗人们的新途径。
- en: Cryptocurrency is a popular medium of exchange for fraudsters. Instead of being
    issued and regulated by governments or central banks, cryptocurrencies are digital
    assets that are transferred between account holders, typically using an open and
    distributed ledger. This technology allows everyone to participate in the exchange
    without identification through a central authority, making money laundering, scams,
    and theft more appealing. In 2021, criminals stole $14 billion in cryptocurrency,
    and crypto-related crime rose to 79% from 2020.^([2](ch12.html#ch01fn49))
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 加密货币是欺诈者流行的交易媒介。与由政府或央行发行和监管的货币不同，加密货币是在账户持有人之间传递的数字资产，通常使用开放和分布式账本。这项技术让每个人都能参与交易，而无需通过中央机构进行身份识别，使洗钱、诈骗和盗窃更具吸引力。2021年，犯罪分子窃取了价值140亿美元的加密货币，与2020年相比，与加密货币有关的犯罪案件增加了79%。^([2](ch12.html#ch01fn49))
- en: 'Solution: Use Relationships to Make a Smarter Model'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决方案：利用关系建立更智能的模型
- en: Fraud can be detected if more facts about the parties and activities involved
    can be gathered and connected to see how they fit together. For example, suppose
    we find unusual transaction behavior, such as moving a high volume of money between
    accounts in a short period of time. Statistically, a certain percentage of such
    behavior is due to fraud. However, if those accounts have a connection to entities
    that central authorities have sanctioned, then the possibility of a fraud case
    becomes greater. In other words, when using transactional data in isolation, we
    can see a limited aspect of the case, but when we connect that data to another
    dataset that identifies sanctioned entities, we can take into account the path
    length between a party and a sanctioned entity. Using those relationships between
    different datasets is greater than the sum of its parts.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果能够收集更多有关参与方和活动的事实，并将它们联系在一起，欺诈行为就能被发现。例如，假设我们发现异常的交易行为，比如在短时间内在账户之间转移大量资金。统计上，这种行为的一定比例是由于欺诈。然而，如果这些账户与中央机构制裁的实体有关联，那么欺诈案的可能性就变得更大。换句话说，在孤立使用交易数据时，我们只能看到案件的有限方面，但当我们将这些数据连接到另一个识别受制裁实体的数据集时，就可以考虑参与方与受制裁实体之间的路径长度。利用不同数据集之间的关系比各部分之和更重要。
- en: A graph is an excellent way to discover these relationships and patterns. In
    an earlier chapter, we saw how to use GSQL queries to detect particular patterns
    of interest. However, relying on the investigators to know the patterns in advance
    is limiting. A more powerful approach is to use machine learning to determine
    which patterns indicate fraud.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图是发现这些关系和模式的绝佳方式。在早前的章节中，我们看到了如何使用GSQL查询来检测感兴趣的特定模式。然而，依赖调查员事先了解这些模式是有限的。一个更强大的方法是利用机器学习确定哪些模式表明欺诈。
- en: Most machine learning methods analyze vectors or matrices. Each vector is a
    list of numerical characteristics or *features* of one type of entity, such as
    a person. The machine learning method looks for patterns among those features.
    The data scientist provides the system with a representative sample of actual
    fraudulent (and nonfraudulent) cases for the machine learning system to analyze.
    The machine learning system’s job is to extract a *model* that says, “When you
    have *these* feature values, you are likely to have fraud.”
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习方法都分析向量或矩阵。每个向量是一个数字特征或*特征*列表，表示某种类型实体（如个人）。机器学习方法寻找这些特征之间的模式。数据科学家为系统提供了一组实际欺诈（和非欺诈）案例的代表样本，供机器学习系统分析。机器学习系统的任务是提取一个*模型*，表明：“当你有*这些*特征数值时，很可能发生欺诈。”
- en: This approach has been a powerful tool in fighting fraud, but it is not perfect.
    One limitation is that a model is only as good as the training data provided.
    If our feature vectors are only describing direct characteristics of entities,
    then we are missing out on the deeper graph-oriented relationships that could
    be valuable. By combining the deeper insight that is available with graph analytics,
    we can enrich the input or training data, thereby producing more accurate machine
    learning models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在打击欺诈行为方面是一个强大的工具，但并非完美。一个局限性是模型只能像提供的训练数据那样好。如果我们的特征向量只描述实体的直接特征，那么我们就会错过可能有价值的更深层次的基于图的关系。通过结合可通过图形分析获得的更深入见解，我们可以丰富输入或训练数据，从而产生更准确的机器学习模型。
- en: In the following hands-on example, we will use the TigerGraph Machine Learning
    Workbench to help us extract graph features automatically to enrich training data
    as well as to run a graph neural network (GNN).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的实例中，我们将使用 TigerGraph 机器学习 Workbench 来帮助我们自动提取图特征，以丰富训练数据，并运行图神经网络（GNN）。
- en: Using the TigerGraph Machine Learning Workbench
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TigerGraph 机器学习 Workbench
- en: For this hands-on exercise, which is focused on machine learning, we will be
    using the TigerGraph Machine Learning Workbench, or ML Workbench for short. Based
    on the open source JupyterLab IDE for Python-oriented data scientists, and including
    TigerGraph’s Python library pyTigerGraph, the ML Workbench makes it simple to
    develop a machine learning pipeline that includes graph data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此集中练习，重点是机器学习，我们将使用 TigerGraph 机器学习 Workbench，简称 ML Workbench。基于开源的面向 Python
    数据科学家的 JupyterLab IDE，并包含 TigerGraph 的 Python 库 pyTigerGraph，ML Workbench 可以简化开发包含图数据的机器学习流程。
- en: Setting Up the ML Workbench
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 ML Workbench
- en: We will first obtain an instance of the ML Workbench on the TigerGraph Cloud
    service and then connect it to a database instance.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将在 TigerGraph 云服务上获取 ML Workbench 的一个实例，然后将其连接到数据库实例。
- en: Create a TigerGraph Cloud ML Bundle
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建 TigerGraph 云 ML Bundle
- en: 'The easiest way to set up the ML Workbench is to deploy a TigerGraph Cloud
    ML Bundle, which adds the ML Workbench as one of the tools available to use with
    a TigerGraph Cloud database instance:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 ML Workbench 的最简单方法是部署 TigerGraph 云 ML Bundle，它将 ML Workbench 添加为可用于 TigerGraph
    云数据库实例的工具之一：
- en: There is a small charge for using the ML Bundle, so you will need to set up
    payment information on your account.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 ML Bundle 需要支付少量费用，因此您需要在您的帐户上设置付款信息。
- en: From the Clusters screen of your TigerGraph Cloud account, click on the Create
    Cluster button.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 TigerGraph 云帐户的集群屏幕上，单击创建集群按钮。
- en: At the top of the Create Cluster page, select the ML Bundle option on the right.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在创建集群页面的顶部，选择右侧的 ML Bundle 选项。
- en: Select an instance size. The smallest one available is fine for this exercise.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个实例大小。此练习中可用的最小大小即可。
- en: We will be using a dataset and queries that are built into the ML Workbench,
    so it doesn’t matter what use case you select here. Finish setting any other options
    that you wish, then click Create Cluster at the bottom of the page.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用内置于 ML Workbench 中的数据集和查询，因此您在此处选择的用例并不重要。完成设置其他选项，然后在页面底部单击创建集群。
- en: The cluster takes a few minutes to provision.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 集群需要几分钟来配置。
- en: Create and copy database credentials
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建并复制数据库凭据
- en: 'The ML Workbench includes a robust series of example Jupyter notebooks that
    use pyTigerGraph to download datasets and create graphs in your cluster. Before
    it can do this, it must first gain access to the TigerGraph database using credentials
    that you provide:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ML Workbench 包括一系列强大的示例 Jupyter 笔记本，使用 pyTigerGraph 下载数据集并在您的集群中创建图表。但在此之前，它必须首先使用您提供的凭据访问
    TigerGraph 数据库：
- en: You should still be on the Clusters page of TigerGraph Cloud. For the cluster
    you just created, click Access Management.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您应该仍然在 TigerGraph 云的集群页面上。对于您刚创建的集群，请单击访问管理。
- en: Click the Database Access tab and then Add Database Users.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单击数据库访问选项卡，然后单击添加数据库用户。
- en: Enter a username and password. Be sure to remember both of these, since you
    will use them later in the ML Workbench.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入用户名和密码。请务必记住这两者，因为稍后在 ML Workbench 中会用到它们。
- en: Go to the Role Management tab next to Database Access.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到数据库访问旁边的角色管理选项卡。
- en: Select the checkbox next to your new user, set the role to globaldesigner, then
    click Save.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择您的新用户旁边的复选框，将角色设置为全局设计师，然后单击保存。
- en: Go to the Details tab. Copy the Domain, which ends in *i.tgcloud.io*.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到详细信息选项卡。复制域名，以 *i.tgcloud.io* 结尾。
- en: Connect the ML Workbench to your graph database
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 ML Workbench 连接到您的图数据库
- en: Go to GraphStudio for this database instance.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到此数据库实例的 GraphStudio。
- en: In the upper right corner, click on the Tools menu icon (an icon of a 3 × 3
    grid), and then select ML Workbench.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在右上角，单击工具菜单图标（一个 3 × 3 网格图标），然后选择 ML Workbench。
- en: After the workbench opens, find *config.json* in the left-side panel and double-click
    to edit it.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在工作台打开后，在左侧面板中找到 *config.json* 并双击进行编辑。
- en: Replace the URL value of `host` with the domain value that you copied. The resulting
    value should still start with *https://* and end with *i.tgcloud.io*.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用您复制的域名值替换 `host` 的 URL 值。结果值仍应以 *https://* 开头，并以 *i.tgcloud.io* 结尾。
- en: Change the `username` and `password` values to the username and password of
    the new user you created.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改 `username` 和 `password` 的值为您创建的新用户的用户名和密码。
- en: Although this process involves several steps, once you get used to the TigerGraph
    Cloud and ML Workbench interfaces, it will become second nature to grant ML Workbench
    access to your cluster through a new database user.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个过程涉及几个步骤，但一旦您习惯了 TigerGraph Cloud 和 ML Workbench 的界面，将会变得很容易通过一个新的数据库用户为
    ML Workbench 授予访问您的集群的权限。
- en: Working with ML Workbench and Jupyter Notes
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 ML Workbench 和 Jupyter Notes
- en: Double-click *README.md* in the file browser’s left panel in the ML Workbench,
    as shown in [Figure 12-1](#ml_workbench_and_the_readme_file), to get an overview
    of the general structure and capabilities of pyTigerGraph and the ML Workbench
    component.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 双击 ML Workbench 左侧面板的 *README.md*，如 [图 12-1](#ml_workbench_and_the_readme_file)
    所示，以获取有关 pyTigerGraph 和 ML Workbench 组件的一般结构和功能的概述。
- en: '![Image](assets/gpam_1201.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图像](assets/gpam_1201.png)'
- en: Figure 12-1\. ML Workbench and the README file
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-1\. ML Workbench 和 README 文件
- en: You just completed the Set Up section. Scroll down to the Learn section. Here
    you’ll see lists of tutorial and example notebooks for getting started, graph
    algorithms, GNNs, and end-to-end applications.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 您刚刚完成了设置部分。滚动到学习部分。在这里，您将看到教程和示例笔记本的列表，用于入门、图算法、GNN 和端到端应用程序。
- en: The remainder of this section walks through the *Datasets.ipynb* notebook, for
    users not familiar with Jupyter. If you are familiar with Jupyter, you should
    still go through this quickly to verify that your database connection is working.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此部分剩余内容介绍 *Datasets.ipynb* 笔记本，供不熟悉 Jupyter 的用户参考。如果您熟悉 Jupyter，您仍应快速浏览以验证数据库连接是否正常。
- en: 'Open the *Datasets.ipynb* notebook in the *Basics* folder. This file is a Jupyter
    notebook, which combines Python code snippets with explanatory comments. Python
    blocks are enumerated [1], [2], and so on. A thick blue bar at the left highlights
    the next section to be executed. Clicking the right-facing arrow at the command
    menu at the top will execute the next code block:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 打开 *Datasets.ipynb* 笔记本，位于 *Basics* 文件夹中。这个文件是一个 Jupyter 笔记本，结合了 Python 代码片段和解释性评论。Python
    代码块被编号为 [1]、[2] 等等。左侧的厚蓝条突出显示要执行的下一部分。单击顶部命令菜单上的右箭头将执行下一个代码块：
- en: Click the arrow until block [1] Download dataset starts to run.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击箭头直到块 [1] 下载数据集 开始运行。
- en: While it is running, the number in the brackets will change to an asterisk (*).
    When it completes, the asterisk will change back to the number. Pay attention
    to any output for information or error messages.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当它正在运行时，方括号中的数字将变成一个星号 (*)。当完成时，星号将再次变成数字。注意任何输出中的信息或错误消息。
- en: Note
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: When running an ML Workbench notebook, be sure your database is active (not
    paused). If it is paused, then when you try to run a block, the square brackets
    will contain an empty space [ ] instead of an asterisk [*].
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行 ML Workbench 笔记本时，请确保您的数据库处于活动状态（非暂停状态）。如果它被暂停了，那么当您尝试运行一个代码块时，方括号将包含一个空格
    [ ] 而不是一个星号 [*]。
- en: 'Run the next three Python blocks: Create connection, Ingest data, and Visualize
    schema.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行接下来的三个 Python 代码块：创建连接、导入数据和可视化模式。
- en: 'If you have a problem with Create connection, then you probably did not set
    up the *config.json* file correctly. The Ingest data step will take several seconds.
    The last step should conclude by showing you an image of a simple schema with
    **`Paper`** vertices and **`Cite`** edges. Looking back at the code blocks, we
    see that we made use of two pyTigerGraph libraries (`datasets` and `visualization`)
    and a few classes and methods: `TigerGraphConnection.ingestDataset` and `visualization.drawSchema`.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在创建连接时遇到问题，则可能没有正确设置 *config.json* 文件。导入数据步骤将需要几秒钟的时间。最后一步应该通过显示一个简单模式的图像来结束，其中包括
    **`Paper`** 顶点和 **`Cite`** 边。回顾代码块，我们看到我们使用了两个 pyTigerGraph 库（`datasets` 和 `visualization`）以及一些类和方法：`TigerGraphConnection.ingestDataset`
    和 `visualization.drawSchema`。
- en: The rest of the notebook ingests another dataset, IMDB. These two datasets are
    used by some of the other notebooks.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本的其余部分导入另一个数据集，IMDB。这两个数据集被一些其他笔记本使用。
- en: Graph Schema and Dataset
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 绘制模式和数据集
- en: For our graph machine learning example, we will now turn to the `fraud_detection`
    notebook inside the *applications* folder. The data used here are transactions
    on the Ethereum platform; Ether is the second-largest cryptocurrency in terms
    of market capitalization. The transactions form a graph, where vertices are wallets
    (i.e., accounts) on the platform, and edges are transactions between the accounts.
    There are 32,168 vertices and 84,088 directed edges pointing from the sending
    account to the receiving account. The dataset is derived from research by Liang
    Chen et al. in “Phishing Scams Detection in Ethereum Transaction Network,”^([3](ch12.html#ch01fn50))
    available from XBlock.^([4](ch12.html#ch01fn51)) Having directed edges tells us
    how the money is moving, which is important for any financial analysis, including
    fraud detection.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的图机器学习示例，现在我们将转向 *applications* 文件夹内的 `fraud_detection` 笔记本。这里使用的数据是以太坊平台上的交易数据；以太币是按市值计算第二大的加密货币。这些交易形成一个图形，其中顶点是平台上的钱包（即账户），边是账户之间的交易。数据集包含
    32,168 个顶点和 84,088 条从发送账户到接收账户的有向边。数据集源自梁晨等人在“以太坊交易网络中的网络钓鱼骗局检测”[^3] 中的研究，可从 XBlock[^4]
    获取。有向边告诉我们资金如何流动，这对于任何金融分析（包括欺诈检测）都非常重要。
- en: Each account vertex has an `is_fraud` parameter. The dataset has 1,165 accounts
    that are labeled as fraudulent. These were reported to be accounts belonging to
    phishing scammers, one of the most common forms of fraud in the cryptocurrency
    community.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 每个账户顶点都有一个 `is_fraud` 参数。数据集中有 1,165 个被标记为欺诈的账户。这些账户被报告为参与网络钓鱼骗局的账户，这是加密货币社区中最常见的欺诈形式之一。
- en: A typical phishing scam in the crypto economy occurs when the attacker sets
    up a site that promises to return a big reward with a small investment, usually
    promising that the victim is getting in early on a scheme that will see huge gains.
    The gains never come, and the initial investment is lost forever.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在加密经济中，典型的网络钓鱼骗局是指攻击者建立一个网站，承诺以小额投资获得巨大回报，通常宣称受害者早早参与了一个会带来巨大收益的计划。然而，这些承诺从未兑现，最初的投资永远丧失。
- en: Because these scammers accept many small transactions in a short amount of time
    and then move the money in larger chunks to other accounts, their transaction
    activity usually doesn’t match the profile of the typical legitimate cryptocurrency
    user. The vertices in the dataset have seven parameters—detailed in [Table 12-1](#graph_based_features_for_the_ethereum_t)—corresponding
    to the features described in Chen et al.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些骗子在短时间内接受许多小额交易，然后将资金以较大笔移至其他账户，他们的交易活动通常与典型合法加密货币用户的活动模式不匹配。数据集中的顶点具有七个参数，详细说明见[表 12-1](#graph_based_features_for_the_ethereum_t)，对应陈等人描述的特征。
- en: Importantly, none of these graph features are actually represented in the dataset
    upon loading. The dataset only contains the ID and `is_fraud` flag on the vertices
    (accounts) and the amount and timestamp on the edges (transactions). We use the
    given information to generate the graph features during the tutorial walkthrough.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，加载数据集时，这些图形特征实际上并不包含在内。数据集只包含顶点（账户）的 ID 和 `is_fraud` 标志，以及边（交易）上的金额和时间戳。在教程演示过程中，我们使用给定的信息生成图形特征。
- en: Table 12-1\. Graph-based features for the Ethereum transaction dataset
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12-1\. 以太坊交易数据集的基于图形的特征
- en: '| Feature | Description |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 特征 | 描述 |'
- en: '| --- | --- |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| FT1 | Indegree, or the number of incoming transactions for an account vertex
    |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| FT1 | 入度，或账户顶点的入向交易数 |'
- en: '| FT2 | Outdegree, or the number of outgoing transactions for an account vertex
    |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| FT2 | 出度，或账户顶点的外向交易数 |'
- en: '| FT3 | Degree, or the total number of transactions involving an account |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| FT3 | 度数，或涉及账户的总交易数 |'
- en: '| FT4 | In-strength, or the total monetary amount of all incoming transactions
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| FT4 | 入度，即所有入向交易的总金额 |'
- en: '| FT5 | Out-strength, or the total monetary amount of all outgoing transactions
    |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| FT5 | 出度，即所有外向交易的总金额 |'
- en: '| FT6 | Strength, or the total monetary amount of all transactions involving
    an account |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| FT6 | 强度，或涉及账户的所有交易的总金额 |'
- en: '| FT7 | Number of neighbors |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| FT7 | 邻居数量 |'
- en: '| FT8 | Inverse transaction frequency: this is the time interval between the
    account’s first and last transaction divided by FT3 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| FT8 | 反向交易频率：账户第一笔和最后一笔交易之间的时间间隔除以 FT3 |'
- en: Fraudulent accounts tend to have higher values for features 4, 5, and 6 and
    smaller values for feature 8\. Phishing attackers steal a great deal of money
    overall through many smaller transactions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 欺诈账户往往具有特征4、5和6的较高值，而特征8的较小值。网络钓鱼攻击者通过许多较小的交易总体上窃取了大量资金。
- en: FT7 (number of neighbors) differs from FT3 (number of transactions) because
    one neighbor could be responsible for multiple transactions. When we load the
    data into TigerGraph, we merge all the transactions between a pair of accounts
    into a single edge, so we do not use FT7\. Despite this simplification, we still
    achieve good results, as will soon be demonstrated. Additional features in a similar
    dataset may very well improve performance even further. While the exact graph
    features that a bank uses for fraud detection are both data dependent and trade
    secrets, it is generally believed that centrality algorithms like PageRank and
    community detection algorithms like Louvain have often been helpful.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: FT7（邻居数量）不同于FT3（交易数量），因为一个邻居可能负责多个交易。当我们将数据加载到TigerGraph时，我们将一对账户之间的所有交易合并为单个边缘，因此我们不使用FT7。尽管如此简化，我们仍然取得了良好的结果，很快将进行演示。在类似数据集中的其他特征可能会进一步改善性能。虽然银行用于欺诈检测的确切图特征既依赖于数据又是商业机密，但普遍认为像PageRank这样的中心性算法和像Louvain这样的社区检测算法通常是有帮助的。
- en: Although these metrics provide a quick intuitive look at the behavior of phishing
    attacks, both a traditional machine learning approach and a GNN are able to figure
    out a more precise relationship among all of the features in order to discriminate
    between accounts used for phishing and accounts used legitimately. In this chapter,
    we’ll compare the approaches and outcomes for both methods.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些指标提供了对网络钓鱼攻击行为的快速直观查看，但传统的机器学习方法和GNN能够找出所有特征之间更精确的关系，以区分用于网络钓鱼和合法用途的账户。在本章中，我们将比较这两种方法的方法和结果。
- en: Check that the first code block in the `fraud_detection` notebook has the same
    connection and credentials information that you set up in *config.json* before.
    Run the Database Preparation steps of the `fraud_detection` notebook to create
    the graph schema and load the data.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 检查“fraud_detection”笔记本中的第一个代码块，确保它具有您在*config.json*中设置的相同连接和凭据信息。运行“fraud_detection”笔记本的数据库准备步骤，以创建图模式并加载数据。
- en: Graph Feature Engineering
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图特征工程
- en: 'You should now be at the section entitled Graph Feature Engineering. As the
    notebook says, we use a pyTigerGraph `featurizer` object to generate features:
    two features from built-in algorithms (PageRank and betweenness centrality), and
    two from GSQL queries of our own. The Featurizer provides a high-level simplified
    process for generating and storing graph-based features. Algorithms in the GDS
    Library are automatically available to the Texturizer; users only need to specify
    some parameters.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在应该在标题为“图特征工程”的部分。正如笔记本上所说，我们使用一个名为pyTigerGraph的`featurizer`对象来生成特征：两个来自内置算法（PageRank和介数中心性）的特征，以及两个来自我们自己的GSQL查询。Featurizer提供了一个高级简化的过程，用于生成和存储基于图的特征。GDS库中的算法对Texturizer自动可用；用户只需指定一些参数。
- en: 'We call our object `f`. Run code block 4 to create it:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称我们的对象为`f`。运行代码块4以创建它：
- en: '[PRE0]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the PageRank section, we use the `tg_pagerank` algorithm included in the
    pre-installed Featurizer algorithm set. PageRank measures the influence of vertices
    in a graph. If a vertex is pointed to by many other vertices that *themselves*
    are pointed to by many vertices, it receives a high PageRank score. Each algorithm
    uses a set of input parameters. You can check the documentation for the [*TigerGraph
    GDS Library*](https://oreil.ly/2TAhH) to see what the parameters are for a particular
    algorithm. In the PageRank code block, we specify a Python dictionary of parameters
    and their values to pass to PageRank. Since this graph schema is so simple, the
    choice of vertex and edge type is made for us. We store the ranking value in each
    vertex under the attribute `pagerank`, then return the top five vertices with
    the highest values. There is a similar code block to generate betweenness centrality
    as a vertex feature. Betweenness is a slow algorithm; be patient.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在PageRank部分，我们使用预安装的Featurizer算法集中包含的`tg_pagerank`算法。PageRank衡量了图中顶点的影响力。如果一个顶点被许多其他顶点指向，而这些顶点本身又被许多顶点指向，它将获得高的PageRank分数。每种算法都使用一组输入参数。您可以查看[TigerGraph
    GDS Library](https://oreil.ly/2TAhH)的文档，了解特定算法的参数。在PageRank代码块中，我们指定一个Python字典，包含要传递给PageRank的参数及其值。由于此图架构非常简单，顶点和边类型的选择由系统决定。我们将排名值存储在每个顶点的`pagerank`属性下，然后返回排名值最高的五个顶点。还有一个类似的代码块生成介数中心性作为顶点特征。介数中心性是一个较慢的算法，请耐心等待。
- en: Next we calculate features for the transactions based on their degree (FT3 in
    the feature chart) and their amount (FT6, also known as strength). These use custom
    queries that can be found in the *GraphML/applications/fraud_detection/gsql* folder
    of the notebook. Run code blocks under Degree Features and Amount Features. Each
    one takes about 10 to 20 seconds.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们基于其度（FT3在特征图表中）和金额（FT6，也称为强度）计算交易的特征。这些使用可以在笔记本的*GraphML/applications/fraud_detection/gsql*文件夹中找到的自定义查询。运行“Degree
    Features”和“Amount Features”下的代码块。每个代码块大约需要10到20秒。
- en: 'Look at the queries to check your understanding of GSQL. The `amounts` query
    sets four vertex attributes for each vertex in the graph: the minimum received,
    the total amount received, the minimum sent, and the total amount sent. The `degrees`
    query is even simpler, just checking the number of transactions received (the
    indegree) and the number sent to other vertices (the outdegree).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 查看查询以检查您对GSQL的理解。`amounts`查询为图中的每个顶点设置四个顶点属性：最小接收量、总接收量、最小发送量和总发送量。`degrees`查询更简单，只检查接收到的交易数量（入度）和发送到其他顶点的数量（出度）。
- en: Now that we have a set of graph-related features on each vertex, including the
    ground truth of whether or not they are fraudulent accounts, we can use traditional
    supervised machine learning methods to try to predict fraud.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们在每个顶点上有一组与图相关的特征，包括它们是否为欺诈账户的真实情况，我们可以使用传统的监督学习方法尝试预测欺诈。
- en: Run the next code block for FastRP^([5](ch12.html#ch01fn52)) Embeddings. [FastRP](https://oreil.ly/PgE0O)
    is a vertex embedding algorithm based on the principle of random projection (RP)
    to perform dimensionality reduction. FastRP is also part of the TigerGraph built-in
    algorithm library. For relatively small datasets like this one, it provides very
    good performance with reasonable resource cost.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 运行下一个代码块以获取快速RP^([5](ch12.html#ch01fn52))嵌入。[FastRP](https://oreil.ly/PgE0O)是一种基于随机投影（RP）原理进行降维的顶点嵌入算法。对于像这样的相对小数据集，它提供了非常好的性能，而且资源成本合理。
- en: 'Run the Check Labels block to check the number of fraud and normal accounts
    in the dataset. You should get the following statistics about the labeled accounts:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 运行Check Labels块以检查数据集中欺诈和正常账户的数量。您应该得到关于标记账户的以下统计信息：
- en: '[PRE1]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the Train/Test Split code blocks, we split the vertices using the `vertexSplitter`
    function into 80% training data and 20% validation data. The `vertexSplitter`
    function as included in the notebook assigns two Boolean features, `is_training`
    and `is_validation`, to each vertex, then randomly assigns each one `true` or
    `false` values to create the 80-20 split.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在Train/Test Split代码块中，我们使用`vertexSplitter`函数将顶点分为80%的训练数据和20%的验证数据。笔记本中包含的`vertexSplitter`函数为每个顶点分配两个布尔特征`is_training`和`is_validation`，然后随机分配`true`或`false`值以创建80-20的分割。
- en: Next, we create two Vertex Loaders, which load all vertices of the graph onto
    the machine learning server in batches. We pass a list of attributes to include;
    all of these we recently created in the last few steps, except for the `is_fraud`
    label. We display the first five vertices in each set to make sure they were loaded
    in correctly.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建两个顶点加载器，将所有顶点批量加载到机器学习服务器上。我们传递一个包含的属性列表；这些属性都是我们最近几步创建的，但不包括 `is_fraud`
    标签。我们显示每组中的前五个顶点，以确保它们已正确加载。
- en: Training Traditional Models with Graph Features
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用图特征训练传统模型
- en: Now we are ready to train our fraud detection model. We will use XGBoost, a
    popular classification algorithm for tabular data. We import the `XGBClassifier`
    class from the `xgboost` library and create a classifier instance called `tree_model`,
    as shown in the Create xgboost model code block.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备训练我们的欺诈检测模型。我们将使用 XGBoost，这是一种用于表格数据的流行分类算法。我们从 `xgboost` 库导入 `XGBClassifier`
    类，并创建一个名为 `tree_model` 的分类器实例，如“创建 xgboost 模型”代码块所示。
- en: Next we train XGBoost models using three different sets of features so we can
    compare their results. For each case, we create a list consisting of the selected
    graph features, except for `is_fraud`. Then we use `tree_model.fit()` to say,
    “Using the features of our training data, try to predict the attribute `is_fraud`.”
    After training each model, another code block evaluates each model using the `Accuracy`,
    `BinaryPrecision`, and `BinaryRecall` modules from the `metrics` library in pyTigerGraph.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用三组不同的特征训练 XGBoost 模型，以便比较它们的结果。对于每种情况，我们创建一个列表，其中包含所选的图特征，但不包括 `is_fraud`。然后我们使用
    `tree_model.fit()` 来说，“使用我们的训练数据的特征，尝试预测属性 `is_fraud`。”训练每个模型后，另一个代码块使用 `pyTigerGraph`
    中的 `Accuracy`、`BinaryPrecision` 和 `BinaryRecall` 模块评估每个模型。
- en: Run the first case, which uses only nongraph features. Your model should achieve
    approximately 75% accuracy, 12% precision, and 100% recall. Remember that about
    3.6% of the transactions are fraudulent. A 100% recall means our model will catch
    all the real cases of fraud. Since the overall accuracy is 75%, this means that
    the model is incorrectly classifying about 25% of the normal accounts as fraudsters.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 运行第一个案例，仅使用非图特征。您的模型应该达到大约75%的准确率，12%的精确度和100%的召回率。请记住，大约3.6%的交易是欺诈的。100%的召回率意味着我们的模型将捕捉到所有真实的欺诈案例。由于总体准确率为75%，这意味着模型错误地将大约25%的正常账户分类为欺诈者。
- en: Run the next case, which now includes PageRank and betweenness centrality. You
    should see that accuracy and precision go up by a few percentage points, while
    recall drops to about 98%. Finally, run the third case, which adds the FastRP
    embedding to the feature set. You should see a significant increase in accuracy
    and precision. [Figure 12-2](#prediction_performance_with_and_without) compares
    the prediction performance of the three cases.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 运行下一个案例，现在包括PageRank和介数中心性。您应该看到准确率和精确度提高了几个百分点，而召回率降至约98%。最后，运行第三个案例，将FastRP嵌入添加到特征集中。您应该看到准确率和精确度显著提高。[图 12-2](#prediction_performance_with_and_without)比较了三个案例的预测性能。
- en: '![Image](assets/gpam_1202.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![Image](assets/gpam_1202.png)'
- en: Figure 12-2\. Prediction performance with and without graph features and graph
    embedding (see a larger version of this figure at [https://oreil.ly/gpam1202](https://oreil.ly/gpam1202))
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-2\. 带有图特征和图嵌入的预测性能（请查看此图的更大版本：[https://oreil.ly/gpam1202](https://oreil.ly/gpam1202)）
- en: Run the next cell under the Explain Model section to create a chart like [Figure 12-3](#importance_of_features_for_xgboost_mod),
    showing the feature importance for Case 2, including the graph algorithms but
    not the graph embedding in our training. Note that `pagerank` is the second most
    important feature for predicting fraud, close behind the `send_amount`.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 运行下一个单元格，在“解释模型”部分下创建一个类似[图 12-3](#importance_of_features_for_xgboost_mod)的图表，显示
    Case 2 的特征重要性，包括图算法，但不包括我们的训练中的图嵌入。注意，`pagerank` 是预测欺诈的第二重要特征，仅次于 `send_amount`。
- en: '![Image](assets/gpam_1203.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![Image](assets/gpam_1203.png)'
- en: Figure 12-3\. Importance of features for XGBoost model with graph algorithms
    (see a larger version of this figure at [https://oreil.ly/gpam1203](https://oreil.ly/gpam1203))
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-3\. XGBoost 模型中图算法的特征重要性（请查看此图的更大版本：[https://oreil.ly/gpam1203](https://oreil.ly/gpam1203)）
- en: Next, run the subsequent cell under Explain Model to see the feature importance
    with the embeddings. Here, all the dimensions of the embeddings are summed into
    one feature importance score. [Figure 12-4](#importance_of_features_for_xgboost_mode)
    shows that the embedding heavily contributes to the model’s performance.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在“解释模型”下方运行后续单元，以查看嵌入式特征的重要性。在这里，嵌入式的所有维度被汇总为一个特征重要性分数。[图 12-4](#importance_of_features_for_xgboost_mode)
    显示，嵌入式对模型的性能贡献很大。
- en: '![Image](assets/gpam_1204.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/gpam_1204.png)'
- en: Figure 12-4\. Importance of features for XGBoost model with FastRP embeddings
    (see a larger version of this figure at [https://oreil.ly/gpam1204](https://oreil.ly/gpam1204))
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-4\. 使用 FastRP 嵌入式的 XGBoost 模型特征重要性（请查看此图的更大版本：[https://oreil.ly/gpam1204](https://oreil.ly/gpam1204)）
- en: Using a Graph Neural Network
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用图神经网络
- en: In the next section, we set up a graph neural network to try to predict fraud
    accounts even more accurately.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将设置一个图神经网络，试图更精确地预测欺诈账户。
- en: In the first block of the GNN section, we set some hyperparameters. We’ve already
    selected good hyperparameter values that produce a highly accurate result. However,
    fine-tuning hyperparameters is one of the arts of machine learning, so after you
    finish this section, come back to this stage and experiment with tweaking these
    values.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GNN 部分的第一个块中，我们设置了一些超参数。我们已经选择了产生高度准确结果的良好超参数值。然而，微调超参数是机器学习的一门艺术，因此在完成本节后，请回到此阶段并尝试调整这些值。
- en: 'Just like in the last section, we set up two loaders to get the data into two
    big chunks: training and validation. These, however, use the `neighborLoader`
    method instead of `vertexLoader`. In a GNN, every vertex is influenced by its
    neighboring vertices. Therefore, when we load data, we load not just individual
    vertices but neighborhoods centered around each vertex. As you can see, the syntax
    for these loaders is roughly equivalent to the loaders in the xgboost section,
    though these loaders also incorporate some of the hyperparameters.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在上一节中一样，我们设置了两个加载器来将数据加载到两个大块中：训练和验证。然而，这些加载器使用的是 `neighborLoader` 方法，而不是
    `vertexLoader`。在 GNN 中，每个顶点都受其相邻顶点的影响。因此，当我们加载数据时，我们不仅加载单个顶点，还加载围绕每个顶点中心的邻域。正如您所看到的，这些加载器的语法大致等同于
    xgboost 部分的加载器，尽管这些加载器还包含一些超参数。
- en: Now that our data is all set up, we can create and train the GNN. This notebook
    uses the pyTorch Geometric GDS library, which offers several GNN models. The ML
    Workbench is flexible enough to use any of these; it also supposes DGL and TensorFlow
    graph machine learning libraries. Some of the more common models are built into
    pyTigerGraph, for even easier use.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的数据已经准备好了，我们可以创建和训练 GNN。此笔记本使用了 pyTorch Geometric GDS 库，它提供了几个 GNN 模型。ML
    工作台足够灵活，可以使用其中任何一个；它还假定了 DGL 和 TensorFlow 图机器学习库。一些更常见的模型已经内置到 pyTigerGraph 中，以便更方便地使用。
- en: We will use a graph attention network (`GAT` in the pyTorch Geometric library),
    which combines the fine-grained modeling of attention models with graph neighbor
    convolution. We run the network for 10 epochs.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用图注意力网络（在 pyTorch Geometric 库中的 `GAT`），它结合了注意力模型的细粒度建模和图邻域卷积。我们将网络运行 10
    个时期。
- en: This takes slightly longer than the nongraph XGBoost model. This is partially
    because of the complexity of the neural network compared to the decision tree
    model. In the free cloud instance tier, this takes about five seconds per epoch.
    Enterprise-level datasets normally run on much more capable hardware, taking advantage
    of GPUs to speed up the process significantly more than is possible using a CPU.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这比非图形 XGBoost 模型稍微花费更多时间。部分原因是神经网络与决策树模型相比的复杂性。在免费的云实例层中，每个时期大约需要五秒钟。企业级数据集通常在更强大的硬件上运行，利用
    GPU 大大加快了进程，远远超过使用 CPU 的可能性。
- en: 'The extra wait paid off, though. When the last epoch finishes, look at the
    returned values: we achieved greater than 90% accuracy!'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，额外的等待是值得的。当最后一个时期完成时，请查看返回的值：我们达到了超过 90% 的准确率！
- en: Run the next several cells to get a visual look at the training over time. In
    the Explain Model section, we randomly select a fraudulent vertex to see what
    its network looks like. Since we don’t have a perfectly accurate model, this cell
    may sometimes show vertices with few or no connections. However, if you run the
    cell more times, you’ll probably see vertex neighborhoods closer to the one shown
    in [Figure 12-5](#visual_explanation_of_prediction_for_ve).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 运行下面的几个单元格，可以实时查看随时间的训练情况。在解释模型部分，我们随机选择一个可疑顶点来查看其网络情况。由于我们没有完全准确的模型，这个单元格有时可能显示连接很少或没有连接的顶点。然而，如果多次运行这个单元格，你可能会看到更接近图 12-5
    的顶点邻域。
- en: '![Image](assets/gpam_1205.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![Image](assets/gpam_1205.png)'
- en: Figure 12-5\. Visual explanation of prediction for vertex 311 (see a larger
    version of this figure at [https://oreil.ly/gpam1205](https://oreil.ly/gpam1205))
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-5\. 针对顶点 311 的预测的视觉解释（请在 [https://oreil.ly/gpam1205](https://oreil.ly/gpam1205)
    查看更大版本的图像）
- en: In this case, the vertex receives a high number of transactions from other vertices
    and then makes large transactions out to a few more. If this really is a scammer,
    they could be receiving many payments and then shifting their money to another
    set of accounts that they also have access to.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，该顶点从其他顶点接收大量交易，然后向更多顶点进行大额交易。如果这确实是一个诈骗者，他们可能会接收许多支付，然后将资金转移到另一组他们也能访问的账户。
- en: Running the last code blocks, we get another chart for importance of features,
    like [Figure 12-6](#importance_of_features_for_gnn_model_pr). We see that the
    important features in our more accurate GNN model tend to be different from the
    features that were identified as important in the XGBoost model. Here, PageRank
    is not as important for detecting fraud as amount received, amount sent, and number
    of incoming transactions. Remember, however, that the neighborhood convolution
    of the GNN model is already taking into account the effect of relationships, so
    graph features like PageRank might be redundant.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 运行最后的代码块后，我们得到了另一个关于特征重要性的图表，类似于 [图 12-6](https://oreil.ly/gpam1206)。我们可以看到，在我们更准确的
    GNN 模型中，重要的特征往往与 XGBoost 模型中被识别为重要的特征不同。在这里，与欺诈检测相关的特征中，PageRank 并不像收到的金额、发送的金额和收到的交易数那样重要。然而，请记住，GNN
    模型的邻域卷积已经考虑了关系的影响，因此像 PageRank 这样的图特征可能是多余的。
- en: '![Image](assets/gpam_1206.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![Image](assets/gpam_1206.png)'
- en: Figure 12-6\. Importance of features for GNN model predicting fraud for vertex
    311 (see a larger version of this figure at [https://oreil.ly/gpam1206](https://oreil.ly/gpam1206))
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-6\. GNN 模型预测顶点 311 欺诈特征的重要性（请在 [https://oreil.ly/gpam1206](https://oreil.ly/gpam1206)
    查看更大版本的图像）
- en: Finally, when running the last code block, we get [Figure 12-7](#performance_of_xgboost_versus_xgboost_p),
    which shows the performance of the three models altogether. Here we see that our
    GNN is even better than XGBoost and the embedding when looking at that accuracy
    and precision. The GNN achieves a smaller recall than the two other models. However,
    it is still good, especially compared to XGBoost without embeddings.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，运行最后的代码块后，我们得到了 [图 12-7](https://oreil.ly/gpam1207)，显示了三个模型的综合性能。在这里，我们看到我们的
    GNN 在准确率和精度上甚至比 XGBoost 和嵌入都要好。GNN 达到的召回率比其他两个模型低。然而，它仍然很好，特别是与没有嵌入的 XGBoost 相比。
- en: '![Image](assets/gpam_1207.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![Image](assets/gpam_1207.png)'
- en: Figure 12-7\. Performance of XGBoost versus XGBoost + FastRP versus GNN (see
    a larger version of this figure at [https://oreil.ly/gpam1207](https://oreil.ly/gpam1207))
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-7\. XGBoost 对比 XGBoost + FastRP 对比 GNN 的性能（请在 [https://oreil.ly/gpam1207](https://oreil.ly/gpam1207)
    查看更大版本的图像）
- en: Chapter Summary
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 章节总结
- en: In this chapter, we looked at a specific machine learning problem and compared
    three graph-enhanced approaches to solving it. Our platform was the TigerGraph
    ML Workbench, which includes sample notebooks and datasets ready to be used on
    a TigerGraph Cloud instance.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究了一个特定的机器学习问题，并比较了三种增强图方法来解决它。我们的平台是 TigerGraph ML Workbench，包括可在 TigerGraph
    Cloud 实例上使用的示例笔记本和数据集。
- en: We first used a traditional decision tree machine learning library, XGBoost,
    to classify a dataset of Ethereum transactions into normal accounts and accounts
    suspected of committing fraud, using data that included graph-based features like
    PageRank and degree. Here, graph data, graph analytics, and even graph machine
    learning contributed to the data preparation phase.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用了传统的决策树机器学习库 XGBoost，通过包含 PageRank 和度等基于图的特征的数据，将以太坊交易数据集分类为普通账户和涉嫌欺诈的账户。在这里，图数据，图分析甚至图机器学习都为数据准备阶段做出了贡献。
- en: We then made the same predictions using a GNN, which took graph relationships
    into account during the training phase, resulting in a higher precision model
    than was possible with XGBoost.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们使用了 GNN 进行相同的预测，在训练阶段考虑了图关系，结果比 XGBoost 能实现的模型具有更高的精度。
- en: Connecting with You
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与您建立联系
- en: We hope our book made a connection with you, in the form of information, insight,
    and even some inspiration. We love graphs and graph analytics, so we hope that
    love shone through. Our mission was to help you see data as connected entities,
    to learn to view data searches and analytics from a connected-data perspective,
    and to get started developing solutions to your own tasks by working with use
    case starter kits.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望我们的书能与您建立联系，无论是信息的形式，洞察力，甚至一些灵感。我们热爱图形和图形分析，因此我们希望这种热爱能够显现出来。我们的使命是帮助您将数据视为连接实体，从连接数据的视角学习数据搜索和分析，并通过使用案例起始套件开始开发解决方案以完成您自己的任务。
- en: From our experience as data analysts, writers, and educators, we know that not
    everything makes sense the first time. Doing some hands-on exercises with the
    TigerGraph starter kits or other tutorials is the best way to help you connect
    the dots and to see for yourself what might be the next step. Moreover, this field
    is still growing and evolving rapidly, as is the TigerGraph product. We’ll post
    corrections and updates to the book, supplemental materials, and general FAQs
    at [*https://github.com/TigerGraph-DevLabs/Book-graph-powered-analytics*](https://github.com/TigerGraph-DevLabs/Book-graph-powered-analytics).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们作为数据分析师，作家和教育工作者的经验来看，我们知道并非一切在第一次就能讲清楚。通过使用TigerGraph的入门套件或其他教程进行一些实际操作是帮助您串联思路并亲自了解下一步可能是什么的最佳方式。此外，这个领域仍在快速增长和发展，正如TigerGraph产品一样。我们将在[*https://github.com/TigerGraph-DevLabs/Book-graph-powered-analytics*](https://github.com/TigerGraph-DevLabs/Book-graph-powered-analytics)上发布书籍、补充材料和常见问题的更正和更新。
- en: We would love your feedback. You can reach out to us collectively at [gpaml.book@gmail.com](mailto:gpaml.book@gmail.com).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很乐意收到您的反馈。您可以通过 [gpaml.book@gmail.com](mailto:gpaml.book@gmail.com) 联系我们。
- en: Thank you, and happy exploring!
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 谢谢您，祝您探索愉快！
- en: ^([1](ch12.html#ch01fn48-marker)) “Discover the True Cost of Fraud,” LexisNexis,
    accessed May 29, 2023, [*https://risk.lexisnexis.com/insights-resources/research/us-ca-true-cost-of-fraud-study*](https://risk.lexisnexis.com/insights-resources/research/us-ca-true-cost-of-fraud-study).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch12.html#ch01fn48-marker)) “发现欺诈的真正成本”，LexisNexis，访问于2023年5月29日，[*https://risk.lexisnexis.com/insights-resources/research/us-ca-true-cost-of-fraud-study*](https://risk.lexisnexis.com/insights-resources/research/us-ca-true-cost-of-fraud-study)。
- en: ^([2](ch12.html#ch01fn49-marker)) MacKenzie Sigalos, “Crypto Scammers Took a
    Record $14 Billion in 2021,” CNBC, January 6, 2022, [*https://www.cnbc.com/2022/01/06/crypto-scammers-took-a-record-14-billion-in-2021-chainalysis.html*](https://www.cnbc.com/2022/01/06/crypto-scammers-took-a-record-14-billion-in-2021-chainalysis.html).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch12.html#ch01fn49-marker)) 麦肯齐·希加洛斯，“加密骗子在2021年创下纪录，窃取了140亿美元”，CNBC，2022年1月6日，[*https://www.cnbc.com/2022/01/06/crypto-scammers-took-a-record-14-billion-in-2021-chainalysis.html*](https://www.cnbc.com/2022/01/06/crypto-scammers-took-a-record-14-billion-in-2021-chainalysis.html)。
- en: '^([3](ch12.html#ch01fn50-marker)) Liang Chen et al., “Phishing Scams Detection
    in Ethereum Transaction Network,” *ACM Transactions on Internet Technology* 21,
    no. 1 (February 2021): 1–16, doi: 10.1145/3398071.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '^([3](ch12.html#ch01fn50-marker)) 李昂·陈等，“以太坊交易网络中的网络钓鱼检测”，《ACM互联网技术期刊》第21卷，第1期（2021年2月）：1–16，doi:
    10.1145/3398071。'
- en: ^([4](ch12.html#ch01fn51-marker)) Liang Chen et al., “Ethereum Phishing Transaction
    Network,” XBlock, accessed May 29, 2023, [*https://xblock.pro/#/dataset/13*](https://xblock.pro/#/dataset/13).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch12.html#ch01fn51-marker)) 李昂·陈等，“以太坊网络中的网络钓鱼交易”，XBlock，访问于2023年5月29日，[*https://xblock.pro/#/dataset/13*](https://xblock.pro/#/dataset/13)。
- en: ^([5](ch12.html#ch01fn52-marker)) As graph machine learning is a rapidly developing
    field, the algorithms in the notebook might be updated by the time you obtain
    it.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch12.html#ch01fn52-marker)) 由于图机器学习是一个快速发展的领域，笔记本中的算法可能在您获取它时已经更新。
