- en: Chapter 10\. Trends in Production ML
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章。生产ML的趋势
- en: 'So far in this book, we have looked at computer vision as a problem to be solved
    by data scientists. Because machine learning is used to solve real-world business
    problems, however, there are other roles that interface with data scientists to
    carry out machine learning—for example:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将计算机视觉视为数据科学家要解决的问题。然而，由于机器学习用于解决实际业务问题，因此有其他角色与数据科学家进行接口，以执行机器学习，例如：
- en: ML engineers
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ML工程师
- en: ML models built by data scientists are put into production by ML engineers,
    who tie together all the steps of a typical machine learning workflow, from dataset
    creation to deployment for predictions, into a machine learning pipeline. You
    will often hear this being described as *MLOps*.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家构建的ML模型由ML工程师投入生产，他们将典型的机器学习工作流程的所有步骤，从数据集创建到部署以进行预测，串联成一个机器学习管道。你经常会听到这被描述为*MLOps*。
- en: End users
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 终端用户
- en: People who make decisions based on ML models tend to not trust black-box AI
    approaches. This is especially true in domains such as medicine, where end users
    are highly trained specialists. They will often require that your AI models are
    *explainable*—explainability is widely considered a prerequisite for carrying
    out AI responsibly.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 基于ML模型做出决策的人往往不信任黑匣子AI方法。这在医学等领域尤为明显，终端用户是高度训练有素的专家。他们通常要求你的AI模型是*可解释*的——可解释性被广泛认为是负责任地进行AI的先决条件。
- en: Domain experts
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 领域专家
- en: Domain experts can develop ML models using code-free frameworks. As such, they
    often help with data collection, validation, and problem viability assessment.
    You may hear this being described as ML being “democratized” through *no-code*
    or *low-code* tools.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 领域专家可以使用无代码框架开发ML模型。因此，他们经常帮助进行数据收集、验证和问题可行性评估。你可能听到这被描述为通过*无代码*或*低代码*工具使ML“民主化”。
- en: In this chapter, we’ll look at how the needs and skills of people in these adjacent
    roles increasingly affect the ML workflow in production settings.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨这些相邻角色的需求和技能如何越来越影响生产设置中的ML工作流程。
- en: Tip
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The code for this chapter is in the *10_mlops* folder of the book’s [GitHub
    repository](https://github.com/GoogleCloudPlatform/practical-ml-vision-book).
    We will provide file names for code samples and notebooks where applicable.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码位于该书的[GitHub存储库](https://github.com/GoogleCloudPlatform/practical-ml-vision-book)的*10_mlops*文件夹中。我们将在适用时提供代码示例和笔记本的文件名。
- en: Machine Learning Pipelines
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习管道
- en: '[Figure 10-1](#the_end-to-end_ml_pipelinedot) shows a high-level view of the
    machine learning pipeline. In order to create a web service that takes an image
    file and identifies the flower in it, as we have depicted throughout this book,
    we need to perform the following steps:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-1](#the_end-to-end_ml_pipelinedot) 展示了机器学习管道的高层视图。为了创建一个接受图像文件并识别其中花朵的网络服务，正如我们在本书中所描述的，我们需要执行以下步骤：'
- en: Create our dataset by converting our JPEG images into TensorFlow Records, with
    the data split into training, validation, and test datasets.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将我们的JPEG图像转换为TensorFlow Records来创建我们的数据集，并将数据分为训练、验证和测试数据集。
- en: Train an ML model to classify flowers (we carried out hyperparameter tuning
    to select the best model, but let’s assume that we can predetermine the parameters).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练一个ML模型来分类花朵（我们进行了超参数调整以选择最佳模型，但假设我们可以预先确定参数）。
- en: Deploy the model for serving.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署模型用于服务。
- en: '![](Images/pmlc_1001.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_1001.png)'
- en: Figure 10-1\. The end-to-end ML pipeline.
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-1。端到端ML管道。
- en: 'As you’ll see in this section, in order to complete these steps in an ML pipeline
    we have to:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在本节中所看到的，为了完成ML管道中的这些步骤，我们必须：
- en: Set up a cluster on which to execute the pipeline.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在其上设置一个集群来执行管道。
- en: Containerize our codebase, since the pipeline executes containers.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 封装我们的代码库，因为管道执行容器。
- en: Write pipeline components corresponding to each step of the pipeline.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写与管道的每个步骤对应的管道组件。
- en: Connect the pipeline components so as to run the pipeline in one go.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接管道组件，以便一次性运行管道。
- en: Automate the pipeline to run in response to events such as the arrival of new
    data.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化管道以响应诸如新数据到达之类的事件而运行。
- en: First, though, let’s discuss why we need an ML pipeline in the first place.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们讨论为什么我们首先需要ML管道。
- en: The Need for Pipelines
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管道的需求
- en: After we have trained our model on the original dataset, what happens if we
    get a few hundred more files to train on? We need to carry out the same set of
    operations to process those files, add them to our datasets, and retrain our model.
    In a model that depends heavily on having fresh data (say, one used for product
    identification rather than flower classification), we might need to perform these
    steps on a daily basis.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对原始数据集进行训练模型之后，如果我们获得了更多文件进行训练，会发生什么？我们需要执行相同的操作来处理这些文件，将它们添加到我们的数据集中，并重新训练我们的模型。在一个依赖于有新鲜数据的模型中（例如用于产品识别而不是花卉分类的模型），我们可能需要每天执行这些步骤。
- en: As new data arrives for a model to make predictions on, it is quite common for
    the model’s performance to start to degrade because of *data drift*—that is, the
    newer data might be different from the data it was trained on. Perhaps the new
    images are of a higher resolution, or from a season or place we don’t have in
    our training dataset. We can also anticipate that a month from now, we’ll have
    a few more ideas that we will want to try. Perhaps one of our colleagues will
    have devised a better augmentation filter that we want to incorporate, or a new
    version of MobileNet (the architecture we are doing transfer learning from) might
    have been released. Experimentation to change our model’s code will be quite common
    and will have to be planned for.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型需要对新数据进行预测时，由于*数据漂移*的存在，模型的性能开始下降是相当常见的——也就是说，新数据可能与其训练的数据不同。也许新图像的分辨率更高，或者来自我们训练数据集中没有的季节或地点。我们还可以预期，一个月后，我们会有几个新想法想要尝试。也许我们的同事会设计出更好的数据增强滤波器，我们希望将其纳入，或者MobileNet的新版本（我们正在进行迁移学习的架构）可能已发布。改变模型代码的实验将是相当常见的，并且必须进行计划。
- en: Ideally, we’d like a framework that will help us schedule and operationalize
    our ML pipelines and allow for constant experimentation. Kubeflow Pipelines provides
    a software framework that can represent any ML pipeline we choose in its domain-specific
    language (DSL). It runs on Kubeflow, a Kubernetes framework optimized for running
    TensorFlow models (see [Figure 10-2](#the_kubeflow_pipelines_api_runs_on_tenso)).
    The managed Kubeflow Pipelines executor on Google Cloud is called Vertex Pipelines.
    The pipeline itself can execute the steps on the Kubernetes cluster (for on-premises
    work) or call out to Vertex Training, Vertex Prediction, and Cloud Dataflow on
    Google Cloud. Metadata about experiments and steps can be stored in the cluster
    itself, or in Cloud Storage and Cloud SQL.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望有一个框架可以帮助我们安排和操作我们的ML流水线，并允许进行持续的实验。Kubeflow Pipelines提供了一个软件框架，可以用其领域特定语言（DSL）表示我们选择的任何ML流水线。它在Kubeflow上运行，这是一个针对运行TensorFlow模型优化的Kubernetes框架（见[图 10-2](#the_kubeflow_pipelines_api_runs_on_tenso)）。Google
    Cloud上的托管Kubeflow Pipelines执行器称为Vertex Pipelines。流水线本身可以在Kubernetes集群上执行步骤（用于本地工作），或者在Google
    Cloud上调用Vertex Training、Vertex Prediction和Cloud Dataflow。有关实验和步骤的元数据可以存储在集群本身中，也可以存储在Cloud
    Storage和Cloud SQL中。
- en: '![](Images/pmlc_1002.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_1002.png)'
- en: Figure 10-2\. The Kubeflow Pipelines API runs on TensorFlow and Kubernetes.
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-2\. Kubeflow Pipelines API在TensorFlow和Kubernetes上运行。
- en: Tip
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'Most ML pipelines follow a pretty standard set of steps: data validation, data
    transformation, model training, model evaluation, model deployment, and model
    monitoring. If your pipeline follows these steps, you can take advantage of the
    higher-level abstractions TensorFlow Extended (TFX) provides in the form of Python
    APIs. This way, you don’t need to work at the level of the DSL and containerized
    steps. [TFX](https://oreil.ly/1AOvG) is beyond the scope of this book.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数ML流水线遵循一套相当标准的步骤：数据验证、数据转换、模型训练、模型评估、模型部署和模型监控。如果您的流水线遵循这些步骤，您可以利用TensorFlow
    Extended（TFX）提供的高级抽象形式的Python API。这样，您就不需要在DSL和容器化步骤的级别上工作。[TFX](https://oreil.ly/1AOvG)超出了本书的范围。
- en: Kubeflow Pipelines Cluster
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubeflow Pipelines集群
- en: To execute Kubeflow pipelines, we need a cluster. We can set one up on Google
    Cloud by navigating to the [AI Platform Pipelines console](https://oreil.ly/SYUlx)
    and creating a new instance. Once started, we will get a link to open up the Pipelines
    dashboard and a Settings icon that provides the host URL (see [Figure 10-3](#ai_platform_pipelines_provides_a_managed)).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行Kubeflow流水线，我们需要一个集群。我们可以通过转到[AI Platform Pipelines控制台](https://oreil.ly/SYUlx)并创建一个新实例在Google
    Cloud上设置一个。一旦启动，我们将获得一个链接来打开Pipelines仪表板和一个提供主机URL的设置图标（见[图 10-3](#ai_platform_pipelines_provides_a_managed)）。
- en: '![](Images/pmlc_1003.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_1003.png)'
- en: Figure 10-3\. AI Platform Pipelines provides a managed execution environment
    for Kubeflow Pipelines.
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-3\. AI 平台管道为 Kubeflow 管道提供了托管的执行环境。
- en: We can develop pipelines in a Jupyter notebook, then deploy them to the cluster.
    Follow along with the full code in [*07e_mlpipeline.ipynb* on GitHub](https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/07_training/07e_mlpipeline.ipynb).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 Jupyter 笔记本中开发流水线，然后将其部署到集群中。请在[GitHub 上的*07e_mlpipeline.ipynb*](https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/07_training/07e_mlpipeline.ipynb)中查看完整的代码。
- en: Containerizing the Codebase
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将代码库容器化
- en: Once we have our cluster, the first step of our pipeline needs to transform
    the JPEG files into TensorFlow Records. Recall that we wrote an Apache Beam program
    called [*jpeg_to_tfrecord.py*](https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/05_create_dataset/jpeg_to_tfrecord.py)
    in [Chapter 5](ch05.xhtml#creating_vision_datasets) to handle this task. In order
    to make this repeatable, we need to make it a container where all the dependencies
    are captured.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了集群，我们流水线的第一步需要将 JPEG 文件转换为 TensorFlow 记录。回想一下，我们在[第 5 章](ch05.xhtml#creating_vision_datasets)编写了一个名为[*jpeg_to_tfrecord.py*](https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/05_create_dataset/jpeg_to_tfrecord.py)的
    Apache Beam 程序来处理这个任务。为了使其可重复执行，我们需要将其制作成一个容器，其中捕获了所有依赖关系。
- en: 'We developed it in a Jupyter notebook, and fortunately the Notebooks service
    on Vertex AI releases a container image corresponding to each Notebook instance
    type. Therefore, to build a container that is capable of executing that program,
    we need to do the following:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 Jupyter 笔记本中开发了它，幸运的是，Vertex AI 上的笔记本服务为每种笔记本实例类型发布了相应的容器映像。因此，要构建一个能够执行该程序的容器，我们需要执行以下操作：
- en: Get the container image corresponding to the Notebook instance.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取与笔记本实例对应的容器映像。
- en: 'Install any additional software dependencies. Looking through all our notebooks,
    we see that we need to install two additional Python packages: `apache-beam[gcp]`
    and `cloudml-hypertune`.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装任何额外的软件依赖项。浏览所有我们的笔记本，我们看到我们需要安装两个额外的 Python 包：`apache-beam[gcp]` 和 `cloudml-hypertune`。
- en: Copy over the script. Because we will probably need other code from the repository
    as well for other tasks, it’s probably better to copy over the entire repository.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制脚本。因为我们可能还需要仓库中的其他代码来完成其他任务，最好将整个仓库复制过来。
- en: 'This Dockerfile (the full code is in [*Dockerfile* on GitHub](https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/Dockerfile))
    performs those three steps:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Dockerfile（完整代码在[GitHub 上的*Dockerfile*](https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/Dockerfile)中）执行了这三个步骤：
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Those of you familiar with Dockerfiles will recognize that there is no `ENTRYPOINT`
    in this file. That’s because we will set up the entry point in the Kubeflow component—all
    the components for our pipeline will use this same Docker image.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 熟悉 Dockerfile 的人会注意到这个文件中没有`ENTRYPOINT`。这是因为我们将在 Kubeflow 组件中设置入口点——我们流水线中的所有组件都将使用相同的
    Docker 镜像。
- en: 'We can push the Docker image to a container registry using standard Docker
    functionality:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用标准的 Docker 功能将 Docker 镜像推送到容器注册表中：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Writing a Component
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编写组件
- en: For every component that we need, we’ll first load its definition from a YAML
    file and then use it to create the actual component.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们需要的每个组件，我们首先会从一个 YAML 文件中加载其定义，然后使用它来创建实际的组件。
- en: 'The first component we need to create is the dataset (see [Figure 10-1](#the_end-to-end_ml_pipelinedot)).
    From [Chapter 5](ch05.xhtml#creating_vision_datasets), we know that the step involves
    running *jpeg_to_tfrecord.py*. We define the component in a file named *create_dataset.yaml*.
    It specifies these input parameters:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要创建的第一个组件是数据集（参见[图 10-1](#the_end-to-end_ml_pipelinedot)）。从[第 5 章](ch05.xhtml#creating_vision_datasets)我们知道，这一步涉及运行*jpeg_to_tfrecord.py*。我们在名为*create_dataset.yaml*的文件中定义了该组件。它指定了这些输入参数：
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'It also specifies the implementation, which is to call a script called *create_dataset.sh*
    that you’ll find in [*create_dataset.sh* on GitHub](https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/10_mlops/components/create_dataset.sh).
    The arguments to the script are constructed from the inputs to the component:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 它还指定了实现方式，即调用名为*create_dataset.sh*的脚本，在[GitHub 上的*create_dataset.sh*](https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/10_mlops/components/create_dataset.sh)中可以找到。脚本的参数是从组件输入构建的：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The *create_dataset.sh* script simply forwards everything to the Python program:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*create_dataset.sh*脚本简单地将所有内容转发给 Python 程序：'
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Why do we need the extra level of indirection here? Why not simply specify `python3`
    as the command (instead of the bash call to the shell script)? That’s because,
    besides just calling the converter program, we also need to perform additional
    functionality like creating folders, passing messages to subsequent steps of our
    Kubeflow pipelines, cleaning up intermediate files, and so on. Rather than update
    the Python code to add unrelated Kubeflow Pipelines functionality to it, we’ll
    wrap the Python code within a bash script that will do the setup, message passing,
    and teardown. More on this shortly.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里为什么需要额外的间接层？为什么不简单地将`python3`指定为命令（而不是调用 shell 脚本的 bash 命令）？这是因为除了调用转换程序外，我们还需要执行其他功能，如创建文件夹、向
    Kubeflow 流水线的后续步骤传递消息、清理中间文件等。与其更新 Python 代码以添加与 Kubeflow Pipelines 功能无关的功能，我们将在一个
    bash 脚本中包装 Python 代码来完成设置、消息传递和清理工作。稍后将详细介绍这些内容。
- en: 'We will call the component from the pipeline as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按以下方式从流水线调用组件：
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Tip
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If we pass in `DirectRunner` instead of `DataflowRunner`, the Apache Beam pipeline
    executes on the Kubeflow cluster itself (albeit slowly and on a single machine).
    This is useful for executing on premises.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们传入`DirectRunner`而不是`DataflowRunner`，Apache Beam 流水线将在 Kubeflow 集群本身上执行（虽然速度慢，并且在单台机器上执行）。这对于在本地执行非常有用。
- en: 'Given the `create_dataset_op` component that we have just created, we can create
    a pipeline that runs this component as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 给定我们刚刚创建的`create_dataset_op`组件，我们可以创建一个流水线来运行这个组件：
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We then compile this pipeline into a *.zip* file:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将此流水线编译成一个*.zip*文件：
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'and submit that file as an experiment:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 并将该文件提交为一个实验：
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We can also upload the *.zip* file, submit the pipeline, and carry out experiments
    and runs using the Pipelines dashboard.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以上传*.zip*文件，提交流水线，并在 Pipelines 仪表板上进行实验和运行。
- en: Connecting Components
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接组件
- en: We now have the first step of our pipeline. The next step (see [Figure 10-1](#the_end-to-end_ml_pipelinedot))
    is to train an ML model on the TensorFlow Records created in the first step.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了流水线的第一步。下一步（参见[图 10-1](#the_end-to-end_ml_pipelinedot)）是在第一步创建的 TensorFlow
    Records 上训练 ML 模型。
- en: 'The dependency between the `create_dataset` step and the `train_model` step
    is expressed as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`create_dataset`步骤和`train_model`步骤之间的依赖关系表达如下：'
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this code, notice that one of the inputs to `train_model_op()` depends on
    the output of `create_dataset`. Connecting the two components in this manner makes
    Kubeflow Pipelines wait for the `create_dataset` step to complete before starting
    the `train_model` step.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，请注意`train_model_op()`的一个输入依赖于`create_dataset`的输出。以这种方式连接两个组件使得 Kubeflow
    Pipelines 在启动`train_model`步骤之前等待`create_dataset`步骤完成。
- en: 'The underlying implementation involves the `create_dataset` step writing out
    the value for the `tfrecords_topdir` into a local temporary file whose name will
    be automatically generated by Kubeflow Pipelines. So, our `create_dataset` step
    will have to take this additional input and populate the file. Here’s how we write
    the output directory name to the file in *create_dataset.sh* (the parameters for
    Kubeflow to provide to this script are specified in the YAML file):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 底层实现涉及`create_dataset`步骤将`tfrecords_topdir`的值写入本地临时文件，其名称将由 Kubeflow Pipelines
    自动生成。因此，我们的`create_dataset`步骤必须接受此额外输入并填充文件。以下是在*create_dataset.sh*中如何将输出目录名称写入文件（Kubeflow
    提供给此脚本的参数在 YAML 文件中指定）的方法：
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The script writes the name of the output directory to the component output file,
    removes the two parameters from the command-line arguments (that’s what the `shift`
    does in bash), and passes along the rest of the command-line parameters to `jpeg_to_tfrecord`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本将输出目录名称写入组件输出文件，从命令行参数中移除两个参数（这就是 bash 中`shift`的作用），并将其余的命令行参数传递给`jpeg_to_tfrecord`。
- en: 'The `train_model` step is similar to the `create_dataset` step in that it uses
    the codebase container and invokes a script to train the model:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_model`步骤类似于`create_dataset`步骤，它使用代码库容器并调用一个脚本来训练模型：'
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Tip
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: We can turn this into a local training run on the cluster by replacing the call
    to Vertex AI Training by a call to `gcloud ai-platform local`. See [*train_model_kfp.sh*](https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/07_training/components/train_model_kfp.sh)
    in the book’s GitHub repository for details.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将对 Vertex AI 训练的调用替换为对`gcloud ai-platform local`的调用来在集群上进行本地训练运行。有关详细信息，请参阅书籍的
    GitHub 仓库中的[*train_model_kfp.sh*](https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/07_training/components/train_model_kfp.sh)。
- en: 'The script writes out the directory in which the trained model is stored:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本将写出存储训练模型的目录：
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The deploy step does not require any custom code. To deploy the model, we can
    use the deploy operator that comes with Kubeflow Pipelines:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 部署步骤不需要任何自定义代码。要部署模型，我们可以使用 Kubeflow Pipelines 提供的部署运算符：
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As the pipeline is run, the logs, steps, and artifacts passed between steps
    show up in the console (see [Figure 10-4](#information_about_a_pipeline_that_has_be)).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当管道运行时，日志、步骤和在步骤之间传递的工件会显示在控制台上（参见[图 10-4](#information_about_a_pipeline_that_has_be)）。
- en: '![](Images/pmlc_1004.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_1004.png)'
- en: Figure 10-4\. Information about a pipeline that has been run is displayed in
    the Vertex Pipelines console.
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-4\. 展示已运行的管道信息在 Vertex Pipelines 控制台中显示。
- en: Automating a Run
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动化运行
- en: Because we have a Python API to submit new runs of an experiment, it is quite
    straightforward to incorporate this Python code into a Cloud Function or a Cloud
    Run container. The function will then get invoked in response to a Cloud Scheduler
    trigger, or whenever new files are added to a storage bucket.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们有一个 Python API 来提交一个实验的新运行，将这段 Python 代码整合到 Cloud Function 或 Cloud Run 容器中非常简单。然后，该函数将在响应
    Cloud Scheduler 触发器或在新文件添加到存储桶时调用。
- en: The experiment-launching code can also be invoked in response to continuous
    integration (CI) triggers (such as GitHub/GitLab Actions) to carry out retraining
    any time new code is committed. The necessary continuous integration, continuous
    deployment (CD), permission management, infrastructure authorization, and authentication
    together form the realm of *MLOps*. MLOps is beyond the scope of this book, but
    the [ML Engineering on Google Cloud Platform](https://oreil.ly/Vy94n), [MLOps
    on Azure](https://oreil.ly/lf6ea), and [Amazon SageMaker MLOps Workshop](https://oreil.ly/9Cym0)
    GitHub repositories contain instructions to help get you started on the respective
    platforms.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 实验启动代码也可以在响应持续集成（CI）触发器（如 GitHub/GitLab Actions）时调用，以便在提交新代码时进行重新训练。必要的持续集成、持续部署（CD）、权限管理、基础设施授权和认证共同构成*MLOps*领域。MLOps
    超出了本书的范围，但[Google Cloud 平台上的 ML 工程](https://oreil.ly/Vy94n)、[Azure 上的 MLOps](https://oreil.ly/lf6ea)和[Amazon
    SageMaker MLOps Workshop](https://oreil.ly/9Cym0) GitHub 仓库包含了帮助您在各自平台上入门的说明。
- en: We have seen how pipelines address the need of ML engineers to tie together
    all the steps of a typical machine learning workflow into an ML pipeline. Next,
    let’s look at how explainability meets the needs of decision makers.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到管道如何满足机器学习工程师将典型的机器学习工作流程的所有步骤连接到 ML 管道的需求。接下来，让我们看看可解释性如何满足决策者的需求。
- en: Explainability
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可解释性
- en: 'When we present an image to our model, we get a prediction. But why do we get
    that prediction? What is the model using to decide that a flower is a daisy or
    a tulip? Explanations of how AI models work are useful for several reasons:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们向模型展示一幅图像时，我们会得到一个预测。但是为什么会得到那个预测？模型用什么来决定一朵花是雏菊还是郁金香？解释 AI 模型如何工作对多个原因都是有用的：
- en: Trust
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 信任
- en: Human users may not trust a model that doesn’t explain what it is doing. If
    an image classification model says that an X-ray depicts a fracture but does not
    point out the exact pixels it used to make its determination, few doctors will
    trust the model.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 人类用户可能不会信任一个不解释其行为的模型。如果一个图像分类模型说一张 X 光显示有骨折，但没有指出它用于做出决定的确切像素，很少有医生会信任该模型。
- en: Troubleshooting
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 故障排除
- en: Knowing what parts of an image are important to make a determination can be
    useful to diagnose why the model is making an error. For example, if a dog is
    identified as a fox, and the most relevant pixels happen to be of snow, it is
    likely that the model has wrongly learned to associate the background (snow) with
    foxes. To correct this error we will have to collect examples of foxes in other
    seasons or dogs in snow, or augment the dataset by pasting foxes and dogs into
    each others’ scenes.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 知道图像的哪些部分对于做出决定是重要的，可以帮助诊断模型为何出错。例如，如果将一只狗误认为狐狸，而最相关的像素恰好是雪的一部分，那么模型可能错误地学习将背景（雪）与狐狸关联起来。为了纠正这个错误，我们需要收集其他季节的狐狸示例或在雪中的狗示例，或通过将狐狸和狗粘贴到彼此的场景中来增加数据集。
- en: Bias busting
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 偏见消除
- en: If we are using image metadata as input to our model, examining the importance
    of features associated with sensitive data can be very important to determining
    sources of bias. For example, if a model to identify traffic violations treats
    potholes in the road as an important feature, this might be because the model
    is learning the biases in the training dataset (perhaps more tickets were handed
    out in poorer/less well maintained areas than in wealthy ones).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将图像元数据作为模型的输入，那么检查与敏感数据相关联的特征的重要性对于确定偏差来源可能非常重要。例如，如果模型将道路上的坑洞作为重要特征来识别交通违规行为，这可能是因为模型在训练数据集中学习到了偏差（也许在较贫穷或维护不良的地区发出的罚单比富裕地区多）。
- en: 'There are two types of explanations: global and instance-level. The term *global*
    here highlights that these explanations are a property of the whole model after
    training, as opposed to each individual prediction at inference time. These methods
    rank the inputs to the model by the extent to which they [*explain* the variance](https://oreil.ly/kZi7q)
    of the predictions. For example, we may say that `feature1` explains 36% of the
    variance, `feature2` 23%, and so on. Because global feature importance is based
    on the extent to which different features contribute to the variance, these methods
    are calculated on a dataset consisting of many examples, such as the training
    or the validation dataset. However, global feature importance methods are not
    that useful in computer vision because there are no explicit, human-readable features
    when images are directly used as inputs to models. We will therefore not consider
    global explanations any further.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种解释方法：全局和实例级。这里的术语*全局*突出显示这些解释是模型训练后的整体属性，而不是推断时的每个单独预测。这些方法按照它们解释预测变异性的程度对模型的输入进行排名。例如，我们可以说`feature1`解释了36%的变异性，`feature2`解释了23%，依此类推。因为全局特征重要性是基于不同特征对变异性的贡献程度，所以这些方法是在包含许多示例的数据集上计算的，例如训练或验证数据集。然而，在计算机视觉中全局特征重要性方法并不那么有用，因为当图像直接用作模型的输入时，没有明确的、可读的人工特征。因此，我们将不再考虑全局解释。
- en: The second type of explanation is a measure of *instance-level* feature importance.
    These explanations attempt to explain each individual prediction and are invaluable
    in fostering user trust and for troubleshooting errors. These methods are more
    common in image models, and will be covered next.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种解释方法是度量*实例级*特征重要性。这些解释试图解释每个单独的预测，在促进用户信任和故障排除方面非常宝贵。这些方法在图像模型中更为常见，接下来将进行介绍。
- en: Techniques
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术
- en: 'There are four methods that are commonly employed to interpret or explain the
    predictions of image models. In increasing order of sophistication, they are:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 有四种常用的方法用于解释或说明图像模型的预测。按复杂程度递增的顺序，它们是：
- en: '[Local Interpretable Model-agnostic Explanations (LIME)](https://arxiv.org/abs/1602.04938)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[局部可解释的模型无关解释（LIME）](https://arxiv.org/abs/1602.04938)'
- en: '[Kernel Shapley Additive Explanations (KernelSHAP)](https://arxiv.org/abs/1705.07874)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[核Shapley可加解释（KernelSHAP）](https://arxiv.org/abs/1705.07874)'
- en: '[Integrated Gradients (IG)](https://arxiv.org/abs/1703.01365)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[整合梯度（IG）](https://arxiv.org/abs/1703.01365)'
- en: '[Explainable Representations through AI (xRAI)](https://arxiv.org/abs/2012.06006)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过人工智能实现可解释表示（xRAI）](https://arxiv.org/abs/2012.06006)'
- en: Let’s look at each of these in turn.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们依次看看每一种。
- en: LIME
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LIME
- en: LIME perturbs the input image by first identifying patches of the image that
    consist of contiguous similar pixels (see [Figure 10-5](#how_lime_workscomma_adapted_from_ruberio)),
    then replacing some of the patches with a uniform value, essentially removing
    them. It then asks the model to make a prediction on the perturbed image. For
    each perturbed image, we get a classification probability. These probabilities
    are spatially weighted based on how similar the perturbed image is to the original.
    Finally, LIME presents the patches with the highest positive weights as the explanation.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: LIME通过首先识别图像中由连续相似像素组成的补丁（参见[图 10-5](#how_lime_workscomma_adapted_from_ruberio)），然后用统一值替换其中的一些补丁来扰动输入图像。它然后要求模型对扰动后的图像进行预测。对于每个扰动后的图像，我们得到一个分类概率。这些概率根据扰动图像与原始图像的相似程度进行空间加权。最后，LIME将具有最高正权重的补丁呈现为解释。
- en: '![](Images/pmlc_1005.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_1005.png)'
- en: Figure 10-5\. How LIME works, adapted from [Ruberio et al., 2016](https://oreil.ly/xMFO7).
    In the bottom panel, p represents the predicted probability of the image being
    that of a frog.
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-5\. LIME工作原理图，改编自[Ruberio等人，2016](https://oreil.ly/xMFO7)。在底部面板中，p代表图像预测为青蛙的概率。
- en: KernelSHAP
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: KernelSHAP
- en: KernelSHAP is similar to LIME, but it weights the perturbed instances  differently.
    LIME weights instances that are similar to the original image very low, on the
    theory that they possess very little  extra information. KernelSHAP, on the other
    hand, weights the instances based on a distribution derived from game theory.
    The more patches are included in a perturbed image, the less weight the instance
    gets because theoretically any of those patches could have been important. In
    practice, KernelSHAP tends to be computationally much more expensive than LIME
    but provides somewhat better results.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: KernelSHAP类似于LIME，但是它对扰动实例进行了不同的加权。LIME将与原始图像相似的实例权重设定得非常低，理论上认为它们包含的额外信息非常少。另一方面，KernelSHAP根据从博弈论导出的分布对实例进行加权。如果在扰动图像中包含更多的补丁，则该实例的权重较低，因为理论上这些补丁中的任何一个都可能是重要的。在实践中，与LIME相比，KernelSHAP的计算成本通常要高得多，但提供的结果略好一些。
- en: Integrated Gradients
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成梯度
- en: IG uses the gradient of the model to identify which pixels are important. A
    property of deep learning is that the training initially focuses on the most important
    pixels because the error rate can be reduced the most by using their information
    in the output. Therefore, high gradients are associated with important pixels
    at the start of training. Unfortunately, neural networks *converge* during training,
    and during convergence the network keeps the weights corresponding to the important
    pixels unchanged and focuses on rarer situations. This means that the gradients
    corresponding to the most important pixels are actually close to zero at the end
    of training! Therefore, IG needs the gradient not at the end of training, but
    over the entire training process. However, the only weights that are available
    in the SavedModel file are the final weights. So, how can IG use the gradient
    to identify important pixels?
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: IG使用模型的梯度来识别哪些像素是重要的。深度学习的一个特性是，在训练初始阶段，训练主要集中在最重要的像素上，因为利用它们的信息可以最大程度地减少错误率。因此，高梯度与训练开始时的重要像素相关联。不幸的是，神经网络在训练过程中会*收敛*，在收敛过程中，网络会保持与重要像素对应的权重不变，并集中于更少见的情况。这意味着在训练结束时，与最重要像素对应的梯度实际上接近于零！因此，IG需要整个训练过程中的梯度，而不是在训练结束时的梯度。然而，SavedModel文件中只包含最终的权重。那么，IG如何利用梯度来识别重要像素呢？
- en: IG is based on the intuition that the model will output the a priori class probability
    if provided a baseline image that consists of all 0s, all 1s, or random values
    in the range [0, 1]. The overall gradient change is computed numerically by changing
    each pixel’s value from the baseline value to the actual input in several steps
    and computing the gradient for each such change. The pixels with the highest gradients
    integrated over the change from the baseline value to the actual pixel value are
    then depicted on top of the original image (see [Figure 10-6](#integrated_gradients_on_a_panda_image_le)).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: IG基于这样的直觉：如果给定一个基线图像，该图像由全部为0、全部为1或在范围[0, 1]内的随机值组成，模型将输出先验类概率。通过逐步改变每个像素值从基线值到实际输入，并计算每种改变的梯度，来数值化计算整体梯度变化。然后，在原始图像上显示积分过基线值到实际像素值的最大梯度的像素（见[图10-6](#integrated_gradients_on_a_panda_image_le)）。
- en: '![](Images/pmlc_1006.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_1006.png)'
- en: Figure 10-6\. Integrated Gradients on a panda image (left) and on a fireboat
    image (right). Images from [the IG TensorFlow tutorial](https://oreil.ly/vPhBi).
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-6\. 集成梯度应用于熊猫图像（左）和消防艇图像（右）。图片来自[IG TensorFlow教程](https://oreil.ly/vPhBi)。
- en: Tip
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Choosing an appropriate baseline image is critical when using IG. The explanation
    is relative to the baseline, so you should not use an all-white or all-black image
    as a baseline if your training data contains a lot of black (or white) regions
    that convey meaning in the images. As an example, black areas in X-rays correspond
    to tissue. You should use a baseline of random pixels in that case. On the other
    hand, if your training data contains a lot of high-variance patches that convey
    meaning in the image, you might not want to use random pixels as a baseline. It’s
    worth trying different baselines, as this can significantly affect the quality
    of your attributions.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用IG时选择适当的基线图像非常关键。解释是相对于基线的，因此如果您的训练数据包含许多传达图像中意义的黑色（或白色）区域，您不应该使用全白色或全黑色的图像作为基线。例如，X射线中的黑色区域对应组织。在这种情况下，您应该使用随机像素的基线。另一方面，如果您的训练数据包含许多传达图像中意义的高方差补丁，您可能不希望使用随机像素作为基线。尝试不同的基线是值得的，因为这可能会显著影响归因的质量。
- en: The output of IG on two images was shown in [Figure 10-6](#integrated_gradients_on_a_panda_image_le).
    In the first image, IG identifies the snout and fur texture of the panda’s face
    as the pixels that play the most important part in determining that the image
    is of a panda. The second image, of a fireboat, shows how IG can be used for troubleshooting.
    Here, the fireboat is correctly identified as a fireboat, but the method uses
    the jets of water from the boat as the key feature. This indicates that we may
    need to collect images of fireboats that are not actively shooting water up in
    the air.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: IG对两幅图像的输出显示在[图10-6](#integrated_gradients_on_a_panda_image_le)中。在第一幅图像中，IG确定熊猫面部的鼻子和毛皮纹理是决定该图像是熊猫的最重要部分的像素。第二幅图像是消防船，显示了IG如何用于故障排除。这里，消防船被正确地识别为消防船，但该方法使用船上喷射水柱作为关键特征。这表明我们可能需要收集没有主动向空中喷水的消防船的图像。
- en: However, in practice (as we will see shortly), IG tends to pick up on high-information
    areas in the images regardless of whether that information is used by the model
    for classifying the specific image.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实际操作中（我们很快会看到），IG往往会捕捉到图像中的高信息区域，无论该信息是否被模型用于分类特定图像。
- en: xRAI
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: xRAI
- en: In xRAI, the weights and biases of the trained neural network are used to train
    an interpretation network. The interpretation network outputs a choice among a
    family of algebraic expressions (such as Booleans and low-order polynomials) that
    are well understood. Thus, xRAI aims to find a close approximation to the original
    trained model from within the family of simple functions. This approximation,
    rather than the original model, is then interpreted.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在xRAI中，使用训练有素的神经网络的权重和偏差来训练解释网络。解释网络输出一组在代数表达式家族中选择的选择（例如布尔表达式和低阶多项式），这些表达式是被很好理解的。因此，xRAI旨在从简单函数家族中找到对原始训练模型的近似，而不是原始模型本身。然后解释这种近似。
- en: The xRAI method combines the benefits of the preprocessing method of LIME and
    KernelSHAP to find patches in the image with the pixel-level attribution against
    a baseline image that IG provides (see [Figure 10-7](#xrai_combines_the_patch-based_preprocess)).
    The pixel-level attribution is integrated among all the pixels that form a patch,
    and these patches are then combined into regions based on having similar levels
    of integrated gradients. The regions are then removed from the input image and
    the model is invoked in order to determine how important each region is, and the
    regions are ordered based on how important they are to the given prediction.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: xRAI方法结合了LIME和KernelSHAP的预处理方法的优点，以及IG提供的基于像素级归因的基线图像中的补丁查找（见[图10-7](#xrai_combines_the_patch-based_preprocess)）。像素级归因在形成补丁的所有像素之间进行集成，然后根据具有相似集成梯度水平的区域将这些补丁组合成区域。然后从输入图像中删除这些区域，并调用模型以确定每个区域的重要性，并根据它们对给定预测的重要性对这些区域进行排序。
- en: '![](Images/pmlc_1007.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_1007.png)'
- en: Figure 10-7\. xRAI combines the patch-based preprocessing of LIME and KernelSHAP
    with the pixel-wise attributions of IG and ranks regions based on their effect
    on the prediction. Image adapted from the [Google Cloud documentation](https://oreil.ly/RvZrG).
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-7\. xRAI结合了LIME和KernelSHAP的基于补丁的预处理以及IG的像素级归因，并根据其对预测的影响对区域进行排名。 图片来自[Google
    Cloud文档](https://oreil.ly/RvZrG)。
- en: IG provides pixel-wise attributions. xRAI provides region-based attributions.
    Both have their uses. In a model identifying diseased regions of an eye (the diabetic
    retinopathy use case), for example, knowing the specific pixels that caused the
    diagnosis is very useful, so use IG. IG tends to work best on low-contrast images
    like X-rays or scientific images taken in a lab.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: IG提供像素级归因。xRAI提供基于区域的归因。两者都有其用途。例如，在诊断眼部疾病区域（糖尿病性视网膜病变用例）的模型中，了解导致诊断的具体像素非常有用，因此使用IG。IG在低对比度图像（如X光片或实验室中拍摄的科学图像）中效果最好。
- en: In natural images where you’re detecting the type of animal depicted, for example,
    region-based attributions are preferred, so use xRAI. We do not recommend using
    IG on natural images like pictures taken in nature or around the house.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然图像中，例如检测所描绘动物的类型时，优先选择基于区域的归因，因此使用xRAI。我们不建议在自然图像上使用IG，例如在自然环境中或房子周围拍摄的照片。
- en: '![](Images/pmlc_1008.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_1008.png)'
- en: Figure 10-8\. Tracin works by identifying key proponents and opponents that
    impact the training loss on a selected training example. Proponents are associated
    with a reduction in loss. Image courtesy of the [Google AI Blog](https://oreil.ly/OtbBf).
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-8\. Tracin通过识别影响选择的训练示例的训练损失的关键支持者和反对者来工作。支持者与损失减少相关联。图像由[Google AI Blog](https://oreil.ly/OtbBf)提供。
- en: Let’s now look at how to get explanations for our flowers’ model’s predictions
    using these techniques.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何使用这些技术获取我们花卉模型预测的解释。
- en: Adding Explainability
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加可解释性
- en: Because image explainability is associated with individual predictions, we recommend
    that you use an ML deployment platform that carries out one or all of the explainability
    techniques mentioned in the previous section for every prediction presented to
    it. Explainability methods are computationally expensive, and a deployment platform
    that can distribute and scale the computation can help you do your prediction
    analysis more efficiently.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 因为图像可解释性与个别预测相关联，我们建议您使用能够为其呈现的每个预测执行前述可解释性技术之一或全部的ML部署平台。可解释性方法计算开销大，而能够分发和扩展计算的部署平台可以帮助您更有效地进行预测分析。
- en: In this section, we will demonstrate obtaining explanations using Integrated
    Gradients and xRAI from a model deployed on Google Cloud’s Vertex AI.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示如何使用集成梯度和xRAI从部署在Google Cloud Vertex AI上的模型获取解释。
- en: Tip
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: At the time of writing, [Azure ML supports SHAP](https://oreil.ly/wx2D0), as
    does [Amazon SageMaker Clarify](https://oreil.ly/MSqhJ). Conceptually, the services
    are used similarly even if the syntax is slightly different. Please consult the
    linked documentation for specifics.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，[Azure ML支持SHAP](https://oreil.ly/wx2D0)，[Amazon SageMaker Clarify也支持](https://oreil.ly/MSqhJ)。尽管语法略有不同，但这些服务的概念上的使用方式相似。具体详情请查阅链接的文档。
- en: Explainability signatures
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可解释性签名
- en: 'The explainability methods all need to invoke the model with perturbed versions
    of the original image. Let’s say that our flowers model has the following export
    signature:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的可解释性方法都需要用原始图像的扰动版本来调用模型。比如说，我们的花卉模型有以下导出签名：
- en: '[PRE14]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: It accepts a filename and returns the predictions for the image data in that
    file.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 它接受一个文件名，并返回该文件中的图像数据的预测结果。
- en: 'In order to provide the Explainable AI (XAI) module the ability to create perturbed
    versions of the original images and obtain predictions for them, we will need
    to add two signatures:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使可解释人工智能（XAI）模块能够创建原始图像的扰动版本并对其进行预测，我们需要添加两个签名：
- en: 'A preprocessing signature, to obtain the image that is input to the model.
    This method will take one or more filenames as input (like the original exported
    signature) and produce a 4D tensor of the shape required by the model (the full
    code is in [*09f_explain.ipynb* on GitHub](https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/09_deploying/09f_explain.ipynb)):'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个预处理签名，用于获取输入到模型的图像。该方法将接受一个或多个文件名作为输入（如原始导出签名），并生成模型所需形状的4D张量（完整代码在GitHub上的[*09f_explain.ipynb*](https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/09_deploying/09f_explain.ipynb)中）：
- en: '[PRE15]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note that the return value is a dictionary. The key values of the dictionary
    (`input_images`, here) have to match the parameter names in the second signature
    that is described next so that the two methods can be called one after the other
    in a third model signature that we will discuss shortly.
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，返回值是一个字典。字典的键值（这里是`input_images`）必须与接下来描述的第二个签名中的参数名匹配，以便后面可以调用这两种方法，并在我们稍后讨论的第三个模型签名中一起调用。
- en: 'A model signature, to send in the 4D image tensor (XAI will send in perturbed
    images) and obtain predictions:'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个模型签名，用于发送4D图像张量（XAI将发送扰动图像）并获取预测：
- en: '[PRE16]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This code invokes the model and then pulls out the highest-scoring label and
    its probability.
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此代码调用模型，然后提取出得分最高的标签及其概率。
- en: 'Given the preprocessing and model signatures, the original signature (that
    most clients will use) can be refactored into:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于预处理和模型签名，最初的签名（大多数客户将使用）可以重构为：
- en: '[PRE17]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, we save the model with all three export signatures:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们保存具有所有三个导出签名的模型：
- en: '[PRE18]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: At this point, the model has the signatures it needs to apply XAI, but there
    is some additional metadata needed in order to compute explanations.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，模型已经具备应用XAI所需的签名，但是需要一些额外的元数据来计算解释。
- en: Explanation metadata
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解释元数据
- en: Along with the model, we need to supply XAI a baseline image and some other
    metadata. This takes the form of a JSON file that we can create programmatically
    using the Explainability SDK open-sourced by Google Cloud.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型之外，我们还需要提供XAI一个基线图像和一些其他元数据。这些以一个JSON文件的形式呈现，我们可以使用Google Cloud开源的Explainability
    SDK程序化地创建。
- en: 'We start by specifying which exported signature is the one that takes a perturbed
    image as input, and which of the output keys (`probability`, `flower_type_int`,
    or `flower_type_str`) needs to be explained:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先要指定哪个导出签名是接受扰动图像作为输入的签名，并指定需要解释的输出键（`probability`、`flower_type_int`或`flower_type_str`）：
- en: '[PRE19]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then we create the baseline image that will be used as the starting point for
    gradients. Common choices here are all zeros (`np.zeros`), all ones (`np.ones`),
    or random noise. Let’s do the third option:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建基线图像，它将作为梯度起始点。通常选择是全零（`np.zeros`）、全一（`np.ones`）或随机噪声。我们选择第三个选项：
- en: '[PRE20]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Note that we specified the name of the input parameter to the `xai_model()`
    function, `input_images`.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在`xai_model()`函数的输入参数名为`input_images`。
- en: 'Finally, we save the metadata file:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们保存元数据文件：
- en: '[PRE21]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This creates a file named *explanation_metadata.json* that lives along with
    the SavedModel files.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个名为*explanation_metadata.json*的文件，与SavedModel文件一起存在。
- en: Deploying the model
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署模型
- en: 'The SavedModel and associated explanation metadata are deployed to Vertex AI
    as before, but with a couple of extra parameters to do with explainability. To
    deploy a model version that provides IG explanations, we’d do:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，将SavedModel和相关解释元数据部署到Vertex AI，但需要一些额外的参数来处理可解释性。要部署一个提供IG解释的模型版本，我们会这样做：
- en: '[PRE22]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'whereas to get xRAI explanations we’d do:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 而要获得xRAI解释，我们会这样做：
- en: '[PRE23]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The `--num-integral-steps` argument specifies the number of steps between the
    baseline image and input image for the purposes of numerical integration. The
    more steps there are, the more accurate (and computationally intensive) the gradient
    computation is. A value of 25 is typical.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`--num-integral-steps`参数指定基线图像和输入图像之间的步骤数，用于数值积分的目的。步数越多，梯度计算越准确（但计算量也越大）。典型值为25。'
- en: Tip
  id: totrans-172
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The explanation response contains an approximation error for each prediction.
    Check the approximation error for a representative set of inputs, and if this
    error is too high, increase the number of steps.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 解释响应包含每个预测的近似误差。检查代表性输入的近似误差，如果误差过高，则增加步数。
- en: For this example, let’s employ both image explainability methods—we’ll deploy
    a version that provides IG explanations with the name `ig` and a version that
    provides xRAI explanations with the name `xrai`.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，让我们同时使用两种图像可解释性方法——我们将部署一个版本，它提供名为`ig`的IG解释和一个提供名为`xrai`的xRAI解释的版本。
- en: 'Either deployed version can be invoked as normal with a request whose payload
    looks like this:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 无论部署的版本是哪个，都可以像正常调用一样，请求的负载看起来像这样：
- en: '[PRE24]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'It returns the label and associated probability for each of the input images:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回每个输入图像的标签和相关概率：
- en: '[PRE25]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The XAI versions can be used for normal serving with no performance impact.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: XAI版本可以用于正常服务而无需性能影响。
- en: Obtaining explanations
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 获取解释
- en: There are three ways to get the explanations. The first is through `gcloud`
    and the second through the Explainable AI SDK. Both of these end up invoking the
    third way—a REST API—which we can use directly as well.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 获取解释的三种方式。第一种是通过`gcloud`，第二种是通过可解释 AI SDK。这两种方式最终会调用第三种方式——一个 REST API，我们也可以直接使用它。
- en: 'We’ll look at the `gcloud` method, as it is the simplest and most flexible.
    We can send in a JSON request and obtain a JSON response using:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将研究`gcloud`方法，因为它是最简单和最灵活的。我们可以发送 JSON 请求，并使用以下方法获取 JSON 响应：
- en: '[PRE26]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'To get explanations using IG, we’ll deploy this version (`ig`) with the option:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 IG 获取解释，我们将使用以下选项部署此版本（`ig`）：
- en: '[PRE27]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The JSON response contains the attribution image in base64-encoded form. We
    can decode it using:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: JSON 响应以 base64 编码形式包含归因图像。我们可以使用以下方法对其进行解码：
- en: '[PRE28]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The IG results for five images are shown in [Figure 10-9](#integrated_gradients_explanation_of_the).
    The [*10b_explain.ipynb* notebook on GitHub](https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/10_mlops/10b_explain.ipynb)
    has the necessary plotting code.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 五张图像的 IG 结果显示在[图 10-9](#integrated_gradients_explanation_of_the)中。[*GitHub 上的
    10b_explain.ipynb 笔记本*](https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/10_mlops/10b_explain.ipynb)具有必要的绘图代码。
- en: '![](Images/pmlc_1009.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_1009.png)'
- en: Figure 10-9\. Integrated Gradients explanation of the flowers model. The input
    images are in the top row, and the attributions returned by the XAI routine are
    in the second row.
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-9\. 花朵模型的集成梯度解释。输入图像在顶行，XAI 程序返回的归因在第二行。
- en: For the first image, it appears that the model uses the tall white flower, as
    well as parts of the white pixels in the background, to decide that the image
    is a daisy. In the second image, the yellow-ish center and white petals are what
    the model relies on. Worryingly, in the fourth image, the cat seems to be an important
    part of the determination. Interestingly, the tulips’ determination seems to be
    driven more by the green stalks than the bulb-like flowers. Again, as we will
    see shortly, this attribution is misleading, and this misleading attribution demonstrates
    the limitations of the IG approach.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一张图像，模型似乎使用了高大的白色花朵，以及背景中的部分白色像素，来确定图像是雏菊。在第二张图像中，黄色中心和白色花瓣是模型依赖的部分。令人担忧的是，在第四张图像中，猫似乎是决策的重要部分。有趣的是，郁金香的决策似乎更多地受到绿色茎的驱动，而不是鳞茎状的花朵。再次，正如我们很快会看到的那样，这种归因是误导性的，这种误导性归因展示了
    IG 方法的局限性。
- en: To get xRAI explanations, we invoke `gcloud explain` on the model endpoint for
    the version we deployed with the name `xrai`. The attributions from xRAI for the
    same flower images are shown in [Figure 10-10](#xrai_explanation_of_the_flowers_modeldot).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取 xRAI 解释，我们在部署的模型端点上调用`gcloud explain`，使用名为`xrai`的版本。同一花卉图像的 xRAI 归因显示在[图
    10-10](#xrai_explanation_of_the_flowers_modeldot)中。
- en: '![](Images/pmlc_1010.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_1010.png)'
- en: Figure 10-10\. xRAI explanation of the flowers model. The input images are in
    the top row, and the attributions returned by the XAI routine are in the second
    row. The bottom row contains the same information as the second row, except that
    the attribution images have been recolored for easier visualization on the pages
    of this book.
  id: totrans-194
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-10\. 花朵模型的 xRAI 解释。输入图像在顶行，XAI 程序返回的归因在第二行。底行包含与第二行相同的信息，但归因图像已经重新上色，以便在本书页面上更容易进行可视化。
- en: Recall that xRAI uses the IG approach to identify salient regions, and then
    invokes the model with perturbed versions of the images to determine how important
    each of the regions is. It is clear that the attributions from xRAI in [Figure 10-10](#xrai_explanation_of_the_flowers_modeldot)
    are much more precise than those obtained with IG in [Figure 10-9](#integrated_gradients_explanation_of_the).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，xRAI 使用 IG 方法识别显著区域，然后调用模型以确定图像各个区域的重要性。显然，xRAI 在[图 10-10](#xrai_explanation_of_the_flowers_modeldot)中的归因比在[图
    10-9](#integrated_gradients_explanation_of_the)中使用 IG 方法得到的归因更为精确。
- en: For the first flower image, the model focuses on the tall white flower and only
    that flower. It is clear that the model has learned to ignore the smaller flowers
    in the background. And while IG seemed to indicate that the background was important,
    the xRAI results show that the model discards that information in favor of the
    most prominent flower in the image. In the second image, the yellow-ish center
    and white petals are what the model keys off of (IG got this right too). The precision
    of the xRAI approach is clear for the third image—the model picks up on the narrow
    band of bright yellow where the petals join the center. That is unique to daisies,
    and helps it distinguish them from similarly colored dandelions. In the fourth
    image, we can see that the tulip bulbs are what the model uses for its classification,
    although the cat confuses its attention. The final classification as tulips seems
    to be driven by the presence of so many flowers. The IG method led us astray—the
    stalks are prominent, but it is the bulbs that drive the prediction probability.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一幅花卉图像，模型专注于高大的白色花朵，只有这朵花。很明显，模型已学会忽略背景中较小的花朵。而 IG 看似表明背景很重要，xRAI 结果显示，模型放弃了背景信息，而选择了图像中最显著的花卉。在第二幅图像中，模型依据的是黄色中心和白色花瓣（IG
    也正确）。xRAI 方法的精度对于第三幅图像也很明显——模型关注的是花瓣与中心相连的明亮黄色窄带。这是雏菊独有的特征，有助于将其与色彩相似的蒲公英区分开来。在第四幅图像中，我们可以看到郁金香的球茎是模型用于分类的特征，尽管猫会分散它的注意力。最终的郁金香分类似乎受到了花朵众多的影响。IG
    方法误导了我们——茎很显眼，但是球茎驱动了预测概率。
- en: IG is useful in certain situations. Had we considered radiology images where
    pixelwise attributions (rather than regions) are important, IG would have performed
    better. However, in images that depict objects, xRAI tends to perform better.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: IG 在某些情况下很有用。如果我们考虑了像素级别的辐射图像，像素级别的归因（而不是区域）很重要，那么 IG 的表现会更好。然而，在描绘对象的图像中，xRAI
    的表现往往更好。
- en: In this section, we have looked at how to add explainability to our prediction
    services in order to meet the need of decision makers to understand what a machine
    learning model is relying on. Next, let’s look at how no-code tools help democratize
    ML.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们研究了如何向我们的预测服务添加可解释性，以满足决策者理解机器学习模型依赖的需求。接下来，让我们看看无代码工具如何帮助民主化机器学习。
- en: No-Code Computer Vision
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无代码计算机视觉
- en: The computer vision problems that we have considered so far in this book—image
    classification, object detection, and image segmentation—are supported out of
    the box by low-code and no-code machine learning systems. For example, [Figure 10-11](#fundamental_computer_vision_problems_sup)
    shows the starting console for Google Cloud AutoML Vision.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书中迄今考虑过的计算机视觉问题——图像分类、物体检测和图像分割——都受到低代码和无代码机器学习系统的支持。例如，[图 10-11](#fundamental_computer_vision_problems_sup)
    显示了 Google Cloud AutoML Vision 的起始控制台。
- en: '![](Images/pmlc_1011.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_1011.png)'
- en: Figure 10-11\. Fundamental computer vision problems supported by Google Cloud
    AutoML Vision, a machine learning tool that you can use without having to write
    any code.
  id: totrans-202
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-11\. Google Cloud AutoML Vision 支持的基本计算机视觉问题，这是一个可以使用的机器学习工具，无需编写任何代码。
- en: Other no-code and low-code tools that work on images include [Create ML](https://oreil.ly/Ft1We)
    by Apple, [DataRobot](https://oreil.ly/7xXOw), and [H2O](https://oreil.ly/DHKiK).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 其他适用于图像的无代码和低代码工具包括 [Create ML](https://oreil.ly/Ft1We)（由 Apple 提供）、[DataRobot](https://oreil.ly/7xXOw)
    和 [H2O](https://oreil.ly/DHKiK)。
- en: Why Use No-Code?
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么使用无代码？
- en: In this book, we have focused on implementing machine learning models using
    code. However, it is worth incorporating a no-code tool into your overall workflow.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们专注于使用代码实现机器学习模型。然而，将无代码工具纳入整体工作流程是值得的。
- en: 'No-code tools are useful when embarking on a computer vision project for several
    reasons, including:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行计算机视觉项目时，无代码工具有几个优点，包括：
- en: Problem viability
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 问题的可行性
- en: Tools such as AutoML serve as a sanity check on the kind of accuracy that you
    can expect. If the accuracy that is achieved is far from what would be acceptable
    in context, this allows you to avoid wasting your time on futile ML projects.
    For example, if identifying a counterfeit ID achieves only 98% precision at the
    desired recall, you know that you have a problem—wrongly rejecting 2% of your
    customers might be an unacceptable outcome.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如AutoML之类的工具充当了您可以期待的准确性的健全性检查。如果达到的准确性远低于上下文中可以接受的水平，这可以避免浪费时间在无效的机器学习项目上。例如，如果在识别伪造身份证时仅达到98%的精度以达到期望的召回率，您就知道出现了问题——错误拒绝2%的客户可能是一个不能接受的结果。
- en: Data quality and quantity
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量和数量
- en: No-code tools provide a check on the quality of your dataset. After data collection,
    the correct next step in many ML projects is to go out and collect more/better
    data, not to train an ML model; and the accuracy that you get from a tool like
    AutoML can help you make that decision. For example, if the confusion matrix the
    tool produces indicates that the model frequently classifies all flowers in water
    as lilies, that might be an indication that you need more photographs of water
    scenes.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 无代码工具可以检查数据集的质量。在数据收集之后，在许多机器学习项目中，正确的下一步是出去收集更多/更好的数据，而不是训练一个机器学习模型；AutoML之类的工具提供的准确性可以帮助您做出这样的决定。例如，如果工具生成的混淆矩阵表明模型经常将水中的所有花卉分类为百合花，这可能表明您需要更多水景照片。
- en: Benchmarking
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试
- en: Starting out with a tool like AutoML gives you a benchmark against which you
    can compare the models that you build.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 使用类似AutoML的工具起始可以为您提供一个基准，用以比较您构建的模型。
- en: Many machine learning organizations arm their domain experts with no-code tools
    so that they can examine problem viability and help collect high-quality data
    before bringing the problem to the data science team.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习组织向其领域专家提供无代码工具，以便他们可以检查问题的可行性，并在将问题带给数据科学团队之前帮助收集高质量的数据。
- en: In the rest of this section, we’ll quickly run through how to use AutoML on
    the 5-flowers dataset, starting with loading data.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的其余部分中，我们将快速介绍如何在5花数据集上使用AutoML，从加载数据开始。
- en: Loading Data
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据加载
- en: The first step is to load the data into the system. We do that by pointing the
    tool at the *all_data.csv* file in the Cloud Storage bucket (see [Figure 10-12](#creating_a_dataset_by_importing_the_file)).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是将数据加载到系统中。我们通过指向云存储桶中的*all_data.csv*文件来完成这一步（参见[图 10-12](#creating_a_dataset_by_importing_the_file)）。
- en: Once the data is loaded, we see that there are 3,667 images with 633 daisies,
    898 dandelions, and so on (see [Figure 10-13](#after_loading_the_datasetcomma_we_can_vi)).
    We can verify that all the images are labeled, and correct the labels if necessary.
    If we had loaded a dataset without labels, we could label the images ourselves
    in the user interface or farm the task out to a labeling service (labeling services
    were covered in [Chapter 5](ch05.xhtml#creating_vision_datasets)).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 数据加载后，我们发现有3,667张图像，其中包括633朵雏菊、898朵蒲公英等（参见[图 10-13](#after_loading_the_datasetcomma_we_can_vi)）。我们可以验证所有图像都已经标记，并在必要时更正标签。如果我们加载的数据集没有标签，我们可以在用户界面中自行标记图像，或者将任务委托给标注服务（标注服务在[第5章](ch05.xhtml#creating_vision_datasets)中有介绍）。
- en: '![](Images/pmlc_1012.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_1012.png)'
- en: Figure 10-12\. Creating a dataset by importing the files from Cloud Storage.
  id: totrans-219
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-12\. 通过从云存储中导入文件来创建数据集。
- en: '![](Images/pmlc_1013.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_1013.png)'
- en: Figure 10-13\. After loading the dataset, we can view the images and their labels.
    This is also an opportunity to add or correct labels if necessary.
  id: totrans-221
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-13\. 加载数据集后，我们可以查看图像及其标签。这也是在必要时添加或更正标签的机会。
- en: Training
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: Once we are happy with the labels, we can click the Train New Model button to
    train a new model. This leads us through the set of screens shown in [Figure 10-14](#user_interface_screens_to_launch_the_tra),
    where we select the model type, the way to split the dataset, and our training
    budget. At the time of writing, the 8-hour training budget we specified would
    have cost about $25.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们对标签感到满意时，可以点击“训练新模型”按钮来训练一个新模型。这将引导我们通过图 10-14所示的一系列屏幕（参见[图 10-14](#user_interface_screens_to_launch_the_tra)），选择模型类型、数据集拆分方式和训练预算。在撰写本文时，我们指定的8小时训练预算大约需要花费25美元。
- en: '![](Images/pmlc_1014.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_1014.png)'
- en: Figure 10-14\. User interface screens to launch the training job.
  id: totrans-225
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-14\. 启动训练作业的用户界面屏幕。
- en: Note that in the last screen we enabled early stopping, so that AutoML can decide
    to stop early if it doesn’t see any more improvement in validation metrics. With
    this option the training finished in under 30 minutes (see [Figure 10-15](#automl_finished_training_in_well_under_a)),
    meaning that the entire ML training run cost us around $3\. The result was 96.4%
    accuracy, comparable to the accuracy we got with the most sophisticated models
    we created in [Chapter 3](ch03.xhtml#image_vision) after a lot of tuning and experimentation.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在最后一个屏幕中，我们启用了早停功能，因此如果 AutoML 在验证指标上看不到进一步的改善，它可以决定提前停止。选择这个选项后，训练在不到 30
    分钟内完成（见[图 10-15](#automl_finished_training_in_well_under_a)），这意味着整个机器学习训练过程花费了我们约
    3 美元。结果是 96.4% 的准确率，与我们在[第 3 章](ch03.xhtml#image_vision)中经过大量调整和实验得到的最复杂模型的准确率相当。
- en: '![](Images/pmlc_1015.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_1015.png)'
- en: Figure 10-15\. AutoML finished training in well under an hour at a cost of less
    than $3, and it achieved an accuracy of 96.4% on the 5-flowers dataset.
  id: totrans-228
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-15\. AutoML 在不到一个小时内完成训练，成本不到 3 美元，并在 5-花数据集上达到了 96.4% 的准确率。
- en: Note
  id: totrans-229
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: 'We should caution you that not all no-code systems are the same—the [Google
    Cloud AutoML](https://oreil.ly/GvLwR) system we used in this section performs
    data preprocessing and augmentation, employs state-of-the-art models, and carries
    out hyperparameter tuning to build a very accurate model. Other no-code systems
    might not be as sophisticated: some train only one model (e.g., ResNet50), some
    train a single model but do hyperparameter tuning, and some others search among
    a family of models (ResNet18, ResNet50, and EfficientNet). Check the documentation
    so that you know what you are getting.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该警告您，并非所有的无代码系统都相同——我们在本节中使用的 [Google Cloud AutoML](https://oreil.ly/GvLwR)
    系统进行数据预处理和增强，采用最先进的模型，并进行超参数调整以构建非常精确的模型。其他无代码系统可能没有那么复杂：有些只训练一个模型（例如 ResNet50），有些训练单一模型但进行超参数调整，还有一些在一系列模型中进行搜索（ResNet18、ResNet50
    和 EfficientNet）。查看文档，以了解您将获得什么。
- en: Evaluation
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估
- en: The evaluation results indicate that the most misclassifications were roses
    wrongly identified as tulips. If we were to continue our experimentation, we would
    examine some of the mistakes (see [Figure 10-16](#examine_the_false_positives_and_negative))
    and attempt to gather more images to minimize the false positives and negatives.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 评估结果表明，最多的误分类是将玫瑰错误地识别为郁金香。如果我们继续我们的实验，我们将检查一些错误（见[图 10-16](#examine_the_false_positives_and_negative)）并尝试收集更多图像以减少假阳性和假阴性。
- en: '![](Images/pmlc_1016.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_1016.png)'
- en: Figure 10-16\. Examine the false positives and negatives to determine which
    kinds of examples to collect more of. This can also be an opportunity to remove
    unrepresentative images from the dataset.
  id: totrans-234
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-16\. 检查假阳性和假阴性，以确定需要收集更多的哪些类型的示例。这也可以是一个机会，从数据集中移除不具代表性的图像。
- en: Once we are satisfied with the model’s performance, we can deploy it to an endpoint,
    thus creating a web service through which clients can ask the model to make predictions.
    We can then send sample requests to the model and obtain predictions from it.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们对模型的性能感到满意，我们可以将其部署到一个端点，从而创建一个 Web 服务，客户可以通过该服务要求模型进行预测。然后，我们可以向模型发送样本请求并从中获得预测结果。
- en: For basic computer vision problems, the ease of use, low cost, and high accuracy
    of no-code systems are extremely compelling. We recommend that you incorporate
    these tools as a first step in your computer vision projects.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基本的计算机视觉问题，无代码系统的易用性、低成本和高准确性非常具有吸引力。我们建议您在计算机视觉项目中作为第一步引入这些工具。
- en: Summary
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at how to operationalize the entire ML process. We
    used Kubeflow Pipelines for this purpose and took a whirlwind tour of the SDK,
    creating Docker containers and Kubernetes components and stringing them into a
    pipeline using data dependencies.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们看到了如何使整个机器学习过程操作化。我们使用 Kubeflow Pipelines 来实现这一目的，并快速浏览了 SDK，创建了 Docker
    容器和 Kubernetes 组件，并使用数据依赖关系将它们串联成一个流水线。
- en: We explored several techniques that allow us to understand what signals the
    model is relying on when it makes a prediction. We also looked at what no-code
    computer vision frameworks are capable of, using Google Cloud’s AutoML to illustrate
    the typical steps.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了几种技术，使我们能够理解模型在进行预测时依赖的信号。我们还看了一下无代码计算机视觉框架的能力，使用 Google Cloud 的 AutoML
    来说明典型的步骤。
- en: No-code tools are used by domain experts to validate problem viability, while
    machine learning pipelines are used by ML engineers in deployment, and explainability
    is used to foster adoption of machine learning models by decision makers. As such,
    these usually form the bookends of many computer vision projects and are the points
    at which data scientists interface with other teams.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 领域专家使用无代码工具来验证问题的可行性，而机器学习工程师在部署中使用机器学习管道，解释性则用于促进决策者对机器学习模型的采纳。因此，这些通常构成许多计算机视觉项目的两端，并且是数据科学家与其他团队接口的重要点。
- en: This concludes the main part of this book, where we have built and deployed
    an image classification model from end to end. In the remainder of the book, we
    will focus on advanced architectures and use cases.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着本书的主要部分的结束，我们已经从头到尾构建并部署了一个图像分类模型。在本书的其余部分，我们将专注于高级架构和使用案例。
