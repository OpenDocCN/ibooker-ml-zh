- en: Chapter 9\. Creating Custom Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。创建自定义模型
- en: In earlier chapters you saw how to use turnkey models for image labeling, object
    detection, entity extraction, and more. What you didn’t see was how you might
    be able to use models that you had created yourself, and indeed *how* you might
    be able to create them by yourself. In this chapter, we’ll look at three scenarios
    for creating models, and then in Chapters [10](ch10.html#using_custom_models_in_android)
    and [11](ch11.html#using_custom_models_in_ios), we’ll look at incorporating those
    models into Android or iOS apps.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期章节中，您了解了如何使用现成的模型进行图像标注、目标检测、实体提取等。但您没有看到的是如何使用您自己创建的模型，以及*如何*您可能自己创建它们。在本章中，我们将研究创建模型的三种场景，然后在第[10章](ch10.html#using_custom_models_in_android)和第[11章](ch11.html#using_custom_models_in_ios)中，我们将探讨如何将这些模型整合到
    Android 或 iOS 应用程序中。
- en: 'Creating models from scratch can be difficult and very time consuming. It’s
    also the realm of pure TensorFlow development and is covered in lots of other
    books such as my book *AI and Machine Learning for Coders (*O’Reilly). If you
    aren’t creating from scratch, and in particular if you’re focused on mobile apps,
    there are some tools to assist you, and we’ll look at three of them in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 从头开始创建模型可能会非常困难且非常耗时。这也是纯 TensorFlow 开发的领域，有许多其他书籍涵盖了这一点，比如我的书 *AI and Machine
    Learning for Coders*（O’Reilly）。如果您不是从头开始创建，并且特别是如果您专注于移动应用程序，则有一些工具可供您使用，我们将在本章中介绍其中的三个：
- en: '*TensorFlow Lite Model Maker* is the preferred choice *if* you are building
    an app that fits a scenario that Model Maker supports. It’s *not* a generic tool
    for building any type of model, but is designed to support common use cases such
    as image classification, object detection, and more. It involves little to no
    neural-network-specific coding, and as such is a great place to start if you don’t
    want to learn that stuff yet!'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*TensorFlow Lite Model Maker*是首选选择*如果*您正在构建符合 Model Maker 支持场景的应用程序。它*不*是用于构建任何类型模型的通用工具，而是设计用于支持如图像分类、目标检测等常见用例。它涉及的神经网络特定编码几乎为零，因此如果您还不想学习这些内容，这是一个很好的起点！'
- en: Creating models using *Cloud AutoML*, and in particular the tools in Cloud AutoML
    designed to minimize the amount of code you have to write and maintain. Similar
    to TensorFlow Model Maker, the scenarios here are focused on the core common ones,
    and if you want to step off the trail, you’ll need to do some custom model coding.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用*Cloud AutoML*创建模型，特别是 Cloud AutoML 中旨在最小化您需编写和维护的代码量的工具。与 TensorFlow Model
    Maker 类似，这里的场景侧重于核心常见的应用场景，如果您想要超出这些场景，您将需要进行一些自定义模型编码。
- en: Creating models with TensorFlow using t*ransfer learning*. In this scenario,
    you don’t create a model from scratch, but reuse an existing model, repurposing
    part of it for your scenario. As you will be getting a little closer to neural
    network modeling, this will require you to do some neural network coding. If you’re
    looking to dip your toes into the world of creating deep learning models, it’s
    a terrific way to start; much of the complexity is already implemented for you,
    but you can be flexible enough to build new model types.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 进行*迁移学习*创建模型。在这种情况下，您不需要从头开始创建模型，而是重用现有模型，为您的场景重新定位部分。当您接近神经网络建模时，这将需要您进行一些神经网络编码。如果您希望尝试一下创建深度学习模型的世界，这是一个绝佳的开始方式；大部分复杂性已经为您实现，但您可以灵活地构建新的模型类型。
- en: A further iOS-only scenario is to create models using Create ML, which also
    uses transfer learning; we’ll explore that in [Chapter 13](ch13.html#create_ml_and_core_ml_for_simple_ios_ap).
    We’ll also explore language models, where as well as the model, your mobile app
    will need to understand metadata about the model such as the dictionary of words
    used in creating the model. In order to get a gentle start, let’s explore TensorFlow
    Lite Model Maker first.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个仅限 iOS 的场景是使用 Create ML 创建模型，它也使用迁移学习；我们将在[第13章](ch13.html#create_ml_and_core_ml_for_simple_ios_ap)中探讨这一点。我们还将探讨语言模型，除了模型本身，您的移动应用还需要理解关于模型的元数据，例如用于创建模型的单词字典。为了轻松开始，让我们首先探索
    TensorFlow Lite Model Maker。
- en: Creating a Model with TensorFlow Lite Model Maker
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow Lite Model Maker 创建模型
- en: A core scenario that TensorFlow Lite Model Maker was designed for is image classification,
    and it claims to be able to help you create a basic model with only four lines
    of code. Furthermore the model will work with Android Studio’s import functionality
    so you don’t have to mess around in the assets folder, and it will generate code
    to get you started. As you’ll see in [Chapter 10](ch10.html#using_custom_models_in_android),
    models made with Model Maker are much easier to use in your apps, because the
    difficult task of translating from your Android representation of data to the
    tensors required by models has been abstracted for you into helper classes that
    are automatically generated when you import the model using Android Studio. Unfortunately,
    at the time of writing, equivalent tools are not available for Xcode, so you’ll
    need to do much of the model data input/output with code you write yourself, but
    we’ll go through some examples to give you a head start in [Chapter 11](ch11.html#using_custom_models_in_ios).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Lite Model Maker 的一个核心场景是图像分类，它声称只需四行代码即可帮助您创建基本模型。此外，该模型将与 Android
    Studio 的导入功能兼容，因此您无需在资源文件夹中进行复杂操作，它还会生成起始代码。正如您将在[第 10 章](ch10.html#using_custom_models_in_android)中看到的，使用
    Model Maker 制作的模型在应用程序中使用起来要容易得多，因为将 Android 数据表示转换为模型所需张量的困难任务已被抽象化为帮助类，在使用 Android
    Studio 导入模型时自动生成。不幸的是，在撰写本文时，Xcode 尚无等效工具可用，因此您需要自己编写代码来处理大部分模型数据的输入/输出，但我们将通过一些示例帮助您在[第
    11 章](ch11.html#using_custom_models_in_ios)中开个好头。
- en: Creating a model with the Model Maker is really simple. You’ll use Python, and
    I’ve created a Python notebook for you already; it’s available in the [code repository
    for this book](https://oreil.ly/9ImxO).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Model Maker 创建模型非常简单。您将使用 Python，我已为您创建了一个 Python 笔记本；它可以在[本书代码存储库](https://oreil.ly/9ImxO)中找到。
- en: 'Start by installing the `tflite-model-maker` package:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 首先安装 `tflite-model-maker` 包：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once that’s done, you can import `tensorflow`, `numpy`, and the various modules
    in TensorFlow Lite Model Maker like this:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些步骤后，您可以像这样导入 `tensorflow`、`numpy` 和 TensorFlow Lite Model Maker 的各种模块：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: To train a model with the Model Maker, you’ll need data, which can be folders
    of images or TensorFlow datasets. So, for this example, there’s a downloadable
    containing pictures of five different types of flower. When you download and unzip
    this, subdirectories will be created for the flowers.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Model Maker 训练模型，您需要数据，这可以是图像文件夹或 TensorFlow 数据集。因此，在本示例中，有一个可下载的包含五种不同类型花朵图片的文件。下载并解压缩后，将为花朵创建子目录。
- en: 'Here’s the code, where `url` is the location of the archive containing the
    flowers. It uses the `tf.keras.utils` libraries to download and extract the file:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是代码，其中 `url` 是包含花朵压缩包的位置。它使用 `tf.keras.utils` 库来下载和解压文件：
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If you want to inspect what it downloaded, you can do so with:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想检查下载内容，可以执行以下操作：
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This will output the contents of that path, including subdirectories, and should
    look something like this:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出该路径的内容，包括子目录，应如下所示：
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Model Maker will train a classifier using these subdirectories as the label
    names. Remember in image classification that training data is labeled (i.e., this
    is a daisy, this is a rose, this is a sunflower, etc.), and the neural network
    matches distinct features in each image to a label, so that over time it learns
    to “see” the difference between the labeled objects. It’s akin to “when you see
    this feature, it’s a sunflower, and when you see this, it’s a dandelion,” etc.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Model Maker 将使用这些子目录作为标签名称来训练分类器。请记住，在图像分类中，训练数据是有标签的（即这是雏菊，这是玫瑰，这是向日葵等），神经网络会将每个图像的不同特征与标签匹配，以便随着时间的推移学会“看到”这些标记对象的不同之处。这类似于“当你看到这个特征时，它是向日葵；当你看到这个时，它是蒲公英”等。
- en: 'You shouldn’t use *all* your data to train the model. It’s a best practice
    to hold some back to see if the model really understands the differences between
    the different types of flower in a general sense, or if it is specialized to everything
    that it sees. That sounds abstract, so consider it like this: you want your model
    to recognize roses *that it hasn’t yet seen* and not just recognize roses based
    on the images you train it on. A simple technique to see if you are successful
    at this is to only use a portion of your data—say 90%—to train on. The other 10%
    is not *seen* while training. This 10% could then be a representation of what
    the network will be asked to classify in the future, and measuring its effectiveness
    on that is better than measuring its effectiveness on what it was trained on.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你不应该用*所有*的数据来训练模型。一个最佳实践是保留一部分数据，以便验证模型是否真正理解了一般意义上不同类型的花之间的区别，或者它是否专门于它所见到的一切。这听起来有点抽象，可以这样理解：你希望你的模型能够识别出*它尚未见过的*玫瑰，而不仅仅是根据训练时使用的图像来识别玫瑰。一个简单的技术是只使用你数据的一部分
    — 比如说90% — 来进行训练。在训练过程中，剩余的10%是不*可见*的。这10%可以代表网络将来需要分类的内容，评估它在这方面的有效性比评估它在训练时的有效性更为重要。
- en: 'To achieve this, you’ll need to create a data loader for Model Maker from a
    folder. As the images are already in the *image_page* folder, you can use the
    `from_folder` method on the `ImageClassifierDataLoader` object to get a dataset.
    Then, once you have your dataset, you can split it using the split method; to
    get `train_data` with 90% of your images, and `test_data` with the other 10%,
    you can use code like this:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现这一点，你需要为Model Maker创建一个数据加载器，从一个文件夹中获取数据。由于图像已经位于*image_page*文件夹中，你可以在`ImageClassifierDataLoader`对象上使用`from_folder`方法来获取数据集。然后，一旦你有了数据集，可以使用split方法将其分割；使用类似以下代码获取包含90%图像的`train_data`和另外10%图像的`test_data`：
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You’ll likely get an output like the following. Note that 3,670 is the number
    of total images in the dataset, and not the 90% used for training. The number
    of labels is the number of distinct types—in this case five—labeled with the five
    types of flower:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会得到以下类似的输出。请注意，3,670是数据集中的总图像数，而不是用于训练的90%图像数。标签数是不同类型的标签数 — 在这种情况下是五种 —
    对应于五种不同类型的花。
- en: '[PRE6]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, in order to train your model, all you have to do is call the `create`
    method on the image classifier and it will do the rest:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了训练你的模型，你只需要在图像分类器上调用`create`方法，它会完成其余的工作：
- en: '[PRE7]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This probably looks a little too simple—it’s because a lot of the complexity
    such as defining the model architecture, performing transfer learning from an
    existing model, specifying a loss function and optimizer, and finally doing the
    training is all encapsulated within the `image_classifier` object. It’s open source,
    so you can peek under the hood by checking out the [TensorFlow Lite Model Maker
    repo](https://oreil.ly/WYfXO).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来有点太简单了 — 因为很多复杂性，比如定义模型架构、从现有模型进行迁移学习、指定损失函数和优化器，最后进行训练，全部封装在`image_classifier`对象内部。它是开源的，所以你可以通过查看[TensorFlow
    Lite Model Maker repo](https://oreil.ly/WYfXO)来深入了解。
- en: 'There’ll be a lot of output and while it might look a little alien to you at
    first, it’s good to take a quick look at it to understand. Here’s the complete
    output:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 输出会有很多内容，虽然一开始可能看起来有点陌生，但快速查看一下是很有必要的，以便理解。以下是完整的输出：
- en: '[PRE8]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The first part of the output is the *model architecture*, telling us how the
    model has been designed. This model has three layers: the first layer is called
    `hub_keras_layer_v1v2_1`, which looks a bit mysterious. We’ll get back to that
    in a second. The second is called `dropout_1`, and the third (and final) is called
    `dense_1`. What’s important is to note the shape of the final layer—it’s `(None,
    5)`, which in TensorFlow terms means that the first dimension can be any size,
    and the second is 5\. Where have we seen 5 before? That’s right, it’s the number
    of classes in this dataset—we have five flowers. So the output of this will be
    a number of items each with five elements. The first number is for handling what’s
    called *batch* inference. So if you throw a number of images, say 20, at the model,
    then it will give you a single 20 × 5 output, containing the inferences for the
    20 images.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的第一部分是*模型架构*，告诉我们模型是如何设计的。这个模型有三层：第一层称为`hub_keras_layer_v1v2_1`，看起来有点神秘。我们稍后会回到这一点。第二层称为`dropout_1`，第三层（也是最后一层）称为`dense_1`。重要的是要注意最后一层的形状——它是`(None,
    5)`，在TensorFlow术语中表示第一维可以是任何大小，第二维是5。我们之前在哪里看到过5？没错，这是数据集中的类别数量——我们有五种花。因此，这个输出将是包含五个元素的每个项目的数量。第一个数字用于处理所谓的*批量*推理。因此，如果您向模型提供一些图像，比如20张，它将为您提供一个20
    × 5的单一输出，其中包含20张图像的推断结果。
- en: Why are there five values, you might wonder, and not a single value? Look back
    to [Chapter 2](ch02.html#introduction_to_computer_vision), and in particular the
    cats versus dogs example ([Figure 2-5](ch02.html#updating_for_cats_or_dogs)),
    and you’ll see that the output of a neural network that recognizes *n* classes
    has *n* neurons, with each neuron representing the probability that the network
    sees that class. So, in this case, the output of the neurons are the probabilities
    of the five classes respectively (i.e., neuron 1 for daisy, neuron 2 for dandelion,
    etc.). And if you were wondering what determines the order—in this case, it’s
    simply alphabetic.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，为什么有五个值，而不是一个单一的值？回头看一下[第二章](ch02.html#introduction_to_computer_vision)，特别是猫和狗的例子（[图 2-5](ch02.html#updating_for_cats_or_dogs)），你会发现一个识别*n*类的神经网络的输出有*n*个神经元，每个神经元表示网络识别该类的概率。因此，在这种情况下，神经元的输出分别是五个类别的概率（即，神经元1代表雏菊，神经元2代表蒲公英，依此类推）。如果你想知道是什么决定了这个顺序——在这种情况下，它仅仅是按字母顺序排列。
- en: So what about the other layers? Well, let’s start with the first one that’s
    called `hub_keras-layer_v1v2_1`. That’s a weird name! But the clue is at the beginning
    with the word “hub.” A common form of training a model is to use an existing,
    pretrained model and adapt it to your needs. Think of it a little like subclassing
    an existing class and overriding some methods. There are many models out there
    that are trained on *millions* of images, and as such have become really good
    at extracting features from those images. So, instead of starting from scratch,
    it’s often easier to use the features that they’ve already learned and match those
    to your images. This process is called *transfer learning*, and TensorFlow Hub
    stores a number of pretrained models, or parts of models with learned features
    (also called feature vectors) that can be reused. It’s almost like a library of
    classes of existing code that you can reuse, but instead of code, it’s neural
    networks that already know how to do something. So, this model then takes an existing
    model from TensorFlow Hub and uses its features. The output from that model will
    be 1,280 neurons, indicating that it has 1,280 features that it tries to “spot”
    in your image.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 那么其他层呢？好吧，我们从第一层开始，称为`hub_keras-layer_v1v2_1`。这是一个奇怪的名字！但提示在开头用了“hub”这个词。一种常见的模型训练形式是使用现有的预训练模型，并根据自己的需求进行调整。可以把它想象成子类化现有类并重写某些方法。有许多模型是在*数百万*张图片上进行训练的，因此它们在从这些图片中提取特征方面非常擅长。因此，与其从头开始，通常更容易使用它们已经学习过的特征，并将其与您的图片匹配。这个过程被称为*迁移学习*，而TensorFlow
    Hub存储了许多预训练模型或具有已学习特征（也称为特征向量）的模型部分，可以重复使用。这几乎就像是一个现有代码类库，但不是代码，而是已经知道如何执行某些任务的神经网络。因此，这个模型使用TensorFlow
    Hub中的现有模型并使用其特征。该模型的输出将是1,280个神经元，表示它尝试在您的图像中“识别”的1,280个特征。
- en: This is followed by a “dropout” layer, which is a common trick in neural network
    design that is used to speed up training of the network. From a high level, it
    just means “ignore a bunch of neurons”! The logic is that something simple like
    flowers, where there are only five classes, don’t need 1,280 features to be spotted,
    so it’s more reliable to ignore some of them at random! Sounds weird, I know,
    but when you look more in depth into how models can be created (my book *AI and
    Machine Learning for Coders* is a good start), it will make a bit more sense.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这之后是一个“dropout”层，这是神经网络设计中常见的技巧，用于加快网络的训练速度。从高层次来看，它只是意味着“忽略一些神经元”！逻辑是，像花这样简单的东西，只有五类，不需要1,280个特征来识别，因此随机忽略其中一些更可靠！听起来有点奇怪，我知道，但当您更深入地了解模型的创建方式时（我的书
    *AI和机器学习编程者* 是一个很好的起点），这就会有点道理。
- en: The rest of the output should make sense if you’ve followed Chapters [1](ch01.html#introduction_to_ai_and_machine_learning)
    and [2](ch02.html#introduction_to_computer_vision) in this book. It’s five epochs
    of training, where in each epoch we make a guess as to the class of an image,
    and repeat across all images. How good or bad those guesses are is then used to
    optimize the network and prepare to make another guess. With five epochs, we only
    do that loop five times. Why so few? Well, it’s because we’re using transfer learning
    as described earlier. The features in the neural network aren’t starting from
    zero—they’re already well trained. We just have to fine-tune them for our flowers,
    so we can do it quickly.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经阅读了本书的第[1](ch01.html#introduction_to_ai_and_machine_learning)章和第[2](ch02.html#introduction_to_computer_vision)章，剩下的输出应该是有意义的。这是五次训练周期，在每个周期中，我们会对图像的类别进行猜测，并在所有图像上重复这个过程。这些猜测的好坏将用来优化网络并准备进行另一次猜测。在五个周期中，我们只执行这个循环五次。为什么这么少？嗯，因为我们使用的是之前描述过的迁移学习。神经网络中的特征并不是从零开始的——它们已经经过良好的训练。我们只需为我们的花朵微调它们，所以我们可以快速地完成。
- en: By the time we reach the end of Epoch 5, we can see that the accuracy is 0.9369,
    so this model is about 94% accurate *on the training set* at distinguishing between
    different types of flowers. If you were wondering what the [===] is before that,
    along with a number, such as 103/103, this is indicating that our data is being
    loaded into the model for training in *batches*. Instead of training the model
    one image at a time, TensorFlow can be more efficient by training it in batches
    of images. Remember that our dataset in this case had 3,670 images in it, and
    we used 90% of them for training? That would give us 3,303 images for training.
    Model Maker defaults to 32 images per batch, so we will get 103 batches of 32
    during training. This is 3,296 images, so we will not use 7 images per epoch.
    Note that if batches are shuffled, each batch in each epoch will have a different
    set of images, so those seven will be used eventually! Training is typically faster
    if done in batches, hence this default.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们达到第5个周期的末尾时，我们可以看到准确率为0.9369，所以这个模型在*训练集*上区分不同类型的花大约达到了94%的准确度。如果你想知道前面的[===]是什么，以及一个数字，比如103/103，这表明我们的数据正在被加载到模型中进行*批处理*训练。与逐个图像训练模型相比，TensorFlow可以通过图像批处理训练来提高效率。请记住，在这种情况下，我们的数据集中有3,670张图像，并且我们用于训练的是其中的90%？这将给我们训练用的3,303张图像。Model
    Maker默认每批次32张图像，所以在训练过程中我们将获得103批次的32张图像。这是3,296张图像，因此每个周期我们将不使用7张图像。请注意，如果批次被随机化，每个周期中的每个批次将具有不同的图像集合，所以这七张图像最终会被使用！如果批处理训练，训练通常会更快，因此采用了这种默认方式。
- en: 'Remember, we held back 10% of the data as a test set, so we can evaluate how
    the model does with that data like this:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们将数据的10%作为测试集保留，这样我们可以评估模型在该数据上的表现，就像这样：
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'And we’d see a result a bit like this:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会看到一个结果有点像这样：
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The accuracy here shows that on the test set—images that the network has *not*
    previously seen—we’re at about 93% accuracy. We have a really good network that
    doesn’t overfit to the training data! This is just a high-level look, and when
    you get into building models in more detail, you’ll want to explore more detailed
    metrics—such as exploring how accurate the model is for each class of image (often
    called a “confusion matrix”) so you can determine if you aren’t overcompensating
    for some types. That’s beyond the scope of this book, but check out other books
    from O’Reilly such as my *AI and Machine Learning for Coders* to learn more details
    about creating models.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的准确率显示，在测试集上——即网络之前*未曾见过*的图像上——我们的准确率约为93%。我们有一个非常好的网络，不会过度拟合训练数据！这只是一个高层次的观察，当你深入构建更详细的模型时，你会希望探索更详细的指标——比如探索模型对每个图像类别的准确率（通常称为“混淆矩阵”），这样你就可以确定是否在过度补偿某些类型。这超出了本书的范围，但请查看O'Reilly的其他书籍，如我的*AI和机器学习
    for Coders*，以了解更多有关创建模型的细节。
- en: 'Now all we need to do is export the trained model. At this point we have a
    *TensorFlow* model that executes in the Python environment. We want a TensorFlow
    *Lite* model that we can use on mobile devices. To that end, the Model Maker tools
    give us an export we can use:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只需要导出训练好的模型。此时，我们有一个在Python环境中执行的*TensorFlow*模型。我们想要一个TensorFlow *Lite*模型，可以在移动设备上使用。为此，Model
    Maker工具提供了一个可以使用的导出：
- en: '[PRE11]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This will convert the model to the TFLite format, similar to what we saw in
    [Chapter 8](ch08.html#going_deeper_understanding_tensorflow_l), but *also* encode
    metadata and label information into the model. This gives us a one-file solution
    that makes it easier to import the model into Android Studio for Android developers
    and have it generate the proper label details. We’ll see that in [Chapter 10](ch10.html#using_custom_models_in_android).
    Unfortunately, this is Android-specific, so iOS developers will need to also have
    the label file on hand. We’ll explore that in [Chapter 11](ch11.html#using_custom_models_in_ios).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把模型转换为TFLite格式，类似于我们在[第8章](ch08.html#going_deeper_understanding_tensorflow_l)中看到的，但*还*会将元数据和标签信息编码到模型中。这为我们提供了一个单一文件解决方案，使得将模型导入到Android
    Studio以供Android开发者使用，并生成正确的标签细节变得更加容易。我们将在[第10章](ch10.html#using_custom_models_in_android)中看到这一点。不幸的是，这是特定于Android的，所以iOS开发者也需要有标签文件。我们将在[第11章](ch11.html#using_custom_models_in_ios)中探讨这一点。
- en: If you’re using Colab, you’ll see the *model.tflite* file in the files explorer;
    otherwise, it will be written to the path you’re using with your Python code.
    You’ll use this in Chapters [10](ch10.html#using_custom_models_in_android) and
    [11](ch11.html#using_custom_models_in_ios) for Android and iOS apps, respectively.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用Colab，你会在文件浏览器中看到*model.tflite*文件；否则，它将被写入你在Python代码中使用的路径。你将在第[10章](ch10.html#using_custom_models_in_android)和[11章](ch11.html#using_custom_models_in_ios)中为Android和iOS应用程序使用它。
- en: Before going there, though, let’s look at some other options for creating models,
    starting with Cloud AutoML. This is an interesting scenario because, as its name
    suggests, it will automatically generate ML for you. But beyond not needing to
    write any code, it also takes some time to explore multiple model architectures
    to find the one that will work best for your data. So, creating models with it
    will be much slower, but you’re going to get far more accurate and optimal ones.
    It also exports in a variety of formats, including TensorFlow Lite.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在那之前，让我们看看创建模型的其他选项，从Cloud AutoML 开始。这是一个有趣的场景，因为正如其名称所示，它会自动生成ML模型。但除了不需要编写任何代码之外，它还需要一些时间来探索多个模型架构，以找到最适合你数据的模型。因此，使用它创建模型会更慢，但你将得到更精确和更优化的模型。它还支持多种格式输出，包括TensorFlow
    Lite。
- en: Creating a Model with Cloud AutoML
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Cloud AutoML创建模型
- en: 'The goal behind AutoML is to give you a set of cloud-based tools that let you
    train custom machine learning models with as little coding effort and expertise
    as possible. There are tools for a number of scenarios:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: AutoML的目标是为您提供一套基于云的工具，让您尽可能少的编写代码和专业知识，让您训练自定义机器学习模型。有一些场景的工具可以使用：
- en: '*AutoML Vision* gives you image classification or object detection, and you
    can run the inference in the cloud or output a model that can be used on devices.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*AutoML Vision*为你提供了图像分类或对象检测，并且你可以在云中运行推理或输出可用于设备的模型。'
- en: '*AutoML Video Intelligence* gives you the ability to detect and track objects
    in video.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*AutoML Video Intelligence*让你能够检测和跟踪视频中的对象。'
- en: '*AutoML Natural Language* lets you understand the structure and sentiment of
    text.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*AutoML Natural Language* 允许您理解文本的结构和情感。'
- en: '*AutoML Translation* lets you translate text between languages.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*AutoML Translation* 允许您在不同语言之间翻译文本。'
- en: '*AutoML Tables* lets you build models using structured data.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*AutoML Tables* 允许您使用结构化数据构建模型。'
- en: For the most part, these tools are designed for models that run on a backend
    server. The one exception is AutoML Vision, and a specific subset called AutoML
    Vision Edge which lets you train image classification and labeling scenarios that
    can run on a device. We’ll explore that next.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，这些工具设计用于在后端服务器上运行的模型。唯一的例外是 AutoML Vision，特别是称为 AutoML Vision Edge 的特定子集，它允许您训练能够在设备上运行的图像分类和标记场景。我们将在下面探讨这个。
- en: Using AutoML Vision Edge
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 AutoML Vision Edge
- en: AutoML Vision Edge uses the Google Cloud Platform (GCP), so in order to continue,
    you’ll need a Google Cloud project with billing enabled. I won’t cover the steps
    for that here, but you can find them on the [GCP site](https://oreil.ly/9GBFq).
    There are a few steps you have to follow to use the tool. Go through these carefully.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: AutoML Vision Edge 使用 Google Cloud 平台（GCP），因此，为了继续，您需要启用计费的 Google Cloud 项目。我不会在这里覆盖这些步骤，但您可以在
    [GCP 网站](https://oreil.ly/9GBFq) 上找到它们。使用该工具需要您按照这些步骤仔细操作。
- en: 'Step 1: Enable the API'
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '步骤 1: 启用 API'
- en: Once you have a project, open it in the Google Cloud Console. This is available
    at *[*console.cloud.google.com*](http://console.cloud.google.com)*. At the top
    lefthand side you can drop down a menu of options, and you’ll see an entry for
    APIs and Services. See [Figure 9-1](#selecting_the_apis_and_services_option).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您有了项目，在 Google Cloud 控制台中打开它。这可通过 [*console.cloud.google.com*](http://console.cloud.google.com)
    访问。在左上角，您可以展开一个选项菜单，您将看到一个名为 API 和服务 的条目。参见 [图 9-1](#selecting_the_apis_and_services_option)。
- en: '![](assets/aiml_0901.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0901.png)'
- en: Figure 9-1\. Selecting the APIs and Services option in the Cloud Console
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-1\. 在 Cloud 控制台中选择 API 和服务选项
- en: From here, select Library to get the Cloud API Library screen. There’ll be lots
    of tiles with a search bar at the top. Use this to search for “AutoML” and you’ll
    find an entry for “Cloud AutoML API.”
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，选择“库”以获取 Cloud API Library 屏幕。顶部有一个带有搜索栏的瓷砖列表。使用它来搜索“AutoML”，您将找到“Cloud
    AutoML API” 的条目。
- en: Select this, and you’ll end up on the landing page for Cloud AutoML API. This
    gives you details about the model, including pricing. Click the Enable button
    to turn it on if you want to use it.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 选择这个选项，您将进入 Cloud AutoML API 的首页面。这里会提供有关模型的详细信息，包括定价。如果您希望使用它，请点击“启用”按钮以打开它。
- en: 'Step 2: Install the gcloud command-line tool'
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '步骤 2: 安装 gcloud 命令行工具'
- en: Depending on your developer workstation, there are a number of options for how
    you can install the Cloud SDK, which includes the gcloud command-line tool. I’m
    using a Mac for this chapter, so I’ll go through installing it with that. (These
    instructions should also work on Linux.) For a full guide, check out the [Google
    Cloud docs](https://oreil.ly/nDKra).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的开发工作站，您可以选择多种安装 Cloud SDK（包括 gcloud 命令行工具）的选项。本章节我使用的是 Mac，所以我会按照那个安装它。（这些说明在
    Linux 上也适用。）完整指南请查看 [Google Cloud 文档](https://oreil.ly/nDKra)。
- en: 'To use the “interactive” installer, which gives you options about your environment,
    do the following. From within a terminal enter the following command:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用“交互式”安装程序，该程序会让你选择环境设置，请执行以下操作。在终端中输入以下命令：
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You’ll be asked for your directory. You’ll usually use your home directory,
    which should be the default, so you can just say “yes” to continue when asked.
    You’ll also be asked if you want to add the SDK command-line tools to your PATH
    environment. If so, make sure you say “yes.” After the install is complete, restart
    your terminal/shell.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 系统会询问您的目录。通常会使用您的主目录，默认情况下应该是这样，所以当询问时您可以直接回答“yes”以继续。还会询问是否将 SDK 命令行工具添加到您的
    PATH 环境变量中。如果是，请确保选择“yes”。安装完成后，请重新启动您的终端/Shell。
- en: 'Once that’s done, you should issue the following command in your terminal:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，在您的终端中发出以下命令：
- en: '[PRE13]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You’ll be taken through a workflow to sign in to your project: a link is generated,
    and when you click on that your browser will open asking you to sign into your
    Google account. After this you’ll be asked to give permissions to the API. To
    use it, you’ll need to give these permissions.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 您将通过一个流程引导来登录您的项目：会生成一个链接，点击链接后，浏览器将打开并要求您登录您的 Google 账号。之后，您将被要求授予 API 的权限。要使用它，您需要授予这些权限。
- en: You’ll be asked also to provide the region that the Compute Engine resources
    will run in. You *must* pick a us-central-1 instance for these APIs to work, so
    be sure to do so. Note that this may change in the future, but at the time of
    writing, this is the only region that supports these APIs. Check out the [Edge
    device model quickstart guide](https://oreil.ly/Sn0ip) before you select any others.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你还将被要求提供计算引擎资源将在其中运行的区域。你 *必须* 选择us-central-1实例，以便这些API正常工作，请确保这样做。请注意，这在将来可能会发生变化，但在撰写本文时，这是唯一支持这些API的区域。在选择其他选项之前，请查看[Edge设备模型快速入门指南](https://oreil.ly/Sn0ip)。
- en: If you have more than one GCP project, you’ll also be asked to select the appropriate
    one. Once all that’s done, you’ll be ready to use the command-line tool with AutoML!
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有多个GCP项目，你还将被要求选择适当的项目。一旦所有操作完成，你就可以准备好使用命令行工具与AutoML！
- en: 'Step 3: Set up a service account'
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第三步：设置一个服务账户
- en: Cloud supports a number of authentication types, but AutoML Edge *only* supports
    service-account-based authentication. So next up you’ll have to create one and
    get the key file for it. You can do this using the Cloud Console. Open the IAM
    & Admin section in the menu (you can see it in [Figure 9-1](#selecting_the_apis_and_services_option))
    and select Service Accounts.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Cloud支持多种身份验证类型，但AutoML Edge *仅*支持基于服务账户的身份验证。因此，接下来你需要创建一个，并获取其密钥文件。你可以使用Cloud控制台完成这个操作。在菜单中打开IAM和管理部分（你可以在[图 9-1](#selecting_the_apis_and_services_option)中看到它），然后选择服务账户。
- en: At the top of the screen, there’ll be an option to “Add a service” account.
    Select it, and you’ll be asked to fill out details for the service account.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 屏幕顶部有一个“添加服务账户”的选项。选择它，然后你将被要求填写服务账户的详细信息。
- en: Give it a name in step 1, and in step 2 you’ll be asked to grant this service
    account access to the project. Make sure you select the role AutoML Editor here.
    You can drop down the box and search for this role. See [Figure 9-2](#figure_nine_two_service_account_details).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中为其命名，在第二步中，你将被要求授予该服务账户对项目的访问权限。确保你在这里选择AutoML编辑器角色。你可以展开框并搜索该角色。参见[图 9-2](#figure_nine_two_service_account_details)。
- en: '![](assets/aiml_0902.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0902.png)'
- en: Figure 9-2\. Service account details
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-2\. 服务账户详细信息
- en: In step 3, you’ll be asked to enter service account users and service account
    admin roles. You can enter email addresses here, and if you’re just doing this
    for the first time to learn about it, enter your email address for both.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三步中，你将被要求输入服务账户用户和服务账户管理员角色。你可以在这里输入电子邮件地址，如果你只是第一次学习这个，可以为两者都输入你的电子邮件地址。
- en: When you’re done, the account will be created and you’ll be returned to a screen
    containing a number of service accounts. You may only have one if you’re doing
    this for the first time. Either way, select the one you just created (you’ll find
    it based on its name), and you’ll be taken to the service account details page.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 当你完成后，账户将被创建，并且你将返回到一个包含多个服务账户的页面。如果你是第一次这样做，可能只会有一个。无论如何，选择你刚刚创建的那个（你可以根据其名称找到它），然后你将进入服务账户详细信息页面。
- en: At the bottom of the screen, there’s an Add Key button. Select this, and you’ll
    be given a number of options for the key type. Select JSON, and a JSON file containing
    your key will download to your machine.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 屏幕底部有一个添加密钥按钮。选择它，你将看到多种密钥类型选项。选择JSON，然后一个包含你的密钥的JSON文件将下载到你的计算机上。
- en: 'Return to your terminal and set up these credentials:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 返回到你的终端并设置这些凭据：
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'While you’re there, here’s a handy shortcut to set up your `PROJECT_ID` environment
    variable:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在那里时，这里有一个便捷的快捷方式来设置你的`PROJECT_ID`环境变量：
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Step 4: Set up a Cloud Storage bucket and store training data in it'
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第四步：设置一个Cloud Storage存储桶并将训练数据存储在其中
- en: 'When training a model with AutoML, the data also has to be stored where the
    cloud services can access it. You’ll do this with a Cloud Storage bucket. You
    can create this with the `mb` command (for make bucket) in the `gsutil`. Earlier
    you set up the `PROJECT_ID` environment variable to point at your project, so
    this code will work. Note that you’ll call the bucket by the same name as your
    project, but with `-vcm` appended using `${PROJECT_ID}-vcm`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用AutoML训练模型时，数据也必须存储在云服务可以访问的地方。你将使用Cloud Storage存储桶来完成这项任务。你可以使用`gsutil`中的`mb`命令（用于创建存储桶）。之前你已经设置了`PROJECT_ID`环境变量来指向你的项目，所以这段代码将有效。请注意，你将使用`${PROJECT_ID}-vcm`的方式命名这个存储桶，与你的项目名称相同，但附加了`-vcm`：
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You can then export an environment variable to point at this bucket:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你可以导出一个环境变量来指向这个存储桶：
- en: '[PRE17]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The flower photos dataset we’ve been using in this chapter is available in
    a public cloud bucket at *cloud-ml-data/img/flower_photos*, so you can copy them
    to *your* bucket with this command:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中一直在使用的花卉照片数据集位于公共云存储桶*cloud-ml-data/img/flower_photos*中，因此您可以使用此命令将它们复制到*您的*存储桶：
- en: '[PRE18]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note that if you have been a heavy user of cloud services and have used the
    same service account in many roles, you may have conflicting permissions, and
    some people have reported not being able to write to their Cloud Storage bucket.
    If this is the case for you, try to ensure that your service account has `storage.admin`,
    `storage.objectAdmin`, and `storage.objectCreator` roles.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果您是云服务的重度用户，并且在许多角色中使用了同一服务帐户，则可能存在冲突的权限，并且一些人报告说无法写入其Cloud Storage存储桶。如果您遇到此情况，请尝试确保您的服务帐户具有`storage.admin`、`storage.objectAdmin`和`storage.objectCreator`角色。
- en: 'When we used Model Maker or Keras earlier in this chapter, there were tools
    that organized the labeling of images based on their directories, but when dealing
    with AutoML like this it’s a little more raw. The dataset provides a CSV file
    with the locations and labels, but it’s pointing at the public bucket’s URLs,
    so you’ll need to update it to your own. This command will download it from the
    public bucket, edit it, and save it as a local file called *all_data.csv*:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在本章的早些时候使用模型制造器或Keras时，有工具根据其目录组织图像的标签，但处理像这样的AutoML时，它会更加原始。数据集提供了一个包含位置和标签的CSV文件，但它指向公共存储桶的URL，因此您需要将其更新为您自己的URL。此命令将从公共存储桶下载它，编辑它，并保存为名为*all_data.csv*的本地文件：
- en: '[PRE19]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'And then this will upload that file to your Cloud Storage bucket:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这将把该文件上传到您的Cloud Storage存储桶：
- en: '[PRE20]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Step 5: Turn your images into a dataset and train the model'
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第5步：将您的图像转换为数据集并训练模型
- en: At this point you have a storage bucket with a lot of images in it. You’ll next
    turn these into a dataset that can be used to train a model. To do this, visit
    the AutoML Vision dashboard at [*https://console.cloud.google.com/vision/dashboard*](https://console.cloud.google.com/vision/dashboard)*.*
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您的存储桶中有大量图像。接下来，您将把它们转换为可以用来训练模型的数据集。要做到这一点，请访问AutoML Vision仪表板，网址为[*https://console.cloud.google.com/vision/dashboard*](https://console.cloud.google.com/vision/dashboard)*.*。
- en: You’ll see cards to get started with AutoML Vision, Vision API, or Vision Product
    Search. Choose the AutoML Vision card and select Get Started. See [Figure 9-3](#automl_vision_options).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到卡片以开始使用AutoML Vision、Vision API或Vision Product Search。选择AutoML Vision卡并选择开始。参见[图 9-3](#automl_vision_options)。
- en: '![](assets/aiml_0903.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0903.png)'
- en: Figure 9-3\. AutoML Vision options
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 9-3\. AutoML Vision options
- en: 'You’ll be taken to a list of datasets, which is probably empty if you haven’t
    used GCP a lot, with a button at the top of the screen saying New Data Set. Click
    it and you’ll get a dialog asking you to create a new dataset. There’ll likely
    be at least three options: Single-Label Classification, Multi-Label Classification,
    or Object Detection. Choose Single-Label Classification and select Create Dataset.
    See [Figure 9-4](#creating_a_new_dataset).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 您将进入数据集列表，如果您没有经常使用GCP，这里可能是空的，并在屏幕顶部看到一个按钮，上面写着新建数据集。单击它，您将收到一个对话框，要求您创建一个新数据集。可能至少会有三个选项：单标签分类、多标签分类或对象检测。选择单标签分类并选择创建数据集。参见[图 9-4](#creating_a_new_dataset)。
- en: You’ll be asked to select files to import. Earlier you created a CSV with the
    details of your dataset, so choose Select a CSV on Cloud Storage and in the dialog
    enter the URL of that. It should be something like *gs://project-name-vcm/csv/all_data.csv*.
    See [Figure 9-5](#importing_a_csv_file_from_cloud_storage).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 您将被要求选择要导入的文件。早些时候，您创建了一个包含数据集详细信息的CSV文件，因此请选择选择Cloud Storage上的CSV，并在对话框中输入其URL。它应该是类似*gs://project-name-vcm/csv/all_data.csv*。参见[图 9-5](#importing_a_csv_file_from_cloud_storage)。
- en: '![](assets/aiml_0904.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0904.png)'
- en: Figure 9-4\. Creating a new dataset
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 9-4\. 创建一个新数据集
- en: '![](assets/aiml_0905.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0905.png)'
- en: Figure 9-5\. Importing a CSV file from Cloud Storage
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 9-5\. 从Cloud Storage导入CSV文件
- en: You can click Browse to find it also. After doing this and selecting continue,
    your files will begin to import. You can return to the list of datasets to see
    this. See [Figure 9-6](#importing_images_to_a_dataset).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 点击浏览来查找它。在执行此操作并选择继续后，您的文件将开始导入。您可以返回到数据集列表以查看这一点。参见[图 9-6](#importing_images_to_a_dataset)。
- en: '![](assets/aiml_0906.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0906.png)'
- en: Figure 9-6\. Importing images to a dataset
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 9-6\. 导入图像到数据集
- en: This can take some time to complete, so keep an eye on the status on the left.
    With this dataset, when it’s done, it will give you a warning, as in [Figure 9-7](#when_the_data_completes_uploading).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能需要一些时间来完成，所以请留意左侧的状态。对于这个数据集，在完成时，它将给出警告，就像 [图 9-7](#when_the_data_completes_uploading)
    中所示。
- en: '![](assets/aiml_0907.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0907.png)'
- en: Figure 9-7\. When the data completes uploading
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-7\. 数据上传完成时
- en: Once it’s done, you can select it, and you’ll be able to browse the dataset.
    See [Figure 9-8](#exploring_the_flowers_dataset).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，您可以选择它，然后浏览数据集。参见 [图 9-8](#exploring_the_flowers_dataset)。
- en: '![](assets/aiml_0908.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0908.png)'
- en: Figure 9-8\. Exploring the flowers dataset
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-8\. 探索花卉数据集
- en: Now that the data has been imported, training is as simple as selecting the
    Train tab and selecting Start Training. In the ensuing dialog, make sure you select
    Edge as the type of model you want to train, before selecting Continue. See [Figure 9-9](#defining_the_model).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已导入，训练就像选择“训练”选项卡并选择“开始训练”那样简单。在随后的对话框中，请确保选择“Edge”作为您想要训练的模型类型，然后选择“继续”。参见
    [图 9-9](#defining_the_model)。
- en: '![](assets/aiml_0909.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0909.png)'
- en: Figure 9-9\. Defining the model
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-9\. 定义模型
- en: After selecting this, you’re given the choice to optimize your model. This will
    give you the choice of a larger model that is more accurate, a smaller one that
    is faster, but maybe less accurate, or something in between. Next there’ll be
    a choice for how many compute hours you want to use to train the model. The default
    is four node hours. Choose this and then select Start Training. It may take some
    time to train, and when it’s done, AutoML will email you with the details of the
    training!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 选择此选项后，您可以选择优化您的模型。这将让您选择一个更精确的更大型模型，或者一个更快但可能不那么准确的更小型模型，或者介于两者之间的某种模型。接下来，您可以选择要用来训练模型的计算小时数。默认为四个节点小时。选择这个选项，然后选择“开始训练”。训练可能需要一些时间，在完成时，AutoML
    将通过电子邮件向您发送训练的详细信息！
- en: 'Step 6: Download the model'
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '步骤 6: 下载模型'
- en: Once the training is complete, you’ll get an email from Google Cloud Platform
    informing you that it’s ready. When you return to the console using the link in
    the email, you’ll see the results of the training. This training will have taken
    quite a while—as many as two to three hours—because it performed a neural architecture
    search to find the optimum architecture to classify these flowers, and you’ll
    see that borne out in the results—the precision here is 99%. See [Figure 9-10](#completing_the_training).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，您将收到来自 Google Cloud Platform 的电子邮件通知您准备就绪。当您通过邮件中的链接返回控制台时，您将看到训练结果。由于进行了神经架构搜索以找到最佳架构来分类这些花卉，这个训练可能需要相当长的时间——多达两到三个小时——而且您将在结果中看到这一点——精确度达到了99%。参见
    [图 9-10](#completing_the_training)。
- en: '![](assets/aiml_0910.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0910.png)'
- en: Figure 9-10\. Completing the training
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-10\. 训练完成
- en: From here you can go to the Test and Use tab, where the model can be exported
    in a variety of formats, including TensorFlow Lite, TensorFlow.js and Core ML!
    Select the appropriate one (TensorFlow Lite mostly for this book, though we’ll
    explore some Core ML in later chapters) and you can download it. See [Figure 9-11](#export_options_for_your_model).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，您可以转到“测试和使用”选项卡，可以将模型导出为多种格式，包括 TensorFlow Lite、TensorFlow.js 和 Core ML！选择适当的格式（本书大多数情况下使用
    TensorFlow Lite，尽管我们将在后面章节中探索一些 Core ML），然后您可以下载它。参见 [图 9-11](#export_options_for_your_model)。
- en: Beyond these two methods of making models—Model Maker, and Cloud AutoML—which
    largely involve you avoiding writing code and having APIs handle the model training
    for you, there’s a third method that’s worth exploring where you’ll have to do
    some coding, but the ML model is mostly created by others, and you take advantage
    of their architecture using transfer learning.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这两种制作模型的方法——Model Maker 和 Cloud AutoML——这两种方法大多数情况下都避免了编写代码，并且有 API 处理模型训练，还有第三种方法值得探索，您将需要进行一些编码，但是
    ML 模型大部分由他人创建，您可以利用他们的架构使用迁移学习。
- en: '![](assets/aiml_0911.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0911.png)'
- en: Figure 9-11\. Export options for your model
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-11\. 模型的导出选项
- en: Creating a Model with TensorFlow and Transfer Learning
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 和迁移学习创建模型
- en: As discussed earlier, the concept of transfer learning can underpin rapid development
    of machine learning models. The concept is to use parts of a neural network that
    were trained on a similar problem, and then override them for your own scenario.
    For example, the EfficientNet model architecture is designed for the ImageNet
    scenario where there are 1,000 classes of image; it is trained on millions of
    images. The resources to train a model like this for yourself would be incredibly
    expensive in time and money. When a model is trained on such a large dataset,
    it can be a very efficient feature selector.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，迁移学习的概念可以支持快速开发机器学习模型。该概念是利用已在类似问题上训练过的神经网络的部分，然后覆盖它们以适应您自己的情景。例如，EfficientNet模型架构是为ImageNet情景设计的，其中有1,000类图像；它经过数百万张图像的训练。为了自己训练这样一个模型所需的资源将非常昂贵，不论是时间还是金钱。当模型在如此大的数据集上训练时，它可以是一个非常高效的特征选择器。
- en: What does that mean? Well, in short, a typical computer vision neural network
    uses what’s called a convolutional neural network (CNN) architecture. A CNN consists
    of many filters where once the filter is applied to an image it will transform
    it. Over time, a CNN will learn the filters that will help distinguish images
    from each other. For example, [Figure 9-12](#example_of_cnn_filters_activating_diffe)
    shows an example of images from a cats versus dogs classifier that illustrates
    the areas of the image the CNN used to tell the difference between these types
    of animal. It was clear, in this case, that filters were learned that determined
    what a dog’s eyes look like, and other filters were learned that determined what
    a cat’s eyes look like. Of all the filters in the network, the decision of what
    was in a picture could be as simple as the results of those filters showing something.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这是什么意思？简而言之，典型的计算机视觉神经网络使用所谓的卷积神经网络（CNN）架构。 CNN由许多滤波器组成，其中一旦滤波器应用于图像，它将对其进行转换。随着时间的推移，CNN将学习有助于区分图像的滤波器。例如，[图 9-12](#example_of_cnn_filters_activating_diffe)显示了猫狗分类器中的图像示例，说明了CNN用来区分这些动物类型的图像区域。在这种情况下很明显，学习到了确定狗眼睛外观的滤波器，以及学习到了确定猫眼睛外观的其他滤波器。在网络的所有滤波器中，决定图片内容可能仅仅是这些滤波器的结果显示的内容。
- en: '![](assets/aiml_0912.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0912.png)'
- en: Figure 9-12\. Example of CNN filters activating different areas in different
    images
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-12\. CNN滤波器在不同图像中激活不同区域的示例
- en: So, with an existing network, like EfficientNet, if the creators of the model
    expose the already-learned filters (usually called *feature vectors*), you can
    just use them with the logic being that if it has a set of filters that can be
    used to pick between 1,000 classes, the same set of filters will probably give
    you a decent classification for your datasets—in the case of flowers, the filters
    from EfficientNet can probably be used to pick between your five classes of flower.
    And thus, you don’t need to train a whole new neural network; just add what’s
    called a “classification head” to the existing one. This head can be as simple
    as a single layer with *n* neurons, where *n* is the number of classes you have
    in your data—in the case of flowers, this would be five.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用像EfficientNet这样的现有网络，如果模型的创建者公开了已学习的滤波器（通常称为*特征向量*），您可以直接使用它们。逻辑上，如果它具有一组用于选择1,000类之间的滤波器，那么相同的滤波器组可能会为您的数据集提供一个合理的分类——在花朵的情况下，EfficientNet的滤波器可能会被用于选择五类花之间的分类。因此，您不需要训练一个全新的神经网络；只需向现有的网络添加所谓的“分类头”。该头部可以是一个简单的单层，其中包含*n*个神经元，其中*n*是您数据中的类数——在花卉的情况下，这将是五个。
- en: 'Indeed, your entire neural network could look like this, and be defined with
    just three lines of code:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，您的整个神经网络可能看起来像这样，并且可以用仅三行代码定义：
- en: '[PRE21]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Of course to do this you’ll have to define the feature extractor and load it
    from an existing model like EfficientNet.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，您必须定义特征提取器并从现有模型（如EfficientNet）加载它。
- en: That’s where TensorFlow Hub is your friend. It’s a repository of models and
    parts of models, including feature extractors. You can find it at [*tfhub.dev*](https://tfhub.dev).
    Using the filters on the left side of the screen, you can access the different
    model types—for example, if you want image feature vectors, you can use them to
    get a set of [feature vectors](https://oreil.ly/yWULK).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是TensorFlow Hub派上用场的地方。它是一个模型和模型部件的存储库，包括特征提取器。您可以在[*tfhub.dev*](https://tfhub.dev)找到它。使用屏幕左侧的过滤器，您可以访问不同的模型类型——例如，如果您想要图像特征向量，您可以使用它们来获得一组[特征向量](https://oreil.ly/yWULK)。
- en: When you have a model, it will have a URL—for example, the EfficientNet model,
    optimized for mobile and trained on ImageNet, can be found at [*https://tfhub.dev/tensorflow/efficientnet/lite0/feature-vector/2*](https://tfhub.dev/tensorflow/efficientnet/lite0/feature-vector/2).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当你拥有一个模型时，它会有一个网址—例如，优化为移动设备并在ImageNet上训练的EfficientNet模型可以在[*https://tfhub.dev/tensorflow/efficientnet/lite0/feature-vector/2*](https://tfhub.dev/tensorflow/efficientnet/lite0/feature-vector/2)找到。
- en: 'You can use this URL with the TensorFlow Hub Python libraries to download the
    feature vector as a layer in your neural network:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用此URL与TensorFlow Hub Python库将特征向量下载为神经网络中的一层：
- en: '[PRE22]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: And that’s it—that’s all you need to create your own model architecture that
    uses the learned features of EfficientNet!
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样—这就是你需要创建自己的模型架构并利用EfficientNet学习特征的所有内容！
- en: 'With this approach, you can create models that use the foundations from state-of-the-art
    models to create your own! Exporting the model is as simple as converting it to
    TensorFlow Lite using the same techniques you saw in [Chapter 8](ch08.html#going_deeper_understanding_tensorflow_l):'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方法，您可以创建使用最先进模型基础的模型！导出模型就像使用相同技术将其转换为TensorFlow Lite一样简单，您在[第8章](ch08.html#going_deeper_understanding_tensorflow_l)中看到了这些技术：
- en: '[PRE23]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We’ll explore using these models in Chapters [10](ch10.html#using_custom_models_in_android)
    and [11](ch11.html#using_custom_models_in_ios).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第[10](ch10.html#using_custom_models_in_android)章和第[11](ch11.html#using_custom_models_in_ios)章探讨在Android和iOS中使用这些模型的情况。
- en: Transfer learning is a powerful technique, and what we cover here is merely
    a very light introduction. To learn more about it, check out books like *Hands-On
    Machine Learning with Scikit-Learn, Keras, and TensorFlow* by Aurelien Geron,
    or Andrew Ng’s excellent tutorials such as the [“Transfer Learning” video](https://oreil.ly/MDOEu).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是一种强大的技术，我们在这里涵盖的仅仅是一个非常轻量级的介绍。要了解更多信息，请查阅Aurelien Geron的*Hands-On Machine
    Learning with Scikit-Learn, Keras, and TensorFlow*等书籍，或者Andrew Ng的优秀教程，如[“Transfer
    Learning”视频](https://oreil.ly/MDOEu)。
- en: Creating Language Models
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建语言模型
- en: In this chapter, you saw how to create models in a variety of ways, and how
    to convert them to TensorFlow Lite so that they can be deployed to mobile apps,
    which you’ll see in the next chapter. An important nuance was that they were all
    image-based models, and with other model types there may be extra metadata that
    you’ll need to deploy alongside the TFLite model so that your mobile app can use
    the model effectively. We won’t go into detail on training natural language processing
    (NLP) models here—this is just a very high-level overview of the concepts that
    will impact creating them for mobile apps. For a more detailed walkthrough of
    creating and training a language model and of how NLP works, please check out
    my book *AI and Machine Learning for Coders*.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您看到了如何以多种方式创建模型，以及如何将它们转换为TensorFlow Lite，以便它们可以部署到移动应用程序中，您将在下一章中看到。一个重要的细节是它们都是基于图像的模型，而对于其他模型类型，可能需要额外的元数据来与TFLite模型一起部署，以便您的移动应用程序可以有效地使用它们。在这里我们不会详细介绍训练自然语言处理（NLP）模型的内容—这只是一个关于将它们创建为移动应用程序的概念的高级概述。要了解有关创建和训练语言模型以及NLP工作原理的更详细步骤，请查看我的书*AI
    and Machine Learning for Coders*。
- en: One such case is when you use language-based models. Building a classifier for
    text doesn’t work on the text itself, but on *encoded* text, where you’ll often
    create a dictionary of words, and build the classifier using them.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 一个这样的例子是当你使用基于语言的模型时。构建文本分类器不是在文本本身上工作，而是在*encoded*文本上工作，你通常会创建一个单词字典，并使用它们构建分类器。
- en: 'So, for example, say you have a sentence like “Today is a sunny day,” and you
    want to use it in a model. An efficient way to do this would be to replace the
    words with numbers, a process called t*okenizing*. Then, if you were to also encode
    the sentence “Today is a rainy day,” you could reuse some of the tokens, with
    a dictionary that might look like this:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，例如，假设你有一个句子像“今天是个晴天”，而你想要在一个模型中使用它。一个高效的方法是用数字替换单词，这个过程称为*tokenizing*。然后，如果你也要对编码为“今天是个雨天”的句子进行编码，你可以重复使用一些标记，带有这样的字典：
- en: '[PRE24]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This would make your sentences look like this:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使您的句子看起来像这样：
- en: '[PRE25]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: So when you train your model, you would train it with this data.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 所以当您训练您的模型时，您将用这些数据来训练它。
- en: However, later, when you want to do inference in your mobile app, your app will
    need the same dictionary, or it won’t be able to convert your user input to the
    sequences of numbers that the model is trained to understand (i.e., it will have
    no idea that “sunny” should use the token 5)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当你稍后想要在你的移动应用程序中进行推理时，你的应用程序将需要相同的字典，否则它将无法将用户输入转换为模型训练时理解的数字序列（即它不会知道“sunny”应该使用令牌5）。
- en: Additionally, when you train a model for language, in particular when you want
    to establish sentiment in language, the tokens for the words will be mapped to
    vectors, and the direction of those vectors will help determine sentiment.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，当你训练一个特定于语言的模型时，尤其是当你想要在语言中建立情感分析时，单词的标记将会映射到向量上，并且这些向量的方向将有助于确定情感。
- en: Note that this technique isn’t limited to sentiment. You can use these vectors
    to establish semantics, where words with similar meanings (such as cat and feline)
    can have similar vectors, but words with different ones (such as dog and canine)
    while similar to each other, will have a different “direction” than the cat/feline
    ones. But for the sake of simplicity, we’ll explore words that map to labeled
    sentiments.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这种技术并不限于情感分析。你可以使用这些向量来建立语义，其中具有类似含义的单词（如“cat”和“feline”）可以具有相似的向量，但具有不同含义的单词（如“dog”和“canine”）虽然相似，但与“cat”/“feline”的方向不同。但为了简单起见，我们将探索映射到有标签情感的单词。
- en: 'Consider these two sentences: “I am very happy today,” which you label as having
    positive sentiment, and “I am very sad today,” which has negative sentiment.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这两个句子：“我今天非常高兴”，你将其标记为积极情感，“我今天非常难过”，则为负面情感。
- en: The words “I,” “am,” “very,” and “today” are present in both sentences. The
    word “happy” is in the one labeled positive, and the word “sad” is in the one
    labeled negative. When a machine learning layer type called an “embedding” is
    used, all of your words will be translated into vectors. The initial direction
    of the vectors is determined by the sentiment; then over time, the directions
    of the vectors will be tweaked as new sentences are fed into the model. But just
    taking our very simple case where we only have these two sentences, these vectors
    might look like [Figure 9-13](#establishing_words_as_vectors).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: “I”、“am”、“very”和“today”这些单词在两个句子中都存在。单词“happy”出现在被标记为积极的句子中，“sad”出现在被标记为消极的句子中。当使用称为“嵌入”的机器学习层类型时，你所有的单词都将被转换为向量。向量的初始方向由情感决定；随着时间的推移，随着新的句子被输入模型，向量的方向将被微调。但在我们只有这两个句子的非常简单的情况下，这些向量可能看起来像[图9-13](#establishing_words_as_vectors)。
- en: '![](assets/aiml_0913.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0913.png)'
- en: Figure 9-13\. Establishing words as vectors
  id: totrans-163
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-13. 建立单词向量
- en: So, consider this space where the “direction” of the vectors determines the
    sentiment. A vector pointing to the right has positive sentiment. A vector pointing
    to the left has negative sentiment. Because the words “today,” “I,” “am,” and
    “very” are in both sentences, their sentiment cancels out, so they don’t point
    in either direction. Because “happy” is only present in the sentence labeled positive,
    it points in the positive direction, and similarly, “sad” points in the negative
    direction.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，请考虑这个空间，其中向量的“方向”决定了情感。向右指向的向量具有积极情感，向左指向的向量具有消极情感。因为“today”、“I”、“am”和“very”这些单词在两个句子中都出现，它们的情感相互抵消，所以它们不指向任何方向。因为“happy”仅在标记为积极的句子中出现，它指向积极方向；类似地，“sad”指向消极方向。
- en: When a model is trained on many labeled sentences, vectors such as these are
    learned by an embedding, and are then ultimately used to classify the sentence.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个模型被训练在许多有标签的句子上时，类似这些向量是通过嵌入进行学习的，最终被用来对句子进行分类。
- en: Earlier in this chapter when we explored transfer learning for images, we were
    able to use feature extractors that were already learned from other models, with
    the logic being that they were trained on millions of images, with many labels,
    and as such became very good at learning features that you could reuse.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章早些时候，当我们探讨图像的迁移学习时，我们能够使用已经从其他模型中学习到的特征提取器。这些模型经过了对数百万张图片的训练，并且有许多标签，因此它们非常擅长学习可以被重复使用的特征。
- en: The same happens with language models, where vectors of words could have been
    prelearned, and you would simply use them for your scenario. This saves a lot
    of time and complexity in training your model!
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的情况也发生在语言模型中，单词的向量可能已经被预先学习，你只需在你的场景中使用它们。这样可以节省训练模型时的大量时间和复杂性！
- en: In the next section, you’ll explore how to use Model Maker to create a language-based
    model that can then be used in Android or iOS apps!
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，您将探讨如何使用Model Maker创建一个基于语言的模型，然后可以在Android或iOS应用程序中使用！
- en: Create a Language Model with Model Maker
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Model Maker创建语言模型
- en: 'Model Maker makes it really simple to create language-based models with just
    a few lines of code. There’s a full notebook in the download for this chapter,
    and we’ll go through the highlights here. For this example, we’ll use a data file
    I created that has emotional sentiment from tweets. I’ve abbreviated the URL in
    this code listing to make it fit, but the full URL is [*https://storage.googleapis.com/laurencemoroney-blog.appspot.com/binary-emotion-withheaders.csv*](https://storage.googleapis.com/laurencemoroney-blog.appspot.com/binary-emotion-withheaders.csv):'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Model Maker使使用几行代码创建基于语言的模型变得非常简单。本章的下载中有一个完整的笔记本，我们将在这里介绍亮点。在本例中，我们将使用我创建的一个数据文件，其中包含来自推文的情感。我在此代码列表中缩写了URL以使其适合，但完整的URL是[*https://storage.googleapis.com/laurencemoroney-blog.appspot.com/binary-emotion-withheaders.csv*](https://storage.googleapis.com/laurencemoroney-blog.appspot.com/binary-emotion-withheaders.csv)：
- en: '[PRE26]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next up you’ll have Model Maker create the base model for you. It supports
    several model types, with more being added all the time, but the one we’ll use
    is the simplest—it uses transfer learning from a preexisting set of word vectors:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，Model Maker将为您创建基础模型。它支持几种模型类型，随着时间的推移还会添加更多，但我们将使用的是最简单的模型——它使用从现有单词向量集进行的迁移学习：
- en: '[PRE27]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The CSV file can be loaded into a training dataset using the `from_csv` method
    in the `TextClassifierDataLoader` (which is available in the Model Maker APIs),
    and you’ll need to specify which column in the CSV contains the text and which
    contains the label. If you inspect the CSV file, you’ll see there’s a column called
    “label,” which contains 0 for negative sentiment and 1 for positive. The tweet
    text is in the “tweet” column. You’ll also need to define the model spec, so that
    Model Maker can start mapping the words in these tweets to the embedding vectors
    that the model uses. In the previous step, you specified that the model spec uses
    the average word vectors template:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`TextClassifierDataLoader`中的`from_csv`方法（在Model Maker API中可用）将CSV文件加载到训练数据集中，并且您需要指定CSV中包含文本的列以及包含标签的列。如果您检查CSV文件，您会看到一个名为“label”的列，其中包含负面情绪为0和正面情绪为1的内容。推文文本位于“tweet”列中。您还需要定义模型规范，以便Model
    Maker可以开始将这些推文中的单词映射到模型使用的嵌入向量。在前面的步骤中，您指定了模型规范使用平均单词向量模板：
- en: '[PRE28]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now building the model is as simple as calling *text_classifier.create*, passing
    it the data, the model spec, and a number of epochs to train for:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，构建模型就像调用*text_classifier.create*一样简单，将数据、模型规范和训练的epoch数传递给它：
- en: '[PRE29]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Because your model doesn’t need to learn the embeddings for each word, and instead
    uses existing ones, training is very fast—in Colab with GPU, I experienced it
    at about 5 seconds per epoch. After 20 epochs, it will show about 75% accuracy.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 因为您的模型不需要为每个单词学习嵌入，而是使用现有的嵌入，所以训练非常快速——在Colab中使用GPU，我体验到大约每个epoch 5秒。经过20个epoch后，它将显示约75%的准确性。
- en: 'Once the model is done training, you can output the TFLite model simply with:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，您可以简单地输出TFLite模型：
- en: '[PRE30]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'For convenience for Android Studio users, this bundles the labels *and* the
    dictionary of words into the model file. You’ll explore how to use this model,
    including that metadata, in the next chapter. For iOS developers, there’s no add-in
    to Xcode to handle processing the built-in metadata, so you can export this separately,
    using:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便Android Studio用户，这将标签*和*单词字典捆绑到模型文件中。您将在下一章中了解如何使用此模型，包括元数据。对于iOS开发人员，没有添加到Xcode的插件来处理内置元数据，因此您可以单独导出它，使用：
- en: '[PRE31]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This will give you a file called *labels.txt* with the label specification,
    and another called *vocab* (no extension) containing the dictionary details.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成一个名为*labels.txt*的文件，其中包含标签规范，另一个名为*vocab*（无扩展名），其中包含字典的详细信息。
- en: 'If you want to inspect the model architecture used to created the model, you
    can see it by calling `model.summary()`:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想检查用于创建模型的模型架构，可以通过调用`model.summary()`来查看：
- en: '[PRE32]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The key thing to note is the embedding at the top, where the 256 denotes the
    length of the sentence that the model is designed for—it expects each sentence
    to be 256 words long. Thus, when passing strings to the model, you don’t only
    encode them into the tokens for the words, but also need to pad them out to 256
    tokens. So, if you want to use a 5 word sentence, you would have to create a list
    of 256 numbers, with the first 5 being the tokens for your 5 words, and the rest
    being 0.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的事情是顶部的嵌入，其中 256 表示模型设计用于处理的句子长度—它期望每个句子长达 256 个单词。因此，当传递字符串到模型时，您不仅仅需要将它们编码为单词的标记，还需要将它们填充到
    256 个标记。因此，如果您想要使用一个由 5 个单词组成的句子，您将不得不创建一个包含 256 个数字的列表，其中前 5 个是您的 5 个单词的标记，其余为
    0。
- en: The 16 is the number of dimensions for the sentiment of the words. Recall [Figure 9-13](#establishing_words_as_vectors)
    where we showed sentiment in two dimensions—in this case, in order to capture
    more nuanced meaning, the vectors will be in 16 dimensions!
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 16 是单词情感的维度数量。回顾[图 9-13](#establishing_words_as_vectors)，我们展示了两个维度的情感—在这种情况下，为了捕捉更加微妙的含义，向量将会是
    16 维的！
- en: Summary
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter you looked at several tools that can be used to create models,
    including TensorFlow Lite Model Maker, Cloud AutoML Edge, and TensorFlow with
    transfer learning. You also explored some of the nuances when using language-based
    models, such as needing to have an associated dictionary so your mobile clients
    can understand how words are encoded in your model.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您看到了几种用于创建模型的工具，包括 TensorFlow Lite Model Maker、Cloud AutoML Edge 和 TensorFlow
    与迁移学习。您还探讨了使用基于语言的模型时的一些微妙之处，例如需要一个关联的字典，以便您的移动客户端可以理解单词在模型中的编码方式。
- en: These hopefully gave you a taste for how models can be created. It’s not the
    primary focus of this book to teach you how to create different model types, and
    you can check my other book, *AI and Machine Learning for Coders*, to explore
    how to do that. In [Chapter 10](ch10.html#using_custom_models_in_android), you’ll
    take the models that you learned in this chapter and see how to implement them
    on Android, before going into using them in iOS in [Chapter 11](ch11.html#using_custom_models_in_ios).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这些内容让您对模型创建有所了解。本书的主要重点不是教授如何创建不同类型的模型，您可以查看我的另一本书《*AI 和机器学习与编程者*》来探索如何做到这一点。在[第十章](ch10.html#using_custom_models_in_android)，您将会使用本章学到的模型，并看到如何在
    Android 上实现它们，然后在[第十一章](ch11.html#using_custom_models_in_ios)中使用它们在 iOS 上。
