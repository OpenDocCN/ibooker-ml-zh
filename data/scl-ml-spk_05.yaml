- en: Chapter 5\. Feature Engineering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章 特征工程
- en: In machine learning, *feature engineering* is the process of using domain knowledge
    to select and transform the most relevant variables in the data to reach the goal
    of the machine learning process. *Domain knowledge* here refers to an understanding
    of the data and where it originated. In data science, it’s less about the tools
    and more about the data and problem itself, encompassing the general background
    in a specific field or vertical. For example, in finance it might involve familiarity
    with financial terms and the various possible applications of the data, such as
    loan scoring. Depending on the experience of the team working on the project,
    it might be necessary to consult a financial expert to create a representative
    feature to solve the problem at hand. Similarly, in healthcare tech you might
    work with a medical doctor to design the features, and knowledge of anatomy, biological
    systems, and medical conditions may be required.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，*特征工程*是使用领域知识来选择和转换数据中最相关变量的过程，以达到机器学习过程的目标。这里的*领域知识*指的是对数据及其来源的理解。在数据科学中，关注的不是工具本身，而是数据和问题本身，涵盖特定领域或垂直领域的一般背景。例如，在金融领域，可能涉及对金融术语的熟悉以及数据的各种可能应用，如贷款评分。根据项目团队的经验，可能需要咨询金融专家来创建一个代表性特征，以解决当前的问题。同样，在医疗技术领域，您可能需要与医生合作设计特征，需要了解解剖学、生物系统和医疗状况的知识。
- en: The goal of feature engineering is for the final features to serve as proxies
    to the information the data contains about the world or the specific context where
    the problem takes place. Domain experience is what enables you to make those links,
    often intuitively. Using domain knowledge can help you simplify the challenges
    inherent in machine learning, improving your chances of reaching your business
    goal. Indeed, to be successful, you must have a solid understanding of the problem
    and the data you are working with.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程的目标是使最终的特征充当数据对世界或问题所包含信息的代理。领域经验使您能够直观地建立这些联系。使用领域知识可以帮助简化机器学习中固有的挑战，提高实现业务目标的成功率。的确，要成功，您必须对问题及其相关数据有深入的理解。
- en: Here’s another example. Let’s assume you have a set of images of vehicles, and
    you are in charge of detecting the make, model, and year from a set of images.
    That can be a tough problem to solve! After all, a picture of a car can be taken
    from many angles, and you will need to differentiate and narrow down the options
    based on very specific features. Rather than asking your machine learning model
    to assess many individual features, instead, you can detect the vehicle’s registration
    plate and translate the pixels into numbers and letters. Once you have this information,
    you can match it with other datasets to extract the information you need. Since
    extracting numbers/letters from images is a solved machine learning problem, you
    have leveraged your domain knowledge together with feature engineering to reach
    your business goal.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这里再举一个例子。假设您有一组车辆的图像，并且负责从这些图像中检测出车辆的品牌、型号和年份。这可能是一个难以解决的问题！毕竟，一辆车的照片可以从多个角度拍摄，您需要根据非常具体的特征区分和缩小选项。与其要求您的机器学习模型评估许多单独的特征，不如直接检测车辆的注册牌照并将像素转化为数字和字母。一旦获得这些信息，您可以与其他数据集匹配以提取所需信息。由于从图像中提取数字/字母是一个解决了的机器学习问题，您已经利用领域知识与特征工程来达到您的业务目标。
- en: As this example suggests, your goal and your data will determine which features
    you will need for training your models. Interestingly enough, given that machine
    learning algorithms can be the same for different scenarios, like the classification
    of emails into spam or nonspam or the classification of Twitter accounts into
    bots or real users, *good quality features* might be the main driver for your
    model’s performance. By “good” here, I mean features that are going to provide
    a lot of information about the data, explain the most variance, and have a strong
    relationship with the target variable.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 正如这个例子所示，您的目标和数据将决定您需要用来训练模型的特征。有趣的是，鉴于机器学习算法在不同场景下可能相同，例如将电子邮件分类为垃圾邮件或非垃圾邮件，或将Twitter账号分类为机器人或真实用户，*高质量的特征*可能是模型性能的主要驱动因素。这里所说的“好”的特征是指那些能提供关于数据大量信息、解释最大差异并与目标变量有强关系的特征。
- en: Most machine learning algorithms are not intelligent enough to automatically
    extract meaningful features from raw data on their own. As a result, stacking
    multiple algorithms in a chain of executions, using *extractors* and *transformers*
    together with smart *selectors*, can help us identify noteworthy features. This
    process is also referred to as *featurization*, and it is a crucial part of processing
    the data in your machine learning pipeline.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习算法并不智能到可以自动从原始数据中提取有意义的特征。因此，通过在执行链中堆叠多个算法，同时使用*提取器*和*转换器*以及智能*选择器*，可以帮助我们识别显著的特征。这个过程也被称为*特征化*，是处理机器学习管道中数据的关键部分。
- en: 'In this chapter, we will explore some of the tools and techniques Spark provides
    for feature extraction. Before we proceed, here are a couple of terms you should
    know:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨Spark提供的一些特征提取工具和技术。在我们继续之前，这里有几个您应该了解的术语：
- en: Estimator
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 估计器
- en: An algorithm that can be fit on a DataFrame to produce a transformer
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在DataFrame上拟合以生成转换器的算法
- en: Hashing function
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希函数
- en: A function that is used to map data of arbitrary size to fixed-size values
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 用于将任意大小的数据映射到固定大小值的函数
- en: Derived feature
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 派生特征
- en: A feature obtained from feature engineering
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 通过特征工程获得的特征
- en: Raw feature
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 原始特征
- en: A feature obtained directly from the dataset with no extra data manipulation
    or engineering
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 直接从数据集中获取的特征，无需额外的数据操作或工程化
- en: 'At a high level, this chapter covers the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，本章涵盖以下内容：
- en: Features and their impact on machine learning models
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征及其对机器学习模型的影响
- en: MLlib featurization tools
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLlib特征化工具
- en: The image featurization process
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像特征化过程
- en: The text featurization process
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本特征化过程
- en: How to enrich your dataset
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何丰富您的数据集
- en: Let’s get started by taking a closer look at features and the effect they have
    on our models.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从更深入地了解特征及其对我们模型的影响开始。
- en: Features and Their Impact on Models
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征及其对模型的影响
- en: 'Since our models are mathematical and statistical representations of the space
    from which the data was (estimated to be) drawn/generated, they might be impacted
    by outliers or suffer from internal biases or other problems that we would prefer
    to avoid. Suppose your data contains missing values, but your algorithm doesn’t
    support these. You have to decide what to do: Do you fill in the missing values?
    Drop the columns with missing values altogether? Try using a different algorithm?
    The decisions you make can have a significant impact on your results with algorithms
    such as linear models, support vector machines (SVMs), neural networks, principal
    component analysis (PCA), and nearest neighbors, since each data point changes
    the outcome. Stated plainly: *if there are more missing values than real values,
    filling them with a default value can completely tilt the equation in favor of
    the default value.*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的模型是从数据空间中（估计为）抽取/生成的数学和统计表示，它们可能受到异常值的影响，或者受到内部偏差或其他我们宁愿避免的问题的困扰。假设您的数据包含缺失值，但您的算法不支持这些缺失值。您必须决定如何处理：是填补缺失值？还是完全删除具有缺失值的列？尝试使用其他算法？您做出的决定可能会对线性模型、支持向量机（SVM）、神经网络、主成分分析（PCA）和最近邻等算法的结果产生显著影响，因为每个数据点都会改变结果。简单来说：*如果缺失值比实际值多，则用默认值填充它们可能会完全倾斜方程以偏向默认值*。
- en: To better understand this, let’s take a look at an SVM algorithm, as shown in
    [Figure 5-1](#svm_with_linear_hard_margincomma_descri), where we need to develop
    a linear vector/function (H[1], H[2], H[3]) to distinguish between two categories,
    an empty circle or a full circle, given X[1] and X[2] as inputs. If we had missing
    values for most of the X[2] data points in the empty circle category and we filled
    them with a default value, that would completely change our linear vector’s direction
    and degree. So we have to think carefully about how we fill in missing data and
    how our features impact the models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 要更好地理解这一点，让我们看一下支持向量机（SVM）算法，如[图5-1](#svm_with_linear_hard_margincomma_descri)所示，我们需要开发一个线性向量/函数（H[1]，H[2]，H[3]），来区分两个类别，即空心圆或实心圆，给定X[1]和X[2]作为输入。如果我们在空心圆类别的大多数X[2]数据点上有缺失值，并且我们用默认值填充它们，那将完全改变我们线性向量的方向和程度。因此，我们必须仔细考虑如何填补缺失数据以及我们的特征如何影响模型。
- en: '![](assets/smls_0501.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/smls_0501.png)'
- en: Figure 5-1\. SVM with linear hard margin, describing (H1, H2, H3) as the hypotheses
    and dots as the actual data values
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-1。线性硬边界支持向量机（SVM），描述（H1，H2，H3）为假设，点为实际数据值
- en: Another important aspect of featurization is the ability to identify and reduce
    noise, especially when using automated machine learning models that have built-in
    feature extraction mechanisms. Noisy data in algorithms creates a risk of producing
    an incorrect pattern that the algorithm starts to generalize from, which in turn
    creates an undesired model outcome that doesn’t represent the business domain
    well. Understanding the data together with featurization is key here.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个特征提取的重要方面是能够识别和减少噪声，特别是在使用具有内置特征提取机制的自动化机器学习模型时。算法中的噪声数据会导致产生不正确的模式，算法开始从中进行泛化，从而产生不符合业务领域的不良模型结果。理解数据以及特征提取在这里至关重要。
- en: As shown in [Figure 5-2](#high_level_view_of_feature_engineering), feature engineering
    is a rich world of tools and techniques.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图5-2](#high_level_view_of_feature_engineering)所示，特征工程是一个丰富的工具和技术世界。
- en: '![](assets/smls_0502.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/smls_0502.png)'
- en: Figure 5-2\. High-level view of feature engineering requirements and techniques
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-2\. 特征工程需求和技术的高层视图
- en: 'The best ones to use depend on the type of data you are working with and your
    requirements. There are no hard rules, but here are some of the things you’ll
    need to keep in mind, depending on the problem you’re dealing with:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳选择取决于您正在处理的数据类型和需求。没有硬性规定，但根据您处理的问题，有一些需要牢记的事项：
- en: Handling missing data
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 处理缺失数据
- en: Do I use this data for a specific case analysis? What is the “price” of dropping
    the whole column/row versus filling it in with default data? What is the percentage
    of the missing data in my dataset? Is data missing in a whole row, or specific
    columns? Answer those questions and you’ve got yourself a plan!
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我是否将这些数据用于特定案例分析？整列/行删除与填充默认数据的“价格”是多少？我的数据集中缺失数据的百分比是多少？数据是整行缺失还是特定列？回答这些问题，您就有了一个计划！
- en: Extracting features from text
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从文本中提取特征
- en: Do I have only unstructured text data? How would I make sense of it? What is
    the nature of my data? How long is the text—a tweet with a limited number of characters,
    or a feature engineering chapter in a book? (More on this in [“The Text Featurization
    Process”](#the_text_featurization_process).)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我是否只有非结构化文本数据？我该如何理解它？我的数据的性质是什么？文本有多长——是一个限制字符数的推文，还是书中的特征工程章节？（有关详细信息，请参阅[“文本特征提取过程”](#the_text_featurization_process)。）
- en: Categorical encoding
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 分类编码
- en: 'This is the process of transforming categorical variables into a set of binary
    variables—yes or no, true or false, 0 or 1\. The goal here is to boost the performance
    of the model. An example of categorical data is the city where a person lives:
    San Francisco, New York, Berlin, Tel Aviv, etc. Those are nominal categories,
    as there isn’t a specific order to them. Categories can have an inherent order
    as well, as in the case of a student’s grades: A+, A, A–, B+, etc. We will look
    at classification in [Chapter 6](ch06.xhtml#training_models_with_spark_mllib);
    many algorithms take categorical features as input.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这是将分类变量转换为一组二进制变量的过程——是或否，真或假，0或1。这里的目标是提升模型的性能。分类数据的一个例子是一个人居住的城市：旧金山、纽约、柏林、特拉维夫等。这些是名义分类，因为它们之间没有具体的顺序。类别也可以有固有的顺序，例如学生的成绩：A+、A、A-、B+等。我们将在[第6章](ch06.xhtml#training_models_with_spark_mllib)中探讨分类；许多算法将分类特征作为输入。
- en: Feature scaling
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 特征缩放
- en: We use this technique, also referred to as *data normalization*, to standardize
    the independent features in the dataset so they are all on a similar scale. We
    attend to it during the preprocessing phase, or as part of feature engineering
    when new, highly varying values show up. Feature scaling is commonly used when
    working with gradient descent algorithms, as it helps speed up convergence. [Figure 5-3](#gradient_descent_moving_toward_the_mini)
    demonstrates the ideal trajectory of how *x* moves closer to the smallest possible
    value in each gradient descent algorithm iteration until it finds the minimum
    value or reaches the maximum number of allowed iterations—this is what it means
    for normalization to facilitate convergence. Without normalization, the difference
    in the ranges of the features will result in each one having a different step
    size, which means they are updated at different rates and it takes longer to reach
    overall optimization. Normalization smooths things out and speeds up the process.
    You can think of it like a person stuck in the woods and trying to find their
    way back to their car; that person will wander around, collect information, and
    try to optimize their route. But what if most of the trees were transparent and
    the person could see through them? This is the effect that feature scaling has.
    Feature scaling is also useful for distance-based algorithms and regression. To
    accomplish it, we can leverage PCA, an unsupervised technique that enables us
    to filter a noisy dataset and reduce the dimensionality of the data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用这种技术，也称为*数据归一化*，以标准化数据集中的独立特征，使它们都在一个相似的尺度上。我们在预处理阶段或在出现新的高度变化值时，作为特征工程的一部分处理它。特征缩放在使用梯度下降算法时很常见，因为它有助于加快收敛速度。[图
    5-3](#gradient_descent_moving_toward_the_mini)演示了*x*在每次梯度下降算法迭代中如何朝向最小可能值的理想轨迹，直到找到最小值或达到允许的最大迭代次数—这就是归一化促进收敛的含义。如果没有归一化，特征的范围差异会导致每个特征具有不同的步长，这意味着它们以不同的速度更新，需要更长时间才能达到总体优化。归一化使事情变得平稳并加快了过程。你可以把它想象成一个人困在树林中，试图找回自己的车；这个人会四处走动，收集信息，尝试优化路线。但如果大部分树木都是透明的，这个人能够透过它们看到外面？这就是特征缩放的效果。特征缩放也对基于距离的算法和回归很有用。为了实现它，我们可以利用
    PCA，这是一种无监督技术，使我们能够过滤嘈杂的数据集并减少数据的维度。
- en: '![](assets/smls_0503.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/smls_0503.png)'
- en: Figure 5-3\. Gradient descent moving toward the minimum of a function on each
    iteration, from x[1] to x[2] and so on
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-3\. 梯度下降在每次迭代中朝向函数的最小值移动，从 x[1] 到 x[2] 等等
- en: MLlib Featurization Tools
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLlib 特征化工具
- en: MLlib provides many featurization functions in its `pyspark.ml.feature` package
    (the Scala-based APIs are in `spark.ml.feature`). The full list of up-to-date
    APIs can be found in the [Apache Spark MLlib documentation](https://oreil.ly/8rrMh),
    and code examples can be found in the [Apache Spark GitHub repository](https://oreil.ly/SQxG9),
    under the *examples/src/main/python/ml/* directory.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在其`pyspark.ml.feature`包中，MLlib 提供了许多特征化函数（基于 Scala 的 API 在`spark.ml.feature`中）。最新的
    API 列表可以在[Apache Spark MLlib 文档](https://oreil.ly/8rrMh)中找到，并且代码示例可以在[Apache Spark
    GitHub 仓库](https://oreil.ly/SQxG9)的*examples/src/main/python/ml/*目录下找到。
- en: In this section, we will cover MLlib extractors and selectors.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将涵盖 MLlib 提取器和选择器。
- en: Extractors
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取器
- en: Extractors are MLlib APIs for drawing out features that are not necessarily
    meaningful on their own but that can help us with the exploration process. Remember
    [Figure 4-1](ch04.xhtml#interlocked_procedures_during_machine_l) from the previous
    chapter? Preprocessing, feature engineering, and statistical description are interlocking
    processes. Transformers (refer to [“Preprocessing with MLlib Transformers”](ch04.xhtml#preprocessing_with_mllib_transformers)
    if you need a refresher on these) can be used as extractors, and vice versa. Some
    extractors require us to use transformers first, like `TF-IDF`, which cannot operate
    directly on the raw data and requires preprocessing with the `Tokenizer` API for
    extracting words and `HashingTF` or `CountVectorizer` for converting the collections
    of words to vectors of token counts.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 提取器是 MLlib 的 API，用于提取那些单独来看可能没有意义但可以帮助我们进行探索的特征。还记得上一章节中的 [Figure 4-1](ch04.xhtml#interlocked_procedures_during_machine_l)
    吗？预处理、特征工程和统计描述是相互交织的过程。转换器（如果需要复习，请参考 [“使用 MLlib 转换器进行预处理”](ch04.xhtml#preprocessing_with_mllib_transformers)）可以用作提取器，反之亦然。有些提取器要求我们先使用转换器，例如
    `TF-IDF`，它无法直接在原始数据上操作，需要使用 `Tokenizer` API 提取单词，以及 `HashingTF` 或 `CountVectorizer`
    将单词集合转换为标记计数的向量。
- en: '[Table 5-1](#spark_mllib_extractor_apis) presents the available extractor APIs
    and when to use each of them.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[Table 5-1](#spark_mllib_extractor_apis) 展示了可用的提取器 API 及其使用情况。'
- en: Table 5-1\. Spark MLlib extractor APIs
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Table 5-1\. Spark MLlib 提取器 API
- en: '| API | Usage |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| API | 用法 |'
- en: '| --- | --- |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `TF-IDF` | Calculates the term frequency–inverse document frequency, where
    term frequency is the number of times a term appears in a document in the corpus,
    and the document frequency is the number of documents the term appears in. Used
    in text mining for weighing the importance of a term to a document. |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| `TF-IDF` | 计算词频-逆文档频率，其中词频是术语在语料库中出现在文档中的次数，文档频率是术语出现在的文档数。在文本挖掘中用于衡量术语对文档的重要性。
    |'
- en: '| `Word2Vec` | Converts sequences of words into a fixed-size vector. Used for
    natural language processing. |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| `Word2Vec` | 将单词序列转换为固定大小的向量。用于自然语言处理。 |'
- en: '| `Count​Vec⁠torizer` | Converts fixed-size sequences of words into vectors
    of token counts, to be used in text classification. When sequence size varies,
    it will use the minimal size as the vocabulary size. |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| `CountVectorizer` | 将固定大小的单词序列转换为标记计数的向量，用于文本分类。当序列大小不同时，将使用最小大小作为词汇表大小。
    |'
- en: '| `FeatureHasher` | Takes a set of categorical or numerical features and hashes
    them into one feature vector. Often used to reduce features without significantly
    losing their value. |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| `FeatureHasher` | 接受一组分类或数值特征并将它们哈希为一个特征向量。通常用于减少特征数目而不显著损失其价值。 |'
- en: Selectors
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择器
- en: After we use our transformers and extractors to develop a bunch of features,
    it’s time to select the ones we would like to keep. For this we use selectors,
    which are APIs for selecting a subset from a large set of features. This can be
    done manually or adaptively using algorithms that estimate feature importance
    and aim to improve the performance of our machine learning models; it’s important
    because too many features might lead to overfitting.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用转换器和提取器开发出一堆特征后，就到了选择想要保留的特征的时候了。为此，我们使用选择器，这些是从大量特征集中选择子集的 API。这可以手动完成，也可以使用估算特征重要性并旨在提高我们的机器学习模型性能的算法自适应地完成；这很重要，因为过多的特征可能导致过度拟合。
- en: Spark provides the simple options that are listed in [Table 5-2](#spark_mllib_selector_apis).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 提供了在 [Table 5-2](#spark_mllib_selector_apis) 中列出的简单选项。
- en: Table 5-2\. Spark MLlib selector APIs
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Table 5-2\. Spark MLlib 选择器 API
- en: '| API | Usage |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| API | 用法 |'
- en: '| --- | --- |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `VectorSlicer` | Takes a vector column of features and outputs a new column
    with a subset of the original features, determined by the indices we pass in.
    |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| `VectorSlicer` | 接受特征向量列并输出一个由我们传入的索引决定的原始特征子集的新列。 |'
- en: '| `RFormula` | Selects columns with an R model formula (for users of the R
    programming language) using a set of basic supported operations and produces a
    vector column of features and a double or string column of labels. |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| `RFormula` | 使用一组基本支持的操作选择具有 R 模型公式（供 R 编程语言用户使用）的列，并生成一个特征向量列和一个双精度或字符串标签列。
    |'
- en: '| `ChiSq​Selec⁠tor` | Takes a column with a vector of all the features and
    a label column and generates a DataFrame with a new column with a vector of selected
    features. Performs feature selection using a chi-squared statistical test based
    on one of five selection methods (top *x* most predictive features, top % most
    predictive features, features with a false positive rate below a threshold, etc.).
    |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| `ChiSq​Selec⁠tor` | 接受一个包含所有特征向量和一个标签列的列，并生成一个带有选定特征向量的新列的DataFrame。使用基于卡方统计检验的特征选择，根据五种选择方法之一进行（预测性能最高的前*x*个特征、预测性能最高的前%特征、误报率低于阈值的特征等）。
    |'
- en: '| `Univariate​Fea⁠ture​Selector` | Takes as input categorical/continuous labels
    with categorical/continuous vector features and generates a DataFrame with a new
    column with a vector of selected features. Spark chooses the score function to
    use for feature selection based on the specified label and feature types (chi-squared,
    ANOVA F-test, or F-value); the selection methods are the same as for `ChiSqSelector`.
    |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| `Univariate​Fea⁠ture​Selector` | 接受带有分类/连续标签和分类/连续向量特征的输入，并生成一个带有选定特征向量的新列的DataFrame。Spark根据指定的标签和特征类型选择用于特征选择的评分函数（卡方检验、ANOVA
    F检验或F值）；选择方法与`ChiSqSelector`相同。 |'
- en: '| `Variance​Thres​hold​Selector` | Takes a vector column of features and outputs
    a new column with all features below the provided variance threshold removed.
    |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| `Variance​Thres​hold​Selector` | 接受特征向量列，并输出一个新列，移除所有低于提供的方差阈值的特征。 |'
- en: 'Example: Word2Vec'
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例：Word2Vec
- en: Many of the selectors and later machine learning models take vectors of features
    in the shape of one column in the DataFrame. This is why using a columnar storage
    format such as Parquet is not always more efficient, as all the features are represented
    as one column anyway. However, it can be more efficient when given a large DataFrame
    with many columns, where each one represents a different transformation or set
    of features. To accommodate this usage, the Spark MLlib API generates a new DataFrame
    with the same columns as the previous one and a new column to represent the selected
    features.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 许多选择器和后续的机器学习模型将特征向量表示为DataFrame中的一个列。这也是为什么使用列存储格式如Parquet并不总是更高效的原因，因为所有特征都以一个列的形式表示。然而，当给定具有许多列的大型DataFrame时，其中每一列表示不同的转换或特征集时，它可能更有效。为了适应这种用法，Spark
    MLlib API生成一个与前一个DataFrame具有相同列的新DataFrame，并添加一个新列来表示选定的特征。
- en: 'The output of many transformers can be persisted to disk and reused later.
    The following code snippet shows how we can stack multiple transformers, persist
    them to disk, and load them from disk:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 许多转换器的输出可以持久化到磁盘并在以后重复使用。以下代码片段展示了如何堆叠多个转换器、将它们持久化到磁盘并从磁盘加载它们：
- en: '[PRE0]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In this code:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中：
- en: '`sentence_data_frame` is a mock DataFrame for demonstrating the functionality.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sentence_data_frame` 是一个模拟DataFrame，用于展示功能。'
- en: '`Tokenizer` creates a `Tokenizer` instance, providing input column and output
    column names.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Tokenizer` 创建一个`Tokenizer`实例，提供输入列和输出列名称。'
- en: '`transform` transforms the DataFrame and generates a new one named `token⁠ized`.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transform` 转换DataFrame并生成一个名为`token⁠ized`的新DataFrame。'
- en: '`Word2Vec` creates a `Word2Vec` instance with the specified input column, output
    column, and vector size. `Word2Vec` is an estimator, so we need to run `fit` on
    the data before it is used for transformation. More on that in [Chapter 6](ch06.xhtml#training_models_with_spark_mllib).'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Word2Vec` 创建一个具有指定输入列、输出列和向量大小的`Word2Vec`实例。`Word2Vec`是一个估算器，因此在使用其进行转换之前，我们需要对数据运行`fit`。更多详情见[第6章](ch06.xhtml#training_models_with_spark_mllib)。'
- en: '`.save` persists the `Word2Vec` model to disk using the `write` function.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.save` 使用`write`函数将`Word2Vec`模型持久化到磁盘。'
- en: '`Word2VecModel.load` loads the `Word2Vec` model from disk.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Word2VecModel.load` 从磁盘加载`Word2Vec`模型。'
- en: '`model_from_disk.transform` uses the loaded model to transform the tokenized
    data.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_from_disk.transform` 使用加载的模型转换标记化数据。'
- en: '`VarianceThresholdSelector` creates a `VarianceThresholdSelector` instance
    for selecting features, with a threshold of `0`, meaning it filters out the features
    with the same value in all samples. `VarianceThresholdSelector` expects a column
    named `features` as input. We call `fit` to fit the `VarianceThresholdSelector`
    on the data and use the model for selecting features.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VarianceThresholdSelector` 创建一个`VarianceThresholdSelector`实例，用于选择特征，阈值为`0`，表示过滤掉所有样本中具有相同值的特征。`VarianceThresholdSelector`期望输入名为`features`的列。我们调用`fit`来拟合数据上的`VarianceThresholdSelector`，并使用模型来选择特征。'
- en: '[Example 5-1](#the_dataframe_with_all_the_new_columns) shows what the resulting
    DataFrame will look like.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例5-1](#the_dataframe_with_all_the_new_columns)展示了最终DataFrame的外观。'
- en: Example 5-1\. The DataFrame with all the new columns
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-1\. 具有所有新列的DataFrame
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For simplicity, we didn’t normalize our feature vector to fit the range of `[0,1]`,
    although doing so might be helpful with the selection process later. This is another
    route you should explore during the feature engineering process. As a side note,
    while in this example we saved the model to disk, you can save it to any store
    that you have a connector to and that supports Parquet format (since the model
    itself is saved in Parquet format unless you specify otherwise). We will discuss
    this further in [Chapter 10](ch10.xhtml#deployment_patterns_for_machine_learnin).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 简单起见，我们没有将特征向量归一化到`[0,1]`范围内，尽管这样做可能有助于后续的选择过程。这是特征工程过程中你应该探索的另一条路。顺便说一句，虽然在这个示例中我们将模型保存到了磁盘上，但你可以将它保存到任何你有连接器并支持
    Parquet 格式的存储中（因为模型本身是以 Parquet 格式保存的，除非你另有指定）。我们将在[第10章](ch10.xhtml#deployment_patterns_for_machine_learnin)进一步讨论这个问题。
- en: The Image Featurization Process
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像特征化过程
- en: There is a misconception in the industry regarding feature engineering when
    working with images. Many assume there is no need for it, but in practice, even
    when we are using a neural network algorithm that extracts and maps features—like
    a convolutional neural network (CNN), as shown in [Figure 5-4](#typical_cnn_architecture_describing_con)—there
    is still a chance of introducing noise and computation overhead and missing features,
    since the algorithm itself doesn’t possess any business knowledge or domain understanding
    of the given problem.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理图像时，业界存在一个关于特征工程的误解。许多人认为这是没有必要的，但实际上，即使我们使用了像卷积神经网络（CNN）这样的神经网络算法来提取和映射特征，如[图5-4](#typical_cnn_architecture_describing_con)所示，仍然有可能引入噪声和计算开销，并且可能会漏掉一些特征，因为算法本身并不具备给定问题的业务知识或领域理解。
- en: '![](assets/smls_0504.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/smls_0504.png)'
- en: Figure 5-4\. Typical CNN architecture describing convolutions, subsampling,
    etc. all the way to the output
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-4\. 典型的CNN架构描述卷积、子采样等直到输出的过程
- en: 'There  are many features we can extract from image data, depending on the domain
    we’re working in. Our Caltech 256 image classification dataset is quite diverse,
    and we can try out multiple techniques until we find the best features. Also bear
    in mind that when it comes to images and layered neural network models, we can
    leverage an existing model and just add the last layer. I’ll talk more about that
    later, but I mention it here because that model might require specific image features,
    such as a specific width, height, and number of channels:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从图像数据中提取许多特征，这取决于我们所处的领域。我们的Caltech 256图像分类数据集非常多样化，我们可以尝试多种技术，直到找到最佳特征。还要记住，当涉及到图像和分层神经网络模型时，我们可以利用现有模型，只需添加最后一层。我稍后会详细讨论这个问题，但我在这里提到它是因为该模型可能需要特定的图像特征，比如特定的宽度、高度和通道数：
- en: Width and height
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 宽度和高度
- en: These represent the number of pixels in an image. For example, an image with
    the dimensions 180 × 200 is 180 pixels wide by 200 pixels high.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这些代表了图像中的像素数。例如，一个尺寸为180 × 200的图像宽度为180像素，高度为200像素。
- en: Channels
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 通道
- en: 'This is the number  of conventional primary color layers that make up an image.
    For example, an RGB (red, green, blue) image is actually composed of three images
    (matrices of pixels): one for the red channel, one for the green, and one for
    the blue. Grayscale images often have only one channel. GeoTIFF and other formats
    used to encode satellite images can have up to 13 layers (or channels), 3 of which
    are RGB, the human-visible layers.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这是组成图像的传统主要颜色层的数量。例如，RGB（红色，绿色，蓝色）图像实际上由三个图像（像素矩阵）组成：一个红色通道，一个绿色通道和一个蓝色通道。灰度图像通常只有一个通道。用于编码卫星图像的GeoTIFF和其他格式可以具有多达13个层（或通道），其中3个是RGB，即人眼可见的层。
- en: So for a full-color image in RGB format, we actually need three matrices (or
    channels) with values between 0 and 255, where a smaller number represents a dark
    color and a larger number indicates a light color (0 is black, 255 is white).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 所以对于RGB格式的全彩色图像，我们实际上需要三个矩阵（或通道），其值在0到255之间，较小的数字代表暗色，较大的数字表示亮色（0为黑色，255为白色）。
- en: Note
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: There are other formats for storing image data; however, RGB is the most popular
    one, so that’s what I’ll focus on here.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 存储图像数据的其他格式还有很多，但是RGB是最流行的格式，所以这里我将重点放在这里。
- en: How should we think about and define our features? Let’s get a basic feel for
    what we can do with image manipulation.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何思考和定义我们的特征？让我们先大致了解一下我们可以通过图像处理做些什么。
- en: Understanding Image Manipulation
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解图像处理
- en: How best to proceed depends on the image complexity—how many objects are in
    the image, the image background, the number of channels, whether color provides
    actionable value, etc. Based on these considerations, we have multiple options
    for manipulating the existing data and extracting features.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳处理方式取决于图像的复杂性——图像中有多少对象、图像背景、通道数、颜色是否提供可操作的价值等。基于这些考虑，我们有多种选项来处理现有数据并提取特征。
- en: As shown in [Figure 5-5](#rgb_image_with_three_matrices_and_grays), colored
    images have three layers of matrices that we can manipulate. We can add filtering,
    change the pixels, group them, or extract one channel only to get a grayscale
    image (as shown at the bottom of the figure).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 5-5](#rgb_image_with_three_matrices_and_grays)所示，彩色图像有三层矩阵可以进行操作。我们可以添加滤镜、改变像素、分组像素，或者仅提取一个通道以获得灰度图像（如图底部所示）。
- en: '![](assets/smls_0505.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/smls_0505.png)'
- en: Figure 5-5\. RGB image with three matrices (top) and grayscale image with one
    matrix (bottom)
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-5\. 带有三个矩阵的 RGB 图像（顶部）和一个矩阵的灰度图像（底部）
- en: Grayscale
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 灰度
- en: When there is no meaning to the colors in RGB images, converting them to grayscale
    can assist in eliminating noise in our data. Grayscale images are single-channel
    images that carry information about the intensity of light in each pixel, using
    integer values between 0 and 255\. These images are exclusively made up of shades
    of gray, encompassing the continuous range between white and black.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当 RGB 图像中的颜色没有意义时，将其转换为灰度图像可以帮助消除数据中的噪音。灰度图像是单通道图像，每个像素点的光强信息使用介于 0 到 255 之间的整数值来表示。这些图像完全由灰度色调组成，覆盖了从白色到黑色的连续范围。
- en: Defining image boundaries using image gradients
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用图像梯度定义图像边界
- en: Let’s take a look at another image from our repository, image 196_0070\. This
    image is classified as spaghetti, but in the color version on the left, we can
    also see some other things in the picture, like tomatoes, some bread, a jar, a
    glass, a bowl, and what look like mussels. Without defining features, like image
    boundaries, such an image could introduce a lot of noise to our algorithms, and
    we might end up with a model that incorrectly classifies images of tomatoes, mussels,
    and other items as spaghetti!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再看看仓库中的另一张图片，图像 196_0070\. 这张图片被归类为意大利面，但在左侧的彩色版本中，我们还可以看到图片中的一些其他物品，如番茄、面包、罐子、玻璃杯、碗以及看起来像是贻贝。如果不定义特征，比如图像边界，这样的图片可能会给我们的算法引入大量噪音，导致模型错误地将番茄、贻贝和其他物品的图片分类为意大利面！
- en: '![](assets/smls_0506.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/smls_0506.png)'
- en: Figure 5-6\. An image with multiple filters, classified simply as spaghetti
    in our dataset although it includes several other items
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-6\. 一张具有多个滤镜的图像，在我们的数据集中简单分类为意大利面，尽管它包含了几个其他物品
- en: 'Extracting image edges using gradient calculations can be done by leveraging
    vector operators such as Laplace, which highlights areas of rapid intensity change.
    Using Pillow, we can define our own convolution kernel for edge detection^([1](ch05.xhtml#ch01fn13))
    with the `Kernel` method, or we can use the built-in `FIND_EDGES` filter that
    the library provides us with. The following code sample illustrates both approaches:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 利用梯度计算提取图像边缘可以通过利用诸如拉普拉斯这样的矢量运算符来完成，它突出显示快速强度变化的区域。使用 Pillow，我们可以使用`Kernel`方法定义自己的卷积核进行边缘检测^([1](ch05.xhtml#ch01fn13))，或者使用库提供的内置`FIND_EDGES`滤镜。以下代码示例展示了这两种方法：
- en: '[PRE2]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Pillow provides us with many other filters out of the box, such as `BLUR`, `SMOOTH`,
    and `EDGE_ENHANCE`, all of which are based on adding a filter to the image based
    on pixel gradient manipulation. [Figure 5-6](#an_image_with_multiple_filterscomma_cla)
    captures how grayscale and edge filters are rendered.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Pillow 还为我们提供了许多其他预设滤镜，如`BLUR`、`SMOOTH`和`EDGE_ENHANCE`，所有这些都基于对图像进行像素梯度操作的滤镜添加。[图 5-6](#an_image_with_multiple_filterscomma_cla)
    展示了灰度和边缘滤镜的渲染效果。
- en: Extracting Features with Spark APIs
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Spark API 提取特征
- en: We can use multiple techniques at once on the Caltech 256 dataset to extract
    features from the data. In [Chapter 4](ch04.xhtml#data_ingestioncomma_preprocessingcomma),
    we touched a bit on UDFs and how to use them for extracting image size. Let’s
    start by diving deeper into this topic, as it is our main tool for extracting
    features from images.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以同时在Caltech 256数据集上使用多种技术来提取数据的特征。在[第四章](ch04.xhtml#data_ingestioncomma_preprocessingcomma)中，我们简要讨论了UDF及其如何用于提取图像大小。让我们深入探讨这个主题，因为这是我们从图像中提取特征的主要工具。
- en: Until 2017, PySpark supported UDFs that operated on one row at a time. Those
    functions missed out on the Spark query engine’s optimization capabilities, and
    because behind the scenes PySpark is translated into Scala code, many UDFs written
    in PySpark had high serialization and invocation overhead. To overcome these problems,
    data engineers and data scientists worked together to define UDFs in Java and
    Scala that could be invoked from Python. This made the code messy, however, and
    hard to navigate and maintain. Fortunately, Spark 2.3 introduced pandas UDFs,
    which allow vectorized operations and leverage Apache Arrow optimization for reducing
    serialization and deserialization operations. This addition enabled data scientists
    not only to scale their workloads but also to make use of pandas APIs within Spark.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 直到2017年，PySpark支持的UDF一次处理一行。这些函数错过了Spark查询引擎的优化能力，因为在幕后PySpark被转换成Scala代码，许多在PySpark中编写的UDF具有高的序列化和调用开销。为了解决这些问题，数据工程师和数据科学家们合作，在Java和Scala中定义了可以从Python调用的UDF。然而，这使得代码变得混乱，难以导航和维护。幸运的是，Spark
    2.3引入了pandas UDFs，允许向量化操作并利用Apache Arrow优化来减少序列化和反序列化操作。这个新增功能不仅使数据科学家能够扩展他们的工作负载，还可以在Spark中使用pandas
    API。
- en: 'pyspark.sql.functions: pandas_udf and Python type hints'
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: pyspark.sql.functions：pandas_udf 和 Python 类型提示
- en: As mentioned in [Chapter 4](ch04.xhtml#data_ingestioncomma_preprocessingcomma),
    pandas UDFs are defined using the `pandas_udf` decorator, which is part of the
    `pyspark.sql.functions` library. Prior to Apache Spark 3.0, which leverages Python
    type hints, you needed to specify the type of pandas UDF you were creating (depending
    on the type of transformation) with `PandasUDFType`, as shown in [Table 5-3](#breakdown_of_spark_pandas_udf_types).
    Each type expects a certain kind of input and produces a certain kind of output.
    Use of Python type hints is preferred, as it gives a clear indication of what
    the function is supposed to do and makes it easier to perform static analysis;
    specifying the type with `PandasUDFType` will likely be deprecated in future releases.
    Certain types of transformations can now be performed with the dedicated functions
    `applyInPandas` and `mapInPandas`, discussed in the next section.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第四章](ch04.xhtml#data_ingestioncomma_preprocessingcomma)所述，pandas UDFs是使用`pandas_udf`装饰器定义的，它是`pyspark.sql.functions`库的一部分。在Apache
    Spark 3.0之前，使用Python类型提示来指定创建的pandas UDF的类型（根据转换类型）是必要的，如[表格5-3](#breakdown_of_spark_pandas_udf_types)所示。每种类型都期望特定类型的输入，并产生特定类型的输出。推荐使用Python类型提示，因为它清楚地指示了函数的预期功能，并且使得静态分析更加容易；在未来的版本中，可能会弃用使用`PandasUDFType`来指定类型的做法。现在可以使用专用函数`applyInPandas`和`mapInPandas`执行某些类型的转换，这些函数将在下一节讨论。
- en: Table 5-3\. Breakdown of Spark pandas UDF types
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 表格5-3。Spark pandas UDF 类型详细信息
- en: '| Spark PandasUDFType | Inputs and outputs | Python type hints |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Spark PandasUDFType | 输入和输出 | Python 类型提示 |'
- en: '| --- | --- | --- |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `SCALAR` (default) | A pandas Series | Doesn’t need a specified scalar pandas
    data type; can take `long`, `double`, `float`, `int`, or `boolean` |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| `SCALAR`（默认） | pandas Series | 不需要指定特定的标量pandas数据类型；可以使用`long`、`double`、`float`、`int`或`boolean`
    |'
- en: '| `SCALAR_ITER` | An iterator of pandas Series | Requires pandas data type
    to be specified in the function signature to enable the compiler and runtime engine
    to create the correct iterator |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| `SCALAR_ITER` | pandas Series 的迭代器 | 函数签名中需要指定 pandas 数据类型，以便编译器和运行时引擎创建正确的迭代器
    |'
- en: '| `GROUPED_MAP` | A pandas DataFrame | Doesn’t need a specific pandas data
    type; you can use the `mapInPandas` or `applyInPandas` function instead on top
    of a Spark DataFrame |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| `GROUPED_MAP` | pandas DataFrame | 不需要特定的pandas数据类型；可以在Spark DataFrame上使用`mapInPandas`或`applyInPandas`函数
    |'
- en: '| `GROUPED_AGG` | A pandas DataFrame | Similar to `GROUPED_MAP`, doesn’t need
    a specific pandas data type; you can use `applyInPandas` instead |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| `GROUPED_AGG` | pandas DataFrame | 类似于`GROUPED_MAP`，不需要特定的pandas数据类型；可以使用`applyInPandas`替代
    |'
- en: In [Chapter 4](ch04.xhtml#data_ingestioncomma_preprocessingcomma), we used a
    pandas UDF to calculate the size of each image in our dataset. Now let’s calculate
    the average size for each image category so we can decide if we want to resize
    the images accordingly.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第四章](ch04.xhtml#data_ingestioncomma_preprocessingcomma)中，我们使用了一个 pandas UDF
    来计算数据集中每个图像的大小。现在让我们计算每个图像类别的平均大小，以便决定是否要相应地调整图像大小。
- en: 'First, we’ll flatten the `size` struct into two columns:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将把 `size` 结构展平为两列：
- en: '[PRE3]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: An excerpt from the resulting flattened DataFrame is shown in [Example 5-2](#flattened_width_and_height_columns).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 展示了生成的展平 DataFrame 的一部分摘录，如[示例 5-2](#flattened_width_and_height_columns)所示。
- en: Example 5-2\. Flattened width and height columns
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-2\. 展平的宽度和高度列
- en: '[PRE4]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we’ll extract the mean width and height for each column, leveraging a
    pandas UDF with Python type hints:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将提取每列的平均宽度和高度，利用带有 Python 类型提示的 pandas UDF：
- en: '[PRE5]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `@pandas_udf("int")` at the beginning and the `-> (int)` in the Python function
    give PySpark a hint about the data type we are using. This example shows how to
    calculate the mean width for a column, but we can use the same function for height
    as well.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 函数中，`@pandas_udf("int")` 开头和 `-> (int)` 给 PySpark 提供了关于我们正在使用的数据类型的提示。这个示例展示了如何计算一列的平均宽度，但我们也可以用同样的函数来计算高度。
- en: '[Example 5-3](#example_mean_width_and_height_values) shows some example output.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 5-3](#example_mean_width_and_height_values) 展示了一些示例输出。'
- en: Example 5-3\. Example mean width and height values
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-3\. 示例平均宽度和高度值
- en: '[PRE6]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Our final step is to decide whether or not we want to resize the images, based
    on the results. In our case, we can skip this step because we are going to leverage
    PyTorch and TensorFlow machine learning algorithms and resize them according to
    the algorithms’ requirements.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最后一步是根据结果决定是否要调整图像大小。在我们的情况下，我们可以跳过此步骤，因为我们将利用 PyTorch 和 TensorFlow 的机器学习算法，并根据算法的要求调整图像大小。
- en: 'pyspark.sql.GroupedData: applyInPandas and mapInPandas'
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'pyspark.sql.GroupedData: applyInPandas 和 mapInPandas'
- en: '`applyInPandas` and `mapInPandas` are functions that operate over a grouped
    Spark DataFrame and return a new DataFrame with the results. When we define a
    `pandas_udf` to work with them, it receives a pandas DataFrame as input and returns
    a pandas DataFrame as an output.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`applyInPandas` 和 `mapInPandas` 是操作分组 Spark DataFrame 并返回带有结果的新 DataFrame 的函数。当我们定义一个
    `pandas_udf` 来处理它们时，它接收一个 pandas DataFrame 作为输入，并返回一个 pandas DataFrame 作为输出。'
- en: Although they have the same name, pandas DataFrames and Spark DataFrames are
    not the same thing, and it’s important not to confuse the two. [Table 5-4](#key_features_of_spark_and_pandas_datafr)
    highlights the main differences between the two.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它们有相同的名称，但 pandas DataFrame 和 Spark DataFrame 不是同一回事，不要混淆它们是非常重要的。[表 5-4](#key_features_of_spark_and_pandas_datafr)
    强调了两者之间的主要区别。
- en: Table 5-4\. Key features of Spark and pandas DataFrames
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5-4\. Spark 和 pandas DataFrame 的主要特性
- en: '|   | Spark DataFrame | pandas DataFrame |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|   | Spark DataFrame | pandas DataFrame |'
- en: '| --- | --- | --- |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Supports parallel execution** | Yes | No |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| **支持并行执行** | 是 | 否 |'
- en: '| **Lazy operation** | Yes | No |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| **惰性操作** | 是 | 否 |'
- en: '| **Immutable** | Yes | No |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| **不可变** | 是 | 否 |'
- en: 'As an example, let’s take a look at how we can extract grayscale versions of
    our images using the `applyInPandas` function. Here are the steps we’ll follow:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，让我们看看如何使用 `applyInPandas` 函数提取我们图像的灰度版本。以下是我们将要遵循的步骤：
- en: 'Define the function `add_grayscale_img`:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义函数 `add_grayscale_img`：
- en: '[PRE7]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Within this function, we call the `convert` function from the PIL library to
    extract a grayscale version of the image. `get_image_bytes` is a supporting function
    that helps us get the right object class to use with `.convert('L')`.
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个函数内部，我们调用 PIL 库的 `convert` 函数来提取图像的灰度版本。`get_image_bytes` 是一个辅助函数，帮助我们获取与
    `.convert('L')` 一起使用的正确对象类。
- en: Test the function. Image processing is often an expensive procedure, since it
    requires the Spark application to iterate over all the data and process the images.
    Therefore, it’s best practice to test the function on a subset of the image data,
    so you can tweak and tune it and make sure it delivers on its expectations before
    running it on the whole dataset.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试这个函数。图像处理通常是一个昂贵的过程，因为它需要 Spark 应用程序迭代所有数据并处理图像。因此，在图像数据的子集上测试函数是最佳实践，这样你可以调整和优化它，并确保在整个数据集上达到期望效果之前进行测试。
- en: 'Specify the schema for the output DataFrame. We can do this easily by leveraging
    the existing DataFrame and adding dedicated blank (`None`) columns:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过利用现有的 DataFrame 并添加专用的空（`None`）列，指定输出 DataFrame 的模式是非常容易的：
- en: '[PRE8]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We can then extract the schema later by calling `DataFrame.schema`. This process
    makes schema definition in the function invocation easy and reduces the likelihood
    of schema mismatches.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后我们可以通过调用 `DataFrame.schema` 来提取模式。这个过程使得在函数调用中定义模式变得简单，并减少了模式不匹配的可能性。
- en: 'Reduce the DataFrame columns to the minimum required, since (as described in
    the following sidebar) `groupBy` and `applyInPandas` are relatively expensive
    operations:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 减少 DataFrame 的列到最小必需的数量，因为（如下面的侧边栏所述）`groupBy` 和 `applyInPandas` 是比较昂贵的操作：
- en: '[PRE9]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Run the Python function on the Spark executors. Start by calling `groupby`
    to group the data (you can use any function to do this, but it’s a precondition
    to run the next function). We can use any column to group the data and call the
    `applyInPandas` function with a pointer to the function to execute and the schema.
    In our example, `add_grayscale_img` is the function we want to execute, and `rtn_schema.schema`
    is our schema:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Spark 执行器上运行 Python 函数。首先调用 `groupby` 来对数据进行分组（您可以使用任何函数来执行此操作，但这是运行下一个函数的前提）。我们可以使用任何列来对数据进行分组，并使用指向要执行的函数和模式的指针来调用
    `applyInPandas` 函数。在我们的示例中，`add_grayscale_img` 是我们要执行的函数，`rtn_schema.schema` 是我们的模式：
- en: '[PRE10]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Finally, rejoin the data from the original DataFrame with the augmented DataFrame,
    leveraging `leftouter` in case the image transform needs to skip some rows:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，利用 `leftouter` 将原始 DataFrame 中的数据与增强的 DataFrame 重新连接，以防止图像转换需要跳过某些行：
- en: '[PRE11]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Those six steps provided us with two new features: one of type `Array[Byte]`
    named `grayscale_image` and one of type `String` named `grayscale_format`.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这六个步骤为我们提供了两个新特征：一个类型为 `Array[Byte]` 的名为 `grayscale_image` 的特征，以及一个类型为 `String`
    的名为 `grayscale_format` 的特征。
- en: '[Figure 5-9](#output_example_with_the_new_columns) shows the `output_df` table
    structure and some sample rows of data.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-9](#output_example_with_the_new_columns) 显示了 `output_df` 表的结构和一些数据样本行。'
- en: '![](assets/smls_0509.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/smls_0509.png)'
- en: Figure 5-9\. Output example with the new columns
  id: totrans-161
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-9\. 带有新列的输出示例
- en: The Text Featurization Process
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本特征化过程
- en: 'In [“Example: Word2Vec”](#example_wordtwovec) you learned how to use the `Tokenizer`,
    `Word2Vec`, and other tools. In this section, we will use the [Bot or Not dataset](https://oreil.ly/v_Rba)
    to learn about the featurization process for short free-text strings such as Twitter
    user descriptions (bios).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [“示例：Word2Vec”](#example_wordtwovec) 中，您学习了如何使用 `Tokenizer`、`Word2Vec` 和其他工具。在本节中，我们将使用
    [Bot or Not 数据集](https://oreil.ly/v_Rba) 来了解如何对短自由文本字符串（例如 Twitter 用户描述（简介））进行特征化处理。
- en: Since our data is supervised (i.e., labeled), we can explore combinations of
    existing features with labels to develop new ones. Of course, this might result
    in the features being highly correlated, which means we need to think outside
    of the box. It’s true that interpretable features (features that we have combined
    and where we fundamentally understand the relationships between them) and models
    are easier to debug than complex ones. However, interpretability does not always
    lead to the most accurate model.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的数据是有监督的（即有标签的），我们可以探索现有特征与标签的组合以开发新的特征。当然，这可能导致特征高度相关，这意味着我们需要超越传统思维。诚然，可解释的特征（我们已经组合的特征，我们基本上理解它们之间的关系）和模型比复杂的模型更容易调试。然而，可解释性并不总是能够导致最准确的模型。
- en: Our dataset has a `description` column of plain text and a `label` column. [Example 5-4](#twitter_bot_or_not_description_and_labe)
    gives some examples of what these look like.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集有一个 `description` 列和一个 `label` 列。[示例 5-4](#twitter_bot_or_not_description_and_labe)
    展示了这些列的一些例子。
- en: Example 5-4\. Twitter Bot or Not `description` and `label` columns
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-4\. Twitter Bot 或 Not `description` 和 `label` 列
- en: '[PRE12]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Let’s take a look at some of the options we have for extracting features out
    of this data.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们从这些数据中提取特征的一些选项。
- en: Bag-of-Words
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词袋模型
- en: Bag-of-words is one of the “bag-of-*x*” methods for turning text into flat vectors.
    In this approach, a text—in our case, a Twitter bio—is represented as a “bag”
    (multiset) of its component words, disregarding grammar and even word order. Often,
    the final representation will be the terms and their counts. Hence, as part of
    building a bag-of-words, we also extract *term frequency vectors* containing the
    words and their frequency (number of occurrences) in the given text. MLlib provides
    us with functionality to do just that, such as the `Tokenizer` (for splitting
    the text) together with the `HashingTF` transformer or `CountVectorizer` selector.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋法是将文本转换为平面向量的“bag-of-*x*”方法之一。在这种方法中，文本（在我们的情况下是 Twitter 个人简介）被表示为其组成单词的“袋子”（多重集合），忽略语法甚至单词顺序。通常，最终的表示将是术语及其计数。因此，在构建词袋法的过程中，我们还提取包含给定文本中单词及其频率（出现次数）的
    *术语频率向量*。MLlib 提供了这样做的功能，例如 `Tokenizer`（用于拆分文本）与 `HashingTF` 转换器或 `CountVectorizer`
    选择器。
- en: 'As an example, let’s assume we have the following Twitter account description:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，假设我们有以下 Twitter 帐户描述：
- en: Tweets on programming best practices, open-source, distributed systems, data
    & machine learning, dataops, Apache Spark, MLlib, PyTorch & TensorFlow.
  id: totrans-172
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Tweets on programming best practices, open-source, distributed systems, data
    & machine learning, dataops, Apache Spark, MLlib, PyTorch & TensorFlow.
- en: 'In this case, there is no real need to generate the term frequency vectors,
    as each word has a frequency of 1\. However, if we group the descriptions of all
    accounts that are labeled as bots, we’ll almost certainly find many terms that
    appear more than once, which might allow some interesting insights. This brings
    us to a second option: TF-IDF.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，没有真正的必要生成词频向量，因为每个词的频率都是 1。然而，如果我们将所有标记为机器人的帐户的描述分组，几乎肯定会发现许多术语出现超过一次，这可能会带来一些有趣的见解。这将我们带到第二个选项：TF-IDF。
- en: TF-IDF
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TF-IDF
- en: TF-IDF is a method from the field of information retrieval that is highly used
    in text mining. It allows us to quantify the importance of a term in a given document
    or corpus. As a reminder, *term frequency* (`TF(t,d)`) is the number of times
    that term `t` appears in document `d`, and *document frequency* (`DF(t,D)`) is
    the number of documents in the corpus `D` that contain term `t`.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF 是信息检索领域中广泛使用的一种方法，用于文本挖掘。它允许我们量化术语在给定文档或语料库中的重要性。作为提醒，*术语频率* (`TF(t,d)`)
    是术语 `t` 在文档 `d` 中出现的次数，而 *文档频率* (`DF(t,D)`) 是语料库 `D` 中包含术语 `t` 的文档数。
- en: IDF is the *inverse* document frequency, which provides us with a numerical
    measure of how much information a specific term provides based on how rare or
    frequent it is in the corpus; the more frequent the usage, the lower the score.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: IDF 是 *逆* 文档频率，它为我们提供了一个数值度量，根据术语在语料库中的稀有或频繁程度，来衡量术语提供的信息量；使用越频繁，得分越低。
- en: 'TF-IDF multiplies those outputs:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF 将这些输出相乘：
- en: '[PRE13]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The TF and IDF functionality is separated in MLlib, which provides us with the
    flexibility of deciding how to work with them and whether to use them together
    or separately.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib 中的 TF 和 IDF 功能是分开的，这为我们提供了灵活性，可以决定如何处理它们，以及是否要一起使用它们或分开使用。
- en: 'Let’s calculate the TF for our dataset, using the `Tokenizer` API to extract
    words and the `HashingTF` function to map the terms to their frequencies:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `Tokenizer` API 提取单词并使用 `HashingTF` 函数将术语映射到它们的频率来计算我们数据集的 TF：
- en: '[PRE14]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`HashingTF` creates a new column named `frequencyFeatures` that contains a
    sparse vector of hashed words and the documents they appear in, as shown in [Example 5-5](#hashed_term_frequency).'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`HashingTF` 创建一个名为 `frequencyFeatures` 的新列，其中包含一组哈希词的稀疏向量及其出现在文档中的情况，如 [示例
    5-5](#hashed_term_frequency) 所示。'
- en: Example 5-5\. Hashed term frequency
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-5\. 哈希术语频率
- en: '[PRE15]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Our second step is to calculate the inverse document frequency. Since Spark’s
    `IDF` is an estimator, we need to build it first using the `fit` method:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二步是计算逆文档频率。由于 Spark 的 `IDF` 是一个估计器，我们首先需要使用 `fit` 方法来构建它：
- en: '[PRE16]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This creates an `IDFModel` object instance named `idfModel`. Now, we can transform
    the data using the model:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这创建了一个名为 `idfModel` 的 `IDFModel` 对象实例。现在，我们可以使用该模型来转换数据：
- en: '[PRE17]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The `rescaledData` DataFrame instance has a column named `features` with the
    calculated importance of a term in a description given the whole dataset. Although
    this information is nice to have, TF-IDF is an unsupervised method that completely
    neglects the labels in the data. Next, we’ll look at a supervised approach.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`rescaledData` DataFrame 实例具有名为`features`的列，该列显示了在整个数据集中给定描述中术语的重要性。尽管这些信息很有用，但
    TF-IDF 是一种无监督方法，完全忽视了数据中的标签。接下来，我们将看看一种监督方法。'
- en: N-Gram
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: N-Gram
- en: 'MLlib’s `NGram` transformer allows us to take an array of strings (words) and
    convert it into an array of *n*-grams, where *n* is a parameter that we can define
    in the `NGram` function:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib 的 `NGram` 转换器允许我们将字符串数组（单词）转换为 *n*-grams 数组，其中 *n* 是我们可以在 `NGram` 函数中定义的参数：
- en: '[PRE18]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`NGram` can produce a nice feature if we know what to do with it later and
    the output provides value. For example, the output of:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们知道如何处理后续的`NGram`输出并且该输出具有价值，`NGram`可以生成一个不错的特征。例如：
- en: '[PRE19]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'given `n=2` provides us with the following array of strings:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 给定 `n=2`，我们得到以下字符串数组：
- en: '[PRE20]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As you can see from the output, `NGram` works as a sliding window, repeating
    words. It’s useful when we are building tools for automatically completing sentences,
    checking spelling or grammar, extracting topics, etc.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中可以看出，`NGram`作为一个滑动窗口来重复单词。在构建自动完成句子、检查拼写或语法、提取主题等工具时非常有用。
- en: Additional Techniques
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 额外技术
- en: 'There are many other techniques that you can leverage to extract features from
    text. For example, *topic extraction aims to identify* the topic of a given document
    by scanning for known impactful terms or a combination of terms. You can also
    leverage the frequent pattern mining functionality in MLlib, provided by its implementations
    of the FP-growth and PrefixSpan algorithms. All those methods, at their core,
    are based on the feature extraction methods discussed previously: bag-of-words,
    identifying frequency of terms or patterns in a corpus versus in a given document,
    N-gram, etc. To learn more, check out the following reputable resources from O’Reilly:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他技术可以利用从文本中提取特征。例如，*主题提取旨在识别*给定文档的主题，通过扫描已知的有影响力的术语或术语组合来实现。您还可以利用 MLlib
    中提供的 FP-growth 和 PrefixSpan 算法的频繁模式挖掘功能。所有这些方法，在其核心上都是基于先前讨论的特征提取方法：词袋模型、识别语料库中术语或模式在给定文档中的频率、N-gram
    等。要了解更多信息，请查阅 O’Reilly 的以下可靠资源：
- en: '[*Applied Text Analysis with Python*](https://oreil.ly/ata-py) by Benjamin
    Bengfort, Rebecca Bilbro, and Tony Ojeda'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*使用 Python 进行应用文本分析*](https://oreil.ly/ata-py) 由 Benjamin Bengfort、Rebecca
    Bilbro 和 Tony Ojeda 撰写。'
- en: '[*Natural Language Processing with Spark NLP*](https://oreil.ly/nlp-spark)
    by Alex Thomas'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*使用 Spark NLP 进行自然语言处理*](https://oreil.ly/nlp-spark) 由 Alex Thomas 撰写。'
- en: '[*Practical Natural Language Processing*](https://oreil.ly/prac-nlp) by Sowmya
    Vajjala et al.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*实用自然语言处理*](https://oreil.ly/prac-nlp) 由 Sowmya Vajjala 等人撰写。'
- en: Enriching the Dataset
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 丰富数据集
- en: Often, our datasets will require more data points and new features. Finding
    the right features is the beating heart of the machine learning workflow, and
    it is often more art than science. We have a few options for enriching our datasets.
    With the Bot or Not bot detection dataset, for example, we can leverage the Twitter
    API to extract new tweets and updates if we need more data. We can also make use
    of *transfer learning*, which is the process of leveraging knowledge gained while
    solving one problem to solve a different problem.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们的数据集需要更多的数据点和新特征。找到正确的特征是机器学习工作流程的核心，通常更多的是艺术而不是科学。我们有几种方法可以丰富我们的数据集。例如，使用
    Bot or Not 机器人检测数据集时，如果需要更多数据，我们可以利用 Twitter API 提取新的推文和更新。我们还可以利用*迁移学习*，这是利用解决一个问题时获得的知识来解决另一个问题的过程。
- en: How can we leverage transfer learning for our Twitter bot detection problem?
    One option would be to pull in data from other social media with similar account
    names, such as LinkedIn. Learning if an account is real on LinkedIn can provide
    us with another data point to leverage.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何利用迁移学习解决我们的 Twitter 机器人检测问题？一个选项是从其他社交媒体中获取类似帐户名称的数据，例如 LinkedIn。了解 LinkedIn
    上的帐户是否真实可以为我们提供另一个数据点来利用。
- en: Summary
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Spark and Spark MLlib implement many additional feature extraction functions
    that you might want to look into when building your models. There are numerous
    features that can improve model accuracy; for example, with images, you might
    want to try out a feature descriptor that only preserves the image outline and
    very unique characteristics of an image to differentiate one feature of the image
    from another.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Spark和Spark MLlib实现了许多额外的特征提取功能，您在构建模型时可能会感兴趣。有很多特征可以提高模型的准确性；例如，在处理图像时，您可能希望尝试一种只保留图像轮廓和图像非常独特特征描述符的方法，以区分图像的一个特征与另一个特征。
- en: You can leverage Spark’s generic functions to do anything that you need with
    code, but be mindful of the resources and compute costs involved. Domain knowledge
    is the key for a successful featurization process, and it’s important to handle
    feature engineering with great care. Next, we’ll take a look at building a machine
    learning model using Spark MLlib.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以利用Spark的通用函数来处理任何您需要的代码，但请注意所涉及的资源和计算成本。领域知识是成功特征化过程的关键，因此在进行特征工程时务必小心处理。接下来，我们将介绍如何使用Spark
    MLlib构建机器学习模型。
- en: ^([1](ch05.xhtml#ch01fn13-marker)) A *kernel* is a matrix that acts like an
    image filter. It slides across the image and multiplies the kernel values with
    the input data, extracting certain “features” from the input image.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch05.xhtml#ch01fn13-marker)) *核*是一个像图像滤波器一样操作的矩阵。它在图像上滑动，并将核值与输入数据相乘，从输入图像中提取特定的“特征”。
