- en: Chapter 10\. Creating ML Models to Predict Sequences
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章\. 创建用于预测序列的ML模型
- en: '[Chapter 9](ch09.xhtml#understanding_sequence_and_time_series_) introduced
    sequence data and the attributes of a time series, including seasonality, trend,
    autocorrelation, and noise. You created a synthetic series to use for predictions
    and explored how to do basic statistical forecasting. Over the next couple of
    chapters, you’ll learn how to use ML for forecasting. But before you start creating
    models, you need to understand how to structure the time series data for training
    predictive models, by creating what we’ll call a *windowed dataset.*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[第9章](ch09.xhtml#understanding_sequence_and_time_series_)介绍了序列数据和时间序列的属性，包括季节性、趋势、自相关性和噪声。您创建了一个合成序列用于预测，并探索了如何进行基本的统计预测。在接下来的几章中，您将学习如何使用ML进行预测。但在开始创建模型之前，您需要了解如何为训练预测模型结构化时间序列数据，这将创建我们称之为*窗口数据集*的内容。'
- en: To understand why you need to do this, consider the time series you created
    in [Chapter 9](ch09.xhtml#understanding_sequence_and_time_series_). You can see
    a plot of it in [Figure 10-1](#synthetic_time_series).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解为什么需要这样做，请考虑您在[第9章](ch09.xhtml#understanding_sequence_and_time_series_)中创建的时间序列。您可以在[图10-1](#synthetic_time_series)中看到其图表。
- en: '![Synthetic time series](Images/aiml_1001.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![合成时间序列](Images/aiml_1001.png)'
- en: Figure 10-1\. Synthetic time series
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-1\. 合成时间序列
- en: If at any point you want to predict a value at time *t*, you’ll want to predict
    it as a function of the values preceding time *t*. For example, say you want to
    predict the value of the time series at time step 1,200 as a function of the 30
    values preceding it. In this case, the values from time steps 1,170 to 1,199 would
    determine the value at time step 1,200, as shown in [Figure 10-2](#previous_values_impacting_prediction).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想在时间*t*预测某个值，您将希望将其预测为时间*t*之前值的函数。例如，假设您希望预测时间步骤1,200的时间序列值，作为前30个时间步骤的函数。在这种情况下，从时间步骤1,170到1,199的值将确定时间步骤1,200的值，如[图10-2](#previous_values_impacting_prediction)所示。
- en: '![Previous values impacting prediction](Images/aiml_1002.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![前值影响预测](Images/aiml_1002.png)'
- en: Figure 10-2\. Previous values impacting prediction
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-2\. 前值影响预测
- en: 'Now this begins to look familiar: you can consider the values from 1,170–1,199
    to be your *features* and the value at 1,200 to be your *label*. If you can get
    your dataset into a condition where you have a certain number of values as features
    and the following one as the label, and you do this for every known value in the
    dataset, you’ll end up with a pretty decent set of features and labels that can
    be used to train a model.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现在开始看起来很熟悉：您可以将从1,170到1,199的值视为*特征*，并将1,200处的值视为*标签*。如果您可以使数据集的一定数量的值成为特征，并使后续的值成为标签，并且对数据集中的每个已知值执行此操作，那么您将获得一组非常不错的特征和标签，可用于训练模型。
- en: Before doing this for the time series dataset from [Chapter 9](ch09.xhtml#understanding_sequence_and_time_series_),
    let’s create a very simple dataset that has all the same attributes, but with
    a much smaller amount of data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在为来自[第9章](ch09.xhtml#understanding_sequence_and_time_series_)的时间序列数据集做这些操作之前，让我们创建一个非常简单的数据集，具有相同的属性，但数据量要小得多。
- en: Creating a Windowed Dataset
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建窗口数据集
- en: 'The `tf.data` libraries contain a lot of APIs that are useful for manipulating
    data. You can use these to create a basic dataset containing the numbers 0–9,
    emulating a time series. You’ll then turn that into the beginnings of a windowed
    dataset. Here’s the code:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.data`库包含许多用于数据操作的有用API。您可以使用这些API创建一个基本数据集，其中包含0到9的数字，模拟一个时间序列。然后，您将把它转换为窗口数据集的开端。以下是代码：'
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: First it creates the dataset using a range, which simply makes the dataset contain
    the values 0 to *n* – 1, where *n* is, in this case, 10.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，它使用一个范围创建数据集，这简单地使数据集包含值0到*n* – 1，其中*n*在本例中为10。
- en: 'Next, calling `dataset.window` and passing a parameter of `5` specifies to
    split the dataset into windows of five items. Specifying `shift=1` causes each
    window to then be shifted one spot from the previous one: the first window will
    contain the five items beginning at 0, the next window the five items beginning
    at 1, etc. Setting `drop_remainder` to `True` specifies that once it reaches the
    point close to the end of the dataset where the windows would be smaller than
    the desired size of five, they should be dropped.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，调用`dataset.window`并传递一个参数`5`，指定将数据集分割成五个项目的窗口。设置`shift=1`会导致每个窗口向前移动一个位置：第一个窗口将包含从0开始的五个项目，下一个窗口将包含从1开始的五个项目，依此类推。将`drop_remainder`设置为`True`指定，一旦它接近数据集末尾并且窗口小于所需的五个项目，它们应该被丢弃。
- en: Given the window definition, the process of splitting the dataset can take place.
    You do this with the `flat_map` function, in this case requesting a batch of five
    windows.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 给定窗口定义，可以进行数据集分割的过程。您可以使用`flat_map`函数来完成这个过程，在本例中请求一个包含五个窗口的批次。
- en: 'Running this code will give the following result:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这段代码将得到以下结果：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'But earlier you saw that we want to make training data out of this, where there
    are *n* values defining a feature and a subsequent value giving a label. You can
    do this by adding another lambda function that splits each window into everything
    before the last value, and then the last value. This gives an `x` and a `y` dataset,
    as shown here:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 但是之前您看到，我们希望从中创建训练数据，其中有*n*个值定义一个特征，并且后续的值提供一个标签。您可以通过添加另一个lambda函数来完成这个操作，该函数将每个窗口分割为最后一个值之前的所有内容，然后是最后一个值。这会生成一个`x`和一个`y`数据集，如下所示：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The results are now in line with what you’d expect. The first four values in
    the window can be thought of as the features, with the subsequent value being
    the label:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在结果与您期望的一致。窗口中的前四个值可以被视为特征，后续的值是标签：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'And because this is a dataset, it can also support shuffling and batching via
    lambda functions. Here, it’s been shuffled and batched with a batch size of 2:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 而且因为这是一个数据集，它也可以通过lambda函数支持洗牌和分批处理。在这里，它已经被洗牌和批处理，批处理大小为2：
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The results show that the first batch has two sets of `x` (starting at 2 and
    3, respectively) with their labels, the second batch has two sets of `x` (starting
    at 1 and 5, respectively) with their labels, and so on:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，第一个批次有两组`x`（分别从2和3开始）及其标签，第二个批次有两组`x`（分别从1和5开始）及其标签，依此类推：
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: With this technique, you can now turn any time series dataset into a set of
    training data for a neural network. In the next section, you’ll explore how to
    take the synthetic data from [Chapter 9](ch09.xhtml#understanding_sequence_and_time_series_)
    and create a training set from it. From there you’ll move on to creating a simple
    DNN that is trained on this data and can be used to predict future values.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种技术，您现在可以将任何时间序列数据集转换为神经网络的训练数据集。在下一节中，您将探讨如何从[第9章](ch09.xhtml#understanding_sequence_and_time_series_)的合成数据中创建训练集。从那里，您将继续创建一个简单的DNN，该网络经过训练可以用于预测未来的值。
- en: Creating a Windowed Version of the Time Series Dataset
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建时间序列数据集的窗口化版本
- en: 'As a recap, here’s the code used in the previous chapter to create a synthetic
    time series dataset:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，在上一章中使用的代码来创建一个合成的时间序列数据集：
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This will create a time series that looks like [Figure 10-1](#synthetic_time_series).
    If you want to change it, feel free to tweak the values of the various constants.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个类似于[图10-1](#synthetic_time_series)的时间序列。如果您想要进行更改，请随意调整各种常量的值。
- en: 'Once you have the series, you can turn it into a windowed dataset with code
    similar to that in the previous section. Here it is, defined as a standalone function:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您有了这个系列，您可以像前一节中的代码一样将其转换为窗口化的数据集。这里定义为一个独立的函数：
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that it uses the `from_tensor_slices` method of `tf.data.Dataset`, which
    allows you to turn a series into a `Dataset`. You can learn more about this method
    in the [TensorFlow documentation](https://oreil.ly/suj2x).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，它使用了`tf.data.Dataset`的`from_tensor_slices`方法，该方法允许您将一个系列转换为`Dataset`。您可以在[TensorFlow文档](https://oreil.ly/suj2x)中了解更多关于这个方法的信息。
- en: 'Now, to get a training-ready dataset you can simply use the following code.
    First you split the series into training and validation datasets, then you specify
    details like the size of the window, the batch size, and the shuffle buffer size:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，要获取一个可用于训练的数据集，您可以简单地使用以下代码。首先，将系列分为训练集和验证集，然后指定细节，如窗口大小、批量大小和洗牌缓冲区大小：
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The important thing to remember now is that your data is a `tf.data.Dataset`,
    so it can easily be passed to `model.fit` as a single parameter and `tf.keras`
    will take care of the rest.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在要记住的重要一点是，你的数据是一个`tf.data.Dataset`，因此可以轻松地将其作为单个参数传递给`model.fit`，`tf.keras`会照顾其余的工作。
- en: 'If you want to inspect what the data looks like, you can do so with code like
    this:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想查看数据的样子，可以用这样的代码来做：
- en: '[PRE9]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here the `batch_size` is set to `1`, just to make the results more readable.
    You’ll end up with output like this, where a single set of data is in the batch:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这里将`batch_size`设置为`1`，只是为了使结果更易读。你将得到类似这样的输出，其中一个数据集在批次中：
- en: '[PRE10]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The first batch of numbers are the features. We set the window size to 20, so
    it’s a 1 × 20 tensor. The second number is the label (67.47085 in this case),
    which the model will try to fit the features to. You’ll see how that works in
    the next section.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 第一批数字是特征。我们将窗口大小设置为20，因此这是一个1 × 20的张量。第二个数字是标签（在这种情况下为67.47085），模型将尝试将特征拟合到标签。你将在下一节看到它是如何工作的。
- en: Creating and Training a DNN to Fit the Sequence Data
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建和训练一个DNN来拟合序列数据
- en: 'Now that you have the data in a `tf.data.Dataset`, creating a neural network
    model in `tf.keras` becomes very straightforward. Let’s first explore a simple
    DNN that looks like this:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经有了一个`tf.data.Dataset`中的数据，使用`tf.keras`创建神经网络模型变得非常简单。让我们首先探索一个看起来像这样的简单DNN：
- en: '[PRE11]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It’s a super simple model with two dense layers, the first of which accepts
    the input shape of `window_size` before an output layer that will contain the
    predicted value.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个超级简单的模型，有两个稠密层，第一个接受`window_size`的输入形状，然后是一个包含预测值的输出层。
- en: 'The model is compiled with a loss function and optimizer, as before. In this
    case the loss function is specified as `mse`, which stands for mean squared error
    and is commonly used in regression problems (which is what this ultimately boils
    down to!). For the optimizer, `sgd` (stochastic gradient descent) is a good fit.
    I won’t go into detail on these types of functions in this book, but any good
    resource on machine learning will teach you about them—Andrew Ng’s seminal [Deep
    Learning Specialization](https://oreil.ly/A8QzN) on Coursera is a great place
    to start. SGD takes parameters for learning rate (`lr`) and momentum, and these
    tweak how the optimizer learns. Every dataset is different, so it’s good to have
    control. In the next section, you’ll see how you can figure out the optimal values,
    but, for now, just set them like this:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型使用了与之前相同的损失函数和优化器。在这种情况下，损失函数被指定为`mse`，代表均方误差，通常用于回归问题（最终就是这种问题！）。对于优化器，`sgd`（随机梯度下降）是一个很好的选择。我不会在这本书中详细讨论这些函数类型，但是任何关于机器学习的良好资源都会教你它们——Andrew
    Ng 在 Coursera 的开创性 [深度学习专项课程](https://oreil.ly/A8QzN) 就是一个很好的起点。SGD有学习率（`lr`）和动量的参数，它们调整优化器的学习方式。每个数据集都不同，所以控制是很重要的。在下一节中，你将看到如何确定最佳值，但是现在，只需像这样设置它们：
- en: '[PRE12]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Training then becomes as simple as calling `model.fit`, passing it your dataset,
    and specifying the number of epochs to train for:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程只需调用`model.fit`，将数据集传递给它，并指定训练的周期数即可：
- en: '[PRE13]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As you train, you’ll see the loss function report a number that starts high
    but will decline steadily. Here’s the result of the first 10 epochs:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，你会看到损失函数报告一个起初很高但会稳步下降的数字。这是前10个周期的结果：
- en: '[PRE14]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Evaluating the Results of the DNN
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估DNN的结果
- en: Once you have a trained DNN, you can start predicting with it. But remember,
    you have a windowed dataset, so, the prediction for a given point is based on
    the values of a certain number of time steps before it.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了训练好的DNN，你可以开始用它进行预测。但要记住，你有一个窗口化的数据集，因此，对于给定时间点的预测是基于它之前的若干时间步的值。
- en: In other words, as your data is in a list called `series`, to predict a value
    you have to pass the model values from time *t* to time *t*+`window_size`. It
    will then give you the predicted value for the next time step.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，由于你的数据是一个名为`series`的列表，要预测一个值，你必须将模型值从时间 *t* 到时间 *t* + `window_size` 传递给它。然后它会给你预测的下一个时间步的值。
- en: 'For example, if you wanted to predict the value at time step 1,020, you would
    take the values from time steps 1,000 through 1,019 and use them to predict the
    next value in the sequence. To get those values, you use the following code (note
    that you specify this as `series[1000:1020]`, not `series[1000:1019]`!):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你想预测时间步1,020的值，你需要从时间步1,000到1,019的值，并用它们来预测序列中的下一个值。要获取这些值，你可以使用以下代码（注意，你要指定为`series[1000:1020]`，而不是`series[1000:1019]`！）：
- en: '[PRE15]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then, to get the value at step 1,020, you simply use `series[1020]` like this:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，要获取步骤 1,020 的值，你只需像这样使用 `series[1020]`：
- en: '[PRE16]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To get the prediction for that data point, you then pass the series into `model.predict`.
    Note, however, that in order to keep the input shape consistent you’ll need an
    `[np.newaxis]`, like this:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取该数据点的预测值，然后将系列传递给 `model.predict`。然而，请注意，为了保持输入形状一致，你需要 `[np.newaxis]`，像这样：
- en: '[PRE17]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Or, if you want code that’s a bit more generic, you can use this:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果你想要更通用的代码，你可以使用这个：
- en: '[PRE18]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Do note that all of this is assuming a window size of 20 data points, which
    is quite small. As a result, your model may lack some accuracy. If you want to
    try a different window size, you’ll need to reformat the dataset by calling the
    `windowed_dataset` function again and then retraining the model.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这一切都假设窗口大小为 20 个数据点，这是相当小的。因此，你的模型可能会缺乏一些准确性。如果你想尝试不同的窗口大小，你需要再次调用 `windowed_dataset`
    函数重新格式化数据集，然后重新训练模型。
- en: 'Here is the output for this dataset when taking a start point of 1,000 and
    predicting the next value:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在从 1,000 开始并预测下一个值时该数据集的输出：
- en: '[PRE19]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The first tensor contains the list of values. Next, we see the *actual* next
    value, which is 106.258606\. Finally, we see the *predicted* next value, 105.36248\.
    We’re getting a reasonable prediction, but how do we measure the accuracy over
    time? We’ll explore that in the next section.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个张量包含值列表。接下来，我们看到 *实际* 的下一个值为 106.258606。最后，我们看到 *预测* 的下一个值为 105.36248。我们得到了一个合理的预测，但如何测量随时间的准确性？我们将在下一节中探讨这个问题。
- en: Exploring the Overall Prediction
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索总体预测
- en: In the previous section, you saw how to get a prediction for a given point in
    time by taking the previous set of values based on the window size (in this case
    20) and passing them to the model. To see the overall results of the model you’ll
    have to do the same for every time step.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，你看到了如何通过采用窗口大小（在本例中为 20）的先前一组值并将它们传递给模型来获取特定时间点的预测值。要查看模型的整体结果，你将不得不对每个时间步骤做同样的事情。
- en: 'You can do this with a simple loop like this:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以像这样使用一个简单的循环来完成：
- en: '[PRE20]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: First, you create a new array called `forecast` that will store the predicted
    values. Then, for every time step in the original series, you call the `predict`
    method and store the results in the `forecast` array. You can’t do this for the
    first *n* elements in the data, where *n* is the `window_size`, because at that
    point you won’t have enough data to make a prediction, as every prediction requires
    *n* previous values.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你创建一个名为 `forecast` 的新数组，用于存储预测值。然后，对于原始系列中的每个时间步长，你调用 `predict` 方法并将结果存储在
    `forecast` 数组中。对于数据的前 *n* 个元素，你无法这样做，其中 *n* 是 `window_size`，因为在那时你没有足够的数据来进行预测，因为每次预测都需要前
    *n* 个先前的值。
- en: When this loop is finished, the `forecast` array will have the values of the
    predictions for time step 21 onwards.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当这个循环结束时，`forecast` 数组将包含从时间步长 21 开始的预测值。
- en: 'If you recall, you also split the dataset into training and validation sets
    at time step 1,000\. So, for the next step you should also only take the forecasts
    from this time onwards. As your forecast data is already off by 20 (or whatever
    your window size is), you can split it and turn it into a Numpy array like this:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你回想一下，你还将数据集在时间步骤 1,000 处分成了训练集和验证集。因此，对于接下来的步骤，你也应该只取从此时间点开始的预测。由于你的预测数据已经错位了
    20 个（或者你的窗口大小是多少），你可以将其拆分并将其转换为一个 Numpy 数组，像这样：
- en: '[PRE21]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'It’s now in the same shape as the prediction data, so you can plot them against
    each other like this:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在它与预测数据的形状相同，所以你可以像这样将它们相互绘制：
- en: '[PRE22]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The plot will look something like [Figure 10-3](#plotting_predictions_against_values).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 绘图看起来会像 [图 10-3](#plotting_predictions_against_values)。
- en: '![Plotting predictions against values](Images/aiml_1003.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![绘制预测值对比图](Images/aiml_1003.png)'
- en: Figure 10-3\. Plotting predictions against values
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-3\. 绘制预测值对比图
- en: From a quick visual inspection, you can see that the prediction isn’t bad. It’s
    generally following the curve of the original data. When there are rapid changes
    in the data the prediction takes a little time to catch up, but on the whole it
    isn’t bad.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 从快速的视觉检查中，你可以看到预测并不差。它通常会跟随原始数据的曲线。当数据发生快速变化时，预测需要一些时间来赶上，但总体上并不差。
- en: 'However, it’s hard to be precise when eyeballing the curve. It’s best to have
    a good metric, and in [Chapter 9](ch09.xhtml#understanding_sequence_and_time_series_)
    you learned about one—the MAE. Now that you have the valid data and the results,
    you can measure the MAE with this code:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仅凭肉眼观察曲线很难准确。最好有一个良好的度量标准，在[第 9 章](ch09.xhtml#understanding_sequence_and_time_series_)中，你学到了一个——MAE（平均绝对误差）。现在，你已经有了有效的数据和结果，可以使用以下代码来测量
    MAE：
- en: '[PRE23]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: There was randomness introduced to the data, so your results may vary, but when
    I tried it I got a value of 4.51 as the MAE.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中引入了随机性，因此你的结果可能会有所不同，但当我尝试时，得到了 MAE 值为 4.51。
- en: You could argue that the process of getting the predictions as accurate as possible
    then becomes the process of minimizing that MAE. There are some techniques that
    you can use to do this, including the obvious changing of the window size. I’ll
    leave you to experiment with that, but in the next section you’ll do some basic
    hyperparameter tuning on the optimizer to improve how your neural network learns,
    and see what impact that will have on the MAE.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以说，尽可能准确地获取预测结果，然后将其最小化成为最小化 MAE 的过程。有一些技术可以帮助你做到这一点，包括明显的更改窗口大小。我让你去尝试一下，但在接下来的章节中，你将对优化器进行基本的超参数调整，以改善神经网络的学习效果，并了解这对
    MAE 的影响。
- en: Tuning the Learning Rate
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整学习率
- en: 'In the previous example, you might recall that you compiled the model with
    an optimizer that looked like this:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，你可能还记得，你使用了一个如下所示的优化器来编译模型：
- en: '[PRE24]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In this case, you used a learning rate of 1 × 10^(–6). But that seemed to be
    a really arbitrary number. What if you changed it? And how should you go about
    changing it? It would take a lot of experimentation to find the best rate.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你使用了一个学习率为 1 × 10^(–6)。但这似乎是一个非常随意的数字。如果你改变它会怎样？你应该如何去改变它？需要大量的实验来找到最佳的学习率。
- en: One thing that `tf.keras` offers you is a callback that helps you adjust the
    learning rate over time. You learned about callbacks—functions that are called
    at the end of every epoch—way back in [Chapter 2](ch02.xhtml#introduction_to_computer_vision),
    where you used one to cancel training when the accuracy reached a desired value.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.keras` 为你提供的一项功能是一个回调函数，帮助你随着时间调整学习率。你在[第 2 章](ch02.xhtml#introduction_to_computer_vision)早些时候学习过回调函数——在那里，你使用一个回调函数在每个
    epoch 结束时取消训练，当准确率达到预期值时。'
- en: You can also use a callback to adjust the learning rate parameter, plot the
    value of that parameter against the loss for the appropriate epoch, and from there
    determine the best learning rate to use.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用回调函数来调整学习率参数，将该参数的值与适当 epoch 的损失绘制在一起，从而确定最佳的学习率使用。
- en: 'To do this, simply create a `tf.keras.callbacks.LearningRateScheduler` and
    have it populate the `lr` parameter with the desired starting value. Here’s an
    example:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，只需创建一个 `tf.keras.callbacks.LearningRateScheduler`，并让它使用所需的起始值填充 `lr`
    参数。以下是一个例子：
- en: '[PRE25]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In this case you’re going to start the learning rate at 1e–8, and then every
    epoch increase it by a small amount. By the time it’s completed one hundred epochs,
    the learning rate will be up to about 1e–3.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你将从 1e–8 开始学习率，并在每个 epoch 增加一个小量。到完成一百个 epochs 时，学习率将增加到约 1e–3。
- en: 'Now you can initialize the optimizer with the learning rate of 1e–8, and specify
    that you want to use this callback within the `model.fit` call:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以使用学习率为 1e–8 初始化优化器，并指定在 `model.fit` 调用中使用此回调函数：
- en: '[PRE26]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As you used `history=model.fit`, the training history is stored for you, including
    the loss. You can then plot this against the learning rate per epoch like this:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你使用了 `history=model.fit`，训练历史已经为你存储好了，包括损失。你可以像这样将其与每个 epoch 的学习率绘制在一起：
- en: '[PRE27]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This just sets the `lrs` value using the same formula as the lambda function,
    and plots this against loss between 1e–8 and 1e–3\. [Figure 10-4](#plotting_loss_versus_learning_rate)
    shows the result.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是使用与 lambda 函数相同的公式设置 `lrs` 值，并在 1e–8 到 1e–3 之间绘制其与损失的关系。[图 10-4](#plotting_loss_versus_learning_rate)展示了结果。
- en: '![Plotting loss versus learning rate](Images/aiml_1004.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![绘制损失与学习率的关系](Images/aiml_1004.png)'
- en: Figure 10-4\. Plotting loss versus learning rate
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-4\. 绘制损失与学习率的关系
- en: So, while earlier you had set the learning rate to 1e–6, it looks like 1e–5
    has a smaller loss, so now you can go back to the model and redefine it with 1e–5
    as the new learning rate.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，虽然之前你将学习率设置为 1e–6，但看起来 1e–5 的损失更小，所以现在你可以回到模型中，并用 1e–5 重新定义它作为新的学习率。
- en: After training the model, you’ll likely notice that the loss has reduced a bit.
    In my case, with the learning rate at 1e–6 my final loss was 36.5, but with the
    learning rate at 1e–5 it was reduced to 32.9\. However, when I ran predictions
    for all the data, the result was the chart in [Figure 10-5](#chart_with_the_adjusted_learning_rate),
    which as you can see looks a little off.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型之后，您可能会注意到损失有所减少。在我的情况下，学习率为1e-6时，最终损失为36.5，但学习率为1e-5时，损失降至32.9。然而，当我对所有数据运行预测时，结果是图表在[图10-5](#chart_with_the_adjusted_learning_rate)中，可以看到看起来有点偏差。
- en: '![Chart with the adjusted learning rate](Images/aiml_1005.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![调整后的学习率图表](Images/aiml_1005.png)'
- en: Figure 10-5\. Chart with the adjusted learning rate
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-5\. 调整后的学习率图表
- en: And when I measured the MAE it ended up as 4.96, so it has taken a small step
    backward! That being said, once you know you have the best learning rate, you
    can start exploring other methodologies to optimize the network’s performance.
    An easy place to begin is with the size of the window—20 days data to predict
    1 day may not be enough, so you might want to try a window of 40 days. Also, try
    training for more epochs. With a bit of experimentation you could get an MAE of
    close to 4, which isn’t bad.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当我测量MAE时，结果为4.96，所以它略有退步！话虽如此，一旦您确定了最佳学习率，您可以开始探索其他优化网络性能的方法。一个简单的起点是窗口的大小——预测1天的20天数据可能不足够，所以您可能希望尝试40天的窗口。另外，尝试增加训练的epochs数。通过一些实验，您可能可以将MAE降到接近4，这还算不错。
- en: Exploring Hyperparameter Tuning with Keras Tuner
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Keras调参工具进行超参数调优
- en: In the previous section, you saw how to do a rough optimization of the learning
    rate for the stochastic gradient descent loss function. It was certainly a very
    rough effort, changing the learning rate every few epochs and measuring the loss.
    It also was somewhat tainted by the fact that the loss function was *already*
    changing epoch to epoch, so you may not have actually been finding the best value,
    but an approximation. To really find the best value, you would have to train for
    the full set of epochs with each potential value, and then compare the results.
    And that’s just for one hyperparameter, the learning rate. If you want to find
    the best momentum, or tune other things like the model architecture—how many neurons
    per layer, how many layers, etc.—you can end up with many thousands of options
    to test, and training across all of these would be hard to code.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，您看到了如何为随机梯度下降损失函数进行粗略的学习率优化。这确实是一个非常粗糙的尝试，每隔几个epochs更改一次学习率并测量损失。它也受到损失函数每个epoch之间已经改变的影响，因此您可能实际上并没有找到最佳值，而是一个近似值。要真正找到最佳值，您需要对每个潜在值进行完整epochs的训练，然后比较结果。这仅仅是一个超参数——学习率。如果您想找到最佳的动量或调整其他事物，如模型架构——每层多少个神经元，多少层等等——您可能需要测试成千上万个选项，并且跨所有这些进行训练将是很难编码的。
- en: 'Fortunately, the [Keras Tuner tool](https://oreil.ly/QDFVd) makes this relatively
    easy. You can install Keras Tuner using a simple `pip` command:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，[Keras调参工具](https://oreil.ly/QDFVd)使这相对容易。您可以使用简单的`pip`命令安装Keras调参工具：
- en: '[PRE28]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: You can then use it to parameterize your hyperparameters, specifying ranges
    of values to test. Keras Tuner will train multiple models, one with each possible
    set of parameters, evaluate the model to a metric you want, and then report on
    the top models. I won’t go into all of the options the tool offers here, but I
    will show how you can use it for this specific model.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用它来参数化您的超参数，指定要测试的值范围。Keras调参器将训练多个模型，每个可能的参数集合一个，评估模型到您想要的指标，然后报告排名靠前的模型。我不会在这里详细介绍工具提供的所有选项，但我会展示如何为这个特定模型使用它。
- en: Say we want to experiment with just two things, the first being the number of
    input neurons in the model architecture. All along you’ve had a model architecture
    of 10 input neurons, followed by a hidden layer of 10, followed by the output
    layer. But could the network do better with more? What if you could experiment
    with up to 30 neurons in the input layer, for example?
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要尝试两件事，第一件事是模型架构中输入神经元的数量。一直以来，您的模型架构是10个输入神经元，后面跟着一个10个神经元的隐藏层，然后是输出层。但是如果增加更多的神经元在输入层，例如30个，网络能否表现更好呢？
- en: 'Recall that the input layer was defined like this:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，输入层的定义如下：
- en: '[PRE29]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'If you want to test different values than the hardcoded 10, you can set it
    to cycle through a number of integers like this:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想测试不同于硬编码的10的值，您可以设置它循环遍历一些整数，就像这样：
- en: '[PRE30]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Here you define that the layer will be tested with several input values, starting
    with 10 and increasing to 30 in steps of 2\. Now, instead of training the model
    just once and seeing the loss, Keras Tuner will train the model 11 times!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您定义该层将使用多个输入值进行测试，从 10 开始，以步长 2 增加到 30。现在，不再是仅训练一次模型并查看损失，Keras Tuner 将训练该模型
    11 次！
- en: 'Also, when you compiled the model, you hardcoded the value of the `momentum`
    parameter to be `0.9`. Recall this code from the model definition:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在编译模型时，您将`momentum`参数的值硬编码为`0.9`。请回顾一下模型定义中的此代码：
- en: '[PRE31]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'You can change this to cycle through a few options instead by using the `hp.Choice`
    construct. Here’s an example:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用`hp.Choice`结构将此更改为循环浏览几个选项。以下是一个示例：
- en: '[PRE32]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This provides four possible choices, so, when combined with the model architecture
    defined previously you’ll end up cycling through 44 possible combinations. Keras
    Tuner can do this for you, and report back on the model that performed the best.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了四种可能的选择，因此，当与先前定义的模型架构结合使用时，您将循环浏览 44 种可能的组合。Keras Tuner 可以为您完成这些操作，并报告表现最佳的模型。
- en: 'To finish setting this up, first create a function that builds the model for
    you. Here’s an updated model definition:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成设置，请首先创建一个为您构建模型的函数。以下是更新后的模型定义：
- en: '[PRE33]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, with Keras Tuner installed, you can create a `RandomSearch` object that
    manages all the iterations for this model:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，安装了 Keras Tuner 后，您可以创建一个`RandomSearch`对象来管理该模型的所有迭代：
- en: '[PRE34]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Note that you define the model by passing it the function that you described
    earlier. The hyperparameter parameter (`hp`) is used to control which values get
    changed. You specify the `objective` to be `loss`, indicating that you want to
    minimize the loss. You can cap the overall number of trials to run with the `max_trials`
    parameter, and specify how many times to train and evaluate the model (eliminating
    random fluctuations somewhat) with the `executions_per_trial` parameter.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您通过传递先前描述的函数来定义模型。超参数参数（`hp`）用于控制哪些值会发生变化。您指定`objective`为`loss`，表示您希望最小化损失。您可以使用`max_trials`参数限制要运行的试验总数，并使用`executions_per_trial`参数指定要训练和评估模型的次数（从而在某种程度上消除随机波动）。
- en: 'To start the search, you simply call `tuner.search` like you would `model.fit`.
    Here’s the code:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动搜索，只需调用`tuner.search`，就像调用`model.fit`一样。以下是代码：
- en: '[PRE35]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Running this with the synthetic series you’ve been working on in this chapter
    will then train models with every possible hyperparameter according to your definition
    of the options you want to try.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 使用本章中您一直在工作的合成系列运行此操作，然后将会根据您定义的选项尝试所有可能的超参数来训练模型。
- en: 'When it’s complete, you can call `tuner.results_summary` and it will give you
    your top 10 trials based on the objective:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 当完成时，您可以调用`tuner.results_summary`，它将显示基于目标的前 10 个试验结果：
- en: '[PRE36]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'You should see output like this:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该会看到像这样的输出：
- en: '[PRE37]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'From the results, you can see that the best loss score was achieved with a
    momentum of 0.5 and 28 input units. You can retrieve this model and other top
    models by calling `get_best_models` and specifying how many you want—for example,
    if you want the top four models you’d call it like this:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中，您可以看到最佳损失得分是在动量为 0.5 和输入单元为 28 时实现的。您可以通过调用`get_best_models`来检索此模型和其他前几个模型，并指定您想要的数量——例如，如果您想要前四个模型，可以这样调用：
- en: '[PRE38]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: You can then test those models.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以测试这些模型。
- en: 'Alternatively, you could create a new model from scratch using the learned
    hyperparameters, like this:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用学到的超参数从头开始创建一个新模型，就像这样：
- en: '[PRE39]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: When I trained using these hyperparameters, and forecasting for the entire validation
    set as earlier, I got a chart that looked like [Figure 10-6](#the_prediction_chart_with_optimized_hyp).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用这些超参数进行训练并对整个验证集进行预测时，我得到了一个类似于[图 10-6](#the_prediction_chart_with_optimized_hyp)的图表。
- en: '![The prediction chart with optimized hyperparameters](Images/aiml_1006.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![使用优化后的超参数的预测图表](Images/aiml_1006.png)'
- en: Figure 10-6\. The prediction chart with optimized hyperparameters
  id: totrans-141
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-6\. 使用优化后的超参数的预测图表
- en: A calculation of the MAE on this yielded 4.47, which is a slight improvement
    on the original of 4.51 and a big improvement over the last chapter’s statistical
    approach that gave me a result of 5.13\. This was done with the learning rate
    changed to 1e–5, which may not have been optimal. Using Keras Tuner you can tweak
    hyperparameters like that, adjust the number of neurons in the middle layer, or
    even experiment with different loss functions and optimizers. Give it a try and
    see if you can improve this model!
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这次计算中，平均绝对误差（MAE）为4.47，比原始结果4.51略有改善，并且比上一章节的统计方法（结果为5.13）有了很大的提升。这是通过将学习速率改为1e-5实现的，这可能并不是最佳选择。使用
    Keras 调参器，你可以调整类似这样的超参数，调整中间层的神经元数量，甚至尝试不同的损失函数和优化器。试一试，看看是否能改进这个模型！
- en: Summary
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you took the statistical analysis of the time series from [Chapter 9](ch09.xhtml#understanding_sequence_and_time_series_)
    and applied machine learning to try to do a better job of prediction. Machine
    learning really is all about pattern matching, and, as expected, you were able
    to take the mean average error down almost 10% by first using a deep neural network
    to spot the patterns, and then using hyperparameter tuning with Keras Tuner to
    improve the loss and increase the accuracy. In [Chapter 11](ch11.xhtml#using_convolutional_and_recurrent_metho),
    you’ll go beyond a simple DNN and examine the implications of using a recurrent
    neural network to predict sequential values.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你对时间序列的统计分析进行了处理（来自[第9章](ch09.xhtml#understanding_sequence_and_time_series_)），并应用机器学习试图做出更好的预测。机器学习真正关键在于模式匹配，正如预期的那样，通过首先使用深度神经网络来识别模式，然后使用
    Keras 调参器调整超参数来改进损失并提高准确性，你成功将平均绝对误差降低了近10%。在[第11章](ch11.xhtml#using_convolutional_and_recurrent_metho)，你将超越简单的深度神经网络，探索使用递归神经网络预测序列值的影响。
