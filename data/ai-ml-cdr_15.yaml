- en: Chapter 12\. An Introduction to TensorFlow Lite
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章。TensorFlow Lite简介
- en: 'In all of the chapters of this book so far, you’ve been exploring how to use
    TensorFlow to create machine learning models that can provide functionality such
    as computer vision, natural language processing, and sequence modeling without
    the explicit programming of rules. Instead, using labeled data, neural networks
    are able to learn the patterns that distinguish one thing from another, and this
    can then be extended into solving problems. For the rest of the book we’re going
    to switch gears and look at *how* to use these models in common scenarios. The
    first, most obvious and perhaps most useful topic we’ll cover is how to use models
    in mobile applications. In this chapter, I’ll go over the underlying technology
    that makes it possible to do machine learning on mobile (and embedded) devices:
    TensorFlow Lite. Then, in the next two chapters we’ll explore scenarios of using
    these models on Android and iOS.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本书的所有章节都在探索如何使用TensorFlow创建机器学习模型，这些模型可以提供计算机视觉、自然语言处理和序列建模等功能，而无需明确编程规则。相反，使用标记数据，神经网络能够学习区分一件事物与另一件事物的模式，然后可以将其扩展为解决问题。在本书的其余部分，我们将转向并查看*如何*在常见场景中使用这些模型。第一个、最明显且可能最有用的主题是如何在移动应用程序中使用模型。在本章中，我将介绍使在移动（和嵌入式）设备上进行机器学习成为可能的基础技术：TensorFlow
    Lite。然后，在接下来的两章中，我们将探讨在Android和iOS上使用这些模型的场景。
- en: TensorFlow Lite is a suite of tools that complements TensorFlow, achieving two
    main goals. The first is to make your models mobile-friendly. This often involves
    reducing their size and complexity, with as little impact as possible on their
    accuracy, to make them work better in a battery-constrained environment like a
    mobile device. The second is to provide a runtime for different mobile platforms,
    including Android, iOS, mobile Linux (for example, Raspberry Pi), and various
    microcontrollers. Note that you cannot *train* a model with TensorFlow Lite. Your
    workflow will be to train it using TensorFlow and then *convert* it to the TensorFlow
    Lite format, before loading and running it using a TensorFlow Lite interpreter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Lite是一套工具，用于补充TensorFlow，实现两个主要目标。第一个目标是使您的模型适用于移动设备。这通常包括减小模型的大小和复杂性，尽可能少地影响其准确性，以使它们在像移动设备这样受电池限制的环境中更好地工作。第二个目标是为不同的移动平台提供运行时，包括Android、iOS、移动Linux（例如，树莓派）和各种微控制器。请注意，您不能使用TensorFlow
    Lite来*训练*模型。您的工作流程将是使用TensorFlow进行训练，然后*转换*为TensorFlow Lite格式，然后使用TensorFlow Lite解释器加载和运行它。
- en: What Is TensorFlow Lite?
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是TensorFlow Lite？
- en: TensorFlow Lite started as a mobile version of TensorFlow aimed at Android and
    iOS developers, with the goal of being an effective ML toolkit for their needs.
    When building and executing models on computers or cloud services, issues like
    battery consumption, screen size, and other aspects of mobile app development
    aren’t a concern, so when mobile devices are targeted a new set of constraints
    need to be addressed.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Lite最初是针对Android和iOS开发者的TensorFlow移动版本，旨在成为他们需求的有效ML工具包。在计算机或云服务上构建和执行模型时，电池消耗、屏幕大小和移动应用程序开发的其他方面不是问题，因此当针对移动设备时，需要解决一组新的约束条件。
- en: The first is that a mobile application framework needs to be *lightweight*.
    Mobile devices have far more limited resources than the typical machine that is
    used for training models. As such, developers have to be very careful about the
    resources that are used not just by the application, but also the application
    framework. Indeed, when users are browsing an app store they’ll see the size of
    each application and have to make a decision about whether to download it based
    on their data usage. If the framework that runs a model is large, and the model
    itself is also large, this will bloat the file size and turn the user off.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个是移动应用程序框架需要*轻量级*。移动设备的资源远远有限于用于训练模型的典型机器。因此，开发人员不仅需要非常小心地使用应用程序所需的资源，还需要使用应用程序框架的资源。实际上，当用户浏览应用商店时，他们会看到每个应用程序的大小，并根据其数据使用情况决定是否下载。如果运行模型的框架很大，而且模型本身也很大，这将增加文件大小，并使用户失去兴趣。
- en: The framework also has to be *low latency*. Apps that run on mobile devices
    need to perform well, or the user may stop using them. Only 38% of apps are used
    more than 11 times, meaning 62% of apps are used 10 times or less. Indeed, 25%
    of all apps are only used once. High latency, where an app is slow to launch or
    process key data, is a factor in this abandonment rate. Thus, a framework for
    ML-based apps needs to be fast to load and fast to perform the necessary inference.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 框架还必须是*低延迟*的。在移动设备上运行的应用程序需要表现良好，否则用户可能会停止使用它们。只有38%的应用程序被使用超过11次，意味着62%的应用程序被使用10次或更少。事实上，所有应用程序中有25%仅被使用一次。高延迟，即应用程序启动慢或处理关键数据慢，是导致这种放弃率的一个因素。因此，基于ML的应用程序需要加载快速并执行必要的推断。
- en: In partnership with low latency, a mobile framework needs an *efficient model
    format*. When training on a powerful supercomputer, the model format generally
    isn’t the most important signal. As we’ve seen in earlier chapters, high model
    accuracy, low loss, avoiding overfitting, etc. are the metrics a model creator
    will chase. But when running on a mobile device, in order to be lightweight and
    have low latency, the model format will need to be taken into consideration as
    well. Much of the math in the neural networks we’ve seen so far is floating-point
    operations with high precision. For scientific discovery, that’s essential. For
    running on a mobile device it may not be. A mobile-friendly framework will need
    to help you with trade-offs like this and give you the tools to convert your model
    if necessary.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 与低延迟合作，移动框架需要一个*高效的模型格式*。在强大的超级计算机上训练时，模型格式通常不是最重要的信号。正如我们在早期章节中看到的，高模型准确度、低损失、避免过拟合等是模型创建者追求的指标。但是在移动设备上运行时，为了轻量化和低延迟，模型格式也需要考虑在内。到目前为止，我们所见的神经网络中的大部分数学都是高精度的浮点运算。对于科学发现来说，这是必不可少的。但是对于在移动设备上运行来说，可能并非如此。一个移动友好的框架将需要帮助您处理这种权衡，并为您提供必要的工具以便必要时转换您的模型。
- en: Having your models run on-device has a major benefit in that they don’t need
    to pass data to a cloud service to have inference performed on them. This leads
    to improvements in *user privacy* as well as *power consumption*. Not needing
    to use the radio for a cellular or WiFi signal to send the data and receive the
    predictions is good, so long as the on-device inference doesn’t cost more power-wise.
    Keeping data on the device in order to run predictions is also a powerful and
    increasingly important feature, for obvious reasons! (Later in this book we’ll
    discuss *federated learning*, which is a hybrid of on-device and cloud-based machine
    learning that gives you the best of both worlds, while also maintaining privacy.)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在设备上运行您的模型具有一个重要的好处，即它们无需将数据传递给云服务以进行推断。这导致*用户隐私*以及*能量消耗*的改善。不需要使用无线电发送数据并接收预测的WiFi或蜂窝信号是好事，只要在设备上的推断不会在功耗方面成本过高。出于显而易见的原因，保持数据在设备上以运行预测也是一个强大且越来越重要的功能！（在本书的后面我们将讨论*联邦学习*，这是一种设备本地和基于云的混合机器学习方法，既能享受两全其美，又能保持隐私。）
- en: So, with all of this in mind, TensorFlow Lite was created. As mentioned earlier,
    it’s not a framework for *training* models, but a supplementary set of tools designed
    to meet all the constraints of mobile and embedded systems.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，考虑到所有这些，TensorFlow Lite应运而生。正如前面提到的，它不是一个用于*训练*模型的框架，而是一套补充工具，专门设计用于满足移动和嵌入式系统的所有限制。
- en: 'It should broadly be seen as two main things: a converter that takes your TensorFlow
    model and converts it to the *.tflite* format, shrinking and optimizing it, and
    a suite of interpreters for various runtimes ([Figure 12-1](#the_tensorflow_lite_suite)).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 它应该广泛被视为两个主要部分：一个转换器，将您的TensorFlow模型转换为*.tflite*格式，缩小并优化它，以及一套解释器，用于各种运行时环境（[图12-1](#the_tensorflow_lite_suite)）。
- en: '![The TensorFlow Lite suite](Images/aiml_1201.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![TensorFlow Lite套件](Images/aiml_1201.png)'
- en: Figure 12-1\. The TensorFlow Lite suite
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-1\. TensorFlow Lite套件
- en: The interpreter environments also support acceleration options within their
    particular frameworks. For example, on Android the [Neural Networks API](https://oreil.ly/wXjpm)
    is supported, so TensorFlow Lite can take advantage of it on devices where it’s
    available.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 解释器环境还支持其特定框架内的加速选项。例如，在Android上支持[神经网络API](https://oreil.ly/wXjpm)，因此TensorFlow
    Lite可以在支持该API的设备上利用它。
- en: Note that not every operation (or “op”) in TensorFlow is presently supported
    in TensorFlow Lite or the TensorFlow Lite converter. You may encounter this issue
    when converting models, and it’s always a good idea to check the [documentation](https://oreil.ly/otEIp)
    for details. One helpful piece of workflow, as you’ll see later in this chapter,
    is to take an existing mobile-friendly model and use transfer learning for your
    scenario. You can find lists of models optimized to work with TensorFlow Lite
    on the [TensorFlow website](https://oreil.ly/s28gE) and [TensorFlow Hub](https://oreil.ly/U8siI).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，并非每个TensorFlow中的操作（或“op”）目前都受到TensorFlow Lite或TensorFlow Lite转换器的支持。在转换模型时可能会遇到此问题，建议查看[文档](https://oreil.ly/otEIp)获取详细信息。本章后面的一个有用的工作流程是，获取一个现有的移动友好型模型，并为您的场景使用迁移学习。您可以在[TensorFlow网站](https://oreil.ly/s28gE)和[TensorFlow
    Hub](https://oreil.ly/U8siI)上找到与TensorFlow Lite优化工作的模型列表。
- en: 'Walkthrough: Creating and Converting a Model to TensorFlow Lite'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 演练：创建并将模型转换为TensorFlow Lite
- en: We’ll begin with a step-by-step walkthrough showing how to create a simple model
    with TensorFlow, convert it to the TensorFlow Lite format, and then use the TensorFlow
    Lite interpreter. For this walkthrough I’ll use the Linux interpreter because
    it’s readily available in Google Colab. In [Chapter 13](ch13.xhtml#using_tensorflow_lite_in_android_apps)
    you’ll see how to use this model on Android, and in [Chapter 14](ch14.xhtml#using_tensorflow_lite_in_ios_apps)
    you’ll explore using it on iOS.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从逐步演练开始，展示如何创建一个简单的TensorFlow模型，将其转换为TensorFlow Lite格式，然后使用TensorFlow Lite解释器。在这个演练中，我将使用Linux解释器，因为它在Google
    Colab中很容易获取。在[第13章](ch13.xhtml#using_tensorflow_lite_in_android_apps)中，您将看到如何在Android上使用这个模型，在[第14章](ch14.xhtml#using_tensorflow_lite_in_ios_apps)中，您将探索如何在iOS上使用它。
- en: 'Back in [Chapter 1](ch01.xhtml#introduction_to_tensorflow) you saw a very simple
    TensorFlow model that learned the relationship between two sets of numbers that
    ended up as Y = 2X – 1\. For convenience, here’s the complete code:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 回到[第1章](ch01.xhtml#introduction_to_tensorflow)，您看到了一个非常简单的TensorFlow模型，学习了两组数字之间的关系，最终得到Y
    = 2X - 1。为方便起见，这里是完整的代码：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Once this has been trained, as you saw, you can do `model.predict[x]` and get
    the expected `y`. In the preceding code, `x=10`, and the `y` the model will give
    us back is a value close to 19.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，如您所见，您可以执行`model.predict[x]`并得到预期的`y`。在前面的代码中，`x=10`，模型将返回一个接近19的值。
- en: As this model is small and easy to train, we can use it as an example that we’ll
    convert to TensorFlow Lite to show all the steps.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个模型体积小、易于训练，我们可以将其作为示例，演示转换为TensorFlow Lite的所有步骤。
- en: Step 1\. Save the Model
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1步。保存模型
- en: The TensorFlow Lite converter works on a number of different file formats, including
    SavedModel (preferred) and the Keras H5 format. For this exercise we’ll use SavedModel.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Lite转换器支持多种不同的文件格式，包括SavedModel（推荐）和Keras H5格式。在这个示例中，我们将使用SavedModel。
- en: 'To do this, simply specify a directory in which to save the model and call
    `tf.saved_model.save`, passing it the model and directory:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现这一点，只需指定一个目录来保存模型，并调用`tf.saved_model.save`，将模型和目录传递给它：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The model will be saved out as assets and variables as well as a *saved_model.pb*
    file, as shown in [Figure 12-2](#savedmodel_structure).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将保存为资产和变量以及一个*saved_model.pb*文件，如[图 12-2](#savedmodel_structure)所示。
- en: '![SavedModel structure](Images/aiml_1202.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![SavedModel结构](Images/aiml_1202.png)'
- en: Figure 12-2\. SavedModel structure
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-2。SavedModel结构
- en: Once you have the saved model, you can convert it using the TensorFlow Lite
    converter.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您拥有了保存的模型，您就可以使用TensorFlow Lite转换器将其转换。
- en: Note
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The TensorFlow team recommends using the SavedModel format for compatibility
    across the entire TensorFlow ecosystem, including future compatibility with new
    APIs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow团队建议使用SavedModel格式以确保在整个TensorFlow生态系统中的兼容性，包括未来与新API的兼容性。
- en: Step 2\. Convert and Save the Model
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步。转换并保存模型
- en: 'The TensorFlow Lite converter is in the `tf.lite` package. You can call it
    to convert a saved model by first invoking it with the `from_saved_model` method,
    passing it the directory containing the saved model, and then invoking its `convert`
    method:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Lite转换器位于`tf.lite`包中。您可以调用它来通过首先使用`from_saved_model`方法调用它，并传递包含保存模型的目录，然后调用其`convert`方法来转换保存的模型：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can then save out the new *.tflite* model using `pathlib`:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用`pathlib`保存新的*.tflite*模型：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: At this point, you have a *.tflite* file that you can use in any of the interpreter
    environments. Later we’ll use it on Android and iOS, but, for now, let’s use the
    Python-based interpreter so you can run it in Colab. This same interpreter can
    be used in embedded Linux environments like a Raspberry Pi!
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，你已经有一个名为 *.tflite* 的文件，可以在任何解释器环境中使用。稍后我们将在 Android 和 iOS 上使用它，但现在，让我们在基于
    Python 的解释器中使用它，这样你就可以在 Colab 中运行它。这个相同的解释器也可以在嵌入式 Linux 环境中使用，比如树莓派！
- en: Step 3\. Load the TFLite Model and Allocate Tensors
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 3\. 加载 TFLite 模型并分配张量
- en: The next step is to load the model into the interpreter, allocate tensors that
    will be used for inputting data to the model for prediction, and then read the
    predictions that the model outputs. This is where using TensorFlow Lite, from
    a programmer’s perspective, greatly differs from using TensorFlow. With TensorFlow
    you can just say `model.predict(*something*)` and get the results, but because
    TensorFlow Lite won’t have many of the dependencies that TensorFlow does, particularly
    in non-Python environments, you now have to get a bit more low-level and deal
    with the input and output tensors, formatting your data to fit them and parsing
    the output in a way that makes sense for your device.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将模型加载到解释器中，分配张量以将数据输入到模型进行预测，然后读取模型输出的预测结果。从程序员的角度来看，这是使用 TensorFlow Lite
    与使用 TensorFlow 的巨大区别。在 TensorFlow 中，你可以简单地说 `model.predict(*something*)` 并得到结果，但因为
    TensorFlow Lite 不会像 TensorFlow 那样有许多依赖项，特别是在非 Python 环境中，你现在必须变得更加低级，处理输入和输出张量，格式化数据以适应它们，并以对设备有意义的方式解析输出。
- en: 'First, load the model and allocate the tensors:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，加载模型并分配张量：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then you can get the input and output details from the model, so you can begin
    to understand what data format it expects, and what data format it will provide
    back to you:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以从模型中获取输入和输出的详细信息，以便开始理解它期望的数据格式，以及它将返回给你的数据格式：
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You’ll get a lot of output!
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到大量输出！
- en: 'First, let’s inspect the input parameter. Note the `shape` setting, which is
    an array of type `[1,1]`. Also note the class, which is `numpy.float32`. These
    settings will dictate the shape of the input data and its format:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们检查输入参数。请注意 `shape` 设置，它是一个类型为 `[1,1]` 的数组。还请注意类别，它是 `numpy.float32`。这些设置将决定输入数据的形状和格式：
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'So, in order to format the input data, you’ll need to use code like this to
    define the input array shape and type if you want to predict the `y` for `x=10.0`:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了格式化输入数据，如果你想预测 `x=10.0` 对应的 `y`，你需要像这样使用代码定义输入数组的形状和类型：
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The double brackets around the `10.0` can cause a little confusion—the mnemonic
    I use for the `array[1,1]` here is to say that there is 1 list, giving us the
    first set of `[]`, and that list contains just 1 value, which is `[10.0]`, thus
    giving `[[10.0]]`. It can also be confusing that the shape is defined as `dtype=int32`,
    whereas you’re using `numpy.float32`. The `dtype` parameter is the data type defining
    the shape, not the contents of the list that is encapsulated in that shape. For
    that, you’ll use the class.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里 `[1,1]` 的数组周围有双括号，可能会引起一些混淆——我在这里使用的助记符是说这里有 1 个列表，给了我们第一组 `[]`，而该列表只包含一个值，即
    `[10.0]`，因此得到 `[[10.0]]`。可能会让人困惑的是形状定义为 `dtype=int32`，而你使用的是 `numpy.float32`。`dtype`
    参数是定义形状的数据类型，而不是包含在该形状中封装的列表的内容。对于这个，你将使用类别。
- en: 'The output details are very similar, and what you want to keep an eye on here
    is the shape. Because it’s also an array of type `[1,1]`, you can expect the answer
    to be `[[y]]` in much the same way as the input was `[[x]]`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 输出细节非常相似，你需要关注的是其形状。因为它也是一个类型为 `[1,1]` 的数组，你可以期待答案也会像输入 `[[x]]` 一样是 `[[y]]`：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Step 4\. Perform the Prediction
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 4\. 执行预测
- en: 'To get the interpreter to do the prediction, you set the input tensor with
    the value to predict, telling it what input value to use:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要使解释器执行预测，你需要将输入张量设置为要预测的值，并告诉它使用哪个输入值：
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The input tensor is specified using the index of the array of input details.
    In this case you have a very simple model that has only a single input option,
    so it’s `input_details[0]`, and you’ll address it at the index. Input details
    item 0 has only one index, indexed at 0, and it expects a shape of `[1,1]` as
    defined earlier. So, you put the `to_predict` value in there. Then you invoke
    the interpreter with the `invoke` method.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 输入张量是使用输入详细信息数组的索引来指定的。在这种情况下，您有一个非常简单的模型，只有一个单独的输入选项，因此它是`input_details[0]`，您将在索引处进行处理。输入详细信息项目0只有一个索引，索引为0，并且它希望一个形状为`[1,1]`，如前所述。因此，您将`to_predict`值放入其中。然后使用`invoke`方法调用解释器。
- en: 'You can then read the prediction by calling `get_tensor` and supplying it with
    the details of the tensor you want to read:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以通过调用`get_tensor`并提供您要读取的张量的详细信息来读取预测：
- en: '[PRE10]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Again, there’s only one output tensor, so it will be `output_details[0]`, and
    you specify the index to get the details beneath it, which will have the output
    value.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，只有一个输出张量，因此它将是`output_details[0]`，您指定索引以获取其下的详细信息，这将具有输出值。
- en: 'So, for example, if you run this code:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，例如，如果您运行此代码：
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'you should see output like:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到如下输出：
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: where 10 is the input value and 18.97 is the predicted value, which is very
    close to 19, which is 2X – 1 when X = 10\. For why it’s not 19, look back to [Chapter 1](ch01.xhtml#introduction_to_tensorflow)!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中10是输入值，18.97是预测值，非常接近于19，当X = 10时为2X - 1。为什么不是19，请参阅[第1章](ch01.xhtml#introduction_to_tensorflow)！
- en: Given that this is a very simple example, let’s look at something a little more
    complex next—using transfer learning on a well-known image classification model,
    and then converting that for TensorFlow Lite. From there we’ll also be able to
    better explore the impacts of optimizing and quantizing the model.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这只是一个非常简单的例子，让我们来看看接下来的一些稍微复杂的东西——在一个知名的图像分类模型上使用迁移学习，然后将其转换为TensorFlow Lite。从那里，我们还能更好地探索优化和量化模型的影响。
- en: 'Walkthrough: Transfer Learning an Image Classifier and Converting to TensorFlow
    Lite'
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 演示：转移学习图像分类器并转换为TensorFlow Lite
- en: In this section we’ll build a new version of the Dogs vs. Cats computer vision
    model from Chapters [3](ch03.xhtml#going_beyond_the_basics_detecting_featu) and
    [4](ch04.xhtml#using_public_datasets_with_tensorflow_d) that uses transfer learning.
    This will use a model from TensorFlow Hub, so if you need to install it, you can
    follow the [instructions on the site](https://www.tensorflow.org/hub).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将从第[3章](ch03.xhtml#going_beyond_the_basics_detecting_featu)和第[4章](ch04.xhtml#using_public_datasets_with_tensorflow_d)的Dogs
    vs. Cats计算机视觉模型中构建一个使用迁移学习的新版本。这将使用来自TensorFlow Hub的模型，因此如果您需要安装它，可以按照[网站上的说明](https://www.tensorflow.org/hub)进行操作。
- en: Step 1\. Build and Save the Model
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1步：构建并保存模型
- en: 'First, get all of the data:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，获取所有数据：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This will download the Dogs vs. Cats dataset and split it into training, test,
    and validation sets.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这将下载Dogs vs. Cats数据集，并将其分割为训练、测试和验证集。
- en: 'Next, you’ll use the `mobilenet_v2` model from TensorFlow Hub to create a Keras
    layer called `feature_extractor`:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将使用来自TensorFlow Hub的`mobilenet_v2`模型创建一个名为`feature_extractor`的Keras层：
- en: '[PRE14]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now that you have the feature extractor, you can make it the first layer in
    your neural network and add an output layer with as many neurons as you have classes
    (in this case, two). You can then compile and train it:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经有了特征提取器，可以将其作为神经网络的第一层，并添加一个输出层，其神经元数量与类别数量相同（在本例中为两个）。然后您可以编译并训练它：
- en: '[PRE15]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'With just five epochs of training, this should give a model with 99% accuracy
    on the training set and 98%+ on the validation set. Now you can simply save the
    model out:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 只需五个训练周期，这应该给出一个在训练集上达到99%准确度，在验证集上超过98%的模型。现在您只需简单地保存模型即可：
- en: '[PRE16]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Once you have the saved model, you can convert it.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您有了保存的模型，您可以进行转换。
- en: Step 2\. Convert the Model to TensorFlow Lite
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步：将模型转换为TensorFlow Lite
- en: 'As before, you can now take the saved model and convert it into a *.tflite*
    model. You’ll save it out as *converted_model.tflite*:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，您现在可以取出保存的模型并将其转换为*.tflite*模型。您将其保存为*converted_model.tflite*：
- en: '[PRE17]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Once you have the file, you can instantiate an interpreter with it. When this
    is done, you should get the input and output details as before. Load them into
    variables called `input_index` and `output_index`, respectively. This makes the
    code a little more readable!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您有了文件，您可以使用它实例化一个解释器。完成此操作后，您应该像以前一样获取输入和输出详细信息。将它们加载到名为`input_index`和`output_index`的变量中。这使得代码更易读！
- en: '[PRE18]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The dataset has lots of test images in `test_batches`, so if you want to take
    one hundred of the images and test them, you can do so like this (feel free to
    change the `100` to any other value):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中有很多测试图像在`test_batches`中，所以如果您想取其中的一百张图像并对它们进行测试，可以这样做（可以自由更改`100`为任何其他值）：
- en: '[PRE19]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Earlier, when reading the images, they were reformatted by the mapping function
    called `format_image` to be the right size for both training and inference, so
    all you have to do now is set the interpreter’s tensor at the input index to the
    image. After invoking the interpreter, you can then get the tensor at the output
    index.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期读取图像时，它们通过称为`format_image`的映射函数重新格式化，以便在训练和推断中都具有正确的大小，因此现在您所需做的就是将解释器的张量设置为输入索引处的图像。调用解释器后，您可以获取输出索引处的张量。
- en: 'If you want to see how the predictions did against the labels, you can run
    code like this:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想查看预测与标签的比较情况，可以运行如下代码：
- en: '[PRE20]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This should give you a score of 99 or 100 correct predictions.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会给你一个99或100的正确预测分数。
- en: 'You can also visualize the output of the model against the test data with this
    code:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用此代码将模型的输出与测试数据进行可视化：
- en: '[PRE21]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: You can see some of the results of this in [Figure 12-3](#results_of_inference).
    (Note that all the code is available in the book’s [GitHub repo](https://github.com/lmoroney/tfbook),
    so if you need it, take a look there.)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[图 12-3](#results_of_inference)中看到这些结果的一些内容。（请注意，所有代码都可以在书的[GitHub仓库](https://github.com/lmoroney/tfbook)中找到，所以如果需要的话，请去那里查看。）
- en: '![Results of inference](Images/aiml_1203.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![推断结果](Images/aiml_1203.png)'
- en: Figure 12-3\. Results of inference
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-3\. 推断结果
- en: This is just the plain, converted model without any optimizations for mobile
    added. In the next step you’ll explore how to optimize this model for mobile devices.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是普通的、转换后的模型，没有为移动设备添加任何优化。接下来，您将探索如何为移动设备优化此模型。
- en: Step 3\. Optimize the Model
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 3\. 优化模型
- en: Now that you’ve seen the end-to-end process of training, converting, and using
    a model with the TensorFlow Lite interpreter, let’s look at how to get started
    with optimizing and quantizing the model.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经看到了训练、转换和使用TensorFlow Lite解释器的端到端过程，接下来我们将看看如何开始优化和量化模型。
- en: 'The first type of optimization, called *dynamic range quantization*, is achieved
    by setting the `optimizations` property on the converter, prior to performing
    the conversion. Here’s the code:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种优化类型，称为*动态范围量化*，是通过在转换器上设置`optimizations`属性来实现的，在执行转换之前进行设置。以下是代码：
- en: '[PRE22]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'There are several optimization options available at the time of writing (more
    may be added later). These include:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时有几个可用的优化选项（稍后可能会添加更多）。这些选项包括：
- en: '`OPTIMIZE_FOR_SIZE`'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`OPTIMIZE_FOR_SIZE`'
- en: Perform optimizations that make the model as small as possible.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 执行优化，使模型尽可能小。
- en: '`OPTIMIZE_FOR_LATENCY`'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`OPTIMIZE_FOR_LATENCY`'
- en: Perform optimizations that reduce inference time as much as possible.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 执行优化，尽量减少推断时间。
- en: '`DEFAULT`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`DEFAULT`'
- en: Find the best balance between size and latency.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 找到在大小和延迟之间的最佳平衡。
- en: In this case, the model size was close to 9 MB before this step but only 2.3
    MB afterwards—a reduction of almost 70%. Various experiments have shown that models
    can be made up to 4× smaller, with a 2–3× speedup. Depending on the model type,
    however, there can be a loss in accuracy, so it’s a good idea to test the model
    thoroughly if you quantize like this. In this case, I found that the accuracy
    of the model dropped from 99% to about 94%.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，此步骤之前模型的大小接近9 MB，但之后仅为2.3 MB，几乎减少了70%。各种实验表明，模型可以缩小至原来的4倍，速度提高2到3倍。然而，根据模型类型的不同，可能会出现精度损失，因此如果像这样量化，建议对模型进行全面测试。在这种情况下，我发现模型的精度从99%降至约94%。
- en: You can enhance this with *full integer quantization* or *float16 quantization*
    to take advantage of specific hardware. Full integer quantization changes the
    weights in the model from 32-bit floating point to 8-bit integer, which (particularly
    for larger models) can have a huge impact on model size and latency with a relatively
    small impact on accuracy.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用*完整整数量化*或*float16量化*来优化这个模型，以利用特定的硬件优势。完整整数量化将模型中的权重从32位浮点数更改为8位整数，这对模型的大小和延迟（尤其是对于较大的模型）可能会产生巨大影响，但对准确性的影响相对较小。
- en: 'To get full integer quantization, you’ll need to specify a representative dataset
    that tells the convertor roughly what range of data to expect. Update the code
    as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得完整的整数量化，您需要指定一个代表性数据集，告诉转换器大致可以期待什么范围的数据。更新代码如下：
- en: '[PRE23]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Having this representative data allows the convertor to inspect data as it flows
    through the model and find where to best make the conversions. Then, by setting
    the supported ops (in this case to `INT8`), you can ensure that the precision
    is quantized in only those parts of the model. The result might be a slightly
    larger model—in this case, it went from 2.3 MB when using `convertor.optimizations`
    only to 2.8 MB. However, the accuracy went back up to 99%. Thus, by following
    these steps you can reduce the model’s size by about two-thirds, while maintaining
    its accuracy!
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这些代表性数据使得转换器能够在数据通过模型时检查，并找到最佳的转换点。然后，通过设置支持的操作（在这种情况下设置为`INT8`），可以确保精度仅在模型的这些部分进行量化。结果可能是一个稍大一些的模型——在这种情况下，使用`convertor.optimizations`时从
    2.3 MB 增加到了 2.8 MB。然而，精确度提高到了 99%。因此，通过遵循这些步骤，您可以将模型的大小减少约三分之二，同时保持其准确性！
- en: Summary
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter you got an introduction to TensorFlow Lite and saw how it is
    designed to get your models ready for running on smaller, lighter devices than
    your development environment. These include mobile operating systems like Android,
    iOS, and iPadOS, as well as mobile Linux-based computing environments like the
    Raspberry Pi and microcontroller-based systems that support TensorFlow. You built
    a simple model and used it to explore the conversion workflow. You then worked
    through a more complex example, using transfer learning to retrain an existing
    model for your dataset, converting it to TensorFlow Lite, and optimizing it for
    a mobile environment. In the next chapter you’ll take this knowledge and explore
    how you can use the Android-based interpreter to use TensorFlow Lite in your Android
    apps.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您了解了 TensorFlow Lite，并看到它是如何设计来使您的模型能够在比开发环境更小、更轻的设备上运行的。这些设备包括移动操作系统如
    Android、iOS 和 iPadOS，以及移动 Linux 环境，比如树莓派和支持 TensorFlow 的微控制器系统。您构建了一个简单的模型，并使用它来探索转换工作流程。然后，您通过一个更复杂的例子来学习，使用迁移学习重新训练现有模型以适应您的数据集，将其转换为
    TensorFlow Lite，并对移动环境进行优化。在下一章中，您将深入探讨如何使用基于 Android 的解释器在您的 Android 应用中使用 TensorFlow
    Lite。
