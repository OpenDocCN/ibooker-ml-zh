- en: Chapter 1\. Privacy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一章 隐私
- en: If you’ve been paying any attention to the media, then you’re at least somewhat
    aware of the damage that can follow when a company’s customer data or proprietary
    algorithms are leaked. Given that the field of machine learning (ML) requires
    enormous amounts of data almost by definition, the risk is especially glaring.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有关注媒体报道，那么你至少对一家公司的客户数据或专有算法泄露可能导致的损害有所了解。考虑到机器学习（ML）领域基本上需要大量数据，风险尤为突出。
- en: Attack Vectors for Machine Learning Pipelines
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习流水线的攻击向量
- en: Shortly after computers were invented, methods for attacking them were invented.
    To illustrate this, the MITRE corporation has created a taxonomy of tactics and
    techniques used by hackers to attack systems.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机发明不久后，攻击方法就被发明出来了。为了说明这一点，MITRE公司创建了一个攻击者用来攻击系统的战术和技术分类法。
- en: 'The emergence of machine learning created a bunch of additional ways in which
    computer systems could be attacked. In fact, there’s a machine learning–specific
    version of MITRE ATT&CK: [MITRE ATLAS (Adversarial Threat Landscape for Artificial-Intelligence
    Systems)](https://atlas.mitre.org). Just as attackers and adversaries have sought
    to steal data from and control computer systems in general, machine learning pipelines
    are faced with the same risks.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的出现创造了许多额外的方式，通过这些方式可以攻击计算机系统。事实上，MITRE ATT&CK有一个专门针对机器学习的版本：[MITRE ATLAS（人工智能系统的对抗威胁景观）](https://atlas.mitre.org)。正如攻击者和对手一直试图从普通计算机系统中窃取数据并控制它们一样，机器学习流水线也面临着同样的风险。
- en: This chapter goes into a series of techniques and technologies that can mitigate
    the risk of privacy leaks. While these techniques represent the intersection of
    practical best practices and state-of-the-art research, no tool is perfect. Some
    of these technologies can backfire if not properly implemented or if you focus
    on only one definition of privacy.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了一系列可以减少隐私泄露风险的技术和技术。尽管这些技术代表了实际最佳实践和最新研究的交集，但没有一种工具是完美的。如果没有正确实施或者只关注隐私的一个定义，其中一些技术可能会产生反效果。
- en: 'Improperly Implemented Privacy Features in ML: Case Studies'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中实施不当的隐私功能：案例研究
- en: Before we dive into mathematical privacy definitions, let’s first get an understanding
    of what improperly implemented privacy features look like in the real world and
    what consequences might arise from them.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨数学隐私定义之前，让我们先了解一下在现实世界中实施不当的隐私功能是什么样子，以及可能引起的后果。
- en: A lot of the data privacy laws described are aimed at punishing data leaks.
    Where laws do not deter people, organizational and technological safeguards are
    needed. All of these are designed to place an enormous cost on obtaining the data
    in question. The problem is that for some bad actors, the value of the data still
    far exceeds the time and monetary costs in obtaining it.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 描述的许多数据隐私法律旨在惩罚数据泄露。在法律无法阻止人们的情况下，需要组织和技术保障措施。所有这些措施旨在给获取相关数据的人造成巨大成本。问题在于，对于一些不良行为者来说，数据的价值仍远远超过获取它所需的时间和金钱成本。
- en: On the consumer side, in China there’s an [extensive black market for personal
    data](https://oreil.ly/E2rkk). Malicious actors can buy mobile phone location
    and movement data, credit information, academic records, and phone records for
    as little as $0.01 (though these will fetch higher prices depending on the individual).
    For a data breach of thousands or millions of individuals, the financial incentive
    becomes clear. Information like healthcare records typically fetches more. According
    to Experian, a single patient record can sell for [upwards of $1,000 on the black
    market](https://oreil.ly/0p5Je), depending on how complete the record is; this
    is nearly 50 times higher than standard credit card records.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 就消费者而言，在中国存在着一个大量的个人数据黑市。恶意行为者可以以极低的价格购买移动电话定位和移动数据、信用信息、学术记录和电话记录，每个数据可能仅需$0.01（尽管根据个体情况价格会更高）。对于数以千计或百万计的个人数据泄露，财务激励显而易见。像医疗保健记录这样的信息通常会更值钱。据Experian称，一条单个患者记录在黑市上可能卖到高达$1,000以上，具体取决于记录的完整性；这几乎比标准信用卡记录高出50倍。
- en: There is also a large market for proprietary company information. It’s difficult
    to quantify the value of having your competitors’ information. In most cases it’s
    pretty high, especially if the information is the data that their analytics pipeline
    was trained on or the mission-critical models that were trained over hundreds
    or thousands of computing hours.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 还存在大量专有公司信息的市场。很难量化获取竞争对手信息的价值。在大多数情况下，这是非常高的，特别是如果这些信息是他们分析管道训练的数据或经过数百或数千小时计算训练的关键模型的话。
- en: Of course, there’s much more than money to be gained from stealing information.
    Nation-state actors may have motivations ranging from achieving clear cut national-security
    objectives to gathering blackmail material to causing destabilization, or even
    the more vague principle that “It’s better to have data and not need it than to
    need data and not have it.”
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，从窃取信息中获利不仅仅是金钱问题。国家行为者可能有从实现明确的国家安全目标到收集敲诈材料、引起不稳定，甚至更模糊的“最好是有数据而不需要，也不是需要数据而没有”的原则的动机。
- en: As of this writing, we haven’t seen attacks on machine learning models on a
    scale comparable to some of the larger data leaks (though comparing yourself favorably
    to [Meta’s breach of 530 million users’ information](https://oreil.ly/8g84r) is
    a low bar). Part of the reason is that the usual routes of attacks on unsecured
    frontends and backends are still easy enough to be profitable. If a product or
    service has removed much of the low-hanging fruit, hackers may turn to attacks
    on ML models themselves to get what they want.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 就目前而言，我们还没有看到与一些较大的数据泄露相媲美的规模的机器学习模型攻击（尽管将自己与[Meta泄露的5.3亿用户信息](https://oreil.ly/8g84r)进行比较是一个低水平）。部分原因是攻击未安全防护的前端和后端的常规途径仍然足够容易获利。如果某产品或服务已经移除了大部分低
    hanging fruit，黑客们可能会转向攻击ML模型本身来获取他们想要的东西。
- en: 'Case 1: Apple’s CSAM'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 情况1：苹果的CSAM
- en: Apple made headlines in 2021 when it announced [a new system for tackling child
    abuse and child trafficking](https://oreil.ly/H26d8). The Child Sexual Abuse Material
    (CSAM) detection system was originally planned for release with iOS 15. The most
    notable feature of the system was an on-device ML model that would check all photos
    sent and received for CSAM, as well as on-device matching and tagging of photos
    before sending to iCloud. This matching would be done via [Apple’s NeuralHash
    algorithm](https://oreil.ly/oIw1A). Inspired by the [checksum hash matching for
    determining software integrity](https://oreil.ly/VvpNE), the model would base
    the image hash on the presence or absence of certain high-level details in the
    photo.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 2021年，苹果宣布了[一种新系统，用于应对儿童虐待和儿童贩运](https://oreil.ly/H26d8)，这一消息成为头条。儿童性虐待材料（CSAM）检测系统最初计划在iOS
    15发布时发布。该系统最显著的特点是一个设备端ML模型，将检查所有发送和接收的照片中的CSAM，并在将照片发送到iCloud之前进行设备端匹配和标记。这种匹配将通过[苹果的NeuralHash算法](https://oreil.ly/oIw1A)完成。该模型受[用于确定软件完整性的校验和哈希匹配](https://oreil.ly/VvpNE)的启发，将基于照片中特定高级细节的存在与否来生成图像哈希。
- en: The key detail here is the use of *on-device* networks to do the matching. Instead
    of collecting data from all devices, storing it on a central oracle, and then
    running an ML model on the collected data, the NeuralHash model would only be
    run at the user endpoint and alert Apple if a certain threshold of hits were detected.
    In theory, this would allow the system to respect end-to-end encryption while
    still being able to run the model on customer data. Unfortunately, the general
    public did not take kindly to this approach and saw it as an invasion of privacy.
    Much can be said about the public relations debacle stemming from Apple scanning
    private photos while labeling itself as a “privacy first” company, but we’ll focus
    on the much more important technical errors in the CSAM-scanning system.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键细节是使用*设备端*网络进行匹配。与收集所有设备数据、存储在中央预言机上，然后在收集的数据上运行ML模型不同，NeuralHash模型仅在用户端点运行，并在检测到一定阈值的命中时向苹果发出警报。理论上，这将允许系统尊重端到端加密，同时仍能在客户数据上运行模型。不幸的是，公众并不认同这种方法，并认为这是一种侵犯隐私的行为。关于苹果在标榜自己为“隐私第一”的公司的同时扫描私人照片所引发的公共关系危机，有很多讨论，但我们将专注于CSAM扫描系统中更为重要的技术错误。
- en: Apple’s first mistake was putting so much stake in the integrity of the NeuralHash
    algorithm. Hashing algorithms used in security contexts typically go through decades-long
    competitions before being adopted as standards. The exact behavior of a neural
    network in all possible scenarios is impossible to verify with certainty. In fact,
    shortly after the release of NeuralHash, users created collision attacks that
    could add imperceptible modifications to any photo to make the network identify
    the image as offensive content.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 苹果的第一个错误是过度依赖神经哈希算法的完整性。在安全环境中使用的哈希算法通常经过几十年的竞争才被采纳为标准。无法确切验证神经网络在所有可能情况下的确切行为。事实上，就在神经哈希发布不久后，用户就创造了碰撞攻击，可以对任何照片进行微小修改，使网络识别图像为不良内容。
- en: The second mistake was a perceived lack of control of the training data by journalists,
    developers, and security engineers following the situation. Apple claimed that
    it was only training the NeuralHash algorithm to match photo features found in
    law enforcement databases. In countries like the US, state and federal law enforcement
    agencies maintain databases of confiscated child exploitation, and arresting pedophiles
    is generally an uncontroversial subject in most of the world. However, Apple products
    and services are sold in [over 52 countries as of 2020](https://oreil.ly/1fFrj).
    Much of this distribution is dependent on Apple cooperating with governments.
    What happens if some nation wants to scan for something different? For example,
    what if an authoritarian government or political faction wants to use NeuralHash
    to scan for slogans of opposition parties or images of opposition politicians
    or activists?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个错误是记者、开发人员和安全工程师在此事件后对训练数据控制的感知缺失。苹果声称仅训练神经哈希算法以匹配扣押儿童色情照片数据库中的照片特征。在像美国这样的国家，州和联邦执法机构维护着被没收的儿童色情数据库，逮捕恋童癖者在大部分世界上都是一个不受争议的主题。然而，截至2020年，苹果产品和服务在[超过52个国家销售](https://oreil.ly/1fFrj)。这种分布很大程度上依赖于苹果与各国政府的合作。如果某个国家希望用于扫描其他内容会发生什么呢？例如，如果某个威权政府或政治派别希望使用神经哈希来扫描反对党的口号或反对党政治家或活动人士的图像？
- en: This lack of specificity in the NeuralHash algorithm, plus a public lack of
    confidence that its use would not be restricted to Apple’s narrow stated aims,
    eventually made Apple [delay (though not completely cancel) the release of this
    feature](https://oreil.ly/vGFvg).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 苹果在[推迟（尽管未完全取消）发布此功能](https://oreil.ly/vGFvg)时，主要是因为神经哈希算法缺乏具体性，加上公众对其使用范围不仅限于苹果狭隘声明目标的信心不足。
- en: 'Case 2: GitHub Copilot'
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 案例2：GitHub Copilot
- en: In June 2021, and in partnership with OpenAI, GitHub released Copilot, a tool
    that can autocomplete code based on training on public GitHub repos. Copilot is
    run by an ML model called Codex, itself based on OpenAI’s GPT-3 (except trained
    on code instead of raw text). As a consequence, Codex can take a raw-text prompt
    and convert it to working code in a variety of programming languages. While it
    can’t completely replace human programmers, Codex is adept at solving the kinds
    of algorithm problems one could expect in a whiteboard interview at Meta, Apple,
    Amazon, Netflix, or Alphabet’s Google.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 2021年6月，GitHub与OpenAI合作发布了Copilot，这是一个工具，可以根据在公共GitHub存储库上的训练自动完成代码。Copilot由一个名为Codex的ML模型运行，它本身基于OpenAI的GPT-3（但是训练的是代码而不是原始文本）。因此，Codex可以接受原始文本提示，并在多种编程语言中生成工作代码。虽然它不能完全替代人类程序员，但Codex擅长解决像在Meta、Apple、Amazon、Netflix或Alphabet的Google的白板面试中可以预期的算法问题。
- en: Codex’s generalization ability is impressive, but it carries some of the [same
    issues as GPT-3’s model](https://oreil.ly/b5jFZ), which has been shown to be susceptible
    to memorizing when asked to complete particularly rare or unusual prompts. Codex
    has the same issue, except some of the information it has memorized is either
    copyrighted code or accidentally exposed secrets.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Codex的泛化能力令人印象深刻，但它也带有与[GPT-3模型相同的一些问题](https://oreil.ly/b5jFZ)，即在要求完成特别罕见或不寻常提示时，它易于记忆。Codex也有同样的问题，只不过它记住的信息可能是受版权保护的代码或意外暴露的秘密。
- en: This issue with exposed secrets was first reported by a SendGrid engineer who
    demonstrated that if you asked Copilot for API keys (the same kinds of keys that
    would grant selective access to mission-critical databases), [Copilot would show
    them](https://oreil.ly/wz4dE). Soon after, people discovered that they could prompt
    Codex for secrets, like AWS secret keys (e.g., someone could get privileged access
    to the AWS backends used by entire companies) or cryptocurrency wallet secret
    keys (e.g., a Bitcoin secret key would allow someone to steal any amount of Bitcoin
    contained in that wallet, potentially worth millions of dollars).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 首次报告此问题的是一位 SendGrid 工程师，他演示了如果您向 Copilot 请求 API 密钥（同样类型的密钥可以授予对重要数据库的有选择访问权限），[Copilot
    会显示它们](https://oreil.ly/wz4dE)。不久之后，人们发现他们可以向 Codex 请求秘密，比如 AWS 的秘密密钥（例如，某人可以获得对整个公司使用的
    AWS 后端的特权访问）或者加密货币钱包的秘密密钥（例如，比特币的秘密密钥将允许某人窃取该钱包中的任意数量比特币，可能价值数百万美元）。
- en: There are a few approaches to solving this problem. One would be to search for
    API keys within the training data codebase and censor them. Replacing hashes and
    passwords with the same X for each character would be easy, though the process
    of finding every single exposed password, hash, and API key would be much harder
    and its success could never be guaranteed. Also, legal questions were raised about
    Copilot’s training data and outputs. Many open source developers were rankled
    by GitHub’s unauthorized and unlicensed use of copyrighted source code as training
    data for the model and began moving away from GitHub on these grounds. It’s not
    always provable whether the outputs are based on proprietary code, but there have
    been some obvious examples. [In one particularly blatant case](https://oreil.ly/UydLa),
    Copilot could reproduce Carmack’s famous inverse-square-root function from the
    game *Quake 3*. Even a skilled C developer would be unlikely to come up with this
    solution from scratch, but the copying is made more obvious by the inclusion of
    someone’s code comments.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种解决此问题的方法。一种方法是在训练数据代码库中搜索 API 密钥并对其进行审查。用相同的 X 替换哈希和密码每个字符将会很容易，尽管找到每个暴露的密码、哈希和
    API 密钥的过程将会更加困难，其成功也无法保证。此外，关于 Copilot 的训练数据和输出引发了法律问题。许多开源开发者对 GitHub 未经授权和未经许可使用受版权保护的源代码作为模型训练数据感到愤怒，并基于这些理由开始远离
    GitHub。不总是可以证明输出是基于专有代码的，但确实存在一些明显的例子。[在一个尤为明目张胆的案例中](https://oreil.ly/UydLa)，Copilot
    可以复制 Carmack 在游戏 *Quake 3* 中著名的反平方根函数。即使是熟练的 C 开发者也不太可能从头开始想出这个解决方案，但通过包含某人的代码注释，复制变得更加明显。
- en: This is a much trickier problem to solve; it’s not susceptible to just censoring
    small numbers of characters. A simple approach would have been to exclude codebases
    with certain kinds of license files from the training corpus. However, it’s not
    clear whether including other codebases based on the absence of such files really
    counts as informed consent. Software IP lawyer [Kate Downing argued](https://oreil.ly/OiZQQ)
    that while the creation of Copilot might be technically legal, there is still
    much that needs to be settled in a court of law (not to mention the situation
    is still morally questionable). This is because GitHub has for years offered licenses
    like GNU General Public License (GPL) versions 2 and 3. However, they’ve never
    really advertised that you can choose one license now and another later or that
    users are given different permissions in the future. Both of these are features
    of GitHub, and had users been made more aware, they might not have granted GitHub
    such far-reaching permissions with the users’ code. Given how many open source
    developers are leaving GitHub because of this usage, it’s likely that many would
    not have consented to this use of their code.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个更加棘手的问题需要解决；它不仅仅是删除少量字符就能解决的。一个简单的方法可能是从训练语料库中排除带有特定类型许可证文件的代码库。然而，是否应该根据缺少这类文件来包含其他代码库，是否算作知情同意却并不明确。软件知识产权律师[Kate
    Downing 提出](https://oreil.ly/OiZQQ)，尽管创造 Copilot 可能在技术上是合法的，但在法庭上仍有许多问题需要解决（更别提在道德上仍然具有争议）。这是因为
    GitHub 多年来一直提供诸如 GNU 通用公共许可证（GPL）第2版和第3版的许可证。然而，他们从未真正宣传过你现在可以选择一种许可证，以后又可以选择另一种许可证，或者用户在未来可以获得不同的权限。这两个都是
    GitHub 的特点，如果用户被更多告知，他们可能不会授予 GitHub 如此广泛的权限来使用他们的代码。考虑到有多少开源开发者因为这种使用而离开 GitHub，很可能他们不会同意这种用法。
- en: 'Case 3: Model and Data Theft from No-Code ML Tools'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 案例3：从无代码 ML 工具中窃取模型和数据
- en: Plenty of companies have been working on no-code models for training and deploying
    ML systems. For example, [Google’s Teachable Machine](https://oreil.ly/Hc648)
    and [Microsoft’s Lobe.ai](https://www.lobe.ai) offer ways for anyone to train
    computer vision models. For mobile or frontend developers with very little experience
    in machine learning, these tools might seem magical—but they’re perfect targets
    for a type of attack known as a *gray-box attack*.^([1](ch01.html#idm45621849607696))
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 许多公司一直在开发无代码模型，用于训练和部署机器学习系统。例如，[Google的Teachable Machine](https://oreil.ly/Hc648)和[Microsoft的Lobe.ai](https://www.lobe.ai)提供了任何人都可以训练计算机视觉模型的方式。对于在机器学习方面经验不足的移动端或前端开发人员来说，这些工具可能看起来像魔法一样——但它们是一种称为*灰盒攻击*的攻击类型的完美目标。^([1](ch01.html#idm45621849607696))
- en: Consider a project made with Lobe.ai, a tool that allows anyone, regardless
    of machine learning knowledge, to train a vision model on data from a regular
    file directory. If you wanted to train your model to determine whether someone
    is wearing a mask, you could simply take a set of images, cover them up with face
    masks, and make that the training data. However, a few Lobe.ai users demonstrated
    that its classifier is running a Resnet150V2 model. If you know the model, you
    can find out a lot of information about its model architecture, which makes it
    much easier to steal the model weights (these are the numbers assigned to neurons
    on a neural network that let it store the all-important patterns, functions, and
    information it has learned during the computationally intensive training). Such
    a theft would be dangerous for any organization that has spent many GPU-hours
    training a model and many human-hours iterating on and building a pipeline around
    it. After all, why spend all that time and money if it’s easier to steal a competitor’s
    proprietary model?
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑使用Lobe.ai制作的项目，这是一种允许任何人（无论其机器学习知识如何）在来自常规文件目录的数据上训练视觉模型的工具。如果您想训练您的模型以确定某人是否戴着口罩，您只需拿一组图像，用口罩遮住它们，将其作为训练数据。然而，少数Lobe.ai用户表明其分类器正在运行Resnet150V2模型。如果您了解该模型，您可以获取关于其模型架构的大量信息，这使得窃取模型权重（这些权重是神经网络上分配给神经元的数字，它们允许它在计算密集的训练期间存储所有重要的模式、函数和信息）变得更加容易。对于任何已经花费大量GPU小时训练模型并花费大量人力小时进行迭代和构建管道的组织来说，这种窃取都将是危险的。毕竟，如果窃取竞争对手的专有模型更容易，那为什么要花费所有的时间和金钱呢？
- en: This is not to say that no-code tools are not valuable, but only to raise the
    concerns that come about when someone knows a lot about the machine learning model
    in question. Countless organizations use out-of-the-box architectures that can
    be found as part of Keras or PyTorch. With companies selling their ML models as
    products to be used as API interfaces, some malicious actors may take the opportunity
    to steal the models themselves.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管无代码工具很有价值，但是当有人对所讨论的机器学习模型非常了解时，这时就会引发一些担忧。无数组织使用作为Keras或PyTorch的一部分可以找到的开箱即用的架构。随着公司将其机器学习模型作为产品出售，供作为API接口使用，一些恶意行为者可能会趁机窃取这些模型本身。
- en: Definitions
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义
- en: After seeing the preceding examples, you might think you have a pretty good
    understanding of what privacy is. When it comes to building privacy-preserving
    systems, definitions matter. In this section, we’ll go through some key terms
    that you’ll see throughout this book.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在看过前述例子之后，您可能会认为自己对隐私有了相当好的理解。但在构建保护隐私的系统时，定义非常重要。在本节中，我们将介绍一些您将在本书中看到的关键术语。
- en: Definition of Privacy
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 隐私的定义
- en: Privacy is defined by Merriam-Webster dictionary as [“the quality or condition
    of being secluded from the presence or view of others”](https://oreil.ly/LoLIb),
    or “the state of being free from public attention or unsanctioned intrusion.”
    This definition might make it seem like privacy is something that’s either “on
    or off,” but this is an oversimplification. If we have data that’s not viewable
    by anyone for any reason (not even the application that would use the data), that’s
    technically private but functionally useless for most applications. There is a
    lot of middle ground between data being completely open and completely closed.
    Because privacy in practical settings falls on a continuum instead of being binary,
    we need ways of measuring it.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 根据韦伯斯特词典的定义，隐私是[“远离他人视线或视图的质量或条件”](https://oreil.ly/LoLIb)，或者“摆脱公众关注或未经授权的侵入状态”。这个定义可能会让人觉得隐私要么是“有或无”，但这是一个过于简化的看法。如果我们有数据因任何原因对任何人都不可见（甚至不是使用数据的应用程序），那从技术上来说是私有的，但对大多数应用程序来说功能上是无用的。在数据不完全开放和完全关闭之间有许多中间地带。因为在实际环境中隐私是一个连续的过程而不是二元的，所以我们需要一些衡量方式。
- en: Proxies and Metrics for Privacy
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代理和隐私度量
- en: 'Measuring privacy is a separate matter from defining it. One review, by Isabel
    Wagner and David Eckhoff,^([2](ch01.html#idm45621850872048)) classified many of
    the measures out there into categories: adversarial success, indistinguishability,
    data similarity, accuracy and precision, uncertainty, information gain/loss, and
    time spent.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 测量隐私是一个与定义隐私分开的问题。伊莎贝尔·瓦格纳和大卫·艾克霍夫^([2](ch01.html#idm45621850872048))进行的一项回顾将许多现有的度量分类为敌对成功、不可区分性、数据相似性、准确性和精确度、不确定性、信息增益/损失和时间消耗等类别。
- en: Adversarial success
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 敌对成功
- en: Assume that some kind of hostile party (we’ll refer to them as the *adversary*)
    wants to get the contents of whatever data we have or communication we’re sending
    or receiving. We don’t want them to see or piece together that information. What
    are the adversary’s chances of succeeding?
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 假设某种敌对方（我们将其称为*敌手*）想要获取我们拥有的任何数据或我们发送或接收的通信的内容。我们不希望他们看到或拼凑出这些信息。敌手成功的机会是多少？
- en: 'This is a very general category of privacy metrics. We don’t know the goals,
    knowledge, capabilities, or tools at the adversary’s disposal. The adversary could
    be anyone or anything: a curious user, a corporate spy, a nation-state spy, a
    lone thief, a company’s preventative penetration tester, or even a DEFCON conference
    attendee who will just mock you for not securing your data or communication correctly.^([3](ch01.html#idm45621850244480))
    The adversary could be a complete outsider with no knowledge of the technical
    backend, or they could already know exactly what protocols or techniques you’re
    using.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这是隐私度量的一个非常普遍的类别。我们不知道敌手的目标、知识、能力或工具。敌手可能是任何人或任何事物：一个好奇的用户，一个企业间谍，一个国家间谍，一个孤独的小偷，一家公司的预防性渗透测试员，甚至是只是嘲笑你没有正确保护数据或通信的DEFCON会议与会者^([3](ch01.html#idm45621850244480))。敌手可能是一个完全不了解技术后端的外部人，或者他们可能已经确切地了解你正在使用的协议或技术。
- en: Given how vague and open-ended this metric is, there are other definitions that
    build on this concept of anticipating an attack from an adversary.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这种度量的模糊性和开放性，还有其他建立在预期敌手攻击概念基础上的定义。
- en: Indistinguishability
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不可区分性
- en: Indistinguishability refers to how well an adversary can distinguish between
    two entities in a process or dataset. This definition of privacy is the focus
    of private ML techniques like differential privacy (see [“k-Anonymity”](#k-an-ch1-sect)).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 不可区分性是指敌手在流程或数据集中分辨两个实体的能力。隐私的这个定义是私有机器学习技术（如差分隐私，见[“k-匿名性”](#k-an-ch1-sect)）的焦点。
- en: Data similarity
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据相似性
- en: Privacy definitions based on data similarity focus on how easily the features
    and subgroups within the data can be separated (e.g., distinguishing one person’s
    records from another person’s). This is the focus of ML privacy techniques like
    k-anonymity (which we discuss in [“Differential Privacy”](#dp-section)).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 基于数据相似性的隐私定义侧重于数据中的特征和子组的分离程度（例如，区分一个人的记录和另一个人的记录）。这是机器学习隐私技术如k-匿名性（我们在[“差分隐私”](#dp-section)中讨论）的焦点。
- en: Accuracy and precision
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准确性和精确度
- en: Accuracy-based metrics of privacy focus on the accuracy and precision of an
    adversary’s estimate of the data or communication. This could involve using metrics
    like F1 scores, precision, or recall to gauge how closely the adversary has estimated
    the bits of information of your data. If the adversary’s estimate is less accurate,
    the privacy is greater.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 针对隐私的基于准确度的度量侧重于对手对数据或通信的估计的准确性和精度。这可能涉及使用F1分数、精确度或召回率等度量标准，以评估对手对数据信息的估计有多接近。如果对手的估计不够准确，隐私性就更高。
- en: Uncertainty
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不确定性
- en: Metrics of uncertainty assume that greater uncertainty means an adversary has
    a lesser chance of violating privacy promises. The greater the degree of error
    or entropy in the adversary’s estimate of the true information, the more private
    it is. This shares some similarities with accuracy-based metrics, though they
    should not be confused. *Accuracy* is the proximity of a reading to its actual
    value, whereas *uncertainty* relates to the outliers and anomalies that may skew
    accuracy readings.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性度量假设更大的不确定性意味着对手违反隐私承诺的可能性较小。对手对真实信息的估计存在较大误差或熵时，其隐私性就越高。这与基于准确度的度量有些相似，尽管它们不应混淆。*准确度*是读数接近实际值的程度，而*不确定性*则涉及可能偏离准确读数的异常和离群值。
- en: Information gain/loss
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 信息增益/损失
- en: Information gain/loss metrics measure how much an adversary can gain or lose
    from the data. If less information can be gained, then privacy is greater. This
    metric differs slightly from uncertainty, since it takes into account how much
    information the attacker had at the start.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 信息增益/损失度量衡量对手从数据中可以获取或丢失多少信息。如果可以获取的信息较少，则隐私性较高。这个度量标准与不确定性略有不同，因为它考虑了攻击者在开始时已经掌握的信息量。
- en: Time spent
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 花费时间
- en: A motivated adversary may try to violate privacy repeatedly until they succeed.
    Definitions of privacy based on time assume that privacy mechanisms will inevitably
    fail, but certain privacy protection mechanisms would require more time-investment
    to break than others. ML privacy techniques like homomorphic encryption (which
    we cover in [“Homomorphic Encryption”](#he-ch1-sect)) work with this definition
    of privacy.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一个积极主动的对手可能会反复尝试违反隐私，直到成功为止。基于时间的隐私定义假设隐私机制最终会失败，但某些隐私保护机制要比其他机制需要更多的时间投资才能破解。像同态加密这样的ML隐私技术（我们在[“同态加密”](#he-ch1-sect)中讨论）就是依据这种隐私定义工作的。
- en: Legal Definitions of Privacy
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 隐私的法律定义
- en: The aforementioned proxies and metrics for privacy are good ways of assigning
    a number to how private a system is. While this landscape is useful, we need to
    know where to draw lines—but part of that decision may already be made for you.
    If you’re releasing any machine learning–based product, you are by definition
    going to be dealing with someone’s data. As such, you will invariably run into
    the boundaries of privacy laws.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 上述隐私的代理和度量是评估系统隐私性的好方法。虽然这一领域很有用，但我们需要知道在哪里划线——但这个决定的一部分可能已经为您做出。如果您发布任何基于机器学习的产品，您就会无可避免地涉及到某些人的数据。因此，您将不可避免地遇到隐私法律的界限。
- en: k-Anonymity
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k-匿名
- en: The concept of k-anonymity, first proposed by Pierangela Samarati and Latanya
    Sweeney in 1998,^([4](ch01.html#idm45621849762320)) can be thought of as a specific
    version of “hiding in the crowd.” It relies on increasing the uncertainty that
    a given record belongs to a certain individual. For this to happen, a dataset
    needs at least k individuals who share common values for the set of attributes
    that might identify them. K-anonymity is a powerful tool when used correctly.
    It’s also one of the precursors to more advanced privacy tools like differential
    privacy (discussed in [“Differential Privacy”](#dp-section)).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: k-匿名的概念最早由Pierangela Samarati和Latanya Sweeney在1998年提出，^([4](ch01.html#idm45621849762320))可以被视为“众多中的隐藏”的一个特定版本。它依赖于增加一个给定记录属于某个特定个体的不确定性。为了实现这一点，数据集至少需要k个个体共享一组可能用于识别他们的属性。在正确使用时，k-匿名是一个强大的工具。它也是更高级隐私工具如差分隐私的先驱之一（详见[“差分隐私”](#dp-section)）。
- en: Types of Privacy-Invading Attacks on ML Pipelines
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习管道上侵犯隐私的攻击类型
- en: You should have a good conceptual overview now of what privacy is from a machine
    learning perspective, why it’s important, and how it can be violated in an ML
    pipeline. When it comes to violating privacy, outside attackers have a variety
    of tools at their disposal. The biggest general categories of attacks are membership
    attacks (identifying the model’s training data), model inversion (using the model
    to steal proprietary data), and model theft (exactly what it sounds like).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你应该从机器学习的角度有了对隐私的良好概念性概述，为什么它很重要，以及它如何在ML管道中被侵犯。当涉及到侵犯隐私时，外部攻击者可以利用多种工具。攻击的最大一般类别包括成员攻击（识别模型的训练数据），模型反演（使用模型窃取专有数据）和模型盗窃（确切地说就是这样）。
- en: Membership Attacks
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成员攻击
- en: One of the privacy risks of machine learning models is that an adversary may
    be able to reconstruct the data used in the model creation.^([5](ch01.html#idm45621850884208)))
    The Membership Inference Attack is the process of determining whether a sample
    comes from the training dataset of a trained ML model or not. For the company
    whose ML model is being attacked, this could mean an adversary gaining insight
    into how a proprietary model was constructed, or even where the input data is
    located (especially risky if it is coming from a poorly secured external server).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型的一个隐私风险是对手可能能够重构用于模型创建的数据。^([5](ch01.html#idm45621850884208)) 成员推断攻击是确定样本是否来自训练ML模型的数据集的过程。对于被攻击的公司来说，这可能意味着对手能够洞悉专有模型的构建方式，甚至是输入数据的位置（特别是如果来自安全性较差的外部服务器，则风险更大）。
- en: 'There are three main models used in a membership attack:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在成员攻击中使用的三个主要模型：
- en: '*The target model*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*目标模型*'
- en: This is the model trained on the initial dataset. The model outputs confidence
    levels for each class, and the one with the highest confidence value is the chosen
    output class. The membership inference attack is based on the idea that samples
    from the training dataset would have higher average confidence value in their
    actual class than samples not seen in training.^([6](ch01.html#idm45621850686608))
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在初始数据集上训练的模型。该模型为每个类别输出置信水平，具有最高置信值的类别被选择为输出类别。成员推断攻击基于这样一个思想：训练数据集中的样本在其实际类别中具有更高的平均置信值，而不在训练中看到的样本则不然。^([6](ch01.html#idm45621850686608))
- en: '*The shadow model*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*影子模型*'
- en: In black-box conditions, an attacker cannot do statistical analysis on the confidence
    levels because they do not have access to the training dataset. The shadow models
    are an ensemble of models (which may or may not be exact copies of the model’s
    architecture and hyperparameters) designed to mimic the behavior of the target
    model. Once the shadow models have been trained, the attacker can generate training
    samples for the attack models.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在黑盒条件下，攻击者无法对置信水平进行统计分析，因为他们无法访问训练数据集。影子模型是一组模型（可能是模型架构和超参数的精确或非精确副本），旨在模仿目标模型的行为。一旦影子模型训练完成，攻击者可以为攻击模型生成训练样本。
- en: '*The attack model*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*攻击模型*'
- en: This is the model that will predict whether a sample is from the training set
    or not. The inputs for the attack models are the confidence levels, and the output
    label is either “in” or “out.”
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型将预测样本是否来自训练集。攻击模型的输入是置信水平，输出标签是“in”或“out”。
- en: Unless the defender has a very specific unfavorable setup, a membership inference
    attack is an illusory threat.^([7](ch01.html#idm45621850558528)) This is especially
    so compared to attacks that are much better at stealing training data, or even
    the machine learning model itself.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 除非防御者有非常特定的不利设置，否则成员推断攻击是一种虚幻的威胁。^([7](ch01.html#idm45621850558528)) 这尤其如此，与更擅长窃取训练数据，甚至是机器学习模型本身的攻击相比。
- en: Model Inversion
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型反演
- en: Most early attacks of this type were too time-consuming for what little information
    was gained from them. Membership attacks might seem low risk for most ML applications,
    but there are far more dangerous types. A reconstruction attack takes membership
    attack principles further by reconstructing the data used in training an ML model.
    This can be used to directly steal the information of individuals whose data was
    used in training or reveal enough about how a model interprets data to find ways
    of breaking it further.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数早期的此类攻击由于从中获得的信息少而耗时过长。对大多数ML应用程序来说，成员攻击可能风险较低，但还存在更危险的类型。重建攻击通过重建用于训练ML模型的数据，将成员攻击原则推向更深入的程度。这可以直接用于窃取训练数据中使用的个体的信息，或者揭示关于模型如何解释数据的足够信息，以找到进一步破坏其的方法。
- en: First proposed in 2015,^([8](ch01.html#idm45621849826816)) a model inversion
    attack is a much more direct way of stealing data. Rather than determining whether
    an input is part of a dataset, this kind of attack reconstructs very exact representations
    of actual data. In the original paper, this technique was used on a classifier
    trained on several faces (see [Figure 1-1](#inversion-attack)). Rather than exhausting
    every possible pixel value that could belong to an individual, this [technique
    used gradient descent](https://oreil.ly/QmDIa) to train pixel values to match
    one of the model’s output classes.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 首次提出于2015年，^([8](ch01.html#idm45621849826816)) 模型反演攻击是一种更直接的窃取数据的方法。与确定输入是否属于数据集不同，这种攻击重建了实际数据的非常精确的表示。在原始论文中，该技术用于一个分类器训练的几个面部（见[图1-1](#inversion-attack)）。与穷举每个可能的像素值可能属于个体不同，这种[技术使用梯度下降](https://oreil.ly/QmDIa)来训练像素值以匹配模型输出的一个类别。
- en: '![ptml 0101](assets/ptml_0101.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0101](assets/ptml_0101.png)'
- en: Figure 1-1\. Original face image (right) and restored one through model inversion
    (left) (from Fredrikson et al.)
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-1。原始面部图像（右）和模型反演恢复的图像（左）（来自Fredrikson等人）
- en: It’s worth noting that the data that is returned by this model inversion attack
    is an average representation of the data that belongs to the specific class in
    question. In the presented setting, it does not allow for an inversion of individual
    training data points. In Fredrikson et al., however, every individual within the
    face classifier represents their own class. Therefore, the attack can be used
    in order to retrieve information about individuals and violate their privacy.
    This is especially the case in applications like facial recognition, where you
    only need a face that triggers the same keypoint recognition, not one that looks
    like an actual face.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，通过这种模型反演攻击返回的数据是属于特定类别的数据的平均表示。在所呈现的设置中，它不允许对单个训练数据点进行反演。然而，在Fredrikson等人的研究中，每个面部分类器中的个体都代表其自己的类别。因此，该攻击可以用来获取关于个体的信息并侵犯其隐私。在应用如面部识别的情况下尤其如此，您只需要一个触发相同关键点识别的面部，而不是一个看起来像实际面部的面部。
- en: Model inversion attacks have gotten much more sophisticated since 2015, when
    this technique was first demonstrated. Even worse, the concepts of model inversion
    have been extended to steal much more than just the data.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 自2015年首次展示此技术以来，模型反演攻击已经变得更加复杂。更糟糕的是，模型反演的概念已扩展到窃取不仅仅是数据。
- en: Model Extraction
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型提取
- en: Model extraction goes several steps further. Instead of just reconstructing
    the input data for a model, a model extraction attack involves stealing the entire
    model. This kind of attack was first described in “Stealing Machine Learning Models
    via Prediction APIs.”^([9](ch01.html#idm45621865229008)) Model theft attacks can
    range from just stealing the hyperparameters of a model,^([10](ch01.html#idm45621849442128))
    to outright stealing the model weights.^([11](ch01.html#idm45621850789216)) [Figure 1-2](#attack-overview)
    gives a general overview of the model-stealing attack. The attacker approximates
    the gradients that would cause the model to output the predictions it’s giving.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 模型提取迈出了几步。与仅重建模型输入数据不同，模型提取攻击涉及窃取整个模型。这种类型的攻击首次描述于“通过预测API窃取机器学习模型”。^([9](ch01.html#idm45621865229008))
    模型窃取攻击的范围可以从仅窃取模型的超参数^([10](ch01.html#idm45621849442128))，到直接窃取模型权重。^([11](ch01.html#idm45621850789216))
    [图1-2](#attack-overview) 概述了模型窃取攻击的一般情况。攻击者近似梯度，导致模型输出其当前预测。
- en: Developing high-performing models is expensive. More than just the computational
    cost (which can be millions of dollars for some models), there’s also the cost
    of acquiring the massive and likely private dataset. Devising a novel training
    method is also intellectually taxing. With all this in mind, a malicious actor
    may decide to just extract the model itself.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 开发高性能模型是昂贵的。除了计算成本（对于某些模型可能达到数百万美元）之外，还有获取大规模且可能是私有数据集的成本。设计新颖的训练方法也是知识上的负担。考虑到所有这些因素，恶意行为者可能决定直接提取模型本身。
- en: 'The model extraction process typically involves three steps:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 模型提取过程通常包括三个步骤：
- en: Gathering a dataset to query the victim model
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集数据集以查询受害模型
- en: Recording predictions from the API on these data points
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录API在这些数据点上的预测
- en: Training a surrogate model to mimic the victim
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练替代模型以模仿受害者
- en: '![ptml 0102](assets/ptml_0102.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0102](assets/ptml_0102.png)'
- en: Figure 1-2\. General overview of the structure of a model-stealing attack
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-2\. 模型窃取攻击结构的一般概述
- en: There can be enormous variation in this attack pattern. Early model extraction
    attacks were highly dependent on which dataset was chosen for the queries. The
    surrogate model could have wildly different accuracy depending on whether CIFAR10,
    CIFAR100, or MNIST was chosen, for example. More recent attack mechanisms forego
    this choice of attack altogether by feeding in noise from controlled probability
    distributions. Different choices of probability distribution can change the number
    of queries needed in step 2 to approach a satisfactory surrogate. In step 3, an
    attacker may know nothing about the model architecture (i.e., “black-box” attack),
    or they may have some details about the architecture (i.e., “gray-box” attack).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这种攻击模式可能有很大的变化。早期的模型提取攻击高度依赖于选择哪个数据集用于查询。例如，根据选择CIFAR10、CIFAR100还是MNIST，替代模型的准确性可能大不相同。更近期的攻击机制完全放弃了攻击选择，通过从控制概率分布中引入噪声来进行。在第2步中，不同的概率分布选择可以改变需要的查询数量，以接近一个令人满意的替代模型。在第3步中，攻击者可能对模型架构一无所知（即“黑盒”攻击），或者他们可能对架构的一些细节有所了解（即“灰盒”攻击）。
- en: The end result is still the same. The surrogate model uses gradient approximation
    conditioned by how similar the surrogate’s output probabilities are to the victim
    model’s output probabilities.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果仍然相同。替代模型使用梯度近似，条件是替代输出概率与受害模型的输出概率有多相似。
- en: 'If you have access to a computer vision model’s output logits, this information
    leakage has enormous potential for abuse.^([12](ch01.html#idm45621849715952))
    These techniques take advantage of the fundamental properties of convolutional
    neural networks. This means that any kind of pipeline that uses them, not just
    those that train on images, is at risk. This was seen in the case of graph neural
    networks in “Model Extraction Attacks on Graph Neural Networks: Taxonomy and Realization.”^([13](ch01.html#idm45621850186960))'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您可以访问计算机视觉模型的输出对数，这种信息泄露具有巨大的滥用潜力。^([12](ch01.html#idm45621849715952)) 这些技术利用卷积神经网络的基本属性。这意味着任何使用它们的管道，不仅仅是训练图像的管道，都面临风险。这在“图神经网络上的模型提取攻击：分类和实现”案例中有所体现。^([13](ch01.html#idm45621850186960))
- en: Much of what makes computer vision models vulnerable is the reuse of common
    architectures. Common machine learning libraries contain pre-built versions of
    networks like ResNet and InceptionV3 (see the PyTorch and Keras Model Zoos). Even
    worse, many of these models can be loaded with ImageNet weights. Fine-tuning a
    computer vision model gives potential attackers much more information to work
    with when stealing model weights. An attacker has the starting conditions of the
    weights and doesn’t need to reconstruct the architecture from scratch. Because
    of this partial foreknowledge of the neural network, some of these attacks are
    gray-box attacks.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉模型易受攻击的部分原因是常见架构的重复使用。常见的机器学习库包含预构建的网络版本，如ResNet和InceptionV3（参见PyTorch和Keras模型动物园）。更糟糕的是，许多这些模型可以使用ImageNet权重加载。微调计算机视觉模型使潜在攻击者在窃取模型权重时获得更多信息。攻击者拥有权重的起始条件，无需从头开始重建架构。由于对神经网络部分先验知识的掌握，一些攻击是灰盒攻击。
- en: Stealing a BERT-Based Language Model
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 窃取基于BERT的语言模型
- en: This section was inspired by demonstrations of NLP model theft by the CleverHans
    team and the latest techniques for stealing weights from BERT models.^([14](ch01.html#idm45621849855216))^,^([15](ch01.html#idm45621851858192))^,^([16](ch01.html#idm45621849983808))
    In this section, we explore training a text classifier with differential privacy
    by taking a model pre-trained on public text data and fine-tuning it for a task.
    The first step in this process is to train the BERT model.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 本节灵感来自CleverHans团队对NLP模型窃取的演示，以及从BERT模型中窃取权重的最新技术。^([14](ch01.html#idm45621849855216))^,^([15](ch01.html#idm45621851858192))^,^([16](ch01.html#idm45621849983808))
    在本节中，我们探讨了通过使用在公共文本数据上预训练过的模型，并对其进行微调以执行任务的差分隐私文本分类器训练。此过程的第一步是训练BERT模型。
- en: Note
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You can find all the code associated with this tutorial in the [*BERT_attack*
    notebook](https://oreil.ly/fFiQ5).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[*BERT_attack* notebook](https://oreil.ly/fFiQ5)中找到与本教程相关的所有代码。
- en: When training a model with differential privacy, one almost always faces a trade-off
    between model size and accuracy on the task. The fewer parameters the model has,
    the easier it is to get a good performance with differential privacy.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用差分隐私训练模型时，几乎总会面临模型大小与任务准确性之间的权衡。模型参数越少，使用差分隐私时获得良好性能就越容易。
- en: Most state-of-the-art NLP models are quite deep and large (BERT-base has over
    100 million parameters), which makes training text models on private datasets
    challenging. One to address this problem is to divide the training process into
    two stages. First, the model is pre-trained on a public dataset, exposing the
    model to generic text data. Assuming that the generic text data is public, we
    will not be using differential privacy at this step. Then, most of the layers
    are frozen, leaving only a few upper layers to be trained on the private dataset
    using DP-SGD. This approach is the best of both worlds—it produces a deep and
    powerful text-understanding model, while only training a small number of parameters
    with a differentially private algorithm.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数最先进的NLP模型非常深且庞大（BERT-base具有超过1亿个参数），这使得在私有数据集上训练文本模型具有挑战性。解决此问题的一种方法是将训练过程分为两个阶段。首先，在公共数据集上预训练模型，将模型暴露于通用文本数据中。假设通用文本数据是公开的，在此阶段我们将不使用差分隐私。然后，冻结大部分层，只留下少数上层，在私有数据集上使用DP-SGD进行训练。这种方法融合了两种最佳实践——产生深度且强大的文本理解模型，同时只对少数参数使用差分隐私算法。
- en: This tutorial will take the pre-trained BERT-base model and fine-tune it to
    recognize sentiment classification on the IMDB movie review dataset.^([17](ch01.html#idm45621850610176))
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程将采用预训练的BERT-base模型，并将其微调以在IMDB电影评论数据集上识别情感分类。^([17](ch01.html#idm45621850610176))
- en: '[PRE0]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: BERT (Bidirectional Encoder Representations from Transformers) is a state-of-the-art
    approach to various NLP tasks. It uses a transformer architecture and relies heavily
    on the concept of pre-training. We’ll use a pre-trained BERT-base model, provided
    in a HuggingFace transformers repository. It gives us a PyTorch implementation
    for the classic BERT architecture, as well as a tokenizer and weights pre-trained
    on Wikipedia, a public English corpus.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: BERT（双向编码器表示转换器）是用于各种NLP任务的先进方法。它使用变换器架构，并且在概念上依赖于预训练。我们将使用一个在HuggingFace变换器库中提供的预训练BERT-base模型。它提供了经过PyTorch实现的经典BERT架构，以及在维基百科（一个公共英语语料库）上预训练的分词器和权重。
- en: The model has the following structure. It uses a combination of word, positional,
    and token embeddings to create a sequence representation, then passes the data
    through 12 transformer encoders, and finally uses a linear classifier to produce
    the final label. As the model is already pre-trained and we only plan to fine-tune
    a few upper layers, we want to freeze all layers, except for the last encoder
    and above (BertPooler and Classifier). [Figure 1-3](#bert-overview) shows the
    BERT model’s architecture.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型具有以下结构。它使用单词、位置和标记嵌入的组合来创建序列表示，然后通过12个变换器编码器传递数据，最后使用线性分类器生成最终标签。由于模型已经预训练，我们只计划对几个上层进行微调，因此希望冻结所有层，除了最后一个编码器及以上的层（BertPooler和Classifier）。[图 1-3](#bert-overview)展示了BERT模型的架构。
- en: '![ptml 0103](assets/ptml_0103.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0103](assets/ptml_0103.png)'
- en: Figure 1-3\. BERT architecture
  id: totrans-98
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-3\. BERT架构
- en: Thus, by using a pre-trained model, we reduce the number of trainable parameters
    from over 100 million to just above 7.5 million. This will help both performance
    and convergence with added noise. Here is the code that trains the model.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过使用预训练模型，我们将可训练参数的数量从超过 1 亿减少到略高于 750 万。这将有助于提高性能并在增加噪声的同时加快收敛速度。以下是训练模型的代码。
- en: '[PRE1]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The inference of this system is where our model theft opportunity lies. Let’s
    try to run inference on the Yelp polarity dataset.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 此系统的推断是我们模型盗窃的机会所在。让我们尝试在 Yelp 极性数据集上运行推断。
- en: '[PRE3]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This training scheme will result in a model that produces outputs that behave
    very similarly to the original model.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这种训练方案将产生一个输出行为与原始模型非常相似的模型。
- en: Defenses Against Model Theft from Output Logits
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 防御模型输出日志的模型盗窃
- en: If models can be reconstructed using their output logits alone, then this bodes
    poorly for model security. Fortunately, there are two modes of defending against
    this kind of inference attack.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可以仅通过其输出 logits 重建模型，则这对模型安全性不利。幸运的是，有两种防御方式来抵御这种推断攻击。
- en: The first type of defense is to make it costly to query the model. Xuanli He
    et al.^([18](ch01.html#idm45621859477040)) explored the real-world use of public
    datasets to steal model weights. Based on the sizes of these datasets and the
    costs of Google and IBM’s language model APIs (assuming these are the lower bounds
    of an API call cost), they came up with the cost estimates shown in [Table 1-1](#table-questions)
    for using those datasets to steal a BERT-based language model.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种防御方法是增加查询模型的成本。Xuanli He 等人^([18](ch01.html#idm45621859477040)) 探讨了使用公共数据集窃取模型权重的实际应用。基于这些数据集的大小和
    Google 和 IBM 语言模型 API 的成本（假设这些是 API 调用成本的下限），他们提出了在 [表 1-1](#table-questions)
    中显示的使用这些数据集窃取基于 BERT 的语言模型的成本估算。
- en: Table 1-1\. Attack cost estimates
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1-1\. 攻击成本估算
- en: '| Dataset | Number of queries | Google price | IBM price |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 查询次数 | Google 价格 | IBM 价格 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| TP-US | 22,142 | $22.10 | $66.30 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| TP-US | 22,142 | $22.10 | $66.30 |'
- en: '| Yelp | 520 K | $520.00 | $1,560.00 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Yelp | 520 K | $520.00 | $1,560.00 |'
- en: '| AG | 112 K | $112.00 | $336.00 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| AG | 112 K | $112.00 | $336.00 |'
- en: '| Blog | 7,098 | $7.10 | $21.30 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 博客 | 7,098 | $7.10 | $21.30 |'
- en: Depending on the cloud provider, the cost of an attack could range from tens
    to thousands of dollars. The same research demonstrates that you wouldn’t even
    need to pick a matching transformer architecture to make a closely matching copycat
    model (e.g., training a DistilBERT model on the outputs of a BERT model is a viable
    attack option). As such, increasing the cost of a machine learning model API call
    will go far to protect against this kind of attack. (This was the strategy OpenAI
    took with GPT-3; thanks to the API call costs, the final price of mounting an
    inversion attack on the GPT-3 API would probably be more than that of training
    a GPT-3 model from scratch.)
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 根据云服务提供商的不同，攻击的成本可能从几十美元到几千美元不等。同一研究表明，甚至无需选择匹配的变压器架构，即可制作一个紧密匹配的模仿模型（例如，在 BERT
    模型的输出上训练 DistilBERT 模型是一种可行的攻击选项）。因此，增加机器学习模型 API 调用的成本将大大有助于防范此类攻击（这是 OpenAI
    在 GPT-3 上采取的策略；由于 API 调用成本，对 GPT-3 API 进行反演攻击的最终成本可能超过从头开始训练一个 GPT-3 模型的成本）。
- en: There’s a second (and more clever) type of defense. Much like how obscuring
    your face with frosted glass would frustrate facial recognition, one can also
    add obfuscating noise to the output logits. You can either add in the output noise
    during the model training,^([19](ch01.html#idm45621852089536)) or you can take
    an ordinary trained model and add random noise to the prediction probabilities
    afterward.^([20](ch01.html#idm45621852087568)) This “prediction poisoning” additive
    noise is the strategy we’ll demonstrate in the next section.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有第二种（更巧妙的）防御方式。就像用磨砂玻璃遮挡你的脸会阻碍面部识别一样，你也可以向输出 logits 添加混淆噪声。你可以在模型训练期间加入输出噪声^([19](ch01.html#idm45621852089536))，或者在普通训练模型的预测概率后添加随机噪声^([20](ch01.html#idm45621852087568))。这种“预测污染”的添加噪声是我们将在下一节中展示的策略。
- en: Note
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Given how scary model theft is, and how creative attackers can be, this is an
    area of constant research. There are ways of spotting an attack in progress.^([21](ch01.html#idm45621852083888))^,^([22](ch01.html#idm45621852081840))
    There are also methods for “hardening” your training data samples.^([23](ch01.html#idm45621852080432))
    You can confuse model theft attacks further simply by using ensembles of models.^([24](ch01.html#idm45621852078528))
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到模型盗窃的危险性及攻击者的创造力，这是一个持续研究的领域。有方法可以发现正在进行的攻击。^([21](ch01.html#idm45621852083888))^,^([22](ch01.html#idm45621852081840))
    也有方法可以“加固”你的训练数据样本。^([23](ch01.html#idm45621852080432)) 你可以通过使用模型集成来进一步混淆模型盗窃攻击。^([24](ch01.html#idm45621852078528))
- en: All these proposed ideas and defense strategies can seem daunting if you’re
    trying to figure out the most important attack to defend against. This is especially
    the case if the research is very new and you haven’t heard of many successful
    real-world use cases. Ultimately, it may be worth simulating these attacks on
    your own system to see how they go.^([25](ch01.html#idm45621852076896))^,^([26](ch01.html#idm45621852074944))
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您试图弄清楚最重要的防御攻击，所有这些提议的想法和防御策略可能会显得令人生畏。特别是如果研究非常新，并且您还没有听说过许多成功的真实用例。最终，值得在您自己的系统上模拟这些攻击，以查看其效果。^([25](ch01.html#idm45621852076896))^,^([26](ch01.html#idm45621852074944))
- en: This is by no means a comprehensive assortment of attacks one could use to target
    an ML pipeline. As mentioned earlier, attackers will follow the path of least
    resistance. This will be made harder for attackers if you can incorporate some
    kind of privacy-testing tooling into your pipeline.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这绝不是可用于针对ML管道进行攻击的全面选择。如前所述，攻击者将选择最不费力的路径。如果您能将某种形式的隐私测试工具整合到您的管道中，将会使攻击者的工作更加困难。
- en: Privacy-Testing Tools
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 隐私测试工具
- en: 'The Google Cloud Platform (GCP) has a tool for computing the [k-anonymity of
    a given dataset](https://oreil.ly/7KA2v). The exact computation method can be
    done from the GCP console, a GCP protocol, Java, Node.js, Python, Go, PHP, or
    C#. Further Python examples of this can be found on the Google [python-dlp GitHub](https://oreil.ly/CsTc1).
    Other Python modules for k-anonymization include:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud平台（GCP）具有用于计算给定数据集的[k-匿名性](https://oreil.ly/7KA2v)的工具。精确的计算方法可以通过GCP控制台、GCP协议、Java、Node.js、Python、Go、PHP或C#完成。更多关于此的Python示例可以在Google的[python-dlp
    GitHub](https://oreil.ly/CsTc1)上找到。其他用于k-匿名化的Python模块包括：
- en: '[Nuclearstar/K-Anonymity](https://oreil.ly/lrfGa)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[Nuclearstar / K-匿名](https://oreil.ly/lrfGa)'
- en: Clustering-based k-anonymity implementation
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 基于聚类的K-匿名实现
- en: '[qiyuangong/Clustering_based_K_Anon](https://oreil.ly/aRZA1)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[qiyuangong / 基于聚类的K-匿名](https://oreil.ly/aRZA1)'
- en: Another clustering-based k-anonymity implementation
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种基于聚类的K-匿名实现
- en: '[qiyuangong/Mondrian](https://oreil.ly/pcU5J)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[qiyuangong / Mondrian](https://oreil.ly/pcU5J)'
- en: Python implementation for Mondrian multidimensional k-anonymity
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Mondrian多维K-匿名的Python实现
- en: '[kedup/python-datafly](https://oreil.ly/ijJcJ)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[kedup / python-datafly](https://oreil.ly/ijJcJ)'
- en: Python implementation of Datafly algorithm for k-anonymity on tabular data
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 用于表格数据K-匿名的Datafly算法的Python实现
- en: 'Additional privacy-testing tools include:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 其他隐私测试工具包括：
- en: '[PrivacyRaven](https://oreil.ly/vCrSc), created by Trail of Bits'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PrivacyRaven](https://oreil.ly/vCrSc)，由Trail of Bits创建'
- en: '[TensorFlow Privacy](https://oreil.ly/EDXl7), created by TensorFlow'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TensorFlow隐私](https://oreil.ly/EDXl7)，由TensorFlow创建'
- en: '[Machine Learning Privacy Meter](https://oreil.ly/p6Gcs), created by NUS Data
    Privacy and Trustworthy Machine Learning Lab'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习隐私计量器](https://oreil.ly/p6Gcs)，由新加坡国立大学数据隐私和可信机器学习实验室创建'
- en: '[CypherCat (archive-only)](https://oreil.ly/LFBGy), created by IQT Labs/Lab
    41'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CypherCat（仅存档）](https://oreil.ly/LFBGy)，由IQT Labs / Lab 41创建'
- en: '[Adversarial Robustness Toolbox (ART)](https://oreil.ly/7kUTP), created by
    IBM'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[对抗鲁棒性工具箱（ART）](https://oreil.ly/7kUTP)，由IBM创建'
- en: The [Machine Learning Privacy Meter](https://oreil.ly/itB0m), a tool to quantify
    the privacy risks of machine learning models with respect to inference attacks,
    notably membership inference attacks
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习隐私计量器](https://oreil.ly/itB0m)，一种工具，用于量化机器学习模型在推断攻击（特别是成员推断攻击）方面的隐私风险'
- en: Methods for Preserving Privacy
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保护隐私的方法
- en: Just as there are multiple ways to steal information from an ML model, there
    are multiple approaches for making that theft hard to the point where it’s impractical.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 就像从ML模型中窃取信息有多种方法一样，确保这种窃取变得难以实施的方法也有多种。
- en: Differential Privacy
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 差分隐私
- en: Differential privacy (DP) is a method for sharing insights about a dataset by
    using high-level patterns of subgroups within the data while masking or omitting
    data about specific individuals. The main assumption behind DP is that if the
    effect of making a single change in the data is small enough, then it’s difficult
    to reliably extract information about the individual from queries.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私（DP）是一种通过使用数据内子群体的高级模式来分享数据集洞见的方法，同时掩盖或省略特定个体的数据。DP背后的主要假设是，如果对数据进行单一更改的影响足够小，则从查询中提取有关个体的信息将变得困难。
- en: DP can be thought of as an extension of concepts like k-anonymity. The difference
    is that differential privacy is often extended to much higher dimensional data.
    Most modern implementations draw on what’s known as <math alttext="epsilon"><mi>ϵ</mi></math>
    -differential privacy.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私可以被视为诸如 k-匿名性等概念的扩展。其不同之处在于，差分隐私通常被扩展到更高维度的数据上。大多数现代实现都依赖于所谓的 <math alttext="epsilon"><mi>ϵ</mi></math>
    -差分隐私。
- en: 'Suppose <math alttext="epsilon"><mi>ϵ</mi></math> is a real number and <math
    alttext="script upper A"><mi>𝒜</mi></math> is a randomized algorithm that takes
    in a dataset as an input. <math alttext="upper D 1"><msub><mi>D</mi> <mn>1</mn></msub></math>
    and <math alttext="upper D 2"><msub><mi>D</mi> <mn>2</mn></msub></math> refer
    to any two datasets that differ by a change to just one element (e.g., the data
    of one person). The algorithm <math alttext="script upper A"><mi>𝒜</mi></math>
    provides <math alttext="epsilon"><mi>ϵ</mi></math> -differential privacy for all
    possible <math alttext="upper D 1"><msub><mi>D</mi> <mn>1</mn></msub></math> and
    <math alttext="upper D 2"><msub><mi>D</mi> <mn>2</mn></msub></math> combos, and
    for all subsets of the possible outputs of <math alttext="script upper A"><mi>𝒜</mi></math>
    :'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 <math alttext="epsilon"><mi>ϵ</mi></math> 是一个实数，<math alttext="script upper
    A"><mi>𝒜</mi></math> 是一个以数据集作为输入的随机化算法。 <math alttext="upper D 1"><msub><mi>D</mi>
    <mn>1</mn></msub></math> 和 <math alttext="upper D 2"><msub><mi>D</mi> <mn>2</mn></msub></math>
    指的是仅改变一个元素（例如，一个人的数据）的两个数据集。算法 <math alttext="script upper A"><mi>𝒜</mi></math>
    对所有可能的 <math alttext="upper D 1"><msub><mi>D</mi> <mn>1</mn></msub></math> 和 <math
    alttext="upper D 2"><msub><mi>D</mi> <mn>2</mn></msub></math> 组合，以及 <math alttext="script
    upper A"><mi>𝒜</mi></math> 的可能输出子集，提供 <math alttext="epsilon"><mi>ϵ</mi></math>
    -差分隐私：
- en: <math alttext="upper P left-parenthesis script upper A left-parenthesis upper
    D 1 right-parenthesis element-of upper S right-parenthesis less-than-or-equal-to
    exp left-parenthesis epsilon right-parenthesis ModifyingAbove upper P With dot
    left-parenthesis script upper A left-parenthesis upper D 1 right-parenthesis element-of
    upper S right-parenthesis" display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <mi>𝒜</mi>
    <mrow><mo>(</mo> <msub><mi>D</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mo>∈</mo>
    <mi>S</mi> <mo>)</mo></mrow> <mo>≤</mo> <mo form="prefix">exp</mo> <mrow><mo>(</mo>
    <mi>ϵ</mi> <mo>)</mo></mrow> <mover accent="true"><mi>P</mi> <mo>˙</mo></mover>
    <mrow><mo>(</mo> <mi>𝒜</mi> <mrow><mo>(</mo> <msub><mi>D</mi> <mn>1</mn></msub>
    <mo>)</mo></mrow> <mo>∈</mo> <mi>S</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper P left-parenthesis script upper A left-parenthesis upper
    D 1 right-parenthesis element-of upper S right-parenthesis less-than-or-equal-to
    exp left-parenthesis epsilon right-parenthesis ModifyingAbove upper P With dot
    left-parenthesis script upper A left-parenthesis upper D 1 right-parenthesis element-of
    upper S right-parenthesis" display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <mi>𝒜</mi>
    <mrow><mo>(</mo> <msub><mi>D</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mo>∈</mo>
    <mi>S</mi> <mo>)</mo></mrow> <mo>≤</mo> <mo form="prefix">exp</mo> <mrow><mo>(</mo>
    <mi>ϵ</mi> <mo>)</mo></mrow> <mover accent="true"><mi>P</mi> <mo>˙</mo></mover>
    <mrow><mo>(</mo> <mi>𝒜</mi> <mrow><mo>(</mo> <msub><mi>D</mi> <mn>1</mn></msub>
    <mo>)</mo></mrow> <mo>∈</mo> <mi>S</mi> <mo>)</mo></mrow></mrow></math>
- en: There are a variety of specific techniques for implementing differential privacy.
    These include additive noise mechanisms like the Laplace mechanism, randomized
    responses for local differential privacy, and feeding data through some kind of
    Hamming distance-preserving transformation. This formulation is designed to make
    sure that privacy is robust in the face of post-processing and that, if faced
    with highly correlated features, it can at least degrade gracefully and noticeably.
    Another bonus of differential privacy is its usefulness in defending against certain
    kinds of model extraction attacks.^([27](ch01.html#idm45621852730304))
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种具体的技术用于实现差分隐私。这些包括像拉普拉斯机制这样的加性噪声机制，用于本地差分隐私的随机响应，以及通过某种保持汉明距离的转换来传递数据。这种形式设计的目的是确保在后处理中隐私具有鲁棒性，并且如果面对高度相关的特征，至少能够逐渐和显著地退化。差分隐私的另一个优点是它在防御某些模型提取攻击方面的有用性。^([27](ch01.html#idm45621852730304))
- en: Stealing a Differentially Privately Trained Model
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偷取经过差分隐私训练的模型
- en: We’ve discussed concepts like differential privacy and resilience to model theft.^([28](ch01.html#idm45621852722384))
    Here we will examine exactly how one would go about stealing model weights in
    a scenario like this. We can take a pre-trained network (done via differential
    privacy) and then see how it stands up to various types of attacks. Let’s take
    the BERT architecture from before and try training it using differential privacy.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了差分隐私和对模型盗窃的抵抗力等概念。^([28](ch01.html#idm45621852722384)) 在这里，我们将详细讨论如何在这种情况下窃取模型权重。我们可以采用一个经过差分隐私训练的预训练网络，然后看它如何抵御各种类型的攻击。让我们继续采用之前的BERT架构，并尝试使用差分隐私进行训练。
- en: Note
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You can find all the code associated with this tutorial in the [*Chapter_1_PyTorch_DP_Demo*
    notebook](https://oreil.ly/wZQjf). Much of this was written shortly before the
    release of the most recent version of Opacus v1.1.0 and the most recent version
    of PyTorch v11.0.0. These interactive code tutorials will be adjusted to reflect
    the most recent versions in the final release. And be warned, they require a lot
    of RAM.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[*Chapter_1_PyTorch_DP_Demo* notebook](https://oreil.ly/wZQjf)中找到与本教程相关的所有代码。大部分代码是在Opacus
    v1.1.0和PyTorch v11.0.0最新版本发布之前不久编写的。这些交互式代码教程将根据最终版本进行调整。请注意，它们需要大量的RAM。
- en: The main difference in this training, compared to our vanilla implementation,
    is that we’re using the Opacus library from Meta. This is a library that lets
    us incorporate differential privacy into PyTorch models. We modify a typical PyTorch
    DataLoader-based training process by defining and attaching the Opacus Privacy
    engine into the DataLoader object.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们的原始实现相比，这次训练的主要区别在于我们使用了Meta的Opacus库。这是一个允许我们将差分隐私整合到PyTorch模型中的库。我们通过在DataLoader对象中定义并附加Opacus隐私引擎，修改了典型的PyTorch
    DataLoader-based训练过程。
- en: '[PRE4]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Beyond the usual hyperparameters encountered in model training, DP introduces
    a privacy cost hyperparameter, which in turn benefits from larger batch sizes
    since the noise is scaled to the norm of one sample in the batch.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练中遇到的通常超参数之外，DP引入了一个隐私成本超参数，这又因为批量大小较大而受益，因为噪声被缩放到批量中一个样本的范数。
- en: '[PRE5]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The trade-off to consider is that this means an increasing batch size relative
    to the amount of noise epsilon grows at `O(sqrt(batch_size)`. Opacus has a peak
    memory footprint of `O(batch_size^2)` compared to a non-differentially private
    model. Fortunately, Opacus supports a hyperparameter called `virtual_batch_size`
    that can separate the gradient computation from the noise addition and parameter
    updates (at the cost of convergence and privacy guarantee).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 要考虑的权衡是，这意味着相对于噪声epsilon增长的批量大小增加为`O(sqrt(batch_size)`。 Opacus的峰值内存占用量为`O(batch_size^2)`，与非差分隐私模型相比。幸运的是，Opacus支持一个名为`virtual_batch_size`的超参数，可以将梯度计算与噪声添加和参数更新分离（以收敛性和隐私保证为代价）。
- en: '[PRE6]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Once the engine is built, we can train the model:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 引擎构建完成后，我们可以训练模型：
- en: '[PRE7]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For the test accuracy, you’ll notice that the noise comes at a cost. The higher
    the epsilon, the more protected the input data is, and the less accurate the final
    model is. What value one chooses for epsilon comes down to how much model accuracy
    one is willing to sacrifice for the sake of privacy. There are unfortunately no
    free lunches when it comes to implementing differential privacy.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 对于测试准确性，你会注意到噪声是有代价的。epsilon值越高，输入数据就越受保护，最终模型的准确性就越低。选择epsilon值取决于愿意为隐私而牺牲多少模型准确性。在实现差分隐私时，很遗憾没有免费午餐。
- en: Further Differential Privacy Tooling
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多的差分隐私工具
- en: We’ve established many definitions of differential privacy and listed multiple
    tools. For privacy-preserving AI, the OpenMined project has by far the most extensive
    ecosystem of implementations for PyTorch-based models.^([29](ch01.html#idm45621853006400))^,^([30](ch01.html#idm45621853004864))
    While OpenMined has a lot of tools for the PyTorch ecosystem, there are plenty
    of other PyTorch-based tools such as [Opacus](https://oreil.ly/Xtgzx) (as we discussed).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经确定了许多差分隐私的定义并列出了多个工具。对于保护隐私的AI，OpenMined项目拥有迄今为止最广泛的基于PyTorch模型的实现生态系统。^([29](ch01.html#idm45621853006400))^,^([30](ch01.html#idm45621853004864))
    虽然OpenMined为PyTorch生态系统提供了大量工具，但还有许多其他基于PyTorch的工具，如[Opacus](https://oreil.ly/Xtgzx)（正如我们所讨论的）。
- en: IBM has its own set of DP tools, which can be found in [IBM’s DP library](https://oreil.ly/CRixM).
    CleverHans for TensorFlow (and by extension, its Mr. Ed counterpart for PyTorch)
    has some of the most comprehensive tools for both DP and adversarial hardening.
    These include PATE, DP-SGD, Moments Accountant, Laplace and Exponential Mechanisms,
    and other such mechanisms we haven’t discussed here.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: IBM拥有其自己的一套DP工具，可以在[IBM的DP库](https://oreil.ly/CRixM)中找到。为TensorFlow提供的CleverHans（以及其PyTorch的Mr.
    Ed对应物）拥有一些最全面的工具，用于DP和对抗强化。这些包括PATE，DP-SGD，Moments Accountant，Laplace和Exponential
    Mechanisms等机制，我们在这里没有讨论。
- en: Homomorphic Encryption
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 同态加密
- en: 'Encrypting mission-critical data before storage is a standard best practice
    in any kind of high-stakes engineering. Homomorphic encryption (HE) is the conversion
    of data into ciphertext that can be analyzed and worked with as if it were still
    in its original form. The idea behind HE is to extend public-key cryptography
    by being able to run mathematical operations on the encrypted data without having
    access to the secret key. The output of the mathematical operation will still
    be encrypted. This technique has been in development for decades and may refer
    to one of several variants:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何高风险工程领域，将关键任务数据在存储前进行加密是一种标准的最佳实践。同态加密是将数据转换为密文，可以像原始数据一样进行分析和处理的技术。同态加密的理念是通过能够在加密数据上运行数学操作来扩展公钥密码学，而无需访问秘密密钥。数学操作的输出仍将是加密的。这种技术已经发展了数十年，可能指的是几种变体之一：
- en: '*Partially homomorphic encryption*'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '*部分同态加密*'
- en: The system can evaluate only one kind of encrypted operation (e.g., addition
    or multiplication).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 系统只能评估一种加密操作类型（例如，加法或乘法）。
- en: '*Somewhat homomorphic encryption*'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '*部分同态加密*'
- en: The system can evaluate two types of operations (e.g., both addition and multiplication)
    but only for a subset of the system.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 系统可以评估两种类型的操作（例如，加法和乘法），但仅限于系统的某个子集。
- en: '*Leveled fully homomorphic encryption*'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '*分级全同态加密*'
- en: The system can evaluate arbitrary computations made up of multiple layers of
    operations (though there are limits on how deep these operations can be nested).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 系统可以评估由多层操作组成的任意计算（尽管对这些操作可以嵌套多深有所限制）。
- en: '*Fully homomorphic encryption (FHE)*'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*全同态加密（FHE）*'
- en: This is the strongest (and ideal) form of HE. FHE allows the evaluation of arbitrary
    algorithms composed of multiple types of operations with no restrictions on the
    depth of the nesting.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最强大（也是理想的）加密形式。全同态加密允许对由多种操作类型组成的任意算法进行评估，而不限制嵌套深度。
- en: There are two big drawbacks to HE. The first is the need to carefully store
    the encryption keys responsible for encrypting and decrypting. This has been a
    problem in many other types of engineering for decades, and as such there is plenty
    of literature on how best to do this.^([31](ch01.html#idm45621852989088)) The
    second is that HE brings an enormous computation cost. In the early days, this
    was on the order of making programs take millions of times longer. More recently
    it has been reduced to the order of hundreds of times longer. There are many approaches
    to applying HE to machine learning. These range from encrypting the data, to encrypting
    the neural network or decision tree, to encrypting some combination of both.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: HE 存在两个主要缺点。首先是需要精心存储负责加密和解密的加密密钥。几十年来，这一问题在许多其他工程领域都是一个问题，因此关于如何最佳实施有大量文献可供参考。^([31](ch01.html#idm45621852989088))
    第二个问题是HE 带来的巨大计算成本。在早期阶段，这使程序运行时间增加了数百万倍。最近，计算成本已经降低到增加数百倍的程度。有许多方法可以将HE 应用到机器学习中。这些方法从数据加密，到神经网络或决策树加密，再到两者的组合加密，涵盖了多种应用场景。
- en: Like many privacy-preserving ML techniques, the OpenMined ecosystem has HE tools.
    These include a Python interface to TenSEAL, which is Microsoft’s SEAL library
    for homomorphic encryption.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 像许多保护隐私的机器学习技术一样，OpenMined 生态系统提供了HE 工具。这些工具包括一个针对TenSEAL 的Python 接口，TenSEAL
    是微软的同态加密库。
- en: Secure Multi-Party Computation
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全多方计算
- en: If full homomorphic encryption is limited by computational complexity, then
    the next best thing is secure multi-party computation (SMPC). The idea behind
    SMPC is having multiple parties compute a function on their inputs, all while
    keeping those inputs private. Rather than focusing on protection from an outside
    adversary or the protection of stored data, this privacy approach protects participants’
    privacy from each other.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果全同态加密受到计算复杂性的限制，那么下一个最好的选择就是安全多方计算（SMPC）。SMPC 的理念是多个参与方在保持各自输入私密的同时计算一个函数。与专注于防范外部对手或存储数据保护不同，这种隐私方法保护了参与者之间的隐私。
- en: 'Consider the following workflow: One takes original data, represented by the
    number 12. Each party involved gets some share of the data (such as 5 or 7), and
    computes some operation (e.g., “multiply by 3”). When the outputs are combined
    ( <math alttext="15 plus 21 equals 36"><mrow><mn>15</mn> <mo>+</mo> <mn>21</mn>
    <mo>=</mo> <mn>36</mn></mrow></math> ), the result is identical to the outcome
    of running the operation on the original data directly. If Party A and Party B
    are kept from knowing the final output 36, they cannot deduce the original data
    point 12. This is a super-simplified addition example, but now imagine this is
    a machine learning pipeline. Our original data is a bunch of user data instead
    of the number 12. Party A and B get shards or tranches of this data instead of
    the numbers 5 or 7. The operation they’re running is certainly multiplication,
    but it’s the large-scale matrix multiplication done when training a ResNet model.
    The goal behind SMPC is to be able to turn these outputs into a combined decision
    boundary.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下工作流程：一个人获取原始数据，用数字12表示。每个涉及的方都获取数据的一部分（如5或7），并执行某些操作（例如“乘以3”）。当输出合并时（ <math
    alttext="15 plus 21 equals 36"><mrow><mn>15</mn> <mo>+</mo> <mn>21</mn> <mo>=</mo>
    <mn>36</mn></mrow></math> ），结果与直接在原始数据上运行操作的结果相同。如果保持A方和B方不知道最终输出的36，则他们无法推断出原始数据点12。这是一个超简化的加法示例，但现在想象一下这是一个机器学习流水线。我们的原始数据是一堆用户数据，而不是数字12。A方和B方获得这些数据的片段，而不是数字5或7。他们正在运行的操作当然是乘法，但在训练ResNet模型时进行的是大规模的矩阵乘法。SMPC的目标是将这些输出转化为一个合并的决策边界。
- en: Being able to train models on aggregated data without allowing anyone access
    to that aggregated data would be extremely valuable, especially if the training
    data presents a bunch of security, privacy, policy, or legal risks. For example,
    medical researchers would be able to perform population studies on genetic data
    without needing to share data between research institutions. Being able to study
    the gender pay gap across companies would be much more tenable if salary data
    never actually left the companies in question.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 能够在聚合数据上进行模型训练，而无需让任何人访问这些聚合数据，将是非常宝贵的，特别是如果训练数据涉及一系列安全、隐私、政策或法律风险。例如，医学研究人员可以在基因数据上进行人口研究，而无需在研究机构之间共享数据。如果工资数据实际上从相关公司中永远不离开，研究公司间的性别工资差距将更为可行。
- en: Secure multi-party computation is sometimes used interchangeably with “remote
    execution” or “trusted execution.” These latter terms do not always describe secure
    multi-party computation, however. SMPC is a subset of “remote/trusted execution.”
    Full homomorphic encryption can be implemented within SMPC, but SMPC does not
    require it.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 安全多方计算有时与“远程执行”或“可信执行”互换使用。然而，后者并不总是描述安全多方计算。SMPC是“远程/可信执行”的一个子集。全同态加密可以在SMPC内实现，但SMPC并不需要它。
- en: SMPC Example
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SMPC 示例
- en: For ML systems that make use of the PyTorch ecosystem, one can use Facebook
    Research’s CrypTen library. The goal of CrypTen is to ensure that the server-to-server
    interactions required for SMPC can be implemented with minimal friction.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用PyTorch生态系统的ML系统，可以使用Facebook Research的CrypTen库。CrypTen的目标是确保实现SMPC所需的服务器间交互时最小化摩擦。
- en: Note
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You can see the full code for this tutorial in the accompanying Jupyter [*Chapter_1_SMPC_Example*
    notebook](https://oreil.ly/3niF4). This tutorial follows a pre-release version
    of OpenMined, and is based on code by Ayoub Benaissa (a prominent OpenMined contributor).
    The details will be finalized prior to publication, but until then this should
    not be used to secure important data. The code tutorial will be updated accordingly
    to demonstrate the best practices for the most up-to-date version of OpenMined
    until its release.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在附带的Jupyter [*Chapter_1_SMPC_Example* notebook](https://oreil.ly/3niF4) 中查看本教程的完整代码。本教程基于OpenMined的预发布版本，并基于Ayoub
    Benaissa（一位知名的OpenMined贡献者）的代码。详细信息将在出版前最终确定，但在此之前不应用于保护重要数据。代码教程将相应更新，以展示最新版本OpenMined的最佳实践。
- en: CrypTen was created with an “honest but curious” intruder in mind. Initially,
    it was built with internal participants in mind, not for protection against outside
    attackers. The OpenMined SMPC project extends CrypTen further, answering some
    of the unanswered questions in the original CrypTen announcement. Nothing changes
    about how CrypTen parties synchronize and exchange information. However, PySyft
    can be used to initiate the computation among workers, as well as exchange the
    final results between workers.^([32](ch01.html#idm45621851994768))
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: CrypTen的创建考虑到了“诚实但好奇”的入侵者。最初，它是为内部参与者而建立的，而不是为了防范外部攻击者。OpenMined SMPC项目进一步扩展了CrypTen，回答了原始CrypTen公告中一些未解答的问题。关于CrypTen各方如何同步和交换信息，没有任何改变。然而，PySyft可以用于启动工作人员之间的计算，并在工作人员之间交换最终结果。
- en: '[PRE8]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: For this deep dive, you’ll need to install both PySyft and CrypTen. You should
    also install MNIST using the MNIST_utils from `Crypten`. In addition, start two
    `GridNode`s with IDs `'ALICE'` and `'BOB'` listening to ports `'3000'` and `'3001'`,
    respectively. You can do this by initializing `GridNode` in two separate terminals.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这次深入学习，您需要同时安装PySyft和CrypTen。您还应使用`Crypten`的MNIST_utils安装MNIST。此外，在两个分别监听端口`'3000'`和`'3001'`的`GridNode`中启动两个`GridNode`，分别具有ID为`'ALICE'`和`'BOB'`。您可以在两个独立的终端中初始化`GridNode`来完成此操作。
- en: '[PRE9]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: For this tutorial, we can define a simple neural network in standard PyTorch.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本教程，我们可以在标准PyTorch中定义一个简单的神经网络。
- en: '[PRE10]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You can now connect to `ALICE` and `BOB` via their respective ports, followed
    by preparing and sending the data to the different workers (this is just for demonstration;
    in a real-life implementation, data would already be stored privately). If you’re
    using different ports or running workers in a remote machine, you should update
    the URLs.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以通过它们各自的端口连接到`ALICE`和`BOB`，然后准备并发送数据给不同的工作人员（这仅用于演示；在实际实现中，数据应该已经被私下存储）。如果您使用不同的端口或在远程机器上运行工作人员，则应更新URL。
- en: '[PRE11]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: With the workers set up, instantiate your model and create a placeholder input
    for building the entire CrypTen model.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 配置完工作人员后，实例化您的模型并创建一个占位符输入，用于构建整个CrypTen模型。
- en: '[PRE12]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Defining the CrypTen computation for training the neural network is relatively
    straightforward. You only need to decorate your training loop function with the
    `@run_multiworkers` decorator to run it across the different workers.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 定义用于训练神经网络的CrypTen计算相对简单。您只需要用`@run_multiworkers`装饰器装饰您的训练循环函数，就可以在不同的工作人员之间运行它。
- en: '[PRE13]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: You can now complete the distributed computation. This produces a dictionary
    containing the result from every worker, indexed by the rank of the party it was
    running. For instance, `result[0]` contains the result of party `0` that was running
    in `'alice'`, and `result[0][i]` contains the `i`th value, depending on how many
    values were returned.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以完成分布式计算。这将生成一个字典，其中包含每个工作人员的结果，由其运行的派对排名索引。例如，`result[0]`包含在`'alice'`上运行的派对`0`的结果，而`result[0][i]`包含根据返回的值数量而定的第`i`个值。
- en: '[PRE14]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The model output is a CrypTen model, but you can use PySyft to share the parameters
    as long as the model is not encrypted.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 模型输出是一个CrypTen模型，但只要模型未加密，您可以使用PySyft共享参数。
- en: '[PRE15]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Further SMPC Tooling
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更进一步的SMPC工具
- en: OpenMined has also been working on many non-ML applications of SMPC. For example,
    it has a demo project for using private set intersection to [alert individuals
    that they’ve been exposed to COVID-19](https://oreil.ly/LIH3G).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: OpenMined还在研究SMPC的许多非ML应用。例如，它有一个演示项目，用于使用私有集交集来警示个体，他们已经接触到COVID-19。
- en: Federated Learning
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 联邦学习
- en: Federated learning (FL) is a subset of secure multi-party computation.^([33](ch01.html#idm45621853550272))
    It can also be combined with other privacy-preserving ML techniques like differential
    privacy and HE. FL specifically refers to sending copies of a trainable model
    to wherever the data is located, training on this data at the source, and then
    recalling the training updates into one global model. At no point is the data
    itself aggregated into one database. Only the models, model updates, or pieces
    of the model are transferred.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习（FL）是安全多方计算的一个子集。它还可以与其他保护隐私的ML技术如差分隐私和HE结合使用。FL特指将可训练模型的副本发送到数据所在的任何位置，对此数据进行训练，然后将训练更新回收到一个全局模型中。在任何时候，数据本身都不会聚合到一个数据库中。只有模型、模型更新或模型片段会传输。
- en: Google used FL to improve text autocompletion in Android’s keyboard without
    exposing users’ text or uploading it to a cloud intermediary.^([34](ch01.html#idm45621853547984))
    Since 2019, Apple has been using FL to improve Siri’s voice recognition.^([35](ch01.html#idm45621853545472))
    As time goes on, more complex models have become trainable. Thanks to advances
    in offline reinforcement learning, it is also possible to do FL with reinforcement
    learning agents. FL is extremely attractive for any context where aggregating
    data is a liability, especially healthcare.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌使用 FL 提升 Android 键盘的文本自动补全功能，同时不会暴露用户的文本或将其上传到云中介。^([34](ch01.html#idm45621853547984))
    自 2019 年以来，苹果一直在使用 FL 提升 Siri 的语音识别能力。^([35](ch01.html#idm45621853545472)) 随着时间的推移，更复杂的模型也可以进行训练。由于离线强化学习的进展，现在还可以使用
    FL 来进行强化学习代理。
- en: FL can theoretically be implemented within CrypTen,^([36](ch01.html#idm45621853543232))
    but OpenMined has additional support for implementing federated learning in PyTorch.^([37](ch01.html#idm45621853507008))
    The TensorFlow ecosystem supports FL through [TensorFlow Federated](https://oreil.ly/wMMSm).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: FL 理论上可以在 CrypTen 中实现，^([36](ch01.html#idm45621853543232)) 但 OpenMined 提供了额外的支持来在
    PyTorch 中实现联邦学习。^([37](ch01.html#idm45621853507008)) TensorFlow 生态系统通过 [TensorFlow
    Federated](https://oreil.ly/wMMSm) 支持 FL。
- en: Warning
  id: totrans-205
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Technologies like differential privacy, FL, and SMPC are useful in general for
    stopping data leakage and securing ML models. However, this should not be confused
    with compliance with data privacy laws (some of which have specific lists of requirements,
    lists that do not mention any of these technologies yet). These technologies can
    help with compliance in some cases, but they do not grant automatic compliance,
    nor are they ever the only best security practice to use. For example, using FL
    in your ML pipeline is a good practice, but it will not automatically make you
    HIPAA compliant in the US.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私、FL 和 SMPC 等技术通常用于防止数据泄漏和保护机器学习模型的安全。但是，这不应与遵守数据隐私法律混淆（其中一些法律具有具体的要求列表，不包括这些技术）。在某些情况下，这些技术可以帮助遵守法律要求，但它们并不自动实现遵守，也不是唯一的最佳安全实践。例如，在美国使用
    FL 作为 ML 流程的一部分是一个良好的实践，但并不会自动使您符合 HIPAA 法规。
- en: Conclusion
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: You’ve learned that techniques like homomorphic encryption, federated learning,
    differential privacy, and secure multi-party computation are all different parts
    of the ML privacy stack (which itself is just one part of the cybersecurity space).
    These techniques encompass different areas in which data can leak, from data inputs
    to model parameters to decision outputs.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经了解到，同态加密、联邦学习、差分隐私和安全多方计算等技术都是机器学习隐私堆栈的不同部分（本身又仅是网络安全空间的一部分）。这些技术涵盖了数据泄漏的不同领域，从数据输入到模型参数再到决策输出。
- en: Several groups have begun combining these techniques. A recent collaboration
    between MIT, the Swiss Laboratory for Data Security, and several hospitals in
    Lausanne, Switzerland, demonstrated a real-world application of combining federated
    learning, differential privacy, homomorphic encryption, and multi-party computation
    into a combined analytics system (designated FAHME), shown in [Figure 1-4](#fahme-overview).^([38](ch01.html#idm45621853500992))
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 几个团体已经开始结合这些技术。最近 MIT、瑞士数据安全实验室以及瑞士洛桑的几家医院之间的合作展示了联合学习、差分隐私、同态加密和多方计算组合到一个联合分析系统中（称为
    FAHME），在 [图 1-4](#fahme-overview) 中展示。^([38](ch01.html#idm45621853500992))
- en: '![ptml 0104](assets/ptml_0104.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0104](assets/ptml_0104.png)'
- en: 'Figure 1-4\. System model and FAMHE workflow (credit: based on a figure from
    Froelicher et al.)'
  id: totrans-211
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-4\. 系统模型和 FAMHE 工作流程（来源：基于 Froelicher 等人的图）
- en: The collaborators used the FAHME system to conduct research in oncology and
    genetics. The purpose was to demonstrate that multiple institutions could collaborate
    without any one of them having access to the full data, without introducing any
    errors into the results. The final results were identical to those resulting from
    using the pooled dataset. The authors also showed that this is much easier and
    more accurate than using a meta-analysis, which involves working with summary
    statistics of datasets in the absence of the original data.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 合作者们使用 FAHME 系统进行了肿瘤学和遗传学研究。目的是展示多个机构可以合作而无需其中任何一个机构访问完整数据，同时也不会在结果中引入任何错误。最终结果与使用汇总数据集的结果完全一致。作者还表明，这比使用元分析更容易且更精确，后者在原始数据不可用时涉及使用数据集的汇总统计信息。
- en: 'The problem with a meta-analysis is getting around Simpson’s paradox. This
    is a problem where trends that appear in several groups of data disappear or reverse
    completely when the groups are combined. Fixing Simpson’s paradox in meta-analysis
    is a difficult problem,^([39](ch01.html#idm45621853495712)) but FAHME offers a
    promising solution: skip the meta-analysis stage entirely and work directly with
    the pooled data in encrypted form. In a FAHME workflow, a querier submits a differentially
    private query to the FAHME system, which uses HE in the computation of the results.
    The resulting analytics are combined with multi-party computation.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 使用元分析时的问题在于如何解决辛普森悖论。这是一个问题，即在多组数据中出现的趋势在合并后可能完全消失或反转。修正元分析中的辛普森悖论是一个困难的问题，^([39](ch01.html#idm45621853495712-marker))
    但 FAHME 提供了一个有希望的解决方案：完全跳过元分析阶段，直接使用加密形式的汇总数据进行工作。在 FAHME 工作流程中，查询者向 FAHME 系统提交具有差分隐私的查询，该系统在结果计算中使用同态加密。生成的分析结果与多方计算结合。
- en: This was a great real-world demonstration of the concepts discussed in this
    chapter. However, there’s much more to robust and trustworthy machine learning
    pipelines than just privacy.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对本章讨论的概念进行了出色的现实世界演示。然而，构建健壮可信的机器学习流水线远不止涉及隐私问题。
- en: ^([1](ch01.html#idm45621849607696-marker)) This term is derived from “black
    box” and “white box” attacks. While some people are avoiding these terms out of
    sensitivity for the unconscious bias they can introduce around Blackness and Whiteness,
    we were unable to find a wholly suitable alternative for this book and we still
    recommend outside resources that use this terminology. We hope that calling your
    attention to the potential for bias will prevent the perpetuation of it.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch01.html#idm45621849607696-marker)) 这个术语源自“黑盒子”和“白盒子”攻击。虽然一些人出于对黑色和白色所引入的潜意识偏见的敏感性，正在避免使用这些术语，但我们在本书中未能找到完全合适的替代方案，仍建议外部资源使用这些术语。我们希望提醒您注意偏见可能性，以免延续这种偏见。
- en: '^([2](ch01.html#idm45621850872048-marker)) Isabel Wagner and David Eckhoff,
    [“Technical Privacy Metrics: A Systematic Survey”](https://dl.acm.org/doi/10.1145/3168389),
    *ACM Computing Surveys (CSUR)* 51, no. 3 (2018): 1–38.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '^([2](ch01.html#idm45621850872048-marker)) Isabel Wagner 和 David Eckhoff，[“技术隐私度量：系统性调查”](https://dl.acm.org/doi/10.1145/3168389)，*ACM
    计算调查 (CSUR)* 51, no. 3 (2018): 1–38。'
- en: ^([3](ch01.html#idm45621850244480-marker)) See examples in this report on the
    [Hacker Wall of Shame](https://youtu.be/je-nq0lLiAs). You may also have heard
    preventative penetration testers called “white-hat” hackers, a name that comes
    from the white hats archetypically worn by protagonists in Western films.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch01.html#idm45621850244480-marker)) 请参阅此报告中关于[Hacker Wall of Shame](https://youtu.be/je-nq0lLiAs)的例子。您可能也听说过预防性渗透测试人员被称为“白帽”黑客，这个名称源于西部电影中主角典型穿着的白色帽子。
- en: '^([4](ch01.html#idm45621849762320-marker)) Pierangela Samarati and Latanya
    Sweeney, [“Protecting Privacy When Disclosing Information: K-Anonymity and Its
    Enforcement Through Generalization and Suppression”](https://oreil.ly/cyQqH),
    1998.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch01.html#idm45621849762320-marker)) Pierangela Samarati 和 Latanya Sweeney，[“在信息披露时保护隐私：K-匿名及其通过概括和抑制实施”](https://oreil.ly/cyQqH)，1998
    年。
- en: '^([5](ch01.html#idm45621850884208-marker)) Membership inference attacks were
    first described in Reza Shokri et al., [“Membership Inference Attacks Against
    Machine Learning Models”](https://arxiv.org/pdf/1610.05820.pdf), *2017 IEEE symposium
    on security and privacy (SP)*, (2017): 3–18.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '^([5](ch01.html#idm45621850884208-marker)) 首次描述成员推断攻击的是 Reza Shokri 等人，[“针对机器学习模型的成员推断攻击”](https://arxiv.org/pdf/1610.05820.pdf)，*2017
    年 IEEE 安全与隐私研讨会 (SP)*，(2017): 3–18。'
- en: ^([6](ch01.html#idm45621850686608-marker)) Shokri et al., “Membership Inference
    Attacks Against Machine Learning Models,” 3–18.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch01.html#idm45621850686608-marker)) Shokri 等人，“针对机器学习模型的成员推断攻击”，3–18。
- en: ^([7](ch01.html#idm45621850558528-marker)) For more on why membership inference
    attacks are particularly high-risk, low-reward, see Paul Irolla, [“Demystifying
    the Membership Inference Attack”](https://oreil.ly/9dW54), *Disaitek*, September
    19, 2019.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch01.html#idm45621850558528-marker)) 想要了解更多关于为何成员推断攻击尤为高风险低回报的内容，请参阅 Paul
    Irolla，[“揭秘成员推断攻击”](https://oreil.ly/9dW54)，*Disaitek*，2019 年 9 月 19 日。
- en: '^([8](ch01.html#idm45621849826816-marker)) Matt Fredrikson et al., [“Model
    Inversion Attacks that Exploit Confidence Information and Basic Countermeasures”](https://dl.acm.org/doi/10.1145/2810103.2813677),
    *Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications
    Security* (2015): 1322–33.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '^([8](ch01.html#idm45621849826816-marker)) Matt Fredrikson 等人，["利用置信信息和基本对策的模型反转攻击"](https://dl.acm.org/doi/10.1145/2810103.2813677)，*第22届ACM
    SIGSAC计算机与通信安全会议论文集* (2015): 1322–33。'
- en: '^([9](ch01.html#idm45621865229008-marker)) Florian Tramèr et al., [“Stealing
    Machine Learning Models via Prediction APIs”](https://oreil.ly/zuR9Q), *25th USENIX
    Security Symposium (USENIX Security 16)* (2016): 601–18.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '^([9](ch01.html#idm45621865229008-marker)) Florian Tramèr 等人，["通过预测API窃取机器学习模型"](https://oreil.ly/zuR9Q)，*第25届USENIX安全研讨会(USENIX
    Security 16)* (2016): 601–18.'
- en: '^([10](ch01.html#idm45621849442128-marker)) Binghui Wang and Neil Z. Gong,
    [“Stealing Hyperparameters in Machine Learning”](https://oreil.ly/h0JuN), *2018
    IEEE Symposium on Security and Privacy (SP)* (2018): 36–52.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '^([10](ch01.html#idm45621849442128-marker)) Binghui Wang 和 Neil Z. Gong，["在机器学习中窃取超参数"](https://oreil.ly/h0JuN)，*2018年IEEE安全与隐私研讨会(SP)*
    (2018): 36–52。'
- en: '^([11](ch01.html#idm45621850789216-marker)) Antonio Barbalau et al., [“Black-Box
    Ripper: Copying Black-Box Models Using Generative Evolutionary Algorithms”](https://arxiv.org/abs/2010.11158),
    *Advances in Neural Information Processing Systems* 33 (2020). For the full code,
    visit [GitHub](https://oreil.ly/MVOx0).'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch01.html#idm45621850789216-marker)) Antonio Barbalau 等人，["黑盒ripper：使用生成进化算法复制黑盒模型"](https://arxiv.org/abs/2010.11158)，*第33届神经信息处理系统进展*
    (2020)。有关完整代码，请访问[GitHub](https://oreil.ly/MVOx0)。
- en: '^([12](ch01.html#idm45621849715952-marker)) J. R. Correia-Silva et al., [“Copycat
    CNN: Stealing Knowledge by Persuading Confession with Random Non-Labeled Data”](https://oreil.ly/WA6OT),
    *2018 International Joint Conference on Neural Networks (IJCNN)*, (2018): 1–8.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '^([12](ch01.html#idm45621849715952-marker)) J. R. Correia-Silva 等人，["通过说服随机非标记数据实现知识窃取的Copycat
    CNN"](https://oreil.ly/WA6OT)，*2018国际联合神经网络会议(IJCNN)*，(2018): 1–8.'
- en: '^([13](ch01.html#idm45621850186960-marker)) Bang Wu et al., [“Model Extraction
    Attacks on Graph Neural Networks: Taxonomy and Realization”](https://arxiv.org/abs/2010.12751),
    *Proceedings of the 2022 ACM on Asia Conference on Computer and Communications
    Security* (2022): 337-50.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '^([13](ch01.html#idm45621850186960-marker)) Bang Wu 等人，["关于图神经网络的模型提取攻击：分类与实现"](https://arxiv.org/abs/2010.12751)，*2022年亚洲计算机与通信安全ACM会议论文集*
    (2022): 337-50。'
- en: ^([14](ch01.html#idm45621849855216-marker)) Kalpesh Khrisha and Nicolas Papernot,
    [“How to Steal Modern NLP Systems with Gibberish”](https://oreil.ly/4E9VV), *cleverhans-blog*,
    vol. 28, 2020.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch01.html#idm45621849855216-marker)) Kalpesh Khrisha 和 Nicolas Papernot，["如何通过胡言乱语窃取现代NLP系统"](https://oreil.ly/4E9VV)，*cleverhans-blog*，第28卷，2020年。
- en: ^([15](ch01.html#idm45621851858192-marker)) See the [CleverHans team’s code
    example](https://oreil.ly/ocqi5).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch01.html#idm45621851858192-marker)) 查看[CleverHans团队的代码示例](https://oreil.ly/ocqi5)。
- en: ^([16](ch01.html#idm45621849983808-marker)) Xuanli He et al., [“Model Extraction
    and Adversarial Transferability, Your BERT Is Vulnerable!”](https://arxiv.org/abs/2103.10013),
    *CoRR*, vol. abs/2103.10013 (2021); extraction and transfer code available on
    [GitHub](https://oreil.ly/sz8lf).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch01.html#idm45621849983808-marker)) Xuanli He 等人，["模型提取与对抗传递性，你的BERT是脆弱的！"](https://arxiv.org/abs/2103.10013)，*CoRR*，第abs/2103.10013卷
    (2021)；提取和转移代码可在[GitHub](https://oreil.ly/sz8lf)上找到。
- en: ^([17](ch01.html#idm45621850610176-marker)) Samuel R. Bowman et al., [“A Large
    Annotated Corpus for Learning Natural Language Inference”](https://arxiv.org/abs/1508.05326),
    *arXiv preprint* (2015). The [project page](https://oreil.ly/KdtFA) includes papers
    that use this along with download links.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch01.html#idm45621850610176-marker)) Samuel R. Bowman 等人，["用于学习自然语言推理的大型注释语料库"](https://arxiv.org/abs/1508.05326)，*arXiv预印本*
    (2015)。项目页面包括使用此数据的论文及下载链接。
- en: ^([18](ch01.html#idm45621859477040-marker)) Xuanli He et al., [“Model Extraction
    and Adversarial Transferability, Your BERT Is Vulnerable!”](https://arxiv.org/abs/2103.10013),
    *arXiv preprint* (2021).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ^([18](ch01.html#idm45621859477040-marker)) Xuanli He 等人，["模型提取与对抗传递性，你的BERT是脆弱的！"](https://arxiv.org/abs/2103.10013)，*arXiv预印本*
    (2021)。
- en: '^([19](ch01.html#idm45621852089536-marker)) Yuto Mori et al., [“BODAME: Bilevel
    Optimization for Defense Against Model Extraction”](https://arxiv.org/abs/2103.06797),
    *arXiv preprint* (2021).'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ^([19](ch01.html#idm45621852089536-marker)) Yuto Mori 等人，["BODAME：胆汁优化用于抵御模型提取"](https://arxiv.org/abs/2103.06797)，*arXiv预印本*
    (2021)。
- en: '^([20](ch01.html#idm45621852087568-marker)) Tribhuvanesh Orekondy et al., [“Prediction
    Poisoning: Towards Defenses Against DNN Model Stealing Attacks”](https://arxiv.org/abs/1906.10908),
    *arXiv preprint* (2019). Code example available on [GitHub](https://oreil.ly/XoJyX).'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ^([20](ch01.html#idm45621852087568-marker)) Tribhuvanesh Orekondy 等人的文章，“预测污染：防范
    DNN 模型窃取攻击的防御方法”，发表于《arXiv 预印本》（2019）。代码示例可在 [GitHub](https://oreil.ly/XoJyX)
    查看。
- en: ^([21](ch01.html#idm45621852083888-marker)) Soham Pal et al., [“Stateful Detection
    of Model Extraction Attacks”](https://arxiv.org/abs/2107.05166), *arXiv preprint*
    (2021).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ^([21](ch01.html#idm45621852083888-marker)) Soham Pal 等人的文章，“检测模型提取攻击的状态性方法”，发表于《arXiv
    预印本》（2021）。
- en: ^([22](ch01.html#idm45621852081840-marker)) Zhanyuan Zhang et al., [“Towards
    Characterizing Model Extraction Queries and How to Detect Them”](https://oreil.ly/NJfez)
    Research Project, University of California, Berkeley, 2021.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ^([22](ch01.html#idm45621852081840-marker)) Zhanyuan Zhang 等人的文章，“关于模型提取查询及其检测方法的初步研究”，加州大学伯克利分校研究项目，2021
    年。
- en: '^([23](ch01.html#idm45621852080432-marker)) Amir Mahdi Sadeghzadeh et al.,
    [“Hardness of Samples Is All You Need: Protecting Deep Learning Models Using Hardness
    of Samples”](https://arxiv.org/abs/2106.11424), *arXiv preprint* (2021).'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ^([23](ch01.html#idm45621852080432-marker)) Amir Mahdi Sadeghzadeh 等人的文章，“只需样本难度即可：利用样本难度保护深度学习模型”，发表于《arXiv
    预印本》（2021）。
- en: ^([24](ch01.html#idm45621852078528-marker)) Sanjay Kariyappa et al., [“Protecting
    DNNs From Theft Using an Ensemble of Diverse Models”](https://oreil.ly/VKqBy)
    (2020).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ^([24](ch01.html#idm45621852078528-marker)) Sanjay Kariyappa 等人的文章，“利用多样化模型集合保护
    DNN 免受窃取攻击”，发表于 2020 年。
- en: '^([25](ch01.html#idm45621852076896-marker)) Mika Juuti et al., [“PRADA: Protecting
    Against DNN Model Stealing Attacks”](https://oreil.ly/AvNR3), *2019 IEEE European
    Symposium on Security and Privacy (EuroS\&P)*, (2019): 512–27.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ^([25](ch01.html#idm45621852076896-marker)) Mika Juuti 等人的文章，“PRADA：防范 DNN 模型窃取攻击”，发表于《2019
    IEEE 欧洲安全与隐私研讨会（EuroS\&P）》，2019 年：512–27。
- en: ^([26](ch01.html#idm45621852074944-marker)) Chen Ma et al., [“Simulating Unknown
    Target Models for Query-Efficient Black-Box Attacks”](https://arxiv.org/abs/2009.00960)
    *arXiv preprint* (2020). The code is available on [GitHub](https://oreil.ly/Kdov3).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ^([26](ch01.html#idm45621852074944-marker)) Chen Ma 等人的文章，“模拟未知目标模型以进行查询效率高的黑盒攻击”，发表于《arXiv
    预印本》（2020）。代码可在 [GitHub](https://oreil.ly/Kdov3) 查看。
- en: '^([27](ch01.html#idm45621852730304-marker)) Huadi Zheng et al. [“Protecting
    Decision Boundary of Machine Learning Model with Differentially Private Perturbation”](https://oreil.ly/6XSyL),
    *IEEE Transactions on Dependable and Secure Computing* (2020): 2007-22.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ^([27](ch01.html#idm45621852730304-marker)) Huadi Zheng 等人的文章，“用差分隐私扰动保护机器学习模型的决策边界”，发表于《IEEE
    可靠与安全计算期刊》（2020）：2007-22。
- en: ^([28](ch01.html#idm45621852722384-marker)) For an example, see Google’s [differential
    privacy GitHub repo](https://oreil.ly/WInYR).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ^([28](ch01.html#idm45621852722384-marker)) 例如，请参阅 Google 的 [差分隐私 GitHub 仓库](https://oreil.ly/WInYR)。
- en: ^([29](ch01.html#idm45621853006400-marker)) See Lex Fridman’s [slides on the
    project](https://oreil.ly/2nHKv).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ^([29](ch01.html#idm45621853006400-marker)) 查看 Lex Fridman 的项目 [演示文稿](https://oreil.ly/2nHKv)。
- en: '^([30](ch01.html#idm45621853004864-marker)) Adam James Hall et al., [“Syft
    0.5: A Platform for Universally Deployable Structured Transparency”](https://arxiv.org/pdf/2104.12385.pdf),
    *arXiv preprint* (2021).'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ^([30](ch01.html#idm45621853004864-marker)) Adam James Hall 等人的文章，“Syft 0.5：一个可以普遍部署的结构透明平台”，发表于《arXiv
    预印本》（2021）。
- en: ^([31](ch01.html#idm45621852989088-marker)) Aaron Rinehart and Kelly Shortridge,
    [“Security Chaos Engineering”](https://oreil.ly/gSogE), (O’Reilly, 2020).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ^([31](ch01.html#idm45621852989088-marker)) Aaron Rinehart 和 Kelly Shortridge
    的书籍，“安全混乱工程”（O’Reilly，2020 年）。
- en: '^([32](ch01.html#idm45621851994768-marker)) More of the best practices and
    philosophies of the PySyft Library are detailed in Alexander Ziller et al., “Pysyft:
    A Library for Easy Federated Learning,” in *Federated Learning Systems*, edited
    by Muhammad Habib ur Rehman and Mohamed Medhat Gaber, 111–39\. New York: Springer,
    2021.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '^([32](ch01.html#idm45621851994768-marker)) PySyft 图书馆的最佳实践和哲学更多详情请参见 Alexander
    Ziller 等人的文章，“Pysyft: A Library for Easy Federated Learning”，收录于《联邦学习系统》（Muhammad
    Habib ur Rehman 和 Mohamed Medhat Gaber 编辑），111–39\. 纽约：Springer，2021 年。'
- en: ^([33](ch01.html#idm45621853550272-marker)) If you want to get into the exact
    taxonomy, see Huafei Zhu et al., [“On the Relationship Between (Secure) Multi-Party
    Computation and (Secure) Federated Learning”](https://oreil.ly/c8gFN) *DeepAI.org*,
    2020.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ^([33](ch01.html#idm45621853550272-marker)) 如果你想深入了解分类法，请参阅 Huafei Zhu 等人的文章，“关于（安全）多方计算与（安全）联邦学习之间的关系”，发表于
    *DeepAI.org*，2020 年。
- en: '^([34](ch01.html#idm45621853547984-marker)) Brendan McMahan and Daniel Ramage,
    [“Federated Learning: Collaborative Machine Learning Without Centralized Training
    Data”](https://oreil.ly/Yk9Er), *Google Research* (blog), April 6, 2017.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ^([34](ch01.html#idm45621853547984-marker)) Brendan McMahan 和 Daniel Ramage，[“联合学习：无需集中训练数据的协作机器学习”](https://oreil.ly/Yk9Er)，*Google
    Research*（博客），2017年4月6日。
- en: ^([35](ch01.html#idm45621853545472-marker)) Karen Hao, [“How Apple Personalizes
    Siri Without Hoovering up Your Data”](https://oreil.ly/hUC8B), *MIT Technology
    Review*, December 11, 2019.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ^([35](ch01.html#idm45621853545472-marker)) Karen Hao，[“苹果如何个性化 Siri 而不收集您的数据”](https://oreil.ly/hUC8B)，*MIT
    Technology Review*，2019年12月11日。
- en: '^([36](ch01.html#idm45621853543232-marker)) David Gunning et al., [“CrypTen:
    A New Research Tool for Secure Machine Learning with PyTorch”](https://oreil.ly/BBPVl),
    *MetaAI*, October 10, 2019.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ^([36](ch01.html#idm45621853543232-marker)) David Gunning 等人，[“CrypTen：一种新的基于
    PyTorch 的安全机器学习研究工具”](https://oreil.ly/BBPVl)，*MetaAI*，2019年10月10日。
- en: ^([37](ch01.html#idm45621853507008-marker)) OpenMined has a [blog on federated
    learning](https://oreil.ly/CvE3U).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ^([37](ch01.html#idm45621853507008-marker)) OpenMined 发表了关于联合学习的[博客](https://oreil.ly/CvE3U)。
- en: '^([38](ch01.html#idm45621853500992-marker)) David Froelicher et al., [“Truly
    Privacy-Preserving Federated Analytics for Precision Medicine with Multiparty
    Homomorphic Encryption”](https://oreil.ly/Cfvjs), *Nature Communications* 12,
    no. 1 (2021): 1–10.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ^([38](ch01.html#idm45621853500992-marker)) David Froelicher 等人，[“多方同态加密精准医学中真正隐私保护的联邦分析”](https://oreil.ly/Cfvjs)，*Nature
    Communications* 12卷，第1期（2021年）：1–10。
- en: '^([39](ch01.html#idm45621853495712-marker)) For example, see Gerta Rücker and
    Martin Schumacher, [“Simpson’s Paradox Visualized: The Example of the Rosiglitazone
    Meta-Analysis”](https://oreil.ly/XPoFV), *BMC Medical Research Methodology* 8,
    no. 34 (2008).'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ^([39](ch01.html#idm45621853495712-marker)) 例如，参见 Gerta Rücker 和 Martin Schumacher，[“辛普森悖论可视化：罗西格利塔Zone
    Meta 分析的例子”](https://oreil.ly/XPoFV)，*BMC Medical Research Methodology* 8卷，第34期（2008年）。
