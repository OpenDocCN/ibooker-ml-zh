- en: Chapter 9\. Advanced Model Deployments with TensorFlow Serving
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第 9 章。使用 TensorFlow Serving 进行高级模型部署
- en: In the previous chapter, we discussed the efficient deployment of TensorFlow
    or Keras models with TensorFlow Serving. With the knowledge of a basic model deployment
    and TensorFlow Serving configuration, we now introduce advanced use cases of machine
    learning model deployments in this chapter. The use cases touch a variety of topics,
    for example, deploying model A/B testing, optimizing models for deployment and
    scaling, and monitoring model deployments. If you haven’t had the chance to review
    the previous chapter, we recommend doing so because it provides the fundamentals
    for this chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了如何使用 TensorFlow 或 Keras 模型以及 TensorFlow Serving 进行高效部署。现在我们在本章介绍机器学习模型部署的高级用例。这些用例涉及多种主题，例如模型
    A/B 测试、优化模型以进行部署和扩展，以及监视模型部署。如果您还没有机会查看前一章，我们建议您这样做，因为它为本章提供了基础知识。
- en: Decoupling Deployment Cycles
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 解耦部署周期
- en: 'The basic deployments shown in [Chapter 8](index_split_013.html#filepos764992)
    work well, but they have one restriction: the trained and validated model needs
    to be either included in the deployment container image during the build step
    or mounted into the container during the container runtime, as we discussed in
    the previous chapter. Both options require either knowledge of DevOps processes
    (e.g., updating Docker container images) or coordination between the data science
    and DevOps teams during the deployment phase of a new model version.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 8 章](index_split_013.html#filepos764992) 中展示的基本部署方式效果良好，但有一个限制：训练和验证过的模型需要在构建步骤中包含在部署容器镜像中，或在容器运行时挂载到容器中，正如我们在前一章中讨论的那样。这两个选项都需要了解
    DevOps 过程（例如更新 Docker 容器镜像）或在部署新模型版本的阶段协调数据科学和 DevOps 团队。
- en: As we briefly mentioned in [Chapter 8](index_split_013.html#filepos764992),
    TensorFlow Serving can load models from remote storage drives (e.g., AWS S3 or
    GCP Storage buckets). The standard loader policy of TensorFlow Serving frequently
    polls the model storage location, unloads the previously loaded model, and loads
    a newer model upon detection. Due to this behavior, we only need to deploy our
    model serving container once, and it continuously updates the model versions once
    they become available in the storage folder location.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 [第 8 章](index_split_013.html#filepos764992) 中简要提到的，TensorFlow Serving
    可以从远程存储驱动器（例如 AWS S3 或 GCP 存储桶）加载模型。TensorFlow Serving 的标准加载器策略频繁轮询模型存储位置，在检测到新模型后卸载先前加载的模型并加载新模型。由于这种行为，我们只需要部署我们的模型服务容器一次，它会持续更新存储文件夹位置中可用的模型版本。
- en: Workflow Overview
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流程概述
- en: Before we take a closer look at how to configure TensorFlow Serving to load
    models from remote storage locations, let’s take a look at our proposed workflow.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入了解如何配置 TensorFlow Serving 从远程存储位置加载模型之前，让我们先看看我们提议的工作流程。
- en: '[Figure 9-1](#filepos999736) shows the separation of workflows. The model serving
    container is deployed once. Data scientists can upload new versions of models
    to storage buckets either through the buckets’ web interface or through command-line
    copy operations. Any changes to model versions will be discovered by the serving
    instances. A new build of the model server container or a redeployment of the
    container is not necessary.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-1](#filepos999736) 显示了工作流程的分离。模型服务容器只需部署一次。数据科学家可以通过存储桶的 Web 界面或命令行复制操作上传新版本的模型到存储桶中。任何模型版本的更改都将被服务实例发现。不需要重新构建模型服务器容器或重新部署容器。'
- en: '![](images/00017.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00017.jpg)'
- en: Figure 9-1\. Split of the data science and DevOps deployment cycles
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9-1。数据科学与 DevOps 部署周期的分离
- en: 'If your bucket folders are publicly accessible, you can serve the remote models
    by simply updating the model base path to the remote path:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的存储桶文件夹是公开访问的，您可以通过简单地更新模型基本路径到远程路径来提供远程模型服务：
- en: '`docker run -p 8500:8500` `\` `-p 8501:8501` `\` `-e` `MODEL_BASE_PATH``=``s3://bucketname/model_path/`
    `\` ![](images/00002.jpg) `-e` `MODEL_NAME``=``my_model` `\` ![](images/00075.jpg)
    `-t tensorflow/serving`'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker run -p 8500:8500` `\` `-p 8501:8501` `\` `-e` `MODEL_BASE_PATH``=``s3://bucketname/model_path/`
    `\` ![](images/00002.jpg) `-e` `MODEL_NAME``=``my_model` `\` ![](images/00075.jpg)
    `-t tensorflow/serving`'
- en: '![](images/00002.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: Remote bucket path
  id: totrans-13
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 远程存储桶路径
- en: '![](images/00075.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00075.jpg)'
- en: Remaining configuration remains the same
  id: totrans-15
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 其余配置保持不变
- en: 'If your models are stored in private cloud buckets, you need to configure TensorFlow
    Serving a bit more to provide access credentials. The setup is provider specific.
    We will cover two provider examples in this chapter: AWS and GCP.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的模型存储在私有云存储桶中，您需要更详细地配置 TensorFlow Serving 以提供访问凭据。设置与提供程序有关。本章将涵盖两个提供商的示例：AWS
    和 GCP。
- en: Accessing private models from AWS S3
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 从 AWS S3 访问私有模型
- en: AWS authenticates users through a user-specific access key and access secret.
    To access private AWS S3 buckets, you need to create a user access key and secret.[1](#filepos1071170)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 通过用户特定的访问密钥和访问密钥验证用户。要访问私有 AWS S3 存储桶，您需要创建用户访问密钥和密钥。[1](#filepos1071170)
- en: 'You can provide the AWS access key and secret as environment variables to the
    `docker run` command. This allows TensorFlow Serving to pick up the credentials
    and access private buckets:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将 AWS 访问密钥和密钥作为环境变量提供给 `docker run` 命令。这使得 TensorFlow Serving 可以获取凭据并访问私有存储桶：
- en: '`docker run -p 8500:8500` `\` `-p 8501:8501` `\` `-e` `MODEL_BASE_PATH``=``s3://bucketname/model_path/`
    `\` `-e` `MODEL_NAME``=``my_model` `\` `-e` `AWS_ACCESS_KEY_ID``=``XXXXX``\` ![](images/00002.jpg)
    `-e` `AWS_SECRET_ACCESS_KEY``=``XXXXX``\` `-t tensorflow/serving`'
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker run -p 8500:8500` `\` `-p 8501:8501` `\` `-e` `MODEL_BASE_PATH``=``s3://bucketname/model_path/`
    `\` `-e` `MODEL_NAME``=``my_model` `\` `-e` `AWS_ACCESS_KEY_ID``=``XXXXX``\` ![](images/00002.jpg)
    `-e` `AWS_SECRET_ACCESS_KEY``=``XXXXX``\` `-t tensorflow/serving`'
- en: '![](images/00002.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: The name of the environment variables is important.
  id: totrans-22
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 环境变量的名称非常重要。
- en: TensorFlow Serving relies on the standard AWS environment variables and its
    default values. You can overwrite the default values (e.g., if your bucket isn’t
    located in the `us-east-1` region or if you want to change the S3 endpoint).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving 依赖于标准的 AWS 环境变量及其默认值。您可以覆盖默认值（例如，如果您的存储桶不位于 `us-east-1` 地区，或者您想更改
    S3 端点）。
- en: 'You have the following configuration options:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 您有以下配置选项：
- en: '`AWS_REGION=us-east-1`'
  id: totrans-25
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`AWS_REGION=us-east-1`'
- en: '`S3_ENDPOINT=s3.us-east-1.amazonaws.com`'
  id: totrans-26
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`S3_ENDPOINT=s3.us-east-1.amazonaws.com`'
- en: '`S3_USE_HTTPS=1`'
  id: totrans-27
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`S3_USE_HTTPS=1`'
- en: '`S3_VERIFY_SSL=1`'
  id: totrans-28
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`S3_VERIFY_SSL=1`'
- en: 'The configuration options can be added as environment variables or added to
    the `docker run` command as shown in the following example:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 配置选项可以作为环境变量添加，或者如下示例所示添加到 `docker run` 命令中：
- en: '`docker run -p 8500:8500` `\` `-p 8501:8501` `\` `-e` `MODEL_BASE_PATH``=``s3://bucketname/model_path/`
    `\` `-e` `MODEL_NAME``=``my_model` `\` `-e` `AWS_ACCESS_KEY_ID``=``XXXXX``\` `-e`
    `AWS_SECRET_ACCESS_KEY``=``XXXXX``\` `-e` `AWS_REGION``=``us-west-1` `\` ![](images/00002.jpg)
    `-t tensorflow/serving`'
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker run -p 8500:8500` `\` `-p 8501:8501` `\` `-e` `MODEL_BASE_PATH``=``s3://bucketname/model_path/`
    `\` `-e` `MODEL_NAME``=``my_model` `\` `-e` `AWS_ACCESS_KEY_ID``=``XXXXX``\` `-e`
    `AWS_SECRET_ACCESS_KEY``=``XXXXX``\` `-e` `AWS_REGION``=``us-west-1` `\` ![](images/00002.jpg)
    `-t tensorflow/serving`'
- en: '![](images/00002.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: Additional configurations can be added through environment variables.
  id: totrans-32
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 可以通过环境变量添加额外的配置。
- en: With these few additional environment variables provided to TensorFlow Serving,
    you are now able to load models from remote AWS S3 buckets.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 通过为 TensorFlow Serving 提供这几个额外的环境变量，您现在可以从远程 AWS S3 存储桶加载模型。
- en: Accessing private models from GCP Buckets
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从 GCP 存储桶访问私有模型
- en: GCP authenticates users through service accounts. To access private GCP Storage
    buckets, you need to create a service account file.[2](#filepos1071523)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: GCP 通过服务账户对用户进行验证。要访问私有 GCP 存储桶，您需要创建一个服务账户文件。[2](#filepos1071523)
- en: Unlike in the case of AWS, we can’t simply provide the credential as an environment
    variable since the GCP authentication expects a JSON file with the service account
    credentials. In the GCP case, we need to mount a folder on the host machine containing
    the credentials inside a Docker container and then define an environment variable
    to point TensorFlow Serving to the correct credential file.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 与 AWS 不同，在 GCP 的情况下，我们无法简单地将凭证作为环境变量提供，因为 GCP 验证需要一个包含服务账户凭据的 JSON 文件。在 GCP
    情况下，我们需要在主机机器上挂载一个文件夹到 Docker 容器中，该文件夹包含凭证，并定义一个环境变量指向正确的凭证文件。
- en: 'For the following example, we assume that you have saved your newly created
    service account credential file under `/home/``your_username``/.credentials/`
    on your host machine. We downloaded the service account credentials from GCP and
    saved the file as `sa-credentials.json`. You can give the credential file any
    name, but you need to update the environmental variable `GOOGLE_APPLICATION_CREDENTIALS`
    with the full path inside of the Docker container:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，假设您已将新创建的服务帐户凭据文件保存在主机机器上的`/home/``your_username``/.credentials/`目录下。我们从
    GCP 下载了服务帐户凭据并将文件保存为`sa-credentials.json`。您可以为凭据文件指定任何名称，但需要在 Docker 容器内更新环境变量`GOOGLE_APPLICATION_CREDENTIALS`的完整路径：
- en: '`docker run -p 8500:8500` `\` `-p 8501:8501` `\` `-e` `MODEL_BASE_PATH``=``gcp://``bucketname``/``model_path``/`
    `\` `-e` `MODEL_NAME``=``my_model``\` `           -v /home/``your_username``/.credentials/:/credentials/`
    ![](images/00002.jpg) `-e` `GOOGLE_APPLICATION_CREDENTIALS``=``/``credentials``/``sa-credentials.json``\`
    ![](images/00075.jpg) `-t tensorflow/serving`'
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker run -p 8500:8500` `\` `-p 8501:8501` `\` `-e` `MODEL_BASE_PATH``=``gcp://``bucketname``/``model_path``/`
    `\` `-e` `MODEL_NAME``=``my_model``\` `           -v /home/``your_username``/.credentials/:/credentials/`
    ![](images/00002.jpg) `-e` `GOOGLE_APPLICATION_CREDENTIALS``=``/``credentials``/``sa-credentials.json``\`
    ![](images/00075.jpg) `-t tensorflow/serving`'
- en: '![](images/00002.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: Mount host directory with credentials.
  id: totrans-40
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 挂载主机目录以使用凭据。
- en: '![](images/00075.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00075.jpg)'
- en: Specify path inside of the container.
  id: totrans-42
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 指定容器内的路径。
- en: With a couple steps, you have configured a remote GCP bucket as a storage location.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 几个步骤即可将远程 GCP 存储桶配置为存储位置。
- en: Optimization of Remote Model Loading
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 远程模型加载的优化
- en: 'By default, TensorFlow Serving polls any model folder every two seconds for
    updated model versions, regardless of whether the model is stored in a local or
    remote location. If your model is stored in a remote location, the polling operation
    generates a bucket list view through your cloud provider. If you continuously
    update your model versions, your bucket might contain a large number of files.
    This results in large list-view messages and therefore consumes a small, but over
    time not insignificant, amount of traffic. Your cloud provider will most likely
    charge for the network traffic generated by these list operations. To avoid billing
    surprises, we recommend reducing the polling frequency to every 120 seconds, which
    still provides you up to 30 potential updates per hour but generates 60 times
    less traffic:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，TensorFlow Serving 每两秒轮询一次任何模型文件夹，以查找更新的模型版本，无论该模型存储在本地还是远程位置。如果您的模型存储在远程位置，轮询操作将通过您的云提供商生成一个存储桶列表视图。如果您持续更新模型版本，您的存储桶可能包含大量文件。这将导致大型列表视图消息，并因此消耗少量但随时间积累的流量。您的云提供商很可能会为这些列表操作生成的网络流量收费。为避免费用意外，我们建议将轮询频率降低到每
    120 秒，这仍然可以提供每小时高达 30 次的更新可能性，但会减少 60 倍的流量：
- en: '`docker run -p 8500:8500` `\` `...            -t tensorflow/serving` `\` `--file_system_poll_wait_seconds``=``120`'
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker run -p 8500:8500` `\` `...            -t tensorflow/serving` `\` `--file_system_poll_wait_seconds``=``120`'
- en: TensorFlow Serving arguments need to be added after the image specification
    of the `docker run` command. You can specify any polling wait time greater than
    one second. If you set the wait time to zero, TensorFlow Serving will not attempt
    to refresh the loaded model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在`docker run`命令的镜像规范之后需要添加 TensorFlow Serving 参数。您可以指定大于一秒的任何轮询等待时间。如果将等待时间设置为零，TensorFlow
    Serving 将不会尝试刷新加载的模型。
- en: Model Optimizations for Deployments
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 部署模型的优化
- en: With the increasing size of machine learning models, model optimization becomes
    more important to efficient deployments. Model quantization allows you to reduce
    the computation complexity of a model by reducing the precision of the weight’s
    representation. Model pruning allows you to implicitly remove unnecessary weights
    by zeroing them out of your model network. And model distillation will force a
    smaller neural network to learn the objectives of a larger neural network.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习模型尺寸的增加，模型优化对于高效部署变得更加重要。模型量化允许您通过减少权重表示的精度来减少模型的计算复杂度。模型修剪允许您通过将其归零来隐式地删除不必要的权重。而模型蒸馏将强制较小的神经网络学习较大神经网络的目标。
- en: All three optimization methods aim for smaller models that allow faster model
    inferences. In the following sections, we will explain the three optimization
    options further.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 所有三种优化方法旨在实现更小的模型，从而实现更快的模型推理。在接下来的几节中，我们将进一步解释这三种优化选项。
- en: Quantization
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 量化
- en: 'The weights of a neural network are often stored as float 32-bit data types
    (or, as the IEEE 754 standard calls it, single-precision binary floating-point
    format). A floating-point number is stored as the following: 1 bit storing the
    sign of the number, 8 bits for the exponent, and 23 bits for the precision of
    the floating number.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的权重通常存储为32位浮点数据类型（或者，IEEE 754 标准称之为单精度二进制浮点格式）。浮点数存储如下：1位存储数的符号，8位存储指数，以及23位存储浮点数的精度。
- en: The network weights, however, can be expressed in bfloat16 floating-point format
    or as 8-bit integers. As shown in [Figure 9-2](#filepos1016791), we still need
    1 bit to store the sign of the number. The exponent is also still represented
    through 8 bits when we store weights as bfloat16 floating points because it is
    used by TensorFlow. However, the fraction representation is reduced from 23 bits
    to 7 bits. The weights can even sometimes be represented as integers using only
    8 bits.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，网络权重可以以 bfloat16 浮点格式或8位整数表示。如图[9-2](#filepos1016791)所示，存储权重时，我们仍然需要1位来存储数的符号。当我们将权重存储为
    bfloat16 浮点数时，指数仍然使用8位，因为它被 TensorFlow 使用。但是，分数表示从23位减少到7位。有时，权重甚至可以仅使用8位整数表示。
- en: '![](images/00029.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00029.jpg)'
- en: Figure 9-2\. Reduction of the floating precision
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图9-2. 浮点精度的降低
- en: 'By changing the network’s weight representation to 16-bit floating points or
    integers, we can achieve the following benefits:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将网络权重表示改为16位浮点数或整数，我们可以获得以下好处：
- en: The weights can be represented with fewer bytes, requiring less memory during
    the model inference.
  id: totrans-57
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 权重可以用更少的字节表示，在模型推理期间需要更少的内存。
- en: Due to the weights’ reduced representation, predictions can be inferred faster.
  id: totrans-58
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 由于权重的减少表示，预测可以更快地推断出来。
- en: The quantization allows the execution of neural networks on 16-bit, or even
    8-bit, embedded systems.
  id: totrans-59
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 量化允许在16位甚至8位嵌入式系统上执行神经网络。
- en: Current workflows for model quantization are applied after model training and
    are often called post-training quantization. Since a quantized model can be underfitted
    due to the lack of precision, we highly recommend analyzing and validating any
    model after quantization and before deployment. As an example of model quantizations,
    we discuss Nvidia’s TensorRT library (see [“Using TensorRT with TensorFlow Serving”](#filepos1021149))
    and TensorFlow’s TFLite library (see [“TFLite”](#filepos1025140)).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的模型量化工作流在模型训练后应用，通常称为后训练量化。由于量化模型可能由于精度不足而导致欠拟合，我们强烈建议在部署之前分析和验证任何模型。作为模型量化的示例，我们讨论了
    Nvidia 的 TensorRT 库（参见 [“使用 TensorRT 与 TensorFlow Serving”](#filepos1021149)）和
    TensorFlow 的 TFLite 库（参见 [“TFLite”](#filepos1025140)）。
- en: Pruning
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝
- en: An alternative to reducing the precision of network weights is model pruning.
    The idea here is that a trained network can be reduced to a smaller network by
    removing unnecessary weights. In practice, this means that “unnecessary” weights
    are set to zero. By setting unnecessary weights to zero, the inference or prediction
    can be sped up. Also, the pruned models can be compressed to smaller models sizes
    since sparse weights lead to higher compression rates.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 除了降低网络权重精度的方法外，模型剪枝是一种替代方法。其核心思想是通过移除不必要的权重，将训练过的网络压缩成一个更小的网络。实际操作中，这意味着将“不必要”的权重设为零。通过将不必要的权重设为零，可以加快推理或预测的速度。此外，由于稀疏权重导致更高的压缩率，剪枝模型还可以压缩成更小的模型尺寸。
- en: HOW TO PRUNE MODELS
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如何剪枝模型
- en: Models can be pruned during their training phase through tools like TensorFlow’s
    model optimization package `tensorflow-model-optimization`.[3](#filepos1071893)
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 模型可以在训练阶段进行剪枝，使用类似于 TensorFlow 的模型优化包 `tensorflow-model-optimization`.[3](#filepos1071893)
- en: Distillation
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 蒸馏
- en: Instead of reducing the network connections, we can also train a smaller, less
    complex neural network to learn trained tasks from a much more extensive network.
    This approach is called distillation. Instead of simply training a smaller machine
    learning model with the same objective as the bigger model, the predictions of
    the bigger model (the teacher neural network) influence the update of the smaller
    model’s (the student neural network) weights, as shown in [Figure 9-3](#filepos1020909).
    By using the predictions from the teacher and student neural networks, the student
    network can be forced to learn an objective from the teacher neural network. Ultimately,
    we can express the same model objective with fewer weights and with a model architecture
    that wouldn’t have been able to learn the objective without the teacher forcing
    it.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与其减少网络连接，我们也可以训练一个较小、复杂度较低的神经网络，从一个更广泛的网络中学习已经训练好的任务。这种方法被称为蒸馏。不同于简单地训练一个与较大模型相同目标的较小机器学习模型，较大模型（教师神经网络）的预测会影响较小模型（学生神经网络）权重的更新，如在[图
    9-3](#filepos1020909)中所示。通过使用来自教师和学生神经网络的预测，可以强制学生网络从教师网络中学习一个目标。最终，我们可以用更少的权重和一个原本无法学习目标的模型架构来表达相同的模型目标。
- en: '![](images/00042.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00042.jpg)'
- en: Figure 9-3\. Student network learning from a teacher network
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9-3\. 学生网络从教师网络中学习
- en: Using TensorRT with TensorFlow Serving
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TensorRT 与 TensorFlow Serving
- en: One option for performing quantization on a trained TensorFlow model before
    deploying it to production is converting the model with Nvidia’s TensorRT.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在将训练好的 TensorFlow 模型部署到生产环境之前，进行量化的一种选择是将模型转换为 Nvidia 的 TensorRT。
- en: If you are running computationally intensive deep learning models on an Nvidia
    GPU, you can use this additional way of optimizing your model server. Nvidia provides
    a library called TensorRT that optimizes the inference of deep learning models
    by reducing the precision of the numerical representations of the network weights
    and biases. TensorRT supports int8 and float16 representations. The reduced precision
    will lower the inference latency of the model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在 Nvidia GPU 上运行计算密集型深度学习模型，可以使用这种额外的方式来优化您的模型服务器。Nvidia 提供了一个名为 TensorRT
    的库，通过降低网络权重和偏差的数值表示精度来优化深度学习模型的推理。TensorRT 支持 int8 和 float16 表示。降低精度将降低模型的推理延迟。
- en: After your model is trained, you need to optimize the model with TensorRT’s
    own optimizer or with `saved_model_cli`.[4](#filepos1072387) The optimized model
    can then be loaded into TensorFlow Serving. At the time of writing this chapter,
    TensorRT was limited to certain Nvidia products, including Tesla V100 and P4.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的模型训练完成后，您需要使用 TensorRT 自带的优化器或 `saved_model_cli`[4](#filepos1072387) 来优化模型。优化后的模型可以加载到
    TensorFlow Serving 中。在撰写本章时，TensorRT 仅限于某些 Nvidia 产品，包括 Tesla V100 和 P4。
- en: 'First, we’ll convert our deep learning model with `saved_model_cli`:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用 `saved_model_cli` 转换我们的深度学习模型：
- en: '`$` `saved_model_cli convert --dir saved_models/` `\` `--output_dir trt-savedmodel/`
    `\` `--tag_set serve tensorrt`'
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `saved_model_cli convert --dir saved_models/` `\` `--output_dir trt-savedmodel/`
    `\` `--tag_set serve tensorrt`'
- en: 'After the conversion, you can load the model in our GPU setup of TensorFlow
    Serving as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 转换后，您可以按以下方式在我们的 TensorFlow Serving GPU 设置中加载模型：
- en: '`$` `docker run --runtime``=``nvidia` `\` `-p 8500:8500` `\` `-p 8501:8501`
    `\` `--mount` `type``=``bind``,source``=``/path/to/models,target``=``/models/my_model`
    `\` `-e` `MODEL_NAME``=``my_model` `\` `-t tensorflow/serving:latest-gpu`'
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `docker run --runtime``=``nvidia` `\` `-p 8500:8500` `\` `-p 8501:8501`
    `\` `--mount` `type``=``bind``,source``=``/path/to/models,target``=``/models/my_model`
    `\` `-e` `MODEL_NAME``=``my_model` `\` `-t tensorflow/serving:latest-gpu`'
- en: If you are inferring your models on Nvidia GPUs, the hardware is supported by
    TensorRT. Switching to TensorRT can be an excellent way to lower your inference
    latencies further.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在 Nvidia GPU 上推理您的模型，则 TensorRT 支持该硬件。切换到 TensorRT 可以进一步降低推理延迟。
- en: TFLite
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: TFLite
- en: If you want to optimize your machine learning model but you’re not running Nvidia
    GPUs, you can use TFLite to perform optimizations on your machine learning.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望优化您的机器学习模型但又没有运行 Nvidia GPU，则可以使用 TFLite 来对机器学习进行优化。
- en: TFLite has traditionally been used to convert machine learning models to smaller
    model sizes for deployment to mobile or IoT devices. However, these models can
    also be used with TensorFlow Serving. So instead of deploying a machine learning
    model to an edge device, you can deploy a machine learning model with TensorFlow
    Serving that will have a low inference latency and a smaller memory footprint.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: TFLite传统上用于将机器学习模型转换为较小的模型尺寸，以部署到移动设备或物联网设备。但是，这些模型也可以与TensorFlow Serving一起使用。因此，与其将机器学习模型部署到边缘设备，不如使用TensorFlow
    Serving部署具有低推理延迟和较小内存占用的机器学习模型。
- en: 'While optimizing with TFLite looks very promising, there are a few caveats:
    at the time of writing this section, the TensorFlow Serving support for TFLite
    models is only in experimental stages. And furthermore, not all TensorFlow operations
    can be converted to TFLite instructions. However, the number of supported operations
    is continuously growing.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用TFLite进行优化看起来非常有前途，但是有一些注意事项：在撰写本节时，TensorFlow Serving对TFLite模型的支持仅处于实验阶段。此外，并非所有TensorFlow操作都可以转换为TFLite指令。然而，支持的操作数量正在不断增加。
- en: Steps to Optimize Your Model with TFLite
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TFLite优化模型的步骤
- en: TFLite can also be used to optimize TensorFlow and Keras models. The library
    provides a variety of optimization options and tools. You can either convert your
    model through command-line tools or through the Python library.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: TFLite还可以用于优化TensorFlow和Keras模型。该库提供了多种优化选项和工具。您可以通过命令行工具或Python库转换模型。
- en: 'The starting point is always a trained and exported model in the `SavedModel`
    format. In the following example, we focus on Python instructions. The conversion
    process consists of four steps:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 起始点始终是以`SavedModel`格式训练并导出的模型。在以下示例中，我们关注Python指令。转换过程包括四个步骤：
- en: Loading the exported saved model
  id: totrans-85
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 加载导出的保存模型
- en: Defining your optimization goals
  id: totrans-86
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 定义您的优化目标
- en: Converting the model
  id: totrans-87
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 转换模型
- en: Saving the optimized model as a TFLite model
  id: totrans-88
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 将优化后的模型保存为TFLite模型
- en: '`import``tensorflow``as``tf``saved_model_dir``=``"path_to_saved_model"``converter``=``tf``.``lite``.``TFLiteConverter``.``from_saved_model``(``saved_model_dir``)``converter``.``optimizations``=``[``tf``.``lite``.``Optimize``.``DEFAULT`![](images/00002.jpg)`]``tflite_model``=``converter``.``convert``()``with``open``(``"/tmp/model.tflite"``,``"wb"``)``as``f``:``f``.``write``(``tflite_model``)`'
  id: totrans-89
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`import``tensorflow``as``tf``saved_model_dir``=``"path_to_saved_model"``converter``=``tf``.``lite``.``TFLiteConverter``.``from_saved_model``(``saved_model_dir``)``converter``.``optimizations``=``[``tf``.``lite``.``Optimize``.``DEFAULT`![](images/00002.jpg)`]``tflite_model``=``converter``.``convert``()``with``open``(``"/tmp/model.tflite"``,``"wb"``)``as``f``:``f``.``write``(``tflite_model``)`'
- en: '![](images/00002.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: Set the optimization strategy.
  id: totrans-91
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 设置优化策略。
- en: TFLITE OPTIMIZATIONS
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: TFLITE 优化
- en: TFLite provides predefined optimization objectives. By changing the optimization
    goal, the converter will optimize the models differently. A few options are `DEFAULT`,
    `OPTIMIZE_FOR_LATENCY`, and `OPTIMIZE_FOR_SIZE`.
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: TFLite提供了预定义的优化目标。通过更改优化目标，转换器将以不同的方式优化模型。几个选项是`DEFAULT`、`OPTIMIZE_FOR_LATENCY`和`OPTIMIZE_FOR_SIZE`。
- en: 'In the `DEFAULT` mode, your model will be optimized for latency and size, whereas
    the other two options prefer one option over the other. You can set the convert
    options as follows:'
  id: totrans-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在`DEFAULT`模式下，您的模型将针对延迟和大小进行优化，而另外两个选项则更倾向于一个选项。您可以设置转换选项如下：
- en: '`...``converter``.``optimizations``=``[``tf``.``lite``.``Optimize``.``OPTIMIZE_FOR_SIZE``]``converter``.``target_spec``.``supported_types``=``[``tf``.``lite``.``constants``.``FLOAT16``]``tflite_model``=``converter``.``convert``()``...`'
  id: totrans-95
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`...``converter``.``optimizations``=``[``tf``.``lite``.``Optimize``.``OPTIMIZE_FOR_SIZE``]``converter``.``target_spec``.``supported_types``=``[``tf``.``lite``.``constants``.``FLOAT16``]``tflite_model``=``converter``.``convert``()``...`'
- en: 'If your model includes a TensorFlow operation that is not supported by TFLite
    at the time of exporting your model, the conversion step will fail with an error
    message. You can enable an additional set of selected TensorFlow operations to
    be available for the conversion process. However, this will increase the size
    of your TFLite model by ca. 30 MB. The following code snippet shows how to enable
    the additional TensorFlow operations before the converter is executed:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的模型包含在导出模型时TFLite不支持的TensorFlow操作，则转换步骤将失败并显示错误消息。您可以在执行转换器之前启用额外的选定TensorFlow操作，以使其在转换过程中可用。但是，这将增加约30
    MB的TFLite模型大小。以下代码段显示了如何在执行转换器之前启用额外的TensorFlow操作：
- en: '`...``converter``.``target_spec``.``supported_ops``=``[``tf``.``lite``.``OpsSet``.``TFLITE_BUILTINS``,``tf``.``lite``.``OpsSet``.``SELECT_TF_OPS``]``tflite_model``=``converter``.``convert``()``...`'
  id: totrans-97
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`...``converter``.``target_spec``.``supported_ops``=``[``tf``.``lite``.``OpsSet``.``TFLITE_BUILTINS``,``tf``.``lite``.``OpsSet``.``SELECT_TF_OPS``]``tflite_model``=``converter``.``convert``()``...`'
- en: If the conversion of your model still fails due to an unsupported TensorFlow
    operation, you can bring it to the attention of the TensorFlow community. The
    community is actively increasing the number of operations supported by TFLite
    and welcomes suggestions for future operations to be included in TFLite. TensorFlow
    ops can be nominated via the [TFLite Op Request form](https://oreil.ly/rPUqr).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的模型转换由于不支持的 TensorFlow 操作而失败，可以向 TensorFlow 社区寻求帮助。社区正在积极增加 TFLite 支持的操作数量，并欢迎未来操作的建议。您可以通过
    [TFLite Op 请求表单](https://oreil.ly/rPUqr) 提名 TensorFlow 操作。
- en: Serving TFLite Models with TensorFlow Serving
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TensorFlow Serving 提供 TFLite 模型
- en: 'The latest TensorFlow Serving versions can read TFLite models without any major
    configuration change. You only need to start TensorFlow Serving with the enabled
    `use_tflite_model` flag and it will load the optimized model as shown in the following
    example:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最新的 TensorFlow Serving 版本可以无需进行任何主要配置更改读取 TFLite 模型。您只需启动 TensorFlow Serving，并使用启用的
    `use_tflite_model` 标志加载优化模型，如以下示例所示：
- en: '`docker run -p 8501:8501 \            --mount type=bind,\             source=/path/to/models,\
                target=/models/my_model \            -e MODEL_BASE_PATH=/models \
               -e MODEL_NAME=my_model \            -t tensorflow/serving:latest \
               --use_tflite_model=true` ![](images/00002.jpg)'
  id: totrans-101
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker run -p 8501:8501 \            --mount type=bind,\             source=/path/to/models,\
                target=/models/my_model \            -e MODEL_BASE_PATH=/models \
               -e MODEL_NAME=my_model \            -t tensorflow/serving:latest \
               --use_tflite_model=true` ![](images/00002.jpg)'
- en: '![](images/00002.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: Enable TFLite model loading.
  id: totrans-103
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 启用 TFLite 模型加载。
- en: TensorFlow Lite optimized models can provide you with low-latency and low-memory
    footprint model deployments.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Lite 优化模型可以提供低延迟和低内存占用的模型部署。
- en: DEPLOY YOUR MODELS TO EDGE DEVICES
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 部署您的模型到边缘设备
- en: 'After optimizing your TensorFlow or Keras model and deploying your TFLite machine
    learning model with TensorFlow Serving, you can also deploy the model to a variety
    of mobile and edge devices; for example:'
  id: totrans-106
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在优化 TensorFlow 或 Keras 模型并使用 TensorFlow Serving 部署 TFLite 机器学习模型之后，您还可以将模型部署到各种移动和边缘设备；例如：
- en: Android and iOS mobile phones
  id: totrans-107
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Android 和 iOS 手机
- en: ARM64-based computers
  id: totrans-108
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 基于 ARM64 的计算机
- en: Microcontrollers and other embedded devices (e.g., Raspberry Pi)
  id: totrans-109
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 微控制器和其他嵌入式设备（例如 Raspberry Pi）
- en: Edge devices (e.g., IoT devices)
  id: totrans-110
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 边缘设备（例如 IoT 设备）
- en: Edge TPUs (e.g., Coral)
  id: totrans-111
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 边缘 TPU（例如 Coral）
- en: If you are interested in deployments to mobile or edge devices, we recommend
    the publication Practical Deep Learning for Cloud, Mobile, and Edge by Anirudh
    Koul et al. (O’Reilly) for further reading. If you are looking for materials on
    edge devices with a focus on TFMicro, we recommend TinyML by Pete Warden and Daniel
    Situnayake (O’Reilly).
  id: totrans-112
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果您对部署到移动或边缘设备感兴趣，我们推荐阅读 Anirudh Koul 等人编著的《Practical Deep Learning for Cloud,
    Mobile, and Edge》（O’Reilly）。如果您正在寻找关于以 TFMicro 为重点的边缘设备的材料，我们推荐 Pete Warden 和
    Daniel Situnayake 编著的《TinyML》（O’Reilly）。
- en: Monitoring Your TensorFlow Serving Instances
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 监控您的 TensorFlow Serving 实例
- en: TensorFlow Serving allows you to monitor your inference setup. For this task,
    TensorFlow Serving provides metric endpoints that can be consumed by Prometheus.
    Prometheus is a free application for real-time event logging and alerting, currently
    under Apache License 2.0\. It is used extensively within the Kubernetes community,
    but it can easily be used without Kubernetes.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving 允许您监视推理设置。为此，TensorFlow Serving 提供了可以被 Prometheus 消费的度量端点。Prometheus
    是一个实时事件记录和警报的免费应用程序，目前在 Apache License 2.0 下发布。它在 Kubernetes 社区广泛使用，但也可以轻松在没有
    Kubernetes 的环境中使用。
- en: To track your inference metrics, you need to run TensorFlow Serving and Prometheus
    side by side. Prometheus can then be configured to pull metrics from TensorFlow
    Serving continuously. The two applications communicate via a REST endpoint, which
    requires that REST endpoints are enabled for TensorFlow Serving even if you are
    only using gRPC endpoints in your application.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟踪推理指标，您需要同时运行 TensorFlow Serving 和 Prometheus。然后，可以配置 Prometheus 连续地从 TensorFlow
    Serving 拉取指标。这两个应用程序通过 REST 端点进行通信，即使您的应用程序仅使用 gRPC 端点，TensorFlow Serving 也需要启用
    REST 端点。
- en: Prometheus Setup
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 设置
- en: Before configuring TensorFlow Serving to provide metrics to Prometheus, we need
    to set up and configure our Prometheus instance. For the simplicity of this example,
    we are running two Docker instances (TensorFlow Serving and Prometheus) side by
    side, as shown in [Figure 9-4](#filepos1044518). In a more elaborate setup, the
    applications would be Kubernetes deployments.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置TensorFlow Serving将指标提供给Prometheus之前，我们需要设置和配置我们的Prometheus实例。为了简化这个示例，我们将两个Docker实例（TensorFlow
    Serving和Prometheus）并排运行，如[图9-4](#filepos1044518)所示。在更详细的设置中，这些应用程序将是Kubernetes部署。
- en: '![](images/00052.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00052.jpg)'
- en: Figure 9-4\. Prometheus Docker setup
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图9-4\. Prometheus Docker设置
- en: 'We need to create a Prometheus configuration file before starting up Prometheus.
    For this purpose, we will create a configuration file located at /tmp/prometheus.yml
    and add the following configuration details:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动Prometheus之前，我们需要创建一个Prometheus配置文件。为此，我们将创建一个位于/tmp/prometheus.yml的配置文件，并添加以下配置细节：
- en: '`global``:``scrape_interval``:` `15s` `evaluation_interval``:` `15s` `external_labels``:``monitor``:``''tf-serving-monitor''``scrape_configs``:`
    `-` `job_name``:``''prometheus''``scrape_interval``:` `5s` ![](images/00002.jpg)`metrics_path``:`
    `/monitoring/prometheus/metrics` ![](images/00075.jpg)`static_configs``:` `-`
    `targets``:` `[``''host.docker.internal:8501''``]` ![](images/00064.jpg)'
  id: totrans-121
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`global``:``scrape_interval``:` `15s` `evaluation_interval``:` `15s` `external_labels``:``monitor``:``''tf-serving-monitor''``scrape_configs``:`
    `-` `job_name``:``''prometheus''``scrape_interval``:` `5s` ![](images/00002.jpg)`metrics_path``:`
    `/monitoring/prometheus/metrics` ![](images/00075.jpg)`static_configs``:` `-`
    `targets``:` `[``''host.docker.internal:8501''``]` ![](images/00064.jpg)'
- en: '![](images/00002.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: Interval when metrics are pulled.
  id: totrans-123
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 拉取指标的间隔。
- en: '![](images/00075.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00075.jpg)'
- en: Metrics endpoints from TensorFlow Serving.
  id: totrans-125
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: TensorFlow Serving的指标端点。
- en: '![](images/00064.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00064.jpg)'
- en: Replace with the IP address of your application.
  id: totrans-127
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 用你的应用程序的IP地址替换。
- en: In our example configuration, we configured the target host to be `host.docker.internal`.
    We are taking advantage of Docker’s domain name resolution to access the TensorFlow
    Serving container via the host machine. Docker automatically resolves the domain
    name `host.docker.internal` to the host’s IP address.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例配置中，我们配置目标主机为`host.docker.internal`。我们利用Docker的域名解析功能，通过主机机器访问TensorFlow
    Serving容器。Docker会自动将域名`host.docker.internal`解析为主机的IP地址。
- en: 'Once you have created your Prometheus configuration file, you can start the
    Docker container, which runs the Prometheus instance:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 创建完你的Prometheus配置文件后，可以启动运行Prometheus实例的Docker容器：
- en: '`$` `docker run -p 9090:9090` `\` ![](images/00002.jpg) `-v /tmp/prometheus.yml:/etc/prometheus/prometheus.yml`
    `\` ![](images/00075.jpg) `prom/prometheus`'
  id: totrans-130
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `docker run -p 9090:9090` `\` ![](images/00002.jpg) `-v /tmp/prometheus.yml:/etc/prometheus/prometheus.yml`
    `\` ![](images/00075.jpg) `prom/prometheus`'
- en: '![](images/00002.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: Enable port 9090.
  id: totrans-132
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 启用9090端口。
- en: '![](images/00075.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00075.jpg)'
- en: Mount your configuration file.
  id: totrans-134
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 挂载你的配置文件。
- en: Prometheus provides a dashboard for the metrics, which we will later access
    via port 9090.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus提供了一个仪表盘用于指标，稍后我们将通过9090端口访问。
- en: TensorFlow Serving Configuration
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving配置
- en: Similar to our previous configuration for the inference batching, we need to
    write a small configuration file to configure the logging settings.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们之前为推断批处理配置的配置，我们需要编写一个小的配置文件来配置日志记录设置。
- en: 'With a text editor of your choice, create a text file containing the following
    configuration (in our example, we saved the configuration file to /tmp/monitoring_config.txt):'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 使用你选择的文本编辑器，创建一个包含以下配置的文本文件（在我们的示例中，我们将配置文件保存为/tmp/monitoring_config.txt）：
- en: '`prometheus_config {    enable: true,    path:` `"/monitoring/prometheus/metrics"``}`'
  id: totrans-139
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`prometheus_config {    enable: true,    path:` `"/monitoring/prometheus/metrics"``}`'
- en: In the configuration file, we are setting the URL path for the metrics data.
    The path needs to match the path we specified in the Prometheus configuration
    that we previously created (/tmp/prometheus.yml).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置文件中，我们设置了用于指标数据的URL路径。该路径需要与我们之前创建的Prometheus配置文件中指定的路径匹配（/tmp/prometheus.yml）。
- en: 'To enable the monitoring functionality, we only need to add the path of the
    `monitoring_config_file` and TensorFlow Serving will provide a REST endpoint with
    the metrics data for Prometheus:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用监控功能，我们只需添加`monitoring_config_file`的路径，TensorFlow Serving将为Prometheus提供包含指标数据的REST端点：
- en: '`$` `docker run -p 8501:8501` `\` `--mount` `type``=``bind``,source``=[PRE0]pwd[PRE1],target``=``/models/my_model`
    `\` `--mount` `type``=``bind``,source``=``/tmp,target``=``/model_config` `\` `tensorflow/serving`
    `\` `--monitoring_config_file``=``/model_config/monitoring_config.txt`'
  id: totrans-142
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `docker run -p 8501:8501` `\` `--mount` `type``=``bind``,source``=[PRE0]pwd[PRE1],target``=``/models/my_model`
    `\` `--mount` `type``=``bind``,source``=``/tmp,target``=``/model_config` `\` `tensorflow/serving`
    `\` `--monitoring_config_file``=``/model_config/monitoring_config.txt`'
- en: PROMETHEUS IN ACTION
  id: totrans-143
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 《PROMETHEUS IN ACTION》
- en: With the Prometheus instance running, you can now access the Prometheus dashboard
    to view the TensorFlow Serving metrics with the Prometheus UI, as shown in [Figure 9-5](#filepos1054024).
  id: totrans-144
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用 Prometheus 实例运行后，您现在可以访问 Prometheus 仪表盘，通过 Prometheus UI 查看 TensorFlow Serving
    的度量标准，如 [图 9-5](#filepos1054024) 所示。
- en: '![](images/00062.jpg)'
  id: totrans-145
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](images/00062.jpg)'
- en: Figure 9-5\. Prometheus dashboard for TensorFlow Serving
  id: totrans-146
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图 9-5\. TensorFlow Serving 的 Prometheus 仪表盘
- en: Prometheus provides a standardized UI for common metrics. Tensorflow Serving
    provides a variety of metric options, including the number of session runs, the
    load latency, or the time to run a particular graph, as shown in [Figure 9-6](#filepos1054691).
  id: totrans-147
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Prometheus 提供了一个标准化的 UI 来查看常见的度量标准。Tensorflow Serving 提供了多种度量选项，包括会话运行次数、加载延迟或特定图形的运行时间，如
    [图 9-6](#filepos1054691) 所示。
- en: '![](images/00072.jpg)'
  id: totrans-148
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](images/00072.jpg)'
- en: Figure 9-6\. Prometheus metric options for TensorFlow Serving
  id: totrans-149
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图 9-6\. TensorFlow Serving 的 Prometheus 指标选项
- en: Simple Scaling with TensorFlow Serving and Kubernetes
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TensorFlow Serving 和 Kubernetes 进行简单扩展
- en: So far, we have discussed the deployment of a single TensorFlow Serving instance
    hosting one or more model versions. While this solution is sufficient for a good
    number of deployments, it isn’t enough for applications experiencing a high volume
    of prediction requests. In these situations, your single Docker container with
    TensorFlow Serving needs to be replicated to reply to the additional prediction
    requests. The orchestration of the container replication is usually managed by
    tools like Docker Swarm or Kubernetes. While it would go beyond the scope of this
    publication to introduce Kubernetes in depth, we would like to provide a small
    glimpse of how your deployment could orchestrate through Kubernetes.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了部署单个 TensorFlow Serving 实例来托管一个或多个模型版本。虽然这种解决方案对于许多部署来说已经足够，但对于经历高预测请求量的应用程序来说还不够。在这些情况下，您的单个
    TensorFlow Serving Docker 容器需要复制以响应额外的预测请求。通常由像 Docker Swarm 或 Kubernetes 这样的工具来管理容器的复制编排。虽然深入介绍
    Kubernetes 超出了本出版物的范围，但我们想提供一个小的预览，说明您的部署如何通过 Kubernetes 进行编排。
- en: For the following example, we assume that you’ll have a Kubernetes cluster running
    and that access to the cluster will be via `kubectl`. Because you can deploy TensorFlow
    models without building specific Docker containers, you will see that, in our
    example, we reused Google-provided Docker containers and configured Kubernetes
    to load our models from a remote storage bucket.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以下示例，我们假设您将运行一个 Kubernetes 集群，并且通过 `kubectl` 访问该集群。因为您可以在不构建特定 Docker 容器的情况下部署
    TensorFlow 模型，所以在我们的示例中，您将看到我们重用了由 Google 提供的 Docker 容器，并配置了 Kubernetes 从远程存储桶加载我们的模型。
- en: 'The first source code example highlights two aspects:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个源代码示例突出了两个方面：
- en: Deploying via Kubernetes without building specific Docker containers
  id: totrans-154
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 通过 Kubernetes 部署而无需构建特定的 Docker 容器
- en: Handling the Google Cloud authentication to access the remote model storage
    location
  id: totrans-155
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 处理 Google Cloud 认证以访问远程模型存储位置
- en: In the following example, we use the GCP as the cloud provider for our deployment:[5](#filepos1072709)
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们使用 GCP 作为我们部署的云提供商：[5](#filepos1072709)
- en: '`apiVersion``:` `apps/v1` `kind``:` `Deployment` `metadata``:``labels``:``app``:`
    `ml-pipelines` `name``:` `ml-pipelines` `spec``:``replicas``:` `1` ![](images/00002.jpg)`selector``:``matchLabels``:``app``:`
    `ml-pipelines` `template``:``spec``:``containers``:` `-` `args``:` `- --rest_api_port=8501
                - --model_name=my_model             - --model_base_path=gs://your_gcp_bucket/my_model`
    ![](images/00075.jpg)`command``:` `- /usr/bin/tensorflow_model_server` `env``:`
    `-` `name``:` `GOOGLE_APPLICATION_CREDENTIALS` `value``:` `/secret/gcp-credentials/user-gcp-sa.json`
    ![](images/00064.jpg)`image``:` `tensorflow/serving` ![](images/00055.jpg)`name``:`
    `ml-pipelines` `ports``:` `-` `containerPort``:` `8501` `volumeMounts``:` `-`
    `mountPath``:` `/secret/gcp-credentials` ![](images/00082.jpg)`name``:` `gcp-credentials`
    `volumes``:` `-` `name``:` `gcp-credentials` `secret``:``secretName``:` `gcp-credentials`
    ![](images/00094.jpg)'
  id: totrans-157
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`apiVersion``:` `apps/v1` `kind``:` `Deployment` `metadata``:``labels``:``app``:`
    `ml-pipelines` `name``:` `ml-pipelines` `spec``:``replicas``:` `1` ![](images/00002.jpg)`selector``:``matchLabels``:``app``:`
    `ml-pipelines` `template``:``spec``:``containers``:` `-` `args``:` `- --rest_api_port=8501
                - --model_name=my_model             - --model_base_path=gs://your_gcp_bucket/my_model`
    ![](images/00075.jpg)`command``:` `- /usr/bin/tensorflow_model_server` `env``:`
    `-` `name``:` `GOOGLE_APPLICATION_CREDENTIALS` `value``:` `/secret/gcp-credentials/user-gcp-sa.json`
    ![](images/00064.jpg)`image``:` `tensorflow/serving` ![](images/00055.jpg)`name``:`
    `ml-pipelines` `ports``:` `-` `containerPort``:` `8501` `volumeMounts``:` `-`
    `mountPath``:` `/secret/gcp-credentials` ![](images/00082.jpg)`name``:` `gcp-credentials`
    `volumes``:` `-` `name``:` `gcp-credentials` `secret``:``secretName``:` `gcp-credentials`
    ![](images/00094.jpg)'
- en: '![](images/00002.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: Increase replicas if needed.
  id: totrans-159
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如有需要增加副本。
- en: '![](images/00075.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00075.jpg)'
- en: Load model from remote location.
  id: totrans-161
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 从远程位置加载模型。
- en: '![](images/00064.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00064.jpg)'
- en: Provide cloud credentials here for GCP.
  id: totrans-163
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在此处提供用于 GCP 的云凭据。
- en: '![](images/00055.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00055.jpg)'
- en: Load the prebuilt TensorFlow Serving image.
  id: totrans-165
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 加载预构建的 TensorFlow Serving 镜像。
- en: '![](images/00082.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00082.jpg)'
- en: Mount the service account credential file (if Kubernetes cluster is deployed
    through the GCP).
  id: totrans-167
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 挂载服务帐号凭据文件（如果 Kubernetes 集群通过 GCP 部署）。
- en: '![](images/00094.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00094.jpg)'
- en: Load credential file as a volume.
  id: totrans-169
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 将凭据文件作为卷加载。
- en: With this example, we can now deploy and scale your TensorFlow or Keras models
    without building custom Docker images.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个示例，我们现在可以部署和扩展您的 TensorFlow 或 Keras 模型，而无需构建自定义 Docker 镜像。
- en: 'You can create your service account credential file within the Kubernetes environment
    with the following command:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下命令在 Kubernetes 环境中创建您的服务帐号凭据文件：
- en: '`$` `kubectl create secret generic gcp-credentials` `\` `--from-file``=``/path/to/your/user-gcp-sa.json`'
  id: totrans-172
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `kubectl create secret generic gcp-credentials` `\` `--from-file``=``/path/to/your/user-gcp-sa.json`'
- en: 'A corresponding service setup in Kubernetes for the given model deployment
    could look like the following configuration:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 给定模型部署的相应 Kubernetes 服务设置可能如下所示配置：
- en: '`apiVersion``:` `v1` `kind``:` `Service` `metadata``:``name``:` `ml-pipelines`
    `spec``:``ports``:` `-` `name``:` `http` `nodePort``:` `30601` `port``:` `8501`
    `selector``:``app``:` `ml-pipelines` `type``:` `NodePort`'
  id: totrans-174
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`apiVersion``:` `v1` `kind``:` `Service` `metadata``:``name``:` `ml-pipelines`
    `spec``:``ports``:` `-` `name``:` `http` `nodePort``:` `30601` `port``:` `8501`
    `selector``:``app``:` `ml-pipelines` `type``:` `NodePort`'
- en: With a few lines of YAML configuration code, you can now deploy and, most importantly,
    scale your machine learning deployments. For more complex scenarios like traffic
    routing to deployed ML models with Istio, we highly recommend a deep dive into
    Kubernetes and Kubeflow.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 使用几行 YAML 配置代码，您现在可以部署并且最重要的是扩展您的机器学习部署。对于更复杂的情况，如使用 Istio 对部署的 ML 模型进行流量路由，我们强烈建议深入了解
    Kubernetes 和 Kubeflow。
- en: FURTHER READING ON KUBERNETES AND KUBEFLOW
  id: totrans-176
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 深入阅读 Kubernetes 和 Kubeflow
- en: 'Kubernetes and Kubeflow are amazing DevOps tools, and we couldn’t provide a
    holistic introduction here. It requires its own publications. If you are looking
    for further reading on the two topics, we can recommend the following publications:'
  id: totrans-177
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Kubernetes 和 Kubeflow 是令人惊叹的 DevOps 工具，我们无法在这里提供全面的介绍。它们需要单独的出版物。如果您想进一步了解这两个主题，我们可以推荐以下出版物：
- en: 'Kubernetes: Up and Running, 2nd edition by Brendan Burns et al. (O’Reilly)'
  id: totrans-178
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[Kubernetes 实战](https://wiki.example.org/kubernetes_up_and_running)，第二版，由布兰登·伯恩斯等人（O’Reilly）编写。'
- en: Kubeflow Operations Guide by Josh Patterson et al. (O’Reilly)
  id: totrans-179
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[Kubeflow 操作指南](https://wiki.example.org/kubeflow_operations_guide) 由乔什·帕特森等人（O’Reilly）编写。'
- en: Kubeflow for Machine Learning (forthcoming) by Holden Karau et al. (O’Reilly)
  id: totrans-180
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[Kubeflow 机器学习](https://wiki.example.org/kubeflow_for_machine_learning)（即将推出）由霍尔登·卡劳等人（O’Reilly）编写。'
- en: Summary
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we discussed advanced deployment scenarios, such as splitting
    the data science and DevOps deployment life cycles by deploying models via remote
    cloud storage buckets, optimizing models to reducing the prediction latency and
    the model memory footprint, or how to scale your deployment.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了高级部署场景，例如通过远程云存储桶部署模型来分离数据科学和DevOps部署生命周期，优化模型以减少预测延迟和模型内存占用，或者如何扩展您的部署。
- en: In the following chapter, we now want to combine all the individual pipeline
    components into a single machine learning pipeline to provide reproducible machine
    learning workflows.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们现在希望将所有单独的流水线组件组合成一个单一的机器学习流水线，以提供可重复使用的机器学习工作流。
- en: '[1  ](#filepos1002414) More details on managing AWS access keys can be found
    [in the documentation](https://oreil.ly/pHJ5N).'
  id: totrans-184
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[1  ](#filepos1002414) 有关管理AWS访问密钥的更多详细信息，请查看[文档](https://oreil.ly/pHJ5N)。'
- en: '[2  ](#filepos1009227) More details on how to create and manage service accounts
    can be found [in the documentation](https://oreil.ly/pbO8q).'
  id: totrans-185
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[2  ](#filepos1009227) 有关如何创建和管理服务帐户的详细信息，请查看[文档](https://oreil.ly/pbO8q)。'
- en: '[3  ](#filepos1019737) See TensorFlow’s website for more information about
    [optimzation methods](https://oreil.ly/UGjss) and [an in-depth pruning example](https://oreil.ly/n9rWc).'
  id: totrans-186
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[3  ](#filepos1019737) 请访问TensorFlow网站获取有关[优化方法](https://oreil.ly/UGjss)和[深度修剪示例](https://oreil.ly/n9rWc)的更多信息。'
- en: '[4  ](#filepos1022170) See [Nvidia’s documentation about TensorRT](https://oreil.ly/Ft8Y2).'
  id: totrans-187
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[4  ](#filepos1022170) 请参阅[Nvidia TensorRT文档](https://oreil.ly/Ft8Y2)。'
- en: '[5  ](#filepos1057174) Deployments with AWS are similar; instead of the credential
    file, the AWS `secret` and `key` need to be provided as environment variables.'
  id: totrans-188
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[5  ](#filepos1057174) AWS的部署类似；需要提供AWS的`secret`和`key`作为环境变量，而不是凭证文件。'
