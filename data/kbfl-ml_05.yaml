- en: Chapter 4\. Kubeflow Pipelines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章 Kubeflow Pipelines
- en: In the previous chapter we described [Kubeflow Pipelines](https://oreil.ly/387tH),
    the component of Kubeflow that orchestrates machine learning applications. Orchestration
    is necessary because a typical machine learning implementation uses a combination
    of tools to prepare data, train the model, evaluate performance, and deploy. By
    formalizing the steps and their sequencing in code, pipelines allow users to formally
    capture all of the data processing steps, ensuring their reproducibility and auditability,
    and training and deployment steps.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了[Kubeflow Pipelines](https://oreil.ly/387tH)，这是Kubeflow中用于编排机器学习应用程序的组件。编排是必要的，因为典型的机器学习实现使用一系列工具来准备数据，训练模型，评估性能和部署。通过在代码中形式化步骤及其顺序，流水线允许用户正式捕捉所有数据处理步骤，确保其可重现性和可审计性，以及训练和部署步骤。
- en: We will start this chapter by taking a look at the Pipelines UI and showing
    how to start writing simple pipelines in Python. We’ll explore how to transfer
    data between stages, then continue by getting into ways of leveraging existing
    applications as part of a pipeline. We will also look at the underlying workflow
    engine—Argo Workflows, a standard Kubernetes pipeline tool—that Kubeflow uses
    to run pipelines. Understanding the basics of Argo Workflows allows you to gain
    a deeper understanding of Kubeflow Pipelines and will aid in debugging. We will
    then show what Kubeflow Pipelines adds to Argo.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从查看Pipelines UI开始本章，并展示如何开始使用Python编写简单的流水线。我们将探讨如何在各个阶段之间传输数据，然后继续探讨如何利用现有应用作为流水线的一部分。我们还将查看底层工作流引擎——Argo
    Workflows，这是Kubeflow用来运行流水线的标准Kubernetes流水线工具。理解Argo Workflows的基础知识可以帮助您更深入地了解Kubeflow
    Pipelines，并有助于调试。接下来，我们将展示Kubeflow Pipelines在Argo上的增强功能。
- en: We’ll wrap up Kubeflow Pipelines by showing how to implement conditional execution
    in pipelines and how to run pipelines execution on schedule. Task-specific components
    of pipelines will be covered in their respective chapters.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将展示如何在Kubeflow Pipelines中实现条件执行以及如何按计划运行流水线执行，来完成Kubeflow Pipelines的总结。流水线的任务特定组件将在各自的章节中进行详细介绍。
- en: Getting Started with Pipelines
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用流水线
- en: 'The Kubeflow Pipelines platform consists of:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow Pipelines平台包括：
- en: A UI for managing and tracking pipelines and their execution
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一个用于管理和跟踪流水线及其执行的UI。
- en: An engine for scheduling a pipeline’s execution
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于调度流水线执行的引擎
- en: An SDK for defining, building, and deploying pipelines in Python
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用Python定义、构建和部署流水线的SDK
- en: Notebook support for using the SDK and pipeline execution
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 笔记本支持使用SDK和流水线执行
- en: The easiest way to familiarize yourself with pipelines is to take a look at
    prepackaged examples.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 熟悉流水线的最简单方法是查看预装的示例。
- en: Exploring the Prepackaged Sample Pipelines
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索预打包的示例流水线
- en: To help users understand pipelines, Kubeflow installs with a few sample pipelines.
    You can find these prepackaged in the Pipeline web UI, as seen in [Figure 4-1](#kubeflow_pipelines_ui_prepackage_pipelines).
    Note that at the time of writing, only the Basic to Conditional execution pipelines
    are generic, while the rest of them will run only on Google Kubernetes Engine
    (GKE). If you try to run them on non-GKE environments, they will fail.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为帮助用户理解流水线，Kubeflow预装了一些示例流水线。您可以在Pipeline Web UI中找到这些预打包流水线，如[图 4-1](#kubeflow_pipelines_ui_prepackage_pipelines)中所示。请注意，在撰写时，只有从基本到条件执行的流水线是通用的，而其余流水线仅在Google
    Kubernetes Engine（GKE）上运行。如果您尝试在非GKE环境中运行它们，它们将失败。
- en: '![Kubeflow Pipelines UI - Prepackaged Pipelines](Images/kfml_0401.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![Kubeflow Pipelines UI - 预打包的流水线](Images/kfml_0401.png)'
- en: 'Figure 4-1\. Kubeflow pipelines UI: prepackaged pipelines'
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4-1\. Kubeflow pipelines UI: 预打包的流水线'
- en: Clicking a specific pipeline will show its execution graph or source, as seen
    in [Figure 4-2](#kubeflow_pipelines_ui_pipeline_graph_view).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 单击特定流水线将显示其执行图或源代码，如在[图 4-2](#kubeflow_pipelines_ui_pipeline_graph_view)中所示。
- en: '![Kubeflow Pipelines UI - Pipeline Graph View](Images/kfml_0402.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![Kubeflow Pipelines UI - Pipeline Graph View](Images/kfml_0402.png)'
- en: 'Figure 4-2\. Kubeflow pipelines UI: pipeline graph view'
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4-2\. Kubeflow pipelines UI: 流水线图视图'
- en: 'Clicking the source tab will show the pipeline’s compiled code, which is an
    Argo YAML file (this is covered in more detail in [“Argo: the Foundation of Pipelines”](#argo_foundation)).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '单击源代码选项卡将显示流水线的编译代码，这是一个Argo YAML文件（详细内容请参见[“Argo: Pipelines的基础”](#argo_foundation)）。'
- en: In this area you are welcome to experiment with running pipelines to get a better
    feel for their execution and the capabilities of the Pipelines UI.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此领域进行实验，运行流水线以更好地了解其执行和流水线 UI 的功能。
- en: To invoke a specific pipeline, simply click it; this will bring up Pipeline’s
    view as presented in [Figure 4-3](#kubeflow_pipelines_ui_pipeline_view).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要调用特定流水线，只需点击它；这将显示如[图 4-3](#kubeflow_pipelines_ui_pipeline_view)中所示的流水线视图。
- en: '![Kubeflow Pipelines UI - Pipeline View](Images/kfml_0403.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![Kubeflow 流水线 UI - 流水线视图](Images/kfml_0403.png)'
- en: 'Figure 4-3\. Kubeflow pipelines UI: pipeline view'
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-3\. Kubeflow 流水线 UI：流水线视图
- en: To run the pipeline, click the “Create Run” button and follow the instructions
    on the screen.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行流水线，请单击“创建运行”按钮，然后按照屏幕上的说明操作。
- en: Tip
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: When running a pipeline you must choose an experiment. Experiment here is just
    a convenience grouping for pipeline executions (runs). You can always use the
    “Default” experiment created by Kubeflow’s installation. Also, pick “One-off”
    for the Run type to execute the pipeline once. We will talk about recurring execution
    in [“Running Pipelines on Schedule”](#run_pipelines_onsched).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 运行流水线时，必须选择一个实验。这里的实验只是流水线执行（运行）的便利分组。您始终可以使用 Kubeflow 安装创建的“默认”实验。此外，选择“一次性”作为运行类型以执行一次流水线。我们将在[“定期运行流水线”](#run_pipelines_onsched)中讨论定期执行。
- en: Building a Simple Pipeline in Python
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Python 构建简单流水线
- en: 'We have seen how to execute precompiled Kubeflow Pipelines, now let’s investigate
    how to author our own new pipelines. Kubeflow Pipelines are stored as YAML files
    executed by a program called Argo (see [“Argo: the Foundation of Pipelines”](#argo_foundation)).
    Thankfully, Kubeflow exposes a Python [domain-specific language (DSL)](https://oreil.ly/7LdzK)
    for authoring pipelines. The DSL is a Pythonic representation of the operations
    performed in the ML workflow and built with ML workloads specifically in mind.
    The DSL also allows for some simple Python functions to be used as pipeline stages
    without you having to explicitly build a container.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '我们已经看到如何执行预编译的 Kubeflow 流水线，现在让我们来研究如何编写我们自己的新流水线。Kubeflow 流水线存储为 YAML 文件，由名为
    Argo 的程序执行（参见[“Argo: the Foundation of Pipelines”](#argo_foundation)）。幸运的是，Kubeflow
    提供了一个 Python [领域特定语言（DSL）](https://oreil.ly/7LdzK) 用于编写流水线。DSL 是对在 ML 工作流中执行的操作的
    Python 表示，并专门针对 ML 工作负载构建。DSL 还允许使用一些简单的 Python 函数作为流水线阶段，而无需显式构建容器。'
- en: Tip
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The Chapter 4 examples can be found in the notebooks in [this book’s GitHub
    repository](https://oreil.ly/Kubeflow_for_ML_ch04).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 本书第 4 章的示例可以在[此书的 GitHub 仓库的笔记本中找到](https://oreil.ly/Kubeflow_for_ML_ch04)。
- en: A pipeline is, in its essence, a graph of container execution. In addition to
    specifying which containers should run in which order, it also allows the user
    to pass arguments to the entire pipeline and between participating containers.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 流水线本质上是容器执行的图形。除了指定容器应按顺序运行外，它还允许用户将参数传递给整个流水线和参与容器之间。
- en: 'For each container (when using the Python SDK), we must:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个容器（使用 Python SDK 时），我们必须：
- en: Create the container—either as a simple Python function, or with any Docker
    container (read more in [Chapter 9](ch09.xhtml#beyond_tf)).
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建容器——可以是简单的 Python 函数，也可以是任何 Docker 容器（在[第 9 章](ch09.xhtml#beyond_tf)中了解更多）。
- en: Create an operation that references that container as well as the command line
    arguments, data mounts, and variable to pass the container.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建引用该容器以及要传递给容器的命令行参数、数据挂载和变量的操作。
- en: Sequence the operations, defining which may happen in parallel and which must
    complete before moving on to a further step.^([1](ch04.xhtml#idm45831179676952))
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顺序化操作，定义哪些操作可以并行进行，哪些必须在继续进行下一步之前完成。^([1](ch04.xhtml#idm45831179676952))
- en: Compile this pipeline, defined in Python, into a YAML file that Kubeflow Pipelines
    can consume.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将此在 Python 中定义的流水线编译成 Kubeflow Pipelines 可以消费的 YAML 文件。
- en: Pipelines are a key feature of Kubeflow and you will see them again throughout
    the book. In this chapter we are going to show the simplest examples possible
    to illustrate the basic principles of Pipelines. This won’t feel like “machine
    learning” and that is by design.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 流水线是 Kubeflow 的一个关键功能，你将在整本书中多次看到它们。在本章中，我们将展示一些最简单的示例，以说明流水线的基本原理。这不会感觉像“机器学习”，这是有设计意图的。
- en: For our first Kubeflow operation, we are going to use a technique known as *lightweight
    Python functions*. We should not, however, let the word *lightweight* deceive
    us. In a lightweight Python function, we define a Python function and then let
    Kubeflow take care of packaging that function into a container and creating an
    operation.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的第一个 Kubeflow 操作，我们将使用一种称为*轻量级 Python 函数*的技术。然而，我们不应该让*轻量级*这个词愚弄我们。在轻量级
    Python 函数中，我们定义一个 Python 函数，然后让 Kubeflow 负责将该函数打包到一个容器中并创建一个操作。
- en: For the sake of simplicity, let’s declare the simplest of functions an echo.
    That is a function that takes a single input, an integer, and returns that input.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，让我们声明最简单的函数作为回显。这是一个接受单个输入（整数）并返回该输入的函数。
- en: 'Let’s start by importing `kfp` and defining our function:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从导入 `kfp` 并定义我们的函数开始：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Warning
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Note that we use `snake_case`, not `camelCase`, for our function names. At
    the time of writing there exists a bug (feature?) such that camel case names (for
    example: naming our function `simpleEcho`) will produce errors.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们使用 `snake_case`，而不是 `camelCase` 作为我们的函数名称。在撰写本文时，存在一个错误（特性？），即使用驼峰命名（例如：将我们的函数命名为
    `simpleEcho`）会导致错误。
- en: 'Next, we want to wrap our function `simple_echo` into a Kubeflow Pipeline operation.
    There’s a nice little method to do this: `kfp.components.func_to_container_op`.
    This method returns a factory function with a strongly typed signature:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们想要将我们的函数 `simple_echo` 包装到一个 Kubeflow 流水线操作中。有一个很好的方法可以做到这一点：`kfp.components.func_to_container_op`。此方法返回一个工厂函数，具有强类型签名：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'When we create a pipeline in the next step, the factory function will construct
    a ContainerOp, which will run the original function (echo_fn) in a container:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在下一步创建流水线时，工厂函数将构造一个 ContainerOp，该操作将在容器中运行原始函数（echo_fn）：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Tip
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If your code can be accelerated by a GPU it is easy to mark a stage as using
    GPU resources; simply add `.set_gpu_limit(NUM_GPUS)` to your `ContainerOp`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的代码可以通过 GPU 加速，那么很容易将一个阶段标记为使用 GPU 资源；只需将 `.set_gpu_limit(NUM_GPUS)` 添加到您的
    `ContainerOp` 中。
- en: Now let’s sequence the ContainerOp(s) (there is only one) into a pipeline. This
    pipeline will take one parameter (the number we will echo). The pipeline also
    has a bit of metadata associated with it. While echoing numbers may be a trivial
    use of parameters, in real-world use cases you would include variables you might
    want to tune later such as hyperparameters for machine learning algorithms.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将 ContainerOp(s)（只有一个）按顺序排列到一个流水线中。这个流水线将接受一个参数（我们将要回显的数字）。此流水线还带有一些与其关联的元数据。虽然回显数字可能是参数使用的一个微不足道的例子，但在实际用例中，您将包括稍后可能想要调整的变量，如机器学习算法的超参数。
- en: Finally, we compile our pipeline into a zipped YAML file, which we can then
    upload to the Pipelines UI.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将我们的流水线编译成一个压缩的 YAML 文件，然后我们可以将其上传到 Pipelines UI。
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Tip
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: It is also possible to run the pipeline directly from the notebook, which we’ll
    do in the next example.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以直接从笔记本运行流水线，我们将在下一个示例中这样做。
- en: A pipeline with only one component is not very interesting. For our next example,
    we will customize the containers of our lightweight Python functions. We’ll create
    a new pipeline that installs and imports additional Python libraries, builds from
    a specified base image, and passes output between containers.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 只有一个组件的流水线并不是非常有趣。对于我们的下一个示例，我们将自定义轻量级 Python 函数的容器。我们将创建一个新的流水线，安装和导入额外的 Python
    库，从指定的基础镜像构建，并在容器之间传递输出。
- en: We are going to create a pipeline that divides a number by another number, and
    then adds a third number. First let’s create our simple `add` function, as shown
    in [Example 4-1](#simple_python_function).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个流水线，将一个数字除以另一个数字，然后再加上第三个数字。首先让我们创建我们简单的 `add` 函数，如 [示例 4-1](#simple_python_function)
    中所示。
- en: Example 4-1\. A simple Python function
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-1\. 一个简单的 Python 函数
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, let’s create a slightly more complex function. Additionally, let’s have
    this function require and import from a nonstandard Python library, `numpy`. This
    must be done within the function. That is because global imports from the notebook
    will not be packaged into the containers we create. Of course, it is also important
    to make sure that our container has the libraries we are importing installed.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建一个稍微复杂一些的函数。此外，让我们让这个函数需要从一个非标准的 Python 库 `numpy` 进行导入。这必须在函数内完成。这是因为从笔记本进行的全局导入不会打包到我们创建的容器中。当然，确保我们的容器安装了我们导入的库也是很重要的。
- en: To do that we’ll pass the specific container we want to use as our base image
    to `.func_to_container(`, as in [Example 4-2](#less_simple_pyfunction).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将传递我们想要用作基础镜像的特定容器到 `.func_to_container(`，就像 [示例 4-2](#less_simple_pyfunction)
    中一样。
- en: Example 4-2\. A less-simple Python function
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-2\. 一个稍复杂的Python函数
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](Images/1.png)](#co_kubeflow_pipelines_CO1-1)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_kubeflow_pipelines_CO1-1)'
- en: Importing libraries inside the function.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在函数内部导入库。
- en: '[![2](Images/2.png)](#co_kubeflow_pipelines_CO1-2)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_kubeflow_pipelines_CO1-2)'
- en: Nested functions inside lightweight Python functions are also OK.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 轻量级Python函数内部的嵌套函数也是可以的。
- en: '[![3](Images/3.png)](#co_kubeflow_pipelines_CO1-3)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_kubeflow_pipelines_CO1-3)'
- en: Calling for a specific base container.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 调用特定的基础容器。
- en: Now we will build a pipeline. The pipeline in [Example 4-3](#simple_pipeline)
    uses the functions defined previously, `my_divmod` and `add`, as stages.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将构建一个流水线。[示例 4-3](#simple_pipeline) 中的流水线使用之前定义的函数 `my_divmod` 和 `add` 作为阶段。
- en: Example 4-3\. A simple pipeline
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-3\. 一个简单的流水线
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](Images/1.png)](#co_kubeflow_pipelines_CO2-1)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_kubeflow_pipelines_CO2-1)'
- en: Values being passed between containers. Order of operations is inferred from
    this.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 传递在容器之间传递的值。操作顺序可从此推断。
- en: 'Finally, we use the client to submit the pipeline for execution, which returns
    the links to execution and experiment. Experiments group the executions together.
    You can also use `kfp.compiler.Compiler().compile` and upload the zip file as
    in the first example if you prefer:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用客户端提交流水线进行执行，返回执行和实验的链接。实验将执行组合在一起。如果您更喜欢，也可以使用 `kfp.compiler.Compiler().compile`
    并上传zip文件，就像在第一个示例中一样：
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Following the link returned by `create_run_from_pipeline_func`, we can get to
    the execution web UI, which shows the pipeline itself and intermediate results,
    as seen in [Figure 4-4](#pipeline_execution).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 跟随 `create_run_from_pipeline_func` 返回的链接，我们可以进入执行的Web UI，显示流水线本身和中间结果，如 [图 4-4](#pipeline_execution)
    中所示。
- en: '![Pipeline Execution](Images/kfml_0404.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![流水线执行](Images/kfml_0404.png)'
- en: Figure 4-4\. Pipeline execution
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-4\. 流水线执行
- en: As we’ve seen, the *lightweight* in *lightweight Python functions* refers to
    the ease of making these steps in our process and not the power of the functions
    themselves. We can use custom imports, base images, and how to hand off small
    results between containers.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，*轻量级* 在 *轻量级Python函数* 中指的是我们流程中这些步骤的易于完成，而不是函数本身的功能强大。我们可以使用自定义导入、基础镜像，并学习如何在容器之间传递小结果。
- en: In the next section, we’ll show how to hand larger data files between containers
    by mounting volumes to the containers.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将展示如何通过为容器挂载卷来传递更大的数据文件。
- en: Storing Data Between Steps
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在步骤之间存储数据
- en: In the previous example, the data passed between containers was small and of
    primitive types (such as numeric, string, list, and arrays). In practice however,
    we will likely be passing much larger data (for instance, entire datasets). In
    Kubeflow, there are two primary methods for doing this—persistent volumes inside
    the Kubernetes cluster, and cloud storage options (such as S3), though each method
    has inherent problems.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，容器之间传递的数据很小，是原始类型（如数值、字符串、列表和数组）。然而，在实际中，我们很可能传递更大的数据（例如，整个数据集）。在Kubeflow中，有两种主要的方法可以做到这一点——Kubernetes集群内的持久卷和云存储选项（如S3），尽管每种方法都有其固有的问题。
- en: Persistent volumes abstract the storage layer. Depending on the vendor, persistent
    volumes can be slow with provisioning and have IO limits. Check to see if your
    vendor supports read-write-many storage classes, allowing for storage access by
    multiple pods, which is required for some types of parallelism. Storage classes
    can be one of the following.^([2](ch04.xhtml#idm45831178994504))
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 持久卷抽象了存储层。根据供应商不同，持久卷的配置可能较慢，并且具有IO限制。请检查供应商是否支持读写多种存储类，以允许多个Pod访问存储，这对于某些类型的并行性是必需的。存储类可以是以下之一。^([2](ch04.xhtml#idm45831178994504))
- en: ReadWriteOnce
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ReadWriteOnce
- en: The volume can be mounted as read-write by a single node.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 卷可以被单个节点以读写方式挂载。
- en: ReadOnlyMany
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ReadOnlyMany
- en: The volume can be mounted read-only by many nodes.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 卷可以被多个节点以只读方式挂载。
- en: ReadWriteMany
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ReadWriteMany
- en: The volume can be mounted as read-write by many nodes.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 卷可以被多个节点以读写方式挂载。
- en: Your system/cluster administrator may be able to add read-write-many support.^([3](ch04.xhtml#idm45831178988264))
    Additionally, many cloud providers include their proprietary read-write-many implementations,
    see for example [dynamic provisioning](https://oreil.ly/je18X) on GKE. but make
    sure to ask if there is a single node bottleneck.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 您的系统/集群管理员可能能够添加读写多支持。^([3](ch04.xhtml#idm45831178988264)) 另外，许多云服务提供商包括他们专有的读写多实现，例如在
    GKE 上查看 [动态配置](https://oreil.ly/je18X)，但请确保询问是否存在单节点瓶颈。
- en: 'Kubeflow Pipelines’ `VolumeOp` allows you to create an automatically managed
    persistent volume, as shown in [Example 4-4](#make_volume_ch4). To add the volume
    to your operation you can just call `add_pvolumes` with a dictionary of mount
    points to volumes, e.g., `download_data_op(year).add_pvolumes({"/data_processing":
    dvop.volume})`.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 'Kubeflow Pipelines 的 `VolumeOp` 允许您创建自动管理的持久卷，如 [Example 4-4](#make_volume_ch4)
    所示。要将卷添加到操作中，只需调用 `add_pvolumes`，并传递一个挂载点到卷的字典，例如 `download_data_op(year).add_pvolumes({"/data_processing":
    dvop.volume})`。'
- en: Example 4-4\. Mailing list data prep
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 4-4\. 邮件列表数据准备
- en: '[PRE8]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: While less common in the Kubeflow examples, using an object storage solution,
    in some cases, may be more suitable. MinIO provides cloud native object storage
    by working either as a gateway to an existing object storage engine or on its
    own.^([4](ch04.xhtml#idm45831178937864)) We covered how to configure MinIO back
    in [Chapter 3](ch03.xhtml#kubeflow_design_beyond_basics).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在 Kubeflow 示例中不太常见，但在某些情况下，使用对象存储解决方案可能更合适。MinIO 通过作为现有对象存储引擎的网关或独立运行，提供云原生对象存储。^([4](ch04.xhtml#idm45831178937864))
    我们在 [第三章](ch03.xhtml#kubeflow_design_beyond_basics) 中介绍了如何配置 MinIO。
- en: Kubeflow’s built-in `file_output` mechanism automatically transfers the specified
    local file into MinIO between pipeline steps for you. To use `file_output`, write
    your files locally in your container and specify the parameter in your `ContainerOp`,
    as shown in [Example 4-5](#file_output_ex).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow 的内置 `file_output` 机制可以在流水线步骤之间自动传输指定的本地文件到 MinIO。要使用 `file_output`，请在容器中将文件写入本地，并在
    `ContainerOp` 中指定参数，如 [Example 4-5](#file_output_ex) 所示。
- en: Example 4-5\. File output example
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 4-5\. 文件输出示例
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If you don’t want to use MinIO, you can also directly use your provider’s object
    storage, but this may compromise some portability.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不想使用 MinIO，您也可以直接使用您提供商的对象存储，但这可能会影响一些可移植性。
- en: The ability to mount data locally is an essential task in any machine learning
    pipeline. Here we have briefly outlined multiple methods and provided examples
    for each.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 挂载数据到本地是任何机器学习流水线中的基本任务。我们在此简要概述了多种方法，并提供了每种方法的示例。
- en: Introduction to Kubeflow Pipelines Components
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubeflow Pipelines 组件介绍
- en: Kubeflow Pipelines builds on [Argo Workflows](https://oreil.ly/S2GuQ), an open
    source, container-native workflow engine for Kubernetes. In this section we will
    describe how Argo works, what it does, and how Kubeflow Pipeline supplements Argo
    to make it easier to use by data scientists.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow Pipelines 基于 [Argo Workflows](https://oreil.ly/S2GuQ)，这是一个针对 Kubernetes
    的开源、容器本地的工作流引擎。在本节中，我们将描述 Argo 的工作原理、其功能，以及 Kubeflow Pipeline 如何补充 Argo 以便数据科学家更容易使用。
- en: 'Argo: the Foundation of Pipelines'
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Argo：流水线的基础
- en: Kubeflow installs all of the Argo components. Though having Argo installed on
    your computer is not necessary to use Kubeflow Pipelines, having the Argo command-line
    tool makes it easier to understand and debug your pipelines.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow 安装了所有 Argo 组件。虽然在您的计算机上安装 Argo 不是使用 Kubeflow Pipelines 的必要条件，但使用 Argo
    命令行工具可以更轻松地理解和调试您的流水线。
- en: Tip
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Tip
- en: By default, Kubeflow configures Argo to use the Docker executor. If your platform
    does not support the Docker APIs, you need to switch your executor to a compatible
    one. This is done by changing the `containerRuntimeExecutor` value in the Argo
    *params* file. See [Appendix A](app01.xhtml#appendix_executors) for details on
    the trade-offs. The majority of the examples in this book use the Docker executor
    but can be adapted to other executors.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Kubeflow 配置 Argo 使用 Docker 执行器。如果您的平台不支持 Docker API，则需要将执行器切换为兼容的执行器。这可以通过在
    Argo *params* 文件中更改 `containerRuntimeExecutor` 值来完成。有关权衡的详细信息，请参见 [附录 A](app01.xhtml#appendix_executors)。本书中的大多数示例使用
    Docker 执行器，但可以调整为其他执行器。
- en: On macOS, you can install Argo with Homebrew, as shown in [Example 4-6](#argo_dl_linux).^([5](ch04.xhtml#idm45831178821608))
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在 macOS 上，您可以使用 Homebrew 安装 Argo，如 [Example 4-6](#argo_dl_linux) 所示。^([5](ch04.xhtml#idm45831178821608))
- en: Example 4-6\. Argo installation
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 4-6\. 安装 Argo
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You can verify your Argo installation by running the Argo examples with the
    command-line tool in the Kubeflow namespace: follow [these Argo instructions](https://oreil.ly/QFxv2).
    When you run the Argo examples the pipelines are visible with the `argo` command,
    as in [Example 4-7](#List_Argo_executions).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 'You can verify your Argo installation by running the Argo examples with the
    command-line tool in the Kubeflow namespace: follow [these Argo instructions](https://oreil.ly/QFxv2).
    When you run the Argo examples the pipelines are visible with the `argo` command,
    as in [Example 4-7](#List_Argo_executions).'
- en: Example 4-7\. Listing Argo executions
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 4-7\. Listing Argo executions
- en: '[PRE11]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Since pipelines are implemented with Argo, you can use the same technique to
    check on them as well. You can also get information about specific workflow execution,
    as shown in [Example 4-8](#Get_Argo_execution_details).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Since pipelines are implemented with Argo, you can use the same technique to
    check on them as well. You can also get information about specific workflow execution,
    as shown in [Example 4-8](#Get_Argo_execution_details).
- en: Example 4-8\. Getting Argo execution details
  id: totrans-112
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 4-8\. Getting Argo execution details
- en: '[PRE12]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](Images/1.png)](#co_kubeflow_pipelines_CO3-1)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_kubeflow_pipelines_CO3-1)'
- en: '`hello-world-wsxbr` is the name that we got using `argo list -n kubeflow` above.
    In your case the name will be different.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`hello-world-wsxbr` is the name that we got using `argo list -n kubeflow` above.
    In your case the name will be different.'
- en: We can also view the execution logs by using the command in [Example 4-9](#Get_log_Argo_execution).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: We can also view the execution logs by using the command in [Example 4-9](#Get_log_Argo_execution).
- en: Example 4-9\. Getting the log of Argo execution
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 4-9\. Getting the log of Argo execution
- en: '[PRE13]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This produces the result shown in [Example 4-10](#Argo_execution_log).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: This produces the result shown in [Example 4-10](#Argo_execution_log).
- en: Example 4-10\. Argo execution log
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 4-10\. Argo execution log
- en: '[PRE14]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You can also delete a specific workflow; see [Example 4-11](#Deleting_Argo_execution).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: You can also delete a specific workflow; see [Example 4-11](#Deleting_Argo_execution).
- en: Example 4-11\. Deleting Argo execution
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 4-11\. Deleting Argo execution
- en: '[PRE15]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Alternatively, you can get pipeline execution information using the Argo UI,
    as seen in [Figure 4-5](#argo_ui_for_pipeline_execution).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Alternatively, you can get pipeline execution information using the Argo UI,
    as seen in [Figure 4-5](#argo_ui_for_pipeline_execution).
- en: '![Argo UI for pipelines execution](Images/kfml_0405.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![Argo UI for pipelines execution](Images/kfml_0405.png)'
- en: Figure 4-5\. Argo UI for pipeline execution
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 4-5\. Argo UI for pipeline execution
- en: You can also look at the details of the flow execution graph by clicking a specific
    workflow, as seen in [Figure 4-6](#argo_ui_execution_graph).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: You can also look at the details of the flow execution graph by clicking a specific
    workflow, as seen in [Figure 4-6](#argo_ui_execution_graph).
- en: '![Argo UI - Execution Graph](Images/kfml_0406.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![Argo UI - Execution Graph](Images/kfml_0406.png)'
- en: Figure 4-6\. Argo UI execution graph
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 4-6\. Argo UI execution graph
- en: For any Kubeflow pipeline you run, you can also view that pipeline in the Argo
    CLI/UI. Note that because ML pipelines are using the Argo CRD, you can also see
    the result of the pipeline execution in the Argo UI (as in [Figure 4-7](#viewing_kuberflow_pipelines_in_argo_ui)).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: For any Kubeflow pipeline you run, you can also view that pipeline in the Argo
    CLI/UI. Note that because ML pipelines are using the Argo CRD, you can also see
    the result of the pipeline execution in the Argo UI (as in [Figure 4-7](#viewing_kuberflow_pipelines_in_argo_ui)).
- en: '![Viewing Kubeflow Pipelines in Argo UI](Images/kfml_0407.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![Viewing Kubeflow Pipelines in Argo UI](Images/kfml_0407.png)'
- en: Figure 4-7\. Viewing Kubeflow pipelines in Argo UI
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 4-7\. Viewing Kubeflow pipelines in Argo UI
- en: Tip
  id: totrans-134
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Tip
- en: Currently, the Kubeflow community is actively looking at alternative foundational
    technologies for running Kubeflow pipelines, one of which is [Tekton](https://tekton.dev).
    The paper by A. Singh et al., [“Kubeflow Pipelines with Tekton”](https://oreil.ly/rrg-V),
    gives “initial design, specifications, and code for enabling Kubeflow Pipelines
    to run on top of Tekton.” The basic idea here is to create an intermediate format
    that can be produced by pipelines and then executed using Argo, Tekton, or other
    runtimes. The initial code for this implementation is found in [this Kubeflow
    GitHub repo](https://oreil.ly/nes4r).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Currently, the Kubeflow community is actively looking at alternative foundational
    technologies for running Kubeflow pipelines, one of which is [Tekton](https://tekton.dev).
    The paper by A. Singh et al., [“Kubeflow Pipelines with Tekton”](https://oreil.ly/rrg-V),
    gives “initial design, specifications, and code for enabling Kubeflow Pipelines
    to run on top of Tekton.” The basic idea here is to create an intermediate format
    that can be produced by pipelines and then executed using Argo, Tekton, or other
    runtimes. The initial code for this implementation is found in [this Kubeflow
    GitHub repo](https://oreil.ly/nes4r).
- en: What Kubeflow Pipelines Adds to Argo Workflow
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: What Kubeflow Pipelines Adds to Argo Workflow
- en: Argo underlies the workflow execution; however, using it directly requires you
    to do awkward things. First, you must define your workflow in YAML, which can
    be difficult. Second, you must containerize your code, which can be tedious. The
    main advantage of KF Pipelines is that you can use Python APIs for defining/creating
    pipelines, which automates the generation of much of the YAML boilerplate for
    workflow definitions and is extremely friendly for data scientists/Python developers.
    Kubeflow Pipelines also has hooks that add building blocks for machine learning-specific
    components. These APIs not only generate the YAML but can also simplify container
    creation and resource usage. In addition to the APIs, Kubeflow adds a recurring
    scheduler and UI for configuration and execution.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Argo 是工作流执行的基础；然而，直接使用它需要您做一些笨拙的事情。首先，您必须在 YAML 中定义工作流，这可能很困难。其次，您必须将您的代码容器化，这可能很繁琐。KF
    Pipelines 的主要优势在于，您可以使用 Python API 定义/创建管道，这自动化了工作流定义的大部分 YAML 样板，并且非常适合数据科学家/Python
    开发人员。Kubeflow Pipelines 还添加了用于机器学习特定组件的构建块的钩子。这些 API 不仅生成 YAML，还可以简化容器创建和资源使用。除了
    API 外，Kubeflow 还添加了一个定期调度程序和用于配置和执行的 UI。
- en: Building a Pipeline Using Existing Images
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用现有镜像构建管道
- en: Building pipeline stages directly from Python provides a straightforward entry
    point. It does limit our implementation to Python, though. Another feature of
    Kubeflow Pipelines is the ability to orchestrate the execution of a multilanguage
    implementation leveraging prebuilt Docker images (see [Chapter 9](ch09.xhtml#beyond_tf)).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 直接从 Python 构建管道阶段提供了一个简单的入口点。尽管如此，这限制了我们的实现仅限于 Python。Kubeflow Pipelines 的另一个特性是能够编排执行多语言实现，利用预构建的
    Docker 镜像（见 [第 9 章](ch09.xhtml#beyond_tf)）。
- en: In addition to our previous imports, we also want to import the Kubernetes client,
    which allows us to use Kubernetes functions directly from Python code (see [Example 4-12](#Export_kubernetes_cli)).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 除了之前的导入外，我们还希望导入 Kubernetes 客户端，这使我们可以直接从 Python 代码中使用 Kubernetes 函数（见 [示例 4-12](#Export_kubernetes_cli)）。
- en: Example 4-12\. Exporting Kubernetes client
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-12\. 导出 Kubernetes 客户端
- en: '[PRE16]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Again, we create a client and experiment to run our pipeline. As mentioned earlier,
    experiments group the runs of pipelines. You can only create a given experiment
    once, so [Example 4-13](#Obtain_pipeline_exper) shows how to either create a new
    experiment or use an existing one.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们创建一个客户端和实验来运行我们的管道。正如前面提到的，实验将管道运行分组。您只能创建给定实验一次，因此 [示例 4-13](#Obtain_pipeline_exper)
    展示了如何创建新实验或使用现有实验。
- en: Example 4-13\. Obtaining pipeline experiment
  id: totrans-144
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-13\. 获取管道实验
- en: '[PRE17]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now we create our pipeline ([Example 4-14](#ex_rec_pipeline)). The images used
    need to be accessible, and we’re specifying the full names, so they resolve. Since
    these containers are prebuilt, we need to configure them for our pipeline.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们创建我们的管道（见 [示例 4-14](#ex_rec_pipeline)）。所使用的镜像需要是可访问的，并且我们正在指定完整名称，以便解析。由于这些容器是预构建的，我们需要为我们的管道配置它们。
- en: The pre-built containers we are using have their storage configured by the `MINIO_*`
    environment variables. So we configure them to use our local MinIO install by
    calling `add_env_variable`.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用的预构建容器通过 `MINIO_*` 环境变量配置其存储。因此，我们通过调用 `add_env_variable` 来配置它们以使用我们的本地
    MinIO 安装。
- en: In addition to the automatic dependencies created when passing parameters between
    stages, you can also specify that a stage requires a previous stage with `after`.
    This is most useful when there is an external side effect, like updating a database.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在各阶段之间传递参数时自动生成的依赖关系外，您还可以使用 `after` 指定某个阶段需要前一个阶段。当存在外部副作用（例如更新数据库）时，这是非常有用的。
- en: Example 4-14\. Example recommender pipeline
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-14\. 示例推荐管道
- en: '[PRE18]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Since the pipeline definition is just code, you can make it more compact by
    using a loop to set the MinIO parameters instead of doing it on each stage.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 由于管道定义本质上是代码，您可以通过使用循环设置 MinIO 参数来使其更加紧凑，而不是在每个阶段都这样做。
- en: As before, we need to compile the pipeline, either explicitly with `compiler.Compiler().compile`
    or implicitly with `create_run_from_pipeline_func`. Now go ahead and run the pipeline
    (as in [Figure 4-8](#execution_recomm_pipelines_ex)).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，我们需要编译管道，可以使用 `compiler.Compiler().compile` 明确编译，也可以使用 `create_run_from_pipeline_func`
    隐式编译。现在，继续运行管道（如 [图 4-8](#execution_recomm_pipelines_ex)）。
- en: '![Execution of Recommender Pipelines Example](Images/kfml_0408.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![推荐管道执行示例](Images/kfml_0408.png)'
- en: Figure 4-8\. Execution of recommender pipelines example
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-8\. 推荐管道执行示例
- en: Kubeflow Pipeline Components
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubeflow 管道组件
- en: In addition to container operations which we’ve just discussed, Kubeflow Pipelines
    also exposes additional operations with components. Components expose different
    Kubernetes resources or external operations (like `dataproc`). Kubeflow components
    allow developers to package machine learning tools while abstracting away the
    specifics on the containers or CRDs used.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们刚讨论过的容器操作外，Kubeflow Pipelines 还公开了使用组件的其他操作。组件公开了不同的 Kubernetes 资源或外部操作（如
    `dataproc`）。Kubeflow 组件允许开发人员打包机器学习工具，同时抽象掉容器或 CRD 的具体细节。
- en: We have used Kubeflow’s building blocks fairly directly, and we have used the
    `func_to_container` component.^([6](ch04.xhtml#idm45831178079080)) Some components,
    like `func_to_container`, are available as Python code and can be imported like
    normal. Other components are specified using Kubeflow’s `component.yaml` system
    and need to be loaded. In our opinion, the best way to work with Kubeflow components
    is to download a specific tag of the repo, allowing us to use `load_component_from_file`,
    as shown in [Example 4-15](#dl_pipeline_release).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经相对直接地使用了 Kubeflow 的构建模块，并且使用了 `func_to_container` 组件。^([6](ch04.xhtml#idm45831178079080))
    一些组件，如 `func_to_container`，以普通的 Python 代码形式提供，并且可以像普通的库一样导入。其他组件使用 Kubeflow 的
    `component.yaml` 系统来指定，并需要加载。在我们看来，使用 Kubeflow 组件的最佳方式是下载仓库的特定标签，允许我们使用 `load_component_from_file`，如示例
    [4-15](#dl_pipeline_release) 所示。
- en: Example 4-15\. Pipeline release
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-15\. 管道发布
- en: '[PRE19]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Warning
  id: totrans-160
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: There is a `load_component` function that takes a component’s name and attempts
    to resolve it. We don’t recommend using this function since it defaults to a search
    path that includes fetching, from Github, the master branch of the pipelines library,
    which is unstable.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个 `load_component` 函数，它接受组件的名称并尝试解析它。我们不建议使用此函数，因为它默认搜索路径包括从 Github 获取 pipelines
    库的主分支，这是不稳定的。
- en: We explore data preparation components in depth in the next chapter; however,
    let’s quickly look at a file-fetching component as an example. In our recommender
    example earlier in the chapter, we used a special prebuilt container to fetch
    our data since it was not already in a persistent volume. Instead, we can use
    the Kubeflow GCS component `google-cloud/storage/download/` to download our data.
    Assuming you’ve downloaded the pipeline release as in [Example 4-15](#dl_pipeline_release),
    you can load the component with `load_component_from_file` as in [Example 4-16](#ex_load_gcs).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们深入探讨数据准备组件；然而，让我们快速看一个文件获取组件的例子。在本章的推荐器示例中，我们使用了一个特殊的预构建容器来获取我们的数据，因为它不在持久卷中。相反，我们可以使用
    Kubeflow GCS 组件 `google-cloud/storage/download/` 来下载我们的数据。假设您已经像 [示例 4-15](#dl_pipeline_release)
    中所示下载了管道发布，您可以使用 `load_component_from_file` 如同 [示例 4-16](#ex_load_gcs) 中所示加载组件。
- en: Example 4-16\. Load GCS download component
  id: totrans-163
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-16\. 加载 GCS 下载组件
- en: '[PRE20]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: When a component is loaded, it returns a function that produces a pipeline stage
    when called. Most components take parameters to configure their behavior. You
    can get a list of the components’ options by calling `help` on the loaded component,
    or looking at the *component.yaml*. The GCS download component requires us to
    configure what we are downloading with `gcs_path`, shown in [Example 4-17](#ex_dl_gcs).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 当加载组件时，它会返回一个函数，调用该函数将生成一个管道阶段。大多数组件接受参数以配置它们的行为。通过在加载的组件上调用 `help` 或查看 *component.yaml*，您可以获取组件选项的列表。GCS
    下载组件要求我们使用 `gcs_path` 配置下载内容，如 [示例 4-17](#ex_dl_gcs) 所示。
- en: Example 4-17\. Loading pipeline storage component from relative path and web
    link
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-17\. 从相对路径和网络链接加载管道存储组件
- en: '[PRE21]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In [Chapter 5](ch05.xhtml#data_and_feature_prep), we explore more common Kubeflow
    pipeline components for data and feature preparation.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 5 章](ch05.xhtml#data_and_feature_prep) 中，我们深入探讨了更常见的 Kubeflow 管道数据和特征准备组件。
- en: Advanced Topics in Pipelines
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管道的高级主题
- en: All of the examples that we have shown so far are purely sequential. There are
    also cases in which we need the ability to check conditions and change the behavior
    of the pipeline accordingly.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们展示的所有示例都是纯顺序执行的。还有一些情况下，我们需要能够检查条件并相应地更改管道的行为。
- en: Conditional Execution of Pipeline Stages
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管道阶段的条件执行
- en: Kubeflow Pipelines allows conditional executions via `dsl.Condition`. Let’s
    look at a very simple example, where, depending on the value of a variable, different
    calculations are executed.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow Pipelines 允许通过 `dsl.Condition` 进行条件执行。让我们看一个非常简单的示例，在这个示例中，根据变量的值执行不同的计算。
- en: A simple notebook implementing this example follows. It starts with the imports
    necessary for this, in [Example 4-18](#Import_req_components).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的笔记本实现了这个例子。它从[示例 4-18](#Import_req_components)中必需的导入开始。
- en: Example 4-18\. Importing required components
  id: totrans-174
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-18\. 导入所需组件
- en: '[PRE22]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Once the imports are in place, we can implement several simple functions, as
    shown in [Example 4-19](#Functions_implement).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦导入完成，我们可以实现几个简单的函数，如[示例 4-19](#Functions_implement)所示。
- en: Example 4-19\. Functions implementation
  id: totrans-177
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-19\. 函数实现
- en: '[PRE23]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We implement all of the functions directly using Python (as in the previous
    example). The first function generates an integer between 0 and 100, and the next
    three constitute a simple skeleton for the actual processing. The pipeline is
    implemented as in [Example 4-20](#Pipeline_implement).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们直接使用Python实现所有函数（与前面的示例相同）。第一个函数生成一个介于0和100之间的整数，接下来的三个函数构成了实际处理的简单框架。管道的实现如[示例 4-20](#Pipeline_implement)中所示。
- en: Example 4-20\. Pipeline implementation
  id: totrans-180
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-20\. 管道实现
- en: '[PRE24]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[![1](Images/1.png)](#co_kubeflow_pipelines_CO5-1)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_kubeflow_pipelines_CO5-1)'
- en: Depending on the number we get here…
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这里我们得到的数字…
- en: '[![2](Images/2.png)](#co_kubeflow_pipelines_CO5-2)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_kubeflow_pipelines_CO5-2)'
- en: We will continue on to one of these operations.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续进行其中一个操作。
- en: '[![3](Images/3.png)](#co_kubeflow_pipelines_CO5-5)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_kubeflow_pipelines_CO5-5)'
- en: Note here that we are specifying empty arguments—required parameter.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这里我们正在指定空参数——必需参数。
- en: Finally, the execution graph, as shown in [Figure 4-9](#fig_conditional_pipelines_example).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，执行图表，如[图 4-9](#fig_conditional_pipelines_example)所示。
- en: '![Execution of conditional Pipelines Example](Images/kfml_0409.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![执行条件管道示例](Images/kfml_0409.png)'
- en: Figure 4-9\. Execution of conditional pipelines example
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-9\. 执行条件管道示例
- en: From this graph, we can see that the pipeline really splits into three branches
    and process-large-op execution is selected in this run. To validate that this
    is correct, we look at the execution log, shown in [Figure 4-10](#fig_conditional_pipelines_log).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个图表中，我们可以看到管道确实分成了三个分支，并且在此运行中选择了处理大操作。为了验证这一点，我们查看执行日志，如[图 4-10](#fig_conditional_pipelines_log)所示。
- en: '![Viewing Conditional Pipeline Log](Images/kfml_0410.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![查看条件管道日志](Images/kfml_0410.png)'
- en: Figure 4-10\. Viewing conditional pipeline log
  id: totrans-193
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-10\. 查看条件管道日志
- en: Here we can see that the generated number is 67\. This number is larger than
    50, which means that the *process_large_op* branch should be executed.^([7](ch04.xhtml#idm45831177531704))
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们可以看到生成的数字是67。这个数字大于50，这意味着应执行*process_large_op*分支。^([7](ch04.xhtml#idm45831177531704))
- en: Running Pipelines on Schedule
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在计划上运行管道
- en: We have run our pipeline manually. This is good for testing, but is often insufficient
    for production environments. Fortunately, you can run pipelines on a schedule,
    as described on [thisKubeflow documentation page](https://oreil.ly/8v3fb). First,
    you need to upload a pipeline definition and specify a description. When this
    is done, you can create a periodic run by creating a run and selecting a run type
    of “Recurring,” then following the instructions on the screen, as seen in [Figure 4-11](#setting_up_periodic_execution_of_a_pipeline).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们手动运行了我们的管道。这对于测试很好，但通常不足以满足生产环境的需求。幸运的是，您可以按计划运行管道，如[thisKubeflow documentation
    page](https://oreil.ly/8v3fb)所述。首先，您需要上传管道定义并指定描述。完成后，您可以通过创建一个运行并选择“重复”运行类型来创建定期运行，并按照屏幕上的说明操作，如[图 4-11](#setting_up_periodic_execution_of_a_pipeline)所示。
- en: In this figure we are setting a pipeline to run every day.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在此图中，我们正在设置每天运行一次的管道。
- en: Warning
  id: totrans-198
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: When creating a periodic run we are specifying how often to run a pipeline,
    not when to run it. In the current implementation, the time of execution is defined
    by when the run is created. Once it is created, it is executed immediately and
    then executed with the defined frequency. If, for example, a daily run is created
    at 10 am, it will be executed at 10 am daily.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 创建定期运行时，我们正在指定管道运行的频率，而不是运行时间。在当前实现中，执行时间是在创建运行时定义的。一旦创建，它会立即执行，然后按照定义的频率执行。例如，如果每天上午10点创建一个每天运行，它将每天上午10点执行。
- en: Setting periodic execution of pipelines is an important functionality, allowing
    you to completely automate pipeline execution.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 设置管道的定期执行是一个重要的功能，允许您完全自动化管道的执行。
- en: '![Setting Up Periodic Execution of a Pipeline](Images/kfml_0411.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![设置定期运行管道](Images/kfml_0411.png)'
- en: Figure 4-11\. Setting up periodic execution of a pipeline
  id: totrans-202
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-11\. 设置管道的周期性执行
- en: Conclusion
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: You should now have the basics of how to build, schedule, and run some simple
    pipelines. You also learned about the tools that pipelines use for when you need
    to debug. We showed how to integrate existing software into pipelines, how to
    implement conditional execution inside a pipeline, and how to run pipelines on
    a schedule.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在应该掌握了如何构建、调度和运行一些简单管道的基础知识。您还学习了管道工具在调试时的使用。我们展示了如何将现有软件集成到管道中，如何在管道内实现条件执行，以及如何按计划运行管道。
- en: In our next chapter, we look at how to use pipelines for data preparation with
    some examples.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的下一章中，我们将看看如何使用管道进行数据准备，并提供一些示例。
- en: ^([1](ch04.xhtml#idm45831179676952-marker)) This can often be automatically
    inferred when passing the result of one pipeline stage as the input to others.
    You can also specify additional dependencies manually.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch04.xhtml#idm45831179676952-marker)) 当将一个管道阶段的结果作为其他管道的输入时，通常可以自动推断出这一点。您还可以手动指定额外的依赖关系。
- en: ^([2](ch04.xhtml#idm45831178994504-marker)) Kubernetes persistent volumes can
    provide different [access modes](https://oreil.ly/KbGrQ).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch04.xhtml#idm45831178994504-marker)) Kubernetes 持久卷可以提供不同的 [访问模式](https://oreil.ly/KbGrQ)。
- en: ^([3](ch04.xhtml#idm45831178988264-marker)) Generic read-write-many implementation
    is [NFS server](https://oreil.ly/QXEBX).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch04.xhtml#idm45831178988264-marker)) 通用的读写多实现是 [NFS 服务器](https://oreil.ly/QXEBX)。
- en: ^([4](ch04.xhtml#idm45831178937864-marker)) Usage of the cloud native access
    storage can be handy if you need to ensure portability of your solution across
    multiple cloud providers.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch04.xhtml#idm45831178937864-marker)) 如果需要确保解决方案在多个云提供商之间具有可移植性，则可以使用云原生访问存储。
- en: ^([5](ch04.xhtml#idm45831178821608-marker)) For installation of Argo Workflow
    on another OS, refer to [these Argo instructions](https://oreil.ly/s9CZM).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch04.xhtml#idm45831178821608-marker)) 如果需要在另一个操作系统上安装 Argo Workflow，请参考
    [这些 Argo 指令](https://oreil.ly/s9CZM)。
- en: ^([6](ch04.xhtml#idm45831178079080-marker)) Many of the standard components
    are in [this Kubeflow GitHub repo](https://oreil.ly/0WX6k).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch04.xhtml#idm45831178079080-marker)) 许多标准组件都在 [这个 Kubeflow GitHub 仓库](https://oreil.ly/0WX6k)
    中。
- en: ^([7](ch04.xhtml#idm45831177531704-marker)) A slightly more complex example
    of conditional processing (with nested conditions) can be found [in this GitHub
    site](https://oreil.ly/WBwD1).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch04.xhtml#idm45831177531704-marker)) 在 [这个 GitHub 网站](https://oreil.ly/WBwD1)
    上可以找到更复杂的条件处理示例（包括嵌套条件）。
