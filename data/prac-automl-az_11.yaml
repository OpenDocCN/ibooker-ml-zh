- en: Chapter 7\. Model Interpretability and Transparency with Automated ML
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。自动化机器学习中的模型可解释性和透明性
- en: We discussed earlier how building good machine learning models is a pretty time-consuming
    process. What is a “good” machine learning model? We saw that this is usually
    defined by performance of the model, as measured by accuracy or similar metrics.
    As companies get ready to adopt machine learning for business-critical scenarios,
    interpretability and transparency of machine learning models becomes vital.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们早些时候讨论了如何构建良好的机器学习模型是一个相当耗时的过程。什么是“好”的机器学习模型？我们看到，通常情况下这是由模型的性能来定义的，通过准确性或类似的指标来衡量。随着公司准备采用面向业务关键场景的机器学习，模型的可解释性和透明性变得至关重要。
- en: In this chapter, we cover key aspects around interpretability and transparency
    of machine learning that leads to customer trust. Interpretability and transparency
    become even more important when you are trying to use or customize a machine learning
    pipeline developed by others, including those generated by Automated Machine Learning
    systems. Let’s take a deeper look at how automated ML on Microsoft Azure Machine
    Learning enables model interpretability and transparency.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了关于机器学习可解释性和透明性的关键方面，这些方面有助于建立客户信任。当你尝试使用或定制他人开发的机器学习管道时，包括由自动化机器学习系统生成的管道时，解释性和透明性变得更加重要。让我们深入了解微软Azure机器学习上的自动化ML如何实现模型的可解释性和透明性。
- en: Model Interpretability
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型解释性
- en: Most machine learning models are considered black boxes because it’s usually
    difficult to understand or explain how they work. Without this understanding,
    it is difficult to trust the model, and therefore difficult to convince executive
    stakeholders and customers of the business value of machine learning and machine
    learning–based systems.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习模型被认为是黑匣子，因为通常很难理解或解释它们的工作原理。没有这种理解，很难信任模型，因此很难说服高管利益相关者和客户相信机器学习和基于机器学习的系统的商业价值。
- en: Some models, like linear regression, are considered to be fairly straightforward
    and therefore easy to understand, but as we add more features or use more complicated
    machine learning models like neural networks, understanding them becomes more
    and more difficult. Usually, more complex (and not-so-easy-to-understand) models
    perform much better—that is, they achieve greater accuracy—than those that are
    simpler, and easier to understand. [Figure 7-1](#interpretability_solidus_explainability)
    shows this relationship.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 有些模型，如线性回归，被认为相对直观，因此易于理解，但随着我们添加更多特征或使用更复杂的机器学习模型如神经网络，理解它们变得越来越困难。通常，更复杂（不那么易于理解）的模型比简单且易于理解的模型表现得更好，即它们实现了更高的准确性。[图 7-1](#interpretability_solidus_explainability)展示了这种关系。
- en: '![paml 0801](assets/paml_0801.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0801](assets/paml_0801.png)'
- en: Figure 7-1\. Interpretability/explainability versus model performance
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-1。可解释性/解释性与模型性能
- en: 'Businesses run on transparency and trust, and being able to open the machine
    learning black box to explain a model helps build transparency and trust. In heavily
    regulated industries like health care and banking, interpretability and transparency
    are critical. Here are few real-world scenarios to illustrate the value of interpretability
    and transparency in machine learning:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 企业运营在透明度和信任的基础上，打开机器学习的黑匣子以解释模型有助于建立透明度和信任。在像医疗保健和银行业这样严格监管的行业中，可解释性和透明性至关重要。以下是一些现实场景，用以说明机器学习中可解释性和透明性的价值：
- en: A manufacturing company using machine learning to predict future instrument
    failure so that it can proactively perform maintenance activity.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制造公司利用机器学习预测未来设备故障，以便能够主动进行维护活动。
- en: When you know an instrument is about to fail, what’s the most likely cause going
    to be so that you can quickly perform preventive maintenance?
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你知道一个设备即将故障时，最可能的原因是什么，以便你可以快速进行预防性维护？
- en: A financial institution using machine learning to process loan or credit card
    applications.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 金融机构利用机器学习处理贷款或信用卡申请。
- en: How do you know whether the model is doing the right thing?
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你如何知道模型是否在做正确的事情？
- en: If a customer asks for more details on why their application was rejected, how
    will you respond to them?
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果客户要求更多关于为什么他们的申请被拒绝的详细信息，你将如何回应？
- en: An online retailer or an independent software vendor (ISV) using machine learning
    to predict customer churn—in other words, whether a customer is going to stop
    using their product/service soon.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在线零售商或独立软件供应商（ISV）使用机器学习预测客户流失——换句话说，客户是否会很快停止使用其产品/服务。
- en: What are the key contributors to customer churn?
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是客户流失的主要因素？
- en: How can you prevent customers from churning?
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何防止客户流失？
- en: '*Feature importance* is a popular approach used for model interpretability.
    Feature importance indicates how each input column (or feature) affects the model’s
    predictions. This allows data scientists to explain the resulting model and predictions
    so that stakeholders can see which data points are most important in the model.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*特征重要性* 是一种用于模型解释的流行方法。特征重要性指示每个输入列（或特征）如何影响模型的预测。这使得数据科学家能够解释生成的模型和预测，从而让利益相关者看到哪些数据点在模型中最重要。'
- en: Model Interpretability with Azure Machine Learning
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Azure机器学习进行模型解释性分析
- en: The Azure Machine Learning Python SDK offers various interpretability packages
    to help you understand feature importance. Using these packages, you can explain
    machine learning models globally on all data, or locally on a specific data point.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Azure机器学习Python SDK提供各种可解释性包，帮助您理解特征重要性。使用这些包，您可以全局解释所有数据上的机器学习模型，或者在特定数据点上进行局部解释。
- en: Explainers
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解释器
- en: 'There are two sets of explainers in the Azure Machine Learning SDK, specifically
    the `azureml.explain.model` package: direct explainers and meta explainers.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在Azure机器学习SDK中有两组解释器，特别是`azureml.explain.model`包：直接解释器和元解释器。
- en: '*Direct explainers* come from integrated libraries. A popular approach for
    explaining the output of machine learning model is SHAP (short for “SHapley Additive
    exPlanations”). The following is a list of the direct explainers available in
    the SDK:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*直接解释器* 来自集成库。解释机器学习模型输出的一种流行方法是SHAP（SHapley Additive exPlanations的简称）。以下是SDK中可用的直接解释器列表：'
- en: SHAP Tree Explainer
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP树解释器
- en: SHAP’s Tree Explainer focuses on trees and ensembles of trees.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP的树解释器专注于树和树的集成。
- en: SHAP Deep Explainer
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP深度解释器
- en: Based on the explanation from SHAP, Deep Explainer focuses on deep learning
    models. TensorFlow models and Keras models using the TensorFlow backend are supported
    (there is also preliminary support for PyTorch).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 基于SHAP的解释，Deep Explainer专注于深度学习模型。支持TensorFlow模型和使用TensorFlow后端的Keras模型（还有对PyTorch的初步支持）。
- en: SHAP Kernel Explainer
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP核解释器
- en: SHAP’s Kernel Explainer uses a specially weighted local linear regression to
    estimate SHAP values for any model.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP的核解释器使用特殊加权的局部线性回归来估计任何模型的SHAP值。
- en: Mimic Explainer
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 模仿解释器
- en: Mimic Explainer is based on the idea of global surrogate models. A global surrogate
    model is an intrinsically interpretable model that is trained to approximate the
    predictions of a black-box model as accurately as possible. You can interpret
    a surrogate model to draw conclusions about the black-box model.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 模仿解释器基于全局替代模型的理念。全局替代模型是一种内在可解释的模型，它被训练来尽可能精确地近似黑盒模型的预测。您可以解释一个替代模型来得出关于黑盒模型的结论。
- en: PFI Explainer
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: PFI解释器
- en: Permutation Feature Importance (PFI) Explainer is a technique used to explain
    classification and regression models. At a high level, the way it works is by
    randomly shuffling data one feature at a time for the entire dataset and calculating
    how much the performance metric of interest decreases. The larger the change,
    the more important that feature is.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 排列特征重要性（PFI）解释器是一种用于解释分类和回归模型的技术。在高层次上，它的工作方式是逐个特征地对整个数据集随机重新排列数据，并计算感兴趣的性能度量减少的程度。变化越大，该特征越重要。
- en: LIME Explainer
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: LIME解释器
- en: Local interpretable model-agnostic explanations (LIME) Explainer uses the state-of-the-art
    LIME algorithm to create local surrogate models. Unlike the global surrogate models,
    LIME focuses on training local surrogate models to explain individual predictions.
    This is currently available in only the contrib/preview package `azureml.contrib.explain.model`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 本地可解释的模型无关解释（LIME）解释器使用最先进的LIME算法创建本地替代模型。与全局替代模型不同，LIME专注于训练本地替代模型来解释单个预测。目前仅在`azureml.contrib.explain.model`的contrib/preview包中提供。
- en: HAN Text Explainer
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: HAN文本解释器
- en: 'HAN Text Explainer uses a hierarchical attention network for getting model
    explanations from text data for a given black-box text model. This is currently
    available only in the contrib/preview package: `azureml.contrib.explain.model`.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: HAN文本解释器使用层次注意力网络，用于从文本数据中获取黑盒文本模型的模型解释。目前仅在`azureml.contrib.explain.model`的`contrib/preview`包中可用。
- en: '*Meta explainers* automatically select a suitable direct explainer and generate
    the best explanation information based on the given model and datasets. Currently,
    the following meta explainers are available in the Azure Machine Learning SDK:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*元解释器*会自动选择适合的直接解释器，并基于给定的模型和数据集生成最佳解释信息。目前在Azure机器学习SDK中提供以下元解释器：'
- en: Tabular Explainer
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 表格解释器
- en: Used with tabular datasets
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 用于表格数据集
- en: Text Explainer
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 文本解释器
- en: Used with text datasets
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 用于文本数据集
- en: Image Explainer
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图像解释器
- en: Used with image datasets
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 用于图像数据集
- en: Text Explainer and Image Explainer are currently available only in the contrib/preview
    package `azureml.contrib.explain.model`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，`Text Explainer`和`Image Explainer`仅在`azureml.contrib.explain.model`的`contrib/preview`包中可用。
- en: 'In addition to automatically selecting direct explainers, meta explainers develop
    additional features on top of the underlying libraries and improve the speed and
    scalability over the direct explainers. Currently `TabularExplainer` employs the
    following logic to invoke the direct explainers:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 除了自动选择直接解释器外，元解释器在底层库的基础上开发了额外的功能，并提高了速度和可扩展性。目前，`TabularExplainer`使用以下逻辑来调用直接解释器：
- en: If it’s a tree-based model, apply `TreeExplainer`, else
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果是基于树的模型，请应用`TreeExplainer`，否则
- en: If it’s a DNN model, apply `DeepExplainer`, else
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果是DNN模型，请应用`DeepExplainer`，否则
- en: Treat it as a black-box model and apply `KernelExplainer`.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其视为黑盒模型并应用`KernelExplainer`。
- en: The intelligence built into `TabularExplainer` will become more sophisticated
    as more libraries are integrated into the SDK.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 集成到SDK中的更多库后，`TabularExplainer`内置的智能将变得更加复杂。
- en: '[Figure 7-2](#direct_and_meta_explainers) shows the relationship between direct
    and meta explainers and which ones are suitable for different types of data. The
    SDK wraps all of the explainers so that they expose a common API and output format.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-2](#direct_and_meta_explainers)展示了直接解释器和元解释器之间的关系，以及适用于不同类型数据的解释器。SDK封装了所有解释器，以便它们公开一个通用的API和输出格式。'
- en: '![paml 0802](assets/paml_0802.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0802](assets/paml_0802.png)'
- en: Figure 7-2\. Direct and meta explainers
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-2\. 直接解释器和元解释器
- en: 'You’ll now see how to use these explainers in generating feature importance
    for the following two scenarios: a regression model trained using sklearn and
    a classification model trained using automated ML.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在将看到如何使用这些解释器为以下两种场景生成特征重要性：使用sklearn训练的回归模型和使用自动化ML训练的分类模型。
- en: Regression model trained using sklearn
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用sklearn训练的回归模型
- en: 'We will build a regression model to predict housing prices using the [Boston
    house price dataset from sklearn](https://oreil.ly/xUiJb). The dataset has 506
    rows, 13 input columns (features), and 1 target column. Here are the input columns:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将建立一个回归模型，使用[来自sklearn的波士顿房价数据集](https://oreil.ly/xUiJb)来预测房价。该数据集有506行，13个输入列（特征），和1个目标列。以下是输入列：
- en: 'CRIM: Per capita crime rate by town'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'CRIM: 每人均犯罪率'
- en: 'ZN: Proportion of residential land zoned for lots over 25,000 sq. ft.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ZN: 25,000平方英尺以上住宅用地的比例'
- en: 'INDUS: Proportion of nonretail business acres per town'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'INDUS: 每个城镇非零售营业英亩的比例'
- en: 'CHAS: Charles River dummy variable (1 if tract bounds river; 0 otherwise)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'CHAS: 查尔斯河虚拟变量（如果地块靠近河流为1；否则为0）'
- en: 'NOX: Nitric oxides concentration (parts per 10 million)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'NOX: 一氧化氮浓度（每千万分之一）'
- en: 'RM: Average number of rooms per dwelling'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'RM: 每个住宅的平均房间数'
- en: 'AGE: Proportion of owner-occupied units built prior to 1940'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'AGE: 1940年前建成的自住单位的比例'
- en: 'DIS: Weighted distances to five Boston employment centers'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'DIS: 到波士顿五个就业中心的加权距离'
- en: 'RAD: Index of accessibility to radial highways'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'RAD: 径向公路的可达性指数'
- en: 'TAX: Full-value property-tax rate per $10,000'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'TAX: 每$10,000全额财产税率'
- en: 'PTRATIO: Student-teacher ratio by town'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'PTRATIO: 按城镇计算的学生-教师比例'
- en: 'B: 1,000 * (Bk–0.63)², where Bk is the proportion of African Americans by town
    (this dataset is from 1978)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'B: 1,000 * (Bk–0.63)²，其中Bk是城镇中非洲裔美国人的比例（此数据集来源于1978年）'
- en: 'LSTAT: % lower status of the population'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LSTAT: 人口的低收入阶层百分比'
- en: 'And this is the one target column:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个目标列：
- en: 'MEDV: Median value of owner-occupied homes in $1,000s'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'MEDV: 以千美元为单位的自住房屋的中位数价值'
- en: 'After loading the dataset and splitting it into train and test sets, we train
    a simple regression model using sklearn `GradientBoostingRegressor`. Next we’ll
    use `TabularExplainer` from the `azureml.explain.model` package to generate *global
    feature importance* for the trained model. After the global explanations are generated,
    we use the methods `get_ranked_global_values()` and `get_ranked_global_names()`
    to get ranked feature importance values and the corresponding feature names:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载数据集并将其分割为训练集和测试集后，我们使用 sklearn 的 `GradientBoostingRegressor` 训练了一个简单的回归模型。接下来，我们将使用
    `azureml.explain.model` 包中的 `TabularExplainer` 生成训练模型的 *全局特征重要性*。生成全局解释后，我们使用方法
    `get_ranked_global_values()` 和 `get_ranked_global_names()` 来获取排名的特征重要性值和相应的特征名称：
- en: '[PRE0]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[Figure 7-3](#global_feature_importance) shows ranked global feature importance
    output. This indicates that the LSTAT (% lower status of the population) feature
    is most influential on the output of the model.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-3](#global_feature_importance) 显示了排名靠前的全局特征重要性输出。这表明人口的LSTAT（%低社会地位）特征对模型输出影响最大。'
- en: '![paml 0803](assets/paml_0803.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0803](assets/paml_0803.png)'
- en: Figure 7-3\. Global feature importance
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-3\. 全局特征重要性
- en: 'Next, we look at how to compute *local feature importance* for a specific row
    of data. This is especially relevant at prediction time. We pass one row from
    the test set to `explain_local()` method and print the local feature importance:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看如何计算特定数据行的 *局部特征重要性*。这在预测时尤为重要。我们将一行从测试集传递给 `explain_local()` 方法并打印局部特征重要性：
- en: '[PRE1]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As seen in [Figure 7-4](#local_feature_importance), although LSTAT remains as
    the topmost feature in terms of importance for this specific test record, AGE
    is the second most impactful feature.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如在 [图 7-4](#local_feature_importance) 中所见，尽管在这个特定测试记录中，LSTAT 仍然是最重要的特征，但 AGE
    是第二重要的特征。
- en: '![paml 0804](assets/paml_0804.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0804](assets/paml_0804.png)'
- en: Figure 7-4\. Local feature importance
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-4\. 局部特征重要性
- en: As discussed in [Chapter 4](ch04.html#feature_engineering_and_automated_machin),
    raw data usually goes through multiple transformations before going through the
    training process. Features produced through this process are called *engineered
    features*, whereas the raw input columns are known as *raw features*. By default,
    explainers explain the model in terms of features used for training (i.e., engineered
    features) and not on the raw features.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如在 [第 4 章](ch04.html#feature_engineering_and_automated_machin) 中讨论的，原始数据通常在训练过程之前经过多次转换。通过这个过程产生的特征称为
    *工程化特征*，而原始输入列称为 *原始特征*。默认情况下，解释器使用用于训练的特征来解释模型（即工程化特征），而不是原始特征。
- en: However, in most real-world situations, you would like to understand *raw feature
    importance*. Raw feature importance informs you how each raw input column influences
    the model prediction, whereas *engineered feature importance* is not directly
    based on your inputs columns, but on columns generated through transformations
    on input columns. Hence, raw feature importance is a lot more understandable and
    actionable than engineered feature importance.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在大多数真实世界的情况下，您希望理解 *原始特征重要性*。原始特征重要性告诉您每个原始输入列如何影响模型预测，而 *工程化特征重要性* 不直接基于您的输入列，而是基于输入列转换生成的列。因此，原始特征重要性比工程化特征重要性更易于理解和操作。
- en: 'Using the SDK, you can pass your feature transformation pipeline to the explainer
    to receive raw feature importance. If you skip this, the explainer provides engineered
    feature importance. In general, any of the transformations on a single column
    will be supported:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SDK，您可以将特征转换管道传递给解释器以接收原始特征重要性。如果您跳过此步骤，解释器将提供工程化的特征重要性。通常，将在单列上的任何转换都将受到支持：
- en: '[PRE2]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'So far, you have seen how to generate feature importance during model training
    time. It is also important to understand feature importance at inference time
    for a specific row of data. Let’s consider this scenario: suppose that you own
    a machine learning–powered application to do credit card application processing.
    If your application rejects a credit card application, you need to explain why
    the model rejects that specific applicant.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经看到如何在模型训练时生成特征重要性。同样重要的是，在推断时间理解特定数据行的特征重要性。让我们考虑这种情况：假设您拥有一个机器学习驱动的应用程序来处理信用卡申请。如果您的应用程序拒绝了一张信用卡申请，您需要解释为什么模型拒绝了该特定申请人。
- en: To enable *inference-time feature importance*, the explainer can be deployed
    along with the original model and can be used at scoring time to provide the local
    explanation information. Next, we examine how to enable feature importance with
    the automated ML tool in Azure Machine Learning.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了启用*推理时特征重要性*，解释器可以与原始模型一起部署，并在评分时提供本地解释信息。接下来，我们将探讨如何在 Azure 机器学习中使用自动化机器学习工具启用特征重要性。
- en: Classification model trained using automated ML
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用自动化机器学习训练的分类模型
- en: 'We will use the [sklearn iris dataset](https://oreil.ly/1aaIj). This is a well-known
    classification scenario for flowers. There are three classes of flowers and four
    input features: petal length, petal width, sepal length, and sepal width. The
    dataset has 150 rows (50 rows per flower class).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 [sklearn 鸢尾花数据集](https://oreil.ly/1aaIj)。这是一个著名的花卉分类场景。有三类花和四个输入特征：花瓣长度、花瓣宽度、萼片长度和萼片宽度。数据集有
    150 行（每类花 50 行）。
- en: 'After loading the dataset and splitting it into train and test sets, we train
    a classification model using automated ML. To enable feature importance for each
    of the models trained by automated ML, we set `model_explainability=True` in `AutoMLConfig`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据集并将其分割为训练集和测试集后，我们使用自动化机器学习训练分类模型。为了使每个由自动化机器学习训练的模型都能获得特征重要性，我们在`AutoMLConfig`中设置了`model_explainability=True`：
- en: '[PRE3]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Because this is a classification problem, you can get not only overall model-level
    feature importance, but also feature importance per class.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这是一个分类问题，您不仅可以获取整体模型级别的特征重要性，还可以获取每个类别的特征重要性。
- en: 'Let’s review how to use the `azureml.train.automl.automlexplainer` package
    to extract feature importance values from models generated in automated ML. We
    use the best run here as an example, but you can retrieve any run from automated
    ML training:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾如何使用`azureml.train.automl.automlexplainer`包从自动化机器学习生成的模型中提取特征重要性值。这里以最佳运行示例为例，但您可以从自动化机器学习训练中检索任何运行：
- en: '[PRE4]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[Figure 7-5](#feature_importance_for_automated_ml_mode) shows the output: global
    feature importance for the model and class-level feature importance.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-5](#feature_importance_for_automated_ml_mode) 显示了输出：模型的全局特征重要性和类别级别的特征重要性。'
- en: '![paml 0805](assets/paml_0805.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0805](assets/paml_0805.png)'
- en: Figure 7-5\. Feature importance for automated ML model
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-5\. 自动化机器学习模型的特征重要性
- en: In addition to using the SDK to get feature importance values, you can also
    get it through widget UX in the notebook or Azure portal. Let’s see how to do
    that from widget UX. After automated ML training is complete, you can use `RunDetails`
    from the `azureml.widgets` package to visualize the automated ML training including
    all of the machine learning pipelines tried, which you can see in [Figure 7-6](#automated_ml_widget_ux).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用 SDK 获取特征重要性值外，您还可以通过笔记本或 Azure 门户中的小部件用户体验获取它。让我们从小部件用户体验中看看如何做到这一点。在自动化机器学习训练完成后，您可以使用来自`azureml.widgets`包的`RunDetails`来可视化自动化机器学习训练，包括所有尝试的机器学习流水线，这些可以在
    [图 7-6](#automated_ml_widget_ux) 中查看。
- en: '![paml 0806](assets/paml_0806.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0806](assets/paml_0806.png)'
- en: Figure 7-6\. Automated ML widget UX
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-6\. 自动化机器学习小部件用户体验
- en: You can click any of the machine learning pipelines to explore more. In addition
    to a bunch of charts, you will see a feature importance chart. Use the legend
    to see overall model-level as well as class-level feature importance. In [Figure 7-7](#feature_importance_in_the_automated_ml),
    you can see that “petal width (cm)” is the most important feature from the overall
    model perspective, but “sepal width (cm)” is the most important feature for class
    1.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以单击任何机器学习流水线以进行更多探索。除了一堆图表外，您还将看到一个特征重要性图表。使用图例查看整体模型级别以及类级别的特征重要性。在 [图 7-7](#feature_importance_in_the_automated_ml)
    中，您可以看到“花瓣宽度（cm）”是从整体模型视角最重要的特征，但对于类别 1，“萼片宽度（cm）”是最重要的特征。
- en: '![paml 0807](assets/paml_0807.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0807](assets/paml_0807.png)'
- en: Figure 7-7\. Feature importance in the automated ML widget UX
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-7\. 自动化机器学习小部件用户体验中的特征重要性
- en: Model Transparency
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型透明化
- en: In the previous section, you learned how feature importance is a powerful way
    to understand machine learning models. It is also important to understand the
    training process, from input data leading to the machine learning model. In this
    section, we will discuss how automated ML makes the end-to-end training process
    transparent.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，您学习了特征重要性是理解机器学习模型强大的一种方式。理解从输入数据到机器学习模型的训练过程也同样重要。在本节中，我们将讨论自动化机器学习如何使端到端的训练过程变得透明。
- en: Understanding the Automated ML Model Pipelines
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解自动化机器学习模型流水线
- en: 'As discussed in earlier chapters, automated ML recommends model pipelines with
    the goal of producing high-quality ML models based on user inputs. Each model
    pipeline includes the following steps:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几章所讨论的，自动化ML推荐基于用户输入以产生高质量ML模型的模型管道。每个模型管道包括以下步骤：
- en: Data preprocessing and feature engineering
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据预处理和特征工程
- en: Model training based on selected algorithm and hyperparameter values
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于选择的算法和超参数值进行模型训练
- en: With automated ML, you can analyze steps of each recommended pipeline before
    using them in your application or scenario. This transparency not only allows
    you to trust the model better, but also enables you to customize it further. For
    details on how to get visibility into the end-to-end process, refer to [Chapter 4](ch04.html#feature_engineering_and_automated_machin),
    which covers all of the steps in automated ML–recommended machine learning pipelines.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自动化机器学习，您可以在将其用于应用程序或场景之前分析每个推荐管道的步骤。这种透明度不仅让您更加信任模型，还能进一步定制它。有关如何获得端到端过程可见性的详细信息，请参阅[第四章](ch04.html#feature_engineering_and_automated_machin)，该章节涵盖了自动化ML推荐的机器学习管道中的所有步骤。
- en: Guardrails
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保护栏
- en: In previous chapters, you saw that automated ML makes it easy to get started
    with machine learning by automating most of the iterative and time-consuming steps.
    In addition, there are many best practices that you need to apply to achieve reliable
    results. *Guardrails* help users understand potential issues with their data and
    training model, so they know what to expect and can correct the issues for improved
    results.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，您看到自动化机器学习通过自动化大部分迭代和耗时步骤，使得开始机器学习变得更加容易。此外，还有许多最佳实践需要应用以实现可靠的结果。*保护栏*帮助用户理解数据和训练模型的潜在问题，因此他们知道可以期望什么并能够纠正问题以获得改进的结果。
- en: 'Following are some common issues to be aware of:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些要注意的常见问题：
- en: Missing values
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失值
- en: As we’ve discussed in earlier chapters, real-world data isn’t clean and could
    be missing a lot of values. Before using it for machine learning, data with missing
    values needs to be “fixed.” Various techniques can be used to fix missing values,
    from dropping entire rows to using various techniques to intelligently populate
    missing values based on the rest of the data; this is called *imputation*.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在早些章节中讨论的那样，现实世界的数据并不干净，可能缺少很多值。在将其用于机器学习之前，需要“修复”具有缺失值的数据。可以使用各种技术来修复缺失值，从删除整行数据到使用其他数据的智能填充方法来填充缺失值；这称为*插补*。
- en: Class imbalance
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 类别不平衡
- en: Class imbalance is a major problem in machine learning because most machine
    learning algorithms assume that data is equally distributed. In the case of imbalanced
    data, majority classes dominate over minority classes, causing the machine learning
    models to be more biased toward majority classes. This results in poor classification
    of minority classes. Some real-world examples involve anomaly detection, fraud
    detection, and disease detection.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 类别不平衡是机器学习中的一个主要问题，因为大多数机器学习算法假设数据是均匀分布的。在不平衡数据的情况下，多数类别会主导少数类别，导致机器学习模型更倾向于多数类别，从而导致少数类别的分类效果较差。一些实际例子涉及异常检测、欺诈检测和疾病检测。
- en: 'Sampling is a commonly used strategy to overcome class imbalance. There are
    two ways to sample:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 采样是克服类别不平衡的常用策略。有两种采样方式：
- en: Undersampling
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 欠采样
- en: Balancing the dataset by removing some instance of majority class.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 通过删除大多数类别的一些实例来平衡数据集。
- en: Oversampling
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 过采样
- en: Adding similar instances of the minority class to balance.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 添加类似的少数类别实例以达到平衡。
- en: Data leakage
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 数据泄露
- en: 'Data leakage is another key problem when building machine learning models.
    This occurs when the training dataset includes information that would not be available
    at the time of prediction. Because the actual outcome is already known (due to
    leakage), the model performance will be almost perfect for the training data but
    will be very bad during prediction. There are a few tricks you can use to overcome
    data leakage:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 数据泄露是构建机器学习模型时的另一个关键问题。这种情况发生在训练数据集包含预测时不可用的信息时。由于实际结果已知（由于泄露），模型在训练数据上的性能几乎完美，但在预测时性能会非常差。您可以使用一些技巧来克服数据泄露：
- en: Remove leaky features
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 删除泄露特征
- en: Use simple rule-based models to identify leaky features and remove them.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 使用简单的基于规则的模型来识别泄露特征并将其删除。
- en: Hold out dataset
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 留出数据集
- en: Hold back an unseen test set as a final sanity check of your model before you
    use it.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用模型之前，保留一个未见的测试集作为最终的模型健全性检查。
- en: Add noise
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 添加噪声
- en: Add noise to input data to smooth out the effects of possibly leaky features.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 向输入数据添加噪声，以平滑可能泄漏特征的影响。
- en: As you can see, understanding and safeguarding against common issues like these
    can be critical to the performance of the model as well as transparency to users.
    Automated ML offers guardrails to show and protect against common issues and will
    continue to add more sophisticated ones over time.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，理解和防范这类常见问题对模型的性能以及对用户透明度至关重要。自动化 ML 提供了防护栏，显示并防范常见问题，并将随时间推移继续增加更复杂的防护栏。
- en: Conclusion
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'In this chapter, we discussed two key aspects that become very important when
    establishing trust in a trained machine learning model: interpretability and transparency.
    Almost every company or team using machine learning models requires the models
    to be interpretable and transparent–to a degree–to gain confidence.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了建立对训练好的机器学习模型信任时变得非常重要的两个关键方面：可解释性和透明度。几乎每家使用机器学习模型的公司或团队都要求模型在一定程度上是可解释的和透明的，以获得信心。
- en: You learned how to take advantage of interpretability/explainability features
    by using the Azure Machine Learning Python SDK, as well as the automated ML Python
    SDK and widget UX. We also touched upon gaining visibility into end-to-end model-training
    pipelines as well as pitfalls to avoid, and why setting up guardrails against
    these pitfalls is important to ensure the transparency of your model.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 Azure 机器学习 Python SDK，以及自动化 ML Python SDK 和小部件 UX，您学会了如何利用解释性/可解释性功能。我们还触及了了解端到端模型训练管道以及避免陷阱的重要性，以及为什么建立防护栏来防范这些陷阱对于确保模型透明度至关重要。
