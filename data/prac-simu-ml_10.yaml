- en: Chapter 8\. Introducing Curriculum Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章。引入课程学习
- en: Think back to your first few days in school. What strange times they were…the
    teacher standing at the front of the class presenting you all with a quadratic
    equation and asking you to solve it.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下你在学校的头几天。那是多么奇怪的时光……老师站在班级前面向你们展示一个二次方程，要求你们解决它。
- en: “What is the value of *x*?” you find yourself being asked.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: “*x*的值是多少？”你发现自己被问到。
- en: Confused, you have no idea what’s going on; it’s your first day, after all.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑的是，你不知道发生了什么；毕竟，这是你的第一天。
- en: 'Still you take a guess: “Three.” The teacher stares at you before pronouncing
    you exceptionally wrong. You are sent home.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 但你仍然要猜测：“三。”老师瞪视着你然后宣布你非常错误。你被送回家。
- en: 'The next day this repeats. The teacher gives you another quadratic equation;
    you once again fail and are sent home. Day after day this happens: you show up
    and are given an equation, you take a guess, get it exceptionally wrong, and are
    then sent home.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 第二天这种情况重复。老师再次给你一个二次方程；你再次失败并被送回家。日复一日，这种情况发生：你出现并得到一个方程式，你猜测，得到一个异常错误的答案，然后被送回家。
- en: One day you guess and the teacher says, “Wrong, but close.”
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 有一天你猜测，老师说：“错了，但接近了。”
- en: Finally, some progress.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 终于，有些进展了。
- en: You are still sent home.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你仍然被送回家。
- en: The next day this repeats, and the next and the next, again and again. Each
    time you are guessing closer and closer. Each time you get sent home, and each
    time you show up the next day and guess again.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 第二天这种情况重复，再次重复，每次你都更接近猜测。每次你被送回家，每次你第二天再次出现并猜测。
- en: Finally you start to piece it together, you start to understand the individual
    parts that make up the equations, the way they interact, the way they influence
    the value of *x*. When asked, this time things are different. You respond, "*x*
    is -1 plus or minus root 2,” and you are confident in your answer. Your teacher
    slowly nods. “Correct.”
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最终你开始把它们拼在一起，你开始理解构成方程式的个别部分，它们相互作用的方式，它们如何影响*x*的值。这次当被问及时，情况有所不同。你回答说，“*x*等于-1加或减根号2”，并且你对你的答案感到自信。你的老师慢慢点头。“正确。”
- en: You’ve been at school now for 600 years, but you finally know how to solve a
    quadratic equation. You are sent home.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经在学校待了600年，但是最终你知道如何解决一个二次方程。你被送回家。
- en: Of course, this sounds like a terrible way to learn and an exceptionally cruel
    way to teach, yet it is how we ask ML to work.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这听起来像是一种糟糕的学习方式和一种极其残酷的教学方式，然而这就是我们要求ML工作的方式。
- en: Actual people are taught in stages.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上人们是按阶段教授的。
- en: We begin with the basics that are needed for more complex problems, and once
    we have the hang of them we move on to harder problems and more complex information.
    We build upon our previous knowledge like a pyramid, adding to it layer by layer
    until we are able to solve the actual problems we care about.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从需要解决更复杂问题的基础开始，一旦掌握了它们，我们就转向更困难的问题和更复杂的信息。我们像金字塔一样建立在我们以前的知识之上，一层一层地添加，直到我们能够解决我们关心的实际问题。
- en: We teach people this way because it’s been shown to work; it turns out that
    just asking kids to solve quadratic equations isn’t hugely effective, but teaching
    the basics of numbers, then numeracy, and then algebra and formulas means you
    can one day ask them to solve a quadratic equation and they can do it. It is done
    this way in various forms by all cultures, for all domains of knowledge.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以这种方式教导人们是因为已经证明这种方法有效；事实证明，仅仅让孩子们解决二次方程并不是非常有效的，但是教授数字的基础知识，然后是数学能力，接着是代数和公式，这样有一天你可以让他们解决一个二次方程并且他们能够做到。各种文化在所有知识领域中以各种形式采取这种方法。
- en: Curriculum learning (CL) in ML asks the question, “If it works well for people,
    does it also work for ML?” In this chapter, we are going to take a look at how
    we can use curriculum learning to solve a problem by building it up in stages.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的课程学习（CL）提出了这样一个问题：“如果它对人类有效，那么它对ML也有效吗？”在本章中，我们将看看如何使用课程学习通过分阶段地构建来解决问题。
- en: Curriculum Learning in ML
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中的课程学习
- en: 'The main reason to use curriculum learning in your ML models is the same reason
    that we as humans use it: it’s generally easier to master the basics of something
    before moving on to more advanced portions of a task.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 使用课程学习（Curriculum Learning）在你的ML模型中的主要原因与人类使用它的原因相同：通常在移动到任务的更高级部分之前，掌握某事物的基础知识会更容易。
- en: Note
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: When curriculum learning is successful, it is very successful. For example,
    in Unity, when teaching an agent to reach a goal (say, learning how to hop over
    a fence), the curriculum learning model learned both faster and better than a
    more traditional model. Other work in [domains related to simulations](https://oreil.ly/XwMcW)
    have shown similar [promising results](https://oreil.ly/04bAa). However, this
    isn’t to say that the answer is always to “just use curriculum learning.”
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当课程学习成功时，它是非常成功的。例如，在Unity中，当教导一个代理人去达到一个目标（比如学习如何越过篱笆）时，课程学习模型学习速度和效果都比传统模型更快更好。其他与[模拟相关的领域](https://oreil.ly/XwMcW)也显示了类似的[有希望的结果](https://oreil.ly/04bAa)。然而，这并不意味着答案总是“只使用课程学习”。
- en: As with many things in ML, knowing when to use curriculum learning is not clear-cut.
    If the problem you’re trying to solve has a clearly defined element of difficulty,
    or if the task itself has obvious stages, it may be an excellent candidate for
    CL. Unfortunately, you can’t really tell whether CL is appropriate until you try
    it and see.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 像许多机器学习中的事情一样，知道何时使用课程学习并不是一成不变的。如果您试图解决的问题具有明确定义的难度元素，或者任务本身具有明显的阶段，那么它可能是课程学习的一个极好候选者。不幸的是，在尝试并查看之前，您实际上无法确定CL是否合适。
- en: As an example, let’s say you want to train an agent to chase after a goal that
    will be moving around; perhaps you are making a simulation of a dog chasing squirrels.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设您想训练一个代理人去追逐一个会四处移动的目标；也许您正在制作一只追逐松鼠的狗的模拟。
- en: Note
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We, the authors, are from Australia, and we don’t have squirrels in Australia,
    so we are assuming that dogs chase them for completely harmless reasons. Also,
    these squirrels can’t climb trees; after all, we’ve never seen one do it!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们，作者，来自澳大利亚，我们这里没有松鼠，所以我们假设狗追赶它们是完全无害的原因。此外，这些松鼠不能爬树；毕竟，我们从未见过有松鼠这么做过！
- en: We could start the dog off against a squirrel that moves itself about a space
    and let the agent just figure it out, or we could use curriculum learning.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以让狗对抗一个在空间中自己移动的松鼠，并让代理人自己想出解决方法，或者我们可以使用课程学习。
- en: To start our curriculum learning dog agent, we’ll begin by having our dog move
    toward a squirrel that just happens to be over two meters wide, making it easier
    to reach.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始我们的课程学习狗代理，我们将从让我们的狗朝一个恰好超过两米宽的松鼠移动开始。
- en: Note
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Again, we don’t have squirrels, so we are pretty sure they can get this big.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们这里没有松鼠，所以我们很确定它们可以长到这么大。
- en: Additionally, the squirrel doesn’t move, making things even easier for our dog.
    The squirrel could then start shrinking, forcing the dog to move more precisely
    to reach it.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，松鼠不会移动，这对我们的狗来说更容易。然后，松鼠可能开始缩小，迫使狗更精确地移动以达到它。
- en: Once the dog has the hang of reaching a stationary squirrel-sized squirrel,
    we could start moving the goal. We could even make it so that the squirrel slowly
    ramps up in speed from very slow to squirrel-speedy, eventually reaching all the
    way up to hypersquirrel.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦狗掌握了达到一个静止的松鼠大小的松鼠，我们可以开始移动目标。我们甚至可以让松鼠的速度从非常慢逐渐加快到松鼠速度，最终达到超松鼠的速度。
- en: So, our curriculum here is we first taught our agent to move, then to follow,
    and then to follow against faster and faster targets, essentially teaching it
    how to chase.^([1](ch08.html#idm45900986434304))
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们这里的课程是先教我们的代理人移动，然后是追随，然后是追随更快和更快的目标，基本上教它如何追逐。^([1](ch08.html#idm45900986434304))
- en: Often, curriculum learning is shown being used in very complex scenarios that
    make it feel almost a bit like a magic bullet. Some problems it is used for feel
    like they’d be next to impossible to achieve if it weren’t for the magic wand
    of curriculum learning being waved over it, but the thing to remember is that
    at its core, it is a means of improving training, not doing the impossible. In
    general, any problem you can solve with curriculum learning could be solved without
    it, but it will usually just take a lot longer.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，课程学习被展示为在非常复杂的场景中使用，使其几乎感觉像是一种魔法弹药。一些它用于的问题如果没有课程学习的魔法棒可能几乎不可能实现，但要记住的是，从根本上说，它是一种改进训练的手段，而不是做不可能的事情。一般来说，任何您可以用课程学习解决的问题，都可以不用它解决，只是通常需要更长的时间。
- en: It turns out that just throwing more compute power at a problem can solve it,
    although it is a bit inelegant. Curriculum learning is at its best, in our opinion,
    when it is used to speed up or improve your agent’s training, and in that role
    it may well be the future of training models.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，仅仅通过增加计算能力来解决问题，虽然有点不够优雅，但确实有效。在我们看来，课程学习在最佳状态下时，是用来加快或改善你的代理训练的，而在这个角色中，它很可能是模型训练的未来。
- en: A Curriculum Learning Scenario
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 课程学习场景
- en: Let’s create and then solve a problem using curriculum learning. The problem
    we want to solve will be teaching an agent how to throw a ball at a target.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用课程学习创建并解决一个问题。我们要解决的问题是教代理如何向目标扔球。
- en: While throwing a ball is something that we as humans are intrinsically good
    at, it’s actually a really complex task. You have to take into account distance,
    throw force, angles, and ballistic arcs if you want to hit something.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然作为人类，我们天生擅长扔球，但这实际上是一个非常复杂的任务。如果你想要击中目标，你必须考虑距离、投掷力量、角度以及弹道弧线。
- en: The agent will always start in the center of the room, but the target will be
    scattered randomly around the space. The agent will have to figure out how strongly
    to throw the ball, on which vertical angle to aim, and in which direction to face
    before throwing.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 代理始终会从房间中心开始，但目标会随机散布在空间中。在投掷球之前，代理将不得不确定要多大力量扔球，以及垂直角度和投掷方向。
- en: This begs the question of what our curriculum will be and how we will ramp up
    the difficulty of the scenario.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了一个问题，即我们的课程将是什么，以及我们如何增加这种情景的难度。
- en: Like all reinforcement learning approaches, we’ll have a reward structure that
    encourages the agent to improve by giving it a small reward for near misses.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 像所有强化学习方法一样，我们将有一个奖励结构，通过给予接近未中的小奖励来鼓励代理改进。
- en: This reward structure will be the basis of our curriculum. We will start with
    a very large radius that counts as “near miss,” and over time, that radius will
    shrink, thus encouraging the agent to become more accurate to keep gaining rewards.
    Much like our earlier example (where the squirrel started off at an extremely
    large size, then was repeatedly shrunk down as the agent got the hang of what
    it was meant to do), the same applies here.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这种奖励结构将是我们课程的基础。我们将从一个非常大的半径开始，这被视为“接近未中”，随着时间的推移，该半径会缩小，从而鼓励代理变得更加准确以继续获得奖励。就像我们之前的例子（松鼠从一个极大的尺寸开始，然后随着代理理解其意图而不断缩小）一样，这里也是一样的。
- en: Our curriculum, then, will be broken up into several lessons of increasing difficulty
    where the distance the agent has to successfully throw near the target will be
    shrunk.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的课程将分为几个难度逐渐增加的课程，其中代理必须成功地将球扔到靠近目标的距离缩小。
- en: We can make as many levels of difficulty here as we feel we want or will need,
    but the core approach of our curriculum will be the same each time.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据需要设定多少难度级别，但我们的课程的核心方法每次都将是相同的。
- en: Building in Unity
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Unity中构建
- en: Let’s begin by building the environment in Unity. The finished environment will
    look something like [Figure 8-1](#fig:cl-final-scene).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从在Unity中构建环境开始。完成的环境将类似于[图8-1](#fig:cl-final-scene)。
- en: Unlike every other example we have created so far in the book, we will be doing
    something a little bit different with the simulation side in this one.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在本书中创建的所有其他例子不同，我们将在模拟侧面做一些略有不同的事情。
- en: We won’t be throwing our object and then waiting for the Unity physics engine
    to move it. Instead, we’ll calculate the landing point instantly and use that
    calculated point.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会扔出物体然后等待Unity物理引擎移动它。相反，我们会即时计算着陆点并使用该计算点。
- en: '![psml 0801](assets/psml_0801.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![psml 0801](assets/psml_0801.png)'
- en: Figure 8-1\. The environment in our scene, ready for curriculum learning
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-1\. 我们场景中的环境，准备进行课程学习
- en: Once the model is trained, we will make it so that we fling objects around because
    it will look cool, but not for training. We don’t need to throw the object in
    a way we can see, because we can calculate whether it would have hit our target.
    Simulating the visual component of actual flinging would unnecessarily slow things
    down during training, and we don’t need that. Once we train the model, we can
    add an appropriate visual that maps to what’s happening under the hood.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完毕，我们将使物体四处飞溅，因为这看起来很酷，但不是为了训练。我们不需要以我们能看到的方式扔物体，因为我们可以计算它是否会击中我们的目标。在训练期间模拟实际抛掷的视觉组件会不必要地减慢速度，我们不需要这样做。一旦训练好模型，我们可以添加适当的视觉效果，映射到底层发生的事情。
- en: The reason for this is because otherwise we’d have to add memory into our agent
    so that it learns to associate the throw action it took with the reward it gets
    a little while later. So we can do some testing and hook up heuristics (for manual
    human control, for testing, like we’ve done before), we will be visualizing the
    arc and the end point of the throw.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的原因是，否则我们必须在我们的代理中添加记忆，以便它学会将它采取的投掷动作与稍后得到的奖励联系起来。因此，我们可以进行一些测试，并连接启发式（用于手动人类控制、测试，如以前所做的那样），我们将可视化抛物线和投掷结束点。
- en: Note
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The ML-Agents framework does support adding memory to your agents, but because
    it *massively* increases complexity and training time, we are shying away from
    it where possible.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ML-Agents框架确实支持为您的代理添加记忆，但因为*显著*增加了复杂性和训练时间，所以我们尽可能地避免使用它。
- en: The [math of ballistics](https://oreil.ly/DU3M0) is well understood, so we don’t
    have to actually go through the steps; instead, we can mathematically model it.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 弹道数学是众所周知的，因此我们不必真正进行步骤；相反，我们可以进行数学建模。
- en: When we later make it so that our agent is throwing actual game objects, they
    will land exactly where the math says they will land. In situations where this
    isn’t possible, memory will be needed, but not here.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们稍后使我们的代理真正投掷游戏对象时，它们将准确地落在数学上说它们将落在的地方。在这种情况下不可能的情况下，将需要记忆，但在这里不需要。
- en: Create a new Unity project, add the Unity ML-Agents package, and get an empty
    scene ready before continuing. Our project is, quite imaginatively, named “CurriculumLearning.”
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的Unity项目，添加Unity ML-Agents包，并准备一个空场景继续。我们的项目名叫“CurriculumLearning”，相当有创意。
- en: Creating the Ground
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建地面
- en: 'First up we will need some ground; for this environment, we will be creating
    a single plane:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要一些地面；对于这个环境，我们将创建一个单一的平面：
- en: Create a new plane in the Hierarchy.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在层次结构中创建一个新的平面。
- en: Use the Inspector to name it “ground,” and set its position to `(0, 0, 0)` and
    its scale to `(20, 1, 20)`.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用检查器将其命名为“地面”，将其位置设置为`(0, 0, 0)`，并将其比例设置为`(20, 1, 20)`。
- en: 'With our ground created, we’re going to apply a color to it (as usual), so
    it is easier to visually distinguish the different pieces of the simulation from
    each other (for us, the human):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 创建好我们的地面后，我们将给它涂上颜色（通常情况下），这样可以更容易地在视觉上区分模拟中的不同部分（对于我们，人类来说）：
- en: Create a new material, and give it a name like “GroundGrass_Mat.”
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的材料，并给它取名为“GroundGrass_Mat”。
- en: Use the Inspector to set the material’s albedo color to a nice green color.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用检查器将材料的反射率颜色设置为漂亮的绿色。
- en: Drag the material from the Project pane onto the ground plane in either the
    Hierarchy or the scene itself.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从项目面板将材料拖动到层次结构或场景本身的地平面上。
- en: Now our ground has a material to help us tell everything apart. With that done,
    we’ll move on to make our target. Don’t forget to save your scene.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的地面有了材料，帮助我们区分一切。完成后，我们将继续制作我们的目标。不要忘记保存你的场景。
- en: Creating the Target
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建目标
- en: For this part of the scenario, our target will just be a simple cube. During
    training, it will be used to extract a position. However, during inference, it
    will be shot by our agent and flung around based on the laws of physics, so we
    need to set up the target to cover both scenarios.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个场景的这一部分中，我们的目标将只是一个简单的立方体。在训练期间，它将用于提取一个位置。但是，在推理期间，它将被我们的代理射击并根据物理法则四处飞溅，所以我们需要设置目标来涵盖这两种情况。
- en: Note
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The “inference phase” is when you’re using a trained model. Your agent infers
    actions from the model, hence inference phase.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: “推理阶段”是指您正在使用经过训练的模型时。您的代理从模型推断出动作，因此是推理阶段。
- en: Create a new cube in the Hierarchy.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在层次结构中创建一个新的立方体。
- en: Use the Inspector to name it “Target,” and set its position to `(0, 0.5, 0)`.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用检查器将其命名为“目标”，并将其位置设置为`(0, 0.5, 0)`。
- en: Add a Rigidbody component to it.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向其添加刚体组件。
- en: Our target needs to have a Rigidbody for later, when we run our simulation in
    inference mode, as we want it to react to having something thrown at it. This
    will have no impact on the training, but we need to set it up now.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在推理模式下运行模拟时，我们的目标需要有一个刚体，因为我们希望它对被投掷的东西作出反应。这对训练没有影响，但我们现在需要设置它。
- en: With that done, we can start work on our agent. Save the scene before continuing.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些后，我们可以开始工作我们的代理。在继续之前保存场景。
- en: The Agent
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代理
- en: 'Now it’s time to build our agent: the basis of our agent is very straightforward,
    as it’s going to be based on a cube (much like our target). What a surprise!'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是构建我们的代理的时候了：我们的代理的基础非常简单，因为它将基于一个立方体（很像我们的目标）。真是个惊喜！
- en: Create a new cube in the Hierarchy.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在层级中创建一个新的立方体。
- en: Name the cube “Agent,” and set its position to `(0, 0, 0)`.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将立方体命名为“Agent”，并将其位置设置为`(0, 0, 0)`。
- en: 'It’s simple, but it’s useful to have something to visualize here. However,
    there is one problem: it has a collider, because cubes by default have one. This
    will only get in the way when it comes time to spawn projectiles later during
    inference.'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这很简单，但在这里有可视化的东西是很有用的。然而，有一个问题：它有一个碰撞器，因为默认情况下立方体有一个。在后面推断时，这只会妨碍产生抛射物。
- en: In the Inspector, remove the Box Collider component from the agent.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在检视器中，从代理中移除框碰撞器组件。
- en: Now, the whole reason to have a mesh for our agent is to help us visualize what
    it is doing, but not all aspects of a ballistic trajectory are easily shown.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们为代理使用网格的整个原因是帮助我们可视化它正在做的事情，但不是所有弹道轨迹的方面都能轻松显示。
- en: Because the agent is a cube, we can kind of see in which direction it’s facing,
    but we will want another element to show the pitch of its aim.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因为代理是一个立方体，我们可以看到它朝向的方向，但我们希望有另一个元素来显示它瞄准的俯仰角。
- en: Add a new cylinder in the Hierarchy.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在层级中添加一个新的圆柱体。
- en: Drag the cylinder underneath the agent (in the Hierarchy), so it is a child
    of the agent.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将圆柱体拖到代理的下方（在层级中），使其成为代理的子对象。
- en: Use the Inspector to set the position of the cylinder to `(0, 1, 0)` and the
    rotation to `(0.2, 1, 0.2)`.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用检视器将圆柱体的位置设置为`(0, 1, 0)`，并将其旋转设置为`(0.2, 1, 0.2)`。
- en: In the Inspector, remove the Collider component from the cylinder.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在检视器中，从圆柱体中移除碰撞器组件。
- en: This gives us a small cylinder on top of the box that we can use to visually
    judge elevation.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我们一个小圆柱体放在箱子上方，我们可以用它来视觉上判断高度。
- en: While this will hinge in a strange way and clip through the box, it is fine
    for this as we only need it for the heuristic stage, and it has no impact on the
    simulation either in training or in inference.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这将以一种奇怪的方式关联并穿过盒子，但对于启发阶段来说没问题，对于训练或推理阶段，它对模拟没有影响。
- en: That’s most of the scene done. Save it before we continue.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分场景已完成。在继续之前保存它。
- en: Building the Simulation
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模拟
- en: With the basic structure of the scene ready to go, it’s time to start building
    out the simulation side of things. Specifically, as you hopefully guessed by now,
    we will be creating the actions, observations, and rewards our agent will need.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 随着场景的基本结构准备就绪，是时候开始构建模拟部分了。具体来说，正如你现在希望的那样，我们将创建代理需要的动作、观察和奖励。
- en: Making the Agent an Agent
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使代理成为代理
- en: While we may have the agent physically ready in the scene, it has no code; if
    nothing else, it will need to be made an `Agent` subclass.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可能已经在场景中准备好了代理，但它没有代码；至少，它将需要成为一个`Agent`子类。
- en: Create a new C# file, and name it *Launcher.cs.*
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的C#文件，并将其命名为*Launcher.cs*。
- en: 'Open *Launcher.cs* and replace it with the following:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开*Launcher.cs*并用以下内容替换它：
- en: '[PRE0]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the preceding code, we are declaring a number of variables that control
    the minimum and maximum values, along with some additional values we’ll use later
    on for adjusting rewards. Finally, there are two references: one for the target
    and one for the cylinder. We’ll need these elements shortly, so we might as well
    handle them now.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们声明了一些控制最小和最大值的变量，以及一些稍后用于调整奖励的额外值。最后，还有两个引用：一个用于目标，一个用于圆柱体。我们很快就需要这些元素，所以现在就处理它们吧。
- en: 'Each of these numeric variables relate to different elements of a ballistic
    throw:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字变量每个与弹道抛掷的不同元素相关：
- en: '*Elevation* is the vertical throw angle; it is capped between 0 and 90, and
    *Elevation Change Speed* is how quickly the elevation angle can be changed.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Elevation* 是垂直抛掷角度；它被限制在0到90之间，*Elevation Change Speed* 是俯仰角速度。'
- en: '*Power* is how much force the throw is to be delivered with, and *Power Max*
    and *Power Change Speed* put a limit on the maximum power and how quickly the
    agent can ramp its power up and down.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*力量*是投掷时施加的力量大小，*最大力量*和*力量变化速度*限制了最大力量和代理可以加速和减速其力量的速度。'
- en: '*Max Turn Speed* is how quickly the agent can rotate its facing; we don’t need
    to track the facing itself because unlike elevation, we are just going to be throwing
    forward.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*最大转向速度*是代理能够旋转其面向的速度；我们不需要追踪面向本身，因为与仰角不同，我们只会向前投掷。'
- en: '*Hit Radius* and *Reward Radius* are used to give rewards, so we will speak
    more about them shortly.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*击中半径*和*奖励半径*用于给予奖励，所以我们很快会详细讨论它们。'
- en: '*Firing Threshold* caps how often the agent is allowed to throw a projectile.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*发射阈值* 限制了代理允许投掷投射物的频率。'
- en: 'Now we need to hook up these various pieces:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要连接这些各种组件：
- en: Drag *Launcher.cs* onto the agent object in the scene.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *Launcher.cs* 文件拖放到场景中的代理对象上。
- en: Drag the target object from the Hierarchy into the `target` field on the Launcher
    component in the Inspector.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将目标对象从层级拖放到检查器中的`Launcher`组件的`target`字段中。
- en: Drag the cylinder object from the Hierarchy into the `cylinder` field on the
    Launcher component in the Inspector.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将圆柱体对象从层级中拖放到检查器中`Launcher`组件的`cylinder`字段中。
- en: 'Lastly, we need to add and configure the other necessary components for our
    agent:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要添加并配置代理的其他必要组件：
- en: Select the agent from the Hierarchy.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从层级中选择代理。
- en: In the Inspector, click the Add Component button and choose ML Agents → Decision
    Requester.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在检查器中，点击“添加组件”按钮，然后选择 ML Agents → 决策请求者。
- en: In the Inspector, click the Add Component button and choose ML Agents → Behavior
    Parameters.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在检查器中，点击“添加组件”按钮，然后选择 ML Agents → 行为参数。
- en: In the Behavior Parameters component section, rename the behavior to “Launcher.”
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在行为参数组件部分，将行为重命名为“Launcher”。
- en: Set the Vector Observations Space Size to `9`.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将向量观察空间大小设置为`9`。
- en: Set the number of Continuous Actions to `4`.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将连续动作的数量设置为`4`。
- en: In the Agent component, set the Max Step to `2000`.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在代理组件中，将最大步数设置为`2000`。
- en: Now that our agent has its basics done, we can start adding in the actions.
    Don’t forget to save everything.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的代理基础就绪，我们可以开始添加动作了。别忘了保存所有内容。
- en: Actions
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动作
- en: The actions by our agent are quite simple. It will be able to rotate its facing
    (or yaw), rotate its vertical aim direction (or pitch), increase or decrease its
    throwing strength, and finally, throw or fire its projectile.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代理的动作非常简单。它将能够旋转其朝向（或偏航角），旋转其垂直瞄准方向（或俯仰角），增加或减少其投掷力量，并最终投掷或发射其投射物。
- en: 'This means the actions buffer will have four values: *yaw change*, *pitch change*,
    *force change*, and *decision to fire* (or not).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着动作缓冲区将有四个值：*偏航变化*、*俯仰变化*、*力量变化* 和 *是否开火的决定*。
- en: Tip
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Here we’ll gate the value of the firing action based on a threshold value (`firingThreshold`),
    which essentially forces a continuous action into a discrete one.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将根据阈值 (`firingThreshold`) 限制发射动作的频率，从而将连续动作强制转换为离散动作。
- en: A *discrete action* is something the agent can do that is either happening or
    not, as compared to a *continuous action*, which is a spectrum of possible values
    the agent responds to.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*离散动作* 是代理可以执行的一种动作，要么发生，要么不发生，而 *连续动作* 是代理响应的可能值范围。'
- en: Unity supports using both continuous and discrete actions in a single agent;
    however, we are not doing that here.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Unity 支持在单个代理中同时使用连续和离散动作；但是，在这里我们不这样做。
- en: It’s extremely useful to be able to adjust this value on the fly so that we
    can make the model more aggressive just by tweaking how often it releases a projectile.
    Just know that you can mix and match continuous and discrete actions if you want.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 能够动态调整此值非常有用，这样我们可以通过调整释放投射物的频率来使模型更具侵略性。只需知道，您可以混合和匹配连续和离散动作。
- en: 'Before we create our action code, we need to create a few helper functions:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建动作代码之前，我们需要创建几个辅助函数：
- en: 'Add the following code to the `Launcher` class:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`Launcher`类中添加以下代码：
- en: '[PRE1]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The first of these methods, `LocalImpactPoint`, will give us a point on the
    ground plane where the projectile, once loosed, will land. `GetDisplacement`,
    the second method, returns the specific point in space where the projectile is
    based on its current point in time based on the ballistic arc.
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些方法中的第一个，`LocalImpactPoint`，将给出一个地面平面上的点，投射物一旦释放，将会着陆在这个点上。第二个方法 `GetDisplacement`
    根据弹道弧线上的当前时间点返回投射物所在的具体空间点。
- en: Because the physics of ballistic projection is so well understood, these two
    functions will give us the exact same results as if we used the Unity physics
    simulation directly, just without having to wait for all that pesky time to pass.
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因为弹道投射的物理学非常被理解，这两个函数将为我们提供与直接使用Unity物理仿真相同的确切结果，只是不必等待所有那些讨厌的时间过去。
- en: Note
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The `GetDisplacement` method is a slightly modified version of [Freya Holmer’s
    Trajectory class](https://oreil.ly/5GuvT) available under the MIT License. It’s
    part of a library of mathematical functions you might find useful; for more details
    and license information, see the [repository](https://oreil.ly/NilGU). We only
    needed this one function, so we didn’t include everything, but it’s a really cool
    library of code; you should check it out.
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`GetDisplacement`方法是[Freya Holmer的轨迹类](https://oreil.ly/5GuvT)的稍作修改版本，可在MIT许可下使用。它是一组您可能会发现有用的数学函数库的一部分；有关详细信息和许可信息，请参阅[存储库](https://oreil.ly/NilGU)。我们只需要这一个函数，所以我们没有包括所有内容，但这是一个非常棒的代码库，您应该去看看。'
- en: 'Next up we want to be able to visualize the ballistic arc so that we can check
    if it’s working and so that it can help us when we are testing the system heuristically:'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们希望能够可视化弹道弧线，以便在测试系统时，我们可以检查其是否正常工作，并帮助我们启发式地测试系统：
- en: 'Add the following method to the `Launcher.cs` class:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下方法添加到`Launcher.cs`类中：
- en: '[PRE2]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This method will draw out the arc of the trajectory as well as a small sphere
    at the point where it will intersect with the ground plane.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此方法将绘制出轨迹的弧线以及它将与地面平面相交的点处的小球。
- en: This works by iterating through the imagined throw, drawing a new segment of
    a line after each iteration, and finally just drawing a sphere at the end point.
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这通过迭代想象的投掷过程实现，每次迭代后绘制新的线段，并最终在终点处仅绘制一个球体。
- en: The `OnDrawGizmos` method is built into Unity itself, and is called on every
    frame. It can draw a variety of useful debug and assistance information (most
    of the helper visuals, like the translation arrows in the Scene view, are drawn
    using gizmos). These gizmos only appear in the Scene view and are never shown
    in the game or in builds.
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`OnDrawGizmos`方法是内置于Unity中的，并在每一帧调用。它可以绘制各种有用的调试和辅助信息（大多数辅助视觉，如场景视图中的翻译箭头，都是使用gizmos绘制的）。这些gizmos仅在场景视图中显示，不会在游戏或构建中显示。'
- en: Now we are ready to build out our actions.
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们准备好扩展我们的动作。
- en: 'Add the following method to the `Launcher.cs` class:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下方法添加到`Launcher.cs`类中：
- en: '[PRE3]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: There is quite a lot going on in this method. First we get the actions out of
    the buffer, and we check if the firing action is above its threshold.
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这种方法中有很多内容。首先我们从缓冲区中获取动作，然后检查释放动作是否超过其阈值。
- en: Then we start working our way through the actions, adjusting the pitch, yaw,
    and throw force based on the action values.
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，我们开始通过动作调整俯仰、偏航和抛出力。
- en: The real magic happens inside the `if` `(shouldFire)` section. It’s where we
    grant rewards.
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真正的魔力发生在`if` `(shouldFire)`部分。这是我们授予奖励的地方。
- en: First up we determine how far away from the target our projectile will land
    and then we give a scaling reward based on that distance. The distance factor
    conforms to a sigmoid shape.
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，我们确定我们的抛射物距离目标有多远，然后根据该距离提供一个比例奖励。距离因子符合sigmoid形状。
- en: Tip
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: We decided to scale the reward like this because, during our testing, we found
    that it worked better than a linear reward. Although linear does still work, it
    just takes longer. A lot of reinforcement learning involves this kind of trial
    and error.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们决定按照这种方式缩放奖励，因为在测试过程中，我们发现它比线性奖励效果更好。尽管线性仍然有效，但需要更长时间。很多强化学习都涉及这种试错。
- en: Next, if we are within the `hitRadius`, as in we directly hit the box, we give
    the agent a very large reward. Finally, we end the episode.
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，如果我们在`hitRadius`范围内，即直接击中箱子，我们给代理人一个非常大的奖励。最后，我们结束剧集。
- en: Note
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Because the agent only ends the episode and gets rewards once it releases a
    throw, sometimes it might take a little while before it gets any points.
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因为代理人只有在释放投掷后结束剧集并获得奖励，有时可能需要一段时间才能获得任何分数。
- en: You can wait this out, try adjusting the throw threshold, or cancel the training
    and restart it, hoping its random start is a bit more release-eager the next time
    around.
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以等待此过程，尝试调整抛出阈值，或取消训练并重新启动，希望其随机启动时会更愿意释放。
- en: 'All options work: just pick the one you find the most appealing.'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有选项都可以使用：只需选择您最喜欢的那个。
- en: Observations
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 观察
- en: The next step is to make our agent perceive the world through observations.
    We mentioned earlier that there would be nine observations, so let’s make them
    now. In this activity, we’ll be providing all the observations via code within
    an overriden `CollectObservations` and we won’t be adding any sensors inside the
    Unity Editor.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是让我们的代理通过观察感知世界。我们之前提到会有九个观察值，所以现在让我们来创建它们。在这个活动中，我们将通过代码在一个重写的`CollectObservations`方法中提供所有观察值，并且我们不会在Unity编辑器中添加任何传感器。
- en: 'Add the following method to *Launcher.cs*:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下方法添加到*Launcher.cs*中：
- en: '[PRE4]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The first three observations we add represent the *facing relative to the target*,
    the *elevation of the throw*, and the *force of the throw*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加的前三个观察值代表了与目标的*朝向*，*投掷的仰角*和*投掷的力量*。
- en: The next two are the *position where it will hit if thrown* and the *distance
    of the hit point relative to the goal*.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两个是*如果投掷，它将击中的位置*和*击中点相对目标的距离*。
- en: So, if we pretend the agent is an arm, we are essentially telling the agent
    the setup of its arm and how far off it would be if it were to throw based on
    this current setup.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们假装代理是一只手臂，实质上我们告诉代理其手臂的设置及基于当前设置抛出的位置有多偏离。
- en: Even though this seems like a lot of information, it’s not that different from
    how we (as humans) think about throwing things. We are pretty good at estimating
    the landing point of a throw before we make it, and we use this information to
    adjust.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这看起来像是大量信息，但这与我们（作为人类）思考如何投掷物体并没有太大不同。我们非常擅长在投掷之前估计着陆点，并使用这些信息进行调整。
- en: Our agent is just given a bit of extra help in that their “estimate” will be
    perfect. However, as humans, we can also estimate travel time and can even adjust
    our throws to hit a moving target that compensates for movement—our agent would
    have no idea how to handle that.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代理只是得到了一点额外的帮助，因为他们的“估计”将是完美的。然而，作为人类，我们也可以估计旅行时间，甚至可以调整我们的投掷来击中一个移动的目标以弥补其运动—我们的代理不知道如何处理这一点。
- en: Heuristic Controls for Humans
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人类的启发式控制
- en: This is a pretty complex scenario, so we want to test it first to make sure
    we haven’t made some sort of huge mistake before we start our agent off on training.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当复杂的情景，所以我们希望先测试一下，以确保在我们的代理开始训练之前没有犯任何巨大的错误。
- en: Let’s hook up some heuristic controls so that we can do a quick test of the
    basics of the scenario.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们添加一些启发式控制，以便我们可以快速测试场景基础知识。
- en: Tip
  id: totrans-163
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Also, it’s fun to play with.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，这样做很有趣。
- en: 'As with all our previous activities, we’re doing this so that we, as humans,
    can control things during testing, and not for anything that will be used in training
    (unlike recording demonstrations for BC):'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的所有活动一样，我们这样做是为了让我们作为人类在测试期间控制事物，并不是为了任何将用于训练的东西（不像为BC记录演示）：
- en: 'Add the following method to the `Launcher` class:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下方法添加到`Launcher`类中：
- en: '[PRE5]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This looks more complex than it is. While there is a fair chunk of code here,
    all it does is detect whether any of the specified QWEASD keys are held down,
    and if so, it increments the relevant action (W and S for pitch, A and D for rotation,
    Q and E for power) in the actions buffer.
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这看起来比实际复杂得多。虽然这里有一大块代码，但它只是检测是否按住了指定的QWEASD键之一，如果是，则在动作缓冲区中增加相应的动作（W和S用于俯仰，A和D用于旋转，Q和E用于力量）。
- en: This gives us full control over the facing, the pitch, and the power of the
    throw.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这使我们完全控制了朝向、投掷的角度和力量。
- en: Then, if the space bar is pressed, we also set the firing threshold to `1` to
    ensure that the agent will release its throw.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，如果按下空格键，我们还将发射阈值设置为`1`，以确保代理会释放其投掷。
- en: Let’s give it a go and see how it all works.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们试一试，看看它如何运作。
- en: Select the agent in the Hierarchy.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在层次结构中选择代理。
- en: In the Inspector, find the Behavior Parameters component.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在检视器中找到行为参数组件。
- en: Change the Behavior Type property from Default to Heuristic.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将行为类型属性从默认更改为启发式。
- en: Play the scene and try your best to hit the target.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行场景，尽力击中目标。
- en: Because we clearly made no mistakes and didn’t have to fix anything (right?),
    we are good to move on to the curriculum side of things.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 因为显然我们没有犯任何错误，也没有必要修复任何东西（对吧？），所以我们可以继续进行课程方面的工作。
- en: Creating the Curriculum
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建课程
- en: 'We said earlier that the curriculum will ramp up the difficulty by reducing
    the size of the radius that gives rewards, thus forcing the agent to get closer
    and closer to the target in order to keep getting rewards. This means we need
    to do several things: define a curriculum, determine the values that map to the
    difficulty, and make the environment reset be based on the curriculum values.
    Let’s start with resetting the values first.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前说过，课程将通过减小奖励半径的大小来增加难度，从而迫使代理人靠近目标以继续获得奖励。这意味着我们需要做几件事情：定义一个课程，确定映射到难度的值，并使环境重置基于课程值。让我们先从重置数值开始。
- en: Resetting the Environment
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重置环境
- en: We haven’t actually done any of the work to reset the environment yet. As mentioned
    earlier, we want to change the environment based on the curriculum, but this isn’t
    all we need to change.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上还没有做任何重置环境的工作。正如前面提到的，我们希望根据课程改变环境，但这并不是我们需要改变的全部。
- en: 'There are three pieces in play in our simulation: the agent, the target, and
    the reward signal.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的模拟中有三个元素在起作用：代理、目标和奖励信号。
- en: We want all three to be modified for each reset, but only one of these is to
    be impacted by the curriculum. This means we can reset most of the environment
    without caring at all about the curriculum itself.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望这三个元素在每次重置时都被修改，但只有其中一个会受到课程的影响。这意味着我们可以在不关心课程本身的情况下重置大部分环境。
- en: 'Add the following method to the `Launcher` class:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下方法添加到 `Launcher` 类中：
- en: '[PRE6]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This will be called by the training when a new training episode starts. Now,
    it doesn’t have a lot of code, but it does include all our curriculum learning-relevant
    code.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在每次新的训练周期开始时由训练调用。现在它没有太多的代码，但它确实包含了所有与我们的课程学习相关的代码。
- en: Here we randomize the initial facing, elevation, and power of the throw. Then
    we also pick a random point on our ground plane and move the target there. Finally,
    we set our `rewardRadius`, which is the element that determines how close we have
    to be to the target to get any rewards at all.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们随机设置初始方向、仰角和投掷力量。然后我们在地平面上随机选取一个点并将目标移动到那里。最后，我们设置我们的 `rewardRadius`，这是决定我们必须接近目标才能获得任何奖励的元素。
- en: This final step is the magic one as far as curriculum learning is concerned.
    The `rewardRadius` value is going to be set based on the value it gets from the
    `Academy` environment variables, specifically from the environment variable `rewardRadius`,
    which we haven’t set yet but will shortly. The environment variable here has a
    default value set, which in our case is going to be `25`, and this is used in
    case the environment variable can’t be found.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 对于课程学习来说，这最后一步是关键。`rewardRadius` 值将根据从 `Academy` 环境变量中获取的值进行设置，具体来说是从环境变量 `rewardRadius`
    获取的值，我们尚未设置，但很快会设置。这里的环境变量已经设置了一个默认值，在我们的情况下是 `25`，如果找不到环境变量，则使用此值。
- en: With that done, if you were to run the simulation as is and check the value
    of `rewardRadius` on the agent, you will see it’s always `25`, because we haven’t
    actually created a configuration yet, so it’s using the default. Still, that is
    our resetting done, so we can now move on to creating the curriculum side of things.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些后，如果你按照目前的配置运行模拟并检查代理的 `rewardRadius` 值，你会发现它始终是 `25`，因为我们实际上还没有创建配置，所以它正在使用默认值。尽管如此，这是我们的重置工作完成了，所以现在我们可以继续创建课程的一面。
- en: Curriculum Config
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 课程配置
- en: Our environment is correctly configured now to use the values the curriculum
    provides to ramp up difficulty, but we haven’t actually created a curriculum yet,
    so let’s fix that.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的环境已经正确配置为使用课程提供的值来增加难度，但我们实际上还没有创建课程，让我们来修正一下这个问题。
- en: 'To create our curriculum, we will be using a section of the YAML config file
    we’ve not hugely explored: the environment parameters. Here is where we can configure
    our curriculum to slowly build up the difficulty of the scenario.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建我们的课程，我们将使用 YAML 配置文件的一个我们尚未深入探讨的部分：环境参数。在这里，我们可以配置我们的课程以逐步增加场景的难度。
- en: 'All our curriculum essentially boils down to an extra set of values we add
    to our YAML file for training:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所有的课程本质上都归结为我们为训练添加到 YAML 文件中的额外一组值：
- en: Create a new YAML file, and name it “launcher.yaml”.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的 YAML 文件，并将其命名为“launcher.yaml”。
- en: 'Add the following text to the YAML file:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下文本添加到 YAML 文件中：
- en: '[PRE7]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This is a mostly straightforward setup of using PPO to train an agent, and if
    we weren’t showing off curriculum learning, this would be where we would stop.
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是一个基本上直接的设置，使用PPO来训练一个代理，如果我们不展示课程学习，那么这将是我们停止的地方。
- en: Now we are going to add in the parts relevant for our curriculum.
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们将添加适用于我们课程的相关部分。
- en: 'Add the following to the bottom of the YAML file:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下内容添加到 YAML 文件底部：
- en: '[PRE8]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here we are declaring a new environment parameter called `rewardRadius`, which
    is the same one we were using earlier in our code, and then setting it up to be
    modified by a curriculum.
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们声明了一个名为 `rewardRadius` 的新环境参数，这与我们之前在代码中使用的相同，然后设置它以便由课程修改。
- en: 'We only have a single lesson in our curriculum currently: we named it `Lesson0`,
    but we could call it anything we want. We will be adding some more in a moment,
    but for now let’s take a look at this one alone.'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目前我们的课程只有一节：我们将其命名为`Lesson0`，但我们可以随意更改。我们稍后将添加更多内容，但现在让我们单独看看这一节。
- en: First up, we have the `name`. As we mentioned before, we aren’t going to explicitly
    refer to this by name, but logs will make use of it, so it’s worth setting one.
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，我们有 `name`。正如我们之前提到的，我们不会明确地按名称引用它，但日志将会使用它，因此设定一个是值得的。
- en: Next we have two different properties, the `completion_criteria` and the `value`.
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来我们有两个不同的属性，`completion_criteria` 和 `value`。
- en: The `completion_criteria` is responsible for handling when the current lesson
    is finished and the next one can begin.
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`completion_criteria` 负责处理何时结束当前课程并开始下一课。'
- en: In particular, the two most important elements are the `measure` and the `m⁠i⁠n⁠_​l⁠e⁠s⁠s⁠o⁠n⁠_length`.
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 特别是，两个最重要的元素是 `measure` 和 `m⁠i⁠n⁠_​l⁠e⁠s⁠s⁠o⁠n⁠_length`。
- en: The `measure` can be either `reward` or `progress`. Instead of taking cues from
    the reward signal for when to change, `progress` uses the ratio of steps taken
    to maximum steps.
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`measure` 可以是`reward`或`progress`。而不是从奖励信号中获取变化时机，`progress`使用的是步骤数与最大步骤数的比率。'
- en: In our case, we want the reward itself to be the process by which we get better,
    hence why we are using it.
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们希望奖励本身成为我们进步的过程，因此我们正在使用它。
- en: Next, the `min_lesson_length` is a control to prevent essentially a lucky start
    from putting the agent into a harder environment than it is ready for.
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，`min_lesson_length` 是一个控制，防止幸运的开端将代理置于尚未准备好的更难环境中。
- en: The value of `100` means the agent has to perform a minimum of 100 iterations
    at or above the `threshold` value before the next lesson will begin.
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 值为`100`意味着代理必须在达到或超过`threshold`值的情况下执行至少100次迭代，然后下一课才会开始。
- en: Finally, the `value` property is where we can control the actual values that
    our environment parameter will get.
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，`value` 属性是我们可以控制环境参数实际值的地方。
- en: In our case, we are setting it to a value of `100`, which gives it a nice large
    initial region in which to acquire rewards.
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们将其设置为`100`的值，这使其在获取奖励的大初始区域内。
- en: Tip
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Instead of a single value, you can set the `value` to have a minimum and maximum
    value range and have the curriculum pick randomly from that range.
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 而不是单个值，您可以将 `value` 设置为具有最小和最大值范围，并让课程从该范围内随机选择。
- en: You can even configure if you want it to be a uniform or Gaussian sampling of
    the values across the range.
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您甚至可以配置它是在范围内均匀采样还是高斯采样。
- en: Now with our first lesson done, it’s time to finish off our curriculum.
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们已经完成了第一课，是时候完成我们的课程了。
- en: 'Add the following to the curriculum section of the YAML file:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下内容添加到 YAML 文件的课程部分：
- en: '[PRE9]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We’ve added three more lessons to the model here, and each is basically the
    same as the first—except that the values for how far away the box will be upon
    reset have increased.
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在这里模型中添加了另外三节课，每节课基本上与第一节相同——只是重置时箱子距离的值已经增加。
- en: Note
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Environment parameters set in the YAML file here don’t have to be used just
    for curriculum learning. Unity ML-Agents also uses them for any environment randomization.
    They are simply variables ML-Agents can access and then inject into the simulation,
    and you can use them however you want.
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里设置的 YAML 文件中的环境参数不仅限于课程学习。Unity ML-Agents还将它们用于任何环境随机化。它们只是 ML-Agents 可以访问并注入到模拟中的变量，您可以根据需要使用它们。
- en: The only difference is that in the final lesson, we are setting the reward radius
    to be quite small and don’t have completion criteria because we want the agent
    to be fully challenged. It’s important to mention the lessons in the curriculum
    are all part of a list, which is why the little dash (`-`) is next to the named
    lesson in the YAML file. Without this, the lessons won’t work.
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 唯一的区别是在最后一课中，我们将奖励半径设置得非常小，并且没有完成标准，因为我们希望代理受到充分挑战。重要的是要提到课程中的所有课程都是列表的一部分，这就是为什么在YAML文件中命名的课程旁边有一个小短线（`-`）。如果没有这个，课程将无法正常工作。
- en: Tip
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you are planning or need a lot of lessons in your curriculum, you can declare
    them as arrays instead of fully constructing each one like we have. We only have
    a few lessons, so writing them all out like we have is fine.
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果您计划或需要在课程中包含大量课程，可以将它们声明为数组，而不是像我们一样完全构建每个课程。我们只有几节课，所以像我们这样全部写出来是可以的。
- en: With our curriculum now written, we are finally ready to move on to training
    our model.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的课程已经编写完成，我们终于可以开始训练我们的模型了。
- en: Note
  id: totrans-225
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We only modified a single variable, `rewardRadius`, but you can have as many
    variables modified as you want, or you can use them to fundamentally alter the
    environment far more than we are doing. Another good candidate for this scenario
    would be to also reduce the radius of the `hitDistance` so that the perfect hits
    have to get more accurate as well. We are trying to keep things simple so that
    you can see how to use curriculum learning, but the principle is the same regardless
    of how many variables you modify in the lessons. As with all things in ML, working
    out the right touch to use a technique is often more “guess and check” than we
    might like.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只修改了一个变量，即`rewardRadius`，但您可以修改尽可能多的变量，或者您可以使用它们来在环境中进行更根本性的改变，远远超出我们所做的范围。对于这种情况的另一个很好的选择可能是减小`hitDistance`的半径，以便完美命中需要更高的精度。我们试图保持简单，以便您可以看到如何使用课程学习，但无论您在课程中修改多少变量，其原理始终如一。正如在所有ML中一样，确定何时使用一种技术的正确时机通常更多地是“猜测和检查”。
- en: Training
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练
- en: 'With everything set up, it’s time to get going with our training:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 一切准备就绪，现在是时候开始我们的训练了：
- en: In the Inspector, in the Behavior Parameters component of the agent, change
    the Behavior Type from Heuristic to Default.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在检查器中，在代理的行为参数组件中，将行为类型从启发式更改为默认。
- en: 'Run the following command:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令：
- en: '[PRE10]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Sit back, relax, and wait for the training to finish.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 坐下来，放松一下，等待训练完成。
- en: Note
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'There are a bunch of different variables you can tweak on the agent to adjust
    how the ballistic arc feels. You should play around with the settings in heuristic
    mode and see if you can find some you like. If you just want the ones we used,
    however, here they are:'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有许多不同的变量可以调整代理的设置，以调整弹道弧线的感觉。您应该在启发式模式下尝试各种设置，看看您是否能找到一些您喜欢的设置。但如果您只想使用我们使用的设置，这里是它们：
- en: Elevation Change Speed = 45
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 海拔变化速度 = 45
- en: Power Change Speed = 5
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 功率变化速度 = 5
- en: Power Max = 50
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大功率 = 50
- en: Max Turn Speed = 90
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大转向速度 = 90
- en: Hit Radius = 3
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命中半径 = 3
- en: Reward Radius = 100
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励半径 = 100
- en: Firing Threshold = 0.9
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发射阈值 = 0.9
- en: In our case, we found that training took approximately an entire day, so definitely
    don’t wait up for this one. But once it is done, you will have a nice neat little
    agent that is able to very accurately throw projectiles toward a target.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们发现训练大约需要整整一天时间，所以绝对不能等待完成。但一旦完成，您将拥有一个非常精确地向目标投掷抛射物的小而整洁的代理。
- en: Running It
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行它
- en: Because we are training the agent without actually lobbing virtual rocks around
    the scene, it would be quite dull to watch the trained agent.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们正在训练代理而不实际在场景中抛出虚拟岩石，所以观看训练代理会相当无聊。
- en: If you add your freshly trained model into Unity and hook it up to the agent,
    you’ll see it rotate and adjust its angle, but the only part of the process you’ll
    actually be able to see is it changing the yellow line arc that we added as a
    gizmo.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您将新训练的模型添加到Unity中并将其连接到代理，您将看到它旋转并调整角度，但您实际上能看到的过程只有我们添加为gizmo的黄线弧形的变化。
- en: What we want to see, though, is the agent adjusting pitch and yaw before throwing
    a virtual rock off into the distance; a yellow parabola just doesn’t cut it.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，我们想看到的是在将虚拟岩石抛出远处之前，代理调整俯仰和偏航；黄色抛物线简直不足以描述这个过程。
- en: Additionally, because the episode ends as soon as it launches a projectile,
    you wouldn’t get to see the target actually being hit, even if it did spawn projectiles,
    because it would suddenly be teleported to another part of the world, so we’d
    also have to change that.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，因为情节在发射抛射物后立即结束，所以即使它生成了抛射物，你也看不到目标实际被击中，因为它会突然被传送到世界的另一个部分，所以我们还需要改变这一点。
- en: This chapter is already quite long, however, and to add in all the steps necessary
    to show off the agent actually doing this would make it truly enormous.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，本章已经相当长了，如果要添加所有必要的步骤来展示代理实际执行这些步骤，将会使它变得非常庞大。
- en: As such, we are going to skip over that side of things and, in the time-honored
    tradition of cooking shows everywhere, say, “Here’s one we prepared earlier,”
    and direct you to [our website](https://oreil.ly/1efRA) if you want to check out
    the scene we created.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将跳过这一方面，并且以各地烹饪节目的传统，说：“这是我们之前准备好的一份”，并引导您访问[我们的网站](https://oreil.ly/1efRA)，如果您想查看我们创建的场景。
- en: This is a scene not hugely different from the one we created for training, but
    designing it to make watching the agent play out its actions is a bit more exciting.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这个场景与我们为训练创建的场景并没有太大的不同，但设计它使观看代理执行其动作更加令人兴奋。
- en: The core of this scene is similar to the previous training discussed earlier
    in this chapter. We have a target that will be placed randomly on the ground plane
    when the episode is reset. Unlike the training environment, we won’t end the episode
    when the agent fires. Instead, we will spawn a projectile with the physical properties
    determined by the agent. This projectile then flies out from the agent, and hopefully
    hits the target.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这个场景的核心与本章早些时候讨论的先前训练类似。当重置情节时，我们会在地面平面上随机放置一个目标。与训练环境不同的是，当代理发射时我们不会结束情节。相反，我们将生成一个具有代理确定的物理属性的抛射物。然后，这个抛射物从代理处飞出，希望能击中目标。
- en: Because it will likely hit the target a great deal of the time, we don’t immediately
    reset the environment. Instead, we fling the target up in the air by applying
    an explosive force to the target. After three hits on the target, or if it falls
    off the edge of the world, we then reset the episode.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 因为它可能会大部分时间击中目标，我们不会立即重置环境。相反，我们通过对目标施加爆炸力将其抛向空中。在目标被击中三次或者掉落到世界的边缘后，我们然后重置情节。
- en: Tip
  id: totrans-253
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: We picked three hits arbitrarily, as giving too many rewards can often muddy
    the waters a bit. Experiment to figure out what’s best for each scenario you build.
    Trial and error! Likewise, in this context, “edge of the world” means falling
    below the y-axis that the plane sits on (in other words, the projectile didn’t
    hit anything, and kept falling).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随意选择了三次命中，因为给予太多的奖励往往会使情况变得复杂。试验以找出对每个构建的情景最佳的解决方案。反复试验！同样，在这种情况下，“世界的边缘”指的是落在平面所在的y轴下方（换句话说，抛射物没有击中任何物体，并且继续下落）。
- en: Now, we aren’t limiting how many projectiles it can fire, so sometimes it will
    just be blasting them out, but we are deleting the projectiles if they fall off
    the edge of the world or if they hit something.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们不限制它可以发射多少个抛射物，所以有时它会不断地发射它们出去，但如果抛射物掉落到世界的边缘或者击中某物时，我们会将它们删除。
- en: If you want to increase or decrease the number of projectiles it lets loose,
    the easiest parameter to tweak is the `firingThreshold`. Increasing it will make
    it less likely to spawn a projectile, and decreasing it makes it more likely.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想增加或减少它释放的抛射物的数量，最简单的参数调整是`firingThreshold`。增加它会使生成抛射物的可能性较小，而减小它则会增加可能性。
- en: We found `0.6` was a good threshold for firing lots of projectiles; try some
    different values and see what works for you.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现`0.6`是一个很好的释放大量抛射物的阈值；尝试一些不同的值，看看哪个对您有效。
- en: Modifying the code to only support a single projectile isn’t too difficult and
    is left as an exercise for the reader.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 修改代码以仅支持单个抛射物并不太困难，留作读者的练习。
- en: If you are curious, the majority of the changes are in *InferenceLauncher.cs*
    and *Projectile.cs*, which contain all the code for managing the agent and projectiles
    themselves.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您感兴趣，大部分更改都在*InferenceLauncher.cs*和*Projectile.cs*中，这两个文件包含了管理代理和抛射物本身的所有代码。
- en: From an agent perspective, however, it is identical to the original launcher
    code we wrote earlier. The only real difference is we don’t have any rewards here,
    since they are unnecessary, so we took them out.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从代理的角度来看，它与我们之前编写的原始发射器代码完全相同。唯一的真正区别在于，我们这里没有任何奖励，因为它们是不必要的，所以我们把它们拿掉了。
- en: All other changes are visual tweaks. You can find these files in the book’s
    resources, which are available on the book’s special website.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 其他所有更改都是视觉上的微调。你可以在该书的资源中找到这些文件，这些资源可在该书的特定网站上找到。
- en: Curriculum Versus Other Approaches
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 课程学习与其他方法
- en: The entire point of curriculum training is to improve training; that is, to
    either increase the speed of training or the overall score of training (i.e.,
    the quality), or both. If you were starting to design a scenario like the one
    we used as the initial lesson, the reward radius of 100 might feel pretty large,
    as it covers a significant portion of the ground. It’s much more likely that designing
    something with a reward radius of, say, 25 will suit your needs better. In fact,
    we also created a training with a smaller radius of 25, and you can see the difference
    in [Figure 8-2](#fig:cl-rewards-1).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 课程训练的整个目的是改善训练；也就是说，要么增加训练的速度，要么提高训练的整体分数（即质量），要么两者兼而有之。如果你开始设计一个类似于我们用作初始课程的场景，奖励半径为100可能会感觉相当大，因为它涵盖了地面的大部分。设计一个奖励半径为25的东西更有可能更适合您的需求。事实上，我们还创建了一个奖励半径较小的25的训练，您可以在[图8-2](#fig:cl-rewards-1)中看到差异。
- en: '![psml 0802](assets/psml_0802.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![psml 0802](assets/psml_0802.png)'
- en: Figure 8-2\. TensorBoard rewards showing curriculum learning (upper line) versus
    traditional training (lower line)
  id: totrans-265
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-2\. TensorBoard 显示课程学习（上方线）与传统训练（下方线）的奖励
- en: When you show the plots side by side, you can see that not only was the curriculum
    learning faster, it was also learning better; that is to say, it got a higher
    average reward and it got to that higher reward with fewer steps. We actually
    stopped training the curriculum learning at a bit under 7 million iterations because
    it was basically getting the maximum rewards possible, whereas even after 10 million
    iterations the traditional learning approach was only getting about 90% of the
    maximum rewards possible. It’s worth pointing out that we also had some situations
    in which curriculum learning with the same setup was significantly slower (shown
    in [Figure 8-3](#fig:cl-rewards-2)) than the traditional learning approach, although
    it did still get a higher overall score.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将这些图并排显示时，你会发现课程学习不仅更快，而且学习效果更好；也就是说，它获得了更高的平均奖励，并且在更少的步骤内达到了更高的奖励。实际上，我们在接近700万次迭代时停止了课程学习的训练，因为它基本上已经获得了可能的最大奖励，而即使在1000万次迭代后，传统学习方法也只获得了可能最大奖励的约90%。值得指出的是，我们也遇到了一些情况，其中课程学习在相同设置下的速度显著较慢（如[图8-3](#fig:cl-rewards-2)所示），尽管它仍然获得了更高的总体得分。
- en: '![psml 0803](assets/psml_0803.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![psml 0803](assets/psml_0803.png)'
- en: Figure 8-3\. TensorBoard rewards showing slower curriculum learning (lower,
    curved line) versus traditional training (upper, straighter line)
  id: totrans-268
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-3\. TensorBoard 显示较慢的课程学习（较低的弯曲线）与传统训练（较高的直线）的奖励
- en: Our guess on this is that when it was seeded with initial random weighting in
    the neural net, it coincidentally happened to result in a low firing threshold,
    making it hesitant to throw a projectile. We believe this to be the case because
    it wasn’t struggling to learn once it worked out `Lesson0` in our curriculum;
    it just seemed to struggle to learn that first part. If you essentially shift
    the slower curriculum learning line over to the left, it’s basically identical
    to the first, resulting in a nigh identical shape and the same total reward. So,
    it goes to show that curriculum learning isn’t a magic bullet for every situation
    and those early stages of training do heavily influence the overall training,
    but even then it still has advantages in complex scenarios.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的猜测是，在神经网络中初始随机加权时，偶然地导致了低发射阈值，使其在投掷投射物时犹豫不决。我们认为这是因为在我们课程的`Lesson0`完成后，它并没有努力学习；它似乎只是在学习那个第一部分时遇到了困难。如果你将更慢的课程学习线移到左边，基本上与第一个相同，结果是几乎相同的形状和相同的总奖励。所以，这表明课程学习并不是每种情况的万能药，在训练的早期阶段确实会对整体训练产生重大影响，但即使如此，在复杂场景中它仍然具有优势。
- en: What’s Next?
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接下来做什么？
- en: With that done, we’ve unpacked a simple curriculum learning example. You can
    use curriculum learning for almost any kind of problem in which it makes more
    sense to break it down into steps.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些后，我们已经解析了一个简单的课程学习示例。你可以将课程学习应用于几乎任何需要分阶段解决的问题。
- en: Unity’s documentation comes with a couple of great curriculum learning examples,
    and if you want to explore this more, we’ve got some links to the best starting
    points in the [book’s online materials](https://oreil.ly/9WmyP).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: Unity的文档中有几个很棒的课程学习示例，如果你想进一步探索，我们在[书籍的在线材料](https://oreil.ly/9WmyP)中提供了一些最佳起点链接。
- en: '^([1](ch08.html#idm45900986434304-marker)) Editor’s note: this is a joke, which
    the Australians have informed us is hilarious.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch08.html#idm45900986434304-marker)) 编者注：这是一个笑话，澳大利亚人告诉我们这很搞笑。
