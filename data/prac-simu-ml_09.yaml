- en: Chapter 7\. Advanced Imitation Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。高级模仿学习
- en: In this chapter, we’re going to look at imitation learning (IL) using generative
    adversarial imitation learning (GAIL). We could use GAIL in an almost identical
    fashion to what we did when we used IL for behavioral cloning (BC), but that wouldn’t
    really be showing you anything new other than changing the configuration YAML
    file.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨使用生成对抗仿真学习（GAIL）的模仿学习（IL）。我们可以像我们使用行为克隆（BC）时那样几乎相同地使用GAIL，但这不会展示给您任何新内容，除了更改配置YAML文件外。
- en: With our simulations so far, we’ve done the basics, built upon them, and created
    a simple self-driving car, all using reinforcement learning. And in the previous
    chapter, we used IL to train an agent using human behavior. The IL we used for
    behavioral cloning attempted to maximize its similarity to our provided training
    data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，通过我们的模拟，我们已经完成了基础工作，建立在此基础上，并使用强化学习创建了一个简单的自动驾驶汽车。在上一章中，我们使用了IL来训练一个代理人，使用人类行为。我们用于行为克隆的IL试图最大化其与我们提供的训练数据的相似性。
- en: IL is not the only BC technique we can use. This time, we’ll use GAIL. GAIL
    can help improve the training of our agent, allowing it to essentially jump over
    the early hurdles in the learning process and let it focus on improving itself
    from then on.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: IL并不是我们能够使用的唯一的BC技术。这一次，我们将使用GAIL。GAIL可以帮助改善我们代理的训练，使其基本上跳过学习过程中的早期障碍，并让它专注于从那时起的改进。
- en: Tip
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: BC and GAIL can also be combined so that you can hopefully extract the benefits
    of both and mitigate the weaknesses of either. Toward the end of this chapter,
    we’ll cover how you can combine GAIL and BC, but for now the focus will be on
    GAIL.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: BC和GAIL也可以结合使用，以便您可以希望提取两者的优点并减少任何一者的弱点。在本章末尾，我们将介绍如何结合GAIL和BC，但目前的重点将放在GAIL上。
- en: Meet GAIL
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 遇见GAIL
- en: Before we start working on a GAIL-based activity with Unity and ML-Agents, we’re
    going to unpack a little bit of what makes GAIL tick.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始使用Unity和ML-Agents进行基于GAIL的活动之前，我们将解析一些使GAIL运行的要点。
- en: 'GAIL is, as its name implies, an adversarial approach to imitation learning
    and is based on a type of machine learning network called a *GAN*: a generative
    adversarial network. A GAN effectively plays two trained models, named a *discriminator*
    and a *generator*, against each other. The discriminator model judges how well
    the generator is copying some desired training data or behavior, and the feedback
    from the discriminator is used by the generator to direct and hopefully improve
    its actions.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: GAIL正如其名称所示，是一种对抗性的模仿学习方法，基于一种称为*GAN*的机器学习网络：生成对抗网络。GAN有效地将两个训练好的模型，称为*鉴别器*和*生成器*，相互对抗。鉴别器模型评估生成器复制所需的训练数据或行为的能力，来自鉴别器的反馈被生成器用来指导和希望改进其行动。
- en: These actions and behaviors are then fed back into the discriminator so that
    it learns more about the scenario. The discriminator is learning the rules and
    rewards of the scenario based on the actions taken by the generator and the provided
    demonstrations.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这些行动和行为然后被反馈给鉴别器，以便它更多地了解场景。鉴别器根据生成器采取的行动和提供的演示学习场景的规则和奖励。
- en: Tip
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: GAIL is a much newer approach to imitation learning than BC, but that doesn’t
    necessarily mean it is better; it is just different. Machine learning as a field
    is constantly in flux.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: GAIL是一种比BC更新得多的模仿学习方法，但这并不一定意味着它更好；它只是不同而已。机器学习作为一个领域处于不断变化之中。
- en: The natural question that arises, then, is *When should I use GAIL, and when
    should I use BC?* As with most things in machine learning, the answer isn’t simple.
    In general, choosing which to use depends more on the scenario you are intending
    to use. Just to confuse things, you can also combine them to (often) get better
    results than when using them in isolation.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然后自然出现的问题是*我应该何时使用GAIL，何时使用BC？*和大多数机器学习中的事物一样，答案并不简单。一般来说，选择使用哪种更多地取决于您打算使用的场景。令人困惑的是，您还可以将它们结合起来（通常）比单独使用它们效果更好。
- en: Note
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Academic research around GAIL often talks about inverse reinforcement learning
    and model-free learning, and other very fancy-sounding terms.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 学术研究围绕GAIL经常讨论逆强化学习和无模型学习，以及其他听起来非常花哨的术语。
- en: These basically mean that GAIL doesn’t have an intrinsic understanding of the
    world; it has to figure out both the rules of the scenario and the actions to
    maximize the scenario rewards.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本意味着GAIL没有对世界的固有理解；它必须找出场景的规则和最大化场景奖励的行动。
- en: As such, it does quite well when it’s just thrown into the deep end and has
    to work things out with minimal assistance.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当它被抛入深水区并且几乎没有帮助的情况下，它表现得相当不错。
- en: If you have a lot of human-generated training data that covers the gamut of
    possible changes in the environment, BC with IL will generally do better than
    GAIL.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您拥有大量覆盖环境中可能变化的人类生成的训练数据，则BC与IL结合的效果通常会优于GAIL。
- en: If you only have a little bit of human-generated data, GAIL will be able to
    extrapolate better from this to work out the best approach. GAIL also tends to
    work better than BC when combined with extrinsic rewards as defined by humans
    (using the `AddReward` function in ML-Agents).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您只有少量的人类生成数据，GAIL将能够更好地推断出最佳方法。在与由人类定义的外部奖励结合时（使用ML-Agents中的`AddReward`函数），GAIL的表现也往往优于BC。
- en: It’s often very tricky to work out the correct reward structure for your simulations
    when you’re using reinforcement learning. Using GAIL helps with this, because
    it operates without knowledge of exactly what your scenario is and somewhat has
    to work out what you want. It does this by relying on the information contained
    in the demonstration data. In complex scenarios where designing a good reward
    structure is difficult, you may be able to use GAIL to work out the scenario based
    on what you did even if you aren’t able to essentially explain why what you are
    doing is good.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用强化学习时，为模拟设置正确的奖励结构通常非常棘手。使用GAIL可以帮助解决这个问题，因为它在不知道具体场景的情况下运作，并且在某种程度上试图弄清楚您的期望。它通过依赖示范数据中包含的信息来实现这一点。在设计良好的奖励结构困难的复杂场景中，即使您不能本质上解释您的行为为何有效，您也可以使用GAIL来基于您的行为模式工作出解决方案。
- en: GAIL is a bit more flexible than BC with IL, but that isn’t why we are using
    it here; we are using it because GAIL works better when combined with extrinsic
    rewards. BC with IL is better when you have demonstrated exactly what to do, and
    GAIL is better when you have only provided partial information as demos.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: GAIL比BC与IL更加灵活，但这并不是我们在这里使用它的原因；我们之所以使用它，是因为GAIL在与外部奖励结合时效果更好。当您只提供部分示范信息时，GAIL的表现比BC与IL更好。
- en: Tip
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'Essentially, inside GAIL there are two wolves: the first wolf is working to
    better understand the world it is in, and the second wolf is performing actions
    it hopes will please the first wolf.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，在GAIL内部有两只“狼”：第一只“狼”致力于更好地理解它所处的世界，而第二只“狼”则执行希望能够取悦第一只“狼”的行动。
- en: Do What I Say and Do
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 按我说的去做和做
- en: There’s an old proverb, “Do what I say, not what I do,” that always comes to
    mind when training ML-Agents agents.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 有一句古老的谚语，“说到做到”，每当训练ML-Agent代理时我就会想起它。
- en: We basically just set up some rewards and tell the agent to figure it out from
    there. If we did this with children as they were growing up, it would be considered
    a pretty bad form of imparting knowledge, so instead we tend to show them what
    to do a few times, and then present them with the rules and let them improve from
    there.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基本上只设置了一些奖励，然后告诉代理从那里开始解决问题。如果我们在孩子成长过程中这样做，那会被认为是一种非常糟糕的知识传授方式，所以我们倾向于展示他们如何做几次，然后给他们规则并让他们在此基础上改进。
- en: Almost anytime you, as a human, are trained, you will generally be shown the
    correct way of doing it a few times before you’re expected to do it by yourself.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎任何时候，作为人类，当您接受训练时，通常会在您独立操作之前展示正确的操作方式几次。
- en: This is what we are going to attempt to reproduce here; we want to use GAIL
    to kick-start the training of our agent. We want to show the correct approach
    a few times and then let it work out the best approach from then on.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们在这里尝试复现的内容；我们希望使用GAIL来启动我们代理的训练。我们希望展示正确的方法几次，然后让它从那时开始找出最佳方法。
- en: A GAIL Scenario
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个GAIL场景
- en: 'For this scenario we are going to use a problem and environment similar to
    the activity we did earlier, when we trained an agent using IL for BC in [Chapter 6](ch06.html#chapter-introduction-to-imitation).
    Our activity concerned an environment with the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种情况，我们将使用类似于我们之前在第6章中使用IL进行BC训练的问题和环境。我们的活动涉及一个环境，具有以下特征：
- en: A goal area
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个目标区域
- en: A ball, acting as the agent, that needed to move to the goal
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个充当代理的球，需要移动到目标位置
- en: It looked like [Figure 7-1](#fig:ilenv).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 它看起来像[图7-1](#fig:ilenv)。
- en: '![psml 0701](assets/psml_0701.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![psml 0701](assets/psml_0701.png)'
- en: Figure 7-1\. Our IL environment, before modifying it for GAIL
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-1. 我们的IL环境，在修改为GAIL之前
- en: If the ball falls off the world, it ends the episode unsuccessfully; if the
    ball reaches the goal, it ends the episode successfully.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果球掉出世界，这将以失败结束本集；如果球达到目标，这将以成功结束本集。
- en: 'For our activity with GAIL, we’re going to use the same environment with a
    small addition:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们与 GAIL 的活动，我们将使用同一个环境，并进行小幅添加：
- en: There will be a “key” that the agent needs to reach before the goal is unlocked.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理需要先触摸“key”，然后才能解锁目标。
- en: Touching the goal without first having touched the key will do nothing.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在没有触摸到“key”之前触摸目标将不会有任何效果。
- en: 'At this point, you can either duplicate the Unity project you made for [Chapter 6](ch06.html#chapter-introduction-to-imitation),
    or just straight-up modify it. We’re choosing to duplicate the scene inside the
    project, so we open the Unity Editor and do the following:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，您可以复制您为[第 6 章](ch06.html#chapter-introduction-to-imitation)创建的 Unity 项目，或者直接进行修改。我们选择在项目内复制场景，因此我们打开
    Unity 编辑器并执行以下操作：
- en: Select the scene in the Project pane, as shown in [Figure 7-2](#fig:sel1).
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如图[7-2](#fig:sel1)所示，在项目窗格中选择场景。
- en: '![psml 0702](assets/psml_0702.png)'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![psml 0702](assets/psml_0702.png)'
- en: Figure 7-2\. Selecting the scene in the Project pane
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-2\. 在项目窗格中选择场景
- en: Select the Edit menu → Duplicate, as shown in [Figure 7-3](#fig:sel2).
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择“编辑”菜单 → 复制，如图[7-3](#fig:sel2)所示。
- en: '![psml 0703](assets/psml_0703.png)'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![psml 0703](assets/psml_0703.png)'
- en: Figure 7-3\. Choosing Duplicate
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-3\. 选择复制
- en: Rename the duplicated scene “GAIL” or something similar.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将复制的场景重命名为“GAIL”或类似名称。
- en: 'Make sure the new scene is open and ready to go. Then it’s time to add the
    key:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 确保新场景已经打开并准备就绪。然后是添加 key 的时间：
- en: Add a new cube to the project hierarchy.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在项目层次结构中添加一个新的立方体。
- en: Rename this cube “key.”
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将此立方体重命名为“key”。
- en: The cube will be partially embedded in the ground, but that’s OK for now. The
    next step is to modify the agent and the agent’s scripts.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 立方体将部分嵌入在地面中，但目前这没有问题。接下来的步骤是修改代理和代理的脚本。
- en: Note
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If you haven’t worked through [Chapter 6](ch06.html#chapter-introduction-to-imitation),
    we highly suggest you do that before trying this.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还没有完成[第 6 章](ch06.html#chapter-introduction-to-imitation)，我们强烈建议您在尝试这之前完成。
- en: Modifying the Agent’s Actions
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修改代理的动作
- en: Our agent as it currently exists only uses the training data in the demonstrations
    we recorded for it earlier—it has no reward structure otherwise set up.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当前的代理仅在我们为其录制的演示中使用训练数据——否则没有设置奖励结构。
- en: Having no rewards is great for BC with IL, but it’s not what we’re after for
    this activity with GAIL. We want the agent to use the training data to help start
    learning and then have the values from the rewards be the component the agent
    optimizes for.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 BC with IL 来说，没有奖励是很好的，但对于我们在 GAIL 活动中所追求的不是这样。我们希望代理使用训练数据来帮助开始学习，然后从奖励中获得值，作为代理优化的组成部分。
- en: Note
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Because we’re working in the same project as we did with BC-based IL, we are
    going to be modifying the `roller agent` class directly (which will impact the
    function of the scene we made in the previous chapter), but if you want to keep
    it as is, you can duplicate that file or create a new C# file to be your new agent.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们在与基于 BC 的 IL 相同的项目中工作，所以我们将直接修改`roller agent`类（这将影响我们在上一章中创建的场景的功能），但如果您希望保持原样，可以复制该文件或创建一个新的
    C# 文件作为新的代理。
- en: Just remember to hook it up to your agent in the scene and remove the old one.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 只需记住将其连接到场景中的代理并移除旧代理。
- en: Open *Roller.cs*.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 *Roller.cs*。
- en: 'Add the following member variables to the class:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在类中添加以下成员变量：
- en: '[PRE0]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The first of these two variables, `key`, will be used so that we have a reference
    to the `key` object in the scene, and the second will be used to know if we have
    picked up the `key`.
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这两个变量中的第一个变量`key`将被用于在场景中引用`key`对象，第二个变量将用于判断我们是否已经拿起了`key`。
- en: Now we could use some GameObject-specific information on the `key` itself to
    know whether it’s been hit and do that instead of having another variable lying
    around, but this isn’t such a huge savings to be bothered by it.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们可以在`key`本身上使用一些特定于 GameObject 的信息，以了解它是否被击中，而不是让另一个变量在那里浪费空间，但这并没有节省多少，以至于不值得被打扰。
- en: 'Replace the `OnActionReceived` method with the following code:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用以下代码替换`OnActionReceived`方法：
- en: '[PRE1]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The first part of this works similarly to our earlier code: it applies a force
    based on the movement action values. We are still resetting the environment if
    the agent rolls off the edge of the plane, but now we are punishing it for doing
    so.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分的工作方式与我们之前的代码类似：它基于移动动作值施加力。如果代理人滚出平面边缘，我们仍然会重置环境，但现在我们会因此惩罚它。
- en: Next, we work out if we have touched the key. If we have touched it, we deactivate
    the key (so that it no longer appears in the scene) and flag the key as found.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们要判断是否触碰到了关键物体。如果我们触碰到了它，我们会使关键物体失效（因此它不再出现在场景中），并标记关键物体已找到。
- en: Finally, we do something similar but with the goal instead of the key, and if
    we have the key, we give a reward and end the episode.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们做了类似的事情，但是针对的是目标而不是关键物体；如果我们有了关键物体，我们会给予奖励并结束这一集。
- en: 'We are using a distance value here, 1.2 units, to work out whether we are close
    enough. We chose that number because it is just a teensy bit bigger than the combined
    center distance to center distance of a unit sphere up against a unit cube. We
    are doing this because it is nice and simple code to show off. It isn’t perfect,
    however: what we are crudely doing is drawing a sphere around the agent of radius
    0.6 and seeing if anything is inside.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用一个距离值，1.2单位，来判断我们是否足够接近。我们选择这个数字是因为它比单位球体与单位立方体之间的中心距离要稍微大一点。我们这样做是因为这段代码简单明了，很好地展示了这个概念。然而，它并不完美：我们粗略地在代理周围画了一个半径为0.6的球体，并查看是否有东西在里面。
- en: Note
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Unity has a built-in method to do exactly that: `Physics.OverlapSphere`, which
    lets you define a center point and radius and see what `collider`s are inside
    that imaginary sphere. We aren’t using that method because it looks a little bit
    clunky, and to properly determine *what* you hit you should be using tags, which
    we’d need to set up. As such, we are keeping it simple and doing a distance check,
    but the built-in methods do have a lot of flexibility in letting you define collision
    layer masks, and if we had a more complex example, that’s what we’d do.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Unity有一个内置方法可以做到这一点：`Physics.OverlapSphere`，它允许你定义一个中心点和半径，并查看在这个想象的球体内有哪些`collider`。我们没有使用这种方法，因为它看起来有点笨拙，为了正确地确定你击中了什么，你应该使用标签，这需要我们设置起来。因此，我们保持简单，进行距离检查，但是内置方法在让你定义碰撞层蒙版方面有很大的灵活性，如果我们有一个更复杂的示例，那就是我们要做的。
- en: 'If you are curious, here are the basics of an `OverlapSphere` call. To work
    out what you hit or to filter the collisions to only relevant ones is an exercise,
    as they say, left to the reader:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你感兴趣，这里是一个`OverlapSphere`调用的基础。弄清楚你击中了什么或者将碰撞过滤到只有相关的碰撞是一个练习，正如他们所说，留给读者自己去做：
- en: '[PRE2]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Those are our modified actions; now on to the observations. Don’t forget to
    save your code.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 那些是我们修改后的动作；现在是关于观察结果的内容。别忘了保存你的代码。
- en: Modifying the Observations
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修改观察结果
- en: Working with observations is probably quite familiar by now, and we’ll be doing
    it all by passing observations to ML-Agents via the `CollectObservations()` function.
    The core changes to the observations from the IL-powered version are the additition
    of information about the key, and the status of the key.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在对观察结果的处理可能已经非常熟悉了，我们将通过将观察结果传递给ML-Agents的`CollectObservations()`函数来完成所有操作。与IL版本相比，对观察结果的核心更改是添加关于关键信息和关键状态的信息。
- en: 'With your code open, replace the `CollectObservations` method with the following
    code:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的代码打开时，用以下代码替换`CollectObservations`方法：
- en: '[PRE3]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This is not conceptually hugely different from what we had before; we are just
    tracking more things.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，这与以前并没有太大的不同；我们只是跟踪更多的事物。
- en: We still have our velocity and the direction to the goal, but we are adding
    a new observation of whether we have the key or not, and the direction to the
    key. If the key has been picked up, we don’t bother working out its direction;
    we just send over zero, which is about as close as we can get to not sending over
    an observation.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然拥有我们的速度和朝向目标的方向，但我们正在添加一个新的观察值来表明我们是否有关键物体，以及朝向关键物体的方向。如果关键物体已被捡起，我们就不再计算它的方向；我们只发送零，这几乎等同于不发送观察结果。
- en: The reason we do this is because we have to send over the same number of observations
    each time.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这样做的原因是因为我们必须每次发送相同数量的观察结果。
- en: All of this code changes the number of observations being sent over to the agent,
    as compared to the previous IL-powered version. We’ll fix that shortly.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些代码都改变了发送给代理的观察数量，与以前的基于IL的版本相比。我们很快会修复这个问题。
- en: Resetting the Agent
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重置代理
- en: For our final bit of code, we need to update the `OnEpisodeBegin()` function
    to appropriately reset everything. Specifically, we now need to reset the state
    and status of the key.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们最后的一段代码，我们需要更新`OnEpisodeBegin()`函数以适当地重置一切。具体来说，我们现在需要重置关键状态和状态。
- en: 'Replace the `OnEpisodeBegin` method body with the following code:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 替换`OnEpisodeBegin`方法体的代码如下：
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As with the observations, this isn’t hugely different from before: we are still
    resetting the agent back to the center and removing all forces on it, and we are
    still picking a random point and moving the goal to that. However, we are also
    flagging us as not having the key, ensuring that the key game object is active
    in the scene, and finally moving it to a random position.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 与观察一样，这与以前并没有太大不同：我们仍然将代理重置到中心并移除其所有力量，并且我们仍然选择一个随机点并将目标移动到那里。然而，我们还标记了我们没有钥匙，确保钥匙游戏对象在场景中处于活动状态，并最终将其移动到一个随机位置。
- en: With that change, our code is done. We don’t have to touch the heuristic code
    because nothing there has changed. Don’t forget to save before you return to the
    Unity Editor.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个改变，我们的代码就完成了。我们不需要触及启发式代码，因为那里的内容没有改变。在返回Unity编辑器之前别忘了保存。
- en: Updating the Agent Properties
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新代理属性
- en: 'Our agent’s code has changed a fair bit, so a lot of the component values set
    in the Inspector are no longer correct for this agent; let’s fix that. In the
    Unity Editor, with your scene open:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代理代码发生了相当大的变化，因此检查器中设置的许多组件值对于该代理不再正确；让我们来修复这个问题。在Unity编辑器中，打开您的场景：
- en: Select the agent in the Hierarchy.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在层次结构中选择代理。
- en: In the Inspector, find the Agent component.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在检查器中找到代理组件。
- en: Drag the key game object from the Hierarchy into the Key field in the Inspector.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将钥匙游戏对象从层次结构拖放到检查器中的键字段中。
- en: In the Inspector, find the Behavior Parameters component.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在检查器中找到行为参数组件。
- en: Set the space size of the observations to 7.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将观察的空间大小设置为7。
- en: With that, our agent is now correctly coded and configured. Let’s give it some
    training data next.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们的代理现在已经正确编码和配置。接下来让我们给它一些训练数据。
- en: Demonstration Time
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 演示时间
- en: 'This slightly modified world is no longer the same as the ones we previously
    used, so we should create some new demonstration data for the agent:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这个略微修改过的世界不再与我们之前使用的相同，所以我们应该为代理创建一些新的演示数据：
- en: Select the agent in the Hierarchy.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在层次结构中选择代理。
- en: In the Inspector, find the Behavior component.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在检查器中找到行为组件。
- en: Change the Type from Default to Heuristic.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将类型从默认更改为启发式。
- en: In the Heuristic Recorder component, set it to Record.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在启发式记录器组件中，将其设置为记录。
- en: Play the scenario.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 播放场景。
- en: Do your best to record some demonstration data.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尽力记录一些演示数据。
- en: Note
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You might be wondering why are we bothering to record new demonstration data
    considering we already did that back when we were using BC. We are doing this
    because that data has no rewards as part of it, which means GAIL will never be
    able to associate the actions and the reward. If we used the old data, we would
    be training GAIL without any extrinsic reward. This will work, but it isn’t the
    point of this chapter and very likely isn’t going to give you the results you
    are after.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会想知道为什么我们要记录新的演示数据，考虑到我们在使用BC时已经这样做了。我们这样做是因为这些数据没有奖励作为其一部分，这意味着GAIL将无法将动作与奖励关联起来。如果我们使用旧数据，我们将会在没有外部奖励的情况下训练GAIL。这样做是有效的，但不是本章的重点，很可能不会给您想要的结果。
- en: Once you feel you have enough data recorded, stop the scene. You should now
    have some data you can use to feed into GAIL.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您觉得已经记录足够的数据，请停止场景。现在您应该有一些数据可以用来输入到GAIL中。
- en: Tip
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you select the demonstration file that was created, inside the Unity Inspector
    you can see the average reward we were getting. It also shows some other information,
    but the mean reward is the main thing we care about here. If it is too low, it
    might not be a particularly good demonstration file.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您选择了创建的演示文件，在Unity检查器中可以看到我们获得的平均奖励。它还显示一些其他信息，但在这里我们关心的主要是平均奖励。如果太低，这可能不是一个特别好的演示文件。
- en: Next up, training.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是训练。
- en: Training with GAIL
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GAIL进行训练
- en: If you guessed, “Do I just set some weird settings in a YAML file to enable
    GAIL?” you’d be correct, so once again it is time for another exciting round of
    *Let’s Edit Some Magic Numbers in YAML!* In the case of GAIL, the relevant bits
    we want to play around with are all part of the reward settings.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你猜想，“我只需在 YAML 文件中设置一些奇怪的设置来启用 GAIL 吗？” 那么你是正确的，所以我们再次迎来令人兴奋的一轮*让我们在 YAML
    文件中编辑一些魔法数字*。对于 GAIL 来说，我们想要调整的相关部分都是奖励设置的一部分。
- en: 'For this scenario, we are going to be using the same training configuration
    file we used earlier on when we had BC, but we will be making some changes. First
    up, we will want to make a new configuration file:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种情况，我们将使用早期使用 BC 时使用的相同训练配置文件，但我们会做一些更改。首先，我们需要创建一个新的配置文件：
- en: Duplicate *rollerball_config.yaml* and name it *rollerball_gail_config.yaml*.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复制 *rollerball_config.yaml* 并将其命名为 *rollerball_gail_config.yaml*。
- en: Next, you need to remove the BC-relevant sections of the configuration.
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，您需要移除配置中与 BC 相关的部分。
- en: Delete the `behavioral_cloning` line and all lines below and indented from it.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除 `behavioral_cloning` 行以及其下和缩进的所有行。
- en: Finally, we want to add in GAIL.
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，我们希望添加 GAIL。
- en: 'Under the `reward_signals` section, add a new section for GAIL:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `reward_signals` 部分下，添加一个新的 GAIL 部分：
- en: '[PRE5]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: There are a few different GAIL parameters we can tweak; here we are only setting
    two, and only one of them is required.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以调整几个不同的 GAIL 参数；在这里，我们只设置了两个，而且只有一个是必需的。
- en: The required one is `demo_path`, which is pointing to the demo file we created
    just a moment ago. We have also set `strength`, which has a default value of `1`,
    and we are setting it well below that because `strength` is used by GAIL to scale
    the reward signal.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 必需的是 `demo_path`，它指向我们刚刚创建的演示文件。我们还设置了 `strength`，其默认值为 `1`，我们将其设置得远低于默认值，因为
    `strength` 被 GAIL 用来调整奖励信号的比例。
- en: We are setting it so low because we have less-than-optimal demonstration data
    and are planning on the extrinsic reward signal being the main indicator of what
    action to take. If we gave it a stronger signal, it would learn more like our
    demonstration file and less like the optimal play for the scenario.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其设置得很低，因为我们的演示数据不太理想，并且计划外部奖励信号是确定采取何种行动的主要指标。如果我们给它一个更强的信号，它会更像我们的演示文件学习，而不是像场景的最佳玩法。
- en: Other settings we can configure here for GAIL (but are leaving at their defaults)
    include the size of the discriminator, the learning rate, and the gamma, among
    others.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在这里配置 GAIL 的其他设置（但保持它们的默认值），包括鉴别器的大小、学习率和伽马等。
- en: We don’t need any of these here, so we are leaving them at their default settings,
    but if you are interested in them, the [official docs](https://oreil.ly/6w9DO)
    have a description of them all should GAIL not be working the way you want.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这里不需要任何这些设置，所以我们将它们保持在默认设置，但如果您对它们感兴趣，[官方文档](https://oreil.ly/6w9DO)中对它们都有描述，如果
    GAIL 不按照您希望的方式工作。
- en: Warning
  id: totrans-125
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Because of how GAIL was designed, it has the habit of introducing [various biases](https://oreil.ly/gNIgb)
    into the agent; that is to say, it often tries to extend the episode length even
    if this is in direct conflict with the goals of the scenario.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 GAIL 的设计方式，它有引入[各种偏见](https://oreil.ly/gNIgb)到代理中的习惯；也就是说，即使这与场景目标直接冲突，它经常试图延长情节长度。
- en: Due to this, if during training you find your agent basically hanging around
    and not completing the task at hand, you very likely need to lower the GAIL reward
    signal to prevent it from overpowering the extrinsic reward.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个原因，在训练过程中，如果您发现您的代理基本上只是闲逛而不完成手头的任务，很可能需要降低 GAIL 奖励信号，以防止其压倒外部奖励。
- en: 'When done, the finished YAML file should look like the following:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，完成的 YAML 文件应该如下所示：
- en: '[PRE6]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'With our configuration file configurated, it’s time to start the actual training:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 配置文件配置完成后，现在是开始实际训练的时候了：
- en: Select the agent inside Unity.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Unity 中选择代理程序。
- en: In the Inspector, inside the Behavior Parameters component, change the Behavior
    Type to Default.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在检视器中，在行为参数组件内，将行为类型更改为默认。
- en: In the Inspector, inside the Demonstration Recorder, untick the Record box.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在检视器中，在演示录制器中，取消选中记录框。
- en: 'In the command line, run the following command:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在命令行中，运行以下命令：
- en: '[PRE7]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Once that has started, go back to Unity and press Play.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦开始，返回 Unity 并按 Play 按钮。
- en: The agent should now be training. Take a quick (or not so quick) coffee break,
    and we will return once that finishes.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 代理现在应该正在训练中。快速（或不那么快）地喝杯咖啡，让我们在训练完成后再继续。
- en: Running It and Beyond
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行及其后续
- en: 'Once our training has finished, we can run it like we’ve been doing so far:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的训练完成，我们可以像之前一样运行它：
- en: Add the trained *.onnx* model file into Unity.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练好的 *.onnx* 模型文件添加到 Unity 中。
- en: Select the agent inside Unity.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Unity 中选择代理。
- en: In the Inspector, inside the Behavior Parameters component, change the Behavior
    Type to Inference Only.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Inspector 中，在行为参数组件内，将行为类型更改为仅推理。
- en: Drag the model file into the model slot.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型文件拖放到模型槽中。
- en: Click Play, and sit back and enjoy watching your agent roll about, picking up
    cubes.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击播放，坐下来享受观看你的代理在周围滚动，捡起方块。
- en: In the case of our agent, after being trained for 500,000 iterations it was
    getting an average reward score of 0.99, which is about as perfect as could ever
    be expected.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的代理来说，在训练了 500,000 次迭代后，它的平均奖励分数达到了 0.99，这几乎是完美的表现。
- en: 'When compared to our demo file, its average reward was 0.95, so the agent has
    edged us out, which is what we expected: the student has become the master.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们的演示文件相比，其平均奖励为 0.95，所以代理已经超越了我们，这正是我们预期的：学生已经成为了大师。
- en: Now we’ve covered the basics of combining GAIL with extrinsic reward factors,
    but before we move on to other topics and chapters, it is a good time to talk
    about combining GAIL. In this example we combined GAIL with extrinsic rewards,
    but we can also combine it with imitation learning and behavioral cloning as well.
    To do so, all we have to do is add the BC configuration elements back into the
    YAML config file.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了将 GAIL 与外部奖励因素结合的基础知识，但在我们继续其他主题和章节之前，现在是时候谈谈如何结合 GAIL 了。在这个例子中，我们将
    GAIL 与外部奖励结合在一起，但我们也可以将其与模仿学习和行为克隆结合起来。为此，我们只需将 BC 配置元素添加回 YAML 配置文件中即可。
- en: The trick, however, comes from balancing the relative strength values of extrinsic,
    GAIL, and BC rewards.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，诀窍在于平衡外部奖励、GAIL 和 BC 奖励的相对强度值。
- en: For this scenario, we tried a variety of different values of all three, tweaked
    other configuration settings, and even tried limiting the BC to the first part
    of the training, but we didn’t see any significant improvement in the training.
    In one case, when attempting to best blend the various elements, we ended up with
    an agent that was so bad its average reward was -0.4, which means most of the
    time it just fell clean off the edge of the ground; our just GAIL or just BC both
    worked great.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种情况，我们尝试了各种不同的三个值，调整了其他配置设置，甚至尝试将 BC 限制在训练的前半部分，但我们并没有看到训练有任何显著改善。在某些情况下，当试图最佳地融合各种元素时，我们最终得到的代理表现非常糟糕，其平均奖励为
    -0.4，这意味着大部分时间它只是干脆从地面边缘掉下去；我们的 GAIL 或仅 BC 都表现出色。
- en: It may well be that in this scenario it is simple enough that such adjustments
    don’t provide enough value, or maybe we just haven’t found the right values to
    make it all click.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 或许在这种情况下，这些调整并没有提供足够的价值，或者也许我们只是还没有找到合适的值让一切顺利运行。
- en: Unity found in its [Pyramid example](https://oreil.ly/dP67X) that when training
    with the combination of different techniques, the agent trained faster and better
    than with any other approach done by itself.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Unity 在其 [金字塔示例](https://oreil.ly/dP67X) 中发现，当结合不同的技术进行训练时，与单独使用任何其他方法相比，代理训练速度更快且效果更好。
- en: There definitely is something that makes sense about combining different approaches;
    after all, it isn’t that different from how we learn. We try and combine many
    different techniques to get the best outcome we can when we are growing up, so
    why should an agent be any different? There is a great amount of potential in
    imitation learning, and because it is relatively easy to add into your training,
    it is well worth checking out.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 结合不同方法确实有其合理之处；毕竟，这与我们学习的方式并没有太大不同。我们尝试结合许多不同的技术以获得尽可能好的结果，那么为什么代理就应该有所不同呢？模仿学习有着巨大的潜力，并且因为它相对容易添加到你的训练中，所以非常值得一试。
