- en: Chapter 2\. End-to-End Machine Learning Project
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 2 章：端到端机器学习项目
- en: Before we begin exploring unsupervised learning algorithms in detail, we will
    review how to set up and manage machine learning projects, covering everything
    from acquiring data to building and evaluating a model and implementing a solution.
    We will work with supervised learning models in this chapter—an area most readers
    should have some experience in—before jumping into unsupervised learning models
    in the next chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们详细探讨无监督学习算法之前，我们将回顾如何设置和管理机器学习项目，涵盖从获取数据到构建和评估模型以及实现解决方案的所有内容。我们将在本章使用监督学习模型——大多数读者应该对此有所了解——然后在下一章跳入无监督学习模型。
- en: Environment Setup
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 环境设置
- en: Let’s set up the data science environment before going further. This environment
    is the same for both supervised and unsupervised learning.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们先设置数据科学环境。这个环境对于监督学习和无监督学习都是相同的。
- en: Note
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: These instructions are optimized for the Windows operating system but installation
    packages are available for Mac and Linux, too.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这些说明针对的是 Windows 操作系统的优化，但也提供了适用于 Mac 和 Linux 的安装包。
- en: 'Version Control: Git'
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 版本控制：Git
- en: If you have not already, you will need to install [Git](https://git-scm.com/).
    Git is a version control system for code, and all the coding examples in this
    book are available as Jupyter notebooks from [the GitHub repository](http://bit.ly/2Gd4v7e).
    Review Roger Dudler’s [Git guide](http://rogerdudler.github.io/git-guide/) to
    learn how to clone repositories; add, commit, and push changes; and maintain version
    control with branches.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有安装 [Git](https://git-scm.com/)，你需要安装它。Git 是一个用于代码版本控制的系统，本书中的所有代码示例都可以在
    [GitHub 仓库](http://bit.ly/2Gd4v7e) 的 Jupyter notebooks 中找到。请参阅 Roger Dudler 的
    [Git 指南](http://rogerdudler.github.io/git-guide/)，了解如何克隆仓库、添加、提交和推送更改，并使用分支进行版本控制。
- en: Clone the Hands-On Unsupervised Learning Git Repository
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 克隆《实战无监督学习》Git 仓库
- en: 'Open the command-line interface (i.e., command prompt on Windows, terminal
    on Mac, etc.). Navigate to the directory where you will store your unsupervised
    learning projects. Use the following prompt to clone the repository associated
    with this book from GitHub:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 打开命令行界面（例如 Windows 上的命令提示符，Mac 上的终端等）。导航至你将存储无监督学习项目的目录。使用以下提示从 GitHub 克隆与本书相关的仓库：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Alternatively, you can visit [the repository](http://bit.ly/2Gd4v7e) on the
    GitHub website and manually download the repository for your use. You can *watch*
    or *star* the repository to stay updated on changes.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以访问 [仓库](http://bit.ly/2Gd4v7e) 的 GitHub 网站，手动下载仓库供你使用。你可以 *watch* 或 *star*
    该仓库以便随时了解更新。
- en: Once the repository has been pulled or manually downloaded, use the command-line
    interface to navigate into the *handson-unsupervised-learning* repository.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦仓库被拉取或手动下载，使用命令行界面导航至 *handson-unsupervised-learning* 仓库。
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For the rest of the installations, we will continue to use the command-line
    interface.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于接下来的安装步骤，我们将继续使用命令行界面。
- en: 'Scientific Libraries: Anaconda Distribution of Python'
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 科学计算库：Python 的 Anaconda 发行版
- en: To install Python and the scientific libraries necessary for machine learning,
    download the [Anaconda distribution](https://www.anaconda.com/download/) of Python
    (version 3.6 is recommended because version 3.7 is relatively new as of the writing
    of this book and not supported by all the machine libraries we will use).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装 Python 和机器学习所需的科学计算库，请下载 Python 的 [Anaconda 发行版](https://www.anaconda.com/download/)（推荐使用版本
    3.6，因为本书编写时版本 3.7 较新，不是所有我们将使用的机器学习库都支持该版本）。
- en: 'Create an isolated Python environment so that you can import different libraries
    for each project separately:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个孤立的 Python 环境，以便你可以为每个项目单独导入不同的库：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This creates an isolated Python 3.6 environment—with all of the scientific libraries
    that come with the Anaconda distribution—called `unsupervisedLearning`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个名为 `unsupervisedLearning` 的孤立的 Python 3.6 环境——其中包含 Anaconda 发行版提供的所有科学计算库。
- en: 'Now, activate this for use:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，激活它以便使用：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Neural Networks: TensorFlow and Keras'
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络：TensorFlow 和 Keras
- en: 'Once unsupervisedLearning is activated, you will need to install TensorFlow
    and Keras to build neutral networks. TensorFlow is an open source project by Google
    and is not part of the Anaconda distribution:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦激活 unsupervisedLearning，你需要安装 TensorFlow 和 Keras 来构建神经网络。TensorFlow 是由 Google
    开源的项目，不是 Anaconda 发行版的一部分：
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Keras is an open source netural network library that offers a higher-level
    API for us to use the lower-level functions in TensorFlow. In other words, we
    will use Keras on top of TensorFlow (the backend) to have a more intuitive set
    of API calls to develop our deep learning models:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 是一个开源的神经网络库，它为我们提供了一个更高级的 API，用于在 TensorFlow 的底层函数上进行操作。换句话说，我们将在 TensorFlow（后端）之上使用
    Keras，以便使用更直观的 API 调用来开发我们的深度学习模型：
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Gradient Boosting, Version One: XGBoost'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度增强，第一版：XGBoost
- en: Next, install one version of gradient boosting known as XGBoost. To make this
    simple (for Windows users, at least), you can navigate into the *xgboost* folder
    in the *handson-unsupervised-learning* repository and find the package there.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，安装一种称为 XGBoost 的梯度增强的版本。为了简化操作（至少对 Windows 用户而言），您可以导航到 *handson-unsupervised-learning*
    存储库中的 *xgboost* 文件夹，并在那里找到包。
- en: 'To install the package, use `pip install`:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装该包，请使用 `pip install`：
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Alternatively, download the correct version of [XGBoost](http://bit.ly/2G1jBxs)
    based on your system—either the 32-bit or the 64-bit version.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，根据您的系统下载正确版本的 [XGBoost](http://bit.ly/2G1jBxs) —— 32 位或 64 位版本。
- en: 'In the command-line interface, navigate to the folder with this newly downloaded
    file. Use `pip install`:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在命令行界面中，导航到具有此新下载文件的文件夹。使用 `pip install`：
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Your XGBoost WHL filename may be slightly different as newer versions of the
    software are released publicly.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 您的 XGBoost WHL 文件名可能会略有不同，因为新版本的软件已公开发布。
- en: Once XGBoost has been successfully installed, navigate back to the *handson-unsupervised-learning*
    folder.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 安装成功后，回到 *handson-unsupervised-learning* 文件夹。
- en: 'Gradient Boosting, Version Two: LightGBM'
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度增强，第二版：LightGBM
- en: 'Install another version of gradient boosting, Microsoft’s LightGBM:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 安装另一个梯度增强版本，Microsoft 的 LightGBM：
- en: '[PRE8]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Clustering Algorithms
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类算法
- en: Let’s install a few clustering algorithms we will use later in the book. One
    clustering package, *fastcluster*, is a C++ library with an interface in Python/SciPy.^([1](ch02.html#idm140637564820880))
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们安装一些在本书后面将要使用的聚类算法。其中一个聚类包 *fastcluster* 是一个 C++ 库，具有 Python/SciPy 的接口。^([1](ch02.html#idm140637564820880))
- en: 'This fastcluster package can be installed with the following command:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下命令安装这个 fastcluster 包：
- en: '[PRE9]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Another clustering algorithm is *hdbscan*, which can also be installed via
    pip:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个聚类算法是 *hdbscan*，也可以通过 pip 安装：
- en: '[PRE10]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'And, for time series clustering, let’s install *tslearn*:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，为了时间序列聚类，让我们安装 *tslearn*：
- en: '[PRE11]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Interactive Computing Environment: Jupyter Notebook'
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交互式计算环境：Jupyter Notebook
- en: 'Jupyter notebook is part of the Anaconda distribution, so we will now activate
    it to launch the environment we just set up. Make sure you are in the *handson-unsupervised-learning*
    repository before you enter the following command (for ease of use):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter notebook 是 Anaconda 发行版的一部分，因此我们现在将其激活，以启动我们刚刚设置的环境。在输入以下命令之前，请确保您位于
    *handson-unsupervised-learning* 存储库中（为了方便使用）：
- en: '[PRE12]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You should see your browser open up and launch the *[*http://localhost:8888/*](http://localhost:8888/)*
    page. Cookies must be enabled for proper access.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到浏览器打开并启动 *[*http://localhost:8888/*](http://localhost:8888/)* 页面。必须启用 Cookie
    才能正常访问。
- en: We are now ready to build our first machine learning project.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备构建我们的第一个机器学习项目。
- en: Overview of the Data
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据概述
- en: In this chapter, we will use a real dataset of anonymized credit card transactions
    made by European cardholders from September 2013.^([2](ch02.html#idm140637564803888))
    These transactions are labeled as fraudulent or genuine, and we will build a fraud
    detection solution using machine learning to predict the correct labels for never-before-seen
    instances.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用一个真实的数据集，该数据集包含 2013 年 9 月由欧洲持卡人进行的匿名信用卡交易。^([2](ch02.html#idm140637564803888))
    这些交易被标记为欺诈或真实，我们将使用机器学习构建欺诈检测解决方案，以预测从未见过的实例的正确标签。
- en: This dataset is highly imbalanced. Of the 284,807 transactions, only 492 are
    fraudulent (0.172%). This low percentage of fraud is pretty typical for credit
    card transactions.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集高度不平衡。在 284,807 笔交易中，只有 492 笔是欺诈交易（0.172%）。这种低欺诈比例对于信用卡交易来说相当典型。
- en: There are 28 features, all of which are numerical, and there are no categorical
    variables.^([3](ch02.html#idm140637564800304)) These features are not the original
    features but rather the output of principal component analysis, which we will
    explore in [Chapter 3](ch03.html#Chapter_3). The original features were distilled
    to 28 principal components using this form of dimensionality reduction.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 共有 28 个特征，全部为数值特征，没有分类变量。^([3](ch02.html#idm140637564800304)) 这些特征不是原始特征，而是通过主成分分析得出的，我们将在
    [第 3 章](ch03.html#Chapter_3) 中探索这种降维方法，其将 28 个原始特征精简为主成分。
- en: In addition to the 28 principal components, we have three other variables—the
    time of the transaction, the amount of the transaction, and the true class of
    the transaction (one if fraud, zero if genuine).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 28 个主成分外，我们还有三个其他变量——交易时间、交易金额以及交易的真实类别（如果是欺诈则为一，否则为零）。
- en: Data Preparation
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: Before we can use machine learning to train on the data and develop a fraud
    detection solution, we need to prepare the data for the algorithms.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在可以使用机器学习训练数据并开发欺诈检测解决方案之前，我们需要为算法准备数据。
- en: Data Acquisition
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据采集
- en: The first step in any machine learning project is data acquisition.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 任何机器学习项目的第一步是数据采集。
- en: Download the data
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 下载数据
- en: Download the dataset and, within the *handson-unsupervised-learning* directory,
    place the CSV file in a folder called */datasets/credit_card_data/*. If you downloaded
    the GitHub repository earlier, you already have this file in this folder in the
    repository.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 下载数据集，并在 *handson-unsupervised-learning* 目录中将 CSV 文件放置在 */datasets/credit_card_data/*
    文件夹中。如果您之前已经下载了 GitHub 仓库，则已在该仓库的此文件夹中有此文件。
- en: Import the necessary libraries
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 导入必要的库
- en: 'Import the Python libraries that we will need to build our fraud detection
    solution:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 导入我们构建欺诈检测解决方案所需的 Python 库：
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Read the data
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 读取数据
- en: '[PRE14]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Preview the data
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预览数据
- en: '[Table 2-1](#preview_of_the_data) shows the first five rows of the dataset.
    As you can see, the data has been properly loaded:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[表格 2-1](#preview_of_the_data) 显示数据集的前五行。您可以看到，数据已经正确加载：'
- en: '[PRE15]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Table 2-1\. Preview of the data
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 2-1\. 数据预览
- en: '|  | Time | V1 | V2 | V3 | V4 | V5 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | 时间 | V1 | V2 | V3 | V4 | V5 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 0 | 0.0 | –1.359807 | –0.072781 | 2.536347 | 1.378155 | –0.338321 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.0 | –1.359807 | –0.072781 | 2.536347 | 1.378155 | –0.338321 |'
- en: '| 1 | 0.0 | 1.191857 | 0.266151 | 0.166480 | 0.448154 | 0.060018 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.0 | 1.191857 | 0.266151 | 0.166480 | 0.448154 | 0.060018 |'
- en: '| 2 | 1.0 | –1.358354 | –1.340163 | 1.773209 | 0.379780 | –0.503198 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1.0 | –1.358354 | –1.340163 | 1.773209 | 0.379780 | –0.503198 |'
- en: '| 3 | 1.0 | –0.966272 | –0.185226 | 1.792993 | –0.863291 | –0.010309 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1.0 | –0.966272 | –0.185226 | 1.792993 | –0.863291 | –0.010309 |'
- en: '| 4 | 2.0 | –1.158233 | 0.877737 | 1.548718 | 0.403034 | –0.407193 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 2.0 | –1.158233 | 0.877737 | 1.548718 | 0.403034 | –0.407193 |'
- en: '| 5 rows x 31 columns |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 5 行 × 31 列 |'
- en: Data Exploration
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据探索
- en: Next, let’s get a deeper understanding of the data. We will generate summary
    statistics for the data, identify any missing values or categorical features,
    and count the number of distinct values by feature.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们深入了解数据。我们将为数据生成摘要统计信息，识别任何缺失值或分类特征，并按特征计算不同值的数量。
- en: Generate summary statistics
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成摘要统计信息
- en: '[Table 2-2](#simple_summary_statistics) describes the data, column by column.
    The block of code that follows lists all the column names for easy reference.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[表格 2-2](#simple_summary_statistics) 逐列描述数据。接下来的代码块列出了所有列名，以便参考。'
- en: '[PRE16]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Table 2-2\. Simple summary statistics
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 2-2\. 简单的摘要统计
- en: '|  | Time | V1 | V2 | V3 | V4 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | 时间 | V1 | V2 | V3 | V4 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| count | 284807.000000 | 2.848070e+05 | 2.848070e+05 | 2.848070e+05 | 2.848070e+05
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 总数 | 284807.000000 | 2.848070e+05 | 2.848070e+05 | 2.848070e+05 | 2.848070e+05
    |'
- en: '| mean | 94813.859575 | 3.919560e–15 | 5.688174e–16 | –8.769071e–15 | 2.782312e–15
    |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 均值 | 94813.859575 | 3.919560e–15 | 5.688174e–16 | –8.769071e–15 | 2.782312e–15
    |'
- en: '| std | 47488.145955 | 1.958696e+00 | 1.651309e+00 | 1.516255e+00 | 1.415869e+00
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 标准差 | 47488.145955 | 1.958696e+00 | 1.651309e+00 | 1.516255e+00 | 1.415869e+00
    |'
- en: '| min | 0.000000 | –5.640751e+01 | –7.271573e+01 | –4.832559e+01 | –5.683171e+00
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 最小值 | 0.000000 | –5.640751e+01 | –7.271573e+01 | –4.832559e+01 | –5.683171e+00
    |'
- en: '| 25% | 54201.500000 | –9.203734e–01 | –5.985499e–01 | –8.903648e–01 | –8.486401e–01
    |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 25% | 54201.500000 | –9.203734e–01 | –5.985499e–01 | –8.903648e–01 | –8.486401e–01
    |'
- en: '| 50% | 84692.000000 | 1.810880e–02 | 6.548556e–02 | 1.798463e–01 | –1.984653e–02
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 50% | 84692.000000 | 1.810880e–02 | 6.548556e–02 | 1.798463e–01 | –1.984653e–02
    |'
- en: '| 75% | 139320.500000 | 1.315642e+00 | 8.037239e–01 | 1.027196e+00 | 7.433413e–01
    |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 75% | 139320.500000 | 1.315642e+00 | 8.037239e–01 | 1.027196e+00 | 7.433413e–01
    |'
- en: '| max | 172792.000000 | 2.454930e+00 | 2.205773e+01 | 9.382558e+00 | 1.687534e+01
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 最大值 | 172792.000000 | 2.454930e+00 | 2.205773e+01 | 9.382558e+00 | 1.687534e+01
    |'
- en: '| 8 rows x 31 columns |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 31列 x 8行 |'
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The total number of positive labels, or fraudulent transactions, is 492\. There
    are 284,807 instances and 31 columns as expected—28 numerical features (V1 through
    V28), Time, Amount, and Class.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 正标签的总数，或欺诈交易，为492。如预期，共有284,807个实例和31列——28个数值特征（V1至V28），时间，金额和类别。
- en: The timestamps range from 0 to 172,792, the amounts range from 0 to 25,691.16,
    and there are 492 fraudulent transactions. These fraudulent transactions are also
    referred to as positive cases or positive labels (labeled as one); the normal
    transactions are negative cases or negative labels (labeled as zero).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 时间戳范围从0到172,792，金额范围从0到25,691.16，有492笔欺诈交易。这些欺诈交易也称为正案例或正标签（标记为一）；正常交易称为负案例或负标签（标记为零）。
- en: The 28 numerical features are not standardized yet, but we will standardize
    the data soon. *Standardization* rescales the data to have a mean of zero and
    standard deviation of one.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这28个数值特征目前尚未标准化，但我们很快将对数据进行标准化。*标准化*会将数据重新缩放，使其均值为零，标准差为一。
- en: Tip
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Some machine learning solutions are very sensitive to the scale of the data,
    so having all the data on the same relative scale—via standardization—is a good
    machine learning practice.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一些机器学习解决方案对数据的规模非常敏感，因此通过标准化使所有数据在相同的相对比例上具有良好的机器学习实践。
- en: Another common method to scale data is *normalization*, which rescales the data
    to a zero to one range. Unlike the standardized data, all the normalized data
    is on a positive scale.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的数据缩放方法是*归一化*，它将数据重新缩放到零到一的范围内。与标准化数据不同，所有归一化数据都在正数范围内。
- en: Identify nonnumerical values by feature
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过特征识别非数字值
- en: Some machine learning algorithms cannot handle nonnumerical values or missing
    values. Therefore, it is best practice to identify nonnumerical values (also known
    as *not a number*, or *NaNs*).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一些机器学习算法无法处理非数字值或缺失值。因此，最佳实践是识别非数字值（也称为*非数字*或*NaN*）。
- en: In the case of missing values, we can impute the value—for example, by replacing
    the missing points with the mean, median, or mode of the feature—or substitute
    with some user-defined value. In the case of categorical values, we can encode
    the data such that all the categorical values are represented with a sparse matrix.
    This sparse matrix is then combined with the numerical features. The machine learning
    algorithm trains on this combined feature set.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在缺失值的情况下，我们可以填充值——例如，用特征的平均值、中位数或众数替换缺失点——或用某个用户定义的值替换。对于分类值，我们可以对数据进行编码，以便所有分类值都用稀疏矩阵表示。然后，这个稀疏矩阵与数值特征结合。机器学习算法基于这个组合特征集进行训练。
- en: 'The following code shows that none of the observations have NaNs, so we will
    not need to impute or encode any of the values:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示，观察中没有NaN值，因此我们不需要填充或编码任何值：
- en: '[PRE20]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Identify distinct values by feature
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过特征识别不同的值
- en: To develop a better understanding of the credit card transactions dataset, let’s
    count the number of distinct values by feature.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解信用卡交易数据集，让我们按特征计算不同值的数量。
- en: The following code shows that we have 124,592 distinct timestamps. But we know
    from earlier that we have 284,807 observations in total. That means that there
    are multiple transactions at some timestamps.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示，我们有124,592个不同的时间戳。但是我们从之前知道总共有284,807个观测值。这意味着某些时间戳上有多次交易。
- en: 'And, as expected, there are just two classes—one for fraud, zero for not fraud:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 不出所料，只有两类——一类是欺诈，零类是非欺诈：
- en: '[PRE22]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Generate Feature Matrix and Labels Array
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成特征矩阵和标签数组
- en: Let’s create and standardize the feature matrix X and isolate the labels array
    y (one for fraud, zero for not fraud). Later on we will feed these into the machine
    learning algorithms during training.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建并标准化特征矩阵X，并分离标签数组y（欺诈为一，非欺诈为零）。稍后在训练期间，我们将把它们输入到机器学习算法中。
- en: Create the feature matrix X and the labels array Y
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建特征矩阵X和标签数组Y
- en: '[PRE24]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Standardize the feature matrix X
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标准化特征矩阵X
- en: 'Let’s rescale the feature matrix so that each feature, except for time, has
    a mean of zero and standard deviation of one:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新缩放特征矩阵，使得每个特征（时间除外）的均值为零，标准差为一：
- en: '[PRE25]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As shown in [Table 2-3](#summary_of_scaled_features), the standardized features
    now have a mean of zero and a standard deviation of one.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 正如[Table 2-3](#summary_of_scaled_features)所示，标准化后的特征现在均值为零，标准差为一。
- en: Table 2-3\. Summary of scaled features
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Table 2-3\. Summary of scaled features
- en: '|  | Time | V1 | V2 | V3 | V4 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | Time | V1 | V2 | V3 | V4 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| count | 284807.000000 | 2.848070e+05 | 2.848070e+05 | 2.848070e+05 | 2.848070e+05
    |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| count | 284807.000000 | 2.848070e+05 | 2.848070e+05 | 2.848070e+05 | 2.848070e+05
    |'
- en: '| mean | 94813.859575 | –8.157366e–16 | 3.154853e–17 | –4.409878e–15 | –6.734811e–16
    |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| mean | 94813.859575 | –8.157366e–16 | 3.154853e–17 | –4.409878e–15 | –6.734811e–16
    |'
- en: '| std | 47488.145955 | 1.000002e+00 | 1.000002e+00 | 1.000002e+00 | 1.000002e+00
    |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| std | 47488.145955 | 1.000002e+00 | 1.000002e+00 | 1.000002e+00 | 1.000002e+00
    |'
- en: '| min | 0.000000 | –2.879855e+01 | –4.403529e+01 | –3.187173e+01 | –4.013919e+00
    |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| min | 0.000000 | –2.879855e+01 | –4.403529e+01 | –3.187173e+01 | –4.013919e+00
    |'
- en: '| 25% | 54201.500000 | –4.698918e–01 | –3.624707e–01 | –5.872142e–01 | –5.993788e–01
    |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 25% | 54201.500000 | –4.698918e–01 | –3.624707e–01 | –5.872142e–01 | –5.993788e–01
    |'
- en: '| 50% | 84692.000000 | 9.245351e–03 | 3.965683e–02 | 1.186124e–02 | –1.401724e–01
    |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 50% | 84692.000000 | 9.245351e–03 | 3.965683e–02 | 1.186124e–02 | –1.401724e–01
    |'
- en: '| 75% | 139320.500000 | 6.716939e–01 | 4.867202e–01 | 6.774569e–01 | 5.250082e–01
    |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 75% | 139320.500000 | 6.716939e–01 | 4.867202e–01 | 6.774569e–01 | 5.250082e–01
    |'
- en: '| max | 172792.000000 | 1.253351e+00 | 1.335775e+01 | 6.187993e+00 | 1.191874e+01
    |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| max | 172792.000000 | 1.253351e+00 | 1.335775e+01 | 6.187993e+00 | 1.191874e+01
    |'
- en: '| 8 rows x 30 columns |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 8 rows x 30 columns |'
- en: Feature Engineering and Feature Selection
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Feature Engineering and Feature Selection
- en: In most machine learning projects, we should consider *feature engineering*
    and *feature selection* as part of the solution. Feature engineering involves
    creating new features—for example, calculating ratios or counts or sums from the
    original features—to help the machine learning algorithm extract a stronger signal
    from the dataset.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数机器学习项目中，我们应该将*特征工程*和*特征选择*视为解决方案的一部分。特征工程涉及创建新特征，例如从原始特征计算比率、计数或总和，以帮助机器学习算法从数据集中提取更强的信号。
- en: Feature selection involves selecting a subset of the features for training,
    effectively removing some of the less relevant features from consideration. This
    may help prevent the machine learning algorithm from overfitting to the noise
    in the dataset.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择涉及选择用于训练的特征子集，有效地从考虑中移除一些不太相关的特征。这有助于防止机器学习算法过度拟合数据集中的噪声。
- en: For this credit card fraud dataset, we do not have the original features. We
    have only the principal components, which were derived from PCA, a form of dimensionality
    reduction that we will explore in [Chapter 3](ch03.html#Chapter_3). Since we do
    not know what any of the features represent, we cannot perform any intelligent
    feature engineering.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个信用卡欺诈数据集，我们没有原始特征。我们只有从PCA中得出的主成分，PCA是一种我们将在[第3章](ch03.html#Chapter_3)中探讨的降维形式。由于我们不知道任何特征代表什么，我们无法进行任何智能特征工程。
- en: Feature selection is not necessary either since the number of observations (284,807)
    vastly outnumbers the number of features (30), which dramatically reduces the
    chances of overfitting. And, as [Figure 2-1](#correlation_matrix) shows, the features
    are only slightly correlated to each other. In other words, we do not have redundant
    features. If we did, we could remove or reduce the redundancy via dimensionality
    reduction. Of course, this is not a surprise. PCA was already performed on this
    credit card dataset, removing the redundancy for us.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 由于观测值（284,807）远远超过特征数（30），因此特征选择也是不必要的，这显著降低了过拟合的可能性。而且，正如[Figure 2-1](#correlation_matrix)所示，特征之间的相关性只是轻微的。换句话说，我们没有冗余特征。如果有的话，我们可以通过降维来消除或减少冗余。当然，这并不奇怪。PCA已经在这个信用卡数据集上执行过了，为我们消除了冗余。
- en: Check correlation of features
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查特征之间的相关性
- en: '[PRE26]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![Correlation matrix](assets/hulp_0201.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![Correlation matrix](assets/hulp_0201.png)'
- en: Figure 2-1\. Correlation matrix
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 2-1\. Correlation matrix
- en: Data Visualization
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Data Visualization
- en: 'As a final step, let’s visualize the data to appreciate just how imbalanced
    the dataset is ([Figure 2-2](#frequency_percentage_of_labels)). Since there are
    so few cases of fraud to learn from, this is a difficult problem to solve; fortunately,
    we have labels for the entire dataset:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步，让我们来可视化数据，以了解数据集的不平衡程度([Figure 2-2](#frequency_percentage_of_labels))。由于欺诈案例很少，这是一个难题；幸运的是，我们有整个数据集的标签：
- en: '[PRE27]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![Frequency percentage of labels](assets/hulp_0202.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![Frequency percentage of labels](assets/hulp_0202.png)'
- en: Figure 2-2\. Frequency percentage of labels
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 2-2\. Frequency percentage of labels
- en: Model Preparation
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Model Preparation
- en: Now that the data is ready, let’s prepare for the model. We need to split the
    data into a training and a test set, select a cost function, and prepare for *k*-fold
    cross-validation.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据准备好了，让我们为模型做准备。我们需要将数据分割为训练集和测试集，选择成本函数，并为*k*折交叉验证做准备。
- en: Split into Training and Test Sets
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据集分割为训练集和测试集
- en: As you may recall from [Chapter 1](ch01.html#Chapter_1), machine learning algorithms
    learn from data (i.e., train on the data) to have good performance (i.e., accurately
    predict) on never-before-seen cases. The performance on these never-before-seen
    cases is known as the generalization error—this is the most important metric in
    determining the goodness of a machine learning model.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可能从[第 1 章](ch01.html#Chapter_1)中回忆起的，机器学习算法从数据中学习（即在数据上进行训练），以在以前未见过的案例上表现良好（即准确预测）。在这些以前未见过的案例上的表现被称为泛化误差——这是确定机器学习模型好坏的最重要指标。
- en: We need to set up our machine learning project so that we have a training set
    from which the machine learning algorithm learns. We also need a test set (the
    never-before-seen cases) the machine learning algorithm can make predictions on.
    The performance on this test set will be the ultimate gauge of success.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要设置我们的机器学习项目，以便从中学习的机器学习算法具有训练集。我们还需要一个测试集（以前未见过的案例），机器学习算法可以对其进行预测。这个测试集上的性能将是成功的最终标准。
- en: Let’s go ahead and split our credit card transactions dataset into a training
    set and a test set.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续将我们的信用卡交易数据集分割为训练集和测试集。
- en: '[PRE28]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We now have a training set with 190,280 instances (67% of the original dataset)
    and a test set with 93,987 instances (the remaining 33%). To preserve the percentage
    of fraud (~0.17%) for both the training and the test set, we have set the stratify
    parameter. We also fixed the random state to 2018 to make it easier to reproduce
    results.^([4](ch02.html#idm140637556238384))
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个包含190,280个实例的训练集（原始数据集的67%）和一个包含93,987个实例的测试集（剩余的33%）。为了保持训练集和测试集中欺诈比例（约0.17%）的一致性，我们设置了分层参数。我们还将随机状态设置为2018，以便更容易地重现结果。^([4](ch02.html#idm140637556238384))
- en: We will use the test set for a final evaluation of our generalization error
    (also known as out-of-sample error).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用测试集来最终评估我们的泛化误差（也称为样本外误差）。
- en: Select Cost Function
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择成本函数
- en: Before we train on the training set, we need a cost function (also referred
    to as the error rate or value function) to pass into the machine learning algorithm.
    The machine learning algorithm will try to minimize this cost function by learning
    from the training examples.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对训练集进行训练之前，我们需要一个成本函数（也称为错误率或值函数），将其传递给机器学习算法。机器学习算法将尝试通过从训练示例中学习来最小化这个成本函数。
- en: Since this is a supervised classification problem—with two classes—let’s use
    *binary classification log loss* (as shown in [Equation 2-1](#log_loss_function)),
    which will calculate the cross-entropy between the true labels and the model-based
    predictions.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个监督分类问题——有两个类别——让我们使用*二元分类对数损失*（如[方程式 2-1](#log_loss_function)所示），它将计算真实标签与基于模型的预测之间的交叉熵。
- en: Equation 2-1\. Log loss function
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 2-1\. 对数损失函数
- en: <math class="center" display="block"><mi>log loss</mi><mo>=</mo> <mrow><mo>–</mo>
    <mfrac><mn>1</mn><mi>N</mi></mfrac></mrow> <munderover><mo mathsize="200%">Σ</mo>
    <mrow><mi>i</mi><mo>=</mo><mi>1</mi></mrow> <mi>N</mi></munderover> <munderover><mo
    mathsize="200%">Σ</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>M</mi></munderover>
    <msub><mi>y</mi> <mrow><mi>i</mi><mtext>,</mtext><mi>j</mi></mrow></msub> <mi>log</mi>
    <mo stretchy="false">(</mo> <msub><mi>p</mi> <mrow><mi>i</mi><mtext>,</mtext><mi>j</mi></mrow></msub>
    <mo stretchy="false">)</mo></math>
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: <math class="center" display="block"><mi>log loss</mi><mo>=</mo> <mrow><mo>–</mo>
    <mfrac><mn>1</mn><mi>N</mi></mfrac></mrow> <munderover><mo mathsize="200%">Σ</mo>
    <mrow><mi>i</mi><mo>=</mo><mi>1</mi></mrow> <mi>N</mi></munderover> <munderover><mo
    mathsize="200%">Σ</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>M</mi></munderover>
    <msub><mi>y</mi> <mrow><mi>i</mi><mtext>,</mtext><mi>j</mi></mrow></msub> <mi>log</mi>
    <mo stretchy="false">(</mo> <msub><mi>p</mi> <mrow><mi>i</mi><mtext>,</mtext><mi>j</mi></mrow></msub>
    <mo stretchy="false">)</mo></math>
- en: Where *N* is the number of observations; *M* is the number of class labels (in
    this case, two); log is the natural logarithm; [*yi,j*] is 1 if observation *i*
    is in class *j* and 0 otherwise; and [*pi,j*] is the predicted probability that
    observation *i* is in class *j*.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*N*是观察数；*M*是类别标签数（在本例中为两个）；log 是自然对数；[*yi,j*] 如果观察*i*属于类别*j*则为1，否则为0；[*pi,j*]
    是观察*i*属于类别*j*的预测概率。
- en: The machine learning model will generate the fraud probability for each credit
    card transaction. The closer the fraud probabilities are to the true labels (i.e.,
    one for fraud or zero for not fraud), the lower the value of the log loss function.
    This is what the machine learning algorithm will try to minimize.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型将为每笔信用卡交易生成欺诈概率。欺诈概率越接近真实标签（即欺诈为1或非欺诈为0），对数损失函数的值越低。这是机器学习算法将尝试最小化的目标。
- en: Create k-Fold Cross-Validation Sets
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建k折交叉验证集
- en: To help the machine learning algorithm estimate what its performance will be
    on the never-before-seen examples (the test set), it is best practice to further
    split the training set into a training set and a validation set.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助机器学习算法估计其在以前未见过的示例（测试集）上的性能，最佳做法是进一步将训练集分割为训练集和验证集。
- en: For example, if we split the training set into fifths, we can train on four-fifths
    of the original training set and evalulate the newly training model by making
    predictions on the fifth slice of the original training set, known as the validation
    set.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们将训练集分为五分之一，我们可以在原始训练集的四分之一上进行训练，并通过对原始训练集的第五个切片进行预测来评估新的训练模型，称为验证集。
- en: It is possible to train and evaluate like this five times—leaving aside a different
    fifth slice as the validation set each time. This is known as *k-fold cross-validation*,
    where *k* in this case is five. With this approach, we will have not one estimate
    but five estimates for the generalization error.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 可以像这样训练和评估五次——每次留出一个不同的五分之一作为验证集。这被称为*k*折交叉验证，其中*k*在本例中为五。通过这种方法，我们将不是一个估计值，而是五个泛化误差的估计值。
- en: We will store the training score and the cross-validation score for each of
    the five runs, and we will store the cross-validation predictions each time. After
    all five runs are complete, we will have cross-validation predictions for the
    entire dataset. This will be the best all-in estimate of the performance the test
    set.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为五次运行中的每一次存储训练得分和交叉验证得分，并且我们将每次存储交叉验证预测。在所有五次运行完成后，我们将对整个数据集进行交叉验证预测。这将是测试集性能的最佳整体估计。
- en: 'Here’s how to set up for the *k*-fold validation, where *k* is five:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何为*k*折验证设置，其中*k*为五：
- en: '[PRE29]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Machine Learning Models (Part I)
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习模型（第一部分）
- en: Now we’re ready to build the machine learning models. For each machine algorithm
    we consider, we will set hyperparameters, train the model, and evaluate the results.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备构建机器学习模型。对于我们考虑的每个机器算法，我们将设置超参数，训练模型，并评估结果。
- en: 'Model #1: Logistic Regression'
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '模型 #1：逻辑回归'
- en: Let’s start with the most basic classification algorithm, logistic regression.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从最基本的分类算法开始，逻辑回归。
- en: Set hyperparameters
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置超参数
- en: '[PRE30]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We will set the penalty to the default value L2 instead of L1\. Compared to
    L1, L2 is less sensitive to outliers and will assign nonzero weights to nearly
    all the features, resulting in a stable solution. L1 will assign high weights
    to the most important features and near-zero weights to the rest, essentially
    performing feature selection as the algorithm trains. However, because the weights
    vary so much feature to feature, the L1 solution is not as stable to changes in
    data points as the L2 solution.^([5](ch02.html#idm140637556129056))
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把惩罚设置为默认值L2而不是L1。与L1相比，L2对异常值不太敏感，并且将为几乎所有特征分配非零权重，从而产生一个稳定的解决方案。L1将为最重要的特征分配高权重，并为其余特征分配接近零的权重，实际上在算法训练时执行特征选择。然而，由于权重在特征之间变化很大，所以L1解决方案对数据点的变化不如L2解决方案稳定。^([5](ch02.html#idm140637556129056))
- en: C is the regularization strength. As you may recall from [Chapter 1](ch01.html#Chapter_1),
    regularization helps address overfitting by penalizing complexity. In other words,
    the stronger the regularization, the greater the penalty the machine learning
    algorithm applies to complexity. Regularization nudges the machine learning algorithm
    to prefer simpler models to more complex ones, all else equal.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: C是正则化强度。如您可能还记得的来自[第1章](ch01.html#Chapter_1)，正则化通过惩罚复杂性来帮助解决过拟合问题。换句话说，正则化越强，机器学习算法对复杂性的惩罚就越大。正则化促使机器学习算法更喜欢简单的模型而不是更复杂的模型，其他条件相等。
- en: This regularization constant, C, must be a positive floating number. The smaller
    the value, the stronger the regularization. We will keep the default 1.0.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这个正则化常数C必须是一个正浮点数。数值越小，正则化越强。我们将保持默认值1.0。
- en: Our credit card transactions dataset is very imbalanced—out of all the 284,807
    cases, only 492 are fraudulent. As the machine learning algorithm trains, we want
    the algorithm to focus more attention on learning from the positive labeled transactions—in
    other words, the fraudulent transactions—because there are so few of them in the
    dataset.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的信用卡交易数据集非常不平衡——在所有的284,807个案例中，只有492个是欺诈性的。随着机器学习算法的训练，我们希望算法更多地关注学习来自正标记交易的情况，换句话说，就是欺诈交易，因为在数据集中这样的交易很少。
- en: For this logistic regression model, we will set the `class_weight` to balanced.
    This signals to the logistic regression algorithm that we have an imbalanced class
    problem; the algorithm will need to weigh the positive labels more heavily as
    it trains. In this case, the weights will be inversely proportional to the class
    frequencies; the algorithm will assign higher weights to the rare positive labels
    (i.e., fraud) and lower weights to the more frequent negative labels (i.e., not
    fraud).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个逻辑回归模型，我们将设置`class_weight`为平衡。这向逻辑回归算法表示我们有一个类别不平衡的问题；算法在训练时将需要更重视正标签。在这种情况下，权重将与类别频率成反比；算法将给罕见的正标签（即欺诈）分配更高的权重，给更常见的负标签（即非欺诈）分配较低的权重。
- en: The random state is fixed to 2018 to help others—such as you, the reader—reproduce
    results. We will keep the default solver liblinear.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 随机状态固定为2018，以帮助其他人——例如你，读者——复现结果。我们将保持默认的solver liblinear。
- en: Train the model
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练模型
- en: Now that the hyperparameters are set, we will train the logistic regression
    model on each of the five *k*-fold cross-validation splits, training on four-fifths
    of the training set and evaulating the performance on the fifth slice that is
    held aside.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在超参数已经设定好，我们将在每个五折交叉验证分割上训练逻辑回归模型，用训练集的四分之四来训练，并在留置的第五切片上评估性能。
- en: 'As we train and evaluate like this five times, we will calculate the cost function—log
    loss for our credit card transactions problem—for the training (i.e., the four-fifths
    slice of the original training set) and for the validation (i.e., the one-fifth
    slice of the original training set). We will also store the predictions for each
    of the five cross-validation sets; by the end of the fifth run, we will have predictions
    for the entire training set:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们像这样训练和评估五次后，我们将计算成本函数——我们信用卡交易问题的对数损失——对训练集（即原始训练集的五分之四切片）和验证集（即原始训练集的五分之一切片）。我们还将存储每个五折交叉验证集的预测；到第五次运行结束时，我们将得到整个训练集的预测：
- en: '[PRE31]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Evaluate the results
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估结果
- en: The training log loss and cross-validation log loss are shown for each of the
    five runs in the following code. Generally (but not always) the training log loss
    will be lower than the cross-validation log loss. Because the machine learning
    algorithm has learned directly from the training data, its performance (i.e.,
    log loss) should be better on the training set than on the cross-validation set.
    Remember, the cross-validation set has the transactions that were explicitly held
    out from the training exercise.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码显示了五次运行中每次的训练对数损失和交叉验证对数损失。一般来说（但不总是），训练对数损失会低于交叉验证对数损失。因为机器学习算法直接从训练数据中学习，所以它在训练集上的表现（即对数损失）应该比在交叉验证集上好。请记住，交叉验证集包含了在训练过程中明确保留的交易。
- en: '[PRE32]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Note
  id: totrans-195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: For our credit card transactions dataset, it is important to keep in mind that
    we are building a fraud detection solution. When we refer to the *performance*
    of the machine learning model, we mean how good the model is at predicting fraud
    among the transactions in the dataset.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的信用卡交易数据集，重要的是要记住我们正在构建一个欺诈检测解决方案。当我们提到机器学习模型的*性能*时，我们指的是模型在数据集中预测欺诈的能力有多好。
- en: The machine learning model outputs a prediction probability for each transaction,
    where one is fraud and zero is not fraud. The closer the probability is to one,
    the more likely the transaction is fraudulent; the closer the probability is to
    zero, the more likely the transaction is normal. By comparing the model’s probabilities
    with the true labels, we can assess the goodness of the model.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型为每笔交易输出一个预测概率，其中1表示欺诈，0表示非欺诈。预测概率越接近1，交易越可能是欺诈；越接近0，交易越可能是正常的。通过将模型的预测概率与真实标签进行比较，我们可以评估模型的好坏。
- en: For each of the five runs, their training and cross-validation log losses are
    similar. The logistic regression model does not exhibit severe overfitting; if
    it did, we would have a low training log loss and comparably high cross-validation
    log loss.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 对于五次运行中的每一次，它们的训练和交叉验证对数损失是相似的。逻辑回归模型没有表现出严重的过拟合；如果有的话，我们将会看到低的训练对数损失和相对较高的交叉验证对数损失。
- en: 'Since we stored the predictions for each of the five cross-validation sets,
    we can combine the predictions into a single set. This single set is the same
    as the original training set, and we can now calculate the overall log loss for
    this entire training set. This is the best estimate for the logistic regression
    model’s log loss on the test set:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们存储了每个五折交叉验证集的预测结果，我们可以将这些预测合并成一个单一集合。这个单一集合与原始训练集相同，现在我们可以计算整个训练集的总体对数损失。这是对测试集上逻辑回归模型对数损失的最佳估计：
- en: '[PRE33]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Evaluation Metrics
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估指标
- en: Although the log loss is a great way to estimate the performance of the machine
    learning model, we may want a more intuitive way to understand the results. For
    example, of the fraudulent transactions in the training set, how many did we catch?
    This is known as the *recall*. Or, the transactions that were flagged as fraudulent
    by the logistic regression model, how many were truly fraudulent? This is known
    as the *precision* of the model.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然对数损失是评估机器学习模型性能的好方法，但我们可能希望有更直观的方法来理解结果。例如，在训练集中的欺诈交易中，我们捕获了多少个？这就是*召回率*。或者，逻辑回归模型标记为欺诈交易的交易中，有多少是真正的欺诈交易？这就是模型的*精确率*。
- en: Let’s take a look at these and other similar evaluation metrics to help us more
    intuitively grasp the results.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看这些及其他类似的评估指标，以帮助我们更直观地理解结果。
- en: Note
  id: totrans-204
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: These evaluation metrics are very important because they empower data scientists
    to intuitively explain results to business people, who may be less familiar with
    log loss, cross-entropy, and other cost functions. The ability to convey complex
    results as simply as possible to nondata scientists is one of the essential skills
    for applied data scientists to master.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这些评估指标非常重要，因为它们使数据科学家能够直观地向不熟悉对数损失、交叉熵和其他成本函数的业务人员解释结果。将复杂结果尽可能简单地传达给非数据科学家是应用数据科学家必须掌握的基本技能之一。
- en: Confusion Matrix
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: In a typical classification problem (without class imbalance) we can evaluate
    the results using a confusion matrix, which is a table that summarizes the number
    of true positives, true negatives, false positives, and false negatives ([Figure 2-3](#confusion_matrix)).^([6](ch02.html#idm140637555940304))
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的分类问题（没有类别不平衡情况）中，我们可以使用混淆矩阵来评估结果，它是一个总结真正例、真负例、假正例和假负例数量的表格（[图 2-3](#confusion_matrix)）。
- en: '![Confusion matrix](assets/hulp_0203.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![混淆矩阵](assets/hulp_0203.png)'
- en: Figure 2-3\. Confusion matrix
  id: totrans-209
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-3\. 混淆矩阵
- en: Given that our credit card transactions dataset is highly imbalanced, using
    the confusion matrix would be meaningful. For example, if we predict that every
    transaction is not fraudulent, we would have 284,315 true negatives, 492 false
    negatives, zero true positives, and zero false positives. We would have a 0% accuracy
    in identifying the truly fraudulent transactions. The confusion matrix does a
    poor job of capturing this suboptimal outcome given this imbalanced class problem.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们的信用卡交易数据集类别高度不平衡，使用混淆矩阵将是有意义的。例如，如果我们预测每笔交易都不是欺诈交易，我们将得到 284,315 个真负例，492
    个假负例，零个真正例和零个假正例。我们在识别真正欺诈交易方面的准确率为 0%。在这种类别不平衡问题下，混淆矩阵未能有效捕捉到这种次优结果。
- en: For problems involving more balanced classes (i.e., the number of true positives
    is roughly similar to the number of true negatives), the confusion matrix may
    be a good, straightforward evaluation metric. We need to find a more appropriate
    evaluation metric given our imbalanced dataset.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 对于涉及更平衡类别的问题（即真正例数量大致与真负例数量相似），混淆矩阵可能是一个好的、直接的评估指标。考虑到我们的不平衡数据集，我们需要找到一个更合适的评估指标。
- en: Precision-Recall Curve
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 精确率-召回率曲线
- en: For our imbalanced credit card transactions dataset, a better way to evaluate
    the results is to use precision and recall. *Precision* is the number of true
    positives over the number of total positive predictions. In other words, how many
    of the fraudulent transactions does the model catch?
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的不平衡信用卡交易数据集，评估结果的更好方法是使用精确率和召回率。*精确率*是真正例的数量除以总的正例预测数量。换句话说，模型捕获了多少个欺诈交易？
- en: <math><mi>Precision</mi> <mo>=</mo> <mi>True Positives</mi> <mo>∕</mo> <mo>(</mo><mi>True
    Positives</mi> <mo>+</mo> <mi>False Positives</mi><mo>)</mo></math>
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mi>精确率</mi> <mo>=</mo> <mfrac><mi>真正例</mi> <mrow><mo>+</mo> <mi>假正例</mi></mrow></mfrac></math>
- en: A high precision means that—of all our positive predictions—many are true positives
    (in other words, it has a low false positive rate).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 高精度意味着——在所有我们的正面预测中——许多是真正例（换句话说，它具有较低的假阳性率）。
- en: '*Recall* is the number of true positives over the number of total actual positives
    in the dataset. In other words how many of the fraudulent transactions does the
    model catch?^([7](ch02.html#idm140637555924656))'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '*召回率*是数据集中实际正例的数量中捕捉到的欺诈交易数量。换句话说，模型捕捉了多少欺诈交易？^([7](ch02.html#idm140637555924656))'
- en: <math><mi>Recall</mi> <mo>=</mo> <mi>True Positives</mi> <mo>∕</mo> <mo>(</mo><mi>True
    Positives</mi> <mo>+</mo> <mi>False Positives</mi><mo>)</mo></math>
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mi>召回率</mi> <mo>=</mo> <mi>真正例</mi> <mo>∕</mo> <mo>(</mo><mi>真正例</mi>
    <mo>+</mo> <mi>假正例</mi><mo>)</mo></math>
- en: A high recall means that the model has captured most of the true positives (in
    other words, it has a low false negative rate).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 高召回率意味着模型捕捉到了大部分真正例（换句话说，它具有较低的假阴性率）。
- en: A solution with high recall but low precision returns many results—capturing
    many of the positives—but with many false alarms. A solution with high precision
    but low recall is the exact opposite; it returns few results—capturing a fraction
    of all the positives in the dataset—but most of its predictions are correct.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 高召回率但低精度的解决方案返回许多结果——捕捉到许多正例，但也有许多误报。高精度但低召回率的解决方案则恰恰相反；返回很少的结果——捕捉到数据集中所有正例的一部分，但其大多数预测是正确的。
- en: To put this into context, if our solution had high precision but low recall,
    there would be a very small number of fraudulent transactions found but most would
    be truly fraudulent.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的解决方案精度高但召回率低，那么找到的欺诈交易数量很少，但大多数确实是欺诈交易。
- en: However, if the solution had low precision but high recall it would flag many
    of the transactions as fraudulent, thus catching a lot of the fraud, but most
    of the flagged transactions would not be fraudulent.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果解决方案精度低但召回率高，则会标记许多交易为欺诈，从而捕获大部分欺诈行为，但被标记的交易中大多数并非欺诈。
- en: Obviously, both solutions have major problems. In the high precision–low recall
    case, the credit card company would lose a lot of money due to fraud, but it would
    not antagonize customers by unnecessarily rejecting transactions. In the low precision-high
    recall case, the credit card company would catch a lot of the fraud, but it would
    most certainly anger customers by unnecessarily rejecting a lot of normal, non-fraudulent
    transactions.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，两种解决方案都存在重大问题。在高精度-低召回率的情况下，信用卡公司会因为欺诈而损失很多钱，但不会因不必要地拒绝交易而激怒客户。在低精度-高召回率的情况下，信用卡公司会捕捉到很多欺诈行为，但肯定会因不必要地拒绝大量正常非欺诈交易而惹怒客户。
- en: An optimal solution needs to have high precision and high recall, rejecting
    only those transactions that are truly fraudulent (i.e., high precision) and catching
    most of the fraudulent cases in the dataset (high recall).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳解决方案需要具有高精度和高召回率，仅拒绝那些真正欺诈的交易（即高精度），并捕捉数据集中大部分的欺诈案例（高召回率）。
- en: There is generally a trade-off between precision and recall, which is usually
    determined by the threshold set by the algorithm to separate the positive cases
    from the negative cases; in our example, positive is fraud and negative is not
    fraud. If the threshold is set too high, very few cases are predicted as positive,
    resulting in high precision but low recall. As the threshold is lowered, more
    cases are predicted as positive, generally decreasing the precision and increasing
    the recall.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 精度和召回率通常存在折衷，通常由算法设置的阈值决定，以将正例与负例分开；在我们的例子中，正例是欺诈，负例是非欺诈。如果阈值设置得太高，预测为正例的案例很少，导致高精度但低召回率。随着阈值的降低，预测为正例的案例增加，通常降低精度并增加召回率。
- en: For our credit card transactions dataset, think of the threshold as the sensitivity
    of the machine learning model in rejecting transactions. If the threshold is too
    high/strict, the model will reject few transactions, but the ones it does reject
    will be very likely to be fraudulent.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的信用卡交易数据集来说，可以把阈值看作是机器学习模型在拒绝交易方面的敏感性。如果阈值过高/严格，模型会拒绝很少的交易，但被拒绝的交易很可能是欺诈的。
- en: As the threshold moves lower (i.e., becomes less strict), the model will reject
    more transactions, catching more of the fraudulent cases but also unnecessarily
    rejecting more of the normal cases as well.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 阈值降低（即变得不那么严格），模型会拒绝更多交易，捕获更多的欺诈案例，但也不必要地拒绝更多正常案例。
- en: A graph of the trade-off between precision and recall is known as the precision-recall
    curve. To evaluate the precision-recall curve, we can calculate the average precision,
    which is the weighted mean of the precision achieved at each threshold. The higher
    the average precision, the better the solution.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 精确率-召回率曲线的图形展示了精确率和召回率之间的权衡。为了评估精确率-召回率曲线，我们可以计算平均精度，即在每个阈值下达到的精确率的加权平均值。平均精度越高，解决方案越好。
- en: Note
  id: totrans-228
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The choice of the threshold is a very important one and usually involves the
    input of business decision makers. Data scientists can present the precision-recall
    curve to these business decision makers to figure out where the threshold should
    be.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 阈值的选择非常重要，并且通常需要业务决策者的输入。数据科学家可以向这些业务决策者展示精确率-召回率曲线，以确定阈值应该设定在何处。
- en: For our credit card transactions dataset, the key question is how do we balance
    customer experience (i.e., avoid rejecting normal transactions) with fraud detection
    (i.e., catch the fraudulent transactions)? We cannot answer this without business
    input, but we can find the model with the best precision-recall curve. Then, we
    can present this model to business decision makers to set the appropriate threshold.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的信用卡交易数据集，关键问题是如何平衡客户体验（即避免拒绝正常交易）与欺诈检测（即捕捉到欺诈交易）？没有业务输入，我们无法回答这个问题，但我们可以找到具有最佳精确率-召回率曲线的模型。然后，我们可以将该模型呈现给业务决策者，以设定适当的阈值。
- en: Receiver Operating Characteristic
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 接收者操作特征曲线
- en: Another good evaluation metric is the area under the receiver operating characteristic
    (auROC). The receiver operating characteristic (ROC) curve plots the true positive
    rate on the Y axis and the false positive rate on the X axis. The true positive
    rate can also be referred to as the sensitivity, and the false positive rate can
    also be referred to as the 1-specificity. The closer the curve is to the top-left
    corner of the plot, the better the solution—with a value of (0.0, 1.0) as the
    absolute optimal point, signifying a 0% false positive rate and a 100% true positive
    rate.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个很好的评估指标是接收者操作特征曲线下的面积（auROC）。接收者操作特征（ROC）曲线将真阳性率绘制在Y轴上，将假阳性率绘制在X轴上。真阳性率也可以称为灵敏度，假阳性率也可以称为1-特异度。曲线越接近绘图的左上角，解决方案越好——绝对最优点的值为（0.0,
    1.0），表示假阳性率为0%，真阳性率为100%。
- en: To evaluate the solution, we can compute the area under this curve. The larger
    the auROC, the better the solution.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 要评估解决方案，我们可以计算这条曲线下的面积。auROC越大，解决方案越好。
- en: Evaluating the logistic regression model
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估逻辑回归模型
- en: Now that we understand some of the evaluation metrics used, let’s use them to
    better understand the logistic regression model’s results.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了一些使用的评估指标，让我们使用它们更好地理解逻辑回归模型的结果。
- en: 'First, let’s plot the precision-recall curve and calculate the average precision:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们绘制精确率-召回率曲线并计算平均精度：
- en: '[PRE34]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[Figure 2-4](#precision_recall_curve_of_logistic_regression) shows the plot
    of the precision-recall curve. Putting together what we discussed earlier, you
    can see that we can achieve approximately 80% recall (i.e., catch 80% of the fraudulent
    transactions) with approximately 70% precision (i.e., of the transactions the
    model flags as fraudulent, 70% are truly fraudulent while the remaining 30% were
    incorrectly flagged as fraudulent).'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-4](#precision_recall_curve_of_logistic_regression) 展示了精确率-召回率曲线的图表。综合我们之前讨论的内容，你可以看到我们可以实现大约80%的召回率（即捕获80%的欺诈交易），精确率约为70%（即模型标记为欺诈的交易中，70%确实是欺诈交易，而其余30%则错误地被标记为欺诈）。'
- en: '![Precision-recall curve of logistic regression](assets/hulp_0204.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归的精确率-召回率曲线](assets/hulp_0204.png)'
- en: Figure 2-4\. Precision-recall curve of logistic regression
  id: totrans-240
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-4\. 逻辑回归的精确率-召回率曲线
- en: We can distill this precision-recall curve into a single number by calculating
    the average precision, which is 0.73 for this logistic regression model. We cannot
    yet tell whether this is good or bad average precision yet since we have no other
    models to compare our logistic regression against.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过计算平均精度将这条精确率-召回率曲线简化为一个数字，对于这个逻辑回归模型来说，这个平均精度为0.73。目前我们还不能确定这个平均精度是好是坏，因为我们没有其他模型可以与我们的逻辑回归模型进行比较。
- en: 'Now, let’s measure the auROC:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们测量auROC：
- en: '[PRE35]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: As shown in [Figure 2-5](#area_under_the_roc_curve_of_logistic_regression),
    the auROC curve is 0.97\. This metric is just another way to evaluate the goodness
    of the logistic regression model, allowing you to determine how much of the fraud
    you can catch while keeping the false positive rate as low as possible. As with
    the average precision, we do not know whether this auROC curve of 0.97 is good
    or not, but we will once we compare it with those of other models.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [Figure 2-5](#area_under_the_roc_curve_of_logistic_regression) 所示，auROC 曲线为
    0.97\. 这个指标是评估逻辑回归模型优劣的另一种方式，它可以帮助您确定在保持尽可能低的误报率的情况下能够捕获多少欺诈。和平均精度一样，我们不知道这个 0.97
    的 auROC 曲线是好还是坏，但一旦与其他模型进行比较，我们就会知道。
- en: '![Area under the ROC curve of logistic regression](assets/hulp_0205.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归的 ROC 曲线下面积](assets/hulp_0205.png)'
- en: Figure 2-5\. auROC curve of logistic regression
  id: totrans-246
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-5\. 逻辑回归的 auROC 曲线
- en: Machine Learning Models (Part II)
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习模型（第二部分）
- en: To compare the goodness of the logistic regression model, let’s build a few
    more models using other supervised learning algorithms.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较逻辑回归模型的优劣，让我们使用其他监督学习算法构建几个更多的模型。
- en: 'Model #2: Random Forests'
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '模型 #2：随机森林'
- en: Let’s start with random forests.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从随机森林开始。
- en: As with logistic regression, we will set the hyperparameters, train the model,
    and evaluate the results using the precision-recall curve and the auROC.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 与逻辑回归一样，我们将设置超参数，训练模型，并使用精确-召回曲线和 auROC 评估结果。
- en: Set the hyperparameters
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置超参数
- en: '[PRE36]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Let’s start with the default hyperparameters. The number of estimators is set
    at 10; in other words, we will build 10 trees and average the results across these
    10 trees. For each tree, the model will consider the square root of the total
    number of features (in this case, the square root of 30 total features, which
    is 5 features, rounded down).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从默认的超参数开始。估计器的数量设置为 10；换句话说，我们将建立 10 棵树，并在这 10 棵树上的结果上取平均值。对于每棵树，模型将考虑总特征数的平方根（在本例中，总共
    30 个特征的平方根，即 5 个特征，向下取整）。
- en: By setting the `max_depth` to none, the tree will grow as deep as possible,
    splitting as much as possible given the subset of features. Similar to what we
    did for logistic regression, we set the random state to 2018 for reproducibility
    of results and class weight to balanced given our imbalanced dataset.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `max_depth` 设为 none 后，决策树会尽可能深地生长，在给定特征子集的情况下进行尽可能多的分裂。与逻辑回归相似，我们将随机状态设置为
    2018 以保证结果的可复现性，并考虑到数据集不平衡将类别权重设置为平衡。
- en: Train the model
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'We will run *k*-fold cross-validation five times, training on four-fifths of
    the training data and predicting on the fifth slice. We will store the predictions
    as we go:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将运行 *k*-折交叉验证五次，在四分之四的训练数据上进行训练，并在第五个切片上进行预测。我们将逐步存储预测结果：
- en: '[PRE37]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Evaluate the results
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估结果
- en: 'The training and cross-validation log loss results are as follows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和交叉验证的对数损失结果如下：
- en: '[PRE38]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Notice that the training log losses are considerably lower than the cross-validation
    log losses, suggesting that the random forests classifier—with the mostly default
    hyperparameters—overfits the data during the training somewhat.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，训练集的训练对数损失要远远低于交叉验证的对数损失，这表明随机森林分类器在训练过程中在某种程度上对数据进行了过度拟合，尽管使用了大多数默认的超参数。
- en: 'The following code shows the log loss over the entire training set (using cross-validation
    predictions):'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码显示了整个训练集上的对数损失（使用交叉验证预测）：
- en: '[PRE39]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Even though it overfits the training data somewhat, the random forests has a
    validation log loss that is about one-tenth that of the logistic regression—significant
    improvement over the previous machine learning solution. The random forests model
    is better at correctly flagging the fraud among credit card transactions.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它在某种程度上过度拟合了训练数据，但随机森林的验证对数损失约为逻辑回归的十分之一——相对于先前的机器学习解决方案，这是显著的改进。随机森林模型在正确标记信用卡交易中的欺诈方面表现更好。
- en: '[Figure 2-6](#precision_recall_curve_of_random_forests) shows the precision-recall
    curve of random forests. As you can see from the curve, the model can catch approximately
    80% of all the fraud with approximately 80% precision. This is more impressive
    than the approximately 80% of all the fraud the logistic regression model caught
    with 70% precision.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[Figure 2-6](#precision_recall_curve_of_random_forests) 显示了随机森林的精确-召回曲线。从曲线可以看出，该模型可以以大约
    80% 的精确度捕获大约 80% 的欺诈情况。这比逻辑回归模型以 70% 精确度捕获的大约 80% 的欺诈情况更为显著。'
- en: '![Precision-recall curve of random forests](assets/hulp_0206.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![随机森林的精确-召回曲线](assets/hulp_0206.png)'
- en: Figure 2-6\. Precision-recall curve of random fores"ts
  id: totrans-268
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-6\. 随机森林的精确-召回曲线
- en: The average precision of 0.79 of the random forests model is a clear improvement
    over the 0.73 average precision of the logistic regression model. However, the
    auROC, shown in [Figure 2-7](#area_under_the_roc_curve_of_random_forests), is
    somewhat worse—0.93 for random forests versus 0.97 for logistic regression.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林模型的平均精度为 0.79，明显优于逻辑回归模型的 0.73 平均精度。然而，随机森林的auROC，如图 [Figure 2-7](#area_under_the_roc_curve_of_random_forests)，稍微差一些，为
    0.93，而逻辑回归为 0.97。
- en: '![Area under the ROC curve of random forests](assets/hulp_0207.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![随机森林的ROC曲线下面积](assets/hulp_0207.png)'
- en: Figure 2-7\. auROC curve of random forests
  id: totrans-271
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-7\. 随机森林的auROC曲线
- en: 'Model #3: Gradient Boosting Machine (XGBoost)'
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '模型 #3：梯度提升机（XGBoost）'
- en: Now let’s train using gradient boosting and evaluate the results. There are
    two popular versions of gradient boosting—one known as XGBoost and another, much
    faster version by Microsoft called LightGBM. Let’s build a model using each one,
    starting with XGBoost.^([8](ch02.html#idm140637555072000))
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用梯度提升进行训练并评估结果。梯度提升有两个流行版本，一个是被称为XGBoost，另一个是微软快速版本LightGBM。让我们使用每个版本构建模型，首先是XGBoost。^([8](ch02.html#idm140637555072000))
- en: Set the hyperparameters
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置超参数
- en: 'We will set this up as a binary classification problem and use log loss as
    the cost function. We will set the max depth of each tree to the default six and
    a default learning rate of 0.3\. For each tree, we will use all the observations
    and all the features; these are the default settings. We will set a random state
    of 2018 to ensure the reproducibility of the results:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其设置为一个二元分类问题，并使用对数损失作为成本函数。我们将每棵树的最大深度设置为默认值六，并设置默认学习率为 0.3。对于每棵树，我们将使用所有观测值和所有特征；这些是默认设置。我们将设置随机状态为
    2018，以确保结果的可重现性：
- en: '[PRE40]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Train the model
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练模型
- en: As before, we will use *k*-fold cross-validation, training on a different four-fifths
    of the training data and predicting on the fifth slice for a total of five runs.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，我们将使用*k*-折交叉验证，在不同的四分之四的训练数据上训练，并在第五部分进行预测，总共进行五次运行。
- en: 'For each of the five runs, the gradient boosting model will train for as many
    as two thousand rounds, evaluating whether the cross-validation log loss is decreasing
    as it goes. If the cross-validation log loss stops improving (over the previous
    two hundred rounds), the training process will stop to avoid overfitting. The
    results of the training process are verbose, so we will not print them here, but
    they can be found via the [code on GitHub](http://bit.ly/2Gd4v7e):'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 对于五次运行中的每一次，梯度提升模型将训练多达两千轮，评估交叉验证的对数损失是否在进行中减少。如果交叉验证的对数损失在前两百轮停止改善，则训练过程将停止，以避免过拟合。训练过程的结果很详细，所以我们不会在这里打印它们，但可以通过
    [GitHub 上的代码](http://bit.ly/2Gd4v7e) 找到：
- en: '[PRE41]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Evaluate the results
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估结果
- en: 'As shown in the following results, the log loss over the entire training set
    (using the cross-validation predictions) is one-fifth that of the random forests
    and one-fiftieth that of logistic regression. This is a substantial improvement
    over the previous two models:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下结果所示，整个训练集上的对数损失（使用交叉验证预测）仅为随机森林的五分之一，逻辑回归的五十分之一。这是对前两个模型的显著改进：
- en: '[PRE42]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: As shown in [Figure 2-8](#precision_recall_curve_of_xgboost_gradient_boosting),
    the average precision is 0.82, just shy of that of random forests (0.79) and considerably
    better than that of logistic regression (0.73).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [Figure 2-8](#precision_recall_curve_of_xgboost_gradient_boosting) 所示，平均精度为
    0.82，略低于随机森林（0.79），但明显优于逻辑回归（0.73）。
- en: '![Precision-recall curve of XGBoost Gradient Boosting](assets/hulp_0208.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![XGBoost梯度提升的精确-召回曲线](assets/hulp_0208.png)'
- en: Figure 2-8\. Precision-recall curve of XGBoost gradient boosting
  id: totrans-286
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-8\. XGBoost梯度提升的精确-召回曲线
- en: As shown in [Figure 2-9](#area_under_the_roc_curve_of_xgboost_gradient_boosting),
    the auROC curve is 0.97, the same as that of logistic regression (0.97) and an
    improvement over random forests (0.93). So far, gradient boosting is the best
    of the three models based on the log loss, the precision-recall curve, and the
    auROC.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [Figure 2-9](#area_under_the_roc_curve_of_xgboost_gradient_boosting) 所示，auROC曲线为
    0.97，与逻辑回归相同（0.97），比随机森林（0.93）更好。到目前为止，基于对数损失、精确-召回曲线和auROC，梯度提升是三个模型中最好的。
- en: '![Area under the ROC curve of XGBoost Gradient Boosting](assets/hulp_0209.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![XGBoost梯度提升的ROC曲线下面积](assets/hulp_0209.png)'
- en: Figure 2-9\. auROC curve of XGBoost gradient boosting
  id: totrans-289
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-9\. XGBoost梯度提升的auROC曲线
- en: 'Model #4: Gradient Boosting Machine (LightGBM)'
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '模型 #4：梯度提升机（LightGBM）'
- en: Let’s now train using another version of gradient boosting known as LightGBM.^([9](ch02.html#idm140637554683376))
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用另一个名为LightGBM的梯度提升版本进行训练。^([9](ch02.html#idm140637554683376))
- en: Set the hyperparameters
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置超参数
- en: 'We will set this up as a binary classification problem and use log loss as
    the cost function. We will set the max depth of each tree to 4 and use a learning
    rate of 0.1\. For each tree, we will use all the samples and all the features;
    these are the default settings. We will use the default number of leaves for one
    tree (31) and set a random state to ensure reproducibility of the results:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其设置为二元分类问题，并使用对数损失作为成本函数。 我们将每棵树的最大深度设置为4，并使用学习率为0.1。 对于每棵树，我们将使用所有样本和所有特征；
    这些是默认设置。 我们将使用一个树的默认叶子节点数（31），并设置一个随机状态以确保结果的可重现性：
- en: '[PRE43]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Train the model
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'As before, we will use *k*-fold cross-validation and cycle through this five
    times, storing the predictions on the validation sets as we go:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，我们将使用*k*-fold交叉验证，并在这五次循环中进行存储验证集上的预测：
- en: '[PRE44]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: For each of the five runs, the gradient boosting model will train for as many
    as two thousand rounds, evaluating whether the cross-validation log loss is decreasing
    as it goes. If the cross-validation log loss stops improving (over the previous
    two hundred rounds), the training process will stop to avoid overfitting. The
    results of the training process are verbose, so we will not print them here, but
    they can be found via the [code on GitHub](http://bit.ly/2Gd4v7e).
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 对于五次运行中的每一次，梯度提升模型将训练多达两千轮，评估交叉验证对数损失是否在进行中减少。 如果交叉验证对数损失停止改善（在前两百轮中），则训练过程将停止以避免过拟合。
    训练过程的结果很冗长，所以我们不会在这里打印出来，但可以通过 [GitHub 上的代码](http://bit.ly/2Gd4v7e) 找到。
- en: Evaluate the results
  id: totrans-299
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估结果
- en: 'The following results show that the log loss over the entire training set (using
    the cross-validation predictions) is similar to that of XGBoost, one-fifth that
    of the random forests and one-fiftieth that of logistic regression. But compared
    to XGBoost, LightGBM is considerably faster:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的结果显示，整个训练集上的对数损失（使用交叉验证预测）与XGBoost相似，是随机森林的五分之一，是逻辑回归的五十分之一。 但与XGBoost相比，LightGBM速度要快得多：
- en: '[PRE45]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: As shown in [Figure 2-10](#precision_recall_curve_of_lightgbm_gradient_boosting),
    the average precision is 0.82, the same as that of XGboost (0.82), better than
    that of random forests (0.79), and considerably better than that of logistic regression
    (0.73).
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [图2-10](#precision_recall_curve_of_lightgbm_gradient_boosting) 所示，平均精度为0.82，与XGBoost相同（0.82），优于随机森林（0.79），远优于逻辑回归（0.73）。
- en: '![Precision-recall curve of LightGBM gadient boosting](assets/hulp_0210.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![LightGBM梯度提升的精确度-召回率曲线](assets/hulp_0210.png)'
- en: Figure 2-10\. Precision-recall curve of LightGBM gradient boosting
  id: totrans-304
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-10\. LightGBM梯度提升的精确度-召回率曲线
- en: As shown in [Figure 2-11](#area_under_the_roc_curve_of_lightgbm_gradient_boosting),
    the auROC curve is 0.98, an improvement over that of XGBoost (0.97), logistic
    regression (0.97), and random forests (0.93).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [图2-11](#area_under_the_roc_curve_of_lightgbm_gradient_boosting) 所示，auROC曲线为0.98，比XGBoost（0.97），逻辑回归（0.97）和随机森林（0.93）都有所改进。
- en: '![Area under the ROC curve of LightGBM gradient boosting](assets/hulp_0211.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![LightGBM梯度提升的ROC曲线下面积](assets/hulp_0211.png)'
- en: Figure 2-11\. auROC curve of LightGBM gradient boosting
  id: totrans-307
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-11\. LightGBM梯度提升的auROC曲线
- en: Evaluation of the Four Models Using the Test Set
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用测试集评估四个模型
- en: 'So far in this chapter, we have learned how to:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们学习了如何：
- en: Set up the environment for machine learning projects
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置机器学习项目的环境
- en: Acquire, load, explore, clean, and visualize data
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取、加载、探索、清洗和可视化数据
- en: Split the dataset into training and test sets and set up *k*-fold cross-validation
    sets
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据集分割为训练集和测试集，并设置*k*-fold交叉验证集
- en: Choose the appropriate cost function
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择适当的成本函数
- en: Set the hyperparameters and perform training and cross-validation
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置超参数并进行训练和交叉验证
- en: Evaluate the results
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估结果
- en: We have not explored how to adjust the hyperparameters (a process known as hyperparameter
    fine-tuning) to improve the results of each machine learning solution and address
    underfitting/overfitting, but the [code on GitHub](http://bit.ly/2Gd4v7e) will
    allow you to conduct these experiments very easily.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尚未探索如何调整超参数（即超参数微调过程），以改善每个机器学习解决方案的结果并解决欠拟合/过拟合问题，但是 [GitHub 上的代码](http://bit.ly/2Gd4v7e)
    将使您能够非常轻松地进行这些实验。
- en: Even without such fine-tuning, the results are pretty clear. Based on our training
    and *k*-fold cross-validation, LightGBM gradient boosting is the best solution,
    closely followed by XGBoost. Random forests and logistic regression are worse.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 即使没有进行这样的精细调整，结果也很明显。根据我们的训练和*k*折交叉验证，LightGBM梯度提升是最佳解决方案，紧随其后的是XGBoost。随机森林和逻辑回归则较差。
- en: Let’s use the test set as a final evaluation of each of the four models.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用测试集作为四个模型的最终评估。
- en: 'For each model, we will use the trained model to predict the fraud probabilities
    for the test set transactions. Then, we will calculate the log loss for each model
    by comparing the fraud probabilities predicted by the model against the true fraud
    labels:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个模型，我们将使用训练好的模型预测测试集交易的欺诈概率。然后，通过比较模型预测的欺诈概率与真实欺诈标签，计算每个模型的对数损失：
- en: '[PRE46]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: There are no surprises in the following log loss block. LightGBM gradient boosting
    has the lowest log loss on the test set, followed by the rest.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的对数损失块中没有什么意外。LightGBM梯度提升在测试集上有最低的对数损失，其次是其他模型。
- en: '[PRE47]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Figures [2-12](#test_set_precision_recall_curve_of_logistic_regression) through
    [2-19](#test_set_area_under_the_roc_curve_of_lightgbm_gradient_boosting) are the
    precision-recall curves, average precisions, and auROC curve for all four models,
    corroborating our findings above.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [2-12](#test_set_precision_recall_curve_of_logistic_regression) 到 [2-19](#test_set_area_under_the_roc_curve_of_lightgbm_gradient_boosting)
    是四个模型的精确率-召回率曲线、平均精度和auROC曲线，验证了我们以上的发现。
- en: Logistic regression
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: '![Test set precision-recall curve of logistic regression](assets/hulp_0212.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归的测试集精确率-召回率曲线](assets/hulp_0212.png)'
- en: Figure 2-12\. Test set precision-recall curve of logistic regression
  id: totrans-326
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-12\. 逻辑回归的测试集精确率-召回率曲线
- en: '![Test set area under the ROC curve of logistic regression](assets/hulp_0213.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归的测试集auROC曲线下面积](assets/hulp_0213.png)'
- en: Figure 2-13\. Test set auROC curve of logistic regression
  id: totrans-328
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-13\. 逻辑回归的测试集auROC曲线
- en: Random forests
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机森林
- en: '![Test set precision-recall curve of random forests](assets/hulp_0214.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![随机森林的测试集精确率-召回率曲线](assets/hulp_0214.png)'
- en: Figure 2-14\. Test set precision-recall curve of random forests
  id: totrans-331
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-14\. 随机森林的测试集精确率-召回率曲线
- en: '![Test set area under the ROC curve of random forests](assets/hulp_0215.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![随机森林的测试集auROC曲线下面积](assets/hulp_0215.png)'
- en: Figure 2-15\. Test set auROC curve of logistic regression
  id: totrans-333
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-15\. 逻辑回归的测试集auROC曲线
- en: XGBoost gradient boosting
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: XGBoost梯度提升
- en: '![Test set precision-recall curve of XGBoost gradient boosting](assets/hulp_0216.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![XGBoost梯度提升的测试集精确率-召回率曲线](assets/hulp_0216.png)'
- en: Figure 2-16\. Test set precision-recall curve of XGBoost gradient boosting
  id: totrans-336
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-16\. XGBoost梯度提升的测试集精确率-召回率曲线
- en: '![Test set area under the ROC curve of XGBoost gradient boosting](assets/hulp_0217.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![XGBoost梯度提升的测试集ROC曲线下面积](assets/hulp_0217.png)'
- en: Figure 2-17\. Test set auROC curve of XGBoost gradient boosting
  id: totrans-338
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-17\. XGBoost梯度提升的测试集auROC曲线
- en: LightGBM gradient boosting
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LightGBM梯度提升
- en: '![Test set precision-recall curve of LightGBM gradient boosting](assets/hulp_0218.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![LightGBM梯度提升的测试集精确率-召回率曲线](assets/hulp_0218.png)'
- en: Figure 2-18\. Test set precision-recall curve of LightGBM gradient boosting
  id: totrans-341
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-18\. LightGBM梯度提升的测试集精确率-召回率曲线
- en: '![Test set area under the ROC curve of LightGBM gradient boosting](assets/hulp_0219.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![LightGBM梯度提升的测试集auROC曲线下面积](assets/hulp_0219.png)'
- en: Figure 2-19\. Test set auROC curve of LightGBM gradient boosting
  id: totrans-343
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-19\. LightGBM梯度提升的测试集auROC曲线
- en: The results of LightGBM gradient boosting are impressive—we can catch over 80%
    of the fraudulent transactions with nearly 90% precision (in other words, in catching
    80% of the total fraud the LightGBM model gets only 10% of the cases wrong).
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM梯度提升的结果令人印象深刻——我们可以捕捉超过80%的欺诈交易，并且准确率接近90%（换句话说，捕捉到80%的总欺诈交易中，LightGBM模型仅有10%的错误）。
- en: Considering how few cases of fraud our dataset has, this is a great accomplishment.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们的数据集中欺诈案例很少，这是一项很大的成就。
- en: Ensembles
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成模型
- en: Instead of picking just one of the machine learning solutions we have developed
    for use in production, we can evaluate whether an ensemble of the models leads
    to an improved fraud detection rate.^([10](ch02.html#idm140637554057392))
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以评估是否将这些开发的机器学习解决方案集成到生产中，以提高欺诈检测率^([10](ch02.html#idm140637554057392))，而不是仅选择一个。
- en: Generally, if we include similarly strong solutions from different machine learning
    families (such as one from random forests and one from neural networks), the ensemble
    of the solutions will lead to a better result than any of the standalone solutions.
    This is because each of the standalone solutions has different strengths and weaknesses.
    By including the standalone solutions together in an ensemble, the strengths of
    some of the models compensate for the weaknesses of the others, and vice versa.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，如果我们包含来自不同机器学习家族的同样强大的解决方案（例如来自随机森林和神经网络的解决方案），这些解决方案的集成将比任何一个独立的解决方案产生更好的结果。这是因为每个独立的解决方案都有不同的优势和劣势。通过在集成中包含这些独立解决方案，一些模型的优势弥补了其他模型的劣势，反之亦然。
- en: There are important caveats, though. If the standalone solutions are similarly
    strong, the ensemble will have better performance than any of the standalone solutions.
    But if one of the solutions is much better than the others, the ensemble’s performance
    will equal the performance of the best standalone solution; the subpar solutions
    will contribute nothing to the ensemble’s performance.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 不过有重要的注意事项。如果独立的解决方案同样强大，集成模型的性能将优于任何一个独立的解决方案。但如果其中一个解决方案远远优于其他解决方案，集成模型的性能将等于最佳独立解决方案的性能；而次优解决方案将对集成模型的性能毫无贡献。
- en: Also, the standalone solutions need to be relatively uncorrelated. If they are
    very correlated, the strengths of one will mirror those of the rest, and the same
    will be true with the weaknesses. We will see little benefit from diversifying
    via an ensemble.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，独立的解决方案需要相对不相关。如果它们高度相关，一个解决方案的优点会反映在其余解决方案上，同样的情况也会出现在缺点上。通过集成来实现多样化将不会有太多好处。
- en: Stacking
  id: totrans-351
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 堆叠
- en: In our problem here, two of the models (LightGBM gradient boosting and XGBoost
    gradient boosting) are much stronger than the others (random forests and logistic
    regression). But the two strongest models are from the same family, which means
    their strengths and weaknesses will be highly correlated.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的问题中，两个模型（LightGBM梯度提升和XGBoost梯度提升）比其他两个模型（随机森林和逻辑回归）强大得多。但是最强大的两个模型来自同一个家族，这意味着它们的优势和劣势高度相关。
- en: We can use stacking (which is a form of ensembling) to determine whether we
    can get an improvement in performance compared to the standalone models from earlier.
    In stacking, we take the predictions from the *k*-fold cross-validation from each
    of the four standalone models (known as *layer one predictions*) and append them
    to the original training dataset. We then train on this original features plus
    layer one predictions dataset using *k*-fold cross-validation.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用堆叠（一种集成形式）来确定是否能够相比之前的独立模型获得性能改进。在堆叠中，我们从每个四个独立模型的*k*-折交叉验证预测（称为第一层预测）中获取预测，并将它们附加到原始训练数据集上。然后，我们使用该原始特征加上第一层预测数据集进行*k*-折交叉验证训练。
- en: This will result in a new set of *k*-fold cross-validation predictions, known
    as layer two predictions, which we will evaluate to see if we have an improvement
    in performance over any of the standalone models.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生一个新的*k*-折交叉验证预测集，称为第二层预测，我们将评估是否在性能上比任何单独的模型有所改进。
- en: Combine layer one predictions with the original training dataset
  id: totrans-355
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将第一层预测与原始训练数据集结合
- en: 'First, let’s combine the predictions from each of the four machine learning
    models that we have built with the original training dataset:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们将每个构建的四个机器学习模型的预测与原始训练数据集相结合：
- en: '[PRE48]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Set the hyperparameters
  id: totrans-358
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置超参数
- en: 'Now we will use LightGBM gradient boosting—the best machine learning algorithm
    from the earlier exercise—to train on this original features plus layer one predictions
    dataset. The hyperparameters will remain the same as before:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用LightGBM梯度提升——之前练习中的最佳机器学习算法——在该原始特征加上第一层预测数据集上进行训练。超参数将保持与之前相同：
- en: '[PRE49]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Train the model
  id: totrans-361
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'As before, we will use *k*-fold cross-validation and generate fraud probabilities
    for the five different cross-validation sets:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，我们将使用*k*-折交叉验证，并为五个不同的交叉验证集生成欺诈概率：
- en: '[PRE50]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Evaluate the results
  id: totrans-364
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估结果
- en: 'In the following results, we do not see an improvement. The ensemble log loss
    is very similar to the standalone gradient boosting log loss. Since the best standalone
    solutions are from the same family (gradient boosting), we do not see an improvement
    in the results. They have highly correlated strengths and weaknesses in detecting
    fraud. There is no benefit in diversifying across models:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下结果中，我们没有看到任何改进。集成对数损失非常类似于独立梯度提升对数损失。由于最佳的独立解决方案来自相同的家族（梯度提升），我们没有看到结果的改进。它们在检测欺诈方面具有高度相关的优势和劣势。在模型之间进行多样化并没有好处：
- en: '[PRE51]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: As shown in Figures [2-20](#precision_recall_curve_of_the_ensemble) and [2-21](#area_under_the_roc_curve_of_the_ensemble),
    the precision-recall curve, the average precision, and the auROC also corroborate
    the lack of improvement.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [2-20](#precision_recall_curve_of_the_ensemble) 和 [2-21](#area_under_the_roc_curve_of_the_ensemble)
    所示，精确率-召回率曲线、平均精度和 auROC 也证实了改进的缺乏。
- en: '![Precision-recall curve of ensemble](assets/hulp_0220.png)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
  zh: '![集成的精确率-召回率曲线](assets/hulp_0220.png)'
- en: Figure 2-20\. Precision-recall curve of the ensemble
  id: totrans-369
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-20\. 集成的精确率-召回率曲线
- en: '![Area under the ROC curve of the ensemble](assets/hulp_0221.png)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
  zh: '![集成的 ROC 曲线下的面积](assets/hulp_0221.png)'
- en: Figure 2-21\. auROC curve of the ensemble
  id: totrans-371
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-21\. 集成的 auROC 曲线
- en: Final Model Selection
  id: totrans-372
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终模型选择
- en: Since the ensemble does not improve performance, we favor the simplicity of
    the standalone LightGBM gradient boosting model and will use it in production.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 由于集成并没有提高性能，我们更倾向于使用独立的 LightGBM 梯度提升模型的简洁性，并将其用于生产。
- en: Before we create a pipeline for new, incoming transactions, let’s visualize
    how well the LightGBM model separates the fraudulent transactions from the normal
    transactions for the test set.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们为新进入的交易创建流水线之前，让我们可视化一下 LightGBM 模型在测试集中如何将欺诈交易与正常交易分开。
- en: '[Figure 2-22](#plot_of_prediction_probabilities_and_the_true_label) displays
    the predicted probabilities on the x-axis. Based on this plot, the model does
    a reasonably good job of assigning a high probability of fraud to the transactions
    that are actually fraudulent. Vice versa, the model generally assigns a low probability
    to the transactions that are not fraudulent. Occasionally, the model is wrong,
    and assigns a low probability to a case of actual fraud and a high probability
    to a case of not fraud.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-22](#plot_of_prediction_probabilities_and_the_true_label) 在 x 轴上显示了预测概率。基于这个图表，模型在将实际欺诈交易分配高欺诈概率方面表现相当不错。反之，该模型通常会给不欺诈的交易分配低概率。偶尔，模型会错误地给实际欺诈案例分配低概率，而给非欺诈案例分配高概率。'
- en: Overall, the results are pretty impressive.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，结果相当令人印象深刻。
- en: '![Plot of prediction probabilities and the true label](assets/hulp_0222.png)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
  zh: '![预测概率和真实标签的绘图](assets/hulp_0222.png)'
- en: Figure 2-22\. Plot of prediction probabilities and the true label
  id: totrans-378
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-22\. 预测概率和真实标签的绘图
- en: Production Pipeline
  id: totrans-379
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生产流水线
- en: 'Now that we have selected a model for production, let’s design a simple pipeline
    that performs three simple steps on new, incoming data: load the data, scale the
    features, and generate predictions using the LightGBM model we have already trained
    and selected for use in production:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经选择了一个模型进行生产，让我们设计一个简单的流水线，对新进入的数据执行三个简单的步骤：加载数据，缩放特征，并使用我们已经训练并选择用于生产的
    LightGBM 模型生成预测：
- en: '[PRE52]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Once these predictions are generated, analysts can act on (i.e., investigate
    further) the ones with the highest predicted probability of being fraudulent and
    work through the list. Or, if automation is the goal, analysts can use a system
    that automatically rejects transactions that have a predicted probability of being
    fraudulent above a certain threshold.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦生成了这些预测，分析师可以对预测为欺诈概率最高的交易采取行动（即进一步调查），并逐一处理列表。或者，如果自动化是目标，分析师可以使用一个自动拒绝预测为欺诈概率高于某个阈值的交易的系统。
- en: For example, based on [Figure 2-13](#test_set_area_under_the_roc_curve_of_logistic_regression),
    if we automatically reject transactions with a predicted probability above 0.90,
    we will reject cases that are almost certain to be fraudulent without accidentally
    rejecting a case of not fraud.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，基于 [图 2-13](#test_set_area_under_the_roc_curve_of_logistic_regression)，如果我们自动拒绝预测概率高于
    0.90 的交易，我们将拒绝几乎肯定是欺诈的案例，而不会意外地拒绝非欺诈案例。
- en: Conclusion
  id: totrans-384
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Congratulations! You have built a credit card fraud detection system using supervised
    learning.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已经使用监督学习构建了一个信用卡欺诈检测系统。
- en: Together, we set up a machine learning environment, acquired and prepared the
    data, trained and evaluated multiple models, selected the final model for production,
    and designed a pipeline for new, incoming transactions. You have successfully
    created an applied machine learning solution.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一起建立了一个机器学习环境，获取并准备了数据，训练和评估了多个模型，选择了最终用于生产的模型，并设计了一个新的、流入交易的管道。你已经成功创建了一个应用的机器学习解决方案。
- en: Now we will use this same hands-on approach to develop applied machine learning
    solutions using unsupervised learning.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将采用同样的实践方法，利用无监督学习开发应用的机器学习解决方案。
- en: Note
  id: totrans-388
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The solution above will need to be retrained over time as the patterns of fraud
    change. Also, we should find other machine learning algorithms—from different
    machine learning families—that perform just as well as gradient boosting and include
    them in an ensemble to improve fraud detection performance overall.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 随着欺诈模式的变化，上述解决方案需要随时间重新训练。此外，我们应该找到其他机器学习算法——来自不同机器学习家族的算法——它们能像梯度提升一样表现良好，并将它们组合起来以改善整体的欺诈检测性能。
- en: Finally, interpretability is very important for real-world applications of machine
    learning. Because the features in this credit card transactions dataset are the
    output of PCA (a form of dimensionality reduction that we will explore in [Chapter 3](ch03.html#Chapter_3))
    we cannot explain in plain English why certain transactions are being flagged
    as potentially fraudulent. For greater interpretability of the results, we need
    access to the original pre-PCA features, which we do not have for this sample
    dataset.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，解释性对于机器学习在实际应用中非常重要。因为这个信用卡交易数据集的特征是PCA的输出（一种我们将在第3章探讨的降维形式），我们无法用简单的英语解释为什么某些交易被标记为潜在的欺诈行为。为了更好地解释结果，我们需要访问原始的PCA前特征，但对于这个示例数据集，我们没有这些特征。
- en: ^([1](ch02.html#idm140637564820880-marker)) For more on fastcluster, consult
    the [documentation](https://pypi.org/project/fastcluster/).
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch02.html#idm140637564820880-marker)) 想了解更多关于fastcluster的信息，请参阅[文档](https://pypi.org/project/fastcluster/)。
- en: ^([2](ch02.html#idm140637564803888-marker)) This dataset is available via [Kaggle](https://www.kaggle.com/dalpozz/creditcardfraud)
    and was collected during a research collaboration by Worldline and the Machine
    Learning Group of Universite Libre de Bruxelles. For more information, see Andrea
    Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi, “Calibrating
    Probability with Undersampling for Unbalanced Classification” in Symposium on
    Computational Intelligence and Data Mining (CIDM), IEEE, 2015.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch02.html#idm140637564803888-marker)) 这个数据集可以通过[Kaggle](https://www.kaggle.com/dalpozz/creditcardfraud)获取，并且是由Worldline和Universite
    Libre de Bruxelles的机器学习小组在研究合作期间收集的。更多信息请参见Andrea Dal Pozzolo、Olivier Caelen、Reid
    A. Johnson和Gianluca Bontempi的论文《Calibrating Probability with Undersampling for
    Unbalanced Classification》，发表于IEEE的计算智能与数据挖掘研讨会（CIDM），2015年。
- en: ^([3](ch02.html#idm140637564800304-marker)) Categorical variables take on one
    of a limited number of possible qualitative values and often have to be encoded
    for use in machine learning algorithms.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch02.html#idm140637564800304-marker)) 分类变量取可能的有限数量的可能质量值之一，通常需要进行编码以在机器学习算法中使用。
- en: ^([4](ch02.html#idm140637556238384-marker)) For more on how the stratify parameter
    preserves the ratio of positive labels, visit [the official website](http://bit.ly/2NiKWfi).
    To reproduce the same split in your experiments, set the random state to 2018\.
    If you set this to another number or don’t set it at all, the results will be
    different.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch02.html#idm140637556238384-marker)) 想了解stratify参数如何保留正标签比例，请访问[官方网站](http://bit.ly/2NiKWfi)。为了在你的实验中复制相同的分割，请将随机状态设置为2018。如果设置为其他数字或者不设置，结果将不同。
- en: ^([5](ch02.html#idm140637556129056-marker)) For more on L1 versus L2, refer
    to the blog post [“Differences Between L1 and L2 as Loss Function and Regularization.”](http://bit.ly/2Bcx413)
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch02.html#idm140637556129056-marker)) 想了解L1与L2的区别，请参考博文[“L1和L2作为损失函数和正则化的区别。”](http://bit.ly/2Bcx413)
- en: ^([6](ch02.html#idm140637555940304-marker)) True positives are instances where
    the prediction and the actual label are both true. True negatives are instances
    where the prediction and the actual label are both false. False positives are
    instances where the prediction is true but the actual label is false (also known
    as a false alarm or Type I error). False negatives are instances where the prediction
    is false but the actual label is true (also known as a miss or Type II error).
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch02.html#idm140637555940304-marker)) 真正例是预测和实际标签都为真的实例。真负例是预测和实际标签都为假的实例。假正例是预测为真但实际标签为假的实例（也称为误报或类型I错误）。假负例是预测为假但实际标签为真的实例（也称为漏报或类型II错误）。
- en: ^([7](ch02.html#idm140637555924656-marker)) Recall is also known as sensitivity
    or true positive rate. Related to sensitivity is a concept called specificity,
    or the true negative rate. This is defined as the number of true negatives over
    the total number of total actual negatives in the dataset. Specificity = true
    negative rate = true negatives / (true negatives + false positives).
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch02.html#idm140637555924656-marker)) 召回率也称为敏感性或真正率。与敏感性相关的概念是特异性或真负率。特异性定义为数据集中真负例数除以总实际负例数。特异性
    = 真负率 = 真负例 / (真负例 + 假正例)。
- en: ^([8](ch02.html#idm140637555072000-marker)) For more on XGBoost gradient boosting,
    consult the [GitHub repository](https://github.com/dmlc/xgboost).
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch02.html#idm140637555072000-marker)) 关于XGBoost梯度提升的更多信息，请参阅[GitHub代码库](https://github.com/dmlc/xgboost)。
- en: ^([9](ch02.html#idm140637554683376-marker)) For more on Microsoft’s LightGBM
    gradient boosting, consult the [GitHub repository](https://github.com/Microsoft/LightGBM).
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch02.html#idm140637554683376-marker)) 关于Microsoft的LightGBM梯度提升的更多信息，请参阅[GitHub代码库](https://github.com/Microsoft/LightGBM)。
- en: ^([10](ch02.html#idm140637554057392-marker)) For more on ensemble learning,
    refer to the [“Kaggle Ensembling Guide,”](https://mlwave.com/kaggle-ensembling-guide/)
    [“Introduction to Ensembling/Stacking in Python,”](http://bit.ly/2RYV4iF) and
    [“A Kaggler’s Guide to Model Stacking in Practice”](http://bit.ly/2Rrs1iI).
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch02.html#idm140637554057392-marker)) 关于集成学习的更多信息，请参考[“Kaggle集成指南,”](https://mlwave.com/kaggle-ensembling-guide/)
    [“Python中的集成/堆叠介绍,”](http://bit.ly/2RYV4iF) 和 [“实践中的模型堆叠指南”](http://bit.ly/2Rrs1iI)。
