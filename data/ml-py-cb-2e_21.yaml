- en: Chapter 21\. Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第21章 神经网络
- en: 21.0 Introduction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 21.0 引言
- en: At the heart of basic neural networks is the *unit* (also called a *node* or
    *neuron*). A unit takes in one or more inputs, multiplies each input by a parameter
    (also called a *weight*), sums the weighted input’s values along with some bias
    value (typically 0), and then feeds the value into an activation function. This
    output is then sent forward to the other neurons deeper in the neural network
    (if they exist).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 基本神经网络的核心是*单元*（也称为*节点*或*神经元*）。一个单元接收一个或多个输入，将每个输入乘以一个参数（也称为*权重*），将加权输入的值与一些偏置值（通常为0）求和，然后将值馈送到激活函数中。然后，该输出被发送到神经网络中更深层的其他神经元（如果存在）。
- en: Neural networks can be visualized as a series of connected layers that form
    a network connecting an observation’s feature values at one end and the target
    value (e.g., observation’s class) at the other end. *Feedforward* neural networks—​also
    called *multilayer perceptron*—are the simplest artificial neural networks used
    in any real-world setting. The name “feedforward” comes from the fact that an
    observation’s feature values are fed “forward” through the network, with each
    layer successively transforming the feature values with the goal that the output
    is the same as (or close to) the target’s value.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可以被视为一系列连接的层，形成一个网络，将观察的特征值连接在一端，目标值（例如，观察的类）连接在另一端。*前馈*神经网络—也称为*多层感知器*—是任何实际设置中使用的最简单的人工神经网络。名称“前馈”来自于这样一个事实：观察的特征值被“前向”传递到网络中，每一层逐渐地转换特征值，目标是输出与目标值相同（或接近）。
- en: Specifically, feedforward neural networks contain three types of layers. At
    the start of the neural network is an input layer, where each unit contains an
    observation’s value for a single feature. For example, if an observation has 100
    features, the input layer has 100 units. At the end of the neural network is the
    output layer, which transforms the output of intermediate layers (called *hidden
    layers*) into values useful for the task at hand. For example, if our goal is
    binary classification, we can use an output layer with a single unit that uses
    a sigmoid function to scale its own output to between 0 and 1, representing a
    predicted class probability.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，前馈神经网络包含三种类型的层。在神经网络的开始处是输入层，每个单元包含单个特征的观察值。例如，如果一个观察有100个特征，输入层有100个单元。在神经网络的末端是输出层，它将中间层（称为*隐藏层*）的输出转换为对任务有用的值。例如，如果我们的目标是二元分类，可以使用一个输出层，其中一个单元使用sigmoid函数将自己的输出缩放到0到1之间，表示预测的类概率。
- en: Between the input and output layers are the so-called hidden layers. These hidden
    layers successively transform the feature values from the input layer to something
    that, once processed by the output layer, resembles the target class. Neural networks
    with many hidden layers (e.g., 10, 100, 1,000) are considered “deep” networks.
    Training deep neural networks is a process known as *deep learning*.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入层和输出层之间是所谓的隐藏层。这些隐藏层逐步转换从输入层获取的特征值，以使其在被输出层处理后类似于目标类。具有许多隐藏层（例如，10、100、1,000）的神经网络被认为是“深”网络。训练深度神经网络的过程称为*深度学习*。
- en: Neural networks are typically created with all parameters initialized as small
    random values from a Gaussian or normal uniform distribution. Once an observation
    (or more often a set number of observations called a *batch*) is fed through the
    network, the outputted value is compared with the observation’s true value using
    a loss function. This is called *forward propagation*. Next an algorithm goes
    “backward” through the network identifying how much each parameter contributed
    to the error between the predicted and true values, a process called *back propagation*.
    At each parameter, the optimization algorithm determines how much each weight
    should be adjusted to improve the output.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络通常是用高斯或正态均匀分布中的小随机值初始化所有参数。一旦观察到（或更频繁地说是一组称为*批量*的观察），通过网络，输出的值与观察到的真实值使用损失函数进行比较。这称为*前向传播*。接下来，算法通过网络“向后”传播，识别每个参数在预测值和真实值之间误差中的贡献，这个过程称为*反向传播*。在每个参数处，优化算法确定每个权重应该调整多少以改善输出。
- en: Neural networks learn by repeating this process of forward propagation and back
    propagation for every observation in the training data multiple times (each time
    all observations have been sent through the network is called an *epoch* and training
    typically consists of multiple epochs), iteratively updating the values of the
    parameters utilizing a process called *gradient descent* to slowly optimize the
    values of the parameters for the given output.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络通过重复进行前向传播和反向传播的过程来学习，每个观察结果都会多次（每次所有观察结果都通过网络称为*epoch*，训练通常包含多个epoch），通过使用*梯度下降*过程来逐步优化参数值，从而优化给定输出的参数值。
- en: In this chapter, we will use the same Python library used in the last chapter,
    PyTorch, to build, train, and evaluate a variety of neural networks. PyTorch is
    a popular tool within the deep learning space due to its well-written APIs and
    intuitive representation of the low-level tensor operations that power neural
    networks. One key feature of PyTorch is called *autograd*, which automatically
    computes and stores the gradients used to optimize the parameters of the network
    after undergoing forward propagation and back propagation.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用上一章节中使用的同一Python库PyTorch来构建、训练和评估各种神经网络。PyTorch是深度学习领域内流行的工具，因为其良好编写的API和直观表示低级张量操作的能力。PyTorch的一个关键特性被称为*autograd*，它在前向传播和反向传播后自动计算和存储用于优化网络参数的梯度。
- en: Neural networks created using PyTorch code can be trained using both CPUs (i.e.,
    on your laptop) and GPUs (i.e., on a specialized deep learning computer). In the
    real world with real data, it is often necessary to train neural networks using
    GPUs, as the training process on large data for complex networks runs orders of
    magnitude faster on GPUs than CPUs. However, all the neural networks in this book
    are small and simple enough to be trained on a CPU-only laptop in only a few minutes.
    Just be aware that when we have larger networks and more training data, training
    using CPUs is *significantly* slower than training using GPUs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PyTorch代码创建的神经网络可以使用CPU（例如，您的笔记本电脑）和GPU（例如，专门的深度学习计算机）进行训练。在现实世界中使用真实数据时，通常需要使用GPU来训练神经网络，因为对于大数据和复杂网络，使用GPU比使用CPU快数个数量级。然而，本书中的所有神经网络都足够小和简单，可以仅使用CPU在几分钟内训练。只需注意，当我们有更大的网络和更多的训练数据时，使用CPU训练比使用GPU训练*显著*慢。
- en: 21.1 Using Autograd with PyTorch
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 21.1 使用PyTorch的Autograd
- en: Problem
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to use PyTorch’s autograd features to compute and store the gradients
    after undergoing forward propagation and back propagation.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望在前向传播和反向传播后使用PyTorch的自动微分功能来计算和存储梯度。
- en: Solution
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Create tensors with the `requires_grad` option set to `True`:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`requires_grad`选项设置为`True`创建张量：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Discussion
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Autograd is one of the core features of PyTorch and a big factor in its popularity
    as a deep learning library. The ability to easily compute, store, and visualize
    gradients makes PyTorch very intuitive for researchers and enthusiasts building
    neural networks from scratch.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 自动微分是PyTorch的核心特性之一，也是其作为深度学习库受欢迎的重要因素之一。能够轻松计算、存储和可视化梯度使得PyTorch对于从头构建神经网络的研究人员和爱好者来说非常直观。
- en: 'PyTorch uses a directed acyclic graph (DAG) to keep a record of all data and
    computational operations being performed on that data. This is incredibly useful,
    but it also means we need to be careful with what operations we try to apply on
    our PyTorch data that requires gradients. When working with autograd, we can’t
    easily convert our tensors to NumPy arrays and back without “breaking the graph,”
    a phrase used to describe operations that don’t support autograd:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch使用有向无环图（DAG）来记录在数据上执行的所有数据和计算操作。这非常有用，但也意味着我们在尝试应用需要梯度的PyTorch数据的操作时需要小心。在使用自动微分时，我们不能轻松地将张量转换为NumPy数组，也不能将其转换回来，而不会“破坏图”，这是用来描述不支持自动微分的操作的术语：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: To convert this tensor into a NumPy array, we need to call the `detach()` method
    on it, which will break the graph and thus our ability to automatically compute
    gradients. While this can definitely be useful, it’s worth knowing that detaching
    the tensor will prevent PyTorch from automatically computing the gradient.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要将此张量转换为NumPy数组，我们需要在其上调用`detach()`方法，这将中断计算图，从而无法自动计算梯度。虽然这确实有用，但值得注意的是，分离张量将阻止PyTorch自动计算梯度。
- en: See Also
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[PyTorch Autograd Tutorial](https://oreil.ly/mOWSw)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch Autograd 教程](https://oreil.ly/mOWSw)'
- en: 21.2 Preprocessing Data for Neural Networks
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 21.2 为神经网络预处理数据
- en: Problem
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to preprocess data for use in a neural network.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你想为神经网络预处理数据。
- en: Solution
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Standardize each feature using scikit-learn’s `StandardScaler`:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用scikit-learn的`StandardScaler`标准化每个特征：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Discussion
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: While this recipe is very similar to [Recipe 4.2](ch04.xhtml#standardizing-a-feature),
    it is worth repeating because of how important it is for neural networks. Typically,
    a neural network’s parameters are initialized (i.e., created) as small random
    numbers. Neural networks often behave poorly when the feature values are much
    larger than the parameter values. Furthermore, since an observation’s feature
    values are combined as they pass through individual units, it is important that
    all features have the same scale.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个配方与[配方 4.2](ch04.xhtml#standardizing-a-feature)非常相似，但由于对神经网络的重要性，值得重复。通常情况下，神经网络的参数被初始化（即创建）为小的随机数。当特征值远远大于参数值时，神经网络的表现通常不佳。此外，由于观察的特征值在通过各个单元时被合并，因此重要的是所有特征具有相同的尺度。
- en: For these reasons, it is best practice (although not always necessary; for example,
    when we have all binary features) to standardize each feature such that the feature’s
    values have the mean of 0 and the standard deviation of 1\. This can be accomplished
    easily with scikit-learn’s `StandardScaler`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 出于这些原因，最佳实践是（虽然不总是必要；例如，当所有特征都是二进制时）标准化每个特征，使得特征值具有均值为0和标准差为1。使用scikit-learn的`StandardScaler`可以轻松实现这一点。
- en: 'However, if you need to perform this operation after having created tensors
    with `requires_grad=True`, you’ll need to do this natively in PyTorch, so as not
    to break the graph. While you’ll typically standardize features prior to starting
    to train the network, it’s worth knowing how to accomplish the same thing in PyTorch:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你需要在创建了`requires_grad=True`的张量之后执行此操作，则需要在PyTorch中原生地执行，以避免破坏图形。虽然通常会在开始训练网络之前标准化特征，但了解如何在PyTorch中完成相同的事情也是值得的：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 21.3 Designing a Neural Network
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 21.3 设计一个神经网络
- en: Problem
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to design a neural network.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你想设计一个神经网络。
- en: Solution
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use the PyTorch `nn.Module` class to define a simple neural network architecture:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PyTorch的`nn.Module`类定义一个简单的神经网络架构：
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Discussion
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Neural networks consist of layers of units. However, there’s incredible variety
    in the types of layers and how they are combined to form the network’s architecture.
    While there are commonly used architecture patterns (which we’ll cover in this
    chapter), the truth is that selecting the right architecture is mostly an art
    and the topic of much research.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络由多层单元组成。然而，关于层类型及其如何组合形成网络架构有很多不同的选择。虽然有一些常用的架构模式（我们将在本章中介绍），但选择正确的架构大多是一门艺术，并且是大量研究的主题。
- en: 'To construct a feedforward neural network in PyTorch, we need to make a number
    of choices about both the network architecture and training process. Remember
    that each unit in the hidden layers:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 要在PyTorch中构建一个前馈神经网络，我们需要就网络架构和训练过程做出许多选择。请记住，每个隐藏层中的每个单元：
- en: Receives a number of inputs.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接收若干个输入。
- en: Weights each input by a parameter value.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过参数值加权每个输入。
- en: Sums together all weighted inputs along with some bias (typically 0).
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有加权输入与一些偏差（通常为0）相加。
- en: Most often then applies some function (called an *activation function*).
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最常见的是应用一些函数（称为*激活函数*）。
- en: Sends the output on to units in the next layer.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出发送到下一层的单元。
- en: First, for each layer in the hidden and output layers we must define the number
    of units to include in the layer and the activation function. Overall, the more
    units we have in a layer, the more complex patterns our network is able to learn.
    However, more units might make our network overfit the training data in a way
    detrimental to the performance on the test data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，对于隐藏层和输出层中的每一层，我们必须定义包括在该层中的单元数和激活函数。总体来说，一个层中有更多的单元，我们的网络就能够学习更复杂的模式。然而，更多的单元可能会使我们的网络过度拟合训练数据，从而损害测试数据的性能。
- en: 'For hidden layers, a popular activation function is the *rectified linear unit*
    (ReLU):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于隐藏层，一个流行的激活函数是*修正线性单元*（ReLU）：
- en: <math display="block"><mrow><mi>f</mi> <mo>(</mo> <mi>z</mi> <mo>)</mo> <mo>=</mo>
    <mo form="prefix" movablelimits="true">max</mo> <mo>(</mo> <mn>0</mn> <mo>,</mo>
    <mi>z</mi> <mo>)</mo></mrow></math>
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>f</mi> <mo>(</mo> <mi>z</mi> <mo>)</mo> <mo>=</mo>
    <mo form="prefix" movablelimits="true">max</mo> <mo>(</mo> <mn>0</mn> <mo>,</mo>
    <mi>z</mi> <mo>)</mo></mrow></math>
- en: where <math display="inline"><mi>z</mi></math> is the sum of the weighted inputs
    and bias. As we can see, if <math display="inline"><mi>z</mi></math> is greater
    than 0, the activation function returns <math display="inline"><mi>z</mi></math>;
    otherwise, the function returns 0\. This simple activation function has a number
    of desirable properties (a discussion of which is beyond the scope of this book),
    and this has made it a popular choice in neural networks. We should be aware,
    however, that many dozens of activation functions exist.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math display="inline"><mi>z</mi></math>是加权输入和偏差的总和。正如我们所见，如果<math display="inline"><mi>z</mi></math>大于0，则激活函数返回<math
    display="inline"><mi>z</mi></math>；否则，函数返回0。这个简单的激活函数具有许多理想的特性（其讨论超出了本书的范围），这使其成为神经网络中的热门选择。然而，我们应该注意，存在许多十几种激活函数。
- en: Second, we need to define the number of hidden layers to use in the network.
    More layers allow the network to learn more complex relationships, but with a
    computational cost.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步，我们需要定义网络中要使用的隐藏层的数量。更多的层允许网络学习更复杂的关系，但需要计算成本。
- en: 'Third, we have to define the structure of the activation function (if any)
    of the output layer. The nature of the output function is often determined by
    the goal of the network. Here are some common output layer patterns:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 第三步，我们必须定义输出层激活函数（如果有的话）的结构。输出函数的性质通常由网络的目标确定。以下是一些常见的输出层模式：
- en: Binary classification
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 二元分类
- en: One unit with a sigmoid activation function
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一个带有sigmoid激活函数的单元
- en: Multiclass classification
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 多类别分类
- en: '*k* units (where *k* is the number of target classes) and a softmax activation
    function'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*个单元（其中*k*是目标类别的数量）和softmax激活函数'
- en: Regression
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 回归
- en: One unit with no activation function
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一个没有激活函数的单元
- en: 'Fourth, we need to define a loss function (the function that measures how well
    a predicted value matches the true value); again, this is often determined by
    the problem type:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 第四步，我们需要定义一个损失函数（衡量预测值与真实值匹配程度的函数）；同样，这通常由问题类型决定：
- en: Binary classification
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 二元分类
- en: Binary cross-entropy
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 二元交叉熵
- en: Multiclass classification
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 多类别分类
- en: Categorical cross-entropy
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 分类交叉熵
- en: Regression
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 回归
- en: Mean square error
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 均方误差
- en: Fifth, we have to define an optimizer, which intuitively can be thought of as
    our strategy “walking around” the loss function to find the parameter values that
    produce the lowest error. Common choices for optimizers are stochastic gradient
    descent, stochastic gradient descent with momentum, root mean square propagation,
    and adaptive moment estimation (for more information on these optimizers, see
    [“See Also”](#see-also-ch20)).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 第五步，我们需要定义一个优化器，直观上可以将其视为我们在损失函数上“漫步”以找到产生最低误差的参数值的策略。常见的优化器选择包括随机梯度下降、带动量的随机梯度下降、均方根传播以及自适应矩估计（有关这些优化器的更多信息，请参见[“参考文献”](#see-also-ch20)）。
- en: Sixth, we can select one or more metrics to use to evaluate the performance,
    such as accuracy.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 第六步，我们可以选择一个或多个指标来评估性能，如准确性。
- en: 'In our example, we use the `torch.nn.Module` namespace to compose a simple,
    sequential neural network that can make binary classifications. The standard PyTorch
    approach for this is to create a child class that inherits from the `torch.nn.Module`
    class, instantiating a network architecture in the `__init__` method, and defining
    the mathematical operations we want to perform upon each forward pass in the `forward`
    method of the class. There are many ways to define networks in PyTorch, and although
    in this case we use functional methods for our activation functions (such as `nn.functional.relu`)
    we can also define these activation functions as layers. If we wanted to compose
    everything in the network as a layer, we could use the `Sequential` class:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们使用`torch.nn.Module`命名空间来组成一个简单的顺序神经网络，可以进行二元分类。在PyTorch中，标准的方法是创建一个子类，继承`torch.nn.Module`类，在`__init__`方法中实例化网络架构，并在类的`forward`方法中定义我们希望在每次前向传递中执行的数学操作。在PyTorch中定义网络的方法有很多种，虽然在本例中我们使用了函数式方法作为我们的激活函数（如`nn.functional.relu`），我们也可以将这些激活函数定义为层。如果我们希望将网络中的所有东西组成一层，我们可以使用`Sequential`类：
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In both cases, the network itself is a two-layer neural network (when counting
    layers we don’t include the input layer because it does not have any parameters
    to learn) defined using PyTorch’s sequential model. Each layer is “dense” (also
    called “fully connected”), meaning that all the units in the previous layer are
    connected to all the units in the next layer.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，网络本身都是一个两层神经网络（当计算层数时，不包括输入层，因为它没有任何要学习的参数），使用 PyTorch 的顺序模型进行定义。每一层都是“密集的”（也称为“全连接的”），意味着前一层中的所有单元都连接到下一层中的所有单元。
- en: 'In the first hidden layer we set `out_features=16`, meaning that layer contains
    16 units. These units have ReLU activation functions as defined in the `forward`
    method of our class: `x = nn.functional.relu(self.fc1(x))`. The first layer of
    our network has the size `(10, 16)`, which tells the first layer to expect each
    observation from our input data to have 10 feature values. This network is designed
    for binary classification so the output layer contains only one unit with a sigmoid
    activation function, which constrains the output to between 0 and 1 (representing
    the probability an observation is class 1).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个隐藏层中，我们设置 `out_features=16`，意味着该层包含 16 个单元。这些单元在我们类的 `forward` 方法中使用 ReLU
    激活函数定义为 `x = nn.functional.relu(self.fc1(x))`。我们网络的第一层大小为 `(10, 16)`，这告诉第一层期望从输入数据中每个观测值有
    10 个特征值。这个网络设计用于二元分类，因此输出层只包含一个单元，使用 sigmoid 激活函数将输出约束在 0 到 1 之间（表示观测为类别 1 的概率）。
- en: See Also
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '[PyTorch tutorial: Build the Neural Network](https://oreil.ly/iT8iv)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch 教程：构建神经网络](https://oreil.ly/iT8iv)'
- en: '[Loss functions for classification, Wikipedia](https://oreil.ly/4bPXv)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分类中的损失函数，维基百科](https://oreil.ly/4bPXv)'
- en: '[On Loss Functions for Deep Neural Networks in Classification, Katarzyna Janocha
    and Wojciech Marian Czarnecki](https://oreil.ly/pplP-)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度神经网络分类中的损失函数，Katarzyna Janocha 和 Wojciech Marian Czarnecki](https://oreil.ly/pplP-)'
- en: 21.4 Training a Binary Classifier
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 21.4 训练二元分类器
- en: Problem
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to train a binary classifier neural network.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望训练一个二元分类器神经网络。
- en: Solution
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use PyTorch to construct a feedforward neural network and train it:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PyTorch 构建一个前馈神经网络并对其进行训练：
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Discussion
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: In [Recipe 21.3](#designing-a-neural-network), we discussed how to construct
    a neural network using PyTorch’s sequential model. In this recipe we train that
    neural network using 10 features and 1,000 observations of fake classification
    generated from scikit-learn’s `make_classification` function.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [Recipe 21.3](#designing-a-neural-network) 中，我们讨论了如何使用 PyTorch 的顺序模型构建神经网络。在这个配方中，我们使用了来自
    scikit-learn 的 `make_classification` 函数生成的具有 10 个特征和 1,000 个观测值的假分类数据集来训练该神经网络。
- en: The neural network we are using is the same as the one in [Recipe 21.3](#designing-a-neural-network)
    (see that recipe for a detailed explanation). The difference there is that we
    only created the neural network; we didn’t train it.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的神经网络与 [Recipe 21.3](#designing-a-neural-network) 中的相同（详见该配方进行详细解释）。不同之处在于，我们只是创建了神经网络，而没有对其进行训练。
- en: At the end, we use `with torch.no_grad()` to evaluate the network. This says
    that we should not compute gradients for any tensor operations conducted in this
    section of code. Since we use gradients only during the model training process,
    we don’t want to store new gradients for operations that occur outside of it (such
    as prediction or evaluation).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用 `with torch.no_grad()` 来评估网络。这表示我们不应计算在代码这一部分中进行的任何张量操作的梯度。由于我们只在模型训练过程中使用梯度，因此我们不希望为在其外部发生的操作（如预测或评估）存储新梯度。
- en: The `epochs` variable defines how many epochs to use when training the data.
    `batch_size` sets the number of observations to propagate through the network
    before updating the parameters.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`epochs` 变量定义了在训练数据时使用的 epochs 数量。`batch_size` 设置了在更新参数之前要通过网络传播的观测值数量。'
- en: We then iterate over the number of epochs, making forward passes through the
    network using the `forward` method, and then backward passes to update the gradients.
    The result is a trained model.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们迭代多个 epochs，通过网络进行前向传递使用 `forward` 方法，然后反向传递以更新梯度。结果是一个经过训练的模型。
- en: 21.5 Training a Multiclass Classifier
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 21.5 训练多类分类器
- en: Problem
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to train a multiclass classifier neural network.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望训练一个多类分类器神经网络。
- en: Solution
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use PyTorch to construct a feedforward neural network with an output layer
    with softmax activation functions:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PyTorch 构建一个具有 softmax 激活函数输出层的前馈神经网络：
- en: '[PRE14]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Discussion
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'In this solution we created a similar neural network to the binary classifier
    from the last recipe, but with some notable changes. In the classification data
    we generated, we set `N_CLASSES=3`. To handle multiclass classification, we also
    use `nn.CrossEntropyLoss()`, which expects the target to be one-hot encoded. To
    accomplish this, we use the `torch.nn.functional.one_hot` function and end up
    with a one-hot encoded array where the position of `1.` indicates the class for
    a given observation:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个解决方案中，我们创建了一个类似于上一个示例中的二元分类器的神经网络，但是有一些显著的改变。在我们生成的分类数据中，我们设置了`N_CLASSES=3`。为了处理多类分类问题，我们还使用了`nn.CrossEntropyLoss()`，该函数期望目标是独热编码的。为了实现这一点，我们使用了`torch.nn.functional.one_hot`函数，最终得到一个独热编码的数组，其中`1.`的位置表示给定观察的类别：
- en: '[PRE16]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Since this is a multiclass classification problem, we used an output layer of
    size 3 (one per class) containing a softmax activation function. The softmax activation
    function will return an array of 3 values summing to 1\. These 3 values represent
    an observation’s probability of being a member of each of the 3 classes.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这是一个多类分类问题，我们使用了大小为 3 的输出层（每个类别一个）并包含 softmax 激活函数。Softmax 激活函数将返回一个数组，其中的
    3 个值相加为 1。这 3 个值表示一个观察结果属于每个类别的概率。
- en: 'As mentioned in this recipe, we used a loss function suited to multiclass classification,
    the categorical cross-entropy loss function: `nn.CrossEntropyLoss()`.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如本文提到的，我们使用了适合多类分类的损失函数，即分类交叉熵损失函数：`nn.CrossEntropyLoss()`。
- en: 21.6 Training a Regressor
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 21.6 训练回归器
- en: Problem
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to train a neural network for regression.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望为回归训练一个神经网络。
- en: Solution
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use PyTorch to construct a feedforward neural network with a single output
    unit that has no activation function:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PyTorch 构建一个只有一个输出单元且没有激活函数的前馈神经网络：
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Discussion
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: It’s completely possible to create a neural network to predict continuous values
    instead of class probabilities. In the case of our binary classifier ([Recipe
    21.4](#training-a-binary-classifier)) we used an output layer with a single unit
    and a sigmoid activation function to produce a probability that an observation
    was class 1\. Importantly, the sigmoid activation function constrained the outputted
    value to between 0 and 1\. If we remove that constraint by having no activation
    function, we allow the output to be a continuous value.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 完全可以创建一个神经网络来预测连续值，而不是类概率。在我们的二元分类器的情况下（[Recipe 21.4](#training-a-binary-classifier)），我们使用了一个具有单个单元和
    sigmoid 激活函数的输出层，以生成观察是类 1 的概率。重要的是，sigmoid 激活函数将输出值限制在 0 到 1 之间。如果我们去除这种约束，即没有激活函数，我们允许输出为连续值。
- en: 'Furthermore, because we are training a regression, we should use an appropriate
    loss function and evaluation metric, in our case the mean square error:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，因为我们正在训练回归模型，我们应该使用适当的损失函数和评估指标，在我们的情况下是均方误差：
- en: <math display="block"><mrow><mo form="prefix">MSE</mo> <mo>=</mo> <mfrac><mn>1</mn>
    <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mrow>
    <mi>n</mi></munderover> <msup><mrow><mo>(</mo><msub><mover accent="true"><mi>y</mi>
    <mo>^</mo></mover> <mi>i</mi></msub> <mo>-</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mo form="prefix">MSE</mo> <mo>=</mo> <mfrac><mn>1</mn>
    <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mrow>
    <mi>n</mi></munderover> <msup><mrow><mo>(</mo><msub><mover accent="true"><mi>y</mi>
    <mo>^</mo></mover> <mi>i</mi></msub> <mo>-</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
- en: where <math display="inline"><mi>n</mi></math> is the number of observations;
    <math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math> is the true value
    of the target we are trying to predict, <math display="inline"><mi>y</mi></math>,
    for observation <math display="inline"><mi>i</mi></math>; and <math display="inline"><msub><mover
    accent="true"><mi>y</mi> <mo>^</mo></mover><mi>i</mi></msub></math> is the model’s
    predicted value for <math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math>.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math display="inline"><mi>n</mi></math>是观察数量；<math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math>是我们试图预测的目标<math
    display="inline"><mi>y</mi></math>的真实值，对于观察<math display="inline"><mi>i</mi></math>；<math
    display="inline"><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover><mi>i</mi></msub></math>是模型对<math
    display="inline"><msub><mi>y</mi><mi>i</mi></msub></math>的预测值。
- en: Finally, because we are using simulated data using scikit-learn `make_regression`,
    we didn’t have to standardize the features. It should be noted, however, that
    in almost all real-world cases, standardization would be necessary.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，由于我们使用了使用 scikit-learn 的`make_regression`生成的模拟数据，我们不需要对特征进行标准化。然而，需要注意的是，在几乎所有实际情况下，标准化是必要的。
- en: 21.7 Making Predictions
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 21.7 进行预测
- en: Problem
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to use a neural network to make predictions.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望使用神经网络进行预测。
- en: Solution
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use PyTorch to construct a feedforward neural network, then make predictions
    using `forward`:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PyTorch 构建一个前馈神经网络，然后使用`forward`进行预测：
- en: '[PRE20]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Discussion
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Making predictions is easy in PyTorch. Once we have trained our neural network
    we can use the `forward` method (already used as part of the training process),
    which takes as input a set of features and does a forward pass through the network.
    In our solution the neural network is set up for binary classification, so the
    predicted output is the probability of being class 1\. Observations with predicted
    values very close to 1 are highly likely to be class 1, while observations with
    predicted values very close to 0 are highly likely to be class 0\. Hence, we use
    the `round` method to convert these values to 1s and 0s for our binary classifier.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中进行预测非常容易。一旦我们训练了神经网络，我们可以使用 `forward` 方法（已作为训练过程的一部分使用），该方法接受一组特征作为输入，并通过网络进行前向传递。在我们的解决方案中，神经网络被设置为二元分类，因此预测的输出是属于类
    1 的概率。预测值接近 1 的观察结果高度可能属于类 1，而预测值接近 0 的观察结果高度可能属于类 0。因此，我们使用 `round` 方法将这些值转换为二元分类器中的
    1 和 0。
- en: 21.8 Visualize Training History
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 21.8 可视化训练历史
- en: Problem
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to find the “sweet spot” in a neural network’s loss and/or accuracy
    score.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望找到神经网络损失和/或准确率得分的“甜蜜点”。
- en: Solution
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use Matplotlib to visualize the loss of the test and training set over each
    epoch:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Matplotlib 可视化每个 epoch 中测试集和训练集的损失：
- en: '[PRE22]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![mpc2 21in01](assets/mpc2_21in01.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![mpc2 21in01](assets/mpc2_21in01.png)'
- en: Discussion
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: When our neural network is new, it will have poor performance. As the neural
    network learns on the training data, the model’s error on both the training and
    test set will tend to decrease. However, at a certain point, a neural network
    can start “memorizing” the training data and overfit. When this starts happening,
    the training error may decrease while the test error starts increasing. Therefore,
    in many cases, there is a “sweet spot” where the test error (which is the error
    we mainly care about) is at its lowest point. This effect can be seen in the solution,
    where we visualize the training and test loss at each epoch. Note that the test
    error is lowest around epoch 6, after which the training loss plateaus while the
    test loss starts increasing. From this point onward, the model is overfitting.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的神经网络是新的时，它的性能会比较差。随着神经网络在训练数据上的学习，模型在训练集和测试集上的错误通常会减少。然而，在某个点上，神经网络可能会开始“记忆”训练数据并过拟合。当这种情况发生时，训练错误可能会减少，而测试错误则开始增加。因此，在许多情况下，存在一个“甜蜜点”，在这个点上测试错误（我们主要关心的错误）达到最低点。这种效果可以在解决方案中看到，我们可视化了每个
    epoch 的训练和测试损失。请注意，测试错误在第 6 个 epoch 左右达到最低点，此后训练损失趋于平稳，而测试损失开始增加。从这一点开始，模型开始过拟合。
- en: 21.9 Reducing Overfitting with Weight Regularization
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 21.9 使用权重正则化来减少过拟合
- en: Problem
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to reduce overfitting by regularizing the weights of your network.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望通过正则化网络的权重来减少过拟合。
- en: Solution
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Try penalizing the parameters of the network, also called *weight regularization*:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试对网络参数进行惩罚，也称为 *weight regularization*：
- en: '[PRE23]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Discussion
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: One strategy to combat overfitting neural networks is by penalizing the parameters
    (i.e., weights) of the neural network such that they are driven to be small values,
    creating a simpler model less prone to overfit. This method is called weight regularization
    or weight decay. More specifically, in weight regularization a penalty is added
    to the loss function, such as the L2 norm.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 抑制过拟合神经网络的一种策略是通过对神经网络的参数（即权重）施加惩罚，使它们趋向于较小的值，从而创建一个不容易过拟合的简单模型。这种方法称为权重正则化或权重衰减。具体而言，在权重正则化中，将惩罚项添加到损失函数中，如
    L2 范数。
- en: In PyTorch, we can add weight regularization by including `weight_decay=1e-5`
    in the optimizer where regularization happens. In this example, `1e-5` determines
    how much we penalize higher parameter values. Values greater than 0 indicate L2
    regularization in PyTorch.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中，我们可以通过在优化器中包含 `weight_decay=1e-5` 来添加权重正则化，在这里正则化发生。在这个例子中，`1e-5`
    决定了我们对较高参数值施加的惩罚程度。大于 0 的数值表示在 PyTorch 中使用 L2 正则化。
- en: 21.10 Reducing Overfitting with Early Stopping
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 21.10 使用早停策略来减少过拟合
- en: Problem
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to reduce overfitting by stopping training when your train and test
    scores diverge.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望通过在训练和测试得分发散时停止训练来减少过拟合。
- en: Solution
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use PyTorch Lightning to implement a strategy called *early stopping*:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PyTorch Lightning 实现一种名为 *early stopping* 的策略：
- en: '[PRE25]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Discussion
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: As we discussed in [Recipe 21.8](#visualize-training-history), typically in
    the first several training epochs, both the training and test errors will decrease,
    but at some point the network will start “memorizing” the training data, causing
    the training error to continue to decrease even while the test error starts increasing.
    Because of this phenomenon, one of the most common and very effective methods
    to counter overfitting is to monitor the training process and stop training when
    the test error starts to increase. This strategy is called *early stopping*.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[Recipe 21.8](#visualize-training-history)中讨论的，通常在最初的几个训练 epoch 中，训练和测试错误都会减少，但是在某个时候，网络将开始“记忆”训练数据，导致训练错误继续减少，而测试错误开始增加。因此，对抗过拟合最常见且非常有效的方法之一是监控训练过程，并在测试错误开始增加时停止训练。这种策略称为*早期停止*。
- en: In PyTorch, we can implement early stopping as a callback function. Callbacks
    are functions that can be applied at certain stages of the training process, such
    as at the end of each epoch. However, PyTorch itself does not define an early
    stopping class for you, so here we use the popular library `lightning` (known
    as PyTorch Lightning) to use an out-of-the-box one. PyTorch Lightning is a high-level
    library for PyTorch that provides a lot of useful features. In our solution, we
    included PyTorch Lightning’s `EarlyStopping(monitor="val_loss", mode="min", patience=3)`
    to define that we wanted to monitor the test (validation) loss at each epoch,
    and if the test loss has not improved after three epochs (the default), training
    is interrupted.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中，我们可以将早期停止作为回调函数来实现。回调函数是在训练过程的特定阶段应用的函数，例如在每个 epoch 结束时。然而，PyTorch
    本身并没有为您定义一个早期停止的类，因此在这里我们使用流行的库`lightning`（即 PyTorch Lightning）来使用现成的早期停止功能。PyTorch
    Lightning 是一个为 PyTorch 提供大量有用功能的高级库。在我们的解决方案中，我们包括了 PyTorch Lightning 的`EarlyStopping(monitor="val_loss",
    mode="min", patience=3)`，以定义我们希望在每个 epoch 监控测试（验证）损失，并且如果经过三个 epoch（默认值）后测试损失没有改善，则中断训练。
- en: 'If we did not include the `EarlyStopping` callback, the model would train for
    the full 1,000 max epochs without stopping on its own:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们没有包含`EarlyStopping`回调，模型将在完整的 1,000 个最大 epoch 中训练而不会自行停止：
- en: '[PRE27]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 21.11 Reducing Overfitting with Dropout
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 21.11 使用 Dropout 减少过拟合
- en: Problem
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to reduce overfitting.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望减少过拟合。
- en: Solution
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Introduce noise into your network’s architecture using dropout:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 dropout 在您的网络架构中引入噪声：
- en: '[PRE29]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Discussion
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: '*Dropout* is a fairly common method for regularizing smaller neural networks.
    In dropout, every time a batch of observations is created for training, a proportion
    of the units in one or more layers is multiplied by zero (i.e., dropped). In this
    setting, every batch is trained on the same network (e.g., the same parameters),
    but each batch is confronted by a slightly different version of that network’s
    *architecture*.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*Dropout* 是一种相对常见的正则化较小神经网络的方法。在 dropout 中，每次为训练创建一批观察时，一个或多个层中的单位比例被乘以零（即被删除）。在此设置中，每个批次都在相同的网络上训练（例如相同的参数），但是每个批次都面对稍微不同版本的该网络的*架构*。'
- en: Dropout is thought to be effective because by constantly and randomly dropping
    units in each batch, it forces units to learn parameter values able to perform
    under a wide variety of network architectures. That is, they learn to be robust
    to disruptions (i.e., noise) in the other hidden units, and this prevents the
    network from simply memorizing the training data.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 被认为是有效的，因为通过在每个批次中不断随机删除单位，它强制单位学习能够在各种网络架构下执行的参数值。也就是说，它们学会了对其他隐藏单元中的干扰（即噪声）具有鲁棒性，从而防止网络简单地记住训练数据。
- en: It is possible to add dropout to both the hidden and input layers. When an input
    layer is dropped, its feature value is not introduced into the network for that
    batch.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将 dropout 添加到隐藏层和输入层中。当输入层被删除时，其特征值在该批次中不会被引入网络中。
- en: In PyTorch, we can implement dropout by adding an `nn.Dropout` layer into our
    network architecture. Each `nn.Dropout` layer will drop a user-defined hyperparameter
    of units in the previous layer every batch.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中，我们可以通过在网络架构中添加一个`nn.Dropout`层来实现 dropout。每个`nn.Dropout`层将在每个批次中删除前一层中用户定义的超参数单位。
- en: 21.12 Saving Model Training Progress
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 21.12 保存模型训练进度
- en: Problem
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Given a neural network that will take a long time to train, you want to save
    your progress in case the training process is interrupted.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于神经网络训练时间较长，您希望在训练过程中保存进度以防中断。
- en: Solution
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use the `torch.save` function to save the model after every epoch:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `torch.save` 函数在每个 epoch 后保存模型：
- en: '[PRE31]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Discussion
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'In the real world, it is common for neural networks to train for hours or even
    days. During that time a lot can go wrong: computers can lose power, servers can
    crash, or inconsiderate graduate students can close your laptop.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，神经网络通常需要训练几个小时甚至几天。在此期间，可能发生很多问题：计算机断电、服务器崩溃，或者不体贴的研究生关掉你的笔记本电脑。
- en: We can use `torch.save` to alleviate this problem by saving the model after
    every epoch. Specifically, after every epoch, we save a model to the location
    `model.pt`, the second argument to the `torch.save` function. If we include only
    a filename (e.g., *model.pt*) that file will be overridden with the latest model
    every epoch.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `torch.save` 函数来缓解这个问题，通过在每个 epoch 后保存模型。具体来说，在每个 epoch 后，我们将模型保存到位置
    `model.pt`，这是 `torch.save` 函数的第二个参数。如果我们只包含一个文件名（例如 *model.pt*），那么该文件将在每个 epoch
    都被最新的模型覆盖。
- en: As you can imagine, we can introduce additional logic to save the model every
    few epochs, only save a model if the loss goes down, etc. We could even combine
    this approach with the early stopping approach in PyTorch Lightning to ensure
    we save a model no matter at what epoch the training ends.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可以想象的，我们可以引入额外的逻辑，在每几个 epochs 保存模型，仅在损失减少时保存模型等。我们甚至可以将这种方法与 PyTorch Lightning
    的提前停止方法结合起来，以确保无论训练在哪个 epoch 结束，我们都能保存模型。
- en: 21.13 Tuning Neural Networks
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 21.13 调整神经网络
- en: Problem
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to automatically select the best hyperparameters for your neural network.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 您想自动选择神经网络的最佳超参数。
- en: Solution
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use the `ray` tuning library with PyTorch:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PyTorch 的 `ray` 调优库：
- en: '[PRE33]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Discussion
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: In Recipes [12.1](ch12.xhtml#selecting-best-models-using-exhaustive-search)
    and [12.2](ch12.xhtml#selecting-best-models-using-randomized-search), we covered
    using scikit-learn’s model selection techniques to identify the best hyperparameters
    of a scikit-learn model. While in general the scikit-learn approach can also be
    applied to neural networks, the `ray` tuning library provides a sophisticated
    API that allows you to schedule experiments on both CPUs and GPUs.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [12.1](ch12.xhtml#selecting-best-models-using-exhaustive-search) 和 [12.2](ch12.xhtml#selecting-best-models-using-randomized-search)
    节中，我们介绍了使用 scikit-learn 的模型选择技术来识别 scikit-learn 模型的最佳超参数。尽管一般来说，scikit-learn 的方法也可以应用于神经网络，但
    `ray` 调优库提供了一个复杂的 API，允许您在 CPU 和 GPU 上调度实验。
- en: The hyperparameters of a model *are* important and should be selected with care.
    However, running experiments to select hyperparameters can be both cost and time
    prohibitive. Therefore, automatic hyperparameter tuning of neural networks is
    not the silver bullet, but it is a useful tool to have in certain circumstances.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的超参数*很重要*，应该仔细选择。然而，运行实验来选择超参数可能会成本高昂且耗时。因此，神经网络的自动超参数调整并非万能药，但在特定情况下是一个有用的工具。
- en: In our solution we conducted a search of different parameters for layer sizes
    and the learning rate of our optimizer. The `best_trial.config` shows the parameters
    in our `ray` tuning configuration that led to the lowest loss and best experiment
    outcome.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的解决方案中，我们对不同的参数进行了搜索，包括层大小和优化器的学习率。`best_trial.config` 显示了在我们的 `ray` 调优配置中导致最低损失和最佳实验结果的参数。
- en: 21.14 Visualizing Neural Networks
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 21.14 可视化神经网络
- en: Problem
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to quickly visualize a neural network’s architecture.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 您想快速可视化神经网络的架构。
- en: Solution
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use the `make_dot` function from `torch_viz`:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `torch_viz` 的 `make_dot` 函数：
- en: '[PRE35]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'If we open the image that was saved to our machine, we can see the following:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们打开保存到我们机器上的图像，我们可以看到以下内容：
- en: '![mpc2 21in02](assets/mpc2_21in02.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![mpc2 21in02](assets/mpc2_21in02.png)'
- en: Discussion
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: The `torchviz` library provides easy utility functions to quickly visualize
    our neural networks and write them out as images.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchviz` 库提供了简单的实用函数，可以快速可视化我们的神经网络并将其输出为图像。'
