- en: Chapter 15\. Ethics, Fairness, and Privacy for Mobile Apps
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第15章。移动应用的伦理、公平和隐私
- en: While recent advances in machine learning and AI have brought the concepts of
    ethics and fairness into the spotlight, it’s important to note that disparity
    and unfairness have always been topics of concern in computer systems. In my career,
    I have seen many examples where a system has been engineered for one scenario
    without considering the overall impact with regard to fairness and bias.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然最近机器学习和人工智能的进展将伦理和公平的概念推向了聚光灯下，但需要注意的是，在计算机系统中，不平等和不公平一直是关注的话题。在我的职业生涯中，我见过许多例子，系统为一个场景而设计，但没有考虑到公平和偏见对整体影响的影响。
- en: 'Consider this example: your company has a database of its customers and wants
    to target a marketing campaign to get more customers in a particular zip code
    where it has identified a growth opportunity. To do so, the company will send
    discount coupons to people in that zip code whom it has connected with, but who
    haven’t yet purchased anything. You could write SQL like this to identify these
    potential customers:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这个例子：您的公司拥有客户数据库，并希望针对识别出的增长机会的特定邮政编码区域推出营销活动，以获得更多客户。为此，公司将向那些位于该邮政编码区域的与之建立联系但尚未购买任何东西的人发送折扣券。您可以编写如下SQL来识别这些潜在客户：
- en: '[PRE0]'
  id: totrans-3
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This might seem to be perfectly sensible code. But consider the demographics
    of that zip code. What if the majority of people who live there are of a particular
    race or age? Instead of growing your customer base evenly, you could be overtargeting
    one segment of the population, or worse, discriminating against another by offering
    discounts to people of one race but not another. Over time, continually targeting
    like this could result in a customer base that is skewed against the demographics
    of society, ultimately painting your company into a corner of primarily serving
    one segment. In this case, the variable—zip code—was explicit, but systems with
    proxy variables that aren’t so explicit can still grow into biased ones without
    careful monitoring.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来是完全合理的代码。但考虑一下该邮政编码区域的人口统计数据。如果那里的大多数人属于特定种族或年龄组，会怎么样呢？与均衡增长您的客户群体不同，您可能会过度定位一个人群，或者更糟糕的是，通过向一种种族提供折扣而对另一种族进行歧视。随着时间的推移，持续这样的定位可能导致客户群体对社会人口统计数据偏向，最终将您的公司限制在主要为一个人群服务的困境中。在这种情况下，变量——邮政编码——是明确的，但是具有不那么明确的代理变量的系统在没有仔细监控的情况下仍可能发展为具有偏见的系统。
- en: The promise of AI-based systems is that you will be able to deliver more powerful
    applications more quickly...but if you do so at the cost of not mitigating bias
    in your systems, then you could potentially *accelerate* disparity through the
    use of AI.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: AI系统的承诺是，您将能够更快地交付更强大的应用程序...但如果您这样做的代价是不减少系统中的偏见，那么您可能会通过使用AI*加速*差距。
- en: The process of understanding and eliminating this, where possible, is a huge
    field that would fill many books, so in this single chapter we’ll just get a general
    overview of where you need to be aware of potential bias issues, and methodologies
    and tools that can help you fix them.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 理解和在可能的情况下消除这一过程是一个庞大的领域，可以填写很多书籍，因此在这一章中，我们只会对您需要注意潜在偏见问题的地方进行概述，并介绍可以帮助您解决这些问题的方法和工具。
- en: Ethics, Fairness, and Privacy with Responsible AI
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 伦理、公平与隐私的责任AI
- en: 'Building an AI system with responsibility as part of your DNA means that responsible
    AI practices can be incorporated at every step in the ML workflow. While there
    are many patterns for this, I’m going to follow the very general one with the
    following steps:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个AI系统，并将责任作为您ML工作流程的一部分，这意味着可以在每个步骤中都纳入负责任的AI实践。虽然有许多这样的模式，但我将遵循以下非常一般的步骤：
- en: 'Defining the problem: who is your ML system for'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义问题：您的ML系统是为谁设计的
- en: Constructing and preparing your data
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建和准备您的数据
- en: Building and training your model
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建和训练您的模型
- en: Evaluating your model
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估您的模型
- en: Deploying and monitoring your model’s usage
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署和监控您的模型使用情况
- en: Let’s look at some tools that are available for you as you navigate through
    these steps.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在您通过这些步骤时可用的一些工具。
- en: Responsibly Defining Your Problem
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 负责地定义您的问题
- en: When you are creating an app to solve a problem, it’s good to consider issues
    that might arise from the very existence of the app. You might build something
    as innocent as a bird song detector, where you classify birds based on the sounds
    they make. But how might that impact your users? What if your data is limited
    to birds that are prevalent in a particular area, and what if that area is populated
    primarily by a single demographic. You would have, unconsciously, written an app
    that is only available to that demographic. Is that what you want? With an app
    like this, accessibility concerns can also arise. If your concept is that you
    hear a bird singing, and you want to identify it...you are assuming that the person
    *can* hear the birds, and as a result you are not being inclusive to people with
    reduced or no hearing. Now while this is a very trivial example, extend the concept
    to apps or services that can deeply impact someone’s life. What if your rideshare
    app avoided certain neighborhoods, excluding people? What if your app is useful
    for healthcare, such as helping one manage their medication, but it fails for
    a certain demographic? It’s easy to imagine where you could do *harm* with an
    app , even though those consequences were unintentional. Thus, it’s very important
    to be mindful of all of your potential users and have tools that help guide you
    through this.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当您创建一个解决问题的应用程序时，考虑应用程序存在可能引发的问题是很重要的。您可能会开发像鸟鸣检测器这样无害的东西，用于根据它们发出的声音对鸟类进行分类。但这可能如何影响您的用户呢？如果您的数据仅限于某一特定地区常见的鸟类，而该地区主要由单一人口统治，那会怎样？您可能会无意识地开发了一个只能供特定人口群体使用的应用程序。这是您想要的吗？对于这样的应用程序，也可能会引起可访问性问题。如果您的概念是您听到一只鸟在唱歌，然后您希望识别它……您假设这个人*能够*听到鸟叫声，因此您并未考虑到听力受损或无听力的人群。虽然这只是一个非常琐碎的例子，但将这一概念扩展到可以深刻影响某人生活的应用程序或服务。如果您的共享乘车应用程序避开了某些社区，从而排斥了某些人？如果您的应用程序对健康有所帮助，比如帮助管理药物，但对某一特定人群失败了呢？可以很容易地想象您如何通过应用程序造成*伤害*，即使这些后果是无意识的。因此，非常重要的是要对所有潜在用户保持警惕，并具备帮助指导您的工具。
- en: It’s impossible to predict all scenarios where you could introduce a bias unintentionally,
    so, with that in mind, Google has prepared the [*People + AI Guidebook*](https://oreil.ly/enDYK).
    The guidebook has six chapters, taking you from understanding user needs and defining
    success, all the way through data preparation, model building, gathering feedback
    fairly, and more. It’s particularly useful in helping you understand *which* types
    of problems AI is uniquely positioned to solve. I strongly recommend referring
    to this before you start coding any app!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 不可能预测所有可能无意中引入偏见的情景，因此，在这种情况下，谷歌已准备好了[*人+AI指南*](https://oreil.ly/enDYK)。该指南共有六章，从理解用户需求和定义成功开始，一直到数据准备、模型构建、公平收集反馈等。它在帮助您理解AI独特解决问题类型方面特别有用。我强烈建议在开始编写任何应用程序之前参考这本指南！
- en: The people who created this book also have a set of [AI Explorables](https://oreil.ly/ldhCV),
    which give you interactive workbooks that help you find issues such as hidden
    bias in your data. These will lead you through understanding core scenarios not
    just in your data, but in how your model behaves after being trained on that data.
    These can help you come up with a strategy for testing your models postdeployment.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由创建本书的人们也拥有一套[AI可探索资源](https://oreil.ly/ldhCV)，这些资源提供了交互式工作手册，帮助您发现数据中的隐藏偏见等问题。这些资源将引导您理解核心场景，不仅限于数据本身，还包括模型在训练后的行为。这些资源可以帮助您制定上线后模型测试的策略。
- en: Once you’ve defined and understood your problem, and eliminated potential sources
    of bias within it, the next step is to construct and prepare the data you’ll use
    in your system. Again, this is a way that biases may be unintentionally introduced.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您定义和理解了问题，并消除了其中潜在的偏见来源，下一步就是构建和准备您将在系统中使用的数据。同样，这也是可能无意中引入偏见的方式。
- en: Note
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Often bias in AI is solely attributed to the data that was used to train the
    model. While data is often the primary suspect, it’s not the sole one. Bias can
    creep in through feature engineering, transfer learning, and a myriad of other
    ways. You’ll often be “sold” on fixing your data to fix the bias, but one cannot
    simply clean up their data and declare victory. Keep this in mind as you create
    your systems. We’ll focus a lot on data in this chapter, as that’s where generic
    tools are possible, but again, try to avoid the mindset that bias is only introduced
    via data!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 通常认为 AI 中的偏见仅仅归因于用于训练模型的数据。虽然数据通常是主要嫌疑人，但并不是唯一的。偏见可以通过特征工程、迁移学习以及其他许多方式渗入。通常你会被“出售”，告诉你通过修复数据来解决偏见，但不能简单地清理数据并宣布胜利。在设计系统时要记住这一点。在本章中我们将重点关注数据，因为这是可能的通用工具所在，但再次强调，避免认为偏见仅仅是通过数据引入的心态！
- en: Avoiding Bias in Your Data
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 避免数据中的偏见
- en: Not all data biases are easy to spot. I once attended a student competition
    where the participants took on the challenge of image generation using generative
    adversarial networks (GANs) to predict what the lower half of a face looks like
    based on the upper half of the face. It was prior to the COVID-19 pandemic, but
    still flu season in Japan, and many people would wear face masks to protect themselves
    and others.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有的数据偏见都容易发现。我曾参加过一个学生竞赛，参赛者利用生成对抗网络（GANs）挑战图像生成，预测基于面部上半部分的下半部分面貌。那时还未爆发
    COVID-19 疫情，但在日本依然是流感季节，很多人会戴口罩来保护自己和他人。
- en: The idea was to see if one could predict the face below the mask. For this task
    they needed access to facial data, so they used the IMDb dataset of [face images
    with age and gender labels](https://oreil.ly/wR5Vl). The problem? Given that the
    source is IMDb, the vast majority of the faces in this dataset are *not* Japanese.
    As such, their model did a great job of predicting *my* face, but not their own.
    In the rush to produce an ML solution when there wasn’t adequate data coverage,
    the students produced a biased solution. This was just a show-and-tell competition,
    and their work was brilliant, but it was a great reminder that rushing to market
    with an ML product when one isn’t necessarily needed, or when there isn’t sufficient
    data to build a proper model, can lead you down the road of building biased models
    and incurring heavy future technical debt.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 初始想法是看是否能预测口罩下面的面部。为此，他们需要访问面部数据，因此使用了 IMDb 的数据集，其中包含带有年龄和性别标签的 [面部图像](https://oreil.ly/wR5Vl)。问题在于？考虑到数据源是
    IMDb，这个数据集中绝大部分面孔都*不是*日本人。因此，他们的模型在预测*我的*面部时表现出色，但却不能预测他们自己的面部。在没有足够数据覆盖的情况下急于推出机器学习解决方案，这些学生产生了一个带有偏见的解决方案。这只是一个展示性的比赛，他们的工作非常出色，但它提醒我们，当并不需要或者没有足够数据来构建正确模型时，急于推出机器学习产品可能导致构建带有偏见的模型并造成未来严重的技术债务。
- en: It won’t always be as easy as that to spot potential biases, and there are many
    tools out there to help you avoid it. I’d like to look into a couple of freely
    available ones next.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要发现潜在的偏见并不总是那么容易，市面上有许多工具可以帮助你避免这种情况。接下来我想介绍几款免费的工具。
- en: The What-If Tool
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么如果工具
- en: One of my favorites is the What-If Tool from Google. Its aim is to let you inspect
    an ML model with minimal coding required. With this tool, you can inspect the
    data and the output of the model for that data together. It has a [walkthrough](https://oreil.ly/dX7Qm)
    that uses a model based on about 30,000 records from the 1994 US Census dataset
    that is trained to predict what a person’s income might be. Imagine, for example,
    that this is used by a mortgage company to determine whether a person may be able
    to pay back a loan, and thus to determine whether or not to grant them the loan.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我最喜欢的之一是谷歌的 What-If 工具。它的目的是让您无需编写大量代码即可检查机器学习模型。通过这个工具，您可以同时检查数据和模型对数据的输出。它有一个[演练](https://oreil.ly/dX7Qm)，使用的是基于
    1994 年美国人口普查数据集的大约 30,000 条记录训练的模型，用于预测一个人的收入可能是多少。例如，假设这被一个抵押贷款公司用来确定一个人是否有能力偿还贷款，从而决定是否向其发放贷款。
- en: One part of the tool allows you to select an inference value and see the data
    points from the dataset that led to that inference. For example, consider [Figure 15-1](#using_the_what_if_tool).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 工具的一部分允许你选择一个推断值并查看导致该推断的数据点。例如，考虑 [图 15-1](#using_the_what_if_tool)。
- en: This model returns a probability of low income from 0 to 1, with values below
    0.5 indicating high income and others low. This user had a score of 0.528, and
    in our hypothetical mortgage application scenario could be rejected as having
    too low an income. With the tool, you can actually change some of the user’s data—for
    example, their age—and see what the effect on the inference would be. In the case
    of this person, changing their age from 42 to 48 gave them a score on the other
    side of the 0.5 threshold, and as a result changed them from being a “reject”
    on the loan application to an “accept.” Note that nothing else about the user
    was changed—just their age. This gives a strong signal that there’s a potential
    age bias in the model.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型返回一个从0到1的低收入概率，值低于0.5表示高收入，其他表示低收入。此用户得分为0.528，在我们的假设抵押申请场景中可能因收入太低而被拒绝。使用该工具，您可以实际改变用户的某些数据，例如他们的年龄，然后查看推断结果的影响。在这个例子中，将他们的年龄从42岁改变到48岁，使他们的得分超过0.5的阈值，结果从“拒绝”变为“接受”。请注意，用户的其他信息没有改变——只改变了年龄。这表明模型可能存在年龄偏见。
- en: The What-If Tool allows you to experiment with various signals like this, including
    details like gender, race, and more. To prevent a one-off situation being the
    tail that wags the dog, causing you to change your entire model to prevent an
    issue that lies with one customer and not the model itself, the tool includes
    the ability to find the nearest counterfactuals—that is, it finds the closest
    set of data that results in a different inference so you can start to dive into
    your data (or model architecture) in order to find biases.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: What-If工具允许您尝试各种信号，包括性别、种族等细节。为了避免因为一个特定情况而改变整个模型，而实际问题出在某个客户身上而不是模型本身，该工具包括寻找最近的反事实情形的能力——也就是说，它会找到最接近的一组数据，这些数据会导致不同的推断结果，因此您可以开始深入挖掘您的数据（或模型架构）以找出偏见。
- en: I’m just touching the surface of what the What-If Tool can do here, but I’d
    strongly recommend checking it out. There are lots of examples of what you can
    do with it [on the site](https://oreil.ly/kgZkZ). At its core—as the name suggests—it
    gives you tools to test “what if” scenarios before you deploy. As such, I believe
    it can be an essential part of your ML toolbox.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我只是在这里初探What-If工具的功能表面，但强烈建议您去了解一下。您可以在[该网站](https://oreil.ly/kgZkZ)上找到很多使用示例。其核心功能正如其名，它提供了在部署前测试“假设情景”的工具。因此，我相信它可以成为您机器学习工具箱中不可或缺的一部分。
- en: '![](assets/aiml_1501.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1501.png)'
- en: Figure 15-1\. Using the What-If Tool
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-1。使用What-If工具
- en: Facets
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Facets
- en: '[Facets](https://oreil.ly/I1x6L) is a tool that can work in complement to the
    What-If Tool to give you a deep dive into your data through visualizations. The
    goal of Facets is to help you understand the distribution of values across features
    in your dataset. It’s particularly useful if your data is split into multiple
    subsets for training, testing, validation, or other uses. In such cases, you can
    easily end up in a situation where data in one split is skewed in favor of a particular
    feature, leading you to having a faulty model. This tool can help you determine
    whether you have sufficient coverage of each feature for each split.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[Facets](https://oreil.ly/I1x6L)是一个可以与What-If工具协同工作的工具，通过可视化技术深入挖掘您数据的工具。Facets的目标是帮助您理解数据集中特征值的分布情况。如果您的数据被分割成多个子集用于训练、测试、验证或其他用途，这将特别有用。在这种情况下，您可能会发现一个子集中的数据偏向于某个特定特征，从而导致模型出现故障。该工具可以帮助您确定每个子集中每个特征的覆盖是否足够。'
- en: For example, using the same US Census dataset as in the previous example with
    the What-If Tool, a little examination shows that the training/test splits are
    very good, but use of the capital gain and capital loss features might have a
    skewing effect on the training. Note in [Figure 15-2](#using_facets_to_explore_a_dataset),
    when inspecting quantiles, that the large crosses are very well balanced across
    all of the features except these two. This indicates that the majority of the
    data points for these values are zeros, but there are a few values in the dataset
    that are much higher. In the case of capital gain, you can see that 91.67% of
    the training set is zeros, with the other values being close to 100k. This might
    skew your training, and can be seen as a debugging signal. This could introduce
    a bias in favor of a very small part of your population.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用与前一个示例中相同的美国人口普查数据集和 What-If 工具，稍加检查即可发现训练/测试分割非常良好，但使用资本增益和资本损失特征可能会对训练产生偏倚影响。请注意在
    [图 15-2](#using_facets_to_explore_a_dataset) 中检查分位数时，除这两个特征外，大的十字交叉点在所有特征上非常平衡。这表明这些值的大多数数据点为零，但数据集中有少量远高于零的值。以资本增益为例，你可以看到训练集中有
    91.67% 的数据点为零，其余值接近 100k。这可能会导致训练偏倚，并可视为调试信号。这可能会引入偏向于人群中极小部分的偏见。
- en: '![](assets/aiml_1502.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1502.png)'
- en: Figure 15-2\. Using Facets to explore a dataset
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-2\. 使用 Facets 探索数据集
- en: Facets also includes a tool called Facets Dive that lets you visualize the contents
    of your dataset according to a number of axes. It can help identify errors in
    your dataset, or even preexisting biases that exist in it so you know how to handle
    them. For example, consider [Figure 15-3](#a_deep_dive_with_facets), where I split
    the dataset by target, education level, and gender.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Facets 还包括一个称为 Facets Dive 的工具，可以根据多个轴向可视化数据集的内容。它可以帮助识别数据集中的错误，甚至是现有的偏见，以便你知道如何处理它们。例如，考虑
    [图 15-3](#a_deep_dive_with_facets)，我在这里通过目标、教育水平和性别对数据集进行了分割。
- en: 'Red means “high income predicted,” and left to right are levels of education.
    In almost every case the probability of a male having a high income is greater
    than a female, and in particular with higher levels of education the contrast
    becomes stark. Look for example at the 13–15 column (which is the equivalent of
    a bachelor’s degree): the data shows a far higher percentage of men being high
    earners than women with the same education level. While there are many other factors
    in the model to determine earning level, having such a disparity for highly educated
    people is a likely indicator of bias in the model.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 红色表示“高收入预测”，从左到右是教育水平。几乎每种情况下男性高收入的概率都大于女性，特别是在更高教育水平下，这种对比变得非常明显。例如看看 13-15
    列（相当于学士学位）：数据显示相同教育水平下男性高收入的比例远高于女性。虽然模型中还有许多其他因素来确定收入水平，但对高度受教育人群出现这种差异很可能是模型中偏见的一个指标。
- en: '![](assets/aiml_1503.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1503.png)'
- en: Figure 15-3\. A deep dive with Facets
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-3\. 使用 Facets 进行深入分析
- en: To help you identify features such as these, along with the What-If Tool, I
    strongly recommend using Facets to explore your data and your model’s output.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助你识别这类特征，以及使用 What-If 工具，我强烈建议使用 Facets 探索你的数据和模型输出。
- en: TensorFlow Model Card Toolkit
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow 模型卡工具包
- en: If you intend to publish your models for other people to use, and want to be
    transparent about the data that was used to build the model, the TensorFlow Model
    Card Toolkit can help. The goal of this toolkit is to provide context and transparency
    into the metadata about your model. The toolkit is fully open-sourced, so you
    can explore how it works, and available at [*https://github.com/tensorflow/model-card-toolkit*](https://github.com/tensorflow/model-card-toolkit).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打算发布你的模型供他人使用，并希望透明地展示用于构建模型的数据，TensorFlow 模型卡工具包可以帮助你。该工具包的目标是提供关于模型元数据的背景信息和透明度。该工具包完全开源，因此你可以探索其工作原理，地址为
    [*https://github.com/tensorflow/model-card-toolkit*](https://github.com/tensorflow/model-card-toolkit)。
- en: To explore a trivial example of a model card, you might be familiar with the
    famous cats versus dogs computer vision training example. A model card produced
    for this could look something like [Figure 15-4](#model_card_for_cats_versus_dogs),
    where transparency about the model is published. While this model is very simple,
    the cards show the data split as an example, and it’s clear that there are more
    dogs in the dataset than cats, so a bias is introduced. Also, the folks that produced
    the model, who have expertise in it, were able to share other ethical considerations,
    such as the fact that it *assumes* the image will always contain either a cat
    or a dog, and as such might be harmful if passed an image that contains neither.
    It could, for example, be used to insult a human, by classifying them as a cat
    or a dog. For me, personally, this was a major “a-ha!” moment, because being too
    close to teaching ML, I never considered that eventuality, and now need to ensure
    that it stays part of my workflow!
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要探索一个模型卡的简单示例，您可能熟悉著名的猫狗计算机视觉训练示例。为此制作的模型卡可能看起来像 [Figure 15-4](#model_card_for_cats_versus_dogs)，透明地公布了模型的情况。尽管这个模型非常简单，但卡片显示了数据分割的示例，清楚地显示数据集中狗的数量比猫多，因此引入了偏差。此外，制作该模型的专家还能分享其他伦理考虑，例如它
    *假设* 图像始终包含猫或狗，因此如果传递了不包含二者的图像，可能会有害。例如，它可以用来侮辱人类，将其分类为猫或狗。对我个人而言，这是一个重要的“啊哈！”时刻，因为在教授机器学习时，我从未考虑过这种可能性，现在需要确保它成为我的工作流的一部分！
- en: '![](assets/aiml_1504.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1504.png)'
- en: Figure 15-4\. Model card for cats versus dogs
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-4\. 猫狗模型卡
- en: A more complex model card, demonstrating a model that was trained to predict
    income based on demographic features, can be found on [GitHub](https://oreil.ly/LiEkn).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 可在 [GitHub](https://oreil.ly/LiEkn) 上找到一个更复杂的模型卡，展示了一个基于人口统计特征预测收入的模型。
- en: In it, you can see transparency around the demographics in both the training
    and evaluation sets, as well as quantitative analysis about the datasets. Thus,
    someone using this model is *prewarned* about biases that the model may introduce
    to their workflow and can mitigate accordingly.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在其中，您可以看到关于训练和评估集中人口统计信息的透明度，以及关于数据集的定量分析。因此，使用此模型的人可以 *预先警告* 关于模型可能引入其工作流的偏见，并可以相应地进行缓解。
- en: TensorFlow data validation
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow 数据验证
- en: If you use TensorFlow Extended (TFX), and have your data in a TFX pipeline,
    there are components within TFX to analyze and transform it. It can help you find
    things like missing data, such as features with empty labels, or with values outside
    the ranges you might expect and other anomalies. Going into TensorFlow Data Validation
    is beyond the scope of this book, but you can learn more by looking through [the
    TFDV guide](https://oreil.ly/7qydA).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用 TensorFlow Extended（TFX），并且在 TFX 管道中拥有数据，则有 TFX 组件可以分析和转换它。它可以帮助您发现诸如缺失数据（例如具有空标签的特征）或超出您预期范围的值等其他异常情况。深入讨论
    TensorFlow 数据验证超出了本书的范围，但您可以通过查阅 [TFDV 指南](https://oreil.ly/7qydA) 了解更多信息。
- en: Building and Training Your Model
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建和训练您的模型
- en: Beyond exploring your data, and any models you might be deriving from as outlined
    above, there are considerations you can take into account when building and training
    your model. Again, each of these are super-detailed, and not all of them may apply
    to you. I won’t go into them in detail here, but I’ll link you to resources where
    you can learn more.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索数据及以上提到的任何模型之外，建立和训练您的模型时可以考虑的因素。再次强调，这些细节非常详细，并非所有内容都适用于您。我不会在这里详细介绍它们，但我会为您提供可以了解更多信息的资源链接。
- en: Model remediation
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型修复
- en: When you create your model, you may introduce biases to outcomes that result
    from the use of that model. One source of this is that your model may underperform
    on certain slices of data. This can be massively harmful. For example, what if
    you build a model for disease diagnosis that performs extremely well for males
    and females, but not so well on people who don’t express a gender or are nonbinary,
    due to lack of data for these classes. There are typically three methods to fix
    this—changing the input data, intervening on the model by updating the architecture,
    or postprocessing your results. A process called *MinDiff* can be used to equalize
    distributions of data, and balance error rates across slices of your data. Thus,
    *while training*, differences in distributions can be brought closer together,
    so that the outcome of a future prediction can be more equitable across the slices
    of data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当您创建您的模型时，可能会引入导致模型使用结果偏见的偏见。这其中的一个来源是，您的模型可能在某些数据片段上表现不佳。这可能会造成巨大的伤害。例如，假设您为疾病诊断建立了一个模型，该模型对男性和女性表现非常出色，但对不表达性别或非二元性别的人表现不佳，原因是这些类别的数据较少。通常有三种方法可以解决这个问题——改变输入数据，通过更新架构对模型进行干预，或者对结果进行后处理。可以使用一种称为*MinDiff*的过程来使数据分布均衡，并在数据的各个片段之间平衡错误率。因此，在*训练*过程中，可以使分布差异变得更加接近，从而在未来的预测结果中，可以在数据的各个片段之间更加公平地进行。
- en: So, for example, consider [Figure 15-5](#using_mindiff). On the left are the
    prediction scores for two different slices of data where the MinDiff algorithm
    was *not* applied during training. The result was that the predicted outcomes
    were vastly different. On the right the same prediction curves are overlaid, but
    they’re much closer to each other.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，例如，考虑[图15-5](#using_mindiff)。左边是未在训练过程中应用MinDiff算法的两个不同数据片段的预测分数。结果是预测的结果差异很大。右边重叠了相同的预测曲线，但它们彼此更加接近。
- en: '![](assets/aiml_1505.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1505.png)'
- en: Figure 15-5\. Using MinDiff
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-5\. 使用MinDiff
- en: This technique is something worth exploring, and a detailed tutorial is available
    on the [TensorFlow website](https://oreil.ly/3LgAl).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术值得探索，详细的教程可以在[TensorFlow网站](https://oreil.ly/3LgAl)找到。
- en: Model privacy
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型隐私
- en: In some cases, a smart attacker can use your model to infer some of the data
    that the model was trained on. One method to prevent this is to train models using
    *differential privacy*. The idea behind differential privacy is to prevent an
    observer using the output from telling if a particular piece of information was
    used in the computation. For example, on a model that is trained to infer salary
    from demographics, if an attacker knows that a person is in the dataset, they
    may know the demographics for that person, enter it into the model, and given
    that their salary was in the training set, expect a very accurate value for their
    salary. Or, for example, if a model was created using health metrics, an attacker
    could potentially, knowing that their neighbor is in the training set, use a portion
    of the data to derive more data about their target.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，聪明的攻击者可以利用您的模型推断出模型训练所使用的一些数据。预防此类问题的一种方法是使用*差分隐私*训练模型。差分隐私的理念是防止观察者利用输出来判断某个特定信息是否用于计算中。例如，在一个模型上，该模型经过训练，可以从人口统计学中推断工资。如果攻击者知道某人在数据集中，他们可以知道该人的人口统计信息，然后将其输入模型中，由于他们的工资在训练集中，可以预期其工资的值非常精确。或者，例如，如果使用健康指标创建了一个模型，攻击者可能知道他们的邻居在训练集中，可以使用数据的一部分来推断关于他们目标的更多数据。
- en: With this in mind, [TensorFlow Privacy](https://oreil.ly/anZhq) provides implementations
    of optimizers for training models using differential privacy.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，[TensorFlow隐私](https://oreil.ly/anZhq)提供了使用差分隐私训练模型的优化器实现。
- en: Federated learning
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 联邦学习
- en: Perhaps most of interest to mobile developers but not widely available yet,
    is federated learning. In this case you can continually update and improve your
    model based on how your users are using it. As such, users are sharing their personal
    data with you in order to help you improve your model. One such use is having
    their keyboard autopredict the word that they’re typing. Everybody is different,
    so if I start typing “anti,” I might be typing antibiotic, or I might be typing
    “antidisestablishmentarianism,” and the keyboard should be smart enough to provide
    suggestions based on *my* previous usage. With that in mind, federated learning
    techniques have been created. The privacy implications here are obvious—you would
    want to be able to provide a way that your users can share very personal information—like
    the words they type—with you, in a way that cannot be misused.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对移动开发者最感兴趣但目前尚未广泛可用的可能是联合学习。在这种情况下，您可以根据用户的使用方式持续更新和改进您的模型。因此，用户正在与您分享他们的个人数据，以帮助您改进模型。一个这样的用例是使他们的键盘自动预测他们正在输入的单词。每个人都不同，所以如果我开始输入“anti”，我可能在输入抗生素，也可能在输入“反教会主义”，键盘应该足够智能，基于*我的*先前使用提供建议。考虑到这一点，联合学习技术应运而生。这里的隐私影响显而易见
    — 您需要能够以一种不会被滥用的方式提供给用户分享非常个人化的信息 — 例如他们键入的单词。
- en: As I mentioned, this is not yet available as an open API for you to use in your
    apps, but you can *simulate* how to do it with TensorFlow Federated.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我所提到的，这个API目前还不是您可以在应用程序中使用的开放API，但您可以使用 TensorFlow Federated 进行模拟。
- en: '[TensorFlow Federated](https://oreil.ly/GpiID) (TFF) is an open source framework
    that gives you federated learning functionality in a simulated server environment.
    At the time of writing it’s still experimental, but it’s worth looking into. TFF
    is designed with two core APIs. The first is the Federated Learning API, which
    gives you a set of interfaces that add federated learning and evaluation capabilities
    to your existing models. It allows you to, for example, define distributed variables
    that are impacted by learned values from distributed clients. The second is the
    Federated Core API, which implements the federated communication operations within
    a functional programming environment. It’s the foundation for existing deployed
    scenarios such as the Google keyboard, [Gboard](https://oreil.ly/csPTi).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[TensorFlow Federated](https://oreil.ly/GpiID)（TFF）是一个开源框架，在模拟服务器环境中提供联合学习功能。目前仍处于实验阶段，但值得关注。TFF
    设计有两个核心 API。第一个是联合学习 API，为现有模型添加联合学习和评估能力提供一组接口。例如，它允许您定义受分布式客户端学习值影响的分布式变量。第二个是联合核心
    API，在函数式编程环境中实现了联合通信操作。它是现有部署场景的基础，如 Google 键盘[Gboard](https://oreil.ly/csPTi)。'
- en: Evaluating Your Model
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估您的模型
- en: In addition to the tools mentioned above that can be used to evaluate your model
    during the training and deployment process, there are several others that are
    worth exploring.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述在训练和部署过程中用于评估模型的工具之外，还有几个其他值得探索的工具。
- en: Fairness Indicators
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 公平性指标
- en: The Fairness Indicators tool suite is designed to enable computation and visualization
    of commonly identified fairness metrics in classification models—such as false
    positives and false negatives—with a view to comparing performance in these across
    different data slices. It’s integrated into the What-If Tool as outlined earlier
    if you want to get started playing with it. You can also use it standalone by
    using the open source [fairness-indicators package](https://oreil.ly/I9A2f).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 公平性指标工具套件旨在计算和可视化分类模型中常见的公平性指标，如假阳性和假阴性，以比较不同数据切片中的性能。如前所述，它已集成到 What-If 工具中，如果您想要开始使用它，也可以单独使用开源的
    [fairness-indicators package](https://oreil.ly/I9A2f)。
- en: So, for example, when using Fairness Indicators to explore the false negative
    rate in a model that was trained on human labeling of comments where the human
    attempted to label that the comment was written by male, female, transgender,
    or other creators, the lowest error was amongst males and the highest amongst
    “other gender.” See [Figure 15-6](#fairness_indicators_for_gender_inferenc). Female
    and other gender also showed above the overall rate.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，例如，当使用公平性指标来探索在基于人类评论标记的模型中的假阴性率时，人类试图标记评论是由男性、女性、跨性别者或其他创作者写的，最低的错误率出现在男性中，最高的则出现在“其他性别”中。参见[图 15-6](#fairness_indicators_for_gender_inferenc)。
- en: '![](assets/aiml_1506.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1506.png)'
- en: Figure 15-6\. Fairness Indicators for gender inference from text model
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-6\. 从文本模型中推断性别的公平性指标
- en: When looking at false positives in the same model with the same data, in [Figure 15-7](#false_positives_presented_by_fairness_i),
    the results were flipped. The model is more likely to give a false positive about
    male or transgender.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当在相同模型和相同数据中查看假阳性时，在[图 15-7](#false_positives_presented_by_fairness_i)中，结果发生了翻转。该模型更有可能对男性或跨性别者做出假阳性判断。
- en: '![](assets/aiml_1507.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1507.png)'
- en: Figure 15-7\. False positives presented by Fairness Indicators
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-7\. Fairness Indicators 提示的假阳性
- en: With this tool, you can explore your model and tweak the architecture, learning,
    or data to try to balance it out. You can explore this example for yourself at
    [*https://github.com/tensorflow/fairness-indicators*](https://github.com/tensorflow/fairness-indicators).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此工具，您可以探索您的模型并调整架构、学习或数据，以尝试平衡它。您可以在[*https://github.com/tensorflow/fairness-indicators*](https://github.com/tensorflow/fairness-indicators)中自行探索此示例。
- en: TensorFlow Model Analysis
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow 模型分析
- en: TensorFlow Model Analysis (TFMA) is a library that’s designed to evaluate TensorFlow
    models. At the time of writing, it’s in prerelease stage, so it may change by
    the time you’re reading this! Details about how to use it and how to get started
    are available on the [TensorFlow website](https://oreil.ly/oduzl). It’s particularly
    useful in letting you analyze slices of training data and how the model would
    perform on them.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 模型分析（TFMA）是一个旨在评估 TensorFlow 模型的库。在撰写本文时，它处于预发布阶段，所以在您阅读本文时可能会有所改变！有关如何使用它和如何入门的详细信息，请访问[TensorFlow
    网站](https://oreil.ly/oduzl)。它特别有助于您分析训练数据的切片以及模型在这些数据上的表现。
- en: Language Interpretability Toolkit
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语言可解释性工具包
- en: If you’re building models that use language, the Language Interpretability Tool
    (LIT) will help you understand things like the types of examples your model performs
    poorly on, or the signals that drove a prediction helping you to determine undesirable
    training data or adversarial behavior. You can also test model consistency if
    you change things like the style of text, verb tenses, or pronouns. Details on
    how to set it up and use it are available at [*https://pair-code.github.io/lit/tutorials/tour*](https://pair-code.github.io/lit/tutorials/tour/).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在构建使用语言的模型，语言可解释性工具（LIT）将帮助你理解像是你的模型在哪些类型的示例上表现不佳，或是推动预测的信号，帮助你确定不良训练数据或对抗行为。您还可以测试模型的一致性，如果更改文本的风格、动词时态或代词等方面。如何设置和使用它的详细信息请参见[*https://pair-code.github.io/lit/tutorials/tour*](https://pair-code.github.io/lit/tutorials/tour/)。
- en: Google’s AI Principles
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Google 的 AI 原则
- en: 'TensorFlow was created by Google’s engineers as an outcropping of many existing
    projects built by the company for its products and internal systems. After it
    was open sourced, many new avenues for machine learning were discovered, and the
    pace of innovation in the fields of ML and AI is staggering. With this in mind,
    Google decided to put out a [public statement](https://oreil.ly/OAqyB) outlining
    its principles with regard to how AI should be created and used. They’re a great
    guideline for responsible adoption and worth exploring. In summary, the principles
    are:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 是由 Google 的工程师们创建的，是公司产品和内部系统中许多现有项目的一部分。在它开源后，发现了许多机器学习领域的新方向，ML
    和 AI 领域的创新步伐是惊人的。考虑到这一点，Google 决定发布一份[公开声明](https://oreil.ly/OAqyB)，概述了其关于如何创建和使用
    AI 的原则。这些原则是负责任采用的重要指南，值得深入探讨。总结起来，这些原则包括：
- en: Be socially beneficial
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对社会有益
- en: Advances in AI are transformative, and as that change happens the goal is to
    take into account all social and economic factors, proceeding only where the overall
    likely benefits outstrip the foreseeable risks and downsides.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 'AI 的进展是变革性的，随着这种变化的发生，目标是考虑所有社会和经济因素，只有在总体上可能的收益超过可预见的风险和不利因素时才进行。 '
- en: Avoid creating or reinforcing unfair bias
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 避免创建或加强不公平的偏见
- en: As discussed in this chapter, bias can easily creep into any system. AI—particularly
    in cases where it transforms industry—presents an opportunity to *remove* existing
    biases, as well as to ensure that *new* biases don’t arise. One should be mindful
    of this.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章讨论的那样，偏见可能会悄悄地侵入任何系统中。AI——尤其是在其转变行业的情况下——提供了消除现有偏见以及确保不会产生新偏见的机会。我们应当对此保持警觉。
- en: Be built and tested for safety
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 被构建和测试以确保安全
- en: Google continues to develop strong safety and security practices to avoid unintended
    harm from AI. This includes developing AI technologies in constrained environments
    and continually monitoring their operation after deployment.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌继续开发强大的安全和保护实践，以避免人工智能带来的意外伤害。这包括在受限环境中开发AI技术，并在部署后持续监控其运行。
- en: Be accountable to people
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对人民负责。
- en: The goal is to build AI systems that are subject to appropriate human direction
    and control. This means that appropriate opportunities for feedback, appeal, and
    relevant explanations must always be provided. Tooling to enable this will be
    a vital part of the ecosystem.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是建立受适当人类指导和控制的人工智能系统。这意味着必须始终提供适当的反馈、申诉和相关解释的机会。能够实现这一点的工具将成为生态系统的重要组成部分。
- en: Incorporate privacy design principles
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 结合隐私设计原则。
- en: AI systems must incorporate safeguards that ensure adequate privacy and inform
    users of how their data will be used. Opportunities for notice and consent should
    be obvious.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能系统必须包含确保充分隐私和告知用户其数据使用方式的保障措施。提供通知和同意的机会应当是明显的。
- en: Uphold high standards of scientific excellence
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 坚持高标准的科学卓越。
- en: Technological innovation is at its best when it is done with scientific rigor
    and a commitment to open enquiry and collaboration. If AI is to help unlock knowledge
    in critical scientific domains, it should aspire to the high standards of scientific
    excellence that are expected in those areas.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当科技创新以科学严谨和开放调查与合作的承诺进行时，其表现最佳。如果人工智能要帮助揭示关键科学领域的知识，它应当朝着那些领域期望的高科学卓越标准努力。
- en: Be made available for uses that accord with these principles
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 符合这些原则的使用方式应该是可用的。
- en: While this point might seem a little meta, it’s important to reinforce that
    the principles don’t stand alone, nor are they just for the people building systems.
    They’re also intended to give guidelines for how the systems you build can be
    used. It’s good to be mindful of how someone might use your systems in a way you
    didn’t intend, and as such good to have a set of principles for your users too!
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这一点可能有点抽象，但重要的是要强调这些原则不是孤立存在的，也不仅仅是给构建系统的人。它们也旨在为您构建的系统如何使用提供指导。要注意某人可能会以您未曾预料的方式使用您的系统，因此为您的用户制定一套原则也很重要！
- en: Summary
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结。
- en: And that’s where this waypoint on your journey to become an AI and ML engineer
    for mobile and web comes to an end, but your real journey of building solutions
    that can change the world may really begin. I hope this book was useful for you,
    and while we didn’t go very deep into any particular topics, we were able to encapsulate
    and simplify some of the complexity of bridging the worlds of machine learning
    and mobile development.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是您成为移动和Web AI和ML工程师旅程中的一个航标的结束，但您真正构建可以改变世界的解决方案的旅程可能才刚刚开始。希望这本书对您有所帮助，虽然我们没有深入探讨任何特定主题，但我们能够概括和简化机器学习与移动开发领域之间的复杂性。
- en: It’s my firm belief that if AI is to reach its full, positive potential, it
    will be through its use in low-powered, small models that are focused on solving
    common problems. While research has been going bigger and bigger, I think that
    the real growth potential that everyone can take advantage of is in models that
    are smaller and smaller, and this book gives you the platform where you can see
    how to take advantage of that!
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我坚信，如果人工智能要实现其全部的积极潜力，关键在于使用低功率、小型模型，专注于解决常见问题。尽管研究越来越大，但我认为每个人都可以利用的真正增长潜力在于越来越小的模型，而这本书为你提供了一个平台，可以看到如何利用这一点！
- en: I’m looking forward to seeing what you build, and I’d love it if you gave me
    the opportunity to share that with the world. Reach me on Twitter @lmoroney.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我期待看到您构建的作品，并且如果您能给我分享的机会，我会非常乐意。在Twitter上联系我 @lmoroney。
