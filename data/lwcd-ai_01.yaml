- en: Chapter 1\. How Data Drives Decision Making in Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一章。数据如何在机器学习中驱动决策制定
- en: This chapter explores the role of data in the enterprise and its influence on
    business decision making. You also learn the components of a machine learning
    (ML) workflow. You may have seen many books, articles, videos, and blogs begin
    any discussion of the ML workflow with the gathering of data. However, before
    data is gathered, you need to understand what kind of data to gather. This *data
    understanding* can only be achieved by knowing what kind of problem you need to
    solve or decision you need to make.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了数据在企业中的角色及其对业务决策制定的影响。您还将了解机器学习（ML）工作流程的组成部分。您可能已经看过许多书籍、文章、视频和博客，它们在讨论ML工作流程时通常从数据收集开始。但是，在收集数据之前，您需要了解要收集什么类型的数据。这种
    *数据理解* 只能通过了解您需要解决的问题或需要做出的决定来实现。
- en: Business case/problem definition and data understanding can then be used to
    formulate a no-code or low-code ML strategy. A no-code or low-code strategic approach
    to ML projects has several advantages/benefits. As mentioned in the introduction,
    a no-code AutoML approach enables anyone with domain knowledge in their area of
    expertise and no coding experience to develop ML models quickly, without needing
    to write a single line of code. This is a fast and efficient way to develop ML
    applications. A low-code approach enables those with some coding or deep coding
    experience, to develop ML applications quickly because basic code is autogenerated—and
    any additional custom code can be added. But, again, any ML project must begin
    with defining a goal, use case, or problem.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 业务案例/问题定义和数据理解可以用来制定无代码或低代码的机器学习策略。无代码或低代码的机器学习项目战略方法具有几个优点/好处。正如介绍中所提到的，无代码自动ML方法使具有其专业领域知识但没有编码经验的任何人能够快速开发ML模型，而无需编写一行代码。这是开发ML应用程序的一种快速高效的方式。低代码方法使那些具有一些编码或深度编码经验的人能够快速开发ML应用程序，因为基本代码是自动生成的——并且可以添加任何额外的自定义代码。但是，任何ML项目都必须从定义目标、使用案例或问题开始。
- en: What Is the Goal or Use Case?
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标或使用情况是什么？
- en: 'Businesses, educational institutions, government agencies, and practitioners
    face many decisions that reflect real-world examples of ML. For example:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 企业、教育机构、政府机构和从业者面临许多反映ML实际应用的决策问题。例如：
- en: How can we increase patient engagement with our diabetes web app?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何增加我们的糖尿病网络应用程序中患者的参与度？
- en: How can we increase our student feedback numbers on course surveys?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何增加我们课程调查中学生反馈数量？
- en: How can we increase our speed in detecting cyberattacks against our company
    networks?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何提高我们在发现针对公司网络的网络攻击速度上的效率？
- en: Can we decrease the number of spam emails entering our email servers?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否可以减少进入我们电子邮件服务器的垃圾邮件数量？
- en: How do we decrease downtime on our manufacturing production line?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何减少我们制造生产线停机时间？
- en: How can we increase our customer retention rate?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何增加我们的客户保留率？
- en: How do we reduce our customer churn (customer attrition) rate?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何减少我们的客户流失（客户流失）率？
- en: 'In each of those examples, numerous data sources must be examined to determine
    what ML solution is most appropriate to solve the problem or aid in decision making.
    Let’s take the use case of reducing customer churn or loss rate—using a very simplistic
    example. *Churn prediction* is identifying customers that are most likely to leave
    your service or product. This problem falls into a supervised learning bucket
    as a classification problem with two classes: the “Churn-Yes” class and the “Churn-No”
    class.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些示例中，必须检查许多数据源，以确定什么样的机器学习解决方案最适合解决问题或帮助决策制定。让我们以减少客户流失或损失率的使用案例为例——使用一个非常简单的例子。
    *流失预测* 是识别最有可能离开您的服务或产品的客户。这个问题属于监督学习桶中的一个分类问题，有两个类别： “流失-是” 类和 “流失-否” 类。
- en: From a data source perspective, you may need to examine customer profile information
    (name, address, age, job title, employment statement), purchase information (purchases
    and billing history), interaction information (customer experiences interacting
    with your products [both digitally and physically]), your customer service teams,
    or your digital support services. Popular data sources of customer information
    are customer relationship management systems, system ecommerce analytics services,
    and customer feedback. In essence, everything the customer “touches” as a data
    point should be tracked and captured as a data source.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据源的角度来看，您可能需要检查客户档案信息（姓名、地址、年龄、职位、就业声明）、购买信息（购买和账单历史记录）、互动信息（客户与您的产品进行互动的体验[数字化和实体]）、您的客户服务团队或您的数字支持服务。客户信息的流行数据来源包括客户关系管理系统、系统电子商务分析服务和客户反馈。实质上，客户“接触”的一切作为数据点应该被跟踪和捕捉为数据源。
- en: The nature of the decision you must make is tied directly to the data you will
    need to gather to make that decision—which needs to be formulated into a problem
    statement. Let’s say you are in charge of marketing for a company that makes umbrellas,
    and the *business goal* is to increase sales. If you reduce the selling price
    of your existing umbrellas, can you predict how many umbrellas you will sell?
    [Figure 1-1](#data_elements_that_impact_a_price_redu) shows the data elements
    to consider for this option.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须做出的决策的性质直接与您需要收集的数据紧密相关，这些数据需要制定为问题陈述。假设您负责一家生产雨伞的公司的市场营销工作，*业务目标*是增加销量。如果您降低现有雨伞的销售价格，您能预测将销售多少把雨伞吗？[图 1-1](#data_elements_that_impact_a_price_redu)展示了考虑此选项的数据元素。
- en: '![Data elements that impact a price reduction strategy to increase sales](assets/lcai_0101.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![影响价格降低策略以增加销量的数据元素](assets/lcai_0101.png)'
- en: Figure 1-1\. Data elements that impact a price reduction strategy to increase
    sales.
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-1\. 影响价格降低策略以增加销量的数据元素。
- en: As you can see in this data-driven business illustration, your business goal
    (to increase sales) takes on a new dimension. You realize now that to understand
    a product price reduction, you need to include additional data dimensions aside
    from the selling price. You will need to know the rainy seasons in specific regions,
    population density, and whether your inventory is sufficient to meet the demand
    of a price reduction that will increase sales. You will also need to look at historical
    data versus data that can be captured in real time. Historical data is typically
    referred to as *batch*, whereas real-time data capture is typically called *streaming*.
    With these added dimensions, the business goal suddenly becomes a very complex
    problem as these additional columns may be required. For any organization, there
    could ostensibly exist dozens of discrete data sources—with each source requiring
    certain skills to understand the relationships between them. [Figure 1-2](#a_typical_business_data_and_ml_experien)
    is an illustration of this challenge.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在这个数据驱动的业务插图中所见，您的业务目标（增加销量）呈现出新的维度。您现在意识到，要理解产品价格降低，除了销售价格外，还需要包括其他数据维度。您需要了解特定地区的雨季情况、人口密度，以及您的库存是否足以满足通过降价增加销量的需求。您还需要查看历史数据与实时捕获的数据之间的差异。历史数据通常称为*批处理*，而实时数据捕获通常称为*流式处理*。随着这些额外的维度，业务目标突然变成了一个非常复杂的问题，因为可能需要这些附加列。对于任何组织来说，可能会存在数十个离散的数据源，每个数据源都需要特定的技能来理解它们之间的关系。[图 1-2](#a_typical_business_data_and_ml_experien)展示了这一挑战的插图。
- en: '![A typical business data and ML experience today](assets/lcai_0102.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![今天的典型业务数据和机器学习经验](assets/lcai_0102.png)'
- en: Figure 1-2\. A typical business data and ML experience today.
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-2\. 今天的典型业务数据和机器学习经验。
- en: So what is your use case here? It depends. You would need to undergo a business
    decision-making process, which is the process of making choices by asking questions,
    collecting data, and assessing alternative resolutions. Once you figure out the
    use case or business goal, you can use the same data to train machines to learn
    about your customer patterns, spot trends, and predict outcomes using AutoML or
    low-code AI. [Figure 1-3](#business_case_that_leads_to_predictions) shows our
    umbrella example as a business use case that then leads to data source determination,
    ML framework, and then a prediction.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这里的用例是什么？这取决于情况。您需要进行业务决策过程，这是通过提出问题、收集数据和评估备选方案来进行选择的过程。一旦确定用例或业务目标，您可以使用相同的数据训练机器以了解客户模式、发现趋势，并使用AutoML或低代码AI预测结果。[图1-3](#business_case_that_leads_to_predictions)显示我们的雨伞示例作为业务用例，然后导致数据源确定、ML框架，最终进行预测。
- en: '![Business case that leads to predictions using ML framework](assets/lcai_0103.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![导致使用ML框架进行预测的业务案例](assets/lcai_0103.png)'
- en: Figure 1-3\. Business case that leads to predictions using ML framework.
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-3\. 使用ML框架进行预测的业务案例。
- en: An Enterprise ML Workflow
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 企业ML工作流程
- en: While decision-making processes help you identify your problem or use case,
    it is the ML workflow that helps you implement the solution to your problem. This
    section presents a typical ML workflow. In our ongoing umbrella example, you could
    use your data to train an ML model using an AutoML service that provides a no-code
    solution for running unsupervised ML clustering. From there, you could examine
    *clusters* of data points to see what patterns were derived. Or, you could decide
    to simply focus on historical data so that you could predict a specific target
    based on a certain number of data input features. What would your enterprise ML
    workflow look like? Not surprisingly, it is data-driven and requires decision
    making in the process.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 决策过程帮助您识别问题或用例，而ML工作流帮助您实施解决方案。本节介绍了一个典型的ML工作流程。在我们持续讨论的雨伞示例中，您可以使用数据训练一个ML模型，使用提供无代码解决方案的AutoML服务来运行无监督ML聚类。从那里，您可以检查*数据点的聚类*以查看推导出的模式。或者，您可以决定仅仅专注于历史数据，以便基于一定数量的数据输入特征预测特定目标。您的企业ML工作流程将是什么样子呢？毫不奇怪，它是数据驱动的，并且在过程中需要决策。
- en: The ML workflow can be shown as a series of steps, and the steps can be combined
    into phases. [Figure 1-4](#ten_step_ml_workflow) shows the 10 steps, and then
    we briefly discuss each. Later chapters provide more detailed examples of each
    step.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ML工作流程可以显示为一系列步骤，并且这些步骤可以组合成阶段。[图1-4](#ten_step_ml_workflow)显示了10个步骤，我们稍后会简要讨论每个步骤。后续章节将提供每个步骤的更详细的示例。
- en: '![Ten-step ML workflow](assets/lcai_0104.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![十步ML工作流程](assets/lcai_0104.png)'
- en: Figure 1-4\. Ten-step ML workflow.
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-4\. 十步ML工作流程。
- en: '>'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '>'
- en: Defining the Business Objective or Problem Statement
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确定业务目标或问题陈述
- en: The ML workflow starts with defining a specific question or problem with a defined
    boundary. In this phase you are attempting to define scope and feasibility. The
    right question will lead you to what data is required and potential ways data
    must be prepared. It is important to note that any question that may arise in
    analyzing data can be grouped in one of the five ML categories as shown in [Table 1-1](#categories_of_analyzing_data).
    Let’s continue with our umbrella example.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ML工作流程始于定义具有明确边界的特定问题或问题。在此阶段，您试图定义范围和可行性。正确的问题将引导您确定需要的数据以及必须准备数据的潜在方法。值得注意的是，在分析数据时可能出现的任何问题都可以归类为[表1-1](#categories_of_analyzing_data)中显示的五种ML类别之一。让我们继续使用我们的雨伞示例。
- en: Table 1-1\. Categories of analyzing data
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 表1-1\. 分析数据的类别
- en: '| Algorithm/model | Problem or question |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 算法/模型 | 问题或疑问 |'
- en: '| --- | --- |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Regression problem | How many umbrellas do you expect to sell this month/season?
    |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 回归问题 | 本月/本季度预计销售多少把雨伞？ |'
- en: '| Classification problem | Did they buy straight umbrellas (A) or foldable
    umbrellas (B)? |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 分类问题 | 他们购买了直杆雨伞（A）还是可折叠雨伞（B）？ |'
- en: '| Clustering problem | How many straight umbrellas were sold by month or by
    region? |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 聚类问题 | 每月或每个地区销售了多少直杆雨伞？ |'
- en: '| Anomaly detection problem | Did the company sell more umbrellas in the Mojave
    Desert than in Portland, OR? |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 异常检测问题 | Mojave沙漠地区的雨伞销量是否高于俄勒冈州波特兰地区？ |'
- en: '| Reinforcement learning | Company policy is to only ship to customers with
    a balance owed of $500 or less. Can a manufacturing robot be trained to extract,
    package, load, and ship straight umbrellas to our customers based upon this policy?
    |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 强化学习 | 公司政策是仅向欠款不超过$500的客户发货。可以训练制造机器人根据此政策提取、包装、装载和发运直杆伞吗？ |'
- en: Data Collection
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据收集
- en: 'In the early part of the 21st century, companies, universities, and researchers
    typically relied on local servers/hard drives or data centers to host their database
    applications and store their data. Relying on on-premises data centers or even
    renting server space in a data center was costly: server infrastructure needed
    to be maintained, software needed to be updated, security patches had to be installed,
    physical hardware was swapped out, and so on. In some cases, large amounts of
    data were stored across a cluster of machines.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在21世纪初期，公司、大学和研究人员通常依赖本地服务器/硬盘或数据中心来托管其数据库应用程序并存储其数据。依赖本地数据中心或甚至在数据中心租用服务器空间是昂贵的：需要维护服务器基础设施，更新软件，安装安全补丁，更换物理硬件等。在某些情况下，大量数据存储在一组机器中。
- en: Today, to save on costs, enterprises and educational institutions have moved
    to the cloud to host their database applications and store their data. Cloud storage,
    a service offered by cloud vendors to store files, allows you to upload different
    file formats or can be configured to automatically receive files from different
    data sources. Because most ML models are trained using data from files, storing
    your data in a cloud storage *bucket* makes for easy data collection. Cloud storage
    buckets can be used for storing both structured and unstructured data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，为了节省成本，企业和教育机构已经转向云端来托管其数据库应用程序和存储其数据。云存储是云供应商提供的一种服务，用于存储文件，允许您上传不同的文件格式，或配置为自动接收来自不同数据源的文件。由于大多数机器学习模型是使用来自文件的数据进行训练的，将数据存储在云存储*存储桶*中可以实现轻松的数据收集。云存储桶可用于存储结构化和非结构化数据。
- en: Another option to store data files for data collection is [*GitHub*](https://github.com),
    a service designed for collaborating on coding projects. You can store data in
    the cloud for future use (for free), track changes, and make data publicly available
    for replication. This option has strict file size limits of 100 MB, but there
    is an option to use Git Large File Storage (LFS), an open source GitHub extension
    for versioning large files. Git LFS replaces large files such as datasets, audio
    samples, graphics, and videos with text pointers inside Git, while storing the
    file contents on a remote server like [GitHub.com](https://github.com) or [GitHub
    Enterprise](https://github.com/enterprise).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个存储数据文件以进行数据收集的选择是[*GitHub*](https://github.com)，这是一个设计用于协作编程项目的服务。您可以将数据存储在云中以备将来使用（免费），跟踪更改，并使数据公开可用以进行复制。这个选项有严格的文件大小限制为100
    MB，但有使用Git Large File Storage (LFS) 的选项，这是一个用于对大文件进行版本控制的GitHub开源扩展。Git LFS会在Git中用文本指针替换大文件（如数据集、音频样本、图形和视频），同时将文件内容存储在像[GitHub.com](https://github.com)或[GitHub
    Enterprise](https://github.com/enterprise)这样的远程服务器上。
- en: The challenge of data collection is compounded within large organizations, where
    many different types of operations management software such as enterprise resource
    planning, customer relationship management, and production systems exist and may
    run on different databases. Data may also need to be pulled from external sources
    in real time, such as Internet of Things (IoT) sensor devices from delivery trucks.
    Thus, organizations are challenged with collecting not only structured data, but
    also unstructured and semistructured data formats in batches or real time (streaming).
    [Figure 1-5](#goalsolidusproblem_flow_to_data_collect) shows various data elements
    that feed data collection for structured, unstructured, and semistructured data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 数据收集的挑战在大型组织中更加复杂，因为存在许多不同类型的运营管理软件，例如企业资源规划、客户关系管理和生产系统，并且可能在不同的数据库上运行。数据也可能需要实时从外部来源获取，例如交货卡车的物联网传感器设备。因此，组织面临的挑战不仅是收集结构化数据，还包括批处理或实时（流式处理）中的非结构化和半结构化数据格式。[图
    1-5](#goalsolidusproblem_flow_to_data_collect) 显示了为结构化、非结构化和半结构化数据提供数据收集的各种数据元素。
- en: '![Goal/problem flow to data collection](assets/lcai_0105.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![目标/问题流向数据收集](assets/lcai_0105.png)'
- en: Figure 1-5\. Goal/problem flow to data collection.
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-5\. 目标/问题流向数据收集。
- en: Note
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: It is possible to have streaming structured data. Structured versus unstructured
    is a property of data format. Streaming versus batch is a property of latency.
    [Chapter 2](ch02.html#data_is_the_first_step) presents more information on data
    format and properties.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 可能存在流式结构化数据。结构化与非结构化是数据格式的属性。流式与批处理是延迟的属性。[第2章](ch02.html#data_is_the_first_step)提供了关于数据格式和属性的更多信息。
- en: Data Preprocessing
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理
- en: To perform data cleanup you’ll need to deal with missing data values, duplicates,
    outlier data, formatting issues, or data that is inconsistent due to human error.
    This is because real-world data is raw and messy and filled with assumptions.
    One assumption could be that your data has a normal distribution, meaning that
    data is symmetrically distributed with no skew, and that most values cluster around
    a central region, with the frequency of the values decreasing further away from
    the center (mean or average).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行数据清理，您需要处理缺失的数据值、重复项、异常数据、格式问题或由于人为错误导致不一致的数据。这是因为现实世界的数据通常是原始和混乱的，充满了假设。一个假设可能是你的数据具有正态分布，这意味着数据对称分布且没有偏斜，大多数值聚集在中心区域，从中心（均值或平均数）向外值的频率逐渐减少。
- en: Suppose your data showed, for the first time, an increase in the number of umbrellas
    sold in August in Palm Springs, the California desert town. Would your data be
    normally distributed, or would this be considered an outlier? Would it skew the
    results of predictions for monthly umbrella sales in August? When data does not
    have a normal distribution, it needs to be *normalized*, made *normal* by grouping
    all the records in a range of [0,1] or [–1,1], for example. You normalize a dataset
    to make it easier and faster to train an ML model. Normalization is covered in
    [Chapter 7](ch07.html#training_custom_ml_models_in_python).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你的数据首次显示，八月份在加利福尼亚沙漠小镇帕姆斯普林斯售出的雨伞数量增加了。你的数据是否服从正态分布，还是被视为异常值？这是否会偏离对八月份雨伞销售预测结果的影响？当数据不服从正态分布时，需要进行*归一化*，即将所有记录分组到[0,1]或[-1,1]范围内，例如。你通过归一化数据集来更轻松、更快速地训练机器学习模型。归一化在[第7章](ch07.html#training_custom_ml_models_in_python)中有详细介绍。
- en: Note
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This min-max normalization example can have detrimental effects if there are
    outliers. For example, when scaling to [0,1], it essentially maps the outlier
    to 1 and squashes all other values to 0\. Addressing outliers and anomalies is
    beyond the scope of our book.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果存在异常值，这种最小-最大归一化的示例可能会产生不利影响。例如，当缩放到[0,1]时，它基本上将异常值映射为1，并将所有其他值压缩为0。处理异常值和异常情况超出了我们书本的范围。
- en: Thus, data preprocessing can mean normalizing the data (such that numeric columns
    in the dataset use a common scale) and *scaling* the data, which means transforming
    your data so that it fits within a specific range. Fortunately, normalization
    and standardization are easily performed in Python with just a [few simple lines
    of code](https://oreil.ly/50v98). [Figure 1-6](#three_images_showing_actualcomma_normal)
    shows actual data before and after normalization and standardization.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，数据预处理可以意味着对数据进行归一化（使数据集中的数值列使用共同的尺度）和*缩放*数据，即转换数据以适应特定范围。幸运的是，Python中只需几行[简单的代码](https://oreil.ly/50v98)就可以进行归一化和标准化。[图1-6](#three_images_showing_actualcomma_normal)显示了归一化和标准化前后的实际数据。
- en: '![Three images showing actual, normalized, and standardized data](assets/lcai_0106.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![实际数据、归一化和标准化数据的三张图片](assets/lcai_0106.png)'
- en: Figure 1-6\. Three images showing actual, normalized, and standardized data.
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-6\. 实际数据、归一化和标准化数据的三张图片。
- en: Note
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Collecting data from a single source may be a relatively straightforward process.
    However, if you are aggregating several data sources into one file, make sure
    that data formats match and that any assumptions regarding time-series data (or
    timestamp and date ranges needed for your ML model) are validated. A common assumption
    is that the data is stationary—that the statistical properties (mean, variance,
    etc.) do not change over time.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 从单一来源收集数据可能是一个相对简单的过程。但是，如果你要将几个数据源聚合到一个文件中，请确保数据格式匹配，并验证关于时间序列数据的任何假设（或者你的机器学习模型所需的时间戳和日期范围）。一个常见的假设是数据是静止的——即统计属性（均值、方差等）随时间不变。
- en: Data Analysis
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据分析
- en: Exploratory data analysis (EDA) is a process used to explore and analyze the
    structure of data. In this step, you are looking to discover trends, patterns,
    feature relevance, and correlations, such as how one variable (feature) might
    correlate with another. You must select relevant feature data for your ML model
    based on the type of problem you are trying to solve. The outcome of this step
    is a feature list of input variables that can potentially be used for ML. Our
    hands-on exercise using EDA can be found in [Chapter 6](ch06.html#using_bigquery_ml_to_train_a_linear_reg).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 探索性数据分析（EDA）是用于探索和分析数据结构的过程。在这一步骤中，您正在寻找趋势、模式、特征相关性以及诸如一个变量（特征）如何与另一个变量相关的信息。根据您尝试解决的问题类型，您必须选择相关的特征数据用于您的
    ML 模型。这一步骤的结果是一个输入变量的特征列表，可以潜在地用于 ML。我们在[第6章](ch06.html#using_bigquery_ml_to_train_a_linear_reg)中介绍的实际操作的
    EDA 可以找到。
- en: Figures [1-7](#seaborn_regplot_showing_that_more_energ) and [1-8](#seaborn_correlation_matrix_left_parenth)
    are a result of an EDA process plotted using Seaborn, a Python data visualization
    library (see [Chapter 6](ch06.html#using_bigquery_ml_to_train_a_linear_reg) for
    more detail on the dataset). [Figure 1-7](#seaborn_regplot_showing_that_more_energ)
    shows an *inverse* relationship between *x* and *y*. [Figure 1-8](#seaborn_correlation_matrix_left_parenth)
    shows a *heat map* (or correlation matrix) and illustrates that more energy is
    produced when temperatures are lower.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [1-7](#seaborn_regplot_showing_that_more_energ) 和 [1-8](#seaborn_correlation_matrix_left_parenth)
    是使用 Python 数据可视化库 Seaborn 绘制的 EDA 过程的结果（详见[第6章](ch06.html#using_bigquery_ml_to_train_a_linear_reg)中的数据集详情）。[图1-7](#seaborn_regplot_showing_that_more_energ)
    展示了 *x* 和 *y* 之间的*反向*关系。[图1-8](#seaborn_correlation_matrix_left_parenth) 展示了 *热图*（或相关矩阵），并说明在温度较低时产生更多能量。
- en: '![Seaborn regplot showing that more energy is produced when temperatures are
    lower](assets/lcai_0107.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![Seaborn regplot显示温度较低时产生更多能量](assets/lcai_0107.png)'
- en: Figure 1-7\. Seaborn `regplot` showing that more energy is produced when temperatures
    are lower.
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-7\. Seaborn `regplot` 显示在温度较低时产生更多能量。
- en: '![Seaborn correlation matrix (heat map) showing a strong inverse relationship
    between Temp and Energy_Production](assets/lcai_0108.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![Seaborn相关矩阵（热图）显示温度与能量生产之间的强烈反向关系](assets/lcai_0108.png)'
- en: Figure 1-8\. Seaborn correlation matrix (heat map) showing a strong inverse
    relationship between `Temp` and `Energy_Production`, -0.75.
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-8\. Seaborn相关矩阵（热图）显示 `Temp` 和 `Energy_Production` 之间的强烈反向关系，-0.75。
- en: Data Transformation and Feature Selection
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据转换和特征选择
- en: After data has been cleaned and analyzed, you obtain a list of the features
    you think you need to help you solve your ML problem. But might other features
    be relevant? This is where *feature engineering* comes into play, where you *engineer*
    or *create new* features that were not in the original dataset. For example, if
    your dataset has separate fields/columns for month, day, and year, you can combine
    all three for a “month-day-year” time feature. Feature engineering is the final
    step before feature selection.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据清理和分析之后，您会得到一个特征列表，用于解决 ML 问题。但是，其他特征可能也是相关的吗？这就是 *特征工程* 发挥作用的地方，您可以在原始数据集中创建新的特征。例如，如果您的数据集分别包含月份、日期和年份的字段/列，您可以将所有三个组合为“月-日-年”时间特征。特征工程是特征选择之前的最后一步。
- en: 'In reality, feature selection occurs at two stages: after EDA and after data
    transformation. For example, after EDA, you should have a potential list of features
    that may be candidates to create new features—for example, combining time and
    day of week to get an hour of day. After you perform feature engineering, you
    then have a final list of features from which to select. [Figure 1-9](#position_of_data_transformation_and_fea)
    shows the position of data transformation and feature selection in the workflow.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，特征选择分两个阶段进行：在 EDA 之后和数据转换之后。例如，在完成 EDA 后，您应该有一个潜在的特征列表，可以用来创建新的特征，例如，结合时间和星期几以获取一天中的小时。在进行特征工程之后，您将从中选择最终的特征列表。[图1-9](#position_of_data_transformation_and_fea)
    显示了在工作流程中进行数据转换和特征选择的位置。
- en: '![Position of data transformation and feature selection in the ML workflow](assets/lcai_0109.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![数据转换和特征选择在ML工作流程中的位置](assets/lcai_0109.png)'
- en: Figure 1-9\. Position of data transformation and feature selection in the ML
    workflow.
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-9\. 在 ML 工作流程中数据转换和特征选择的位置。
- en: Researching the Model Selection or Using AutoML (a No-Code Solution)
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 研究模型选择或使用 AutoML（无代码解决方案）
- en: In this step, you either research the model that will be best for the type of
    data that fits your problem—or you could use AutoML, a no-code solution that,
    based on the dataset you uploaded, selects the appropriate model, trains, tests,
    and generates evaluation metrics. Essentially, if you use AutoML, the heavy lifting
    of model selection, model training, model tuning, and generating evaluation metrics
    is done for you. [Chapter 3](ch03.html#machine_learning_libraries_and_framewor)
    introduces AutoML, and [Chapter 4](ch04.html#use_automl_to_predict_advertising_media)
    starts getting hands-on with AutoML. Note that with a low-code solution, you would
    need to know what model to select.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，您可以研究最适合您问题类型的模型，或者您可以使用AutoML，这是一种基于您上传的数据集选择适当模型、训练、测试和生成评估指标的无代码解决方案。实质上，如果您使用AutoML，模型选择、模型训练、模型调优和生成评估指标的繁重工作都为您完成。[第3章](ch03.html#machine_learning_libraries_and_framewor)介绍了AutoML，而[第4章](ch04.html#use_automl_to_predict_advertising_media)开始实际使用AutoML。请注意，使用低代码解决方案，您需要知道选择哪种模型。
- en: Although AutoML might cover about 80% of your ML problems, you may want to build
    a more customized solution. In that case, having a general understanding of the
    types of problems ML algorithms can solve is helpful. Choosing the algorithm is
    solely dependent upon the problem (as discussed earlier). In [Table 1-2](#describing_the_model_type),
    a “Description” column is added to further describe the ML model problem type.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然AutoML可能覆盖了您80%的ML问题，但您可能希望构建一个更定制化的解决方案。在这种情况下，了解ML算法可以解决的问题类型是有帮助的。选择算法完全取决于问题本身（如前所述）。在[表1-2](#describing_the_model_type)中，“描述”列被添加以进一步描述ML模型的问题类型。
- en: Table 1-2\. Describing the model type
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 表1-2. 描述模型类型
- en: '| Problem or question | Problem | Description |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 问题或疑问 | 问题 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| How much or how many umbrellas? | Regression problem | Regression algorithms
    are used to deal with problems with continuous and numeric output. These are usually
    used for problems that deal with questions like *how much* or *how many*. |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 多少或多少把雨伞？ | 回归问题 | 回归算法用于处理连续和数值输出的问题。通常用于处理像“多少”或“多少”的问题。 |'
- en: '| Did they buy straight umbrellas (A) or foldable umbrellas (B)? | Classification
    problem | A problem in which the output can be only one of a fixed number of output
    classes, like Yes/No or True/False, is called a classification problem. Depending
    on the number of output classes, the problem can be a binary or multiclass classification
    problem. |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 他们购买了直杆雨伞（A）还是折叠雨伞（B）？ | 分类问题 | 输出只能是一组固定的输出类别之一，如是/否或真/假，称为分类问题。根据输出类别的数量，问题可以是二元分类问题或多元分类问题。
    |'
- en: '| Company policy is to only ship to customers with a balance owed of $500 or
    less. Can our manufacturing robot be trained to extract, package, load, and ship
    straight umbrellas to our customers based upon this policy? | Reinforcement learning
    | Reinforcement algorithms are used when a decision is to be made based on experiences
    of learning. The machine agent learns the behavior using trial and error in interaction
    with the continuously changing environment. This provides a way to program agents
    using the concept of rewards and penalties without specifying how the task is
    to be accomplished. Game-playing programs and programs for temperature control
    are some popular examples using reinforcement learning. |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 公司政策是仅向欠款$500或更少的客户发货。我们的制造机器人可以根据这一政策被训练来提取、包装、装载和发运直杆雨伞到我们的客户吗？ | 强化学习
    | 当根据学习经验做出决策时，使用强化算法。机器代理通过与不断变化的环境进行交互的试错学习行为。这提供了一种使用奖励和惩罚的概念来编程代理的方法，而无需指定任务如何完成。游戏程序和温度控制程序是使用强化学习的一些流行示例。
    |'
- en: Model Training, Evaluation, and Tuning
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型训练、评估和调优
- en: Before an ML model can be deployed to a production environment, it has to be
    trained, evaluated, and tested. Training an ML model is a process in which stored
    data instances are fed (input) into an ML model (algorithm). Since every stored
    data instance has a specific characteristic (recall our umbrella examples of the
    different types, prices, regions sold, and so forth), patterns of these data instances
    can be detected using hundreds of variables, and the algorithm is thus able to
    learn from the training data how to make a generalized prediction based on the
    data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在将ML模型部署到生产环境之前，必须对其进行训练、评估和测试。训练ML模型是一个过程，其中存储的数据实例被输入到ML模型（算法）中。由于每个存储的数据实例都有特定的特征（回想我们对不同类型、价格、销售地区等的雨伞示例），因此可以使用数百个变量检测这些数据实例的模式，算法因此能够从训练数据中学习如何基于数据进行一般化预测。
- en: Every ML model needs to not only be trained but also evaluated. Thus, you hold
    out a sample of data, called a *validation dataset*. The validation set measures
    how well the model generalizes to unseen or new data. The training error is used
    to determine how well the model fits the data because that is what the model is
    trained on.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 每个ML模型不仅需要进行训练，还需要进行评估。因此，需要保留一部分数据样本，称为*验证数据集*。验证集用于衡量模型对未见或新数据的一般化程度。训练错误用于确定模型对数据的拟合程度，因为这是模型训练的内容。
- en: Model evaluation metrics should be chosen or defined so that they align with
    the problem or business goals. Model tuning should improve the model performance
    as measured by the evaluation metrics. For example, how accurate were the sales
    of umbrella predictions during the month of December? Can these predictions be
    generalized for future forecasting efforts? Note that satisfactory performance
    is something that should be dictated by the business needs and should be agreed
    upon before starting any ML engagement.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 应选择或定义模型评估指标，以使其与问题或业务目标保持一致。模型调优应该通过评估指标来改善模型性能。例如，在12月销售的雨伞预测中，准确性如何？这些预测是否可以用于未来的预测工作？请注意，令人满意的性能应该由业务需求确定，并应在开始任何ML参与之前达成共识。
- en: Note
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The validation set is also used to determine if the model is overfitting. [Chapter 8](ch08.html#improving_custom_model_performance)
    discusses overfitting.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 验证集也用于确定模型是否过拟合。[第8章](ch08.html#improving_custom_model_performance)讨论了过拟合问题。
- en: Model Testing
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型测试
- en: There is no way to know if your umbrella prediction app can be generalized for
    future forecasting efforts without testing the model. Once the training dataset
    is used to fit the model to the data, and the validation dataset is used to improve
    model accuracy, you test the model on data it has never seen before. Testing data
    is used to assess model performance.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有测试模型之前，无法知道你的雨伞预测应用程序是否可以推广到未来的预测工作。一旦训练数据集用于将模型拟合到数据上，验证数据集用于提高模型准确性，你就需要在模型从未见过的数据上测试模型。测试数据用于评估模型性能。
- en: For example, let’s say you want to build an application that can recognize an
    umbrella’s color or pattern based on images of the umbrellas. You train a model
    by providing it with images of all umbrellas that are each tagged with a certain
    color or pattern. You use that model in a mobile application to recognize any
    umbrella’s color or pattern. The test would be how well the model performs in
    differentiating between umbrella colors and patterns.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你想构建一个可以根据雨伞图像识别颜色或图案的应用程序。你通过向模型提供每个都标记有特定颜色或图案的雨伞图像来训练模型。你在移动应用程序中使用该模型来识别任何雨伞的颜色或图案。测试就是要看模型在区分雨伞颜色和图案方面表现如何。
- en: '[Figure 1-10](#relationship_between_trainingcomma_vali) shows the relationship
    between the training, validation, and testing datasets.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1-10](#relationship_between_trainingcomma_vali)显示了训练、验证和测试数据集之间的关系。'
- en: '![Relationship between training, validation, and testing datasets in model
    deployment and model evaluation](assets/lcai_0110.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![模型部署和模型评估中训练、验证和测试数据集之间的关系](assets/lcai_0110.png)'
- en: Figure 1-10\. Relationship between training, validation, and testing datasets
    in model deployment and model evaluation.
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-10. 模型部署和模型评估中训练、验证和测试数据集之间的关系。
- en: '[Figure 1-11](#five_process_steps_of_the_ml_workflow) illustrates this relationship
    among the training, validation, and test datasets in five process steps. For simplicity,
    the arrow going back to the dataset in Step 5 is not shown, since once a model
    is deployed as an application and it begins collecting data, new data enters the
    *pipeline* that may *skew* the original model’s results. (At this point you enter
    the fascinating realm of machine learning operations, or MLOps, which is beyond
    the scope of the book.)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1-11](#five_process_steps_of_the_ml_workflow)展示了训练、验证和测试数据集在五个步骤中的关系。为简单起见，没有显示回到第
    5 步数据集的箭头，因为一旦将模型部署为应用程序并开始收集数据，新数据进入*管道*可能会*偏离*原始模型的结果。（在这一点上，您进入了机器学习运营的迷人领域，即
    MLOps，这超出了本书的范围。）'
- en: '![Five process steps of the ML workflow](assets/lcai_0111.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![ML工作流程的五个步骤](assets/lcai_0111.png)'
- en: Figure 1-11\. Five process steps of the ML workflow.
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-11\. 机器学习工作流程的五个步骤。
- en: Model Deployment (Serving)
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型部署（服务）
- en: Once the ML model is trained, evaluated, and tested, it is deployed into a live
    production environment where it can be used. Note that by the time the model reaches
    production, it more than likely has a web app frontend (using a browser) that
    communicates with the production system through an *application programming interface*
    (API). Data can be captured in real time and streamed (ingested) into an MLOps
    pipeline. Or data can be captured in batch and stored for ingestion into the pipeline.
    Or both.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦机器学习模型经过训练、评估和测试，就会部署到实时生产环境中以供使用。请注意，当模型达到生产环境时，它很可能具有 Web 应用程序前端（使用浏览器），通过*应用程序编程接口*（API）与生产系统通信。数据可以实时捕获并流式传输（摄取）到
    MLOps 管道中。或者数据可以批量捕获并存储以供后续摄入管道使用。或者两者兼而有之。
- en: Maintaining Models
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维护模型
- en: Models can become *stale* when predictions do not align with the original business
    goal or use case metrics. Staleness might occur when the world changes or business
    requirements change. These changes then impact the model. Post-deployment, you
    need to monitor your model to ensure it continues to perform as expected. Model
    and data drift is a phenomenon you should both expect and be prepared to mitigate
    through regular retraining using MLOps. Let’s look at an example of data *drift*,
    which means changes in the data that you trained with and the data that is now
    being received from the web app.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型的预测与原始业务目标或用例指标不一致时，模型可能会变得*陈旧*。当世界变化或业务需求变化时，可能会发生陈旧现象。这些变化会影响模型。在部署后，您需要监控您的模型，以确保它继续按预期表现。模型和数据漂移是您应该预期并通过定期使用
    MLOps 进行重新训练来减轻的现象。让我们来看一个数据*漂移*的例子，这意味着您训练的数据和现在从 Web 应用程序接收到的数据发生了变化。
- en: In our umbrella example, a region that once experienced heavy rainfall is now
    experiencing drought conditions. Similarly, a region that once experienced drought
    conditions is now experiencing heavy rainfall. Any prediction tied to weather
    and climate and the need for umbrellas and umbrella type will be impacted. In
    this scenario, you would need to retrain and test a new model with new data.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的综合示例中，曾经经历过大雨的地区现在正经历干旱条件。同样地，曾经经历干旱条件的地区现在正经历大雨。与天气和气候以及需要雨伞和伞类型相关的任何预测都将受到影响。在这种情况下，您需要使用新数据重新训练和测试新模型。
- en: Summary
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Businesses, educational institutions, government agencies, and practitioners
    face many decisions that reflect real-world examples of ML, from increasing customer
    engagement to reducing customer churn. Data—its collection, analysis, and use—drives
    the decision making used in ML to determine the best ML strategic approach that
    provides real-world solutions to real-world problems.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 企业、教育机构、政府机构和从业者面临许多决策，这些决策反映了机器学习的实际示例，从增加客户参与到减少客户流失。数据——其收集、分析和使用——驱动了用于机器学习的决策制定，以确定提供解决实际问题的最佳机器学习战略方法。
- en: 'While decision-making processes help you identify your problem or use case,
    it is the ML workflow that helps you implement the solution to your problem. An
    enterprise ML workflow is data-driven and requires decision making in the process.
    The ML workflow can be shown as a series of 10 steps, and the steps can be combined
    into four phases:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然决策过程帮助您确定问题或用例，但是机器学习工作流程帮助您实施解决方案。企业机器学习工作流程是数据驱动的，需要在流程中做出决策。机器学习工作流程可以显示为一系列
    10 步，并且这些步骤可以组合成四个阶段：
- en: Decision making
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 决策制定
- en: Data processing
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据处理
- en: Modeling
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建模
- en: Deployment
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署
- en: Each phase of the ML workflow can be implemented using AutoML or low-code AI.
    AutoML does all of the heavy lifting for you. AutoML will train the model, tune
    it, test it, and present you with evaluation metrics. Your role is simply to evaluate
    the metrics and determine if they meet your business objective or solve your problem.
    AutoML is recommended for quick experiments and prototypes. It is also used in
    production environments. A low-code approach enables those with some coding or
    deep coding experience to use autogenerated code that can be further customized
    during any phase of the ML workflow.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 每个机器学习工作流的阶段都可以使用AutoML或低代码AI来实现。AutoML为您完成所有繁重的工作。AutoML将训练模型、调整模型、测试模型，并向您展示评估指标。您的角色仅仅是评估这些指标，确定它们是否符合您的业务目标或解决您的问题。AutoML推荐用于快速实验和原型开发。它也被用于生产环境中。低代码方法使得那些具有一定编程或深度编程经验的人可以使用自动生成的代码，在机器学习工作流的任何阶段进行进一步定制。
- en: In this chapter, you learned about data collection and analysis as part of the
    ML workflow. [Chapter 2](ch02.html#data_is_the_first_step) provides an overview
    of the datasets used in the book, where to find data sources, data file types,
    and the difference between batch, streaming, structured, semistructured, and unstructured
    data. You also get hands-on experience using basic Python code to help you perform
    EDA and solve dirty data problems.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学习了数据收集和分析作为机器学习工作流的一部分。[第2章](ch02.html#data_is_the_first_step) 概述了本书使用的数据集，数据来源、数据文件类型，以及批处理、流处理、结构化、半结构化和非结构化数据之间的区别。您还可以通过基本的Python代码进行实践，帮助您进行探索性数据分析（EDA），解决数据质量问题。
