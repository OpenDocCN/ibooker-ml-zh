- en: Chapter 19\. Clustering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第19章 聚类
- en: 19.0 Introduction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 19.0 介绍
- en: 'In much of this book we have looked at supervised machine learning—​where we
    have access to both the features and the target. This is, unfortunately, not always
    the case. Frequently, we run into situations where we only know the features.
    For example, imagine we have records of sales from a grocery store and we want
    to break up sales by whether the shopper is a member of a discount club. This
    would be impossible using supervised learning because we don’t have a target to
    train and evaluate our models. However, there is another option: unsupervised
    learning. If the behavior of discount club members and nonmembers in the grocery
    store is actually disparate, then the average difference in behavior between two
    members will be smaller than the average difference in behavior between a member
    and nonmember shopper. Put another way, there will be two clusters of observations.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的大部分内容中，我们已经研究了监督机器学习——我们既可以访问特征又可以访问目标的情况。不幸的是，这并不总是事实。经常情况下，我们遇到的情况是我们只知道特征。例如，想象一下我们有一家杂货店的销售记录，并且我们想要按照购物者是否是折扣俱乐部会员来分割销售记录。使用监督学习是不可能的，因为我们没有一个目标来训练和评估我们的模型。然而，还有另一种选择：无监督学习。如果杂货店的折扣俱乐部会员和非会员的行为实际上是不同的，那么两个会员之间的平均行为差异将小于会员和非会员购物者之间的平均行为差异。换句话说，会有两个观察结果的簇。
- en: The goal of clustering algorithms is to identify those latent groupings of observations,
    which, if done well, allows us to predict the class of observations even without
    a target vector. There are many clustering algorithms, and they have a wide variety
    of approaches to identifying the clusters in data. In this chapter, we will cover
    a selection of clustering algorithms using scikit-learn and how to use them in
    practice.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法的目标是识别那些潜在的观察结果分组，如果做得好，即使没有目标向量，我们也能够预测观察结果的类别。有许多聚类算法，它们有各种各样的方法来识别数据中的簇。在本章中，我们将介绍一些使用scikit-learn的聚类算法以及如何在实践中使用它们。
- en: 19.1 Clustering Using K-Means
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 19.1 使用K均值进行聚类
- en: Problem
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to group observations into *k* groups.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要将观察结果分成*k*组。
- en: Solution
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use *k-means clustering*:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*k均值聚类*：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Discussion
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'K-means clustering is one of the most common clustering techniques. In k-means
    clustering, the algorithm attempts to group observations into *k* groups, with
    each group having roughly equal variance. The number of groups, *k*, is specified
    by the user as a hyperparameter. Specifically, in k-means:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: k均值聚类是最常见的聚类技术之一。在k均值聚类中，算法试图将观察结果分成*k*组，每组的方差大致相等。组数*k*由用户作为超参数指定。具体来说，在k均值聚类中：
- en: '*k* cluster “center” points are created at random locations.'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在随机位置创建*k*聚类的“中心”点。
- en: 'For each observation:'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个观察结果：
- en: The distance between each observation and the *k* center points is calculated.
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个观察结果与*k*中心点的距离。
- en: The observation is assigned to the cluster of the nearest center point.
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察结果被分配到最近中心点的簇中。
- en: The center points are moved to the means (i.e., centers) of their respective
    clusters.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 中心点被移动到各自簇的平均值（即，中心）。
- en: Steps 2 and 3 are repeated until no observation changes in cluster membership.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤2和3重复，直到没有观察结果在簇成员资格上发生变化。
- en: At this point the algorithm is considered converged and stops.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，算法被认为已经收敛并停止。
- en: It is important to note three things about k-means. First, k-means clustering
    assumes the clusters are convex shaped (e.g., a circle, a sphere). Second, all
    features are equally scaled. In our solution, we standardized the features to
    meet this assumption. Third, the groups are balanced (i.e., have roughly the same
    number of observations). If we suspect that we cannot meet these assumptions,
    we might try other clustering approaches.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 关于k均值聚类有三点需要注意。首先，k均值聚类假设簇是凸形的（例如，圆形，球形）。其次，所有特征都是等比例缩放的。在我们的解决方案中，我们标准化了特征以满足这个假设。第三，各组是平衡的（即，观察结果的数量大致相同）。如果我们怀疑无法满足这些假设，我们可以尝试其他聚类方法。
- en: In scikit-learn, k-means clustering is implemented in the `KMeans` class. The
    most important parameter is `n_clusters`, which sets the number of clusters *k*.
    In some situations, the nature of the data will determine the value for *k* (e.g.,
    data on a school’s students will have one cluster per grade), but often we don’t
    know the number of clusters. In these cases, we will want to select *k* based
    on using some criteria. For example, silhouette coefficients (see [Recipe 11.9](ch11.xhtml#evaluating-clustering-models))
    measure the similarity within clusters compared with the similarity between clusters.
    Furthermore, because k-means clustering is computationally expensive, we might
    want to take advantage of all the cores on our computer. We can do this by setting
    `n_jobs=-1`.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中，k-means 聚类是在 `KMeans` 类中实现的。最重要的参数是 `n_clusters`，它设置聚类数 *k*。在某些情况下，数据的性质将决定
    *k* 的值（例如，学校学生的数据将有一个班级对应一个聚类），但通常我们不知道聚类数。在这些情况下，我们希望基于某些准则选择 *k*。例如，轮廓系数（参见[第11.9节](ch11.xhtml#evaluating-clustering-models)）可以衡量聚类内部的相似性与聚类间的相似性。此外，由于
    k-means 聚类计算开销较大，我们可能希望利用计算机的所有核心。我们可以通过设置 `n_jobs=-1` 来实现这一点。
- en: 'In our solution, we cheated a little and used the iris flower data, which we
    know contains three classes. Therefore, we set *k = 3*. We can use `labels_` to
    see the predicted classes of each observation:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的解决方案中，我们有点作弊，使用了已知包含三个类别的鸢尾花数据。因此，我们设置 *k = 3*。我们可以使用 `labels_` 查看每个观测数据的预测类别：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If we compare this to the observation’s true class, we can see that, despite
    the difference in class labels (i.e., `0`, `1`, and `2`), k-means did reasonably
    well:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将其与观测数据的真实类别进行比较，可以看到，尽管类标签有所不同（即 `0`、`1` 和 `2`），k-means 的表现还是相当不错的：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: However, as you might imagine, the performance of k-means drops considerably,
    even critically, if we select the wrong number of clusters.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如你所想象的那样，如果我们选择了错误的聚类数，k-means 的性能将显著甚至可能严重下降。
- en: 'Finally, as with other scikit-learn models, we can use the trained cluster
    to predict the value of new observations:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，与其他 scikit-learn 模型一样，我们可以使用训练好的聚类模型来预测新观测数据的值：
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The observation is predicted to belong to the cluster whose center point is
    closest. We can even use `cluster_centers_` to see those center points:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 预测观测数据属于距离其最近的聚类中心点。我们甚至可以使用 `cluster_centers_` 查看这些中心点：
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: See Also
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Introduction to K-means Clustering](https://oreil.ly/HDfUz)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[介绍 K-means 聚类](https://oreil.ly/HDfUz)'
- en: 19.2 Speeding Up K-Means Clustering
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 19.2 加速 K-Means 聚类
- en: Problem
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to group observations into *k* groups, but k-means takes too long.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望将观测数据分组成 *k* 组，但 k-means 太耗时。
- en: Solution
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use mini-batch k-means:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 mini-batch k-means：
- en: '[PRE9]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Discussion
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: '*Mini-batch k-means* works similarly to the k-means algorithm discussed in
    [Recipe 19.1](#clustering-using-k-means). Without going into too much detail,
    the difference is that in mini-batch k-means the most computationally costly step
    is conducted on only a random sample of observations as opposed to all observations.
    This approach can significantly reduce the time required for the algorithm to
    find convergence (i.e., fit the data) with only a small cost in quality.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*Mini-batch k-means* 类似于讨论中的 k-means 算法（见[第19.1节](#clustering-using-k-means)）。不详细讨论的话，两者的区别在于，mini-batch
    k-means 中计算成本最高的步骤仅在随机抽样的观测数据上进行，而不是全部观测数据。这种方法可以显著减少算法找到收敛（即拟合数据）所需的时间，只有少量的质量损失。'
- en: '`MiniBatchKMeans` works similarly to `KMeans`, with one significant difference:
    the `batch_size` parameter. `batch_size` controls the number of randomly selected
    observations in each batch. The larger the size of the batch, the more computationally
    costly the training process.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`MiniBatchKMeans` 类似于 `KMeans`，但有一个显著的区别：`batch_size` 参数。`batch_size` 控制每个批次中随机选择的观测数据数量。批次大小越大，训练过程的计算成本越高。'
- en: 19.3 Clustering Using Mean Shift
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 19.3 使用均值漂移进行聚类
- en: Problem
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to group observations without assuming the number of clusters or their
    shape.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望在不假设聚类数或形状的情况下对观测数据进行分组。
- en: Solution
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use mean shift clustering:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用均值漂移聚类：
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Discussion
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: One of the disadvantages of k-means clustering we discussed previously is that
    we needed to set the number of clusters, *k*, prior to training, and the method
    made assumptions about the shape of the clusters. One clustering algorithm without
    these limitations is mean shift.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论过 k-means 聚类的一个缺点是在训练之前需要设置聚类数 *k*，并且该方法对聚类形状作出了假设。一种无此限制的聚类算法是均值漂移。
- en: '*Mean shift* is a simple concept, but it’s somewhat difficult to explain. Therefore,
    an analogy might be the best approach. Imagine a very foggy football field (i.e.,
    a two-dimensional feature space) with 100 people standing on it (i.e., our observations).
    Because it is foggy, a person can see only a short distance. Every minute each
    person looks around and takes a step in the direction of the most people they
    can see. As time goes on, people start to group together as they repeatedly take
    steps toward larger and larger crowds. The end result is clusters of people around
    the field. People are assigned to the clusters in which they end up.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*均值漂移*是一个简单的概念，但有些难以解释。因此，通过类比可能是最好的方法。想象一个非常雾蒙蒙的足球场（即，二维特征空间），上面站着100个人（即，我们的观察结果）。因为有雾，一个人只能看到很短的距离。每分钟，每个人都会四处张望，并朝着能看到最多人的方向迈出一步。随着时间的推移，人们开始团结在一起，重复向更大的人群迈步。最终的结果是围绕场地的人群聚类。人们被分配到他们最终停留的聚类中。'
- en: scikit-learn’s actual implementation of mean shift, `MeanShift`, is more complex
    but follows the same basic logic. `MeanShift` has two important parameters we
    should be aware of. First, `bandwidth` sets the radius of the area (i.e., kernel)
    an observation uses to determine the direction to shift. In our analogy, bandwidth
    is how far a person can see through the fog. We can set this parameter manually,
    but by default a reasonable bandwidth is estimated automatically (with a significant
    increase in computational cost). Second, sometimes in mean shift there are no
    other observations within an observation’s kernel. That is, a person on our football
    field cannot see a single other person. By default, `MeanShift` assigns all these
    “orphan” observations to the kernel of the nearest observation. However, if we
    want to leave out these orphans, we can set `cluster_all=False`, wherein orphan
    observations are given the label of `-1`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn的实际均值漂移实现，`MeanShift`，更为复杂，但遵循相同的基本逻辑。`MeanShift`有两个重要的参数我们应该注意。首先，`bandwidth`设置了观察使用的区域（即核心）的半径，以确定向何处移动。在我们的类比中，bandwidth代表一个人透过雾能看到的距离。我们可以手动设置此参数，但默认情况下会自动估算一个合理的带宽（计算成本显著增加）。其次，在均值漂移中有时没有其他观察在观察的核心中。也就是说，在我们的足球场上，一个人看不到其他人。默认情况下，`MeanShift`将所有这些“孤儿”观察分配给最近观察的核心。但是，如果我们希望排除这些孤儿，我们可以设置`cluster_all=False`，其中孤儿观察被赋予标签`-1`。
- en: See Also
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[The mean shift clustering algorithm, EFAVDB](https://oreil.ly/Gb3VG)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[均值漂移聚类算法，EFAVDB](https://oreil.ly/Gb3VG)'
- en: 19.4 Clustering Using DBSCAN
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 19.4 使用DBSCAN进行聚类
- en: Problem
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to group observations into clusters of high density.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望将观察结果分组为高密度的聚类。
- en: Solution
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use DBSCAN clustering:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DBSCAN聚类：
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Discussion
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: '*DBSCAN* is motivated by the idea that clusters will be areas where many observations
    are densely packed together and makes no assumptions of cluster shape. Specifically,
    in DBSCAN:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*DBSCAN*的动机在于，聚类将是许多观察结果密集堆积的区域，并且不对聚类形状做出假设。具体来说，在DBSCAN中：'
- en: A random observation, *x[i]*, is chosen.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个随机观察结果，*x[i]*。
- en: If *x[i]* has a minimum number of close neighbors, we consider it to be part
    of a cluster.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果*x[i]*有足够数量的近邻观察，我们认为它是聚类的一部分。
- en: Step 2 is repeated recursively for all of *x[i]*’s neighbors, then neighbor’s
    neighbor, and so on. These are the cluster’s core observations.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤2递归地重复对*x[i]*的所有邻居，邻居的邻居等的处理。这些是聚类的核心观察结果。
- en: Once step 3 runs out of nearby observations, a new random point is chosen (i.e.,
    restart at step 1).
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦步骤3耗尽附近的观察，就会选择一个新的随机点（即，在步骤1重新开始）。
- en: Once this is complete, we have a set of core observations for a number of clusters.
    Finally, any observation close to a cluster but not a core sample is considered
    part of a cluster, while any observation not close to the cluster is labeled an
    outlier.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成这一步骤，我们就得到了多个聚类的核心观察结果集。最终，任何靠近聚类但不是核心样本的观察被认为是聚类的一部分，而不靠近聚类的观察则被标记为离群值。
- en: '`DBSCAN` has three main parameters to set:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`DBSCAN`有三个主要的参数需要设置：'
- en: '`eps`'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`eps`'
- en: The maximum distance from an observation for another observation to be considered
    its neighbor.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一个观察到另一个观察的最大距离，以便将其视为邻居。
- en: '`min_samples`'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`min_samples`'
- en: The minimum number of observations less than `eps` distance from an observation
    for it to be considered a core observation.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 小于`eps`距离的观察数目最少的观察，被认为是核心观察结果。
- en: '`metric`'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`metric`'
- en: The distance metric used by `eps`—for example, `minkowski` or `euclidean` (note
    that if Minkowski distance is used, the parameter `p` can be used to set the power
    of the Minkowski metric).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`eps`使用的距离度量——例如，`minkowski`或`euclidean`（注意，如果使用Minkowski距离，参数`p`可以用来设置Minkowski度量的幂）。'
- en: 'If we look at the clusters in our training data we can see two clusters have
    been identified, `0` and `1`, while outlier observations are labeled `-1`:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看我们的训练数据中的集群，我们可以看到已经识别出两个集群，`0`和`1`，而异常值观测被标记为`-1`：
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: See Also
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '[DBSCAN, Wikipedia](https://oreil.ly/QBx3a)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DBSCAN，维基百科](https://oreil.ly/QBx3a)'
- en: 19.5 Clustering Using Hierarchical Merging
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 19.5 使用分层合并进行聚类
- en: Problem
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to group observations using a hierarchy of clusters.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 您想使用集群的层次结构对观测进行分组。
- en: Solution
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use agglomerative clustering:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用聚类：
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Discussion
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: '*Agglomerative clustering* is a powerful, flexible hierarchical clustering
    algorithm. In agglomerative clustering, all observations start as their own clusters.
    Next, clusters meeting some criteria are merged. This process is repeated, growing
    clusters until some end point is reached. In scikit-learn, `AgglomerativeClustering`
    uses the `linkage` parameter to determine the merging strategy to minimize:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*凝聚式聚类*是一种强大、灵活的分层聚类算法。在凝聚式聚类中，所有观测都开始作为自己的集群。接下来，满足一些条件的集群被合并。这个过程重复进行，直到达到某个结束点为止。在scikit-learn中，`AgglomerativeClustering`使用`linkage`参数来确定最小化合并策略：'
- en: Variance of merged clusters (`ward`)
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合并集群的方差（`ward`）
- en: Average distance between observations from pairs of clusters (`average`)
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自成对集群的观察之间的平均距离（`average`）
- en: Maximum distance between observations from pairs of clusters (`complete`)
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自成对集群的观察之间的最大距离（`complete`）
- en: Two other parameters are useful to know. First, the `affinity` parameter determines
    the distance metric used for `linkage` (`minkowski`, `euclidean`, etc.). Second,
    `n_clusters` sets the number of clusters the clustering algorithm will attempt
    to find. That is, clusters are successively merged until only `n_clusters` remain.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 还有两个有用的参数需要知道。首先，`affinity`参数确定用于`linkage`的距离度量（`minkowski`、`euclidean`等）。其次，`n_clusters`设置聚类算法将尝试找到的聚类数。也就是说，集群被连续合并，直到只剩下`n_clusters`。
- en: 'As with other clustering algorithms we have covered, we can use `labels_` to
    see the cluster in which every observation is assigned:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们讨论过的其他聚类算法一样，我们可以使用`labels_`来查看每个观察被分配到的集群：
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
