- en: Chapter 12\. How to Succeed in High-Risk Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章。如何在高风险机器学习中取得成功
- en: While artificial intelligence and machine learning have been researched for
    decades, and used in some spaces for almost as long, we are in the early stages
    of the adoption of ML in the broader economy. ML is an often immature and sometimes
    high-risk technology. ML is exciting and holds great promise, but it’s not magic,
    and people who practice ML don’t have magical superpowers. We and our ML technologies
    can fail. If we want to succeed, we need to proactively address our systems’ risks.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然人工智能和机器学习已经研究了几十年，并且在某些领域几乎同样长时间使用，但我们在广泛经济中采用机器学习的初期阶段。机器学习往往是不成熟的，有时是高风险的技术。机器学习令人兴奋并且充满了希望，但它不是魔法，从事机器学习的人也没有神奇的超能力。我们和我们的机器学习技术都有可能失败。如果我们希望成功，我们需要积极应对系统风险。
- en: This entire book has put forward technical risk mitigants and some governance
    approaches. This final chapter aims to leave you with some commonsense advice
    that should empower you to take on more difficult problems in ML. However, our
    recommendations are probably not going to be easy. Solving hard problems almost
    always requires hard work. Solving hard problems with ML is no different. How
    do we succeed in high-risk technology endeavors? Usually not by moving fast and
    breaking things. While moving fast and breaking things might work well enough
    for buggy social apps and simple games, it’s not how we got to the moon, fly around
    the world safely on jets, power our economy, or fabricate microchips. High-risk
    ML, like each of these other disciplines, requires serious commitments to safety
    and quality.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 整本书提出了技术风险缓解措施和一些治理方法。这一最后一章旨在为您留下一些常识性建议，这些建议应能使您能够在机器学习中处理更困难的问题。然而，我们的建议可能并不容易实现。解决难题几乎总是需要艰苦的工作。用机器学习解决难题也不例外。在高风险技术项目中如何取得成功？通常不是通过快速行动和破坏事物。虽然快速行动和破坏事物在有缺陷的社交应用和简单游戏中可能表现得足够好，但这不是我们登上月球、安全飞行在全球各地的喷气机、推动我们的经济或制造微处理器的方式。高风险的机器学习与其他每个学科一样，都需要对安全和质量做出严肃的承诺。
- en: If we are in the early days of ML adoption, we are at the very dawn of ML risk
    management. Only in 2022 did the National Institute for Standards and Technology
    (NIST) release the first draft of its AI Risk Management Framework. Following
    along with that guidance and others, and in alignment with our practical experience
    and the content of this book, we think a major way to succeed in high-risk ML
    settings is by applying governance to ML systems and data scientists, and by building
    transparent, tested, fair, and secure tech. However, there are a few bits of advice
    and lessons that go beyond these process and technology goals we’d like to share.
    In this chapter, we put forward additional consideration of diversity, equity,
    inclusion, accessibility, the scientific method, evaluation of published claims,
    external standards, and a handful of other commonsense pointers to help us manage
    risk holistically, and increase our chances of success for important ML projects.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们处于机器学习采纳的早期阶段，那么我们正处于机器学习风险管理的黎明时期。直到2022年，国家标准与技术研究所（NIST）才发布了其首个AI风险管理框架的草案。遵循这一指导及其他指南，并与我们的实际经验和本书内容保持一致，我们认为在高风险的机器学习环境中取得成功的一种重要方式是对机器学习系统和数据科学家应用治理，并建立透明、经过测试的、公平和安全的技术。然而，除了这些过程和技术目标外，我们还想分享一些超出这些目标的建议和教训。在本章中，我们提出了对多样性、公平性、包容性、可访问性、科学方法、评估已发表声明、外部标准以及其他几个常识性指南的额外考虑，以帮助我们全面管理风险，并提高我们在重要机器学习项目中成功的机会。
- en: Note
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'It takes more than technical skills and tools to succeed in high-risk ML applications.
    In addition to technical prowess, you’ll need the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 要在高风险的机器学习应用中成功，不仅需要技术能力和工具，还需要以下内容：
- en: A team with diverse perspectives
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有多元化视角的团队
- en: An understanding of when and how to apply scientific experimentation versus
    software engineering methods
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解何时以及如何应用科学实验与软件工程方法
- en: The ability to evaluate published results and claims
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估已发表结果和声明的能力
- en: The ability to apply authoritative external standards
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用权威的外部标准的能力
- en: Common sense
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常识
- en: This chapter discusses these key sociotechnical lessons we’ve learned over the
    years, so that readers can jump-start their next important project with expertise
    beyond governance, code, and math.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章讨论了多年来我们学到的关键社会技术教训，以便读者能够在下一个重要项目中以超越治理、代码和数学的专业知识快速起步。
- en: Who Is in the Room?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谁在房间里？
- en: From the very outset of an ML project, meaning the meetings about the meetings
    for the project, or even when an organization begins discussing ML adoption, the
    involvement of diverse humans is a fundamental risk control. To understand why,
    consider that the former ML Ethics, Transparency, and Accountability (META) team
    at Twitter has shown that several features of the platform may be biased, and
    that this is at least partially due to the types of people involved in system
    development. While it’s not ML, Twitter’s original 140-character limit was seen
    as incentivizing pithy dialog for English speakers, but as discussed in [“Giving
    You More Characters to Express Yourself”](https://oreil.ly/pRNEZ), the character
    limit was truly problematic for some users of the platform. These issues would
    not have been immediately apparent to the mostly English-speaking initial designers.
    As for ML, the recent META bias bounty for the now-defunct Twitter image cropper
    showed biases against groups of people that are less often well-represented in
    ML engineering groups, such as biases against [users of non-Latin scripts (e.g.,
    Arabic), biases against people with white hair](https://oreil.ly/MEXn9), and biases
    against those [wearing religious headdresses](https://oreil.ly/yhNGv). It was
    only by engaging their *global* user community that Twitter discovered these specific
    issues.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从ML项目的最早阶段开始，即关于项目会议的会议，或者组织开始讨论ML采纳时，多样化的人员参与是一项基本的风险控制措施。要理解其中原因，可以考虑Twitter之前的ML伦理、透明度和问责（META）团队曾显示出平台的几个特征可能存在偏见，这至少部分是由于参与系统开发的人员类型所致。尽管这并非ML问题，但Twitter最初的140字符限制被视为激励英语使用者进行简洁对话，但正如在[“给您更多表达空间”](https://oreil.ly/pRNEZ)中讨论的那样，字符限制对平台的某些用户确实存在问题。这些问题最初设计者多为英语使用者并未立即显现。至于ML，最近Twitter针对现已停用的图像裁剪器提出的META偏见悬赏显示出对少数在ML工程团队中很少得到充分代表的人群存在偏见，例如[非拉丁脚本用户（例如阿拉伯语）的偏见，对白发人群的偏见](https://oreil.ly/MEXn9)，以及对[戴宗教头饰者的偏见](https://oreil.ly/yhNGv)。只有通过全球用户社区的参与，Twitter才发现了这些具体问题。
- en: Warning
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Diversity, equity, inclusion, and accessibility are serious ethical, legal,
    business, and ML performance considerations. Look around the room (or the video
    call). If everyone looks the same or has the same technical background, we probably
    have big blind spots that are increasing our risks, and we are likely missing
    important perspectives that could improve our models. Consult [NIST SP1270](https://oreil.ly/OAw2q)
    for additional ideas, resources, and mitigants related to increased diversity,
    equity, inclusion, and accessibility in ML.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 多样性、公平性、包容性和可访问性是严肃的伦理、法律、商业和ML表现考量。看看周围的房间（或视频通话）。如果每个人看起来都一样或具有相同的技术背景，那么我们可能存在增加风险的重大盲点，并且可能会错过可以改善我们模型的重要视角。请参考[NIST
    SP1270](https://oreil.ly/OAw2q)获取有关在ML中增加多样性、公平性、包容性和可访问性的额外想法、资源和缓解措施。
- en: A recent study entitled [“Biased Programmers? Or Biased Data? A Field Experiment
    in Operationalizing AI Ethics”](https://oreil.ly/bl7xW) may shed light on how
    these biases emerge when coding ML models. In this study, prediction errors in
    ML models were correlated based on the demographics of the developers. Different
    types of people tend to have different blind spots. The more different kinds of
    people involved in an ML project, the less their blind spots tend to overlap,
    and the more perspective the group has altogether. It’s also crucial to have professional
    and demographic diversity from the very beginning of our ML efforts because planning
    or governance blind spots can doom ML systems the same way bad models or poor
    testing does. Moreover, it’s fairly well-understood that [diversity can drive
    financial performance](https://oreil.ly/xTeoX). Diversity isn’t just about risk
    management, it’s about better business too.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究题为[“偏见程序员？还是偏见数据？操作化AI伦理的实地实验”](https://oreil.ly/bl7xW)可能揭示了在编码ML模型时这些偏见是如何产生的。在这项研究中，ML模型的预测错误与开发者的人口统计学特征相关。不同类型的人倾向于有不同的盲点。参与ML项目的人种类越多，他们的盲点重叠就越少，整个团队的视角也更广泛。从我们ML努力的最开始，拥有职业和人口统计多样性非常关键，因为规划或治理的盲点会像糟糕的模型或差劲的测试一样毁掉ML系统。此外，人们普遍理解的是，[多样性可以推动财务表现](https://oreil.ly/xTeoX)。多样性不仅仅是风险管理，也关乎更好的业务表现。
- en: 'Scholars and practitioners are already thinking about improving diversity in
    the ML field, and many are convinced that [it is important to hire more diverse
    teams in order to build less-biased AI](https://oreil.ly/gx8Rp), or are at least
    asking important questions like [“How Can Human-Centered AI Fight Bias in Machines
    and People?”](https://oreil.ly/7YC_J) However, we have to be honest and acknowledge
    that today there is little diversity in ML. According to the AI Now report “Discriminating
    Systems: Gender, Race, and Power in AI,” 80% of AI professors are men, “women
    comprise 15% of AI research staff at Facebook and just 10% at Google,” and only
    “2.5% of Google’s workforce is black, while Facebook and Microsoft are each at
    4%.” While it may require extending timelines, educating and learning from colleagues
    and stakeholders, more meetings and emails, and likely difficult realizations
    about our own biases and blind spots, having a professionally and demographically
    diverse group of people in the room (or video call) from the very beginning of
    an organization’s ML journey often leads to better ML system performance and less
    overall risk.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 学者和从业者已经在思考如何改善ML领域的多样性，许多人确信[雇佣更多多样化的团队可以建立更少偏见的AI](https://oreil.ly/gx8Rp)，或者至少在问诸如[“人本AI如何对抗机器和人类的偏见？”](https://oreil.ly/7YC_J)等重要问题。然而，我们必须诚实地承认，今天在ML领域中缺乏多样性。根据AI
    Now报告“歧视性系统：AI中的性别、种族和权力”，80%的AI教授是男性，“女性在Facebook的AI研究人员中占15%，而在Google仅占10%”，而仅“2.5%的Google员工是黑人，而Facebook和微软的比例分别为4%”。尽管这可能需要延长时间表，与同事和利益相关者进行教育和学习，更多的会议和电子邮件交流，以及可能对我们自身偏见和盲点的艰难认知，但在组织的ML旅程的起点上，拥有一个职业和人口统计多样化的团队常常会导致更好的ML系统表现和更少的总体风险。
- en: If we find ourselves on homogeneous teams, we have to talk to our managers and
    get involved in the interview process to help achieve better diversity and inclusion.
    We can ask about the possibility of an external audit of models, or of accessing
    other kinds of external expertise that could provide diverse perspectives. Also,
    consult NIST SP1270 for authoritative advice, reviewed by many leading experts,
    for increasing diversity, equity, inclusion, and accessibility in ML.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们发现自己所在的团队过于同质化，我们必须与我们的管理者沟通，并参与面试过程，帮助实现更好的多样性和包容性。我们可以询问关于对模型进行外部审计的可能性，或者获取其他类型的外部专业知识，这些知识可以提供多元化的观点。此外，可以参考NIST
    SP1270提供的权威建议，该建议由许多领先的专家审阅，旨在增强ML领域的多样性、公平性、包容性和可访问性。
- en: Next time we start a project, we can do a better job of including practitioners
    with more diverse demographic backgrounds, but also legal or oversight personnel,
    traditional statisticians and economists, user experience researchers, customer
    or stakeholder voices, and others who can expand our view of the system and its
    outcomes. And, if we’re *really* worried about bias or other harms emerging from
    a specific ML project today, we can consider reaching out to our internal legal
    teams—in particular product or data privacy counsels—or whistleblowing, if relevant
    protections exist at our organization.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 下次我们启动项目时，我们可以更好地包括具有更多不同人口统计背景的从业者，还有法律或监督人员，传统的统计学家和经济学家，用户体验研究人员，客户或利益相关者的声音，以及其他可以扩展我们对系统及其结果看法的人。而且，如果我们真的对从某个特定ML项目中出现的偏见或其他危害感到担忧，我们可以考虑联系我们内部的法律团队——特别是产品或数据隐私顾问——或者进行举报，如果我们组织内存在相关的保护措施的话。
- en: Science Versus Engineering
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 科学与工程的对立
- en: Deploying a high-risk ML system is more like a science experiment than a rote
    engineering task. Despite what we hear about how software, hardware, containerization,
    and monitoring solutions can help us operationalize ML, ML systems are not guaranteed
    to be operationalizable. After all, it’s not like we’re building a table, or even
    a car, that we can assume will work if we follow some set of instructions. In
    ML, we can do everything this book or any other perceived authority tells us to
    do, and the system can still fail for any number of reasons. At least one of those
    reasons is that building ML systems tends to involve many hypotheses that we often
    treat as assumptions—with the primary hypothesis being that we can achieve the
    system’s intended effect in the real world.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 部署高风险的 ML 系统更像是一项科学实验，而不是机械的工程任务。尽管我们听到软件、硬件、容器化和监控解决方案如何帮助我们实现 ML 的运营，但 ML
    系统并不保证可运行。毕竟，我们并不像建造一张桌子或甚至一辆汽车那样，可以假设只要按照一套说明就能工作。在 ML 中，我们可以按照这本书或其他任何被认为权威的指导所告诉我们的做，但系统仍然可能因为许多原因失败。至少其中一个原因是，构建
    ML 系统往往涉及许多我们经常视为假设的假设，其中主要假设是我们能够在现实世界中实现系统预期效果。
- en: Warning
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Much of AI and ML is still an evolving sociotechnical science, and not yet ready
    to be directly productized using only software engineering techniques.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: AI 和 ML 的大部分仍然是一个不断发展的社会技术科学，尚未准备好仅使用软件工程技术进行直接产品化。
- en: Generally speaking, we as data scientists seem to have forgotten that we need
    to apply the scientific method carefully to have a good chance at success in high-risk
    deployments, because we are often conducting implicit experiments. We tend to
    walk into projects assuming it will work out if we can get the engineering right.
    Doing this puts way too much faith in correlations detected in observational data,
    and in general, we put too much faith in training data itself, which is typically
    biased and often inaccurate. If we’re being honest, we’re usually horrible at
    making reproducible results too. And when we do make more formal hypotheses, they’re
    often about which algorithm to use. Yet, for high-risk ML systems, we should be
    making formal hypotheses about the intended real-world outcome of our system.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，作为数据科学家，我们似乎忘记了需要仔细应用科学方法，以便在高风险部署中取得成功的良好机会，因为我们经常进行隐式实验。我们倾向于认为，如果我们能把工程做对，项目就会成功。这样做过于信任从观察数据中检测到的相关性，通常也过于信任训练数据本身，后者通常存在偏见且经常不准确。坦率地说，我们通常很难产生可重现的结果。而且，当我们确实提出更正式的假设时，通常是关于使用哪种算法。然而，对于高风险的
    ML 系统，我们应该对系统预期的实际世界结果提出正式假设。
- en: The Data-Scientific Method
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据科学方法
- en: 'We’ve seen these fundamental antipatterns in data science workflows so many
    times, even in our own work, that we have a name for it: [the data-scientific
    method](https://oreil.ly/22Zmt). It really feels like success in many ML projects
    is premised on using the “right” technology, and worse, if we do that then we
    can’t fail. Most colleagues we’ve shown the data-scientific method to agree that
    it sounds all too familiar.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在数据科学工作流程中多次看到了这些基本反模式，甚至在我们自己的工作中也是如此，我们给它起了个名字：[数据科学方法](https://oreil.ly/22Zmt)。许多
    ML 项目的成功似乎是建立在使用“正确”的技术基础上的，更糟糕的是，如果我们这样做，那么我们就不会失败。我们向看过数据科学方法的大多数同事展示后，他们都同意这听起来太过熟悉了。
- en: 'Read the steps that follow and think about the data science teams and projects
    you’ve been involved in. Here’s how the data-scientific method works:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读以下步骤，思考一下您参与过的数据科学团队和项目。以下是数据科学方法的运作方式：
- en: Assume we’ll make millions of dollars.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们会赚数百万美元。
- en: Install GPU, download Python.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 GPU，下载 Python。
- en: Collect inaccurate, biased data from the internet or the exhaust of some business
    process.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从互联网或某些业务流程的排放中收集不准确、有偏见的数据。
- en: 'Surrender to confirmation bias:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 屈服于确认偏见：
- en: Study collected data to form a hypothesis (i.e., which `X`, `y`, and ML algorithm
    to use).
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 研究收集的数据形成假设（即使用哪些`X`、`y`和 ML 算法）。
- en: Use essentially the same data from hypothesis generation to test our hypothesis.
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用基本上相同的数据从假设生成到测试我们的假设。
- en: Test our hypothesis with a high-capacity learning algorithm that can fit almost
    any set of loosely correlated `X` and `y` well.
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用能够很好地拟合几乎任何一组松散相关的`X`和`y`的高容量学习算法来测试我们的假设。
- en: Change our hypothesis until our results are “good.”
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不断修改我们的假设，直到结果“好”为止。
- en: Don’t worry about reproducing; we’re all good, bruh.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不用担心复制；我们都很好，兄弟。
- en: The data-scientific method can’t lead to our system demonstrating its intended
    real-world purpose, except by luck. Put another way, the data-scientific method
    cannot provide evidence toward, or falsify, a formal hypothesis about the in vivo
    outcomes of our system. As crazy as it sounds, we have to change our whole approach
    to ML if we want to systematically increase our chances of success in high-risk
    deployments. We can’t assume we will succeed (or get rich). In fact, if we want
    to succeed, we should probably be more adversarial, and assume that success is
    very difficult and whatever we’re doing now isn’t going to work. We should constantly
    be looking for holes in our approach and experimental setup.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学方法无法导致我们的系统展示其预期的现实世界目的，除非碰巧。换句话说，数据科学方法无法提供证据支持或反驳关于我们系统体内结果的正式假设。尽管听起来很疯狂，但如果我们想要在高风险部署中系统地增加成功的机会，我们必须改变我们的整体机器学习方法。我们不能假设我们会成功（或发财）。事实上，如果我们想成功，我们可能应该更加对抗，假设成功非常困难，我们现在所做的任何事情都不会奏效。我们应该不断地寻找我们方法和实验设置中的漏洞。
- en: While choosing the right tools is important to success, getting the basic science
    right is much more important—mainly because there is typically no “right” technology.
    Recall that Bjarne Stroustrup, the inventor of C++, [often says](https://oreil.ly/J9uWR),
    “someone who claims to have a perfect programming language is either a salesman
    or a fool, or both.” Like many things in life, technology is much more about managing
    trade-offs than finding the perfect tool.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管选择正确的工具对成功至关重要，正确掌握基本科学更为重要——主要因为通常并不存在“正确”的技术。回想起C++的发明者**比雅尼·斯特劳斯特鲁普**[经常说](https://oreil.ly/J9uWR)：“声称自己拥有完美编程语言的人要么是推销员，要么是傻子，或者两者兼而有之。”像生活中的许多事物一样，技术更多地是关于权衡而非寻找完美工具。
- en: We also have to question basic ideas and methods in ML. Correlation in observational
    data, the phenomenon that nearly all ML models rely on, can be meaningless, spurious,
    or wrong. Statisticians and other empirical scientists have understood this problem
    for a long time. It simply may not matter that an overparameterized model with
    millions, billions, or trillions of parameters can find correlation patterns in
    a large dataset, especially if we’re running massive grid searches or other repeated
    comparisons on the same data over and over again. Moreover, we have to question
    the objectivity and the accuracy of the data we’re using. Just because a dataset
    is digital or large, that doesn’t mean it contains the information we need in
    a way that an ML model can learn about it. The final kicker is a lack of reproducibility.
    Is it any wonder that data science and ML have well-known reproducibility problems
    if we’re applying the data-scientific method? Reproducing experimental setups
    and tedious technical steps is hard enough, but asking others to apply confirmation
    bias and other—usually undocumented—experimental design mistakes just the way
    we did to replicate our flawed results is going to be nearly impossible.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须质疑机器学习中的基本思想和方法。观察数据中的相关性，几乎所有机器学习模型都依赖的现象，可能是毫无意义、伪相关或错误的。统计学家和其他经验科学家长期以来就已经意识到了这个问题。如果我们在相同数据上进行大规模的网格搜索或其他重复比较，那么模型中的数百万、数十亿或数万亿个参数可能会发现大数据集中的相关模式，这可能并不重要。此外，我们必须质疑我们正在使用的数据的客观性和准确性。仅仅因为一个数据集是数字化的或大规模的，并不意味着它以我们需要的方式包含了机器学习模型可以学习的信息。最后的关键是缺乏可重复性。如果我们应用数据科学方法，难怪数据科学和机器学习存在众所周知的可重复性问题。重现实验设置和繁琐的技术步骤已经够难了，但是要求其他人像我们一样应用确认偏见和其他——通常未记录的——实验设计错误来复制我们有缺陷的结果几乎是不可能的。
- en: The Scientific Method
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 科学方法
- en: 'Although the data-scientific method is often exciting, fast, and easy, we have
    to find ways to apply the tried-and-true scientific method to our work instead
    for high-risk ML systems. The steps in this subsection illustrate just one way
    that applying the traditional scientific method to an ML project could work. When
    reading the following steps, think about them in comparison to the data-scientific
    method. Notice the focus on avoiding confirmation bias, outcomes versus technology,
    collecting appropriate data, and reproducing results:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数据科学方法通常令人兴奋、快速且简单，但我们必须找到方法将传统的科学方法应用于我们的工作，尤其是对于高风险的机器学习系统。本小节的步骤仅展示了将传统科学方法应用于机器学习项目的一种方式。阅读以下步骤时，请将其与数据科学方法进行比较。注意避免确认偏见、结果与技术的对比、收集适当的数据以及重现结果的重点：
- en: Develop a credible hunch (e.g., based on prior experiments or literature review).
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 发展一个可信的直觉（例如，基于先前的实验或文献回顾）。
- en: Record our hypothesis (i.e., the intended real-world outcome of our ML system).
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录我们的假设（即，我们机器学习系统预期的真实世界结果）。
- en: Collect appropriate data (e.g., using design of experiment approaches).
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集适当的数据（例如，使用实验设计方法）。
- en: 'Test the hypothesis that the ML system has the intended in vivo effect on a
    treatment group:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试假设：机器学习系统对治疗组是否具有预期的体内效应。
- en: Use [CEM](https://oreil.ly/RxSH-) or [FLAME](https://oreil.ly/AFb4z) to construct
    control and treatment groups from collected observational data, or design a controlled
    experiment (e.g., using a double-blind random construct).
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 [CEM](https://oreil.ly/RxSH-) 或 [FLAME](https://oreil.ly/AFb4z) 从收集的观察数据中构建对照组和治疗组，或设计一个双盲随机实验。
- en: Test for statistically significant in vivo effects in the treatment group.
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试治疗组在体内是否存在统计学显著效应。
- en: Reproduce.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复现。
- en: This proposal represents a wholesale change in most data science workflows,
    so we’ll go into more detail on each step. First, we try to base our system designs
    on in-depth literature reviews or past successful experiments. Then we document
    our hypothesis, somewhere public like a GitHub repository, where others will notice
    if we change it. That hypothesis should make sense (i.e., have construct validity),
    be about the intended real-world outcome of the system, and not about, say, XGBoost
    beating LightGBM. We should try not to use whatever data is available. Instead,
    we should try to use data that is appropriate. This might mean collecting specific
    data, and working with statisticians or survey experts to ensure our collected
    data obeys known tenets of experimental design. Also, we have to remind ourselves
    that it doesn’t necessarily matter that our validation and test error metrics
    look good; if we’re doing data analysis and engaging in multiple subexperiments
    using grid searches, we are [overfitting to test data](https://oreil.ly/rnNWq).
    While we value positive results in test data, our focus should be on measuring
    a significant treatment effect in the real world. Did our system do the thing
    we said it would? Can we measure that in some credible way, say, using [coarsened
    exact matching](https://oreil.ly/ahsMd) to create treatment and control groups,
    paired with A/B testing and statistical hypothesis testing on people who were
    treated with the system and people who were not? Finally, we try not to assume
    that our system really works until someone else—like a skilled model validator—reproduces
    our results.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这个提议代表了大多数数据科学工作流程的彻底改变，因此我们将更详细地讨论每个步骤。首先，我们试图基于深入的文献回顾或过去成功的实验来设计我们的系统。然后我们将我们的假设记录在一个公共的地方，比如
    GitHub 仓库，这样其他人如果改变它们就会注意到。那个假设应该是有意义的（即具有构建有效性），关于系统预期的真实世界结果，并且不是关于例如 XGBoost
    能否击败 LightGBM。我们应该尽量避免使用现有的任何数据。相反，我们应该尝试使用合适的数据。这可能意味着收集特定的数据，并与统计学家或调查专家合作，以确保我们收集的数据遵循已知的实验设计原则。此外，我们必须提醒自己，我们的验证和测试错误度量看起来很好并不一定重要；如果我们正在进行数据分析，并参与使用网格搜索进行多个子实验，我们正在过拟合测试数据。虽然我们重视测试数据中的积极结果，但我们的重点应该是在真实世界中测量显著的治疗效果。我们的系统是否做到了我们说它会做的事情？我们能以某种可信的方式测量这一点吗，比如使用粗略的精确匹配来创建治疗和对照组，配合
    A/B 测试和统计假设检验，用系统处理过的人和未处理过的人？最后，我们尽量不要假设我们的系统真的有效，直到其他人，比如熟练的模型验证者，重现我们的结果。
- en: We acknowledge that such a drastic change in data science is at best aspirational,
    but when we approach high-risk ML projects, we need to try. We have an obligation
    to avoid the data-scientific method for high-risk ML projects, because system
    failures impact real human beings, and they do so quickly and at scale.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们承认这种数据科学上的激烈变化最多只是理想化的，但当我们接近高风险的机器学习项目时，我们需要尝试。我们有责任避免高风险机器学习项目的数据科学方法，因为系统失败会迅速而大规模地影响真正的人类。
- en: Evaluation of Published Results and Claims
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估已发布的结果和声明。
- en: Another issue that prevents us from applying the scientific method is that we
    may have forgotten how to validate published claims in all the excitement around
    ML. Many of the sources we look to for information—Medium, Substack, Quora, LinkedIn,
    Twitter, and other social-oriented platforms—are not peer-reviewed publications.
    Just because it’s fun to publish on Medium or Substack (and we do), and they are
    convenient places to learn new things, we have to remember anyone can say anything
    in these outlets. We should be skeptical of results reported on social media unless
    they are directly restating results published in more credible outlets or proven
    out in some other independent experiment.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个阻止我们应用科学方法的问题是，在ML的兴奋中，我们可能已经忘记了如何验证所有这些声称的发表。我们获取信息的许多来源——Medium、Substack、Quora、LinkedIn、Twitter和其他社交定向平台——通常不是经过同行评审的出版物。仅仅因为在Medium或Substack上发布是有趣的（我们确实这样做了），它们是学习新事物的方便场所，但我们必须记住，这些平台上的任何人都可以说任何事情。除非它们直接重申了更可信出版物中发布的结果或在其他独立实验中得到了证实，否则我们应对社交媒体上报告的结果持怀疑态度。
- en: Also, preprint services like arXiv are not peer-reviewed. If we find something
    interesting there, we should look to see if it was actually published later in
    a respected journal, or at least in a conference proceeding, before acting on
    the idea. Even for peer-reviewed journal publications or textbooks, we should
    take the time to independently understand and validate the claims when possible.
    If all the citations on a paper are for debunked pseudoscience, that’s a very
    bad sign. Finally, we do acknowledge that our experience dictates that academic
    approaches often must be adapted to real-world applications. But it’s still better
    to build on the solid foundation of well-cited academic research, than to build
    on the shifting sands of blog and social posts.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，像 arXiv 这样的预印本服务并未经过同行评审。如果我们在那里找到了有趣的东西，我们应该查看是否实际上在尊重的期刊中发表，或者至少在会议记录中发表后再考虑采取行动。即使对于经过同行评审的期刊出版物或教科书，我们也应该花时间独立理解和验证这些主张。如果一篇论文上所有的引用都是已经揭穿的伪科学，那是一个非常糟糕的信号。最后，我们确实承认，我们的经验告诉我们，学术方法通常必须适应现实世界的应用。但是，还是建议在稳固的学术研究基础上建立，而不是在博客和社交媒体帖子的流变沙上建立。
- en: Warning
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Blog, newsletter, and social media content are usually not sources of authoritative
    science and engineering information.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 博客、新闻通讯和社交媒体内容通常不是权威科学和工程信息的来源。
- en: Some well-funded research groups at tech companies may also be pushing the limits
    of what is considered a research achievement versus an engineering feat. Think
    for a moment about language models (LMs), often the prize AI achievements of tech
    research groups. Could even a moderately well-funded academic research group rebuild
    one of these models? Do we know what training data has been used, or have we seen
    the code? Haven’t these systems failed [fairly frequently](https://oreil.ly/4blT4)?
    Traditionally, scientifically accepted research results have been reproducible,
    or at minimum, verifiable. While we’re not doubting the benchmark scores tech
    companies publish for their LMs, we are questioning if they’re meaningfully reproducible,
    verifiable, or transparent enough to be considered research, rather than engineering,
    achievements.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一些资金充足的科技公司的研究团队可能也在推动被视为研究成就与工程成就的界限。请花一点时间思考语言模型（LMs），通常是技术研究团队的宝贵AI成就。即使是一支资金适中的学术研究团队能否重建其中一个模型？我们知道使用了哪些训练数据，或者我们看到了代码吗？这些系统难道没有经常[失败](https://oreil.ly/4blT4)吗？传统上，科学上被接受的研究结果应该是可重复的，或者至少是可验证的。虽然我们不怀疑技术公司发布的LMs的基准分数，但我们质疑它们是否足够具有意义地可重复、可验证或透明，以便被视为研究而非工程成就。
- en: Note
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Readers should understand that although it underwent editorial and technical
    review, this book is not peer-reviewed. This is one more reason we attempted to
    align the book to external standards such as the NIST AI Risk Management Framework.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 读者应理解，尽管这本书经过了编辑和技术审查，但它并没有经过同行评审。这是我们试图将该书与 NIST AI 风险管理框架等外部标准对齐的另一个原因。
- en: Furthermore, because ML is a commercial field—where the aim of a great deal
    of research and ML engineering is to be implemented in a commercial solution,
    and where many researchers are plucked away from academia for high-paying industry
    engineering jobs—we have to be honest about conflicts of interest. If a company
    plans to sell a technology, we should take any reported results with a grain of
    salt. It’s probably not a fully credible claim for tech company X to publish impressive
    results about an AI system, when they are planning on selling the technology.
    And it’s worse if the published results don’t undergo external, independent, and
    objective peer-review. To be blunt, company X saying its own technology is great
    is not really credible, no matter how long the whitepaper or how much the LaTeX
    template looks like a NeurIPS paper. We have to be careful with self-reported
    results from commercial entities and ML vendors.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于ML是一个商业领域——在这个领域中，大量研究和ML工程的目标是在商业解决方案中实施，并且许多研究人员被从学术界挖掘到高薪的工业工程工作中——我们必须诚实地面对利益冲突。如果一家公司计划销售某项技术，我们应该对其报道的结果持谨慎态度。当一家技术公司计划销售技术时，它发表关于AI系统的惊人结果可能不是完全可信的声明。如果这些发布的结果没有经过外部、独立和客观的同行评审，情况会更糟。坦率地说，公司X说他们自己的技术很棒并不真实可信，无论白皮书有多长或LaTeX模板看起来多像NeurIPS论文。我们必须小心处理商业实体和ML供应商自报的结果。
- en: There’s a lot of hype and slick advertisements out there. When getting ideas
    about our next project, or just trying to understand what is hype and what is
    real, we should be more selective. While we might miss out on some new ideas by
    focusing on well-cited academic journals and textbooks, we’ll have a much clearer
    idea of what’s actually possible. We’ll also be more likely to base our next project
    on solid, reproducible ideas instead of hype or marketing copy. And because it’s
    much harder to fake the success of high-risk ML applications, as compared to demos,
    blog posts, or lower-risk applications, we’ll be more likely to succeed in the
    long run, even if it takes us longer to get started and our plans don’t sound
    as exciting. In the end, real success on hard problems is better than more demos,
    blog posts, and success in trivial use cases.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多夸大和花哨的广告。在获取关于下一个项目的想法或者只是试图理解什么是炒作和什么是真实的时候，我们应该更加慎重。虽然通过关注广受引用的学术期刊和教科书可能会错过一些新的想法，但我们将更清楚地了解到实际可能发生的事情。我们也更有可能基于坚实且可重复的想法而不是炒作或市场宣传来规划我们的下一个项目。因为相比于演示、博客文章或低风险应用，高风险ML应用的成功更难伪造，所以即使我们花费更长时间开始并且我们的计划听起来并不那么令人激动，我们在长远来看更有可能成功。最终，在解决难题时真正的成功比更多的演示、博客文章和在琐碎用例中的成功更为重要。
- en: Apply External Standards
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用外部标准
- en: 'For a long time, standards around AI and ML were largely absent. Not so anymore.
    Standards are starting to be defined. If we’re doing something hard with ML, and
    we’re honest with ourselves, we want help and advice. A great place to get help
    and advice for high-risk ML projects is authoritative standards. In this section,
    we’ll focus on standards from the US Federal Reserve Bank (FRB), NIST, the EU
    AI Act, and the International Organization for Standardization (ISO), and how
    we think they can best be used. The FRB model risk management (MRM) guidance and
    NIST AI Risk Management Framework (RMF) both have a very strong culture and process
    focus, although NIST also gets into some technical details. The annexes of the
    EU AI Act are great for definitions and documentation, and ISO provides a lot
    of definitions too, and also good technical advice. These sources help us think
    through many different types of risk and risk mitigation, and help ensure we’re
    not forgetting something obvious in high-risk ML projects:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 长期以来，关于人工智能（AI）和机器学习（ML）的标准大多缺失。但现在不同了。标准开始被定义。如果我们在处理复杂的ML问题时，如果对自己诚实，我们希望得到帮助和建议。获取高风险ML项目帮助和建议的好地方是权威标准。在本节中，我们将重点关注来自美国联邦储备银行（FRB）、NIST、欧盟AI法案和国际标准化组织（ISO）的标准，以及我们认为如何最好地利用它们。FRB模型风险管理（MRM）指南和NIST
    AI风险管理框架（RMF）都非常注重文化和流程，尽管NIST还涉及一些技术细节。欧盟AI法案的附件对于定义和文档非常有帮助，ISO也提供了许多定义和良好的技术建议。这些资源帮助我们思考许多不同类型的风险和风险缓解，并帮助确保在高风险ML项目中我们没有忽视明显的事情：
- en: Model risk management guidance
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 模型风险管理指南
- en: 'We’ve extolled the virtues of the [“Supervisory Guidance on Model Risk Management”](https://oreil.ly/Gy_ol)
    earlier in the book, and we’ll do it one more time here. Just don’t look to this
    guidance for low-level technical advice. Look to it when trying to set up governance
    or risk management structures for an organization. Universal lessons that can
    be gleaned from this guidance include the following:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书早些时候就赞美了[“模型风险管理监管指南”](https://oreil.ly/Gy_ol)，我们现在再次赞美一下。只是不要期望这些指导为低级技术提供建议。在尝试为组织建立治理或风险管理结构时，请参考这些指导。可以从这些指导中获得的通用经验包括以下内容：
- en: Culture reigns.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 文化至上。
- en: If an organization’s culture doesn’t respect risk management, risk management
    doesn’t work.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个组织的文化不尊重风险管理，那么风险管理就不起作用。
- en: Risk management starts from the top.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 风险管理从最高层开始。
- en: Boards and senior executives must be active in ML risk management.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 董事会和高级管理人员必须在ML风险管理中积极参与。
- en: Documentation is a fundamental risk control.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 文档是基本的风险控制措施。
- en: Write out how our models work so others can review our thinking.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 写出我们的模型工作原理，以便他人可以审查我们的思路。
- en: Testing should be an independent and high-stature function.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 测试应是独立且高声望的功能。
- en: Testers should be empowered to pause or terminate development work.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 测试人员应被授权暂停或终止开发工作。
- en: People must be incentivized for engaging in risk management.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 必须激励人们参与风险管理。
- en: It’s too hard to do for free.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于免费来说太难了。
- en: 'Additionally, if readers are looking to have their mind blown by ML risk management,
    then check out the [*Comptroller’s Handbook: Model Risk Management*](https://oreil.ly/jR7Wl),
    and particularly the internal control questionnaire. These are the steps banking
    regulators go through when conducting regulatory exams, and we’d suggest taking
    a peek just for art-of-the-possible purposes, and keeping in mind it’s only part
    of what large banks are expected to do to keep their ML risks under control. Also,
    these risk controls were highly influential to the NIST AI RMF, which cites both
    supervisory guidance and the *Comptroller’s Handbook* many times. It’s good to
    familiarize yourself with them, as they may shape future regulation or risk management
    guidance in your industry, sector, or vertical. These resources themselves, the
    supervisory guidance and the *Comptroller’s Handbook*, are also likely to continue
    to mature slowly.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果读者希望通过ML风险管理大开眼界，请查阅[*“监理指南：模型风险管理”*](https://oreil.ly/jR7Wl)，特别是内部控制问卷。这些是银行监管机构在进行监管检查时要经历的步骤，我们建议您仅出于可能性的艺术目的偷看一下，并记住这只是大银行为控制其ML风险而预期做的部分工作。此外，这些风险控制对于NIST
    AI RMF具有重大影响，该框架多次引用了监督指导和*监理手册*。熟悉这些资源很重要，因为它们可能塑造您所在行业、部门或垂直领域的未来监管或风险管理指导。这些资源本身——监督指导和*监理手册*——也可能会继续缓慢发展。
- en: NIST AI Risk Management Framework
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: NIST AI风险管理框架
- en: The [NIST AI Risk Management Framework](https://oreil.ly/8yGFz) expands upon
    MRM guidance in meaningful ways. In banking, where MRM is practiced, model risk
    managers can usually count on other functions in the bank to worry about privacy,
    security, and fairness, allowing them to focus primarily on system performance.
    The RMF brings these and other trustworthy characteristics—validity, reliability,
    safety, bias management, security, resiliency, transparency, accountability, explainability,
    interpretability, and privacy—under one banner of AI risk management, which is
    more realistic for nonbank organizations.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[NIST AI风险管理框架](https://oreil.ly/8yGFz)在银行业实践的MRM指导基础上进行了有意义的扩展。在银行业实践MRM时，模型风险管理人员通常可以指望银行中的其他职能部门关注隐私、安全和公平性问题，从而能够主要专注于系统性能。RMF将这些以及其他可信特征——有效性、可靠性、安全性、偏见管理、安全性、弹性、透明度、问责制、可解释性、可解读性和隐私——统一纳入AI风险管理的旗帜下，对于非银行机构更为现实。'
- en: The AI RMF provides high-level advice across all these desiderata, and importantly,
    states outright that they are all connected. Unlike MRM guidance, the RMF highlights
    diversity and inclusion as risk controls, and brings cyber risk controls like
    incident response and bug bounties into the fold of AI risk controls. The NIST
    guidance is also broken into a number of documents and interactive websites. While
    the [core RMF document](https://oreil.ly/q27WB) provides higher-level guidance,
    a number of additional resources get deeper into technical and risk management
    details. For example, the [AI Risk Management Playbook](https://oreil.ly/hd5oV)
    provides exhaustive guidance on risk management with accompanying documentation
    advice and references. Related documents, such as NIST SP1270 and NISTIR 8367,
    [“Psychological Foundations of Explainability and Interpretability in Artificial
    Intelligence”](https://oreil.ly/UJ2EM), provide immensely useful and detailed
    guidance on specific topics. The RMF is a long-term project. Watch for more high-quality
    risk management advice to emerge in coming years.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: AI RMF提供跨所有这些愿望的高层建议，并且重要的是，它明确指出它们都是相互关联的。与MRM指南不同，RMF强调多样性和包容性作为风险控制，并将类似事件响应和漏洞悬赏这样的网络安全风险控制引入AI风险控制范畴。NIST指南也分为多个文件和互动网站。虽然[核心RMF文件](https://oreil.ly/q27WB)提供了更高层次的指导，但许多额外的资源深入探讨了技术和风险管理细节。例如，[AI风险管理手册](https://oreil.ly/hd5oV)提供了有关风险管理的详尽指导，以及相关文档建议和参考资料。相关文件，如NIST
    SP1270和NISTIR 8367，《人工智能中可解释性和可理解性的心理基础》，提供了特定主题的极其有用和详细的指导。RMF是一个长期项目。请期待未来几年出现更多高质量的风险管理建议。
- en: EU AI Act Annexes
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 欧盟AI法案附件
- en: Go here for high-level definitions, including a definition of high-risk ML,
    and for documentation advice. Annex I of the [EU AI Act](https://oreil.ly/5WVMj)
    lays out a solid definition for AI. We need uniform and agreed-upon definitions
    for risk management. It’s important because if a policy or test is supposed to
    be applied to all AI systems in an organization, we can expect at least one group
    or individual to wiggle out of the requirement by claiming they don’t do AI. Annex
    III describes specific applications that are considered high risk, such as biometric
    identification, management of infrastructure, education, employment, government
    or utility services, credit scoring, law enforcement, immigration and border control,
    and criminal justice. Finally, Annex IV provides good direction on what should
    be documented about an ML system. If our organizational preference is somewhere
    between massive MRM documents and minimal model cards, we’ll appreciate that the
    annexes have also put forward a good framework for ML system documentation. Keep
    in mind, the AI Act is draft regulation as of the publishing of this book, but
    passage is considered likely.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 前往此处查看高层次定义，包括高风险机器学习的定义以及文档建议。[欧盟AI法案](https://oreil.ly/5WVMj)的附件I为AI提供了一个坚实的定义。我们需要统一和达成一致的风险管理定义。这很重要，因为如果一个政策或测试应该适用于组织中的所有AI系统，我们可以预期至少有一组或一个人会通过声称他们不从事AI来逃避要求。附件III描述了被视为高风险的具体应用，例如生物识别身份验证、基础设施管理、教育、就业、政府或公共事业服务、信用评分、执法、移民和边境控制以及刑事司法。最后，附件IV为ML系统应该记录的内容提供了良好的指导。如果我们的组织偏向于庞大的MRM文件和最小化的模型卡之间的某个地方，我们将欣赏到附件还为ML系统文档提出了良好的框架。请注意，AI法案截至本书出版时为草案法规，但通过的可能性被认为很大。
- en: ISO AI Standards
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ISO AI标准
- en: The burgeoning body of [ISO AI Standards](https://oreil.ly/BxcQz) is the place
    to look for lower-level technical guidance and mountains of technical defini­tions.
    While a great deal of the standards are still under development, many like [ISO/IEC
    PRF TS 4213—​Assessment of Machine Learning Classification Performance](https://oreil.ly/bMczF),
    [ISO/IEC TR 24029-1:2021—​Assessment of the Robustness of Neural Networks](https://oreil.ly/AvPNZ),
    and [ISO/IEC TR 29119-11:2020—​Guidelines on the Testing of AI-Based Systems](https://oreil.ly/MwV_T)
    are available now. The standards that are available can be really helpful in making
    sure that our technical approaches are complete and thorough. Unlike the other
    guidance discussed in this section, ISO standards are usually not free. But they
    are not terribly expensive either, and much less so than an AI incident. Watch
    the ISO AI standards as they become more fulsome over time for additional valuable
    technical guidance and risk management resources.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 不断发展的[ISO AI标准](https://oreil.ly/BxcQz)是寻找低级技术指导和大量技术定义的好地方。虽然许多标准仍在开发中，但像[ISO/IEC
    PRF TS 4213—​机器学习分类性能评估](https://oreil.ly/bMczF)、[ISO/IEC TR 24029-1:2021—​神经网络稳健性评估](https://oreil.ly/AvPNZ)和[ISO/IEC
    TR 29119-11:2020—​AI系统测试指南](https://oreil.ly/MwV_T)等已经可以使用。这些可用的标准确实有助于确保我们的技术方法是完整和彻底的。与本节讨论的其他指南不同，ISO标准通常不是免费的。但它们也不是特别昂贵，比AI事件要便宜得多。随着时间的推移，监视ISO
    AI标准的增量完善会为额外有价值的技术指导和风险管理资源提供支持。
- en: Note
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Applying external standards, like those from ISO and NIST, increases the quality
    of our work and increases defensibility when something inevitably goes wrong.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 应用外部标准，如ISO和NIST的标准，可以提高我们的工作质量，并在不可避免发生问题时增加防御性。
- en: There are other standards from groups like the Institute of Electrical and Electronics
    Engineers (IEEE), the American National Standards Institute (ANSI), or the Organisation
    for Economic Co-operation and Development (OECD) that may also work well for your
    organization. One more thing to remember about these standards is that applying
    them not only helps us do better work, but they also help us justify our choices
    when scrutiny arises. If we’re doing high-risk work in ML, we should expect scrutiny
    and oversight. Justifying our workflow and risk controls with such standards is
    going to play out much better than basing them off something we made up or found
    on a blog or social site. In short, using these standards makes us and our work
    look better because they are known to make technology better.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他组织提供的标准，比如电气和电子工程师学会(IEEE)、美国国家标准学会(ANSI)或经济合作与发展组织(OECD)的标准，这些标准可能也适合您的组织。关于这些标准，还有一件事要记住，应用它们不仅有助于我们做得更好，而且在审查时也有助于我们证明我们的选择。如果我们在机器学习的高风险工作中，应该期待审查和监督。用这些标准来证明我们的工作流程和风险控制会比基于我们自己编造或在博客或社交网站上找到的东西更好。简而言之，使用这些标准使我们和我们的工作看起来更好，因为它们已知能够提升技术水平。
- en: Commonsense Risk Mitigation
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常识性风险缓解
- en: 'The more time we spend working on high-risk ML projects, the more we develop
    instincts for what will go wrong and what will go right. The advice detailed in
    this section can probably be found in some standards or authoritative guidance,
    but we’ve learned it the hard way. These points are a collection of commonsense
    advice that should help to fast-forward a practitioner’s instincts for working
    with high-risk ML systems. They may seem basic or obvious, but making ourselves
    stick to these hard-won lessons is difficult. There is always market pressure
    to move faster, test less, and do less about risk. That may be fine for lower-risk
    applications, but for serious use cases, it pays to slow down and think. The steps
    we detail here help to elucidate why and how we do that. Basically, we should
    think before we code, test our code, and allow enough time and resources for these
    processes to happen:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在高风险的机器学习项目上花费的时间越多，我们对什么可能出错和什么可能顺利的直觉就越发展。这一部分详细的建议可能可以在一些标准或权威指南中找到，但我们是通过艰辛的方式学到的。这些观点是一系列常识性建议的集合，应该有助于加速从业者在处理高风险机器学习系统时的直觉。它们可能看起来基础或显而易见，但是让我们坚持这些艰难赢得的经验教训并不容易。市场总是在推动我们更快地前进、测试更少、对风险做更少的事情。这对于低风险应用可能没问题，但对于严肃的使用场景来说，放慢节奏并思考是值得的。我们在这里详细描述的步骤有助于阐明我们为什么以及如何做到这一点。基本上，我们应该在编码之前思考，测试我们的代码，并为这些过程提供足够的时间和资源：
- en: Start simple.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 简单开始。
- en: It can be exciting to use complex ML systems, based on deep learning, stacked
    generalization, or other sophisticated techniques, for high-risk applications.
    However, we shouldn’t do this unless the problem calls for that level of complexity.
    Complexity tends to mean more failure modes, and less transparency. Less transparency
    usually means systems are harder to fix and harder to review. When approaching
    a high-risk project, we must weigh the possibility of failure and resultant harms
    against our desire to play with cool tech. Sometimes it’s better to start with
    a simpler, more clearly understood approach, and then iterate to more complex
    solutions as we prove out our system over time.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基于深度学习、堆叠泛化或其他复杂技术的复杂机器学习系统可能会令人兴奋，尤其是在高风险应用中。然而，除非问题需要那种复杂度水平，否则我们不应这样做。复杂性往往意味着更多的故障模式和更少的透明度。透明度较少通常意味着系统更难修复和审查。在处理高风险项目时，我们必须权衡失败可能性及其带来的损害与我们玩弄高科技的愿望之间的关系。有时候，最好从更简单、更清晰理解的方法开始，然后随着时间的推移逐步迭代到更复杂的解决方案。
- en: Avoid past failed designs.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 避免过去的设计失败。
- en: Don’t repeat the mistakes of the past with ML. When approaching a high-stakes
    problem, we should review past failed attempts to solve similar problems. This
    is the change thesis of the [AI incident database](https://oreil.ly/VlclU). It’s
    one of several resources we should check out to help ourselves avoid past ML mistakes.
    We should also ask around internally at our organizations. People have probably
    attempted to solve the problem we’re trying to solve before, especially if it’s
    an important problem.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 不要重复过去的机器学习失败。在处理高风险问题时，我们应该回顾过去类似问题的失败尝试。这正是[AI事故数据库](https://oreil.ly/VlclU)的变革论点之一。这是我们应该查看的几个资源之一，以帮助我们避免过去的机器学习错误。我们还应该在组织内部询问。可能有人之前就尝试过解决我们正在尝试解决的问题，尤其是如果这是一个重要的问题。
- en: Allocate time and resources for risk management.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为风险管理分配时间和资源。
- en: Risk management takes time, people, money, and other resources. The same team
    that built a demo of the system is probably not big or broad enough to build a
    production version of the system and manage its risks. If we’re working on a high-risk
    ML system, we’ll need more resources for hardened engineering, testing, documentation,
    handling user feedback, and reviewing risks. We also need more time. Organizations,
    managers, and data scientists themselves tend to underestimate the time needed
    to build even a mundane ML system. If you’re working with high-risk ML, you’ll
    need to extend your timeline, perhaps even by multiples of what might be required
    for a lower-risk system.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 风险管理需要时间、人力、资金和其他资源。建立系统演示的团队可能不够大或者广泛，以至于无法构建系统的生产版本并管理其风险。如果我们正在处理高风险的机器学习系统，我们需要更多的资源进行强化工程、测试、文档编制、处理用户反馈以及风险评审。我们还需要更多的时间。组织、管理者以及数据科学家往往低估了即使是构建平凡机器学习系统所需的时间。如果你在处理高风险的机器学习项目，你可能需要延长项目周期，甚至可能是低风险系统所需周期的几倍。
- en: Apply standard software quality approaches.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 应用标准的软件质量方法。
- en: 'We’ve said it before and we’ll say it again. There is no reason ML systems
    should be exempt from standard software QA processes. For high-risk systems, we
    probably need to apply the kitchen sink of software QA: unit tests, integration
    tests, functional tests, chaos testing, random attacks, and more. If you need
    a refresher on how these techniques can be applied to ML systems, review [Chapter 3](ch03.html#unique_chapter_id_3).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以前说过，现在再说一遍。没有理由让机器学习系统免于标准软件质量保证流程。对于高风险系统，我们可能需要应用全方位的软件质量保证措施：单元测试、集成测试、功能测试、混乱测试、随机攻击等等。如果你需要回顾这些技术如何应用于机器学习系统，可以参考[第三章](ch03.html#unique_chapter_id_3)。
- en: Limit software, hardware, and network dependencies.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 限制软件、硬件和网络依赖。
- en: Every piece of third-party software we use, whether it’s open source or proprietary,
    increases the risk of our system. We can’t always know how risks were managed
    in those dependencies. Are they secure? fair? compliant with data privacy laws?
    It’s simply hard to know. The same notions apply for network dependencies. Are
    the machines we’re connecting to secure? Are they always going to be available?
    The answer is, at least over a longer period of time, probably not. While specialized
    hardware tends to bring less security and failure risks than third-party software
    and extra network connections, it does increase complexity. Increased complexity
    tends to increase risk by default. Minimizing and simplifying software, hardware,
    and network dependencies will likely cut down on surprises, necessary change-management
    processes, and required risk management resources.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 每一个我们使用的第三方软件，无论是开源还是专有的，都会增加我们系统的风险。我们并不能总是知道这些依赖项是如何管理风险的。它们是否安全？是否公平？是否符合数据隐私法？这些都很难确认。同样的概念也适用于网络依赖性。我们连接的机器是否安全？它们是否始终可用？答案是，至少在较长的时间内，可能并不是。虽然专用硬件通常比第三方软件和额外的网络连接带来更少的安全和故障风险，但它确实增加了复杂性。增加的复杂性通常默认会增加风险。减少和简化软件、硬件和网络依赖性可能会减少意外情况，必要的变更管理流程以及所需的风险管理资源。
- en: Limit connectivity between multiple ML systems.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 限制多个机器学习系统之间的连接。
- en: If the risks of one ML system are difficult to enumerate, then what happens
    when we start making pipelines of ML-based decisions or technologies? The results
    can be extremely unpredictable. Be careful when connecting ML systems to large
    networks, like the internet, or connecting many ML systems together. Both scenarios
    can lead to surprise harms, or even systemic failures.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个机器学习系统的风险难以列举，那么当我们开始构建基于机器学习的决策或技术流程时会发生什么？结果可能极不可预测。在连接机器学习系统到像互联网这样的大型网络或将多个机器学习系统连接在一起时要小心。这两种情况都可能导致意外的伤害，甚至系统性的故障。
- en: Restrict system outputs to avoid foreseeable incidents.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 限制系统输出，以避免可预见的事件发生。
- en: If certain outcomes of an ML system are foreseeably problematic, say, allowing
    a self-driving car to accelerate to 200 miles per hour, we don’t have to sit idly
    by and allow our systems to make bad decisions. Use business rules, model assertions,
    numeric limits, or other safeguards to prevent systems from making foreseeable
    bad decisions.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某个机器学习系统的某些结果可预见地存在问题，比如允许自动驾驶汽车加速到每小时200英里，我们不必坐视不管，让我们的系统做出错误的决策。使用业务规则、模型断言、数值限制或其他保障措施，防止系统做出可预见的错误决策。
- en: Remember that games are not the real world.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 记住游戏并非现实世界。
- en: Data science contest leaderboards that rank models based on single metrics,
    with no consideration of variance or real-world trade-offs, are not adequate for
    the evaluation of real-world decision making. Neither is ML systems playing games
    successfully. Just because an ML system succeeds in a game, does not mean it will
    succeed in the real world. In games, we know all the rules, and the rules don’t
    change. In some cases, we have access to all possible data relating to games—e.g.,
    all possible outcomes or all possible moves. This isn’t realistic. In the real
    world, we don’t know all the rules, and the rules governing a system can change
    dramatically and quickly. We also don’t have access to all the data we need for
    good decision making. An ML system succeeding at a game can be a tremendous research
    achievement, and also irrelevant to high-risk sociotechnical ML systems deployed
    in the world.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学竞赛的排行榜根据单一指标排名模型，没有考虑到方差或现实世界中的权衡，不足以评估现实世界的决策。机器学习系统在游戏中成功也不是。仅因为一个机器学习系统在游戏中成功，并不意味着它在现实世界中也会成功。在游戏中，我们知道所有的规则，规则不会改变。在某些情况下，我们可以访问与游戏相关的所有可能数据，例如所有可能的结果或所有可能的移动。但这并不现实。在现实世界中，我们不知道所有的规则，系统的规则可以急剧而迅速地改变。我们也无法获得所有需要做出良好决策所需的数据。一个机器学习系统在游戏中的成功可能是一个巨大的研究成就，但对于部署在世界上的高风险社会技术机器学习系统来说可能是无关紧要的。
- en: Monitor unsupervised or self-updating systems carefully.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细监控无监督或自我更新的系统。
- en: Unsupervised systems, trained without ground truth, and self-updating systems
    (e.g., reinforcement, adaptive, or online learning) are inherently higher-risk.
    It’s hard to understand whether unsupervised systems perform well enough before
    we deploy them, and it’s hard to predict how a self-updating system might behave.
    While all ML systems should be monitored, unsupervised and self-updating systems
    deployed for high-stakes applications require real-time monitoring for performance,
    bias, and security issues. Such monitoring should also alert humans as soon as
    problems are detected, and these systems should likely be built with kill switches.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督系统、在没有地面真相的情况下训练的系统以及自更新系统（例如，强化、适应或在线学习）本质上具有更高风险。在部署这些系统之前，很难理解无监督系统是否表现足够良好，并且难以预测自更新系统可能的行为。虽然所有机器学习系统都应该进行监控，但部署到高风险应用程序的无监督和自更新系统需要实时监控性能、偏差和安全问题。这类监控也应尽快在检测到问题时提醒人类，并且这些系统可能需要配备关闭开关。
- en: Understand ethical and legal obligations for human subjects.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 理解人类受试者的伦理和法律义务。
- en: Given that many ML deployments involve collecting sensitive data or are themselves
    implicit or explicit experiments on human users, we should familiarize ourselves
    with our organization’s institutional review board (IRB) policies, [basic guidelines
    for human experimentation](https://oreil.ly/1ptk7), and other legal and ethical
    obligations for conducting experiments on human users.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于许多机器学习部署涉及收集敏感数据或本身就是对人类用户进行的隐含或显性实验，我们应熟悉我们组织的机构审查委员会（IRB）政策，[人类实验的基本指南](https://oreil.ly/1ptk7)，以及进行人类实验的其他法律和伦理义务。
- en: Restrict anonymous use.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 限制匿名使用。
- en: If a system doesn’t require anonymous usage, then having users authenticate
    or otherwise prove their identity before using it can drastically cut down on
    hacks, abuses, and other bad behavior involving the system.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果系统不需要匿名使用，则要求用户在使用之前进行身份验证或以其他方式证明其身份可以显著减少涉及系统的黑客攻击、滥用和其他不良行为。
- en: Apply watermarks to AI-generated content.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对AI生成的内容应用水印。
- en: Adding tell-tale markings, characters, and sounds into any AI-generated content
    can help to identify it later, and decrease risks that such content is used for
    deceptive acts.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 向任何AI生成的内容添加显著标记、字符和声音可以帮助识别它，并减少这类内容被用于欺骗行为的风险。
- en: Know when not to use ML.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 知道何时不使用机器学习。
- en: ML doesn’t solve every problem. In fact, there’s a large class of problems we
    know it doesn’t solve well at all. ML doesn’t outperform people or simple models
    in [predicting life outcomes](https://oreil.ly/UyX10), and people and simple models
    aren’t great at this either. ML can’t really learn who will get good grades, face
    eviction, or be laid off from their job. ML also can’t tell from a video who will
    do well in a job, according to [NIST](https://oreil.ly/1QY4W). Prominent ML researchers,
    including Arvind Narayanan, have [called out issues](https://oreil.ly/jMXY7) in
    ML predictions for criminal recidivism, policing, and spotting terrorists. ML
    just isn’t that good at understanding or predicting many human and social outcomes.
    While these are interesting and high-value problems, we shouldn’t try to solve
    them with ML unless we know something that NIST and the US National Academies
    don’t yet know about ML. And social outcomes aren’t the only area where ML systems
    are known to have problems. Remember to look into past failures before getting
    in too deep with a high-risk ML system.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习并不是解决所有问题的万能药。事实上，有一大类问题我们知道它并不擅长解决。在[预测生活结果](https://oreil.ly/UyX10)方面，机器学习并不比人类或简单模型表现更优，而人类和简单模型在这方面也表现不佳。从视频中，机器学习也无法准确预测谁在工作中表现良好，根据[NIST](https://oreil.ly/1QY4W)的说法。包括阿尔温德·纳拉亚南在内的知名机器学习研究人员，指出了机器学习在预测犯罪再犯率、执法和发现恐怖分子方面的问题。机器学习在理解或预测许多人类和社会结果方面表现并不理想。虽然这些问题有趣且具有高价值，但我们不应该试图用机器学习解决它们，除非我们知道一些NIST和美国国家科学院尚不知道的事情。社会结果并不是机器学习系统已知存在问题的唯一领域。在深入涉及高风险机器学习系统之前，记得查看过去的失败案例。
- en: Note
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Don’t be afraid to ask rudimentary questions about design, timing, resources,
    outcomes, and users in your next important ML project.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在你下一个重要的机器学习项目中，不要害怕询问关于设计、时机、资源、成果和用户的基础性问题。
- en: By combining these commonsense controls with increased demographic and professional
    diversity, better adherence to the scientific method, more rigorous evaluation
    of published claims, the application of authoritative external standards, and
    all the governance and technical goodies in previous chapters, you should be on
    your way to better outcomes in difficult ML applications. Of course, it’s difficult
    to get buy-in for all this extra work and, if you do, to find the time to do it
    all. Don’t try to boil the ocean. Recall [Chapter 1](ch01.html#unique_chapter_id_1)
    and risk management basics. Try to understand what your most serious risks are,
    and mitigate them first.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合这些常识性控制措施与增加的人口和专业多样性，更好地坚持科学方法，更严格地评估已发表声明，应用权威外部标准，以及在前几章中提到的所有治理和技术好处，您应该能够在困难的机器学习应用中取得更好的结果。当然，要获取所有这些额外工作的支持并找到时间来完成这一切并不容易。不要试图做得太多。回想一下[第1章](ch01.html#unique_chapter_id_1)和风险管理基础知识。尝试理解您最严重的风险是什么，并首先减轻它们。
- en: Conclusion
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This book began with lessons on governing the people who build and maintain
    ML systems. It then discussed how to make ML models more understandable to people
    with explainable models and explainable AI. It outlined how to make ML models
    more trustworthy to people with model debugging and security approaches, and it
    highlighted how to make them more fair for people too. This focus on people is
    not a coincidence. Technology is about people. There is almost no reason to make
    technology except for some type of human benefit, and machines don’t feel pain,
    anger, and sadness when they are harmed. People do. Moreover, at least by our
    judgment, people are still smarter than computers. The last decade of ML was all
    about the success of massive unexplainable models trained with almost no human
    input, and we suspect it’s time for the pendulum to swing back the other way to
    some degree. Many ML successes in coming years will entail legal and regulatory
    compliance, improved human interaction with ML, risk management, and tangible
    business outcomes. Make maximizing the benefit and minimizing the harm for people
    the core of your high-risk ML project and you’ll have more success.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 本书从管理构建和维护机器学习系统的人员的教训开始。然后讨论了如何通过可解释模型和可解释人工智能使机器学习模型更加易于理解。它概述了通过模型调试和安全方法使机器学习模型对人们更加可信的方法，并强调了如何使它们对人们更加公平。这种对人们的关注并非偶然。技术是关于人的。除了某种类型的人类利益外，几乎没有理由去开发技术，而机器在受到伤害时不会感到痛苦、愤怒和悲伤，人们会。此外，根据我们的判断，人类仍然比计算机更聪明。过去十年的机器学习主要是关于使用几乎没有人类输入训练的大规模不可解释模型的成功，我们怀疑现在是时候在某种程度上把钟摆摆回来了。未来几年的许多机器学习成功将涉及法律和监管合规性、改进人类与机器学习的互动、风险管理和切实的业务成果。将最大化人类利益和最小化伤害作为您的高风险机器学习项目的核心，您将取得更多成功。
- en: Resources
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: Further Reading
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '[EU AI Act Annexes](https://oreil.ly/CcERN)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[欧盟AI法案附件](https://oreil.ly/CcERN)'
- en: '[ISO AI Standards](https://oreil.ly/cUmGz)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ISO AI标准](https://oreil.ly/cUmGz)'
- en: '[NIST AI Risk Management Framework](https://oreil.ly/fN5BS)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NIST AI风险管理框架](https://oreil.ly/fN5BS)'
- en: '[NIST SP1270: “Towards a Standard for Identifying and Managing Bias in Artificial
    Intelligence”](https://oreil.ly/udvYe)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NIST SP1270：“向标准化人工智能中的偏见识别与管理迈进”](https://oreil.ly/udvYe)'
- en: '[“Supervisory Guidance on Model Risk Management”](https://oreil.ly/IuzZx)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“监管模型风险管理指导”](https://oreil.ly/IuzZx)'
