- en: Chapter 8\. Using TensorFlow to Create Text
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 8 章。使用 TensorFlow 创建文本
- en: '*You know nothing, Jon Snow*'
  id: totrans-1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*你什么都不知道，琼·雪诺*'
- en: '*the place where he’s stationed*'
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*他所驻扎的地方*'
- en: '*be it Cork or in the blue bird’s son*'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*无论是在科克还是在蓝鸟的儿子那里*'
- en: '*sailed out to summer*'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*航行到夏天*'
- en: '*old sweet long and gladness rings*'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*旧甜长和喜悦的戒指*'
- en: '*so i’ll wait for the wild colleen dying*'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*所以我会等待野生的小姑娘去世*'
- en: This text was generated by a very simple model trained on a small corpus. I’ve
    enhanced it a little by adding line breaks and punctuation, but other than the
    first line, the rest was all generated by the model you’ll learn how to build
    in this chapter. It’s kind of cool that it mentions a *wild colleen dying*—if
    you’ve watched the show that Jon Snow comes from, you’ll understand why!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这段文字是由一个在小语料库上训练的非常简单的模型生成的。我稍微增强了它，通过添加换行和标点，但除了第一行外，其余都是由你将在本章学习如何构建的模型生成的。提到*野生的小姑娘去世*有点酷——如果你看过琼·雪诺来自的那部剧，你会明白为什么！
- en: 'In the last few chapters you saw how you can use TensorFlow with text-based
    data, first tokenizing it into numbers and sequences that can be processed by
    a neural network, then using embeddings to simulate sentiment using vectors, and
    finally using deep and recurrent neural networks to classify text. We used the
    Sarcasm dataset, a small and simple one, to illustrate how all this works. In
    this chapter we’re going to switch gears: instead of classifying existing text,
    you’ll create a neural network that can *predict* text. Given a corpus of text,
    it will attempt to understand the patterns of words within it so that it can,
    given a new piece of text called a *seed*, predict what word should come next.
    Once it has that, the seed and the predicted word become the new seed, and the
    next word can be predicted. Thus, when trained on a corpus of text, a neural network
    can attempt to write new text in a similar style. To create the piece of poetry
    above, I collected lyrics from a number of traditional Irish songs, trained a
    neural network with them, and used it to predict words.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几章中，你看到如何使用基于文本的数据来使用 TensorFlow，首先将其标记化为可以由神经网络处理的数字和序列，然后使用嵌入来模拟使用向量的情感，最后使用深度和递归神经网络来分类文本。我们使用了Sarcasm数据集，一个小而简单的数据集，来说明所有这些是如何工作的。在本章中，我们将转换方向：不再分类现有文本，而是创建一个能够*预测*文本的神经网络。给定一个文本语料库，它将尝试理解其中的词语模式，以便在给出一个称为*种子*的新文本时，预测下一个应该出现的词语。一旦有了这个预测，种子和预测的词语将成为新的种子，然后可以预测下一个词语。因此，当神经网络在文本语料库上训练完成后，它可以尝试以类似的风格编写新的文本。为了创建上面的诗歌片段，我收集了许多传统爱尔兰歌曲的歌词，用它们来训练神经网络，并用它来预测词语。
- en: We’ll start simple, using a small amount of text to illustrate how to build
    up to a predictive model, and we’ll end by creating a full model with a lot more
    text. After that you can try it out to see what kind of poetry it can create!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会从简单开始，用少量文字来说明如何建立预测模型，最终会创建一个含有更多文字的完整模型。之后，你可以尝试一下，看它能创造出怎样的诗歌！
- en: To get started, you’ll have to treat the text a little differently from what
    you’ve been doing thus far. In the previous chapters, you took sentences and turned
    them into sequences that were then classified based on the embeddings for the
    tokens within them.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，你必须对待这段文本有所不同，与你迄今为止所做的不同。在前几章中，你将句子转换为序列，然后根据其中标记的嵌入进行分类。
- en: When it comes to creating data that can be used for training a predictive model
    like this one, there’s an additional step where the sequences need to be transformed
    into *input sequences* and *labels*, where the input sequence is a group of words
    and the label is the next word in the sentence. You can then train a model to
    match the input sequences to their labels, so that future predictions can pick
    a label that’s close to the input sequence.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建用于训练这种预测模型的数据时，还有一个额外的步骤，需要将序列转换为*输入序列*和*标签*，其中输入序列是一组词，标签是句子中的下一个词。然后你可以训练一个模型来将输入序列与它们的标签匹配，以便未来的预测可以选择接近输入序列的标签。
- en: Turning Sequences into Input Sequences
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将序列转换为输入序列
- en: When predicting text, you need to train a neural network with an input sequence
    (feature) that has an associated label. Matching sequences to labels is the key
    to predicting text.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测文本时，你需要用一个带有相关标签的输入序列（特征）来训练神经网络。将序列与标签匹配是预测文本的关键。
- en: So, for example, if in your corpus you have the sentence “Today has a beautiful
    blue sky,” you could split this into ‘Today has a beautiful blue” as the feature
    and “sky” as the label. Then, if you were to get a prediction for the text “Today
    has a beautiful blue,” it would likely be “sky.” If in the training data you also
    have “Yesterday had a beautiful blue sky,” split in the same way, and you were
    to get a prediction for the text “Tomorrow will have a beautiful blue,” then there’s
    a high probability that the next word will be “sky.”
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，例如，如果在你的语料库中有句子“今天有美丽的蓝天”，你可以将其分割为“今天有美丽的蓝”作为特征，“天空”作为标签。然后，如果你对文本“今天有美丽的蓝”进行预测，它很可能是“天空”。如果在训练数据中你还有“昨天有美丽的蓝天”，同样方式分割，如果你对文本“明天会有美丽的蓝色”进行预测，那么很可能下一个词是“天空”。
- en: Given lots of sentences, training on sequences of words with the next word being
    the label, you can quickly build up a predictive model where the most likely next
    word in the sentence can be predicted from an existing body of text.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 给定大量的句子，训练使用单词序列及其下一个单词作为标签的模型，可以快速建立起一个预测模型，在文本的现有体系中预测出句子中最可能的下一个单词。
- en: 'We’ll start with a very small corpus of text—an excerpt from a traditional
    Irish song from the 1860s, some of the lyrics of which are as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个非常小的文本语料库开始——这是一首传统爱尔兰歌曲的摘录，这首歌的部分歌词如下：
- en: '*In the town of Athy one Jeremy Lanigan*'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在阿西镇有位杰里米·兰尼根*'
- en: '*Battered away til he hadnt a pound.*'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一直打到他一文不名。*'
- en: '*His father died and made him a man again*'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*他的父亲去世了，使他重新成为一个男子汉。*'
- en: '*Left him a farm and ten acres of ground.*'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*留给他一块农场和十英亩地。*'
- en: '*He gave a grand party for friends and relations*'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*他为亲朋好友举办了一场盛大的派对*'
- en: '*Who didnt forget him when come to the wall,*'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*谁在到达墙壁时没有忘记他，*'
- en: '*And if youll but listen Ill make your eyes glisten*'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如果你愿意听，我会让你的眼睛闪闪发亮，*'
- en: '*Of the rows and the ructions of Lanigan’s Ball.*'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在兰尼根的舞会上的争执和混乱。*'
- en: '*Myself to be sure got free invitation,*'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*我自己确实得到了自由邀请，*'
- en: '*For all the nice girls and boys I might ask,*'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对于所有的好女孩和好男孩，我可能会问，*'
- en: '*And just in a minute both friends and relations*'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*仅仅一分钟内，亲朋好友都来了，*'
- en: '*Were dancing round merry as bees round a cask.*'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*像蜜蜂围着一桶酒快乐地跳舞。*'
- en: '*Judy ODaly, that nice little milliner,*'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*朱迪·奥达利，那位可爱的小帽匠，*'
- en: '*She tipped me a wink for to give her a call,*'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*她给我眨了一下眼睛让我去找她，*'
- en: '*And I soon arrived with Peggy McGilligan*'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*我很快和佩吉·麦吉利根一起到了，*'
- en: '*Just in time for Lanigans Ball.*'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*刚好赶上兰尼根的舞会。*'
- en: 'Create a single string with all the text, and set that to be your data. Use
    `\n` for the line breaks. Then this corpus can be easily loaded and tokenized
    like this:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个包含所有文本的单个字符串，并将其设置为您的数据。使用`\n`表示换行。然后，这个语料库可以像这样轻松加载和分词化：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The result of this process is to replace the words by their token values, as
    shown in [Figure 8-1](#tokenizing_a_sentence).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程的结果是用它们的标记值替换单词，如[图 8-1](#tokenizing_a_sentence)所示。
- en: '![Tokenizing a sentence](Images/aiml_0801.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![句子分词](Images/aiml_0801.png)'
- en: Figure 8-1\. Tokenizing a sentence
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-1\. 句子分词
- en: To train a predictive model, we should take a further step here—splitting the
    sentence into multiple smaller sequences, so, for example, we can have one sequence
    consisting of the first two tokens, another of the first three, etc. ([Figure 8-2](#turning_a_sequence_into_a_number_of_inp)).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个预测模型，我们应该在这里进一步进行一步操作——将句子分割成多个较小的序列，例如，我们可以有一个由前两个标记组成的序列，另一个由前三个标记组成，依此类推（[图 8-2](#turning_a_sequence_into_a_number_of_inp)）。
- en: '![Turning a sequence into a number of input sequences](Images/aiml_0802.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![将序列转换为多个输入序列](Images/aiml_0802.png)'
- en: Figure 8-2\. Turning a sequence into a number of input sequences
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-2\. 将序列转换为多个输入序列
- en: To do this you’ll need to go through each line in the corpus and turn it into
    a list of tokens using `texts_to_sequences`. Then you can split each list by looping
    through each token and making a list of all the tokens up to it.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，您需要逐行遍历语料库中的每一行，并使用`texts_to_sequences`将其转换为标记列表。然后，您可以通过循环遍历每个标记并制作一个包含所有标记的列表，来拆分每个列表。
- en: 'Here’s the code:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是代码：
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Once you have these input sequences, you can pad them into a regular shape.
    We’ll use prepadding ([Figure 8-3](#padding_the_input_sequences)).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了这些输入序列，你可以将它们填充成常规形状。我们将使用预填充（[图 8-3](#padding_the_input_sequences)）。
- en: '![Padding the input sequences](Images/aiml_0803.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![填充输入序列](Images/aiml_0803.png)'
- en: Figure 8-3\. Padding the input sequences
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-3\. 填充输入序列
- en: 'To do this, you’ll need to find the longest sentence in the input sequences,
    and pad everything to that length. Here’s the code:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，你需要找到输入序列中最长的句子，并将所有内容填充到该长度。以下是代码：
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Finally, once you have a set of padded input sequences, you can split these
    into features and labels, where the label is simply the last token in the input
    sequence ([Figure 8-4](#turning_the_padded_sequences_into_featu)).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一旦你有一组填充的输入序列，你可以将它们分为特征和标签，其中标签只是输入序列中的最后一个令牌（[图 8-4](#turning_the_padded_sequences_into_featu)）。
- en: '![Turning the padded sequences into features (x) and labels (y)](Images/aiml_0804.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![将填充的序列转换为特征（x）和标签（y）](Images/aiml_0804.png)'
- en: Figure 8-4\. Turning the padded sequences into features (x) and labels (y)
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-4\. 将填充序列转换为特征（x）和标签（y）
- en: When training a neural network, you’re going to match each feature to its corresponding
    label. So, for example, the label for [0 0 0 0 4 2 66 8 67 68 69] will be [70].
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练神经网络时，你将要将每个特征与其对应的标签匹配。例如，[0 0 0 0 4 2 66 8 67 68 69] 的标签将是 [70]。
- en: 'Here’s the code to separate the labels from the input sequences:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这是分离输入序列中标签的代码：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Next, you need to encode the labels. Right now they’re just tokens—for example,
    the number 2 at the top of [Figure 8-4](#turning_the_padded_sequences_into_featu).
    But if you want to use a token as a label in a classifier, it will have to be
    mapped to an output neuron. Thus, if you’re going to classify *n* words, with
    each word being a class, you’ll need to have *n* neurons. Here’s where it’s important
    to control the size of the vocabulary, because the more words you have, the more
    classes you’ll need. Remember back in Chapters [2](ch02.xhtml#introduction_to_computer_vision)
    and [3](ch03.xhtml#going_beyond_the_basics_detecting_featu) when you were classifying
    fashion items with the Fashion MNIST dataset, and you had 10 types of items of
    clothing? That required you to have 10 neurons in the output layer. In this case,
    what if you want to predict up to 10,000 vocabulary words? You’ll need an output
    layer with 10,000 neurons!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你需要对标签进行编码。目前它们只是令牌—例如，在[图 8-4](#turning_the_padded_sequences_into_featu)的顶部的数字
    2。但是如果你想要在分类器中使用令牌作为标签，它将必须映射到一个输出神经元。因此，如果你要分类 *n* 个单词，每个单词都是一个类，你将需要 *n* 个神经元。这里控制词汇量的大小非常重要，因为你拥有的单词越多，你就需要更多的类。回想一下在第
    [2](ch02.xhtml#introduction_to_computer_vision) 和第 [3](ch03.xhtml#going_beyond_the_basics_detecting_featu)
    章中，当你用 Fashion MNIST 数据集对时尚物品进行分类时，你有 10 种类型的服装？那就需要在输出层有 10 个神经元。在这种情况下，如果你想预测多达
    10,000 个词汇单词，你将需要一个包含 10,000 个神经元的输出层！
- en: Additionally, you need to one-hot encode your labels so that they match the
    desired output from a neural network. Consider [Figure 8-4](#turning_the_padded_sequences_into_featu).
    If a neural network is fed the input X consisting of a series of 0s followed by
    a 4, you’ll want the prediction to be 2, but how the network delivers that is
    by having an output layer of *vocabulary_size* neurons, where the second one has
    the highest probability.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你需要对标签进行独热编码，以便它们与神经网络的期望输出匹配。考虑到[图 8-4](#turning_the_padded_sequences_into_featu)。如果神经网络被输入
    X，其中包含一系列 0，然后是一个 4，你希望预测结果是 2，但网络是通过具有 *词汇量* 个神经元的输出层来实现这一点，其中第二个神经元具有最高的概率。
- en: 'To encode your labels into a set of Ys that you can then use to train, you
    can use the `to_categorical` utility in `tf.keras`:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 要将标签编码为一组 Y，然后用于训练，你可以使用 `tf.keras` 中的 `to_categorical` 实用程序：
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can see this visually in [Figure 8-5](#one_hot_encoding_labels_).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[图 8-5](#one_hot_encoding_labels_)中看到这一点。
- en: '![One-hot encoding labels ](Images/aiml_0805.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![独热编码标签](Images/aiml_0805.png)'
- en: Figure 8-5\. One-hot encoding labels
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-5\. 独热编码标签
- en: This is a very sparse representation, which, if you have a lot of training data
    and a lot of potential words, will eat memory very quickly! Suppose you had 100,000
    training sentences, with a vocabulary of 10,000 words—you’d need 1,000,000,000
    bytes just to hold the labels! But it’s the way we have to design our network
    if we’re going to classify and predict words.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种非常稀疏的表示方法，如果你有大量的训练数据和大量的可能单词，内存消耗会非常快！假设你有 100,000 个训练句子，词汇量为 10,000 个词，你需要
    1,000,000,000 字节来存储标签！但如果我们要设计我们的网络来分类和预测单词，这就是我们不得不采取的方式。
- en: Creating the Model
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建模型
- en: Let’s now create a simple model that can be trained with this input data. It
    will consist of just an embedding layer, followed by an LSTM, followed by a dense
    layer.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一个可以用这些输入数据进行训练的简单模型。它将只包括一个嵌入层，然后是一个 LSTM，再后面是一个稠密层。
- en: For the embedding you’ll need one vector per word, so the parameters will be
    the total number of words and the number of dimensions you want to embed on. In
    this case we don’t have many words, so eight dimensions should be enough.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于嵌入，你将需要每个单词一个向量，因此参数将是单词总数和你想要嵌入的维度数。在这种情况下，我们单词不多，因此八个维度应该足够了。
- en: You can make the LSTM bidirectional, and the number of steps can be the length
    of a sequence, which is our max length minus 1 (because we took one token off
    the end to make the label).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使LSTM双向运行，步骤数可以是序列的长度，即我们的最大长度减1（因为我们取出了末尾的一个标记来作为标签）。
- en: 'Finally, the output layer will be a dense layer with the total number of words
    as a parameter, activated by softmax. Each neuron in this layer will be the probability
    that the next word matches the word for that index value:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，输出层将是一个密集层，参数为单词的总数，由softmax激活。该层中的每个神经元将是下一个单词与该索引值的单词匹配的概率：
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Compile the model with a categorical loss function such as categorical cross
    entropy and an optimizer like Adam. You can also specify that you want to capture
    metrics:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 编译模型时，使用像分类交叉熵这样的分类损失函数和像Adam这样的优化器。你也可以指定你想要捕捉的指标：
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'It’s a very simple model without a lot of data, so you can train for a long
    time—say, 1,500 epochs:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的模型，没有太多的数据，所以你可以训练很长时间——比如1,500个时期：
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: After 1,500 epochs, you’ll see that it has reached very high accuracy ([Figure 8-6](#training_accuracy)).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过1,500个时期后，你会发现它已经达到了非常高的准确率（[图表 8-6](#training_accuracy)）。
- en: '![Training accuracy](Images/aiml_0806.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![训练准确率](Images/aiml_0806.png)'
- en: Figure 8-6\. Training accuracy
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图表 8-6\. 训练准确率
- en: With the model at around 95% accuracy, we can be assured that if we have a string
    of text that it has already seen it will predict the next word accurately about
    95% of the time. Note, however, that when generating text it will continually
    see words that it hasn’t previously seen, so despite this good number, you’ll
    find that the network will rapidly end up producing nonsensical text. We’ll explore
    this in the next section.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型达到大约95%的准确率时，我们可以确信，如果我们有一段它已经见过的文本，它会大约95%的时间准确地预测下一个单词。然而，请注意，当生成文本时，它将不断看到以前未曾见过的单词，因此尽管有这么好的数字，你会发现网络很快就会开始生成毫无意义的文本。我们将在下一节中探讨这一点。
- en: Generating Text
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成文本
- en: Now that you’ve trained a network that can predict the next word in a sequence,
    the next step is to give it a sequence of text and have it predict the next word.
    Let’s take a look at how to do that.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经训练出一个能够预测序列中下一个单词的网络，接下来的步骤是给它一段文本序列，并让它预测下一个单词。让我们看看如何实现这一点。
- en: Predicting the Next Word
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测下一个单词
- en: You’ll start by creating a phrase called the seed text. This is the initial
    expression on which the network will base all the content it generates. It will
    do this by predicting the next word.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你将首先创建一个称为种子文本的短语。这是网络将基于其生成所有内容的初始表达式。它将通过预测下一个单词来完成这一操作。
- en: 'Start with a phrase that the network has *already* seen, “in the town of athy”:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 从网络已经*看到*的短语开始，“在阿西镇”：
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next you need to tokenize this using `texts_to_sequences`. This returns an
    array, even if there’s only one value, so take the first element in that array:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来你需要使用`texts_to_sequences`对其进行标记化。即使只有一个值，它也会返回一个数组，因此取该数组中的第一个元素：
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then you need to pad that sequence to get it into the same shape as the data
    used for training:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你需要填充该序列，使其与训练时使用的数据形状相同：
- en: '[PRE10]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now you can predict the next word for this token list by calling `model.predict`
    on the token list. This will return the probabilities for each word in the corpus,
    so pass the results to `np.argmax` to get the most likely one:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以通过在标记列表上调用`model.predict`来预测该标记列表的下一个单词。这将返回语料库中每个单词的概率，因此将结果传递给`np.argmax`以获取最有可能的一个：
- en: '[PRE11]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This should give you the value `68`. If you look at the word index, you’ll
    see that this is the word “one”:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会给你一个值`68`。如果你查看单词索引，你会发现这是单词“one”：
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You can look it up in code by searching through the word index items until
    you find `predicted` and printing it out:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过搜索单词索引项目直到找到`predicted`并将其打印出来，在代码中查找它：
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'So, starting from the text “in the town of athy,” the network predicted the
    next word should be “one”—which if you look at the training data is correct, because
    the song begins with the line:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从文本“在阿西镇”开始，网络预测下一个单词应该是“one”—如果你查看训练数据，这是正确的，因为歌曲以以下行开始：
- en: '***In the town of Athy** one Jeremy Lanigan*'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***在阿西镇** 一个杰里米·拉尼根*'
- en: '*Battered away til he hadnt a pound*'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '-   *打击直到他没了一磅*'
- en: Now that you’ve confirmed the model is working, you can get creative and use
    different seed text. For example, when I used the seed text “sweet jeremy saw
    dublin,” the next word it predicted was “then.” (This text was chosen because
    all of those words are in the corpus. You should expect more accurate results,
    at least at the beginning, for the predicted words in such cases.)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已确认模型正在工作，您可以发挥创造力，并使用不同的种子文本。例如，当我使用种子文本“甜美的杰里米看到了都柏林”时，它预测的下一个单词是“然后”。（选择这段文本是因为所有这些词汇都在语料库中。您应该期望在这种情况下，至少在开始时，对预测单词的预期结果更为准确。）
- en: Compounding Predictions to Generate Text
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合并预测以生成文本
- en: In the previous section you saw how to use the model to predict the next word
    given a seed text. To have the neural network now create new text, you simply
    repeat the prediction, adding new words each time.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，您看到了如何使用模型预测给定种子文本的下一个单词。现在，要让神经网络创建新文本，只需重复预测，每次添加新单词即可。
- en: For example, earlier when I used the phrase “sweet jeremy saw dublin,” it predicted
    the next word would be “then.” You can build on this by appending “then” to the
    seed text to get “sweet jeremy saw dublin then” and getting another prediction.
    Repeating this process will give you an AI-created string of text.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，稍早时，当我使用短语“甜美的杰里米看到了都柏林”时，它预测下一个单词将是“然后”。您可以通过在种子文本后附加“然后”来扩展此过程，以获取“甜美的杰里米看到了都柏林然后”，并获得另一个预测。重复此过程将为您生成一个由AI创建的文本字符串。
- en: 'Here’s the updated code from the previous section that performs this loop a
    number of times, with the number set by the `next_words` parameter:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这是前一节更新的代码，它执行了多次循环，次数由`next_words`参数设置：
- en: '[PRE14]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This will end up creating a string something like this:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这最终会创建一个类似这样的字符串：
- en: '[PRE15]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: It rapidly descends into gibberish. Why? The first reason is that the body of
    training text is really small, so it has very little context to work with. The
    second is that the prediction of the next word in the sequence depends on the
    previous words in the sequence, and if there is a poor match on the previous ones,
    even the best “next” match will have a low probability. When you add this to the
    sequence and predict the next word after that, the likelihood of it having a low
    probability is even higher—thus, the predicted words will seem semirandom.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 它迅速陷入了胡言乱语。为什么呢？首先，训练文本的主体非常小，因此可用的上下文非常有限。其次，序列中下一个单词的预测取决于序列中的前一个单词，如果前几个单词匹配不好，即使是最佳的“下一个”匹配也会有很低的概率。当您将此添加到序列并预测其后的下一个单词时，它的概率会更高——因此，预测的单词看起来似乎是半随机的。
- en: So, for example, while all of the words in the phrase “sweet jeremy saw dublin”
    exist in the corpus, they never exist in that order. When the first prediction
    was done, the word “then” was chosen as the most likely candidate, and it had
    quite a high probability (89%). When it was added to the seed to get “sweet jeremy
    saw dublin then,” we had another phrase not seen in the training data, so the
    prediction gave the highest probability to the word “got,” at 44%. Continuing
    to add words to the sentence reduces the likelihood of a match in the training
    data, and as such the prediction accuracy will suffer—leading to a more random
    “feel” to the words being predicted.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，例如，“甜美的杰里米看到了都柏林”中的所有单词虽然都存在于语料库中，但它们从未按照那种顺序出现过。在进行第一次预测时，选择了“然后”作为最可能的候选词，其概率相当高（89%）。当它被添加到种子中以得到“甜美的杰里米看到了都柏林然后”，我们得到了另一句在训练数据中未见过的短语，因此预测将最高概率赋予了单词“got”，概率为44%。继续向句子添加单词会降低在训练数据中的匹配概率，因此预测准确性会降低——导致预测的单词具有更随机的“感觉”。
- en: This leads to the phenomenon of AI-generated content getting increasingly nonsensical
    over time. For an example, check out the excellent sci-fi short [*Sunspring*](https://oreil.ly/hTBtJ),
    which was written entirely by an LSTM-based network, like the one you’re building
    here, trained on science fiction movie scripts. The model was given seed content
    and tasked with generating a new script. The results were hilarious, and you’ll
    see that while the initial content makes sense, as the movie progresses it becomes
    less and less comprehensible.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了人工智能生成的内容随着时间的推移变得越来越荒谬。例如，看看优秀的科幻短片[*Sunspring*](https://oreil.ly/hTBtJ)，这部片子完全由基于LSTM的网络编写，就像您正在构建的这个网络，训练于科幻电影剧本。模型被给予种子内容，并被要求生成新的剧本。结果令人捧腹，您将看到，虽然初始内容是有意义的，但随着电影的进行，变得越来越难以理解。
- en: Extending the Dataset
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展数据集
- en: The same pattern that you used for the hardcoded dataset can be extended to
    use a text file very simply. I’ve hosted a text file containing about 1,700 lines
    of text gathered from a number of songs that you can use for experimentation.
    With a little modification, you can use this instead of the single hardcoded song.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以很简单地将用于硬编码数据集的相同模式扩展到使用文本文件。我提供了一个文本文件，里面包含约1700行来自多首歌曲的文本，供你进行实验。稍作修改，你就可以使用这个文本文件代替单一的硬编码歌曲。
- en: 'To download the data in Colab, use the following code:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Colab中下载数据，请使用以下代码：
- en: '[PRE16]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then you can simply load the text from it into your corpus like this:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你可以简单地从中加载文本到你的语料库中，像这样：
- en: '[PRE17]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The rest of your code will then work without modification!
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你的其余代码就可以正常工作了！
- en: Training this for one thousand epochs brings you to about 60% accuracy, with
    the curve flattening out ([Figure 8-7](#training_on_a_larger_dataset)).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 将其训练一千个时期会使准确率达到约60%，曲线趋于平缓（[图 8-7](#training_on_a_larger_dataset)）。
- en: '![Training on a larger dataset](Images/aiml_0807.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![在更大的数据集上训练](Images/aiml_0807.png)'
- en: Figure 8-7\. Training on a larger dataset
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-7\. 在更大的数据集上训练
- en: Trying the phrase “in the town of athy” again yields a prediction of “one,”
    but this time with only a 40% probability.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 再次尝试“在阿西镇”的短语，预测得到“one”，但这次只有40%的概率。
- en: 'For “sweet jeremy saw dublin” the predicted next word is “drawn,” with a probability
    of 59%. Predicting the next 10 words yields:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于“甜美的杰里米看到了都柏林”，预测的下一个词是“drawn”，概率为59%。预测接下来的10个单词：
- en: '[PRE18]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: It’s looking a little better! But can we improve it further?
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来好了一点！但我们能进一步改进吗？
- en: Changing the Model Architecture
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改变模型架构
- en: 'One way that you can improve the model is to change its architecture, using
    multiple stacked LSTMs. This is pretty straightforward—just ensure that you set
    `return_sequences` to `True` on the first of them. Here’s the code:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以改变模型的架构来改进它，使用多个堆叠的LSTM。这很简单 —— 只需确保在第一个LSTM上设置`return_sequences`为`True`。这里是代码：
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: You can see the impact this has on training for one thousand epochs in [Figure 8-8](#adding_a_second_lstm_layer).
    It’s not significantly different from the previous curve.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这对一千个时期的训练的影响在[图 8-8](#adding_a_second_lstm_layer)中，它与先前的曲线没有显著不同。
- en: '![Adding a second LSTM layer](Images/aiml_0808.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![添加第二个LSTM层](Images/aiml_0808.png)'
- en: Figure 8-8\. Adding a second LSTM layer
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-8\. 添加第二个LSTM层
- en: When testing with the same phrases as before, this time I got “more” as the
    next word after “in the town of athy” with a 51% probability, and after “sweet
    jeremy saw dublin” I got “cailín” (the Gaelic word for “girl”) with a 61% probability.
    Again, when predicting more words, the output quickly descended into gibberish.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用之前相同的短语进行测试时，这一次在“在阿西镇”后，我得到了“more”作为下一个词，概率为51%，而在“甜美的杰里米看到了都柏林”后，我得到了“cailín”（盖尔语中的“女孩”）的概率为61%。同样地，在预测更多单词时，输出很快就变得支离破碎了。
- en: 'Here are some examples:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些例子：
- en: '[PRE20]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: If you get different results, don’t worry—you didn’t do anything wrong, but
    the random initialization of the neurons will impact the final scores.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你得到了不同的结果，别担心 —— 你没有做错任何事情，但神经元的随机初始化会影响最终的得分。
- en: Improving the Data
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进数据
- en: There’s a small trick that you can use to extend the size of this dataset without
    adding any new songs, called *windowing* the data. Right now, every line in every
    song is read as a single line and then turned into input sequences, as you saw
    in [Figure 8-2](#turning_a_sequence_into_a_number_of_inp). While humans read songs
    line by line in order to hear rhyme and meter, the model doesn’t have to, in particular
    when using bidirectional LSTMs.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个小技巧，你可以用来扩展这个数据集的大小，而不添加任何新的歌曲，称为数据的“窗口化”。现在，每首歌的每一行都被读作一个单独的行，然后转换为输入序列，正如你在[图 8-2](#turning_a_sequence_into_a_number_of_inp)中看到的那样。虽然人类阅读歌曲时是逐行进行的，以听到韵律和节奏，但模型不必如此，尤其是在使用双向LSTM时。
- en: So, instead of taking the line “In the town of Athy, one Jeremy Lanigan,” processing
    that, and then moving to the next line (“Battered away till he hadn’t a pound”)
    and processing that, we could treat all the lines as one long, continuous text.
    We can then create a “window” into that text of *n* words, process that, and then
    move the window forward one word to get the next input sequence ([Figure 8-9](#a_moving_word_window)).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，与其处理“在阿西镇，一个名叫杰里米·兰尼根”的这行，然后处理下一行（“击打直到他一文不剩”），我们可以将所有行视为一段长连续文本。我们可以创建一个*n*个单词的“窗口”来处理这段文本，然后移动窗口一个单词以获取下一个输入序列（[图 8-9](#a_moving_word_window)）。
- en: '![A moving word window](Images/aiml_0809.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![一个移动的单词窗口](Images/aiml_0809.png)'
- en: Figure 8-9\. A moving word window
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-9\. 一个移动的单词窗口
- en: In this case, far more training data can be yielded in the form of an increased
    number of input sequences. Moving the window across the entire corpus of text
    would give us ((*number_of_words* – *window_size*) × *window_size*) input sequences
    that we could train with.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，可以通过增加输入序列的数量来产生更多的训练数据。将窗口移动到整个文本语料库上，我们可以得到((*number_of_words* – *window_size*)
    × *window_size*)个输入序列，我们可以用来训练。
- en: 'The code is pretty simple—when loading the data, instead of splitting each
    song line into a “sentence,” we can create them on the fly from the words in the
    corpus:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 代码非常简单——在加载数据时，我们可以直接从语料库中的单词创建“句子”，而不是将每个歌词行分割为“句子”：
- en: '[PRE21]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In this case, because we no longer have sentences and we’re creating sequences
    the same size as the moving window, `max_sequence_len` is the size of the window.
    The full file is read, converted to lowercase, and split into an array of words
    using string splitting. The code then loops through the words and makes sentences
    of each word from the current index up to the current index plus the window size,
    adding each of those newly constructed sentences to the sentences array.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，因为我们不再有句子，而是创建与移动窗口大小相同的序列，`max_sequence_len`就是窗口的大小。完整的文件被读取，转换为小写，并使用字符串分割分成一个单词数组。然后，代码循环遍历单词，并将每个从当前索引到当前索引加上窗口大小的单词构造句子，将每个新构造的句子添加到句子数组中。
- en: When training, you’ll notice that the extra data makes it much slower per epoch,
    but the results are greatly improved, and the generated text descends into gibberish
    much more slowly.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练时，您会注意到额外的数据使每个时期变得更慢，但结果大大改善，并且生成的文本降入胡言乱语的速度明显减慢。
- en: Here’s an example that caught my eye—particularly the last line!
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个引起我注意的例子——尤其是最后一行！
- en: '*you know nothing, jon snow is gone*'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*你什么都不知道，琼·雪已经离开*'
- en: '*and the young and the rose and wide*'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*和年轻人和玫瑰和广阔*'
- en: '*to where my love i will play*'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*到我爱的地方我会玩*'
- en: '*the heart of the kerry*'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*咖哩的心*'
- en: '*the wall i watched a neat little town*'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*墙壁上，我看到一个小小的整洁小镇*'
- en: There are many hyperparameters you can try tuning. Changing the window size
    will change the amount of training data—a smaller window size can yield more data,
    but there will be fewer words to give to a label, so if you set it too small you’ll
    end up with nonsensical poetry. You can also change the dimensions in the embedding,
    the number of LSTMs, or the size of the vocab to use for training. Given that
    percentage accuracy isn’t the best measurement—you’ll want to make a more subjective
    examination of how much “sense” the poetry makes—there’s no hard-and-fast rule
    to follow to determine whether your model is “good” or not.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多超参数可以尝试调整。改变窗口大小将改变训练数据的数量——较小的窗口大小可以产生更多的数据，但给标签的词将更少，所以如果设置得太小，最终会得到毫无意义的诗歌。您还可以改变嵌入的维度、LSTM的数量或用于训练的词汇量大小。考虑到百分比准确度并非最佳衡量标准——您需要对生成的诗歌“意义”的主观评估——没有硬性的规则可以确定您的模型是否“好”。
- en: For example, when I tried using a window size of 6, increasing the number of
    dimensions for the embedding to 16, changing the number of LSTMs from the window
    size (which would be 6) to 32, and upping the learning rate on the Adam optimizer,
    I got a nice, smooth, learning curve ([Figure 8-10](#learning_curve_with_adjusted_hyperparam))
    and some of the poetry began to make more sense.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当我尝试使用窗口大小为6时，将嵌入的维度增加到16，将LSTM的数量从窗口大小（即6）改为32，并提高Adam优化器的学习率时，我得到了一个漂亮、平滑的学习曲线（[图8-10](#learning_curve_with_adjusted_hyperparam)），并且一些诗歌开始变得更有意义了。
- en: '![Learning curve with adjusted hyperparameters](Images/aiml_0810.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![使用调整后的超参数的学习曲线](Images/aiml_0810.png)'
- en: Figure 8-10\. Learning curve with adjusted hyperparameters
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-10\. 使用调整后的超参数的学习曲线
- en: 'When using “sweet jeremy saw dublin” as the seed (remember, all of the words
    in the seed are in the corpus), I got this poem:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用“甜脆的杰里米看到都柏林”作为种子（记住，种子中的所有单词都在语料库中），我得到了这首诗：
- en: '*sweet jeremy saw dublin*'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*甜脆的杰里米看到都柏林*'
- en: '*whack fol*'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*猛击，傻瓜*'
- en: '*all the watch came*'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*所有看守来了*'
- en: '*and if ever you love get up from the stool*'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如果你爱从凳子上站起来*'
- en: '*longs to go as i was passing my aged father*'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*渴望去，我经过了我的年迈父亲*'
- en: '*if you can visit new ross*'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如果你能访问新的罗斯*'
- en: '*gallant words i shall make*'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*英勇的话，我会做*'
- en: '*such powr of her goods*'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*她的商品的力量*'
- en: '*and her gear*'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*和她的装备*'
- en: '*and her calico blouse*'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*和她的卡利科衬衫*'
- en: '*she began the one night*'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*她开始了那一个晚上*'
- en: '*rain from the morning so early*'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*清晨的雨*'
- en: '*oer railroad ties and crossings*'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在铁路和十字路口*'
- en: '*i made my weary way*'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*我走过我疲惫的道路*'
- en: '*through swamps and elevations*'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*穿越沼泽和高地*'
- en: '*my tired feet*'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*我疲惫的双脚*'
- en: '*was the good heavens*'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*是好的天堂*'
- en: While the phrase “whack fol” may not make sense to many readers, it’s commonly
    heard in some Irish songs, kind of like “la la la” or “doobie-doobie-doo.” What
    I really liked about this was how some of the *later* phrases kept some kind of
    sense, like “such power of her good and her gear, and her calico blouse”—but this
    could be due to overfitting to the phrases that already exist in the songs within
    the corpus. For example, the lines beginning “oer railroad ties…” through “my
    tired feet” are taken directly from a song called “The Lakes of Pontchartrain”
    which is in the corpus. If you encounter issues like this, it’s best to reduce
    the learning rate and maybe decrease the number of LSTMs. But above all, experiment
    and have fun!
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管“whack fol”这个短语对许多读者来说可能毫无意义，但在一些爱尔兰歌曲中经常听到，有点像“la la la”或“doobie-doobie-doo”。我真正喜欢的是一些*后来*的短语保留了某种意义，比如“她的好和她的装备的力量，以及她的卡里古布衬衫”—但这可能是由于过度拟合到已存在于语料库中的短语。例如，从“oer
    railroad ties…”到“我疲惫的双脚”直接取自一首名为“Pontchartrain湖”的歌曲，该歌曲包含在语料库中。如果您遇到此类问题，最好降低学习速率，可能减少LSTM的数量。但最重要的是，进行实验并享受乐趣！
- en: Character-Based Encoding
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于字符的编码
- en: For the last few chapters we’ve been looking at NLP using word-based encoding.
    I find that much easier to get started with, but when it comes to generating text,
    you might also want to consider character-based encoding because the number of
    unique *characters* in a corpus tends to be a lot less than the number of unique
    *words*. As such, you can have a lot fewer neurons in your output layer, and your
    output predictions are spread across fewer probabilities. For example, when looking
    at the dataset of [the complete works of Shakespeare](https://oreil.ly/XW_ab),
    you’ll see that there are only 65 unique characters in the entire set. So, when
    you are making predictions, instead of looking at probabilities of the next word
    across 2,700 words as in the Irish songs dataset, you’re only looking at 65\.
    This makes your model a bit simpler!
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几章中，我们一直在讨论使用基于单词的编码的NLP。我发现这样更容易入门，但在生成文本时，您可能还想考虑基于字符的编码，因为语料库中独特*字符*的数量往往比独特*词*的数量少得多。因此，您的输出层中可以有更少的神经元，并且您的输出预测分布在更少的概率上。例如，当查看[莎士比亚全集](https://oreil.ly/XW_ab)的数据集时，您会看到整个集合中只有65个唯一的字符。因此，当您进行预测时，与爱尔兰歌曲数据集中的2700个单词的下一个单词的概率相比，您只需查看65个。这使得您的模型变得简单一些！
- en: 'What’s also nice about character encoding is that punctuation characters are
    also included, so line breaks etc. can be predicted. As an example, when I used
    an RNN trained on the Shakespeare corpus to predict the text following on from
    my favorite *Game of Thrones* line, I got:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 字符编码的另一个好处是包括了标点符号字符，因此可以预测换行等。例如，当我使用在莎士比亚语料库上训练的RNN来预测接下来跟随我最喜欢的*权力的游戏*台词的文本时，我得到了：
- en: 'YGRITTE:'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 伊格利特：
- en: You know nothing, Jon Snow.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你什么都不知道，乔恩·雪诺。
- en: Good night, we’ll prove those body’s servants to
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 晚安，我们将证明那些身体的仆人
- en: '**The traitor be these mine:**'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**叛徒是这些我的：**'
- en: '**So diswarl his body in hope in this resceins,**'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**所以在这个resceins中希望在这个diswarl他的身体，**'
- en: I cannot judg appeal’t.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我不能判断是否上诉。
- en: 'MENENIUS:'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梅内尼乌斯：
- en: Why, ’tis pompetsion.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么，这是pompetsion。
- en: 'KING RICHARD II:'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理查德二世国王：
- en: I think he make her thought on mine;
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我认为他让她想起了我的想法；
- en: 'She will not: suffer up thy bonds:'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 她不会：忍受你的束缚：
- en: How doched it, I pray the gott,
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我祈祷，他会做到，
- en: We’ll no fame to this your love, and you were ends
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将不会为你的爱而得名，你的结束
- en: It’s kind of cool that she identifies him as a traitor and wants to tie him
    up (“diswarl his body”), but I have no idea what “resceins” means! If you watch
    the show, this is part of the plot, so maybe Shakespeare was on to something without
    realizing it!
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 她将他视为叛徒并想要捆绑他(“diswarl his body”)的方式很酷，但我不知道“resceins”是什么意思！如果你看过这个节目，这是情节的一部分，所以也许莎士比亚在没有意识到的情况下表达了某种观点！
- en: Of course, I do think we tend to be a little more forgiving when using something
    like Shakespeare’s texts as our training data, because the language is already
    a little unfamiliar.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我确实认为当我们使用像莎士比亚的文本作为我们的训练数据时，我们倾向于更加宽容，因为语言已经有些不熟悉。
- en: As with the Irish songs model, the output does quickly degenerate into nonsensical
    text, but it’s still fun to play with. To try it for yourself, you can check out
    the [Colab](https://oreil.ly/cbz9c).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 与爱尔兰歌曲模型一样，输出很快就会变成毫无意义的文本，但玩起来仍然很有趣。要自己尝试一下，请查看[Colab](https://oreil.ly/cbz9c)。
- en: Summary
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter we explored how to do basic text generation using a trained
    LSTM-based model. You saw how you can split text into training features and labels,
    using words as labels, and create a model that, when given seed text, can predict
    the next likely word. You iterated on this to improve the model for better results,
    exploring a dataset of traditional Irish songs. You also saw a little about how
    this could potentially be improved with character-based text generation with an
    example that uses Shakespearian text. Hopefully this was a fun introduction to
    how machine learning models can synthesize text!
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们探讨了如何使用训练过的基于LSTM的模型进行基本文本生成。你看到了如何将文本分割为训练特征和标签，使用单词作为标签，并创建一个模型，当给定种子文本时，可以预测下一个可能的单词。你通过迭代来改进模型以获得更好的结果，探索了传统爱尔兰歌曲的数据集。你还看到了如何通过使用莎士比亚文本的示例，可能通过基于字符的文本生成来改进这一过程。希望这是一个有趣的介绍，展示了机器学习模型如何合成文本！
