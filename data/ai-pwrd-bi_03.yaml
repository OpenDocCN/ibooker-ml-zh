- en: Chapter 3\. Machine Learning Fundamentals
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 3 章\. 机器学习基础
- en: This chapter contains everything you need to know about machine learning—at
    least for this book. And it’s a great primer for the rest of your learning. The
    following sections should give you enough knowledge to follow along with the use
    cases in this book and help you build your own first prototypes. We’ll cover supervised
    machine learning, popular ML algorithms, and key terms, and you will learn how
    to evaluate ML models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包含了您需要了解的关于机器学习的一切——至少对于本书来说是如此。它也是您进一步学习的一个很好的起点。接下来的部分将为您提供足够的知识，以便跟随本书中的用例，并帮助您构建自己的第一个原型。我们将涵盖监督机器学习、流行的
    ML 算法和关键术语，并且您将学会如何评估 ML 模型。
- en: If you’re already familiar with these topics, feel free to consider this chapter
    a refresher. Let’s get started with supervised machine learning!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经熟悉这些主题，可以将本章视为复习。让我们开始学习监督机器学习！
- en: The Supervised Machine Learning Process
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督机器学习过程
- en: 'Let’s consider a simple example: imagine you want to sell your house and are
    wondering how to come up with a listing price. To get a realistic price, you would
    most likely look at other similar houses and the prices they were sold for. To
    come up with a good estimate, you would probably also compare your house to other
    houses in terms of some key features, such as overall size, bedrooms, location,
    and age. Without knowing it, you would have just acted like a supervised machine
    learning system.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个简单的例子：想象一下，您想要出售自己的房子，并且正在考虑如何确定上市价格。为了得到一个真实的价格，您很可能会查看其他类似房屋以及它们的售价。为了得出一个良好的估计，您可能还会比较您的房子与其他房屋在一些关键特征方面的情况，例如总体大小、卧室数量、位置和年龄。不知不觉中，您已经像一个监督机器学习系统一样行事了。
- en: '*Supervised machine learning* is a process of training an ML model based on
    historical data when the ground truth is known. For example, if you want to estimate
    (or predict) the real estate price as in our example, a supervised learning algorithm
    would look at historical house prices (the label) and other information that describes
    the houses (the features). Supervised machine learning is in contrast to *unsupervised
    machine learning*, where this ground truth isn’t known and the computer has to
    group data points into similar categories. *Clustering* is a popular example of
    unsupervised machine learning.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*监督学习* 是基于已知真实值的历史数据训练 ML 模型的过程。例如，如果您想要像我们的示例那样估计（或预测）房地产价格，监督学习算法会查看历史房价（标签）以及描述房屋的其他信息（特征）。监督机器学习与
    *无监督机器学习* 相对，后者未知真实值，计算机必须将数据点分组成相似类别。*聚类* 是无监督机器学习的一个常见例子。'
- en: Most enterprise ML problems fall into the realm of supervised learning. I’ll
    walk you through the basic process of supervised machine learning by showing you
    the key steps so you can apply them later when we build our first ML model starting
    in [Chapter 7](ch07.xhtml#ai_powered_predictive_analytics).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数企业机器学习问题属于监督学习范畴。我将通过展示关键步骤来为您介绍监督机器学习的基本过程，这样您以后可以在我们从第 7 章开始构建第一个 ML 模型时应用这些步骤。
- en: 'Step 1: Collect Historical Data'
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 1：收集历史数据
- en: In order for an ML algorithm to learn, it must have historical data. As such,
    you need to collect data and provide it in a form that the computer can process
    efficiently.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让 ML 算法学习，它必须有历史数据。因此，您需要收集数据并以计算机能够高效处理的形式提供它。
- en: 'Most algorithms expect your data to be tidy. *Tidy data* has three characteristics:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数算法期望您的数据是整洁的。*整洁数据* 具有三个特征：
- en: Every observation is in its own row.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个观察结果都在其自己的行中。
- en: Every variable is in its own column.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个变量都在其自己的列中。
- en: Every measurement is a cell.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个测量值都是一个单元。
- en: '[Figure 3-1](#tidy_data_left_parenthesissource_r_for) shows an example of tidy
    data. Bringing your data into tidy form might be the most cumbersome process of
    the entire ML workflow, depending on where you got the data from.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-1](#tidy_data_left_parenthesissource_r_for) 展示了整洁数据的示例。将数据整理成整洁形式可能是整个机器学习工作流程中最繁琐的过程，这取决于数据的来源。'
- en: '![Tidy data (source: R for Data Science by Hadley Wickham and Garrett Grolemund
    [O’Reilly])](Images/apbi_0301.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![整洁数据（来源：数据科学的R，由哈德利·威克姆和加勒特·格罗勒蒙德 [O’Reilly]）](Images/apbi_0301.png)'
- en: 'Figure 3-1\. Tidy data. Source: [R for Data Science](https://www.oreilly.com/library/view/r-for-data/9781491910382)
    by Hadley Wickham and Garrett Grolemund (O’Reilly)'
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-1\. 整洁数据。来源：[数据科学的R](https://www.oreilly.com/library/view/r-for-data/9781491910382)
    由哈德利·威克姆和加勒特·格罗勒蒙德（O'Reilly）
- en: 'Step 2: Identify Features and Labels'
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 2：确定特征和标签
- en: All that your algorithm is trying to do is to build a model that will predict
    some output *y* given inputs *x*. Let’s unpack this concept briefly.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 您的算法试图做的一切就是构建一个模型，该模型将根据输入的*x*来预测某些输出*y*。让我们简要解释这个概念。
- en: '*Labels* (also called *targets*, *outputs*, or *dependent variables*) are the
    variables that you want to predict based on other variables. These other variables
    are called *features* (or *attributes*, *inputs*, or *independent variables*)
    in ML jargon. For example, if you want to predict house prices, the house price
    would be your label, and variables such as bedrooms, size, and location would
    be your features. In a supervised learning setting, you have historical data that
    contains features and labels for a given number of observations, also called *training
    examples*.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*标签*（也称为*目标*、*输出*或*因变量*）是基于其他变量进行预测的变量。这些其他变量在机器学习术语中称为*特征*（或*属性*、*输入*或*自变量*）。例如，如果您想要预测房价，房价将是您的标签，而卧室数、大小和位置等变量将是您的特征。在监督学习设置中，您有包含一定数量观测值的历史数据，这些数据包含了特征和标签，也称为*训练样本*。'
- en: If your label is numeric, this process is called a *regression* problem. If
    your label is categorical, this process is called a *classification* problem.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的标签是数值型，这个过程称为*回归*问题。如果您的标签是分类的，这个过程称为*分类*问题。
- en: Different algorithms are used for classification and regression problems. In
    [“Popular Machine Learning Algorithms”](#popular_machine_learning_algorithm),
    I will introduce three popular algorithms that will be enough for our prototyping
    purposes.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的算法用于分类和回归问题。在[“流行的机器学习算法”](#popular_machine_learning_algorithm)中，我将介绍三种流行的算法，这些算法足以满足我们的原型开发目的。
- en: 'Step 3: Split Your Data into Training and Test Sets'
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三步：将数据分割为训练集和测试集
- en: When you have your data organized in tidy form and have defined your features
    and labels, you usually have to split your dataset into a training set and a test
    set. The *training set* is the part of the historical dataset that will be used
    to train the ML model (usually between 70% to 80% of the data).The *test set*
    (or *holdout set*) is the part of your data that will be used for the final evaluation
    of your ML model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当您的数据整理成整洁的形式并且已经定义了特征和标签后，通常需要将数据集分割为训练集和测试集。*训练集*是历史数据集的一部分，用于训练机器学习模型（通常占数据的70%至80%）。*测试集*（或*保留集*）是您数据的一部分，将用于最终评估您的机器学习模型。
- en: In most AutoML scenarios, you don’t need to take care of this split manually,
    as it will be handled automatically by the AutoML tool. Sometimes, however, AutoML
    services will not be able to guess the best split for your dataset and you’ll
    need to adjust manually (for example, in the case of time-series data). That’s
    why it is important to understand what data splits are, why you need them, and
    why you can evaluate your model only once on the test set.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数AutoML场景中，您不需要手动处理这个分割，因为AutoML工具会自动处理。然而，有时AutoML服务无法猜测出最佳的数据分割方式，因此您需要手动调整（例如，在时间序列数据的情况下）。这就是为什么理解数据分割的重要性，为什么需要它们，以及为什么只能在测试集上评估您的模型的原因。
- en: You split the data because you want to make sure that the model will perform
    well not only on the data it knows, but also on new, unseen data from the same
    distribution. That’s why it’s essential to keep some data aside for final evaluation
    purposes. The performance that your model has on the test set will be your performance
    indicator for new, unseen data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 您将数据分割，因为您希望模型不仅在它已知的数据上表现良好，而且在来自相同分布的新的未见数据上也能表现良好。这就是为什么保留一些数据用于最终评估非常重要的原因。您模型在测试集上的表现将成为对新的未见数据的性能指标。
- en: 'Step 4: Use Algorithms to Find the Best Model'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第四步：使用算法找到最佳模型
- en: In this step, you try to find the model that represents your data best and that
    has the highest predictive power. You do that by trying out various ML algorithms
    with multiple parameters. Again, let’s quickly go through this concept.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，您尝试找到最能代表您的数据并具有最高预测能力的模型。您可以通过尝试多种具有多个参数的机器学习算法来实现这一点。再次，让我们快速浏览这个概念。
- en: The *model* is the final deliverable of your ML training process. It is an arbitrarily
    complex function that calculates an output value for any given set of input values.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*模型*是您的机器学习训练过程的最终产物。它是一个任意复杂的函数，用于计算任何给定输入值的输出值。'
- en: 'Let’s consider a simple example. Your model could be the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个简单的例子。您的模型可能如下所示：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is an equation of a simple linear regression model. Given any input value
    for `x` (for example, house size), this formula would calculate the final price
    `y` by multiplying the size by 200 and then adding 1,000 to it. Of course, ML
    models are usually much more complex than that, but the idea stays the same.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单线性回归模型的方程式。给定任何`x`的输入值（例如，房屋大小），该公式将通过将尺寸乘以200并加上1000来计算最终价格`y`。当然，ML模型通常比这复杂得多，但思想是一样的。
- en: 'So, how can a computer, given some historical input and output data, come up
    with such a formula? Two components are needed. First, we need to provide the
    computer with the *algorithm* (in this case, a linear regression algorithm). The
    computer will know that the output must look something like the regression equation:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，计算机如何在给定一些历史输入和输出数据的情况下提出这样的公式呢？需要两个组成部分。首先，我们需要为计算机提供*算法*（在本例中为线性回归算法）。计算机将知道输出必须类似于回归方程：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The computer will then use the historical training data to find out the best
    parameters `b1` and `b0` for this problem. This process of parameter estimation
    is called *learning*, or *training*, in ML. We can achieve and accelerate this
    learning process for big datasets in various ways, but they are beyond the scope
    of this book and unnecessary for our purposes.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机将使用历史训练数据来找出此问题的最佳参数`b1`和`b0`。这个参数估计过程在ML中称为*学习*或*训练*。我们可以通过各种方式实现和加速大数据集的这个学习过程，但它们超出了本书的范围，对我们的目的来说是不必要的。
- en: The only thing you need to know is that you need to define an ML algorithm in
    order to train a model on training data. This is the place where experts like
    data scientists or ML experts can ace. In our case, we will rely mostly on AutoML
    solutions to find the best model for us.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 您唯一需要知道的是，您需要定义一个ML算法来在训练数据上训练模型。这是像数据科学家或ML专家这样的专家可以胜任的地方。在我们的案例中，我们将主要依赖AutoML解决方案来为我们找到最佳模型。
- en: 'Step 5: Evaluate the Final Model'
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 5：评估最终模型
- en: Once our training process is complete, one model will show the best performance
    on the test set. Different evaluation criteria are used for different ML problems,
    which we explore further in [“Machine Learning Model Evaluation”](#machine_learning_model_evaluation).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的训练过程完成时，一个模型将在测试集上展现出最佳性能。不同的评估标准用于不同的机器学习问题，我们将在[“机器学习模型评估”](#machine_learning_model_evaluation)中进一步探讨。
- en: It is important to understand how these metrics work, as you will need to specify
    these metrics to the AutoML service to find the best model. Again, don’t worry;
    your AutoML service will suggest a good first-shot evaluation metric.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要理解这些指标的工作原理，因为您将需要指定这些指标给AutoML服务以找到最佳模型。再次强调，不用担心；您的AutoML服务会建议一个很好的首选评估指标。
- en: 'Step 6: Deploy'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 6：部署
- en: Once you decide on a model, you usually want to deploy it somewhere so users
    or applications can use it. The technical term for this part of the ML process
    is *inference*, *prediction*, or *scoring*. At this stage, your model doesn’t
    learn anymore but is only calculating the outputs as it receives new input values.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您决定了一个模型，通常希望将其部署到某个地方，以便用户或应用程序可以使用它。这个ML过程的技术术语是*推断*、*预测*或*评分*。在这个阶段，您的模型不再学习，而是在接收新的输入值时仅计算输出。
- en: The way this works depends on your setup. Often the model is hosted as an HTTP
    API that takes input data and returns the predictions (*online prediction*). Alternatively,
    the model can be used to score a lot of data at once, which is called *batch prediction*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这种工作方式取决于您的设置。通常，模型被托管为一个接受输入数据并返回预测的HTTP API（*在线预测*）。或者，该模型可以用于一次评分大量数据，这称为*批处理预测*。
- en: 'Step 7: Perform Maintenance'
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 7：执行维护
- en: Although the development process of the ML model has finished after deployment,
    the process never actually ends. As data patterns change, models need to be retrained,
    and we need to look at whether the initial performance of the ML model can be
    kept high over time.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然ML模型的开发过程在部署后已经结束，但这个过程实际上永远不会结束。随着数据模式的变化，模型需要重新训练，我们需要观察ML模型的初始性能是否可以随时间保持高水平。
- en: We don’t have to deal with these things during the prototyping phase, but it’s
    important to note that ML models need a considerable amount of maintenance after
    deployment, which you should consider in your feasibility analysis. We will explore
    this topic further in [Chapter 11](ch11.xhtml#taking_the_next_steps_from_prototype_to).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在原型阶段我们不必处理这些问题，但重要的是注意到机器学习模型在部署后需要大量的维护，这是您在可行性分析中需要考虑的。我们将在[第11章](ch11.xhtml#taking_the_next_steps_from_prototype_to)进一步探讨这个话题。
- en: Now that you understand on a high level how the process for supervised machine
    learning looks in general, let’s look at the most popular ML algorithms used to
    train a model for both classification and regression problems.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经从高层次理解了监督机器学习的一般过程，让我们来看看用于训练分类和回归问题模型的最流行的机器学习算法。
- en: Popular Machine Learning Algorithms
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流行的机器学习算法
- en: 'In this section, I will introduce you to three popular families of ML algorithms:
    linear regression, decision trees, and ensemble methods. If you know these three
    classes, you’ll be able to tackle probably 90% of all supervised ML problems in
    business.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您介绍三种流行的机器学习算法家族：线性回归、决策树和集成方法。如果您了解这三类算法，您将能够解决大约90%的商业监督学习问题。
- en: '[Table 3-1](#popular_machine_learning_algorithms) gives an overview of these
    classes of algorithms, and then we’ll look at them in a little more detail.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 3-1](#popular_machine_learning_algorithms) 概述了这些算法类别，然后我们将更详细地研究它们。'
- en: Table 3-1\. Popular machine learning algorithms
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3-1\. 流行的机器学习算法
- en: '| Algorithm class | Used for | Performance | Interpretability |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 算法类别 | 用于 | 性能 | 可解释性 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Linear regression | Regression problems | Low to high | High |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 线性回归 | 回归问题 | 从低到高 | 高 |'
- en: '| Decision trees | Regression and classification problems | Average | High
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 回归和分类问题 | 平均 | 高 |'
- en: '| Ensemble methods (bagging and boosting) | High | Low |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 集成方法（bagging 和 boosting） | 高 | 低 |'
- en: '| High | Low |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 高 | 低 |'
- en: Linear Regression
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归
- en: '*Linear regression* is one of the most essential algorithms to understand in
    ML because it powers so many concepts. Logistic regression, regression trees,
    time-series analysis, and even neural networks use linear regression at their
    core.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*线性回归* 是理解机器学习中最基本算法之一，因为它支持许多概念。逻辑回归、回归树、时间序列分析，甚至神经网络都在其核心使用线性回归。'
- en: The way linear regression works is simple to explain, but hard to master. Given
    some numeric input data, the regression algorithm will fit a linear function that
    calculates (predicts) corresponding numeric output data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的工作原理很简单，但很难掌握。给定一些数值输入数据，回归算法将拟合一个线性函数，计算（预测）相应的数值输出数据。
- en: 'Two main assumptions are important for regression:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 回归的两个主要假设很重要：
- en: The features should be independent from one another (we should have low correlation
    among features).
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征之间应该相互独立（特征之间的相关性应该很低）。
- en: The relationship between the features and the outcome variable should be linear
    (e.g., when one variable increases, the output variable should also increase or
    decrease with a fixed pattern).
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征与结果变量之间的关系应该是线性的（例如，当一个变量增加时，输出变量也应该以固定模式增加或减少）。
- en: '[Figure 3-2](#simple_linear_regression) shows an example of a simple linear
    regression: the input variable on the x-axis predicts the variable on the y-axis.
    The dots in the plot are the actual data points (labels) that were observed for
    each value of *x* in the historical dataset. The prediction from the regression
    would be the *y* value on the straight line for each value of *x*. Of course,
    this is just a basic example for illustration purposes. Linear regression also
    works with many more than just one input variable and can also model different
    shapes than just straight lines (for example, in polynomial regression).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-2](#simple_linear_regression) 展示了一个简单线性回归的示例：x 轴上的输入变量预测了 y 轴上的变量。图中的点是观察到的实际数据点（标签），它们对应历史数据集中每个
    x 值。回归的预测将是直线上每个 x 值的 y 值。当然，这只是一个用于说明目的的基本示例。线性回归不仅仅适用于单一输入变量，还可以模拟比直线更复杂的形状（例如，多项式回归）。'
- en: '![Simple linear regression](Images/apbi_0302.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![简单线性回归](Images/apbi_0302.png)'
- en: Figure 3-2\. Simple linear regression
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-2\. 简单线性回归
- en: As you can see in the first row of [Table 3-1](#popular_machine_learning_algorithms),
    the model performance of linear regression can range anywhere from very low to
    very high. That’s because the performance of the linear regression algorithm depends
    heavily on the data and how well the data meets the assumptions of linear regression.
    If the input variables are really independent and have a linear relationship to
    the output variables, regression can beat any neural network.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在 [表 3-1](#popular_machine_learning_algorithms) 的第一行中所见，线性回归的模型性能可以从非常低到非常高不等。这是因为线性回归算法的性能严重依赖于数据以及数据如何符合线性回归的假设。如果输入变量确实是独立的，并且与输出变量具有线性关系，那么回归可以击败任何神经网络。
- en: That said, regression is not a good choice if your data contains nonlinear patterns.
    For example, if a certain value is met, then suddenly the relationship to the
    target variable changes. These sort of if-then-else rules can be better modeled
    with other algorithms, such as decision trees.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，如果您的数据包含非线性模式，回归并不是一个好选择。例如，如果满足某个特定值，然后突然与目标变量的关系发生变化。这类 if-then-else
    规则可以通过其他算法更好地建模，比如决策树。
- en: Decision Trees
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树
- en: In contrast to a regression model, tree-based models can work with both categorical
    and numerical data (regression trees), as you can see in [Table 3-1](#popular_machine_learning_algorithms).
    *Decision trees* split the data at various levels, variable after variable, until
    the data slices become so small that you can make a prediction. Imagine decision
    trees as a hierarchical order of if-then rules.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 与回归模型相比，基于树的模型可以处理分类和数值数据（回归树），正如你在 [表 3-1](#popular_machine_learning_algorithms)
    中所见。*决策树* 在各个级别、变量之后分割数据，直到数据片段变得足够小，你可以进行预测。想象决策树是一个层次化的 if-then 规则序列。
- en: Decision trees are usually good all-round algorithms that can be used on almost
    any tabular dataset as a first baseline model. Decision trees can be easily shared
    and explained even to nontechnical stakeholders. On the downside, single decision
    trees often don’t deliver the best results in all cases because the algorithm
    is “greedy.” That means it will do those splits first at the point where the data
    shows the highest discrepancy. This process isn’t ideal in all situations.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树通常是一种非常通用的算法，可以作为几乎任何表格数据集的首选基线模型。决策树易于分享和解释，即使对非技术相关的利益相关者也是如此。然而，单个决策树在某些情况下未必能够在所有情况下取得最佳结果，因为这种算法是“贪婪”的。这意味着它会首先在数据显示最大差异的地方进行分裂。这一过程并不在所有情况下都是理想的。
- en: '[Figure 3-3](#decision_tree) shows an example decision tree that predicts the
    survival of passengers on the *Titanic*, a popular research dataset. You read
    the tree from the top to the bottom, and each node represents a decision step.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-3](#decision_tree) 展示了一个预测 *泰坦尼克号* 上乘客生存情况的决策树示例，这是一个流行的研究数据集。你从上到下阅读树，每个节点代表一个决策步骤。'
- en: '![Decision tree](Images/apbi_0303.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![决策树](Images/apbi_0303.png)'
- en: Figure 3-3\. Decision tree
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-3\. 决策树
- en: Because the decision tree is greedy, it will make the first split on `gender`.
    If you pick a random female passenger from the Titanic dataset, the chances that
    this person survived are 73%. Not bad for a first guess. For men, the tree looks
    a little more complicated. The prediction will be `died` or `survived`, considering
    multiple factors such as the person’s `age` and the number of siblings or spouses
    (`sibsp`).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因为决策树是贪婪的，它会首先在 `gender` 上进行第一次分裂。如果你从泰坦尼克号数据集中随机选取一名女性乘客，那么这个人幸存的可能性为73%。作为第一个猜测，这并不差。对于男性，树看起来复杂一些。预测将是
    `died` 或 `survived`，考虑到诸如人的 `age` 和兄弟姐妹或配偶的数量 (`sibsp`) 等多个因素。
- en: 'We don’t go much more into detail here. It’s important only that you understand
    how decision trees work conceptually, because we will build on them in the next
    concept: ensemble learning.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里不会深入详细讨论。重要的是你理解决策树的工作原理，因为我们将在下一个概念中继续深入探讨：集成学习。
- en: Ensemble Learning Methods
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成学习方法
- en: Linear regression and decision trees are pretty straightforward, and the way
    they work is rather easy to explain. *Ensemble methods* sit pretty much at the
    other end of the spectrum. As you can see in [Table 3-1](#popular_machine_learning_algorithms),
    ensemble methods provide generally high predictive power. However, interpreting
    the way they make a decision is not so easy.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归和决策树都相当直观，它们的工作方式相对容易解释。*集成方法* 则处于谱系的另一端。正如你在 [表 3-1](#popular_machine_learning_algorithms)
    中所见，集成方法通常提供较高的预测能力。然而，解释它们做出决策的方式并不那么简单。
- en: Both linear regression and decision trees are typically considered *weak learners*,
    which means that they struggle if data relationships become more complex. For
    example, regression can’t work with nonlinear data, and decision trees can make
    a split only once for each variable in your dataset.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归和决策树通常被认为是*弱学习器*，这意味着它们在数据关系变得更复杂时表现不佳。例如，回归无法处理非线性数据，而决策树在数据集中的每个变量只能进行一次分裂。
- en: To combat these issues, ensemble methods allow you to combine several weak learners
    into a strong learner that shows a strong performance on complex datasets. The
    two typical methods of ensemble learning are bagging and boosting, which are differentiated
    by the way they combine weak learners.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 要应对这些问题，集成方法允许你将多个弱学习器组合成一个在复杂数据集上表现强大的强学习器。集成学习的两种典型方法是装袋（bagging）和提升（boosting），它们通过组合弱学习器的方式进行区分。
- en: '*Bagging* (from *bootstrap aggregating*) tries to combine multiple weak learners
    by training various models individually and then combining them by using an averaging
    process. [Figure 3-4](#bagging) shows a schema of how bagging works. In the middle
    layer, you can see four models that are all trained on the same training dataset.
    This could be four decision trees, for example, with different parameters for
    the tree depths or the minimum node sizes. For each data point, each of these
    four models will make an individual prediction (top layer). The final prediction
    will be an aggregation of these four results (for example, a majority vote in
    the case of a classification example, or an average value for a regression problem).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*装袋*（来自*自助法聚合*）试图通过分别训练多个弱学习器并使用平均过程将它们组合起来。[图 3-4](#bagging) 展示了装袋的工作原理图。在中间层，您可以看到四个模型，它们都在同一训练数据集上进行训练。例如，这可能是四棵决策树，具有不同的树深度或最小节点大小参数。对于每个数据点，这四个模型都将进行个别预测（顶层）。最终预测将是这四个结果的聚合（例如，在分类示例中的多数投票，或在回归问题中的平均值）。'
- en: '![Bagging](Images/apbi_0304.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![装袋](Images/apbi_0304.png)'
- en: Figure 3-4\. Bagging
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-4\. 装袋
- en: A popular bagging ensemble method is called *random forest*. This method splits
    the decision trees into multiple subtrees so we have less correlation between
    them.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行的装袋集成方法称为*随机森林*。这种方法将决策树分成多个子树，以减少它们之间的相关性。
- en: '*Boosting*, on the other hand, sequentially adds models to an ensemble, each
    one correcting the errors of its predecessor. It’s a popular and powerful technique.
    Two popular boosting methods are AdaBoost (adaptive boosting) and gradient boosting.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*提升* 相反，是顺序地将模型添加到集成中，每个模型都纠正其前任的错误。这是一种流行且强大的技术。两种流行的提升方法是AdaBoost（自适应提升）和梯度提升。'
- en: '[Figure 3-5](#boosting) shows a conceptual overview of how bagging works. The
    training process starts at the bottom left as the first model (for example, a
    decision tree) is trained and gives a first prediction with relatively high errors
    (top left). Next, another decision tree is trained that tries to correct the errors
    that the preceding model made (the second model from the left in the bottom row).
    This process is repeated several times until the errors become smaller and smaller
    and a well-performing model has been found. [Figure 3-5](#boosting) has three
    iterations, which then result in the final model at the bottom right.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-5](#boosting) 展示了装袋的概念概述。训练过程从左下角开始，第一个模型（例如决策树）训练并给出了一个具有相对高误差的第一次预测（左上角）。接下来，训练另一个决策树，它试图纠正前一个模型的错误（底行左边的第二个模型）。这个过程重复进行多次，直到误差变得越来越小，找到一个表现良好的模型。[图 3-5](#boosting)
    有三次迭代，最终在右下角得到最终模型。'
- en: '![Boosting](Images/apbi_0305.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![提升](Images/apbi_0305.png)'
- en: Figure 3-5\. Boosting
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-5\. 提升
- en: As you might see from this example, ensemble methods are much harder to interpret
    than linear regression or decision trees. However, in most cases they provide
    better performance.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您从这个例子中看到的，集成方法比线性回归或决策树更难解释。然而，在大多数情况下，它们提供了更好的性能。
- en: The approach you should take depends on your goals. If explainability is not
    as important as accuracy for your projects, choosing a strong learner such as
    adaptive boosting is typically the better choice. On the other hand, if your ML
    model needs to be well understood and interpreted by users or other stakeholders,
    choosing a slightly less-accurate model such as a decision tree or linear regression
    might be the better choice. When in doubt, choose the simpler model if it provides
    similar performance.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 您应采取的方法取决于您的目标。如果对于您的项目来说，解释性不如准确性重要，那么选择像自适应增强这样的强学习器通常是更好的选择。另一方面，如果您的机器学习模型需要用户或其他利益相关者深入理解和解释，那么选择稍微不太准确的模型，如决策树或线性回归，可能是更好的选择。如果犹豫不决，如果简单模型提供类似性能，则选择简单模型。
- en: Deep Learning
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习
- en: The algorithms we have covered so far work well on structured, tabular data
    typically found in spreadsheets or data warehouses. For other data types, such
    as images or text, you need different algorithms. This area is often referred
    to as *deep learning*.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖的算法在通常在电子表格或数据仓库中找到的结构化表格数据上表现良好。对于其他数据类型，如图像或文本，您需要不同的算法。这个领域通常被称为*深度学习*。
- en: We won’t go into much detail about that here. The AI services we will later
    use from Azure Cognitive Services are based on deep learning techniques, but we
    are not going to build them out ourselves. For our purposes, it’s enough to have
    a general idea of what these deep learning concepts mean and how they work.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们不会详细讨论这个问题。我们将稍后使用来自Azure认知服务的AI服务，这些服务基于深度学习技术，但我们不打算自己构建它们。对于我们的目的来说，了解这些深度学习概念的一般概念及其工作原理就足够了。
- en: '*Deep learning* refers to a variety of ML approaches and techniques that are
    specialized to work with *high-dimensional datasets*—data with many features.
    No formal definition explicitly details where “shallow” ML ends and “deep” learning
    starts, but it mostly comes down to the data types.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*深度学习*指的是各种专门处理*高维数据集*（即具有许多特征的数据）的机器学习方法和技术。并没有明确的正式定义说明“浅层”机器学习在哪里结束，“深度”学习开始，但大多数情况下关键在于数据类型。'
- en: 'Consider the following example: even if you build an ultra-high-dimensional
    sales predictor, you will probably not create more than a few dozen, maybe even
    hundreds, of features from your tabular CRM data. Now imagine you’re analyzing
    images for object detection. Even for rather small images with a resolution of
    300 × 300 pixels, you would get 90,000 dimensions (one dimension for each pixel).
    With colored red, green, and blue (RGB) pictures, this amount triples.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下示例：即使您构建了一个超高维销售预测器，您可能也不会从您的表格CRM数据中创建超过几十个，甚至数百个特征。现在想象一下，您正在分析用于对象检测的图像。即使对于分辨率为300×300像素的相当小的图像，您也会得到90000个维度（每个像素一个维度）。对于彩色红、绿和蓝（RGB）图片，这个数量会增加三倍。
- en: You can see that a considerable difference still remains between a high-dimensional
    table of maybe 1,000 columns (dimensions) and a small image representation of
    90,000 dimensions—for every picture. And that’s why deep learning is often considered
    for nontabular data such as images, videos, and unstructured text files.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，即使在可能有1000列（维度）的高维表格数据和每张图片都有90000个维度的小图像表示之间仍存在显著差异。这就是为什么深度学习通常用于像图像、视频和非结构化文本文件这样的非表格数据的原因。
- en: 'Two broad categories have emerged in deep learning: computer vision (dealing
    with picture or video data) and natural language processing (dealing with text
    data).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习分为两大类：计算机视觉（处理图片或视频数据）和自然语言处理（处理文本数据）。
- en: Natural Language Processing
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: You have probably already heard about AI services that are able to interpret
    human language, do translations more accurately than ever before, or even generate
    new texts completely automatically. Large language models that have been trained
    on billions of human text examples combined with a breakthrough technology called
    [transformers](https://oreil.ly/kDnMj) has catapulted the NLP field from a small
    research domain to one of the hottest areas in AI development.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经听说过能够解释人类语言、比以往更准确地进行翻译甚至完全自动生成新文本的AI服务。已经在数十亿人类文本示例上进行了训练的大型语言模型，结合一种称为[transformers](https://oreil.ly/kDnMj)的突破性技术，将NLP领域从一个小的研究领域推向了AI开发中最热门的领域之一。
- en: We won’t train NLP models ourselves in this book, but you will be exposed to
    using state-of-the-art language models to analyze language data at scale. This
    doesn’t apply to just written text. NLP technologies are also excellent for analyzing
    spoken words. This area is often referred to as *speech to text*.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会在本书中自行训练NLP模型，但你将接触使用最先进的语言模型来规模化分析语言数据。这不仅仅适用于书面文本。NLP技术还非常适合分析口语。这个领域通常被称为*语音转文本*。
- en: Computer Vision
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算机视觉
- en: '*Computer vision* is a technology that enables machines to “see” and interpret
    image and video files. A technology called *convolutional neural networks* (*CNNs*)
    has led to incredible breakthroughs in this field in recent years. Applications
    range from identifying commodity entities in images such as cars, bicycles, street
    signs, animals, and people, to face recognition and extracting text structures
    from images.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*计算机视觉*是一种技术，使得机器能够“看到”和解释图像和视频文件。一种名为*卷积神经网络*（*CNNs*）的技术近年来在这个领域取得了令人难以置信的突破。应用范围从在图像中识别如汽车、自行车、街道标志、动物和人类等商品实体，到人脸识别和从图像中提取文本结构。'
- en: It’s a diverse field with lots of things happening. In [Chapter 9](ch09.xhtml#leveraging_unstructured_data_with_ai),
    we will use a computer vision AI service to detect and count cars in image files.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个变化多端、充满活力的领域。在[第9章](ch09.xhtml#leveraging_unstructured_data_with_ai)中，我们将使用计算机视觉AI服务来检测和计数图像文件中的汽车。
- en: Reinforcement Learning
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习
- en: The last deep learning category I want to highlight is *reinforcement learning*.
    We will see a reinforcement learning service in action in [Chapter 8](ch08.xhtml#ai_powered_prescriptive_analytics).
    Reinforcement learning works differently than supervised or unsupervised learning
    and has become a separate field of its own.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我想要重点介绍的最后一个深度学习类别是*强化学习*。我们将在[第8章](ch08.xhtml#ai_powered_prescriptive_analytics)中看到强化学习服务的实际应用。强化学习的工作方式与监督学习或无监督学习有所不同，并已成为一个独立的领域。
- en: The reason lies in the way reinforcement learning models learn. Instead of relying
    on historical data, reinforcement learning approaches require a constant stream
    of new data coming in so the model can learn the best strategies, given the current
    state of a system and policies the model is allowed to act in.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习模型学习方式的原因在于它们学习的方式。强化学习方法不依赖于历史数据，而是需要持续涌入新数据流，以便模型可以根据系统当前状态和允许模型操作的策略学习最佳策略。
- en: Reinforcement learning has seen major breakthroughs by being able to beat world-class
    human players in games like Go and *StarCraft*. But as you will find out later,
    it is also a great way to personalize user experiences and learn over time what
    users want and how to interact with them.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习已经取得了重大突破，能够击败像围棋和*星际争霸*这样的世界级人类选手。但正如你后面将发现的那样，它也是个性化用户体验和随时间学习用户需求以及如何与用户交互的绝佳方式。
- en: Machine Learning Model Evaluation
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习模型评估
- en: With the knowledge you’ve gained from this book so far, you should be confident
    enough to build and apply ML models from a prototyping stage to real-world use
    cases. One component is still missing, though. And that is how to assess how well
    your model is actually working.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通过你迄今为止从本书中获得的知识，你应该足够自信能够从原型阶段到真实世界应用ML模型。然而，还缺少一个组成部分。那就是如何评估你的模型实际运行效果的方法。
- en: Being able to measure the performance of an ML model is critical not only during
    prototyping but also later in production. The evaluation metric we choose should
    not differ, only the value of the metric. So how do we evaluate the performance
    of an ML model?
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 能够测量ML模型性能的能力在原型设计阶段和后期投产阶段至关重要。我们选择的评估指标不应该有所不同，只是指标值不同。那么我们如何评估ML模型的性能呢？
- en: 'Welcome to the world of evaluation metrics! Evaluation metrics are calculated
    for two reasons:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到评估指标的世界！评估指标有两个计算目的：
- en: To compare models to one another in order to find the best predictive model
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了比较模型之间的优劣，以找到最佳的预测模型
- en: To continuously measure the model’s true performance on new, unseen data
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持续测量模型在新的未见数据上的真实性能
- en: 'The basic idea behind model evaluation is always the same: we compare the values
    that our model predicts to the values that the model should have predicted according
    to our ground truth (e.g., the labels in our dataset). What sounds intuitively
    simple carries a good amount of complexity. In fact, you could write a book on
    evaluation metrics alone.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 模型评估的基本思想始终如一：我们将模型预测的值与地面真相中模型应该预测的值（例如数据集中的标签）进行比较。这听起来直观简单，但实际上涵盖了相当多的复杂性。事实上，仅仅是评估指标本身就可以写成一本书。
- en: In our case, we’ll keep it short and focus on the most important concepts you
    will most likely encounter when working with ML algorithms. We use different evaluation
    metrics for classification and regression problems.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们将保持简洁，重点关注您在处理机器学习算法时可能会遇到的最重要概念。我们针对分类和回归问题使用不同的评估指标。
- en: Evaluating Regression Models
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估回归模型
- en: 'Remember that regression models predict continuous numeric variables such as
    revenues, quantities, and sizes. The most popular metric to evaluate regression
    models is the *root-mean-square error* (*RMSE*). It is the square root of the
    average squared error in the regression’s predicted values (*ŷ*). The RMSE can
    be defined as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，回归模型预测连续数值变量，如收入、数量和大小。评估回归模型最流行的指标是*均方根误差*（*RMSE*）。它是回归预测值（*ŷ*）平均平方误差的平方根。RMSE可以定义如下：
- en: <math><mrow><mrow><mtext mathvariant="italic">RMSE</mtext><mo>=</mo><msqrt><mfrac><mrow><mstyle
    displaystyle="true" scriptlevel="0"><msubsup><mrow><mi mathvariant="normal">Σ</mi></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi></mrow></msubsup><msup><mrow><mo>(</mo><msub><mrow><mi>y</mi></mrow><mrow><mi>i</mi></mrow></msub><mo>−</mo><msub><mrow><mover><mrow><mi>y</mi></mrow><mrow><mo>^</mo></mrow></mover></mrow><mrow><mi>i</mi></mrow></msub><mo>)</mo></mrow><mrow><mn>2</mn></mrow></msup></mstyle></mrow><mrow><mstyle
    displaystyle="true" scriptlevel="0"><mi>n</mi></mstyle></mrow></mfrac></msqrt></mrow></mrow></math>
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><mtext mathvariant="italic">RMSE</mtext><mo>=</mo><msqrt><mfrac><mrow><mstyle
    displaystyle="true" scriptlevel="0"><msubsup><mrow><mi mathvariant="normal">Σ</mi></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi></mrow></msubsup><msup><mrow><mo>(</mo><msub><mrow><mi>y</mi></mrow><mrow><mi>i</mi></mrow></msub><mo>−</mo><msub><mrow><mover><mrow><mi>y</mi></mrow><mrow><mo>^</mo></mrow></mover></mrow><mrow><mi>i</mi></mrow></msub><mo>)</mo></mrow><mrow><mn>2</mn></mrow></msup></mstyle></mrow><mrow><mstyle
    displaystyle="true" scriptlevel="0"><mi>n</mi></mstyle></mrow></mfrac></msqrt></mrow></mrow></math>
- en: RMSE measures the model’s overall accuracy and can be used to compare regression
    models to one another. The smaller the value, the better the model seems to perform.
    The RMSE comes in the original scale of the predicted values. For example, if
    your model predicts a house price in US dollars and you see an RMSE of 256.6,
    this translates to “the model predictions are wrong by 256.6 dollars on average.”
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE衡量了模型的整体准确度，并可以用来比较各个回归模型之间的表现。数值越小，模型的表现似乎越好。RMSE的结果与原始预测值的规模一致。例如，如果你的模型预测房价的美元数值，并且看到RMSE为256.6，那么这意味着“模型的预测平均偏差为256.6美元”。
- en: 'Another popular metric that you will see in the context of regression evaluation
    is the *coefficient of determination*, also called the *R-squared* statistic.
    R-squared ranges from 0 to 1 and measures the proportion of variation in the data
    accounted for in the model. It is useful when you want to assess how well the
    model fits the data. The formula for R-squared is as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归评估的背景下，你还会看到另一个流行的度量指标，*决定系数*，也称为*R-squared*统计量。R-squared的取值范围是从0到1，用来衡量模型中数据变化的比例。当你想评估模型与数据的拟合程度时，这个指标非常有用。R-squared的公式如下：
- en: <math><mrow><mrow><msup><mrow><mi>R</mi></mrow><mrow><mn>2</mn></mrow></msup><mo>=</mo><mn>1</mn><mo>−</mo><mfrac><mrow><mstyle
    displaystyle="true" scriptlevel="0"><msubsup><mrow><mi mathvariant="normal">Σ</mi></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi></mrow></msubsup><msup><mrow><mo>(</mo><msub><mrow><mi>y</mi></mrow><mrow><mi>i</mi></mrow></msub><mo>−</mo><msub><mrow><mover><mrow><mi>y</mi></mrow><mrow><mo>^</mo></mrow></mover></mrow><mrow><mi>i</mi></mrow></msub><mo>)</mo></mrow><mrow><mn>2</mn></mrow></msup></mstyle></mrow><mrow><mstyle
    displaystyle="true" scriptlevel="0"><msubsup><mrow><mi mathvariant="normal">Σ</mi></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi></mrow></msubsup><msup><mrow><mo>(</mo><msub><mrow><mi>y</mi></mrow><mrow><mi>i</mi></mrow></msub><mo>−</mo><mover><mrow><mi>y</mi></mrow><mrow><mo>¯</mo></mrow></mover><mo>)</mo></mrow><mrow><mn>2</mn></mrow></msup></mstyle></mrow></mfrac></mrow></mrow></math>
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><msup><mrow><mi>R</mi></mrow><mrow><mn>2</mn></mrow></msup><mo>=</mo><mn>1</mn><mo>−</mo><mfrac><mrow><mstyle
    displaystyle="true" scriptlevel="0"><msubsup><mrow><mi mathvariant="normal">Σ</mi></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi></mrow></msubsup><msup><mrow><mo>(</mo><msub><mrow><mi>y</mi></mrow><mrow><mi>i</mi></mrow></msub><mo>−</mo><msub><mrow><mover><mrow><mi>y</mi></mrow><mrow><mo>^</mo></mrow></mover></mrow><mrow><mi>i</mi></mrow></msub><mo>)</mo></mrow><mrow><mn>2</mn></mrow></msup></mstyle></mrow><mrow><mstyle
    displaystyle="true" scriptlevel="0"><msubsup><mrow><mi mathvariant="normal">Σ</mi></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi></mrow></msubsup><msup><mrow><mo>(</mo><msub><mrow><mi>y</mi></mrow><mrow><mi>i</mi></mrow></msub><mo>−</mo><mover><mrow><mi>y</mi></mrow><mrow><mo>¯</mo></mrow></mover><mo>)</mo></mrow><mrow><mn>2</mn></mrow></msup></mstyle></mrow></mfrac></mrow></mrow></math>
- en: You can interpret this as 1 minus the explained variance of the model divided
    by the total variance of the target variable. Generally, the higher R-squared,
    the better. An R-squared value of 1 would mean that the model could explain all
    variance in the data.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以解释为模型解释的方差与目标变量总方差之比的一减。通常情况下，R-squared 值越高，模型越好。R-squared 值为1意味着模型能够解释数据中的所有方差。
- en: While summary statistics are a great way to compare models to one another at
    scale, figuring out whether our regression is working as expected from these metrics
    alone is still difficult. Therefore, a good approach in regression modeling is
    to also look at the distribution of the regression errors, called the *residuals*
    of our model. The distribution of the residuals gives us good visual feedback
    on how the regression model is performing.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然汇总统计数据是比较不同模型的好方法，但仅仅依靠这些指标来判断我们的回归是否按预期工作仍然很困难。因此，在回归建模中的一个良好方法是查看回归误差的分布，即我们模型的*残差*。残差的分布为我们提供了关于回归模型表现的良好视觉反馈。
- en: 'A residual diagram plots the predicted values against the residuals as a simple
    scatterplot. Ideally, a residual plot should look like [Figure 3-6](#residual_analysis)
    to back up the following assumptions:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 残差图将预测值与残差作为简单的散点图绘制。理想情况下，残差图应该看起来像 [图 3-6](#residual_analysis)，以支持以下假设：
- en: Linearity
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 线性性
- en: The points in the residual plot should be randomly scattered without exposing
    too much curvature.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 残差图中的点应该随机分布，而不应该显示出太多的曲线。
- en: Heteroscedasticity
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 异方差性
- en: The spread of the points across predicted values should be more or less the
    same for all predicted values.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 预测值在各个预测值上的点的分布应该基本相同。
- en: '![Residual analysis](Images/apbi_0306.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![残差分析](Images/apbi_0306.png)'
- en: Figure 3-6\. Residual analysis
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-6\. 残差分析
- en: Residual plots will show you on which parts of your data the regression model
    works well and on which parts it doesn’t. Depending on your use case, you can
    decide whether this is problematic or still acceptable, given all other factors.
    Looking at the summary statistics alone would not give you these insights. Keeping
    an eye on the residuals is always a good idea when evaluating regression models.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 残差图将展示你的数据中回归模型表现良好和表现不佳的部分。根据你的使用情况，你可以决定这是否是一个问题，或者在考虑所有其他因素时是否仍然可接受。仅仅查看汇总统计数据不能带给你这些见解。在评估回归模型时，始终注意残差是一个好主意。
- en: Evaluating Classification Models
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估分类模型
- en: How do we evaluate the performance of a predictive model if the predicted values
    are not numeric but categorical? We can’t just apply the metrics we learned from
    the regression model. To understand why, let’s take a look at the data in [Table 3-2](#example_classification_outputs).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果预测值不是数值而是分类的话，我们如何评估预测模型的性能呢？我们不能简单地应用从回归模型中学到的度量标准。为了理解原因，让我们看一下 [表 3-2](#example_classification_outputs)
    中的数据。
- en: The table shows the true labels of a classification problem (in the column Targets)
    as well as the corresponding outputs (in the Predicted and “Probabilistic output”
    columns) of a classification model.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 该表显示了分类问题的真实标签（目标列）以及分类模型的相应输出（预测和“概率输出”列）。
- en: Table 3-2\. Example classification outputs
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3-2\. 分类输出示例
- en: '| Targets | Predicted | Probabilistic output |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 预测 | 概率输出 |'
- en: '| --- | --- | --- |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | 1 | 0.954 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 0.954 |'
- en: '| 1 | 0 | 0.456 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 0.456 |'
- en: '| 0 | 0 | 0.012 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0.012 |'
- en: '| 0 | 1 | 0.567 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 0.567 |'
- en: '| 0 | 0 | 0.234 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0.234 |'
- en: '| … | … | … |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| … | … | … |'
- en: 'If you look at the first four rows, you’ll see four things that could happen
    here:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看看前四行，你会看到可能会发生的四件事：
- en: The true label is `1`, and our prediction is `1` (correct prediction).
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真实标签为 `1`，我们的预测是 `1`（正确预测）。
- en: The true label is `0`, and our prediction is `0` (correct prediction).
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真实标签为 `0`，我们的预测是 `0`（正确预测）。
- en: The true label is `1`, and our prediction is not `1` (incorrect prediction).
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真实标签为 `1`，我们的预测不是 `1`（预测错误）。
- en: The true label is `0`, and our prediction is not `0` (incorrect prediction).
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真实标签为 `0`，我们的预测不是 `0`（预测错误）。
- en: These four outcomes would also happen if we had more than two categories. We
    could simply observe these four outcomes for each category in our dataset.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有超过两个类别，这四种结果也会发生。我们可以简单地观察数据集中每个类别的这四种结果。
- en: The most popular way to measure the performance of a classification model is
    to display these four outcomes in a *confusion table*, or *confusion matrix*,
    as shown in [Table 3-3](#confusion_matrix).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 评估分类模型性能的最流行方式是在 *混淆表* 或 *混淆矩阵* 中展示这四种结果，如 [表3-3](#confusion_matrix) 所示。
- en: Table 3-3\. Confusion matrix
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3-3\. 混淆矩阵
- en: '|   |   | Predicted class |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|   |   | 预测类别 |'
- en: '| --- | --- | --- |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|   |   | Negative | Positive |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|   |   | 负面 | 正面 |'
- en: '| **Actual class** | Negative | 2,388 true negative (TN) | 558 false positive
    (FP) |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| **实际类别** | 负面 | 2,388 真负例 (TN) | 558 假正例 (FP) |'
- en: '| Positive | 415 false negative (FN) | 2,954 true positive (TP) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 正面 | 415 假负例 (FN) | 2,954 真正例 (TP) |'
- en: This confusion table tells us that for our example classification problem, there
    were 2,388 observations where the actual label was `0` and our model labeled them
    correctly. These observations are called *true negatives*, because the model correctly
    predicted the negative class label. Likewise, 2,954 observations were identified
    by our model as *true positives*, as both the actual and the predicted outcome
    had the positive class label (in this case, the number `1`).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这个混淆表告诉我们，在我们的例子分类问题中，有 2,388 个观察结果实际标签为 `0`，我们的模型正确预测了它们。这些观察结果被称为 *真负例*，因为模型正确地预测了负类标签。同样，有
    2,954 个观察结果被我们的模型标识为 *真正例*，因为实际和预测的结果都有正类标签（在本例中是数字 `1`）。
- en: But our model also made two kinds of errors. First, it wrongly predicted negative
    classes to be positive (558 cases), also known as a *type I error*. Second, the
    model incorrectly predicted 415 cases to be negative that were, in fact, positive.
    These errors are known as *type II errors*. Statisticians were rather less creative
    in naming these things.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们的模型也犯了两种错误。首先，它错误地将负类预测为正类（558 个案例），也称为 *I 型错误*。其次，模型错误地预测了实际上是正的 415 个案例为负。这些错误称为
    *II 型错误*。统计学家在给这些事物命名时没有那么有创意。
- en: 'Now, we can calculate various metrics from this confusion table. The most popular
    metric that you will see and hear often is accuracy. *Accuracy* describes how
    often our model was correct, based on all predictions. We can easily calculate
    the model’s accuracy by dividing the number of correct predictions by the number
    of all predictions:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以从这个混淆表中计算各种指标。你会经常看到和听到的最流行指标是准确率。*准确率* 描述了我们的模型多频繁地正确，基于所有预测。我们可以通过将正确预测的数量除以所有预测的数量来轻松计算模型的准确率：
- en: <math><mrow><mrow><mtext>ACC</mtext><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mstyle
    displaystyle="true" scriptlevel="0"><mtext>TP</mtext><mo>+</mo><mtext>TN</mtext></mstyle></mrow><mrow><mtext>TP</mtext><mo>+</mo><mtext>TN</mtext><mo>+</mo><mtext>FP</mtext><mo>+</mo><mtext>FN</mtext></mrow></mfrac></mstyle></mrow></mrow></math>
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><mtext>ACC</mtext><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mstyle
    displaystyle="true" scriptlevel="0"><mtext>TP</mtext><mo>+</mo><mtext>TN</mtext></mstyle></mrow><mrow><mtext>TP</mtext><mo>+</mo><mtext>TN</mtext><mo>+</mo><mtext>FP</mtext><mo>+</mo><mtext>FN</mtext></mrow></mfrac></mstyle></mrow></mrow></math>
- en: For the confusion matrix in [Table 3-3](#confusion_matrix), the accuracy is
    (2,388 + 2,954) / (2,954 + 2,388 + 558 + 415) = 0.8459, meaning that the predictions
    of our model were correct in 84.59% of all cases.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 对于[表 3-3](#confusion_matrix)中的混淆矩阵，准确度为 (2,388 + 2,954) / (2,954 + 2,388 + 558
    + 415) = 0.8459，这意味着我们模型的预测在所有情况下都正确率达到了 84.59%。
- en: 'While accuracy is a widely used and popular metric, it has one big caveat:
    accuracy works only for balanced classification problems. Imagine the following
    example: we want to predict credit card fraud, which happens in only 0.1% of all
    credit card transactions. If our model consistently predicts no fraud, it would
    yield an astonishing accuracy score of 99.9%, because it would be correct in most
    cases.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然准确度是一个广泛使用和受欢迎的度量标准，但它有一个重要的限制：准确度仅适用于平衡分类问题。想象一个这样的例子：我们想预测信用卡欺诈，这在所有信用卡交易中只发生在
    0.1% 的情况下。如果我们的模型一直预测没有欺诈，它将得到令人惊讶的 99.9% 的准确度分数，因为在大多数情况下它都是正确的。
- en: While this model has an excellent accuracy metric, it’s clear that this model
    isn’t useful at all. That’s why the confusion matrix allows us to calculate even
    more metrics that are more balanced toward correctly identifying positive or negative
    class labels.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个模型具有优秀的准确度度量标准，但显然这个模型完全没有用。这就是为什么混淆矩阵允许我们计算更多更平衡地正确识别正负类别标签的度量标准。
- en: 'The first metric we could look at is *precision*. This measures the proportion
    of true positives (predicted true labels that were actually true). Precision,
    also called the *positive predictive value* (*PPV*) is defined as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看一下的第一个度量标准是*精确度*。它衡量了真正例的比例（预测的真标签实际上是真的）。精确度，也称为*正预测值*（*PPV*），定义如下：
- en: <math><mrow><mrow><mtext>PPV</mtext><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"
    scriptlevel="0"><mtext>TP</mtext></mstyle></mrow><mrow><mstyle displaystyle="true"
    scriptlevel="0"><mtext>TP</mtext><mo>+</mo><mtext>FP</mtext></mstyle></mrow></mfrac></mrow></mrow></math>
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><mtext>PPV</mtext><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"
    scriptlevel="0"><mtext>TP</mtext></mstyle></mrow><mrow><mstyle displaystyle="true"
    scriptlevel="0"><mtext>TP</mtext><mo>+</mo><mtext>FP</mtext></mstyle></mrow></mfrac></mrow></mrow></math>
- en: In our example, the precision is 2,954 / (2,954 + 558) = 0.8411\. This value
    can be interpreted to mean that when the model labeled a data point as positive
    (`1`), it was correct in 84% of all cases. We will pick this metric if the costs
    of a false positive are very high and the costs of a false negative are rather
    low (for example, when we want to decide whether we are going to show an expensive
    ad to an online user).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，精确度为 2,954 / (2,954 + 558) = 0.8411。这个数值可以解释为当模型将数据点标记为正类别（`1`）时，它在所有情况下都是正确的
    84%。如果一个假阳性的成本非常高而一个假阴性的成本较低（例如，我们要决定是否向在线用户展示昂贵的广告），我们将选择这个度量标准。
- en: 'Another metric that looks at the positive classes but has a slightly different
    focus is *recall*, sometimes also called *sensitivity*, or *true positive rate*
    (*TPR*). It gives the percentage of all positive classes that were correctly classified
    as positive. Recall is defined as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关注正类别但具有稍有不同焦点的度量标准是*召回率*，有时也称为*敏感性*或*真正率*（*TPR*）。它给出了所有正类别中被正确分类为正类别的百分比。召回率定义如下：
- en: <math><mrow><mrow><mtext>TPR</mtext><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"
    scriptlevel="0"><mtext>TP</mtext></mstyle></mrow><mrow><mstyle displaystyle="true"
    scriptlevel="0"><mtext>TP</mtext><mo>+</mo><mtext>FN</mtext></mstyle></mrow></mfrac></mrow></mrow></math>
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><mtext>TPR</mtext><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"
    scriptlevel="0"><mtext>TP</mtext></mstyle></mrow><mrow><mstyle displaystyle="true"
    scriptlevel="0"><mtext>TP</mtext><mo>+</mo><mtext>FN</mtext></mstyle></mrow></mfrac></mrow></mrow></math>
- en: In our example, this would be 2,954 / (2,954 + 415) = 0.8768\. The value means
    that the model could identify 87.68% of all positive classes in the dataset. We
    would pick this metric to optimize our model for finding the positive classes
    in our dataset (e.g., for fraud detection). Using recall as an evaluation metric
    makes sense when the costs of missing these positive classes (false negatives)
    are very high.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，这将是 2,954 / (2,954 + 415) = 0.8768。这个数值表示模型能够识别数据集中 87.68% 的所有正类别。我们会选择这个度量标准来优化我们的模型，以便在数据集中找到正类别（例如，用于欺诈检测）。在成本极高的情况下错过这些正类别（假阴性）时，使用召回率作为评估度量是合理的。
- en: If you don’t want to lean toward precision or recall, and accuracy seems to
    be not capturing your problem well enough, you could consider another commonly
    used metric called the *F-score*. Despite its technical name, this metric is relatively
    intuitive as it combines precision and recall. The most popular F-score is the
    F1-score, which is simply the harmonic mean of precision and recall.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不想倾向于精确度或召回率，并且准确性似乎不能很好地捕捉您的问题，您可以考虑另一种常用的度量标准，称为*F分数*。尽管其技术名称，但此度量标准相对直观，因为它结合了精确度和召回率。最流行的F分数是F1分数，它只是精确度和召回率的调和平均数。
- en: 'We can thus easily calculate the F1-score if we already know the precision
    and the recall:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果已知精确度和召回率，我们可以轻松计算F1分数：
- en: <math><mrow><mrow><msub><mrow><mtext>F</mtext></mrow><mrow><mn>1</mn></mrow></msub><mo>=</mo><mn>2</mn><mo>×</mo><mstyle
    displaystyle="true" scriptlevel="0"><mfrac><mrow><mtext>PPV</mtext><mo>×</mo><mtext>TPR</mtext></mrow><mrow><mtext>PPV</mtext><mo>+</mo><mtext>TPR</mtext></mrow></mfrac></mstyle><mo>=</mo><mstyle
    displaystyle="true" scriptlevel="0"><mfrac><mrow><mn>2</mn><mtext>TP</mtext></mrow><mrow><mn>2</mn><mtext>TP</mtext><mo>+</mo><mtext>FP</mtext><mo>+</mo><mtext>FN</mtext></mrow></mfrac></mstyle></mrow></mrow></math>
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><msub><mrow><mtext>F</mtext></mrow><mrow><mn>1</mn></mrow></msub><mo>=</mo><mn>2</mn><mo>×</mo><mstyle
    displaystyle="true" scriptlevel="0"><mfrac><mrow><mtext>PPV</mtext><mo>×</mo><mtext>TPR</mtext></mrow><mrow><mtext>PPV</mtext><mo>+</mo><mtext>TPR</mtext></mrow></mfrac></mstyle><mo>=</mo><mstyle
    displaystyle="true" scriptlevel="0"><mfrac><mrow><mn>2</mn><mtext>TP</mtext></mrow><mrow><mn>2</mn><mtext>TP</mtext><mo>+</mo><mtext>FP</mtext><mo>+</mo><mtext>FN</mtext></mrow></mfrac></mstyle></mrow></mrow></math>
- en: In our case, the F1-score would thus be 2 × (0.8411 × 0.8768) / (0.8411 + 0.8768)
    = 0.8586.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，因此F1分数将是 2 × (0.8411 × 0.8768) / (0.8411 + 0.8768) = 0.8586。
- en: A higher F1-score indicates a better-performing model, but what does it tell
    us exactly? Well, the F1-score isn’t as easily explained in one sentence as the
    preceding metrics, but it is usually a good go-to metric to assess the quality
    of a model. Since the F1-score uses the harmonic of precision and recall instead
    of the arithmetic mean, it will tend to be lower if one of these would be very
    bad.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更高的F1分数表明模型表现更好，但它究竟告诉了我们什么呢？嗯，F1分数并不能像前面的指标那样用一句话轻松解释，但通常是评估模型质量的一个好去处。由于F1分数使用精确度和召回率的调和平均数而非算术平均数，如果其中一个非常低，它将倾向于较低分数。
- en: Imagine an extreme example with a precision of 0 and recall of 1\. The arithmetic
    mean would return a score of 0.5, which roughly translates to “If one is great,
    but the other one is poor, then the average is OK.” The harmonic mean, however,
    would be 0 in this case. Instead of averaging out high and low numbers, the F1-score
    will raise a flag if either recall or precision is extremely low. And that is
    much closer to what you would expect from a combined performance metric.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个极端的例子，精确度为0，召回率为1。算术平均数将返回0.5的分数，大致翻译为“如果一个很好，但另一个很差，那么平均值还行。”而调和平均数在这种情况下将为0。F1分数不会将高低数值平均化，而是在召回率或精确度极低时发出警告。这更接近你对联合表现指标的期望。
- en: Evaluating Multiclassification Models
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估多分类模型
- en: 'The metrics we used for binary classification problems also apply to *multiclassification
    problems* (which have more than two classes to predict). For example, consider
    the following targets and predicted values:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用于二分类问题的指标也适用于*多分类问题*（其中有超过两个类别要预测）。例如，请考虑以下目标和预测值：
- en: '[PRE2]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The only difference between a binary and a multiclassification problem is that
    you would calculate the evaluation metrics for each class (0, 1, 2, 3) and compare
    it against all other classes. The way you do that can follow different approaches,
    usually referred to as the *micro* and *macro* value.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 二分类和多分类问题之间唯一的区别在于，您将计算每个类别（0、1、2、3）的评估指标，并将其与所有其他类别进行比较。您可以使用不同的方法进行此操作，通常称为*微观*和*宏观*值。
- en: '*Micro scores* typically calculate metrics globally by counting all the total
    true positives, false negatives, and false positives together and only then dividing
    them by each other. *Macro scores*, on the other hand, calculate precision, recall,
    and so forth for each label first and then calculate the average of all of them.
    These approaches produce similar, but still different, results.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '*微观分数*通常通过将所有真正例、假负例和假正例总数一起计算，然后仅通过彼此除以彼此来计算指标。*宏观分数*则首先计算每个标签的精确度、召回率等，然后计算所有标签的平均值。这些方法产生类似但仍然不同的结果。'
- en: For example, our dummy series for the four classification labels has a micro
    F1-score of 0.667 and a macro F1-score of 0.583\. If you like, try it out and
    calculate the numbers by hand to foster your understanding of these metrics.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们为四个分类标签的虚拟系列的微观 F1 分数为 0.667，宏观 F1 分数为 0.583。如果愿意，可以尝试手动计算这些数字，以促进对这些指标的理解。
- en: Neither of the micro or macro metrics is better or worse. These are just different
    ways to calculate the same summary statistic based on different approaches. These
    nuances will become more critical after you work on more complex multiclass problems.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 微观或宏观度量都没有好坏之分。这只是基于不同方法计算同一摘要统计的不同方式。在你处理更复杂的多类问题后，这些细微差别会变得更加关键。
- en: There are even more performance metrics to look at, but these are beyond the
    scope of this book. For example, so far we haven’t even looked at performance
    metrics that consider the probabilistic outputs of the models, such as the area
    under the ROC curve (*AUC*).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 还有更多的性能指标可供参考，但这超出了本书的范围。例如，到目前为止，我们甚至还没有看过考虑模型概率输出的性能指标，比如 ROC 曲线下的面积（*AUC*）。
- en: There’s no silver bullet when it comes to evaluation metrics. It is important
    that you at least understand on a high level what these metrics are trying to
    capture and identify which metric is practical for your use case. And you need
    to know which performance is good enough to stop training more and more complex
    and (seemingly) accurate ML models. We will dive deeper into that in the next
    section.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估指标方面并不存在银弹。重要的是，你至少要高层次地理解这些指标试图捕捉什么，并确定哪个指标对你的用例实用。你还需要知道何时停止训练更复杂和（表面上）准确的
    ML 模型是足够好的。我们将在下一节深入探讨这一点。
- en: Common Pitfalls of Machine Learning
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的常见陷阱
- en: ML is a powerful tool for solving many modern problems, but like any other tool,
    it is easy to misuse and can lead to poor results. This section outlines some
    beginner’s mistakes you should avoid.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是解决许多现代问题的强大工具，但像任何其他工具一样，容易被误用并导致糟糕的结果。本节概述了一些初学者应避免的错误。
- en: 'Pitfall 1: Using Machine Learning When You Don’t Need It'
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 陷阱 1：在不需要时使用机器学习
- en: 'If you have a hammer, everything looks like a nail. The worst thing you could
    do is take your newly acquired knowledge of ML and look for business problems
    that seem like a good fit. Instead, look at it from the opposite angle: once you
    have identified a relevant business problem, consider whether ML could help solve
    it.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你手里有一把锤子，看什么都像钉子。你最糟糕的做法是利用你新学到的机器学习知识，寻找看起来适合的商业问题。相反，应该从相反的角度来看待：一旦确定了一个相关的商业问题，再考虑机器学习是否能帮助解决它。
- en: And even then, it’s important to start with a simple baseline first. This baseline
    is important to get a performance benchmark that the ML algorithm needs to outperform.
    If you have never done churn analysis before, you should not start with an ML-based
    approach, but rather use simple heuristics or hardcoded rules. These have low
    complexity and are easy for business stakeholders to understand. Switch to ML
    models as soon as you find that they perform significantly better than your baseline
    solution.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 即便如此，从一个简单的基线开始是很重要的。这个基线对于获取一个 ML 算法需要超越的性能基准至关重要。如果你以前从未进行过客户流失分析，你不应该从一个基于
    ML 的方法开始，而是使用简单的启发式方法或硬编码规则。这些方法复杂度低，业务利益相关者容易理解。一旦发现它们的表现显著优于你的基线解决方案，再转向 ML
    模型。
- en: 'Pitfall 2: Being Too Greedy'
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 陷阱 2：过于贪心
- en: 'Don’t be too greedy in maximizing the performance metrics of your training
    set. This can cause your model to perform poorly on new data that it has never
    seen before. It may sound counterintuitive, but you should be more conservative
    in your training, especially on smaller datasets, as this can lead to better generalization
    of your model to unseen data. This concept is also known as *Occam’s razor*: when
    in doubt, a simple solution is better than a complex one.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 不要贪图在训练集上最大化性能指标。这可能导致你的模型在未曾见过的新数据上表现不佳。这听起来可能违反直觉，但在训练时应更加保守，特别是在较小的数据集上，这可以更好地推广模型到未见数据的泛化能力。这个概念也被称为*奥卡姆剃刀*：当你犹豫不决时，简单的解决方案比复杂的解决方案更好。
- en: 'Pitfall 3: Building Overly Complex Models'
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 陷阱 3：构建过于复杂的模型
- en: 'Building models that are too complex goes hand in hand with the previous point:
    to achieve high accuracy, inexperienced people tend to create overly complex models
    such as neural networks. In many cases, a less accurate model with lower complexity
    is preferred to a highly complex, highly accurate model.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 建立过于复杂的模型往往与前述观点相辅相成：为了达到高准确度，经验不足的人往往倾向于创建过于复杂的模型，例如神经网络。在许多情况下，比起高度复杂且高准确度的模型，更偏向于一个准确度较低但复杂度较低的模型。
- en: 'Complex models have two disadvantages:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂模型存在两个缺点：
- en: They can be difficult to maintain and debug in production. If something goes
    wrong or your model predictions are off (which they certainly will be from time
    to time), highly complex models are harder to correct than simpler models.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生产环境中，复杂模型可能难以维护和调试。如果出现问题或者你的模型预测出现偏差（这种情况肯定会偶尔发生），相较于简单模型，高度复杂的模型更难纠正。
- en: 'Complex models can lead to *overfitting*: your model performs well on your
    training data but poorly on new data. Predictive analysis is about finding a sweet
    spot (see [Figure 3-7](#training_error_versus_testing_error)) where the prediction
    errors in training (historical data) and testing (new data) are approximately
    equal. To avoid overfitting, you should always monitor the performance of your
    model on new data as well as the complexity of your model.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复杂模型可能导致*过拟合*：你的模型在训练数据上表现良好，但在新数据上表现不佳。预测分析就是要找到一个甜点（见[图 3-7](#training_error_versus_testing_error)），在这个甜点上，训练（历史数据）和测试（新数据）中的预测误差大致相等。为了避免过拟合，你应该始终监控模型在新数据上的表现以及模型的复杂度。
- en: '![Training error versus testing error](Images/apbi_0307.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![训练误差与测试误差](Images/apbi_0307.png)'
- en: Figure 3-7\. Training error versus testing error
  id: totrans-195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-7\. 训练误差与测试误差
- en: 'Pitfall 4: Not Stopping When You Have Enough Data'
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 陷阱4：没有在数据足够时停止
- en: Rarely will you have perfectly labeled ML data lying around in your organization.
    As you have already learned, usually the more examples (observations) we have,
    the better. However, experience shows that ML algorithms reach a plateau at a
    certain point, where additional training examples no longer significantly increase
    accuracy. This effect is illustrated in [Figure 3-8](#model_accuracy_versus_labeling_costs).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的组织中很少会有完全标记完美的机器学习数据。正如你已经了解到的，通常情况下，我们拥有的示例（观察结果）越多，效果就越好。然而，经验表明，机器学习算法会在某一点达到平台期，在这一点之后，额外的训练示例不再显著提高准确性。这种效应在[图 3-8](#model_accuracy_versus_labeling_costs)中有所体现。
- en: When you reach this point, it is simply no longer cost-effective to spend more
    money on data labeling. For this reason, it is important to have a clear goal
    in mind. How accurate does your model need to be for you to use it? The threshold
    can range from “anything better than baseline” to “99.99 accuracy required,” such
    as in medical cases. Knowing your goal and what you want to achieve can help you
    avoid high costs.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 当你达到这一点时，再多花费在数据标记上的资金就变得不划算了。因此，设定一个明确的目标非常重要。你的模型需要多准确才能使用？阈值可以从“比基准值更好的任何情况”到“需要99.99%的准确度”，例如在医疗案例中。了解你的目标和想要实现的内容可以帮助你避免高昂的成本。
- en: '![Model accuracy versus labeling costs](Images/apbi_0308.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![模型准确度与标记成本](Images/apbi_0308.png)'
- en: Figure 3-8\. Model accuracy versus labeling costs
  id: totrans-200
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-8\. 模型准确度与标记成本
- en: 'Pitfall 5: Falling for the Curse of Dimensionality'
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 陷阱5：陷入维度灾难
- en: While the “the more data, the better” principle works for observations (rows),
    it can be counterproductive for features (columns). Imagine the following example.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然“数据越多越好”的原则适用于观察（行），但对于特征（列）来说可能会适得其反。想象一下以下的例子。
- en: You want to predict US home prices based on the variables size, bedrooms, and
    location (zip code). Size and bedrooms can be modeled as two individual features
    (columns), which increases the dimensionality of your dataset by only an order
    of two, as shown in [Figure 3-9](#data_points_across_multiple_dimensions).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 你想基于大小、卧室数量和位置（邮政编码）来预测美国房价。大小和卧室可以建模为两个独立的特征（列），这样可以将数据集的维度增加一个数量级，如[图 3-9](#data_points_across_multiple_dimensions)所示。
- en: However, once you start using zip codes, most ML algorithms require that you
    encode this categorical variable into a single-column feature that is exactly
    as wide as the number of unique values. This increases the dimensionality of the
    data not just by an order of 1, but by an order of 41,692 (possible US zip codes),
    since it would be single columns containing either the value `0` or `1` for each
    example.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一旦你开始使用邮政编码，大多数机器学习算法要求你将此类分类变量编码为一个单列特征，其宽度与唯一值的数量完全相同。这会将数据的维度增加不仅仅是一阶，而是41692阶（可能的美国邮政编码数量），因为它将是单列，每个示例包含值`0`或`1`。
- en: 'Most ML algorithms will struggle to find actual patterns in the data because
    the data is too sparse. No absolute rules can help you avoid the curse of dimensionality,
    but here are some tips:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习算法将难以在数据中找到实际模式，因为数据太稀疏。没有绝对的规则能帮助你避免维度灾难，但这里有一些建议：
- en: Be careful when adding new features to your dataset and do not hesitate to delete
    redundant or irrelevant features. This can sometimes be difficult, as you may
    need solid domain expertise for this step.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在向数据集添加新特征时要小心，不要犹豫地删除冗余或无关的特征。有时这一步骤可能很困难，因为你可能需要扎实的领域专业知识。
- en: Try to reduce the number of relations between attributes by coding these dependencies
    as a single attribute. For example, instead of having two attributes, price and
    size, you could calculate a new attribute, price per square foot. The fewer dependencies
    you have between variables in your dataset, the easier it will be for your ML
    algorithm to understand them.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试通过将这些依赖关系编码为单一属性来减少属性之间的关系数量。例如，不要有两个属性，价格和大小，你可以计算一个新的属性，每平方英尺价格。数据集中变量之间的依赖关系越少，机器学习算法就越容易理解它们。
- en: '![Data points across multiple dimensions](Images/apbi_0309.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![跨多个维度的数据点](Images/apbi_0309.png)'
- en: Figure 3-9\. Data points across multiple dimensions
  id: totrans-209
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-9\. 跨多个维度的数据点
- en: 'Pitfall 6: Ignoring Outliers'
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'Pitfall 6: 忽略异常值'
- en: '*Outliers* are data points that are far above the average value of your dataset.
    For example, imagine a dataset that contains people’s salaries and net worth.
    If you plot the data, it might look like the chart at the top in [Figure 3-10](#sample_data_with_outliers_left_parenthe).'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '*异常值*是远高于数据集平均值的数据点。例如，想象一个包含人们工资和净值的数据集。如果绘制数据，它可能看起来像[图 3-10](#sample_data_with_outliers_left_parenthe)顶部的图表。'
- en: '![Sample data with outliers (top) and without (bottom)](Images/apbi_0310.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![跨多个维度的数据点](Images/apbi_0310.png)'
- en: Figure 3-10\. Sample data with outliers (top) and without (bottom)
  id: totrans-213
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-10\. 带异常值（顶部）和不带异常值（底部）的样本数据
- en: You can see that a strong correlation seems to exist between the two variables
    *salary* and *net worth*. But a few data points are very far away from all others.
    These represent people who have a high salary, but even higher net worth. The
    regression line (dashed) in this case is skewed toward these outliers, resulting
    in a poor fit for all other data points. Once we remove this group of high-net-worth
    individuals (on the bottom in [Figure 3-10](#sample_data_with_outliers_left_parenthe)),
    the regression line seems to fit the remaining points much better.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到*工资*和*净值*之间似乎存在很强的相关性。但有一些数据点与所有其他数据点相距甚远。这些数据点代表的是工资高，但净值更高的人群。在这种情况下，回归线（虚线）偏向这些异常值，导致其他所有数据点拟合效果不佳。一旦我们移除这些高净值个体组（在[图
    3-10](#sample_data_with_outliers_left_parenthe)左括号下）后，回归线似乎更好地适应了其余数据点。
- en: The impact of outliers can be huge for many algorithms, especially those that
    work with regression tasks. This means that you should pay close attention to
    detecting outliers in your dataset.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 对许多算法来说，异常值的影响可能非常巨大，尤其是那些处理回归任务的算法。这意味着你应该密切注意检测数据集中的异常值。
- en: 'Pitfall 7: Taking Cloud Infrastructure for Granted'
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'Pitfall 7: 对云基础设施理所当然'
- en: 'In this book, I make the bold assumption that you have access to cloud computing
    for both model training and model inference. But let’s face it: in many companies
    that may not be the case (yet). Although cloud computing adoption is growing rapidly,
    most companies are still largely using on-premises solutions. The reasons for
    this are many, but one important reason is the fear of losing control of data.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我大胆地假设您可以访问云计算进行模型训练和推理。但让我们面对现实：在许多公司中，情况可能并非如此（尚）。尽管云计算的采用速度正在迅速增长，但大多数公司仍然主要使用本地解决方案。造成这种情况的原因很多，但一个重要原因是害怕失去数据控制权。
- en: I strongly recommend that you try cloud computing (AIaaS or ML platforms) at
    least for prototyping with noncritical data, as described in the next chapter.
    While this may not yet be a lived practice in your organization, it will give
    you a quick head start and put you in touch with cutting-edge ML workflows and
    tools. This will also enable you to discuss with your team how cloud computing
    could be an investment that benefits your overall AI/ML strategy.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈建议您至少尝试使用云计算（AIaaS或ML平台）进行原型设计，即使是使用非关键数据，正如下一章所述。虽然这可能还不是您组织中的惯常做法，但它将使您快速启动，并与前沿的ML工作流程和工具保持联系。这还将使您能够与团队讨论云计算如何成为一个有利于整体AI/ML战略的投资。
- en: Summary
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned the basic concepts of supervised machine learning
    and got an idea of which algorithms are important for ML. We also touched on deep
    learning, computer vision, and NLP.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学习了监督式机器学习的基本概念，并了解了哪些算法对机器学习非常重要。我们还涉及了深度学习、计算机视觉和自然语言处理。
- en: Of course, you have much more to learn about ML, but these pages will give you
    everything you need to build your own ML-powered prototypes. I hope you feel a
    little more confident to do this by now. If not, don’t worry!
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，您在机器学习方面还有很多东西要学习，但这些页面将为您提供构建自己的ML驱动原型所需的一切。我希望您现在对此有了一些信心。如果没有，不要担心！
- en: 'Later, as we approach practical use cases, the concepts from this chapter will
    probably seem much more accessible to you, as they’ll be connected to some practice.
    If you want to dig deeper into ML, I recommend any of the following books:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 等到我们接近实际应用案例时，本章的概念可能会对您更易于理解，因为它们将与某些实践联系在一起。如果您想深入了解机器学习，我推荐阅读以下任何一本书：
- en: '[*Machine Learning Design Patterns*](https://www.oreilly.com/library/view/machine-learning-design/9781098115777)
    by Valliappa Lakshmanan et al. (O’Reilly)'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*机器学习设计模式*](https://www.oreilly.com/library/view/machine-learning-design/9781098115777)
    由Valliappa Lakshmanan 等人（O’Reilly）'
- en: '[*Introduction to Machine Learning with Python*](https://www.oreilly.com/library/view/introduction-to-machine/9781449369880)
    by Andreas C. Müller and Sarah Guido (O’Reilly)'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*Python机器学习入门*](https://www.oreilly.com/library/view/introduction-to-machine/9781449369880)
    由Andreas C. Müller 和 Sarah Guido（O’Reilly）'
- en: '[*Building Machine Learning Powered Applications*](https://www.oreilly.com/library/view/building-machine-learning/9781492045106)
    by Emmanuel Ameisen (O’Reilly)'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*构建机器学习驱动的应用*](https://www.oreilly.com/library/view/building-machine-learning/9781492045106)
    由Emmanuel Ameisen（O’Reilly）'
- en: I also recommend bookmarking this chapter and coming back here anytime you need
    a refresher on ML fundamentals.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我还建议您收藏本章，并在需要时随时回来温习机器学习基础知识。
