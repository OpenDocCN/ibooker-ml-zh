- en: Chapter 15\. Bias in Recommendation Systems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第15章 偏见在推荐系统中的体现
- en: 'We’ve spent much time in this book dissecting how to improve our recommendations,
    making them more personalized and relevant to an individual user. Along the way,
    you’ve learned that latent relationships between users and user personas encode
    important information about shared preferences. Unfortunately, all of this has
    a serious downside: bias.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们花了很多时间剖析如何改进我们的推荐，使它们更个性化、更相关于单个用户。在这个过程中，你已经了解到用户和用户人物之间的潜在关系编码了关于共享偏好的重要信息。不幸的是，所有这些都有一个严重的缺点：偏见。
- en: 'For the purposes of our discussion, we’ll talk about the two most important
    kinds of bias for recommendation systems:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们讨论的目的，我们将谈论推荐系统中两种最重要的偏见：
- en: Overly redundant or self-similar sets of recommendations
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过于冗余或自相似的推荐集
- en: Stereotypes learned by AI systems
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI系统学到的刻板印象
- en: First, we’ll delve into the crucial element of diversity in recommendation outputs.
    As critical as it is for a recommendation system to offer relevant choices to
    users, ensuring a variety of recommendations is also essential. Diversity not
    only safeguards against overspecialization but also promotes novel and serendipitous
    discoveries, enriching the overall user experience.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将深入探讨推荐输出中多样性的关键要素。尽管推荐系统为用户提供相关的选择至关重要，但确保各种推荐也是至关重要的。多样性不仅防止了过度专业化，还促进了新颖和意外的发现，丰富了整体用户体验。
- en: The balance between relevance and diversity is delicate and can be tricky. This
    balance challenges the algorithm to go beyond merely echoing users’ past behavior
    and encourages an exploration of new territories, hopefully providing a more holistically
    positive experience with the content.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 相关性与多样性之间的平衡是微妙而棘手的。这种平衡挑战着算法不仅仅是简单地重复用户的过去行为，而且鼓励探索新的领域，希望提供更全面积极的内容体验。
- en: This kind of bias is primarily a technical challenge; how do we satisfy the
    multiobjectives of diverse recommendations and highly relevant ones?
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这种偏见主要是一个技术挑战；我们如何满足多样化推荐和高度相关推荐的多重目标？
- en: We’ll consider the intrinsic and extrinsic biases in recommendation systems
    as an often unintended yet significant consequence of both the underlying algorithms
    and the data they learn from. Systemic biases in data collection or algorithmic
    design can result in prejudiced outputs, leading to ethical and fairness issues.
    Moreover, they may create echo chambers or filter bubbles, curtailing users’ exposure
    to a broader range of content and inadvertently reinforcing preexisting beliefs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把推荐系统中的内在和外在偏见视为潜在的但常常不经意的重要后果，这是由基础算法和它们所学习的数据引起的。数据收集或算法设计中的系统性偏见可能导致有偏见的输出，从而引发道德和公平性问题。此外，它们可能会形成闭环或过滤泡沫，限制用户接触更广泛范围的内容，无意中加强现有的信念。
- en: At the end of this chapter, we will discuss the risks and provide resources
    to learn more about them. We are not experts in AI fairness and bias, but all
    ML practitioners should understand and seriously consider these topics. We aim
    to provide an introduction and signposts.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，我们将讨论这些风险，并提供更多学习资源。我们不是AI公平性和偏见方面的专家，但所有机器学习从业者都应该了解并认真考虑这些话题。我们的目标是提供一个介绍和指引。
- en: Diversification of Recommendations
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推荐多样化
- en: 'Our first investment into fighting bias is to explicitly target more diversity
    in our recommendation outputs. We’ll briefly cover two of the many goals you may
    pursue: intra-list diversity and serendipitous recommendations.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对抗偏见的第一个投资是明确地在我们的推荐输出中针对更多的多样性。我们将简要介绍您可能追求的许多目标中的两个：列表内多样性和意外推荐。
- en: '*Intra-list diversity* attempts to ensure that there are a variety of types
    of items within a single recommendation list. The idea is to minimize similarity
    between the recommended items to reduce overspecialization and encourage exploration.
    High intra-list diversity within a set of recommendations increases the user’s
    exposure to many items they may like; however, the recommendations for any particular
    interest will be shallower, reducing the recall.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表内多样性*试图确保在单个推荐列表中存在各种类型的项目。这个想法是尽量减少推荐项目之间的相似性，以减少过度专业化并鼓励探索。在一组推荐中的高列表内多样性增加了用户接触到许多他们可能喜欢的项目的机会；然而，对于任何特定的兴趣，推荐将会更浅，降低了召回率。'
- en: '*Serendipitous recommendations* are both surprising and interesting to the
    user. These are often items that the user might not have discovered independently
    or that are generally far less popular in the system. Serendipity can be introduced
    into the recommendation process by injecting nonobvious or unexpected choices—even
    if those have a relatively lower affinity score with the user—to improve overall
    serendipity. In an ideal world, these serendipitous choices are high affinity
    relative to other items of their popularity, so they’re the “best of the outside
    choices.”'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*意外推荐*对用户来说既惊喜又有趣。这些通常是用户可能独立发现或系统中普遍不太受欢迎的物品。通过注入非显而易见或意想不到的选择，即使这些选择与用户的亲和力得分相对较低，也可以在推荐过程中引入意外性，以提高整体的意外性。在理想情况下，这些意外选择相对于其流行度的其他物品具有较高的亲和力，因此它们是“外部选择中的精品”。'
- en: Improving Diversity
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提高多样性
- en: Now that we have our measures of diversity, we can explicitly attempt to improve
    them. Importantly, by adding diversity metrics as one of our objectives, we will
    potentially sacrifice performance on things like recall or NDCG. It can be useful
    to think of this as a Pareto problem, or to impose a lower bound on ranking metric
    performance that you’ll accept in pursuit of diversity.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了多样性的度量标准，我们可以明确地尝试去改善它们。重要的是，通过将多样性指标作为我们目标之一，我们可能会在诸如召回率或NDCG等方面牺牲性能。把这看作是一个帕累托问题或者在追求多样性时强加一个排名度量性能的下限可能是有用的。
- en: Note
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In a *Pareto problem*, you have two priorities that often trade off with each
    other. In many areas of ML, and more generally applied mathematics, certain outcomes
    have a natural tension. Diversity in recommendations is an important example of
    a Pareto problem in recommendation systems, but it’s not the only one. In [Chapter 14](ch14.html#HardRanking),
    you briefly saw global optimization, which is an extreme case of trade-offs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在*帕累托问题*中，你经常需要权衡两个优先级。在许多机器学习领域，以及更普遍的应用数学中，某些结果存在自然的紧张关系。在推荐系统中，推荐多样性是帕累托问题的一个重要例子，但这并不是唯一的情况。在[第14章](ch14.html#HardRanking)中，你简要了解了全局优化，这是权衡的一个极端案例。
- en: 'One simple approach to improve diversity metrics is *reranking*: a post-processing
    step in which the initially retrieved recommendation list is reordered to enhance
    diversity. Various algorithms for re-ranking consider not just the relevance scores
    but also the dissimilarity among the items in the recommendation list. Re-ranking
    is a strategy that can operationalize any external loss function, so using it
    for diversity is a straightforward approach.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 改进多样性度量的一个简单方法是*重新排序*：这是一个后处理步骤，其中最初检索到的推荐列表被重新排序以增强多样性。各种重新排序算法不仅考虑相关性分数，还考虑推荐列表中物品之间的不相似性。重新排序是一种可以操作任何外部损失函数的策略，因此将其用于多样性是一个直接的方法。
- en: Another strategy is to break out of the closed loop of recommendation feedback
    that we discussed in the section [“Propensity Weighting for Recommendation System
    Evaluation”](ch10.html#propensity). As in multiarmed bandit problems, *explore-exploit
    trade-offs* can choose between exploiting what the model knows the user will like
    and exploring less certain options that may yield higher rewards. This trade-off
    can be used in recommendation systems to ensure diversity by occasionally choosing
    to *explore* and recommend less obvious choices. To implement a system like this,
    we can use affinity as a reward estimate and propensity as an exploitation measure.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种策略是打破我们在[“推荐系统评估的倾向性加权”](ch10.html#propensity)部分讨论的推荐反馈的封闭循环。就像多臂老虎机问题一样，*探索-利用的权衡*可以在选择利用模型知道用户喜欢的内容和探索不太确定但可能获得更高回报的选项之间进行选择。通过偶尔选择*探索*并推荐不太明显的选择，可以在推荐系统中使用这种权衡来确保多样性。为了实现这样的系统，我们可以使用亲和力作为奖励估计值，使用倾向性作为利用度量。
- en: Instead of using these posterior strategies, an alternative is to *incorporate
    diversity as an objective in the learning process* or include a diversity regularization
    term in the loss function. Multiobjective loss including pairwise similarity as
    a regularizer can help train the model to learn diverse sets of recommendations.
    You previously saw that kinds of regularization can coach the training process
    to minimize certain behaviors. One regularization term that can be used explicitly
    is *similarity among recommendations*; the dot product of each embedding vector
    in the recommendations to each other can approximate this self-similarity. Let
    <math alttext="script upper R equals left-parenthesis upper R 1 comma upper R
    2 comma ellipsis comma upper R Subscript k Baseline right-parenthesis"><mrow><mi>ℛ</mi>
    <mo>=</mo> <mo>(</mo> <msub><mi>R</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>R</mi>
    <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>R</mi> <mi>k</mi></msub>
    <mo>)</mo></mrow></math> be the list of embeddings for the recommendations, and
    then consider <math alttext="script upper R"><mi>ℛ</mi></math> as a column matrix—with
    each row a recommendation. Calculating <math alttext="script upper R"><mi>ℛ</mi></math>
    ’s Gramian would yield all our dot-product similarity calculations, and thus we
    can regularize by this term with appropriate hyperparameter weighting. Note that
    this differs from our previous Gramian regularization because we’re considering
    the recommendations for only an individual query in this case.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是使用这些后验策略，一个替代方法是*将多样性作为学习过程中的一个目标*或在损失函数中包含一个多样性正则化项。包括成对相似性的多目标损失可以帮助模型学习多样化的推荐集合。您之前看到过，各种正则化可以指导训练过程以最小化某些行为。一个可以显式使用的正则化项是*推荐间的相似性*；推荐中每个嵌入向量的点积可以近似表示这种自相似性。让
    <math alttext="script upper R equals left-parenthesis upper R 1 comma upper R
    2 comma ellipsis comma upper R Subscript k Baseline right-parenthesis"><mrow><mi>ℛ</mi>
    <mo>=</mo> <mo>(</mo> <msub><mi>R</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>R</mi>
    <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>R</mi> <mi>k</mi></msub>
    <mo>)</mo></mrow></math> 成为推荐的嵌入列表，然后将 <math alttext="script upper R"><mi>ℛ</mi></math>
    视为一个列矩阵——其中每行都是一个推荐。计算 <math alttext="script upper R"><mi>ℛ</mi></math> 的格拉姆矩阵将产生所有点积相似性计算，因此我们可以通过适当的超参数权重来通过这个项进行正则化。请注意，这与我们先前的格拉姆矩阵正则化不同，因为这种情况下我们只考虑个别查询的推荐。
- en: Finally, we can use rankings from multiple domains to boost recommendation diversity.
    By integrating various ranking measures, the recommendation system can suggest
    items from outside the user’s “mode,” thus broadening the range of recommendations.
    A vibrant discipline exists around multimodal recommendations, with the [PinnerSage
    paper](https://oreil.ly/KOQK2) from Pinterest a particularly impressive implementation.
    In many of the works about multimodal recommendations, the retrieval step returns
    too many recommendations near to the user’s query vector. This forces self-similarity
    among the retrieved list. Multimodality forces multiple query vectors to be used
    for each request, allowing a built-in diversity.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以利用多个领域的排名来提高推荐的多样性。通过整合各种排名措施，推荐系统可以建议用户“模式”之外的项目，从而扩展推荐范围。围绕多模态推荐存在着活跃的学科，Pinterest
    的 [PinnerSage 论文](https://oreil.ly/KOQK2) 就是一个特别引人注目的实现。在许多关于多模态推荐的作品中，检索步骤返回的推荐列表中有太多接近用户查询向量的推荐。这强制了检索列表中的自相似性。多模态强制使用多个查询向量来处理每个请求，从而实现内置的多样性。
- en: Let’s look at another perspective on item self-similarity and think about how
    the pairwise relationships between items can be used to this end.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从另一个角度看待项目的自相似性，并考虑如何利用项目之间的成对关系来实现这一目标。
- en: Applying Portfolio Optimization
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用投资组合优化
- en: '*Portfolio optimization*, a concept borrowed from finance, can be an effective
    approach to enhance diversity in recommendation systems. The goal here is to create
    a “portfolio” of recommended items that balances the two key parameters: relevance
    and diversity.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*投资组合优化*，这是从金融中借鉴的概念，可以是增强推荐系统多样性的有效方法。这里的目标是创建一个平衡关键参数（相关性和多样性）的“投资组合”推荐项目列表。'
- en: 'At its heart, portfolio optimization is about risk (in our case, relevance)
    and return (diversity). Here’s a basic approach for applying this optimization
    to recommendation systems:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，投资组合优化关乎风险（在我们的案例中是相关性）和回报（多样性）。以下是将此优化应用于推荐系统的基本方法：
- en: Formulate an item representation such that the distance in the space is a good
    measure of similarity. This is in line with our previous discussions about what
    makes a good latent space.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 制定一个项目表示，以便空间中的距离是相似性的良好度量。这与我们先前讨论过的构建良好潜在空间的理念一致。
- en: Calculate pairwise distance between items. You can do this by using whatever
    distance metric that enriches your latent space. It is important to calculate
    these pairwise distances across all items retrieved and be ready for consideration
    to return. Note that how you aggregate these distributions of distances can be
    nuanced.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算项目之间的成对距离。您可以通过使用丰富您的潜在空间的任何距离度量来完成此操作。重要的是要计算检索到的所有项目之间的这些成对距离，并准备考虑回报。请注意，如何聚合这些距离分布可能是微妙的。
- en: Evaluate affinity for the retrieved set. Note that calibrated affinity scores
    will perform better as they provide a more realistic estimate of return.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估检索集的亲和力。请注意，校准后的亲和力分数表现更好，因为它们提供了对回报的更现实的估计。
- en: 'Solve the optimization problem. Solving the problem will yield a weight for
    each item that balances the trade-off between relevance and diversity. Items with
    higher weights are more valuable in terms of both diversity and relevance, and
    they should be prioritized in the recommendation list. Mathematically, the problem
    looks like this:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解决优化问题。解决问题将为每个项目产生一个权重，平衡关联性和多样性之间的权衡。具有更高权重的项目在关联性和多样性方面更有价值，应优先考虑放在推荐列表中。从数学上讲，问题看起来是这样的：
- en: <math alttext="less-than u l c l a s s equals quotation-mark s i m p l e l i
    s t quotation-mark greater-than less-than l i greater-than upper M a x i m i z
    e left-parenthesis w Superscript upper T Baseline asterisk r minus lamda asterisk
    w Superscript upper T Baseline asterisk upper C asterisk w right-parenthesis less-than
    slash l i greater-than less-than slash u l greater-than" display="block"><mrow><mi>M</mi>
    <mi>a</mi> <mi>x</mi> <mi>i</mi> <mi>m</mi> <mi>i</mi> <mi>z</mi> <mi>e</mi> <mo>(</mo>
    <msup><mi>w</mi> <mi>T</mi></msup> <mo>*</mo> <mi>r</mi> <mo>-</mo> <mi>λ</mi>
    <mo>*</mo> <msup><mi>w</mi> <mi>T</mi></msup> <mo>*</mo> <mi>C</mi> <mo>*</mo>
    <mi>w</mi> <mo>)</mo></mrow></math>
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <math alttext="less-than u l c l a s s equals quotation-mark s i m p l e l i
    s t quotation-mark greater-than less-than l i greater-than upper M a x i m i z
    e left-parenthesis w Superscript upper T Baseline asterisk r minus lamda asterisk
    w Superscript upper T Baseline asterisk upper C asterisk w right-parenthesis less-than
    slash l i greater-than less-than slash u l greater-than" display="block"><mrow><mi>M</mi>
    <mi>a</mi> <mi>x</mi> <mi>i</mi> <mi>m</mi> <mi>i</mi> <mi>z</mi> <mi>e</mi> <mo>(</mo>
    <msup><mi>w</mi> <mi>T</mi></msup> <mo>*</mo> <mi>r</mi> <mo>-</mo> <mi>λ</mi>
    <mo>*</mo> <msup><mi>w</mi> <mi>T</mi></msup> <mo>*</mo> <mi>C</mi> <mo>*</mo>
    <mi>w</mi> <mo>)</mo></mrow></math>
- en: Here, <math alttext="w"><mi>w</mi></math> is a vector representing the weights
    (i.e., the proportion of each item in the recommendation list), <math alttext="r"><mi>r</mi></math>
    is the relevance score vector, <math alttext="upper C"><mi>C</mi></math> is the
    covariance matrix (which captures the diversity), and <math alttext="lamda"><mi>λ</mi></math>
    is a parameter to balance relevance and diversity. The constraint here is that
    the sum of the weights equals 1.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，<math alttext="w"><mi>w</mi></math>是表示权重的向量（即推荐列表中每个项目的比例），<math alttext="r"><mi>r</mi></math>是关联性分数向量，<math
    alttext="upper C"><mi>C</mi></math>是协方差矩阵（捕获多样性），<math alttext="lamda"><mi>λ</mi></math>是平衡关联性和多样性的参数。约束条件是权重的总和等于1。
- en: Remember, the hyperparameter <math alttext="lamda"><mi>λ</mi></math> trades
    off between relevance and diversity. This makes it a critical part of this process
    and may require experimentation or tuning based on the specific needs of your
    system and its users. This would be straightforward via hyperparameter optimization
    in one of many packages such as Weights & Biases.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 记住，超参数<math alttext="lamda"><mi>λ</mi></math>在关联性和多样性之间进行权衡。这使其成为该过程的关键部分，并可能根据系统及其用户的具体需求进行实验或调整。这可以通过诸如Weights
    & Biases等许多包的超参数优化来直接进行。
- en: Multiobjective Functions
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多目标函数
- en: Another related approach to diversity is to rank based on a multiobjective loss.
    Instead of the ranking stage being purely personalization affinity, introducing
    a second (or more!) ranking term can dramatically improve diversity.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 多样性的另一个相关方法是基于多目标损失进行排名。与排名阶段纯粹的个性化亲和力不同，引入第二（或更多！）排名项可以显著提高多样性。
- en: 'The simplest approach here is something similar to what you learned in [Chapter 14](ch14.html#HardRanking):
    hard ranking. A business rule that may apply to diversity is limiting each item
    category to only one item. This is the simplest case of multiobjective ranking
    because sorting by a categorical column and selecting the max in each group will
    achieve explicit diversity with respect to that covariate. Let’s move on to something
    more subtle.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这里最简单的方法类似于您在[第 14 章](ch14.html#HardRanking)学到的：硬排名。可能适用于多样性的业务规则是将每个项目类别限制为仅一个项目。这是多目标排名的最简单情况，因为按照分类列排序并选择每组中的最大值将实现相对于该协变量的显式多样性。让我们转向更微妙的内容。
- en: In [“Stitching Together Spaces for Query-Based Recommendations”](https://oreil.ly/OREt2),
    one of this book’s authors worked with coauthor Ian Horn to implement a multiobjective
    recommendation system that balanced both personalization and relevance to an image-retrieval
    problem.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在[“为基于查询的推荐拼接空间”](https://oreil.ly/OREt2)中，本书的一位作者与共同作者Ian Horn合作实现了一个多目标推荐系统，该系统在解决图像检索问题时平衡了个性化和关联性。
- en: 'The goal was to provide personalized recommendations for clothing that were
    similar to clothes in an image the user uploaded. This means there are two latent
    spaces:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是为用户上传的图像中与衣物相似的服装提供个性化推荐。这意味着存在两个潜在空间：
- en: The latent space of personalized clothes to a user
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 个性化服装到用户的潜在空间
- en: The latent space of images of clothing
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服装图像的潜在空间
- en: 'To solve this problem, we first had to make a decision: what was more important
    for relevance? Personalization or image similarity? Because the product was centered
    around a photo-upload experience, we chose image similarity. However, we had another
    fact to consider: each uploaded image contained several pieces of clothing. As
    is popular in computer vision, we first segmented the model into several items
    and then treated each item as its own query (which we called *anchor-items*).
    This meant our image-similarity retrieval was multimodal, as we searched with
    several different query vectors. After we gathered them all, we had to make one
    final ranking—a multiobjective ranking for image similarity and personalization.
    The loss function we optimized is shown here:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决这个问题，我们首先需要做出一个决定：在相关性方面，什么更重要？个性化还是图像相似性？因为产品围绕着照片上传体验，我们选择了图像相似性。然而，我们还需要考虑另一个事实：每个上传的图像包含多件服装。正如在计算机视觉中流行的那样，我们首先将模型分割成几个单独的项目，然后将每个项目视为其自身的查询（我们称之为*锚点项目*）。这意味着我们的图像相似性检索是多模态的，因为我们使用了几个不同的查询向量进行搜索。在我们收集完所有这些数据后，我们需要进行最终排名——一个关于图像相似性和个性化的多目标排名。我们优化的损失函数如下所示：
- en: <math alttext="s Subscript i Baseline equals alpha times left-parenthesis 1
    minus d Subscript i Baseline right-parenthesis plus left-parenthesis 1 minus alpha
    right-parenthesis times a Subscript i" display="block"><mrow><msub><mi>s</mi>
    <mi>i</mi></msub> <mo>=</mo> <mi>α</mi> <mo>×</mo> <mrow><mo>(</mo> <mn>1</mn>
    <mo>-</mo> <msub><mi>d</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>+</mo> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <mi>α</mi> <mo>)</mo></mrow> <mo>×</mo> <msub><mi>a</mi>
    <mi>i</mi></msub></mrow></math>
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="s Subscript i Baseline equals alpha times left-parenthesis 1
    minus d Subscript i Baseline right-parenthesis plus left-parenthesis 1 minus alpha
    right-parenthesis times a Subscript i" display="block"><mrow><msub><mi>s</mi>
    <mi>i</mi></msub> <mo>=</mo> <mi>α</mi> <mo>×</mo> <mrow><mo>(</mo> <mn>1</mn>
    <mo>-</mo> <msub><mi>d</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>+</mo> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <mi>α</mi> <mo>)</mo></mrow> <mo>×</mo> <msub><mi>a</mi>
    <mi>i</mi></msub></mrow></math>
- en: The <math alttext="alpha"><mi>α</mi></math> is a hyperparameter that represents
    the weighting, <math alttext="d Subscript i"><msub><mi>d</mi> <mi>i</mi></msub></math>
    is the image distance, and <math alttext="a Subscript i"><msub><mi>a</mi> <mi>i</mi></msub></math>
    is the personalization. We learn α experimentally. The last step was to impose
    some hard ranking to ensure that one recommendation came from each anchor.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="alpha"><mi>α</mi></math> 是一个超参数，表示权重，<math alttext="d Subscript
    i"><msub><mi>d</mi> <mi>i</mi></msub></math> 是图像距离，而 <math alttext="a Subscript
    i"><msub><mi>a</mi> <mi>i</mi></msub></math> 是个性化的参数。我们通过实验来学习 α。最后一步是施加一些严格的排名规则，以确保每个推荐来自每个锚点。
- en: 'So let’s sum this up:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们总结一下：
- en: We used two latent spaces with distances to provide rankings.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用两个潜在空间的距离来提供排名。
- en: We did multimodal retrieval via image segmentation.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过图像分割进行了多模态检索。
- en: We retrieved using only one of the rankings.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们只使用了其中一个排名来检索。
- en: Our final ranking was multiobjective, with hard ranking utilizing all our latent
    spaces and business logic.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的最终排名是多目标的，利用了所有我们的潜在空间和业务逻辑。
- en: This allowed our recommendations to be *diverse* in the sense that they achieved
    relevance in several areas of the query that corresponded to different items.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得我们的推荐在一定程度上是*多样化*的，因为它们在与不同项目对应的查询的几个领域中实现了相关性。
- en: Predicate Pushdown
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谓词下推
- en: You may be happy and comfortable applying these metrics during serving—after
    all, that’s the title for this part of the book—but before we move on from this
    topic, we should talk about an edge case that can have quite disastrous consequences.
    When you impose the hard rules from [Chapter 14](ch14.html#HardRanking) and the
    diversity expectations discussed earlier in this chapter, and do a little multiobjective
    ranking, sometimes you arrive at…​no recommendations.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在服务期间，您可能会很高兴并且舒适地应用这些指标——毕竟，这是本书的这一部分的标题——但在我们离开这个话题之前，我们应该讨论一个可能会带来严重后果的边缘情况。当您从[第14章](ch14.html#HardRanking)中强制施加严格规则以及本章早些时候讨论的多样性期望，并进行一些多目标排名时，有时您会得出……没有推荐。
- en: 'Say you start by retrieving *k* items, but after the sufficiently diverse combinations
    that also satisfy business rules, there’s simply nothing left. You might say,
    “I’ll just retrieve more items; let’s crank up *k*!” But this has some serious
    issues: it can really increase latency, depress match quality, and throw off your
    ranking model that is more tuned to lower-cardinality sets.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你开始通过检索 *k* 个项目，但在足够多的满足业务规则的多样化组合之后，实际上没有任何剩余项目了。你可能会说：“我只是多检索一些项目；让我们增加
    *k*！”但这会带来一些严重问题：它可能会显著增加延迟，降低匹配质量，并扰乱你的排名模型，该模型更适合较低基数集。
- en: 'A common experience, especially with diversity, is that different modes for
    the retrieval have vastly different match scores. To take an example from our
    fashion recommender world: all jeans might be a better match than any shirt we
    have, but if you’re looking for diverse categories of clothes to recommend, no
    matter how big the *k*, you’ll potentially be missing out on shirts.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的经验，特别是在多样性方面，是检索的不同模式具有极大不同的匹配分数。举一个我们时尚推荐器世界的例子：所有牛仔裤可能比我们拥有的任何衬衫都更匹配，但如果您正在寻找多样化的服装类别进行推荐，无论
    *k* 有多大，您可能会错过衬衫。
- en: One solution to this problem is *predicate pushdown*. This optimization technique
    is used in databases, specifically in the context of data retrieval. The main
    idea of predicate pushdown is to filter data as early as possible in the data-retrieval
    process, to reduce the amount of data that needs to be processed later in the
    query execution plan.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的一个解决方案是*谓词下推*。这种优化技术用于数据库，特别是在数据检索的上下文中。谓词下推的主要思想是尽早在数据检索过程中进行数据过滤，以减少后续查询执行计划中需要处理的数据量。
- en: For traditional databases, you see predicate pushdown applied, for example,
    as “apply my query’s `where` clause in the database to cut down on I/O.” It may
    achieve this by explicitly pulling the relevant columns to check the `where` clause
    first, and then getting the row IDs from those that pass before executing the
    rest of the query.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于传统数据库，例如“将我的查询的`where`子句应用于数据库以减少I/O。”，谓词下推可以通过显式地首先拉取相关列以检查`where`子句，然后获取通过的行ID，再执行其余查询，来实现此目的。
- en: 'How does this help us in our case? The simple idea is if your vector store
    also has features for the vectors, you can include the feature comparisons as
    part of retrieval. Let’s take an overly simple example: assume your items have
    a categorical feature called `color`, and for good diverse recommendations you
    want a nice set of at least three colors in your five recommendations. To achieve
    this, you can do a top-*k* search across each of the colors in your store (the
    downside is that your retrieval is *C* times as large, where *C* is the number
    of colors that exist) and then do ranking and diversity on the union of these
    sets. This has a much higher likelihood of surviving your diversity rule in the
    eventual recommendations. This is great! We expect that latency is relatively
    low in retrieval, so this tax of extra retrievals isn’t bad if we know where to
    look.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这如何帮助我们的案例呢？简单来说，如果你的向量存储还具有向量的特征，你可以将特征比较作为检索的一部分。让我们举一个过于简单的例子：假设你的项目有一个名为`color`的分类特征，为了获得良好的多样化推荐，你希望在你的五个推荐中有至少三种颜色的好组合。为了实现这一点，你可以在你的存储中的每种颜色上进行top-*k*搜索（缺点是你的检索增加了*C*倍，其中*C*是存在的颜色数量），然后在这些集合的并集上进行排名和多样性评估。这有很高的可能性能够在最终推荐中符合你的多样性规则。这是很棒的！我们期望检索的延迟相对较低，因此如果我们知道在哪里查找，这种额外的检索负担并不糟糕。
- en: This optimization technique can be applied on quite complicated predicates if
    your vector store is set up well for the kinds of filters you wish to impose.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的向量存储为所需的过滤器设置得当，这种优化技术可以应用于相当复杂的谓词。
- en: Fairness
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 公平性
- en: 'Fairness in ML in general is a particularly nuanced subject that is ill-served
    by short summaries. The following topics are important, and we invite you to consider
    the robust references included here:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，机器学习中的公平性是一个非常微妙的主题，短小的摘要往往难以服务。以下主题至关重要，我们建议您考虑这里包含的强大参考资料：
- en: Nudging
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 推动
- en: Fairness does not need to be only “equal probabilities for all outcomes”; it
    can be fair with respect to a specific covariate. Nudging via a recommender—i.e.,
    recommending items to emphasize certain behavior or buying patterns—can increase
    fairness. Consider the work by Karlijn Dinnissen and Christine Bauer from Spotify
    about [using nudging to improve gender representation in music recommendations](https://oreil.ly/fit3j).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 公平性不需要仅仅是“所有结果的等概率”，它可以在特定协变量的背景下公平。通过推荐者进行推动，即推荐项目以强调某些行为或购买模式，可以增加公平性。考虑Spotify的Karlijn
    Dinnissen和Christine Bauer关于使用推动来改善音乐推荐中性别表示的工作。
- en: Filter bubbles
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 滤泡效应
- en: 'Filter bubbles are a downside of extreme collaborative filtering: a group of
    users begin liking similar recommendations, the system learns that they should
    receive similar recommendations, and the feedback loop perpetuates this. For a
    deep look into not only the concept but also mitigation strategies, consider [“Mitigating
    the Filter Bubble While Maintaining Relevance”](https://oreil.ly/2jyeJ) by Zhaolin
    Gao et al.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 滤泡效应是极端协同过滤的一个不利方面：一组用户开始喜欢类似的推荐，系统学习到他们应该接收类似的推荐，这种反馈循环会持续下去。要深入了解这一概念及缓解策略，可以参考Zhaolin
    Gao等人的《“缓解过滤泡效应同时保持相关性”》。
- en: High risk
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 高风险
- en: Not all applications of AI are equal in risk. Some domains are particularly
    harmful when AI systems are poorly guardrailed. For a general overview of the
    most high-risk circumstances and mitigation, consult [*Machine Learning for High-Risk
    Applications*](https://learning.oreilly.com/library/view/machine-learning-for/9781098102425/)
    by Patrick Hall et al. (O’Reilly).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有的AI应用在风险上都是相等的。一些领域在AI系统的保护不力时尤其有害。有关最高风险情况和缓解措施的一般概述，请参阅Patrick Hall等人(O’Reilly)的[*Machine
    Learning for High-Risk Applications*](https://learning.oreilly.com/library/view/machine-learning-for/9781098102425/)。
- en: Trustworthiness
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 可信度
- en: Explainable models is a popular mitigation strategy for risky applications of
    AI. While explainability does not *solve* the problem, it frequently provides
    a path toward identification and resolution. For a deep dive on this, [*Practicing
    Trustworthy Machine Learning*](https://learning.oreilly.com/library/view/practicing-trustworthy-machine/9781098120269/)
    by Yada Pruksachatkun et al. (O’Reilly) provides tools and techniques.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性模型是AI风险应用的一种流行的缓解策略。虽然可解释性并不能*解决*问题，但它经常提供了一条向识别和解决问题的路径。关于这一点的深入探讨，Yada
    Pruksachatkun等人(O’Reilly)的[*Practicing Trustworthy Machine Learning*](https://learning.oreilly.com/library/view/practicing-trustworthy-machine/9781098120269/)提供了工具和技术。
- en: Fairness in recommendations
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐中的公平性
- en: Because recommendation systems are so obviously susceptible to issues of AI
    fairness, much has been written on the topic. Each of the major social media giants
    has employed teams working in AI safety. One particular highlight is the Twitter
    Responsible AI team led by Rumman Chowdhury. You can read about the team’s work
    in [“Can Auditing Eliminate Bias from Algorithms?”](https://oreil.ly/uvFep) by
    Alfred Ng.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 由于推荐系统显然容易受到人工智能公平性问题的影响，关于这个主题已经有很多文章写作。每个主要社交媒体巨头都设有从事AI安全工作的团队。其中一个特别亮点是由Rumman
    Chowdhury领导的Twitter负责AI团队。您可以阅读Alfred Ng的文章["Can Auditing Eliminate Bias from
    Algorithms?"](https://oreil.ly/uvFep)了解团队的工作。
- en: Summary
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: While these techniques provide pathways to enhance diversity, it’s important
    to remember to strike a balance between diversity and relevance. The exact method
    or combination of methods used may vary depending on the specific use case, the
    available data, the intricacies of the user base, and the kind of feedback you’re
    collecting. As you implement recommendation systems, think about which aspects
    are the most key in your diversity problem.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些技术提供了增强多样性的途径，但重要的是要记住在多样性和相关性之间取得平衡。使用的确切方法或方法组合可能会因具体用例、可用数据、用户群体的复杂性以及收集反馈的类型而有所不同。在实施推荐系统时，请考虑哪些方面在解决多样性问题中最为关键。
