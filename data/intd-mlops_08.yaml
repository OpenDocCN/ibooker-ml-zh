- en: Chapter 6\. Deploying to Production
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第六章。部署到生产环境
- en: Joachim Zentici
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Joachim Zentici
- en: Business leaders view the rapid deployment of new systems into production as
    key to maximizing business value. But this is only true if deployment can be done
    smoothly and at low risk (software deployment processes have become more automated
    and rigorous in recent years to address this inherent conflict). This chapter
    dives into the concepts and considerations when deploying machine learning models
    to production that impact—and indeed, drive—the way MLOps deployment processes
    are built ([Figure 6-1](#deployment_to_production_highlighted_in) presents this
    phase in the context of the larger life cycle).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 业务领导者认为将新系统快速部署到生产环境是最大化业务价值的关键。但前提是部署必须顺利进行且风险低（近年来，为解决这一固有冲突，软件部署流程已变得更加自动化和严格）。本章深入探讨了将机器学习模型部署到生产环境时的概念和考虑因素，这些因素影响并驱动着MLOps部署流程的建立（[图6-1](#deployment_to_production_highlighted_in)在更大生命周期的背景下展示了此阶段）。
- en: '![](assets/imlo_0601.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/imlo_0601.png)'
- en: Figure 6-1\. Deployment to production highlighted in the larger context of the
    ML project life cycle
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1。在ML项目生命周期的更大背景下突出显示的生产部署
- en: CI/CD Pipelines
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CI/CD管道
- en: CI/CD is a common acronym for continuous integration and continuous delivery
    (or put more simply, deployment). The two form a modern philosophy of agile software
    development and a set of practices and tools to release applications more often
    and faster, while also better controlling quality and risk.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: CI/CD是连续集成和连续交付（或更简单地说，部署）的常见缩写。这两者构成了敏捷软件开发的现代理念，是一组实践和工具，能够更频繁、更快地发布应用程序，同时更好地控制质量和风险。
- en: While these ideas are decades old and already used to various extents by software
    engineers, different people and organizations use certain terms in very different
    ways. Before digging into how CI/CD applies to machine learning workflows, it
    is essential to keep in mind that these concepts should be tools to serve the
    purpose of delivering quality fast, and the first step is always to identify the
    specific risks present at the organization. In other words, as always, CI/CD methodology
    should be adapted based on the needs of the team and the nature of the business.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些思想已有几十年历史，并且已被软件工程师在各种程度上广泛使用，但不同的人和组织对某些术语的使用方式差异很大。在深入讨论CI/CD如何应用于机器学习工作流程之前，有一点很重要，那就是这些概念应该是服务于快速交付高质量的工具，并且第一步始终是识别组织中存在的具体风险。换句话说，CI/CD方法论应根据团队的需求和业务的性质进行调整。
- en: 'CI/CD concepts apply to traditional software engineering, but they apply just
    as well to machine learning systems and are a critical part of MLOps strategy.
    After successfully developing a model, a data scientist should push the code,
    metadata, and documentation to a central repository and trigger a CI/CD pipeline.
    An example of such pipeline could be:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: CI/CD的概念不仅适用于传统软件工程，而且同样适用于机器学习系统，并且是MLOps战略的关键部分。成功开发模型后，数据科学家应将代码、元数据和文档推送到中央存储库并触发CI/CD管道。这种管道的一个示例可能是：
- en: Build the model
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建模型
- en: Build the model artifacts
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建模型工件
- en: Send the artifacts to long-term storage
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将工件发送到长期存储
- en: Run basic checks (smoke tests/sanity checks)
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行基本检查（烟雾测试/合理性检查）
- en: Generate fairness and explainability reports
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成公平性和可解释性报告
- en: Deploy to a test environment
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署到测试环境
- en: Run tests to validate ML performance, computational performance
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行测试以验证ML性能、计算性能
- en: Validate manually
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 手动验证
- en: Deploy to production environment
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署到生产环境
- en: Deploy the model as canary
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型部署为金丝雀
- en: Fully deploy the model
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 全面部署模型
- en: 'Many scenarios are possible and depend on the application, the risks from which
    the system should be protected, and the way the organization chooses to operate.
    Generally speaking, an incremental approach to building a CI/CD pipeline is preferred:
    a simple or even naïve workflow on which a team can iterate is often much better
    than starting with complex infrastructure from scratch.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 可能存在多种场景，这些场景取决于应用程序、系统需要保护的风险以及组织选择运作的方式。一般来说，偏好于增量构建CI/CD管道的方法：一个团队可以迭代的简单甚至是天真的工作流程通常比从头开始构建复杂基础设施要好得多。
- en: A starting project does not have the infrastructure requirements of a tech giant,
    and it can be hard to know up front which challenges deployments will present.
    There are common tools and best practices, but there is no one-size-fits-all CI/CD
    methodology. This means the best path forward is starting from a simple (but fully
    functional) CI/CD workflow and introducing additional or more sophisticated steps
    along the way as quality or scaling challenges appear.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 起始项目没有技术巨头的基础设施要求，并且很难事先知道部署将提出哪些挑战。有一些常见的工具和最佳实践，但没有一种大小适合所有的CI/CD方法论。这意味着最佳路径是从一个简单（但完全功能的）CI/CD工作流开始，并在质量或扩展挑战出现时逐步引入额外或更复杂的步骤。
- en: Building ML Artifacts
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建ML构件
- en: The goal of a continuous integration pipeline is to avoid unnecessary effort
    in merging the work from several contributors as well as to detect bugs or development
    conflicts as soon as possible. The very first step is using centralized version
    control systems (unfortunately, working for weeks on code stored only on a laptop
    is still quite common).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 持续集成流水线的目标是避免合并来自多个贡献者的工作的不必要的工作量，尽早检测错误或开发冲突。第一步是使用集中式版本控制系统（不幸的是，在仅存储在笔记本电脑上的代码上工作几周仍然相当常见）。
- en: The most common version control system is Git, an open source software initially
    developed to manage the source code for the Linux kernel. The majority of software
    engineers across the world already use Git, and it is increasingly being adopted
    in scientific computing and data science. It allows for maintaining a clear history
    of changes, safe rollback to a previous version of the code, multiple contributors
    to work on their own branches of the project before merging to the main branch,
    etc.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的版本控制系统是Git，这是一个开源软件，最初开发用于管理Linux内核的源代码。全世界的大多数软件工程师已经在使用Git，并且它越来越多地被科学计算和数据科学所采用。它允许保持清晰的变更历史记录，安全地回滚到代码的先前版本，多个贡献者可以在他们自己的项目分支上工作，然后合并到主分支，等等。
- en: While Git is appropriate for code, it was not designed to store other types
    of assets common in data science workflows, such as large binary files (for example,
    trained model weights), or to version the data itself. Data versioning is a more
    complex topic with numerous solutions, including Git extensions, file formats,
    databases, etc.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Git适用于代码，但它并非设计用于存储数据科学工作流中常见的其他类型的资产，如大型二进制文件（例如，训练模型权重），或者对数据本身进行版本控制。数据版本控制是一个更复杂的话题，有许多解决方案，包括Git扩展、文件格式、数据库等。
- en: What’s in an ML Artifact?
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ML构件中包含什么？
- en: 'Once the code and data is in a centralized repository, a testable and deployable
    bundle of the project must be built. These bundles are usually called *artifacts*
    in the context of CI/CD. Each of the following elements needs to be bundled into
    an artifact that goes through a testing pipeline and is made available for deployment
    to production:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦代码和数据位于集中存储库中，项目必须构建一个可测试和可部署的包。在CI/CD的背景下，这些包通常称为*构件*。每个以下元素都需要打包成一个通过测试流水线的构件，并可供部署到生产环境使用：
- en: Code for the model and its preprocessing
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型及其预处理的代码
- en: Hyperparameters and configuration
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数和配置。
- en: Training and validation data
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和验证数据
- en: Trained model in its runnable form
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可运行形式的训练模型。
- en: An environment including libraries with specific versions, environment variables,
    etc.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包括具有特定版本、环境变量等的环境。
- en: Documentation
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档
- en: Code and data for testing scenarios
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于测试场景的代码和数据
- en: The Testing Pipeline
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试流水线
- en: As touched on in [Chapter 5](ch05.html#preparing_for_production), the testing
    pipeline can validate a wide variety of properties of the model contained in the
    artifact. One of the important operational aspects of testing is that, in addition
    to verifying compliance with requirements, good tests should make it as easy as
    possible to diagnose the source issue when they fail.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如在[第5章](ch05.html#preparing_for_production)中所述，测试流水线可以验证构件中包含的模型的各种属性。测试的一个重要操作方面是，在验证与需求的一致性之外，良好的测试应尽可能地简化故障排除时的问题源定位。
- en: 'For that purpose, naming the tests is extremely important, and carefully choosing
    a number of datasets to validate the model against can be valuable. For example:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，命名测试非常重要，并且精心选择一些数据集来验证模型的方法可能会很有价值。例如：
- en: A test on a fixed (not automatically updated) dataset with simple data and not-too-restrictive
    performance thresholds can be executed first and called “base case.” If the test
    reports show that this test failed, there is a strong possibility that the model
    is way off, and the cause may be a programming error or a misuse of the model,
    for example.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以首先执行一个固定数据集（不自动更新）、具有简单数据和不太严格的性能阈值的测试，并称之为“基准案例”。如果测试报告显示此测试失败，那么模型严重偏离的可能性很大，原因可能是编程错误或模型的误用等。
- en: Then, a number of datasets that each have one specific oddity (missing values,
    extreme values, etc.) could be used with tests appropriately named so that the
    test report immediately shows the kind of data that is likely to make the model
    fail. These datasets can represent realistic yet remarkable cases, but it may
    also be useful to generate synthetic data that is not expected in production.
    This could possibly protect the model from new situations not yet encountered,
    but most importantly, this could protect the model from malfunctions in the system
    querying or from adversarial examples (as discussed in [“Machine Learning Security”](ch05.html#machine_learning_security)).
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，可以使用多个具有特定异常（缺失值、极端值等）的数据集，并适当命名这些测试，以便测试报告立即显示可能使模型失败的数据类型。这些数据集可以代表现实但显著的案例，但生成预计不会在生产中出现的合成数据也可能很有用。这可能会保护模型免受尚未遇到的新情况的影响，但更重要的是，这可能会保护模型免受系统查询故障或对抗性示例（如在[“机器学习安全性”](ch05.html#machine_learning_security)中讨论的）。
- en: Then, an essential part of model validation is testing on recent production
    data. One or several datasets should be used, extracted from several time windows
    and named appropriately. This category of tests should be performed and automatically
    analyzed when the model is already deployed to production. [Chapter 7](ch07.html#monitoring_and_feedback_loop)
    provides more specific details on how to do that.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型验证的一个重要部分是在最新的生产数据上进行测试。应使用一个或多个数据集，从不同时间窗口提取并适当命名。这类测试应在模型已经部署到生产环境时进行并自动分析。[第7章](ch07.html#monitoring_and_feedback_loop)提供了更详细的关于如何执行的细节。
- en: Automating these tests as much as possible is essential and, indeed, is a key
    component of efficient MLOps. A lack of automation or speed wastes time, but,
    more importantly, it discourages the development team from testing and deploying
    often, which can delay the discovery of bugs or design choices that make it impossible
    to deploy to production.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 尽可能自动化这些测试至关重要，实际上是高效MLOps的关键组成部分。缺乏自动化或速度会浪费时间，但更重要的是，它会使开发团队不愿意经常进行测试和部署，这可能会延迟发现导致无法部署到生产的错误或设计选择。
- en: In extreme cases, a development team can hand over a monthslong project to a
    deployment team that will simply reject it because it does not satisfy requirements
    for the production infrastructure. Also, less frequent deployments imply larger
    increments that are harder to manage; when many changes are deployed at once and
    the system is not behaving in the desired way, isolating the origin of an issue
    is more time consuming.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在极端情况下，开发团队可以将一个长达数月的项目交给部署团队，但因为不满足生产基础设施的要求而被简单拒绝。此外，较少频繁的部署意味着较大的增量，这些增量更难以管理；当系统出现多个更改同时部署且表现不如预期时，问题来源的隔离将更加耗时。
- en: The most widespread tool for software engineering continuous integration is
    Jenkins, a very flexible build system that allows for the building of CI/CD pipelines
    regardless of the programming language, testing framework, etc. Jenkins can be
    used in data science to orchestrate CI/CD pipelines, although there are many other
    options.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 软件工程持续集成中最常见的工具是Jenkins，这是一个非常灵活的构建系统，允许构建无论使用哪种编程语言、测试框架等的CI/CD流水线。Jenkins可用于数据科学中的CI/CD流水线编排，尽管还有许多其他选择。
- en: Deployment Strategies
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署策略
- en: To understand the details of a deployment pipeline, it is important to distinguish
    among concepts often used inconsistently or interchangeably.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解部署流水线的细节，重要的是要区分那些经常不一致或可互换使用的概念。
- en: Integration
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 集成
- en: The process of merging a contribution to a central repository (typically merging
    a Git feature branch to the main branch) and performing more or less complex tests.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 合并贡献到中央仓库的过程（通常是将Git特性分支合并到主分支）并执行更多或更少复杂的测试。
- en: Delivery
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 交付
- en: As used in the continuous delivery (CD) part of CI/CD, the process of building
    a fully packaged and validated version of the model ready to be deployed to production.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在持续交付（CD）的部分中，构建完整打包和验证的模型版本，以便部署到生产环境。
- en: Deployment
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 部署
- en: The process of running a new model version on a target infrastructure. Fully
    automated deployment is not always practical or desirable and is a business decision
    as much as a technical decision, whereas continuous delivery is a tool for the
    development team to improve productivity and quality as well as measure progress
    more reliably. Continuous delivery is required for continuous deployment, but
    it also provides enormous value without.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在目标基础设施上运行新模型版本的过程。完全自动化部署并非始终切实可行或理想，这是一项商业决策，同样也是技术决策，而持续交付则是开发团队提高生产力和质量以及更可靠地衡量进展的工具。持续交付是持续部署所必需的，但即使在没有持续部署的情况下，它也能提供巨大的价值。
- en: Release
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 发布
- en: In principle, release is yet another step, as deploying a model version (even
    to the production infrastructure) does not necessarily mean that the production
    workload is directed to the new version. As we will see, multiple versions of
    a model can run at the same time on the production infrastructure.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 原则上，发布是另一个步骤，因为部署模型版本（即使是到生产基础设施）并不一定意味着生产工作负载会指向新版本。正如我们将看到的那样，多个模型版本可以同时在生产基础设施上运行。
- en: Getting everyone in the MLOps process on the same page about what these concepts
    mean and how they apply will allow for smoother processes on both the technical
    and business sides.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 确保 MLOps 过程中每个人对这些概念的理解达成一致，并了解其应用方式，将有助于技术和业务两方面的流程更加顺畅。
- en: Categories of Model Deployment
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型部署的分类
- en: 'In addition to different deployment strategies, there are two ways to approach
    model deployment:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 除了不同的部署策略外，还有两种方法可以进行模型部署：
- en: Batch scoring, where whole datasets are processed using a model, such as in
    daily scheduled jobs.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量评分，即使用模型处理整个数据集，例如每日定期作业。
- en: Real-time scoring, where one or a small number of records are scored, such as
    when an ad is displayed on a website and a user session is scored by models to
    decide what to display.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时评分，其中一个或少数记录得分，例如在网站上显示广告并且模型根据用户会话决定显示什么。
- en: There is a continuum between these two approaches, and in fact, in some systems,
    scoring on one record is technically identical to requesting a batch of one. In
    both cases, multiple instances of the model can be deployed to increase throughput
    and potentially lower latency.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种方法之间存在一种连续性，实际上，在某些系统中，对一个记录进行评分在技术上与请求一个批次的一样。在这两种情况下，可以部署多个模型实例以增加吞吐量并可能降低延迟。
- en: Deploying many real-time scoring systems is conceptually simpler since the records
    to be scored can be dispatched between several machines (e.g., using a load balancer).
    Batch scoring can also be parallelized, for example by using a parallel processing
    runtime like Apache Spark, but also by splitting datasets (which is usually called
    *partitioning* or *sharding*) and scoring the partitions independently. Note that
    these two concepts of splitting the data and computation can be combined, as they
    can address different problems.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 部署许多实时评分系统在概念上更简单，因为要得分的记录可以分配到多台机器之间（例如使用负载均衡器）。批量评分也可以并行化，例如使用像 Apache Spark
    这样的并行处理运行时，还可以通过分割数据集（通常称为*分区*或*分片*）并独立对分区进行评分。请注意，这两个数据和计算分割的概念可以结合起来，因为它们可以解决不同的问题。
- en: Considerations When Sending Models to Production
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将模型发送到生产时的考虑事项
- en: When sending a new model version to production, the first consideration is often
    to avoid downtime, in particular for real-time scoring. The basic idea is that
    rather than shutting down the system, upgrading it, and then putting it back online,
    a new system can be set up next to the stable one, and when it’s functional, the
    workload can be directed to the newly deployed version (and if it remains healthy,
    the old one is shut down). This deployment strategy is called *blue-green*—or
    sometimes *red-black*—deployment. There are many variations and frameworks (like
    Kubernetes) to handle this natively.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在将新模型版本发送到生产时，首要考虑通常是避免停机时间，特别是对于实时评分。基本思想是，与其关闭系统、升级它，然后重新上线，不如在稳定系统旁边设置一个新系统，在其功能正常后将工作负载指向新部署的版本（如果保持健康，则关闭旧版本）。这种部署策略称为*蓝绿部署*，有时也称为*红黑部署*。有许多变体和框架（如
    Kubernetes）可以本地处理这些。
- en: Another more advanced solution to mitigate the risk is to have canary releases
    (also called *canary deployments*). The idea is that the stable version of the
    model is kept in production, but a certain percentage of the workload is redirected
    to the new model, and results are monitored. This strategy is usually implemented
    for real-time scoring, but a version of it could also be considered for batch.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个更高级的减轻风险的解决方案是拥有金丝雀发布（也称为*金丝雀部署*）。其想法是将模型的稳定版本保留在生产中，但将一定比例的工作负载重定向到新模型，并监控结果。这种策略通常用于实时评分，但也可以考虑用于批处理。
- en: A number of computational performance and statistical tests can be performed
    to decide whether to fully switch to the new model, potentially in several workload
    percentage increments. This way, a malfunction would likely impact only a small
    portion of the workload.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 可以执行一些计算性能和统计测试来决定是否完全切换到新模型，可能分几个工作负载百分比递增。这样，故障可能只会影响到工作负载的一小部分。
- en: Canary releases apply to production systems, so any malfunction is an incident,
    but the idea here is to limit the blast radius. Note that scoring queries that
    are handled by the canary model should be carefully picked, because some issues
    may go unnoticed otherwise. For example, if the canary model is serving a small
    percentage of a region or country before the model is fully released globally,
    it could be the case that (for machine learning or infrastructure reasons) the
    model does not perform as expected in other regions.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 金丝雀发布适用于生产系统，因此任何故障都是事故，但这里的想法是限制影响范围。请注意，由金丝雀模型处理的评分查询应该谨慎选择，因为否则可能会忽略一些问题。例如，如果在模型全球完全发布之前，金丝雀模型只服务于某个地区或国家的一小部分用户，这可能是因为（出于机器学习或基础设施原因）该模型在其他地区的表现不如预期。
- en: A more robust approach is to pick the portion of users served by the new model
    at random, but then it is often desirable for user experience to implement an
    affinity mechanism so that the same user always uses the same version of the model.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更加稳健的方法是随机选择服务于新模型的用户部分，但通常为了用户体验，实现一个亲和机制是有必要的，这样同一用户始终使用相同版本的模型。
- en: Canary testing can be used to carry out A/B testing, which is a process to compare
    two versions of an application in terms of a business performance metric. The
    two concepts are related but not the same, as they don’t operate at the same level
    of abstraction. A/B testing can be made possible through a canary release, but
    it could also be implemented as logic directly coded into a single version of
    an application. [Chapter 7](ch07.html#monitoring_and_feedback_loop) provides more
    details on the statistical aspects of setting up A/B testing.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 金丝雀测试可用于进行A/B测试，这是一个比较应用程序两个版本的业务性能指标的过程。这两个概念是相关的，但并不相同，因为它们不在同一抽象级别上运行。A/B测试可以通过金丝雀发布来实现，但也可以直接编码为应用程序的单个版本中的逻辑。[第7章](ch07.html#monitoring_and_feedback_loop)提供了有关建立A/B测试的统计方面更多细节。
- en: Overall, canary releases are a powerful tool, but they require somewhat advanced
    tooling to manage the deployment, gather the metrics, specify and run computations
    on them, display the results, and dispatch and process alerts.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，金丝雀发布是一个强大的工具，但需要一定的先进工具来管理部署、收集指标、指定和运行计算、显示结果以及分发和处理警报。
- en: Maintenance in Production
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生产环境下的维护
- en: 'Once a model is released, it must be maintained. At a high level, there are
    three maintenance measures:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型发布，就必须进行维护。在高层面上，有三项维护措施：
- en: Resource monitoring
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 资源监控
- en: Just as for any application running on a server, collecting IT metrics such
    as CPU, memory, disk, or network usage can be useful to detect and troubleshoot
    issues.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 与在服务器上运行的任何应用程序一样，收集CPU、内存、磁盘或网络使用等IT指标可以帮助检测和排除问题。
- en: Health check
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 健康检查
- en: To check if the model is indeed online and to analyze its latency, it is common
    to implement a health check mechanism that simply queries the model at a fixed
    interval (on the order of one minute) and logs the results.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查模型是否在线并分析其延迟，通常会实现一个健康检查机制，该机制简单地查询模型，查询间隔固定（大约一分钟），并记录结果。
- en: ML metrics monitoring
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ML指标监控
- en: This is about analyzing the accuracy of the model and comparing it to another
    version or detecting when it is going stale. Since it may require heavy computation,
    this is typically lower frequency, but as always, will depend on the application;
    it is typically done once a week. [Chapter 7](ch07.html#monitoring_and_feedback_loop)
    details how to implement this feedback loop.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这是关于分析模型的准确性，并与另一个版本进行比较或检测其是否过时的过程。由于可能需要大量计算资源，这通常是低频操作，但像往常一样，这将取决于应用程序；通常每周执行一次。[第7章](ch07.html#monitoring_and_feedback_loop)详细介绍了如何实现此反馈循环。
- en: Finally, when a malfunction is detected, a rollback to a previous version may
    be necessary. It is critical to have the rollback procedure ready and as automated
    as possible; testing it regularly can make sure it is indeed functional.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当检测到故障时，可能需要回滚到之前的版本。有必要准备回滚流程，并尽可能自动化；定期测试可以确保其确实可用。
- en: Containerization
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器化
- en: As described earlier, managing the versions of a model is much more than just
    saving its code into a version control system. In particular, it is necessary
    to provide an exact description of the environment (including, for example, all
    the Python libraries used as well as their versions, the system dependencies that
    need to be installed, etc.).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前文所述，管理模型的版本远不止将其代码保存到版本控制系统中。特别是需要提供环境的精确描述（包括所有使用的 Python 库及其版本，需要安装的系统依赖项等）。
- en: But storing this metadata is not enough. Deploying to production should automatically
    and reliably rebuild this environment on the target machine. In addition, the
    target machine will typically run multiple models simultaneously, and two models
    may have incompatible dependency versions. Finally, several models running on
    the same machine could compete for resources, and one misbehaving model could
    hurt the performance of multiple cohosted models.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 但仅仅存储这些元数据是不够的。将其部署到生产环境应该自动且可靠地在目标机器上重建此环境。此外，目标机器通常会同时运行多个模型，而两个模型可能具有不兼容的依赖版本。最后，同一台机器上运行的多个模型可能会竞争资源，一个表现不佳的模型可能会影响多个共存模型的性能。
- en: Containerization technology is increasingly used to tackle these challenges.
    These tools bundle an application together with all of its related configuration
    files, libraries, and dependencies that are required for it to run across different
    operating environments. Unlike virtual machines (VMs), containers do not duplicate
    the complete operating system; multiple containers share a common operating system
    and are therefore far more resource efficient.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 容器技术越来越多地用于解决这些挑战。这些工具将应用程序与其所有相关的配置文件、库和依赖项捆绑在一起，以便在不同的操作环境中运行。与虚拟机（VM）不同，容器不复制完整的操作系统；多个容器共享一个公共操作系统，因此资源效率更高。
- en: The most well-known containerization technology is the open source platform
    Docker. Released in 2014, it has become the de facto standard. It allows an application
    to be packaged, sent to a server (the Docker host), and run with all its dependencies
    in isolation from other applications.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最知名的容器技术是开源平台 Docker。自2014年发布以来，它已成为事实上的标准。它允许将应用程序打包发送到服务器（Docker 主机），并在与其他应用程序隔离的环境中运行其所有依赖项。
- en: 'Building the basis of a model-serving environment that can accommodate many
    models, each of which may run multiple copies, may require multiple Docker hosts.
    When deploying a model, the framework should solve a number of issues:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个可以容纳多个模型的模型服务环境的基础，每个模型可能运行多个副本，可能需要多个 Docker 主机。在部署模型时，框架应该解决一些问题：
- en: Which Docker host(s) should receive the container?
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些 Docker 主机应接收该容器？
- en: When a model is deployed in several copies, how can the workload be balanced?
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当一个模型部署在多个副本中时，如何平衡工作负载？
- en: What happens if the model becomes unresponsive, for example, if the machine
    hosting it fails? How can that be detected and a container reprovisioned?
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型变得无响应，例如，如果托管它的机器失败了会发生什么？如何检测到并重新提供容器？
- en: How can a model running on multiple machines be upgraded, with assurances that
    old and new versions are switched on and off, and that the load balancer is updated
    with a correct sequence?
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何升级在多台机器上运行的模型，并确保旧版本和新版本的切换以及负载均衡器更新正确的顺序？
- en: Kubernetes, an open source platform that has gained a lot of traction in the
    past few years and is becoming the standard for container orchestration, greatly
    simplifies these issues and many others. It provides a powerful declarative API
    to run applications in a group of Docker hosts, called a Kubernetes *cluster*.
    The word *declarative* means that rather than trying to express in code the steps
    to set up, monitor, upgrade, stop, and connect the container (which can be complex
    and error prone), users specify in a configuration file the desired state, and
    Kubernetes makes it happen and then maintains it.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes是一个开源平台，在过去几年中获得了很多关注，并且正在成为容器编排的标准。它极大地简化了这些问题以及许多其他问题。它提供了一个强大的声明式API来在一组Docker主机中运行应用程序，称为Kubernetes
    *集群*。声明式的含义是，用户不需要试图用代码表达设置、监视、升级、停止和连接容器的步骤（这可能会很复杂且容易出错），而是在配置文件中指定所需的状态，Kubernetes将实现并维护它。
- en: For example, users need only specify to Kubernetes “make sure four instances
    of this container run at all times,” and Kubernetes will allocate the hosts, start
    the containers, monitor them, and start a new instance if one of them fails. Finally,
    the major cloud providers all provide managed Kubernetes services; users do not
    even have to install and maintain Kubernetes itself. If an application or a model
    is packaged as a Docker container, users can directly submit it, and the service
    will provision the required machines to run one or several instances of the container
    inside Kubernetes.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，用户只需告诉Kubernetes“确保始终运行四个此容器的实例”，Kubernetes将分配主机、启动容器、监视它们，并在其中一个失败时启动新实例。最后，主要的云服务提供商都提供托管的Kubernetes服务；用户甚至不必安装和维护Kubernetes本身。如果应用程序或模型被打包为Docker容器，用户可以直接提交它，服务将为运行一个或多个容器实例而配置所需的机器。
- en: Docker with Kubernetes can provide a powerful infrastructure to host applications,
    including ML models. Leveraging these products greatly simplifies the implementation
    of the deployment strategies—like blue-green deployments or canary releases—although
    they are not aware of the nature of the deployed applications and thus can’t natively
    manage the ML performance analysis. Another major advantage of this type of infrastructure
    is the ability to easily scale the model’s deployment.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Docker与Kubernetes结合可以提供强大的基础设施来托管应用程序，包括ML模型。利用这些产品极大地简化了部署策略的实施，比如蓝绿部署或金丝雀发布，尽管它们并不了解部署的应用程序的特性，因此无法本地管理ML性能分析。这种类型基础设施的另一个主要优势是轻松扩展模型的部署。
- en: Scaling Deployments
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展部署
- en: 'As ML adoption grows, organizations face two types of growth challenges:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 随着ML的采纳增长，组织面临两种类型的增长挑战：
- en: The ability to use a model in production with high-scale data
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够在高规模数据环境中将模型用于生产
- en: The ability to train larger and larger numbers of models
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够训练更多和更多的模型
- en: Handling more data for real-time scoring is made much easier by frameworks such
    as Kubernetes. Since most of the time trained models are essentially formulas,
    they can be replicated in the cluster in as many copies as necessary. With the
    auto-scaling features in Kubernetes, both provisioning new machines and load balancing
    are fully handled by the framework, and setting up a system with huge scaling
    capabilities is now relatively simple. The major difficulty can then be to process
    the large amount of monitoring data; [Chapter 7](ch07.html#monitoring_and_feedback_loop)
    provides some details on this challenge.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实时评分处理更多数据，像Kubernetes这样的框架大大简化了工作。由于大多数时间训练好的模型本质上是公式，它们可以在集群中复制成任意数量的副本。在Kubernetes的自动扩展功能下，新增机器的配置和负载均衡都完全由框架处理，建立具有巨大扩展能力的系统现在相对简单。主要的困难可能是处理大量监控数据；[第7章](ch07.html#monitoring_and_feedback_loop)提供了关于这一挑战的一些细节。
- en: 'For batch scoring, the situation can be more complex. When the volume of data
    becomes too large, there are essentially two types of strategies to distribute
    the computation:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 对于批量评分，情况可能更复杂。当数据量变得过大时，基本上有两种分布计算的策略：
- en: Using a framework that handles distributed computation natively, in particular
    Spark. Spark is an open source distributed computation framework. It is useful
    to understand that Spark and Kubernetes do not play similar roles and can be combined.
    Kubernetes orchestrates containers, but Kubernetes is not aware of what the containers
    are actually doing; as far as Kubernetes is concerned, they are just containers
    that run an application on one specific host. (In particular, Kubernetes has no
    concept of data processing, as it can be used to run any kind of application.)
    Spark is a computation framework that can split the data and the computation among
    its nodes. A modern way to use Spark is through Kubernetes. To run a Spark job,
    the desired number of Spark containers are started by Kubernetes; once they are
    started, they can communicate to complete the computation, after which the containers
    are destroyed and the resources are available for other applications, including
    other Spark jobs that may have different Spark versions or dependencies.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用本地处理分布式计算的框架，特别是Spark。Spark是一个开源的分布式计算框架。重要的是要理解Spark和Kubernetes不扮演相似的角色，并且可以结合使用。Kubernetes编排容器，但Kubernetes不知道容器实际在做什么；对于Kubernetes来说，它们只是在特定主机上运行应用程序的容器。特别是，Kubernetes没有数据处理的概念，因为它可用于运行任何类型的应用程序。Spark是一个可以将数据和计算分配到其节点的计算框架。使用Spark的现代方式是通过Kubernetes。要运行Spark作业，Kubernetes启动所需数量的Spark容器；一旦启动，它们可以通信完成计算，然后销毁容器并释放资源供其他应用程序使用，包括可能具有不同Spark版本或依赖项的其他Spark作业。
- en: Another way to distribute batch processing is to partition the data. There are
    many ways to achieve this, but the general idea is that scoring is typically a
    row-by-row operation (each row is scored one by one), and the data can be split
    in some way so that several machines can each read a subset of the data and score
    a subset of the rows.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种分发批处理的方法是对数据进行分区。有许多实现方法，但总体思路是评分通常是逐行操作（逐行评分），数据可以以某种方式分割，以便多台机器每台读取数据的子集并评分部分行。
- en: In terms of computation, scaling the number of models is somewhat simpler. The
    key is to add more computing power and to make sure the monitoring infrastructure
    can handle the workload. But in terms of governance and processes, this is the
    most challenging situation.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 就计算而言，增加模型数量相对较简单。关键是增加计算能力并确保监控基础设施能够处理工作负载。但在治理和流程方面，这是最具挑战性的情况。
- en: In particular, scaling the number of models means that the CI/CD pipeline must
    be able to handle large numbers of deployments. As the number of models grows,
    the need for automation and governance grows, as human verification cannot necessarily
    be systematic or consistent.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，增加模型数量意味着CI/CD流水线必须能够处理大量的部署。随着模型数量的增加，自动化和治理的需求也增长，因为人工验证不能保证系统性或一致性。
- en: In some applications, it is possible to rely on fully automated continuous deployment
    if the risks are well controlled by automated validation, canary releases, and
    automated canary analysis. There can be numerous infrastructure challenges since
    training, building models, validating on test data, etc., all need to be performed
    on clusters rather than on a single machine. Also, with a higher number of models,
    the CI/CD pipeline of each model can vary widely, and if nothing is done, each
    team will have to develop its own CI/CD pipeline for each model.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些应用中，如果通过自动化验证、金丝雀发布和自动化金丝雀分析有效控制了风险，完全可以依赖于全自动化持续部署。由于训练、模型构建、测试数据验证等工作都需要在集群上进行而非单台机器上进行，可能会面临许多基础设施挑战。此外，随着模型数量的增加，每个模型的持续集成/持续部署（CI/CD）流水线可能差异很大，如果不加处理，每个团队都必须为每个模型开发自己的CI/CD流水线。
- en: This is suboptimal from efficiency and governance perspectives. While some models
    may need highly specific validation pipelines, most projects can probably use
    a small number of common patterns. In addition, maintenance is made much more
    complex as it may become impractical to implement a new systematic validation
    step, for example, since the pipelines would not necessarily share a common structure
    and would then be impossible to update safely, even programmatically. Sharing
    practices and standardized pipelines can help limit complexity. A dedicated tool
    to manage large numbers of pipelines can also be used; for example, Netflix released
    Spinnaker, an open source continuous deployment and infrastructure management
    platform.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 从效率和治理的角度来看，这是不够理想的。虽然某些模型可能需要高度特定的验证流水线，但大多数项目可能可以使用少量通用模式。此外，由于可能不实用实施新的系统验证步骤，比如流水线不一定共享通用结构，因此维护变得更加复杂，甚至在程序化方面也难以安全地更新。分享实践和标准化的流水线可以帮助限制复杂性。还可以使用专门的工具来管理大量流水线；例如，Netflix发布了Spinnaker，这是一个开源的持续部署和基础设施管理平台。
- en: Requirements and Challenges
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 需求和挑战
- en: 'When deploying a model, there are several possible scenarios:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署模型时，存在几种可能的情况：
- en: One model deployed on one server
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个模型部署在一个服务器上
- en: One model deployed on multiple servers
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个模型部署在多个服务器上
- en: Multiple versions of a model deployed on one server
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多个模型的多个版本部署在一个服务器上
- en: Multiple versions of a model deployed on multiple servers
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个模型的多个版本部署在多个服务器上
- en: Multiple versions of multiple models deployed on multiple servers
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多个模型的多个版本部署在多个服务器上
- en: 'An effective logging system should be able to generate centralized datasets
    that can be exploited by the model designer or the ML engineer, usually outside
    of the production environment. More specifically, it should cover all of the following
    situations:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有效的日志记录系统应该能够生成中心化的数据集，这些数据集可以被模型设计师或ML工程师利用，通常是在生产环境之外。具体来说，它应该涵盖以下所有情况：
- en: The system can access and retrieve scoring logs from multiple servers, either
    in a real-time scoring use case or in a batch scoring use case.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统可以访问并从多个服务器检索评分日志，无论是实时评分用例还是批处理评分用例。
- en: When a model is deployed on multiple servers, the system can handle the mapping
    and aggregation of all information per model across servers.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当一个模型部署在多个服务器上时，系统可以处理每个模型在服务器上的映射和聚合所有信息。
- en: When different versions of a model are deployed, the system can handle the mapping
    and aggregation of all information per version of the model across servers.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当不同版本的模型部署时，系统可以处理每个模型版本在服务器上的映射和聚合所有信息。
- en: In terms of challenges, for large-scale machine learning applications, the number
    of raw event logs generated can be an issue if there are no preprocessing steps
    in place to filter and aggregate data. For real-time scoring use cases, logging
    streaming data requires setting up a whole new set of tooling that entails a significant
    engineering effort to maintain. However, in both cases, because the goal of monitoring
    is usually to estimate aggregate metrics, saving only a subset of the predictions
    may be acceptable.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 就挑战而言，对于大规模机器学习应用程序来说，如果没有预处理步骤来过滤和聚合数据，生成的原始事件日志数量可能是一个问题。对于实时评分的用例，记录流式数据需要设置一整套新的工具，这需要大量的工程努力来维护。然而，在这两种情况下，因为监控的目标通常是估计聚合指标，所以只保存预测的一个子集可能是可以接受的。
- en: Closing Thoughts
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结思考
- en: Deploying to production is a key component of MLOps, and as dissected in this
    chapter, having the right processes and tools in place can ensure that it happens
    quickly. The good news is that many of the elements of success, particularly CI/CD
    best practices, are not new. Once teams understand how they can be applied to
    machine learning models, the organization will have a good foundation on which
    to expand as MLOps scales with the business.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型部署到生产环境是MLOps的关键组成部分，正如本章所分析的，拥有正确的流程和工具可以确保快速部署。好消息是，许多成功的要素，特别是CI/CD的最佳实践，并不是新的。一旦团队了解如何将其应用于机器学习模型，组织将拥有一个良好的基础，可以随着业务的扩展而扩展MLOps。
