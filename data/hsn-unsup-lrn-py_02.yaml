- en: Chapter 1\. Unsupervised Learning in the Machine Learning Ecosystem
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章 机器学习生态系统中的无监督学习
- en: Most of human and animal learning is unsupervised learning. If intelligence
    was a cake, unsupervised learning would be the cake, supervised learning would
    be the icing on the cake, and reinforcement learning would be the cherry on the
    cake. We know how to make the icing and the cherry, but we don’t know how to make
    the cake. We need to solve the unsupervised learning problem before we can even
    think of getting to true AI.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 大多数人类和动物的学习都是无监督学习。如果智能是一个蛋糕，无监督学习将是蛋糕，监督学习将是蛋糕上的糖衣，而强化学习将是蛋糕上的樱桃。我们知道如何制作糖衣和樱桃，但我们不知道如何制作蛋糕。在我们甚至考虑真正的AI之前，我们需要解决无监督学习问题。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yann LeCun
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 伊恩·拉坤
- en: In this chapter, we will explore the difference between a rules-based system
    and machine learning, the difference between supervised learning and unsupervised
    learning, and the relative strengths and weaknesses of each.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨基于规则的系统与机器学习、监督学习与无监督学习之间的区别，以及每种方法的相对优势和劣势。
- en: We will also cover many popular supervised learning algorithms and unsupervised
    learning algorithms and briefly examine how semisupervised learning and reinforcement
    learning fit into the mix.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将介绍许多流行的监督学习算法和无监督学习算法，并简要探讨半监督学习和强化学习如何融入其中。
- en: Basic Machine Learning Terminology
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基础机器学习术语
- en: 'Before we delve into the different types of machine learning, let’s take a
    look at a simple and commonly used machine learning example to help make the concepts
    we introduce tangible: the email spam filter. We need to build a simple program
    that takes in emails and correctly classifies them as either “spam” or “not spam.”
    This is a straightforward classification problem.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨不同类型的机器学习之前，让我们先看一个简单且常用的机器学习示例，以帮助我们更具体地理解我们介绍的概念：电子邮件垃圾过滤器。我们需要构建一个简单的程序，输入电子邮件并正确地将它们分类为“垃圾邮件”或“非垃圾邮件”。这是一个直接的分类问题。
- en: 'Here’s a bit of machine learning terminology as a refresher: the *input variables*
    into this problem are the text of the emails. These input variables are also known
    as *features* or *predictors* or *independent variables*. The *output variable*—what
    we are trying to predict—is the *label* “spam” or “not spam.” This is also known
    as the *target variable*, *dependent variable*, or *response variable* (or *class*
    since this is a classification problem).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些机器学习术语的复习：这个问题的*输入变量*是电子邮件的文本。这些输入变量也被称为*特征*或*预测变量*或*独立变量*。我们试图预测的*输出变量*是标签“垃圾邮件”或“非垃圾邮件”。这也被称为*目标变量*、*依赖变量*或*响应变量*（或*类*，因为这是一个分类问题）。
- en: The set of examples the AI trains on is known as the *training set*, and each
    individual example is called a training *instance* or *sample*. During the training,
    the AI is attempting to minimize its *cost function* or *error rate*, or framed
    more positively, to maximize its *value function*—in this case, the ratio of correctly
    classified emails. The AI actively optimizes for a minimal error rate during training.
    Its error rate is calculated by comparing the AI’s predicted label with the true
    label.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: AI训练的示例集被称为*训练集*，每个单独的示例称为训练*实例*或*样本*。在训练过程中，AI试图最小化其*成本函数*或*错误率*，或者更积极地说，最大化其*价值函数*—在本例中，是正确分类的电子邮件比例。AI在训练期间积极优化以达到最小的错误率。它的错误率是通过将AI预测的标签与真实标签进行比较来计算的。
- en: 'However, what we care about most is how well the AI generalizes its training
    to never-before-seen emails. This will be the true test for the AI: can it correctly
    classify emails that it has never seen before using what it has learned by training
    on the examples in the training set? This *generalization error* or *out-of-sample
    error* is the main thing we use to evaluate machine learning solutions.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们最关心的是AI如何将其训练推广到以前从未见过的电子邮件上。这将是AI的真正测试：它能否使用在训练集示例中学到的知识正确分类它以前从未见过的电子邮件？这种*泛化误差*或*样外误差*是我们用来评估机器学习解决方案的主要指标。
- en: This set of never-before-seen examples is known as the *test set* or *holdout
    set* (because the data is held out from the training). If we choose to have multiple
    holdout sets (perhaps to gauge our generalization error as we train, which is
    advisable), we may have intermediate holdout sets that we use to evaluate our
    progress before the final test set; these intermediate holdout sets are called
    *validation sets*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这组以前从未见过的示例被称为*测试集*或*保留集*（因为这些数据被保留在训练之外）。如果我们选择有多个保留集（也许在训练过程中评估我们的泛化误差是明智的），我们可能会有用于评估我们进展的中间保留集，这些中间保留集称为*验证集*。
- en: To put all of this together, the AI trains on the training data (*experience*)
    to improve its error rate (*performance*) in flagging spam (*task*), and the ultimate
    success criterion is how well its experience generalizes to new, never-before-seen
    data (*generalization error*).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些结合起来，AI在训练数据（*经验*）上进行训练，以提高在标记垃圾邮件（*任务*）中的错误率（*性能*），最终成功的标准是其经验如何推广到新的、以前从未见过的数据上（*泛化误差*）。
- en: Rules-Based vs. Machine Learning
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于规则与机器学习
- en: Using a rules-based approach, we can design a spam filter with explicit rules
    to catch spam such as flag emails with “u” instead of “you,” “4” instead of “for,”
    “BUY NOW,” etc. But this system would be difficult to maintain over time as bad
    guys change their spam behavior to evade the rules. If we used a rules-based system,
    we would have to frequently adjust the rules manually just to stay up-to-date.
    Also, it would be very expensive to set up—think of all the rules we would need
    to create to make this a well-functioning system.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基于规则的方法，我们可以设计一个垃圾邮件过滤器，通过明确的规则捕捉垃圾邮件，比如标记使用“u”代替“you”，“4”代替“for”，“BUY NOW”等的电子邮件。但是随着坏人改变他们的垃圾邮件行为以逃避规则，这种系统在时间上会很难维护。如果我们使用基于规则的系统，我们将不得不经常手动调整规则，以保持系统的最新状态。而且，设置这种系统将非常昂贵——想象一下我们需要创建多少规则才能使其正常运行。
- en: Instead of a rules-based approach, we can use machine learning to train on the
    email data and automatically engineer rules to correctly flag malicious email
    as spam. This machine learning-based system could be automatically adjusted over
    time as well. This system would be much cheaper to train and maintain.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于规则的方法不同，我们可以使用机器学习来训练电子邮件数据，并自动创建规则以正确标记恶意电子邮件为垃圾邮件。这种基于机器学习的系统也可以随着时间的推移自动调整。这种系统的培训和维护成本要低得多。
- en: In this simple email problem, it may be possible for us to handcraft rules,
    but, for many problems, handcrafting rules is not feasible at all. For example,
    consider designing a self-driving car—imagine drafting rules for how the car should
    behave in each and every single instance it ever encounters. This is an intractable
    problem unless the car can learn and adapt on its own based on its experience.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单的电子邮件问题中，我们可能可以手工制定规则，但是对于许多问题来说，手工制定规则根本不可行。例如，考虑设计自动驾驶汽车——想象一下为汽车在每一个遇到的情况下如何行为制定规则，这是一个棘手的问题，除非汽车可以根据自己的经验学习和适应。
- en: We could also use machine learning systems as an exploration or data discovery
    tool to gain deeper insight into the problem we are trying to solve. For example,
    in the email spam filter example, we can learn which words or phrases are most
    predictive of spam and recognize newly emerging malicious spam patterns.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将机器学习系统作为探索或数据发现工具，以深入了解我们尝试解决的问题。例如，在电子邮件垃圾邮件过滤器的示例中，我们可以学习哪些单词或短语最能预测垃圾邮件，并识别新出现的恶意垃圾邮件模式。
- en: Supervised vs. Unsupervised
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习与非监督学习
- en: The field of machine learning has two major branches—*supervised learning* and
    *unsupervised learning*—and plenty of sub-branches that bridge the two.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习领域有两个主要分支——*监督学习*和*无监督学习*——以及许多桥接这两者的子分支。
- en: In supervised learning, the AI agent has access to labels, which it can use
    to improve its performance on some task. In the email spam filter problem, we
    have a dataset of emails with all the text within each and every email. We also
    know which of these emails are spam or not (the so-called *labels*). These labels
    are very valuable in helping the supervised learning AI separate the spam emails
    from the rest.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，AI代理可以访问标签，这些标签可以用来改善其在某些任务上的表现。在电子邮件垃圾邮件过滤问题中，我们有一个包含每封电子邮件中所有文本的数据集。我们还知道哪些邮件是垃圾邮件或非垃圾邮件（所谓的*标签*）。这些标签在帮助监督学习AI区分垃圾邮件和其他邮件方面非常有价值。
- en: In unsupervised learning, labels are not available. Therefore, the task of the
    AI agent is not well-defined, and performance cannot be so clearly measured. Consider
    the email spam filter problem—this time without labels. Now, the AI agent will
    attempt to understand the underlying structure of emails, separating the database
    of emails into different groups such that emails within a group are similar to
    each other but different from emails in other groups.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，没有标签可用。因此，AI代理的任务并不是明确定义的，性能也不能如此清晰地衡量。考虑电子邮件垃圾邮件过滤器问题——这次没有标签。现在，AI代理将尝试理解电子邮件的基本结构，将电子邮件数据库分成不同的组，使得组内的电子邮件彼此相似但与其他组的电子邮件不同。
- en: This unsupervised learning problem is less clearly defined than the supervised
    learning problem and harder for the AI agent to solve. But, if handled well, the
    solution is more powerful.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个无监督学习问题比监督学习问题的定义不太明确，对AI代理来说更难解决。但是，如果处理得当，解决方案将更为强大。
- en: 'Here’s why: the unsupervised learning AI may find several groups that it later
    tags as being “spam”—but the AI may also find groups that it later tags as being
    “important” or categorize as “family,” “professional,” “news,” “shopping,” etc.
    In other words, because the problem does not have a strictly defined task, the
    AI agent may find interesting patterns above and beyond what we initially were
    looking for.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 原因在于：无监督学习AI可能会发现几个后来标记为“垃圾邮件”的组，但AI也可能会发现后来标记为“重要”的组，或者归类为“家庭”、“专业”、“新闻”、“购物”等。换句话说，由于问题没有严格定义的任务，AI代理可能会发现我们最初未曾寻找的有趣模式。
- en: Moreover, this unsupervised system is better than the supervised system at finding
    new patterns in future data, making the unsupervised solution more nimble on a
    go-forward basis. This is the power of unsupervised learning.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这种无监督系统在未来数据中发现新模式的能力优于监督系统，使得无监督解决方案在前进时更加灵活。这就是无监督学习的力量。
- en: The Strengths and Weaknesses of Supervised Learning
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习的优势和劣势
- en: Supervised learning excels at optimizing performance in well-defined tasks with
    plenty of labels. For example, consider a very large dataset of images of objects,
    where each image is labeled. If the dataset is sufficiently large enough and we
    train using the right machine learning algorithms (i.e., convolutional neural
    networks) and with powerful enough computers, we can build a very good supervised
    learning-based image classification system.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习在定义良好的任务和充足标签的情况下优化性能。例如，考虑一个非常大的对象图像数据集，其中每个图像都有标签。如果数据集足够大，并且我们使用正确的机器学习算法（即卷积神经网络）并且使用足够强大的计算机进行训练，我们可以构建一个非常好的基于监督学习的图像分类系统。
- en: As the supervised learning AI trains on the data, it will be able to measure
    its performance (via a cost function) by comparing its predicted image label with
    the true image label that we have on file. The AI will explicitly try to minimize
    this cost function such that its error on never-before-seen images (from a holdout
    set) is as low as possible.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当监督学习AI在数据上进行训练时，它将能够通过比较其预测的图像标签与我们文件中的真实图像标签来测量其性能（通过成本函数）。AI将明确尝试将这个成本函数最小化，使其在以前未见过的图像（从留存集）上的错误尽可能低。
- en: This is why labels are so powerful—they help guide the AI agent by providing
    it with an error measure. The AI uses the error measure to improve its performance
    over time. Without such labels, the AI does not know how successful it is (or
    isn’t) in correctly classifying images.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么标签如此强大——它们通过提供错误度量来指导AI代理。AI使用这个错误度量随着时间的推移来提高其性能。没有这样的标签，AI不知道它在正确分类图像方面有多成功（或不成功）。
- en: However, the costs of manually labeling an image dataset are high. And, even
    the best curated image datasets have only thousands of labels. This is a problem
    because supervised learning systems will be very good at classifying images of
    objects for which it has labels but poor at classifying images of objects for
    which it has no labels.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，手动标记图像数据集的成本很高。即使是最好的策划图像数据集也只有数千个标签。这是一个问题，因为监督学习系统在对具有标签的对象图像分类方面表现非常出色，但在对没有标签的对象图像分类方面表现不佳。
- en: As powerful as supervised learning systems are, they are also limited at generalizing
    knowledge beyond the labeled items they have trained on. Since the majority of
    the world’s data is unlabeled, with supervised learning, the ability of AI to
    expand its performance to never-before-seen instances is quite limited.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管监督学习系统非常强大，但它们在将知识推广到以前未见过的实例上的能力也受到限制。由于世界上大多数数据都没有标签，因此使用监督学习时，AI将其性能扩展到以前未见过的实例的能力是相当有限的。
- en: In other words, supervised learning is great at solving narrow AI problems but
    not so good at solving more ambitious, less clearly defined problems of the strong
    AI type.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，监督学习擅长解决狭义AI问题，但在解决更有雄心、定义不太明确的强AI类型问题时表现不佳。
- en: The Strengths and Weaknesses of Unsupervised Learning
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习的优势和劣势
- en: Supervised learning will trounce unsupervised learning at narrowly defined tasks
    for which we have well-defined patterns that do not change much over time and
    sufficiently large, readily available labeled datasets.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在狭义定义的任务中，有着明确定义的模式并且随时间变化不大以及具有充足可用的标记数据集时，监督学习将在效果上胜过无监督学习。
- en: However, for problems where patterns are unknown or constantly changing or for
    which we do not have sufficiently large labeled datasets, unsupervised learning
    truly shines.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于那些模式未知或不断变化，或者我们没有足够大的标记数据集的问题，无监督学习确实表现出色。
- en: Instead of being guided by labels, unsupervised learning works by learning the
    underlying structure of the data it has trained on. It does this by trying to
    represent the data it trains on with a set of parameters that is significantly
    smaller than the number of examples available in the dataset. By performing this
    representation learning, unsupervised learning is able to identify distinct patterns
    in the dataset.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习不依赖标签，而是通过学习其训练的数据的基本结构来工作。它通过试图用比数据集中可用示例数量显著较小的一组参数来表示其训练的数据来实现这一点。通过执行这种表示学习，无监督学习能够识别数据集中的不同模式。
- en: In the image dataset example (this time without labels), the unsupervised learning
    AI may be able to identify and group images based on how similar they are to each
    other and how different they are from the rest. For example, all the images that
    look like chairs will be grouped together, all the images that look like dogs
    will be grouped together, etc.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像数据集示例中（这次没有标签），无监督学习的AI可能能够根据它们彼此的相似性以及与其余图像的不同性将图像识别并分组。例如，所有看起来像椅子的图像将被分组在一起，所有看起来像狗的图像将被分组在一起，依此类推。
- en: Of course, the unsupervised learning AI itself cannot label these groups as
    “chairs” or “dogs” but now that similar images are grouped together, humans have
    a much simpler labeling task. Instead of labeling millions of images by hand,
    humans can manually label all the distinct groups, and the labels will apply to
    all the members within each group.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，无监督学习的AI本身无法将这些组标记为“椅子”或“狗”，但现在相似的图像被分组在一起后，人类的标记任务变得简单得多。人类可以手动标记所有不同的组，标签将应用于每个组内的所有成员。
- en: After the initial training, if the unsupervised learning AI finds images that
    do not belong to any of the labeled groups, the AI will create separate groups
    for the unclassified images, triggering a human to label the new, yet-to-be-labeled
    groups of images.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 经过初步训练后，如果无监督学习的AI发现了不属于任何已标记组的图像，AI将为未分类的图像创建单独的组，触发人类标记新的、尚未标记的图像组。
- en: Unsupervised learning makes previously intractable problems more solvable and
    is much more nimble at finding hidden patterns both in the historical data that
    is available for training and in future data. Moreover, we now have an AI approach
    for the huge troves of unlabeled data that exist in the world.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习使以前棘手的问题更易解决，并且在找到历史数据和未来数据中隐藏模式方面更为灵活。此外，我们现在有了一种处理世界上存在的大量未标记数据的AI方法。
- en: Even though unsupervised learning is less adept than supervised learning at
    solving specific, narrowly defined problems, it is better at tackling more open-ended
    problems of the strong AI type and at generalizing this knowledge.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管无监督学习在解决特定、狭义定义的问题方面不如监督学习熟练，但在解决更为开放的强AI类型问题和推广这种知识方面表现更佳。
- en: Just as importantly, unsupervised learning can address many of the common problems
    data scientists encounter when building machine learning solutions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 同样重要的是，无监督学习可以解决数据科学家在构建机器学习解决方案时遇到的许多常见问题。
- en: Using Unsupervised Learning to Improve Machine Learning Solutions
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用无监督学习来改善机器学习解决方案
- en: Recent successes in machine learning have been driven by the availability of
    lots of data, advances in computer hardware and cloud-based resources, and breakthroughs
    in machine learning algorithms. But these successes have been in mostly narrow
    AI problems such as image classification, computer vision, speech recognition,
    natural language processing, and machine translation.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的最近成功是由大量数据的可用性、计算硬件和基于云的资源的进步以及机器学习算法的突破推动的。但这些成功主要出现在狭义AI问题，如图像分类、计算机视觉、语音识别、自然语言处理和机器翻译领域。
- en: To solve more ambitious AI problems, we need to unlock the value of unsupervised
    learning. Let’s explore the most common challenges data scientists face when building
    solutions and how unsupervised learning can help.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决更雄心勃勃的AI问题，我们需要发挥无监督学习的价值。让我们探讨数据科学家在构建解决方案时面临的最常见挑战，以及无监督学习如何帮助解决这些挑战。
- en: Insufficient labeled data
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标记不足的数据
- en: I think AI is akin to building a rocket ship. You need a huge engine and a lot
    of fuel. If you have a large engine and a tiny amount of fuel, you won’t make
    it to orbit. If you have a tiny engine and a ton of fuel, you can’t even lift
    off. To build a rocket you need a huge engine and a lot of fuel.
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我认为AI就像建造一艘火箭。你需要一个巨大的引擎和大量的燃料。如果你有一个巨大的引擎和少量的燃料，你无法进入轨道。如果你有一个微小的引擎和大量的燃料，你甚至无法起飞。要建造一艘火箭，你需要一个巨大的引擎和大量的燃料。
- en: ''
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Andrew Ng
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Andrew Ng
- en: If machine learning were a rocket ship, data would be the fuel—without lots
    and lots of data, the rocket ship cannot fly. But not all data is created equal.
    To use supervised algorithms, we need lots of labeled data, which is hard and
    costly to generate.^([1](ch01.html#idm140637564299232))
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果机器学习是一艘火箭，数据就是燃料——没有大量数据，火箭是无法飞行的。但并非所有数据都是平等的。要使用监督算法，我们需要大量标记数据，这在生成过程中是困难且昂贵的。^([1](ch01.html#idm140637564299232))
- en: 'With unsupervised learning, we can automatically label unlabeled examples.
    Here is how it would work: we would cluster all the examples and then apply the
    labels from labeled examples to the unlabeled ones within the same cluster. Unlabeled
    examples would receive the label of the labeled ones they are most similar to.
    We will explore clustering in [Chapter 5](ch05.html#Chapter_5).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用无监督学习，我们可以自动标记未标记的示例。这里是它的工作原理：我们会对所有示例进行聚类，然后将标记示例的标签应用于同一聚类中的未标记示例。未标记的示例将获得它们与之最相似的已标记示例的标签。我们将在[第5章](ch05.html#Chapter_5)中探讨聚类。
- en: Overfitting
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过拟合
- en: If the machine learning algorithm learns an overly complex function based on
    the training data, it may perform very poorly on never-before-seen instances from
    holdout sets such as the validation set or test set. In this case, the algorithm
    has overfit the training data—by extracting too much from the noise in the data—and
    has very poor generalization error. In other words, the algorithm is memorizing
    the training data rather than learning how to generalize knowledge based off of
    it.^([2](ch01.html#idm140637564293600))
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果机器学习算法根据训练数据学习了一个过于复杂的函数，它在从保留集（例如验证集或测试集）中获得的以前未见实例上可能表现非常糟糕。在这种情况下，算法过度拟合了训练数据——从数据中提取了太多的噪声，并且具有非常差的泛化误差。换句话说，该算法是在记忆训练数据，而不是学习如何基于其泛化知识。^([2](ch01.html#idm140637564293600))
- en: To address this, we can introduce unsupervised learning as a *regularizer*.
    *Regularization* is a process used to reduce the complexity of a machine learning
    algorithm, helping it capture the signal in the data without adjusting too much
    to the noise. Unsupervised pretraining is one such form of regularization. Instead
    of feeding the original input data directly into a supervised learning algorithm,
    we can feed a new representation of the original input data that we generate.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以将无监督学习引入作为*正则化器*。*正则化*是一种用来降低机器学习算法复杂度的过程，帮助其捕捉数据中的信号而不是过多地调整到噪声。无监督预训练就是这种正则化的形式之一。我们可以不直接将原始输入数据馈送到监督学习算法中，而是馈送我们生成的原始输入数据的新表示。
- en: This new representation captures the essence of the original data—the true underlying
    structure—while losing some of the less representative noise along the way. When
    we feed this new representation into the supervised learning algorithm, it has
    less noise to wade through and captures more of the signal, improving its generalization
    error. We will explore feature extraction in [Chapter 7](ch07.html#Chapter_7).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这种新的表示捕捉了原始数据的本质——真正的底层结构——同时在过程中减少了一些不太代表性的噪声。当我们将这种新的表示输入监督学习算法时，它需要处理的噪声较少，捕捉到更多的信号，从而改善其泛化误差。我们将在[第七章](ch07.html#Chapter_7)探讨特征提取。
- en: Curse of dimensionality
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 维度诅咒
- en: Even with the advances in computational power, big data is hard for machine
    learning algorithms to manage. In general, adding more instances is not too problematic
    because we can parallelize operations using modern map-reduce solutions such as
    Spark. However, the more features we have, the more difficult training becomes.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管计算能力有所提升，大数据对机器学习算法的管理仍然颇具挑战性。一般来说，增加更多实例并不太成问题，因为我们可以利用现代的映射-减少解决方案（如Spark）并行操作。然而，特征越多，训练就越困难。
- en: In a very high-dimensional space, supervised algorithms need to learn how to
    separate points and build a function approximation to make good decisions. When
    the features are very numerous, this search becomes very expensive, both from
    a time and compute perspective. In some cases, it may be impossible to find a
    good solution fast enough.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在非常高维空间中，监督算法需要学习如何分离点并构建函数逼近，以做出良好的决策。当特征非常多时，这种搜索变得非常昂贵，无论是从时间还是计算资源的角度来看。在某些情况下，可能无法快速找到一个好的解决方案。
- en: This problem is known as the *curse of dimensionality*, and unsupervised learning
    is well suited to help manage this. With dimensionality reduction, we can find
    the most salient features in the original feature set, reduce the number of dimensions
    to a more manageable number while losing very little important information in
    the process, and then apply supervised algorithms to more efficiently perform
    the search for a good function approximation. We will cover dimensionality reduction
    in [Chapter 3](ch03.html#Chapter_3).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题被称为*维度诅咒*，无监督学习非常适合帮助管理这一问题。通过降维，我们可以找到原始特征集中最显著的特征，将维度减少到一个更易管理的数量，同时在过程中几乎不丢失重要信息，然后应用监督算法来更有效地执行寻找良好函数逼近的搜索。我们将在[第三章](ch03.html#Chapter_3)涵盖降维技术。
- en: Feature engineering
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征工程
- en: Feature engineering is one of the most vital tasks data scientists perform.
    Without the right features, the machine learning algorithm will not be able to
    separate points in space well enough to make good decisions on never-before-seen
    examples. However, feature engineering is typically very labor-intensive; it requires
    humans to creatively hand-engineer the right types of features. Instead, we can
    use representation learning from unsupervised learning algorithms to automatically
    learn the right types of feature representations to help solve the task at hand.
    We will explore automatic feature extraction in [Chapter 7](ch07.html#Chapter_7).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是数据科学家执行的最关键任务之一。如果没有合适的特征，机器学习算法将无法在空间中有效分离点，从而不能在以前未见的示例上做出良好的决策。然而，特征工程通常非常耗时，需要人类创造性地手工设计正确类型的特征。相反，我们可以使用无监督学习算法中的表示学习来自动学习适合解决手头任务的正确类型的特征表示。我们将在[第七章](ch07.html#Chapter_7)探索自动特征提取。
- en: Outliers
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 异常值
- en: The quality of data is also very important. If machine learning algorithms train
    on rare, distortive outliers, their generalization error will be lower than if
    they ignored or addressed the outliers separately. With unsupervised learning,
    we can perform outlier detection using dimensionality reduction and create a solution
    specifically for the outliers and, separately, a solution for the normal data.
    We will build an anomaly detection system in [Chapter 4](ch04.html#Chapter_4).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的质量也非常重要。如果机器学习算法在稀有的、扭曲的异常值上进行训练，其泛化误差将低于忽略或单独处理异常值的情况。通过无监督学习，我们可以使用降维技术进行异常检测，并分别为异常数据和正常数据创建解决方案。我们将在[第四章](ch04.html#Chapter_4)构建一个异常检测系统。
- en: Data drift
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据漂移
- en: Machine learning models also need to be aware of drift in the data. If the data
    the model is making predictions on differs statistically from the data the model
    trained on, the model may need to retrain on data that is more representative
    of the current data. If the model does not retrain or does not recognize the drift,
    the model’s prediction quality on current data will suffer.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型还需要意识到数据中的漂移。如果模型用于预测的数据在统计上与模型训练时的数据不同，那么模型可能需要在更能代表当前数据的数据上重新训练。如果模型不重新训练或者没有意识到这种漂移，那么模型在当前数据上的预测质量将会受到影响。
- en: By building probability distributions using unsupervised learning, we can assess
    how different the current data is from the training set data—if the two are different
    enough, we can automatically trigger a retraining. We will explore how to build
    these types of data discriminators in [Chapter 12](ch12.html#Chapter_12).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用无监督学习构建概率分布，我们可以评估当前数据与训练集数据的差异性——如果两者差异足够大，我们可以自动触发重新训练。我们将探讨如何构建这些数据判别器类型的内容在[第12章](ch12.html#Chapter_12)中。
- en: A Closer Look at Supervised Algorithms
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对监督算法的更详细探讨
- en: Before we delve into unsupervised learning systems, let’s take a look at supervised
    learning algorithms and how they work. This will help frame where unsupervised
    learning fits within the machine learning ecosystem.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入研究无监督学习系统之前，让我们先看看监督学习算法及其工作原理。这将有助于我们理解无监督学习在机器学习生态系统中的位置。
- en: 'In supervised learning, there are two major types of problems: *classification*
    and *regression*. In classification, the AI must correctly classify items into
    one of two or more classes. If there are just two classes, the problem is called
    *binary classification*. If there are three or more classes, the problem is classed
    *multiclass classification*.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，存在两种主要类型的问题：*分类*和*回归*。在分类中，AI必须正确地将项目分类为两个或更多类别之一。如果只有两个类别，则该问题称为*二元分类*。如果有三个或更多类别，则该问题被归类为*多类分类*。
- en: Classification problems are also known as *discrete* prediction problems because
    each class is a discrete group. Classification problems also may be referred to
    as *qualitative* or *categorical* problems.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 分类问题也被称为*离散*预测问题，因为每个类别都是一个离散的群体。分类问题也可能被称为*定性*或*分类*问题。
- en: In regression, the AI must predict a *continuous* variable rather than a discrete
    one. Regression problems also may be referred to as *quantitative* problems.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归中，AI必须预测一个*连续*变量而不是离散变量。回归问题也可能被称为*定量*问题。
- en: Supervised machine learning algorithms span the gamut, from very simple to very
    complex, but they are all aimed at minimizing some cost function or error rate
    (or maximizing a value function) that is associated with the labels we have for
    the dataset.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 监督式机器学习算法涵盖了从非常简单到非常复杂的整个范围，但它们的目标都是最小化与数据集标签相关的某个成本函数或错误率（或最大化某个值函数）。
- en: As mentioned before, what we care about most is how well the machine learning
    solution generalizes to never-before-seen cases. The choice of the supervised
    learning algorithm is very important at minimizing this generalization error.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，我们最关心的是机器学习解决方案在前所未见的情况下的泛化能力。选择监督学习算法非常重要，可以最大程度地减少这种泛化误差。
- en: To achieve the lowest possible generalization error, the complexity of the algorithmic
    model should match the complexity of the true function underlying the data. We
    do not know what this true function really is. If we did, we would not need to
    use machine learning to create a model—we would just solve the function to find
    the right answer. But since we do not know what this true function is, we choose
    a machine learning algorithm to test hypotheses and find the model that best approximates
    this true function (i.e., has the lowest possible generalization error).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了达到尽可能低的泛化误差，算法模型的复杂性应该与数据底层真实函数的复杂性相匹配。我们不知道这个真实函数究竟是什么。如果我们知道，我们就不需要使用机器学习来创建模型了——我们只需解决函数以找到正确答案。但由于我们不知道这个真实函数是什么，我们选择机器学习算法来测试假设，并找到最接近这个真实函数的模型（即具有尽可能低的泛化误差）。
- en: If what the algorithm models is less complex than the true function, we have
    *underfit* the data. In this case, we could improve the generalization error by
    choosing an algorithm that can model a more complex function. However, if the
    algorithm designs an overly complex model, we have *overfit* the training data
    and will have poor performance on never-before-seen cases, increasing our generalization
    error.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果算法模拟的内容比真实函数复杂度低，我们就*欠拟合*了数据。在这种情况下，我们可以通过选择能够模拟更复杂函数的算法来改善泛化误差。然而，如果算法设计了一个过于复杂的模型，我们就*过拟合*了训练数据，并且在以前从未见过的情况下表现不佳，增加了我们的泛化误差。
- en: In other words, choosing more complex algorithms over simpler ones is not always
    the right choice—sometimes simpler is better. Each algorithm comes with its set
    of strengths, weaknesses, and assumptions, and knowing what to use when given
    the data you have and the problem you are trying to solve is very important to
    mastering machine learning.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，选择复杂算法而不是简单算法并不总是正确的选择——有时简单才是更好的。每种算法都有其一系列的优点、弱点和假设，知道在给定你拥有的数据和你试图解决的问题时何时使用何种方法对于掌握机器学习非常重要。
- en: In the rest of this chapter, we will describe some of the most common supervised
    algorithms (including some real-world applications) before doing the same for
    unsupervised algorithms.^([3](ch01.html#idm140637564244608))
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分中，我们将描述一些最常见的监督学习算法（包括一些实际应用），然后再介绍无监督算法。^([3](ch01.html#idm140637564244608))
- en: Linear Methods
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性方法
- en: The most basic supervised learning algorithms model a simple linear relationship
    between the input features and the output variable that we wish to predict.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的监督学习算法模拟了输入特征与我们希望预测的输出变量之间的简单线性关系。
- en: Linear regression
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性回归
- en: The simplest of all the algorithms is *linear regression*, which uses a model
    that assumes a linear relationship between the input variables (x) and the single
    output variable (y). If the true relationship between the inputs and the output
    is linear and the input variables are not highly correlated (a situation known
    as *collinearity*), linear regression may be an appropriate choice. If the true
    relationship is more complex or nonlinear, linear regression will underfit the
    data.^([4](ch01.html#idm140637564235360))
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 所有算法中最简单的是*线性回归*，它使用一个模型假设输入变量（x）与单个输出变量（y）之间存在线性关系。如果输入与输出之间的真实关系是线性的，并且输入变量之间不高度相关（称为*共线性*），线性回归可能是一个合适的选择。如果真实关系更为复杂或非线性，线性回归将会欠拟合数据。^([4](ch01.html#idm140637564235360))
- en: Because it is so simple, interpreting the relationship modeled by the algorithm
    is also very straightforward. *Interpretability* is a very important consideration
    for applied machine learning because solutions need to be understood and enacted
    by both technical and nontechnical people in industry. Without interpretability,
    the solutions become inscrutable black boxes.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 因为它非常简单，解释算法模型的关系也非常直接。*可解释性* 对于应用机器学习非常重要，因为解决方案需要被技术和非技术人员在工业中理解和实施。如果没有可解释性，解决方案就会变成不可理解的黑匣子。
- en: Strengths
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 优点
- en: Linear regression is simple, intrepretable, and hard to overfit because it cannot
    model overly complex relationships. It is an excellent choice when the underlying
    relationship between the input and output variables is linear.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归简单、可解释，并且难以过拟合，因为它无法模拟过于复杂的关系。当输入和输出变量之间的基础关系是线性的时，它是一个极好的选择。
- en: Weaknesses
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 弱点
- en: Linear regression will underfit the data when the relationship between the input
    and output variables is nonlinear.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入和输出变量之间的关系是非线性的时，线性回归将欠拟合数据。
- en: Applications
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 应用
- en: Since the true underlying relationship between human weight and human height
    is linear, linear regression is great for predicting weight using height as the
    input variable or, vice versa, for predicting height using weight as the input
    variable.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 由于人类体重与身高之间的真实基础关系是线性的，因此线性回归非常适合使用身高作为输入变量来预测体重，或者反过来，使用体重作为输入变量来预测身高。
- en: Logistic regression
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: The simplest classification algorithm is *logistic regression*, which is also
    a linear method but the predictions are transformed using the logistic function.
    The outputs of this transformation are *class probabilities*—in other words, the
    probabilities that the instance belongs to the various classes, where the sum
    of the probabilities for each instance adds up to one. Each instance is then assigned
    to the class for which it has the highest probability of belonging in.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的分类算法是 *逻辑回归*，它也是一种线性方法，但预测结果经过逻辑函数转换。这种转换的输出是*类别概率*——换句话说，实例属于各个类别的概率，每个实例的概率之和为一。然后将每个实例分配给其最有可能属于的类别。
- en: Strengths
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 优势
- en: Like linear regression, logistic regression is simple and interpretable. When
    the classes we are trying to predict are nonoverlapping and linearly separable,
    logistic regression is an excellent choice.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 与线性回归类似，逻辑回归简单且可解释。当我们尝试预测的类别不重叠且线性可分时，逻辑回归是一个很好的选择。
- en: Weaknesses
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 弱点
- en: When classes are not linearly separable, logistic regression will fail.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当类别不是线性可分时，逻辑回归会失败。
- en: Applications
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 应用场景
- en: When classes are mostly nonoverlapping—for example, the heights of young children
    versus the heights of adults—logistic regression will work well.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当类别大部分不重叠时，例如年幼儿童的身高与成年人的身高，逻辑回归效果很好。
- en: Neighborhood-Based Methods
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于邻居的方法
- en: Another group of very simple algorithms are neighborhood-based methods. Neighborhood-based
    methods are *lazy learners* since they learn how to label new points based on
    the proximity of the new points to existing labeled points. Unlike linear regression
    or logistic regression, neighborhood-based models do not learn a set model to
    predict labels for new points; rather, these models predict labels for new points
    based purely on distance of new points to preexisting labeled points. Lazy learning
    is also referred to as *instance-based learning* or *nonparametric methods*.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 另一组非常简单的算法是基于邻居的方法。基于邻居的方法是*惰性学习器*，因为它们学习如何根据新点与现有标记点的接近程度来标记新点。与线性回归或逻辑回归不同，基于邻居的模型不会学习一个固定的模型来预测新点的标签；相反，这些模型仅基于新点到预先标记点的距离来预测新点的标签。惰性学习也称为*基于实例的学习*或*非参数方法*。
- en: k-nearest neighbors
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: k 近邻算法
- en: The most common neighborhood-based method is *k-nearest neighbors (KNN)*. To
    label each new point, KNN looks at a *k* number (where *k* is an integer value)
    of nearest labeled points and has these already labeled neighbors vote on how
    to label the new point. By default, KNN uses Euclidean distance to measure what
    is closest.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的基于邻居的方法是 *k 近邻算法 (KNN)*。为了给每个新点贴上标签，KNN 查看 *k* 个最近的已标记点（其中 *k* 是整数），并让这些已标记的邻居投票决定如何给新点贴标签。默认情况下，KNN
    使用欧氏距离来衡量最近的点。
- en: The choice of *k* is very important. If *k* is set to a very low value, KNN
    becomes very flexible, drawing highly nuanced boundaries and potentially overfitting
    the data. If *k* is set to a very high value, KNN becomes inflexible, drawing
    a too rigid boundary and potentially underfitting the data.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*k* 的选择非常重要。如果 *k* 设置得非常低，KNN 变得非常灵活，可能会绘制非常微妙的边界并可能过度拟合数据。如果 *k* 设置得非常高，KNN
    变得不够灵活，绘制出过于刚性的边界，可能会欠拟合数据。'
- en: Strengths
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 优势
- en: Unlike linear methods, KNN is highly flexible and adept at learning more complex,
    nonlinear relationships. Yet, KNN remains simple and interpretable.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于线性方法，KNN 非常灵活，能够学习更复杂、非线性的关系。尽管如此，KNN 仍然简单且可解释。
- en: Weaknesses
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 弱点
- en: KNN does poorly when the number of observations and features grow. KNN becomes
    computationally inefficient in this highly populated, high-dimensional space since
    it needs to calculate distances from the new point to many nearby labeled points
    in order to predict labels. It cannot rely on an efficient model with a reduced
    number of parameters to make the necessary prediction. Also, KNN is very sensitive
    to the choice of *k*. When *k* is set too low, KNN can overfit, and when *k* is
    set too high, KNN can underfit.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 当观测数和特征数量增加时，KNN 的表现较差。在这种高度密集且高维的空间中，KNN 变得计算效率低下，因为它需要计算新点到许多附近已标记点的距离，以预测标签。它无法依靠具有减少参数数量的高效模型进行必要的预测。此外，KNN
    对 *k* 的选择非常敏感。当 *k* 设置过低时，KNN 可能过拟合；当 *k* 设置过高时，KNN 可能欠拟合。
- en: Applications
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 应用场景
- en: KNN is regularly used in recommender systems, such as those used to predict
    taste in movies (Netflix), music (Spotify), friends (Facebook), photos (Instagram),
    search (Google), and shopping (Amazon). For example, KNN can help predict what
    a user will like given what similar users like (known as *collaborative filtering*)
    or what the user has liked in the past (known as *content-based filtering*).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: KNN经常被用于推荐系统，比如用来预测电影品味（Netflix）、音乐喜好（Spotify）、朋友（Facebook）、照片（Instagram）、搜索（Google）和购物（Amazon）。例如，KNN可以帮助预测用户会喜欢什么，基于类似用户喜欢的东西（称为*协同过滤*）或者用户过去喜欢的东西（称为*基于内容的过滤*）。
- en: Tree-Based Methods
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于树的方法
- en: Instead of using a linear method, we can have the AI build a *decision tree*
    where all the instances are *segmented* or *stratified* into many regions, guided
    by the labels we have. Once this segmentation is complete, each region corresponds
    to a particular class of label (for classification problems) or a range of predicted
    values (for regression problems). This process is similar to having the AI build
    rules automatically with the explicit goal of making better decisions or predictions.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是使用线性方法，我们可以让AI构建*决策树*，在这些实例中所有的实例都被*分割*或*分层*成许多区域，这些区域由我们的标签引导。一旦完成这种分割，每个区域对应于一个特定的标签类别（用于分类问题）或预测值范围（用于回归问题）。这个过程类似于让AI自动构建规则，其明确目标是做出更好的决策或预测。
- en: Single decision tree
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单一决策树
- en: The simplest tree-based method is a *single decision tree*, in which the AI
    goes once through the training data, creates rules for segmenting the data guided
    by the labels, and uses this tree to make predictions on the never-before-seen
    validation or test set. However, a single decision tree is usually poor at generalizing
    what it has learned during training to never-before-seen cases because it usually
    overfits the training data during its one and only training iteration.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的基于树的方法是*单一决策树*，在这种方法中，AI一次通过训练数据，根据标签创建数据分割规则，并使用这棵树对从未见过的验证或测试集进行预测。然而，单一决策树通常在将其在训练期间学到的内容推广到从未见过的情况时表现不佳，因为它通常在其唯一的训练迭代期间过拟合训练数据。
- en: Bagging
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 装袋
- en: To improve the single decision tree, we can introduce *bootstrap aggregation*
    (more commonly known as *bagging*), in which we take *multiple random samples
    of instances* from the training data, create a decision tree for each sample,
    and then predict the output for each instance by averaging the predictions of
    each of these trees. By using *randomization* of samples and averaging results
    from multiple trees—an approach that is also known as the *ensemble method*—bagging
    will address some of the overfitting that results from a single decision tree.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 要改进单一决策树，我们可以引入*自助聚合*（更常被称为*装袋*），其中我们从训练数据中取多个随机样本实例，为每个样本创建一个决策树，然后通过平均这些树的预测来预测每个实例的输出。通过*随机化*样本和对多个树的预测结果进行平均——这种方法也被称为*集成方法*——装袋将解决由单一决策树导致的过拟合问题的一些方面。
- en: Random forests
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机森林
- en: We can improve overfitting further by sampling not only the instances but also
    the predictors. With *random forests*, we take multiple random samples of instances
    from the training data like we do in bagging, but, for each split in each decision
    tree, we make the split based not on all the predictors but rather a *random sample
    of the predictors*. The number of predictors we consider for each split is usually
    the square root of the total number of predictors.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过对预测变量进行采样来进一步改善过拟合。通过*随机森林*，我们像在装袋中那样从训练数据中取多个随机样本实例，但是，在每个决策树的每次分割中，我们基于*预测变量的随机样本*而不是所有预测变量进行分割。每次分割考虑的预测变量数量通常是总预测变量数量的平方根。
- en: By sampling the predictors in this way, the random forests algorithm creates
    trees that are even less correlated with each other (compared to the trees in
    bagging), reducing overfitting and improving the generalization error.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式对预测变量进行采样，随机森林算法创建的树与彼此更少相关（与装袋中的树相比），从而减少过拟合并改善泛化误差。
- en: Boosting
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提升法
- en: Another approach, known as *boosting*, is used to create multiple trees like
    in bagging but to *build the trees sequentially*, using what the AI learned from
    the previous tree to improve results on the subsequent tree. Each tree is kept
    pretty shallow, with only a few decision splits, and the learning occurs slowly,
    tree by tree. Of all the tree-based methods, *gradient boosting machines* are
    among the best-performing and are commonly used to win machine learning competitions.^([5](ch01.html#idm140637564424384))
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种称为*提升*的方法用于创建多棵树，类似于装袋法，但是*顺序构建树*，使用AI从前一棵树学到的知识来改进后续树的结果。每棵树保持相当浅，只有几个决策分裂点，并且学习是逐步进行的，树与树之间逐步增强。在所有基于树的方法中，*梯度提升机*是表现最佳的，并且常用于赢得机器学习竞赛。^([5](ch01.html#idm140637564424384))
- en: Strengths
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 优点
- en: Tree-based methods are among the best-performing supervised-learning algorithms
    for prediction problems. These methods are able to capture complex relationships
    in the data by learning many simple rules, one rule at a time. They are also capable
    of handling missing data and categorical features.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的方法是预测问题中表现最佳的监督学习算法之一。这些方法通过逐步学习许多简单规则来捕捉数据中的复杂关系。它们还能够处理缺失数据和分类特征。
- en: Weaknesses
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 弱点
- en: Tree-based methods are difficult to interpret, especially if many rules are
    needed to make a good prediction. Performance also becomes an issue as the number
    of features increase.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的方法很难解释，特别是如果需要许多规则来做出良好的预测。随着特征数量的增加，性能也成为一个问题。
- en: Applications
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 应用
- en: Gradient boosting and random forests are excellent for prediction problems.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升和随机森林在预测问题上表现出色。
- en: Support Vector Machines
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持向量机
- en: Instead of building trees to separate data, we can use algorithms to create
    hyperplanes in space that separate the data, guided by the labels that we have.
    The approach is known as *support vector machines (SVMs)*. SVMs allow some violations
    to this separation—not all the points within an area in hyperspace need to have
    the same label—but the distance between boundary-defining points of a certain
    label and the boundary-defining points of another label should be maximized as
    much as possible. Also, the boundaries do not have to be linear—we can use nonlinear
    kernels to more flexibly separate the data.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用算法在空间中创建超平面来分隔数据，这些算法由我们拥有的标签引导。这种方法被称为*支持向量机（SVMs）*。 SVMs允许在这种分隔中存在一些违规情况——并非超空间中的所有点都必须具有相同的标签——但某一标签的边界定义点与另一标签的边界定义点之间的距离应尽可能最大化。此外，边界不一定是线性的——我们可以使用非线性核来更灵活地分隔数据。
- en: Neural Networks
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络
- en: We can learn representations of the data using neural networks, which are composed
    of an input layer, several hidden layers, and an output layer.^([6](ch01.html#idm140637564409376))
    The input layer uses the features, and the output layer tries to match the response
    variable. The hidden layers are a nested hierarchy of concepts—each layer (or
    concept) is trying to understand how the previous layer relates to the output
    layer.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用神经网络来学习数据的表示，神经网络由输入层、多个隐藏层和输出层组成。^([6](ch01.html#idm140637564409376))
    输入层使用特征，输出层试图匹配响应变量。隐藏层是一个嵌套的概念层次结构——每个层（或概念）都试图理解前一层如何与输出层相关联。
- en: Using this hierarchy of concepts, the neural network is able to learn complicated
    concepts by building them out of simpler ones. Neural networks are one of the
    most powerful approaches to function approximation but are prone to overfitting
    and are hard to interpret, shortcomings that we will explore in greater detail
    later in the book.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种概念层次结构，神经网络能够通过将简单的概念组合起来来学习复杂的概念。神经网络是函数逼近中最强大的方法之一，但容易过拟合且难以解释，我们将在本书后面更详细地探讨这些缺点。
- en: A Closer Look at Unsupervised Algorithms
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探讨无监督算法
- en: We will now turn our attention to problems where we do not have labels. Instead
    of trying to make predictions, unsupervised learning algorithms will try to learn
    the underlying structure of the data.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将注意力转向没有标签的问题。无监督学习算法将尝试学习数据的潜在结构，而不是尝试进行预测。
- en: Dimensionality Reduction
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 降维
- en: One family of algorithms—known as *dimensionality reduction algorithms*—projects
    the original high-dimensional input data to a low-dimensional space, filtering
    out the not-so-relevant features and keeping as much of the interesting ones as
    possible. Dimensionality reduction allows unsupervised learning AI to more effectively
    identify patterns and more efficiently solve large-scale, computationally expensive
    problems (often involving images, video, speech, and text).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 一类算法——称为*降维算法*——将原始高维输入数据投影到低维空间，滤除不那么相关的特征并保留尽可能多的有趣特征。降维允许无监督学习AI更有效地识别模式，并更高效地解决涉及图像、视频、语音和文本的大规模计算问题。
- en: Linear projection
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性投影
- en: There are two major branches of dimensionality—linear projection and nonlinear
    dimensionality reduction. We will start with linear projection first.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 维度的两个主要分支是线性投影和非线性降维。我们将首先从线性投影开始。
- en: Principal component analysis (PCA)
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）
- en: One approach to learning the underlying structure of data is to identify which
    features out of the full set of features are most important in explaining the
    variability among the instances in the data. Not all features are equal—for some
    features, the values in the dataset do not vary much, and these features are less
    useful in explaining the dataset. For other features, the values might vary considerably—these
    features are worth exploring in greater detail since they will be better at helping
    the model we design separate the data.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 学习数据的基本结构的一种方法是确定在完整特征集中哪些特征对解释数据实例之间变异性最重要。并非所有特征都是相等的——对于某些特征，数据集中的值变化不大，这些特征在解释数据集方面不那么有用。对于其他特征，其值可能会有显著变化——这些特征值得更详细探讨，因为它们将更有助于我们设计的模型分离数据。
- en: In *PCA*, the algorithm finds a low-dimensional representation of the data while
    retaining as much of the variation as possible. The number of dimensions we are
    left with is considerably smaller than the number of dimensions of the full dataset
    (i.e., the number of total features). We lose some of the variance by moving to
    this low-dimensional space, but the underlying structure of the data is easier
    to identify, allowing us to perform tasks like clustering more efficiently.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在*PCA*中，该算法在保留尽可能多的变化的同时找到数据的低维表示。我们得到的维度数量远远小于完整数据集的维度数（即总特征数）。通过转移到这个低维空间，我们会失去一些方差，但数据的基本结构更容易识别，这样我们可以更有效地执行诸如聚类之类的任务。
- en: There are several variants of PCA, which we will explore later in the book.
    These include mini-batch variants such as *incremental PCA*, nonlinear variants
    such as *kernel PCA*, and sparse variants such as *sparse PCA*.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: PCA有几种变体，我们将在本书后面探讨。这些包括小批量变体，如*增量PCA*，非线性变体，如*核PCA*，以及稀疏变体，如*稀疏PCA*。
- en: Singular value decomposition (SVD)
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 奇异值分解（SVD）
- en: Another approach to learning the underlying structure of the data is to reduce
    the rank of the original matrix of features to a smaller rank such that the original
    matrix can be recreated using a linear combination of some of the vectors in the
    smaller rank matrix. This is known as *SVD*. To generate the smaller rank matrix,
    SVD keeps the vectors of the original matrix that have the most information (i.e.,
    the highest singular value). The smaller rank matrix captures the most important
    elements of the original feature space.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 学习数据的基本结构的另一种方法是将原始特征矩阵的秩降低到一个较小的秩，使得可以用较小秩矩阵中某些向量的线性组合来重建原始矩阵。这就是*SVD*。为了生成较小秩矩阵，SVD保留具有最多信息（即最高奇异值）的原始矩阵向量。较小秩矩阵捕捉了原始特征空间的最重要元素。
- en: Random projection
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 随机投影
- en: A similar dimensionality reduction algorithm involves projecting points from
    a high-dimensional space to a space of much lower dimensions in such a way that
    the scale of distances between the points is preserved. We can use either a *random
    Gaussian matrix* or a *random sparse matrix* to accomplish this.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的降维算法涉及将高维空间中的点投影到远低于其维度的空间中，以保持点之间的距离比例。我们可以使用*随机高斯矩阵*或*随机稀疏矩阵*来实现这一点。
- en: Manifold learning
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 流形学习
- en: Both PCA and random projection rely on projecting the data linearly from a high-dimensional
    space to a low-dimensional space. Instead of a linear projection, it may be better
    to perform a nonlinear transformation of the data—this is known as *manifold learning*
    or *nonlinear dimensionality reduction*.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: PCA和随机投影都依赖于将数据从高维空间线性投影到低维空间。与线性投影不同，执行数据的非线性变换可能更好——这被称为*流形学习*或*非线性降维*。
- en: Isomap
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Isomap
- en: '*Isomap* is one type of manifold learning approach. This algorithm learns the
    intrinsic geometry of the data manifold by estimating the *geodesic* or *curved
    distance* between each point and its neighbors rather than the Euclidean distance.
    Isomap uses this to then embed the original high-dimensional space to a low-dimensional
    one.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '*Isomap*是一种流形学习方法。该算法通过估计每个点及其邻居之间的*测地线*或*曲线距离*而不是欧氏距离来学习数据流形的内在几何结构。Isomap将此用于将原始高维空间嵌入到低维空间。'
- en: t-distributed stochastic neighbor embedding (t-SNE)
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: t-分布随机近邻嵌入（t-SNE）
- en: Another nonlinear dimensionality reduction—known as *t-SNE*—embeds high-dimensional
    data into a space of just two or three dimensions, allowing the transformed data
    to be visualized. In this two- or three-dimensional space, similar instances are
    modeled closer together and dissimilar instances are modeled further away.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种非线性降维方法——称为*t-SNE*——将高维数据嵌入到仅具有两个或三个维度的空间中，使得转换后的数据可以可视化。在这个二维或三维空间中，相似的实例被建模为更接近，而不相似的实例被建模为更远。
- en: Dictionary learning
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 字典学习
- en: An approach known as *dictionary learning* involves learning the sparse representation
    of the underlying data. These representative elements are simple, binary vectors
    (zeros and ones), and each instance in the dataset can be reconstructed as a weighted
    sum of the representative elements. The matrix (known as the *dictionary*) that
    this unsupervised learning generates is mostly populated by zeros with only a
    few nonzero weights.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 一种被称为*字典学习*的方法涉及学习底层数据的稀疏表示。这些代表性元素是简单的二进制向量（零和一），数据集中的每个实例都可以重构为代表性元素的加权和。这种无监督学习生成的矩阵（称为*字典*）大多数由零填充，只有少数非零权重。
- en: By creating such a dictionary, this algorithm is able to efficiently identify
    the most salient representative elements of the original feature space—these are
    the ones that have the most nonzero weights. The representative elements that
    are less important will have few nonzero weights. As with PCA, dictionary learning
    is excellent for learning the underlying structure of the data, which will be
    helpful in separating the data and in identifying interesting patterns.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 通过创建这样一个字典，该算法能够有效地识别原始特征空间中最显著的代表性元素——这些元素具有最多的非零权重。不太重要的代表性元素将具有较少的非零权重。与PCA一样，字典学习非常适合学习数据的基本结构，这对于分离数据和识别有趣的模式将会有所帮助。
- en: Independent component analysis
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 独立分量分析
- en: One common problem with unlabeled data is that there are many independent signals
    embedded together into the features we are given. Using *independent component
    analysis (ICA)*, we can separate these blended signals into their individual components.
    After the separation is complete, we can reconstruct any of the original features
    by adding together some combination of the individual components we generate.
    ICA is commonly used in signal processing tasks (for example, to identify the
    individual voices in an audio clip of a busy coffeehouse).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 无标签数据的一个常见问题是，许多独立信号被嵌入到我们所获得的特征中。使用*独立分量分析（ICA）*，我们可以将这些混合信号分离成它们的个体组成部分。分离完成后，我们可以通过将我们生成的个体组成部分的某种组合相加来重构任何原始特征。ICA在信号处理任务中通常用于（例如，识别繁忙咖啡馆音频剪辑中的个别声音）。
- en: Latent Dirichlet allocation
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配
- en: Unsupervised learning can also explain a dataset by learning why some parts
    of the dataset are similar to each other. This requires learning unobserved elements
    within the dataset—an approach known as *latent Dirichlet allocation (LDA)*. For
    example, consider a document of text with many, many words. These words within
    a document are not purely random; rather, they exhibit some structure.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习还可以通过学习为什么数据集的某些部分相互类似来解释数据集。这需要学习数据集中的未观察元素——一种被称为*潜在狄利克雷分配（LDA）*的方法。例如，考虑一个文本文档，其中有许多词。文档内的这些词并非纯粹随机；相反，它们呈现出一定的结构。
- en: This structure can be modeled as unobserved elements known as topics. After
    training, LDA is able to explain a given document with a small set of topics,
    where for each topic there is a small set of frequently used words. This is the
    hidden structure the LDA is able to capture, helping us better explain a previously
    unstructured corpus of text.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这种结构可以建模为称为主题的未观察元素。经过训练后，LDA能够用一小组主题解释给定的文档，每个主题都有一小组经常使用的单词。这是LDA能够捕捉的隐藏结构，帮助我们更好地解释以前结构不清晰的文本语料库。
- en: Note
  id: totrans-157
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Dimensionality reduction reduces the original set of features to a smaller set
    of just the most important features. From here, we can run other unsupervised
    learning algorithms on this smaller set of features to find interesting patterns
    in the data (see the next section on clustering), or, if we have labels, we can
    speed up the training cycle of supervised learning algorithms by feeding in this
    smaller matrix of features instead of using the original feature matrix.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 降维将原始特征集合减少到仅包含最重要的特征集合。然后，我们可以在这些较小的特征集上运行其他无监督学习算法，以发现数据中的有趣模式（参见下一节关于聚类的内容），或者如果有标签，我们可以通过向这些较小的特征矩阵输入来加快监督学习算法的训练周期，而不是使用原始特征矩阵。
- en: Clustering
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类
- en: Once we have reduced the set of original features to a smaller, more manageable
    set, we can find interesting patterns by grouping similar instances of data together.
    This is known as clustering and can be accomplished with a variety of unsupervised
    learning algorithms and be used for real-world applications such as market segmentation.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将原始特征集减少到一个更小、更易处理的集合，我们可以通过将相似的数据实例分组来找到有趣的模式。这被称为聚类，可以使用各种无监督学习算法来实现，并且可用于市场细分等现实应用中。
- en: k-means
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: k-means
- en: To cluster well, we need to identify distinct groups such that the instances
    within a group are similar to each other but different from instances in other
    groups. One such algorithm is *k-means clustering*. With this algorithm, we specify
    the number of desired clusters *k*, and the algorithm will assign each instance
    to exactly one of these *k* clusters. It optimizes the grouping by minimizing
    the *within-cluster variation* (also known as *inertia*) such that the sum of
    the within-cluster variations across all *k* clusters is as small as possible.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行良好的聚类，我们需要识别出不同的群组，使得群组内的实例彼此相似，但与其他群组内的实例不同。其中一种算法是*k-means聚类*。使用这种算法，我们指定所需的群组数量*k*，算法将每个实例分配到这*k*个群组中的一个。它通过最小化*群内变异性*（也称为*惯性*）来优化分组，使得所有*k*个群组内的群内变异性之和尽可能小。
- en: To speed up this clustering process, *k*-means randomly assigns each observation
    to one of the *k* clusters and then begins to reassign these observations to minimize
    the Euclidean distance between each observation and its cluster’s center point,
    or *centroid*. As a result, different runs of *k*-means—each with a randomized
    start—will result in slightly different clustering assignments of the observations.
    From these different runs, we can choose the one that has the best separation,
    defined as the lowest total sum of within-cluster variations across all *k* clusters.^([7](ch01.html#idm140637564325008))
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加速这一聚类过程，*k*-means随机将每个观测分配到*k*个群组中的一个，然后开始重新分配这些观测，以最小化每个观测与其群组中心点或*质心*之间的欧氏距离。因此，不同运行的*k*-means（每次都从随机起点开始）将导致略有不同的观测聚类分配。从这些不同的运行中，我们可以选择具有最佳分离性能的运行，即所有*k*个群组内的总群内变异性之和最低的运行。^([7](ch01.html#idm140637564325008))
- en: Hierarchical clustering
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 层次聚类
- en: An alternative clustering approach—one that does not require us to precommit
    to a particular number of clusters—is known as *hierarchical clustering*. One
    version of hierarchical clustering called *agglomerative clustering* uses a tree-based
    clustering method, and builds what is called a *dendrogram*. A dendrogram can
    be depicted graphically as an upside-down tree, where the leaves are at the bottom
    and the tree trunk is at the top.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种聚类方法——不需要预先确定特定群组数量的方法被称为*层次聚类*。层次聚类的一种版本称为*聚合聚类*，使用基于树的聚类方法，并构建所谓的*树状图*。树状图可以以图形方式呈现为倒置的树，其中叶子位于底部，树干位于顶部。
- en: The leaves at the very bottom are individual instances in the dataset. Hierarchical
    clustering then joins the leaves together—as we move vertically up the upside-down
    tree—based on how similar they are to each other. The instances (or groups of
    instances) that are most similar to each other are joined sooner, while the instances
    that are not as similar are joined later. With this iterative process, all the
    instances are eventually linked together forming the single trunk of the tree.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集中，最底部的叶子是个体实例。然后，按照它们彼此的相似程度，层次聚类将这些叶子连接在一起——随着我们在颠倒的树上向上移动。最相似的实例（或实例组）会更早地连接在一起，而不那么相似的实例则稍后连接。通过这个迭代过程，所有实例最终链接在一起，形成了树的单一主干。
- en: This vertical depiction is very helpful. Once the hierarchical clustering algorithm
    has finished running, we can view the dendrogram and determine where we want to
    cut the tree—the lower we cut, the more individual branches we are left with (i.e.,
    more clusters). If we want fewer clusters, we can cut higher on the dendrogram,
    closer to the single trunk at the very top of this upside-down tree. The placement
    of this vertical cut is similar to choosing the number of *k* clusters in the
    *k*-means clustering algorithm.^([8](ch01.html#idm140637564993824))
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这种垂直描绘非常有帮助。一旦层次聚类算法运行完成，我们可以查看树状图并确定我们想要切割树的位置——我们在树干上切割得越低，留下的个体分支就越多（即更多的簇）。如果我们想要更少的簇，我们可以在树状图的更高处切割，靠近这颠倒树的顶部的单一主干。这种垂直切割的位置类似于在*k*-means聚类算法中选择*k*簇的数量。^([8](ch01.html#idm140637564993824))
- en: DBSCAN
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DBSCAN
- en: An even more powerful clustering algorithm (based on the density of points)
    is known as *DBSCAN* (density-based spatial clustering of applications with noise).
    Given all the instances we have in space, DBSCAN will group together those that
    are packed closely together, where close together is defined as a minimum number
    of instances that must exist within a certain distance. We specify both the minimum
    number of instances required and the distance.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个更强大的聚类算法（基于点的密度）称为*DBSCAN*（具有噪声的基于密度的空间聚类应用程序）。给定我们在空间中的所有实例，DBSCAN会将那些紧密聚集在一起的实例分组在一起，其中紧密定义为必须存在一定距离内的最小数量的实例。我们同时指定所需的最小实例数和距离。
- en: If an instance is within this specified distance of multiple clusters, it will
    be grouped with the cluster to which it is most densely located. Any instance
    that is not within this specified distance of another cluster is labeled an outlier.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个实例在指定的距离内接近多个簇，则将其与其最密集的簇分组。任何不在另一个簇指定距离内的实例都被标记为异常值。
- en: Unlike *k*-means, we do not need to prespecify the number of clusters. We can
    also have arbitrarily shaped clusters. DBSCAN is much less prone to the distortion
    typically caused by outliers in the data.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 不像*k*-means那样，我们不需要预先指定簇的数量。我们还可以拥有任意形状的簇。DBSCAN在数据中典型的由异常值引起的扭曲问题上要少得多。
- en: Feature Extraction
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征提取
- en: With unsupervised learning, we can learn new representations of the original
    features of data—a field known as *feature extraction*. Feature extraction can
    be used to reduce the number of original features to a smaller subset, effectively
    performing dimensionality reduction. But feature extraction can also generate
    new feature representations to help improve performance on supervised learning
    problems.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 通过无监督学习，我们可以学习数据原始特征的新表示——一个称为*特征提取*的领域。特征提取可用于将原始特征的数量减少到更小的子集，从而有效地执行降维。但是特征提取也可以生成新的特征表示，以帮助在监督学习问题上提高性能。
- en: Autoencoders
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Autoencoders
- en: To generate new feature representations, we can use a feedforward, nonrecurrent
    neural network to perform representation learning, where the number of nodes in
    the output layer matches the number of nodes in the input layer. This neural network
    is known as an *autoencoder* and effectively reconstructs the original features,
    learning a new representation using the hidden layers in between.^([9](ch01.html#idm140637564978592))
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成新的特征表示，我们可以使用前馈、非循环神经网络进行表示学习，其中输出层中的节点数量与输入层中的节点数量相匹配。这种神经网络被称为*自编码器*，有效地重构原始特征，利用隐藏层之间的学习新的表示。^([9](ch01.html#idm140637564978592))
- en: Each hidden layer of the autoencoder learns a representation of the original
    features, and subsequent layers build on the representation learned by the preceding
    layers. Layer by layer, the autoencoder learns increasingly complicated representations
    from simpler ones.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器的每个隐藏层学习原始特征的表示，后续层基于前面层学习的表示构建。逐层，自编码器从简单表示中学习越来越复杂的表示。
- en: The output layer is the final newly learned representation of the original features.
    This learned representation can then be used as an input into a supervised learning
    model with the objective of improving the generalization error.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层是原始特征的最终新学习表示。这个学习表示然后可以用作监督学习模型的输入，目的是改善泛化误差。
- en: Feature extraction using supervised training of feedforward networks
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用前馈网络的监督训练进行特征提取
- en: If we have labels, an alternate feature extraction approach is to use a feedforward,
    nonrecurrent neural network where the output layer attempts to predict the correct
    label. Just like with autoencoders, each hidden layer learns a representation
    of the original features.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有标签，另一种特征提取方法是使用前馈非递归神经网络，其中输出层试图预测正确的标签。就像自编码器一样，每个隐藏层学习原始特征的表示。
- en: However, when generating the new representations, this network is explicitly
    *guided by the labels*. To extract the final newly learned representation of the
    original features in this network, we extract the penultimate layer—the hidden
    layer just before the output layer. This penultimate layer can then be used as
    an input into any supervised learning model.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在生成新表示时，该网络明确地*由标签引导*。为了从这个网络中提取原始特征的最终新学习表示，我们提取倒数第二层——即输出层之前的隐藏层。然后，可以将这个倒数第二层用作任何监督学习模型的输入。
- en: Unsupervised Deep Learning
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督深度学习
- en: Unsupervised learning performs many important functions in the field of deep
    learning, some of which we will explore in this book. This field is known as *unsupervised
    deep learning*.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习领域，无监督学习执行许多重要功能，其中一些我们将在本书中探讨。这个领域被称为*无监督深度学习*。
- en: Until very recently, the training of deep neural networks was computationally
    intractable. In these neural networks, the hidden layers learn internal representations
    to help solve the problem at hand. The representations improve over time based
    on how the neural network uses the *gradient of the error function* in each training
    iteration to update the weights of the various nodes.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 直到最近，深度神经网络的训练在计算上是棘手的。在这些神经网络中，隐藏层学习内部表示来帮助解决手头的问题。这些表示会随着神经网络在每次训练迭代中如何使用误差函数的梯度来更新各个节点的权重而不断改进。
- en: These updates are computationally expensive, and two major types of problems
    may occur in the process. First, the gradient of the error function may become
    very small, and, since *backpropagation* relies on multiplying these small weights
    together, the weights of the network may update very slowly or not at all, preventing
    proper training of the network.^([10](ch01.html#idm140637564962448)) This is known
    as the *vanishing gradient problem*.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这些更新计算成本很高，过程中可能会出现两种主要类型的问题。首先，误差函数的梯度可能变得非常小，由于*反向传播*依赖于这些小权重的乘积，网络的权重可能更新非常缓慢，甚至根本不更新，从而阻止网络的正确训练。^[10](ch01.html#idm140637564962448)
    这被称为*梯度消失问题*。
- en: Conversely, the other issue is that the gradient of the error function might
    become very large; with backprop, the weights throughout the network may update
    in huge increments, making the training of the network very unstable. This is
    known as the *exploding gradient problem*.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，另一个问题是误差函数的梯度可能变得非常大；通过反向传播，网络中的权重可能会大幅度地更新，使得网络的训练非常不稳定。这被称为*梯度爆炸问题*。
- en: Unsupervised pretraining
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无监督预训练
- en: To address these difficulties in training very deep, multilayered neural networks,
    machine learning researchers train neural networks in multiple, successive stages,
    where each stage involves a shallow neural network. The output of one shallow
    network is then used as the input of the next neural network. Typically, the first
    shallow neural network in this pipeline involves an unsupervised neural network,
    but the later networks are supervised.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决训练非常深、多层神经网络的困难，机器学习研究人员采用多阶段训练神经网络的方法，每个阶段涉及一个浅层神经网络。一个浅层网络的输出被用作下一个神经网络的输入。通常，这个流水线中的第一个浅层神经网络涉及无监督神经网络，但后续的网络是有监督的。
- en: This unsupervised portion is known as *greedy layer-wise unsupervised pretraining*.
    In 2006, Geoffrey Hinton demonstrated the successful application of unsupervised
    pretraining to initialize the training of deeper neural network pipelines, kicking
    off the current deep learning revolution. Unsupervised pretaining allows the AI
    to capture an improved representation of the original input data, which the supervised
    portion then takes advantage of to solve the specific task at hand.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这个无监督部分被称为*贪婪逐层无监督预训练*。2006年，Geoffrey Hinton展示了成功应用无监督预训练来初始化更深神经网络管道的情况，从而开启了当前的深度学习革命。无监督预训练使得AI能够捕获原始输入数据的改进表示，随后监督部分利用这些表示来解决手头的具体任务。
- en: This approach is called “greedy” because each portion of the neural network
    is trained independently, not jointly. “Layer-wise” refers to the layers of the
    network. In most modern neural networks, pretraining is usually not necessary.
    Instead, all the layers are trained jointly using backpropagation. Major computer
    advances have made the vanishing gradient problem and the exploding gradient problem
    much more manageable.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法被称为“贪婪”，因为神经网络的每个部分都是独立训练的，而不是联合训练。 “逐层”指的是网络的各层。在大多数现代神经网络中，通常不需要预训练。相反，所有层都使用反向传播联合训练。主要的计算机进步使得梯度消失问题和梯度爆炸问题变得更加可管理。
- en: Unsupervised pretraining not only makes supervised problems easier to solve
    but also facilitates *transfer learning*. Transfer learning involves using machine
    learning algorithms to store knowledge gained from solving one task to solve another
    related task much more quickly and with considerably less data.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督预训练不仅使监督问题更容易解决，还促进了*迁移学习*。迁移学习涉及使用机器学习算法将从解决一个任务中获得的知识存储起来，以更快速且需要更少数据的方式解决另一个相关任务。
- en: Restricted Boltzmann machines
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 受限玻尔兹曼机
- en: One applied example of unsupervised pretraining is the *restricted Boltzmann
    machine (RBM)*, a shallow, two-layer neural network. The first layer is the input
    layer, and the second layer is the hidden layer. Each node is connected to every
    node in the other layer, but nodes are not connected to nodes of the same layer—this
    is where the restriction occurs.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督预训练的一个应用例子是*受限玻尔兹曼机（RBM）*，一个浅层的双层神经网络。第一层是输入层，第二层是隐藏层。每个节点与另一层的每个节点相连接，但节点与同一层的节点不连接——这就是约束的地方。
- en: RBMs can perform unsupervised tasks such as dimensionality reduction and feature
    extraction and provide helpful unsupervised pretraining as part of supervised
    learning solutions. RBMs are similar to autoencoders but differ in some important
    ways. For example, autoencoders have an output layer, while RBMs do not. We will
    explore these and other differences in detail later in the book.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: RBMs可以执行无监督任务，如降维和特征提取，并作为监督学习解决方案的有用无监督预训练的一部分。RBMs类似于自动编码器，但在某些重要方面有所不同。例如，自动编码器有一个输出层，而RBM则没有。我们将在本书的后续部分详细探讨这些及其他差异。
- en: Deep belief networks
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度信念网络
- en: RBMs can be linked together to form a multistage neural network pipeline known
    as a *deep belief network (DBN)*. The hidden layer of each RBM is used as the
    input for the next RBM. In other words, each RBM generates a representation of
    the data that the next RBM then builds upon. By successively linking this type
    of representation learning, the deep belief network is able to learn more complicated
    representations that are often used as *feature detectors*.^([11](ch01.html#idm140637564940720))
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: RBMs可以连接在一起形成多阶段神经网络管道，称为*深度信念网络（DBN）*。每个RBM的隐藏层被用作下一个RBM的输入。换句话说，每个RBM生成数据的表示，然后下一个RBM在此基础上构建。通过成功地链接这种表示学习，深度信念网络能够学习更复杂的表示，通常用作*特征检测器*。^([11](ch01.html#idm140637564940720))
- en: Generative adversarial networks
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: One major advance in unsupervised deep learning has been the advent of *generative
    adversarial networks (GANs)*, introduced by Ian Goodfellow and his fellow researchers
    at the University of Montreal in 2014\. GANs have many applications; for example,
    we can use GANs to create near-realistic synthetic data, such as images and speech,
    or perform anomaly detection.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督深度学习的一个重大进展是*生成对抗网络（GANs）*的出现，由Ian Goodfellow及其蒙特利尔大学的同事于2014年引入。GANs有许多应用，例如，我们可以使用GANs创建接近真实的合成数据，如图像和语音，或执行异常检测。
- en: In GANs, we have two neural networks. One network—known as the generator—generates
    data based on a model data distribution it has created using samples of real data
    it has received. The other network—known as the discriminator—discriminates between
    the data created by the generator and data from the true data distribution.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在GAN中，我们有两个神经网络。一个网络——生成器——基于其创建的模型数据分布生成数据，该模型数据是通过接收的真实数据样本创建的。另一个网络——鉴别器——区分生成器创建的数据和真实数据分布的数据。
- en: As a simple analogy, the generator is the counterfeiter, and the discriminator
    is the police trying to identify the forgery. The two networks are locked in a
    zero-sum game. The generator is trying to fool the discriminator into thinking
    the synthetic data comes from the true data distribution, and the discriminator
    is trying to call out the synthetic data as fake.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 简单类比，生成器是伪造者，鉴别器是试图识别伪造品的警察。这两个网络处于零和博弈中。生成器试图欺骗鉴别器，让其认为合成数据来自真实数据分布，而鉴别器则试图指出合成数据是假的。
- en: GANs are unsupervised learning algorithms because the generator can learn the
    underlying structure of the true data distribution even when there are no labels.
    GANs learn the underlying structure in the data through the training process and
    efficiently capture the structure using a small, manageable number of parameters.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: GAN（生成对抗网络）是无监督学习算法，因为生成器可以在没有标签的情况下学习真实数据分布的潜在结构。GAN通过训练过程学习数据中的潜在结构，并使用少量可管理的参数高效捕捉这种结构。
- en: This process is similar to the representation learning that occurs in deep learning.
    Each hidden layer in the neutral network of a generator captures a representation
    of the underlying data—starting very simply—and subsequent layers pick up more
    complicated representations by building on the simpler preceding layers.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程类似于深度学习中的表征学习。生成器神经网络中的每个隐藏层通过从简单开始捕捉底层数据的表示，随后的层通过建立在较简单前层的基础上，捕捉更复杂的表示。
- en: Using all these layers together, the generator learns the underlying structure
    of the data and, using what it has learned, the generator attempts to create synthetic
    data that is nearly identical to the true data distribution. If the generator
    has captured the essence of the true data distribution, the synthetic data will
    appear real.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 使用所有这些层，生成器学习数据的潜在结构，并利用所学，尝试创建几乎与真实数据分布相同的合成数据。如果生成器已经捕捉到真实数据分布的本质，合成数据将看起来是真实的。
- en: Sequential Data Problems Using Unsupervised Learning
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用无监督学习处理顺序数据问题
- en: Unsupervised learning can also handle sequential data such as time series data.
    One such approach involves learning the hidden states of a *Markov model*. In
    the *simple Markov model*, states are fully observed and change stochastically
    (in other words, randomly). Future states depend only on the current state and
    are not dependent on previous states.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习也可以处理时间序列等顺序数据。一种方法涉及学习*马尔可夫模型*的隐藏状态。在*简单马尔可夫模型*中，状态完全可观察且随机变化（换句话说，随机）。未来状态仅依赖于当前状态，而不依赖于先前状态。
- en: In a *hidden Markov model*, the states are only partially observable, but, like
    with simple Markov models, the outputs of these partially observable states are
    fully observable. Since the observations that we have are insufficient to determine
    the state completely, we need unsupervised learning to help discover these hidden
    states more fully.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在*隐藏马尔可夫模型*中，状态仅部分可观察，但与简单马尔可夫模型一样，这些部分可观察状态的输出是完全可观察的。由于我们的观察不足以完全确定状态，我们需要无监督学习帮助更充分地发现这些隐藏状态。
- en: Hidden Markov model algorithms involve learning the probable next state given
    what we know about the sequence of previously occurring, partially observable
    states and fully observable outputs. These algorithms have had major commercial
    applications in sequential data problems involving speech, text, and time series.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏马尔可夫模型算法涉及学习给定我们所知的先前发生的部分可观察状态和完全可观察输出的可能下一个状态。这些算法在涉及语音、文本和时间序列的顺序数据问题中具有重要的商业应用。
- en: Reinforcement Learning Using Unsupervised Learning
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用无监督学习进行强化学习
- en: Reinforcement learning is the third major branch of machine learning, in which
    an *agent* determines its optimal behavior (*actions*) in an *environment* based
    on feedback (*reward*) that it receives. This feedback is known as the *reinforcement
    signal*. The agent’s goal is to maximize its cumulative reward over time.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是机器学习的第三大主要分支，其中一个*代理人*根据它收到的*奖励*反馈，决定其在*环境*中的最佳行为（*actions*）。这种反馈称为*强化信号*。代理人的目标是随时间最大化其累积奖励。
- en: While reinforcement learning has been around since the 1950s, it has made mainstream
    headline news only in recent years. In 2013, DeepMind—now owned by Google—applied
    reinforcement learning to achieve superhuman-level performance at playing many
    different Atari games. DeepMind’s system achieved this with just raw sensory data
    as input and no prior knowledge of the rules of the games.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管强化学习自1950年代以来就存在，但直到近年来才成为主流新闻头条。2013年，现为谷歌所有的DeepMind应用强化学习实现了超越人类水平的表现，玩转多种不同的Atari游戏。DeepMind的系统仅使用原始感官数据作为输入，并且没有关于游戏规则的先验知识。
- en: In 2016, DeepMind again captured the imagination of the machine learning community—this
    time the DeepMind reinforcement learning-based AI agent AlphaGo beat Lee Sedol,
    one of the world’s best Go players. These successes have cemented reinforcement
    learning as a mainstream AI topic.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 2016年，DeepMind再次吸引了机器学习社区的想象力——这一次，基于强化学习的AI代理AlphaGo击败了李世石，世界顶级围棋选手之一。这些成功奠定了强化学习作为主流AI主题的地位。
- en: 'Today, machine learning researchers are applying reinforcement learning to
    solve many different types of problems including:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，机器学习研究人员正在应用强化学习来解决许多不同类型的问题，包括：
- en: Stock market trading, in which the agent buys and sells (actions) and receives
    profits or losses (rewards) in return
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 股市交易中，代理人买卖股票（*actions*），并获得利润或损失（*rewards*）作为回报。
- en: Video games and board games, in which the agent makes game decisions (actions)
    and wins or loses (rewards)
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频游戏和棋盘游戏中，代理人做出游戏决策（*actions*），并赢得或输掉（*rewards*）。
- en: Self-driving cars, in which the agent directs the vehicle (actions) and either
    stays on course or crashes (rewards)
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动驾驶汽车中，代理人指导车辆（*actions*），并且要么保持在路线上，要么发生事故（*rewards*）。
- en: Machine control, in which the agent moves about its environment (actions) and
    either completes the course or fails (rewards)
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器控制中，代理人在其环境中移动（*actions*），并且要么完成任务，要么失败（*rewards*）。
- en: In the simplest reinforcement learning problems, we have a finite problem—with
    a finite number of states of the environment, a finite number of actions that
    are possible at any given state of the environment, and a finite number of rewards.
    The action taken by the agent given the current state of the environment determines
    the next state, and the agent’s goal is to maximize its long-term reward. This
    family of problems is known as finite *Markov decision processes*.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在最简单的强化学习问题中，我们有一个有限问题——环境的状态有限，任何给定环境状态下可能的动作有限，并且奖励的数量也是有限的。在给定当前环境状态下，代理人采取的行动决定了下一个状态，代理人的目标是最大化其长期奖励。这类问题称为有限的*马尔可夫决策过程*。
- en: However, in the real world, things are not so simple—the reward is unknown and
    dynamic rather than known and static. To help discover this unknown reward function
    and approximate it as best as possible, we can apply unsupervised learning. Using
    this approximated reward function, we can apply reinforcement learning solutions
    to increase the cumulative reward over time.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在现实世界中，事情并不那么简单——奖励是未知的和动态的，而不是已知的和静态的。为了帮助发现这个未知的奖励函数并尽可能地逼近它，我们可以应用无监督学习。利用这个近似的奖励函数，我们可以应用强化学习解决方案，以增加随时间累积的奖励。
- en: Semisupervised Learning
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 半监督学习
- en: Even though supervised learning and unsupervised learning are two distinct major
    branches of machine learning, the algorithms from each branch can be mixed together
    as part of a machine learning pipeline.^([12](ch01.html#idm140637564904048)) Typically,
    this mix of supervised and unsupervised is used when we want to take full advantage
    of the few labels that we have or when we want to find new, yet unknown patterns
    from unlabeled data in addition to the known patterns from the labeled data. These
    types of problems are solved using a hybrid of supervised and unsupervised learning
    known as semisupervised learning. We will explore this area in greater detail
    later in the book.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管监督学习和无监督学习是机器学习的两个明显不同的主要分支，但每个分支的算法可以作为机器学习流水线的一部分混合在一起。^([12](ch01.html#idm140637564904048))
    通常，在我们想充分利用少数标签或者想从无标签数据中找到新的未知模式以及从标记数据中已知的模式时，我们会混合使用监督和无监督学习。这些类型的问题通过一种称为半监督学习的混合方式来解决。我们将在本书后续章节详细探讨这一领域。
- en: Successful Applications of Unsupervised Learning
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习的成功应用
- en: 'In the last ten years, most successful commercial applications of machine learning
    have come from the supervised learning space, but this is changing. Unsupervised
    learning applications have become more commonplace. Sometimes, unsupervised learning
    is just a means to make supervised applications better. Other times, unsupervised
    learning achieves the commercial application itself. Here is a closer look at
    two of the biggest applications of unsupervised learning to date: anomaly detection
    and group segmentation.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的十年中，大多数成功的商业应用来自监督学习领域，但情况正在改变。无监督学习应用变得越来越普遍。有时，无监督学习只是改善监督应用的手段。其他时候，无监督学习本身就实现了商业应用。以下是迄今为止两个最大的无监督学习应用的更详细介绍：异常检测和群体分割。
- en: Anomaly Detection
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异常检测
- en: Performing dimensionality reduction can reduce the original high-dimensional
    feature space into a transformed lower-dimensional space. In this lower-dimensional
    space, we find where the majority of points densely lie. This portion is the *normal
    space*. Points that lie much farther away are called *outliers*—or *anomalies*—and
    are worth investigating in greater detail.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 进行降维可以将原始的高维特征空间转化为一个转换后的低维空间。在这个低维空间中，我们找到了大多数点密集分布的地方。这部分被称为*正常空间*。远离这些点的点被称为*离群点*或*异常*，值得更详细地调查。
- en: Anomaly detection systems are commonly used for fraud detection such as credit
    card fraud, wire fraud, cyber fraud, and insurance fraud. Anomaly detection is
    also used to identify rare, malicious events such as hacking of internet-connected
    devices, maintenance failures in mission-critical equipment such as airplanes
    and trains, and cybersecurity breaches due to malware and other pernicious agents.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测系统通常用于诸如信用卡欺诈、电汇欺诈、网络欺诈和保险欺诈等欺诈检测。异常检测还用于识别罕见的恶意事件，如对互联网连接设备的黑客攻击，对飞机和火车等关键设备的维护故障，以及由恶意软件和其他有害代理引起的网络安全漏洞。
- en: We can use these systems for spam detection, such as the email spam filter example
    we used earlier in the chapter. Other applications include finding bad actors
    to stop activity such as terrorist financing, money laundering, human and narcotics
    trafficking, and arms dealing, identifying high risk events in financial trading,
    and discovering diseases such as cancer.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些系统用于垃圾邮件检测，例如我们在本章前面使用的电子邮件垃圾过滤器示例。其他应用包括寻找如恐怖主义资金、洗钱、人口和毒品贩运以及军火交易等活动的不良行为，识别金融交易中的高风险事件，以及发现癌症等疾病。
- en: To make the analysis of anomalies more manageable, we can use a clustering algorithm
    to group similar anomalies together and then hand-label these clusters based on
    the types of behavior they represent. With such a system, we can have an unsupervised
    learning AI that is able to identify anomalies, cluster them into appropriate
    groups, and, using the cluster labels provided by humans, recommend to business
    analysts the appropriate course of action.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使异常分析更加可管理，我们可以使用聚类算法将相似的异常分组在一起，然后基于它们所代表的行为类型手动标记这些聚类。通过这样的系统，我们可以拥有一个能够识别异常、将它们聚类到适当组中，并且利用人类提供的聚类标签向业务分析师推荐适当行动的无监督学习人工智能。
- en: With anomaly detection systems, we can take an unsupervised problem and eventually
    create a semisupervised one with this cluster-and-label approach. Over time, we
    can run supervised algorithms on the labeled data alongside the unsupervised algorithms.
    For successful machine learning applications, unsupervised systems and supervised
    systems should be used in conjunction, complementing one another.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 通过异常检测系统，我们可以将一个无监督问题逐步转换为半监督问题，通过这种集群和标记的方法。随着时间的推移，我们可以在未标记数据上运行监督算法，并行进行无监督算法。对于成功的机器学习应用程序，无监督系统和监督系统应该同时使用，相辅相成。
- en: The supervised system finds the known patterns with a high level of accuracy,
    while the unsupervised system discovers new patterns that may be of interest.
    Once these patterns are uncovered by the unsupervised AI, the patterns are labeled
    by humans, transitioning more of the data from unlabeled to labeled.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 监督系统以高精度找到已知模式，而无监督系统发现可能感兴趣的新模式。一旦这些模式被无监督AI揭示，人类会对这些模式进行标记，将更多数据从未标记转换为已标记。
- en: Group segmentation
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 群体分割
- en: With clustering, we can segment groups based on similarity in behavior in areas
    such as marketing, customer retention, disease diagnosis, online shopping, music
    listening, video watching, online dating, social media activity, and document
    classification. The amount of data that is generated in each of these areas is
    massive, and the data is only partially labeled.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 通过聚类，我们可以根据行为相似性在市场营销、客户保持、疾病诊断、在线购物、音乐收听、视频观看、在线约会、社交媒体活动和文档分类等领域中对群体进行分割。在这些领域产生的数据量非常庞大，且数据只有部分被标记。
- en: For patterns that we already know and want to reinforce, we can use supervised
    learning algorithms. But often we want to discover new patterns and groups of
    interest—for this discovery process, unsupervised learning is a natural fit. Again,
    it is all about synergy. We should use supervised and unsupervised learning systems
    in conjunction to build a stronger machine learning solution.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们已经了解并希望加强的模式，我们可以使用监督学习算法。但通常我们希望发现新的模式和感兴趣的群体——对于这一发现过程，无监督学习是一个自然的选择。再次强调，这一切都是关于协同作用。我们应该同时使用监督和无监督学习系统来构建更强大的机器学习解决方案。
- en: Conclusion
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'In this chapter, we explored the following:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了以下内容：
- en: The difference between a rules-based system and machine learning
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于规则的系统和机器学习的区别
- en: The difference between supervised and unsupervised learning
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习和无监督学习的区别
- en: How unsupervised learning can help address common problems in training machine
    learning models
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习如何帮助解决训练机器学习模型中的常见问题
- en: Common algorithms for supervised, unsupervised, reinforcement, and semisupervised
    learning
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见的监督、无监督、强化和半监督学习算法
- en: Two major applications of unsupervised learning—anomaly detection and group
    segmentation
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习的两个主要应用——异常检测和群体分割
- en: In [Chapter 2](ch02.html#Chapter_2), we’ll explore how to build machine learning
    applications. Then, we will cover dimensionality reduction and clustering in detail,
    building an anomaly detection system and a group segmentation system in the process.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](ch02.html#Chapter_2)中，我们将探讨如何构建机器学习应用程序。然后，我们将详细讨论降维和聚类，逐步构建异常检测系统和群体分割系统。
- en: ^([1](ch01.html#idm140637564299232-marker)) There are startups such as Figure
    Eight that explicitly provide this *human in the loop* service.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch01.html#idm140637564299232-marker)) 有像Figure Eight这样明确提供*人在循环*服务的初创企业。
- en: ^([2](ch01.html#idm140637564293600-marker)) Underfitting is another problem
    that may occur in building machine learning applications, but this is easier to
    solve. Underfitting occurs because the model is too simple—the algorithm cannot
    build a complex enough function approximation to make good decisions for the task
    at hand. To solve this, we can allow the algorithm to grow in size (have more
    parameters, perform more training iterations, etc.) or apply a more complicated
    machine learning algorithm.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch01.html#idm140637564293600-marker)) 欠拟合是在构建机器学习应用程序时可能出现的另一个问题，但这更容易解决。欠拟合是因为模型过于简单——算法无法构建足够复杂的函数逼近来为当前任务做出良好的决策。为了解决这个问题，我们可以允许算法增加规模（增加参数、执行更多训练迭代等）或者应用更复杂的机器学习算法。
- en: ^([3](ch01.html#idm140637564244608-marker)) This list is by no means exhaustive
    but does include the most commonly used machine learning algorithms.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch01.html#idm140637564244608-marker)) 这个列表并非详尽无遗，但包含了最常用的机器学习算法。
- en: ^([4](ch01.html#idm140637564235360-marker)) There may be other potential issues
    that might make linear regression a poor choice, including outliers, correlation
    of error terms, and nonconstant variance of error terms.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch01.html#idm140637564235360-marker)) 可能有其他潜在问题会使得线性回归成为一个不好的选择，包括异常值、误差项相关性以及误差项方差的非常数性。
- en: ^([5](ch01.html#idm140637564424384-marker)) For more on gradient boosting in
    machine learning competitions, consult Ben Gorman’s [blog post](http://bit.ly/2S1C8Qy).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch01.html#idm140637564424384-marker)) 想要了解机器学习竞赛中梯度提升的更多信息，请查阅Ben Gorman的[博客文章](http://bit.ly/2S1C8Qy)。
- en: ^([6](ch01.html#idm140637564409376-marker)) For more on neutral networks, check
    out [*Deep Learning*](http://www.deeplearningbook.org/) by Ian Goodfellow, Yoshua
    Bengio, and Aaron Courville (MIT Press).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch01.html#idm140637564409376-marker)) 想要了解更多关于神经网络的信息，请参阅Ian Goodfellow、Yoshua
    Bengio和Aaron Courville的《*深度学习*》（MIT Press）。
- en: ^([7](ch01.html#idm140637564325008-marker)) There are faster variants of *k*-means
    clustering such as mini-batch *k*-means, which we cover later in the book.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch01.html#idm140637564325008-marker)) *k*-均值聚类的快速变体包括小批量*k*-均值，我们稍后在书中进行介绍。
- en: ^([8](ch01.html#idm140637564993824-marker)) Hierarchical clustering uses Euclidean
    distance by default, but it can also use other similarity metrics such as correlation-based
    distance, which we will explore in greater detail later in the book.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch01.html#idm140637564993824-marker)) 分层聚类默认使用欧几里得距离，但也可以使用其他相似度度量，比如基于相关的距离，我们稍后在书中详细探讨。
- en: ^([9](ch01.html#idm140637564978592-marker)) There are several types of autoencoders,
    and each learns a different set of representations. These include denoising autoencoders,
    sparse autoencoders, and variational autoencoders, all of which we will explore
    later in the book.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch01.html#idm140637564978592-marker)) 有几种类型的自编码器，每种都学习不同的表示。这些包括去噪自编码器、稀疏自编码器和变分自编码器，我们稍后在书中进行探讨。
- en: ^([10](ch01.html#idm140637564962448-marker)) Backpropagation (also known as
    *backward propagation of errors*) is a gradient descent-based algorithm used by
    neural networks to update weights. In backprop, the weights of the final layer
    are calculated first and then used to update the weights of the preceding layers.
    This process continues until the weights of the very first layer are updated.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch01.html#idm140637564962448-marker)) 反向传播（也称为*误差反向传播*）是神经网络使用的基于梯度下降的算法，用于更新权重。在反向传播中，首先计算最后一层的权重，然后用于更新前面层的权重。这个过程一直持续到第一层的权重被更新。
- en: ^([11](ch01.html#idm140637564940720-marker)) Feature detectors learn good representations
    of the original data, helping separate distinct elements. For example, in images,
    feature detectors help separate elements such as noses, eyes, mouths, etc.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch01.html#idm140637564940720-marker)) 特征检测器学习原始数据的良好表示，帮助分离不同的元素。例如，在图像中，特征检测器帮助分离鼻子、眼睛、嘴等元素。
- en: ^([12](ch01.html#idm140637564904048-marker)) Pipeline refers to a system of
    machine learning solutions that are applied in succession to achieve a larger
    objective.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch01.html#idm140637564904048-marker)) Pipeline 指的是一种机器学习解决方案的系统，这些解决方案依次应用以实现更大的目标。
