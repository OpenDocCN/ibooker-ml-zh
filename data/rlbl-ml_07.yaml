- en: Chapter 6\. Fairness, Privacy, and Ethical ML Systems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章 公平性、隐私和道德机器学习系统
- en: By Aileen Nielsen
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 作者：艾琳·尼尔森
- en: This chapter is devoted to topics related to ethical considerations and legal
    obligations when creating or deploying ML systems. We cannot offer an exhaustive
    source of guidance on these topics, but this resource can point you in the right
    direction. At the end of this chapter, you should have a good sense of fundamental
    ethical considerations for ML deployment as well as concrete language and conceptual
    categories that will get you started in educating yourself more thoroughly in
    the domains most immediately applicable to your own work.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了在创建或部署机器学习系统时涉及的道德考虑和法律义务相关的主题。我们无法在这些主题上提供详尽的指导，但这份资源可以为您指明正确的方向。在本章结束时，您应该对机器学习部署的基本道德考虑有了良好的理解，以及可以让您开始在与您自己工作最直接相关的领域进行更深入教育的具体语言和概念类别。
- en: Note
  id: totrans-3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Editor’s note: When we put together the list of topics for what MLOps folks
    truly need to know, issues of fairness, privacy, and ethical concerns in AI and
    ML systems were right at the top of the list. However, we also knew that it was
    difficult for a group of authors with strong industry affiliations to provide
    truly unbiased views on these complex issues. Therefore, we invited Aileen Nielsen,
    author of [*Practical Fairness*](https://oreil.ly/tsjGP) (O’Reilly, 2020), to
    contribute this chapter independently. While we gave feedback on drafts for clarity,
    the views here are entirely hers, and she had full editorial control on this chapter.
    You’re getting it straight from a world-class expert!'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 编者注：当我们整理 MLOps 需要真正了解的主题列表时，公平性、隐私和人工智能以及机器学习系统中的道德问题位居榜首。然而，我们也知道，一个具有强烈行业背景的作者团队很难提供关于这些复杂问题的真正无偏见的观点。因此，我们邀请了艾琳·尼尔森（Aileen
    Nielsen），《实用公平性》（O'Reilly，2020）的作者，独立撰写这一章节。虽然我们就清晰性提供了草稿反馈，但这里的观点完全是她的，她对本章有完全的编辑控制权。你将直接从一个世界级专家那里获取这些信息！
- en: We should also note from the outset that fairness and ethics in AI remain highly
    contested topics. Indeed, one reasonable position right now is that a quite viable
    approach to promoting fairness in computing systems is *not* to use AI/ML. However,
    for those who find themselves compelled to do so, or who believe that this skepticism
    of algorithmic solutions can be overcome in their specific use cases, this chapter
    provides a starting point for understanding the challenges of how to do it right.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应该从一开始就指出，人工智能和机器学习中的公平性和道德仍然是高度争议的话题。事实上，目前一个合理的立场是，在计算系统中促进公平性的一种相当可行的方法
    *不* 使用人工智能/机器学习。然而，对于那些发现自己被迫这样做，或者相信在特定使用案例中可以克服算法解决方案怀疑的人来说，本章提供了理解如何正确执行的起点。
- en: What’s more, it should be recognized that in some cases existing or otherwise
    traditional solutions to problems in the pre-algorithmic age (e.g., having designated
    human decision makers, or no clear decision makers) haven’t always been great.^([1](ch06.xhtml#ch01fn42))
    For example, a great deal of empirical research suggests that race influences
    judges’ sentencing decisions and even informal decisions by store clerks as to
    whether to allow customers to return items. Therefore, when we think about fairness
    and ethics in AI and ML, we also have to recognize that they may very well provide
    improvements in some cases relative to human decision makers. So, despite the
    gloom and doom you will find in this chapter, we also recognize from the outset
    that some uses of algorithms have been tremendously successful in terms of increasing
    overall fairness, even if there isn’t yet a clear solution to making AI and ML
    fair and just in a guaranteed or global sense.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，需要认识到，在某些情况下，现有或传统的问题解决方案在算法前时代（例如，指定人类决策者，或没有明确的决策者）并不总是很好。[^1](ch06.xhtml#ch01fn42)
    例如，大量经验研究表明，种族影响法官的判决决策，甚至店员关于是否允许顾客退货的非正式决策。因此，当我们考虑人工智能和机器学习中的公平性和道德时，我们也必须承认，在某些情况下，它们可能确实相对于人类决策者有所改进。因此，尽管本章中可能会发现一些忧愁和悲观情绪，但我们也从一开始就认识到，虽然尚无明确的解决方案能够使人工智能和机器学习在全球范围内保持公平和正义，但有些算法的使用确实在提高整体公平性方面取得了巨大成功。
- en: In this chapter, you will find sections that focus on specific hot topics—notably,
    fairness, privacy, and Responsible AI. We recognize the primacy of these topics
    both in terms of public awareness and concern as well as in terms of attention
    devoted from industry and scholarly groups alike. A lot of development is occurring
    in these areas, and we want to empower you to dive into these topics with a good
    initial background that you can pick up from this chapter. Within this chapter,
    we also provide notes on how you might consider refactoring work at your organization,
    in practical ways, so as to enhance fairness and ethics in your own AI/ML work.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节中，你会发现专注于特定热门话题的部分，特别是公平性、隐私和负责任人工智能。我们认识到这些话题在公众意识和关注度以及工业和学术界的关注度方面的重要性。在这些领域正在发生许多发展，我们希望能够为你提供一个良好的初始背景，使你能从本章节中获取这些话题的信息。在本章中，我们还提供了关于如何在你的组织中考虑重构工作的笔记，以实际方式增强你在AI/ML工作中的公平性和道德性。
- en: Fairness (a.k.a. Fighting Bias)
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 公平性（也称为抗击偏见）
- en: '*Algorithmic fairness*, and other variations of this term, are a hot topic
    in ML and have been for many years. When you read about this subject, you’ll most
    often see fairness used as a concept directly related to bias—that is, as fairness
    being the absence of bias, and bias being the condition of unfairness.^([2](ch06.xhtml#ch01fn43))'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*算法公平性*及其各种变体，长期以来一直是机器学习中的热门话题。当你阅读关于这个主题的内容时，最常见的是公平性被用作直接与偏见相关的概念——即公平性被定义为没有偏见，而偏见则是不公平的状态。^([2](ch06.xhtml#ch01fn43))'
- en: For quite some time, ML researchers, legal scholars, and activists alike have
    manifested growing concern about the possibility that ML could perpetuate existing
    social biases, or even create new forms of bias. In framing such discussions,
    many have emphasized that this could happen because data used to train ML systems
    could be taken from biased systems or collected in a biased way. In short, people
    have made arguments akin to the notion of garbage in, garbage out.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 长期以来，机器学习研究人员、法律学者和活动家们一直对机器学习可能会延续现有社会偏见，甚至创造新的偏见形式表示关注。在这些讨论中，许多人强调，这可能发生的原因是用于训练机器学习系统的数据可能来自有偏见的系统或以有偏见的方式收集。简而言之，人们提出了类似于“垃圾进，垃圾出”的观点。
- en: 'But you should be careful to realize that the *garbage in* does not necessarily
    refer to bad data. The garbage is the *outcome* that results from unrepresentative
    training data or algorithmic bias. This is a key point: it’s important to realize
    that the *fundamental* source of bias can come from many steps in the ML pipeline,
    including bad data, but also including bad modeling choices. Here’s a nonexhaustive
    list of some commonly cited sources of bias:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 但你应当注意到，“垃圾进”并不一定指的是糟糕的数据。垃圾指的是由于不具代表性的训练数据或算法偏见而导致的*结果*。这是一个关键点：重要的是要意识到偏见的根本源头可以来自机器学习流程中的多个步骤，包括糟糕的数据，也包括糟糕的建模选择。以下是一些常被引用的偏见来源的非详尽列表：
- en: Sampling bias
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 抽样偏见
- en: In sampling bias, the process of collecting data itself has bias built into
    a system. A commonly given example is that marijuana use among white and Black
    Americans is thought to be roughly equal, but the rate of arrests for marijuana
    possession is far higher for Black Americans than for white Americans. This is
    almost certainly due to (among other reasons) sampling bias. Because of racism
    manifesting at both the level of individual decisions and at a systemic level,
    Black Americans are far more likely to find themselves searched by police for
    marijuana possession than are white Americans (some may be familiar with a related
    manifestation of racism, [driving while Black](https://oreil.ly/4w8oQ)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在抽样偏见中，数据收集过程本身就带有系统性偏见。一个常见的例子是，白人和黑人美国人之间的大麻使用率被认为大致相等，但是关于大麻持有的逮捕率对黑人美国人远高于白人美国人。这几乎可以肯定是由于抽样偏见造成的（除其他原因外）。由于种族主义在个体决策和系统层面上的显现，黑人美国人要比白人美国人更有可能因大麻持有而被警方搜索（有些人可能熟悉相关的种族主义表现，[开车时是黑人](https://oreil.ly/4w8oQ)）。
- en: '*ML-relevant example:*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*与机器学习相关的例子：*'
- en: Crime-prediction algorithms, such as those built to influence police patrolling
    allocations, likely suffer from sampling bias.^([3](ch06.xhtml#ch01fn44)) Across
    many nations and continents, it remains a consistent pattern that policing is
    directed at communities of low socioeconomic status. Data coming from such a process
    oversamples crime in certain areas or among certain demographics and so likely
    misrepresents the underlying base rates of crime in different communities due
    to this sampling bias. Yet, this data, sampled in a biased fashion, then creates
    new biased inputs for more ML modeling to allocate future police patrols. The
    algorithm likely keeps sending police back to the same areas too often because
    other areas are not oversampled in the same way.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 犯罪预测算法，例如那些用来影响警察巡逻分配的算法，很可能受到抽样偏差的影响。^([3](ch06.xhtml#ch01fn44)) 在许多国家和大陆，警务工作往往集中在低社会经济地位的社区。由于这种过程产生的数据在某些地区或人口群体中过度抽样犯罪，因此很可能在不同社区中误代表犯罪的基础率。然而，这种以偏见方式抽样的数据，又为更多机器学习模型分配未来警务巡逻创建了新的偏见输入。由于这种算法，警察往往会过于频繁地回到同一地区，因为其他地区没有以同样的方式过度抽样。
- en: Disparate treatment
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 不平等对待
- en: Bias can result from individuals explicitly being treated differently. Disparate
    treatment is banned both by the government and by some regulated areas of the
    private sector when there is no justified reason for it. In almost all examples
    of disparate treatment on the basis of race (such as segregated schools or policies
    of hiring only whites), courts have not found a justified reason for such discrimination
    (after the Civil Rights movement). For disparate treatment on the basis of gender,
    courts have sometimes found that reasons offered to treat genders differently
    (such as different performance thresholds for physical fitness tests) could be
    justified by compelling interests.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 偏见可能源于对个体明确地进行不同对待。当没有合理理由时，政府和某些私营部门的规范区域都禁止不平等对待。在几乎所有关于基于种族的不平等对待的例子中（如种族隔离学校或只雇佣白人的政策），法院都没有找到正当理由支持这种歧视（民权运动后）。在基于性别的不平等对待中，法院有时认为为性别不同的理由（例如对体能测试的不同表现门槛）可能由于强制性利益而被证明是正当的。
- en: '*ML-relevant example:*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*ML相关示例：*'
- en: ML algorithms are often celebrated for their ability to spot patterns that elude
    humans. However, sometimes these patterns are plain old sexism. Amazon [famously
    developed but did not deploy](https://oreil.ly/zyKHl) an in-house hiring algorithm
    that applied a strong negative parameter for attending a woman’s college.^([4](ch06.xhtml#ch01fn45))
    Thus we see a case of an algorithm using irrelevant factors to make a decision
    in a way that directly discriminates against a group (in this case, women going
    to women’s colleges).^([5](ch06.xhtml#ch01fn46)) If the algorithm had been used,
    this would have looked like a case of disparate treatment.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法常常因其发现超越人类能力的模式而受到赞誉。然而，有时这些模式只是显而易见的性别歧视。亚马逊曾开发但未部署^([4](ch06.xhtml#ch01fn45))
    一种内部招聘算法，该算法对就读女子学院的女性应聘者应用了强烈的负参数。^([5](ch06.xhtml#ch01fn46)) 因此，我们看到了算法在做出决策时使用无关因素，直接对一个群体（在这种情况下是去女子学院的女性）进行歧视的案例。如果这种算法被使用，这将看起来像是一种不平等对待的案例。
- en: Systemic bias
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 制度性偏见
- en: This source of bias can be challenging to both identify and mitigate as compared
    to the preceding examples in this list. Broadly, we can think of systemic bias
    as a host of factors that likely cannot be identified in individual cases as explanatory
    features but that, on an aggregate level, clearly influence differences in outcomes
    for individuals due to the structural limitations (limitations that are baked
    into our social, educational, and employment systems, among others). AI systems
    cannot be separated from the societal contexts in which they are built, so AI
    built without an understanding of context is likely to worsen systemic bias. Indeed,
    even the particular problems to which AI is applied are often highlighted as themselves
    manifestations of systemic bias. For example, some have asked why predictive policing
    has been actively used for so long to predict crimes among socioeconomically vulnerable
    populations, while algorithmic systems to identify or predict *police* misbehavior
    are relatively uncommon.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 与前述例子相比，这种偏见的来源在识别和减轻方面可能更具挑战性。广义上来说，我们可以将系统偏见视为许多因素的集合，这些因素很可能在个别情况下不能作为解释特征被识别，但在集合层面上，显然影响个人结果的差异，这些差异是由于结构性限制（这些限制已经融入到我们的社会、教育和就业系统中，等等）。人工智能系统无法脱离其构建的社会背景，因此没有理解背景的人工智能可能会加剧系统偏见。事实上，即使是人工智能应用的特定问题本身，通常也被视为系统偏见的表现。例如，一些人问为什么长期以来积极使用预测警务来预测社会经济弱势群体中的犯罪，而用于识别或预测警察不当行为的算法系统相对较少。
- en: '*ML-relevant example:*'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*与机器学习相关的例子：*'
- en: In training an ML system, sometimes features are selected because they seem
    to provide “common sense” examples of “merit” as some employers or educational
    institutions might conceptualize that notion. For example, middle class high school
    students are often told to participate in extracurricular activities to show that
    they are highly motivated leaders. However, using data about participation in
    extracurricular activities (in an algorithm or in human decision making) without
    additional context would contribute to a system that rewards middle class students
    who have access to such possibilities. This system would likewise disadvantage
    low-income students whose schools may not offer such activities or who are not
    able to participate in such activities because they need to work or contribute
    to family obligations. Thus what “common sense” dictates to people unfamiliar
    with different social contexts and unversed in the consequences of systemic bias
    could look quite biased once this additional information is incorporated into
    the analysis.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在培训机器学习系统时，有时会选择某些特征，因为它们似乎提供了“常识”示例，例如某些雇主或教育机构可能概念化了这种观念的“优点”。例如，中产阶级高中学生经常被告知要参加课外活动，以显示他们是高度积极的领导者。然而，如果在算法或人类决策中使用有关参与课外活动的数据，没有额外的背景信息，这将促成一个奖励中产阶级学生的系统，因为他们能够接触到这样的可能性。这种系统同样会使得无法参加这些活动的低收入学生处于劣势，因为他们需要工作或履行家庭义务。因此，对于不熟悉不同社会背景并且不了解系统偏见后果的人来说，所谓的“常识”一旦融入到分析中，可能看起来相当偏见。
- en: Tyranny of the majority
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 多数暴政
- en: This source of bias relates to the form of training process that is used. In
    many commonly used modeling systems, the numerical majority category typically
    affects training loss the most through total numbers. Therefore, if the modeling
    process does not account for this, many kinds of models will de facto favor the
    majority and minimize the error for majority groups more directly than for minority
    groups. This same concern sometimes motivates the design of political systems
    so as to protect minority interests specifically rather than trusting to majority
    rule.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这种偏见的来源与使用的培训过程形式相关。在许多常用的建模系统中，数值上的多数类别通常通过总数对训练损失影响最大。因此，如果建模过程没有考虑到这一点，许多类型的模型实际上会偏向于多数群体，并且相比于少数群体，更直接地将错误最小化。同样的担忧有时会激励设计政治系统，以保护少数群体的利益，而不是仅仅依靠多数规则。
- en: '*ML-relevant example:*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*与机器学习相关的例子：*'
- en: It is well known that training ML systems with data that does not involve relatively
    equal distributions of characteristics (imbalanced datasets) can be quite challenging
    if we seek to achieve good performance for all classes. Such a situation is likely
    to arise when working with datasets that include different kinds of people, as
    most contain some kind of imbalance in data, be it gender imbalance, racial imbalance,
    or other forms of human diversity, such as geography or language. Therefore, modeling
    human behavior should require paying particular attention to make sure that the
    model is giving everyone a fair shake rather than modeling everyone according
    to a prototype that in fact is reasonably accurate only for someone in the majority
    class.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，使用不涉及相对平等特征分布的数据来训练机器学习系统（不平衡数据集）可能会非常具有挑战性，如果我们希望为所有类别实现良好的性能。当处理包含不同类人群的数据集时，这种情况很可能会发生，因为大多数数据集在性别、种族或其他人类多样性形式（如地理或语言）上都存在某种形式的数据不平衡。因此，建模人类行为应该特别注意确保模型对每个人都公平对待，而不是根据事实上只对大多数类别中的某个人相对准确的原型进行建模。
- en: Tragically, it remains indisputable that these forms of bias (and many others)
    continue to pop up in a wide range of ML applications, even with increasing media
    attention about algorithmic fairness. There are many reasons for this. Social
    biases are widespread and are not always apparent at the individual level. Someone
    might think they are making an unbiased decision both because they do not have
    all the details about the system in which they are making the decision, and because
    of their own unconscious biases. This is not a problem that will be solved by
    a book chapter, but one that a book chapter can make readers aware of and motivated
    to do something about.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这些偏见形式（及其他许多形式）在广泛的机器学习应用中仍然频繁出现，即使有关算法公平性的媒体关注日益增加。这种情况存在许多原因。社会偏见普遍存在，并不总是在个体层面上显现出来。某人可能认为自己在做出不带偏见的决定，既因为他们没有关于做出决定的系统所有细节，也因为他们自己的无意识偏见。这不是一个可以通过一章书解决的问题，但却是一章书可以让读者意识到并且有动力采取行动的问题。
- en: The ML/AI research community has been developing methods and techniques to systematically
    identify and remediate some of these biases. Practitioners who are trying to develop
    responsible and fair AI systems should be aware of these emerging tools that might
    help. What’s more, ML/AI may very well provide a way forward to make systems fairer
    than they have been with the use of unaccountable humans, so long as ML development
    is done carefully and with appropriate safeguards, when aimed at appropriate problems.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习/人工智能研究社区一直在开发方法和技术，系统地识别和纠正这些偏见。试图开发负责任和公平的人工智能系统的从业者应该了解这些新兴工具可能会有所帮助。更重要的是，机器学习/人工智能很可能提供一种前进的方式，使系统比使用不负责任的人类更加公平，只要机器学习的开发是谨慎的，并在针对适当问题时带有适当的保障。
- en: Definitions of Fairness
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 公平性的定义
- en: In the ML community, we have not cohered around a single definition of fairness,
    but common categories exist that have intuitive and appealing descriptions. For
    example, some definitions of *fairness* emphasize individual fairness. These notions
    of fairness argue that, for two individuals who are “the same” other than irrelevant
    factors (race, gender, etc.), these individuals should be treated the same.^([6](ch06.xhtml#ch01fn47))
    Other definitions of fairness emphasize that ML should look to fairness at the
    level of groups. That is, error rates should be the same and of the same quality
    across groups, and overall the level of performance should likewise be equally
    high among groups. Still other definitions of fairness might get more complicated
    and look to establish causal mechanisms to understand what might drive individuals
    to success or failure, before looking to categorize them algorithmically.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习社区中，我们尚未围绕公平性形成单一的定义，但存在一些常见类别，这些类别具有直观和吸引人的描述。例如，一些关于*公平性*的定义强调个体公平。这些公平性的概念认为，对于除了无关因素（种族、性别等）外“相同”的两个个体，这些个体应该被同等对待。^([6](ch06.xhtml#ch01fn47))
    其他公平性的定义强调机器学习应该在群体层面上看待公平性。也就是说，错误率应该在各个群体中相同且质量相同，并且总体上性能水平在各个群体中也应该同样高。还有其他的公平性定义可能会更加复杂，并试图建立因果机制来理解是什么驱使个体成功或失败，然后再试图算法地对其进行分类。
- en: Two common categories of fairness that are intuitive and appealing are group
    parity and calibration-based assessments. These are far from being the only notions
    of fairness, but we select these two to discuss for two reasons. First, each of
    these definitions is straightforward and has a strong intuitive appeal. Second,
    each highlights two distinct notions of fairness.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 两种常见的公平性类别是直观和吸引人的组公平和基于校准的评估。这些远非公平性的唯一概念，但我们选择这两种讨论的原因有两个。首先，每个定义都很直观，并且具有很强的吸引力。其次，每个都突出了公平的两个不同概念。
- en: As its name suggests, *group parity* emphasizes fairness as matter to be compared
    on the basis of comparing groups. It puts emphasis on recognizing that outcomes
    for individuals should be looked at, at least in part, on the basis of sensitive
    attributes of their identity, and most particularly on legally protected attributes,
    such as gender and race, but also on attributes that may not be legally protected
    but that some consider morally important, such as economic status. There are good
    reasons for this, including that when we look at the world, such features often
    turn out to be horribly influential in actual outcomes.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名，*组公平* 强调公平作为基础上组进行比较的问题。它强调应该至少部分地基于他们身份的敏感属性来审视个体的结果，尤其是法律保护的属性，如性别和种族，但也包括一些可能不受法律保护但某些人认为在道德上重要的属性，如经济地位。这样做有很好的理由，包括我们在看世界时，这些特征通常在实际结果中影响深远。
- en: On the other hand, *calibration* emphasizes individual fairness, which is another
    value most of us find quite intuitive and appealing. We likewise feel strongly
    that individuals should be treated on the basis of who they are and what they
    do as individuals, and not based on where they come from.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，*校准* 强调个体公平，这是我们大多数人都认为相当直观和吸引人的另一个价值观。同样，我们坚信个体应该根据他们是谁以及他们作为个体的所做所为来对待，而不是基于他们来自何处。
- en: It would seem that these two definitions of fairness need not be at odds philosophically.
    In a perfect world, where groups and individuals are treated fairly, we would
    get the same outcomes. Unfortunately, this turns out not to be the case in our
    imperfect world, for a variety of reasons, including fundamental mathematical
    limitations.^([7](ch06.xhtml#ch01fn48)) For this reason—at least for now—practitioners
    must choose their definition of fairness. As we will describe, different use cases
    will have different reasonable definitions of fairness.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在哲学上看来，这两种公平性定义似乎不需要互相对立。在一个完美的世界中，群体和个体都得到公平对待，我们会得到相同的结果。不幸的是，在我们不完美的世界中，由于各种原因，包括基本的数学限制，情况并非如此。（参见
    [7](ch06.xhtml#ch01fn48)）因此，至少目前，从业者必须选择他们的公平定义。正如我们将要描述的那样，不同的用例将有不同合理的公平定义。
- en: While applying different notions of fairness in different situations may seem
    strange, mature readers will realize that we likewise apply the same principles
    in real life. Consider the US, a strongly market-driven economy. For the most
    part—and even in our current era of populism—there tends to be a pervasive individualized
    notion of fairness when it comes to the labor market. Americans seem to think
    people should get what they can get on the open market, albeit with some concerns
    that wages not go too low (thus, there is a legal minimum wage) and likewise with
    some concerns that the highest rates of pay have gone too high (demonstrated by
    increasing concerns about rising inequality and the rise of the billionaire class).
    On the other hand, when it comes to healthcare, most Americans think everyone
    should get good medical care regardless of differences in individual health or
    inborn genetics.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同情况下应用不同的公平概念可能看起来很奇怪，成熟的读者将意识到我们在现实生活中同样应用相同的原则。考虑美国，一个强烈市场驱动的经济体。在很大程度上——即使在我们当前的民粹主义时代——在劳动市场上，存在着普遍的个性化公平观念。美国人似乎认为人们应该在开放市场上得到他们能得到的东西，尽管对工资不要过低有些担忧（因此有法定最低工资），同样对于最高支付率过高也有一些担忧（通过对不断增加的不平等和亿万富翁阶级崛起的担忧表明）。另一方面，当涉及到医疗保健时，大多数美国人认为每个人都应该获得良好的医疗护理，而不管个体健康或先天遗传的差异。
- en: It seems (at least to us armchair anthropologists) that at some level Americans
    are comfortable with an allocation of economic benefits based, likely in part,
    on inborn ability but are not comfortable with this notion when it comes to an
    allocation of medical benefits. Thus, ordinary people in fact seem to commonly
    apply different notions of fairness to different domains of life, and we propose
    that the same could plausibly be true in different contexts or use cases of ML.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来（至少对我们这些坐在家里的人类学家来说），在某种程度上，美国人似乎对经济利益的分配方式感到舒适，可能在某种程度上是基于天生的能力，但当涉及医疗福利的分配时则不然。因此，事实上，普通人似乎确实常常在生活的不同领域应用不同的公平观念，我们认为在ML的不同上下文或使用案例中，同样可能是如此。
- en: Warning
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: It might seem that the ML ethics community should be attempting to converge
    on a single definition of *fairness*. This might seem especially true when we
    can see that different definitions of fairness are divergent or even contradictory
    in practice. At present, given the state of maturity of the industry and this
    field, it is almost certainly not a goal to converge on a single definition of
    fairness. This is for two reasons.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 似乎ML伦理社区应该试图在*公平* 的单一定义上达成一致。当我们看到公平的不同定义在实践中是分歧甚至矛盾的时候，这种观点似乎尤为正确。目前，鉴于行业和该领域的成熟程度，几乎可以肯定的是，目标并不是收敛于一个单一的公平定义。这有两个原因。
- en: First, we don’t need to. The state of the art of most models deployed in the
    real world is still fundamentally unfair by any metric (or at least of unexamined
    fairness). We have many useful definitions of fairness to select from, and picking
    one or several to work toward provides us with the opportunity for rapid progress
    right away without needing to await further theoretical or legal developments.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们不需要这样做。目前部署在现实世界中的大多数模型的技术水平仍然在任何度量标准下基本上是不公平的（或至少是未经审查的公平）。我们有许多有用的公平定义可以选择，并且选择一个或几个来努力实现使我们有机会立即取得快速进展，而不需要等待进一步的理论或法律发展。
- en: Second, and more structurally, the field of AI and ML theory is currently largely
    controlled by people from the dominant groups of society. These are the very people
    most likely to miss the ways that AI and ML systems can negatively impact disempowered
    or underrepresented groups. Surely, these same people shouldn’t be the arbiters
    of what will be set down as the proper definition of fairness to the exclusion
    of all others in the future.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，从结构上看，目前AI和ML理论领域主要由社会中主导群体的人控制。这些人最有可能忽视AI和ML系统可能对无权或代表性不足群体产生负面影响的方式。毫无疑问，这些人不应该成为未来将公正定义为排斥其他一切的仲裁者。
- en: '*Group parity* fairness definitions require that relevant rates of algorithmic
    performance be the same across various groups. A group parity requirement could,
    for example, require that the hiring rate across all ethnic groups be the same,
    but might also require that the accuracy for [diagnosing low oxygen](https://oreil.ly/wU0mJ)
    be the same regardless of skin color. This idea is compelling and intuitive because
    it paints a picture of the society many of us would like to see—one in which bad
    luck or opportunity are distributed equally among communities.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*组平等* 公平性定义要求算法性能的相关比率在各种群体中相同。例如，一个组平等要求可能要求所有族裔的雇佣率相同，但也可能要求在诊断[低氧](https://oreil.ly/wU0mJ)时，无论皮肤颜色如何，准确度也相同。这个想法很引人注目和直观，因为它描绘了我们许多人希望看到的社会图景——一个不幸或机会平等分配给社区的社会。'
- en: In contrast to group parity, *calibration* fairness definitions require that
    a ML model work equally well for all individuals. While this cannot be directly
    measured, what can be measured is that a model score means the same thing for
    an individual regardless of their group membership. So a calibration-oriented
    definition of fairness would require that for any group, the ML score means the
    same thing.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 与组平等相反，*校准* 公平性定义要求ML模型对所有个体工作效果一样好。虽然这不能直接衡量，但可以衡量的是，对于任何群体，模型得分对个体意味着相同。因此，一个以校准为导向的公平定义将要求对于任何群体，ML得分的含义都是相同的。
- en: 'This sounds intimidating and highly technical, but it can be easier to digest
    with a specific example. A commonly given example of calibration is that of the
    Correctional Offender Management Profiling for Alternative Sanctions, or [COMPAS](https://oreil.ly/KEVqN),
    recidivism scoring algorithm, which is used by criminal courts in many US states.
    Because this algorithm has been shown to be calibrated correctly across racial
    categories, the same COMPAS score (say, 0.5) means the same thing for both Black
    and white data subjects: that is, a white offender and a Black offender with the
    same risk score have the same probability of reoffending.^([8](ch06.xhtml#ch01fn49))
    When understood in plain English, this seems intuitive as well—that a model score
    should mean the same thing for everyone, no matter what group they belong to.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来令人生畏和高度技术化，但通过一个具体的例子可能更容易理解。校准的常见示例是用于许多美国州刑事法院的《矫正罪犯管理倾向评估替代刑罚》（Correctional
    Offender Management Profiling for Alternative Sanctions）或[COMPAS](https://oreil.ly/KEVqN)再犯评分算法。因为已经显示该算法在种族类别上正确校准，因此对于黑人和白人数据对象来说，相同的COMPAS分数（比如0.5）意味着相同的事情：即，一个白人罪犯和一个黑人罪犯，如果有相同的风险评分，则有相同的再犯概率。^([8](ch06.xhtml#ch01fn49))
    以通俗的英语理解，这也似乎很直观——即模型分数应该对每个人都有相同的含义，无论他们属于哪个群体。
- en: You may wonder whether having many definitions of fairness is a problem. After
    all, there are many correct and intuitive ways of describing the world in other
    contexts, so why not in the case of fairness? The difficulty comes from the fact
    that—at least in the world we currently live in—these definitions of fairness
    come into conflict. The COMPAS algorithm is a compelling example. That algorithm
    was not necessarily implemented in a careless fashion, but was, in fact, sometimes
    adopted as part of a movement to reduce bias in the criminal justice system. The
    algorithm passed calibration checks that were considered to be the gold standard
    in ensuring fairness and nondiscrimination.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道，公平有多种定义是否是一个问题。毕竟，在其他情境中，描述世界的正确和直观方式有很多，那么为什么在公平的情况下不行呢？困难在于——至少在我们目前生活的世界中——这些公平的定义会发生冲突。COMPAS算法是一个引人注目的例子。该算法并非一定是随意实施的，而实际上有时是作为减少刑事司法系统偏见运动的一部分而采用的。该算法通过被认为是确保公平和非歧视的黄金标准的校准检查。
- en: However, reporters at ProPublica, a nonprofit newsroom, later demonstrated that—despite
    this calibration—the algorithm had higher rates of false positives in labeling
    Black defendants high risk, and higher rates of false negatives in labeling white
    defendants low risk. In other words, Black defendants were more likely than whites
    to be labeled as having a high probability of a violent re-offense and then not
    go on to commit such a re-offense, while whites were more likely to be labeled
    as having a low probability of re-offending but then going on to in fact re-offend.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，《非营利新闻室》（ProPublica）的记者后来证明，尽管进行了这种校准，该算法在标记黑人被告为高风险时有更高的虚警率，并且在标记白人被告为低风险时有更高的漏警率。换句话说，与白人相比，黑人被告更有可能被标记为有高概率再犯的可能性，但实际上未再犯；而白人更可能被标记为低概率再犯，但实际上再犯。
- en: This led to a public outcry, prompting many mathematicians and computer scientists
    to work on the problem. However, scholars quickly realized that their goal—of
    making a risk score that demonstrated both calibration and statistical parity–was
    impossible with most real-world data. Statistical parity and calibration cannot
    be mathematically satisfied at the same time, unless base rates of events are
    the same in each group. However, equality of base rates is a condition rarely
    met in the real world.^([9](ch06.xhtml#ch01fn50))
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这引发了公众的强烈抗议，促使许多数学家和计算机科学家致力于解决这个问题。然而，学者们很快意识到，他们的目标——制定一个同时展示校准和统计平等的风险评分——在大多数真实世界数据中是不可能的。统计平等和校准不能在同一时间数学上满足，除非每个群体的事件基础率相同。然而，在现实世界中，基础率的平等是一个很少满足的条件。^([9](ch06.xhtml#ch01fn50))
- en: Mathematically, not all fairness definitions can be satisfied at the same time,
    given real world conditions. We have to decide which fairness goals to pursue
    in an ML context, and likewise decide whether focusing on a specific fairness
    metric might even tend to reduce the tendency toward a holistic viewpoint that
    might otherwise be more helpful in enhancing fairness and other ethical values.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学上，不同的公平性定义在现实世界的条件下不可能同时满足。我们必须决定在机器学习环境中追求哪些公平性目标，并决定是否专注于特定的公平性指标甚至可能减少朝向本质视角的倾向，这本来可能更有助于增强公平性和其他伦理价值。
- en: For now, we consider that for a specific ML tool, it may make sense to choose
    and emphasize a particular fairness metric, while recognizing that it will not
    be possible to satisfy all intuitive and normatively desirable notions of fairness
    at the same time.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们认为对于特定的机器学习工具，选择和强调特定的公平性指标可能是合理的，同时要认识到不可能同时满足所有直观和规范上期望的公平性概念。
- en: We do not want to allow perfection to become the enemy of the good. Rather,
    practitioners have found that having many definitions of fairness is more actionable
    than having none, and that working toward a metric of fairness is broadly likely
    to enhance many other forms of fairness as well. What’s more, some researchers
    have recognized that different situations may call for different fairness metrics,
    given the relative harms or policy goals of a particular use case of an algorithm,
    and so interested readers can find useful guidance as to how to choose a fairness
    metric for a specific task given specific concerns about the likely consequences
    of various kinds of mistakes.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不希望完美成为良好的敌人。相反，从业者们发现，有多种公平性定义比没有更具操作性，并且努力朝着公平性指标的方向努力，广泛来说也可能增强许多其他形式的公平性。更重要的是，一些研究人员意识到，不同的情况可能需要不同的公平性指标，考虑到算法特定用例的相对伤害或政策目标，因此有兴趣的读者可以找到如何根据特定任务选择公平性指标的有用指导，考虑到各种错误可能导致的后果。
- en: Reaching Fairness
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 争取公平性
- en: 'Concretely, a classic ML setup has three modes of working toward fairer (less
    biased) outcomes. Here we provide a flavor of how these work, so you can approach
    the literature with an overview and understand the benefits and costs of different
    approaches:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 具体地说，经典的机器学习设置有三种工作模式，以朝向更公平（更少偏见）的结果。在这里，我们简要介绍它们的工作原理，以便您可以通过概览的方式探索文献，并了解不同方法的利弊：
- en: Preprocessing
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理
- en: These methods intervene in data rather than in models. Preprocessing methods
    take multiple approaches to reducing unfairness in the inputs into ML training.
    Most bluntly, data points can be relabeled. For example, some approaches, such
    as that of [Kamiran and Calders (2011)](https://oreil.ly/I2fDD), offer ways of
    identifying data that should be relabeled because the data suggests a biased outcome.^([10](ch06.xhtml#ch01fn51))
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法在数据而非模型中进行干预。预处理方法采用多种方法来减少机器学习训练输入中的不公平性。最直接的是，数据点可以被重新标记。例如，一些方法，比如[Kamiran
    和 Calders (2011)](https://oreil.ly/I2fDD)，提供了识别应重新标记数据的方式，因为数据表明存在偏见结果。^([10](ch06.xhtml#ch01fn51))
- en: Another, less drastic approach, is to seek a representation of the data that
    reduces information about the group to which an individual belongs; an approach
    pioneered by [Zemel et al.](https://oreil.ly/vW9fa) (2013), for example, proposes
    describing the data such that an individual’s sensitive attributes can no longer
    be guessed accurately. Because these methods look to the data as the point of
    intervention, they are model agnostic. A general rule of thumb is that *intervening
    at the earliest possible stage in the ML pipeline is most likely to yield best
    results* (and leave options open for subsequent additional interventions).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个较少激进的方法是寻求降低有关个体所属群体信息的数据表示；例如，[Zemel 等人](https://oreil.ly/vW9fa)（2013）首倡的方法建议描述数据，使得无法准确猜测个体的敏感属性。因为这些方法看待数据为干预点，它们不依赖于模型。一个经验法则是，**在机器学习管道的尽早阶段介入最有可能产生最佳结果**（并为后续的额外干预留下选项）。
- en: But, there are, of course, valid concerns about preprocessing approaches. Changing
    the data labels aggressively challenges a data-driven discipline, such as ML,
    in fundamentally removing information. Likewise, methods that look to “transform”
    the data rather than directly manipulate the labels also challenge the fundamental
    tenets of a data-driven approach when data is deliberately removed or changed.
    Also it can be difficult to identify exactly what about the model changes as a
    result of changing the data, since in most cases it will not be possible to know
    all that clearly affects the relabeling a few data points.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，对预处理方法存在有效的关注。激进更改数据标签挑战了数据驱动学科（如机器学习），从根本上删除信息。同样，寻求“转换”数据而不是直接操纵标签的方法，在故意删除或更改数据时也挑战了数据驱动方法的基本原则。此外，很难确定由于更改数据而导致模型变化的具体原因，因为在大多数情况下，将无法清楚地知道所有影响重新标记少数数据点的事实。
- en: In-processing
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理中
- en: These methods intervene during model training. This can manifest in any way
    in which the actual step-by-step training of a model is affected by fairness considerations.
    In many cases, this has been addressed through adjusting the loss function used
    during model training. Various “penalties” can be added to reflect the fairness
    costs imposed by biased outcomes, as is reflected in the work of [Kamishima et
    al.](https://oreil.ly/rEc4P) (2011). Such methods are similar to regularization
    techniques.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法干预模型训练过程。这可以表现为任何实际步骤训练模型时受公平性考虑的影响方式。在许多情况下，这已通过调整模型训练中使用的损失函数来解决。可以添加各种“惩罚”，以反映由偏见结果造成的公平成本，正如[Kamishima等人](https://oreil.ly/rEc4P)（2011）的工作所反映的那样。这些方法类似于正则化技术。
- en: Another method is similar to that described in preprocessing (learned fair representations)
    in terms of motivation to remove identifying information from the model’s knowledge
    about the sensitive attribute. An adversarial model is trained simultaneously
    with the model of interest, such that the adversarial model’s target is to guess
    which sensitive category an output is associated with. The model of interest is
    trained to a task but also optimized to reduce the information that its outputs
    transmit to the adversarial model. Some of these methods, such as the work of
    [Zhang et al.](https://oreil.ly/SVmNt) (2018), can be model agnostic.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法类似于预处理中描述的方法（学习公平的表示），其动机在于从模型对敏感属性的知识中移除识别信息。同时训练对抗模型和感兴趣的模型，使得对抗模型的目标是猜测输出与哪个敏感类别相关联。感兴趣的模型被训练为一项任务，但也被优化以减少其输出传输给对抗模型的信息量。其中一些方法，例如[Zhang等人](https://oreil.ly/SVmNt)（2018）的工作，可以是模型无关的。
- en: Post-processing
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 后处理
- en: These methods intervene on the model’s labels rather than on the model directly.
    In this way, they correct outcomes from the model based on meeting certain targets.
    An examples of post-processing is introducing randomization, as in [Hardt et al.
    (2016)](https://oreil.ly/b9EhM), in the case where—without such randomization—false
    negatives might be different among different groups (an example of applying group
    parity).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法干预模型的标签而不是直接干预模型本身。通过这种方式，它们根据达到某些目标来纠正模型的输出结果。后处理的一个例子是引入随机化，例如[Hardt等人（2016）](https://oreil.ly/b9EhM)，在没有这种随机化的情况下，假阴性可能在不同群体之间不同（这是应用组平等的一个例子）。
- en: Another example of post-processing is to set different thresholds—say, for a
    [credit score](https://oreil.ly/DzYYL) or college admissions scores—for different
    groups such that the predictions or decisions made with the score can be equally
    accurate for different groups.^([11](ch06.xhtml#ch01fn52)) Because these methods
    intervene after a model has run, they are model agnostic.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个后处理的例子是为不同的群体设置不同的阈值，例如[信用评分](https://oreil.ly/DzYYL)或大学入学分数，以便使用分数做出的预测或决策对不同群体同样准确。^([11](ch06.xhtml#ch01fn52))因为这些方法在模型运行后介入，所以它们是模型无关的。
- en: Which method of intervention to choose depends on a variety of factors. In some
    cases, an organization may need to apply a specific stage of intervention because
    that is the intervention over which they have control. For example, an organization
    may have received a pretrained neural network that it will fine-tune. Since the
    organization doesn’t have access to the original training data or method, post-processing
    might prove a more viable option.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 选择哪种干预方法取决于多种因素。在某些情况下，组织可能需要应用特定的干预阶段，因为这是他们可以控制的干预方式。例如，一个组织可能已经获得了一个预训练的神经网络，他们将对其进行微调。由于该组织无法访问原始训练数据或方法，后处理可能是一个更可行的选择。
- en: On the other hand, another organization might find that the options offered
    by post-processing are normatively problematic because they violate certain fundamental
    values held by some people. For example, people may be uncomfortable with the
    idea of having explicitly different score cutoffs for different groups, and they
    also might be uncomfortable with the notion of eliminating, and replacing with
    randomized numbers, those outputs from an algorithm that are, in fact, likely
    to be correct.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，另一个组织可能会发现后处理提供的选项在规范上存在问题，因为它们违反了一些人们持有的某些基本价值观。例如，人们可能对为不同群体明确设定不同的分数截止点感到不适，他们也可能对用随机数字替换那些算法输出的确实可能是正确的内容的想法感到不适。
- en: To date, there are no well-established guidelines from regulators or prominent
    ethical leaders regarding the best way to intervene,^([12](ch06.xhtml#ch01fn53))
    and we suspect that this will have to be a highly context-sensitive analysis.
    But the need for highly contextualized case-by-case decision making is no different
    from other elements of running an organization or making decisions that affect
    other people’s lives. Algorithms will help increase efficiency and uniformity,
    but there will never be a single go-to algorithmic fairness solution for all scenarios.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，监管机构或著名的伦理领袖尚无关于最佳干预方式的明确指导方针^([12](ch06.xhtml#ch01fn53))，我们认为这将需要进行高度依赖于情境的分析。但高度情境化的案例决策需求并不异于组织运作的其他要素或影响他人生活的决策。算法将有助于提高效率和一致性，但在所有情况下都没有一个单一的算法公平解决方案。
- en: Fairness as a Process Rather than an Endpoint
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 公平作为一个过程而不是一个终点
- en: This is a difficult set of topics, and it can be strongly discouraging to know
    that AI can and does cause so many harms. Don’t let this bad news dissuade you
    from all of the advantages that can be achieved with a thoughtfully engineered
    ML/AI system. The news about fairness doesn’t have to be bad.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一系列棘手的话题，了解到AI可能会造成如此多的伤害可能会极大地泄气。但不要因此不再去思考精心设计的ML/AI系统能够带来的所有优势。关于公平性的消息并不一定是坏消息。
- en: 'Let’s consider two problems:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑两个问题：
- en: Fairness wise, you can’t please everyone. There is no “perfectly” fair solution
    to any algorithmic fairness challenge (at least none has been identified so far).
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 就公平而言，你不可能让每个人都满意。在任何算法公平性挑战中，都没有“完全”公平的解决方案（至少目前还没有被确认出来）。
- en: Likewise, there is no perfect way to enforce fairness even if you do settle
    on a particular definition of fairness (that is, defining a metric doesn’t guarantee
    you can make a model perfectly conform to that metric or indicate how you should
    attempt to do so).
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样地，即使你确定了公平的特定定义（即，定义一个度量并不保证你能使模型完全符合该度量，或指示你应该如何尝试这样做），也没有完美的强制公平的方式。
- en: 'Let us rebut both of these points in terms of their relevance to your choices
    in your ML modeling:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从你在ML建模中做出选择的角度来反驳这两点：
- en: You have many good fairness definitions to choose from.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你有许多良好的公平定义可以选择。
- en: Any process to work toward fairness is far better than a world in which we ignore
    these issues—because ignoring hasn’t led to good outcomes.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何朝着公平目标努力的过程都比忽视这些问题的世界要好——因为忽视并没有带来好的结果。
- en: It can be helpful to think of fairness as a process rather than a specific endpoint.
    While this can be disappointing to those who like to think they can build an ML
    algorithm to solve a business problem and move on, the reality is more complicated.
    Products take upkeep for many reasons. The world changes, and so deployment conditions
    change, and so models must evolve. Fairness is no different.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 将公平视为一个过程而不是一个特定的终点可能会有所帮助。对于那些希望通过构建机器学习算法来解决业务问题并前进的人来说，这可能令人失望。现实更加复杂。产品需要维护有很多原因。世界在变化，因此部署条件也在变化，因此模型必须不断发展。公平性并不例外。
- en: A Quick Legal Note
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个快速的法律声明
- en: It is far beyond the scope of this work to provide legal guidance; that is what
    your company’s legal department is for! Nonetheless, it is worth mentioning that
    despite the seeming newness of algorithmic fairness as a topic, the concept of
    discrimination has had an important and explicit place in law for centuries in
    one form or another. The topics most in the forefront today, such as racial and
    gender equality, have likewise occupied a prominent role in law for decades. This
    has especially been the case in areas of civic life that are understood as core
    to human life, such as employment, education, healthcare, housing, and access
    to financial credit services. If you are doing ML work that touches on these core
    areas, you likely need to be deeply concerned about fairness—and, specifically,
    about fairness as it is implemented in your nation’s antidiscrimination laws.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中提供法律指导远超出范围，这正是你公司法律部门的职责！尽管算法公平性似乎是一个新课题，但歧视的概念在法律中的重要性和明确性已经有了几个世纪的历史。今天最前沿的话题，如种族和性别平等，同样在几十年来一直在法律中占据着突出的地位。特别是在被理解为人类生活核心的市民生活领域，如就业、教育、医疗、住房和金融信贷服务的访问。如果您从事涉及这些核心领域的机器学习工作，您很可能需要对公平性
    — 尤其是您国家反歧视法律中的公平性 — 深感关切。
- en: Privacy
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐私
- en: '*Privacy* is a notion that has proven notoriously difficult to define in scholarship.
    Privacy measures have therefore proven difficult to create, particularly with
    respect to future-proofing. The rise of big data has proven a dramatic example
    of this.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*隐私* 是一个在学术界证明极其难以定义的概念。因此，隐私措施在未来保护方面尤为困难。大数据的兴起正是一个明显的例子。'
- en: Experts used to think that de-identified data was appropriate and safe to release.
    Such data would have removed what was thought to be identifying information, such
    as name or address, that could easily be matched to a person. However, information
    about other factors associated with, but seemingly not directly related to, a
    specific person’s identity was still released, such as birthday, race, or zip
    code.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 专家曾认为去标识化数据是适当和安全的。这些数据会移除被认为是可识别信息的内容，如姓名或地址，这些信息很容易与一个人匹配。然而，与特定人的身份看似不直接相关的其他因素的信息仍然被发布，如生日、种族或邮政编码。
- en: With the advent of big data, many datasets were compiled, often about overlapping
    groups of people, and it then became possible to use different datasets together
    to identify people from de-identified information. For example, for most Americans,
    it turned out to be possible to identify them in a de-identified dataset merely
    from knowing their birthday, zip code, and gender.^([13](ch06.xhtml#ch01fn54))
    And, the world is increasingly full of new datasets, sometimes resulting from
    data breaches but other times resulting from people voluntarily sharing information
    about themselves—information that becomes very easy to access [by any casual creepy
    stalker](https://oreil.ly/AOVNs). We might like to think the world is an anonymous
    place, but it simply isn’t the case today.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大数据的出现，许多数据集被编制，通常涉及重叠的人群，因此可以使用不同的数据集一起使用来识别脱身信息中的人们。例如，对于大多数美国人来说，仅凭知道他们的生日、邮政编码和性别就可能在去标识化数据集中识别他们^([13](ch06.xhtml#ch01fn54))。此外，世界越来越充满新的数据集，有时是由于数据泄露，而其他时候是由于人们自愿分享关于自己的信息
    — 这些信息变得非常容易获取[对于任何随意的恶心跟踪者](https://oreil.ly/AOVNs)。我们可能希望认为世界是一个匿名的地方，但今天情况并非如此。
- en: 'Two key ideas about privacy are described here, both in terms of how they relate
    to a notion of privacy, but also in how they relate to taking an existing dataset
    with personal information and turning it into a dataset that can be used or released
    without compromising individual identity:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这里描述了隐私的两个关键理念，既涉及到它们与隐私概念的关系，也涉及到如何将含有个人信息的现有数据集转化为可以使用或发布而不会泄露个体身份的数据集：
- en: k-anonymity
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: k-匿名性
- en: The idea behind k-anonymity is that in a given dataset, for any given combination
    of categories of interest, there should be at least *k* individuals (externally
    specified) who fall into any given bucket. So, for example, we could apply k-anonymity
    to a dataset listing individuals in a town by requiring that data be bucketed
    such that for any zip code / birth information / gender category, there were at
    least 10 individuals. There might be many ways to accomplish this. Two potential
    opportunities would be to report birthdays at the month or year level and to report
    only the first four digits of a zip code rather than all five.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: k-匿名性的理念是，在给定的数据集中，对于任何感兴趣的组合类别，至少应有* k *个个体（外部指定）属于任何给定的桶。例如，我们可以将k-匿名性应用于按城镇列出个体的数据集，要求数据被分桶，以确保对于任何邮政编码/出生信息/性别类别，至少有10个个体。可能有许多实现这一目标的方法。两个潜在的机会之一是报告月份或年份级别的生日，另一个是只报告邮政编码的前四位数字而不是全部五位。
- en: This notion of privacy and preventive measure essentially conceptualizes privacy
    as not being in too small a group within a dataset. [Recommended size](https://oreil.ly/zl6MO)
    is at least 5 in the medical domain, although *k* sizes far larger (even greater
    than 50) have been reported. The appropriate size for *k* will depend on the particular
    domain, which could have different implications regarding sensitivity of the data,
    possibility to build useful datasets with large *k*, and possibility of reidentification
    given other known potentially linkable datasets.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这种隐私和预防措施的概念本质上将隐私概念化为在数据集中不处于太小的群体中。[建议的大小](https://oreil.ly/zl6MO) 在医疗领域至少为5，尽管已经报道的*k*大小远大于50。*k*的适当大小将取决于特定领域，这可能涉及数据的敏感性、能够使用具有大*k*的有用数据集的可能性，以及考虑其他已知可能可链接数据集的再识别可能性。
- en: Differential privacy
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私
- en: This mathematical method adds noise to data such that probabilistic guarantees
    can be made regarding the possibility (or more importantly, lack of possibility)
    to make inferences about a specific individual when given access to that noisified
    data. The idea behind differential privacy is that it would be a privacy problem
    for one individual’s inclusion, or non-inclusion, in a dataset to influence operations,
    such as the calculation of averages or other statistics, so that the dataset’s
    information could be inferred based on aggregate reporting. So for example, if
    the mean age of a class is reported and the size of the group, with and without
    an individual, it is possible to know that individual’s age. However, if differential
    privacy is applied, it would, in probability, not be possible to infer that individual’s
    age at a pre-specified level of precision and given a pre-specified query budget.^([14](ch06.xhtml#ch01fn55))
    Differential privacy can apply quite broadly, not just to the computation of aggregate
    statistics but also to the training of ML models, with methods to ensure that
    a model’s outputs or training is not contingent on inclusion of a particular data
    point.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数学方法向数据添加噪声，以便可以就可能性（或更重要的是，不可能性）在访问到这些噪声化数据时推断出关于特定个体的信息做出概率性保证。差分隐私的理念是，一个人的包含或不包含对数据集的操作（例如计算平均值或其他统计数据）产生影响，使得基于聚合报告可以推断出数据集的信息。例如，如果报告一个班级的平均年龄和组的大小，可以知道该个体的年龄。然而，如果应用了差分隐私，在概率上将不可能以预定的精度水平和给定的查询预算推断出该个体的年龄。^（14）差分隐私可以广泛应用，不仅适用于聚合统计数据的计算，还适用于ML模型的训练，通过确保模型的输出或训练不依赖于特定数据点的包含。
- en: These notions of privacy may seem quite technical, but are relevant to real
    privacy problems, such as managing the responsible and privacy-preserving release
    of open datasets or ensuring that legal rights have a meaningful technical translation.
    For example, the EU’s GDPR gives an individual a right to data deletion. However,
    if an ML model has already been trained, that right to deletion may not be entirely
    meaningful. For example, does an individual’s right to delete their data mean
    that they can also force a company to prove that its ML models have also “forgotten”
    their data?^([15](ch06.xhtml#ch01fn56))
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这些隐私概念可能显得相当技术性，但与真实的隐私问题相关，例如管理负责和保护隐私的开放数据集的发布，或确保法律权利具有有意义的技术转化。例如，欧盟的GDPR赋予个人数据删除权利。然而，如果一个机器学习模型已经被训练过，删除权利可能并不完全具有意义。例如，个人删除其数据的权利是否意味着他们也能强迫一家公司证明其机器学习模型也已经“遗忘”他们的数据？^([15](ch06.xhtml#ch01fn56))
- en: Some have explored how training models with differential privacy might address
    this concern. However, technical challenges remain because producing ML models
    with differential privacy guarantees is quite a technical challenge. It is challenging
    to ensure both differential privacy and a high level of performance at the same
    time.^([16](ch06.xhtml#ch01fn57))
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一些人已经探讨了如何通过差分隐私训练模型来解决这一问题。然而，技术挑战依然存在，因为在保证差分隐私和高性能之间达到平衡是相当具有技术挑战性的。^([16](ch06.xhtml#ch01fn57))
- en: Methods to Preserve Privacy
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保护隐私的方法
- en: The definitions and scenarios described previously with respect to k-anonymity
    and differential privacy refer to very specific notions of privacy and computational
    measures. And, indeed, privacy is itself a highly technical and specialized field,
    perhaps best left to experts in terms of implementation.^([17](ch06.xhtml#ch01fn58))
    However, a variety of accessible privacy-enhancing measures are transparent, straightforward
    to implement, and meaningful. You should include them in your own workflow and
    will likely need to customize them.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 先前描述的k-匿名性和差分隐私的定义和场景涉及非常具体的隐私和计算措施。事实上，隐私本身是一个高度技术化和专业化的领域，最好由专家来实施。^([17](ch06.xhtml#ch01fn58))
    然而，有各种易于实施且有意义的可访问的增强隐私措施。你应该将它们包含在自己的工作流程中，并可能需要根据实际情况进行定制。
- en: These are likely to already be familiar concepts to systems administrators and
    those who deal with compliance issues. However, these sometimes are painfully
    unfamiliar to data scientists and ML engineers. It is my hope that they will become
    part of basic project specification and daily practice and consideration in the
    future.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 系统管理员和涉及合规问题的人员可能已经熟悉这些概念。然而，对于数据科学家和机器学习工程师来说，这些概念有时会非常陌生。我希望它们将成为基本项目规范和日常实践的一部分，今后会得到重视。
- en: Technical measures
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 技术措施
- en: 'The following are some technical measures to take:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些保护隐私的技术措施：
- en: Access controls
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 访问控制
- en: A key way to preserve privacy and reduce threats is to implement robust access
    controls. Any data about people in a database should be treated as a “need to
    know” resource, with ML engineers requesting access for specific purposes rather
    than being able to freely access or browse data. Likewise, access should be revisited
    periodically to make sure that staff members do not retain access to data for
    which they no longer have a valid active reason for access.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 保护隐私和减少威胁的一个关键方法是实施强大的访问控制。数据库中关于个人的任何数据都应被视为“需要知道”的资源，机器学习工程师应该根据具体目的请求访问，而不是自由访问或浏览数据。同样，访问权限应定期审查，以确保员工没有因为已无有效理由而继续访问数据。
- en: Access logging
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 访问日志
- en: Keep track of who is accessing specific forms of data and when. This makes it
    possible to understand data use patterns, see when someone might be inappropriately
    accessing data, and preserve evidence in case allegations of inappropriate use
    are made later. An analysis of such logs may also indicate ways in which data
    storage schema could be refactored to reduce the extent of data that different
    use patterns can access. For example, if an ML model calls for access to a sensitive
    table of data merely to access one column, consider splitting off that column
    of data rather than granting access to a full table of additional but unnecessary
    pieces of information.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪谁在何时访问特定形式的数据。这样可以了解数据使用模式，看到某人可能在不适当地访问数据，并在后来被提出不适当使用的指控时保留证据。对这些日志的分析也可能表明数据存储模式可以重构以减少不同使用模式可以访问的数据量。例如，如果一个
    ML 模型需要访问敏感数据表的一个列，考虑将该数据列拆分出来，而不是授予访问完整表格的额外但不必要的信息访问权限。
- en: Data minimization
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 数据最小化
- en: The collection and use of data should be minimized. Data should not be collected
    merely because it “could be useful” in the future. Data should be logged only
    when there is an immediate use case for this data (preferably, with some benefit
    to those about whom the data is collected).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 应尽量减少数据的收集和使用。数据不应仅仅因为“将来可能有用”而收集。只有在存在该数据的即时用例时（最好对被收集数据的个体有一些益处），才应记录数据。
- en: Data separation
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分离
- en: The data needed for legitimate business uses should be separated from sensitive
    data (such as names and addresses) that is unlikely to be relevant to creating
    an ML model. For example, to predict users’ clicks, there doesn’t seem to be a
    justification for knowing a user’s name or address.^([18](ch06.xhtml#ch01fn59))
    Therefore, there is no reason for that information to be stored with information
    that might be useful for that particular prediction task, such as past browsing
    history or demographic information.^([19](ch06.xhtml#ch01fn60)) As noted previously,
    studying your data access logs can help you identify ways in which data storage
    can be refactored to minimize exposure of data to ML applications.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 需要用于合法业务用途的数据应与不太可能对创建 ML 模型有用的敏感数据（如姓名和地址）分开。例如，为了预测用户的点击，似乎没有理由知道用户的姓名或地址。^([18](ch06.xhtml#ch01fn59))
    因此，没有理由将这些信息存储在可能对特定预测任务有用的信息（如过去的浏览历史或人口统计信息）中。^([19](ch06.xhtml#ch01fn60)) 正如之前所述，研究您的数据访问日志可以帮助您识别可以重构以最小化数据暴露给
    ML 应用程序的方式。
- en: Institutional measures
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机构措施
- en: 'Here are some institutional measures available as well:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这里也有一些机构措施可供选择：
- en: Ethics training
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 道德培训
- en: Everyone needs ethics training when they enter a new domain, and the same goes
    for designing ML products. ML engineers should be given—but usually are not—a
    thorough review not only of general ethics training (such as the possibility of
    bias in data) but also domain-specific training when building algorithms for a
    specific use case. Too often organizations do not have any formal discussions
    or training about privacy or ethics more generally, and even basic training, no
    matter how “corny,” serves to bring the issues to the fore.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 每个人进入新领域时都需要道德培训，为设计 ML 产品也是如此。 ML 工程师应该得到——但通常不会得到——全面审查，不仅包括一般的道德培训（如数据中的偏见可能性），还包括为特定用例构建算法时的领域特定培训。太多时候，组织没有任何关于隐私或伦理更广泛讨论或培训，即使基础培训，无论多么“俗气”，也有助于突显问题。
- en: Data access guidelines
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 数据访问指南
- en: In addition to technical measures already described, it makes sense to have
    explicit rules that are readily available regarding what constitutes appropriate
    access to data and use of that data and what use cases are expressly prohibited.
    A lack of clear and explicit ethics rules can lead to an institutional culture
    without accountability. Organizations should have clear data access and appropriate
    data use guidelines in place in a location that is readily apparent and accessible.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 除了已经描述的技术措施外，制定明确的规则，即使在何时访问数据和使用数据以及明确禁止的用例方面，也是有意义的。缺乏明确和明确的伦理规则可能导致缺乏问责制的机构文化。组织应在一个显而易见和可访问的地方制定清晰的数据访问和适当数据使用指南。
- en: Privacy by design
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私设计
- en: Privacy by design is a set of design principles that can apply to any digital
    product, including ML pipelines and ML-driven products. The notion of privacy
    by design is that privacy is something that shouldn’t be tacked on to the end
    of a process that already exists. Rather, privacy should be intrinsic to design
    considerations from the start and should be a question and concern addressed at
    all working stages. Privacy by design can provide a flexible but holistic way
    to ensure privacy in all elements of ML development.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私设计是一组设计原则，适用于任何数字产品，包括机器学习流水线和机器学习驱动的产品。隐私设计的概念是隐私不应仅仅是附加到已存在流程的最后阶段。相反，隐私应从设计考虑的开始就内在于其中，并且应该是所有工作阶段中要考虑和关注的问题。隐私设计可以为在机器学习开发的所有元素中确保隐私提供一种灵活但全面的方式。
- en: While these may all seem like basic and obvious notions, few organizations,
    large or small, do these basic things, or even pursue anything akin to a privacy
    or fairness agenda. Whether you are at a startup, an academic institution, or
    a large corporation, there is almost surely something you can contribute to enhance
    privacy in your organizer’s ML pipeline.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些可能都看起来是基本和显而易见的观念，但很少有组织，无论是大型还是小型，采取这些基本措施，甚至追求任何类似隐私或公平议程的内容。无论您是在初创公司、学术机构还是大型企业，几乎肯定都可以为增强您组织的机器学习流水线中的隐私做出贡献。
- en: In short, there are a variety of ways to protect privacy by taking concrete
    steps in your organization. Unfortunately, these steps are rarely taken, but they
    can be simple and effective. You have nothing to lose (and lots to gain) by initiating
    such simple efforts in your own workflow.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，有多种方法可以通过在您的组织中采取具体步骤来保护隐私。不幸的是，很少有人采取这些步骤，但它们可以简单而有效。通过在您自己的工作流程中采取这些简单措施，您将无所失（而有很多可以获益）。
- en: A Quick Legal Note
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个快速的法律声明
- en: 'It is far beyond the scope of this work to give an extensive review of privacy
    laws that usually affect digital products. Here we highlight a few key categories
    of laws that are related to privacy and ML:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 超出本工作范围的内容是对通常影响数字产品的隐私法律进行详尽审查。在这里，我们重点介绍与隐私和机器学习相关的几类关键法律：
- en: Data breach notification laws
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 数据泄露通知法律
- en: Data breach notification laws require those who hold data to notify people whose
    data they hold if that data was compromised in a data breach. To ensure compliance,
    such laws usually apply strict penalties if a holder of data becomes aware of
    a data breach and does not make the appropriate notifications. Such laws can sometimes
    also apply to a data holder who should have been aware of a data breach, to ensure
    that firms cannot simply choose to remain ignorant. Empirical research to date
    suggests that such laws have not prevented data breaches from becoming increasingly,
    even exponentially, more common over time. Nonetheless, such laws are useful in
    providing notice to consumers as to when they may be most at risk because of exposure
    of their personal data.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 数据泄露通知法律要求那些持有数据的人在数据泄露中数据受到损害时通知他们持有数据的人。为确保合规性，这类法律通常对数据持有者在意识到数据泄露但未进行适当通知时施加严格处罚。这类法律有时也适用于本应意识到数据泄露的数据持有者，以确保企业不能简单选择保持无知。迄今的实证研究表明，这些法律并未阻止数据泄露随时间变得越来越普遍，甚至呈指数增长。尽管如此，这类法律在提供消费者何时因其个人数据曝光而处于最高风险的通知方面仍然非常有用。
- en: Data protection and personal privacy laws
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 数据保护和个人隐私法律
- en: The most prominent example of data protection law is the EU’s GDPR. Many countries
    around the world have data protection laws that give people basic rights, such
    as the right to know what data is collected about them by online venues, what
    the venues do with that data, and whether the data is even correct. Some laws
    even go further, giving consumers the power to opt out of data collection, or
    even giving consumers the right to have data deleted or blocked from data sales.
    Unfortunately, empirical research has shown that such laws appear to be widely
    disregarded in consumer-facing applications. Nevertheless, such laws give consumers
    active ways that they can take steps to protect their privacy.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 数据保护法律中最显著的例子是欧盟的GDPR。全球许多国家都有数据保护法律，赋予人们基本权利，例如知道在线场所收集了哪些关于他们的数据，这些场所如何处理这些数据，以及这些数据是否正确。一些法律甚至进一步赋予消费者选择退出数据收集的权利，或者甚至赋予消费者删除数据或阻止数据销售的权利。不幸的是，实证研究表明，这类法律似乎在面向消费者的应用中被广泛忽视。尽管如此，这类法律为消费者提供了积极的方式，可以采取措施保护其隐私。
- en: Laws against unfair and deceptive practices
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 反不正当和欺骗行为的法律
- en: This category of more general consumer protection law is important in the US
    context, since the US otherwise lacks a comprehensive national personal data privacy
    regime. The US Federal Trade Commission, an important source of consumer protection
    enforcement, has sometimes brought actions finding that companies infringed basic
    expectations of fairness or honesty in their business practices as a means of
    protecting consumer privacy. A key example of this is companies that do not even
    respect the terms of the privacy policies they themselves author and leave on
    their websites. Thus it is—at the least—essential to ensure that, as your organization
    builds out data collection and ML modeling capabilities, you ensure that such
    practices are consistent with public-facing privacy policies and terms of service.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在美国的背景下，这类更普遍的消费者保护法律至关重要，因为美国缺乏全面的国家个人数据隐私制度。美国联邦贸易委员会，作为消费者保护执法的重要来源，有时会采取行动，发现公司在业务实践中未能遵守公平或诚实的基本期望，以保护消费者隐私。这种情况的一个关键例子是那些甚至不尊重他们自己制定并放在网站上的隐私政策条款的公司。因此，至少有必要确保，在您的组织构建数据收集和机器学习建模能力时，这些实践与面向公众的隐私政策和服务条款一致。
- en: Responsible AI
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负责任的人工智能
- en: '*Responsible AI* has come to be used as a catchall for ethical concerns we
    should contemplate when training or deploying an ML system. This is a growing
    area, and it’s safe to assume that neither industry nor the academic community
    yet has a firm grasp on the scope of harms that should be, or can be, addressed
    by Responsible AI. Here, under this rubric, we address some additional questions
    that have received a good deal of attention in recent years. However, we emphasize
    that the issues here are highlights. We do not purport to offer an exhaustive
    list of Responsible AI values.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*负责任的人工智能*已经成为我们在训练或部署机器学习系统时应考虑的伦理关切的总称。这是一个日益增长的领域，可以安全地假设，无论是工业界还是学术界，都还没有完全掌握应由负责任的人工智能处理的伤害范围。在这里，我们在这一框架下讨论了一些近年来引起广泛关注的附加问题。然而，我们强调这里的问题只是一些亮点。我们并不打算提供负责任的人工智能价值观的详尽清单。'
- en: Explanation
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释
- en: ML model *explanation* is the process of analyzing and presenting information
    about an ML system to describe how that system works. This process and the goal
    of making the model amenable to human understanding is often discussed in shorthand
    as *explainability*, and it’s a key area of interest with respect to ML. Many
    people have a desire to understand why an ML system is working the way that it
    does as part of understanding why it has reached a particular outcome that affects
    them.^([20](ch06.xhtml#ch01fn61))
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ML模型*解释*是分析和展示关于ML系统的信息以描述该系统运作方式的过程。这一过程及其使模型易于人类理解的目标通常简称为*可解释性*，在ML方面是一个关键领域。许多人希望理解ML系统为何以某种方式工作，这是理解其对其影响的特定结果的一部分的愿望。^([20](ch06.xhtml#ch01fn61))
- en: For both technical and ethical reasons, it is desirable to have methods that
    can “explain” how an ML model works or why it reached a particular outcome in
    a particular case. The technical motivation for explainable AI is related to controlling
    model quality and possibly learning about the data through the model. Technologists
    who develop explanations of their models can gain insights into why their model
    is working well or poorly, and may even learn something about the underlying domain
    of the data. Ethicists and advocates who seek explainable ML have similar concerns
    but for different reasons. They find it empowering to know how a model came to
    a conclusion, so that the conclusion can make sense to those affected by it, or
    even be challenged where the ML conclusion doesn’t make sense.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 出于技术和伦理原因，希望有能够“解释”ML模型工作方式或为何在特定案例中达成特定结果的方法是可取的。解释AI的技术动机与控制模型质量以及通过模型了解数据有关。开发模型解释的技术人员可以深入了解为什么他们的模型运行良好或不佳，并可能了解到关于数据基础领域的一些内容。寻求可解释ML的伦理学家和倡导者出于不同的原因有类似的关注。他们发现了解模型是如何得出结论的对于那些受其影响的人来说是有帮助的，甚至可以在ML的结论不合理时提出质疑。
- en: However, explanation is not a simple thing. Explanations can serve any of a
    number of purposes and therefore quite a wide variety of information can constitute
    an explanation, depending on the purpose for which it is sought and the audience
    for whom it is prepared. In this way, model explanation can feel a lot like thinking
    through the problems that highly trained professions, such as doctors or lawyers,
    face in trying to give advice or a diagnosis of a real-world situation to a particular
    audience. For example, when doctors explain a recommended treatment and its associated
    risks, they will tune the explanation to their audience. They’d probably give
    one explanation to a fellow physician, another less technical but still rigorous
    explanation to a patient known to have a bioengineering degree, and still another
    to a person who was suffering from dementia but who might still be competent to
    make their own medical decisions. Thus, we can see that an explanation depends
    on the *audience* or *consumer* of that explanation.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，解释并不是一件简单的事情。解释可以服务于许多目的，因此，根据所需目的和准备的受众，解释所包含的信息可能多种多样。从这个角度看，模型解释可能会感觉很像高度训练的专业人士（如医生或律师）在尝试向特定受众提供建议或诊断真实世界情况时面临的问题。例如，当医生解释推荐的治疗及其相关风险时，他们会根据受众调整解释。他们可能会向一位同行医生提供一个解释，向一个已知拥有生物工程学位的患者提供另一个不那么技术性但仍严谨的解释，并向患有痴呆症但仍可能能够作出自己医疗决定的人提供另一个解释。因此，我们可以看出，解释取决于那个解释的受众或消费者。
- en: The explanation that is provided also depends on its *purpose*. If the purpose
    of an ML explanation is to inspect model quality, such as to make sure the model
    is making the right decisions for the right reasons, then a global explanation
    might be preferred. A *global* explanation indicates how the model works generally
    and why certain general decision rules will be followed. On the other hand, if
    the purpose of the explanation is to enable a specific person who was refused
    credit (or had another undesired outcome) to know why in a way that could help
    her improve her chances in the future, that person would want a local explanation.
    A *local* explanation explains why that person in particular was refused and the
    most actionable adjustments to possibly make in the future.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的解释还取决于其*目的*。如果机器学习解释的目的是检查模型质量，例如确保模型出于正确原因做出正确决策，则可能更喜欢全局解释。*全局*解释指出模型通常如何工作以及为什么会遵循某些一般决策规则。另一方面，如果解释的目的是让某个特定被拒绝信用（或遭遇其他不良结果）的人了解为什么以一种方式，可以帮助她将来改善机会，那么此人会希望得到局部解释。*局部*解释解释了为什么特定的人被拒绝以及可能在将来采取的最具可操作性的调整。
- en: 'We won’t get into more detail on explanations for the moment. For our purposes
    here, we want you to walk away understanding these key points:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们暂时不会详细讨论解释。在这里，我们希望您了解以下关键点：
- en: There is no one single correct explanation of a model.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的解释没有单一正确的解释。
- en: Explanations need to be tuned to audience and purpose.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释需要根据受众和目的进行调整。
- en: Explanations need to be useful and sometimes actionable.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释需要有用，有时还需要采取行动。
- en: 'In terms of what you should do concretely, the following could be the minimum
    steps to take at the start of your journey to get better acquainted with ML explanation
    techniques:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在具体实施方面，您在开始了解机器学习解释技术时可能需要采取的最低步骤如下：
- en: At the least, it is helpful for an end user to know the inputs used for an algorithm
    (even this minimal information is often not available). It is even better if you
    can provide a list of relative feature importances. This would be most meaningful
    if placed in a prominent location and in accessible language so that ordinary
    people can see and understand the model.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少，对于最终用户来说，知道算法使用的输入是有帮助的（即使这些最小信息通常不可用）。如果您能提供相关特征重要性的列表，那就更好了。如果能够将其放在显著位置，并用易于理解的语言编写，以便普通人可以看到和理解模型，那将是最有意义的。
- en: Another easy way to get some intuitive and concrete information that can be
    explanatory is to generate test inputs, perhaps counterfactual pairings, and see
    what these look like. For example, some counterfactual pairings shouldn’t matter
    for some use cases (e.g., gender counterfactuals should not change credit decisions),
    while others most certainly should (e.g., body weight counterfactuals probably
    will often change medical intervention decisions). These can constitute a basic
    smell test as well as a way of providing example explanations to those who are
    the decision subjects.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种获取直观和具体信息的简单方法是生成测试输入，也许是反事实配对，并查看它们的情况。例如，对于某些用例，某些反事实配对不应该影响决策（例如，性别反事实应该不会改变信用决策），而对于其他用例，它们很可能会（例如，体重反事实可能经常会改变医疗干预决策）。这些可以构成基本的“嗅探”测试，同时也是为决策主体提供示例解释的一种方式。
- en: Contemplate whether you can offer global (explaining the model overall) or local
    (explaining a particular ML model decision/classification) explanations and explore
    a few techniques for doing so. Is a certain technique particularly appropriate
    for your audience and the kind of decisions your ML product is making?
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑你是否能够提供全局（解释整体模型）或局部（解释特定 ML 模型决策/分类）的解释，并探索一些实现这些解释的技术。某种技术是否特别适合您的受众以及您的
    ML 产品正在做出的决策类型？
- en: As various explanation techniques have emerged, some researchers have offered
    useful guidance as to what systems might serve different purposes and be appropriate
    to use.^([21](ch06.xhtml#ch01fn62)) The most important consideration is to determine
    the level of sophistication of the end user of the explanation and the purpose
    of providing an explanation. From there, you will often be able to identify at
    least one, and usually more, technical options, some open source and already implemented
    by experts.^([22](ch06.xhtml#ch01fn63))
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 随着各种解释技术的出现，一些研究人员提供了有关什么系统可能为不同目的服务并且适合使用的有用指导。^([21](ch06.xhtml#ch01fn62))
    最重要的考虑因素是确定解释的最终用户的复杂程度以及提供解释的目的。从这里开始，通常可以识别至少一个，通常更多的技术选项，一些是开源的并已由专家实施。^([22](ch06.xhtml#ch01fn63))
- en: Effectiveness
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 效果
- en: 'ML *effectiveness*—that is, an ML product actually achieving its desired target,
    and for the right reasons—is key to responsibly deploying ML. Yet, as highlighted
    quite famously by Cathy O’Neil’s *Weapons of Math Destruction* (Crown, 2016),
    a particularly worrying element of ML is that in many deployment scenarios, ML
    can be a self-fulfilling prophecy: ML might appear to work, but this could be
    for entirely wrong reasons.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ML *效果* — 即，ML 产品实际上达到其预期目标，并且出于正确的原因 — 是负责任地部署 ML 的关键。然而，正如凯西·奥尼尔在她的《*数学毁灭武器*》（Crown，2016）中所强调的那样，ML
    的一个特别令人担忧的元素是，在许多部署场景中，ML 可能会成为一个自我实现的预言：ML 看起来可能有效，但这可能完全是基于错误的原因。
- en: Consider, as O’Neil did, ML products for hiring purposes, in which candidates
    can be flagged to be rejected automatically. Perhaps the ML algorithm is correct
    that someone would be a bad hire, but, also quite likely, we may never know whether
    someone was a good hire because we didn’t hire them. But if a variety of ML algorithms
    are all using the same logic to not hire a particular candidate, that candidate
    never gets the chance at a job, and we never actually know whether that candidate
    could have done a good job (systematically missing data).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一下，就像奥尼尔所做的那样，用于招聘目的的 ML 产品，其中候选人可能会被标记为自动拒绝。也许 ML 算法正确地认为某人会是一位糟糕的雇员，但很有可能，我们永远不会知道某人是否会成为一位优秀的雇员，因为我们没有雇用他们。但是，如果各种
    ML 算法都使用相同的逻辑来不雇用特定候选人，那么该候选人永远不会有机会获得一份工作，我们也永远无法真正知道该候选人是否能够胜任工作（系统性缺失数据）。
- en: If someone is labeled by a ML algorithm in a particular way, sometimes that
    label is trusted to settle the issue, even if humans are supposed to be exercising
    some level of supervision.^([23](ch06.xhtml#ch01fn64)) In the employment scenario,
    someone could be labeled a poor candidate and not be hired by the human who is
    using the algorithmic assistance. That job seeker is perhaps not hired only because
    the algorithm labeled them as a poor option rather than because they actually
    were a poor option. And perhaps the use of the same or similar algorithms across
    many potential employers makes the situation even worse. Perhaps that job seeker
    will face the same algorithm deployed by many potential employers, thus facing
    an extended or even indefinite period of joblessness. At some point, their extended
    period of unemployment will itself become another factor that an ML algorithm
    is likely to use as a flag against hiring someone.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某人被一个机器学习算法以某种特定方式标记，有时这个标记会被信任以解决问题，即使人类应该在一定程度上进行监督。^([23](ch06.xhtml#ch01fn64))
    在就业场景中，某人可能被标记为不合格的候选人，并且因为使用算法辅助决策的人认为他们不适合被雇用。也许这个求职者之所以不被雇用，并不是因为他们实际上是不合适的选项，而是因为算法标记他们为不合格选项。也许在许多潜在雇主中使用相同或类似的算法使得这种情况变得更糟。也许这个求职者将面临许多潜在雇主都在使用的相同算法，因此可能面临延长甚至无限期的失业。在某些时候，他们长期的失业本身可能会成为机器学习算法使用的另一个标志来反对雇佣某人的因素。
- en: This concern isn’t just limited to scenarios that O’Neil identifies in potential
    anecdotes. Lawmakers too are concerned. For example, in the recently proposed
    [Algorithmic Justice and Online Platform Transparency Act of 2021](https://oreil.ly/iVwyj),
    US Senator Ed Markey and US Representative Doris Matsui have proposed that only
    safe and effective algorithms be legal. The effectiveness of an algorithm would
    be established by showing that the ML algorithm “has the ability to produce its
    desired or intended result.”^([24](ch06.xhtml#ch01fn65))
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题不仅仅限于 O’Neil 在潜在轶事中提到的情景。立法者们也很关注。例如，在最近提出的[2021 年算法公正和在线平台透明法案](https://oreil.ly/iVwyj)中，美国参议员埃德·马基和美国众议员多丽丝·松井提议，只有安全有效的算法才能合法。算法的有效性将通过展示机器学习算法“具有产生其期望或预期结果的能力”来确立。^([24](ch06.xhtml#ch01fn65))
- en: Given this law, or similar requirements of efficacy, we can think through how
    O’Neil’s example of self-fulfilling hiring algorithms could play out. The designer
    of a hiring ML algorithm should show that the algorithm actually predicted who
    would be a good employee, rather than simply who was unlikely to be hired. One
    way of thinking about this is *external validity*—that is, the notion that the
    findings or conclusions that power an algorithm translate to a general and real-world
    concept that matters. A requirement of showing efficacy is akin to a requirement
    that the algorithm’s logic is externally valid, that it generalizes to some reasonable
    degree.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这项法律或类似的有效性要求，我们可以思考一下 O’Neil 提到的自我实现招聘算法的例子可能如何发展。招聘机器学习算法的设计者应该展示该算法实际上预测了谁会是一个优秀的员工，而不仅仅是谁不太可能被聘用。从某种角度来看，这就是
    *外部有效性* —— 即，算法驱动的发现或结论是否可以转化为一个重要的泛化到真实世界的概念。展示有效性的要求类似于算法的逻辑具有外部有效性，即它在合理程度上具有普遍性。
- en: 'Self-fulfilling algorithms are, of course, not the only concern related to
    algorithmic effectiveness. Some other terms related to this concept are briefly
    described here:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，自我实现的算法并不是与算法有效性相关的唯一问题。与这一概念相关的其他一些术语在此简要描述：
- en: Robustness
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 鲁棒性
- en: Algorithms can be effective only where they are resistant to foreseeable attacks
    or misuse and are designed to limit or prevent such foreseeable abuse.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 算法只有在能够抵抗可预见的攻击或滥用，并且设计为限制或防止这种可预见的滥用时，才能发挥有效作用。
- en: Validated performance
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 验证的性能
- en: Models must work in their deployed use cases. Models should be checked for good
    performance anytime they are deployed in a new situation, such as on a new population,
    or even simply being applied over a lengthy period of time.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 模型必须在其部署的使用案例中发挥作用。每当在新情况下部署模型时，如在新的人群中或者只是在长时间内应用时，都应检查模型的性能是否良好。
- en: Logic
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑
- en: While some celebrate ML on the basis that such models can “find patterns that
    humans can’t see,” sometimes the patterns identified don’t make any sense, or
    make sense for the wrong reason. If the logic of a particular input’s relevance
    doesn’t pass a basic smell test, it’s time to ask questions. Requiring some degree
    of logic is also consistent with Responsible AI goals related to efficacy.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有些人因为这些模型可以“发现人类看不见的模式”而庆祝ML，有时候识别出的模式却毫无意义，或者由于错误的原因而有意义。如果特定输入的相关逻辑不通过基本的“嗅觉测试”，那么现在是提出问题的时候了。要求一定程度的逻辑也与与效果相关的负责任AI目标保持一致。
- en: Social and Cultural Appropriateness
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 社会和文化的适当性
- en: Another element of consideration for the responsible use of ML relates to the
    acceptable use of technology more generally in social situations or with social
    ramifications. Are all roles, as observer or decider, appropriate for a machine?
    For example, would you want to be told that you were going to die of a terrible
    illness…by an algorithm? Likewise, do you want your child “watched” by an ML product
    or by a human babysitter? Perhaps the answer is that you don’t care, and perhaps
    the answer is that you do. Sometimes, when people refuse algorithmic products
    it is because of ideas related to human dignity or social respect, rather than
    safety concerns about an algorithmic product. That is, just because something
    can be automated well doesn’t guarantee that people will feel respected when they
    are interacting with an algorithm.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 用于负责任地使用ML的另一个考虑因素是技术在社会情境中或在社会影响方面的一般可接受性。机器作为观察者或决策者的所有角色是否都适用？例如，您想通过算法被告知将死于可怕的疾病吗？同样，您希望您的孩子被ML产品“监视”还是由人类保姆？也许答案是您不在乎，也许答案是您在乎。有时候，人们拒绝算法产品是因为与人类尊严或社会尊重相关的想法，而不是对算法产品的安全性问题。也就是说，仅仅因为某事可以很好地自动化，并不保证人们在与算法交互时会感到受到尊重。
- en: These cultural concerns about human dignity can apply to the users of an ML
    product, rather than merely the subjects. Consider, for example, a recent Twitter
    Responsible AI study in which the team ultimately concluded that the best way
    to make a feature fair (in this case, a photo-cropping feature) was to [remove](https://oreil.ly/YsWXs)
    the feature rather than refine it, in part to encourage autonomy and agency by
    those who were posting photos. Sometimes the best solution is the remove a technical
    “solution.”
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 关于人类尊严的这些文化关切可能适用于ML产品的用户，而不仅仅是对象。例如，考虑最近的Twitter负责任AI研究，在这项研究中，团队最终得出结论，使一个功能公平（在这种情况下是一个照片裁剪功能）的最佳方式是[删除](https://oreil.ly/YsWXs)这个功能，而不是完善它，部分原因是为了鼓励发布照片的人的自主权和代理权。有时候，最好的解决方案是移除一个技术“解决方案”。
- en: Responsible AI Along the ML Pipeline
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负责任AI沿着ML管道
- en: The various specific concerns related to Responsible AI that we’ve discussed
    previously will necessarily overlap in the course of a real-world ML pipeline.
    In this section, we include specific points to consider relating to pragmatic
    questions you should pose to yourself and your team, depending on where along
    the ML pipeline you find yourself.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论过的与负责任AI相关的各种具体问题将在现实世界的ML管道中必然重叠。在本节中，我们包括特定的考虑点，涉及您应该根据您所处ML管道的位置向自己和团队提出的实际问题。
- en: Use Case Brainstorming
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用例头脑风暴
- en: 'If you are brainstorming potential use cases, perhaps because you see a new
    business opportunity or see the possibility of obtaining new data, you should
    be thinking about the following fundamental questions related to your potential
    project:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在进行潜在用例的头脑风暴，也许是因为您看到一个新的商业机会或看到获取新数据的可能性，您应该考虑与您的潜在项目相关的以下基本问题：
- en: Is this a use case that will undermine privacy, and if so, how will you take
    precautions from the start to build in privacy protections?
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个可能会损害隐私的用例吗？如果是，您将如何从一开始就采取预防措施来构建隐私保护？
- en: Is this a use case that touches on fundamental concerns about human dignity
    or social expectations, which could add additional limitations to the scope of
    appropriate use of algorithmic approaches?
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个涉及到人类尊严或社会期望基本关注点的用例吗？这可能会对算法方法的适用范围增加额外的限制吗？
- en: Is the decision or classification to be made by ML an important decision, where
    fairness should be particularly guarded, and if so, are there indicators that
    this can be accomplished? What constitutes *important* will vary, but sometimes
    these are understood to be decisions with legal effect or legal-like effects (such
    as hiring or education). Another way of defining *important* would be to first
    identify those models in your own organization as areas where a mistaken decision
    would have the most significant impact for a decision subject.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习决策或分类是否是一个重要决策，公平性应特别受到保护，如果是，是否有迹象表明这可以实现？什么构成*重要*会有所不同，但有时这些被理解为具有法律效力或类似法律效果的决策（如招聘或教育）。另一种定义*重要*的方式是首先确定你自己组织中那些会对决策结果产生最重大影响的领域的模型。
- en: Data Collection and Cleaning
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据收集和清洗
- en: 'At this point in the pipeline, you have decided on a use case and are looking
    for data and preparing data for modeling. Now, the following concerns should be
    addressed:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在管道的这一阶段，你已经确定了一个使用案例，并正在寻找数据并为建模做准备。现在，应解决以下问题：
- en: Will data be acquired in a way that respects informed consent?^([25](ch06.xhtml#ch01fn66))
    Have you disclosed the purposes for which you will use the data to the subjects
    in a reasonably informative and transparent way?
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据是否以一种尊重知情同意的方式获取？^[25](ch06.xhtml#ch01fn66)你是否以合理的信息透明方式向受试者披露将使用数据的目的？
- en: Have you stored data in a manner that promotes privacy and minimizes the likelihood
    of unintentional disclosures?
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否以促进隐私并最大程度减少意外披露的方式存储数据？
- en: Have you done exploratory analyses of the data to look for potential bias in
    that data?
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是否进行了数据的探索性分析，以查找数据中的潜在偏差？
- en: Model Creation and Training
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型创建和训练
- en: 'Now you are in receipt of data, and it’s time to do some modeling:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经收到数据，是时候进行一些建模了：
- en: Have you come up with an affirmative plan to monitor and address bias? How will
    you choose from various forms of fairness interventions based on the potential
    harms of bias and based on the particular normative values or legal restrictions
    of your use case?
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是否制定了积极的计划来监测和解决偏见？你将如何根据偏见可能带来的潜在危害以及你的使用案例的特定规范价值或法律限制，选择各种公平干预形式？
- en: Are you training in a manner that will reduce data leaks from the trained model
    and enhance robustness against [malicious attacks](https://arxiv.org/abs/1412.6572)?
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是否以一种方式进行训练，可以减少训练模型的数据泄漏，并增强对[恶意攻击](https://arxiv.org/abs/1412.6572)的鲁棒性？
- en: Have you considered in your loss function the relative degree of harm from different
    mistakes, rather than lazily setting all mistakes to the same loss value?
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的损失函数中，是否考虑了不同错误造成的相对危害程度，而不是懒惰地将所有错误都设置为相同的损失值？
- en: Have you chosen your model architecture (say, opaque neural network versus interpretable
    linear function) by understanding the relative importance of accuracy and explainability
    for your particular application?
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在选择模型架构（例如，不透明的神经网络与可解释的线性函数）时，是否通过理解准确性和可解释性对你的特定应用的相对重要性来做出选择？
- en: If your domain has something like scientific laws with strong predictive value,
    have you included this domain knowledge in your model architecture and training
    choices?
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的领域有类似具有强预测价值的科学法则，你是否在模型架构和培训选择中包含了这些领域知识？
- en: Model Validation and Quality Assessment
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型验证和质量评估
- en: 'At this stage of the pipeline, you may receive a model that you are told is
    as good as it is going to be in terms of accuracy. Your job is to kick the tires
    in a reasoned way and decide whether to give this model approval to go forward,
    or send it back to training:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在管道的这个阶段，你可能会收到一个被告知在准确性方面已经达到最佳的模型。你的工作是以理性的方式检查并决定是否批准此模型继续前进，或者将其送回进行训练：
- en: Have you asked whether the model was trained on a proxy, and if so, what data
    is available to justify the use of that proxy?
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是否询问过模型是否是在代理上训练的，如果是，是否有数据可证明使用该代理的合理性？
- en: Have you tested the model robustly, with a fair selection of held-out data in
    realistically challenging situations? Basic accuracy remains an ethical obligation
    as well as a business target and technical measure.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是否在现实挑战情况下，充分测试了模型，使用了公正选择的保留数据集？基本准确性仍然是道德义务，也是商业目标和技术指标。
- en: Are you able to identify and understand the logic driving the model globally
    and to generate individual explanations in case an individual might ask for them?
    (See [Chapter 9](ch09.xhtml#monitoring_and_observability_for_models) for more
    information.)
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你能否全局识别和理解推动模型的逻辑，并在需要时生成个体解释？（有关更多信息，请参见[第9章](ch09.xhtml#monitoring_and_observability_for_models)。）
- en: Model Deployment
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型部署
- en: 'Now it’s time to make the model available for its planned use:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候让模型按计划使用了：
- en: Have you put in place monitoring programs to continually assess the performance
    of the system in actual use? Such performance could be assessed with respect to
    traditional performance metrics (such as accuracy) as well as with respect to
    fairness metrics (as discussed earlier).
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是否已经建立了监控程序，持续评估系统在实际使用中的性能？这样的性能评估可以根据传统性能指标（如准确性）以及公平性指标（如前面讨论的那样）进行评估。
- en: Have you established ex ante criteria to assess whether the model is working
    as expected?^([26](ch06.xhtml#ch01fn67))
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是否已经建立了事先的标准来评估模型是否按预期工作？^([26](ch06.xhtml#ch01fn67))
- en: Have you tried running the model in an online mode so that you can watch how
    it performs, counterfactually, in advance of an actual product launch? Or in the
    alternative, you can maintain a shadow model, which makes predictions on productive
    data even when users cannot see the results. This can be an intermediate step
    to understanding likely deployment performance without running the risks associated
    with actual deployment.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是否尝试在线模式运行模型，以便您可以提前观察其如何表现，即使是在实际产品发布之前的反事实情况？或者您可以维护一个影子模型，在用户看不到结果的情况下对生产数据进行预测。这可以作为理解可能的部署性能的中间步骤，而无需承担实际部署所带来的风险。
- en: Products for the Market
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 市场产品
- en: Whether models are intended for internal or external use, they need to meet
    the same standards for fairness and privacy protections. But models that will
    be directly accessed by external users generally have an additional set of requirements
    that must be met.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 无论模型是用于内部还是外部使用，它们都需要符合公平性和隐私保护的同样标准。但是，直接由外部用户访问的模型通常有一组额外的必须满足的要求。
- en: 'If the end goal for your model is to be directly accessed by human users, consider
    the following:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的模型最终目标是直接由人类用户访问，请考虑以下几点：
- en: Will you make available a recourse or method to challenge a decision, and if
    so how?
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是否会提供救济或挑战决定的方法？如果会，具体是怎样的方法？
- en: Will you make available explanations about how the ML system works, possibly
    even specific guidance for individuals about how their outcomes were formed?^([27](ch06.xhtml#ch01fn68))
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是否会提供关于机器学习系统如何运行的解释，甚至可能为个人提供关于他们成果形成方式的具体指导？^([27](ch06.xhtml#ch01fn68))
- en: How will you detect events or problems you haven’t anticipated? How will you
    learn about what you currently do not know that you do not know?
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将如何检测到未预料到的事件或问题？你将如何了解你目前不知道的那些你不知道的内容？
- en: Conclusion
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: We have reviewed many fairness, privacy, and other ethical considerations that
    affect the design, training, and deployment of real-world ML systems. These topics
    are all highly complex. This chapter should give you and your organization a sampling
    of key issues you should factor into designing and deploying ML systems. To progress
    further, many excellent learning materials and academic research papers are out
    there on all these topics for you to look for more information.^([28](ch06.xhtml#ch01fn69))
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经审查了许多涉及设计、训练和部署现实世界机器学习系统的公平性、隐私和其他伦理考量。这些话题都非常复杂。本章应该为您和您的组织提供了一些关键问题的示例，您应该在设计和部署机器学习系统时考虑这些问题。要进一步了解，您可以找到许多优秀的学习资料和学术研究论文，涵盖所有这些话题，供您获取更多信息。^([28](ch06.xhtml#ch01fn69))
- en: 'In the meantime, this complexity shouldn’t discourage you! We recommend the
    following steps:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，这种复杂性不应使您气馁！我们建议采取以下步骤：
- en: Create basic institutional rules and safeguards in your workplace.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在工作场所创建基本的机构规则和保障措施。
- en: Human-readable changes can be easier to understand and more transparent than
    computational techniques.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与计算技术相比，人类可读的变更可能更容易理解和更透明。
- en: Institutional changes encourage people to bring up ethical concerns at meetings.
    It’s better for fairness not to be off in a dark corner somewhere. Try raising
    your hand and mentioning a fairness concern you have. Start with a small one (data
    access, a recourse interface, statistical parity), and you can work your way up
    to more sophisticated targets over time. Over time, organizations can move even
    further—for example, by creating a *red team* that intentionally and proactively
    tries to manipulate products or identify harmful outcomes.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组织变革鼓励人们在会议上提出伦理关切。公平最好不要在黑暗角落中。试着举手提出你的公平关切。从小事情开始（数据访问、追索接口、统计平等），随着时间推移，你可以逐步提高到更复杂的目标。随着时间的推移，组织甚至可以更进一步，例如创建一个*红队*，有意识地和积极地尝试操纵产品或识别有害结果。
- en: Ethical guidelines can also serve as a high-level but practical checklist for
    initiating new projects or approving the launch of completed products.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 伦理指南也可以作为启动新项目或批准已完成产品发布的高层次但实用的检查清单。
- en: Address fairness, privacy, and ethics concerns routinely at product inception.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在产品初始阶段定期解决公平、隐私和伦理问题。
- en: Most fairness, privacy, and other ethical problems in ML originate at the level
    of product conception and creation. Start asking relevant questions when contemplating
    whether a project is even a good idea at the outset.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数机器学习中的公平、隐私和其他伦理问题都源于产品的概念和创建阶段。在考虑一个项目是否甚至是一个好主意时，开始提出相关问题是非常重要的。
- en: Are you using fair data that you obtained in a legal and ethical way? Guidance
    from [Gebru et al. (2021)](https://oreil.ly/6Vo2R) can provide a great framework
    for documenting your data and its appropriate use.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是否在合法和道德的方式获得了公平的数据？来自[Gebru 等人 (2021)](https://oreil.ly/6Vo2R)的指导可以为记录你的数据及其适当使用提供一个很好的框架。
- en: Are appropriate working conditions in place to maintain the security and privacy
    of data?
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有适当的工作条件来维护数据的安全和隐私？
- en: If you succeed at the goal you propose, will that be a good (or at least neutral)
    thing for the world at large?
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你成功实现你提出的目标，这对整个世界来说会是一个好事（或至少是中立的）吗？
- en: Create a virtuous cycle of Responsible AI practices.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建负责任人工智能实践的良性循环。
- en: The more you learn and the longer you keep Responsible AI concerns in mind,
    the more your ML pipeline and ML offerings will reflect good fairness practices.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学到的越多，保持负责任的人工智能关注的时间越长，你的机器学习管道和机器学习产品将反映良好的公平实践。
- en: If you can make a commitment to learning a little about Responsible AI each
    week—and implementing it slowly in your own work—at the end of a year you’ll note
    a remarkable degree of progress.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你能承诺每周学习一点关于负责任人工智能的内容，并且在自己的工作中慢慢实施它，在一年结束时你会注意到显著的进展。
- en: We wish you good luck as you begin practical steps to make the world a better
    place, starting with your ML products (including, sometimes, not using them).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 祝你好运，因为你开始采取实际步骤，让世界变得更美好，从你的机器学习产品开始（有时，包括不使用它们）。
- en: ^([1](ch06.xhtml#ch01fn42-marker)) Examples of designated human decision makers
    include judges as the formal decision makers in legal adjudication, or university
    professors as the formal decision makers in grading decisions. Examples of no
    clear decision makers include “flat” organizations, in which it can sometimes
    be unclear who holds final decision-making authority.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch06.xhtml#ch01fn42-marker)) 指定的人类决策者的例子包括法官作为法律裁决的正式决策者，或大学教授作为评分决策的正式决策者。没有明确决策者的例子包括“扁平”组织，在这些组织中有时不清楚谁持有最终决策权。
- en: ^([2](ch06.xhtml#ch01fn43-marker)) As a general matter, we take issue with this
    undue narrowing of the concept of fairness, and for the sake of clear communication,
    we address a host of other issues that we view as related to *fairness* in [“Responsible
    AI”](#responsible_ai). In this section, we focus on the mainstream use of *fairness*
    as exclusively related to bias.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch06.xhtml#ch01fn43-marker)) 一般而言，我们对公平概念的不适当缩窄表示反对，并且为了明确沟通，我们解决了许多其他问题，我们认为这些问题与*公平*相关，在[“负责任人工智能”](#responsible_ai)部分中，我们重点讨论了作为与偏见相关的*公平*的主流用法。
- en: ^([3](ch06.xhtml#ch01fn44-marker)) [“Machine Bias”](https://oreil.ly/38he8)
    by Julia Angwin et al. is the seminal study, published by ProPublica, that identified
    this bias. This work, discussing the use of algorithmic risk scores in the US
    criminal justice system, is foundational in the understanding of the kind of harms
    that AI can create and in launching the Responsible AI movement. The article discusses
    the use of algorithmic risk scores in the criminal justice system in the US. The
    most common use of these biased risk scores is to determine whether defendants
    should be allowed free while awaiting trial in an overly burdened criminal justice
    system. The factor being predicted in this case should be whether the accused
    will show up for trial and whether they will commit crimes in the meantime, not
    generally at any point in the future. It is not clear that a single prediction
    about “will be accused of a crime in the future” can be useful in decisions about
    bail, sentencing, and parole. This article discusses the COMPAS algorithm, which
    is discussed in some detail later in this chapter.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch06.xhtml#ch01fn44-marker)) [“机器偏见”](https://oreil.ly/38he8)由Julia Angwin等人撰写，是ProPublica发布的开创性研究，揭示了这种偏见。这项工作讨论了在美国刑事司法系统中使用算法风险评分的情况，对理解AI可能造成的伤害以及发起负责任AI运动具有基础性意义。文章讨论了在美国刑事司法系统中使用算法风险评分的情况。这些有偏见的风险评分最常用于确定在负担过重的刑事司法系统中，被告是否应该在等待审判期间获得自由。在这种情况下，被预测的因素应该是被告是否会出庭并且在此期间是否会犯罪，而不是一般将来的任何时候。目前尚不清楚“将来是否会被指控犯罪”的单一预测能否在保释、判刑和假释决策中有用。本文讨论了稍后在本章节中详细讨论的COMPAS算法。
- en: ^([4](ch06.xhtml#ch01fn45-marker)) This was one of several factors that led
    some governments, notably that of New York City, to propose regulation of AI when
    used in hiring. See [“New York City Proposes Regulating Algorithms Used in Hiring”](https://oreil.ly/xjN4P)
    by Tom Simonite.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch06.xhtml#ch01fn45-marker)) 这是一些因素之一，导致一些政府，尤其是纽约市政府，提议对在招聘中使用的AI进行规范。参见Tom
    Simonite的[“纽约市提议规范招聘中使用的算法”](https://oreil.ly/xjN4P)。
- en: ^([5](ch06.xhtml#ch01fn46-marker)) A larger debate arises about whether discrimination
    is “taste based” or “statistical.” Some argue that the decision to attend a women’s
    college might in some way be indicative of a personality type that is relevant
    to a hiring decision. However, such a hypothesis seems unlikely to apply. Those
    interested in learning more can look to research literature in both law and economics
    about the mechanisms of and motivations for discrimination, including disparate
    treatment.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch06.xhtml#ch01fn46-marker)) 关于歧视是“品味导向”还是“统计导向”的辩论更为广泛。一些人认为决定就读女子学院可能在某种程度上表明与雇佣决策相关的人格类型。然而，这样的假设似乎不太可能适用。有兴趣了解更多的人可以查阅法律和经济学中关于歧视机制和动机的研究文献，包括不平等对待。
- en: ^([6](ch06.xhtml#ch01fn47-marker)) We use scare quotes in this sentence because,
    of course, defining what or who is “the same” or “of equal merit” is itself an
    exercise of judgment and not an objective truth.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch06.xhtml#ch01fn47-marker)) 我们在这个句子中使用引号，因为当然，定义“相同”或“同等的价值”的内容本身就是一种判断的行为，而不是客观的真理。
- en: '^([7](ch06.xhtml#ch01fn48-marker)) For an accessible demonstration of the conflict
    between calibration and group parity, we strongly recommend [“Fair Prediction
    with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments”](https://arxiv.org/abs/1610.07524)
    by Alexandra Chouldechova.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch06.xhtml#ch01fn48-marker)) 为了清楚地展示校准与群体平等之间的冲突，强烈推荐阅读Alexandra Chouldechova的[“具有不同影响的公平预测：关于刑事再犯预测工具偏见的研究”](https://arxiv.org/abs/1610.07524)。
- en: ^([8](ch06.xhtml#ch01fn49-marker)) See the [Chouldechova paper](https://arxiv.org/abs/1610.07524)
    referenced in the preceding footnote.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch06.xhtml#ch01fn49-marker)) 参见上文脚注中提到的[Chouldechova文章](https://arxiv.org/abs/1610.07524)。
- en: ^([9](ch06.xhtml#ch01fn50-marker)) If we look for and interpret questions about
    algorithmic fairness broadly, other impossibility theorems are related to fairness
    concerns. For example, Arrow’s impossibility theorem demonstrates that three intuitive
    fairness criteria for a specific form of voting cannot all be met at the same
    time. These three criteria, stated quite roughly, are that (1) universally shared
    individual preferences between two options will necessarily translate into an
    election outcome reflecting that preference, (2) stable individual preferences
    will translate into stable election outcomes, and (3) no single voter will possess
    the power to determine the election outcome. Thus it is a common problem across
    disciplines and technological or organizational mechanisms that we cannot always
    have our fairness cake and eat it too. Many thanks to Niall Murphy for this example.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch06.xhtml#ch01fn50-marker)) 如果我们广泛寻找和解释关于算法公平性的问题，其他不可能定理与公平性关切相关。例如，阿罗不可能定理表明，在特定形式的投票中，三个直观的公平性标准不能同时满足。这三个标准，粗略地说，分别是：（1）在两个选项之间普遍共享的个人偏好必然会转化为反映该偏好的选举结果，（2）稳定的个人偏好会导致稳定的选举结果，以及（3）没有单一选民会拥有决定选举结果的权力。因此，无论是跨学科还是技术或组织机制，我们并非总能在公平性上一举两得。非常感谢Niall
    Murphy提供的这个例子。
- en: ^([10](ch06.xhtml#ch01fn51-marker)) For example, individuals from a favored
    group who receive an improbably favorable outcome given their merit.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch06.xhtml#ch01fn51-marker)) 例如，来自受青睐群体的个人，他们由于他们的优点而获得了难以置信的有利结果。
- en: ^([11](ch06.xhtml#ch01fn52-marker)) This can sometimes be understood as a way
    of correcting where calibration is lacking. For example, it is well documented
    that credit scores used in the US do not seem to mean the same thing across racial
    groups with respect to propensity to repay. So banks that rely exclusively on
    the score and do not adjust the thresholds could inadvertently label credit seekers
    with false-negative rates that are different by racial group. However, different
    thresholds for different groups necessarily raises other legal and ethical concerns.
    And so, the reader can see that fairness in the real world can be quite challenging
    indeed.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch06.xhtml#ch01fn52-marker)) 有时这可以理解为校正缺乏校准的方式。例如，有充分证据表明，在美国使用的信用评分在不同种族群体中的还款意愿方面似乎并不相同。因此，完全依赖评分而不调整阈值的银行可能会无意中标记出不同种族群体的信用申请者存在不同的假阴性率。然而，为不同群体设定不同的阈值必然引起其他法律和道德上的关切。因此，读者可以看到，在现实世界中，公平性确实是非常具有挑战性的。
- en: ^([12](ch06.xhtml#ch01fn53-marker)) At the time this book is going to press
    (August 2022), Europe has put forth a wide range of groundbreaking legislation
    on a host of issues that relate to fairness in digital environments, including
    AI. Most notably, the [EU published a proposed AI regulatory framework in April
    2021](https://oreil.ly/b5DZn) that would entail a risk-based set of regulations,
    with different legal requirements for different levels of risk from an AI use
    case. The proposed law would also ban certain high-risk use cases, including social
    credit scoring. Likewise, [China brought sweeping new AI regulations](https://oreil.ly/U5xVq)
    into force as of March 2022 to regulate a variety of common practices that disadvantage
    consumers, including price discrimination on the basis of personal information
    and content-aggregation algorithms.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch06.xhtml#ch01fn53-marker)) 在本书即将出版的时刻（2022年8月），欧洲已经针对数字环境中公平性等一系列问题提出了一系列开创性的立法。尤其值得注意的是，[欧盟于2021年4月发布了一项提议的AI监管框架](https://oreil.ly/b5DZn)，该框架将实施基于风险的一系列规定，不同的AI使用案例将有不同的法律要求。该提议法律还将禁止某些高风险使用情形，包括社会信用评分。同样地，[中国于2022年3月实施了全面的新AI法规](https://oreil.ly/U5xVq)，以规范包括个人信息价格歧视和内容聚合算法在内的多种常见损害消费者利益的做法。
- en: ^([13](ch06.xhtml#ch01fn54-marker)) For an early example of reidentification,
    see [“‘Anonymized’ Data Really Isn’t—And Here’s Why Not”](https://oreil.ly/7nFhr)
    by Nate Anderson, describing Latanya Sweeney’s work on the topic.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch06.xhtml#ch01fn54-marker)) 早期的重新识别案例可以参见Nate Anderson的[“‘Anonymized’
    Data Really Isn’t—And Here’s Why Not”](https://oreil.ly/7nFhr)，描述了Latanya Sweeney在这一主题上的工作。
- en: ^([14](ch06.xhtml#ch01fn55-marker)) Querying a differentially private dataset
    many times can lead to violations of the differential privacy guarantees. This
    is because such guarantees are valid with only a given privacy budget, which limits
    the number of queries that can be made while ensuring differential privacy guarantees.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch06.xhtml#ch01fn55-marker)) 多次查询差分私密数据集可能会导致违反差分隐私保证。这是因为这些保证仅在给定的隐私预算下有效，该预算限制了可以进行的查询次数，同时确保差分隐私保证。
- en: ^([15](ch06.xhtml#ch01fn56-marker)) In theory, we could retrain a model entirely
    every time a bit of training data was marked for deletion. In practice, that would
    be wildly wasteful and impractical—and also possibly unnecessary. More work needs
    to be done in this area to understand how technical needs, sustainability concerns,
    and individual rights to privacy can successfully coexist.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch06.xhtml#ch01fn56-marker)) 理论上，我们可以在每次将培训数据标记为删除时完全重新训练模型。实际上，这将是极其浪费和不切实际的——也可能是不必要的。在这个领域还需要做更多工作，以了解技术需求、可持续性问题和个人隐私权如何成功共存。
- en: ^([16](ch06.xhtml#ch01fn57-marker)) One reason that differentially private modes
    do not achieve the highest levels of accuracy relates to the methods used to create
    differentially private models. One key technique is noise injection, which by
    definition reduces accuracy.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch06.xhtml#ch01fn57-marker)) 差分隐私模式未能达到最高精度水平的一个原因与用于创建差分隐私模型的方法有关。一个关键技术是注入噪音，这从定义上来说会降低准确性。
- en: ^([17](ch06.xhtml#ch01fn58-marker)) One recommended starting point is [TensorFlow
    Privacy](https://github.com/tensorflow/privacy), which includes training algorithms
    for differentially private models.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch06.xhtml#ch01fn58-marker)) 一个推荐的起点是[TensorFlow Privacy](https://github.com/tensorflow/privacy)，其中包括差分私密模型的训练算法。
- en: ^([18](ch06.xhtml#ch01fn59-marker)) A separate question is when click prediction
    is a useful activity and when it raises troubling AI concerns. No doubt, predicting
    someone’s preferences or interests has beneficial uses, as is the case for many
    examples of click prediction. On the other hand, increasing research both on the
    mutability of human preferences generally and on the particular scenario of manipulation
    of behavior and preferences in online environments also points to the danger of
    confusing click prediction that represents preference *accommodation* (giving
    people what they want) with preference *manipulation* (making people want what
    you have). The latter obviously has lots of fairness concerns and is related to
    a growing literature on *dark patterns*, which are digital design patterns that
    tend to lead users of digital products to do things that are against their interests
    (but in the interests of the designers of those products).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ^([18](ch06.xhtml#ch01fn59-marker)) 单独的问题是，当点击预测是一个有用的活动，而当它引起令人担忧的AI问题时。毫无疑问，预测某人的偏好或兴趣具有益处，就像许多点击预测的例子一样。另一方面，关于人类偏好的可变性以及在线环境中行为和偏好操纵的具体场景的增加研究，也指出了混淆代表偏好*适应*（给予人们他们想要的东西）与偏好*操纵*（让人们想要你所拥有的东西）的危险。后者显然存在大量的公平性问题，并且与关于*黑暗模式*的增长文献有关，这些是数字设计模式，倾向于引导数字产品的用户做违背他们利益的事情（但符合那些产品设计者的利益）。
- en: ^([19](ch06.xhtml#ch01fn60-marker)) Of course, the inclusion of demographic
    information may prove problematic, as some datasets may have a strong prediction
    relationship between seemingly anonymous information and PII, and this should
    be factored into the way data separation is conducted.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ^([19](ch06.xhtml#ch01fn60-marker)) 当然，包含人口统计信息可能会带来问题，因为一些数据集可能在看似匿名信息与PII之间具有强烈的预测关系，这应该考虑在数据分离方式中。
- en: ^([20](ch06.xhtml#ch01fn61-marker)) A good starting point for understanding
    the centrality of explainability to Responsible AI, but also the complexity of
    getting it right, is [“Explaining Explanations in AI”](https://oreil.ly/OOeoT)
    by Brent Mittelstadt et al.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ^([20](ch06.xhtml#ch01fn61-marker)) 了解负责AI的可解释性的核心性的一个好的起点，但同时也要理解把握正确的复杂性，是Brent
    Mittelstadt等人的[“在AI中解释解释”](https://oreil.ly/OOeoT)。
- en: '^([21](ch06.xhtml#ch01fn62-marker)) [“Explainable Machine Learning for Public
    Policy: Use Cases, Gaps, and Research Directions”](https://arxiv.org/pdf/2010.14374.pdf)
    by Kasun Amarasinghe et al. provides a great and accessible example of considerations
    that go into what kind of explanation is likely to be useful.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ^([21](ch06.xhtml#ch01fn62-marker)) 由Kasun Amarasinghe等人撰写的[“公共政策中的可解释机器学习：用例、空白和研究方向”](https://arxiv.org/pdf/2010.14374.pdf)提供了一个极好且易于访问的示例，涉及哪些解释类型可能是有用的考虑因素。
- en: ^([22](ch06.xhtml#ch01fn63-marker)) IBM’s [AI Explainability 360 library](https://aix360.mybluemix.net)
    is an easy-to-use open source library that includes a wide variety of cutting-edge
    research methods for model explanation. The toolkit provides an API as well as
    tutorials that provide a wide range of example use cases for applying methods
    from the library via the explainability API.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ^([22](ch06.xhtml#ch01fn63-marker)) IBM的[AI可解释性360库](https://aix360.mybluemix.net)是一个易于使用的开源库，包括多种前沿研究方法用于模型解释。该工具包提供了API以及通过可解释性API应用库方法的多种示例用例教程。
- en: ^([23](ch06.xhtml#ch01fn64-marker)) Consider as an extreme example Poland’s
    rollout of a classification algorithm to categorize unemployed job seekers into
    three categories. An initial algorithmic assessment was made, from which staff
    could, in theory, deviate. In practice, they deviated from the algorithmic label
    in only 0.58% of cases. See [“Profiling the Unemployed in Poland”](https://oreil.ly/eBvcv)
    by Jędrzej Niklas et al.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ^([23](ch06.xhtml#ch01fn64-marker)) 以波兰将分类算法推广至失业求职者分类为极端案例。理论上，工作人员可以偏离初步的算法评估。实际上，他们在0.58%的情况下偏离算法标签。详见Jędrzej
    Niklas等人的[“波兰失业人员概况”](https://oreil.ly/eBvcv)。
- en: ^([24](ch06.xhtml#ch01fn65-marker)) The full quotation from the bill is, “An
    algorithmic process is effective if the online platform employing or otherwise
    utilizing the algorithmic process has taken reasonable steps to ensure that the
    algorithmic process has the ability to produce its desired or intended result.”
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ^([24](ch06.xhtml#ch01fn65-marker)) 法案的完整引用如下：“如果在线平台采用或以其他方式利用算法过程，且已采取合理措施确保算法过程能够产生其预期或预期结果，则算法过程有效。”
- en: ^([25](ch06.xhtml#ch01fn66-marker)) It’s worth noting that defining *consent*
    can be complicated. After all, no one appreciates all the pop-up consent notices
    that have proliferated after GDPR. *Consent* here should be interpreted in a broad
    rather than narrow sense of the word.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ^([25](ch06.xhtml#ch01fn66-marker)) 值得注意的是，定义*同意*可能会很复杂。毕竟，没有人喜欢欧盟一般数据保护条例（GDPR）后广泛出现的所有弹出同意通知。在这里，*同意*应该以广义而非狭义的词义来解释。
- en: ^([26](ch06.xhtml#ch01fn67-marker)) It’s good to specify these in advance so
    you can’t “cheat” or otherwise rationalize poor performance after the fact. This
    doesn’t have to involve rigid adherence to those criteria, but it will keep you
    honest about what constitutes a good job even if you do reassess those criteria
    over time.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ^([26](ch06.xhtml#ch01fn67-marker)) 最好事先明确这些，这样你就不能在事后“作弊”或以其他方式合理化表现不佳。这不一定要严格遵循这些标准，但即使随时间重新评估这些标准，它也会使你对什么构成良好工作保持诚实。
- en: ^([27](ch06.xhtml#ch01fn68-marker)) Model Cards, as previously mentioned, are
    one system to accomplish just this.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ^([27](ch06.xhtml#ch01fn68-marker)) 如前所述，模型卡是实现这一目标的一种系统。
- en: '^([28](ch06.xhtml#ch01fn69-marker)) As a shameless self-promotion, consider
    Aileen Nielsen’s book-length introduction to hands-on ML ethics: [*Practical Fairness*](https://oreil.ly/tsjGP),
    another O’Reilly title.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ^([28](ch06.xhtml#ch01fn69-marker)) 作为毫不掩饰的自我推广，可以考虑阅读Aileen Nielsen的关于实用ML伦理的书籍长篇介绍：[*实用公平性*](https://oreil.ly/tsjGP)，另一本O'Reilly出版的书籍。
