- en: Chapter 2\. Introduction to Spark and PySpark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章。Spark 和 PySpark 简介
- en: The aim of this chapter is to bring you up to speed on PySpark and Spark, giving
    you enough information so you’re comfortable with the tutorials in the rest of
    the book. Let’s start at the beginning. What exactly is Spark? Originally developed
    at UC Berkeley in 2009, Apache Spark is an open source analytics engine for big
    data and machine learning. It gained rapid adoption by enterprises across many
    industries soon after its release and is deployed at massive scale by powerhouses
    like Netflix, Yahoo, and eBay to process exabytes of data on clusters of many
    thousands of nodes. The Spark community has grown rapidly too, encompassing over
    1,000 contributors from 250+ organizations.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是让您快速了解 PySpark 和 Spark，为您提供足够的信息，以便您在本书的其他教程中感到舒适。让我们从开始说起。究竟什么是 Spark？Spark
    最初是在 2009 年由加州大学伯克利分校开发的，是一个用于大数据和机器学习的开源分析引擎。它在发布后很快被企业广泛采用，并且由像 Netflix、Yahoo
    和 eBay 这样的强大力量在成千上万的节点集群上部署来处理 exabytes 级别的数据。Spark 社区也迅速增长，包括来自 250 多个组织的 1,000
    多名贡献者。
- en: Note
  id: totrans-2
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'For a deep dive into Spark itself, grab a copy of [*Spark: The Definitive Guide*](https://oreil.ly/sparkTDG),
    by Bill Chambers and Matei Zaharia (O’Reilly).'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '深入了解 Spark 本身，请阅读 [*Spark: The Definitive Guide*](https://oreil.ly/sparkTDG)，作者是
    Bill Chambers 和 Matei Zaharia（O''Reilly）。'
- en: 'To set you up for the remainder of this book, this chapter will cover the following
    areas:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让您对本书的剩余部分有所了解，本章将涵盖以下领域：
- en: Apache Spark’s distributed architecture
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark 的分布式架构
- en: Apache Spark basics (software architecture and data structures)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark 基础（软件架构和数据结构）
- en: DataFrame immutability
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataFrame 的不可变性
- en: PySpark’s functional paradigm
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark 的函数式编程范式
- en: How pandas DataFrames differ from Spark DataFrames
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pandas 的 DataFrame 与 Spark 的 DataFrame 有何不同
- en: Scikit-learn versus PySpark for machine learning
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于机器学习的 Scikit-learn 与 PySpark 的比较
- en: Apache Spark Architecture
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark 架构
- en: 'The Spark architecture consists of the following main components:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 架构由以下主要组件组成：
- en: Driver program
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动程序
- en: The driver program (aka Spark driver) is a dedicated process that runs on the
    driver machine. It is responsible for executing and holding the `SparkSession`,
    which encapsulates the `SparkContext`—this is considered the application’s entry
    point, or the “real program.” The `SparkContext` contains all the basic functions,
    context delivered at start time, and information about the cluster. The driver
    also holds the DAG scheduler, task scheduler, block manager, and everything that
    is needed to turn the code into jobs that the worker and executors can execute
    on the cluster. The driver program works in synergy with the cluster manager to
    find the existing machines and allocated resources.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动程序（也称为 Spark 驱动程序）是在驱动机器上运行的专用进程。它负责执行和持有 `SparkSession`，其中封装了 `SparkContext`
    —— 这被认为是应用程序的入口点，或者称为“真正的程序”。`SparkContext` 包含所有基本函数、在启动时传递的上下文以及有关集群的信息。驱动程序还持有
    DAG 调度器、任务调度器、块管理器以及将代码转换为工作单位和执行器可以在集群上执行的所有内容。驱动程序与集群管理器协同工作，找到现有的机器并分配资源。
- en: Executor
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 执行器
- en: An executor is a process launched for a particular Spark application on a worker
    node. Multiple tasks can be assigned to each executor. A JVM process communicates
    with the cluster manager and receives tasks to execute. Tasks on the same executor
    can benefit from shared memory, such as the cache, and global parameters, which
    make the tasks run fast.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 执行器是在工作节点上为特定的 Spark 应用程序启动的进程。每个执行器可以分配多个任务。JVM 进程与集群管理器通信并接收要执行的任务。同一执行器上的任务可以从共享内存（例如缓存）和全局参数中获益，这使得任务运行更快。
- en: Note
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: A task is the smallest unit of schedulable work in Spark. It runs the code assigned
    to it, with the data pieces assigned to it.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 任务是 Spark 中可调度工作的最小单位。它运行分配给它的代码，并处理分配给它的数据片段。
- en: Worker node
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点
- en: A worker node, as its name indicates, is responsible for executing the work.
    Multiple executors can run on a single worker node and serve multiple Spark applications.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Worker 节点，顾名思义，负责执行工作。多个执行器可以在单个工作节点上运行，并为多个 Spark 应用程序提供服务。
- en: Cluster manager
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 集群管理器
- en: Together with the driver program, the cluster manager is responsible for orchestrating
    the distributed system. It assigns executors to worker nodes, assigns resources,
    and communicates information about resource availability to the driver program.
    In addition to Spark’s standalone cluster manager, this can be any other cluster
    manager that can manage machines and network capacity, such as Kubernetes,^([1](ch02.xhtml#ch01fn5))
    Apache Mesos,^([2](ch02.xhtml#ch01fn6)) or Hadoop YARN.^([3](ch02.xhtml#ch01fn7))
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 与驱动程序一起，集群管理器负责编排分布式系统。它将执行器分配给工作节点，分配资源，并将有关资源可用性的信息传达给驱动程序。除了Spark的独立集群管理器外，这也可以是任何其他能够管理机器和网络容量的集群管理器，如Kubernetes^([1](ch02.xhtml#ch01fn5))、Apache
    Mesos^([2](ch02.xhtml#ch01fn6))或Hadoop YARN^([3](ch02.xhtml#ch01fn7))。
- en: '[Figure 2-1](#sparkapostrophes_distributed_architectu) shows how these various
    components fit together.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-1](#sparkapostrophes_distributed_architectu)展示了这些不同组件如何互相配合。'
- en: '![](assets/smls_0201.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/smls_0201.png)'
- en: Figure 2-1\. Spark’s distributed architecture
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1\. Spark的分布式架构
- en: Each of these components plays a critical part in orchestrating Spark programs
    at a large scale. A Spark driver can launch multiple jobs within one application,
    each consisting of multiple tasks. However, it cannot launch more than one application,
    as Spark resource management takes place on a per-application basis, not a per-job
    basis. Tasks run on one or more executors and typically process different chunks
    of the data (see [Figure 2-2](#spark_launching_two_jobs)). Notice that executors
    are not assigned dedicated storage, although in [Figure 2-2](#spark_launching_two_jobs)
    you can see that storage is attached to the executors. In some deployments, like
    on-prem Hadoop clusters, storage can be local to executors, but often in cloud
    solutions, this is not the case; in cloud deployments (AWS, Azure, etc.) there
    is a separation between storage and compute. In general, Spark prefers to schedule
    tasks that access local data, but assigning tasks and executors to local data
    is not a requirement.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这些组件各自在大规模编排Spark程序中发挥关键作用。Spark驱动程序可以在一个应用程序内启动多个作业，每个作业包含多个任务。但是，它不能在同一应用程序中启动多个应用程序，因为Spark资源管理是基于每个应用程序而不是每个作业的基础进行的。任务在一个或多个执行器上运行，通常处理数据的不同部分（见[图 2-2](#spark_launching_two_jobs)）。请注意，执行器没有分配专用存储，尽管在[图 2-2](#spark_launching_two_jobs)中可以看到存储已连接到执行器。在某些部署中，例如本地Hadoop集群，存储可以是执行器本地的，但通常在云解决方案中，情况并非如此；在云部署（AWS、Azure等）中，存储与计算是分离的。通常情况下，Spark更喜欢调度访问本地数据的任务，但将任务和执行器分配给本地数据并非必需。
- en: '![](assets/smls_0202.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/smls_0202.png)'
- en: Figure 2-2\. Spark launching two jobs
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-2\. Spark启动两个作业
- en: Intro to PySpark
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark简介
- en: I mentioned earlier that Python is a different kind of language—it is not part
    of the JVM family. Python is an *interpreted* language, which means that(unlike
    with the JVM) Python code doesn’t go through compilation. You also know that at
    Spark’s core, it runs JVM-based processes and is based on Scala and Java. So how
    does Spark work with Python? Let’s take a look.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前提到过Python是一种不同类型的语言——它不是JVM家族的一部分。Python是一种*解释型*语言，这意味着（与JVM不同）Python代码不经过编译。您还知道，在Spark的核心部分，它运行基于JVM的进程，并且基于Scala和Java。那么Spark如何与Python一起工作呢？让我们来看一下。
- en: At the most basic level, Spark application components communicate through APIs,
    over a shared network. This means that if I have a task running a JVM process,
    it can leverage interprocess communication (IPC) to work with Python.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在最基本的层面上，Spark应用程序组件通过共享网络通过API进行通信。这意味着，如果我有一个运行JVM进程的任务，它可以利用进程间通信（IPC）与Python一起工作。
- en: 'Suppose you’ve written a PySpark app. How does it work with Spark? Essentially,
    every time you launch a PySpark job, it creates two processes under the hood:
    Python and the JVM. Python is the main program where the code is defined, and
    the JVM is the program responsible for Spark query optimization, computation,
    distribution of tasks to clusters, etc. Within a PySpark application, the `SparkContext`
    itself has a parameter called `_gateway`, which is responsible for holding the
    context to pass the Py4J application to a JVM Spark server.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您编写了一个PySpark应用程序。它如何与Spark一起工作呢？基本上，每次启动PySpark作业时，它在幕后创建两个进程：Python和JVM。Python是定义代码的主程序，而JVM是负责Spark查询优化、计算、将任务分发到集群等的程序。在PySpark应用程序中，`SparkContext`本身有一个名为`_gateway`的参数，负责保存上下文以将Py4J应用程序传递给JVM
    Spark服务器。
- en: Wait, what’s a Py4J application? [Py4J](https://www.py4j.org) is a library written
    in Python and Java that enables Python programs running in a Python interpreter
    to dynamically access Java objects and collections in a JVM via standard Python
    methods, as if they resided in the Python interpreter. In other words, it enables
    the Python code to communicate with the JVM, transparently to the user. [Figure 2-3](#pyfourj_acts_as_a_mediator_between_the)
    shows how this works. When the PySpark driver starts, it initiates a Spark JVM
    application with a Py4J server configured to communicate directly with the Spark
    JVM. Information is transferred between the PySpark driver and Py4J in a serialized
    or “pickled” form.^([4](ch02.xhtml#ch01fn8))
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 等等，什么是 Py4J 应用程序？[Py4J](https://www.py4j.org) 是一个用 Python 和 Java 编写的库，允许在 Python
    解释器中运行的 Python 程序通过标准的 Python 方法动态访问 JVM 中的 Java 对象和集合，就像它们驻留在 Python 解释器中一样。换句话说，它使得
    Python 代码能够与 JVM 通信，对用户透明。[图 2-3](#pyfourj_acts_as_a_mediator_between_the) 展示了它的工作原理。当
    PySpark 驱动程序启动时，它会使用配置好的 Py4J 服务器启动一个 Spark JVM 应用程序，以便与 Spark JVM 直接通信。信息在 PySpark
    驱动程序和 Py4J 之间以序列化或“pickled”形式传输。^([4](ch02.xhtml#ch01fn8))
- en: '![](assets/smls_0203.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/smls_0203.png)'
- en: Figure 2-3\. Py4J acting as a mediator between the Python application and the
    Spark JVM
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-3\. Py4J 充当 Python 应用程序和 Spark JVM 之间的中介
- en: Since Python is an interpreter and the code is not being compiled in advance,
    there is an executor that holds Python code within each worker node. The executor
    launches a Python application whenever necessary to execute the logic. Of course,
    I’m simplifying things here—there are some exceptions and some optimizations that
    have been added over the years. You should also be aware that PySpark is often
    less efficient in terms of running time than traditional Scala/Java code.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 Python 是解释器，并且代码不会提前编译，所以每个工作节点都有一个包含 Python 代码的执行器。执行器在需要执行逻辑时启动一个 Python
    应用程序。当然，在这里我是在简化事情——多年来添加了一些例外和优化。你也应该知道，相比传统的 Scala/Java 代码，PySpark 在运行时间上通常效率较低。
- en: Apache Spark Basics
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark 基础知识
- en: In this section, we’ll take a brief look at the basics of Spark itself, starting
    with the software architecture and the key programming abstractions.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将简要介绍 Spark 本身的基础知识，从软件架构和关键编程抽象开始。
- en: Software Architecture
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软件架构
- en: Since Spark was built as a generic engine to enable various distributed computing
    workloads, its software architecture is built in layers, as you can see in [Figure 2-4](#spark_software_architecture).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Spark 被构建为一个通用引擎，可以支持各种分布式计算工作负载，其软件架构是分层的，正如你可以在 [图 2-4](#spark_software_architecture)
    中看到的那样。
- en: '![](assets/smls_0204.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/smls_0204.png)'
- en: Figure 2-4\. Spark software architecture
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-4\. Spark 软件架构
- en: The bottom layer abstracts away the storage using *resilient distributed dataset*
    (RDD) APIs and data source connectors. Storage in Spark can be anything with a
    readable format (more on that in [Chapter 4](ch04.xhtml#data_ingestioncomma_preprocessingcomma)).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 底层通过 *弹性分布式数据集*（RDD）API 和数据源连接器抽象出存储。Spark 中的存储可以是任何具有可读格式的内容（更多内容请参阅 [第 4 章](ch04.xhtml#data_ingestioncomma_preprocessingcomma)）。
- en: 'The top layer holds the APIs and libraries for us to take advantage of. These
    APIs, which work directly with Spark’s DataFrame or Dataset APIs, abstract away
    all of the Spark internals. A *DataFrame* in Spark is a distributed collection
    of data organized in columns, similar to a table in a database. It has a dedicated
    structure and format, and you can run specific operations on it. It also has a
    schema, where each column has support for specific [data types](https://oreil.ly/NE8r5)
    (numeric, string, binary, Boolean, datetime, interval, etc.). The main difference
    between Datasets and DataFrames is that Datasets come with type safety for columns,
    which means that there is no way for us to mistake a column of type `string`,
    say, as an `int`. However, we pay the price for that: operating on Datasets is
    generally slower than operating on a DataFrame.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 顶层包含我们利用的 API 和库。这些 API 直接与 Spark 的 DataFrame 或 Dataset API 一起工作，抽象出所有的 Spark
    内部细节。在 Spark 中，*DataFrame* 是一个分布式的按列组织的数据集合，类似于数据库中的表。它有一个专用的结构和格式，可以对其运行特定的操作。它还有一个模式（schema），其中每列支持特定的
    [数据类型](https://oreil.ly/NE8r5)（数值、字符串、二进制、布尔、日期时间、间隔等）。Dataset 和 DataFrame 的主要区别在于
    Dataset 对列的类型安全性，这意味着我们不会将某列误认为是 `string` 类型而实际上是 `int` 类型。然而，我们为此付出了代价：操作 Dataset
    通常比操作 DataFrame 慢。
- en: Note that the schema is an essential part of a DataFrame. Spark may try to infer
    the schema of a given dataset from the file format itself—for example, it does
    this with Parquet files (more on that in [Chapter 4](ch04.xhtml#data_ingestioncomma_preprocessingcomma)).
    It tries its best to infer the schema from CSV files as well, but it may make
    mistakes due to specific character encodings (such as UTF formats) or issues like
    extra spaces/tabs, and occasionally the delimiters are inferred incorrectly.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，模式是DataFrame的一个重要部分。Spark可能会尝试根据文件格式自动推断给定数据集的模式——例如，对Parquet文件执行此操作（更多详细信息请参见[第4章](ch04.xhtml#data_ingestioncomma_preprocessingcomma)）。它也会尝试从CSV文件中推断模式，但可能由于特定字符编码（如UTF格式）或问题（例如额外的空格/制表符），以及偶尔推断分隔符时出错。
- en: 'To read a CSV file into a DataFrame, all you need to do is call the `read`
    operation with the dedicated format function:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要将CSV文件读取到DataFrame中，您只需调用带有专用格式函数的 `read` 操作：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this case, PySpark will try to infer the schema from the file.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，PySpark将尝试从文件中推断模式。
- en: 'Following that line with `df.printSchema()` will execute the `read` function
    and print the schema for you to examine. If you wish, you can also change the
    delimiter using the `option` function. For example, you could add the following
    to the previous line to set the delimiter to a comma:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行 `df.printSchema()` 后，会执行 `read` 函数并打印模式，以供您查看。如果希望，还可以使用 `option` 函数更改分隔符。例如，您可以在前一行后添加以下内容将分隔符设置为逗号：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Creating a custom schema
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建自定义模式
- en: Often, you’ll want to control the flow and provide your own custom schema. That
    enables collaboration and reproducibility of the code itself. It also saves you
    precious time later on debugging issues.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，您会想要控制流程并提供自定义模式。这样可以促进代码本身的协作和可重现性。这也可以节省后期调试问题的宝贵时间。
- en: 'So how do you do that with PySpark? You will need to create a `StructType`
    and pass it to the reader during reading as the desired schema. In the `StructType`,
    add all the column names and types using the dedicated APIs, as illustrated in
    the following code sample:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何在PySpark中做到这一点呢？您需要创建一个 `StructType` 并在读取时将其作为所需模式传递给读取器。在 `StructType`
    中，使用专用API添加所有列名和类型，就像下面的代码示例中所示：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Passing `True` to the `StructField` object indicates that the value can be null.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `True` 传递给 `StructField` 对象表示该值可以为空。
- en: So how do you convert Python data types to Spark data types? Or rather, how
    does PySpark interpret the types? [Table 2-1](#basic_python_data_types_and_how_to_init)
    provides a guide for some of the most common conversions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如何将Python数据类型转换为Spark数据类型？或者更确切地说，PySpark如何解释这些类型？[表 2-1](#basic_python_data_types_and_how_to_init)
    提供了一些最常见转换的指南。
- en: Table 2-1\. Basic Python data types and how to initiate them when designing
    a schema
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2-1\. 基本Python数据类型及其在设计模式时的初始化方式
- en: '| Value in Python | Spark type |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| Python中的值 | Spark类型 |'
- en: '| --- | --- |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `int` (1 byte) | `DataTypes.ByteType()` |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| `int` (1 字节) | `DataTypes.ByteType()` |'
- en: '| `int` (2 bytes) | `DataTypes.ShortType()` |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| `int` (2 字节) | `DataTypes.ShortType()` |'
- en: '| `int` (4 bytes) | `DataTypes.IntegerType()` |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| `int` (4 字节) | `DataTypes.IntegerType()` |'
- en: '| `int` (8 bytes) | `DataTypes.LongType()` |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| `int` (8 字节) | `DataTypes.LongType()` |'
- en: '| `float` (4 bytes, single precision) | `DataTypes.FloatType()` |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| `float` (4 字节，单精度) | `DataTypes.FloatType()` |'
- en: '| `float` (8 bytes, double precision) | `DataTypes.DoubleType()` |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| `float` (8 字节，双精度) | `DataTypes.DoubleType()` |'
- en: '| `str` | `DataTypes.StringType()` |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| `str` | `DataTypes.StringType()` |'
- en: '| `bool` | `DataTypes.BooleanType()` |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| `bool` | `DataTypes.BooleanType()` |'
- en: '| `decimal.Decimal` | `DataTypes.DecimalType()` |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| `decimal.Decimal` | `DataTypes.DecimalType()` |'
- en: Note that while there are many types in Python, when initiating the types for
    schema definition, there are types that repeat themselves, like `int` and `float`.
    Spark provides different types that `int` values, for example, may translate into,
    representing different numbers of bytes in memory (`ShortType` is 2 bytes, `IntegerType`
    is 4 bytes, etc.). This is one of the challenges you’ll need to navigate when
    working with PySpark.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，虽然Python中有许多类型，在定义模式时，有些类型会重复出现，比如 `int` 和 `float`。Spark提供了不同的类型，例如，`int`
    值可能会转换成不同大小的字节数表示（`ShortType` 是 2 字节，`IntegerType` 是 4 字节等）。这是您在使用PySpark时需要解决的挑战之一。
- en: Key Spark data abstractions and APIs
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关键的Spark数据抽象和API
- en: 'What exactly does an RDD abstract? This is a fantastic question, a full consideration
    of which would take us deep into the concepts of distributed storage and data
    source connectors. RDDs are essentially read-only distributed collections of JVM
    objects. However, Spark hides many complications here: RDDs combine application
    dependencies, partitions, and `iterator[T] =>` compute functions. A partition
    is a logical division of the data itself—you can think of it as a chunk of distributed
    data. The purpose of the RDD is to connect each partition to a logical iterator,
    given the application dependencies that the executors can iterate on. Partitions
    are critical because they provide Spark with the ability to split the work easily
    across executors.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: RDD 究竟抽象了什么？这是一个很棒的问题，对其进行全面考虑将使我们深入到分布式存储和数据源连接器的概念中。RDD 本质上是 JVM 对象的只读分布式集合。然而，Spark
    在这里隐藏了许多复杂性：RDD 结合了应用程序的依赖关系、分区和 `iterator[T] =>` 计算函数。分区是数据本身的逻辑划分 —— 您可以将其视为分布式数据的一个块。RDD
    的目的是将每个分区连接到逻辑迭代器，根据执行器可以迭代的应用程序依赖关系。分区至关重要，因为它们提供了 Spark 能够轻松地跨执行器分割工作的能力。
- en: Spark’s other core data abstractions are DataFrames and Datasets. A DataFrame
    is similar to an RDD, but it organizes the data into named columns, like a table
    in a relational database. Datasets are an extension of the DataFrame API, providing
    type safety. DataFrames and Datasets both benefit from query optimization provided
    by the Spark Catalyst engine. This optimization is important, as it makes running
    Spark faster and cheaper. As a result, whenever possible, it is advisable to leverage
    the top-layer APIs and avoid using RDDs.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的其他核心数据抽象是 DataFrames 和 Datasets。DataFrame 类似于 RDD，但它将数据组织成具有命名列的形式，类似于关系数据库中的表。Datasets
    是 DataFrame API 的扩展，提供类型安全性。DataFrames 和 Datasets 都受益于由 Spark Catalyst 引擎提供的查询优化。这种优化非常重要，因为它使得运行
    Spark 更快更便宜。因此，尽可能利用顶层 API 并避免使用 RDD 是明智的选择。
- en: A lot of innovation went into the Catalyst, and it is truly fascinating to dive
    into this world. However, given that it has little to no impact on machine learning
    workloads, I will not delve further into it here.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 许多创新被用于 Catalyst，这真是一个令人着迷的世界。然而，考虑到它对机器学习工作负载几乎没有影响，我将不在这里进一步深入探讨它。
- en: As [Figure 2-4](#spark_software_architecture) showed, Spark provides many APIs/libraries.
    Let’s take a look at some of the ones that are supported in PySpark*:*
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [Figure 2-4](#spark_software_architecture) 所示，Spark 提供了许多 API/库。让我们来看看一些在
    PySpark 中支持的 API*:*。
- en: MLlib
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib
- en: This library is used for running machine learning workloads at scale. Under
    the hood, there are two libraries, MLlib itself and machine learning (more on
    these in [Chapter 6](ch06.xhtml#training_models_with_spark_mllib)).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这个库用于在规模上运行机器学习工作负载。在底层，有两个库，MLlib 本身和机器学习（更多内容请参见 [Chapter 6](ch06.xhtml#training_models_with_spark_mllib)）。
- en: GraphFrames
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: GraphFrames
- en: This library enables running graph operations on data with edges and nodes.^([5](ch02.xhtml#ch01fn9))
    It optimizes the representation of vertex and edge types when they are primitive
    data types (`int`, `double`, etc.), reducing the in-memory footprint by storing
    them in specialized arrays. While machine learning has a lot to do with graph
    computation, MLlib and GraphFrames are separate and are used for different purposes.
    GraphFrames does, however, have interesting algorithms (such as PageRank) that
    can assist you in enriching your data and performing preprocessing and feature
    engineering.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这个库使得可以在具有边和节点的数据上运行图操作。它在原始数据类型 (`int`, `double` 等) 的顶点和边类型表示优化方面，通过将它们存储在专用数组中，减少了内存占用。尽管机器学习与图计算有很多关系，但
    MLlib 和 GraphFrames 是分开的，用于不同的目的。然而，GraphFrames 有一些有趣的算法（如 PageRank），可以帮助您丰富数据并进行预处理和特征工程。
- en: Structured Streaming
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化流处理
- en: This API launches a never-ending Spark job that processes microbatch data in
    a streaming way. There is also an improved engine for lower latency called Continuous
    Processing. The Spark application itself creates a listener, which waits to collect
    new data. When a new microbatch of data arrives, Spark SQL handles the processing
    and updates the final results accordingly.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 此 API 启动一个处理微批数据的不间断 Spark 作业。还有一个用于低延迟的改进引擎称为连续处理。Spark 应用程序本身创建一个侦听器，等待收集新数据。当新的微批数据到达时，Spark
    SQL 处理并相应地更新最终结果。
- en: DataFrames are immutable
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DataFrames 是不可变的。
- en: 'It is important to note that DataFrames, Datasets, and RDDs are considered
    *immutable* storage. Immutability means that an object’s state cannot be changed
    after its creation. When writing PySpark code, we must keep that in mind. Every
    time we execute an operation on a DataFrame, since it cannot change the original
    one (immutable), to store the results it creates a new DataFrame. Consider the
    following code sample, where we read a DataFrame and run a `select` operation
    on it:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，DataFrame、Dataset 和 RDD 被视为*不可变*存储。不可变性意味着对象创建后其状态不可更改。在编写 PySpark 代码时，我们必须牢记这一点。每次在
    DataFrame 上执行操作时，由于它不能改变原始对象（不可变性），为了存储结果，它会创建一个新的 DataFrame。考虑以下代码示例，我们在 DataFrame
    上读取并运行 `select` 操作：
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Notice that in the first line we assigned the DataFrame to the `train_df` variable.
    What happens to `train_df` after the `select` operation? Nothing! That’s right,
    absolutely nothing. This is the power of immutability: the object itself does
    not change. Essentially, `train_df` is now pointing to the location in memory
    where the DataFrame is represented. The representation itself cannot change. By
    selecting the bot data, we simply created a new pointer to that subset of the
    data. Because we didn’t assign the new pointer to a variable, essentially the
    last line of code does nothing. The underlying DataFrame is still the same, and
    `train_df` still points to the same place in memory that represented the DataFrame.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在第一行中，我们将 DataFrame 分配给了 `train_df` 变量。`select` 操作后 `train_df` 发生了什么？什么也没有！是的，绝对没有任何变化。这就是不可变性的威力：对象本身不会改变。本质上，`train_df`
    现在指向内存中表示 DataFrame 的位置。表示本身不会改变。通过选择 bot 数据，我们只是创建了对数据子集的新指针。因为我们没有将新指针分配给变量，所以代码的最后一行实际上什么也没做。底层
    DataFrame 仍然是相同的，`train_df` 仍然指向表示 DataFrame 的内存中的相同位置。
- en: You should also be aware that Spark uses *lazy execution*. This means it will
    not start the execution of a process until an operation of type *action* is called.
    An action can be anything that returns a value that is not part of a DataFrame—for
    example, getting the count of rows in a DataFrame or performing a write operation.
    The other type of operation is a *transformation*. Transformation operations create
    new DataFrames from existing ones. Spark accumulates all the transformations into
    a DAG and optimizes them but acts on them only when an action requiring them occurs.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你还应该注意，Spark 使用*惰性执行*。这意味着它不会启动进程的执行，直到调用*动作*类型的操作为止。动作可以是任何返回不是 DataFrame 的值的操作，例如获取
    DataFrame 中行数的计数或执行写操作。另一种操作类型是*转换*。转换操作从现有的 DataFrame 创建新的 DataFrame。Spark 将所有转换累积成
    DAG 并优化它们，但只有在需要它们的动作发生时才对其执行。
- en: So, going back to our example, Spark did not even load the data into memory.
    It created a graph of operations that has `read` and `select` in it. If, later,
    there are no action operations on the DataFrame created by the `select`, Spark
    will prune it and won’t execute it at all. Spark’s lazy execution and immutable
    data structures help prevent us from making mistakes and performing unnecessary
    computations; be mindful of these features.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的例子，Spark 甚至没有将数据加载到内存中。它创建了一个操作图，其中包含 `read` 和 `select`。如果稍后在由 `select`
    创建的 DataFrame 上没有动作操作，Spark 将对其进行修剪，并且根本不执行它。Spark 的惰性执行和不可变数据结构有助于防止我们犯错和执行不必要的计算；请注意这些特性。
- en: 'So what can we do to correct the previous mistake? We can start by simply assigning
    the DataFrame resulting from the `select` operation into an instance, as shown
    here:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们该如何纠正之前的错误呢？我们可以从简单地将 `select` 操作的 DataFrame 分配到实例中开始，如下所示：
- en: '[PRE4]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This saves the reference to `tmp_df`. If we later continue to execute on it
    and have an *action* operation in place, Spark will execute this `select` operation
    (fetching the results and saving them to the `tmp_df` variable) as part of its
    operations graph only if an action occurs that requires its result to be returned
    to the driver program. Otherwise, it will be pruned.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这样保存了对 `tmp_df` 的引用。如果我们稍后继续在其上执行并有*动作*操作，Spark 将执行此 `select` 操作（获取结果并将其保存到
    `tmp_df` 变量中），作为其操作图的一部分，只有在需要返回结果到驱动程序的动作发生时才会执行。否则，它将被修剪。
- en: You are probably asking yourself, *Why is immutability important?* Immutability
    of operations is one of the backbones of *functional programming*, a paradigm
    that aims to allow us to build applications that are more reliable and maintainable
    (you’ll learn more about functional programming in the next section). Ensuring
    that an object’s state cannot change after it is created makes it possible for
    the developer to keep track of all the operations that are performed on that object,
    preserving the chain of events. This, in return, makes the application scalable.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能在问自己，*为什么不可变性很重要呢？* 操作的不可变性是*函数式编程*的支柱之一，这种编程范式旨在使我们能够构建更可靠和可维护的应用程序（你将在下一节学到更多关于函数式编程的知识）。确保对象在创建后其状态不能改变，使开发人员能够跟踪所有对该对象执行的操作，从而保留事件链。这反过来使应用程序具有可伸缩性。
- en: 'The same concept applies when working with Spark: essentially, every DataFrame
    is a result of a specific operation which is reproducible and trackable. This
    is critical and becomes even more challenging in a distributed system. The choice
    to go with immutability enables us to achieve resilience, even though, to save
    on memory, Spark does not automatically save every DataFrame that we produce.
    Take this into account and learn to work with Spark in a way that enables you
    to reproduce your machine learning experiments (more on that in [Chapter 3](ch03.xhtml#managing_the_ml_experiment_lifecycle_wi)).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理Spark时也适用相同的概念：实质上，每个DataFrame都是特定操作的结果，这是可重现和可跟踪的。这在分布式系统中尤为重要和具有挑战性。选择不可变性使我们能够实现弹性，尽管为了节省内存，Spark并不会自动保存我们生成的每个DataFrame。考虑这一点，并学会如何以一种使你能够重现机器学习实验的方式来使用Spark（关于此更多内容请参见[第三章](ch03.xhtml#managing_the_ml_experiment_lifecycle_wi)）。
- en: PySpark and Functional Programming
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PySpark和函数式编程
- en: I mentioned functional programming in the previous section, and this was not
    by accident. Spark borrows many concepts from functional programming in addition
    to immutability, starting with *anonymous functions*. These are functions that
    are executed without state yet are not named. The idea originated in mathematics,
    with lambda calculus, and you will often hear these referred to as *lambda functions*.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我在前一节提到了函数式编程，这并非偶然。Spark除了不可变性外，还从函数式编程中借鉴了许多概念，从*匿名函数*开始。这些函数在没有状态的情况下执行，但并非无名。这个想法起源于数学中的λ演算，你经常会听到这些被称为*λ函数*。
- en: 'You can pass an anonymous function to an RDD to execute on the data. Take a
    look at the following code snippet:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将匿名函数传递给RDD来在数据上执行。看下面的代码片段：
- en: '[PRE5]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, we call the `map` function on an RDD instance. Inside that function, we
    call an anonymous function. For every `x`—meaning, for every row in the RDD—this
    function returns a pair `(x,1)`. For example, if our original RDD contained rows
    with values `(1,2,3)`, the function would return `((1,1),(2,1),(3,1))`. This is
    a transformation operation, and `rdd2` holds the graph operations with the requirements
    to turn every `x` into `(x,1)`.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们对RDD实例调用`map`函数。在该函数内部，我们调用一个匿名函数。对于每个`x`——也就是RDD中的每一行——该函数返回一对`(x,1)`。例如，如果我们的原始RDD包含值为`(1,2,3)`的行，该函数将返回`((1,1),(2,1),(3,1))`。这是一个转换操作，`rdd2`保存了具有将每个`x`转换为`(x,1)`的图操作的要求。
- en: 'The `map` function in this example leverages the functionality of passing in
    an independent function to execute on the RDD that is repeatable and independent
    of state. This capability makes it effortless to parallelize the execution. Think
    about it: every executor runs the operation on its partition of the data and returns
    an answer. There is no need to exchange information, which makes this system extremely
    scalable and resilient. If one node is down, there is no need to recompute everything—just
    the portion of the data it was responsible for. The final result is the same as
    if the operation had been executed on a single node.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`map`函数利用了将独立函数传递给RDD来执行的功能，这是可重复且独立于状态的。这种能力使并行执行变得轻而易举。想象一下：每个执行器在其数据分区上运行操作并返回答案。无需交换信息，这使得系统极其可伸缩和弹性。如果一个节点宕机，无需重新计算所有内容——只需处理它负责的部分数据。最终的结果与在单个节点上执行操作时完全一致。
- en: 'To take a more advanced example, suppose the client requests that you find
    the maximum value out of all the values. You can do this by using a `reduce` operation
    together with a `max` operation, as illustrated in the following code snippet:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 举个更高级的例子，假设客户请求您找出所有值中的最大值。您可以通过使用`reduce`操作和`max`操作来实现这一点，如下面的代码片段所示：
- en: '[PRE6]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`reduce` will calculate the maximum value among all values, starting with local
    values, by comparing each pair. After each node finds its local maximum, the data
    will be shuffled (moved) to a dedicated executor which will calculate the final
    result. In our case, `((1,1),(2,1),(3,1))` will return `3`.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduce`将计算所有值中的最大值，从而从本地值开始，通过比较每对来找到本地最大值。每个节点找到其本地最大值后，数据将被移动到专用执行器，该执行器将计算最终结果。在我们的案例中，`((1,1),(2,1),(3,1))`将返回`3`。'
- en: 'Although functional programming has more core principles, Spark doesn’t necessarily
    follow all of them to the fullest extent possible. The main principles are these:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管函数式编程有更多的核心原则，Spark并不一定完全遵循所有这些原则。主要原则如下：
- en: Immutability
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不变性
- en: Disciplined state (minimizing dependence on the state)
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有纪律的状态（最大程度地减少对状态的依赖）
- en: Executing PySpark Code
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行PySpark代码
- en: If you want to run PySpark code where your cluster is configured, ensure that
    you have access to your cluster with the right libraries and that you have the
    required permissions. For a cloud installation, if you are not an IT professional,
    it is best to leverage a managed Spark solution or consult your IT department.
    Executing the code itself is very similar to the JVM process. As discussed previously,
    the PySpark code will take care of the rest.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想在配置了集群的情况下运行PySpark代码，请确保您可以访问具有正确库的集群，并且具有必要的权限。对于云安装，如果您不是IT专业人士，最好利用托管的Spark解决方案或咨询您的IT部门。执行代码本身非常类似于JVM进程。如前所述，PySpark代码将处理其余的部分。
- en: For executing locally with a dedicated notebook, follow the instructions in
    the Quick Start guide in the README file in the book’s [GitHub repo](https://oreil.ly/smls-git).
    This should be sufficient for following along with the tutorials.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 要在专用笔记本上本地执行，请按照书籍的README文件中的[GitHub repo](https://oreil.ly/smls-git)中的快速入门指南中的说明操作。这应该足以跟随教程进行学习。
- en: Spark supports shell commands as well, through the PySpark shell (an interactive
    shell that enables you to try PySpark using the command line). The PySpark shell
    is responsible for linking the Python API to the Spark core and initializing the
    `SparkSession` and `SparkContext`. It is based on a Scala concept called the REPL,
    or Read–Eval–Print Loop. For more information, check out the [Quick Start guide](https://oreil.ly/0NB-X)
    in the Spark documentation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Spark也支持shell命令，通过PySpark shell（一种交互式shell，可以使用命令行尝试PySpark）。PySpark shell负责将Python
    API链接到Spark核心并初始化`SparkSession`和`SparkContext`。它基于Scala概念称为REPL（Read-Eval-Print
    Loop）。有关更多信息，请查阅Spark文档中的[快速入门指南](https://oreil.ly/0NB-X)。
- en: pandas DataFrames Versus Spark DataFrames
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: pandas DataFrames与Spark DataFrames的比较
- en: 'Spark DataFrames were inspired by [pandas](https://oreil.ly/ADLio), which also
    provides an abstraction on top of the data called a DataFrame. pandas is a widely
    adopted library for data manipulation and analytics. Many developers use it to
    extrapolate data using Python. Reading a pandas DataFrame is straightforward—here
    is a code example showing how to do it:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Spark DataFrames的灵感来自[pandas](https://oreil.ly/ADLio)，它也提供了一个称为DataFrame的数据抽象层。pandas是一个广泛采用的用于数据操作和分析的库。许多开发人员使用它来使用Python来推断数据。读取pandas
    DataFrame非常简单——下面是一个展示如何做到这一点的代码示例：
- en: '[PRE7]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'It may seem easy to confuse the two at the beginning, but there are many key
    differences between pandas and Spark. Most importantly, pandas was not built for
    scale; it was built to operate on data that fits into one machine’s memory. Consequently,
    it does not have the distributed Spark architecture we discussed at the beginning
    of the chapter. It also does not adhere to functional programming principles:
    pandas DataFrames are mutable.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 起初可能很容易混淆这两者，但pandas和Spark之间有许多关键区别。最重要的是，pandas不是为大规模设计的；它是为操作适合一个机器内存的数据而构建的。因此，它没有我们在章节开头讨论的分布式Spark架构。它也不遵循函数式编程原则：pandas的DataFrame是可变的。
- en: Note
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The multiple permutations of the data it keeps in memory can cause pandas to
    fail even when the original dataset fits easily in a single machine’s memory.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 它在内存中保留的数据的多个排列可能导致pandas在原始数据集轻松适合单个机器内存的情况下失败。
- en: '[Table 2-2](#spark_dataframes_versus_pandas_datafram) provides a quick comparison
    of some of the key features of Spark and pandas DataFrames.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 2-2](#spark_dataframes_versus_pandas_datafram) 提供了 Spark 和 pandas DataFrame
    一些关键特性的快速比较。'
- en: Table 2-2\. Spark DataFrames versus pandas DataFrames
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2-2\. Spark DataFrame 与 pandas DataFrame
- en: '|   | Spark DataFrame | pandas DataFrame |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|   | Spark DataFrame | pandas DataFrame |'
- en: '| --- | --- | --- |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Operation in parallel** | Yes | Not out of the box |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| **并行操作** | 是 | 不是现成的 |'
- en: '| **Lazy evaluation** | Yes | No |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| **延迟评估** | 是 | 否 |'
- en: '| **Immutable** | Yes | No |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| **不可变** | 是 | 否 |'
- en: Although, as you can see, there is no out-of-the-box way to operate in parallel
    over a pandas DataFrame, that does not mean it is entirely impossible. It simply
    means that you have to create a solution and consider the possible problems you
    might encounter (thread locks, race conditions, etc.) and their impact on the
    end result. Other differences are that Spark supports lazy evaluation, while in
    pandas, operations take place immediately as soon as the Python code line is executed,
    and DataFrames in pandas are not immutable. This makes it easy to operate on pandas
    DataFrames, as you don’t need to remember or be aware of the lazy execution approach—when
    you call a function, it is executed immediately and you can interact with the
    results right away. However, it also makes it challenging to scale using parallel
    or distributed computing.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，正如你所见，没有现成的方法可以在 pandas DataFrame 上并行操作，但这并不意味着完全不可能。这只是意味着你必须创建一个解决方案，并考虑可能遇到的问题（线程锁定，竞态条件等）及其对最终结果的影响。其他区别包括
    Spark 支持延迟评估，而 pandas 中的操作在 Python 代码行执行时立即发生，pandas 中的 DataFrame 不是不可变的。这使得在
    pandas DataFrame 上操作变得更容易，因为你不需要记住或意识到延迟执行的方法——当你调用函数时，它会立即执行，并且你可以立即与结果交互。但是，这也使得使用并行或分布式计算进行扩展变得具有挑战性。
- en: Tip
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'The Spark community has created an open source library called Koalas that provides
    a pandas-like API on Spark. It is also known as pandas-on-Spark. While it is not
    the same as parallelizing pandas, the API does a good job of mimicking the pandas
    API from a user perspective. This library is part of the official Spark APIs,
    which also makes it easier to use right away. To learn more about it, search for
    “pandas API” in the Spark docs. Starting from Spark version 3.2, you can import
    it directly from PySpark like so:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 社区创建了一个名为 Koalas 的开源库，它在 Spark 上提供了类似 pandas 的 API。它也被称为 pandas-on-Spark。虽然这不同于并行化
    pandas，但从用户的角度来看，该 API 在模仿 pandas API 方面做得很好。这个库是官方 Spark API 的一部分，这也使得你可以立即更容易地使用它。要了解更多信息，请在
    Spark 文档中搜索“pandas API”。从 Spark 版本 3.2 开始，你可以像这样直接从 PySpark 中导入它：
- en: '[PRE8]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Scikit-Learn Versus MLlib
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scikit-Learn 与 MLlib 的比较
- en: Scikit-learn (`sklearn`) and Spark MLlib are sometimes confused as well. Scikit-learn
    is a machine learning library written in Python that leverages well-known Python
    libraries such as NumPy, SciPy, and matplotlib. While it is known to have fantastic
    performance when operating on data that fits in RAM, it does it in a nondistributed
    fashion. Spark adds the overhead of configuring a cluster to operate at a larger
    scale but provides algorithms that are tuned for parallel/distributed execution.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn (`sklearn`) 和 Spark MLlib 有时也会被混淆。Scikit-learn 是一个用 Python 编写的机器学习库，利用了众所周知的
    Python 库，如 NumPy、SciPy 和 matplotlib。虽然它在处理适合 RAM 的数据时表现出色，但它以非分布式的方式进行操作。Spark
    添加了配置集群以在更大规模上运行的开销，但提供了针对并行/分布式执行进行调整的算法。
- en: 'When should you go with each tool? Here are a few hints:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在什么情况下应该选择每个工具？以下是一些提示：
- en: It is better to use Spark to process large datasets (GB, TB, or even PB scale),
    and MLlib to perform machine learning on them.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于处理大型数据集（GB、TB 甚至 PB 规模），最好使用 Spark，并使用 MLlib 对其进行机器学习。
- en: It is more efficient to construct your algorithm in Python using scikit-learn
    together with pandas and other libraries when all of your data fits into the memory
    of your machine. Additionally, this way, serving your model in production isn’t
    bound to a specific constellation (more on serving models and deployment in [Chapter 10](ch10.xhtml#deployment_patterns_for_machine_learnin)).
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你的所有数据都适合机器内存时，使用 Python 中的 scikit-learn 与 pandas 和其他库一起构建算法更有效。此外，这种方式使得在生产中提供模型不受特定配置的限制（关于在[第 10
    章](ch10.xhtml#deployment_patterns_for_machine_learnin)中的模型部署模式更多信息）。
- en: Scikit-learn uses the notion of datasets, similar to Spark and other Python
    libraries such as pandas. The datasets are mutable, and scaling out of the box
    is restricted and requires working in conjunction with other tools. Model deployment
    can be similar to with Spark (depending on your serving pattern), and models can
    be saved to disk and reloaded using multiple APIs. Scikit-learn can operate on
    pandas DataFrames and NumPy arrays too.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 使用数据集的概念，与 Spark 和其他 Python 库（如 pandas）类似。数据集是可变的，但直接扩展受限，需要与其他工具配合使用。模型部署与
    Spark 类似（取决于您的服务模式），模型可以保存到磁盘，并使用多个 API 进行重新加载。Scikit-learn 可以操作 pandas DataFrames
    和 NumPy 数组。
- en: '[Table 2-3](#scikit_learn_versus_spark_mllib) provides a quick comparison of
    some of the key features of scikit-learn and MLlib. The similarity will become
    even more evident when you reach [Chapter 6](ch06.xhtml#training_models_with_spark_mllib),
    which discusses Spark’s machine learning pipelines. This is because the concept
    of pipelines in Spark was inspired by the scikit-learn project; the Spark community
    decided to use a similar concept (as it did when designing DataFrames) to ease
    the learning curve of Spark as much as possible.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[表格 2-3](#scikit_learn_versus_spark_mllib) 快速比较了 scikit-learn 和 MLlib 的一些关键特性。当您到达
    [第 6 章](ch06.xhtml#training_models_with_spark_mllib)，讨论 Spark 的机器学习流水线时，这种相似性将更加明显。这是因为
    Spark 中管道的概念受到了 scikit-learn 项目的启发；Spark 社区决定使用类似的概念（就像设计 DataFrames 时一样），尽可能地简化
    Spark 的学习曲线。'
- en: Table 2-3\. scikit-learn versus Spark MLlib
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 2-3\. scikit-learn 对比 Spark MLlib
- en: '|   | Scikit-learn | MLlib |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|   | Scikit-learn | MLlib |'
- en: '| --- | --- | --- |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Datasets** | Mutable (columns can be updated in place) | Immutable (new
    columns must be created) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| **数据集** | 可变的（可以原地更新列） | 不可变的（必须创建新列） |'
- en: '| **Scalability** | Data must fit in a single machine’s memory | Distributed
    (enabling big data analytics) |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| **可伸缩性** | 数据必须适合单机内存 | 分布式（支持大数据分析） |'
- en: '| **Model deployment** |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| **模型部署** |'
- en: Model can be “pickled” to disk and reloaded via a REST API
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型可以通过 REST API 进行“pickling”到磁盘并重新加载。
- en: Is supported by MLflow
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 受 MLflow 支持。
- en: Provides many deployment options
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供许多部署选项。
- en: '|'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Supports Parquet and Snappy file formats and other open file formats
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持 Parquet 和 Snappy 文件格式以及其他开放文件格式。
- en: Is supported by MLflow
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 受 MLflow 支持。
- en: '|'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Summary
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter covered the very basics of Spark, giving you some high-level insights
    into the architecture and APIs and comparing it to some other popular Python tools.
    With this background, you should be ready to embark on the journey ahead and gain
    some hands-on experience with Spark and end-to-end machine learning pipelines.
    One thing to remember is that machine learning projects are long-term, requiring
    time, effort, and collaboration. To improve the maintainability of your code,
    keep it reusable, friendly, and modular.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了 Spark 的基础知识，为您提供了架构和 API 的高层次见解，并将其与其他流行的 Python 工具进行了比较。在此基础上，您应该准备好开始使用
    Spark 和端到端的机器学习流水线，并积累一些实践经验。要记住的一点是，机器学习项目是长期的，需要时间、精力和合作。为了提高代码的可维护性，保持可重用性、友好性和模块化是很重要的。
- en: ^([1](ch02.xhtml#ch01fn5-marker)) [Kubernetes (K8s)](https://kubernetes.io)
    is a tool that automates the deployment, scaling, and management of containerized
    applications.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch02.xhtml#ch01fn5-marker)) [Kubernetes (K8s)](https://kubernetes.io)
    是一个自动化容器化应用程序部署、扩展和管理的工具。
- en: ^([2](ch02.xhtml#ch01fn6-marker)) [Apache Mesos](https://mesos.apache.org) is
    an open source cluster manager.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch02.xhtml#ch01fn6-marker)) [Apache Mesos](https://mesos.apache.org) 是一个开源的集群管理器。
- en: ^([3](ch02.xhtml#ch01fn7-marker)) In Hadoop’s distributed processing framework,
    [YARN](https://oreil.ly/F-3bR) is the resource management and job scheduling technology.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch02.xhtml#ch01fn7-marker)) 在 Hadoop 的分布式处理框架中，[YARN](https://oreil.ly/F-3bR)
    是资源管理和作业调度技术。
- en: ^([4](ch02.xhtml#ch01fn8-marker)) *Pickling* is a process for converting Python
    objects into byte streams.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch02.xhtml#ch01fn8-marker)) *Pickling* 是将 Python 对象转换为字节流的过程。
- en: ^([5](ch02.xhtml#ch01fn9-marker)) Spark also provides the GraphX library; at
    a high level, their functionality is similar, but GraphFrames is based on DataFrames,
    while GraphX is based on RDDs. GraphFrames is the one supported in PySpark.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch02.xhtml#ch01fn9-marker)) Spark 还提供 GraphX 库；在高层次上，它们的功能类似，但 GraphFrames
    基于 DataFrames，而 GraphX 基于 RDDs。在 PySpark 中支持的是 GraphFrames。
