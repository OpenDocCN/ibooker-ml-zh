- en: Chapter 17\. Reusing and Converting Python Models to JavaScript
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 17 章。重用和转换 Python 模型为 JavaScript
- en: While training in the browser is a powerful option, you may not always want
    to do this because of the time involved. As you saw in Chapters [15](ch15.xhtml#an_introduction_to_tensorflowdotjs)
    and [16](ch16.xhtml#coding_techniques_for_computer_vision_i), even training simple
    models can lock up the browser for some time. Having a visualization of the progress
    helped, but it still wasn’t the best of experiences. There are three alternatives
    to this approach. The first is to train models in Python and convert them to JavaScript.
    The second is to use existing models that were trained elsewhere and are provided
    in a JavaScript-ready format. The third is to use transfer learning, introduced
    in [Chapter 3](ch03.xhtml#going_beyond_the_basics_detecting_featu). In that case,
    features, weights, or biases that have been learned in one scenario can be transferred
    to another, instead of doing time-consuming relearning. We’ll cover the first
    two cases in this chapter, and then in [Chapter 18](ch18.xhtml#transfer_learning_in_javascript)
    you’ll see how to do transfer learning in JavaScript.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览器中训练是一个强大的选择，但您可能并不总是希望这样做，因为涉及的时间。正如您在第 [15](ch15.xhtml#an_introduction_to_tensorflowdotjs)
    和 [16](ch16.xhtml#coding_techniques_for_computer_vision_i) 章节中所看到的，即使是训练简单的模型也可能会锁定浏览器一段时间。虽然有进度的可视化有所帮助，但体验仍不是最佳的。这种方法有三种替代方案。第一种是在
    Python 中训练模型，然后将其转换为 JavaScript。第二种是使用已经在其他地方训练并以 JavaScript 准备格式提供的现有模型。第三种是使用迁移学习，介绍见
    [第 3](ch03.xhtml#going_beyond_the_basics_detecting_featu) 章。在这种情况下，已在一个场景中学习的特征、权重或偏差可以转移到另一个场景，而不是进行耗时的重新学习。我们将在本章中涵盖前两种情况，然后在
    [第 18](ch18.xhtml#transfer_learning_in_javascript) 章中您将看到如何在 JavaScript 中进行迁移学习。
- en: Converting Python-Based Models to JavaScript
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将基于 Python 的模型转换为 JavaScript
- en: 'Models that have been trained using TensorFlow may be converted to JavaScript
    using the Python-based `tensorflowjs` tools. You can install these using:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 训练过的模型可以使用基于 Python 的 `tensorflowjs` 工具转换为 JavaScript。您可以使用以下命令安装这些工具：
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For example, consider the following simple model that we’ve been using throughout
    the book:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑我们在整本书中一直在使用的以下简单模型：
- en: '[PRE1]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The trained model can be saved as a saved model with this code:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下代码将训练好的模型保存为保存的模型：
- en: '[PRE2]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once you have the saved model directory, you can then use the TensorFlow.js
    converter by passing it an input format—which in this case is a saved model—along
    with the location of the saved model directory and the desired location for the
    JSON model:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 保存模型目录后，您可以通过传递输入格式给 TensorFlow.js 转换器来使用它——在本例中是一个保存的模型——以及保存模型目录的位置和 JSON
    模型的所需位置：
- en: '[PRE3]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The JSON model will be created in the specified directory (in this case, */tmp/linear*).
    If you look at the contents of this directory, you’ll also see a binary file,
    in this case called *group1-shardof1.bin* ([Figure 17-1](#the_output_of_the_js_converter)).
    This file contains the weights and biases that were learned by the network in
    an efficient binary format.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: JSON 模型将在指定的目录（在本例中为 */tmp/linear*）中创建。如果您查看此目录的内容，您还会看到一个二进制文件，本例中称为 *group1-shardof1.bin*（见
    [图 17-1](#the_output_of_the_js_converter)）。此文件包含了网络学习的权重和偏差，以高效的二进制格式存储。
- en: '![The output of the JS converter](Images/aiml_1701.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![JS 转换器的输出](Images/aiml_1701.png)'
- en: Figure 17-1\. The output of the JS converter
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 17-1\. JS 转换器的输出
- en: 'The JSON file contains text describing the model. For example, within the JSON
    file you’ll see a setting like this:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: JSON 文件包含描述模型的文本。例如，在 JSON 文件中，您将看到如下设置：
- en: '[PRE4]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This indicates the location of the *.bin* file holding the weights and biases,
    and their shape.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这指示了保存权重和偏差的 *.bin* 文件的位置及其形状。
- en: If you examine the contents of the *.bin* file in a hex editor, you’ll see that
    there are 8 bytes in it ([Figure 17-2](#the_bytes_in_the_dotbin_file)).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在十六进制编辑器中检查 *.bin* 文件的内容，您会看到其中有 8 个字节（见 [图 17-2](#the_bytes_in_the_dotbin_file)）。
- en: '![The bytes in the .bin file](Images/aiml_1702.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![*.bin* 文件中的字节](Images/aiml_1702.png)'
- en: Figure 17-2\. The bytes in the .bin file
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 17-2\. *.bin* 文件中的字节
- en: As our network learned with a single neuron to get Y = 2X – 1, the network learned
    a single weight as a `float32` (4 bytes), and a single bias as a `float32` (4
    bytes). Those 8 bytes are written to the *.bin* file.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的网络使用单个神经元学习得到 Y = 2X – 1 时，网络学到了一个单一的权重作为 `float32`（4 字节），以及一个单一的偏差作为 `float32`（4
    字节）。这 8 个字节被写入了 *.bin* 文件中。
- en: 'If you look back to the output from the code:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您回顾一下代码的输出：
- en: '[PRE5]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: you can then convert the weight (1.9966108) to hexadecimal using a tool like
    the [Floating Point to Hex Converter](https://oreil.ly/cLNPG) ([Figure 17-3](#converting_the_float_value_to_hex)).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用类似[浮点数转十六进制转换器](https://oreil.ly/cLNPG)（[图 17-3](#converting_the_float_value_to_hex)）的工具将权重（1.9966108）转换为十六进制。
- en: '![Converting the float value to hex](Images/aiml_1703.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![将浮点值转换为十六进制](Images/aiml_1703.png)'
- en: Figure 17-3\. Converting the float value to hex
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 17-3\. 将浮点值转换为十六进制
- en: You can see that the weight of 1.99661 was converted to hex of F190FF3F, which
    is the value of the first 4 bytes in the hex file from [Figure 17-2](#the_bytes_in_the_dotbin_file).
    You’ll see a similar result if you convert the bias to hex (note that you’ll need
    to swap endianness).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，权重 1.99661 被转换为十六进制 F190FF3F，在十六进制文件的前 4 个字节中的值是来自[图 17-2](#the_bytes_in_the_dotbin_file)。如果您将偏差转换为十六进制，您将看到类似的结果（请注意，您需要交换字节序）。
- en: Using the Converted Models
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用转换后的模型
- en: 'Once you have the JSON file and its associated *.bin* file, you can use them
    in a TensorFlow.js app easily. To load the model from the JSON file, you specify
    a URL where it’s hosted. If you’re using the built-in server from Brackets, it
    will be at 127.0.0.1:*<port>*. When you specify this URL, you can load the model
    with the command `await tf.loadLayersModel(URL)`. Here’s an example:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您有了 JSON 文件及其关联的 *.bin* 文件，您可以轻松在 TensorFlow.js 应用中使用它们。要从 JSON 文件加载模型，您需要指定托管模型的
    URL。如果您正在使用 Brackets 的内置服务器，它将位于 127.0.0.1:*<port>*。在指定此 URL 后，您可以使用命令 `await
    tf.loadLayersModel(URL)` 加载模型。以下是一个例子：
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You might need to change the 35601 to your local server port. The *model.json*
    file and the *.bin* file need to be in the same directory.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能需要将 35601 更改为您的本地服务器端口。*model.json* 文件和 *.bin* 文件需要在同一个目录中。
- en: 'If you want to run a prediction using the model, you use a `tensor2d` as before,
    passing the input value and its shape. So, in this case, if you want to predict
    the value of 10.0, you can create a `tensor2d` containing `[10.0]` as the first
    parameter and `[1,1]` as the second:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想使用该模型进行预测，您可以像以前一样使用 `tensor2d`，传递输入值及其形状。因此，在这种情况下，如果您想预测值为 10.0，您可以创建一个包含
    `[10.0]` 作为第一个参数和 `[1,1]` 作为第二个参数的 `tensor2d`：
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'For convenience, here’s the entire HTML page for this model:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为方便起见，这是用于该模型的整个 HTML 页面：
- en: '[PRE8]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: When you run the page, it will instantly load the model and alert the results
    of the prediction. You can see this in [Figure 17-4](#output_from_the_inference).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当您运行页面时，它将立即加载模型并显示预测结果。您可以在[图 17-4](#output_from_the_inference)中看到这一点。
- en: '![Output from the inference](Images/aiml_1704.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![推理输出](Images/aiml_1704.png)'
- en: Figure 17-4\. Output from the inference
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 17-4\. 推理输出
- en: Obviously this was a very simple example, where the model binary file was only
    8 bytes and easy to inspect. However, hopefully it was useful to help you understand
    how the JSON and binary representations go hand-in-hand. As you convert your own
    models, you’ll see much larger binary files—which ultimately are just the binary
    encoded weights and biases from your model, as you saw here.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 很显然，这是一个非常简单的例子，其中模型二进制文件仅为 8 个字节，易于检查。但愿这个例子有助于帮助您理解 JSON 和二进制表示如何紧密相关。当您转换自己的模型时，您会看到更大的二进制文件——最终只是从您的模型中获取的二进制编码的权重和偏差，就像您在这里看到的那样。
- en: In the next section you’ll look at some models that have already been converted
    for you using this method, and how you can use them in JavaScript.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，您将看到一些已经使用此方法转换过的模型，以及如何在 JavaScript 中使用它们。
- en: Using Preconverted JavaScript Models
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用预转换的 JavaScript 模型
- en: In addition to being able to convert your models to JavaScript, you can also
    use preconverted models. The TensorFlow team has created several of these for
    you to try, which you can find on [GitHub](https://oreil.ly/FOoe5). There are
    models available for different data types, including images, audio, and text.
    Let’s explore some of these models and how you can use them in JavaScript.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 除了能将您的模型转换为 JavaScript，您还可以使用预转换的模型。TensorFlow 团队已经为您创建了几种这样的模型供您尝试，您可以在[GitHub](https://oreil.ly/FOoe5)找到这些模型。这些模型适用于不同的数据类型，包括图像、音频和文本。让我们来探索一些这些模型以及您如何在
    JavaScript 中使用它们。
- en: Using the Toxicity Text Classifier
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用毒性文本分类器
- en: 'One of the text-based models provided by the TensorFlow team is the [Toxicity
    classifier](https://oreil.ly/fJTNg). This takes in a text string and predicts
    whether it contains one of the following types of toxicity:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 团队提供的文本模型之一是[毒性分类器](https://oreil.ly/fJTNg)。它接受一个文本字符串，并预测其是否包含以下类型的毒性：
- en: Identity attack
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 身份攻击
- en: Insult
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 侮辱
- en: Obscenity
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 猥亵
- en: Severe toxicity
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 严重毒性
- en: Sexually explicit
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性别露骨
- en: Threat
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 威胁
- en: General toxicity
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一般毒性
- en: 'It’s trained on the [Civil Comments dataset](https://oreil.ly/jUtEQ), containing
    over two million comments that have been labeled according to these types. Using
    it is straightforward. You can load the model alongside TensorFlow.js like this:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 它是在[Civil Comments数据集](https://oreil.ly/jUtEQ)上训练的，包含超过两百万条根据这些类型标记的评论。使用它非常简单。你可以像这样在加载TensorFlow.js时同时加载模型：
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Once you have the libraries, you can set a threshold value above which the
    sentences will be classified. It defaults to 0.85, but you can change it to something
    else like this, specifying it when you load the model:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了这些库，你可以设置一个阈值，超过这个阈值的句子将被分类。默认为0.85，但你可以像这样改变它，加载模型时指定一个新的数值：
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, if you want to classify a sentence, you can put it in an array. Multiple
    sentences can be classified simultaneously:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，如果你想分类一个句子，你可以将它放入一个数组中。多个句子可以同时进行分类：
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: At this point you can parse the `predictions` object for the results. It will
    be an array with seven entries, one for each of the toxicity types ([Figure 17-5](#the_results_of_a_toxicity_prediction)).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，你可以解析`predictions`对象来获取结果。它将是一个包含七个条目的数组，每个条目对应一个毒性类型（[Figure 17-5](#the_results_of_a_toxicity_prediction)）。
- en: '![The results of a toxicity prediction](Images/aiml_1705.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![毒性预测的结果](Images/aiml_1705.png)'
- en: Figure 17-5\. The results of a toxicity prediction
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 17-5\. 毒性预测的结果
- en: Within each of these are the results for each sentence against that class. So,
    for example, if you look at item 1, for insults, and expand it to explore the
    results, you’ll see that there are four elements. These are the probabilities
    for that type of toxicity for each of the four input sentences ([Figure 17-6](#exploring_toxicity_results)).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个句子的结果中都包含对每个类别的结果。例如，如果你查看项目1，对于侮辱，你可以展开它来探索结果，你会看到有四个元素。这些是每个输入句子对应该类型毒性的概率（[Figure 17-6](#exploring_toxicity_results)）。
- en: The probabilities are measured as [negative, positive], so a high value in the
    second element indicates that type of toxicity is present. In this case, the sentence
    “you suck” was measured as having a .91875 probability of being an insult, whereas
    “I am going to kick your head in,” while toxic, shows a low level of insult probability
    at 0.089.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 概率被测量为[负面，正面]，所以第二个元素中的高值表明该类型的毒性存在。在这种情况下，“你很糟糕”这句话被测量为有0.91875的侮辱概率，而“我要踢你的头”，虽然有毒，但侮辱概率很低，只有0.089。
- en: To parse these out, you can loop through the `predictions` array, loop through
    each of the insult types within the results, and then loop through their results
    in order to find the types of toxicity identified in each sentence. You can do
    this by using the `match` method, which will be positive if the predicted value
    is above the threshold.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 要解析这些内容，你可以循环遍历`predictions`数组，循环遍历结果中的每一种侮辱类型，然后按顺序遍历它们的结果，以找出每个句子中识别的毒性类型。你可以通过使用`match`方法来做到这一点，如果预测值高于阈值，它将是正面的。
- en: '![Exploring toxicity results](Images/aiml_1706.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![探索毒性结果](Images/aiml_1706.png)'
- en: Figure 17-6\. Exploring toxicity results
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 17-6\. 探索毒性结果
- en: 'Here’s the code:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这是代码：
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You can see the results of this in [Figure 17-7](#results_of_the_toxicity_classifier_on_t).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [Figure 17-7](#results_of_the_toxicity_classifier_on_t) 中看到这个结果。
- en: '![Results of the Toxicity classifier on the sample input](Images/aiml_1707.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![毒性分类器对样本输入的结果](Images/aiml_1707.png)'
- en: Figure 17-7\. Results of the Toxicity classifier on the sample input
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 17-7\. 毒性分类器对样本输入的结果
- en: So, if you wanted to implement some kind of toxicity filter on your site, you
    could do so with very few lines of code like this!
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你想在你的网站上实现某种毒性过滤器，你可以像这样用很少的代码来做到！
- en: 'One other useful shortcut is that if you don’t want to catch all seven forms
    of toxicity, you can specify a subset like this:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的快捷方式是，如果你不想捕捉所有七种毒性，你可以指定一个子集，就像这样：
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'and then specify this list alongside the threshold when loading the model:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在加载模型时指定此列表和阈值：
- en: '[PRE14]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The model, of course, can also be used with a Node.js backend if you want to
    capture and filter toxicity on the backend.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，该模型也可以在Node.js后端使用，如果你想在后端捕捉和过滤毒性。
- en: Using MobileNet for Image Classification in the Browser
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在浏览器中使用MobileNet进行图像分类
- en: As well as text classification libraries, the repository includes some libraries
    for image classification, such as [MobileNet](https://oreil.ly/OTRUU). MobileNet
    models are designed to be small and power-friendly while also accurate at classifying
    one thousand classes of image. As a result, they have one thousand output neurons,
    each of which is a probability that the image contains that class. So, when you
    pass an image to the model, you’ll get back a list of one thousand probabilities
    that you’ll need to map to these classes. However, the JavaScript library abstracts
    this for you, picking the top three classes in order of priority and providing
    just those.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 除了文本分类库，存储库还包括一些用于图像分类的库，如[MobileNet](https://oreil.ly/OTRUU)。MobileNet 模型被设计为小巧和节能，同时在分类一千种图像类别时也非常准确。因此，它们有一千个输出神经元，每个都是图像包含该类别的概率。因此，当您将图像传递给模型时，将返回一个包含这些类别的一千个概率的列表。但是，JavaScript
    库会为您抽象出来，按优先顺序选择前三个类，并仅提供这些类。
- en: 'Here’s an excerpt from the [full list of classes](http://bit.ly/mobilenet-labels):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这是来自[完整类列表](http://bit.ly/mobilenet-labels)的摘录：
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To get started, you need to load the TensorFlow.js and `mobilenet` scripts,
    like this:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，您需要加载 TensorFlow.js 和`mobilenet`脚本，就像这样：
- en: '[PRE16]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To use the model, you need to provide it with an image. The simplest way to
    do this is to create an `<img>` tag and load an image into it. You can also create
    a `<div>` tag to hold the output:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用模型，您需要提供一张图像。最简单的方法是创建一个`<img>`标签并加载图像到其中。您还可以创建一个`<div>`标签来容纳输出：
- en: '[PRE17]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'To use the model to classify the image, you then simply load it and pass a
    reference to the `<img>` tag to the classifier:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用模型来对图像进行分类，您只需加载它并将`<img>`标签的引用传递给分类器：
- en: '[PRE18]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This prints the output to the console log, which will look like [Figure 17-8](#exploring_the_mobilenet_output).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这会将输出打印到控制台日志中，看起来像[图17-8](#exploring_the_mobilenet_output)。
- en: '![Exploring the MobileNet output](Images/aiml_1708.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![探索 MobileNet 输出](Images/aiml_1708.png)'
- en: Figure 17-8\. Exploring the MobileNet output
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-8\. 探索 MobileNet 输出
- en: 'The output, as a prediction object, can also be parsed, so you can iterate
    through it and pick out the class name and probability like this:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 输出作为预测对象也可以解析，因此您可以遍历它并像这样选择类名和概率：
- en: '[PRE19]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[Figure 17-9](#classifying_an_image) shows the sample image in the browser
    alongside the predictions.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[图17-9](#classifying_an_image)显示了浏览器中样本图像与预测结果并排的情况。'
- en: '![Classifying an image](Images/aiml_1709.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图像分类](Images/aiml_1709.png)'
- en: Figure 17-9\. Classifying an image
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-9\. 图像分类
- en: 'For convenience, here’s the entire code listing for this simple page. To use
    it you need to have an image in the same directory. I’m using *coffee.jpg*, but
    you can of course replace the image and change the `src` attribute of the `<img>`
    tag to classify something else:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，这里是这个简单页面的完整代码清单。要使用它，您需要在相同的目录中有一张图像。我使用的是*coffee.jpg*，但您当然可以替换图像并更改`<img>`标签的`src`属性以分类其他内容：
- en: '[PRE20]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Using PoseNet
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 PoseNet
- en: 'Another interesting library that has been preconverted for you by the TensorFlow
    team is [PoseNet](https://oreil.ly/FOoe5), which can give you near-real-time pose
    estimation in the browser. It takes an image and returns a set of points for 17
    body landmarks in the image:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个由 TensorFlow 团队预先转换为您的有趣库是[姿势网络](https://oreil.ly/FOoe5)，它可以在浏览器中实时估计姿势。它接收一张图像并返回图像中17个身体标志点的集合：
- en: Nose
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鼻子
- en: Left and right eye
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左右眼
- en: Left and right ear
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左右耳
- en: Left and right shoulder
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左右肩膀
- en: Left and right elbow
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左右肘部
- en: Left and right wrist
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左右手腕
- en: Left and right hip
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左右臀部
- en: Left and right knee
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左右膝盖
- en: Left and right ankle
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左右脚踝
- en: 'For a simple scenario, we can look at estimating the pose for a single image
    in the browser. To do this, first load TensorFlow.js and the `posenet` model:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个简单的场景，我们可以看一下在浏览器中估计单个图像的姿势。要做到这一点，首先加载 TensorFlow.js 和`posenet`模型：
- en: '[PRE21]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In the browser, you can load the image into an `<img>` tag and create a canvas
    on which you can draw the locations of the body landmarks:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览器中，您可以将图像加载到`<img>`标签中，并创建一个画布，在此画布上可以绘制身体标志点的位置：
- en: '[PRE22]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'To get the predictions, you can then just get the image element containing
    the picture and pass it to `posenet`, calling the `estimateSinglePose` method:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取预测结果，您只需获取包含图片的图像元素，并将其传递给`posenet`，调用`estimateSinglePose`方法：
- en: '[PRE23]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This will return the predictions in an object called `pose`. This is an array
    of keypoints of the body parts ([Figure 17-10](#the_returned_positions_for_the_pose)).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回名为`pose`的对象中的预测结果。这是一个包含身体部位关键点的数组（见图17-10）。
- en: '![The returned positions for the pose](Images/aiml_1710.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![姿势返回的位置](Images/aiml_1710.png)'
- en: Figure 17-10\. The returned positions for the pose
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-10\. 姿势返回的位置
- en: Each item contains a text description of the part (e.g., `nose`), an object
    with the *x* and *y* coordinates of the position, and a `score` value indicating
    the confidence that the landmark was correctly spotted. So, for example, in [Figure 17-10](#the_returned_positions_for_the_pose),
    the likelihood that the landmark `nose` was spotted is .999.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 每个项目包含一个部位的文本描述（例如，`nose`），一个带有位置的(*x*, *y*)坐标的对象，以及指示正确识别标志物的置信度值`score`。例如，在[图17-10](#the_returned_positions_for_the_pose)
    中，标志物`nose`被识别的可能性为0.999。
- en: 'You can then use these to plot the landmarks on the image. First load the image
    into the canvas, and then you can draw on it:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '然后，您可以使用这些信息在图像上绘制标志物。首先将图像加载到画布中，然后可以在其上绘制： '
- en: '[PRE24]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You can then loop through the predictions, retrieving the part names and (*x*,*y*)
    coordinates, and plot them on the canvas with this code:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以循环遍历预测结果，检索部位名称和(*x*,*y*)坐标，并使用以下代码在画布上绘制它们：
- en: '[PRE25]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Then, at runtime, you should see something like [Figure 17-11](#estimating_and_drawing_body_part_positi).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在运行时，您应该看到类似于[图17-11](#estimating_and_drawing_body_part_positi) 的内容。
- en: '![Estimating and drawing body part positions on an image](Images/aiml_1711.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![估算和绘制图像上的身体部位位置](Images/aiml_1711.png)'
- en: Figure 17-11\. Estimating and drawing body part positions on an image
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-11\. 估算和绘制图像上的身体部位位置
- en: 'You can also use the `score` property to filter out bad predictions. For example,
    if your picture only contains a person’s face, you can update the code to filter
    out the low-probability predictions in order to focus on just the relevant landmarks:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用`score`属性来过滤掉错误的预测。例如，如果您的图片只包含一个人的面部，您可以更新代码以过滤掉低概率的预测，以便专注于相关的关键部位：
- en: '[PRE26]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: If the image is a closeup of somebody’s face, you don’t want to plot shoulders,
    ankles, etc. These will have very low but nonzero scores, so if you don’t filter
    them out, they’ll be plotted somewhere on the image—and as the image doesn’t contain
    these landmarks, this will clearly be an error!
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果图像是某人面部的特写，您不希望绘制肩膀、脚踝等部位。这些部位的得分可能很低但非零，如果不将它们过滤掉，它们会被绘制在图像的某个地方——由于图像并不包含这些标志物，这显然是一个错误！
- en: '[Figure 17-12](#using_posenet_on_a_face) shows a picture of a face with low-probability
    landmarks filtered out. Note that there is no mouth landmark, because PoseNet
    is primarily intended for estimating body poses, not faces.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[图17-12](#using_posenet_on_a_face) 显示了一张面部图像，已过滤掉低概率的标志物。请注意，因为PoseNet主要用于估算身体姿势而非面部，所以没有口部标志物。'
- en: '![Using PoseNet on a face](Images/aiml_1712.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![在面部使用PoseNet](Images/aiml_1712.png)'
- en: Figure 17-12\. Using PoseNet on a face
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-12\. 在面部使用PoseNet
- en: There are many more features available in the PoseNet model—we’ve barely scratched
    the surface of what’s possible. You can do real-time pose detection in the browser
    with the webcam, edit the accuracy of the poses (lower-accuracy predictions can
    be faster), choose the architecture to optimize for speed, detect poses for multiple
    bodies, and much more.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: PoseNet 模型还提供了许多其他功能——我们仅仅触及了可能性的表面。您可以在浏览器中使用网络摄像头进行实时姿势检测，编辑姿势的准确性（低准确度的预测可能更快），选择优化速度的架构，检测多个人体的姿势等等。
- en: Summary
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter you saw how you can use TensorFlow.js with Python-created models,
    either by training your own model and converting it with the provided tools, or
    by using preexisting models. When converting, you saw how the `tensorflowjs` tools
    created a JSON file with the metadata of your model, as well as a binary file
    containing the weights and biases. It was easy to import your model as a JavaScript
    library and use it directly in the browser.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了如何使用TensorFlow.js与Python创建的模型，可以通过训练自己的模型并使用提供的工具转换，或者使用现有模型。在转换过程中，您看到了`tensorflowjs`工具创建了一个包含模型元数据的JSON文件，以及一个包含权重和偏置的二进制文件。很容易将模型导入为JavaScript库，并直接在浏览器中使用它。
- en: You then looked at a few examples of existing models that have been converted
    for you, and how you can incorporate them into your JavaScript code. You first
    experimented with the Toxicity model for processing text to identify and filter
    out toxic comments. You then explored using the MobileNet model for computer vision
    to predict the contents of an image. Finally, you saw how to use the PoseNet model
    to detect body landmarks in images and plot them, including how to filter out
    low-probability scores to avoid plotting unseen landmarks.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你随后查看了一些已经为你转换的现有模型的示例，以及如何将它们整合到你的JavaScript代码中。你首先尝试了毒性模型，用于处理文本以识别和过滤有毒评论。然后，你探索了使用MobileNet模型进行计算机视觉，以预测图像的内容。最后，你看到了如何使用PoseNet模型来检测图像中的身体地标并绘制它们，包括如何过滤低概率分数，以避免绘制看不见的地标。
- en: 'In [Chapter 18](ch18.xhtml#transfer_learning_in_javascript), you’ll see another
    method for reusing existing models: transfer learning, where you take existing
    pretrained features and use them in your own apps.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第18章](ch18.xhtml#transfer_learning_in_javascript)，你将看到另一种重用现有模型的方法：迁移学习，即利用现有的预训练特征，并将其应用到你自己的应用程序中。
