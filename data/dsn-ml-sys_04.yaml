- en: Chapter 4\. Training Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章\. 训练数据
- en: In [Chapter 3](ch03.xhtml#data_engineering_fundamentals), we covered how to
    handle data from the systems perspective. In this chapter, we’ll go over how to
    handle data from the data science perspective. Despite the importance of training
    data in developing and improving ML models, ML curricula are heavily skewed toward
    modeling, which is considered by many practitioners the “fun” part of the process.
    Building a state-of-the-art model is interesting. Spending days wrangling with
    a massive amount of malformatted data that doesn’t even fit into your machine’s
    memory is frustrating.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第三章](ch03.xhtml#data_engineering_fundamentals)中，我们从系统的角度讨论了如何处理数据。在这一章中，我们将从数据科学的角度讨论如何处理数据。尽管训练数据在开发和改进机器学习模型中非常重要，但许多教育课程都过分偏向于建模，而这被许多从业者认为是整个过程中最“有趣”的部分。构建一个最先进的模型确实很有趣。但是花费数日处理海量格式不正确的数据，甚至不能适应你机器内存的数据，则是非常令人沮丧的。
- en: Data is messy, complex, unpredictable, and potentially treacherous. If not handled
    properly, it can easily sink your entire ML operation. But this is precisely the
    reason why data scientists and ML engineers should learn how to handle data well,
    saving us time and headache down the road.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 数据混乱、复杂、难以预测，可能会引发危险。如果处理不当，它很容易将整个机器学习操作陷入困境。但这正是为什么数据科学家和机器学习工程师应该学会如何处理数据的原因，这将在未来节省我们大量时间和头疼的根源。
- en: In this chapter, we will go over techniques to obtain or create good training
    data. Training data, in this chapter, encompasses all the data used in the developing
    phase of ML models, including the different splits used for training, validation,
    and testing (the train, validation, test splits). This chapter starts with different
    sampling techniques to select data for training. We’ll then address common challenges
    in creating training data, including the label multiplicity problem, the lack
    of labels problem, the class imbalance problem, and techniques in data augmentation
    to address the lack of data problem.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将讨论获取或创建良好训练数据的技术。在本章中，训练数据包括机器学习模型开发阶段使用的所有数据，包括用于训练、验证和测试的不同拆分（训练、验证、测试拆分）。本章从选择训练数据的不同采样技术开始。然后，我们将讨论创建训练数据中的常见挑战，包括标签多样性问题、缺乏标签问题、类别不平衡问题以及解决缺乏数据问题的数据增强技术。
- en: We use the term “training data” instead of “training dataset” because “dataset”
    denotes a set that is finite and stationary. Data in production is neither finite
    nor stationary, a phenomenon that we will cover in the section [“Data Distribution
    Shifts”](ch08.xhtml#data_distribution_shifts). Like other steps in building ML
    systems, creating training data is an iterative process. As your model evolves
    through a project lifecycle, your training data will likely also evolve.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用术语“训练数据”而不是“训练数据集”，因为“数据集”表示一个有限且静态的集合。生产中的数据既不是有限的，也不是静态的，这是我们将在[“数据分布转移”](ch08.xhtml#data_distribution_shifts)一节中讨论的现象。像建立机器学习系统的其他步骤一样，创建训练数据是一个迭代的过程。随着项目生命周期中模型的演变，你的训练数据也很可能会发生变化。
- en: Before we move forward, I just want to echo a word of caution that has been
    said many times yet is still not enough. Data is full of potential biases. These
    biases have many possible causes. There are biases caused during collecting, sampling,
    or labeling. Historical data might be embedded with human biases, and ML models,
    trained on this data, can perpetuate them. Use data but don’t trust it too much!
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，我想再次强调一句话，虽然已经说了很多次，但还是不够。数据充满了潜在的偏见。这些偏见可能有很多原因。在收集、采样或标记过程中可能产生偏见。历史数据可能被人类偏见所影响，而在这些数据上训练的机器学习模型可能会延续这些偏见。使用数据，但不要过于信任它！
- en: Sampling
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 采样
- en: Sampling is an integral part of the ML workflow that is, unfortunately, often
    overlooked in typical ML coursework. Sampling happens in many steps of an ML project
    lifecycle, such as sampling from all possible real-world data to create training
    data; sampling from a given dataset to create splits for training, validation,
    and testing; or sampling from all possible events that happen within your ML system
    for monitoring purposes. In this section, we’ll focus on sampling methods for
    creating training data, but these sampling methods can also be used for other
    steps in an ML project lifecycle.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 采样是机器学习工作流程的一个重要组成部分，但在典型的机器学习课程中往往被忽视。在机器学习项目生命周期的许多步骤中都会进行采样，例如从所有可能的真实世界数据中创建训练数据；从给定数据集中进行采样以创建用于训练、验证和测试的拆分；或者从发生在您的机器学习系统内的所有可能事件中进行采样以进行监控。在本节中，我们将重点讨论用于创建训练数据的采样方法，但这些采样方法也可以用于机器学习项目生命周期的其他步骤。
- en: In many cases, sampling is necessary. One case is when you don’t have access
    to all possible data in the real world, the data that you use to train your model
    is a subset of real-world data, created by one sampling method or another. Another
    case is when it’s infeasible to process all the data that you have access to—because
    it requires too much time or resources—so you have to sample that data to create
    a subset that is feasible to process. In many other cases, sampling is helpful
    as it allows you to accomplish a task faster and cheaper. For example, when considering
    a new model, you might want to do a quick experiment with a small subset of your
    data to see if the new model is promising first before training this new model
    on all your data.^([1](ch04.xhtml#ch01fn82))
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，采样是必要的。一种情况是当您无法访问真实世界中的所有可能数据时，用于训练模型的数据是真实世界数据的一个子集，由某种采样方法创建而成。另一种情况是当处理您可以访问的所有数据变得不可行时——因为它需要太多时间或资源——因此您必须对这些数据进行采样，以创建一个可行处理的子集。在许多其他情况下，采样是有帮助的，因为它可以让您更快速、更便宜地完成任务。例如，考虑一个新模型时，您可能希望先用数据的一个小子集进行快速实验，以查看新模型是否有前途，然后再对所有数据进行训练。^([1](ch04.xhtml#ch01fn82))
- en: Understanding different sampling methods and how they are being used in our
    workflow can, first, help us avoid potential sampling biases, and second, help
    us choose the methods that improve the efficiency of the data we sample.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 理解不同的采样方法及其在工作流程中的应用可以首先帮助我们避免潜在的采样偏差，其次可以帮助我们选择提高我们所采样数据效率的方法。
- en: 'There are two families of sampling: nonprobability sampling and random sampling.
    We’ll start with nonprobability sampling methods, followed by several common random
    sampling methods.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有两大类采样方法：非概率抽样和随机抽样。我们将从非概率抽样方法开始，然后介绍几种常见的随机抽样方法。
- en: Nonprobability Sampling
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非概率抽样
- en: 'Nonprobability sampling is when the selection of data isn’t based on any probability
    criteria. Here are some of the criteria for nonprobability sampling:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 非概率抽样是指数据选择不基于任何概率标准。以下是一些非概率抽样的标准：
- en: Convenience sampling
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 便利抽样
- en: Samples of data are selected based on their availability. This sampling method
    is popular because, well, it’s convenient.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 根据数据的可用性选择数据样本。这种采样方法很受欢迎，因为它很方便。
- en: Snowball sampling
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 雪球抽样
- en: Future samples are selected based on existing samples. For example, to scrape
    legitimate Twitter accounts without having access to Twitter databases, you start
    with a small number of accounts, then you scrape all the accounts they follow,
    and so on.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的样本是根据现有样本选择的。例如，为了在没有访问Twitter数据库的情况下抓取合法的Twitter账户，您从少量账户开始，然后抓取他们关注的所有账户，依此类推。
- en: Judgment sampling
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 判断抽样
- en: Experts decide what samples to include.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 专家决定包括哪些样本。
- en: Quota sampling
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 配额抽样
- en: 'You select samples based on quotas for certain slices of data without any randomization.
    For example, when doing a survey, you might want 100 responses from each of the
    age groups: under 30 years old, between 30 and 60 years old, and above 60 years
    old, regardless of the actual age distribution.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 您根据特定数据切片的配额选择样本，而无需任何随机化。例如，在进行调查时，您可能希望从每个年龄组获取100个回应：30岁以下、30到60岁之间和60岁以上，而不考虑实际年龄分布。
- en: The samples selected by nonprobability criteria are not representative of the
    real-world data and therefore are riddled with selection biases.^([2](ch04.xhtml#ch01fn83))
    Because of these biases, you might think that it’s a bad idea to select data to
    train ML models using this family of sampling methods. You’re right. Unfortunately,
    in many cases, the selection of data for ML models is still driven by convenience.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由非概率标准选取的样本并不代表真实世界的数据，因此充满了选择偏差。^([2](ch04.xhtml#ch01fn83)) 因为这些偏差，您可能会认为使用这类抽样方法来训练机器学习模型是个坏主意。您是对的。不幸的是，在许多情况下，选择用于机器学习模型的数据仍然是出于便利性考虑。
- en: One example of these cases is language modeling. Language models are often trained
    not with data that is representative of all possible texts but with data that
    can be easily collected—Wikipedia, Common Crawl, Reddit.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一个这类情况的例子是语言建模。语言模型通常不是用代表所有可能文本的数据进行训练的，而是用可以轻松收集到的数据——维基百科、Common Crawl、Reddit。
- en: Another example is data for sentiment analysis of general text. Much of this
    data is collected from sources with natural labels (ratings) such as IMDB reviews
    and Amazon reviews. These datasets are then used for other sentiment analysis
    tasks. IMDB reviews and Amazon reviews are biased toward users who are willing
    to leave reviews online, and not necessarily representative of people who don’t
    have access to the internet or people who aren’t willing to put reviews online.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是对一般文本进行情感分析的数据。这些数据很大一部分来自于具有自然标签（评分）的来源，如IMDB评论和亚马逊评论。然后，这些数据集被用于其他情感分析任务。IMDB评论和亚马逊评论偏向于愿意在线留下评论的用户，不一定代表那些没有互联网访问权限或者不愿意在线发表评论的人群。
- en: 'A third example is data for training self-driving cars. Initially, data collected
    for self-driving cars came largely from two areas: Phoenix, Arizona (because of
    its lax regulations), and the Bay Area in California (because many companies that
    build self-driving cars are located here). Both areas have generally sunny weather.
    In 2016, Waymo expanded its operations to Kirkland, Washington, specially for
    Kirkland’s rainy weather,^([3](ch04.xhtml#ch01fn84)) but there’s still a lot more
    self-driving car data for sunny weather than for rainy or snowy weather.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个例子是用于训练自动驾驶汽车的数据。最初，用于自动驾驶汽车的数据主要来自于两个地区：亚利桑那州凤凰城（因其宽松的法规）和加利福尼亚州的湾区（因为许多自动驾驶汽车公司在这里设立）。这两个地区通常天气晴朗。2016年，Waymo将其业务扩展到华盛顿州柯克兰，特别是因为柯克兰的多雨天气，^([3](ch04.xhtml#ch01fn84))
    但仍然有更多晴天自动驾驶汽车数据，而不是雨天或雪天的数据。
- en: Nonprobability sampling can be a quick and easy way to gather your initial data
    to get your project off the ground. However, for reliable models, you might want
    to use probability-based sampling, which we will cover next.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 非概率抽样可以是快速和简便的方式来收集您的初始数据，以推动您的项目的启动。然而，为了可靠的模型，您可能希望使用基于概率的抽样，我们将在下文介绍。
- en: Simple Random Sampling
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单随机抽样
- en: In the simplest form of random sampling, you give all samples in the population
    equal probabilities of being selected.^([4](ch04.xhtml#ch01fn85)) For example,
    you randomly select 10% of the population, giving all members of this population
    an equal 10% chance of being selected.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在最简单形式的随机抽样中，你给予种群中所有样本相等的被选中概率。^([4](ch04.xhtml#ch01fn85)) 例如，你随机选择种群的10%，给予所有这个种群成员相等的10%被选中的机会。
- en: The advantage of this method is that it’s easy to implement. The drawback is
    that rare categories of data might not appear in your selection. Consider the
    case where a class appears only in 0.01% of your data population. If you randomly
    select 1% of your data, samples of this rare class will unlikely be selected.
    Models trained on this selection might think that this rare class doesn’t exist.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优点是易于实施。缺点是稀有类别的数据可能不会出现在您的选择中。考虑一个情况，一个类别仅在您的数据种群中出现了0.01%。如果您随机选择了1%的数据，这个稀有类别的样本可能不会被选择。基于这种选择训练的模型可能会认为这个稀有类别不存在。
- en: Stratified Sampling
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分层抽样
- en: To avoid the drawback of simple random sampling, you can first divide your population
    into the groups that you care about and sample from each group separately. For
    example, to sample 1% of data that has two classes, A and B, you can sample 1%
    of class A and 1% of class B. This way, no matter how rare class A or B is, you’ll
    ensure that samples from it will be included in the selection. Each group is called
    a stratum, and this method is called stratified sampling.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免简单随机抽样的缺点，你可以先将总体分成你关心的各组，并分别从每组中抽样。例如，要从有两类A和B的数据中抽取1%的数据，你可以分别从类A和类B中各抽取1%。这样，无论类A或类B有多么稀少，你都能确保从中抽到样本。每组称为一个层，这种方法称为分层抽样。
- en: One drawback of this sampling method is that it isn’t always possible, such
    as when it’s impossible to divide all samples into groups. This is especially
    challenging when one sample might belong to multiple groups, as in the case of
    multilabel tasks.^([5](ch04.xhtml#ch01fn86)) For instance, a sample can be both
    class A and class B.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这种抽样方法的一个缺点是并非总是可行，比如当将所有样本分成组是不可能的时候。这在某些多标签任务中尤其具有挑战性^([5](ch04.xhtml#ch01fn86))，例如，一个样本可能同时属于类A和类B。
- en: Weighted Sampling
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加权抽样
- en: In weighted sampling, each sample is given a weight, which determines the probability
    of it being selected. For example, if you have three samples, A, B, and C, and
    want them to be selected with the probabilities of 50%, 30%, and 20% respectively,
    you can give them the weights 0.5, 0.3, and 0.2.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在加权抽样中，每个样本都被赋予一个权重，这决定了它被选择的概率。例如，如果你有三个样本，A、B和C，并希望它们的被选中概率分别为50%、30%和20%，那么你可以给它们分别赋予权重0.5、0.3和0.2。
- en: This method allows you to leverage domain expertise. For example, if you know
    that a certain subpopulation of data, such as more recent data, is more valuable
    to your model and want it to have a higher chance of being selected, you can give
    it a higher weight.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法允许你利用领域专业知识。例如，如果你知道数据的某个子群体，比如最近的数据，对你的模型更有价值，并希望它有更高的被选中概率，你可以给它更高的权重。
- en: This also helps with the case when the data you have comes from a different
    distribution compared to the true data. For example, if in your data, red samples
    account for 25% and blue samples account for 75%, but you know that in the real
    world, red and blue have equal probability to happen, you can give red samples
    weights three times higher than blue samples.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这也有助于当你的数据与真实数据的分布不同时的情况。例如，如果在你的数据中，红色样本占25%，蓝色样本占75%，但你知道在真实世界中，红色和蓝色发生的概率是相等的，那么你可以给红色样本的权重高出蓝色样本三倍。
- en: 'In Python, you can do weighted sampling with `random.choices` as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，你可以使用`random.choices`来进行加权抽样，方法如下：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: A common concept in ML that is closely related to weighted sampling is sample
    weights. Weighted sampling is used to select samples to train your model with,
    whereas sample weights are used to assign “weights” or “importance” to training
    samples. Samples with higher weights affect the loss function more. Changing sample
    weights can change your model’s decision boundaries significantly, as shown in
    [Figure 4-1](#how_sample_weights_can_affect_the_decis).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中一个常见的概念，与加权抽样密切相关的是样本权重。加权抽样用于选择用来训练模型的样本，而样本权重用于给训练样本分配“权重”或“重要性”。具有更高权重的样本更显著地影响损失函数。改变样本权重可以显著改变模型的决策边界，如图4-1所示。
- en: '![](Images/dmls_0401.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_0401.png)'
- en: 'Figure 4-1\. Sample weights can affect the decision boundary. On the left is
    when all samples are given equal weights. On the right is when samples are given
    different weights. Source: scikit-learn^([6](ch04.xhtml#ch01fn87))'
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-1。样本权重可以影响决策边界。左边是所有样本被赋予相等权重时的情况。右边是样本被赋予不同权重时的情况。来源：scikit-learn^([6](ch04.xhtml#ch01fn87))
- en: Reservoir Sampling
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 水库抽样
- en: Reservoir sampling is a fascinating algorithm that is especially useful when
    you have to deal with streaming data, which is usually what you have in production.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 水库抽样是一种非常有趣的算法，特别适用于处理流数据，这通常是生产环境中的数据。
- en: 'Imagine you have an incoming stream of tweets and you want to sample a certain
    number, *k*, of tweets to do analysis or train a model on. You don’t know how
    many tweets there are, but you know you can’t fit them all in memory, which means
    you don’t know in advance the probability at which a tweet should be selected.
    You want to ensure that:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你有一系列即将到来的推文，你想要随机抽取一定数量的推文进行分析或模型训练。你不知道有多少推文，但是你知道你无法将它们全部存储在内存中，这意味着你无法预先知道应该选择推文的概率。你想要确保：
- en: Every tweet has an equal probability of being selected.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每条推文被选中的概率是相等的。
- en: You can stop the algorithm at any time and the tweets are sampled with the correct
    probability.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以随时停止算法，推文将按正确的概率进行抽样。
- en: 'One solution for this problem is reservoir sampling. The algorithm involves
    a reservoir, which can be an array, and consists of three steps:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的一个解决方案是蓄水池抽样。该算法涉及一个蓄水池，可以是一个数组，并包括三个步骤：
- en: Put the first *k* elements into the reservoir.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将前 *k* 个元素放入蓄水池中。
- en: For each incoming *n*^(th) element, generate a random number *i* such that 1
    ≤ *i* ≤ *n*.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个到来的 *n*^(th) 元素，生成一个随机数 *i*，使得 1 ≤ *i* ≤ *n*。
- en: 'If 1 ≤ *i* ≤ *k*: replace the *i*^(th) element in the reservoir with the *n*^(th)
    element. Else, do nothing.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 1 ≤ *i* ≤ *k*：用 *n*^(th) 元素替换蓄水池中的 *i*^(th) 元素。否则，不执行任何操作。
- en: This means that each incoming *n*^(th) element has <math alttext="StartFraction
    k Over n EndFraction"><mfrac><mi>k</mi> <mi>n</mi></mfrac></math> probability
    of being in the reservoir. You can also prove that each element in the reservoir
    has <math alttext="StartFraction k Over n EndFraction"><mfrac><mi>k</mi> <mi>n</mi></mfrac></math>
    probability of being there. This means that all samples have an equal chance of
    being selected. If we stop the algorithm at any time, all samples in the reservoir
    have been sampled with the correct probability. [Figure 4-2](#a_visualization_of_how_reservoir_sampli)
    shows an illustrative example of how reservoir sampling works.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着每个到来的 *n*^(th) 元素都有 <math alttext="StartFraction k Over n EndFraction"><mfrac><mi>k</mi>
    <mi>n</mi></mfrac></math> 的概率进入蓄水池。你也可以证明蓄水池中的每个元素有 <math alttext="StartFraction
    k Over n EndFraction"><mfrac><mi>k</mi> <mi>n</mi></mfrac></math> 的概率存在于那里。这意味着所有样本被选中的概率是相等的。如果我们在任何时候停止算法，蓄水池中的所有样本都以正确的概率被抽样到。[图 4-2](#a_visualization_of_how_reservoir_sampli)
    展示了蓄水池抽样工作原理的一个示例。
- en: '![](Images/dmls_0402.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_0402.png)'
- en: Figure 4-2\. A visualization of how reservoir sampling works
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-2\. 演示了蓄水池抽样的工作原理
- en: Importance Sampling
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重要性抽样
- en: Importance sampling is one of the most important sampling methods, not just
    in ML. It allows us to sample from a distribution when we only have access to
    another distribution.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 重要性抽样是最重要的抽样方法之一，不仅在机器学习中如此。它允许我们在只有另一种分布的访问权限时，从一个分布中进行抽样。
- en: 'Imagine you have to sample *x* from a distribution *P*(*x*), but *P*(*x*) is
    really expensive, slow, or infeasible to sample from. However, you have a distribution
    *Q*(*x*) that is a lot easier to sample from. So you sample *x* from *Q*(*x*)
    instead and weigh this sample by <math alttext="StartFraction upper P left-parenthesis
    x right-parenthesis Over upper Q left-parenthesis x right-parenthesis EndFraction"><mfrac><mrow><mi>P</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow>
    <mrow><mi>Q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></math> . *Q*(*x*)
    is called the *proposal distribution* or the *importance distribution*. *Q*(*x*)
    can be any distribution as long as *Q*(*x*) > 0 whenever *P*(*x*) ≠ 0\. The following
    equation shows that in expectation, *x* sampled from *P*(*x*) is equal to *x*
    sampled from *Q*(*x*) weighted by <math alttext="StartFraction upper P left-parenthesis
    x right-parenthesis Over upper Q left-parenthesis x right-parenthesis EndFraction"><mfrac><mrow><mi>P</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow>
    <mrow><mi>Q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></math> :'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你需要从分布 *P*(*x*) 中抽样 *x*，但 *P*(*x*) 的抽样可能非常昂贵、缓慢或不可行。但是，你有一个分布 *Q*(*x*)，它更容易抽样。因此，你从
    *Q*(*x*) 中抽样 *x*，并用 <math alttext="StartFraction upper P left-parenthesis x right-parenthesis
    Over upper Q left-parenthesis x right-parenthesis EndFraction"><mfrac><mrow><mi>P</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow>
    <mrow><mi>Q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></math> 加权这个样本。*Q*(*x*)
    被称为*提议分布*或*重要性分布*。只要 *P*(*x*) ≠ 0，*Q*(*x*) 就可以是任何分布。下面的方程显示了期望中，从 *P*(*x*) 中抽样的
    *x* 等于从 *Q*(*x*) 中抽样的 *x*，乘以 <math alttext="StartFraction upper P left-parenthesis
    x right-parenthesis Over upper Q left-parenthesis x right-parenthesis EndFraction"><mfrac><mrow><mi>P</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow>
    <mrow><mi>Q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></math> 的权重：
- en: <math alttext="upper E Subscript upper P left-parenthesis x right-parenthesis
    Baseline left-bracket x right-bracket equals sigma-summation Underscript x Endscripts
    upper P left-parenthesis x right-parenthesis x equals sigma-summation Underscript
    x Endscripts upper Q left-parenthesis x right-parenthesis x StartFraction upper
    P left-parenthesis x right-parenthesis Over upper Q left-parenthesis x right-parenthesis
    EndFraction equals upper E Subscript upper Q left-parenthesis x right-parenthesis
    Baseline left-bracket x StartFraction upper P left-parenthesis x right-parenthesis
    Over upper Q left-parenthesis x right-parenthesis EndFraction right-bracket" display="block"><mrow><msub><mi>E</mi>
    <mrow><mi>P</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></msub> <mrow><mo>[</mo>
    <mi>x</mi> <mo>]</mo></mrow> <mo>=</mo> <munder><mo>∑</mo> <mi>x</mi></munder>
    <mi>P</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mi>x</mi> <mo>=</mo>
    <munder><mo>∑</mo> <mi>x</mi></munder> <mi>Q</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow> <mi>x</mi> <mfrac><mrow><mi>P</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow>
    <mrow><mi>Q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac> <mo>=</mo> <msub><mi>E</mi>
    <mrow><mi>Q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></msub> <mrow><mo>[</mo>
    <mi>x</mi> <mfrac><mrow><mi>P</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow> <mrow><mi>Q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac>
    <mo>]</mo></mrow></mrow></math>
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper E Subscript upper P left-parenthesis x right-parenthesis
    Baseline left-bracket x right-bracket equals sigma-summation Underscript x Endscripts
    upper P left-parenthesis x right-parenthesis x equals sigma-summation Underscript
    x Endscripts upper Q left-parenthesis x right-parenthesis x StartFraction upper
    P left-parenthesis x right-parenthesis Over upper Q left-parenthesis x right-parenthesis
    EndFraction equals upper E Subscript upper Q left-parenthesis x right-parenthesis
    Baseline left-bracket x StartFraction upper P left-parenthesis x right-parenthesis
    Over upper Q left-parenthesis x right-parenthesis EndFraction right-bracket" display="block"><mrow><msub><mi>E</mi>
    <mrow><mi>P</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></msub> <mrow><mo>[</mo>
    <mi>x</mi> <mo>]</mo></mrow> <mo>=</mo> <munder><mo>∑</mo> <mi>x</mi></munder>
    <mi>P</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mi>x</mi> <mo>=</mo>
    <munder><mo>∑</mo> <mi>x</mi></munder> <mi>Q</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow> <mi>x</mi> <mfrac><mrow><mi>P</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow>
    <mrow><mi>Q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac> <mo>=</mo> <msub><mi>E</mi>
    <mrow><mi>Q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></msub> <mrow><mo>[</mo>
    <mi>x</mi> <mfrac><mrow><mi>P</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow> <mrow><mi>Q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac>
    <mo>]</mo></mrow></mrow></math>
- en: One example where importance sampling is used in ML is policy-based reinforcement
    learning. Consider the case when you want to update your policy. You want to estimate
    the value functions of the new policy, but calculating the total rewards of taking
    an action can be costly because it requires considering all possible outcomes
    until the end of the time horizon after that action. However, if the new policy
    is relatively close to the old policy, you can calculate the total rewards based
    on the old policy instead and reweight them according to the new policy. The rewards
    from the old policy make up the proposal distribution.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中使用重要性采样的一个例子是基于策略的强化学习。考虑这样一种情况，当你想要更新你的策略时。你希望估算新策略的值函数，但是计算采取行动后的总奖励可能很昂贵，因为这需要考虑到所有可能的结果，直到时间顶点结束。然而，如果新策略与旧策略相对接近，你可以基于旧策略计算总奖励，并根据新策略对其进行重新加权。旧策略的奖励构成提案分布。
- en: Labeling
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标签化
- en: Despite the promise of unsupervised ML, most ML models in production today are
    supervised, which means that they need labeled data to learn from. The performance
    of an ML model still depends heavily on the quality and quantity of the labeled
    data it’s trained on.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管无监督机器学习有很大潜力，但今天大多数生产中的机器学习模型仍然是有监督的，这意味着它们需要有标记的数据来学习。机器学习模型的性能仍然严重依赖于它们所训练的有标记数据的质量和数量。
- en: 'In a talk to my students, Andrej Karpathy, director of AI at Tesla, shared
    an anecdote about how when he decided to have an in-house labeling team, his recruiter
    asked how long he’d need this team for. He responded: “How long do we need an
    engineering team for?” Data labeling has gone from being an auxiliary task to
    being a core function of many ML teams in production.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在一次与我的学生交谈中，特斯拉人工智能主管安德烈·卡帕西分享了一个轶事。他决定建立一个内部标注团队时，他的招聘人员问他需要这支团队多久。他回答道：“我们需要工程团队多久？”数据标注已经从辅助任务发展成为许多生产中机器学习团队的核心功能。
- en: 'In this section, we will discuss the challenge of obtaining labels for your
    data. We’ll first discuss the labeling method that usually comes first in data
    scientists’ mind when talking about labeling: hand-labeling. We will then discuss
    tasks with natural labels, which are tasks where labels can be inferred from the
    system without requiring human annotations, followed by what to do when natural
    and hand labels are lacking.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论获取数据标签的挑战。我们首先将讨论数据科学家在谈论标签时通常首先考虑的标注方法：手动标注。然后我们将讨论自然标签任务，即从系统中推断标签的任务，而无需人类注释，接着是在自然标签和手动标签都缺失时应采取的措施。
- en: Hand Labels
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 手动标签
- en: 'Anyone who has ever had to work with data in production has probably felt this
    at a visceral level: acquiring hand labels for your data is difficult for many,
    many reasons. First, hand-labeling data can be expensive, especially if subject
    matter expertise is required. To classify whether a comment is spam, you might
    be able to find 20 annotators on a crowdsourcing platform and train them in 15
    minutes to label your data. However, if you want to label chest X-rays, you’d
    need to find board-certified radiologists, whose time is limited and expensive.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 任何曾经在生产中处理数据的人都可能从直观上感受到这一点：获取数据的手动标签因为种种原因非常困难。首先，手动标注数据可能非常昂贵，特别是如果需要专业的主题专家知识。例如，要分类评论是否为垃圾评论，您可以在众包平台上找到20名标注者，并在15分钟内训练他们标记您的数据。但是，如果您想要标记胸部X光片，您就需要找到经过认证的放射科医生，他们的时间有限且昂贵。
- en: Second, hand labeling poses a threat to data privacy. Hand labeling means that
    someone has to look at your data, which isn’t always possible if your data has
    strict privacy requirements. For example, you can’t just ship your patients’ medical
    records or your company’s confidential financial information to a third-party
    service for labeling. In many cases, your data might not even be allowed to leave
    your organization, and you might have to hire or contract annotators to label
    your data on premises.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，手动标注对数据隐私构成威胁。手动标注意味着有人需要查看您的数据，如果您的数据具有严格的隐私要求，这并不总是可能的。例如，您不能仅仅将患者的医疗记录或公司的机密财务信息发送给第三方服务进行标注。在许多情况下，您的数据甚至可能不允许离开您的组织，您可能需要雇佣或签约标注员在您的数据上标记。
- en: Third, hand labeling is slow. For example, accurate transcription of speech
    utterance at the phonetic level can take 400 times longer than the utterance duration.^([7](ch04.xhtml#ch01fn88))
    So if you want to annotate 1 hour of speech, it’ll take 400 hours or almost 3
    months for a person to do so. In a study to use ML to help classify lung cancers
    from X-rays, my colleagues had to wait almost a year to obtain sufficient labels.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，手动标记速度慢。例如，精确转录语音发音的音素级别可能比发音持续时间长400倍。^([7](ch04.xhtml#ch01fn88)) 因此，如果你想要标注1小时的语音，一个人可能需要花费400个小时，或者几乎3个月的时间。在一项研究中，为了帮助从X射线中分类肺癌，我的同事们不得不等待将近一年才能获得足够的标签。
- en: 'Slow labeling leads to slow iteration speed and makes your model less adaptive
    to changing environments and requirements. If the task changes or data changes,
    you’ll have to wait for your data to be relabeled before updating your model.
    Imagine the scenario when you have a sentiment analysis model to analyze the sentiment
    of every tweet that mentions your brand. It has only two classes: NEGATIVE and
    POSITIVE. However, after deployment, your PR team realizes that the most damage
    comes from angry tweets and they want to attend to angry messages faster. So you
    have to update your sentiment analysis model to have three classes: NEGATIVE,
    POSITIVE, and ANGRY. To do so, you will need to look at your data again to see
    which existing training examples should be relabeled ANGRY. If you don’t have
    enough ANGRY examples, you will have to collect more data. The longer the process
    takes, the more your existing model performance will degrade.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 缓慢的标记速度导致迭代速度慢，使得你的模型难以适应不断变化的环境和需求。如果任务或数据发生变化，你必须等待数据重新标记才能更新模型。想象一下这样的场景：你有一个情感分析模型，用于分析提到你品牌的每条推文的情感。它只有两个类别：负面和正面。然而，在部署后，你的公关团队意识到最大的损害来自愤怒的推文，并希望更快地回应愤怒的消息。因此，你需要更新你的情感分析模型，增加三个类别：负面、正面和愤怒。为此，你需要再次查看你的数据，看看哪些现有的训练示例应该重新标记为愤怒。如果你没有足够的愤怒示例，你将不得不收集更多的数据。这个过程越长，你现有模型的性能就会下降得越多。
- en: Label multiplicity
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标签的多样性
- en: 'Often, to obtain enough labeled data, companies have to use data from multiple
    sources and rely on multiple annotators who have different levels of expertise.
    These different data sources and annotators also have different levels of accuracy.
    This leads to the problem of label ambiguity or label multiplicity: what to do
    when there are multiple conflicting labels for a data instance.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，为了获得足够的标记数据，公司必须使用多个来源的数据，并依赖于具有不同专业水平的多位标注者。这些不同的数据来源和标注者也具有不同的准确性水平。这就导致了标签的歧义性或标签的多样性问题：当一个数据实例存在多个冲突的标签时该如何处理。
- en: 'Consider this simple task of entity recognition. You give three annotators
    the following sample and ask them to annotate all entities they can find:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到实体识别的简单任务。你给了三位标注者以下样本，并要求他们标注出所有能找到的实体：
- en: Darth Sidious, known simply as the Emperor, was a Dark Lord of the Sith who
    reigned over the galaxy as Galactic Emperor of the First Galactic Empire.
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 辛迪厄斯·达斯，简称为皇帝，是西斯领主中统治银河系的银河帝国的银河皇帝。
- en: You receive back three different solutions, as shown in [Table 4-1](#identities_identified_by_different_anno).
    Three annotators have identified different entities. Which one should your model
    train on? A model trained on data labeled by annotator 1 will perform very differently
    from a model trained on data labeled by annotator 2.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你会得到三种不同的解决方案，如[表 4-1](#identities_identified_by_different_anno)所示。三位标注者标识出了不同的实体。你的模型应该基于哪一个进行训练？基于标注者1标记的数据训练的模型与基于标注者2标记的模型表现差异非常大。
- en: Table 4-1\. Entities identified by different annotators might be very different
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-1\. 不同标注者标识的实体可能非常不同
- en: '| Annotator | # entities | Annotation |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 标注者 | # 实体 | 标注 |'
- en: '| --- | --- | --- |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | 3 | [*Darth Sidious*], known simply as the Emperor, was a [*Dark Lord
    of the Sith*] who reigned over the galaxy as [*Galactic Emperor of the First Galactic
    Empire*]. |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 3 | [*辛迪厄斯·达斯*]，简称为[*皇帝*]，是[*西斯领主*]之一，统治着银河系，作为[*第一银河帝国的银河皇帝*]。|'
- en: '| 2 | 6 | [*Darth Sidious*], known simply as the [*Emperor*], was a [*Dark
    Lord*] of the [*Sith*] who reigned over the galaxy as [*Galactic Emperor*] of
    the [*First Galactic Empire*]. |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 6 | [*辛迪厄斯·达斯*]，简称为[*皇帝*]，是[*西斯领主*]之一，统治着银河系，作为[*第一银河帝国的银河皇帝*]。|'
- en: '| 3 | 4 | [*Darth Sidious*], known simply as the [*Emperor*], was a [*Dark
    Lord of the Sith*] who reigned over the galaxy as [*Galactic Emperor of the First
    Galactic Empire*]. |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 4 | [*达斯·西迪厄斯*]，简称为[*皇帝*]，是一个统治宇宙的[*西斯黑暗领主*]，并作为[*第一银河帝国的银河帝国皇帝*]统治着银河系。|'
- en: Disagreements among annotators are extremely common. The higher the level of
    domain expertise required, the higher the potential for annotating disagreement.^([8](ch04.xhtml#ch01fn89))
    If one human expert thinks the label should be A while another believes it should
    be B, how do we resolve this conflict to obtain one single ground truth? If human
    experts can’t agree on a label, what does human-level performance even mean?
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 标注者之间的分歧非常常见。领域专业知识要求越高，标注分歧的可能性就越大。如果一个专家认为标签应该是A，而另一个认为应该是B，我们如何解决这种冲突以获得一个统一的真相？如果人类专家无法就标签达成一致意见，人类水平的表现又意味着什么呢？
- en: To minimize the disagreement among annotators, it’s important to first have
    a clear problem definition. For example, in the preceding entity recognition task,
    some disagreements could have been eliminated if we clarify that in case of multiple
    possible entities, pick the entity that comprises the longest substring. This
    means *Galactic Emperor of the First Galactic Empire* instead of *Galactic Emperor*
    and *First Galactic Empire*. Second, you need to incorporate that definition into
    the annotators’ training to make sure that all annotators understand the rules.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少标记者之间的分歧，首先必须明确定义问题。例如，在前述的实体识别任务中，一些分歧可以通过澄清的方式加以消除，即在存在多个可能实体的情况下，选择包含最长子字符串的实体。这意味着*第一银河帝国的银河帝国皇帝*而不是*银河帝国*和*第一银河帝国*。其次，你需要将这个定义纳入标注者的训练中，以确保所有标记者理解这些规则。
- en: Data lineage
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据血统
- en: Indiscriminately using data from multiple sources, generated with different
    annotators, without examining their quality can cause your model to fail mysteriously.
    Consider a case when you’ve trained a moderately good model with 100K data samples.
    Your ML engineers are confident that more data will improve the model performance,
    so you spend a lot of money to hire annotators to label another million data samples.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 不加区分地使用来自不同标注者生成的多个来源的数据，而不检查它们的质量，可能导致你的模型神秘地失败。考虑这样一种情况，你已经用10万个数据样本训练了一个适度好的模型。你的机器学习工程师确信更多的数据会提高模型性能，所以你花了大量资金雇佣标注者为另外一百万数据样本打标签。
- en: However, the model performance actually decreases after being trained on the
    new data. The reason is that the new million samples were crowdsourced to annotators
    who labeled data with much less accuracy than the original data. It can be especially
    difficult to remedy this if you’ve already mixed your data and can’t differentiate
    new data from old data.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在用新数据训练后，模型性能实际上下降了。原因是新的百万个样本是通过标注者众包来标记的，而这些标注者的标记精度明显低于原始数据。如果你已经混合了数据，并且无法区分新数据和旧数据，那么解决这个问题将会格外困难。
- en: It’s good practice to keep track of the origin of each of your data samples
    as well as its labels, a technique known as *data lineage*. Data lineage helps
    you both flag potential biases in your data and debug your models. For example,
    if your model fails mostly on the recently acquired data samples, you might want
    to look into how the new data was acquired. On more than one occasion, we’ve discovered
    that the problem wasn’t with our model, but because of the unusually high number
    of wrong labels in the data that we’d acquired recently.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 良好的做法是跟踪每个数据样本的来源以及其标签，这一技术被称为*数据血统*。数据血统可以帮助你识别数据中潜在的偏见并调试你的模型。例如，如果你的模型在最近获得的数据样本上表现不佳，你可能需要查看新数据是如何获取的。我们多次发现问题不在于我们的模型，而是因为最近获取的数据中错误标签的数量异常之高。
- en: Natural Labels
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然标签
- en: Hand-labeling isn’t the only source for labels. You might be lucky enough to
    work on tasks with natural ground truth labels. Tasks with natural labels are
    tasks where the model’s predictions can be automatically evaluated or partially
    evaluated by the system. An example is the model that estimates time of arrival
    for a certain route on Google Maps. If you take that route, by the end of your
    trip, Google Maps knows how long the trip actually took, and thus can evaluate
    the accuracy of the predicted time of arrival. Another example is stock price
    prediction. If your model predicts a stock’s price in the next two minutes, then
    after two minutes, you can compare the predicted price with the actual price.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 手动标记并不是唯一的标签来源。您可能有幸可以在具有自然的地面真实标签的任务上工作。具有自然标签的任务是模型的预测可以通过系统自动或部分自动地进行评估的任务。例如，估计Google地图上某条路线的到达时间的模型。如果您采用了该路线，到达后，Google地图知道实际行程需要多长时间，因此可以评估预测到达时间的准确性。另一个例子是股票价格预测。如果您的模型预测某只股票的下一个两分钟的价格，则两分钟后，您可以将预测价格与实际价格进行比较。
- en: The canonical example of tasks with natural labels is recommender systems. The
    goal of a recommender system is to recommend to users items relevant to them.
    Whether a user clicks on the recommended item or not can be seen as the feedback
    for that recommendation. A recommendation that gets clicked on can be presumed
    to be good (i.e., the label is POSITIVE) and a recommendation that doesn’t get
    clicked on after a period of time, say 10 minutes, can be presumed to be bad (i.e.,
    the label is NEGATIVE).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 具有自然标签的任务的典型例子是推荐系统。推荐系统的目标是向用户推荐与他们相关的项目。用户是否点击推荐的项目可以被视为该推荐的反馈。被点击的推荐可以被认为是好的（即标签为正面），而在一段时间后没有被点击的推荐，比如10分钟后，可以被认为是坏的（即标签为负面）。
- en: Many tasks can be framed as recommendation tasks. For example, you can frame
    the task of predicting ads’ click-through rates as recommending the most relevant
    ads to users based on their activity histories and profiles. Natural labels that
    are inferred from user behaviors like clicks and ratings are also known as behavioral
    labels.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 许多任务可以作为推荐任务来进行。例如，您可以将预测广告点击率的任务构建为根据用户的活动历史和资料，向他们推荐最相关的广告。从用户行为如点击和评分中推断的自然标签也被称为行为标签。
- en: Even if your task doesn’t inherently have natural labels, it might be possible
    to set up your system in a way that allows you to collect some feedback on your
    model. For example, if you’re building a machine translation system like Google
    Translate, you can have the option for the community to submit alternative translations
    for bad translations—these alternative translations can be used to train the next
    iteration of your models (though you might want to review these suggested translations
    first). Newsfeed ranking is not a task with inherent labels, but by adding the
    Like button and other reactions to each newsfeed item, Facebook is able to collect
    feedback on their ranking algorithm.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 即使您的任务本质上没有自然标签，也可能可以设置系统以收集模型的一些反馈。例如，如果您正在构建像Google翻译这样的机器翻译系统，您可以让社区提交对糟糕翻译的替代翻译选项，这些替代翻译可以用于训练模型的下一次迭代（尽管您可能需要先审核这些建议的翻译）。新闻推送排名不是一个具有固有标签的任务，但通过在每个新闻推送项目上添加“赞”按钮和其他反应，Facebook能够收集其排名算法的反馈。
- en: Tasks with natural labels are fairly common in the industry. In a survey of
    86 companies in my network, I found that 63% of them work with tasks with natural
    labels, as shown in [Figure 4-3](#sixty_three_percent_of_companies_in_my). This
    doesn’t mean that 63% of tasks that can benefit from ML solutions have natural
    labels. What is more likely is that companies find it easier and cheaper to first
    start on tasks that have natural labels.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在行业中，具有自然标签的任务非常普遍。在我网络中的86家公司的调查中，我发现其中63%的公司处理具有自然标签的任务，如[图 4-3](#sixty_three_percent_of_companies_in_my)所示。这并不意味着可以从ML解决方案中受益的任务中63%具有自然标签。更可能的情况是，公司发现先从具有自然标签的任务开始更容易和更便宜。
- en: '![](Images/dmls_0403.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_0403.png)'
- en: Figure 4-3\. Sixty-three percent of companies in my network work on tasks with
    natural labels. The percentages don’t sum to 1 because a company can work with
    tasks with different label sources.^([9](ch04.xhtml#custom_ch04fn1))
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-3\. 我的网络中63%的公司从事具有自然标签的任务。百分比之和不为1是因为公司可以处理具有不同标签来源的任务。^([9](ch04.xhtml#custom_ch04fn1))
- en: In the previous example, a recommendation that doesn’t get clicked on after
    a period of time can be presumed to be bad. This is called an *implicit label*,
    as this negative label is presumed from the lack of a positive label. It’s different
    from *explicit labels* where users explicitly demonstrate their feedback on a
    recommendation by giving it a low rating or downvoting it.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，如果一个推荐在一段时间后没有被点击，可以推断它不好。这被称为*隐式标签*，因为这种负标签是基于缺少正标签推断出来的。这与*显式标签*不同，显式标签是指用户通过给推荐物品低评分或投反对票来明确表达对推荐的反馈。
- en: Feedback loop length
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反馈循环长度
- en: For tasks with natural ground truth labels, the time it takes from when a prediction
    is served until when the feedback on it is provided is the feedback loop length.
    Tasks with short feedback loops are tasks where labels are generally available
    within minutes. Many recommender systems have short feedback loops. If the recommended
    items are related products on Amazon or people to follow on Twitter, the time
    between when the item is recommended until it’s clicked on, if it’s clicked on
    at all, is short.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有自然真实标签的任务，从提供预测到提供反馈的时间称为反馈循环长度。具有短反馈循环的任务是指标签通常在几分钟内就可用的任务。许多推荐系统具有短反馈循环。如果推荐的物品是亚马逊上的相关产品或者在Twitter上关注的人，推荐物品被推荐到被点击（如果有点击）之间的时间很短。
- en: However, not all recommender systems have minute-long feedback loops. If you
    work with longer content types like blog posts or articles or YouTube videos,
    the feedback loop can be hours. If you build a system to recommend clothes for
    users like the one Stitch Fix has, you wouldn’t get feedback until users have
    received the items and tried them on, which could be weeks later.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并非所有推荐系统都有分钟级的反馈循环。如果你处理像博客文章、文章或YouTube视频这样的长内容类型，反馈循环可能需要几个小时。例如，如果你构建一个像Stitch
    Fix的服装推荐系统，你在用户收到并试穿衣物之前可能不会得到反馈，这可能需要几周的时间。
- en: Choosing the right window length requires thorough consideration, as it involves
    the speed and accuracy trade-off. A short window length means that you can capture
    labels faster, which allows you to use these labels to detect issues with your
    model and address those issues as soon as possible. However, a short window length
    also means that you might prematurely label a recommendation as bad before it’s
    clicked on.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正确的窗口长度需要深思熟虑，因为它涉及速度和准确性的权衡。较短的窗口长度意味着你可以更快地捕获标签，这使你能够使用这些标签来检测模型的问题并尽快解决这些问题。然而，较短的窗口长度也意味着在推荐被点击之前可能会过早地将推荐标记为不好。
- en: No matter how long you set your window length to be, there might still be premature
    negative labels. In early 2021, a study by the Ads team at Twitter found that
    even though the majority of clicks on ads happen within the first five minutes,
    some clicks happen hours after when the ad is shown.^([10](ch04.xhtml#ch01fn90))
    This means that this type of label tends to give an underestimate of the actual
    click-through rate. If you only record 1,000 POSITIVE labels, the actual number
    of clicks might be a bit over 1,000.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你设置窗口长度多长，可能仍会有过早的负标签。2021年初，Twitter广告团队的一项研究发现，尽管大多数广告点击发生在前五分钟内，但有些点击发生在广告展示后的几个小时内^([10](ch04.xhtml#ch01fn90))。这意味着这种类型的标签往往低估了实际的点击率。如果你只记录了1,000个正标签，实际的点击数可能会超过1,000。
- en: For tasks with long feedback loops, natural labels might not arrive for weeks
    or even months. Fraud detection is an example of a task with long feedback loops.
    For a certain period of time after a transaction, users can dispute whether that
    transaction is fraudulent or not. For example, when a customer read their credit
    card statement and saw a transaction they didn’t recognize, they might dispute
    it with their bank, giving the bank the feedback to label that transaction as
    fraudulent. A typical dispute window is one to three months. After the dispute
    window has passed, if there’s no dispute from the user, you might presume the
    transaction to be legitimate.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有长反馈循环的任务，自然标签可能需要几周甚至几个月才会到达。欺诈检测就是一个具有长反馈循环的示例。在交易后的一段时间内，用户可以对该交易是否欺诈提出异议。例如，当客户查看他们的信用卡账单并看到一个他们不认识的交易时，他们可能会向银行提出异议，从而给银行提供反馈，标记该交易为欺诈。典型的争议窗口是一个到三个月。在争议窗口结束后，如果用户没有提出异议，你可以推断该交易是合法的。
- en: Labels with long feedback loops are helpful for reporting a model’s performance
    on quarterly or yearly business reports. However, they are not very helpful if
    you want to detect issues with your models as soon as possible. If there’s a problem
    with your fraud detection model and it takes you months to catch, by the time
    the problem is fixed, all the fraudulent transactions your faulty model let through
    might have caused a small business to go bankrupt.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 具有长反馈循环的标签对于在季度或年度业务报告中报告模型性能非常有帮助。但是，如果您希望尽快检测到模型问题，则并不是非常有帮助。如果您的欺诈检测模型存在问题，并且需要几个月才能发现，那么在问题被修复时，您的错误模型放行的所有欺诈交易可能会导致小企业破产。
- en: Handling the Lack of Labels
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理标签缺失
- en: 'Because of the challenges in acquiring sufficient high-quality labels, many
    techniques have been developed to address the problems that result. In this section,
    we will cover four of them: weak supervision, semi-supervision, transfer learning,
    and active learning. A summary of these methods is shown in [Table 4-2](#summaries_for_four_techniques_for_handl).'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 由于获取足够高质量标签的挑战，许多技术已被开发来解决由此导致的问题。在本节中，我们将涵盖四种方法：弱监督、半监督、迁移学习和主动学习。这些方法的摘要显示在[表4-2](#summaries_for_four_techniques_for_handl)中。
- en: Table 4-2\. Summaries of four techniques for handling the lack of hand-labeled
    data
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 表4-2\. 处理缺乏手动标记数据的四种技术的摘要
- en: '| Method | How | Ground truths required? |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 如何 | 是否需要地面真值？ |'
- en: '| --- | --- | --- |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Weak supervision | Leverages (often noisy) heuristics to generate labels
    | No, but a small number of labels are recommended to guide the development of
    heuristics |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 弱监督 | 利用（通常是嘈杂的）启发式来生成标签 | No，但建议使用少量标签来指导启发式的开发 |'
- en: '| Semi- supervision | Leverages structural assumptions to generate labels |
    Yes, a small number of initial labels as seeds to generate more labels |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 半监督 | 利用结构假设生成标签 | Yes，使用少量初始标签作为种子来生成更多标签 |'
- en: '| Transfer learning | Leverages models pretrained on another task for your
    new task | No for zero-shot learning Yes for fine-tuning, though the number of
    ground truths required is often much smaller than what would be needed if you
    train the model from scratch |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 迁移学习 | 利用在另一个任务上预训练的模型来执行您的新任务 | 对于零-shot学习来说是No，但是对于微调来说是Yes，尽管所需的地面真值数量通常要小得多，而不是您从头开始训练该模型时所需的数量
    |'
- en: '| Active learning | Labels data samples that are most useful to your model
    | Yes |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 主动学习 | 标记对您的模型最有用的数据样本 | Yes |'
- en: Weak supervision
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 弱监督
- en: 'If hand labeling is so problematic, what if we don’t use hand labels altogether?
    One approach that has gained popularity is weak supervision. One of the most popular
    open source tools for weak supervision is Snorkel, developed at the Stanford AI
    Lab.^([11](ch04.xhtml#ch01fn91)) The insight behind weak supervision is that people
    rely on heuristics, which can be developed with subject matter expertise, to label
    data. For example, a doctor might use the following heuristics to decide whether
    a patient’s case should be prioritized as emergent:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果手动标记如此棘手，那么如果我们完全不使用手动标签会怎样？一种获得广泛关注的方法是弱监督。弱监督的最流行的开源工具之一是由斯坦福人工智能实验室开发的Snorkel。^([11](ch04.xhtml#ch01fn91))
    弱监督背后的见解是人们依赖启发式，可以通过主题专业知识来开发，来标记数据。例如，医生可能使用以下启发式来决定是否应将患者病例优先考虑为紧急：
- en: If the nurse’s note mentions a serious condition like pneumonia, the patient’s
    case should be given priority consideration.
  id: totrans-111
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果护士的笔记提到严重病情如肺炎，应优先考虑患者的病例。
- en: 'Libraries like Snorkel are built around the concept of a *labeling function*
    (LF): a function that encodes heuristics. The preceding heuristics can be expressed
    by the following function:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Snorkel等库是围绕标签函数（LF）的概念构建的：一种编码启发式的函数。上述启发式可以通过以下函数表达：
- en: '[PRE1]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'LFs can encode many different types of heuristics. Here are some of them:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: LFs 可以编码许多不同类型的启发式方法。以下是其中一些：
- en: Keyword heuristic
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词启发式
- en: Such as the preceding example
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 例如前面的例子
- en: Regular expressions
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式
- en: Such as if the note matches or fails to match a certain regular expression
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果笔记匹配或未能匹配某个特定正则表达式
- en: Database lookup
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库查找
- en: Such as if the note contains the disease listed in the dangerous disease list
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果笔记包含在危险疾病列表中列出的疾病
- en: The outputs of other models
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 其他模型的输出
- en: Such as if an existing system classifies this as `EMERGENT`
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果现有系统将其分类为`EMERGENT`
- en: After you’ve written LFs, you can apply them to the samples you want to label.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写了LFs之后，您可以将它们应用于要标注的样本。
- en: Because LFs encode heuristics, and heuristics are noisy, labels produced by
    LFs are noisy. Multiple LFs might apply to the same data examples, and they might
    give conflicting labels. One function might think a nurse’s note is `EMERGENT`
    but another function might think it’s not. One heuristic might be much more accurate
    than another heuristic, which you might not know because you don’t have ground
    truth labels to compare them to. It’s important to combine, denoise, and reweight
    all LFs to get a set of most likely to be correct labels. [Figure 4-4](#a_high_level_overview_of_how_labeling_f)
    shows at a high level how LFs work.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 因为LFs编码启发式，而启发式是有噪音的，LFs产生的标签也会有噪音。多个LFs可能适用于同一数据示例，并且它们可能会给出冲突的标签。一个函数可能认为护士的笔记是`EMERGENT`，但另一个函数可能认为不是。一个启发式可能比另一个启发式更准确，但您可能不知道，因为没有地面真实标签进行比较。重要的是将所有LFs组合、去噪声并重新加权，以获取最有可能正确的一组标签。[图 4-4](#a_high_level_overview_of_how_labeling_f)在高层次展示了LFs如何工作。
- en: '![](Images/dmls_0404.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_0404.png)'
- en: 'Figure 4-4\. A high-level overview of how labeling functions are combined.
    Source: Adapted from an image by Ratner et al.^([12](ch04.xhtml#ch01fn92))'
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-4\. 标注函数如何组合的高级概述。来源：Ratner等人的图像^([12](ch04.xhtml#ch01fn92))
- en: In theory, you don’t need any hand labels for weak supervision. However, to
    get a sense of how accurate your LFs are, a small number of hand labels is recommended.
    These hand labels can help you discover patterns in your data to write better
    LFs.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，弱监督不需要任何手动标签。然而，为了了解LFs的准确性，建议对少量手动标签进行评估。这些手动标签可以帮助您发现数据中的模式，以编写更好的LFs。
- en: Weak supervision can be especially useful when your data has strict privacy
    requirements. You only need to see a small, cleared subset of data to write LFs,
    which can be applied to the rest of your data without anyone looking at it.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督在数据有严格隐私要求时尤为有用。您只需查看少量经过清理的数据子集来编写LFs，然后可以将其应用于其他数据，而无需任何人查看。
- en: With LFs, subject matter expertise can be versioned, reused, and shared. Expertise
    owned by one team can be encoded and used by another team. If your data changes
    or your requirements change, you can just reapply LFs to your data samples. The
    approach of using LFs to generate labels for your data is also known as programmatic
    labeling. [Table 4-3](#the_advantages_of_programmatic_labeling) shows some of
    the advantages of programmatic labeling over hand labeling.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 利用LFs，主题专业知识可以进行版本管理、重复使用和共享。一个团队拥有的专业知识可以被编码并被另一个团队使用。如果您的数据或需求发生变化，您只需重新应用LFs到数据样本即可。使用LFs为数据生成标签的方法也被称为程序化标注。[表格 4-3](#the_advantages_of_programmatic_labeling)展示了程序化标注相对于手动标注的一些优势。
- en: Table 4-3\. The advantages of programmatic labeling over hand labeling
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 4-3\. 程序化标注相对于手动标注的优势
- en: '| Hand labeling | Programmatic labeling |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 手动标注 | 程序化标注 |'
- en: '| --- | --- |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| **Expensive**: Especially when subject matter expertise required | **Cost
    saving**: Expertise can be versioned, shared, and reused across an organization
    |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| **昂贵**：特别是需要主题专业知识时 | **节约成本**：专业知识可以在整个组织中进行版本管理、共享和重复使用 |'
- en: '| **Lack of privacy**: Need to ship data to human annotators | **Privacy**:
    Create LFs using a cleared data subsample and then apply LFs to other data without
    looking at individual samples |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| **隐私缺乏**：需要将数据发送给人类标注者 | **隐私**：使用清理后的数据子样本创建标注函数（LFs），然后将LFs应用于其他数据而无需查看单个样本
    |'
- en: '| **Slow**: Time required scales linearly with number of labels needed | **Fast**:
    Easily scale from 1K to 1M samples |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| **缓慢**：所需时间随需要标注的标签数呈线性增长 | **快速**：轻松从1K扩展到1M个样本 |'
- en: '| **Nonadaptive**: Every change requires relabeling the data | **Adaptive**:
    When changes happen, just reapply LFs! |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| **非自适应**：每次更改都需要重新标注数据 | **自适应**：当发生更改时，只需重新应用LFs！ |'
- en: Here is a case study to show how well weak supervision works in practice. In
    a study with Stanford Medicine,^([13](ch04.xhtml#ch01fn93)) models trained with
    weakly supervised labels obtained by a single radiologist after eight hours of
    writing LFs had comparable performance with models trained on data obtained through
    almost a year of hand labeling, as shown in [Figure 4-5](#comparison_of_the_performance_of_a_mode).
    There are two interesting facts about the results of the experiment. First, the
    models continued improving with more unlabeled data even without more LFs. Second,
    LFs were being reused across tasks. The researchers were able to reuse six LFs
    between the CXR (chest X-rays) task and EXR (extremity X-rays) task.^([14](ch04.xhtml#ch01fn94))
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个案例研究，展示弱监督在实践中的良好效果。在与斯坦福医学院的一项研究中^([13](ch04.xhtml#ch01fn93))，使用单个放射科医生编写LFs后获得的弱监督标签训练的模型，其性能与通过近一年的手工标注获得的数据训练的模型相当，如[图4-5](#comparison_of_the_performance_of_a_mode)所示。有关实验结果的两个有趣事实。首先，即使没有更多的LFs，模型仍在随着更多未标记数据而改进。其次，LFs在任务之间被重复使用。研究人员能够在胸部X射线（CXR）任务和四肢X射线（EXR）任务之间重复使用六个LFs^([14](ch04.xhtml#ch01fn94))。
- en: '![](Images/dmls_0405.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_0405.png)'
- en: 'Figure 4-5\. Comparison of the performance of a model trained on fully supervised
    labels (FS) and a model trained with programmatic labels (DP) on CXR and EXR tasks.
    Source: Dunnmon et al.^([15](ch04.xhtml#ch01fn95))'
  id: totrans-139
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-5。完全监督标签（FS）训练的模型与程序标签（DP）训练的模型在CXR和EXR任务上性能的比较。来源：Dunnmon等人^([15](ch04.xhtml#ch01fn95))
- en: My students often ask that if heuristics work so well to label data, why do
    we need ML models? One reason is that LFs might not cover all data samples, so
    we can train ML models on data programmatically labeled with LFs and use this
    trained model to generate predictions for samples that aren’t covered by any LF.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我的学生经常问，如果启发式方法能够如此有效地标记数据，为什么我们还需要机器学习模型呢？一个原因是LFs可能无法覆盖所有数据样本，因此我们可以在由LFs程序标记的数据上训练机器学习模型，并使用此训练模型为没有任何LFs覆盖的样本生成预测。
- en: Weak supervision is a simple but powerful paradigm. However, it’s not perfect.
    In some cases, the labels obtained by weak supervision might be too noisy to be
    useful. But even in these cases, weak supervision can be a good way to get you
    started when you want to explore the effectiveness of ML without wanting to invest
    too much in hand labeling up front.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督是一种简单但强大的范式。然而，它并不完美。在某些情况下，通过弱监督获得的标签可能太嘈杂，以至于无法派上用场。但即使在这些情况下，弱监督也可以是一个很好的起点，当您想要探索机器学习的有效性而又不想在最开始投入过多手工标注时。
- en: Semi-supervision
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 半监督学习
- en: If weak supervision leverages heuristics to obtain noisy labels, semi-supervision
    leverages structural assumptions to generate new labels based on a small set of
    initial labels. Unlike weak supervision, semi-supervision requires an initial
    set of labels.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果弱监督利用启发式方法获取嘈杂的标签，半监督则利用结构假设基于少量初始标签生成新标签。与弱监督不同，半监督需要一组初始标签。
- en: Semi-supervised learning is a technique that was used back in the 90s,^([16](ch04.xhtml#ch01fn96))
    and since then many semi-supervision methods have been developed. A comprehensive
    review of semi-supervised learning is out of the scope of this book. We’ll go
    over a small subset of these methods to give readers a sense of how they are used.
    For a comprehensive review, I recommend [“Semi-Supervised Learning Literature
    Survey”](https://oreil.ly/ULeWD) (Xiaojin Zhu, 2008) and [“A Survey on Semi-Supervised
    Learning”](https://oreil.ly/JYgCH) (Engelen and Hoos, 2018).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习是一种技术，早在90年代就开始使用^([16](ch04.xhtml#ch01fn96))，从那时起，许多半监督方法已经被开发出来。对半监督学习的全面回顾超出了本书的范围。我们将讨论其中的一小部分方法，以让读者了解它们的使用方式。对于全面的回顾，我推荐阅读[“半监督学习文献综述”](https://oreil.ly/ULeWD)（Xiaojin
    Zhu, 2008）和[“关于半监督学习的调查”](https://oreil.ly/JYgCH)（Engelen和Hoos, 2018）。
- en: A classic semi-supervision method is *self-training*. You start by training
    a model on your existing set of labeled data and use this model to make predictions
    for unlabeled samples. Assuming that predictions with high raw probability scores
    are correct, you add the labels predicted with high probability to your training
    set and train a new model on this expanded training set. This goes on until you’re
    happy with your model performance.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的半监督方法是*自训练*。您首先在现有的标记数据集上训练模型，然后使用该模型对未标记样本进行预测。假设具有高原始概率分数的预测是正确的，则将高概率预测的标签添加到训练集中，并在这个扩展的训练集上训练新模型。这个过程一直持续，直到您对模型的表现满意。
- en: Another semi-supervision method assumes that data samples that share similar
    characteristics share the same labels. The similarity might be obvious, such as
    in the task of classifying the topic of Twitter hashtags. You can start by labeling
    the hashtag “#AI” as Computer Science. Assuming that hashtags that appear in the
    same tweet or profile are likely about the same topic, given the profile of MIT
    CSAIL in [Figure 4-6](#because_hashml_and_hashbigdata_appear_i), you can also
    label the hashtags “#ML” and “#BigData” as Computer Science.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种半监督方法假设具有相似特征的数据样本共享相同的标签。这种相似性可能是显而易见的，例如在分类 Twitter 标签主题的任务中。你可以开始将标签为“#AI”的标签标记为计算机科学。假设在同一条推文或个人资料中出现的标签很可能是关于相同主题的，鉴于
    MIT CSAIL 在 [Figure 4-6](#because_hashml_and_hashbigdata_appear_i) 中的资料，你也可以将标签“#ML”和“#BigData”标记为计算机科学。
- en: '![](Images/dmls_0406.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_0406.png)'
- en: 'Figure 4-6\. Because #ML and #BigData appear in the same Twitter profile as
    #AI, we can assume that they belong to the same topic'
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4-6。因为 #ML 和 #BigData 在同一个 Twitter 资料中出现在 #AI 旁边，我们可以假设它们属于同一个主题。'
- en: In most cases, the similarity can only be discovered by more complex methods.
    For example, you might need to use a clustering method or a *k*-nearest neighbors
    algorithm to discover samples that belong to the same cluster.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，只有通过更复杂的方法才能发现相似性。例如，你可能需要使用聚类方法或 *k* 最近邻算法来发现属于同一簇的样本。
- en: A semi-supervision method that has gained popularity in recent years is the
    perturbation-based method. It’s based on the assumption that small perturbations
    to a sample shouldn’t change its label. So you apply small perturbations to your
    training instances to obtain new training instances. The perturbations might be
    applied directly to the samples (e.g., adding white noise to images) or to their
    representations (e.g., adding small random values to embeddings of words). The
    perturbed samples have the same labels as the unperturbed samples. We’ll discuss
    more about this in the section [“Perturbation”](#perturbation).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来备受欢迎的半监督方法之一是基于扰动的方法。它基于这样的假设：对样本进行小的扰动不应改变其标签。因此，你可以对训练实例施加小的扰动以获得新的训练实例。这些扰动可以直接应用于样本（例如，向图像添加白噪声）或者应用于它们的表示（例如，向单词的嵌入添加小的随机值）。扰动后的样本与未扰动的样本具有相同的标签。我们将在
    [“扰动”](#perturbation) 部分进一步讨论这个问题。
- en: In some cases, semi-supervision approaches have reached the performance of purely
    supervised learning, even when a substantial portion of the labels in a given
    dataset has been discarded.^([17](ch04.xhtml#ch01fn97))
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，即使给定数据集中有相当比例的标签被丢弃，半监督方法的表现也达到了纯监督学习的水平^([17](ch04.xhtml#ch01fn97))。
- en: Semi-supervision is the most useful when the number of training labels is limited.
    One thing to consider when doing semi-supervision with limited data is how much
    of this limited data should be used to evaluate multiple candidate models and
    select the best one. If you use a small amount, the best performing model on this
    small evaluation set might be the one that overfits the most to this set. On the
    other hand, if you use a large amount of data for evaluation, the performance
    boost gained by selecting the best model based on this evaluation set might be
    less than the boost gained by adding the evaluation set to the limited training
    set. Many companies overcome this trade-off by using a reasonably large evaluation
    set to select the best model, then continuing training the champion model on the
    evaluation set.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练标签数量有限时，半监督方法变得尤为有用。在使用有限数据进行半监督时，需要考虑的一点是如何利用这些有限数据来评估多个候选模型并选择最佳模型。如果使用少量数据，则在这个小型评估集上表现最佳的模型可能是对这个集合过拟合最严重的模型。另一方面，如果使用大量数据进行评估，则基于这个评估集选择最佳模型所获得的性能提升可能小于通过将评估集添加到有限训练集中所获得的性能提升。许多公司通过在较大的评估集上选择最佳模型，然后继续在评估集上训练冠军模型来克服这种权衡。
- en: Transfer learning
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 迁移学习
- en: 'Transfer learning refers to the family of methods where a model developed for
    a task is reused as the starting point for a model on a second task. First, the
    base model is trained for a base task. The base task is usually a task that has
    cheap and abundant training data. Language modeling is a great candidate because
    it doesn’t require labeled data. Language models can be trained on any body of
    text—books, Wikipedia articles, chat histories—and the task is: given a sequence
    of tokens,^([18](ch04.xhtml#ch01fn98)) predict the next token. When given the
    sequence “I bought NVIDIA shares because I believe in the importance of,” a language
    model might output “hardware” or “GPU” as the next token.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是指一种方法族，其中为一个任务开发的模型被重复用作第二个任务模型的起点。首先，基础模型被训练用于一个基础任务。通常，基础任务是一个拥有廉价且丰富的训练数据的任务。语言建模是一个很好的候选，因为它不需要标记数据。语言模型可以在任何文本体系上进行训练——书籍、维基百科文章、聊天记录——任务是：给定一系列标记，^([18](ch04.xhtml#ch01fn98))
    预测下一个标记。当给定序列“I bought NVIDIA shares because I believe in the importance of,”时，语言模型可能会输出“hardware”或“GPU”作为下一个标记。
- en: The trained model can then be used for the task that you’re interested in—a
    downstream task—such as sentiment analysis, intent detection, or question answering.
    In some cases, such as in zero-shot learning scenarios, you might be able to use
    the base model on a downstream task directly. In many cases, you might need to
    *fine-tune* the base model. Fine-tuning means making small changes to the base
    model, such as continuing to train the base model or a part of the base model
    on data from a given downstream task.^([19](ch04.xhtml#ch01fn99))
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，可以将训练好的模型用于您感兴趣的任务——下游任务——如情感分析、意图检测或问答。在某些情况下，例如零样本学习场景中，您可以直接在下游任务中使用基础模型。在许多情况下，您可能需要*微调*基础模型。微调意味着对基础模型进行小的更改，例如继续训练基础模型或部分基础模型，使用给定下游任务的数据。^([19](ch04.xhtml#ch01fn99))
- en: 'Sometimes, you might need to modify the inputs using a template to prompt the
    base model to generate the outputs you want.^([20](ch04.xhtml#ch01fn100)) For
    example, to use a language model as the base model for a question answering task,
    you might want to use this prompt:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，您可能需要使用模板修改输入，以促使基础模型生成您想要的输出。^([20](ch04.xhtml#ch01fn100)) 例如，要将语言模型用作问答任务的基础模型，您可能想要使用以下提示：
- en: '*Q: When was the United States founded?*'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*Q: 美国是何时成立的？*'
- en: '*A: July 4, 1776.*'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '*A: 1776年7月4日。*'
- en: '*Q: Who wrote the Declaration of Independence?*'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '*Q: 谁写了《独立宣言》？*'
- en: '*A: Thomas Jefferson.*'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '*A: 托马斯·杰斐逊。*'
- en: '*Q: What year was Alexander Hamilton born?*'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '*Q: 亚历山大·汉密尔顿是哪一年出生的？*'
- en: '*A:*'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*A:*'
- en: When you input this prompt into a language model such as [GPT-3](https://oreil.ly/qT0r3),
    it might output the year Alexander Hamilton was born.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 当您将此提示输入到诸如[GPT-3](https://oreil.ly/qT0r3)之类的语言模型中时，它可能会输出亚历山大·汉密尔顿的出生年份。
- en: Transfer learning is especially appealing for tasks that don’t have a lot of
    labeled data. Even for tasks that have a lot of labeled data, using a pretrained
    model as the starting point can often boost the performance significantly compared
    to training from scratch.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 对于没有大量标记数据的任务，迁移学习尤为吸引人。即使对于有大量标记数据的任务，使用预训练模型作为起点通常也能显著提高性能，与从头开始训练相比。
- en: Transfer learning has gained a lot of interest in recent years for the right
    reasons. It has enabled many applications that were previously impossible due
    to the lack of training samples. A nontrivial portion of ML models in production
    today are the results of transfer learning, including object detection models
    that leverage models pretrained on ImageNet and text classification models that
    leverage pretrained language models such as BERT or GPT-3.^([21](ch04.xhtml#ch01fn101))
    Transfer learning also lowers the entry barriers into ML, as it helps reduce the
    up-front cost needed for labeling data to build ML applications.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，迁移学习因合理的原因引起了广泛关注。它使得许多以前由于缺乏训练样本而不可能的应用成为可能。今天生产中的许多ML模型的非微不足道部分都是迁移学习的结果，包括利用在ImageNet上预训练的模型的目标检测模型和利用预训练语言模型（如BERT或GPT-3）的文本分类模型。^([21](ch04.xhtml#ch01fn101))
    迁移学习还降低了ML的进入门槛，因为它有助于减少构建ML应用程序所需的标记数据的前期成本。
- en: A trend that has emerged in the last five years is that (usually) the larger
    the pretrained base model, the better its performance on downstream tasks. Large
    models are expensive to train. Based on the configuration of GPT-3, it’s estimated
    that the cost of training this model is in the tens of millions USD. Many have
    hypothesized that in the future only a handful of companies will be able to afford
    to train large pretrained models. The rest of the industry will use these pretrained
    models directly or fine-tune them for their specific needs.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 过去五年出现的一个趋势是，通常情况下，预训练基模型越大，其在下游任务上的表现越好。大型模型训练成本高昂。根据GPT-3的配置，估计训练此模型的成本在数千万美元。许多人推测，未来只有少数公司能负担得起大型预训练模型的训练成本。其余行业将直接使用这些预训练模型或根据特定需求进行微调。
- en: Active learning
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主动学习
- en: Active learning is a method for improving the efficiency of data labels. The
    hope here is that ML models can achieve greater accuracy with fewer training labels
    if they can choose which data samples to learn from. Active learning is sometimes
    called query learning—though this term is getting increasingly unpopular—because
    a model (active learner) sends back queries in the form of unlabeled samples to
    be labeled by annotators (usually humans).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习是提高数据标签效率的一种方法。希望机器学习模型可以在较少的训练标签下实现更高的准确性，如果能够选择要学习的数据样本。有时称为查询学习的主动学习术语正变得越来越不受欢迎，因为一个模型（主动学习器）会以未标记样本的形式向注释者（通常是人类）发送回查询。
- en: Instead of randomly labeling data samples, you label the samples that are most
    helpful to your models according to some metrics or heuristics. The most straightforward
    metric is uncertainty measurement—label the examples that your model is the least
    certain about, hoping that they will help your model learn the decision boundary
    better. For example, in the case of classification problems where your model outputs
    raw probabilities for different classes, it might choose the data samples with
    the lowest probabilities for the predicted class. [Figure 4-7](#how_uncertainty_based_active_learning_w)
    illustrates how well this method works on a toy example.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 不是随机标记数据样本，而是根据某些指标或启发式标记对模型最有帮助的样本。最直接的度量标准是不确定性测量——标记模型对预测决策边界最不确定的示例，希望这些示例能帮助模型学习得更好。例如，在分类问题中，您的模型为不同类别输出原始概率，它可能选择预测类别概率最低的数据样本。[图 4-7](#how_uncertainty_based_active_learning_w)展示了这种方法在一个玩具示例中的有效性。
- en: '![](Images/dmls_0407.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_0407.png)'
- en: 'Figure 4-7\. How uncertainty-based active learning works. (a) A toy dataset
    of 400 instances, evenly sampled from two class Gaussians. (b) A model trained
    on 30 samples randomly labeled gives an accuracy of 70%. (c) A model trained on
    30 samples chosen by active learning gives an accuracy of 90%. Source: Burr Settles^([22](ch04.xhtml#ch01fn102))'
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-7\. 不确定性基础的主动学习工作原理。 (a) 一个玩具数据集，包括来自两个类高斯分布的均匀采样的400个实例。 (b) 在30个随机标记样本上训练的模型的准确率为70%。
    (c) 在30个由主动学习选择的样本上训练的模型的准确率为90%。来源：Burr Settles^([22](ch04.xhtml#ch01fn102))
- en: Another common heuristic is based on disagreement among multiple candidate models.
    This method is called query-by-committee, an example of an ensemble method.^([23](ch04.xhtml#ch01fn103))
    You need a committee of several candidate models, which are usually the same model
    trained with different sets of hyperparameters or the same model trained on different
    slices of data. Each model can make one vote for which samples to label next,
    and it might vote based on how uncertain it is about the prediction. You then
    label the samples that the committee disagrees on the most.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的启发式方法基于多个候选模型之间的不同意见。这种方法称为委员会查询，是集成方法的一个例子。^([23](ch04.xhtml#ch01fn103))
    您需要一个由几个候选模型组成的委员会，通常是相同的模型，但使用不同的超参数集或在不同数据片段上训练的相同模型。每个模型可以对应该标记哪些样本进行投票，可能会基于其对预测的不确定性而投票。然后，您标记委员会在意见分歧最大的样本。
- en: There are other heuristics such as choosing samples that, if trained on them,
    will give the highest gradient updates or will reduce the loss the most. For a
    comprehensive review of active learning methods, check out [“Active Learning Literature
    Survey”](https://oreil.ly/4RuBo) (Settles 2010).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他启发式方法，如选择使梯度更新最大或将损失最大减少的样本。要全面了解主动学习方法，请参阅《主动学习文献综述》（Settles 2010）。
- en: The samples to be labeled can come from different data regimes. They can be
    synthesized where your model generates samples in the region of the input space
    that it’s most uncertain about.^([24](ch04.xhtml#ch01fn104)) They can come from
    a stationary distribution where you’ve already collected a lot of unlabeled data
    and your model chooses samples from this pool to label. They can come from the
    real-world distribution where you have a stream of data coming in, as in production,
    and your model chooses samples from this stream of data to label.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 待标记的样本可以来自不同的数据模式。它们可以是合成的，其中您的模型生成在输入空间中最不确定的区域内的样本。^([24](ch04.xhtml#ch01fn104))
    它们可以来自稳态分布，您已经收集了大量未标记数据，并且您的模型从此池中选择样本进行标记。它们可以来自真实世界的分布，其中您有一系列数据流入，如在生产中，您的模型从此数据流中选择样本进行标记。
- en: I’m most excited about active learning when a system works with real-time data.
    Data changes all the time, a phenomenon we briefly touched on in [Chapter 1](ch01.xhtml#overview_of_machine_learning_systems)
    and will further detail in [Chapter 8](ch08.xhtml#data_distribution_shifts_and_monitoring).
    Active learning in this data regime will allow your model to learn more effectively
    in real time and adapt faster to changing environments.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 当系统使用实时数据时，我最为激动的是主动学习。数据随时变化，这是我们在第1章中简要提及并将在第8章中进一步详细说明的现象。在这种数据模式中进行主动学习将使您的模型能够更有效地实时学习，并更快地适应变化的环境。
- en: Class Imbalance
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 类不平衡
- en: Class imbalance typically refers to a problem in classification tasks where
    there is a substantial difference in the number of samples in each class of the
    training data. For example, in a training dataset for the task of detecting lung
    cancer from X-ray images, 99.99% of the X-rays might be of normal lungs, and only
    0.01% might contain cancerous cells.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 类不平衡通常指的是分类任务中的问题，其中训练数据集中每个类别的样本数量差异显著。例如，在用于从X射线图像中检测肺癌的训练数据集中，99.99%的X射线可能来自正常肺部，仅有0.01%可能包含癌细胞。
- en: Class imbalance can also happen with regression tasks where the labels are continuous.
    Consider the task of estimating health-care bills.^([25](ch04.xhtml#ch01fn105))
    Health-care bills are highly skewed—the median bill is low, but the 95th percentile
    bill is astronomical. When predicting hospital bills, it might be more important
    to predict accurately the bills at the 95th percentile than the median bills.
    A 100% difference in a $250 bill is acceptable (actual $500, predicted $250),
    but a 100% difference on a $10k bill is not (actual $20k, predicted $10k). Therefore,
    we might have to train the model to be better at predicting 95th percentile bills,
    even if it reduces the overall metrics.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 类不平衡问题也可能发生在回归任务中，其中标签是连续的。考虑估算医疗费用的任务。^([25](ch04.xhtml#ch01fn105)) 医疗费用极不平衡——中位数账单较低，但95分位数账单高得惊人。在预测医院账单时，准确预测95分位数账单可能比预测中位数账单更为重要。对于250美元的账单，100%的差异是可以接受的（实际为500美元，预测为250美元），但对于1万美元的账单，100%的差异是不可以接受的（实际为2万美元，预测为1万美元）。因此，我们可能需要训练模型更好地预测95分位数账单，即使这会降低整体指标。
- en: Challenges of Class Imbalance
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类不平衡的挑战
- en: ML, especially deep learning, works well in situations when the data distribution
    is more balanced, and usually not so well when the classes are heavily imbalanced,
    as illustrated in [Figure 4-8](#ml_works_well_in_situations_where_the_c). Class
    imbalance can make learning difficult for the following three reasons.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习，尤其是深度学习，在数据分布更平衡的情况下效果很好，但通常在类别严重不平衡时效果不佳，如在[图4-8](#ml_works_well_in_situations_where_the_c)中所示。类不平衡可能使学习变得困难，原因如下所述。
- en: '![](Images/dmls_0408.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_0408.png)'
- en: 'Figure 4-8\. ML works well in situations where the classes are balanced. Source:
    Adapted from an image by Andrew Ng^([26](ch04.xhtml#ch01fn106))'
  id: totrans-182
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-8。机器学习在类平衡的情况下效果良好。来源：根据Andrew Ng的图像改编^([26](ch04.xhtml#ch01fn106))
- en: The first reason is that class imbalance often means there’s insufficient signal
    for your model to learn to detect the minority classes. In the case where there
    is a small number of instances in the minority class, the problem becomes a few-shot
    learning problem where your model only gets to see the minority class a few times
    before having to make a decision on it. In the case where there is no instance
    of the rare classes in your training set, your model might assume these rare classes
    don’t exist.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个原因是类别不平衡通常意味着你的模型没有足够的信号来学习检测少数类别。在少数类别实例较少的情况下，问题变成了少样本学习问题，在模型做出决策之前，它只能少次地看到少数类别。在训练集中没有稀有类别实例的情况下，你的模型可能会假设这些稀有类别不存在。
- en: The second reason is that class imbalance makes it easier for your model to
    get stuck in a nonoptimal solution by exploiting a simple heuristic instead of
    learning anything useful about the underlying pattern of the data. Consider the
    preceding lung cancer detection example. If your model learns to always output
    the majority class, its accuracy is already 99.99%.^([27](ch04.xhtml#ch01fn107))
    This heuristic can be very hard for gradient descent algorithms to beat because
    a small amount of randomness added to this heuristic might lead to worse accuracy.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个原因是类别不平衡使得你的模型更容易因为利用简单的启发式而陷入非最优解决方案，而不是学习数据底层模式中的任何有用信息。考虑前面的肺癌检测例子。如果你的模型学会了总是输出多数类别，它的准确率已经达到了99.99%^([27](ch04.xhtml#ch01fn107))。这种启发式很难被梯度下降算法击败，因为对这种启发式稍加随机性可能会导致更差的准确率。
- en: The third reason is that class imbalance leads to asymmetric costs of error—the
    cost of a wrong prediction on a sample of the rare class might be much higher
    than a wrong prediction on a sample of the majority class.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个原因是类别不平衡导致错误的不对称成本——在稀有类别样本上的错误预测成本可能远高于在多数类别样本上的错误预测成本。
- en: For example, misclassification on an X-ray with cancerous cells is much more
    dangerous than misclassification on an X-ray of a normal lung. If your loss function
    isn’t configured to address this asymmetry, your model will treat all samples
    the same way. As a result, you might obtain a model that performs equally well
    on both majority and minority classes, while you much prefer a model that performs
    less well on the majority class but much better on the minority one.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在具有癌细胞的X光片上的误分类比在正常肺部X光片上的误分类更危险。如果你的损失函数没有配置来处理这种不对称性，你的模型将对所有样本采取相同的方式。因此，你可能会得到一个在多数和少数类别上表现一样好的模型，而你更希望得到一个在多数类别上表现较差但在少数类别上表现更好的模型。
- en: When I was in school, most datasets I was given had more or less balanced classes.^([28](ch04.xhtml#ch01fn108))
    It was a shock for me to start working and realize that class imbalance is the
    norm. In real-world settings, rare events are often more interesting (or more
    dangerous) than regular events, and many tasks focus on detecting those rare events.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 当我上学的时候，我得到的大多数数据集都有更或多或少平衡的类别^([28](ch04.xhtml#ch01fn108))。开始工作后意识到类别不平衡是常态，让我感到震惊。在现实世界中，罕见事件通常比普通事件更有趣（或更危险），许多任务集中于检测这些罕见事件。
- en: The classical example of tasks with class imbalance is fraud detection. Most
    credit card transactions are not fraudulent. As of 2018, 6.8¢ for every $100 in
    cardholder spending is fraudulent.^([29](ch04.xhtml#custom_ch04fn2)) Another is
    churn prediction. The majority of your customers are probably not planning on
    canceling their subscription. If they are, your business has more to worry about
    than churn prediction algorithms. Other examples include disease screening (most
    people, fortunately, don’t have terminal illness) and resume screening (98% of
    job seekers are eliminated at the initial resume screening^([30](ch04.xhtml#custom_ch04fn3))).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 类别不平衡任务的经典示例是欺诈检测。大多数信用卡交易并非欺诈性的。截至2018年，每100美元的持卡人消费中有6.8美分是欺诈的^([29](ch04.xhtml#custom_ch04fn2))。另一个示例是客户流失预测。你的大多数客户可能并不打算取消订阅。如果他们这样做了，你的业务比客户流失预测算法更需要担心其他问题。其他示例包括疾病筛查（大多数人，幸运的是，并没有终末期疾病）和简历筛选（98%的求职者在初始简历筛选时被淘汰^([30](ch04.xhtml#custom_ch04fn3))）。
- en: A less obvious example of a task with class imbalance is [object detection](https://oreil.ly/CGEf5).
    Object detection algorithms currently work by generating a large number of bounding
    boxes over an image then predicting which boxes are most likely to have objects
    in them. Most bounding boxes do not contain a relevant object.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 类别不平衡的一个不太明显的例子是[目标检测](https://oreil.ly/CGEf5)。目标检测算法当前的工作方式是在图像上生成大量的边界框，然后预测哪些框最有可能包含对象。大多数边界框不包含相关对象。
- en: Outside the cases where class imbalance is inherent in the problem, class imbalance
    can also be caused by biases during the sampling process. Consider the case when
    you want to create training data to detect whether an email is spam or not. You
    decide to use all the anonymized emails from your company’s email database. According
    to Talos Intelligence, as of May 2021, nearly 85% of all emails are spam.^([31](ch04.xhtml#custom_ch04fn4))
    But most spam emails were filtered out before they reached your company’s database,
    so in your dataset, only a small percentage is spam.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 除了类别不平衡在问题本身固有的情况外，类别不平衡也可能是在采样过程中产生偏差的结果。考虑这样一种情况：你想创建用于检测电子邮件是否为垃圾邮件的训练数据。你决定使用公司电子邮件数据库中的所有匿名电子邮件。根据
    Talos Intelligence 的数据，截至2021年5月，几乎85%的电子邮件是垃圾邮件。^([31](ch04.xhtml#custom_ch04fn4))
    但大多数垃圾邮件在到达公司数据库之前已被过滤掉，所以在你的数据集中，只有很小一部分是垃圾邮件。
- en: Another cause for class imbalance, though less common, is due to labeling errors.
    Annotators might have read the instructions wrong or followed the wrong instructions
    (thinking there are only two classes, POSITIVE and NEGATIVE, while there are actually
    three), or simply made errors. Whenever faced with the problem of class imbalance,
    it’s important to examine your data to understand the causes of it.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 导致类别不平衡的另一个原因，尽管较少见，是由于标记错误。标注者可能误读了说明或者按照错误的说明操作（认为只有两个类别，正面和负面，而实际上有三个），或者只是犯了错误。每当面对类别不平衡问题时，检查数据以理解其原因至关重要。
- en: Handling Class Imbalance
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理类别不平衡
- en: Because of its prevalence in real-world applications, class imbalance has been
    thoroughly studied over the last two decades.^([32](ch04.xhtml#ch01fn109)) Class
    imbalance affects tasks differently based on the level of imbalance. Some tasks
    are more sensitive to class imbalance than others. Japkowicz showed that sensitivity
    to imbalance increases with the complexity of the problem, and that noncomplex,
    linearly separable problems are unaffected by all levels of class imbalance.^([33](ch04.xhtml#ch01fn110))
    Class imbalance in binary classification problems is a much easier problem than
    class imbalance in multiclass classification problems. Ding et al. showed that
    very deep neural networks—with “very deep” meaning over 10 layers back in 2017—performed
    much better on imbalanced data than shallower neural networks.^([34](ch04.xhtml#ch01fn111))
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在现实世界的应用中普遍存在，过去二十年来对类别不平衡进行了深入研究。^([32](ch04.xhtml#ch01fn109)) 类别不平衡会根据不平衡的程度对任务产生不同影响。有些任务对类别不平衡更为敏感。Japkowicz
    指出，对不平衡的敏感性随问题的复杂性增加而增加，并且非复杂、线性可分的问题不受任何类别不平衡的影响。^([33](ch04.xhtml#ch01fn110))
    在二元分类问题中，类别不平衡比在多类别分类问题中要简单得多。Ding 等人表明，从2017年开始，“非常深”的神经网络（指超过10层）在不平衡数据上的表现要比较浅的神经网络好得多。^([34](ch04.xhtml#ch01fn111))
- en: There have been many techniques suggested to mitigate the effect of class imbalance.
    However, as neural networks have grown to be much larger and much deeper, with
    more learning capacity, some might argue that you shouldn’t try to “fix” class
    imbalance if that’s how the data looks in the real world. A good model should
    learn to model that imbalance. However, developing a model good enough for that
    can be challenging, so we still have to rely on special training techniques.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 已经有许多技术被提出来缓解类别不平衡的影响。然而，随着神经网络变得更大、更深，学习能力更强，有人可能会认为如果数据在现实世界中看起来是这样，那么你就不应该试图“修复”类别不平衡。一个好的模型应该学会建模这种不平衡。然而，开发一个足够好的模型可能是具有挑战性的，因此我们仍然需要依赖特殊的训练技术。
- en: 'In this section, we will cover three approaches to handling class imbalance:
    choosing the right metrics for your problem; data-level methods, which means changing
    the data distribution to make it less imbalanced; and algorithm-level methods,
    which means changing your learning method to make it more robust to class imbalance.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍三种处理类别不平衡的方法：选择适合你问题的正确指标；数据级方法，即改变数据分布使其不那么不平衡；以及算法级方法，即改变学习方法使其更能抵御类别不平衡。
- en: These techniques might be necessary but not sufficient. For a comprehensive
    survey, I recommend [“Survey on Deep Learning with Class Imbalance”](https://oreil.ly/9QvBr)
    (Johnson and Khoshgoftaar 2019).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术可能是必要的，但不足以。为了进行全面的调查，我建议阅读 [“类别不平衡深度学习调查”](https://oreil.ly/9QvBr)（Johnson
    和 Khoshgoftaar，2019）。
- en: Using the right evaluation metrics
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用正确的评估指标
- en: The most important thing to do when facing a task with class imbalance is to
    choose the appropriate evaluation metrics. Wrong metrics will give you the wrong
    ideas of how your models are doing and, subsequently, won’t be able to help you
    develop or choose models good enough for your task.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 面对类别不平衡的任务，选择合适的评估指标是最重要的事情。错误的指标会给你错误的模型表现观念，进而无法帮助你开发或选择足够适合你任务的模型。
- en: The overall accuracy and error rate are the most frequently used metrics to
    report the performance of ML models. However, these are insufficient metrics for
    tasks with class imbalance because they treat all classes equally, which means
    the performance of your model on the majority class will dominate these metrics.
    This is especially bad when the majority class isn’t what you care about.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 总体准确率和误差率是报告机器学习模型性能最常用的指标。然而，对于类别不平衡的任务，这些指标不足以，因为它们同等对待所有类别，这意味着模型在多数类上的表现将主导这些指标。当多数类不是你关心的类时，这尤为糟糕。
- en: 'Consider a task with two labels: CANCER (the positive class) and NORMAL (the
    negative class), where 90% of the labeled data is NORMAL. Consider two models,
    A and B, with the confusion matrices shown in Tables [4-4](#model_aapostrophes_confusion_matrixsemi)
    and [4-5](#model_bapostrophes_confusion_matrixsemi).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个具有两个标签的任务：癌症（正类）和正常（负类），其中 90% 的标记数据是正常的。考虑两个模型，A 和 B，其混淆矩阵分别显示在表 [4-4](#model_aapostrophes_confusion_matrixsemi)
    和 [4-5](#model_bapostrophes_confusion_matrixsemi) 中。
- en: Table 4-4\. Model A’s confusion matrix; model A can detect 10 out of 100 CANCER
    cases
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-4\. 模型 A 的混淆矩阵；模型 A 能检测出 100 个癌症病例中的 10 个。
- en: '| Model A | Actual CANCER | Actual NORMAL |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 模型 A | 实际癌症 | 实际正常 |'
- en: '| --- | --- | --- |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Predicted CANCER | 10 | 10 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 预测癌症 | 10 | 10 |'
- en: '| Predicted NORMAL | 90 | 890 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 预测正常 | 90 | 890 |'
- en: Table 4-5\. Model B’s confusion matrix; model B can detect 90 out of 100 CANCER
    cases
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-5\. 模型 B 的混淆矩阵；模型 B 能检测出 100 个癌症病例中的 90 个。
- en: '| Model B | Actual CANCER | Actual NORMAL |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 模型 B | 实际癌症 | 实际正常 |'
- en: '| --- | --- | --- |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Predicted CANCER | 90 | 90 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 预测癌症 | 90 | 90 |'
- en: '| Predicted NORMAL | 10 | 810 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 预测正常 | 10 | 810 |'
- en: If you’re like most people, you’d probably prefer model B to make predictions
    for you since it has a better chance of telling you if you actually have cancer.
    However, they both have the same accuracy of 0.9.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你和大多数人一样，你可能更倾向于模型 B 来为你做预测，因为它有更好的可能性告诉你是否真的得了癌症。然而，它们的准确率都是 0.9。
- en: Metrics that help you understand your model’s performance with respect to specific
    classes would be better choices. Accuracy can still be a good metric if you use
    it for each class individually. The accuracy of model A on the CANCER class is
    10% and the accuracy of model B on the CANCER class is 90%.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 有助于了解模型在特定类别上表现的指标更为合适。如果你为每个类别单独使用准确率，那么准确率仍然是一个不错的指标。模型 A 在癌症类别上的准确率为 10%，模型
    B 在癌症类别上的准确率为 90%。
- en: F1, precision, and recall are metrics that measure your model’s performance
    with respect to the positive class in binary classification problems, as they
    rely on true positive—an outcome where the model correctly predicts the positive
    class.^([35](ch04.xhtml#ch01fn112))
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: F1 值、精确率和召回率是用于衡量二元分类问题中模型性能的指标，因为它们依赖于真正例——模型正确预测正类的情况。^([35](ch04.xhtml#ch01fn112))
- en: F1, precision, and recall are asymmetric metrics, which means that their values
    change depending on which class is considered the positive class. In our case,
    if we consider CANCER the positive class, model A’s F1 is 0.17\. However, if we
    consider NORMAL the positive class, model A’s F1 is 0.95\. Accuracy, precision,
    recall, and F1 scores of model A and model B when CANCER is the positive class
    are shown in [Table 4-7](#both_models_have_the_same_accuracy_even).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: F1 值、精确率和召回率是非对称的指标，这意味着它们的值取决于哪个类被视为正类。在我们的情况下，如果我们将癌症视为正类，模型 A 的 F1 值为 0.17。然而，如果我们将正常视为正类，模型
    A 的 F1 值为 0.95。当癌症被视为正类时，模型 A 和模型 B 的准确率、精确率、召回率和 F1 分数显示在 [表 4-7](#both_models_have_the_same_accuracy_even)
    中。
- en: Table 4-7\. Both models have the same accuracy even though one model is clearly
    superior
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-7\. 尽管一个模型明显优于另一个模型，但两个模型的准确度相同。
- en: '|  | CANCER (1) | NORMAL (0) | Accuracy | Precision | Recall | F1 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  | 癌症 (1) | 正常 (0) | 准确率 | 精确率 | 召回率 | F1 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Model A | 10/100 | 890/900 | 0.9 | 0.5 | 0.1 | 0.17 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 模型 A | 10/100 | 890/900 | 0.9 | 0.5 | 0.1 | 0.17 |'
- en: '| Model B | 90/100 | 810/900 | 0.9 | 0.5 | 0.9 | 0.64 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 模型 B | 90/100 | 810/900 | 0.9 | 0.5 | 0.9 | 0.64 |'
- en: Many classification problems can be modeled as regression problems. Your model
    can output a probability, and based on that probability, you classify the sample.
    For example, if the value is greater than 0.5, it’s a positive label, and if it’s
    less than or equal to 0.5, it’s a negative label. This means that you can tune
    the threshold to increase the *true positive rate* (also known as *recall*) while
    decreasing the *false positive rate* (also known as the *probability of false
    alarm*), and vice versa. We can plot the true positive rate against the false
    positive rate for different thresholds. This plot is known as the *ROC curve*
    (receiver operating characteristics). When your model is perfect, the recall is
    1.0, and the curve is just a line at the top. This curve shows you how your model’s
    performance changes depending on the threshold, and helps you choose the threshold
    that works best for you. The closer to the perfect line, the better your model’s
    performance.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 许多分类问题可以建模为回归问题。您的模型可以输出一个概率，基于该概率对样本进行分类。例如，如果值大于 0.5，则为正标签；如果小于或等于 0.5，则为负标签。这意味着您可以调整阈值以增加*真正阳性率*（也称为*召回率*），同时降低*假阳性率*（也称为*误报概率*），反之亦然。我们可以绘制不同阈值下的真正阳性率与假阳性率的曲线。这种绘图称为*ROC
    曲线*（接收者操作特征曲线）。当您的模型完美时，召回率为 1.0，曲线则位于顶部。这条曲线显示了模型性能如何随阈值变化而变化，并帮助您选择最适合您的阈值。曲线越接近完美直线，您的模型性能越好。
- en: The area under the curve (AUC) measures the area under the ROC curve. Since
    the closer to the perfect line the better, the larger this area the better, as
    shown in [Figure 4-9](#roc_curve).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 曲线下面积（AUC）衡量了ROC曲线下面的面积。由于曲线越接近完美直线越好，因此该面积越大越好，正如图[4-9](#roc_curve)所示。
- en: '![](Images/dmls_0409.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_0409.png)'
- en: Figure 4-9\. ROC curve
  id: totrans-223
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-9\. ROC 曲线
- en: Like F1 and recall, the ROC curve focuses only on the positive class and doesn’t
    show how well your model does on the negative class. Davis and Goadrich suggested
    that we should plot precision against recall instead, in what they termed the
    Precision-Recall Curve. They argued that this curve gives a more informative picture
    of an algorithm’s performance on tasks with heavy class imbalance.^([36](ch04.xhtml#ch01fn113))
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 与 F1 和召回率一样，ROC 曲线仅关注正类，并不显示模型在负类上的表现如何。戴维斯和戈德里奇建议我们应该绘制精确率与召回率的曲线，即他们称之为精确-召回曲线。他们认为，这条曲线更详细地展示了算法在类别不平衡任务上的性能^([36](ch04.xhtml#ch01fn113))。
- en: 'Data-level methods: Resampling'
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据级方法：重新取样
- en: Data-level methods modify the distribution of the training data to reduce the
    level of imbalance to make it easier for the model to learn. A common family of
    techniques is resampling. Resampling includes oversampling, adding more instances
    from the minority classes, and undersampling, removing instances of the majority
    classes. The simplest way to undersample is to randomly remove instances from
    the majority class, whereas the simplest way to oversample is to randomly make
    copies of the minority class until you have a ratio that you’re happy with. [Figure 4-10](#illustrations_of_how_undersampling_and)
    shows a visualization of oversampling and undersampling.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 数据级方法修改训练数据的分布，以降低不平衡程度，使模型更容易学习。一类常见的技术是重新取样。重新取样包括过采样，增加少数类的实例，和欠采样，减少多数类的实例。最简单的欠采样方法是从多数类随机删除实例，而最简单的过采样方法是随机复制少数类的实例，直到您满意为止。[图
    4-10](#illustrations_of_how_undersampling_and) 展示了过采样和欠采样的可视化。
- en: '![](Images/dmls_0410.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_0410.png)'
- en: 'Figure 4-10\. Illustrations of how undersampling and oversampling work. Source:
    Adapted from an image by Rafael Alencar^([37](ch04.xhtml#ch01fn114))'
  id: totrans-228
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-10\. 示范如何工作的欠采样和过采样。来源：根据 Rafael Alencar 的图像调整^([37](ch04.xhtml#ch01fn114))
- en: A popular method of undersampling low-dimensional data that was developed back
    in 1976 is Tomek links.^([38](ch04.xhtml#ch01fn115)) With this technique, you
    find pairs of samples from opposite classes that are close in proximity and remove
    the sample of the majority class in each pair.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 一种早在1976年开发的低维数据欠采样方法是Tomek链接。^([38](ch04.xhtml#ch01fn115)) 使用这种技术，您可以找到来自相对立类别的接近的样本对，并移除每对中大多数类的样本。
- en: While this makes the decision boundary more clear and arguably helps models
    learn the boundary better, it may make the model less robust because the model
    doesn’t get to learn from the subtleties of the true decision boundary.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这使得决策边界更清晰且有助于模型更好地学习边界，但可能会使模型更不稳健，因为模型无法从真实决策边界的微妙之处学习。
- en: A popular method of oversampling low-dimensional data is SMOTE (synthetic minority
    oversampling technique).^([39](ch04.xhtml#idm46868210052544)) It synthesizes novel
    samples of the minority class through sampling convex combinations of existing
    data points within the minority class.^([40](ch04.xhtml#ch01fn116))
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的低维数据过采样方法是SMOTE（合成少数类过采样技术）。^([39](ch04.xhtml#idm46868210052544)) 它通过在少数类内部现有数据点的凸组合抽样来合成新的样本。
- en: Both SMOTE and Tomek links have only been proven effective in low-dimensional
    data. Many of the sophisticated resampling techniques, such as Near-Miss and one-sided
    selection,^([41](ch04.xhtml#ch01fn117)) require calculating the distance between
    instances or between instances and the decision boundaries, which can be expensive
    or infeasible for high-dimensional data or in high-dimensional feature space,
    such as the case with large neural networks.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: SMOTE和Tomek链接仅在低维数据中被证明有效。许多复杂的重新采样技术，如Near-Miss和单边选择，^([41](ch04.xhtml#ch01fn117))
    需要计算实例之间或实例与决策边界之间的距离，这在高维数据或具有大型神经网络等高维特征空间中可能是昂贵或不可行的。
- en: When you resample your training data, never evaluate your model on resampled
    data, since it will cause your model to overfit to that resampled distribution.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 当您对训练数据重新采样时，请勿在重新采样数据上评估模型，因为这会导致模型过度拟合到重新采样的分布上。
- en: Undersampling runs the risk of losing important data from removing data. Oversampling
    runs the risk of overfitting on training data, especially if the added copies
    of the minority class are replicas of existing data. Many sophisticated sampling
    techniques have been developed to mitigate these risks.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 欠采样存在移除数据会丢失重要数据的风险。过采样则存在过拟合训练数据的风险，特别是如果少数类的添加副本是现有数据的复制品。已开发了许多复杂的抽样技术来减轻这些风险。
- en: One such technique is two-phase learning.^([42](ch04.xhtml#ch01fn118)) You first
    train your model on the resampled data. This resampled data can be achieved by
    randomly undersampling large classes until each class has only *N* instances.
    You then fine-tune your model on the original data.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一种技术是两阶段学习。^([42](ch04.xhtml#ch01fn118)) 首先在重新采样的数据上训练模型。这些重新采样的数据可以通过随机欠采样大类直到每个类只有*N*个实例来实现。然后在原始数据上对模型进行微调。
- en: 'Another technique is dynamic sampling: oversample the low-performing classes
    and undersample the high-performing classes during the training process. Introduced
    by Pouyanfar et al.,^([43](ch04.xhtml#ch01fn119)) the method aims to show the
    model less of what it has already learned and more of what it has not.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种技术是动态抽样：在训练过程中对低效能类进行过采样，对高效能类进行欠采样。由Pouyanfar等人引入，^([43](ch04.xhtml#ch01fn119))
    该方法旨在向模型展示更多尚未学习的内容，而非已经学习过的内容。
- en: Algorithm-level methods
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 算法级方法
- en: If data-level methods mitigate the challenge of class imbalance by altering
    the distribution of your training data, algorithm-level methods keep the training
    data distribution intact but alter the algorithm to make it more robust to class
    imbalance.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据级方法通过改变训练数据的分布来缓解类别不平衡的挑战，那么算法级方法则保持训练数据分布不变，但改变算法以使其更能抵御类别不平衡。
- en: Because the loss function (or the cost function) guides the learning process,
    many algorithm-level methods involve adjustment to the loss function. The key
    idea is that if there are two instances, *x*[1] and *x*[2], and the loss resulting
    from making the wrong prediction on *x*[1] is higher than *x*[2], the model will
    prioritize making the correct prediction on *x*[1] over making the correct prediction
    on *x*[2]. By giving the training instances we care about higher weight, we can
    make the model focus more on learning these instances.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 因为损失函数（或成本函数）指导了学习过程，许多算法级别的方法涉及调整损失函数。关键思想是，如果有两个实例*x*[1]和*x*[2]，并且对*x*[1]做出错误预测造成的损失比对*x*[2]做出错误预测造成的损失更高，那么模型将优先考虑对*x*[1]做出正确预测而不是对*x*[2]做出正确预测。通过给我们关心的训练实例更高的权重，我们可以使模型更专注于学习这些实例。
- en: Let <math alttext="upper L left-parenthesis x semicolon theta right-parenthesis"><mrow><mi>L</mi>
    <mo>(</mo> <mi>x</mi> <mo>;</mo> <mi>θ</mi> <mo>)</mo></mrow></math> be the loss
    caused by the instance *x* for the model with the parameter set <math alttext="theta"><mi>θ</mi></math>
    . The model’s loss is often defined as the average loss caused by all instances.
    *N* denotes the total number of training samples.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 让<math alttext="upper L left-parenthesis x semicolon theta right-parenthesis"><mrow><mi>L</mi>
    <mo>(</mo> <mi>x</mi> <mo>;</mo> <mi>θ</mi> <mo>)</mo></mrow></math>表示模型参数设置为<math
    alttext="theta"><mi>θ</mi></math>时实例*x*引起的损失。模型的损失通常被定义为所有实例引起的平均损失。*N*表示训练样本的总数。
- en: <math alttext="upper L left-parenthesis upper X semicolon theta right-parenthesis
    equals sigma-summation Underscript x Endscripts StartFraction 1 Over upper N EndFraction
    upper L left-parenthesis x semicolon theta right-parenthesis"><mrow><mi>L</mi>
    <mrow><mo>(</mo> <mi>X</mi> <mo>;</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>=</mo>
    <msub><mo>∑</mo> <mi>x</mi></msub> <mfrac><mn>1</mn> <mi>N</mi></mfrac> <mi>L</mi>
    <mrow><mo>(</mo> <mi>x</mi> <mo>;</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper L left-parenthesis upper X semicolon theta right-parenthesis
    equals sigma-summation Underscript x Endscripts StartFraction 1 Over upper N EndFraction
    upper L left-parenthesis x semicolon theta right-parenthesis"><mrow><mi>L</mi>
    <mrow><mo>(</mo> <mi>X</mi> <mo>;</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>=</mo>
    <msub><mo>∑</mo> <mi>x</mi></msub> <mfrac><mn>1</mn> <mi>N</mi></mfrac> <mi>L</mi>
    <mrow><mo>(</mo> <mi>x</mi> <mo>;</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math>
- en: This loss function values the loss caused by all instances equally, even though
    wrong predictions on some instances might be much costlier than wrong predictions
    on other instances. There are many ways to modify this cost function. In this
    section, we will focus on three of them, starting with cost-sensitive learning.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这个损失函数平等看待所有实例引起的损失，即使某些实例上的错误预测可能比其他实例上的错误预测要昂贵得多。有许多方法可以修改这个成本函数。在本节中，我们将专注于其中三种，首先是成本敏感学习。
- en: Cost-sensitive learning
  id: totrans-243
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 成本敏感学习
- en: 'Back in 2001, based on the insight that misclassification of different classes
    incurs different costs, Elkan proposed cost-sensitive learning in which the individual
    loss function is modified to take into account this varying cost.^([44](ch04.xhtml#ch01fn120))
    The method started by using a cost matrix to specify *C[ij]*: the cost if class
    *i* is classified as class *j*. If *i* = *j*, it’s a correct classification, and
    the cost is usually 0\. If not, it’s a misclassification. If classifying POSITIVE
    examples as NEGATIVE is twice as costly as the other way around, you can make
    *C*[10] twice as high as *C*[01].'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 早在2001年，基于不同类别的误分类造成不同成本的洞察力，Elkan提出了成本敏感学习，其中个体损失函数被修改以考虑这种不同的成本。^([44](ch04.xhtml#ch01fn120))
    该方法从使用成本矩阵开始，以指定*C[ij]*：如果将类*i*分类为类*j*的成本。如果*i* = *j*，那么是正确分类，成本通常为0。否则，是误分类。如果将正例分类为负例的成本是反之的两倍，那么可以将*C*[10]设置为*C*[01]的两倍。
- en: For example, if you have two classes, POSITIVE and NEGATIVE, the cost matrix
    can look like that in [Table 4-8](#example_of_a_cost_matrix).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果有两类，正例和负例，则成本矩阵可以看起来像表 4-8\. 成本矩阵示例中那样。
- en: Table 4-8\. Example of a cost matrix
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-8\. 成本矩阵示例
- en: '|  | Actual NEGATIVE | Actual POSITIVE |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|  | 实际为负例 | 实际为正例 |'
- en: '| --- | --- | --- |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Predicted NEGATIVE | *C*(0, 0) = *C*[00] | *C*(1, 0) = *C*[10] |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 预测为负例 | *C*(0, 0) = *C*[00] | *C*(1, 0) = *C*[10] |'
- en: '| Predicted POSITIVE | *C*(0, 1) = *C*[01] | *C*(1, 1) = *C*[11] |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 预测为正例 | *C*(0, 1) = *C*[01] | *C*(1, 1) = *C*[11] |'
- en: The loss caused by instance *x* of class *i* will become the weighted average
    of all possible classifications of instance *x*.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 类别*i*的实例*x*引起的损失将成为实例*x*的所有可能分类的加权平均值。
- en: <math alttext="upper L left-parenthesis x semicolon theta right-parenthesis
    equals sigma-summation Underscript j Endscripts upper C Subscript i j Baseline
    upper P left-parenthesis j vertical-bar x semicolon theta right-parenthesis"><mrow><mi>L</mi>
    <mrow><mo>(</mo> <mi>x</mi> <mo>;</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>=</mo>
    <msub><mo>∑</mo> <mi>j</mi></msub> <msub><mi>C</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mi>P</mi> <mrow><mo>(</mo> <mi>j</mi> <mo>|</mo> <mi>x</mi> <mo>;</mo> <mi>θ</mi>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper L left-parenthesis x semicolon theta right-parenthesis
    equals sigma-summation Underscript j Endscripts upper C Subscript i j Baseline
    upper P left-parenthesis j vertical-bar x semicolon theta right-parenthesis"><mrow><mi>L</mi>
    <mrow><mo>(</mo> <mi>x</mi> <mo>;</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>=</mo>
    <msub><mo>∑</mo> <mi>j</mi></msub> <msub><mi>C</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mi>P</mi> <mrow><mo>(</mo> <mi>j</mi> <mo>|</mo> <mi>x</mi> <mo>;</mo> <mi>θ</mi>
    <mo>)</mo></mrow></mrow></math>
- en: The problem with this loss function is that you have to manually define the
    cost matrix, which is different for different tasks at different scales.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这个损失函数的问题在于，你必须手动定义成本矩阵，而这个矩阵在不同任务和不同尺度下是不同的。
- en: Class-balanced loss
  id: totrans-254
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 类平衡损失
- en: What might happen with a model trained on an imbalanced dataset is that it’ll
    bias toward majority classes and make wrong predictions on minority classes. What
    if we punish the model for making wrong predictions on minority classes to correct
    this bias?
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型在不平衡数据集上训练时，可能会偏向主要类别，并在少数类别上做出错误预测。如果我们惩罚模型对少数类别做出错误预测以纠正这种偏差，可能会发生什么？
- en: 'In its vanilla form, we can make the weight of each class inversely proportional
    to the number of samples in that class, so that the rarer classes have higher
    weights. In the following equation, *N* denotes the total number of training samples:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在其原始形式中，我们可以使每个类别的权重与该类别中的样本数成反比，以便稀有类别具有较高的权重。在下面的方程中，*N* 表示训练样本的总数：
- en: <math alttext="upper W Subscript i Baseline equals StartFraction upper N Over
    number of samples of class reverse-solidus emph left-brace i right-brace EndFraction"><mrow><msub><mi>W</mi>
    <mi>i</mi></msub> <mo>=</mo> <mfrac><mi>N</mi> <mrow><mtext>number</mtext><mtext>of</mtext><mtext>samples</mtext><mtext>of</mtext><mtext>class</mtext><mtext>i</mtext></mrow></mfrac></mrow></math>
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper W Subscript i Baseline equals StartFraction upper N Over
    number of samples of class reverse-solidus emph left-brace i right-brace EndFraction"><mrow><msub><mi>W</mi>
    <mi>i</mi></msub> <mo>=</mo> <mfrac><mi>N</mi> <mrow><mtext>number</mtext><mtext>of</mtext><mtext>samples</mtext><mtext>of</mtext><mtext>class</mtext><mtext>i</mtext></mrow></mfrac></mrow></math>
- en: The loss caused by instance *x* of class *i* will become as follows, with Loss(*x*,
    *j*) being the loss when *x* is classified as class *j*. It can be cross entropy
    or any other loss function.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 类别 *i* 的实例 *x* 导致的损失如下，其中 Loss(*x*, *j*) 是当 *x* 被分类为类别 *j* 时的损失。可以是交叉熵或任何其他损失函数。
- en: <math alttext="upper L left-parenthesis x semicolon theta right-parenthesis
    equals upper W Subscript i Baseline sigma-summation Underscript j Endscripts upper
    P left-parenthesis j vertical-bar x semicolon theta right-parenthesis Loss left-parenthesis
    x comma j right-parenthesis"><mrow><mi>L</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>;</mo>
    <mi>θ</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>W</mi> <mi>i</mi></msub> <msub><mo>∑</mo>
    <mi>j</mi></msub> <mi>P</mi> <mrow><mo>(</mo> <mi>j</mi> <mo>|</mo> <mi>x</mi>
    <mo>;</mo> <mi>θ</mi> <mo>)</mo></mrow> <mi>Loss</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>,</mo> <mi>j</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper L left-parenthesis x semicolon theta right-parenthesis
    equals upper W Subscript i Baseline sigma-summation Underscript j Endscripts upper
    P left-parenthesis j vertical-bar x semicolon theta right-parenthesis Loss left-parenthesis
    x comma j right-parenthesis"><mrow><mi>L</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>;</mo>
    <mi>θ</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>W</mi> <mi>i</mi></msub> <msub><mo>∑</mo>
    <mi>j</mi></msub> <mi>P</mi> <mrow><mo>(</mo> <mi>j</mi> <mo>|</mo> <mi>x</mi>
    <mo>;</mo> <mi>θ</mi> <mo>)</mo></mrow> <mi>Loss</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>,</mo> <mi>j</mi> <mo>)</mo></mrow></mrow></math>
- en: A more sophisticated version of this loss can take into account the overlap
    among existing samples, such as class-balanced loss based on effective number
    of samples.^([45](ch04.xhtml#custom_ch04fn5))
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这种损失的更复杂版本可以考虑现有样本之间的重叠，例如基于有效样本数量的类平衡损失。^([45](ch04.xhtml#custom_ch04fn5))
- en: Focal loss
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 焦点损失
- en: In our data, some examples are easier to classify than others, and our model
    might learn to classify them quickly. We want to incentivize our model to focus
    on learning the samples it still has difficulty classifying. What if we adjust
    the loss so that if a sample has a lower probability of being right, it’ll have
    a higher weight? This is exactly what focal loss does.^([46](ch04.xhtml#ch01fn121))
    The equation for focal loss and its performance compared to cross entropy loss
    is shown in [Figure 4-11](#the_model_trained_with_focal_loss_left).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据中，某些示例比其他示例更容易分类，我们的模型可能会快速学习它们的分类。我们希望激励我们的模型专注于学习那些仍然难以分类的样本。如果我们调整损失，使得样本被正确分类的概率较低，则其权重将更高。这正是焦点损失所做的。^([46](ch04.xhtml#ch01fn121))
    焦点损失的方程及其与交叉熵损失的性能如图[4-11](#the_model_trained_with_focal_loss_left)所示。
- en: In practice, ensembles have shown to help with the class imbalance problem.^([47](ch04.xhtml#ch01fn122))
    However, we don’t include ensembling in this section because class imbalance isn’t
    usually why ensembles are used. Ensemble techniques will be covered in [Chapter 6](ch06.xhtml#model_development_and_offline_evaluatio).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，集成已经显示对应类别不平衡问题有所帮助。^([47](ch04.xhtml#ch01fn122)) 然而，我们在本节中不包括集成，因为通常不是因为类别不平衡而使用集成。集成技术将在[第六章](ch06.xhtml#model_development_and_offline_evaluatio)中讨论。
- en: '![](Images/dmls_0411.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_0411.png)'
- en: 'Figure 4-11\. The model trained with focal loss (FL) shows reduced loss values
    compared to the model trained with cross entropy loss (CE). Source: Adapted from
    an image by Lin et al.'
  id: totrans-265
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-11\. 使用焦点损失（FL）训练的模型显示比使用交叉熵损失（CE）训练的模型具有更低的损失值。来源：改编自林等人的一幅图像。
- en: Data Augmentation
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据增强
- en: Data augmentation is a family of techniques that are used to increase the amount
    of training data. Traditionally, these techniques are used for tasks that have
    limited training data, such as in medical imaging. However, in the last few years,
    they have shown to be useful even when we have a lot of data—augmented data can
    make our models more robust to noise and even adversarial attacks.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强是一系列用于增加训练数据量的技术。传统上，这些技术用于具有有限训练数据的任务，例如在医学成像中。然而，在过去几年中，它们已经显示出即使在有大量数据时也很有用——增强数据可以使我们的模型对噪声甚至对抗性攻击更加健壮。
- en: 'Data augmentation has become a standard step in many computer vision tasks
    and is finding its way into natural language processing (NLP) tasks. The techniques
    depend heavily on the data format, as image manipulation is different from text
    manipulation. In this section, we will cover three main types of data augmentation:
    simple label-preserving transformations; perturbation, which is a term for “adding
    noises”; and data synthesis. In each type, we’ll go over examples for both computer
    vision and NLP.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强已经成为许多计算机视觉任务中的标准步骤，并正在逐步应用于自然语言处理（NLP）任务中。这些技术严重依赖于数据格式，因为图像处理与文本处理有所不同。在本节中，我们将涵盖三种主要类型的数据增强：简单的保持标签的转换；扰动，即“添加噪声”的术语；以及数据合成。在每种类型中，我们将讨论计算机视觉和NLP的示例。
- en: Simple Label-Preserving Transformations
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单的保持标签的转换
- en: In computer vision, the simplest data augmentation technique is to randomly
    modify an image while preserving its label. You can modify the image by cropping,
    flipping, rotating, inverting (horizontally or vertically), erasing part of the
    image, and more. This makes sense because a rotated image of a dog is still a
    dog. Common ML frameworks like PyTorch, TensorFlow, and Keras all have support
    for image augmentation. According to Krizhevsky et al. in their legendary AlexNet
    paper, “The transformed images are generated in Python code on the CPU while the
    GPU is training on the previous batch of images. So these data augmentation schemes
    are, in effect, computationally free.”^([48](ch04.xhtml#ch01fn123))
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉中，最简单的数据增强技术是在保持其标签的同时随机修改图像。你可以通过裁剪、翻转、旋转、反转（水平或垂直）、擦除图像的一部分等方式修改图像。这是有道理的，因为一只狗的旋转图像仍然是一只狗。像
    PyTorch、TensorFlow 和 Keras 这样的常见机器学习框架都支持图像增强。根据Krizhevsky等人在其著名的AlexNet论文中所述，“这些转换后的图像是在CPU上的Python代码生成的，而GPU则在前一批图像上进行训练。因此，这些数据增强方案实际上是计算上免费的。”^([48](ch04.xhtml#ch01fn123))
- en: In NLP, you can randomly replace a word with a similar word, assuming that this
    replacement wouldn’t change the meaning or the sentiment of the sentence, as shown
    in [Table 4-9](#three_sentences_generated_from_an_origi). Similar words can be
    found either with a dictionary of synonymous words or by finding words whose embeddings
    are close to each other in a word embedding space.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理中，你可以随机用一个相似的词替换一个词，假设这种替换不会改变句子的含义或情感，如在[表 4-9](#three_sentences_generated_from_an_origi)中所示。相似的词可以通过同义词词典找到，也可以通过在词嵌入空间中找到距离接近的词来找到。
- en: Table 4-9\. Three sentences generated from an original sentence
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-9\. 从原始句子生成的三个句子
- en: '| Original sentence | I’m so happy to see you. |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 原始句子 | 我很高兴见到你。 |'
- en: '| Generated sentences | I’m so *glad* to see you. I’m so happy to see *y’all*.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '| 生成的句子 | 我很*高兴*见到你。我很高兴见到*你们*。'
- en: I’m *very* happy to see you. |
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我*非常*高兴见到你。 |
- en: This type of data augmentation is a quick way to double or triple your training
    data.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数据增强技术是快速增加训练数据量的一种方法。
- en: Perturbation
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扰动
- en: Perturbation is also a label-preserving operation, but because sometimes it’s
    used to trick models into making wrong predictions, I thought it deserves its
    own section.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 扰动也是一种保持标签的操作，但由于有时它被用来欺骗模型做出错误预测，我认为它值得有自己的章节。
- en: Neural networks, in general, are sensitive to noise. In the case of computer
    vision, this means that adding a small amount of noise to an image can cause a
    neural network to misclassify it. Su et al. showed that 67.97% of the natural
    images in the Kaggle CIFAR-10 test dataset and 16.04% of the ImageNet test images
    can be misclassified by changing just one pixel (see [Figure 4-12](#changing_one_pixel_can_cause_a_neural_n)).^([49](ch04.xhtml#ch01fn124))
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，神经网络对噪声很敏感。在计算机视觉中，这意味着向图像添加少量噪声可能导致神经网络错误分类。Su 等人表明，Kaggle CIFAR-10 测试数据集中的自然图像中有
    67.97% 和 ImageNet 测试图像中的 16.04%，仅通过改变一个像素就可能被错误分类（见[图 4-12](#changing_one_pixel_can_cause_a_neural_n)）。^([49](ch04.xhtml#ch01fn124))
- en: '![](Images/dmls_0412.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_0412.png)'
- en: 'Figure 4-12\. Changing one pixel can cause a neural network to make wrong predictions.
    The three models used are AllConv, NiN, and VGG. The original labels made by those
    models are above the labels made after one pixel was changed. Source: Su et al.^([50](ch04.xhtml#ch01fn129))'
  id: totrans-281
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-12\. 改变一个像素可能导致神经网络做出错误预测。所使用的三个模型是 AllConv、NiN 和 VGG。在改变一个像素后，这些模型生成的原始标签显示在改变后的标签上方。来源：Su
    等人。^([50](ch04.xhtml#ch01fn129))
- en: Using deceptive data to trick a neural network into making wrong predictions
    is called adversarial attacks. Adding noise to samples is a common technique to
    create adversarial samples. The success of adversarial attacks is especially exaggerated
    as the resolution of images increases.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 使用欺骗性数据来欺骗神经网络以做出错误预测称为对抗攻击。向样本添加噪声是创建对抗性样本的常见技术。随着图像分辨率的增加，对抗攻击的成功尤为突出。
- en: Adding noisy samples to training data can help models recognize the weak spots
    in their learned decision boundary and improve their performance.^([51](ch04.xhtml#ch01fn125))
    Noisy samples can be created by either adding random noise or by a search strategy.
    Moosavi-Dezfooli et al. proposed an algorithm, called DeepFool, that finds the
    minimum possible noise injection needed to cause a misclassification with high
    confidence.^([52](ch04.xhtml#ch01fn126)) This type of augmentation is called adversarial
    augmentation.^([53](ch04.xhtml#ch01fn127))
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 向训练数据添加噪声样本可以帮助模型识别其学习决策边界的薄弱点并提高其性能。^([51](ch04.xhtml#ch01fn125)) 噪声样本可以通过添加随机噪声或搜索策略来创建。Moosavi-Dezfooli等人提出了一种算法，称为DeepFool，它找到了引入最小可能噪声注入以高置信度导致误分类的方法。^([52](ch04.xhtml#ch01fn126))
    这种增强方式被称为对抗性增强。^([53](ch04.xhtml#ch01fn127))
- en: Adversarial augmentation is less common in NLP (an image of a bear with randomly
    added pixels still looks like a bear, but adding random characters to a random
    sentence will likely render it gibberish), but perturbation has been used to make
    models more robust. One of the most notable examples is BERT, where the model
    chooses 15% of all tokens in each sequence at random and chooses to replace 10%
    of the chosen tokens with random words. For example, given the sentence “My dog
    is hairy,” and the model randomly replacing “hairy” with “apple,” the sentence
    becomes “My dog is apple.” So 1.5% of all tokens might result in nonsensical meaning.
    Their ablation studies show that a small fraction of random replacement gives
    their model a small performance boost.^([54](ch04.xhtml#ch01fn128))
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理中，对抗性增强不常见（将一张带有随机添加像素的熊的图像看起来仍然像熊，但将随机字符添加到随机句子中可能会使其变得无意义），但扰动已被用来使模型更加稳健。其中最显著的例子之一是BERT，在该模型中，模型随机选择每个序列中的15%的所有标记，并选择用随机单词替换所选标记的10%。例如，给定句子“My
    dog is hairy”，并且模型随机将“hairy”替换为“apple”，则句子变为“My dog is apple”。因此，1.5%的所有标记可能导致无意义的含义。他们的消融研究表明，小部分随机替换可以给他们的模型带来轻微的性能提升。^([54](ch04.xhtml#ch01fn128))
- en: In [Chapter 6](ch06.xhtml#model_development_and_offline_evaluatio), we’ll go
    over how to use perturbation not just as a way to improve your model’s performance,
    but also as a way to evaluate its performance.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](ch06.xhtml#model_development_and_offline_evaluatio)中，我们将讨论如何使用扰动不仅作为提高模型性能的一种方法，还作为评估其性能的一种方法。
- en: Data Synthesis
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据合成
- en: Since collecting data is expensive and slow, with many potential privacy concerns,
    it’d be a dream if we could sidestep it altogether and train our models with synthesized
    data. Even though we’re still far from being able to synthesize all training data,
    it’s possible to synthesize some training data to boost a model’s performance.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 由于收集数据既昂贵又缓慢，并且存在许多潜在的隐私问题，如果我们能够完全避开它并使用合成数据来训练我们的模型，那将是一种梦想。尽管我们离能够合成所有训练数据还有很长的路要走，但我们确实可以合成一些训练数据来提升模型的性能。
- en: 'In NLP, templates can be a cheap way to bootstrap your model. One team I worked
    with used templates to bootstrap training data for their conversational AI (chatbot).
    A template might look like: “Find me a [CUISINE] restaurant within [NUMBER] miles
    of [LOCATION]” (see [Table 4-10](#three_sentences_generated_from_a_templa)). With
    lists of all possible cuisines, reasonable numbers (you would probably never want
    to search for restaurants beyond 1,000 miles), and locations (home, office, landmarks,
    exact addresses) for each city, you can generate thousands of training queries
    from a template.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理中，模板可以是启动模型的廉价方式。我曾与一个团队合作，他们使用模板为他们的对话型AI（聊天机器人）启动训练数据。模板可能如下所示：“找一个[CUISINE]餐厅，在[LOCATION]附近[NUMBER]英里以内”（见[表4-10](#three_sentences_generated_from_a_templa)）。对于每个城市的所有可能的美食类型、合理的距离（你可能永远不想搜索超过1000英里的餐厅）、以及位置（家、办公室、地标、确切地址），您可以从模板生成数千个训练查询。
- en: Table 4-10\. Three sentences generated from a template
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 表4-10。从模板生成的三个句子
- en: '| Template | Find me a [CUISINE] restaurant within [NUMBER] miles of [LOCATION].
    |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 模板 | 找一个距离[LOCATION] [NUMBER]英里以内的[CUISINE]餐厅。 |'
- en: '| Generated queries | Find me a *Vietnamese* restaurant within *2* miles of
    *my office*. Find me a *Thai* restaurant within *5* miles of *my home*.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '| 生成的查询 | 找一个*越南*餐厅，在*我的办公室*附近*2*英里以内。找一个*泰国*餐厅，在*我的家*附近*5*英里以内。'
- en: Find me a *Mexican* restaurant within *3* miles of *Google headquarters*. |
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 找一个*墨西哥*餐厅，在*Google总部*附近*3*英里以内。
- en: 'In computer vision, a straightforward way to synthesize new data is to combine
    existing examples with discrete labels to generate continuous labels. Consider
    a task of classifying images with two possible labels: DOG (encoded as 0) and
    CAT (encoded as 1). From example *x*[1] of label DOG and example *x*[2] of label
    CAT, you can generate *x*'' such as:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉中，合成新数据的一种简单方法是将具有离散标签的现有示例与生成连续标签相结合。考虑一个分类猫和狗图像的任务，其标签有两种可能性：狗（编码为0）和猫（编码为1）。从标签为狗的示例*x*[1]和标签为猫的示例*x*[2]，你可以生成*x*'，如下：
- en: <math alttext="x prime equals gamma x 1 plus left-parenthesis 1 minus gamma
    right-parenthesis x 2"><mrow><msup><mrow><mi>x</mi></mrow> <mo>'</mo></msup> <mo>=</mo>
    <mi>γ</mi> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn>
    <mo>-</mo> <mi>γ</mi> <mo>)</mo></mrow> <msub><mi>x</mi> <mn>2</mn></msub></mrow></math>
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="x prime equals gamma x 1 plus left-parenthesis 1 minus gamma
    right-parenthesis x 2"><mrow><msup><mrow><mi>x</mi></mrow> <mo>'</mo></msup> <mo>=</mo>
    <mi>γ</mi> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn>
    <mo>-</mo> <mi>γ</mi> <mo>)</mo></mrow> <msub><mi>x</mi> <mn>2</mn></msub></mrow></math>
- en: 'The label of *x*'' is a combination of the labels of *x*[1] and *x*[2]: <math
    alttext="gamma times 0 plus left-parenthesis 1 minus gamma right-parenthesis times
    1"><mrow><mi>γ</mi> <mo>×</mo> <mn>0</mn> <mo>+</mo> <mo>(</mo> <mn>1</mn> <mo>-</mo>
    <mi>γ</mi> <mo>)</mo> <mo>×</mo> <mn>1</mn></mrow></math> . This method is called
    mixup. The authors showed that mixup improves models’ generalization, reduces
    their memorization of corrupt labels, increases their robustness to adversarial
    examples, and stabilizes the training of generative adversarial networks.^([55](ch04.xhtml#ch01fn130))'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '*x*的标签是*x*[1]和*x*[2]的标签的组合：<math alttext="gamma times 0 plus left-parenthesis
    1 minus gamma right-parenthesis times 1"><mrow><mi>γ</mi> <mo>×</mo> <mn>0</mn>
    <mo>+</mo> <mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>γ</mi> <mo>)</mo> <mo>×</mo> <mn>1</mn></mrow></math>
    。这种方法称为mixup。作者表明，mixup可以提高模型的泛化能力，减少其对错误标签的记忆，增强其对对抗性示例的鲁棒性，并稳定生成对抗网络的训练。^([55](ch04.xhtml#ch01fn130))'
- en: Using neural networks to synthesize training data is an exciting approach that
    is actively being researched but not yet popular in production. Sandfort et al.
    showed that by adding images generated using CycleGAN to their original training
    data, they were able to improve their model’s performance significantly on computed
    tomography (CT) segmentation tasks.^([56](ch04.xhtml#ch01fn131))
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 使用神经网络合成训练数据是一种令人兴奋的方法，目前正在积极研究中，但在生产中还不太流行。Sandfort等人显示，通过将使用CycleGAN生成的图像添加到其原始训练数据中，他们能够显著改善模型在计算机断层扫描（CT）分割任务中的性能。^([56](ch04.xhtml#ch01fn131))
- en: If you’re interested in learning more about data augmentation for computer vision,
    [“A Survey on Image Data Augmentation for Deep Learning”](https://oreil.ly/3TUpK)
    (Shorten and Khoshgoftaar 2019) is a comprehensive review.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对深度学习中的图像数据增强更感兴趣，可以阅读《“深度学习图像数据增强综述”》（Shorten和Khoshgoftaar，2019），这是一篇全面的综述。
- en: Summary
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Training data still forms the foundation of modern ML algorithms. No matter
    how clever your algorithms might be, if your training data is bad, your algorithms
    won’t be able to perform well. It’s worth it to invest time and effort to curate
    and create training data that will enable your algorithms to learn something meaningful.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据仍然是现代机器学习算法的基础。无论你的算法有多聪明，如果你的训练数据不好，你的算法就无法表现良好。投入时间和精力来筛选和创建训练数据是值得的，这样可以让你的算法学到有意义的东西。
- en: In this chapter, we’ve discussed the multiple steps to create training data.
    We first covered different sampling methods, both nonprobability sampling and
    random sampling, that can help us sample the right data for our problem.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了创建训练数据的多个步骤。我们首先涵盖了不同的抽样方法，包括非概率抽样和随机抽样，这些方法可以帮助我们为我们的问题选择合适的数据。
- en: Most ML algorithms in use today are supervised ML algorithms, so obtaining labels
    is an integral part of creating training data. Many tasks, such as delivery time
    estimation or recommender systems, have natural labels. Natural labels are usually
    delayed, and the time it takes from when a prediction is served until when the
    feedback on it is provided is the feedback loop length. Tasks with natural labels
    are fairly common in the industry, which might mean that companies prefer to start
    on tasks that have natural labels over tasks without natural labels.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 今天使用的大多数机器学习算法都是监督学习算法，因此获取标签是创建训练数据的一个重要部分。许多任务，如交货时间估计或推荐系统，都有自然的标签。自然标签通常是延迟的，从服务预测到提供反馈的时间是反馈循环的长度。在行业中，具有自然标签的任务相当普遍，这可能意味着公司更倾向于从具有自然标签的任务开始，而不是从没有自然标签的任务开始。
- en: For tasks that don’t have natural labels, companies tend to rely on human annotators
    to annotate their data. However, hand labeling comes with many drawbacks. For
    example, hand labels can be expensive and slow. To combat the lack of hand labels,
    we discussed alternatives including weak supervision, semi-supervision, transfer
    learning, and active learning.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 对于没有自然标签的任务，公司往往依赖人工注释员来标注他们的数据。然而，手动标注具有许多缺点。例如，手动标注可能既昂贵又缓慢。为了解决手动标注的缺乏，我们讨论了包括弱监督、半监督、迁移学习和主动学习在内的替代方法。
- en: ML algorithms work well in situations when the data distribution is more balanced,
    and not so well when the classes are heavily imbalanced. Unfortunately, problems
    with class imbalance are the norm in the real world. In the following section,
    we discussed why class imbalance made it hard for ML algorithms to learn. We also
    discussed different techniques to handle class imbalance, from choosing the right
    metrics to resampling data to modifying the loss function to encourage the model
    to pay attention to certain samples.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据分布较为平衡时，机器学习算法表现良好，但在类别严重不平衡时表现不佳。不幸的是，在现实世界中，类别不平衡的问题是常态。在接下来的章节中，我们讨论了为何类别不平衡使得机器学习算法难以学习的原因。我们还讨论了处理类别不平衡的不同技术，从选择正确的度量标准到对数据进行重新采样，再到修改损失函数以鼓励模型关注特定样本。
- en: We ended the chapter with a discussion on data augmentation techniques that
    can be used to improve a model’s performance and generalization for both computer
    vision and NLP tasks.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章结束时讨论了数据增强技术，这些技术可以用来提高模型在计算机视觉和自然语言处理任务中的性能和泛化能力。
- en: Once you have your training data, you will want to extract features from it
    to train your ML models, which we will cover in the next chapter.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您有了训练数据，您将希望从中提取特征以训练您的机器学习模型，这将在下一章中讨论。
- en: ^([1](ch04.xhtml#ch01fn82-marker)) Some readers might argue that this approach
    might not work with large models, as certain large models don’t work for small
    datasets but work well with a lot more data. In this case, it’s still important
    to experiment with datasets of different sizes to figure out the effect of the
    dataset size on your model.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch04.xhtml#ch01fn82-marker)) 一些读者可能会认为这种方法在大型模型上可能不适用，因为某些大型模型在小数据集上表现不佳，但在更多数据的情况下表现良好。在这种情况下，尝试不同大小的数据集以了解数据集大小对模型的影响仍然很重要。
- en: '^([2](ch04.xhtml#ch01fn83-marker)) James J. Heckman, “Sample Selection Bias
    as a Specification Error,” *Econometrica* 47, no. 1 (January 1979): 153–61, [*https://oreil.ly/I5AhM*](https://oreil.ly/I5AhM).'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch04.xhtml#ch01fn83-marker)) James J. Heckman，《样本选择偏误作为规范误差》，《计量经济学》47
    卷，第 1 期（1979 年 1 月）：153–61，[*https://oreil.ly/I5AhM*](https://oreil.ly/I5AhM)。
- en: ^([3](ch04.xhtml#ch01fn84-marker)) Rachel Lerman, “Google Is Testing Its Self-Driving
    Car in Kirkland,” *Seattle Times*, February 3, 2016, [*https://oreil.ly/3IA1V*](https://oreil.ly/3IA1V).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch04.xhtml#ch01fn84-marker)) Rachel Lerman，《Google 在柯克兰测试其自动驾驶汽车》，《西雅图时报》，2016
    年 2 月 3 日，[*https://oreil.ly/3IA1V*](https://oreil.ly/3IA1V)。
- en: ^([4](ch04.xhtml#ch01fn85-marker)) Population here refers to a [“statistical
    population”](https://oreil.ly/w7GDX), a (potentially infinite) set of all possible
    samples that can be sampled.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch04.xhtml#ch01fn85-marker)) 这里的“人群”指的是 [“统计人群”](https://oreil.ly/w7GDX)，即可能无限的所有可能被抽样的样本集合。
- en: ^([5](ch04.xhtml#ch01fn86-marker)) Multilabel tasks are tasks where one example
    can have multiple labels.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch04.xhtml#ch01fn86-marker)) 多标签任务是指一个示例可能有多个标签。
- en: '^([6](ch04.xhtml#ch01fn87-marker)) “SVM: Weighted Samples,” scikit-learn, [*https://oreil.ly/BDqbk*](https://oreil.ly/BDqbk).'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '^([6](ch04.xhtml#ch01fn87-marker)) “SVM: 加权样本”，scikit-learn，[*https://oreil.ly/BDqbk*](https://oreil.ly/BDqbk)。'
- en: ^([7](ch04.xhtml#ch01fn88-marker)) Xiaojin Zhu, “Semi-Supervised Learning with
    Graphs” (doctoral diss., Carnegie Mellon University, 2005), [*https://oreil.ly/VYy4C*](https://oreil.ly/VYy4C).
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch04.xhtml#ch01fn88-marker)) Xiaojin Zhu，《使用图进行半监督学习》（博士论文，卡内基梅隆大学，2005
    年），[*https://oreil.ly/VYy4C*](https://oreil.ly/VYy4C)。
- en: ^([8](ch04.xhtml#ch01fn89-marker)) If something is so obvious to label, you
    wouldn’t need domain expertise.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch04.xhtml#ch01fn89-marker)) 如果某些事物显而易见可以标注，你不需要领域专业知识。
- en: ^([9](ch04.xhtml#custom_ch04fn1-marker)) We’ll cover programmatic labels in
    the section [“Weak supervision”](#weak_supervision).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch04.xhtml#custom_ch04fn1-marker)) 我们将在 [“弱监督”](#weak_supervision) 部分讨论编程标签。
- en: ^([10](ch04.xhtml#ch01fn90-marker)) Sofia Ira Ktena, Alykhan Tejani, Lucas Theis,
    Pranay Kumar Myana, Deepak Dilipkumar, Ferenc Huszar, Steven Yoo, and Wenzhe Shi,
    “Addressing Delayed Feedback for Continuous Training with Neural Networks in CTR
    Prediction,” *arXiv*, July 15, 2019, [*https://oreil.ly/5y2WA*](https://oreil.ly/5y2WA).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch04.xhtml#ch01fn90-marker)) Sofia Ira Ktena, Alykhan Tejani, Lucas Theis,
    Pranay Kumar Myana, Deepak Dilipkumar, Ferenc Huszar, Steven Yoo 和 Wenzhe Shi
    的文章，“解决连续训练中神经网络的延迟反馈问题”，*arXiv*，2019 年 7 月 15 日，[*https://oreil.ly/5y2WA*](https://oreil.ly/5y2WA)。
- en: '^([11](ch04.xhtml#ch01fn91-marker)) Alexander Ratner, Stephen H. Bach, Henry
    Ehrenberg, Jason Fries, Sen Wu, and Christopher Ré, “Snorkel: Rapid Training Data
    Creation with Weak Supervision,” *Proceedings of the VLDB Endowment* 11, no. 3
    (2017): 269–82, [*https://oreil.ly/vFPjk*](https://oreil.ly/vFPjk).'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '^([11](ch04.xhtml#ch01fn91-marker)) Alexander Ratner, Stephen H. Bach, Henry
    Ehrenberg, Jason Fries, Sen Wu 和 Christopher Ré 的文章，“Snorkel: 利用弱监督快速创建训练数据”，*VLDB
    Endowment* 11 卷 3 期（2017 年）：269–82，[*https://oreil.ly/vFPjk*](https://oreil.ly/vFPjk)。'
- en: '^([12](ch04.xhtml#ch01fn92-marker)) Ratner et al., “Snorkel: Rapid Training
    Data Creation with Weak Supervision.”'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '^([12](ch04.xhtml#ch01fn92-marker)) Ratner 等人的文章，“Snorkel: 利用弱监督快速创建训练数据”。'
- en: '^([13](ch04.xhtml#ch01fn93-marker)) Jared A. Dunnmon, Alexander J. Ratner,
    Khaled Saab, Matthew P. Lungren, Daniel L. Rubin, and Christopher Ré, “Cross-Modal
    Data Programming Enables Rapid Medical Machine Learning,” *Patterns* 1, no. 2
    (2020): 100019, [*https://oreil.ly/nKt8E*](https://oreil.ly/nKt8E).'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch04.xhtml#ch01fn93-marker)) Jared A. Dunnmon, Alexander J. Ratner, Khaled
    Saab, Matthew P. Lungren, Daniel L. Rubin 和 Christopher Ré 的文章，“跨模态数据编程支持快速医学机器学习”，*Patterns*
    1 卷 2 期（2020 年）：100019，[*https://oreil.ly/nKt8E*](https://oreil.ly/nKt8E)。
- en: ^([14](ch04.xhtml#ch01fn94-marker)) The two tasks in this study use only 18
    and 20 LFs respectively. In practice, I’ve seen teams using hundreds of LFs for
    each task.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch04.xhtml#ch01fn94-marker)) 这项研究中的两个任务分别仅使用了 18 和 20 个 LF。实际上，我见过有些团队为每个任务使用了数百个
    LF。
- en: ^([15](ch04.xhtml#ch01fn95-marker)) Dummon et al., “Cross-Modal Data Programming.”
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch04.xhtml#ch01fn95-marker)) Dummon 等人的文章，“跨模态数据编程”。
- en: '^([16](ch04.xhtml#ch01fn96-marker)) Avrim Blum and Tom Mitchell, “Combining
    Labeled and Unlabeled Data with Co-Training,” in *Proceedings of the Eleventh
    Annual Conference on Computational Learning Theory* (July 1998): 92–100, [*https://oreil.ly/T79AE*](https://oreil.ly/T79AE).'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch04.xhtml#ch01fn96-marker)) Avrim Blum 和 Tom Mitchell 的文章，“利用共训练结合标记和未标记数据”，收录于《第十一届计算学习理论年会论文集》（1998
    年 7 月）：92–100，[*https://oreil.ly/T79AE*](https://oreil.ly/T79AE)。
- en: ^([17](ch04.xhtml#ch01fn97-marker)) Avital Oliver, Augustus Odena, Colin Raffel,
    Ekin D. Cubuk, and Ian J. Goodfellow, “Realistic Evaluation of Deep Semi-Supervised
    Learning Algorithms,” *NeurIPS 2018 Proceedings*, [*https://oreil.ly/dRmPV*](https://oreil.ly/dRmPV).
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch04.xhtml#ch01fn97-marker)) Avital Oliver, Augustus Odena, Colin Raffel,
    Ekin D. Cubuk 和 Ian J. Goodfellow 的文章，“深度半监督学习算法的现实评估”，*NeurIPS 2018 会议论文集*，[*https://oreil.ly/dRmPV*](https://oreil.ly/dRmPV)。
- en: ^([18](ch04.xhtml#ch01fn98-marker)) A token can be a word, a character, or part
    of a word.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: ^([18](ch04.xhtml#ch01fn98-marker)) 一个 token 可以是一个单词、一个字符或者一个单词的一部分。
- en: ^([19](ch04.xhtml#ch01fn99-marker)) Jeremy Howard and Sebastian Ruder, “Universal
    Language Model Fine-tuning for Text Classification,” *arXiv*, January 18, 2018,
    [*https://oreil.ly/DBEbw*](https://oreil.ly/DBEbw).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: ^([19](ch04.xhtml#ch01fn99-marker)) Jeremy Howard 和 Sebastian Ruder 的文章，“通用语言模型微调用于文本分类”，*arXiv*，2018
    年 1 月 18 日，[*https://oreil.ly/DBEbw*](https://oreil.ly/DBEbw)。
- en: '^([20](ch04.xhtml#ch01fn100-marker)) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao
    Jiang, Hiroaki Hayashi, and Graham Neubig, “Pre-train, Prompt, and Predict: A
    Systematic Survey of Prompting Methods in Natural Language Processing,” *arXiv*,
    July 28, 2021, [*https://oreil.ly/0lBgn*](https://oreil.ly/0lBgn).'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '^([20](ch04.xhtml#ch01fn100-marker)) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao
    Jiang, Hiroaki Hayashi 和 Graham Neubig 的文章，“Pre-train, Prompt, and Predict: 自然语言处理中提示方法的系统调查”，*arXiv*，2021
    年 7 月 28 日，[*https://oreil.ly/0lBgn*](https://oreil.ly/0lBgn)。'
- en: '^([21](ch04.xhtml#ch01fn101-marker)) Jacob Devlin, Ming-Wei Chang, Kenton Lee,
    and Kristina Toutanova, “BERT: Pre-training of Deep Bidirectional Transformers
    for Language Understanding,” *arXiv*, October 11, 2018, [*https://oreil.ly/RdIGU*](https://oreil.ly/RdIGU);
    Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
    Dhariwal, Arvind Neelakantan, et al., “Language Models Are Few-Shot Learners,”
    OpenAI, 2020, [*https://oreil.ly/YVmrr*](https://oreil.ly/YVmrr).'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '^([21](ch04.xhtml#ch01fn101-marker)) Jacob Devlin, Ming-Wei Chang, Kenton Lee
    和 Kristina Toutanova 的文章，“BERT: 深度双向转换器的预训练用于语言理解”，*arXiv*，2018 年 10 月 11 日，[*https://oreil.ly/RdIGU*](https://oreil.ly/RdIGU)；Tom
    B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
    Arvind Neelakantan 等人的文章，“语言模型是少样本学习者”，OpenAI，2020 年，[*https://oreil.ly/YVmrr*](https://oreil.ly/YVmrr)。'
- en: '^([22](ch04.xhtml#ch01fn102-marker)) Burr Settles, *Active Learning* (Williston,
    VT: Morgan & Claypool, 2012).'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '^([22](ch04.xhtml#ch01fn102-marker)) Burr Settles，《主动学习》，（Williston, VT: Morgan
    & Claypool, 2012）。'
- en: ^([23](ch04.xhtml#ch01fn103-marker)) We’ll cover ensembles in [Chapter 6](ch06.xhtml#model_development_and_offline_evaluatio).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: ^([23](ch04.xhtml#ch01fn103-marker)) 我们将在[第6章](ch06.xhtml#model_development_and_offline_evaluatio)中涵盖整体学习方法。
- en: '^([24](ch04.xhtml#ch01fn104-marker)) Dana Angluin, “Queries and Concept Learning,”
    *Machine Learning* 2 (1988): 319–42, [*https://oreil.ly/0uKs4*](https://oreil.ly/0uKs4).'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '^([24](ch04.xhtml#ch01fn104-marker)) Dana Angluin, “Queries and Concept Learning,”
    *Machine Learning* 2 (1988): 319–42, [*https://oreil.ly/0uKs4*](https://oreil.ly/0uKs4).'
- en: ^([25](ch04.xhtml#ch01fn105-marker)) Thanks to Eugene Yan for this wonderful
    example!
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: ^([25](ch04.xhtml#ch01fn105-marker)) 感谢Eugene Yan提供这个精彩的例子！
- en: ^([26](ch04.xhtml#ch01fn106-marker)) Andrew Ng, “Bridging AI’s Proof-of-Concept
    to Production Gap” (HAI Seminar, September 22, 2020), video, 1:02:07, [*https://oreil.ly/FSFWS*](https://oreil.ly/FSFWS).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: ^([26](ch04.xhtml#ch01fn106-marker)) Andrew Ng，“填补AI概念验证到生产应用的鸿沟”（HAI Seminar,
    2020年9月22日），视频，1:02:07，[*https://oreil.ly/FSFWS*](https://oreil.ly/FSFWS)。
- en: ^([27](ch04.xhtml#ch01fn107-marker)) And this is why accuracy is a bad metric
    for tasks with class imbalance, as we’ll explore more in the section [“Handling
    Class Imbalance”](#handling_class_imbalance).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ^([27](ch04.xhtml#ch01fn107-marker)) 这也是为什么准确率在类别不平衡的任务中是一个糟糕的指标，我们将在[“处理类别不平衡”](#handling_class_imbalance)部分进一步探讨。
- en: ^([28](ch04.xhtml#ch01fn108-marker)) I imagined that it’d be easier to learn
    ML theory if I didn’t have to figure out how to deal with class imbalance.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: ^([28](ch04.xhtml#ch01fn108-marker)) 我设想如果不必解决类别不平衡问题，学习机器学习理论会更容易。
- en: ^([29](ch04.xhtml#custom_ch04fn2-marker)) The Nilson Report, “Payment Card Fraud
    Losses Reach $27.85 Billion,” PR Newswire, November 21, 2019, [*https://oreil.ly/NM5zo*](https://oreil.ly/NM5zo).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: ^([29](ch04.xhtml#custom_ch04fn2-marker)) Nilson报告，“支付卡欺诈损失达到278.5亿美元”，PR Newswire，2019年11月21日，[*https://oreil.ly/NM5zo*](https://oreil.ly/NM5zo)。
- en: ^([30](ch04.xhtml#custom_ch04fn3-marker)) “Job Market Expert Explains Why Only
    2% of Job Seekers Get Interviewed,” WebWire, January 7, 2014, [*https://oreil.ly/UpL8S*](https://oreil.ly/UpL8S).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: ^([30](ch04.xhtml#custom_ch04fn3-marker)) “职场专家解释为什么只有2%的求职者能被面试”，WebWire，2014年1月7日，[*https://oreil.ly/UpL8S*](https://oreil.ly/UpL8S)。
- en: ^([31](ch04.xhtml#custom_ch04fn4-marker)) “Email and Spam Data,” Talos Intelligence,
    last accessed May 2021, [*https://oreil.ly/lI5Jr*](https://oreil.ly/lI5Jr).
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: ^([31](ch04.xhtml#custom_ch04fn4-marker)) “电子邮件和垃圾邮件数据”，Talos Intelligence，最近访问于2021年5月，[*https://oreil.ly/lI5Jr*](https://oreil.ly/lI5Jr)。
- en: '^([32](ch04.xhtml#ch01fn109-marker)) Nathalie Japkowciz and Shaju Stephen,
    “The Class Imbalance Problem: A Systematic Study,” 2002, [*https://oreil.ly/d7lVu*](https://oreil.ly/d7lVu).'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: ^([32](ch04.xhtml#ch01fn109-marker)) Nathalie Japkowciz 和 Shaju Stephen，“类别不平衡问题：系统研究”，2002年，[*https://oreil.ly/d7lVu*](https://oreil.ly/d7lVu)。
- en: '^([33](ch04.xhtml#ch01fn110-marker)) Nathalie Japkowicz, “The Class Imbalance
    Problem: Significance and Strategies,” 2000, [*https://oreil.ly/Ma50Z*](https://oreil.ly/Ma50Z).'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: ^([33](ch04.xhtml#ch01fn110-marker)) Nathalie Japkowicz，“类别不平衡问题：意义和策略”，2000年，[*https://oreil.ly/Ma50Z*](https://oreil.ly/Ma50Z)。
- en: ^([34](ch04.xhtml#ch01fn111-marker)) Wan Ding, Dong-Yan Huang, Zhuo Chen, Xinguo
    Yu, and Weisi Lin, “Facial Action Recognition Using Very Deep Networks for Highly
    Imbalanced Class Distribution,” *2017 Asia-Pacific Signal and Information Processing
    Association Annual Summit and Conference (APSIPA ASC)*, 2017, [*https://oreil.ly/WeW6J*](https://oreil.ly/WeW6J).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: ^([34](ch04.xhtml#ch01fn111-marker)) Wan Ding, Dong-Yan Huang, Zhuo Chen, Xinguo
    Yu 和 Weisi Lin，“利用极度不平衡类分布的深度网络进行面部动作识别”，*2017年亚太信号与信息处理协会年度峰会与会议（APSIPA ASC）*，2017年，[*https://oreil.ly/WeW6J*](https://oreil.ly/WeW6J)。
- en: ^([35](ch04.xhtml#ch01fn112-marker)) As of July 2021, when you use `scikit-learn.metrics.f1_score`,
    `pos_label` is set to 1 by default, but you can change it to 0 if you want 0 to
    be your positive label.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: ^([35](ch04.xhtml#ch01fn112-marker)) 截至2021年7月，当你使用`scikit-learn.metrics.f1_score`时，`pos_label`默认设置为1，但如果你希望0为正标签，你可以进行更改。
- en: ^([36](ch04.xhtml#ch01fn113-marker)) Jesse Davis and Mark Goadrich, “The Relationship
    Between Precision-Recall and ROC Curves,” *Proceedings of the 23rd International
    Conference on Machine Learning*, 2006, [*https://oreil.ly/s40F3*](https://oreil.ly/s40F3).
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: ^([36](ch04.xhtml#ch01fn113-marker)) Jesse Davis 和 Mark Goadrich，“精确率-召回率与ROC曲线之间的关系”，*第23届国际机器学习大会论文集*，2006年，[*https://oreil.ly/s40F3*](https://oreil.ly/s40F3)。
- en: ^([37](ch04.xhtml#ch01fn114-marker)) Rafael Alencar, “Resampling Strategies
    for Imbalanced Datasets,” Kaggle, [*https://oreil.ly/p8Whs*](https://oreil.ly/p8Whs).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: ^([37](ch04.xhtml#ch01fn114-marker)) Rafael Alencar，“不平衡数据集的重采样策略”，Kaggle，[*https://oreil.ly/p8Whs*](https://oreil.ly/p8Whs)。
- en: '^([38](ch04.xhtml#ch01fn115-marker)) Ivan Tomek, “An Experiment with the Edited
    Nearest-Neighbor Rule,” *IEEE Transactions on Systems, Man, and Cybernetics* (June
    1976): 448–52, [*https://oreil.ly/JCxHZ*](https://oreil.ly/JCxHZ).'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: ^([38](ch04.xhtml#ch01fn115-marker)) Ivan Tomek，“最近邻编辑规则实验”，《IEEE系统、人类和控制论》（1976年6月）：448–52，[*https://oreil.ly/JCxHZ*](https://oreil.ly/JCxHZ)。
- en: '^([39](ch04.xhtml#idm46868210052544-marker)) N.V. Chawla, K.W. Bowyer, L.O.
    Hall, and W.P. Kegelmeyer, “SMOTE: Synthetic Minority Over-sampling Technique,
    *Journal of Artificial Intelligence Research* 16 (2002): 341–78, [*https://oreil.ly/f6y46*](https://oreil.ly/f6y46).'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: ^([39](ch04.xhtml#idm46868210052544-marker)) N.V. Chawla, K.W. Bowyer, L.O.
    Hall, and W.P. Kegelmeyer，“SMOTE:合成少数类过采样技术”，《人工智能研究杂志》16卷（2002年）：341–78，[*https://oreil.ly/f6y46*](https://oreil.ly/f6y46)。
- en: ^([40](ch04.xhtml#ch01fn116-marker)) “Convex” here approximately means “linear.”
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: ^([40](ch04.xhtml#ch01fn116-marker)) “凸”这里大致意味着“线性”。
- en: '^([41](ch04.xhtml#ch01fn117-marker)) Jianping Zhang and Inderjeet Mani, “kNN
    Approach to Unbalanced Data Distributions: A Case Study involving Information
    Extraction” (Workshop on Learning from Imbalanced Datasets II, ICML, Washington,
    DC, 2003), [*https://oreil.ly/qnpra*](https://oreil.ly/qnpra); Miroslav Kubat
    and Stan Matwin, “Addressing the Curse of Imbalanced Training Sets: One-Sided
    Selection,” 2000, [*https://oreil.ly/8pheJ*](https://oreil.ly/8pheJ).'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: ^([41](ch04.xhtml#ch01fn117-marker)) 张建平和Inderjeet Mani，“kNN方法解决不平衡数据分布：涉及信息提取的案例研究”（《ICML不平衡数据集学习研讨会II》，华盛顿特区，2003年），[*https://oreil.ly/qnpra*](https://oreil.ly/qnpra)；Miroslav
    Kubat和Stan Matwin，“解决不平衡训练集的诅咒：单侧选择”（2000年），[*https://oreil.ly/8pheJ*](https://oreil.ly/8pheJ)。
- en: ^([42](ch04.xhtml#ch01fn118-marker)) Hansang Lee, Minseok Park, and Junmo Kim,
    “Plankton Classification on Imbalanced Large Scale Database via Convolutional
    Neural Networks with Transfer Learning,” *2016 IEEE International Conference on
    Image Processing (ICIP)*, 2016, [*https://oreil.ly/YiA8p*](https://oreil.ly/YiA8p).
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: ^([42](ch04.xhtml#ch01fn118-marker)) Hansang Lee, Minseok Park, and Junmo Kim，“通过转移学习的卷积神经网络在大规模不平衡数据库中的浮游生物分类”，《2016年IEEE图像处理国际会议（ICIP）》（2016），[*https://oreil.ly/YiA8p*](https://oreil.ly/YiA8p)。
- en: ^([43](ch04.xhtml#ch01fn119-marker)) Samira Pouyanfar, Yudong Tao, Anup Mohan,
    Haiman Tian, Ahmed S. Kaseb, Kent Gauen, Ryan Dailey, et al., “Dynamic Sampling
    in Convolutional Neural Networks for Imbalanced Data Classification,” *2018 IEEE
    Conference on Multimedia Information Processing and Retrieval (MIPR)*, 2018, [*https://oreil.ly/D3Ak5*](https://oreil.ly/D3Ak5).
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: ^([43](ch04.xhtml#ch01fn119-marker)) Samira Pouyanfar, Yudong Tao, Anup Mohan,
    Haiman Tian, Ahmed S. Kaseb, Kent Gauen, Ryan Dailey等，“卷积神经网络中的动态采样用于不平衡数据分类”，《2018年IEEE多媒体信息处理和检索会议（MIPR）》（2018），[*https://oreil.ly/D3Ak5*](https://oreil.ly/D3Ak5)。
- en: ^([44](ch04.xhtml#ch01fn120-marker)) Charles Elkan, “The Foundations of Cost-Sensitive
    Learning,” *Proceedings of the Seventeenth International Joint Conference on Artificial
    Intelligence* (IJCAI’01), 2001, [*https://oreil.ly/WGq5M*](https://oreil.ly/WGq5M).
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: ^([44](ch04.xhtml#ch01fn120-marker)) Charles Elkan，“成本敏感学习的基础”，《第十七届国际人工智能联合会议》（IJCAI’01）（2001），[*https://oreil.ly/WGq5M*](https://oreil.ly/WGq5M)。
- en: ^([45](ch04.xhtml#custom_ch04fn5-marker)) Yin Cui, Menglin Jia, Tsung-Yi Lin,
    Yang Song, and Serge Belongie, “Class-Balanced Loss Based on Effective Number
    of Samples,” *Proceedings of the Conference on Computer Vision and Pattern*, 2019,
    [*https://oreil.ly/jCzGH*](https://oreil.ly/jCzGH).
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: ^([45](ch04.xhtml#custom_ch04fn5-marker)) Yin Cui, Menglin Jia, Tsung-Yi Lin,
    Yang Song, and Serge Belongie，“基于有效样本数量的类平衡损失”，《计算机视觉与模式识别会议论文集》（2019），[*https://oreil.ly/jCzGH*](https://oreil.ly/jCzGH)。
- en: ^([46](ch04.xhtml#ch01fn121-marker)) Tsung-Yi Lin, Priya Goyal, Ross Girshick,
    Kaiming He, and Piotr Dollár, “Focal Loss for Dense Object Detection,” *arXiv*,
    August 7, 2017, [*https://oreil.ly/Km2dF*](https://oreil.ly/Km2dF).
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: ^([46](ch04.xhtml#ch01fn121-marker)) Tsung-Yi Lin, Priya Goyal, Ross Girshick,
    Kaiming He, and Piotr Dollár，“密集目标检测的焦点损失”，《arXiv》，2017年8月7日，[*https://oreil.ly/Km2dF*](https://oreil.ly/Km2dF)。
- en: '^([47](ch04.xhtml#ch01fn122-marker)) Mikel Galar, Alberto Fernandez, Edurne
    Barrenechea, Humberto Bustince, and Francisco Herrera, “A Review on Ensembles
    for the Class Imbalance Problem: Bagging-, Boosting-, and Hybrid-Based Approaches,”
    *IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and
    Reviews)* 42, no. 4 (July 2012): 463–84, [*https://oreil.ly/1ND4g*](https://oreil.ly/1ND4g).'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: ^([47](ch04.xhtml#ch01fn122-marker)) Mikel Galar, Alberto Fernandez, Edurne
    Barrenechea, Humberto Bustince, and Francisco Herrera，“解决类不平衡问题的集成方法综述：Bagging、Boosting和混合方法”，《IEEE系统、人类和控制论》C部分（应用与评论）42卷，第4期（2012年7月）：463–84，[*https://oreil.ly/1ND4g*](https://oreil.ly/1ND4g)。
- en: ^([48](ch04.xhtml#ch01fn123-marker)) Alex Krizhevsky, Ilya Sutskever, and Geoffrey
    E. Hinton, “ImageNet Classification with Deep Convolutional Neural Networks, 2012,
    [*https://oreil.ly/aphzA*](https://oreil.ly/aphzA).
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: ^([48](ch04.xhtml#ch01fn123-marker)) 亚历克斯·克里兹海夫斯基，伊利亚·苏茨凯弗和杰弗里·E·辛顿，“ImageNet分类与深度卷积神经网络”，2012年，[*https://oreil.ly/aphzA*](https://oreil.ly/aphzA)。
- en: '^([49](ch04.xhtml#ch01fn124-marker)) Jiawei Su, Danilo Vasconcellos Vargas,
    and Sakurai Kouichi, “One Pixel Attack for Fooling Deep Neural Networks,” *IEEE
    Transactions on Evolutionary Computation* 23, no. 5 (2019): 828–41, [*https://oreil.ly/LzN9D*](https://oreil.ly/LzN9D).'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: ^([49](ch04.xhtml#ch01fn124-marker)) 苏嘉伟，达尼洛·瓦斯孔塞洛斯·瓦加斯和樱井浩一，“用于欺骗深度神经网络的单像素攻击”，*IEEE
    Evolutionary Computation*，2019，23卷5期：828–41，[*https://oreil.ly/LzN9D*](https://oreil.ly/LzN9D)。
- en: ^([50](ch04.xhtml#ch01fn129-marker)) Su et al., “One Pixel Attack.”
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: ^([50](ch04.xhtml#ch01fn129-marker)) 苏杰伟等，“单像素攻击”。
- en: ^([51](ch04.xhtml#ch01fn125-marker)) Ian J. Goodfellow, Jonathon Shlens, and
    Christian Szegedy, “Explaining and Harnessing Adversarial Examples,” *arXiv*,
    March 20, 2015, [*https://oreil.ly/9v2No*](https://oreil.ly/9v2No); Ian J. Goodfellow,
    David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio, “Maxout Networks,
    *arXiv*, February 18, 2013, [*https://oreil.ly/L8mch*](https://oreil.ly/L8mch).
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: ^([51](ch04.xhtml#ch01fn125-marker)) 伊恩·J·古德费洛，乔纳森·施伦斯和克里斯蒂安·赛格迪，“解释和利用对抗性示例”，*arXiv*，2015年3月20日，[*https://oreil.ly/9v2No*](https://oreil.ly/9v2No)；伊恩·J·古德费洛，大卫·沃德-法利，梅迪·米尔扎，阿伦·库尔维尔和约书亚·本吉奥，“Maxout网络”，*arXiv*，2013年2月18日，[*https://oreil.ly/L8mch*](https://oreil.ly/L8mch)。
- en: '^([52](ch04.xhtml#ch01fn126-marker)) Seyed-Mohsen Moosavi-Dezfooli, Alhussein
    Fawzi, and Pascal Frossard, “DeepFool: A Simple and Accurate Method to Fool Deep
    Neural Networks,” in *Proceedings of IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR)*, 2016, [*https://oreil.ly/dYVL8*](https://oreil.ly/dYVL8).'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: ^([52](ch04.xhtml#ch01fn126-marker)) 谢义德-莫赛尼-德佐夫利，阿尔胡森·法兹维和帕斯卡尔·弗罗萨德，“DeepFool：一种简单且准确的欺骗深度神经网络的方法”，在*IEEE计算机视觉与模式识别会议（CVPR）*中，2016年，[*https://oreil.ly/dYVL8*](https://oreil.ly/dYVL8)。
- en: '^([53](ch04.xhtml#ch01fn127-marker)) Takeru Miyato, Shin-ichi Maeda, Masanori
    Koyama, and Shin Ishii, “Virtual Adversarial Training: A Regularization Method
    for Supervised and Semi-Supervised Learning,” *IEEE Transactions on Pattern Analysis
    and Machine Intelligence*, 2017, [*https://oreil.ly/MBQeu*](https://oreil.ly/MBQeu).'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: ^([53](ch04.xhtml#ch01fn127-marker)) 宫藤猛，前田伸一，小山真典和石井信，“虚拟对抗训练：一种监督和半监督学习的正则化方法”，*IEEE
    Pattern Analysis and Machine Intelligence*，2017，[*https://oreil.ly/MBQeu*](https://oreil.ly/MBQeu)。
- en: '^([54](ch04.xhtml#ch01fn128-marker)) Devlin et al., “BERT: Pre-training of
    Deep Bidirectional Transformers for Language Understanding.”'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: ^([54](ch04.xhtml#ch01fn128-marker)) Devlin 等，“BERT：深度双向转换器的预训练”。
- en: '^([55](ch04.xhtml#ch01fn130-marker)) Hongyi Zhang, Moustapha Cisse, Yann N.
    Dauphin, and David Lopez-Paz, “*mixup*: Beyond Empirical Risk Minimization,” *ICLR
    2018*, [*https://oreil.ly/lIM5E*](https://oreil.ly/lIM5E).'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: ^([55](ch04.xhtml#ch01fn130-marker)) 张宏毅，穆斯塔法·西塞，扬·N·多芬和大卫·洛佩兹-帕兹，“*mixup*：超越经验风险最小化”，*ICLR
    2018*，[*https://oreil.ly/lIM5E*](https://oreil.ly/lIM5E)。
- en: '^([56](ch04.xhtml#ch01fn131-marker)) Veit Sandfort, Ke Yan, Perry J. Pickhardt,
    and Ronald M. Summers, “Data Augmentation Using Generative Adversarial Networks
    (CycleGAN) to Improve Generalizability in CT Segmentation Tasks,” *Scientific
    Reports* 9, no. 1 (2019): 16884, [*https://oreil.ly/TDUwm*](https://oreil.ly/TDUwm).'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: ^([56](ch04.xhtml#ch01fn131-marker)) 维特·桑德福特，闫科，佩里·J·皮克哈特和罗纳德·M·萨默斯，“使用生成对抗网络（CycleGAN）进行数据增强以提高CT分割任务的泛化能力”，*科学报告*，2019年，9卷1期：16884，[*https://oreil.ly/TDUwm*](https://oreil.ly/TDUwm)。
