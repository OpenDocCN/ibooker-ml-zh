- en: Chapter 12\. Training for Ranking
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章。排名的训练
- en: 'Typical ML tasks usually predict a single outcome, such as the probability
    of being in a positive class for classification tasks, or an expected value for
    regression tasks. Ranking, on the other hand, provides a relative ordering of
    sets of items. This kind of task is typical of search results or recommendations,
    where the order of items presented is important. In these kinds of problems, the
    score of an item usually isn’t shown to the user directly but rather is presented—maybe
    implicitly—with the ordinal rank of the item: the item at the top of the list
    is numbered lower than the next item.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的ML任务通常预测单一结果，例如分类任务中属于正类的概率，或者回归任务中的期望值。而排名则提供物品集合的相对排序。这种任务是搜索结果或推荐中常见的，其中呈现的物品顺序很重要。在这些问题中，物品的分数通常不会直接显示给用户，而是通过物品的序数排名—可能是隐式地—呈现：列表顶部的物品编号低于下一个物品。
- en: This chapter presents various kinds of loss functions that ML algorithms can
    use during training. These scores should estimate list orderings such that when
    compared to one another, they result in sets that are ordered more closely to
    the relevance ordering observed in a training dataset. Here we will focus on introducing
    the concepts and computations, which you’ll put to work in the next chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了ML算法在训练过程中可以使用的各种损失函数。这些分数应该估计列表排序，使得与训练数据集中观察到的相关性排序相比，结果更接近的集合。在这里，我们将重点介绍概念和计算，这些内容将在下一章中投入使用。
- en: Where Does Ranking Fit in Recommender Systems?
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推荐系统中的排名在哪里适用？
- en: Before we dive into the details of loss functions for ranking, we should talk
    about where ranking fits into the larger scheme of recommender systems as a whole.
    Typical large-scale recommenders have a retrieval phase, in which a cheap function
    is used to gather a decent number of candidate items into a candidate set. Usually,
    this retrieval phase is only item based. For example, the candidate set might
    include items related to recently consumed or liked items by a user. Or if freshness
    is important, such as for news data, the set might include the newest popular
    and relevant items for the user. After items are gathered into a candidate set,
    we apply ranking to its items.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入讨论排名的损失函数细节之前，我们应该谈谈排名在整个推荐系统中的位置。典型的大规模推荐系统有一个检索阶段，在此阶段使用廉价函数将大量候选项收集到候选集中。通常，此检索阶段仅基于物品。例如，候选集可能包括用户最近消费或喜欢的物品相关的物品。或者如果新鲜度很重要，比如对于新闻数据，该集合可能包括最新的热门和相关物品。物品被收集到候选集后，我们对其物品应用排名。
- en: Also, since the candidate set is usually much smaller than the entire corpus
    of items, we can use more expensive models and auxiliary features to help the
    ranking. These features could be user features or context features. User features
    could help in determining the items’ usefulness to the user, such as the average
    embedding of recently consumed items. Context features could indicate details
    about the current session, such as time of day or recent queries that a user has
    typed—a feature that differentiates the current session from others and helps
    in determining relevant items. Finally, we have the representation of the items
    themselves, which can be anything from content features to learned embeddings
    that represent the item.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，由于候选集通常比整个物品语料库小得多，我们可以使用更昂贵的模型和辅助特征来帮助排名。这些特征可以是用户特征或上下文特征。用户特征可以帮助确定物品对用户的有用性，例如最近消费物品的平均嵌入。上下文特征可以指示当前会话的详细信息，例如一天中的时间或用户最近键入的查询，这些特征区别于其他会话，并帮助确定相关物品。最后，我们有物品本身的表示，可以是从内容特征到学习嵌入的任何东西。
- en: The user, context, and item features are then concatenated into one feature
    vector that we will use to represent the item; we then score all the candidates
    at once and order them. The rank ordered set might then have extra filtering applied
    to it for business logic, such as removing near duplicates or making the ranked
    set more diverse in the kinds of items displayed.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将用户、上下文和物品特征连接成一个特征向量，我们将用它来表示物品；然后一次评分所有候选项并对其排序。然后可能对排序集合应用额外的过滤业务逻辑，例如删除近似重复项或使排序集合中显示的物品类型更多样化。
- en: In the following examples, we will assume that the items can all be represented
    by a concatenated feature vector of user, context, and item features and that
    the model could be as simple as a linear model with a weight vector `W` that is
    dotted with the item vector to obtain a score for sorting the items. These models
    can be generalized to deep neural networks, but the final layer output is still
    going to be a scalar used to sort the items.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们假设所有项目都可以由用户、上下文和项目特征的连接特征向量表示，并且模型可以简化为具有权重向量`W`的线性模型，该向量与项目向量点乘以获取用于排序项目的分数。这些模型可以推广为深度神经网络，但最终层的输出仍然是用于排序项目的标量。
- en: Now that we have set the context for ranking, let’s consider ways we might rank
    a set of items represented by vectors.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为排序设定了上下文，让我们考虑通过向量表示的一组项目的排名方式。
- en: Learning to Rank
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习排序
- en: '*Learning to rank* (LTR) is the name for the kind of models that score an ordered
    list of items according to their relevancy or importance. This technique is how
    we go from the potentially raw output of retrieval to a sorted list of items based
    on their relevance.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*学习排序*（LTR）是一种根据它们的相关性或重要性对排序列表进行评分的模型类型。这种技术是如何从检索的潜在原始输出到基于它们的相关性的排序列表的方法。'
- en: 'LTR problems have three main types:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: LTR问题有三种主要类型：
- en: Pointwise
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 点对点
- en: The model treats individual documents in isolation and assigns them a score
    or rank. The task becomes a regression or classification problem.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将单独的文档视为孤立的并分配给它们一个分数或等级。任务变成了一个回归或分类问题。
- en: Pairwise
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 逐对
- en: The model considers pairs of documents simultaneously in the loss function.
    The goal is to minimize the number of incorrectly ordered pairs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在损失函数中同时考虑文档对。目标是尽量减少错误排序的对数。
- en: Listwise
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 列表
- en: The model considers the entire list of documents in the loss function. The goal
    is to find the optimal ordering of the entire list.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在损失函数中考虑整个文档列表。目标是找到整个列表的最佳排序。
- en: Training an LTR Model
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练LTR模型
- en: The training data for an LTR model typically consists of a list of items, and
    each item has a set of features and a label (or ground truth). The features might
    include information about the item itself, and the label typically represents
    its relevance or importance. For instance, in our recommender systems, we have
    item features, and in the training dataset, the labels will show if the item is
    relevant to the user. Additionally, LTR models sometimes make use of the query
    or user features.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: LTR模型的训练数据通常包括项目列表，每个项目都有一组特征和一个标签（或地面真实值）。这些特征可能包括有关项目本身的信息，而标签通常表示其相关性或重要性。例如，在我们的推荐系统中，我们有项目特征，在训练数据集中，标签将显示项目是否与用户相关。此外，LTR模型有时会使用查询或用户特征。
- en: The training process is about learning a ranking function by using these features
    and labels. These ranking functions are then applied to the retrieved items before
    serving.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程是通过使用这些特征和标签来学习排名函数。然后将这些排名函数应用于检索到的项目。
- en: Let’s see some examples of how these models are trained.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些这些模型是如何训练的例子。
- en: Classification for Ranking
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于排序的分类
- en: One way to pose the ranking problem is as a multilabel task. Every item appearing
    in the training set that is associated to the user is a positive example, while
    those outside would be negative. This is, in effect, a multilabel approach at
    the scale of the set of items. The network could have an architecture with each
    item’s features as input nodes, and then some user features as well. The output
    nodes would be in correspondence with the items you wish to label.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 将排名问题建模为多标签任务的一种方法是一种方法。训练集中出现的与用户关联的每个项目都是正面例子，而那些在外部的则是负面的。这实际上是在项目集合的规模上的多标签方法。网络可能具有每个项目特征作为输入节点的架构，然后还有一些用户特征。输出节点将与您希望标记的项目对应。
- en: With a linear model, if <math alttext="upper X"><mi>X</mi></math> is the item
    vector and <math alttext="upper Y"><mi>Y</mi></math> is the output, we learn <math
    alttext="upper W"><mi>W</mi></math> , where <math alttext="s i g m o i d left-parenthesis
    upper W upper X right-parenthesis equals 1"><mrow><mi>s</mi> <mi>i</mi> <mi>g</mi>
    <mi>m</mi> <mi>o</mi> <mi>i</mi> <mi>d</mi> <mo>(</mo> <mi>W</mi> <mi>X</mi> <mo>)</mo>
    <mo>=</mo> <mn>1</mn></mrow></math> if <math alttext="upper X"><mi>X</mi></math>
    is an item in the positive set; otherwise, <math alttext="s i g m o i d left-parenthesis
    upper W upper X right-parenthesis equals 0"><mrow><mi>s</mi> <mi>i</mi> <mi>g</mi>
    <mi>m</mi> <mi>o</mi> <mi>i</mi> <mi>d</mi> <mo>(</mo> <mi>W</mi> <mi>X</mi> <mo>)</mo>
    <mo>=</mo> <mn>0</mn></mrow></math> . This corresponds to the [binary cross-entropy
    loss](https://oreil.ly/5Rd14) in Optax.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用线性模型，如果<math alttext="upper X"><mi>X</mi></math>是项目向量，<math alttext="upper
    Y"><mi>Y</mi></math>是输出，我们学习<math alttext="upper W"><mi>W</mi></math>，其中<math
    alttext="s i g m o i d left-parenthesis upper W upper X right-parenthesis equals
    1"><mrow><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>o</mi><mi>i</mi><mi>d</mi><mo>(</mo><mi>W</mi><mi>X</mi><mo>)</mo><mo>=</mo><mn>1</mn></mrow></math>如果<math
    alttext="upper X"><mi>X</mi></math>是正面集中的项目；否则<math alttext="s i g m o i d left-parenthesis
    upper W upper X right-parenthesis equals 0"><mrow><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>o</mi><mi>i</mi><mi>d</mi><mo>(</mo><mi>W</mi><mi>X</mi><mo>)</mo><mo>=</mo><mn>0</mn></mrow></math>。这对应于在Optax中的[二元交叉熵损失](https://oreil.ly/5Rd14)。
- en: Unfortunately, the relative ordering of items isn’t taken into account in this
    setup, so this loss function consisting of sigmoid activation functions for each
    item won’t optimize ranking metrics very well. Effectively, this ranking is merely
    a downstream *relevance model* that only helps to filter those options retrieved
    in a previous step.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，在这种设置中没有考虑项目的相对排序，因此由于每个项目都有一个sigmoid激活函数的损失函数，它不会很好地优化排名度量标准。实际上，这种排名仅仅是一个下游的*相关性模型*，只有帮助过滤在先前步骤中检索到的选项。
- en: Another problem with this approach is that we have labeled everything outside
    of the training set to be negative, but the user might never have seen a new item
    that could be relevant to a query—so it would be incorrect to label this new item
    as a negative when it is simply unobserved.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的另一个问题是，我们已经将训练集之外的所有内容标记为负面，但是用户可能从未看过一个新项目，这个新项目可能与查询相关，因此将这个新项目标记为负面是不正确的，因为它只是未观察到。
- en: You may have realized that the ranking needs to consider the relative positions
    in the list. Let’s consider this next.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经意识到，排名需要考虑列表中的相对位置。让我们接着考虑这个问题。
- en: Regression for Ranking
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于排名的回归
- en: The most naive way to rank a set of items is simply to regress to the rank of
    a similar number like NDCG or our other personalization metrics that are rank
    respective.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 排列一组项目最朴素的方法是简单地回归到类似NDCG或我们其他的个性化度量的排名。
- en: In practice, this is achieved by conditioning the set of items against a query.
    For example, we could pose the problem as regression to the NDCG, given the query
    as the context of the ranking. Furthermore, we can supply the query as an embedding
    context vector to a feed-forward network that is concatenated with the features
    of the items in the set and regress toward the NDCG value.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这通过将项目集条件化为一个查询来实现。例如，我们可以将问题表述为回归到NDCG，将查询作为排名的上下文。此外，我们可以将查询作为嵌入上下文向量提供给一个前馈网络，该网络与集合中项目的特征串联并回归到NDCG值。
- en: The query is needed as a context because a set of item’s ordering might be dependent
    upon the query. Consider, for example, typing into a search bar the query **`flowers`**.
    We would then expect a set of items most representative of flowers to be in the
    top results. This demonstrates that the query is an important consideration of
    the scoring function.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 查询作为上下文是必需的，因为一组项目的排序可能依赖于查询。例如，键入搜索栏中的查询**`flowers`**。然后，我们期望一组最能代表花卉的项目出现在前面的结果中。这表明查询是评分函数的重要考虑因素。
- en: With a linear model, if <math alttext="upper X"><mi>X</mi></math> is the item
    vector and <math alttext="upper Y"><mi>Y</mi></math> is the output, then we learn
    <math alttext="upper W"><mi>W</mi></math> , where <math alttext="upper W upper
    X left-parenthesis i right-parenthesis equals upper N upper D upper C upper G
    left-parenthesis i right-parenthesis"><mrow><mi>W</mi> <mi>X</mi> <mo>(</mo> <mi>i</mi>
    <mo>)</mo> <mo>=</mo> <mi>N</mi> <mi>D</mi> <mi>C</mi> <mi>G</mi> <mo>(</mo> <mi>i</mi>
    <mo>)</mo></mrow></math> and <math alttext="upper N upper D upper C upper G left-parenthesis
    i right-parenthesis"><mrow><mi>N</mi> <mi>D</mi> <mi>C</mi> <mi>G</mi> <mo>(</mo>
    <mi>i</mi> <mo>)</mo></mrow></math> is the NDCG for item <math alttext="i"><mi>i</mi></math>
    . Regression can be learned using the [L2 loss](https://oreil.ly/IHw-Z) in Optax.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于线性模型，如果 <math alttext="upper X"><mi>X</mi></math> 是项目向量，<math alttext="upper
    Y"><mi>Y</mi></math> 是输出，则我们学习 <math alttext="upper W"><mi>W</mi></math> ，其中 <math
    alttext="upper W upper X left-parenthesis i right-parenthesis equals upper N upper
    D upper C upper G left-parenthesis i right-parenthesis"><mrow><mi>W</mi> <mi>X</mi>
    <mo>(</mo> <mi>i</mi> <mo>)</mo> <mo>=</mo> <mi>N</mi> <mi>D</mi> <mi>C</mi> <mi>G</mi>
    <mo>(</mo> <mi>i</mi> <mo>)</mo></mrow></math> 而 <math alttext="upper N upper
    D upper C upper G left-parenthesis i right-parenthesis"><mrow><mi>N</mi> <mi>D</mi>
    <mi>C</mi> <mi>G</mi> <mo>(</mo> <mi>i</mi> <mo>)</mo></mrow></math> 是项目 <math
    alttext="i"><mi>i</mi></math> 的 NDCG。在 Optax 中，可以使用 [L2 loss](https://oreil.ly/IHw-Z)
    进行回归学习。
- en: Ultimately, this approach is about attempting to learn the underlying features
    of items that lead to higher-rank scores in your personalization metric. Unfortunately,
    this also fails to explicitly consider the relative ordering of items. This is
    a pretty serious limitation, which we’ll consider shortly.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，这种方法旨在尝试学习导致个性化指标中得分更高的项目的潜在特征。不幸的是，这也未明确考虑项目的相对排序。这是一个相当严重的限制，我们稍后将考虑。
- en: 'Another consideration: what do we do for items that aren’t ranked outside of
    the top-*k* training items? The rank we would assign them would be essentially
    random, as we do not know what number to assign them. Therefore, this method needs
    improvement, which we’ll explore in the next section.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个考虑因素是：对于在前 *k* 个训练项目之外未排名的项目，我们该怎么办？我们给它们分配的排名基本上是随机的，因为我们不知道要给它们分配什么数字。因此，这种方法需要改进，我们将在下一节中探讨。
- en: Classification and Regression for Ranking
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 排名的分类和回归
- en: Suppose we have a web page such as an online bookstore, and users have to browse
    through and click items in order to purchase them. For such a funnel, we could
    break the ranking into two parts. The first model could predict the probability
    of a click on an item, given a set of items on display. The second model could
    be conditioned on a click-through and could be a regression model estimating the
    purchase price of the item.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个网页，比如一个在线书店，用户必须浏览并点击项目才能购买它们。对于这样的漏斗，我们可以将排名分为两部分。第一个模型可以预测在展示的一组项目中点击项目的概率。第二个模型可以在点击后进行条件化，并且可以是一个回归模型，估计项目的购买价格。
- en: Then, a full ranking model could be the product of two models. The first one
    computes the probability of clicking through an item, given a set of competing
    items. And the second one computes the expected value of a purchase, given that
    it had been clicked. Notice that the first and second model could have different
    features, depending on the stage of the funnel a user is in. The first model has
    access to features of competing items, while the second model might take into
    account shipping costs and discounts applied that might change the value of an
    item. Thus, in this setting, it would be advantageous to model both stages of
    the funnel with different models so as to make use of the most amount of information
    present at each stage of the funnel.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，一个完整的排名模型可以是两个模型的乘积。第一个模型计算在一组竞争项目中点击项目的概率。第二个模型计算被点击后购买的预期值。请注意，第一个和第二个模型可能具有不同的特征，这取决于用户所处的漏斗阶段。第一个模型可以访问竞争项目的特征，而第二个模型可能考虑到可能改变项目价值的运费和应用的折扣。因此，在这种情况下，利用不同的模型对漏斗的每个阶段进行建模是有利的，以便利用每个阶段存在的最多信息。
- en: WARP
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: WARP
- en: 'One possible way to generate a ranking loss stochastically is introduced in
    [“WSABIE: Scaling Up to Large Vocabulary Image Annotation”](https://oreil.ly/bagf-)
    by Jason Weston et al. The loss is called *weighted approximate rank pairwise*
    (WARP). In this scheme, the loss function is broken into what looks like a pairwise
    loss. More precisely, if a higher-ranked item doesn’t have a score that is greater
    than the margin (which is arbitrarily picked to be 1) for a lower-rank item, we
    apply the *hinge loss* to the pair of items. This looks like the following:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '介绍一种随机生成排名损失的可能方式在[“WSABIE: Scaling Up to Large Vocabulary Image Annotation”](https://oreil.ly/bagf-)由Jason
    Weston等人提出。该损失被称为*weighted approximate rank pairwise*（WARP）。在这个方案中，损失函数被分解为看起来像是一对一的损失。更确切地说，如果一个排名更高的项目没有一个分数大于较低排名项目的边界（任意选取为1），我们对这对项目应用*hinge
    loss*。这看起来像下面这样：'
- en: <math alttext="m a x left-parenthesis 0 comma 1 minus s c o r e left-parenthesis
    p o s right-parenthesis plus s c o r e left-parenthesis n e g right-parenthesis
    right-parenthesis" display="block"><mrow><mi>m</mi> <mi>a</mi> <mi>x</mi> <mo>(</mo>
    <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo>-</mo> <mi>s</mi> <mi>c</mi> <mi>o</mi> <mi>r</mi>
    <mi>e</mi> <mo>(</mo> <mi>p</mi> <mi>o</mi> <mi>s</mi> <mo>)</mo> <mo>+</mo> <mi>s</mi>
    <mi>c</mi> <mi>o</mi> <mi>r</mi> <mi>e</mi> <mo>(</mo> <mi>n</mi> <mi>e</mi> <mi>g</mi>
    <mo>)</mo> <mo>)</mo></mrow></math>
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="m a x left-parenthesis 0 comma 1 minus s c o r e left-parenthesis
    p o s right-parenthesis plus s c o r e left-parenthesis n e g right-parenthesis
    right-parenthesis" display="block"><mrow><mi>m</mi> <mi>a</mi> <mi>x</mi> <mo>(</mo>
    <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo>-</mo> <mi>s</mi> <mi>c</mi> <mi>o</mi> <mi>r</mi>
    <mi>e</mi> <mo>(</mo> <mi>p</mi> <mi>o</mi> <mi>s</mi> <mo>)</mo> <mo>+</mo> <mi>s</mi>
    <mi>c</mi> <mi>o</mi> <mi>r</mi> <mi>e</mi> <mo>(</mo> <mi>n</mi> <mi>e</mi> <mi>g</mi>
    <mo>)</mo> <mo>)</mo></mrow></math>
- en: With a linear model, if <math alttext="upper X Subscript p Baseline o s"><mrow><msub><mi>X</mi>
    <mi>p</mi></msub> <mi>o</mi> <mi>s</mi></mrow></math> is the positive item vector,
    and <math alttext="upper X Subscript n Baseline e g"><mrow><msub><mi>X</mi> <mi>n</mi></msub>
    <mi>e</mi> <mi>g</mi></mrow></math> is the negative item vector, then we learn
    <math alttext="upper W"><mi>W</mi></math> , where <math alttext="upper W upper
    X Subscript p Baseline o s minus upper W upper X Subscript n Baseline e g greater-than
    1"><mrow><mi>W</mi> <msub><mi>X</mi> <mi>p</mi></msub> <mi>o</mi> <mi>s</mi> <mo>-</mo>
    <mi>W</mi> <msub><mi>X</mi> <mi>n</mi></msub> <mi>e</mi> <mi>g</mi> <mo>></mo>
    <mn>1</mn></mrow></math> . The loss for this is [hinge loss](https://oreil.ly/88zk3),
    where the predictor output is <math alttext="upper W upper X Subscript p Baseline
    o s minus upper W upper X Subscript n Baseline e g"><mrow><mi>W</mi> <msub><mi>X</mi>
    <mi>p</mi></msub> <mi>o</mi> <mi>s</mi> <mo>-</mo> <mi>W</mi> <msub><mi>X</mi>
    <mi>n</mi></msub> <mi>e</mi> <mi>g</mi></mrow></math> and the target is 1.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用线性模型时，如果<math alttext="upper X Subscript p Baseline o s"><mrow><msub><mi>X</mi>
    <mi>p</mi></msub> <mi>o</mi> <mi>s</mi></mrow></math>是正向项目向量，而<math alttext="upper
    X Subscript n Baseline e g"><mrow><msub><mi>X</mi> <mi>n</mi></msub> <mi>e</mi>
    <mi>g</mi></mrow></math>是负向项目向量，则我们学习<math alttext="upper W"><mi>W</mi></math>，其中<math
    alttext="upper W upper X Subscript p Baseline o s minus upper W upper X Subscript
    n Baseline e g greater-than 1"><mrow><mi>W</mi> <msub><mi>X</mi> <mi>p</mi></msub>
    <mi>o</mi> <mi>s</mi> <mo>-</mo> <mi>W</mi> <msub><mi>X</mi> <mi>n</mi></msub>
    <mi>e</mi> <mi>g</mi> <mo>></mo> <mn>1</mn></mrow></math>。这种情况下的损失是[hinge loss](https://oreil.ly/88zk3)，其中预测输出为<math
    alttext="upper W upper X Subscript p Baseline o s minus upper W upper X Subscript
    n Baseline e g"><mrow><mi>W</mi> <msub><mi>X</mi> <mi>p</mi></msub> <mi>o</mi>
    <mi>s</mi> <mo>-</mo> <mi>W</mi> <msub><mi>X</mi> <mi>n</mi></msub> <mi>e</mi>
    <mi>g</mi></mrow></math>，目标为1。
- en: 'However, to compensate for the fact that an unobserved item might not be a
    true negative, just something unobserved, we count the number of times we had
    to sample from the negative set to find something that violates the ordering of
    the chosen pair. That is, we count the number of times we had to look for something
    where:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了弥补一个未观察到的项目可能不是真负向的事实，而只是未观察到的东西，我们计算了从负向集合中抽样找到违反所选对排序的次数。也就是说，我们计算了需要查找的次数，找到这样的情况：
- en: <math alttext="s c o r e left-parenthesis n e g right-parenthesis greater-than
    s c o r e left-parenthesis p o s right-parenthesis minus 1" display="block"><mrow><mi>s</mi>
    <mi>c</mi> <mi>o</mi> <mi>r</mi> <mi>e</mi> <mo>(</mo> <mi>n</mi> <mi>e</mi> <mi>g</mi>
    <mo>)</mo> <mo>></mo> <mi>s</mi> <mi>c</mi> <mi>o</mi> <mi>r</mi> <mi>e</mi> <mo>(</mo>
    <mi>p</mi> <mi>o</mi> <mi>s</mi> <mo>)</mo> <mo>-</mo> <mn>1</mn></mrow></math>
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="s c o r e left-parenthesis n e g right-parenthesis greater-than
    s c o r e left-parenthesis p o s right-parenthesis minus 1" display="block"><mrow><mi>s</mi>
    <mi>c</mi> <mi>o</mi> <mi>r</mi> <mi>e</mi> <mo>(</mo> <mi>n</mi> <mi>e</mi> <mi>g</mi>
    <mo>)</mo> <mo>></mo> <mi>s</mi> <mi>c</mi> <mi>o</mi> <mi>r</mi> <mi>e</mi> <mo>(</mo>
    <mi>p</mi> <mi>o</mi> <mi>s</mi> <mo>)</mo> <mo>-</mo> <mn>1</mn></mrow></math>
- en: We then construct a monotonically decreasing function of the number of times
    we sample the universe of items (less the positives) for a violating negative
    and look up the weight for this number and multiply the loss with it. If it’s
    very hard to find a violating negative, the gradient should therefore be lower
    because either we are close to a good solution already or the item was never seen
    before, so we should not be so confident as to assign it a low score just because
    it was never shown to the user as a result for a query.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们构建一个随着我们从未看过的项目（减去正向项目）中抽样找到违反负向的次数而单调递减的函数，并查找此次数的权重，并将损失乘以它。如果很难找到违反负向的情况，梯度应该较低，因为要么我们已经接近一个好的解决方案，要么该项目以前从未被用户显示为查询结果。
- en: Note that WARP loss was developed when CPUs were the dominant form of computation
    to train ML models. As such, an approximation to ranking was used to obtain the
    rank of a negative item. The *approximate rank* is defined as the number of samples
    with replacement in the universe of items (less the positive example) before we
    find a negative item whose score is larger than the positive by an arbitrary constant,
    called a *margin*, of 1.0.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当CPU是主要的计算形式来训练机器学习模型时，WARP损失被开发出来。因此，使用了一个排名的近似值来获得负项的排名。*近似排名*被定义为在我们找到一个负项其分数比正项大的任意常数边界1.0之前，从项目宇宙中（减去正例）抽样的次数。
- en: 'To construct the WARP weight for the pairwise loss, we need a function to go
    from the approximate rank of the negative item to the WARP weight. A relatively
    simple bit of code to compute this is as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建成对损失的WARP权重，我们需要一个函数，将负项的近似排名转换为WARP权重。计算这个相对简单的代码片段如下：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As you can see, if we find a negative immediately, the WARP weight is 1.0, but
    if it is very difficult to find a negative that violates the margin, the WARP
    weight will be small.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，如果我们立即找到一个负样本，那么WARP权重为1.0，但是如果很难找到违反间距的负样本，那么WARP权重将很小。
- en: This loss function is approximately optimizing precision@*k*, and thus a good
    step toward improving rank estimates in the retrieved set. Even better, WARP is
    computationally efficient via sampling and thus more memory efficient.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 此损失函数大致优化了precision@*k*，因此是改进检索集中排名估计的良好步骤。更好的是，通过采样，WARP在计算上是高效的，因此更节省内存。
- en: k-order Statistic
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k阶统计量
- en: Is there a way to improve upon the WARP loss and straight-up pairwise hinge
    loss? Turns out there are a whole spectrum of ways. In [“Learning to Rank Recommendations
    with the k-order Statistic Loss”](https://oreil.ly/afphG), Jason Weston et al.
    (including one of this book’s coauthors) show how this can be done by exploring
    the variants of losses between hinge loss and WARP loss. The authors of the paper
    conducted experiments on various corpora and show how the trade-off between optimizing
    for a single pairwise versus selecting a harder negative like WARP affects metrics
    including mean rank and precision and recall at *k*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 有没有办法改进WARP损失和直接成对铰链损失？事实证明，有整个一系列方法。在[“使用k阶统计量损失学习排序推荐”](https://oreil.ly/afphG)，Jason
    Weston等人（包括本书的其中一位合著者）展示了如何通过探索铰链损失和WARP损失之间的变体来完成这一点。本文的作者在各种语料库上进行了实验，并展示了在优化单个成对与选择像WARP这样的更难的负样本之间的权衡如何影响包括平均排名和precision和recall在*k*上的度量。
- en: The key generalization is that instead of a single positive item considered
    during the gradient step, the model uses all of them.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的一般化是，在梯度步骤期间不是考虑单个正项目，而是使用所有正项目。
- en: Recall again that picking a random positive and a random negative pair optimizes
    for the ROC, or AUC. This isn’t great for ranking because it doesn’t optimize
    for the top of the list. WARP loss, on the other hand, optimizes for the top of
    the ranking list for a single positive item but does not specify how to pick the
    positive item.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 再次回顾，随机选择一个正样本和一个负样本对优化ROC或AUC。这对排名不是很好，因为它不会优化列表的顶部。另一方面，WARP损失会优化单个正项目的排名列表的顶部，但不会指定如何选择正项目。
- en: Several alternate strategies can be used for ordering the top of the list, including
    optimizing for mean maximum rank, which tries to group the positive items such
    that the lowest-scoring positive item is as near the top of the list as possible.
    To allow this ordering, we provide a probability distribution function over how
    we pick the positive sample. If the probability is skewed toward the top of the
    positive item list, we get a loss more like WARP loss. If the probability is uniform,
    we get AUC loss. If the probability is skewed toward the end of the positive item
    list, we then optimize for the worst case, like mean maximum rank. The NumPy function
    `np.random.choice` provides a mechanism from sampling from a distribution <math
    alttext="upper P"><mi>P</mi></math> .
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用几种备选策略来对列表顶部进行排序，包括优化均值最大排名，该策略试图将正项目分组，使得得分最低的正项目尽可能靠近列表顶部。为了允许这种排序，我们提供了一个概率分布函数，用于解释我们如何选择正样本。如果概率偏向于正项目列表的顶部，我们会得到类似于WARP损失的损失。如果概率是均匀的，我们会得到AUC损失。如果概率偏向于正项目列表的末尾，那么我们将优化最坏情况，就像均值最大排名一样。NumPy函数`np.random.choice`提供了一种从分布中进行采样的机制<math
    alttext="upper P"><mi>P</mi></math> 。
- en: 'We have one more optimization to consider: <math alttext="upper K"><mi>K</mi></math>
    , the number of positive samples to use to construct the positive set. If <math
    alttext="upper K equals 1"><mrow><mi>K</mi> <mo>=</mo> <mn>1</mn></mrow></math>
    , we pick only a positive random item from the positive set; otherwise, we construct
    the positive set, order the samples by score, and sample from the positive list
    of size <math alttext="upper K"><mi>K</mi></math> by using the probability distribution
    <math alttext="upper P"><mi>P</mi></math> . This optimization made sense in the
    era of CPUs when compute was expensive but might not make that much sense these
    days in the era of GPUs and TPUs, which we will talk about in the following warning.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有一个优化考虑： <math alttext="upper K"><mi>K</mi></math> ，用于构建正样本集的正样本数量。如果 <math
    alttext="upper K equals 1"><mrow><mi>K</mi> <mo>=</mo> <mn>1</mn></mrow></math>
    ，我们只从正样本集中随机选择一个正样本；否则，我们按分数对样本进行排序，并使用概率分布 <math alttext="upper P"><mi>P</mi></math>
    从大小为 <math alttext="upper K"><mi>K</mi></math> 的正列表中采样。这种优化在CPU时代是有意义的，因为计算成本昂贵，但在GPU和TPU时代可能不再那么合理，接下来我们会在下面的警告中讨论这一点。
- en: Stochastic Losses and GPUs
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机损失和GPU
- en: A word of caution about the preceding stochastic losses. They were developed
    for an earlier era of CPUs when it was cheap and easy to sample and exit if a
    negative sample was found. These days, with modern GPUs, making branching decisions
    like this is harder because all the threads on the GPU core have to run the same
    code over different data in parallel. That usually means both sides of a branch
    are taken in a batch, so less computational savings occur from these early exits.
    Consequently, branching code that approximates stochastic losses like WARP and
    *k*-order statistic loss appear less efficient.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 关于上述随机损失需要注意的一点。它们是为早期CPU时代开发的，那时对样本进行抽样并且在发现负样本时退出是廉价且简单的。而在现代GPU时代，做出类似这样的分支决策更加困难，因为GPU核心上的所有线程必须并行运行相同的代码，但在不同的数据上。这通常意味着分支的两侧都会在一个批次中执行，因此这些早期退出的计算节省效果较少。因此，像WARP和*k*阶统计损失这样的近似随机损失的分支代码看起来不那么高效。
- en: What are we to do? We will show in [Chapter 13](ch13.html#ch:spotify) how to
    approximate these losses in code. Long story short, because of the way vector
    processors like GPUs tend to work by processing lots of data in parallel uniformly,
    we have to find a GPU-friendly way to compute these losses. In the next chapter,
    we approximate the negative sampling by generating a large batch of negatives
    and either scoring them all lower than the negative or looking for the most egregious
    violating negative or both together as a blend of loss functions.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们该怎么办？我们将在[第13章](ch13.html#ch:spotify)中展示如何在代码中近似这些损失。长话短说，由于像GPU这样的向量处理器通常通过并行均匀地处理大量数据来工作，我们必须找到一种适合GPU的方式来计算这些损失。在下一章中，我们通过生成大批负样本并且要么将它们全部评分低于负样本，要么寻找最明显违反负样本，或者两者混合作为损失函数的一部分来近似负采样。
- en: BM25
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BM25
- en: While much of this book is targeted at recommending items to users, search ranking
    is a close sister study. In the space of information retrieval, or search ranking
    for documents, *best matching 25* (BM25) is an essential tool.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这本书的大部分内容都是针对向用户推荐物品，但搜索排名是一个紧密相关的研究领域。在信息检索或文档搜索排名空间中，*最佳匹配25*（BM25）是一个必不可少的工具。
- en: BM25 is an algorithm used in information-retrieval systems to rank documents
    based on their relevance to a given query. This relevance is determined by considering
    factors like TF-IDF. It’s a bag-of-words retrieval function that ranks a set of
    documents based on the query terms appearing in each document. It’s also a part
    of the probabilistic relevance framework and is derived from the probabilistic
    retrieval model.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: BM25是信息检索系统中用于根据其与给定查询的相关性对文档进行排名的算法。这种相关性是通过考虑诸如TF-IDF等因素来确定的。它是一个基于词袋模型的检索函数，根据每个文档中出现的查询词来对一组文档进行排名。它还是概率相关性框架的一部分，并且源自概率检索模型。
- en: The BM25 ranking function calculates a score for each document based on the
    query. The document with the highest score is considered the most relevant to
    the query.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: BM25排名函数根据查询为每个文档计算一个分数。得分最高的文档被认为与查询最相关。
- en: 'Here is a simplified version of the BM25 formula:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是BM25公式的简化版本：
- en: <math alttext="left-brace score right-brace left-parenthesis upper D comma upper
    Q right-parenthesis equals sigma-summation Underscript i equals 1 Overscript n
    Endscripts left-brace IDF right-brace left-parenthesis q Subscript i Baseline
    right-parenthesis asterisk StartStartFraction f left-parenthesis q Subscript i
    Baseline comma upper D right-parenthesis asterisk left-parenthesis k Baseline
    1 plus 1 right-parenthesis OverOver f left-parenthesis q Subscript i Baseline
    comma upper D right-parenthesis plus k 1 asterisk left-parenthesis 1 minus b plus
    b asterisk StartFraction StartAbsoluteValue upper D EndAbsoluteValue Over left-brace
    avgdl right-brace EndFraction right-parenthesis EndEndFraction" display="block"><mrow><mtext>score</mtext>
    <mrow><mo>(</mo> <mi>D</mi> <mo>,</mo> <mi>Q</mi> <mo>)</mo></mrow> <mo>=</mo>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover>
    <mtext>IDF</mtext> <mrow><mo>(</mo> <msub><mi>q</mi> <mi>i</mi></msub> <mo>)</mo></mrow>
    <mo>*</mo> <mfrac><mrow><mi>f</mi><mrow><mo>(</mo><msub><mi>q</mi> <mi>i</mi></msub>
    <mo>,</mo><mi>D</mi><mo>)</mo></mrow><mo>*</mo><mrow><mo>(</mo><mi>k</mi><mn>1</mn><mo>+</mo><mn>1</mn><mo>)</mo></mrow></mrow>
    <mrow><mi>f</mi><mrow><mo>(</mo><msub><mi>q</mi> <mi>i</mi></msub> <mo>,</mo><mi>D</mi><mo>)</mo></mrow><mo>+</mo><mi>k</mi><mn>1</mn><mo>*</mo><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mi>b</mi><mo>+</mo><mi>b</mi><mo>*</mo><mfrac><mrow><mo>|</mo><mi>D</mi><mo>|</mo></mrow>
    <mtext>avgdl</mtext></mfrac><mo>)</mo></mrow></mrow></mfrac></mrow></math>
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="left-brace score right-brace left-parenthesis upper D comma upper
    Q right-parenthesis equals sigma-summation Underscript i equals 1 Overscript n
    Endscripts left-brace IDF right-brace left-parenthesis q Subscript i Baseline
    right-parenthesis asterisk StartStartFraction f left-parenthesis q Subscript i
    Baseline comma upper D right-parenthesis asterisk left-parenthesis k Baseline
    1 plus 1 right-parenthesis OverOver f left-parenthesis q Subscript i Baseline
    comma upper D right-parenthesis plus k 1 asterisk left-parenthesis 1 minus b plus
    b asterisk StartFraction StartAbsoluteValue upper D EndAbsoluteValue Over left-brace
    avgdl right-brace EndFraction right-parenthesis EndEndFraction" display="block"><mrow><mtext>score</mtext>
    <mrow><mo>(</mo> <mi>D</mi> <mo>,</mo> <mi>Q</mi> <mo>)</mo></mrow> <mo>=</mo>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover>
    <mtext>IDF</mtext> <mrow><mo>(</mo> <msub><mi>q</mi> <mi>i</mi></msub> <mo>)</mo></mrow>
    <mo>*</mo> <mfrac><mrow><mi>f</mi><mrow><mo>(</mo><msub><mi>q</mi> <mi>i</mi></msub>
    <mo>,</mo><mi>D</mi><mo>)</mo></mrow><mo>*</mo><mrow><mo>(</mo><mi>k</mi><mn>1</mn><mo>+</mo><mn>1</mn><mo>)</mo></mrow></mrow>
    <mrow><mi>f</mi><mrow><mo>(</mo><msub><mi>q</mi> <mi>i</mi></msub> <mo>,</mo><mi>D</mi><mo>)</mo></mrow><mo>+</mo><mi>k</mi><mn>1</mn><mo>*</mo><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mi>b</mi><mo>+</mo><mi>b</mi><mo>*</mo><mfrac><mrow><mo>|</mo><mi>D</mi><mo>|</mo></mrow>
    <mtext>avgdl</mtext></mfrac><mo>)</mo></mrow></mrow></mfrac></mrow></math>
- en: 'The elements of this formula are as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式的要素如下：
- en: <math alttext="upper D"><mi>D</mi></math> represents a document.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math alttext="upper D"><mi>D</mi></math> 代表一个文档。
- en: <math alttext="upper Q"><mi>Q</mi></math> is the query that consists of words
    <math alttext="StartSet q 1 comma q 2 comma period period period comma q Subscript
    n Baseline EndSet"><mfenced close="}" open="{" separators=""><msub><mi>q</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>q</mi> <mn>2</mn></msub> <mo>,</mo> <mo>.</mo>
    <mo>.</mo> <mo>.</mo> <mo>,</mo> <msub><mi>q</mi> <mi>n</mi></msub></mfenced></math>
    .
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math alttext="upper Q"><mi>Q</mi></math> 是由词组成的查询，包括单词 <math alttext="StartSet
    q 1 comma q 2 comma period period period comma q Subscript n Baseline EndSet"><mfenced
    close="}" open="{" separators=""><msub><mi>q</mi> <mn>1</mn></msub> <mo>,</mo>
    <msub><mi>q</mi> <mn>2</mn></msub> <mo>,</mo> <mo>.</mo> <mo>.</mo> <mo>.</mo>
    <mo>,</mo> <msub><mi>q</mi> <mi>n</mi></msub></mfenced></math> 。
- en: <math alttext="f left-parenthesis q i comma upper D right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>q</mi> <mi>i</mi> <mo>,</mo> <mi>D</mi> <mo>)</mo></mrow></math>
    is the frequency of query term <math alttext="q Subscript i"><msub><mi>q</mi>
    <mi>i</mi></msub></math> in document <math alttext="upper D"><mi>D</mi></math>
    .
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math alttext="f left-parenthesis q i comma upper D right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>q</mi> <mi>i</mi> <mo>,</mo> <mi>D</mi> <mo>)</mo></mrow></math>
    是查询项 <math alttext="q Subscript i"><msub><mi>q</mi> <mi>i</mi></msub></math> 在文档
    <math alttext="upper D"><mi>D</mi></math> 中的频率。
- en: <math alttext="StartAbsoluteValue upper D EndAbsoluteValue"><mrow><mo>|</mo>
    <mi>D</mi> <mo>|</mo></mrow></math> is the length of (the number of words in)
    the document <math alttext="upper D"><mi>D</mi></math> .
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math alttext="StartAbsoluteValue upper D EndAbsoluteValue"><mrow><mo>|</mo>
    <mi>D</mi> <mo>|</mo></mrow></math> 是文档 <math alttext="upper D"><mi>D</mi></math>
    的长度（单词数）。
- en: <math alttext="a v g Subscript d Baseline l"><mrow><mi>a</mi> <mi>v</mi> <msub><mi>g</mi>
    <mi>d</mi></msub> <mi>l</mi></mrow></math> is the average document length in the
    collection.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math alttext="a v g Subscript d Baseline l"><mrow><mi>a</mi> <mi>v</mi> <msub><mi>g</mi>
    <mi>d</mi></msub> <mi>l</mi></mrow></math> 是集合中的平均文档长度。
- en: '<math alttext="k 1"><msub><mi>k</mi> <mn>1</mn></msub></math> and <math alttext="b"><mi>b</mi></math>
    are hyperparameters. <math alttext="k 1"><msub><mi>k</mi> <mn>1</mn></msub></math>
    is a positive tuning parameter that calibrates the document term frequency scaling.
    <math alttext="b"><mi>b</mi></math> is a parameter that determines the scaling
    by document length: <math alttext="b equals 1"><mrow><mi>b</mi> <mo>=</mo> <mn>1</mn></mrow></math>
    corresponds to fully scaling the term weight by the document length, while <math
    alttext="b equals 0"><mrow><mi>b</mi> <mo>=</mo> <mn>0</mn></mrow></math> corresponds
    to no length normalization.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math alttext="k 1"><msub><mi>k</mi> <mn>1</mn></msub></math> 和 <math alttext="b"><mi>b</mi></math>
    是超参数。 <math alttext="k 1"><msub><mi>k</mi> <mn>1</mn></msub></math> 是正调节参数，用于校准文档词频的缩放。
    <math alttext="b"><mi>b</mi></math> 是通过文档长度决定缩放的参数： <math alttext="b equals 1"><mrow><mi>b</mi>
    <mo>=</mo> <mn>1</mn></mrow></math> 对应完全按文档长度缩放词项权重，而 <math alttext="b equals
    0"><mrow><mi>b</mi> <mo>=</mo> <mn>0</mn></mrow></math> 则表示不进行长度归一化。
- en: '<math alttext="upper I upper D upper F left-parenthesis q Subscript i Baseline
    right-parenthesis"><mrow><mi>I</mi> <mi>D</mi> <mi>F</mi> <mo>(</mo> <msub><mi>q</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></math> is the inverse document frequency of
    query term <math alttext="q Subscript i"><msub><mi>q</mi> <mi>i</mi></msub></math>
    , which measures the amount of information the word provides (whether it’s common
    or rare across all documents). BM25 applies a variant of IDF that can be computed
    as follows:'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math alttext="upper I upper D upper F left-parenthesis q Subscript i Baseline
    right-parenthesis"><mrow><mi>I</mi> <mi>D</mi> <mi>F</mi> <mo>(</mo> <msub><mi>q</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></math> 是查询项 <math alttext="q Subscript i"><msub><mi>q</mi>
    <mi>i</mi></msub></math> 的逆文档频率，用于衡量单词在所有文档中提供的信息量（无论其在文档中是常见还是罕见）。BM25 应用了一种变体的
    IDF，可以计算如下：
- en: <math alttext="left-brace IDF right-brace left-parenthesis q Subscript i Baseline
    right-parenthesis equals log left-parenthesis StartFraction upper N minus n left-parenthesis
    q Subscript i Baseline right-parenthesis plus 0.5 Over n left-parenthesis q Subscript
    i Baseline right-parenthesis plus 0.5 EndFraction right-parenthesis" display="block"><mrow><mtext>IDF</mtext>
    <mrow><mo>(</mo> <msub><mi>q</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>=</mo>
    <mo form="prefix">log</mo> <mfenced close=")" open="(" separators=""><mfrac><mrow><mi>N</mi><mo>-</mo><mi>n</mi><mo>(</mo><msub><mi>q</mi>
    <mi>i</mi></msub> <mo>)</mo><mo>+</mo><mn>0</mn><mo>.</mo><mn>5</mn></mrow> <mrow><mi>n</mi><mo>(</mo><msub><mi>q</mi>
    <mi>i</mi></msub> <mo>)</mo><mo>+</mo><mn>0</mn><mo>.</mo><mn>5</mn></mrow></mfrac></mfenced></mrow></math>
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <math alttext="left-brace IDF right-brace left-parenthesis q Subscript i Baseline
    right-parenthesis equals log left-parenthesis StartFraction upper N minus n left-parenthesis
    q Subscript i Baseline right-parenthesis plus 0.5 Over n left-parenthesis q Subscript
    i Baseline right-parenthesis plus 0.5 EndFraction right-parenthesis" display="block"><mrow><mtext>IDF</mtext>
    <mrow><mo>(</mo> <msub><mi>q</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>=</mo>
    <mo form="prefix">log</mo> <mfenced close=")" open="(" separators=""><mfrac><mrow><mi>N</mi><mo>-</mo><mi>n</mi><mo>(</mo><msub><mi>q</mi>
    <mi>i</mi></msub> <mo>)</mo><mo>+</mo><mn>0</mn><mo>.</mo><mn>5</mn></mrow> <mrow><mi>n</mi><mo>(</mo><msub><mi>q</mi>
    <mi>i</mi></msub> <mo>)</mo><mo>+</mo><mn>0</mn><mo>.</mo><mn>5</mn></mrow></mfrac></mfenced></mrow></math>
- en: Here, <math alttext="upper N"><mi>N</mi></math> is the total number of documents
    in the collection, and <math alttext="n left-parenthesis q Subscript i Baseline
    right-parenthesis"><mrow><mi>n</mi> <mo>(</mo> <msub><mi>q</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow></math> is the number of documents containing <math alttext="q
    Subscript i"><msub><mi>q</mi> <mi>i</mi></msub></math> .
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，<math alttext="upper N"><mi>N</mi></math> 是集合中的文档总数，<math alttext="n left-parenthesis
    q Subscript i Baseline right-parenthesis"><mrow><mi>n</mi> <mo>(</mo> <msub><mi>q</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></math> 是包含查询项 <math alttext="q Subscript i"><msub><mi>q</mi>
    <mi>i</mi></msub></math> 的文档数。
- en: Simply, BM25 combines both term frequency (how often a term appears in a document)
    and inverse document frequency (how much unique information a term provides) to
    calculate the relevance score. It also introduces the concept of document length
    normalization, penalizing too-long documents and preventing them from dominating
    shorter ones, which is a common issue in simple TF-IDF models. The free parameters
    <math alttext="k 1"><msub><mi>k</mi> <mn>1</mn></msub></math> and <math alttext="b"><mi>b</mi></math>
    allow the model to be tuned based on the specific characteristics of the document
    set.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，BM25结合了词项频率（术语在文档中出现的频率）和逆文档频率（术语提供的唯一信息量大小）来计算相关性分数。它还引入了文档长度归一化的概念，惩罚过长的文档，并防止它们在简短文档面前占据主导地位，这是简单TF-IDF模型中常见的问题。自由参数
    <math alttext="k 1"><msub><mi>k</mi> <mn>1</mn></msub></math> 和 <math alttext="b"><mi>b</mi></math>
    允许根据文档集的特定特性进行调整。
- en: In practice, BM25 provides a robust baseline for most information-retrieval
    tasks, including ad hoc keyword search and document similarity. BM25 is used in
    many open source search engines, such as Lucene and Elasticsearch, and is the
    de facto standard for what is often called *full-text search*.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，BM25为大多数信息检索任务提供了强大的基线，包括即时关键字搜索和文档相似性。BM25被许多开源搜索引擎（如Lucene和Elasticsearch）使用，并成为通常所称的*全文搜索*的事实标准。
- en: So how might we integrate BM25 into the problems we discuss in this book? The
    output from BM25 is a list of documents ranked by relevance to the given query,
    and then LTR comes into play. You can use the BM25 score as one of the features
    in an LTR model, along with other features that you believe might influence the
    relevance of a document to a query.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何将BM25集成到本书中讨论的问题中呢？BM25的输出是根据给定查询排名的文档列表，然后LTR发挥作用。您可以将BM25分数作为LTR模型中的一个特征，以及您认为可能影响文档与查询相关性的其他特征。
- en: 'The general steps to combine BM25 with LTR for ranking are as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 将BM25与LTR结合进行排名的一般步骤如下：
- en: '*Retrieve a list of candidate documents*. Given a query, use BM25 to retrieve
    a list of candidate documents.'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*检索候选文档列表*。给定一个查询，使用BM25来检索候选文档列表。'
- en: '*Compute features for each document*. Compute the BM25 score as one of the
    features, along with other potential features. This could include various document-specific
    features, query-document match features, user interaction features, etc.'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*为每个文档计算特征*。计算BM25分数作为其中一个特征，以及其他潜在的特征。这可能包括各种文档特定特征、查询-文档匹配特征、用户交互特征等。'
- en: '*Train/evaluate the LTR model*. Use these feature vectors and their corresponding
    labels (relevance judgments) to train your LTR model. Or, if you already have
    a trained model, use it to evaluate and rank the retrieved documents.'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*训练/评估LTR模型*。使用这些特征向量及其对应的标签（相关性评判）来训练您的LTR模型。或者，如果您已经有了训练好的模型，可以使用它来评估和排名检索到的文档。'
- en: '*Rank*. The LTR model generates a score for each document. Rank the documents
    based on these scores.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*排名*。LTR模型为每个文档生成一个分数。根据这些分数对文档进行排名。'
- en: This combination of retrieval (with BM25) and ranking (with LTR) allows you
    to first narrow the potential candidate documents from a possibly very large collection
    (where BM25 shines) and then fine-tune the ranking of these candidates with a
    model that can consider more complex features and interactions (where LTR shines).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用BM25进行检索和LTR进行排名的组合，您可以首先从可能非常庞大的文档集合中缩小潜在的候选文档范围（其中BM25表现突出），然后利用一个可以考虑更复杂特征和交互的模型来微调这些候选文档的排名（其中LTR表现突出）。
- en: It is worth mentioning that the BM25 score can provide a strong baseline in
    text document retrieval, and depending on the complexity of the problem and the
    amount of training data you have, LTR may or may not provide significant improvements.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，BM25分数可以为文本文档检索提供强大的基线，取决于问题的复杂性和您拥有的训练数据量，LTR可能会或者不会提供显著的改进。
- en: Multimodal Retrieval
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多模态检索
- en: 'Let’s take another look at this retrieval method, as we can find some powerful
    leverage. Think back to [Chapter 8](ch08.html#ch:wikipedia-e2e): we built a co-occurrence
    model, which illustrated how articles referenced jointly in other articles share
    meaning and mutual relevance. But how would you integrate search into this?'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视一下这种检索方法，因为我们可以找到一些强大的优势。回想一下[第8章](ch08.html#ch:wikipedia-e2e)：我们构建了一个共现模型，展示了在其他文章中共同引用的文章如何共享含义和相互相关性。但是，您如何将搜索集成到这个过程中呢？
- en: You may think, “Oh, I can search the names of the articles.” But that doesn’t
    quite utilize our co-occurrence model; it underleverages that joint meaning we
    discovered. A classic approach may be to use something like BM25 on article titles
    or articles. More modern approaches may do a vector embedding of the query and
    article titles (using something like BERT or other transformer models). However,
    neither of these really capture both sides of what we’re looking for.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，“哦，我可以搜索文章的名称。” 但这并没有充分利用我们的共现模型；它未充分利用我们发现的联合含义。一个经典的方法可能是在文章标题或文章上使用类似BM25的东西。更现代的方法可能会对查询和文章标题进行向量嵌入（使用类似BERT或其他变换器模型的东西）。然而，这两者都没有真正捕捉到我们寻找的两面。
- en: 'Consider instead the following approach:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑改用以下方法：
- en: Search with the initial query via BM25 to get an initial set of “anchors.”
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过BM25使用初始查询进行搜索，以获取初始的“锚点”集。
- en: Search with each anchor as a query via your latent model(s).
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过您的潜在模型以每个锚点作为查询进行搜索。
- en: Train an LTR model to aggregate and rank the union of the searches.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个LTR模型来聚合和排名搜索的并集。
- en: Now we’re using a true multimodal retrieval, leveraging multiple latent spaces!
    One additional highlight in this approach is that queries are often out of distribution
    from documents with respect to encoder-based latent spaces. This means that when
    you type **`Who’s the leader of Mozambique?`**, this question looks fairly dissimilar
    to the article title (Mozambique) or the relevant sentence as of summer 2023 (“The
    new government under President Samora Machel established a one-party state based
    on Marxist principles.”)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们正在使用真正的多模式检索，利用多个潜在空间！这种方法的一个额外亮点是查询通常在基于编码器的潜在空间中与文档的分布不同。这意味着当你输入**`谁是莫桑比克的领导人？`**时，这个问题看起来与文章标题（莫桑比克）或2023年夏季相关的句子（“萨莫拉·马歇尔总统领导下的新政府建立了一个基于马克思主义原则的一党制国家。”）相当不同。
- en: 'When the embeddings are not text at all, this method becomes even more powerful:
    consider typing text to search for an item of clothing and hoping to see an entire
    outfit that goes with it.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当嵌入根本不是文本时，这种方法变得更加强大：考虑输入文本搜索服装项目，并希望看到与之搭配的整套服装。
- en: Summary
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Getting things in the right order is an important aspect of recommendation systems.
    By now, you know that ordering is not the whole story, but it’s an essential step
    in the pipeline. We’ve collected our items and put them in the right order, and
    all that’s left to do is send them off to the user.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 将事物放入正确顺序是推荐系统中的一个重要方面。到目前为止，您知道排序并不是全部内容，但它是管道中的一个关键步骤。我们已经收集了我们的物品并将它们放在正确的顺序中，剩下的就是把它们发送给用户。
- en: We started with the most fundamental concept, learning to rank, and compared
    it with some traditional methods. We then got a big upgrade with WARP and WSABIE.
    That led us to the *k*-order statistic, which involves utilizing more careful
    probabilistic sampling. We finally wrapped up with BM25 as a powerful baseline
    in text settings.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从最基本的概念开始，学习排名，并将其与一些传统方法进行比较。然后我们通过WARP和WSABIE进行了大幅升级。这使我们最终使用了*k*-order统计量，这涉及到更谨慎的概率抽样。最后，我们以BM25作为文本设置中强大的基线结束。
- en: Before we conquer serving, let’s put these pieces together. In the next chapter,
    we’re going to turn up the volume and build some playlists. This will be the most
    intensive chapter yet, so go grab a beverage and a stretch. We’ve got some work
    to do.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们征服服务之前，让我们把这些要素放在一起。在下一章中，我们将加大音量，并创建一些播放列表。这将是最密集的一章，所以去拿杯饮料伸展一下。我们有些工作要做。
