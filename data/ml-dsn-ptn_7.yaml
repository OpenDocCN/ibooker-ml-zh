- en: Chapter 7\. Responsible AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第七章。负责任的人工智能
- en: Until this point, we’ve focused on patterns designed to help data and engineering
    teams prepare, build, train, and scale models for production use. These patterns
    mainly addressed teams directly involved in the ML model development process.
    Once a model is in production, its impact extends far beyond the teams who built
    it. In this chapter, we’ll discuss the other *stakeholders* of a model, both those
    within and outside of an organization. Stakeholders could include executives whose
    business objectives dictate a model’s goals, the end users of a model, auditors,
    and compliance regulators.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们专注于设计模式，以帮助数据和工程团队为生产使用准备、构建、训练和扩展模型。这些模式主要是为直接参与 ML 模型开发过程的团队设计的。一旦模型投入生产，它的影响将远远超出建造它的团队。在本章中，我们将讨论模型的其他*利益相关者*，包括组织内外的人员。利益相关者可能包括制定模型目标的业务执行人员，模型的最终用户，审计员和合规监管者。
- en: 'There are several groups of model stakeholders we’ll be referring to in this
    chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们将提到几个模型利益相关者群体：
- en: Model builders
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 模型构建者
- en: Data scientists and ML researchers directly involved in building ML models.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 直接参与构建 ML 模型的数据科学家和 ML 研究人员。
- en: ML engineers
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ML 工程师
- en: Members of ML Ops teams directly involved in deploying ML models.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 直接参与部署 ML 模型的 ML 运维团队成员。
- en: Business decision makers
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 业务决策者
- en: Decide whether or not to incorporate the ML model into their business processes
    or customer-facing applications and will need to evaluate whether the model is
    fit for this purpose.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 决定是否将 ML 模型纳入其业务流程或面向客户的应用程序，并需要评估模型是否适合此目的。
- en: End users of ML systems
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ML 系统的最终用户
- en: 'Make use of predictions from an ML model. There are many different types of
    model end users: customers, employees, and hybrids of these. Examples include
    a customer getting a movie recommendation from a model, an employee on a factory
    floor using a visual inspection model to determine whether a product is broken,
    or a medical practitioner using a model to aid in patient diagnosis.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 利用 ML 模型的预测。模型的最终用户有多种类型：客户、员工以及两者的混合体。例如，客户从模型中获得电影推荐，工厂车间的员工使用视觉检测模型来判断产品是否损坏，或者医务人员使用模型辅助患者诊断。
- en: Regulatory and compliance agencies
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 监管和合规机构
- en: People and organizations who need an executive-level summary of how a model
    is making decisions from a regulatory compliance perspective. This could include
    financial auditors, government agencies, or governance teams within an organization.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 需要模型决策的高级摘要的人员和组织，从监管合规的角度来看。这可能包括财务审计员、政府机构或组织内的治理团队。
- en: Throughout this chapter, we’ll look at patterns that address a model’s impact
    on individuals and groups outside the team and organization building a model.
    The *Heuristic Benchmark* design pattern provides a way of putting the model’s
    performance in a context that end users and decision makers can understand. The
    *Explainable Predictions* pattern provides approaches to improving trust in ML
    systems by fostering an understanding of the signals a model is using to make
    predictions. The *Fairness Lens* design pattern aims to ensure that models behave
    equitably across different subsets of users and prediction scenarios.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨模型对团队和组织外个人和团体的影响的模式。*启发式基准*设计模式提供了一种将模型的性能置于最终用户和决策者可以理解的背景中的方式。*可解释预测*模式通过促进对模型用于进行预测所使用的信号的理解，提供了提高对
    ML 系统信任的方法。*公平性视角*设计模式旨在确保模型在不同用户子集和预测场景中表现公平。
- en: Taken together, the patterns in this chapter fall under the practice of [*Responsible
    AI*](https://oreil.ly/MlJkM). This is an area of active research and is concerned
    with the best ways to build fairness, interpretability, privacy, and security
    into AI systems. Recommended practices for responsible AI include employing a
    human-centered design approach by engaging with a diverse set of users and use-case
    scenarios throughout project development, understanding the limitations of datasets
    and models, and continuing to monitor and update ML systems after deployment.
    Responsible AI patterns are not limited to the three that we discuss in this chapter—many
    of the patterns in earlier chapters (like Continuous Evaluation, Repeatable Splitting,
    and Neutral Class, to name a few) provide methods to implement these recommended
    practices and attain the goal of building fairness, interpretability, privacy,
    and security into AI systems.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 综合起来，本章的模式属于[*负责任的AI*](https://oreil.ly/MlJkM)实践。这是一个积极研究的领域，关注如何最佳地将公平性、可解释性、隐私性和安全性融入AI系统。负责任的AI推荐实践包括采用人本设计方法，在项目开发过程中与多样化用户和使用案例场景进行互动，理解数据集和模型的局限性，并在部署后继续监测和更新ML系统。负责任的AI模式不仅限于本章讨论的三种——早期章节的许多模式（如连续评估、可重复分割和中立类等）提供了实施这些推荐实践和实现将公平性、可解释性、隐私性和安全性融入AI系统目标的方法。
- en: 'Design Pattern 28: Heuristic Benchmark'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计模式28：启发式基准
- en: The Heuristic Benchmark pattern compares an ML model against a simple, easy-to-understand
    heuristic in order to explain the model’s performance to business decision makers.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 启发式基准模式将ML模型与一个简单易懂的启发式进行比较，以便向业务决策者解释模型的性能。
- en: Problem
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: 'Suppose a bicycle rental agency wishes to use the expected duration of rentals
    to build a dynamic pricing solution. After training an ML model to predict the
    duration of a bicycle’s rental period, they evaluate the model on a test dataset
    and determine that the mean absolute error (MAE) of the trained ML model is 1,200
    seconds. When they present this model to the business decision makers, they will
    likely be asked: “Is an MAE of 1,200 seconds good or bad?” This is a question
    we need to be ready to handle whenever we develop a model and present it to business
    stakeholders. If we train an image classification model on items in a product
    catalog and the mean average precision (MAP) is 95%, we can expect to be asked:
    “Is a MAP of 95% good or bad?”'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一家自行车租赁公司希望利用租赁预期持续时间来建立动态定价解决方案。在训练ML模型以预测自行车租赁期间的持续时间后，他们在测试数据集上评估模型，并确定训练ML模型的平均绝对误差（MAE）为1,200秒。当他们向业务决策者展示此模型时，他们可能会问：“一个1,200秒的MAE是好还是坏？”这是我们在开发模型并向业务利益相关者展示时需要准备好处理的问题。如果我们在产品目录中的物品上训练图像分类模型，并且平均精度（MAP）为95％，我们可以预计会被问到：“95％的MAP是好还是坏？”
- en: It is no good to wave our hands and say that this depends on the problem. Of
    course, it does. So, what is a good MAE for the bicycle rental problem in New
    York City? How about in London? What is a good MAP for the product catalog image
    classification task?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于挥手并说这取决于问题是没有好处的。当然，它确实如此。那么，在纽约市的自行车租赁问题中什么是一个好的MAE？在伦敦呢？对于产品目录图像分类任务，一个好的MAP是多少呢？
- en: Model performance is typically stated in terms of cold, hard numbers that are
    difficult for end users to put into context. Explaining the formula for MAP, MAE,
    and so on does not provide the intuition that business decision makers are asking
    for.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 模型性能通常以难以让最终用户放入背景的冷冰冰的数字来表述。解释MAP、MAE等的公式并不能提供业务决策者所需的直觉。
- en: Solution
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: If this is the second ML model being developed for a task, an easy answer is
    to compare the model’s performance against the currently operational version.
    It is quite easy to say that the MAE is now 30 seconds lower or that the MAP is
    1% higher. This works even if the current production workflow doesn’t use ML.
    As long as this task is already being performed in production and evaluation metrics
    are being collected, we can compare the performance of our new ML model against
    the current production methodology.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是为某项任务开发的第二个ML模型，一个简单的答案是将模型的性能与当前运行版本进行比较。我们可以轻松地说MAE现在降低了30秒，或者MAP高出1%。即使当前的生产工作流程不使用ML，只要正在进行生产和收集评估指标，我们就可以比较我们的新ML模型与当前生产方法的性能。
- en: But what if there is no current production methodology in place, and we are
    building the very first model for a green-field task? In such cases, the solution
    is to create a simple benchmark for the sole purpose of comparing against our
    newly developed ML model. We call this a *heuristic benchmark*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果当前没有现有的生产方法，而我们正在为一个全新的任务构建第一个模型呢？在这种情况下，解决方案是为了与我们新开发的机器学习模型进行比较而创建一个简单的基准。我们称之为*启发式基准*。
- en: A good heuristic benchmark should be intuitively easy to understand and relatively
    trivial to compute. If we find ourselves defending or debugging the algorithm
    used by the benchmark, we should search for a simpler, more understandable one.
    Good examples of a heuristic benchmark are constants, rules of thumb, or bulk
    statistics (such as the mean, median, or mode). Avoid the temptation to train
    even a simple machine learning model, such as a linear regression, on a dataset
    and use that as a benchmark—linear regression is likely not intuitive enough,
    especially once we start to include categorical variables, more than a handful
    of inputs, or engineered features.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的启发式基准应该在直觉上易于理解并且相对容易计算。如果我们发现自己在为基准使用的算法进行辩护或调试，我们应该寻找一个更简单、更易理解的基准。启发式基准的好例子包括常数、经验法则或者大量统计数据（如均值、中位数或众数）。避免诱惑去训练一个简单的机器学习模型，比如线性回归，用其作为基准——尤其是一旦我们开始包含分类变量、超过少数输入或工程特征时，线性回归可能不够直观。
- en: Warning
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Do not use a heuristic benchmark if there is already an operational practice
    in place. Instead, we should compare our model against that existing standard.
    The existing operational practice does not need to use ML—it is simply whatever
    technique is currently being used to solve the problem.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果已经有一个操作实践，请不要使用启发式基准。相反，我们应该将我们的模型与现有的标准进行比较。现有的操作实践不需要使用机器学习——它只是当前用于解决问题的任何技术。
- en: Examples of good heuristic benchmarks and situations where we might employ them
    are shown in [Table 7-1](#heuristic_benchmarks_for_a_few_selected). Example code
    for the implementations of these heuristic benchmarks is [in the GitHub repository](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/07_responsible_ai/heuristic_benchmark.ipynb)
    of this book.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 显示了良好的启发式基准示例以及我们可能在其中应用它们的情况的示例，详见[表 7-1](#heuristic_benchmarks_for_a_few_selected)。这本书的GitHub仓库中包含了这些启发式基准实现的示例代码。
- en: Table 7-1\. Heuristic benchmarks for a few selected scenarios (see [code in
    GitHub](https://oreil.ly/WoESU))
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7-1\. 一些选定场景的启发式基准（参见[GitHub 中的代码](https://oreil.ly/WoESU)）
- en: '| Scenario | Heuristic benchmark | Example task | Implementation for example
    task |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 场景 | 启发式基准 | 示例任务 | 示例任务的实现 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Regression problem where features and interactions between features are not
    well understood by the business. | Mean or median value of the label value over
    the training data. Choose the median if there are a lot of outliers. | Time interval
    before a question on Stack Overflow is answered. | Predict that it will take 2,120
    seconds always. 2,120 seconds is the median time to first answer over the entire
    training dataset. |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 特定场景中特定任务的回归问题，业务对特征及其交互不够理解。 | 在训练数据上标签值的均值或中位数。如果有很多异常值，则选择中位数。 | Stack
    Overflow 上问题被回答前的时间间隔。 | 始终预测需要 2,120 秒。这是整个训练数据集中首次回答的中位数时间。 |'
- en: '| Binary classification problem where features and interactions between features
    are not well understood by the business. | Overall fraction of positives in the
    training data. | Whether or not an accepted answer in Stack Overflow will be edited.
    | Predict 0.36 as the output probability for all answers. 0.36 is the fraction
    of accepted answers overall that are edited. |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 业务对特征及其交互不够理解的二元分类问题。 | 训练数据中正例的总体比例。 | Stack Overflow 上接受回答是否会被编辑。 | 总是预测输出概率为
    0.36。0.36 是所有被接受答案中被编辑的比例。'
- en: '| Multilabel classification problem where features and interactions between
    features are not well understood by the business. | Distribution of the label
    value over the training data. | Country from which a Stack Overflow question will
    be answered. | Predict 0.03 for France, 0.08 for India, and so on. These are the
    fractions of answers written by people from France, India, and so on. |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 多标签分类问题，业务方面对特征及特征之间的交互理解不深。 | 训练数据中标签值的分布。 | 从哪个国家回答 Stack Overflow 问题。
    | 预测法国为 0.03，印度为 0.08，依此类推。这些是法国、印度等国家答案的比例。 |'
- en: '| Regression problem where there is a single, very important, numeric feature.
    | Linear regression based on what is, intuitively, the single most important feature.
    | Predict taxi fare amount given pickup and dropoff locations. The distance between
    the two points is, intuitively, a key feature. | Fare = $4.64 per kilometer. The
    $4.64 is computed from the training data over all trips. |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 有一个非常重要的数值特征的回归问题。 | 基于直觉上最重要的单一特征进行线性回归。 | 预测出租车费用，给定上车和下车位置。这两点之间的距离在直觉上是一个关键特征。
    | 车费 = 每公里 $4.64。$4.64 是从所有行程的训练数据计算出来的。 |'
- en: '| Regression problem with one or two important features. The features could
    be numeric or categorical but should be commonly used heuristics. | Lookup table
    where the rows and columns correspond to the key features (discretized if necessary)
    and the prediction for each cell is the average label in that cell estimated over
    the training data. | Predict duration of bicycle rental. Here, the two key features
    are the station that the bicycle is being rented from and whether or not it is
    peak hours for commuting. | Lookup table of average rental duration from each
    station based on peak hour versus nonpeak hour. |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 有一个或两个重要特征的回归问题。这些特征可以是数值型或分类型，但应是常用的启发式方法。 | 查找表，其中行和列对应于关键特征（如有必要进行离散化），并且每个单元格的预测是在训练数据中估计的该单元格中的平均标签。
    | 预测自行车租赁的持续时间。这里的两个关键特征是租车站点及是否为通勤高峰时间。 | 根据高峰和非高峰时段，基于每个站点的平均租赁时长的查找表。 |'
- en: '| Classification problem with one or two important features. The features could
    be numeric or categorical. | As above, except that the prediction for each cell
    is the distribution of labels in that cell. If the goal is to predict a single
    class, compute the mode of the label in each cell. | Predict whether a Stack Overflow
    question will get answered within one day. The most important feature here is
    the primary tag. | For each tag, compute the fraction of questions that are answered
    within one day. |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 有一个或两个重要特征的分类问题。这些特征可以是数值型或分类型。 | 与上述类似，每个单元格的预测是该单元格中标签的分布。如果目标是预测单一类别，则计算每个单元格中标签的众数。
    | 预测 Stack Overflow 问题是否在一天内得到回答。这里最重要的特征是主要标签。 | 对于每个标签，计算在一天内获得答案的问题的比例。 |'
- en: '| Regression problem that involves predicting the future value of a time series.
    | Persistence or linear trend. Take seasonality into account. For annual data,
    compare against the same day/week/quarter of previous year. | Predict weekly sales
    volume | Predict that next week’s sales = s[0] where s[0] is the sales this week.
    (or)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '| 回归问题，涉及预测时间序列未来值。 | 持续性或线性趋势。考虑季节性因素。对于年度数据，与去年同一天/周/季度进行比较。 | 预测每周销售量 |
    预测下周销售额 = s[0]，其中 s[0] 是本周销售额。 (或者)'
- en: Next week’s sales = s[0] + (s[0] - s[-1]) where s[-1] is last week’s sales.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 下周销售额 = s[0] + (s[0] - s[-1])，其中 s[-1] 是上周销售额。
- en: (or)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: (或者)
- en: Next week’s sales = s[-1y] where s[-1y] is the sales of the corresponding week
    last year.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 下周销售额 = s[-1y]，其中 s[-1y] 是去年同期的销售额。
- en: Avoid the temptation to combine the three options since the value of the relative
    weights is not intuitive. |
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 避免诱惑，不要合并这三个选项，因为相对权重的价值不直观。
- en: '| Classification problem currently being solved by human experts. This is common
    for image, video, and text tasks and includes scenarios where it is cost-prohibitive
    to routinely solve the problem with human experts. | Performance of human experts.
    | Detecting eye disease from retinal scans. | Have three or more physicians examine
    each image. Treat the decision of a majority of physicians as being correct, and
    look at the percentile ranking of the ML model among human experts. |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 当前由人类专家解决的分类问题。这在图像、视频和文本任务中很常见，包括成本高昂，无法经常性地通过人类专家解决问题的情况。 | 人类专家的表现。 |
    从视网膜扫描中检测眼病。 | 每张图像至少由三名医生检查。以多数医生的决定为正确，并查看机器学习模型在人类专家中的百分位排名。 |'
- en: '| Preventive or predictive maintenance. | Perform maintenance on a fixed schedule.
    | Preventive maintenance of a car.  | Bring cars in for maintenance once every
    three months. The three months is the median time to failure of cars from the
    last service date. |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 预防性或预测性维护。 | 按固定时间表进行维护。 | 车辆的预防性维护。 | 每三个月进行一次车辆维护。三个月是车辆从上次服务日期到故障的中位时间。
    |'
- en: '| Anomaly detection. | 99th percentile value estimated from the training dataset.
    | Identify a denial of service (DoS) attack from network traffic. | Find the 99th
    percentile of the number of requests per minute in the historical data. If over
    any one-minute period, the number of requests exceeds this number, flag it as
    a DoS attack. |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 异常检测。 | 从训练数据集估算的第99百分位值。 | 从网络流量中识别拒绝服务（DoS）攻击。 | 在历史数据中找到每分钟请求量的第99百分位数。如果在任何一分钟内，请求量超过此数值，则标记为DoS攻击。
    |'
- en: '| Recommendation model. | Recommend the most popular item in the category of
    the customer’s last purchase. | Recommend movies to users. | If a user just saw
    (and liked) *Inception* (a sci-fi movie), recommend *Icarus* to them (the most
    popular sci-fi movie they haven’t yet watched). |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 推荐模型。 | 推荐客户最后一次购买类别中最受欢迎的商品。 | 向用户推荐电影。 | 如果用户刚刚看过（并喜欢）*盗梦空间*（一部科幻电影），则向他们推荐*伊卡洛斯*（他们尚未观看的最受欢迎的科幻电影）。
    |'
- en: Many of the scenarios in [Table 7-1](#heuristic_benchmarks_for_a_few_selected)
    refer to “important features.” These are important features in the sense that
    they are widely accepted within the business as having a well-understood impact
    on the prediction problem. In particular, these are not features ascertained using
    feature importance methods on your training dataset. As an example, it’s well
    accepted within the taxicab industry that the most important determinant of a
    taxi fare is distance, and that longer trips cost more. That’s what makes distance
    an important feature, not the outcome of a feature importance study.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 7-1](#heuristic_benchmarks_for_a_few_selected)中的许多场景涉及“重要特征”。从业务角度看，这些特征被广泛接受为对预测问题有着深入理解的影响。特别是，这些不是通过对训练数据集使用特征重要性方法确定的特征。例如，在出租车行业中，普遍认为出租车费用的最重要决定因素是距离，长途旅行成本更高。这就是距离成为重要特征的原因，而不是特征重要性研究的结果。'
- en: Trade-Offs and Alternatives
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 折衷和替代方案
- en: We will often find that a heuristic benchmark is useful beyond the primary purpose
    of explaining model performance. In some cases, the heuristic benchmark might
    require special data collection. Finally, there are instances where a heuristic
    benchmark may be insufficient because the comparison itself needs context.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常发现启发式基准在解释模型性能之外也很有用。在某些情况下，启发式基准可能需要特殊的数据收集。最后，有时启发式基准可能不足以满足比较本身的需求。
- en: Development check
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开发检查
- en: It is often the case that a heuristic benchmark proves useful beyond explaining
    the performance of ML models. During development, it can also help with diagnosing
    problems with a particular model approach.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 往往启发式基准证明在解释机器学习模型性能方面非常有用。在开发过程中，它还可以帮助诊断特定模型方法的问题。
- en: 'For example, say that we are building a model to predict the duration of rentals
    and our benchmark is a lookup table of average rental duration given the station
    name and whether or not it is peak commute hour:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们正在建立一个模型来预测租赁的持续时间，我们的基准是一个查找表，根据站点名称和是否高峰通勤时间给出平均租赁持续时间：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As we develop our model, it is a good idea to compare the performance of our
    ML model against this benchmark. In order to do this, we will be evaluating model
    performance on different stratifications of the evaluation dataset. Here, the
    evaluation dataset will be stratified by `start_station_name` and `is_peak`. By
    doing so, we can easily diagnose whether our model is overemphasizing the busy,
    popular stations and ignoring infrequent stations in the training data. If that
    is happening, we can experiment with increasing model complexity or balancing
    the dataset to overweight less popular stations.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发我们的模型时，将模型性能与这一基准进行比较是一个好主意。为了做到这一点，我们将评估评估数据集的不同分层。在这里，评估数据集将按`start_station_name`和`is_peak`分层。通过这样做，我们可以轻松诊断我们的模型是否过度强调繁忙的热门站点，而忽视了训练数据中不频繁的站点。如果发生这种情况，我们可以尝试增加模型复杂性或平衡数据集，以加权不那么受欢迎的站点。
- en: Human experts
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人类专家
- en: We recommended that in classification problems like diagnosing eye disease—where
    the work is carried out by human experts—that the benchmark would involve a panel
    of such experts. By having three or more physicians examine each image, it is
    possible to identify the extent to which human physicians make errors and compare
    the error rate of the model against that of human experts. In the case of such
    image classification problems, this is a natural extension of the labeling phase
    because the labels for eye disease are created through human labeling.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议在诊断眼部疾病等分类问题中，由人类专家执行工作时，基准应涉及这样一个专家组。通过让三名或更多医生检查每张图像，可以确定人类医生出错的程度，并将模型的错误率与人类专家的错误率进行比较。对于这种图像分类问题，这是标注阶段的自然延伸，因为眼部疾病的标签是通过人类标注创建的。
- en: It is sometimes advantageous to use human experts even if we have actual ground
    truth. For example, when building a model to predict the cost of auto repair after
    an accident, we can look at historical data and find the actual cost of the repair.
    We will not typically use human experts for this problem because the ground truth
    is directly available from the historical dataset. However, for the purposes of
    communicating the benchmark, it can be helpful to have insurance agents assess
    the cars for a damage estimate, and compare our model’s estimates to those of
    the agents.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们有实际的地面真相，有时候使用人类专家也是有优势的。例如，当建立一个模型来预测事故后汽车修理的成本时，我们可以查看历史数据并找到实际修理的成本。对于这个问题，我们通常不会使用人类专家，因为地面真相可以直接从历史数据集中获得。然而，为了传达基准，让保险代理人评估车辆损坏估算，并将我们模型的估算与代理人的估算进行比较，这也可能会有帮助。
- en: Using human experts need not be limited to unstructured data as with eye disease
    or damage cost estimation. For example, if we are building a model to predict
    whether or not a loan will get refinanced within a year, the data will be tabular
    and the ground truth will be available in the historical data. However, even in
    this case, we might ask human experts to identify loans that will get refinanced
    for the purposes of communicating how often loan agents in the field would get
    it right.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用人类专家不必仅限于无结构数据，如眼部疾病或损伤成本估算。例如，如果我们正在构建一个模型来预测贷款在一年内是否会进行再融资，数据将是表格形式的，而地面真相将在历史数据中可得。然而，即使在这种情况下，我们可能要求人类专家识别将会再融资的贷款，以便传达现场贷款代理人的正确率。
- en: Utility value
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 效用值
- en: Even if we have an operational model or excellent heuristic to compare against,
    we will still have to explain the impact of the improvement that our model offers.
    Communicating that the MAE is 30 seconds lower or that the MAP is 1% higher might
    not be enough. The next question might very well be, “Is a 1% improvement good?
    Is it worth the hassle of putting an ML model into production rather than the
    simple heuristic rule?”
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们有一个运行中的模型或优秀的启发式规则来进行比较，我们仍然需要解释我们的模型提供的改进带来的影响。仅仅传达MAE降低了30秒或MAP提高了1%可能还不足够。下一个问题很可能是：“1%的改进好吗？是否值得将ML模型投入生产而不是简单的启发式规则？”
- en: If you can, it is important to translate the improvement in model performance
    into the model’s utility value. This value could be monetary, but it could also
    correspond with other measures of utility, like better search results, earlier
    disease detection, or less waste resulting from improved manufacturing efficiency.
    This utility value is useful in deciding whether or not to deploy this model,
    since deploying or changing a production model always carries a certain cost in
    terms of reliability and error budgets. For example, if the image classification
    model is used to pre-fill an order form, we can calculate that a 1% improvement
    will translate to 20 fewer abandoned orders per day, and is therefore worth a
    certain amount of money. If this is more than the threshold set by our Site Reliability
    Engineering team, we’d deploy the model.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可以的话，将模型性能改善翻译成模型的效用值是很重要的。这个值可以是货币性的，但也可以对应于其他效用衡量，如更好的搜索结果、早期疾病检测或由于改善制造效率而减少的浪费。这个效用值在决定是否部署此模型时非常有用，因为部署或更改生产模型总是伴随着可靠性和误差预算成本。例如，如果图像分类模型用于预填充订单表单，我们可以计算出1%的改善将意味着每天减少20个放弃订单，因此这对应一定金额的价值。如果这超过了我们的站点可靠性工程团队设定的阈值，我们将部署该模型。
- en: In our bicycle rental problem, it might be possible to measure the impact on
    the business by using this model. For example, we might be able to calculate the
    increased availability of bicycles or the increased profits based on using the
    model in a dynamic pricing solution.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的自行车租赁问题中，可能可以通过使用这个模型来衡量对业务的影响。例如，我们可以根据使用动态定价解决方案中的模型来计算自行车供应的增加或利润的增加。
- en: 'Design Pattern 29: Explainable Predictions'
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计模式 29：可解释预测
- en: The Explainable Predictions design pattern increases user trust in ML systems
    by providing users with an understanding of how and why models make certain predictions.
    While models such as decision trees are interpretable by design, the architecture
    of deep neural networks makes them inherently difficult to explain. For all models,
    it is useful to be able to interpret predictions in order to understand the combinations
    of features influencing model behavior.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释预测设计模式（[model](https://oreil.ly/5W-2n)）通过向用户提供模型如何以及为何做出特定预测的理解，增强了用户对ML系统的信任。虽然决策树等模型天生具有可解释性，但深度神经网络的架构使其难以解释。对于所有模型，能够解释预测以理解影响模型行为的特征组合是非常有用的。
- en: Problem
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: When evaluating a machine learning model to determine whether it’s ready for
    production, metrics like accuracy, precision, recall, and mean squared error only
    tell one piece of the story. They provide data on how *correct* a model’s predictions
    are relative to ground truth values in the test set, but they carry no insight
    on *why* a model arrived at those predictions. In many ML scenarios, users may
    be hesitant to accept a model’s prediction at face value.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估机器学习模型是否准备投入生产时，像准确率、精确度、召回率和均方误差等指标只能讲述故事的一部分。它们提供了关于模型预测与测试集中真实值相对正确程度的数据，但并没有提供模型为何得出这些预测的洞见。在许多ML场景中，用户可能不愿意单纯接受模型的预测结果。
- en: To understand this, let’s look at a [model](https://oreil.ly/5W-2n) that predicts
    the severity of diabetic retinopathy (DR) from an image of a retina.^([1](ch07.xhtml#ch01fn34))
    The model returns a softmax output, indicating the probability that an individual
    image belongs to 1 of 5 categories denoting the severity of DR in the image—ranging
    from 1 (no DR present) to 5 (proliferative DR, the worst form). Let’s imagine
    that for a given image, the model returns 95% confidence that the image contains
    proliferative DR. This may seem like a high-confidence, accurate result, but if
    a medical professional is relying solely on this model output to provide a patient
    diagnosis, they still have no insight into *how* the model arrived at this prediction.
    Maybe the model identified the correct regions in the image that are indicative
    of DR, but there’s also a chance the model’s prediction is based on pixels in
    the image that show no indication of the disease. As an example, maybe some images
    in the dataset contain doctor notes or annotations. The model could be incorrectly
    using the presence of an annotation to make its prediction, rather than the diseased
    areas in the image.^([2](ch07.xhtml#idm46056078011496)) In the model’s current
    form, there is no way to attribute the prediction to regions in an image, making
    it difficult for the doctor to trust the model.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这一点，让我们来看一个[模型](https://oreil.ly/5W-2n)，该模型根据视网膜图像预测糖尿病视网膜病变（DR）的严重程度。^([1](ch07.xhtml#ch01fn34))
    模型返回一个softmax输出，表示个别图像属于表明图像中DR严重程度的5个类别中的一类的概率，从1（无DR）到5（增生性DR，最严重的形式）。假设对于给定的图像，模型返回95%的置信度，表示图像包含增生性DR。这可能看起来是一个高置信度的准确结果，但如果医务专业人员仅仅依赖于这个模型输出来进行患者诊断，他们仍然不了解模型是如何得出这一预测的。也许模型确定了图像中表明DR的正确区域，但也有可能模型的预测基于图像中显示没有疾病迹象的像素。例如，数据集中的一些图像可能包含医生的笔记或注释。模型可能错误地使用存在注释来进行预测，而不是图像中的疾病区域。^([2](ch07.xhtml#idm46056078011496))
    在模型当前的形式下，无法将预测归因于图像中的区域，这使医生难以信任模型。
- en: Medical imaging is just one example—there are many industries, scenarios, and
    model types where a lack of insight into a model’s decision-making process can
    lead to problems with user trust. If an ML model is used to predict an individual’s
    credit score or other financial health metric, people will likely want to know
    why they received a particular score. Was it a late payment? Too many lines of
    credit? Short credit history? Maybe the model is relying solely on demographic
    data to make its predictions, and subsequently introducing bias into the model
    without our knowledge. With only the score, there is no way to know how the model
    arrived at its prediction.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 医学影像只是一个例子——在许多行业、场景和模型类型中，对模型决策过程缺乏洞察力可能会导致用户信任问题。如果一个机器学习模型用于预测个人的信用评分或其他财务健康指标，人们很可能想知道他们为什么得到了特定的分数。是因为延迟支付吗？信用额度太多？信用历史太短？也许模型仅依赖于人口统计数据来进行预测，并在不知情的情况下引入偏见。只有分数，并没有办法知道模型是如何得出预测的。
- en: In addition to model end users, another group of stakeholders are those involved
    with regulatory and compliance standards for ML models, since models in certain
    industries may require auditing or additional transparency. Stakeholders involved
    in auditing models will likely need a higher-level summary of how the model is
    arriving at its predictions in order to justify its use and impact. Metrics like
    accuracy are not useful in this case—without insight into *why* a model makes
    the predictions it does, its use may become problematic.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型最终用户外，另一组利益相关者是与机器学习模型的监管和合规标准相关的人员，因为某些行业的模型可能需要审核或额外透明度。参与审计模型的利益相关者可能需要模型如何得出其预测的高级摘要，以证明其使用和影响的合理性。在这种情况下，像准确度这样的度量标准是没有用的——没有洞察到模型为何做出其预测，其使用可能会变得问题重重。
- en: Finally, as data scientists and ML engineers, we can only improve our model
    quality to a certain degree without an understanding of the features it’s relying
    on to make predictions. We need a way to verify that models are performing in
    the way we expect. For example, let’s say we are training a model on tabular data
    to predict whether a flight will be delayed. The model is trained on 20 features.
    Under the hood, maybe it’s relying only on 2 of those 20 features, and if we removed
    the rest, we could significantly improve our system’s performance. Or maybe each
    of those 20 features is necessary to achieve the degree of accuracy we need. Without
    more details on what the model is using, it’s difficult to know.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，作为数据科学家和机器学习工程师，我们只能在没有了解模型依赖于哪些特征来进行预测的情况下，将模型质量提高到一定程度。我们需要一种方法来验证模型是否按我们预期的方式运行。例如，假设我们正在对表格数据进行训练，以预测航班是否会延误。该模型基于20个特征进行训练。在幕后，也许它仅依赖于这20个特征中的2个，如果我们去掉其余的特征，我们可以显著提高系统的性能。或者也许这20个特征每一个都是必需的，以达到我们需要的精度水平。没有更多关于模型使用了哪些特征的细节，很难知道。
- en: Solution
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: To handle the inherent unknowns in ML, we need a way to understand how models
    work under the hood. Techniques for understanding and communicating how and why
    an ML model makes predictions is an area of active research. Also called interpretability
    or model understanding, explainability is a new and rapidly evolving field within
    ML, and can take a variety of forms depending on a model’s architecture and the
    type of data it is trained on. Explainability can also help reveal bias in ML
    models, which we cover when discussing the Fairness Lens pattern in this chapter.
    Here, we’ll focus on explaining deep neural networks using feature attributions.
    To understand this in context, first we’ll look at explainability for models with
    less complex architectures.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 要处理机器学习中固有的未知，我们需要一种方法来了解模型在幕后的工作方式。理解和解释机器学习模型如何以及为何做出预测的技术是一个活跃研究领域。也称为可解释性或模型理解，解释性是机器学习中一个新兴且快速发展的领域，可以根据模型的架构和所训练的数据类型采用各种形式。解释性还可以帮助揭示机器学习模型中的偏见，在本章中讨论公平性镜头模式时，我们将重点讨论使用特征归因解释深度神经网络。为了在此背景下理解这一点，首先我们将看看对于具有较简单体系结构的模型的可解释性。
- en: 'Simpler models like decision trees are more straightforward to explain than
    deep models since they are often *interpretable by design*. This means that their
    learned weights provide direct insight into how the model is making predictions.
    If we have a linear regression model with independent, numeric input features,
    the weights may sometimes be interpretable. Take for example a linear regression
    model that predicts fuel efficiency of a car.^([3](ch07.xhtml#ch01fn35)) In [scikit-learn](https://oreil.ly/V9GT5),
    we can get the learned coefficients of a linear regression model with the following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 简单模型如决策树比深度模型更容易解释，因为它们通常是*设计可解释的*。这意味着它们学习到的权重直接揭示了模型进行预测的方式。如果我们有一个具有独立数值输入特征的线性回归模型，权重有时可能是可以解释的。例如，考虑一个预测汽车燃油效率的线性回归模型。^([3](ch07.xhtml#ch01fn35))
    在[scikit-learn](https://oreil.ly/V9GT5)，我们可以通过以下方式获取线性回归模型的学习系数：
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The resulting coefficients for each feature in our model are shown in [Figure 7-1](#the_learned_coefficients_from_our_linea).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的每个特征的学习系数如图7-1所示。
- en: '![The learned coefficients from our linear regression fuel efficiency model,
    which predicts a car’s miles per gallon. We used get_dummies() from pandas to
    convert the origin feature to a boolean column since it is categorical.](Images/mldp_0701.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![我们线性回归燃油效率模型的学习系数，预测汽车每加仑英里数。我们使用pandas的get_dummies()函数将原产地特征转换为布尔列，因为它是分类的。](Images/mldp_0701.png)'
- en: Figure 7-1\. The learned coefficients from our linear regression fuel efficiency
    model, which predicts a car’s miles per gallon. We used get_dummies() from pandas
    to convert the origin feature to a boolean column since it is categorical.
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-1\. 我们线性回归燃油效率模型的学习系数，预测汽车每加仑英里数。我们使用pandas的get_dummies()函数将原产地特征转换为布尔列，因为它是分类的。
- en: The coefficients show us the relationship between each feature and the model’s
    output, predicted miles per gallon (MPG). For example, from these coefficients,
    we can conclude that for each additional cylinder in a car, our model’s predicted
    MPG will decrease. Our model has also learned that as new cars are introduced
    (denoted by the “model year” feature), they often have higher fuel efficiency.
    We can learn much more about the relationships between our model’s features and
    output from these coefficients than we could from the learned weights of a hidden
    layer in a deep neural network. This is why models like the one demonstrated above
    are often referred to as *interpretable by design*.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 系数向我们展示了每个特征与模型输出（预测每加仑英里数）之间的关系。例如，通过这些系数，我们可以得出结论，每增加一汽缸，我们模型预测的每加仑英里数将会下降。我们的模型还学习到，随着新车型的引入（以“车型年份”特征表示），它们通常具有更高的燃油效率。通过这些系数，我们可以从中学到比深度神经网络隐藏层学习的权重更多关于模型特征与输出之间关系的知识。这就是为什么像上面展示的模型经常被称为*设计可解释的*模型。
- en: Warning
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: While it’s tempting to assign significant meaning to the learned weights in
    linear regression or decision tree models, we must be extremely cautious when
    doing so. The conclusions we drew earlier are still correct (i.e., inverse relationship
    between number of cylinders and fuel efficiency), but we cannot conclude from
    the magnitude of coefficients, for example, that the categorical origin feature
    or the number of cylinders are more important to our model than horsepower or
    weight. First, each of these features is represented in a different unit. One
    cylinder bears no equivalence to one pound—the cars in this dataset have a maximum
    of 8 cylinders, but weigh over 3,000 pounds. Additionally, origin is a categorical
    feature represented with dummy values, so each origin value can only be 0 or 1\.
    The coefficients also don’t tell us anything about the relationship *between*
    features in our model. More cylinders are often correlated with more horsepower,
    but we can’t conclude this from the learned weights.^([4](ch07.xhtml#ch01fn36))
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然很容易给线性回归或决策树模型学到的权重分配重要意义，但我们在这样做时必须非常谨慎。我们之前得出的结论仍然正确（例如汽缸数量与燃油效率的反向关系），但我们不能仅仅从系数的大小推断出，例如，分类原产地特征或汽缸数量对我们的模型比马力或重量更重要。首先，每个特征使用不同的单位表示。一个汽缸不等同于一磅——数据集中的汽车最多有8个汽缸，但重量超过3000磅。此外，原产地是一个用虚拟值表示的分类特征，因此每个原产地值只能是0或1。系数也不告诉我们模型特征之间的关系。更多汽缸通常与更多马力相关，但我们不能仅凭学到的权重推断出这一点。^([4](ch07.xhtml#ch01fn36))
- en: 'When models are more complex, we use *post hoc* explainability methods to approximate
    the relationships between a model’s features and its output. Typically, post hoc
    methods perform this analysis without relying on model internals like learned
    weights. This is an area of ongoing research, and there are a variety of proposed
    explanation methods, along with tooling for adding these methods to your ML workflow.
    The type of explanation methods we’ll discuss are known as *feature attributions*.
    These methods aim to attribute a model’s output—whether it be an image, classification,
    or numerical value—to its features, by assigning attribution values to each feature
    indicating how much that feature contributed to the output. There are two types
    of feature attributions:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型更复杂时，我们使用*事后*可解释性方法来近似模型特征与其输出之间的关系。通常，事后方法执行此分析，而不依赖于学习的权重等模型内部信息。这是一个正在研究的领域，提出了各种解释方法以及用于将这些方法添加到您的机器学习工作流程中的工具。我们将讨论的解释方法类型被称为*特征归因*。这些方法旨在将模型的输出（无论是图像、分类还是数值）归因于其特征，通过为每个特征分配归因值来指示该特征对输出的贡献。特征归因有两种类型：
- en: Instance-level
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 实例级别
- en: Feature attributions that explain a model’s output for an individual prediction.
    For example, in a model predicting whether someone should be approved for a line
    of credit, an instance-level feature attribution would provide insight into why
    a specific person’s application was denied. In an image model, an instance-level
    attribution might highlight the pixels in an image that caused it to predict it
    contained a cat.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 特征归因解释模型对个体预测结果的输出。例如，在预测某人是否应该获得信用额度时，实例级别的特征归因将揭示为什么特定人的申请被拒绝的原因。在图像模型中，实例级别的归因可能会突出显示导致模型预测图像包含猫的像素。
- en: Global
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 全局
- en: Global feature attributions analyze the model’s behavior across an aggregate
    to draw conclusions about how the model is behaving as a whole. Typically this
    is done by averaging instance-level feature attributions from a test dataset.
    In a model predicting whether a flight will be delayed, global attributions might
    tell us that overall, extreme weather is the most significant feature when predicting
    delays.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 全局特征归因分析模型在整体上的行为以得出关于模型整体行为的结论。通常，这是通过从测试数据集平均实例级别特征归因来完成的。在预测航班是否延误的模型中，全局归因可能会告诉我们，总体上，极端天气是预测延误时最重要的特征。
- en: The two feature attribution methods we’ll explore^([5](ch07.xhtml#ch01fn37))
    are outlined in [Table 7-2](#descriptions_of_different_explanation_m) and provide
    different approaches that can be used for both instance-level and global explanations.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨的两种特征归因方法^([5](ch07.xhtml#ch01fn37))在 [表 7-2](#descriptions_of_different_explanation_m)
    中概述，并提供可用于实例级别和全局解释的不同方法。
- en: Table 7-2\. Descriptions of different explanation methods and links to their
    research papers
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7-2\. 不同解释方法的描述及其研究论文链接
- en: '| Name | Description | Paper |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 描述 | 论文 |'
- en: '| --- | --- | --- |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Sampled Shapley | Based on the concept of Shapley Value,^([a](ch07.xhtml#ch01fn38))
    this approach determines a feature’s marginal contribution by calculating how
    much adding and removing that feature affects a prediction, analyzed over multiple
    combinations of feature values. | [*https://oreil.ly/ubEjW*](https://oreil.ly/ubEjW)
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 采样 Shapley | 基于 Shapley 值的概念^([a](ch07.xhtml#ch01fn38))，该方法通过计算添加和删除特征如何影响预测（分析多种特征值组合）来确定特征的边际贡献。
    | [*https://oreil.ly/ubEjW*](https://oreil.ly/ubEjW) |'
- en: '| Integrated Gradients (IG) | Using a predefined model baseline, IG calculates
    the derivatives (gradients) along the path from this baseline to a specific input.
    | [*https://oreil.ly/sy8f8*](https://oreil.ly/sy8f8) |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 综合梯度（IG） | 使用预定义的模型基线，IG计算沿着从此基线到特定输入的路径的导数（梯度）。 | [*https://oreil.ly/sy8f8*](https://oreil.ly/sy8f8)
    |'
- en: '| ^([a](ch07.xhtml#ch01fn38-marker)) The Shapley Value was introduced in a
    [paper by Lloyd Shapley](https://oreil.ly/xCrqU) in 1951, and is based on concepts
    from game theory. |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| ^([a](ch07.xhtml#ch01fn38-marker)) Shapley 值是由 Lloyd Shapley 在 1951 年的一篇论文中引入的，基于博弈论的概念。
    |'
- en: While we could implement these approaches from scratch, there is tooling designed
    to simplify the process of getting feature attributions. The available open source
    and cloud-based explainability tools let us focus on debugging, improving, and
    summarizing our models.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以从头开始实施这些方法，但有些工具旨在简化获取特征归因过程。现有的开源和基于云的可解释性工具使我们能够专注于调试、改进和总结我们的模型。
- en: Model baseline
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型基线
- en: In order to use these tools, we first need to understand the concept of a *baseline*
    as it applies to explaining models with feature attributions. The goal of any
    explainability method is to answer the question, “Why did the model predict X?”
    Feature attributions attempt to do this by providing numerical values for each
    feature indicating how much that feature contributed to the final output. Take
    for example a model predicting whether a patient has heart disease given some
    demographic and health data. For a single example in our test dataset, let’s imagine
    that the attribution value for a patient’s cholesterol feature is 0.4, and the
    attribution for their blood pressure is −0.2\. Without context, these attribution
    values don’t mean much, and our first question will likely be, “0.4 and −0.2 relative
    to what?” That “what” is the model’s *baseline*.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用这些工具，我们首先需要理解作为解释模型特征归因时所适用的*基线*概念。任何可解释性方法的目标是回答：“为什么模型预测了 X？” 特征归因试图通过为每个特征提供数值来实现这一点，这些数值指示该特征对最终输出的贡献程度。例如，考虑一个模型预测患者是否患有心脏病，给定一些人口统计和健康数据。对于我们测试数据集中的一个示例，假设一个患者的胆固醇特征的归因值为
    0.4，血压的归因为 −0.2。没有上下文的情况下，这些归因值意义不大，我们的第一个问题可能是：“0.4 和 −0.2 相对于什么？” 这个“什么”就是模型的*基线*。
- en: Whenever we get feature attribution values, they are all relative to a predefined
    baseline prediction value for our model. Baseline predictions can either be *informative*
    or *uninformative*. Uninformative baselines typically compare against some average
    case across a training dataset. In an image model, an uninformative baseline could
    be a solid black or white image. In a text model, an uninformative baseline could
    be 0 values for the model’s embedding matrices or stop words like “the,” “is,”
    or “and.” In a model with numerical inputs, a common approach to choosing a baseline
    is to generate a prediction using the median value for each feature in the model.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们获得特征归因值时，它们都是相对于我们模型预测的预定义基线值的。基线预测可以是*信息性的*或*非信息性的*。非信息性基线通常与训练数据集中某些平均情况进行比较。在图像模型中，非信息性基线可以是纯黑或纯白的图像。在文本模型中，非信息性基线可以是模型嵌入矩阵的
    0 值或诸如“the”、“is”或“and”之类的停用词。在具有数值输入的模型中，选择基线的常见方法是使用模型中每个特征的中位数值生成预测。
- en: '[Figure 7-2](#the_feature_attribution_values_for_a_si) shows instance-level
    feature attributions for a model that predicts the duration of a bike trip. The
    uninformative baseline for this model is a trip duration of 13.6 minutes, which
    we get by generating a prediction using the median value for each feature in our
    dataset. When a model’s prediction is *less than* the baseline prediction value,
    we should expect most attribution values to be negative, and vice versa. In this
    example, we get a predicted duration of 10.71, which is less than the model’s
    baseline, and explains why many of the attribution values are negative. We can
    determine the most important features by taking the absolute value of the feature
    attributions. In this example, the trip’s distance was the most important feature,
    causing our model’s prediction to decrease 2.4 minutes from the baseline. Additionally,
    as a sanity check, we should ensure that the feature attribution values roughly
    add up to the difference between the current prediction and the baseline prediction.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-2](#the_feature_attribution_values_for_a_si) 显示了一个模型的实例级特征归因，该模型预测骑行的持续时间。此模型的非信息性基线是一个骑行持续时间为
    13.6 分钟，我们通过使用数据集中每个特征的中位数值生成预测得到这个值。当模型的预测值*低于*基线预测值时，我们应该期望大多数归因值为负，反之亦然。在这个例子中，我们得到了预测的持续时间为
    10.71，低于模型的基线，这解释了为什么许多归因值为负。我们可以通过取特征归因的绝对值来确定最重要的特征。在这个例子中，行程的距离是最重要的特征，使得我们模型的预测比基线减少了
    2.4 分钟。另外，作为一种合理性检查，我们应确保特征归因值大致等于当前预测与基线预测之间的差值。'
- en: '![The feature attribution values for a single example in a model predicting
    bike trip duration. The model’s baseline, calculated using the median of each
    feature value, is 13.6 minutes, and the attribution values show how much each
    feature influenced the prediction.](Images/mldp_0702.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![在预测自行车行程持续时间的模型中，单个示例的特征归因值。该模型的基线，使用每个特征值的中位数计算得到，为13.6分钟，归因值显示了每个特征对预测的影响。](Images/mldp_0702.png)'
- en: Figure 7-2\. The feature attribution values for a single example in a model
    predicting bike trip duration. The model’s baseline, calculated using the median
    of each feature value, is 13.6 minutes, and the attribution values show how much
    each feature influenced the prediction.
  id: totrans-98
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-2\. 在预测自行车行程持续时间的模型中，单个示例的特征归因值。该模型的基线，使用每个特征值的中位数计算得到，为13.6分钟，归因值显示了每个特征对预测的影响。
- en: Informative baselines, on the other hand, compare a model’s prediction with
    a specific alternative scenario. In a model identifying fraudulent transactions,
    an informative baseline might answer the question, “Why was this transaction flagged
    as fraud instead of nonfraudulent?” Instead of using the median feature values
    across the entire training dataset to calculate the baseline, we would take the
    median of only the nonfraudulent values. In an image model, maybe the training
    images contain a significant portion of solid black and white pixels, and using
    these as a baseline would result in inaccurate predictions. In this case, we’d
    need to come up with a different *informative* baseline image.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 信息性基线相比之下，则是将模型的预测与特定的备选场景进行比较。在识别欺诈交易的模型中，信息性基线可能回答这样一个问题：“为什么将这笔交易标记为欺诈而不是非欺诈？”我们不会使用整个训练数据集中的特征值中位数来计算基线，而是只取非欺诈值的中位数。在图像模型中，训练图像可能包含大量的纯黑色和白色像素，如果将其作为基线则会导致不准确的预测。在这种情况下，我们需要提出一个不同的*信息性*基线图像。
- en: SHAP
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SHAP
- en: The open source library [SHAP](https://github.com/slundberg/shap) provides a
    Python API for getting feature attributions on many types of models, and is based
    on the concept of Shapley Value introduced in [Table 7-2](#descriptions_of_different_explanation_m).
    To determine feature attribution values, SHAP calculates how much adding or removing
    each feature contributes to a model’s prediction output. It performs this analysis
    across many different combinations of feature values and model output.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 开源库[SHAP](https://github.com/slundberg/shap)提供了一个Python API，用于在许多类型的模型上获取特征归因，并且基于Shapley值的概念在[表 7-2](#descriptions_of_different_explanation_m)中进行了介绍。为了确定特征归因值，SHAP计算添加或删除每个特征对模型预测输出的贡献。它通过许多不同的特征值组合和模型输出来执行此分析。
- en: 'SHAP is framework-agnostic and works with models trained on image, text, or
    tabular data. To see how SHAP works in practice, we’ll use the fuel efficiency
    dataset referenced previously. This time, we’ll build a deep model with the Keras
    `Sequential` API:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP不依赖框架，并且适用于在图像、文本或表格数据上训练的模型。为了看到SHAP在实践中的应用，我们将使用之前提到的燃油效率数据集。这次，我们将使用Keras的`Sequential`
    API构建一个深度模型：
- en: '[PRE2]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To use SHAP, we’ll first create a `DeepExplainer` object by passing it our
    model and a subset of examples from our training set. Then we’ll get the attribution
    values for the first 10 examples in our test set:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用SHAP，我们首先会通过将模型和训练集中一部分示例传递给`DeepExplainer`对象来创建它。然后，我们将获取测试集中前10个示例的归因值：
- en: '[PRE3]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'SHAP has some built-in visualization methods that make it easier to understand
    the resulting attribution values. We’ll use SHAP’s `force_plot()` method to plot
    the attribution values for the first example in our test set with the following
    code:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP具有一些内置的可视化方法，使理解生成的归因值更加容易。我们将使用SHAP的`force_plot()`方法来绘制测试集中第一个示例的归因值，代码如下：
- en: '[PRE4]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the code above, `explainer.expected_value` is our model’s baseline. SHAP
    calculates the baseline as the mean of the model’s output across the dataset we
    passed when we created the explainer (in this case, `x_train[:100]`), though we
    could also pass our own baseline value to `force_plot`. The ground truth value
    for this example is 14 miles per gallon, and our model predicts 13.16\. Our explanation
    will therefore explain our model’s prediction of 13.16 with feature attribution
    values. In this case, the attribution values are relative to the model’s baseline
    of 24.16 MPG. The attribution values should therefore add up to roughly 11, the
    difference between the model’s baseline and the prediction for this example. We
    can identify the most important features by looking at the ones with the highest
    absolute value. [Figure 7-3](#the_feature_attribution_values_for_one) shows the
    resulting plot for this example’s attribution values.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，`explainer.expected_value` 是我们模型的基准。SHAP 将基准计算为我们在创建解释器时通过的数据集的模型输出均值（在本例中为
    `x_train[:100]`），尽管我们也可以向 `force_plot` 传递自定义的基准值。此示例的真实值为每加仑14英里，而我们的模型预测为13.16。因此，我们的解释将解释模型对13.16的预测值的特征归因值。在这种情况下，这些归因值相对于模型的24.16
    MPG基线。因此，这些归因值应大致相加到11，即模型基线和此示例预测之间的差异。通过查看具有最高绝对值的特征，我们可以确定最重要的特征。[图 7-3](#the_feature_attribution_values_for_one)
    展示了该示例的归因值结果图。
- en: '![The feature attribution values for one example from our fuel efficiency prediction
    model. In this case, the car’s weight is the most significant indicator of MPG
    with a feature attribution value of roughly 6\. Had our model’s prediction been
    above the baseline of 24.16, we would instead see mostly negative attribution
    values.](Images/mldp_0703.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![我们燃油效率预测模型的一个示例的特征归因值。在本例中，汽车的重量是MPG的最重要指标，其特征归因值约为6。如果我们的模型预测超过24.16的基准，我们将看到主要是负的归因值。](Images/mldp_0703.png)'
- en: Figure 7-3\. The feature attribution values for one example from our fuel efficiency
    prediction model. In this case, the car’s weight is the most significant indicator
    of MPG with a feature attribution value of roughly 6\. Had our model’s prediction
    been above the baseline of 24.16, we would instead see mostly negative attribution
    values.
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-3\. 我们燃油效率预测模型的一个示例的特征归因值。在本例中，汽车的重量是MPG的最重要指标，其特征归因值约为6。如果我们的模型预测超过24.16的基准，我们将看到主要是负的归因值。
- en: 'For this example, the most important indicator of fuel efficiency is weight,
    pushing our model’s prediction down by about 6 MPG from the baseline. This is
    followed by horsepower, displacement, and then the car’s model year. We can get
    a summary (or global explanation) of the feature attribution values for the first
    10 examples from our test set with the following:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，燃油效率最重要的指标是重量，从基准下降了约6 MPG。其次是马力、排量，然后是汽车的年份型号。我们可以通过以下方式获取测试集前10个样本的特征归因值的摘要（或全局解释）：
- en: '[PRE5]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This results in the summary plot shown in [Figure 7-4](#an_example_of_global-level_feature_attr).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了图 7-4 的摘要图的显示（#an_example_of_global-level_feature_attr）。
- en: In practice, we’d have a larger dataset and would want to calculate global-level
    attributions on more examples. We could then use this analysis to summarize the
    behavior on our model to other stakeholders within and outside our organization.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 实际应用中，我们会有更大的数据集，并希望在更多样本上计算全局级别的归因。我们可以利用这些分析来向组织内外的其他利益相关者总结模型的行为。
- en: '![An example of global-level feature attributions for the fuel efficiency model,
    calculated on the first 10 examples from the test dataset. ](Images/mldp_0704.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![燃油效率模型的全局特征归因示例，基于测试数据集的前10个样本计算。 ](Images/mldp_0704.png)'
- en: Figure 7-4\. An example of global-level feature attributions for the fuel efficiency
    model, calculated on the first 10 examples from the test dataset.
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-4\. 燃油效率模型的全局特征归因示例，基于测试数据集的前10个样本计算。
- en: Explanations from deployed models
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署模型的解释
- en: SHAP provides an intuitive API for getting attributions in Python, typically
    used in a script or notebook environment. This works well during model development,
    but there are scenarios where you’d want to get explanations on a deployed model
    in addition to the model’s prediction output. In this case, cloud-based explainability
    tools are the best option. Here, we’ll demonstrate how to get feature attributions
    on a deployed model using Google Cloud’s [Explainable AI](https://oreil.ly/lDocn).
    At the time of this writing, Explainable AI works with custom TensorFlow models
    and tabular data models built with AutoML.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP提供了一个直观的Python API，用于在脚本或笔记本环境中获取归因。这在模型开发期间非常有效，但在某些情况下，您可能希望在部署模型时获取解释，以补充模型的预测输出。在这种情况下，云端可解释性工具是最佳选择。在这里，我们将演示如何使用Google
    Cloud的[Explainable AI](https://oreil.ly/lDocn)获取部署模型的特征归因。在撰写本文时，Explainable AI与使用AutoML构建的自定义TensorFlow模型和表格数据模型兼容。
- en: 'We’ll deploy an image model to AI Platform to show explanations, but we could
    also use Explainable AI with TensorFlow models trained on tabular or text data.
    To start, we’ll deploy a [TensorFlow Hub](https://oreil.ly/Ws8jx) model trained
    on the ImageNet dataset. So that we can focus on the task of getting explanations,
    we won’t do any transfer learning on the model and will use ImageNet’s original
    1,000 label classes:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将部署一个图像模型到AI平台以展示解释，但我们也可以使用基于表格或文本数据训练的TensorFlow模型进行可解释的AI。首先，我们将部署一个[TensorFlow
    Hub](https://oreil.ly/Ws8jx)模型，该模型是在ImageNet数据集上训练的。这样我们就可以专注于获取解释的任务，不会对模型进行任何迁移学习，并且将使用ImageNet原始的1,000个标签类别：
- en: '[PRE6]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To deploy a model to AI Platform with explanations, we first need to create
    a metadata file that will be used by the explanation service to calculate feature
    attributions. This metadata is provided in a JSON file and includes information
    on the baseline we’d like to use and the parts of the model we want to explain.
    To simplify this process, Explainable AI provides an SDK that will generate metadata
    via the following code:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 要将模型部署到具有解释功能的AI平台，我们首先需要创建一个元数据文件，该文件将被解释服务用于计算特征归因。此元数据以JSON文件提供，并包含我们想要使用的基线信息以及我们想要解释的模型部分。为了简化此过程，可解释的AI提供了一个SDK，通过以下代码生成元数据：
- en: '[PRE7]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This code didn’t specify a model baseline, which means it’ll use the default
    (for image models, this is a black and white image). We can optionally add an
    `input_baselines` parameter to `set_image_metadata` to specify a custom baseline.
    Running the `save_metadata` method above creates an *explanation_metadata.json*
    file in a model directory (the full code is in the [GitHub repository](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/07_stakeholder_management/explainability.ipynb)).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码未指定模型基线，这意味着它将使用默认值（对于图像模型，这是黑白图像）。我们可以选择添加`input_baselines`参数到`set_image_metadata`以指定自定义基线。上面的`save_metadata`方法在模型目录中创建一个*explanation_metadata.json*文件（完整代码在[GitHub存储库](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/07_stakeholder_management/explainability.ipynb)中）。
- en: When using this SDK via AI Platform Notebooks, we also have the option to generate
    explanations locally within a notebook instance without deploying our model to
    the cloud. We can do this via the `load_model_from_local_path method`.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 当通过AI平台笔记本使用此SDK时，我们还有选项在笔记本实例内本地生成解释，而无需将我们的模型部署到云端。我们可以通过`load_model_from_local_path`方法实现这一点。
- en: With our exported model and the *explanation_metadata.json* file in a Storage
    bucket, we’re ready to create a new model version. When we do this, we specify
    the explanation method we’d like to use.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们导出的模型和存储桶中的*explanation_metadata.json*文件，我们已准备好创建一个新的模型版本。在执行此操作时，我们需要指定我们想要使用的解释方法。
- en: 'To deploy our model to AI Platform, we can copy our model directory to a Cloud
    Storage bucket and use the gcloud CLI to create a model version. AI Platform has
    three possible explanation methods to choose from:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 要将我们的模型部署到AI平台，我们可以将模型目录复制到Cloud Storage存储桶，并使用gcloud CLI创建一个模型版本。AI平台有三种可能的解释方法可供选择：
- en: Integrated Gradients (IG)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 集成梯度（IG）
- en: This implements the method introduced in the [IG paper](https://oreil.ly/FJhMd)
    and works with any differentiable TensorFlow model—image, text, or tabular. For
    image models deployed on AI Platform, IG returns an image with highlighted pixels,
    indicating the regions that signaled the models prediction.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法是在[IG paper](https://oreil.ly/FJhMd)中介绍的，并且适用于任何可微分的TensorFlow模型——图像、文本或表格。对于部署在AI平台上的图像模型，IG会返回一个高亮像素的图像，指示出导致模型预测的区域。
- en: Sampled Shapley
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Sampled Shapley
- en: Based on the [Sampled Shapley paper](https://oreil.ly/EAS8T), this uses an approach
    similar to the open source SHAP library. On AI Platform, we can use this method
    with tabular and text TensorFlow models. Because IG works only with differentiable
    models, AutoML Tables uses Sampled Shapley to calculate feature attributions for
    all models.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 基于[Sampled Shapley paper](https://oreil.ly/EAS8T)，这使用了类似于开源SHAP库的方法。在AI平台上，我们可以在表格和文本TensorFlow模型中使用此方法。因为IG仅适用于可微分模型，AutoML
    Tables使用Sampled Shapley为所有模型计算特征归因。
- en: XRAI
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: XRAI
- en: '[This approach](https://oreil.ly/niGVQ) is built upon IG and applies smoothing
    to return region-based attributions. XRAI works only with image models deployed
    on AI Platform.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[这种方法](https://oreil.ly/niGVQ)建立在IG之上，并应用平滑处理返回基于区域的归因。XRAI仅适用于部署在AI平台上的图像模型。'
- en: 'In our gcloud command, we specify the explanation method we’d like to use along
    with the number of integral steps or paths we want the method to use when computing
    attribution values.^([6](ch07.xhtml#ch01fn39)) The `steps parameter` refers to
    the number of feature combinations sampled for each output. In general, increasing
    this number will improve explanation accuracy:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的gcloud命令中，我们指定要使用的解释方法以及在计算归因值时希望方法使用的整数步骤或路径的数量。^([6](ch07.xhtml#ch01fn39))
    `steps parameter` 指的是为每个输出采样的特征组合数量。通常情况下，增加这个数字会提高解释的准确性：
- en: '[PRE8]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Once the model is deployed, we can get explanations using the Explainable AI
    SDK:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型部署完成，我们可以使用可解释AI SDK获取解释：
- en: '[PRE9]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In [Figure 7-5](#the_feature_attributions_returned_from), we can see a comparison
    of the IG and XRAI explanations returned from Explainable AI for our ImageNet
    model. The highlighted pixel regions show the pixels that contributed most to
    our model’s prediction of “husky.”
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 7-5](#the_feature_attributions_returned_from)中，我们可以看到从可解释AI返回的IG和XRAI解释的比较，突出显示了对我们模型预测“哈士奇”贡献最大的像素区域。
- en: Typically, IG is recommended for “non-natural” images like those taken in a
    medical, factory, or lab environment. XRAI usually works best for images taken
    in natural environments like the one of this husky. To understand why IG is preferred
    for non-natural images, see the IG attributions for the diabetic retinopathy image
    in [Figure 7-6](#as_part_of_a_study_by_rory_sayres_and_c). In cases like this
    medical one, it helps to see attributions at a fine-grained, pixel level. In the
    dog image, on the other hand, knowing the exact pixels that caused our model to
    predict “husky” is less important, and XRAI gives us a higher-level summary of
    the important regions.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，对于像医疗、工厂或实验室环境中拍摄的“非自然”图像，建议使用IG。XRAI通常适用于自然环境中拍摄的图像，比如这只哈士奇的图像。要了解为什么IG更适用于非自然图像，请参见[图 7-6](#as_part_of_a_study_by_rory_sayres_and_c)中糖尿病视网膜病变图像的IG归因。在这种医疗情况下，有助于在像素级别查看归因。而在狗的图像中，知道导致模型预测“哈士奇”的确切像素不那么重要，XRAI为我们提供了重要区域的更高级别摘要。
- en: '![The feature attributions returned from Explainable AI for an ImageNet model
    deployed to AI Platform. On the left is the original image. The IG attributions
    are shown in the middle, and the XRAI attributions are shown on the right. The
    key below shows what the colored regions in XRAI correspond to—bright yellow regions
    are the most important, and dark violet areas represent the least important regions.
    ](Images/mldp_0705.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![部署到AI平台的ImageNet模型返回的特征归因，由可解释AI提供。左侧是原始图像。中间显示了IG的归因，右侧显示了XRAI的归因。下方的键显示了XRAI中彩色区域的含义——亮黄色区域最重要，深紫色区域最不重要。](Images/mldp_0705.png)'
- en: Figure 7-5\. The feature attributions returned from Explainable AI for an ImageNet
    model deployed to AI Platform. On the left is the original image. The IG attributions
    are shown in the middle, and the XRAI attributions are shown on the right. The
    key below shows what the regions in XRAI correspond to—lighter regions are the
    most important, and darker areas represent the least important regions.
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-5\. 部署到AI平台的ImageNet模型返回的特征归因，由可解释AI提供。左侧是原始图像。中间显示了IG的归因，右侧显示了XRAI的归因。下方的键显示了XRAI中区域的含义——浅色区域最重要，深色区域代表最不重要的区域。
- en: '![As part of a study by Rory Sayres and colleagues in 2019 different groups
    of ophthalmologists were asked to evaluate the degree of DR on an image in three
    scenarios: the image by itself without model predictions, the image with model
    predictions, and the image with predictions and pixel attributions (shown here).
    We can see how pixel attributions can help increase confidence in the model’s
    prediction.](Images/mldp_0706.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![2019年，Rory Sayres及其同事进行的一项研究中，不同组的眼科医生被要求在三种情况下评估一幅图像上的DR程度：单独的图像，没有模型预测的图像，以及带有预测和像素归因（如图所示）的图像。我们可以看到像素归因如何帮助增强模型预测的信心。](Images/mldp_0706.png)'
- en: 'Figure 7-6\. As part of a [study](https://oreil.ly/Xp_vp) by Rory Sayres and
    colleagues in 2019, different groups of ophthalmologists were asked to evaluate
    the degree of DR on an image in three scenarios: the image by itself without model
    predictions, the image with model predictions, and the image with predictions
    and pixel attributions (shown here). We can see how pixel attributions can help
    increase confidence in the model’s prediction.'
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-6。作为一项[研究](https://oreil.ly/Xp_vp)，2019年Rory Sayres及其同事的研究中，不同组的眼科医生被要求在三种情况下评估一幅图像上的DR程度：单独的图像，没有模型预测的图像，以及带有预测和像素归因（如图所示）的图像。我们可以看到像素归因如何帮助增强模型预测的信心。
- en: Tip
  id: totrans-143
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Explainable AI also works in [AutoML Tables](https://oreil.ly/CSQly), a tool
    for training and deploying tabular data models. AutoML Tables handles data preprocessing
    and selects the best model for our data, which means we don’t need to write any
    model code. Feature attributions through Explainable AI are enabled by default
    for models trained in AutoML Tables, and both global and instance-level explanations
    are provided.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释人工智能（Explainable AI）也适用于[AutoML Tables](https://oreil.ly/CSQly)，这是一个用于训练和部署表格数据模型的工具。AutoML
    Tables处理数据预处理并选择最适合我们数据的模型，这意味着我们无需编写任何模型代码。通过Explainable AI进行的特征归因在AutoML Tables中的模型默认启用，并提供全局和实例级别的解释。
- en: Trade-Offs and Alternatives
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 权衡和替代方案
- en: While explanations provide important insight into how a model is making decisions,
    they are only as good as the model’s training data, the quality of your model,
    and the chosen baseline. In this section, we’ll discuss some limitations of explainability,
    along with some alternatives to feature attributions.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然解释提供了对模型如何做出决策的重要见解，但它们只能和模型的训练数据、模型的质量以及所选的基准一样好。在本节中，我们将讨论解释的一些限制，以及特征归因的一些替代方案。
- en: Data selection bias
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据选择偏差
- en: It’s often said that machine learning is “garbage in, garbage out.” In other
    words, a model is only as good as the data used to train it. If we train an image
    model to identify 10 different cat breeds, those 10 cat breeds are all it knows.
    If we show the model an image of a dog, all it can do is try to classify the dog
    into 1 of the 10 cat categories it’s been trained on. It might even do so with
    high confidence. That is to say, models are a direct representation of their training
    data.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 人们经常说机器学习是“垃圾进，垃圾出”。换句话说，模型只能和用于训练它的数据一样好。如果我们训练一个图像模型来识别10种不同的猫种类，那么它所知道的就只有这10种猫的种类。如果我们给模型展示一只狗的图像，它只能试图将这只狗分类为它训练过的10个猫类别中的一个。它甚至可能会以很高的信心这样做。也就是说，模型直接反映了它的训练数据。
- en: If we don’t catch data imbalances before training a model, explainability methods
    like feature attributions can help bring data selection bias to light. As an example,
    say we’re building a model to predict the type of boat present in an image. Let’s
    say it correctly labels an image from our test set as “kayak,” but using feature
    attributions, we find that the model is relying on the boat’s paddle to predict
    “kayak” rather than the shape of the boat. This is a signal that our dataset might
    not have enough variation in training images for each class—we’ll likely need
    to go back and add more images of kayaks at different angles, both with and without
    paddles.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在训练模型之前没有捕捉到数据不平衡，那么像特征归因这样的解释方法可以帮助揭示数据选择偏差的问题。举个例子，假设我们正在构建一个模型来预测图像中存在的船只类型。假设我们正确地将测试集中的一幅图像标记为“kayak”，但是通过特征归因，我们发现模型依赖于船只的桨来预测“kayak”，而不是船只的形状。这表明我们的数据集可能没有足够多角度、带桨和不带桨的kayak图像，我们可能需要回去添加更多这样的图像。
- en: Counterfactual analysis and example-based explanations
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反事实分析和基于示例的解释
- en: 'In addition to feature attributions—described in the Solution section—there
    are many other approaches to explaining the output of ML models. This section
    is not meant to provide an exhaustive list of all explainability techniques, as
    this area is quickly evolving. Here, we will briefly describe two other approaches:
    counterfactual analysis and example-based explanations.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在解决方案部分描述的特征归因之外，还有许多其他解释机器学习模型输出的方法。本节不旨在提供所有可解释性技术的详尽列表，因为这个领域发展迅速。在这里，我们简要介绍另外两种方法：反事实分析和基于示例的解释。
- en: Counterfactual analysis is an instance-level explainability technique that refers
    to finding examples from our dataset with similar features that resulted in different
    predictions from our model. One way to do this is through the [What-If Tool](https://oreil.ly/Vf3D-),
    an open source tool for evaluating and visualizing the output of ML models. We’ll
    provide a more in-depth overview of the What-If Tool in the Fairness Lens design
    pattern—here, we’ll focus specifically on its counterfactual analysis functionality.
    When visualizing data points from our test set in the What-If Tool, we have the
    option to show the nearest counterfactual data point to the one we’re selecting.
    Doing this will let us compare feature values and model predictions for these
    two data points, which can help us better understand the features our model is
    relying on most. In [Figure 7-7](#counterfactual_analysis_in_the_what-if), we
    see a counterfactual comparison for two data points from a mortgage application
    dataset. In bold, we see the features where these two data points are different,
    and at the bottom, we can see the model output for each.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 反事实分析是一种实例级的可解释性技术，指的是在我们的数据集中找到具有类似特征但模型预测结果不同的示例。其中一种方法是通过[What-If Tool](https://oreil.ly/Vf3D-)，这是一个用于评估和可视化机器学习模型输出的开源工具。在“公平性镜头设计模式”中，我们将提供对What-If
    Tool更深入的概述——在这里，我们专注于其反事实分析功能。在What-If Tool中可视化我们测试集中的数据点时，我们可以选择显示与所选数据点最接近的反事实数据点。这样做可以让我们比较这两个数据点的特征值和模型预测，帮助我们更好地理解模型最依赖的特征。在[图7-7](#counterfactual_analysis_in_the_what-if)，我们看到了一个来自抵押申请数据集的两个数据点的反事实比较。在粗体字中，我们看到了这两个数据点不同的特征，底部则展示了每个数据点的模型输出。
- en: Example-based explanations compare new examples and their corresponding predictions
    to similar examples from our training dataset. This type of explanation is especially
    useful for understanding how our training dataset affects model behavior. Example-based
    explanations work best on image or text data, and can be more intuitive than feature
    attributions or counterfactual analysis since they map a model’s prediction directly
    to the data used for training.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 基于示例的解释会比较新示例及其相应的预测与训练数据集中类似示例。这种类型的解释对于理解训练数据集如何影响模型行为尤为有用。基于示例的解释在图像或文本数据上效果最佳，并且比特征归因或反事实分析更直观，因为它们直接将模型的预测映射到用于训练的数据上。
- en: '![Counterfactual analysis in the What-If Tool for two data points from a US
    mortgage application dataset. Differences between the two data points are highlighted
    in green. More information on this dataset can be found in the discussion of the
    Fairness Lens pattern in this chapter.](Images/mldp_0707.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![反事实分析在What-If Tool中针对美国抵押申请数据集中两个数据点的应用。突出显示两个数据点之间的差异为绿色。关于此数据集的更多信息可以在本章中公平性镜头模式的讨论中找到。](Images/mldp_0707.png)'
- en: Figure 7-7\. Counterfactual analysis in the What-If Tool for two data points
    from a US mortgage application dataset. Differences between the two data points
    are bolded. More information on this dataset can be found in the discussion of
    the Fairness Lens pattern in this chapter.
  id: totrans-155
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-7. What-If Tool中针对美国抵押申请数据集中两个数据点的反事实分析。突出显示了这两个数据点的不同之处。有关此数据集的更多信息可以在本章中公平性镜头模式的讨论中找到。
- en: To better understand this approach, let’s look at the game [Quick, Draw](https://oreil.ly/-QsHl)!^([7](ch07.xhtml#ch01fn40))
    The game asks players to draw an item, and guesses what they are drawing in real
    time, using a deep neural network trained on thousands of drawings by others.
    After players finish a drawing, they can see how the neural network arrived at
    its prediction by looking at examples from the training dataset. In [Figure 7-8](#example-based_explanations_from_the_gam),
    we can see the example-based explanations for a drawing of french fries that the
    model successfully recognized.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这种方法，让我们来看看游戏[Quick, Draw](https://oreil.ly/-QsHl)!^([7](ch07.xhtml#ch01fn40))。这款游戏要求玩家实时绘制物品，并使用一个基于成千上万次他人绘图的深度神经网络猜测他们正在绘制的内容。玩家完成绘画后，可以通过查看训练数据集中的示例来了解神经网络是如何进行预测的。在[图7-8](#example-based_explanations_from_the_gam)中，我们可以看到关于一幅薯条绘画的基于示例的解释，模型成功地识别了它。
- en: '![Example-based explanations from the game Quick, Draw! showing how the model
    correctly predicted “french fries” for the given drawing through examples from
    the training dataset.](Images/mldp_0708.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![来自游戏Quick, Draw的基于示例的解释，展示了模型如何通过训练数据集中的示例正确预测了“薯条”的给定绘画。](Images/mldp_0708.png)'
- en: Figure 7-8\. Example-based explanations from the game Quick, Draw! showing how
    the model correctly predicted “french fries” for the given drawing through examples
    from the training dataset.
  id: totrans-158
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-8\. 来自游戏Quick, Draw的基于示例的解释，展示了模型如何通过训练数据集中的示例正确预测了“薯条”的给定绘画。
- en: Limitations of explanations
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解释的局限性
- en: Explainability represents a significant improvement in understanding and interpreting
    models, but we should be cautious about placing too much trust in our model’s
    explanations, or assuming they provide perfect insight into a model. Explanations
    in any form are a direct reflection of our training data, model, and selected
    baseline. That is to say, we can’t expect our explanations to be high quality
    if our training dataset is an inaccurate representation of the groups reflected
    by our model, or if the baseline we’ve chosen doesn’t work well for the problem
    we’re solving.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 解释性代表了对理解和解释模型的显著改进，但我们在对模型的解释过于信任或假设它们提供对模型完美洞察力时应保持谨慎。任何形式的解释都直接反映了我们的训练数据、模型和选择的基准线。换句话说，如果我们的训练数据集不准确地反映了模型所反映的群体，或者我们选择的基准线不适合我们要解决的问题，就不能指望我们的解释具有高质量。
- en: Additionally, the relationship that explanations can identify between a model’s
    features and output is representative only of our data and model, and not necessarily
    the environment outside this context. As an example, let’s say we train a model
    to identify fraudulent credit card transactions and it finds, as a global-level
    feature attribution, that a transaction’s amount is the feature most indicative
    of fraud. Following this, it would be incorrect to conclude that amount is *always*
    the biggest indicator of credit card fraud—this is only the case within the context
    of our training dataset, model, and specified baseline value.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，解释可以识别模型特征与输出之间的关系，这仅代表我们的数据和模型，而不一定代表此上下文之外的环境。例如，假设我们训练一个模型来识别欺诈信用卡交易，并且它作为全局特征归因发现交易金额是欺诈的最明显特征。但是，不能因此得出结论金额*总是*信用卡欺诈的最大指标——这仅适用于我们训练数据集、模型和指定的基准值的上下文中。
- en: We can think of explanations as an important addition to accuracy, error, and
    other metrics used to evaluate ML models. They provide useful insight into a model’s
    quality and potential bias, but should not be the sole determinant of a high-quality
    model. We recommend using explanations as one piece of model evaluation criteria
    in addition to data and model evaluation, and many of the other patterns outlined
    in this and previous chapters.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将解释视为评估ML模型准确性、错误和其他指标的重要补充。它们提供了对模型质量和潜在偏见的有用见解，但不应成为高质量模型的唯一决定因素。我们建议将解释作为模型评估标准的一个组成部分，除了数据和模型评估之外，还可以考虑本章和前几章中提到的许多其他模式。
- en: 'Design Pattern 30: Fairness Lens'
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计模式30：公平性视角
- en: The Fairness Lens design pattern suggests the use of preprocessing and postprocessing
    techniques to ensure that model predictions are fair and equitable for different
    groups of users and scenarios. Fairness in machine learning is a continuously
    evolving area of research, and there is no single catch-all solution or definition
    to making a model “fair.” Evaluating an entire end-to-end ML workflow—from data
    collection to model deployment—through a fairness lens is essential to building
    successful, high-quality models.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 公平镜头设计模式建议使用预处理和后处理技术，以确保模型预测在不同用户群体和场景中是公平和公正的。机器学习中的公平性是一个不断发展的研究领域，没有单一的万能解决方案或定义可以使模型“公平”。通过公平的镜头评估整个端到端的机器学习工作流程——从数据收集到模型部署——对于构建成功的高质量模型至关重要。
- en: Problem
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: With the word “machine” in its name, it’s easy to assume that ML models can’t
    be biased. After all, models are the result of patterns learned by a computer,
    right? The problem with this thinking is that the datasets models learn from are
    created by *humans*, not machines, and humans are full of bias. This inherent
    human bias is inevitable, but is not necessarily always bad. Take for example
    a dataset used to train a financial fraud detection model—this data will likely
    be heavily imbalanced with very few fraudulent examples, since fraud is relatively
    rare in most cases. This is an example of naturally occurring bias, as it is a
    reflection of the statistical properties of the original dataset. Bias becomes
    *harmful* when it affects different groups of people differently. This is known
    as *problematic bias*, and it’s what we’ll be focusing on throughout this section.
    If this type of bias is not accounted for, it can find its way into models, creating
    adverse effects as production models directly reflect the bias present in the
    data.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 带有“机器”一词的名字，很容易就会认为机器学习模型不会存在偏见。毕竟，模型是计算机学习到的模式的结果，对吧？这种想法的问题在于，模型学习的数据集是由*人类*创建的，而不是机器，而人类充满了偏见。这种固有的人类偏见是不可避免的，但并不一定总是坏事。举个例子，考虑用于训练金融欺诈检测模型的数据集——由于大多数情况下欺诈案例相对较少，因此这些数据很可能会严重不平衡。这是自然存在的偏见的一个例子，因为它反映了原始数据集的统计特性。当偏见影响到不同群体时，偏见就会变得*有害*。这被称为*问题性偏见*，这是我们在本节中将重点关注的内容。如果不考虑这种类型的偏见，它可能会进入模型中，在生产模型中直接反映出数据中存在的偏见，从而产生负面影响。
- en: Problematic bias is present even in situations where you may not expect it.
    As an example, imagine we’re building a model to identify different types of clothing
    and accessories. We’ve been tasked with collecting all of the shoe images for
    the training dataset. When we think about shoes, we take note of the first thing
    that comes to mind. Is it a tennis shoe? Loafer? Flip flop? What about a stiletto?
    Let’s imagine that we live in a climate that is warm year-round and most of the
    people we know wear sandals all the time. When we think of a shoe, a sandal is
    the first thing that comes to mind. As a result, we collect a diverse representation
    of sandal images with different types of straps, sole thicknesses, colors, and
    more. We contribute these to the larger clothing dataset, and when we test the
    model on a test set of images of our friend’s shoes, it reaches 95% accuracy on
    the “shoe” label. The model looks promising, but problems arise when our colleagues
    from different locations test the model on images of their heels and sneakers.
    For their images, the label “shoe” is not returned at all.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在你可能没有预料到的情况下，问题性偏见也存在。例如，想象一下我们正在构建一个模型来识别不同类型的服装和配饰。我们被要求收集所有的鞋类图片作为训练数据集。当我们想到鞋子时，我们注意到首先想到的是什么。是网球鞋？便鞋？人字拖？高跟鞋呢？假设我们生活在全年气候温暖的地方，我们认识的大多数人都一直穿凉鞋。当我们想到鞋子时，凉鞋是首先浮现在脑海中的东西。因此，我们收集了各种款式的凉鞋图片，有不同类型的带子、鞋底厚度、颜色等。我们将这些贡献给更大的服装数据集，当我们在朋友的鞋子测试图像集上测试模型时，它在“鞋子”标签上达到了95%的准确率。模型看起来很有前途，但当我们的来自不同地区的同事在他们的高跟鞋和运动鞋图像上测试模型时，问题就出现了。“鞋子”标签根本不返回。
- en: This shoe example demonstrates bias in the training data distribution, and although
    it may seem oversimplified, this type of bias occurs frequently in production
    settings. Data distribution bias happens when the data we collect doesn’t accurately
    reflect the entire population who will use our model. If our dataset is human-centered,
    this type of bias can be especially evident if our dataset fails to include an
    equal representation of ages, races, genders, religions, sexual orientations,
    and other identity characteristics.^([8](ch07.xhtml#ch01fn41))
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这个鞋子的例子展示了训练数据分布中的偏见，尽管它可能看起来过于简化，但这种类型的偏见在生产环境中经常发生。数据分布偏差发生在我们收集的数据不能准确反映将使用我们模型的整个人群时。如果我们的数据集以人为中心，这种类型的偏见特别明显，如果我们的数据集未能包含对年龄、种族、性别、宗教、性取向和其他身份特征的平等代表。^([8](ch07.xhtml#ch01fn41))
- en: Even when our dataset does appear balanced with respect to these identity characteristics,
    it is still subject to bias in the way these groups are represented in the data.
    Suppose we are training a sentiment analysis model to classify restaurant reviews
    on a scale of 1 (extremely negative) to 5 (extremely positive). We’ve taken care
    to get a balanced representation of different types of restaurants in the data.
    However, it turns out that the majority of reviews for seafood restaurants are
    positive, whereas most of the vegetarian restaurant reviews are negative. This
    data representation bias will be directly represented by our model. Whenever new
    reviews are added for vegetarian restaurants, they’ll have a much higher chance
    of being classified as negative, which could then influence someone’s likelihood
    to visit one of these restaurants in the future. This is also known as *reporting
    bias*, since the dataset (here, the “reported” data) doesn’t accurately reflect
    the real world.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们的数据集在这些身份特征方面看起来是平衡的，它仍然受到这些群体在数据中表示方式的偏见影响。假设我们正在训练一个情感分析模型，用于将餐厅评论分类为1（极其负面）到5（极其正面）。我们已经小心翼翼地在数据中获得了不同类型餐厅的平衡代表。然而，事实证明，大多数海鲜餐厅的评论是正面的，而大多数素食餐厅的评论是负面的。这种数据表现偏见将直接由我们的模型表现出来。每当针对素食餐厅添加新评论时，它们被分类为负面的可能性就会大大增加，这可能会影响某人未来访问这些餐厅的意愿。这也被称为*报告偏见*，因为数据集（这里是“报告”的数据）不能准确反映现实世界。
- en: A common fallacy when dealing with data bias issues is that removing the areas
    of bias from a dataset will fix the problem. Let’s say we’re building a model
    to predict the likelihood someone will default on a loan. If we find the model
    is treating people of different races unfairly, we might assume this could be
    fixed by simply removing race as a feature from the dataset. The problem with
    this is that, due to systemic bias, characteristics like race and gender are often
    reflected implicitly in other features like zip code or income. This is known
    as *implicit* or *proxy bias*. Removing obvious features with potential bias like
    race and gender can often be worse than leaving them in, since it makes it harder
    to identify and correct instances of bias in the model.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 处理数据偏见问题时的一个常见谬误是，从数据集中删除偏见区域将解决问题。假设我们正在建立一个模型来预测某人违约贷款的可能性。如果我们发现模型对不同种族的人不公平，我们可能会认为通过简单地从数据集中删除种族作为特征来修复问题。问题在于，由于系统性偏见，诸如种族和性别之类的特征通常会隐含地反映在其他特征如邮政编码或收入中。这被称为*隐性*或*代理偏见*。删除明显具有潜在偏见的特征，如种族和性别，通常比保留它们更糟，因为这使得更难以识别和纠正模型中的偏见实例。
- en: When collecting and preparing data, another area where bias can be introduced
    is in the way the data is labeled. Teams often outsource labeling of large datasets,
    but it’s important to take care in understanding how labelers can introduce bias
    to a dataset, especially if the labeling is subjective. This is known as *experimenter
    bias*. Imagine we’re building a sentiment analysis model, and we have outsourced
    the labeling to a group of 20 people—it’s their job to label each piece of text
    on a scale from 1 (negative) to 5 (positive). This type of analysis is extremely
    subjective and can be influenced by one’s culture, upbringing, and many other
    factors. Before using this data to train our model, we should ensure this group
    of 20 labelers reflects a diverse population.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集和准备数据时，引入偏见的另一个领域是数据标记的方式。团队通常会外包大型数据集的标记，但重要的是要小心理解标记人员如何向数据集引入偏见，特别是如果标记是主观的。这被称为*实验者偏见*。想象一下我们正在构建情感分析模型，我们将标签外包给一组20个人——他们的工作是根据1（负面）到5（正面）的评分标准标记每一段文本。这种分析极具主观性，并可能受文化、成长背景及许多其他因素的影响。在使用这些数据训练我们的模型之前，我们应确保这组20个标签人员反映了多样化的人群。
- en: In addition to data, bias can also be introduced during model training by the
    objective function we choose. For example, if we optimize our model for overall
    accuracy, this may not accurately reflect model performance across all slices
    of data. In cases where datasets are inherently imbalanced, using accuracy as
    our only metric may miss cases where our model is underperforming or making unfair
    decisions on minority classes in our data.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据之外，偏见也可能通过我们选择的目标函数在模型训练过程中引入。例如，如果我们优化模型的整体准确率，这可能不能准确反映模型在所有数据片段上的性能。在数据集天生不平衡的情况下，仅使用准确率作为我们的唯一指标可能会忽略模型在少数类别数据上表现不佳或做出不公平决策的情况。
- en: Throughout this book, we’ve seen that ML has the power to improve productivity,
    add business value, and automate tasks that were previously manual. As data scientists
    and ML engineers, we have a shared responsibility to ensure the models we build
    don’t have adverse effects on the populations that use them.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 本书贯穿始终，我们已经看到机器学习有能力提高生产力，增加业务价值，并自动化以前手工操作的任务。作为数据科学家和机器学习工程师，我们有责任确保我们构建的模型不会对使用它们的人群产生不利影响。
- en: Solution
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: To handle problematic bias in machine learning, we need solutions both for identifying
    areas of harmful bias in data before training a model, and evaluating our trained
    model through a fairness lens. The Fairness Lens design pattern provides approaches
    for building datasets and models that treat all groups of users equally. We’ll
    demonstrate techniques for both types of analysis using the [What-If Tool](https://oreil.ly/Sk36z),
    an open source tool for dataset and model evaluation that can be run from many
    Python notebook environments.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 要处理机器学习中的问题偏见，我们需要解决方案，即在训练模型之前识别数据中有害偏见的领域，并通过公平性视角评估我们训练的模型。公平性镜头设计模式提供了构建数据集和模型以平等对待所有用户群体的方法。我们将使用[What-If
    Tool](https://oreil.ly/Sk36z)，一个开源工具，用于从多个Python笔记本环境中运行数据集和模型评估，演示这两种分析技术。
- en: Tip
  id: totrans-176
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'Before proceeding with the tools outlined in this section, it’s worth analyzing
    both the dataset and prediction task to determine whether there is potential for
    problematic bias. This requires looking closer at *who* will be impacted by a
    model, and *how* those groups will be impacted. If problematic bias seems likely,
    the technical approaches outlined in this section provide a good starting point
    for mitigating this type of bias. If, on the other hand, the skew in the dataset
    contains naturally occurring bias that will not have adverse effects on different
    groups of people, [“Design Pattern 10: Rebalancing ”](ch03.xhtml#design_pattern_onezero_rebalancing)
    in [Chapter 3](ch03.xhtml#problem_representation_design_patterns) provides solutions
    for handling data that is inherently imbalanced.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续本节中列出的工具之前，值得分析数据集和预测任务，以确定是否存在潜在的问题偏见。这需要更仔细地观察*谁*会受到模型影响，以及*如何*这些群体会受到影响。如果存在问题偏见的可能性较大，本节中概述的技术方法提供了缓解这种类型偏见的良好起点。另一方面，如果数据集中的偏斜包含自然产生的偏见，并不会对不同群体产生不利影响，[“设计模式10：重新平衡”](ch03.xhtml#design_pattern_onezero_rebalancing)
    在[第三章](ch03.xhtml#problem_representation_design_patterns) 提供了处理天生不平衡数据的解决方案。
- en: Throughout this section, we’ll be referencing a [public dataset](https://oreil.ly/azFUV)
    of US mortgage applications. Loan agencies in the US are required to report information
    on an individual application, like the type of loan, the applicant’s income, the
    agency handling the loan, and the status of the application. We will train a loan
    application approval model on this dataset in order to demonstrate different aspects
    of fairness. To our knowledge, this dataset is not used as is by any loan agency
    to train ML models, and so the fairness red flags we raise are only hypothetical.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将引用一个[公共数据集](https://oreil.ly/azFUV)，该数据集包含美国抵押贷款申请。美国的贷款机构需要报告关于个人申请的信息，如贷款类型、申请人收入、处理贷款的机构及申请的状态。我们将在这个数据集上训练一个贷款申请批准模型，以展示公平性的不同方面。据我们了解，该数据集并未被任何贷款机构直接用于训练机器学习模型，因此我们提出的公平性问题仅为假设。
- en: We’ve created a subset of this dataset and done some preprocessing to turn this
    into a binary classification problem—whether an application was approved or denied.
    In [Figure 7-9](#a_preview_of_a_few_columns_from_the_us), we can see a preview
    of the dataset.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经创建了该数据集的一个子集，并进行了一些预处理，将其转化为二元分类问题——申请是否批准。在[图 7-9](#a_preview_of_a_few_columns_from_the_us)
    中，我们可以看到数据集的预览。
- en: '![A preview of a few columns from the US mortgage application dataset referenced
    throughout this section.](Images/mldp_0709.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![来自本节引用的美国抵押贷款申请数据集的几列预览。](Images/mldp_0709.png)'
- en: Figure 7-9\. A preview of a few columns from the US mortgage application dataset
    referenced throughout this section.
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-9\. 来自本节引用的美国抵押贷款申请数据集的几列预览。
- en: Before training
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在训练之前
- en: Because ML models are a direct representation of the data used to train them,
    it’s possible to mitigate a significant amount of bias *before* building or training
    a model by performing thorough data analysis, and using the results of this analysis
    to adjust our data. In this phase, focus on identifying data collection or data
    representation bias, outlined in the Problem section. [Table 7-3](#descriptions_of_different_types_of_data)
    shows some questions to consider for each type of bias depending on data type.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 因为机器学习模型直接反映了用于训练它们的数据，通过进行彻底的数据分析并利用分析结果来调整我们的数据，可以在建立或训练模型之前减少大量偏差。在这个阶段，重点是识别数据收集或数据表现中的偏差，详见问题部分。[表 7-3](#descriptions_of_different_types_of_data)
    展示了根据数据类型考虑的一些问题。
- en: Table 7-3\. Descriptions of different types of data bias
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7-3\. 不同类型数据偏差的描述
- en: '|  | Definition | Considerations for analysis |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|  | 定义 | 分析考虑因素 |'
- en: '| --- | --- | --- |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Data distribution bias | Data that doesn’t contain an equal representation
    of all possible groups that will use the model in production |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 数据分布偏差 | 数据不包含所有将在生产中使用模型的可能群体的平衡代表 |'
- en: Does the data contain a balanced set of examples across all relevant demographic
    slices (gender, age, race, religion, etc.)?
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据是否包含跨所有相关人口片段（性别、年龄、种族、宗教等）的平衡示例集？
- en: Does each label in the data contain a balanced split of all possible variations
    of this label? (E.g., the shoe example in the Problem section.)
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个数据标签是否包含该标签所有可能变体的平衡分割？（例如，在问题部分中的鞋子示例。）
- en: '|'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Data representation bias | Data that is well balanced, but doesn’t represent
    different slices of data equally |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 数据表现偏差 | 数据平衡但未平等代表不同数据片段 |'
- en: For classification models, are *labels* balanced across relevant features? For
    example, in a dataset intended for credit worthiness prediction, does the data
    contain an equal representation across gender, race, and other identity characteristics
    of people marked as unlikely to pay back a loan?
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于分类模型，*标签*是否在相关特征上平衡？例如，在用于信用评估预测的数据集中，数据是否包含了跨性别、种族和其他身份特征的人群的均衡代表？
- en: Is there bias in the way different demographic groups are represented in the
    data? This is especially relevant for models predicting sentiment or a rating
    value.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同人口群体在数据中的表现是否存在偏见？这对于预测情绪或评分值的模型特别相关。
- en: Is there subjective bias introduced by data labelers?
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据标签者是否引入了主观偏见？
- en: '|'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Once we’ve examined our data and corrected for bias, we should take these same
    considerations into account when splitting our data into training, test, and validation
    sets. That is to say, once our full dataset is balanced, it’s essential that our
    train, test, and validation splits maintain the same balance. Returning to our
    shoe image example, let’s imagine we’ve improved our dataset to include varied
    images of 10 types of shoes. The training set should contain a similar percentage
    of each type of shoe as the test and validation sets. This will ensure that our
    model reflects and is being evaluated on real-world scenarios.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们检查了数据并纠正了偏见，我们在将数据分成训练、测试和验证集时应考虑相同的考虑因素。也就是说，一旦我们的整个数据集平衡了，我们的训练、测试和验证分割也必须保持相同的平衡。回到我们的鞋子图像示例，假设我们改进了数据集以包含10种类型的鞋的各种图像。训练集应包含与测试和验证集中每种鞋的类似百分比。这将确保我们的模型反映并在真实场景中进行评估。
- en: 'To see what this dataset analysis looks like in practice, we’ll use the What-If
    Tool on the mortgage dataset introduced above. This will let us visualize the
    current balance of our data across various slices. The What-If Tool works both
    with and without a model. Since we haven’t built our model yet, we can initialize
    the What-If Tool widget by passing it only our data:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看这个数据集分析在实践中的表现，我们将使用上面介绍的抵押贷款数据集的What-If工具。这将使我们能够在各种分片上可视化我们数据的当前平衡状态。What-If工具既可以在有模型的情况下使用，也可以在没有模型的情况下使用。由于我们还没有建立模型，我们可以通过仅传递我们的数据来初始化What-If工具小部件：
- en: '[PRE10]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In [Figure 7-10](#the_what-if_toolapostrophes_quotation_m), we can see what
    the tool looks like when it loads when passed 1,000 examples from our dataset.
    The first tab is called the “Datapoint editor,” which provides an overview of
    our data and lets us inspect individual examples. In this visualization, our data
    points are colored by the label—whether or not a mortgage application was approved.
    An individual example is also highlighted, and we can see the feature values associated
    with it.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图7-10](#the_what-if_toolapostrophes_quotation_m)中，我们可以看到工具在加载时的样子，当传递了我们数据集中的1,000个示例时。第一个选项卡称为“数据点编辑器”，它提供了我们数据的概述，并允许我们检查单个示例。在这个可视化中，我们的数据点根据标签着色——无论抵押贷款申请是否获批。还突出显示了一个单个示例，并且我们可以看到与其关联的特征值。
- en: '![The What-If Tool’s “Datapoint editor,” where we can see how our data is split
    by label class and inspect features for individual examples from our dataset.](Images/mldp_0710.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![What-If Tool的“数据点编辑器”，我们可以看到数据如何按标签类别分割，并检查数据集中单个示例的特征。](Images/mldp_0710.png)'
- en: Figure 7-10\. The What-If Tool’s “Datapoint editor,” where we can see how our
    data is split by label class and inspect features for individual examples from
    our dataset.
  id: totrans-201
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-10. What-If工具的“数据点编辑器”，我们可以看到数据如何按标签类别分割，并检查数据集中单个示例的特征。
- en: 'There are many options for customizing the visualization in the Datapoint editor,
    and doing this can help us understand how our dataset is split across different
    slices. Keeping the same color-coding by label, if we select the `agency_code`
    column from the Binning | Y-Axis drop-down, the tool now shows a chart of how
    balanced our data is with regard to the agency underwriting each application’s
    loan. This is shown in [Figure 7-11](#a_subset_of_the_us_mortgage_datasetcomm).
    Assuming these 1,000 datapoints are a good representation of the rest of our dataset,
    there are a few instances of potential bias revealed in [Figure 7-11](#a_subset_of_the_us_mortgage_datasetcomm):'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据点编辑器中，有许多自定义可视化选项，通过这样做可以帮助我们理解数据集如何在不同分片中分布。如果我们从分箱 | Y轴 下拉菜单中选择`agency_code`列，并保持相同的标签颜色编码，工具现在显示了关于每个申请贷款机构承保情况的图表。这在[图7-11](#a_subset_of_the_us_mortgage_datasetcomm)中展示。假设这1,000个数据点能很好地代表我们数据集的其余部分，在[图7-11](#a_subset_of_the_us_mortgage_datasetcomm)中揭示了一些潜在偏见的实例：
- en: Data representation bias
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 数据表示偏见
- en: The percentage of HUD applications *not* approved is higher than other agencies
    represented in our data. A model will likely learn this, causing it to predict
    “not approved” more frequently for applications originating through HUD.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: HUD申请*未*获批准的百分比高于数据集中其他代理机构。模型很可能会学习到这一点，导致它更频繁地预测通过HUD发起的申请“未获批准”。
- en: Data collection bias
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 数据收集偏见
- en: We may not have enough data on loans originating from FRS, OCC, FDIC, or NCUA
    to accurately use `agency_code` as a feature in our model. We should make sure
    the percentage of applications for each agency in our dataset reflects real-world
    trends. For example, if a similar number of loans go through FRS and HUD, we should
    have an equal number of examples for each of those agencies in our dataset.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 对于来自FRS、OCC、FDIC或NCUA的贷款数据，我们可能没有足够的数据来准确地将`agency_code`作为模型的特征使用。我们应确保数据集中每个机构的申请百分比反映现实世界的趋势。例如，如果类似数量的贷款通过FRS和HUD，我们应该在数据集中为这些机构的每个例子提供相同数量的示例。
- en: '![A subset of the US mortgage dataset, binned by the agency_code column in
    the dataset. ](Images/mldp_0711.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![一个美国抵押贷款数据集的子集，按照数据集中的agency_code列分组。](Images/mldp_0711.png)'
- en: Figure 7-11\. A subset of the US mortgage dataset, binned by the agency_code
    column in the dataset.
  id: totrans-208
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-11\. 一个美国抵押贷款数据集的子集，按照数据集中的agency_code列分组。
- en: We can repeat this analysis across other columns in our data and use our conclusions
    to add examples and improve our data. There are many other options for creating
    custom visualizations in the What-If Tool—see the [full code](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/07_responsible_ai/fairness.ipynb)
    on GitHub for more ideas.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在数据的其他列上重复此分析，并使用我们的结论添加示例并改进数据。在 What-If Tool 中创建自定义可视化的许多其他选项，请查看 GitHub
    上的[完整代码](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/07_responsible_ai/fairness.ipynb)获取更多想法。
- en: Another way to understand our data using the What-If Tool is through the Features
    tab, shown in [Figure 7-12](#the_features_tab_in_the_what-if_toolcom). This shows
    how our data is balanced across each column in our dataset. From this we can see
    where we need to add or remove data, or change our prediction task.^([9](ch07.xhtml#ch01fn42))
    For example, maybe we want to limit our model to making predictions only on refinancing
    or home purchase loans since there may not be enough data available for other
    possible values in the `loan_purpose` column.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种使用 What-If Tool 理解我们的数据的方法是通过显示在[图 7-12](#the_features_tab_in_the_what-if_toolcom)中的“特征”选项卡。这显示了我们的数据在数据集的每列中是如何平衡的。从中，我们可以看出我们需要在哪些地方添加或移除数据，或更改我们的预测任务。^([9](ch07.xhtml#ch01fn42))
    例如，也许我们想要限制我们的模型仅在再融资或购房贷款上进行预测，因为在“loan_purpose”列中可能没有足够的其他可能值的数据。
- en: '![The Features tab in the What-If Tool, which shows histograms of how a dataset
    is balanced for each column.](Images/mldp_0712.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![What-If Tool 中的“特征”选项卡显示数据集在每列中如何平衡的直方图。](Images/mldp_0712.png)'
- en: Figure 7-12\. The Features tab in the What-If Tool, which shows histograms of
    how a dataset is balanced for each column.
  id: totrans-212
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-12\. What-If Tool 中的“特征”选项卡，显示了数据集在每列中如何平衡的直方图。
- en: Once we’ve refined our dataset and prediction task, we can consider anything
    else we might want to optimize during model training. For example, maybe we care
    most about our model’s accuracy on applications it predicts as “approved.” During
    model training, we’d want to optimize for AUC (or another metric) on the “approved”
    class in this binary classification model.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们优化了数据集和预测任务，我们可以考虑在模型训练期间想要优化的其他内容。例如，也许我们最关心模型在被预测为“批准”的应用中的准确性。在模型训练期间，我们希望在这个二分类模型中优化“批准”类别的AUC（或其他指标）。
- en: Tip
  id: totrans-214
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Tip
- en: 'If we’ve done all we can to eliminate data collection bias and find that there
    is not enough data available for a specific class, we can follow [“Design Pattern
    10: Rebalancing ”](ch03.xhtml#design_pattern_onezero_rebalancing) in [Chapter 3](ch03.xhtml#problem_representation_design_patterns).
    This pattern discusses techniques for building models to handle imbalanced data.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们已经尽了最大努力消除数据收集偏差，并发现某个特定类别的数据不足，我们可以参考[“设计模式 10：再平衡”](ch03.xhtml#design_pattern_onezero_rebalancing)，位于[第
    3 章](ch03.xhtml#problem_representation_design_patterns)。这个模式讨论了处理不平衡数据的建模技术。
- en: After training
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练后
- en: Even with rigorous data analysis, bias may find its way into a trained model.
    This can happen as a result of a model’s architecture, optimization metrics, or
    data bias that wasn’t identified before training. To solve for this, it’s important
    to evaluate our model from a fairness perspective and dig deeper into metrics
    other than overall model accuracy. The goal of this post-training analysis is
    to understand the trade-offs between model accuracy and the effects a model’s
    predictions will have on different groups.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 即使进行了严格的数据分析，偏见可能会潜入训练模型中。这可能是由于模型的架构、优化指标或在训练之前未能识别的数据偏差所致。为了解决这个问题，重要的是从公平的角度评估我们的模型，并深入研究除整体模型准确度之外的度量标准。这篇文章后训练分析的目标是理解模型准确性与模型预测对不同群体影响之间的权衡。
- en: The What-If Tool is one such option for post-model analysis. To demonstrate
    how to use it on a trained model, we’ll build on our mortgage dataset example.
    Based on our previous analysis, we’ve refined the dataset to only include loans
    for the purpose of refinancing or home purchases,^([10](ch07.xhtml#ch01fn43))
    and trained an XGBoost model to predict whether or not an application will be
    approved. Because we’re using XGBoost, we converted all categorical features into
    boolean columns using the pandas `get_dummies()` method.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: What-If 工具是进行模型后分析的一个选择。为了演示如何在训练模型上使用它，我们将在我们的抵押贷款数据集示例基础上构建。基于先前的分析，我们精炼了数据集，仅包括用于再融资或购房的贷款，^([10](ch07.xhtml#ch01fn43))
    并训练了一个 XGBoost 模型来预测申请是否会被批准。由于我们使用了 XGBoost，我们使用 pandas 的 `get_dummies()` 方法将所有分类特征转换为布尔列。
- en: 'We’ll make a few additions to our What-If Tool initialization code above, this
    time passing in a function that calls our trained model, along with configs specifying
    our label column and the name for each label:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在上述的 What-If 工具初始化代码中进行一些添加，这次传入一个调用我们训练模型的函数，同时配置指定我们的标签列和每个标签的名称：
- en: '[PRE11]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now that we’ve passed the tool our model, the resulting visualization shown
    in [Figure 7-13](#the_what-if_toolapostrophes_datapoint_e) plots our test datapoints
    according to our model’s prediction confidence indicated on the y-axis.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将我们的模型传递给了工具，生成的可视化如 [图 7-13](#the_what-if_toolapostrophes_datapoint_e)
    所示，根据 y 轴上我们模型的预测置信度绘制了我们的测试数据点。
- en: '![The What-If Tool’s Datapoint editor for a binary classification model. The
    y-axis is the model’s prediction output for each datapoint, ranging from 0 (denied)
    to 1 (approved).](Images/mldp_0713.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![What-If 工具用于二元分类模型的数据点编辑器。y 轴是每个数据点的模型预测输出，范围从 0（拒绝）到 1（通过）。](Images/mldp_0713.png)'
- en: Figure 7-13\. The What-If Tool’s Datapoint editor for a binary classification
    model. The y-axis is the model’s prediction output for each datapoint, ranging
    from 0 (denied) to 1 (approved).
  id: totrans-223
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-13\. What-If 工具用于二元分类模型的数据点编辑器。y 轴是每个数据点的模型预测输出，范围从 0（拒绝）到 1（通过）。
- en: The What-If Tool’s Performance & Fairness tab lets us evaluate our model’s fairness
    across different data slices. By selecting one of our model’s features to “Slice
    by,” we can compare our model’s results for different values of this feature.
    In [Figure 7-14](#the_what-if_tool_performance_ampersand), we’ve sliced by the
    `agency_code_HUD` feature—a boolean value indicating whether an application was
    underwritten by HUD (0 for non-HUD loans, 1 for HUD loans).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: What-If 工具的性能和公平性选项卡让我们能够评估我们模型在不同数据切片上的公平性。通过选择一个我们模型特征来“切片”，我们可以比较这个特征不同值的模型结果。在
    [图 7-14](#the_what-if_tool_performance_ampersand) 中，我们通过 `agency_code_HUD` 特征进行了切片，这是一个布尔值，指示申请是否由
    HUD 监督（非 HUD 贷款为 0，HUD 贷款为 1）。
- en: '![The What-If Tool Performance & Fairness tab, showing our XGBoost model performance
    across different feature values.](Images/mldp_0714.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![What-If 工具的性能和公平性选项卡，展示我们的 XGBoost 模型在不同特征值上的表现。](Images/mldp_0714.png)'
- en: Figure 7-14\. The What-If Tool Performance & Fairness tab, showing our XGBoost
    model performance across different feature values.
  id: totrans-226
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-14\. What-If 工具的性能和公平性选项卡，展示我们的 XGBoost 模型在不同特征值上的表现。
- en: 'From these Performance & Fairness charts, we can see:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些性能和公平性图表中，我们可以看到：
- en: Our model’s accuracy on loans supervised by HUD is significantly higher—94%
    compared to 85%.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们模型对由 HUD 监督的贷款的准确率显著更高——94% 相比于 85%。
- en: According to the confusion matrix, non-HUD loans are approved at a higher rate—72%
    compared to 55%. This is likely due to the data representation bias identified
    in the previous section (we purposely left the dataset this way to show how models
    can amplify data bias).
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据混淆矩阵，非 HUD 贷款的批准率更高——72%比55%。这很可能是由于前一节中识别出的数据表现偏差所致（我们特意保留了数据集以展示模型如何放大数据偏差）。
- en: There are a few ways to act on these insights, as shown in the “Optimization
    strategy” box in [Figure 7-14](#the_what-if_tool_performance_ampersand). These
    optimization methods involve changing our model’s *classification threshold*—the
    threshold at which a model will output a positive classification. In the context
    of this model, what confidence threshold are we OK with to mark an application
    as “approved”? If our model is more than 60% confident an application should be
    approved, should we approve it? Or are we only OK approving applications when
    our model is more than 98% confident? This decision is largely dependent on a
    model’s context and prediction task. If we’re predicting whether or not an image
    contains a cat, we may be OK returning the label “cat” even when our model is
    only 60% confident. However, if we have a model that predicts whether or not a
    medical image contains a disease, we’d likely want our threshold to be much higher.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以根据所展示的见解进行操作，如“优化策略”框中在[图 7-14](#the_what-if_tool_performance_ampersand)中所示。这些优化方法涉及更改我们模型的*分类阈值*——模型输出积极分类的阈值。在这个模型的背景下，我们对什么置信阈值表示“批准”申请感到满意？如果我们的模型超过60%的置信度认为申请应该被批准，我们应该批准吗？或者我们只在我们的模型超过98%的置信度时批准申请？这个决定在很大程度上取决于模型的背景和预测任务。如果我们预测一张图像是否包含猫，即使我们的模型只有60%的置信度，我们可能也可以返回标签“猫”。然而，如果我们有一个预测一张医学图像是否包含疾病的模型，我们可能希望我们的阈值要高得多。
- en: The What-If Tool helps us choose a threshold based on various optimizations.
    Optimizing for “Demographic parity,” for example, would ensure that our model
    approves the same percentage of applications for both HUD and non-HUD loans.^([11](ch07.xhtml#ch01fn44))
    Alternatively, using an equality of opportunity^([12](ch07.xhtml#ch01fn45)) fairness
    metric will ensure that datapoints from both the HUD and non-HUD slice with a
    ground truth value of “approved” in the test dataset are given an equal chance
    of being predicted “approved” by the model.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: What-If 工具帮助我们根据各种优化选择阈值。例如，优化“人口统计学平等”将确保我们的模型批准 HUD 和非 HUD 贷款的申请百分比相同^([11](ch07.xhtml#ch01fn44))。或者，使用“机会平等”公平性指标将确保
    HUD 和非 HUD 切片中在测试数据集中具有“批准”地面真实值的数据点被模型预测为“批准”的机会相等^([12](ch07.xhtml#ch01fn45))。
- en: Note that changing a model’s prediction threshold is only one way to act on
    fairness evaluation metrics. There are many other approaches, including rebalancing
    training data, retraining a model to optimize for a different metric, and more.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，更改模型的预测阈值仅是对公平性评估指标进行操作的一种方式。还有许多其他方法，包括重新平衡训练数据、重新训练模型以优化不同的指标等。
- en: Tip
  id: totrans-233
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The What-If Tool is model agnostic and can be used for any type of model regardless
    of architecture or framework. It works with models loaded within a notebook or
    in [TensorBoard](https://oreil.ly/xWV4_), models served via TensorFlow Serving,
    and models deployed to Cloud AI Platform Prediction. The What-If Tool team also
    created a tool for text-based models called the [Language Interpretability Tool
    (LIT)](https://oreil.ly/CZ60B).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: What-If 工具是与模型无关的，可以用于任何类型的模型，无论其架构或框架如何。它可以与加载在笔记本中或在[TensorBoard](https://oreil.ly/xWV4_)中的模型一起工作，也可以与通过
    TensorFlow Serving 提供的模型以及部署到 Cloud AI Platform Prediction 的模型一起工作。What-If 工具团队还为基于文本的模型创建了一个工具，称为[语言可解释性工具（LIT）](https://oreil.ly/CZ60B)。
- en: 'Another important consideration for post-training evaluation is testing our
    model on a balanced set of examples. If there are particular slices of our data
    that we anticipate will be problematic for our model—like inputs that could be
    affected by data collection or representation bias—we should ensure our test set
    includes enough of these cases. After splitting our data, we’ll use the same type
    of analysis we employed in the “Before training” part of this section on *each*
    split of our data: training, validation, and test.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 后训练评估的另一个重要考虑因素是在平衡的示例集上测试我们的模型。如果我们预期数据的特定切片对我们的模型可能存在问题——比如可能受数据收集或表现偏差影响的输入——我们应确保我们的测试集包含足够多这些案例。在拆分我们的数据之后，我们将在本节“训练前”部分使用与我们数据的*每个*拆分相同类型的分析：训练、验证和测试。
- en: As seen from this analysis, there is no one-size-fits-all solution or evaluation
    metric for model fairness. It is a continuous, iterative process that should be
    employed throughout an ML workflow—from data collection to deployed model.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本分析所示，不存在适用于所有情况的模型公平性解决方案或评估指标。这是一个持续的迭代过程，应在整个 ML 工作流程中使用 —— 从数据收集到部署模型。
- en: Trade-Offs and Alternatives
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 权衡与替代方案
- en: There are many ways to approach model fairness in addition to the pre- and post-training
    techniques discussed in the Solution section. Here, we’ll introduce a few alternative
    tools and processes for achieving fair models. ML fairness is a rapidly evolving
    area of research—the tools included in this section aren’t meant to provide an
    exhaustive list, but rather a few techniques and tools currently available for
    improving model fairness. We’ll also discuss the differences between the Fairness
    Lens and Explainable Predictions design patterns, as they are related and often
    used together.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 除了解决方案部分讨论的预训练和后训练技术外，还有许多方法可以处理模型公平性。在这里，我们将介绍一些实现公平模型的替代工具和流程。ML 公平性是一个快速发展的研究领域
    —— 本节包含的工具并非旨在提供详尽列表，而是当前可用于提升模型公平性的几种技术和工具。我们还将讨论公平性镜头和可解释预测设计模式之间的区别，因为它们相关且经常一起使用。
- en: Fairness Indicators
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 公平性指标
- en: '[Fairness Indicators](https://github.com/tensorflow/fairness-indicators) (FI)
    are a suite of open source tools designed to help in understanding a dataset’s
    distribution before training, and evaluating model performance using fairness
    metrics. The tools included in FI are TensorFlow Data Validation (TFDV) and TensorFlow
    Model Analysis (TFMA). Fairness Indicators are most often used as components in
    TFX pipelines (see [“Design Pattern 25: Workflow Pipeline”](ch06_split_000.xhtml#design_pattern_twofive_workflow_pipelin)
    in [Chapter 6](ch06_split_000.xhtml#reproducibility_design_patterns) for more
    details) or via TensorBoard. With TFX, there are two pre-built components that
    utilize Fairness Indicator tools:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[公平性指标](https://github.com/tensorflow/fairness-indicators)（FI）是一套开源工具，旨在帮助理解数据集在训练前的分布，并使用公平性指标评估模型性能。FI
    中包含的工具有 TensorFlow 数据验证（TFDV）和 TensorFlow 模型分析（TFMA）。公平性指标通常作为 TFX 流水线的组件（详见[“设计模式
    25：工作流程管道”](ch06_split_000.xhtml#design_pattern_twofive_workflow_pipelin)在[第 6
    章](ch06_split_000.xhtml#reproducibility_design_patterns)中获取更多细节），或通过 TensorBoard
    使用。在 TFX 中，有两个预构建组件利用公平性指标工具：'
- en: ExampleValidator for data analysis, detecting drift, and training–serving skew
    with TFDV.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ExampleValidator 用于数据分析，检测漂移以及使用 TFDV 进行训练-服务偏差。
- en: Evaluator uses the TFMA library to evaluate a model across subsets of a dataset.
    An example of an interactive visualization generated from TFMA is shown in [Figure 7-15](#comparing_a_modelapostrophes_false_nega).
    This looks at one feature in the data (height) and breaks down the model’s false
    negative rate for each possible categorical value of that feature.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估器使用 TFMA 库评估数据集的模型子集。从 TFMA 生成的交互式可视化示例在 [图 7-15](#comparing_a_modelapostrophes_false_nega)
    中展示。它查看数据中的一个特征（身高），并分析该特征的每个可能分类值的模型假阴性率。
- en: '![Comparing a model’s false negative right over different subsets of data.](Images/mldp_0715.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![比较模型在不同数据子集上的假阴性率。](Images/mldp_0715.png)'
- en: Figure 7-15\. Comparing a model’s false negative rate over different subsets
    of data.
  id: totrans-244
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-15\. 比较模型在不同数据子集上的假阴性率。
- en: From the [Fairness Indicators Python package](https://oreil.ly/pYM1j), TFMA
    can also be used as a standalone tool that works with both TensorFlow and non-TensorFlow
    models.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 从 [公平性指标 Python 包](https://oreil.ly/pYM1j) 看，TFMA 还可以作为一个独立工具，与 TensorFlow 和非
    TensorFlow 模型一起使用。
- en: Automating data evaluation
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动化数据评估
- en: 'The fairness evaluation methods we discussed in the Solution section focused
    on manual, interactive data and model analysis. This type of analysis is important,
    especially in the initial phases of model development. As we operationalize our
    model and shift our focus to maintaining and improving it, finding ways to automate
    fairness evaluation will improve efficiency and ensure that fairness is integrated
    throughout our ML process. We can do this through [“Design Pattern 18: Continued
    Model Evaluation”](ch05.xhtml#design_pattern_oneeight_continued_model) discussed
    in [Chapter 5](ch05.xhtml#design_patterns_for_resilient_serving), or with [“Design
    Pattern 25: Workflow Pipeline”](ch06_split_000.xhtml#design_pattern_twofive_workflow_pipelin)
    in [Chapter 6](ch06_split_000.xhtml#reproducibility_design_patterns) using components
    like those provided by TFX for data analysis and model evaluation.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在解决方案部分讨论的公平评估方法主要侧重于手动交互式数据和模型分析。这种类型的分析在模型开发的初期阶段尤为重要。随着我们将模型投入运行并专注于其维护和改进，找到自动化公平评估的方法将提高效率，并确保公平性贯穿整个
    ML 过程。我们可以通过讨论的[“设计模式 18：持续模型评估”](ch05.xhtml#design_pattern_oneeight_continued_model)在[第
    5 章](ch05.xhtml#design_patterns_for_resilient_serving)，或使用[“设计模式 25：工作流管道”](ch06_split_000.xhtml#design_pattern_twofive_workflow_pipelin)在[第
    6 章](ch06_split_000.xhtml#reproducibility_design_patterns)中进行这样的操作，使用像 TFX 提供的数据分析和模型评估组件。
- en: Allow and disallow lists
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 允许和禁止列表
- en: When we can’t find a way to fix inherent bias in our data or model directly,
    it’s possible to hardcode rules on top of our production model using allow and
    disallow lists. This applies mostly to classification or generative models, when
    there are labels or words we don’t want our model to return. As an example, gendered
    words such as “man” and “woman” [were removed](https://oreil.ly/WY2vp) from Google
    Cloud Vision API’s label detection feature. Because gender cannot be determined
    by appearance alone, it would have reinforced unfair biases to return these labels
    when the model’s prediction is based solely on visual features. Instead, the Vision
    API returns “person.” Similarly, the Smart Compose feature in Gmail [avoids the
    use of gendered pronouns](https://oreil.ly/dtMhK) when completing sentences such
    as “I am meeting an investor next week. Do you want to meet ___?”
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们无法直接修复数据或模型中固有偏见时，可以在生产模型之上硬编码规则，使用允许和禁止列表。这主要适用于分类或生成模型，当我们不希望模型返回某些标签或词语时。例如，如
    Google Cloud Vision API 的标签检测功能中已经移除了类别化的词语“man”和“woman” [详见](https://oreil.ly/WY2vp)。因为仅凭外观无法确定性别，如果仅基于视觉特征返回这些标签，会强化不公平的偏见。因此，Vision
    API 返回“person”。同样，在 Gmail 的智能撰写功能中，避免使用性别化代词 [详见](https://oreil.ly/dtMhK)，例如在完成句子“我下周将见投资人，你要见___吗？”时。
- en: 'These allow and disallow lists can be applied in one of two phases in an ML
    workflow:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ML 工作流的两个阶段之一中可以应用这些允许和禁止列表：
- en: Data collection
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 数据收集
- en: When training a model from scratch or using the Transfer Learning design pattern
    to add our own classification layer, we can define our model’s label set in the
    data collection phase, before a model has been trained.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 当从头开始训练模型或使用迁移学习设计模式添加我们自己的分类层时，我们可以在数据收集阶段定义模型的标签集，这是在模型训练之前。
- en: After training
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后
- en: If we’re relying on a pre-trained model for predictions, and are using the same
    labels from that model, an allow and disallow list can be implemented in production—after
    the model returns a prediction but before those labels are surfaced to end users.
    This could also apply to text generation models, where we don’t have complete
    control of all possible model outputs.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们依赖预训练模型进行预测，并且使用与该模型相同的标签，可以在生产中实施允许和禁止列表——在模型返回预测结果后，但在这些标签呈现给最终用户之前。这也适用于文本生成模型，其中我们无法完全控制所有可能的模型输出。
- en: Data augmentation
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据增强
- en: In addition to the data distribution and representation solutions discussed
    earlier, another approach to minimizing model bias is to perform data *augmentation*.
    Using this approach, data is changed before training with the goal of removing
    potential sources of bias. One specific type of data augmentation is known as
    ablation, and is especially applicable in text models. In a text sentiment analysis
    model, for example, we could remove identity terms from text to ensure they don’t
    influence our model’s predictions. Building on the ice cream example we used earlier
    in this section, the sentence “Mint chip is their best ice cream flavor” would
    become “BLANK is their best ice cream flavor” after applying ablation. We’d then
    replace all other words throughout the dataset that we didn’t want to influence
    the model’s sentiment prediction with the same word (we used BLANK here, but anything
    not present in the rest of the text data will work). Note that while this ablation
    technique works well for many text models, it’s important to be careful when removing
    areas of bias from tabular datasets, as mentioned in the Problem section.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 除了之前讨论的数据分布和表示解决方案之外，减少模型偏差的另一种方法是进行数据*增强*。使用这种方法，在训练之前会修改数据，以消除潜在的偏见源。数据增强的一种具体类型被称为剔除，特别适用于文本模型。例如，在文本情感分析模型中，我们可以从文本中删除身份术语，以确保它们不会影响我们模型的预测。继续我们在本节早些时候使用的冰淇淋示例，句子“Mint
    chip is their best ice cream flavor”在应用剔除后会变成“BLANK is their best ice cream flavor”。然后，我们会将数据集中的所有其他单词替换为相同的词（我们这里使用了BLANK，但是数据集中未出现的任何单词也可以）。请注意，虽然这种剔除技术对许多文本模型效果很好，但是在从表格数据集中移除偏见区域时需要小心，如问题部分所述。
- en: Another data augmentation approach involves generating new data, and it was
    used by Google Translate to [minimize gender bias](https://oreil.ly/3Rkdr) when
    translating text to and from gender-neutral and gender-specific languages. The
    solution involved rewriting translation data such that when applicable, a provided
    translation would be offered in both the feminine and masculine form. For example,
    the gender-neutral English sentence “We are doctors” would yield two results when
    being translated to Spanish, as seen in [Figure 7-16](#when_translating_a_gender-neutral_word).
    In Spanish, the word “we” can have both a feminine and masculine form.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种数据增强方法涉及生成新数据，谷歌翻译在将文本从性别中性到性别特定语言和从性别特定到性别中性语言时使用了这种方法以[减少性别偏见](https://oreil.ly/3Rkdr)。解决方案包括重新编写翻译数据，以便在适用时，提供翻译的女性和男性形式。例如，性别中性的英文句子“We
    are doctors”在翻译成西班牙语时会产生两个结果，如在[图7-16](#when_translating_a_gender-neutral_word)中所示。在西班牙语中，“we”可以有女性和男性形式。
- en: '![When translating a gender-neutral word in one language (here, the word “we”
    in English) to a language where that word is gender-specific, Google Translate
    now provides multiple translations to minimize gender bias.](Images/mldp_0716.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![当将一个语言中的性别中性词（这里是英语中的“we”）翻译成一个性别特定的语言时，Google Translate 现在提供多种翻译选项以减少性别偏见。](Images/mldp_0716.png)'
- en: Figure 7-16\. When translating a gender-neutral word in one language (here,
    the word “we” in English) to a language where that word is gender-specific, Google
    Translate now provides multiple translations to minimize gender bias.
  id: totrans-259
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 7-16\. 当将一个语言中的性别中性词（这里是英语中的“we”）翻译成一个性别特定的语言时，Google Translate 现在提供多种翻译选项以减少性别偏见。
- en: Model Cards
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型卡
- en: Originally introduced in a [research paper](https://oreil.ly/OAIcs), Model Cards
    provide a framework for reporting a model’s capabilities and limitations. The
    goal of Model Cards is to improve model transparency by providing details on scenarios
    where a model should and should not be used, since mitigating problematic bias
    only works if a model is used in the way it was intended. In this way, Model Cards
    encourage accountability for using a model in the correct context.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 最初在一篇[研究论文](https://oreil.ly/OAIcs)中引入，模型卡提供了报告模型能力和限制的框架。模型卡的目标是通过提供模型适用和不适用的场景细节来提高模型的透明性，因为减轻问题性偏差只有在模型按照预期的方式使用时才会起作用。通过模型卡，鼓励在正确的上下文中使用模型的责任感。
- en: The first [Model Cards](https://oreil.ly/OwiJY) released provide summaries and
    fairness metrics for the Face Detection and Object Detection features in Google
    Cloud’s Vision API. To generate Model Cards for our own ML models, TensorFlow
    provides a [Model Card Toolkit](https://github.com/tensorflow/model-card-toolkit)
    (MCT) that can be run as a standalone Python library or as part of a TFX pipeline.
    The toolkit reads exported model assets and generates a series of charts with
    various performance and fairness metrics.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个[模型卡片](https://oreil.ly/OwiJY)发布提供了Google Cloud Vision API中面部检测和物体检测功能的摘要和公平度量。为了为我们自己的ML模型生成模型卡片，TensorFlow提供了一个[模型卡片工具包](https://github.com/tensorflow/model-card-toolkit)（MCT），可以作为独立的Python库运行，也可以作为TFX管道的一部分运行。该工具包读取导出的模型资产，并生成一系列具有各种性能和公平度量的图表。
- en: Fairness versus explainability
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 公平性与可解释性
- en: The concepts of fairness and explainability in ML are sometimes confused since
    they are often used together and are both part of the larger initiative of Responsible
    AI. Fairness applies specifically to identifying and removing bias from models,
    and explainability is *one* approach for diagnosing the presence of bias. For
    example, applying explainability to a sentiment analysis model might reveal that
    the model is relying on identity terms to make its prediction when it should instead
    be using words like “worst,” “amazing,” or “not.”
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在ML中，公平性和可解释性的概念有时会引起混淆，因为它们经常一起使用，并且都是责任AI大计划的一部分。公平性特别适用于识别和消除模型中的偏见，而解释性则是诊断偏见存在的一种方法。例如，将解释性应用于情感分析模型可能会揭示，该模型依赖于身份术语进行预测，而应该使用像“最差”，“惊人”或“不是”等词语。
- en: Explainability can also be used outside the context of fairness to reveal things
    like why a model is flagging particular fraudulent transactions, or the pixels
    that caused a model to predict “diseased” in a medical image. Explainability,
    therefore, is a method for improving model transparency. Sometimes transparency
    can reveal areas where a model is treating certain groups unfairly, but it can
    also provide higher-level insight into a model’s decision-making process.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 解释性也可以在公平性背景之外使用，以揭示模型为什么会标记特定的欺诈交易，或者导致模型在医学图像中预测“患病”的像素。因此，解释性是改进模型透明度的一种方法。有时透明性可以揭示模型在处理某些群体时存在不公平的情况，但也可以提供更高层次的洞察模型的决策过程。
- en: Summary
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: While Peter Parker may not have been referring to machine learning when he said,
    “With great power comes great responsibility,” the quote certainly applies here.
    ML has the power to disrupt industries, improve productivity, and generate new
    insights from data. With this potential, it’s especially important that we understand
    how our models will impact different groups of stakeholders. Model stakeholders
    could include varying demographic slices of model users, regulatory groups, a
    data science team, or business teams within an organization.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然彼得·帕克在说“伴随着强大的力量而来的是巨大的责任”时可能并不是在谈论机器学习，但这句话在这里确实适用。ML有能力颠覆行业、提高生产力，并从数据中获得新的见解。有了这种潜力，我们尤其需要了解我们的模型将如何影响不同利益相关者群体。模型的利益相关者可能包括不同的模型用户人群、监管组织、数据科学团队，或者组织内的业务团队。
- en: The Responsible AI patterns outlined in this chapter are an essential part of
    every ML workflow—they can help us better understand the predictions generated
    by our models and catch potential adverse behavior before models go to production.
    Starting with the *Heuristic Benchmark* pattern, we looked at how to identify
    an initial metric for model evaluation. This metric is useful as a comparison
    point for understanding subsequent model versions and summarizing model behavior
    for business decision makers. In the *Explainable Predictions* pattern, we demonstrated
    how to use feature attributions to see which features were most important in signaling
    a model’s prediction. Feature attributions are one type of explainability method
    and can be used for both evaluating the prediction on a single example or over
    a group of test inputs. Finally, the *Fairness Lens* design pattern presented
    tools and metrics for ensuring a model’s predictions treat all groups of users
    in a way that is fair, equitable, and unbiased.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍的负责任人工智能模式是每个机器学习工作流程的重要组成部分——它们可以帮助我们更好地理解模型生成的预测，并在模型投入生产之前捕捉潜在的不良行为。从*启发式基准*模式开始，我们研究了如何识别模型评估的初始度量标准。这种度量标准对于理解后续模型版本和总结业务决策者的模型行为非常有用。在*可解释预测*模式中，我们展示了如何使用特征归因来查看哪些特征在信号化模型预测中最重要。特征归因是一种解释方法，可以用于评估单个示例或一组测试输入的预测。最后，在*公平性透镜*设计模式中，我们介绍了确保模型预测以公平、公正和无偏见方式对待所有用户群体的工具和度量标准。
- en: ^([1](ch07.xhtml#ch01fn34-marker)) DR is an eye condition affecting millions
    of people around the world. It can lead to blindness, but if caught early, it
    can be successfully treated. To learn more and find the dataset, see [here](https://oreil.ly/ix21h).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch07.xhtml#ch01fn34-marker)) DR是一种影响全球数百万人的眼部疾病。它可能导致失明，但如果早期发现，可以成功治疗。要了解更多信息并找到数据集，请参见[这里](https://oreil.ly/ix21h)。
- en: ^([2](ch07.xhtml#idm46056078011496-marker)) Explanations were used to identify
    and correct for annotations present in radiology images in [this study](https://oreil.ly/qowNO).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch07.xhtml#idm46056078011496-marker)) 解释被用来识别和纠正放射图像中存在的注释，详见[这项研究](https://oreil.ly/qowNO)。
- en: ^([3](ch07.xhtml#ch01fn35-marker)) The model discussed here is trained on a
    [public UCI dataset](https://oreil.ly/cNixp).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch07.xhtml#ch01fn35-marker)) 此处讨论的模型是在[公共UCI数据集](https://oreil.ly/cNixp)上训练的。
- en: ^([4](ch07.xhtml#ch01fn36-marker)) The [scikit-learn documentation](https://oreil.ly/DAmIm)
    goes into more detail on how to correctly interpret the learned weights in linear
    models.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch07.xhtml#ch01fn36-marker)) [scikit-learn文档](https://oreil.ly/DAmIm)详细介绍了如何正确解释线性模型中学到的权重。
- en: ^([5](ch07.xhtml#ch01fn37-marker)) We’re focusing on these two explainability
    methods since they are widely used and cover a variety of model types, but there
    are many other methods and frameworks not included in this analysis, such as [LIME](https://oreil.ly/0c4uB)
    and [ELI5](https://github.com/TeamHG-Memex/eli5).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch07.xhtml#ch01fn37-marker)) 我们专注于这两种解释方法，因为它们被广泛使用并涵盖多种模型类型，但本分析未包括的其他方法和框架包括[LIME](https://oreil.ly/0c4uB)和[ELI5](https://github.com/TeamHG-Memex/eli5)。
- en: ^([6](ch07.xhtml#ch01fn39-marker)) For more details on these explanation methods
    and their implementation, see the [Explainable AI](https://oreil.ly/PYn8P) whitepaper.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch07.xhtml#ch01fn39-marker)) 关于这些解释方法及其实现的更多细节，请参见[可解释人工智能](https://oreil.ly/PYn8P)白皮书。
- en: ^([7](ch07.xhtml#ch01fn40-marker)) For more details on Quick, Draw! and example-based
    explanations, see this [paper](https://oreil.ly/Yvexy).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch07.xhtml#ch01fn40-marker)) 关于Quick, Draw!和基于示例的解释的更多细节，请参见这篇[paper](https://oreil.ly/Yvexy)。
- en: '^([8](ch07.xhtml#ch01fn41-marker)) For a more detailed look on how race and
    gender bias can find their way into image classification models, see Joy Buolamwini
    and Timmit Gebru, [“Gender Shades: Intersectional Accuracy Disparities in Commercial
    Gender Classification”](https://oreil.ly/1zw3e), *Proceedings of Machine Learning
    Research* 81 (2018): 1-15.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '^([8](ch07.xhtml#ch01fn41-marker)) 关于种族和性别偏见如何影响图像分类模型的更详细信息，请参见Joy Buolamwini和Timmit
    Gebru的[“性别阴影：商业性别分类中的交叉准确性差异”](https://oreil.ly/1zw3e)，*机器学习研究会议论文集* 81 (2018):
    1-15。'
- en: '^([9](ch07.xhtml#ch01fn42-marker)) To learn more about changing a prediction
    task, see [“Design Pattern 5: Reframing ”](ch03.xhtml#design_pattern_five_reframing)
    and [“Design Pattern 9: Neutral Class ”](ch03.xhtml#design_pattern_nine_neutral_class)
    in [Chapter 3](ch03.xhtml#problem_representation_design_patterns).'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '^([9](ch07.xhtml#ch01fn42-marker)) 要了解更多关于改变预测任务的信息，请参见[“设计模式 5: 重新构架”](ch03.xhtml#design_pattern_five_reframing)和[“设计模式
    9: 中性类”](ch03.xhtml#design_pattern_nine_neutral_class)，位于[第 3 章](ch03.xhtml#problem_representation_design_patterns)中。'
- en: ^([10](ch07.xhtml#ch01fn43-marker)) There are many more pre-training optimizations
    that could be made on this dataset. We’ve chosen just one here as a demo of what’s
    possible.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch07.xhtml#ch01fn43-marker)) 在这个数据集上，还可以进行许多更多的预训练优化。我们在这里只选择了一个作为演示可能性的示例。
- en: ^([11](ch07.xhtml#ch01fn44-marker)) [This article](https://oreil.ly/wFx_W) provides
    more detail on the What-If Tool’s options for fairness optimization strategies.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch07.xhtml#ch01fn44-marker)) [本文](https://oreil.ly/wFx_W)提供了更多关于What-If工具用于公平优化策略的详细信息。
- en: ^([12](ch07.xhtml#ch01fn45-marker)) More details on equality of opportunity
    as a fairness metric can be found [here](https://oreil.ly/larIS).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch07.xhtml#ch01fn45-marker)) 更多关于机会平等作为公平度量标准的详细信息，请查看[此处](https://oreil.ly/larIS)。
