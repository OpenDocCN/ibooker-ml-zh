- en: Chapter 14\. Data Privacy for Machine Learning
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第14章 机器学习的数据隐私
- en: In this chapter, we introduce some aspects of data privacy as they apply to
    machine learning pipelines. Privacy-preserving machine learning is a very active
    area of research that is just beginning to be incorporated into TensorFlow and
    other frameworks. We’ll explain some of the principles behind the most promising
    techniques at the time of writing and show some practical examples for how they
    can fit into a machine learning pipeline.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍数据隐私的一些方面，以其在机器学习流水线中的应用。隐私保护机器学习是一个非常活跃的研究领域，刚开始被纳入TensorFlow和其他框架。我们将解释写作时最有前途的技术背后的一些原则，并展示它们如何在机器学习流水线中应用的一些实际示例。
- en: 'We’ll cover three main methods for privacy-preserving machine learning in this
    chapter: differential privacy, federated learning, and encrypted machine learning.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍隐私保护机器学习的三种主要方法：差分隐私、联邦学习和加密机器学习。
- en: Data Privacy Issues
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 数据隐私问题
- en: 'Data privacy is all about trust and limiting the exposure of data that people
    would prefer to keep private. There are many different methods for privacy-preserving
    machine learning, and in order to choose between them, you should try to answer
    the following questions:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 数据隐私关乎信任和限制人们希望保密的数据的曝光。有许多不同的隐私保护机器学习方法，为了在它们之间做出选择，您应该尝试回答以下问题：
- en: Who are you trying to keep the data private from?
  id: totrans-5
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你试图保护数据免受谁的侵扰？
- en: Which parts of the system can be private, and which can be exposed to the world?
  id: totrans-6
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 系统的哪些部分可以保护隐私，哪些可以向世界公开？
- en: Who are the trusted parties that can view the data?
  id: totrans-7
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 谁是可以查看数据的可信方？
- en: The answers to these questions will help you decide which of the methods described
    in this chapter best fits your use case.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题的答案将帮助您决定本章描述的方法中哪一个最适合您的用例。
- en: Why Do We Care About Data Privacy?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们关心数据隐私？
- en: Data privacy is becoming an important part of machine learning projects. There
    are many legal requirements surrounding user privacy, such as the EU’s General
    Data Protection Regulation (GDPR), which went into effect in May 2018, and the
    California Consumer Privacy Act of January 2020\. There are ethical considerations
    around the use of personal data for machine learning, and users of products powered
    by ML are starting to care deeply about what happens to their data. Because machine
    learning has traditionally been hungry for data, and because many of the predictions
    made by machine learning models are based on personal data collected from users,
    machine learning is at the forefront of debates around data privacy.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数据隐私正在成为机器学习项目的重要组成部分。有许多法律要求围绕用户隐私，例如2018年5月生效的欧盟通用数据保护条例（GDPR）和2020年1月生效的加利福尼亚消费者隐私法案。关于将个人数据用于机器学习存在伦理考虑，而使用由ML驱动的产品的用户开始关心其数据的去向。因为机器学习传统上需要大量数据，并且因为许多机器学习模型的预测基于从用户收集的个人数据，所以机器学习处于围绕数据隐私展开的辩论前沿。
- en: 'At the time of writing, there’s always a cost to privacy: adding privacy comes
    with a cost in model accuracy, computation time, or both. At one extreme, collecting
    no data keeps an interaction completely private but is completely useless for
    machine learning. At the other extreme, knowing all the details about a person
    might endanger that person’s privacy, but it allows us to make very accurate machine
    learning models. We’re just now starting to see the development of privacy-preserving
    ML, in which privacy can be increased without such a large trade-off in model
    accuracy.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写时，隐私总是有成本的：增加隐私会带来模型精度、计算时间或两者的成本。在一个极端，不收集数据使得交互完全私密，但对机器学习来说完全无用。在另一个极端，了解一个人的所有细节可能会危及其隐私，但这使我们能够创建非常精确的机器学习模型。我们现在刚开始看到隐私保护ML的发展，其中可以增加隐私而不会如此大幅降低模型精度。
- en: In some situations, privacy-preserving machine learning can help you use data
    that would otherwise be unavailable for training a machine learning model due
    to privacy concerns. It doesn’t, however, give you free rein to do whatever you
    like with the data just because you use one of the methods in this chapter. You
    should discuss your plans with other stakeholders, for example, the data owners,
    privacy experts, and even your company’s legal team.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，隐私保护的机器学习可以帮助您使用由于隐私问题而无法用于训练机器学习模型的数据。然而，这并不意味着只要使用本章节中的某种方法，就可以随意处理数据。您应该与其他利益相关者讨论您的计划，例如数据所有者、隐私专家甚至公司的法律团队。
- en: The Simplest Way to Increase Privacy
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 增加隐私的最简单方法
- en: Often, the default strategy for building a product powered by machine learning
    is to collect all the data possible and then decide afterward what is useful for
    training a machine learning model. Even though this is done with the user’s consent,
    the simplest way to increase user privacy is to only collect the data that is
    necessary for the training of a particular model. In the case of structured data,
    fields such as name, gender, or race can simply be deleted. Text or image data
    can be processed to remove much personal information, such as deleting faces from
    images or names from text. However, in some cases this can reduce the utility
    of the data or make it impossible to train an accurate model. And if data on race
    and gender is not collected, it’s impossible to tell whether a model is biased
    against a particular group.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，建立基于机器学习的产品的默认策略是收集所有可能的数据，然后再决定哪些数据对训练特定模型有用。尽管这是在用户同意的情况下进行的，增加用户隐私的最简单方法是仅收集训练特定模型所需的数据。在结构化数据的情况下，诸如姓名、性别或种族等字段可以直接删除。文本或图像数据可以处理以删除大部分个人信息，例如从图像中删除面部或从文本中删除姓名。然而，在某些情况下，这可能会降低数据的效用或使其无法训练出准确的模型。如果不收集种族和性别的数据，就无法判断模型是否对特定群体存在偏见。
- en: 'Control of what data is collected can also be passed to the user: consent to
    collect data can be made more nuanced than a simple opt-in or opt-out selection,
    and the user of a product can specify exactly what data may be collected about
    them. This raises design challenges: should users who provide less data receive
    less accurate predictions than the users who contribute more data? How do we track
    consent through machine learning pipelines? How do we measure the privacy impact
    of a single feature in our models? These are all questions that need more discussion
    in the machine learning community.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的收集控制也可以交给用户：收集数据的同意可以比简单的选择“接受”或“拒绝”更为细致，产品的使用者可以明确指定关于他们的数据可以收集哪些。这带来了设计挑战：提供较少数据的用户是否应该接收到比提供更多数据的用户更不准确的预测？我们如何通过机器学习管道跟踪同意？如何衡量我们模型中单个特征的隐私影响？这些都是机器学习社区需要更多讨论的问题。
- en: What Data Needs to Be Kept Private?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 哪些数据需要保持私密？
- en: In machine learning pipelines, data is often collected from people, but some
    data has a higher need for privacy-preserving machine learning. Personally identifying
    information (PII) is data that can directly identify a single person, such as
    their name, email address, street address, ID number, and so on, and this needs
    to be kept private. PII can appear in free text, such as feedback comments or
    customer service data, not just when users are directly asked for this data. Images
    of people may also be considered PII in some circumstances. There are often legal
    standards around this—if your company has a privacy team, it’s best to consult
    them before embarking on a project using this type of data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习管道中，数据通常来自人们，但某些数据更需要进行隐私保护的机器学习。个人识别信息（PII）是可以直接识别单个人的数据，例如他们的姓名、电子邮件地址、街道地址、ID号码等，这些需要保持私密。PII可能出现在自由文本中，例如反馈评论或客户服务数据，而不仅仅是在直接要求用户提供这些数据时。在某些情况下，人们的图像也可能被视为PII。通常会有法律标准约束这一点——如果您的公司有隐私团队，最好在启动使用此类数据的项目之前咨询他们。
- en: Sensitive data also requires special care. This is often defined as data that
    could cause harm to someone if it were released, such as health data or proprietary
    company data (e.g., financial data). Care should be taken to ensure that this
    type of data is not leaked in the predictions of a machine learning model.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 敏感数据也需要特别小心。这通常被定义为可能会对某人造成伤害的数据，例如健康数据或专有公司数据（例如财务数据）。在确保这类数据不会在机器学习模型的预测中泄露的过程中，应该小心谨慎。
- en: Another category is quasi-identifying data. Quasi-identifiers can uniquely identify
    someone if enough of them are known, such as location tracking or credit card
    transaction data. If several location points are known about the same person,
    this provides a unique trace that can be combined with other datasets to reidentify
    that person. In December 2019, the New York Times published [an in-depth piece](https://oreil.ly/VPea0)
    on reidentification using cellphone data, which represents just one of several
    voices questioning the release of such data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类是准标识数据。如果已知足够多的准标识符，比如位置跟踪或信用卡交易数据，准标识数据可以唯一标识某人。如果了解了同一人的多个位置点，这就提供了一个可以与其他数据集组合以重新识别该人的唯一迹象。2019年12月，《纽约时报》发表了关于使用手机数据重新识别的[深度文章](https://oreil.ly/VPea0)，这只是众多质疑释放此类数据的声音之一。
- en: Differential Privacy
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私
- en: If we have identified a need for additional privacy in the machine learning
    pipeline, there are different methods that can help increase privacy while retaining
    as much data utility as possible. The first one we’ll discuss is differential
    privacy.[1](#filepos1585345) Differential privacy (DP) is a formalization of the
    idea that a query or a transformation of a dataset should not reveal whether a
    person is in that dataset. It gives a mathematical measure of the privacy loss
    that a person experiences by being included in a dataset and minimizes this privacy
    loss through the addition of noise.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们确定了机器学习流程中需要额外隐私的需求，有不同的方法可以在尽可能保留数据效用的同时增加隐私。我们将讨论的第一个方法是差分隐私[1](#filepos1585345)。差分隐私（DP）是一个概念的形式化，即查询或数据集的转换不应揭示一个人是否在该数据集中。它通过添加噪音来给出一个人在数据集中的隐私损失的数学度量，并通过最小化这种隐私损失来减少它。
- en: 'Differential privacy describes a promise, made by a data holder, or curator,
    to a data subject, and the promise is like this: “You will not be affected, adversely
    or otherwise, by allowing your data to be used in any study or analysis, no matter
    what other studies, datasets or information sources are available.”'
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 差分隐私描述了数据持有者或馆长向数据主体作出的承诺，这个承诺如下：“无论其他研究、数据集或信息来源如何，允许您的数据用于任何研究或分析都不会对您产生不利影响。”
- en: Cynthia Dwork[2](#filepos1585724)
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 辛西娅·德沃克[2](#filepos1585724)
- en: To put it another way, a transformation of a dataset that respects privacy should
    not change if one person is removed from that dataset. In the case of machine
    learning models, if a model has been trained with privacy in mind, then the predictions
    that a model makes should not change if one person is removed from the training
    set. DP is achieved by the addition of some form of noise or randomness to the
    transformation.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，尊重隐私的数据集转换，在从该数据集中删除一个人时不应该改变。对于机器学习模型而言，如果一个模型在设计时考虑了隐私，那么模型做出的预测在从训练集中删除一个人后不应该改变。差分隐私通过向转换添加某种形式的噪音或随机性来实现。
- en: To give a more concrete example, one of the simplest ways of achieving differential
    privacy is the concept of randomized response, as shown in [Figure 14-1](#filepos1533669).
    This is useful in surveys that ask sensitive questions, such as “Have you ever
    been convicted of a crime?” To answer this question, the person being asked flips
    a coin. If it comes up heads, they answer truthfully. If it comes up tails, they
    flip again and answer “Yes” if the coin comes up heads, and “No” if the coin comes
    up tails. This gives them deniability—they can say that they gave a random answer
    rather than a truthful answer. Because we know the probabilities for a coin flip,
    if we ask a lot of people this question, we can calculate the proportion of people
    who have been convicted of a crime with reasonable accuracy. The accuracy of the
    calculation increases when larger numbers of people participate in the survey.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要给出一个更具体的例子，实现差分隐私的最简单方法之一是随机响应的概念，如[图 14-1](#filepos1533669)所示。这在问卷调查中尤其有用，这些问卷会问一些敏感问题，比如“你是否曾被判犯罪？”被问者需要抛硬币来回答这个问题。如果硬币正面朝上，他们就如实回答。如果是反面，他们需要再抛一次，并根据硬币的结果回答“是”或“否”。这使他们能够否认，称他们给出的是随机答案而不是真实答案。由于我们知道硬币抛掷的概率，如果我们向很多人询问这个问题，我们可以相对准确地计算出犯罪记录的比例。当更多人参与调查时，计算的准确性会增加。
- en: '![](images/00036.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00036.jpg)'
- en: Figure 14-1\. Randomized response flowchart
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-1\. 随机响应流程图
- en: These randomized transformations are the key to DP.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这些随机变换是差分隐私的关键。
- en: ASSUME ONE TRAINING EXAMPLE PER PERSON
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 假设每个人只有一个训练样本。
- en: Throughout this chapter, for simplicity, we assume that each training example
    in a dataset is associated with or collected from one individual person.
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在本章中，为简化起见，假设数据集中的每个训练样本都与一个独立的个人相关联或收集自个人。
- en: Local and Global Differential Privacy
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本地和全局差分隐私
- en: 'DP can be divided into two main methods: local and global DP. In local DP,
    noise or randomness is added at the individual level, as in the randomized response
    example earlier, so privacy is maintained between an individual and the collector
    of the data. In global DP, noise is added to a transformation on the entire dataset.
    The data collector is trusted with the raw data, but the result of the transformation
    does not reveal data about an individual.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: DP 可以分为两种主要方法：本地 DP 和全局 DP。在本地 DP 中，如前面的随机响应示例中一样，会在个体级别添加噪音或随机性，因此在个体和数据收集者之间保持隐私。在全局
    DP 中，会对整个数据集的转换添加噪音。数据收集者信任原始数据，但转换结果不会泄露有关个人的数据。
- en: Global DP requires us to add less noise compared to local DP, which leads to
    a utility or accuracy improvement of the query for a similar privacy guarantee.
    The downside is that the data collector must be trusted for global DP, whereas
    for local DP only individual users see their own raw data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 全局 DP 要求我们相对于本地 DP 添加更少的噪音，这导致在类似的隐私保证下查询的效用或准确性有所提高。缺点是对于全局 DP，必须信任数据收集者，而对于本地
    DP，只有个体用户看到他们自己的原始数据。
- en: Epsilon, Delta, and the Privacy Budget
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ε、δ 和隐私预算
- en: Probably the most common way of implementing DP is using (epsilon-delta) DP
    . When comparing the result of a randomized transformation on a dataset that includes
    one specific person with another result that does not contain that person, describes
    the maximum difference between the outcomes of these transformations. So, if is
    0, both transformations return exactly the same result. If the value of is smaller,
    the probability that our transformations will return the same result is greater—a
    lower value of is more private because measures the strength of the privacy guarantee.
    If you query a dataset more than once, you need to sum the epsilons of each query
    to get your total privacy budget.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 实现 DP 的最常见方式可能是使用（ε-δ）DP。当比较包含特定个人的数据集上的随机转换结果与不包含该人的另一个结果时，描述这些转换结果之间的最大差异。因此，如果
    ε 是 0，两个转换将完全返回相同的结果。如果 ε 的值较小，则这两个转换返回相同结果的概率较大；ε 值越小，隐私保证越强。如果多次查询数据集，则需要将每个查询的
    ε 相加以获得总的隐私预算。
- en: 'is the probability that does not hold, or the probability that an individual’s
    data is exposed in the results of the randomized transformation. We generally
    set to be approximately the inverse of the population size: for a dataset containing
    2,000 people, we would set to be 1/1,000.[3](#filepos1586227)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 是不满足的概率，或者说个体数据在随机转换结果中暴露的概率。通常情况下，我们将设置为人口规模的倒数：例如，对于包含 2000 人的数据集，我们将设置为 1/1000。[3](#filepos1586227)
- en: What value of epsilon should you choose? allows us to compare the privacy of
    different algorithms and approaches, but the absolute value that gives us “sufficient”
    privacy depends on the use case.[4](#filepos1586522)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该选择多少的值作为 ε？这允许我们比较不同算法和方法的隐私性，但给出“足够”隐私的绝对值取决于使用案例。[4](#filepos1586522)
- en: To decide on a value to use for , it can be helpful to look at the accuracy
    of the system as is decreased. Choose the most private parameters possible while
    retaining acceptable data utility for the business problem. Alternatively, if
    the consequences of leaking data are very high, you may wish to set the acceptable
    values of and first, and then tune your other hyperparameters to get the best
    model accuracy possible. One weakness of DP is that is not easily interpretable.
    Other approaches are being developed to help with this, such as planting secrets
    within a model’s training data and measuring how likely it is that they are exposed
    in a model’s predictions.[5](#filepos1587084)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了决定使用哪个值来设置 ε，可以查看系统在 ε 减小时的准确性。在保留业务问题的可接受数据效用的同时，选择尽可能私密的参数值。或者，如果泄露数据的后果非常严重，则可以先设置
    ε 和 δ 的可接受值，然后调整其他超参数以获得最佳的模型准确性。DP 的一个缺点是 ε 不容易解释。正在开发其他方法来帮助解决这个问题，例如在模型的训练数据中种植秘密，并测量它们在模型预测中暴露的可能性。[5](#filepos1587084)
- en: Differential Privacy for Machine Learning
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的差分隐私
- en: 'If you want to use DP as part of your machine learning pipeline, there are
    a few current options for where it can be added, though we expect to see more
    in the future. First, DP can be included in a federated learning system (see [“Federated
    Learning”](#filepos1561354)), and this can use either global or local DP. Second,
    the TensorFlow Privacy library is an example of global DP: raw data is available
    for model training.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想将DP作为机器学习流程的一部分使用，目前有几个可选项可以添加，尽管我们预计未来会看到更多选项。首先，DP可以包含在联邦学习系统中（参见[“联邦学习”](#filepos1561354)），并且可以使用全局或本地DP。其次，TensorFlow
    Privacy库是全局DP的一个示例：原始数据可用于模型训练。
- en: 'A third option is the Private Aggregation of Teacher Ensembles (PATE) approach.[6](#filepos1587481)
    This is a data-sharing scenario: in the case that 10 people have labelled data,
    but you haven’t, they train a model locally and each make a prediction on your
    data. A DP query is then performed to generate the final prediction on each example
    in your dataset so that you don’t know which of the 10 models has made the prediction.
    A new model is then trained from these predictions—this model includes the information
    from the 10 hidden datasets in such a way that it’s not possible to learn about
    those hidden datasets. The PATE framework shows how is being spent in this scenario.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个选项是Private Aggregation of Teacher Ensembles（PATE）方法。[6](#filepos1587481)
    这是一个数据共享场景：如果有10个人拥有标记数据，但你没有，他们会在本地训练一个模型，并对你的数据进行预测。然后执行DP查询，以生成数据集中每个示例的最终预测，以便您不知道哪个模型生成了预测。然后，从这些预测中训练一个新模型——这个模型以一种不可能了解那些隐藏数据集的方式包含了这10个隐藏数据集的信息。PATE框架展示了在这种情况下如何花费。
- en: Introduction to TensorFlow Privacy
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow隐私简介
- en: '[TensorFlow Privacy](https://oreil.ly/vlcIy) (TFP) adds DP to an optimizer
    during model training. The type of DP used in TFP is an example of global DP:
    noise is added during training so that private data is not exposed in a model’s
    predictions. This lets us offer the strong DP guarantee that an individual’s data
    has not been memorized while still maximizing model accuracy. As shown in [Figure 14-2](#filepos1540062),
    in this situation, the raw data is available to the trusted data store and model
    trainer, but the final predictions are untrusted.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[TensorFlow隐私](https://oreil.ly/vlcIy)（TFP）在模型训练期间将DP添加到优化器中。TFP中使用的DP类型是全局DP的一个示例：在训练过程中添加噪声，以使私密数据不会暴露在模型的预测中。这使我们能够提供强大的DP保证，即个人数据未被记忆，同时最大化模型精度。如[图14-2](#filepos1540062)所示，在此情况下，原始数据可供信任的数据存储和模型训练者使用，但最终预测是不可信的。'
- en: '![](images/00046.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00046.jpg)'
- en: Figure 14-2\. Trusted parties for DP
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-2。DP的受信方
- en: Training with a Differentially Private Optimizer
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 使用差分私密优化器进行训练
- en: The optimizer algorithm is modified by adding random noise to the gradients
    at each training step. This compares the gradient’s updates with or without each
    individual data point and ensures that it is not possible to tell whether a specific
    data point was included in the gradient update. In addition, gradients are clipped
    so that they do not become too large—this limits the contribution of any one training
    example. As a nice bonus, this also helps prevent overfitting.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器算法通过在每个训练步骤中向梯度添加随机噪声来进行修改。这比较了有或没有每个单独数据点的梯度更新，并确保无法确定特定数据点是否包含在梯度更新中。此外，梯度被剪裁，以防止它们变得过大——这限制了任一训练样本的贡献。作为额外的奖励，这也有助于防止过度拟合。
- en: 'TFP can be installed with pip. At the time of writing, it requires TensorFlow
    version 1.X:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: TFP可以通过pip进行安装。在撰写本文时，它需要TensorFlow版本1.X：
- en: '`$` `pip install tensorflow_privacy`'
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `pip install tensorflow_privacy`'
- en: 'We start with a simple `tf.keras` binary classification example:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个简单的`tf.keras`二元分类示例开始：
- en: '`import``tensorflow``as``tf``model``=``tf``.``keras``.``models``.``Sequential``([``tf``.``keras``.``layers``.``Dense``(``128``,``activation``=``''relu''``),``tf``.``keras``.``layers``.``Dense``(``128``,``activation``=``''relu''``),``tf``.``keras``.``layers``.``Dense``(``1``,``activation``=``''sigmoid''``)``])`'
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`import``tensorflow``as``tf``model``=``tf``.``keras``.``models``.``Sequential``([``tf``.``keras``.``layers``.``Dense``(``128``,``activation``=``''relu''``),``tf``.``keras``.``layers``.``Dense``(``128``,``activation``=``''relu''``),``tf``.``keras``.``layers``.``Dense``(``1``,``activation``=``''sigmoid''``)``])`'
- en: 'The differentially private optimizer requires that we set two extra hyperparameters
    compared to a normal `tf.keras` model: the noise multiplier and the L2 norm clip.
    It’s best to tune these to suit your dataset and measure their impact on :'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '要求不同ially privat优化器与正常的`tf.keras`模型相比需要设置两个额外的超参数：噪声倍增器和L2范数剪辑。最好调整这些参数以适应您的数据集，并测量它们对：  '
- en: '`NOISE_MULTIPLIER``=``2``NUM_MICROBATCHES``=``32`![](images/00002.jpg)`LEARNING_RATE``=``0.01``POPULATION_SIZE``=``5760`![](images/00075.jpg)`L2_NORM_CLIP``=``1.5``BATCH_SIZE``=``32`![](images/00064.jpg)`EPOCHS``=``70`'
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`NOISE_MULTIPLIER``=``2``NUM_MICROBATCHES``=``32`![](images/00002.jpg)`LEARNING_RATE``=``0.01``POPULATION_SIZE``=``5760`![](images/00075.jpg)`L2_NORM_CLIP``=``1.5``BATCH_SIZE``=``32`![](images/00064.jpg)`EPOCHS``=``70`  '
- en: '![](images/00002.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: The batch size must be exactly divisible by the number of microbatches.
  id: totrans-55
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '批处理大小必须与微批处理数量完全可除。  '
- en: '![](images/00075.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00075.jpg)  '
- en: The number of examples in the training set.
  id: totrans-57
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '训练集中的示例数。  '
- en: '![](images/00064.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00064.jpg)'
- en: The population size must be exactly divisible by the batch size.
  id: totrans-59
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '人口数量必须与批次大小完全可除。  '
- en: 'Next, initialize the differentially private optimizer:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，初始化不同ially privat优化器：  '
- en: '`from``tensorflow_privacy.privacy.optimizers.dp_optimizer` `\` `import``DPGradientDescentGaussianOptimizer``optimizer``=``DPGradientDescentGaussianOptimizer``(``l2_norm_clip``=``L2_NORM_CLIP``,``noise_multiplier``=``NOISE_MULTIPLIER``,``num_microbatches``=``NUM_MICROBATCHES``,``learning_rate``=``LEARNING_RATE``)``loss``=``tf``.``keras``.``losses``.``BinaryCrossentropy``(``from_logits``=``True``,``reduction``=``tf``.``losses``.``Reduction``.``NONE``)`![](images/00002.jpg)'
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`from``tensorflow_privacy.privacy.optimizers.dp_optimizer` `\` `import``DPGradientDescentGaussianOptimizer``optimizer``=``DPGradientDescentGaussianOptimizer``(``l2_norm_clip``=``L2_NORM_CLIP``,``noise_multiplier``=``NOISE_MULTIPLIER``,``num_microbatches``=``NUM_MICROBATCHES``,``learning_rate``=``LEARNING_RATE``)``loss``=``tf``.``keras``.``losses``.``BinaryCrossentropy``(``from_logits``=``True``,``reduction``=``tf``.``losses``.``Reduction``.``NONE``)`![](images/00002.jpg)  '
- en: '![](images/00002.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)  '
- en: Loss must be calculated on a per-example basis rather than over an entire minibatch.
  id: totrans-63
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '损失必须基于每个示例而不是整个小批量计算。  '
- en: 'Training the private model is just like training a normal `tf.keras` model:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '训练私密模型就像训练正常的`tf.keras`模型一样：  '
- en: '`model``.``compile``(``optimizer``=``optimizer``,``loss``=``loss``,``metrics``=``[``''accuracy''``])``model``.``fit``(``X_train``,``y_train``,``epochs``=``EPOCHS``,``validation_data``=``(``X_test``,``y_test``),``batch_size``=``BATCH_SIZE``)`'
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`model``.``compile``(``optimizer``=``optimizer``,``loss``=``loss``,``metrics``=``[``''accuracy''``])``model``.``fit``(``X_train``,``y_train``,``epochs``=``EPOCHS``,``validation_data``=``(``X_test``,``y_test``),``batch_size``=``BATCH_SIZE``)`  '
- en: Calculating Epsilon
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '计算Epsilon  '
- en: 'Now, we calculate the differential privacy parameters for our model and our
    choice of noise multiplier and gradient clip:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '现在，我们计算我们的模型和我们选择的噪声倍增器和梯度剪辑的差分隐私参数：  '
- en: '`from``tensorflow_privacy.privacy.analysis``import``compute_dp_sgd_privacy``compute_dp_sgd_privacy``.``compute_dp_sgd_privacy``(``n``=``POPULATION_SIZE``,``batch_size``=``BATCH_SIZE``,``noise_multiplier``=``NOISE_MULTIPLIER``,``epochs``=``EPOCHS``,``delta``=``1e-4``)`![](images/00002.jpg)'
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`from``tensorflow_privacy.privacy.analysis``import``compute_dp_sgd_privacy``compute_dp_sgd_privacy``.``compute_dp_sgd_privacy``(``n``=``POPULATION_SIZE``,``batch_size``=``BATCH_SIZE``,``noise_multiplier``=``NOISE_MULTIPLIER``,``epochs``=``EPOCHS``,``delta``=``1e-4``)`![](images/00002.jpg)'
- en: '![](images/00002.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)  '
- en: The value of delta is set to 1/the size of the dataset, rounded to the nearest
    order of magnitude.
  id: totrans-70
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Delta的值设置为数据集大小的倒数，四舍五入到最接近的数量级。  '
- en: TFP SUPPORTS ONLY TENSORFLOW 1.X
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'TFP仅支持TensorFlow 1.X  '
- en: We show how you can convert the example project from previous chapters to a
    DP model in our [GitHub repo](https://oreil.ly/bmlp-git). The differentially private
    optimizer is added into the `get_model` function from [Chapter 6](index_split_011.html#filepos491525).
    However, this model can’t be used in our TFX pipeline until TFP supports TensorFlow
    2.X.
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们展示如何将先前章节的示例项目转换为我们的[GitHub存储库](https://oreil.ly/bmlp-git)中的DP模型。不过，在TFX管道中，TFP支持TensorFlow
    2.X之前，无法使用此模型。
- en: The final output of this calculation, the value of epsilon, tells us the strength
    of the privacy guarantee for our particular model. We can then explore how changing
    the L2 norm clip and noise multiplier hyperparameters discussed earlier affects
    both epsilon and our model accuracy. If the values of these two hyperparameters
    are increased, keeping all others fixed, epsilon will decrease (so the privacy
    guarantee becomes stronger). At some point, accuracy will begin to decrease and
    the model will stop being useful. This trade-off can be explored to get the strongest
    possible privacy guarantees while still maintaining useful model accuracy.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此计算的最终输出，epsilon的值，告诉我们特定模型的隐私保证强度。然后，我们可以探索如何改变先前讨论的L2范数剪辑和噪声乘数超参数会同时影响epsilon和模型精度。如果增加这两个超参数的值，而保持其他参数不变，epsilon将减少（因此隐私保证变得更强）。在某个点上，精度将开始减少，模型将不再有用。可以探索这种权衡，以获得尽可能强大的隐私保证，同时仍保持有用的模型精度。
- en: Federated Learning
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习
- en: Federated learning (FL) is a protocol where the training of a machine learning
    model is distributed across many different devices and the trained model is combined
    on a central server. The key point is that the raw data never leaves the separate
    devices and is never pooled in one place. This is very different from the traditional
    architecture of gathering a dataset in a central location and then training a
    model.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习（FL）是一个协议，其中机器学习模型的训练分布在许多不同的设备上，并在中央服务器上组合训练好的模型。关键点在于原始数据永远不会离开各自的设备，也永远不会集中到一个地方。这与在中心位置收集数据集然后训练模型的传统架构非常不同。
- en: FL is often useful in the context of mobile phones with distributed data, or
    a user’s browser. Another potential use case is in the sharing of sensitive data
    that is distributed across multiple data owners. For example, an AI startup may
    want to train a model to detect skin cancer. Images of skin cancer are owned by
    many hospitals, but they can’t be centralized in one location due to privacy and
    legal concerns. FL lets the startup train a model without the data leaving the
    hospitals.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在移动电话具有分布数据或用户的浏览器的背景下，FL通常非常有用。另一个潜在的用例是在分布在多个数据所有者之间的敏感数据共享方面。例如，AI初创公司可能希望训练一个模型来检测皮肤癌。皮肤癌图像由许多医院拥有，但由于隐私和法律问题，它们不能集中在一个地方。FL允许初创公司在数据不离开医院的情况下训练模型。
- en: In an FL setup, each client receives the model architecture and some instructions
    for training. A model is trained on each client’s device, and the weights are
    returned to a central server. This increases privacy slightly, in that it’s more
    difficult for an interceptor to learn anything about a user from model weights
    than from raw data, but it doesn’t provide any guarantee of privacy. The step
    of distributing the model training doesn’t provide the user with any increased
    privacy from the company collecting the data because the company can often work
    out what the raw data would have been with a knowledge of the model architecture
    and the weights.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在联邦学习（FL）设置中，每个客户端接收模型架构和一些训练指令。每个客户端的设备上训练一个模型，并将权重返回到中央服务器。这在一定程度上增加了隐私性，因为拦截者很难从模型权重中了解用户的任何信息，而不是从原始数据中获取，但这并不能提供任何隐私保证。分发模型训练的步骤并不能为用户提供免于公司收集数据影响的隐私保护，因为公司通常可以通过了解模型架构和权重来推断原始数据可能的内容。
- en: 'However, there is one more very important step to increase privacy using FL:
    the secure aggregation of the weights into the central model. There are a number
    of algorithms for doing this, but they all require that the central party has
    to be trusted to not attempt to inspect the weights before they are combined.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用FL来增加隐私还有一个非常重要的步骤：将权重安全聚合到中央模型中。有许多算法可以做到这一点，但它们都要求中央方信任不会在权重组合之前尝试检查权重。
- en: '[Figure 14-3](#filepos1564051) shows which parties have access to the personal
    data of users in the FL setting. It’s possible for the company collecting the
    data to set up secure averaging such that they don’t see the model weights that
    are returned from users. A neutral third party could also perform the secure aggregation.
    In this case, only the users would see their data.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[图14-3](#filepos1564051) 显示了在FL设置中哪些方可以访问用户的个人数据。收集数据的公司可以建立安全平均值，以便他们不会看到用户返回的模型权重。中立的第三方也可以执行安全聚合。在这种情况下，只有用户可以看到他们的数据。'
- en: '![](images/00057.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00057.jpg)'
- en: Figure 14-3\. Trusted parties in federated learning
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图14-3\. 联邦学习中的信任方
- en: An additional privacy-preserving extension to FL is the incorporation of DP
    into this technique. In this situation, DP limits the amount of information that
    each user can contribute to the final model. Research has shown that the resulting
    models are almost as accurate as non-DP models if the number of users is large.[7](#filepos1587935)
    However, as yet, this hasn’t been implemented for either TensorFlow or PyTorch.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: FL 的一个额外的隐私保护扩展是将 DP 整合到这一技术中。在这种情况下，DP 限制了每个用户对最终模型的贡献信息量。研究表明，如果用户数量庞大，生成的模型几乎和非
    DP 模型一样准确。[7](#filepos1587935) 然而，目前尚未将其实施到 TensorFlow 或 PyTorch 中。
- en: An example of FL in production is [Google’s Gboard keyboard for Android mobile
    phones](https://oreil.ly/LXtSN). Google is able to train a model to make better
    next-word predictions without learning anything about users’ private messaging.
    FL is most useful in use cases that share the following characteristics:[8](#filepos1588378)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: FL 在生产中的一个例子是[Google 的 Gboard 安卓手机键盘](https://oreil.ly/LXtSN)。Google 能够训练模型以更好地预测下一个单词，而无需了解用户的私密消息。FL
    在以下具有相似特征的使用案例中最为有用：[8](#filepos1588378)
- en: The data required for the model can only be collected from distributed sources.
  id: totrans-84
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 模型所需的数据只能从分布源收集。
- en: The number of data sources is large.
  id: totrans-85
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据源数量庞大。
- en: The data is sensitive in some way.
  id: totrans-86
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据在某种程度上是敏感的。
- en: The data does not require extra labelling—the labels are provided directly by
    the user and do not leave the source.
  id: totrans-87
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据不需要额外标记，标签直接由用户提供且不离开源。
- en: Ideally, the data is drawn from close to identical distributions.
  id: totrans-88
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 理想情况下，数据来自接近相同的分布。
- en: 'FL introduces many new considerations into the design of a machine learning
    system: for example, not all data sources may have collected new data between
    one training run and the next, not all mobile devices are powered on all the time,
    and so on. The data that is collected is often unbalanced and practically unique
    to each device. It’s easiest to get sufficient data for each training run when
    the pool of devices is large. New secure infrastructure must be developed for
    any project using FL.[9](#filepos1589038)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: FL 在设计机器学习系统时引入了许多新的考虑因素：例如，不是所有数据源都可能在一次训练运行和下一次之间收集了新数据，不是所有移动设备都一直开机，等等。收集的数据通常不平衡，且几乎每个设备都是独特的。当设备池庞大时，每次训练运行获取足够的数据是最容易的。任何使用
    FL 的项目都必须开发新的安全基础设施。[9](#filepos1589038)
- en: Care must be taken to avoid performance issues on devices that train an FL model.
    Training can quickly drain the battery on a mobile device or cause large data
    usage, leading to expense for the user. Even though the processing power of mobile
    phones is increasing rapidly, they are still only capable of training small models,
    so more complex models should be trained on a central server.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练 FL 模型的设备上需要注意避免性能问题。训练可能会迅速消耗移动设备的电量或造成大量数据使用，从而增加用户的费用。尽管移动电话的处理能力在迅速增加，但它们仍然只能训练小型模型，因此更复杂的模型应该在中央服务器上进行训练。
- en: Federated Learning in TensorFlow
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 中的联邦学习
- en: TensorFlow Federated (TFF) simulates the distributed setup of FL and contains
    a version of stochastic gradient descent (SGD) that can calculate updates on distributed
    data. Conventional SGD requires that updates are computed on batches of a centralized
    dataset, and this centralized dataset doesn’t exist in a federated setting. At
    the time of writing, TFF is mainly aimed at research and experimentation on new
    federated algorithms.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Federated（TFF）模拟了 FL 的分布式设置，并包含一种可以在分布数据上计算更新的随机梯度下降（SGD）版本。传统的 SGD
    要求在集中数据集的批次上计算更新，而在联邦设置中不存在这种集中数据集。截至撰写本文时，TFF 主要用于研究和实验新的联邦算法。
- en: '[PySyft](https://oreil.ly/qlAWh) is an open source Python platform for privacy-preserving
    machine learning developed by the OpenMined organization. It contains an implementation
    of FL using secure multiparty computation (explained further in the following
    section) to aggregate data. It was originally developed to support PyTorch models,
    but a [TensorFlow version has been released](https://oreil.ly/01yw2).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[PySyft](https://oreil.ly/qlAWh) 是由 OpenMined 组织开发的用于隐私保护机器学习的开源 Python 平台。它包含使用安全多方计算来聚合数据的
    FL 实现（在下一节中进一步解释）。最初开发时支持 PyTorch 模型，但已发布了[TensorFlow 版本](https://oreil.ly/01yw2)。'
- en: Encrypted Machine Learning
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 加密机器学习
- en: 'Encrypted machine learning is another area of privacy-preserving machine learning
    that’s currently receiving a lot of attention from both researchers and practitioners.
    It leans on technology and research from the cryptographic community and applies
    these techniques to machine learning. The major methods that have been adopted
    so far are homomorphic encryption (HE) and secure multiparty computation (SMPC).
    There are two ways to use these techniques: encrypting a model that has already
    been trained on plain text data and encrypting an entire system (if the data must
    stay encrypted during training).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 加密机器学习是隐私保护机器学习的另一个领域，目前在研究人员和实践者中受到广泛关注。它依赖于密码学社区的技术和研究成果，并将这些技术应用于机器学习。迄今为止已经采用的主要方法包括同态加密（HE）和安全多方计算（SMPC）。有两种使用这些技术的方式：加密已经在明文数据上训练过的模型，以及在训练期间保持数据加密状态的整个系统。
- en: HE is similar to public-key encryption but differs in that data does not have
    to be decrypted before a computation is applied to it. The computation (such as
    obtaining predictions from a machine learning model) can be performed on the encrypted
    data. A user can provide their data in its encrypted form using an encryption
    key that is stored locally and then receive the encrypted prediction, which they
    can then decrypt to get the prediction of the model on their data. This provides
    privacy to the user because their data is not shared with the party who has trained
    the model.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: HE 类似于公钥加密，但不同之处在于在应用计算之前，数据不需要解密。可以对加密数据执行计算（例如从机器学习模型中获取预测）。用户可以以加密形式提供其数据，使用本地存储的加密密钥，然后接收加密预测结果，随后可以解密以获取模型在其数据上的预测。这为用户提供了隐私保护，因为他们的数据不会与训练模型的一方共享。
- en: SMPC allows several parties to combine data, perform a computation on it, and
    see the results of the computation on their own data without knowing anything
    about the data from the other parties. This is achieved by [secret sharing](https://oreil.ly/kIeOx),
    a process in which any single value is split into shares that are sent to separate
    parties. The original value can’t be reconstructed from any share, but computations
    can still be carried out on each share individually. The result of the computations
    is meaningless until all the shares are recombined.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: SMPC 允许多个参与方合并数据，在其上执行计算，并在自己的数据上查看计算结果，而无需了解其他参与方的数据。这是通过 [秘密分享](https://oreil.ly/kIeOx)
    实现的，即任何单个值被分割成发送到不同参与方的份额的过程。原始值无法从任何份额中重建，但仍然可以对每个份额进行个别计算。只有在重新组合所有份额后，计算的结果才有意义。
- en: 'Both of these techniques come with a cost. At the time of writing, HE is rarely
    used for training machine learning models: it causes several orders of magnitudes
    of slowdown in both training and predictions. Because of this, we won’t discuss
    HE any further. SMPC also has an overhead in terms of networking time when the
    shares and the results are passed between parties, but it is significantly faster
    than HE. These techniques, along with FL, are useful for situations which data
    can’t be gathered in one place. However, they do not prevent models from memorizing
    sensitive data—DP is the best solution for that.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术都有其成本。在撰写本文时，HE 很少用于训练机器学习模型：它导致训练和预测速度大幅减慢。因此，我们不再讨论 HE。SMPC 在网络传输时间方面也有开销，因为需要在各方之间传递份额和结果，但它比
    HE 明显更快。这些技术与 FL 一起，适用于数据无法集中存储的情况。然而，它们无法阻止模型记忆敏感数据—差分隐私（DP）是解决这个问题的最佳方案。
- en: Encrypted ML is provided for TensorFlow by [TF Encrypted](https://tf-encrypted.io)
    (TFE), primarily developed by [Cape Privacy](https://capeprivacy.com). TFE can
    also provide the [secure aggregation required for FL](https://oreil.ly/VVPJx).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 提供了针对加密机器学习的支持，通过 [TF Encrypted](https://tf-encrypted.io)（TFE）实现，主要由
    [Cape Privacy](https://capeprivacy.com) 开发。TFE 还可以提供用于 FL 所需的 [安全聚合](https://oreil.ly/VVPJx)。
- en: Encrypted Model Training
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 加密模型训练
- en: The first situation in which you might want to use encrypted machine learning
    is training models on encrypted data. This is useful when the raw data needs to
    be kept private from the data scientist training the model or when two or more
    parties own the raw data and want to train a model using all parties’ data, but
    don’t want to share the raw data. As shown in [Figure 14-4](#filepos1572698),
    only the data owner or owners are trusted in this scenario.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能希望使用加密机器学习的第一种情况是在加密数据上训练模型。当原始数据需要对训练模型的数据科学家保密，或者两个或更多方拥有原始数据并希望使用所有方的数据训练模型时，这是非常有用的。如图
    [14-4](#filepos1572698) 所示，在这种情况下，只信任数据所有者或所有者。
- en: '![](images/00068.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00068.jpg)'
- en: Figure 14-4\. Trusted parties with encrypted model training
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-4\. 使用加密模型训练的受信任方
- en: 'TFE can be used to train an encrypted model for this use case. It’s installed
    using `pip` as usual:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此用例，TFE 可用于训练加密模型。像往常一样使用 `pip` 安装：
- en: '`$` `pip install tf_encrypted`'
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `pip install tf_encrypted`'
- en: 'The first step in building a TFE model is to define a class that yields training
    data in batches. This class is implemented locally by the data owner(s). It is
    converted to encrypted data using a decorator:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 构建 TFE 模型的第一步是定义一个类，该类以批量方式提供训练数据。这个类由数据所有者（们）在本地实现。它使用装饰器转换为加密数据：
- en: '`@tfe.local_computation`'
  id: totrans-107
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`@tfe.local_computation`'
- en: 'Writing model training code in TFE is almost identical to regular Keras models—simply
    replace `tf` with `tfe`:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TFE 中编写模型训练代码几乎与常规 Keras 模型相同 —— 只需用 `tfe` 替换 `tf`：
- en: '`import``tf_encrypted``as``tfe``model``=``tfe``.``keras``.``Sequential``()``model``.``add``(``tfe``.``keras``.``layers``.``Dense``(``1``,``batch_input_shape``=``[``batch_size``,``num_features``]))``model``.``add``(``tfe``.``keras``.``layers``.``Activation``(``''sigmoid''``))`'
  id: totrans-109
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`import``tf_encrypted``as``tfe``model``=``tfe``.``keras``.``Sequential``()``model``.``add``(``tfe``.``keras``.``layers``.``Dense``(``1``,``batch_input_shape``=``[``batch_size``,``num_features``]))``model``.``add``(``tfe``.``keras``.``layers``.``Activation``(``''sigmoid''``))`'
- en: The only difference is that the argument `batch_input_shape` must be supplied
    to the `Dense` first layer.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的区别是必须向 `Dense` 的第一层提供 `batch_input_shape` 参数。
- en: Working examples of this are given in the [TFE documentation](https://oreil.ly/ghGnu).
    At the time of writing, not all functionality of regular Keras was included in
    TFE, so we can’t show our example project in this format.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 本文档提供了这方面的工作示例 [TFE documentation](https://oreil.ly/ghGnu)。截至撰写本文时，TFE 尚未包含常规
    Keras 的所有功能，因此我们无法以此格式展示我们的示例项目。
- en: Converting a Trained Model to Serve Encrypted Predictions
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 将训练好的模型转换为提供加密预测
- en: The second scenario where TFE is useful is when you’d like to serve [encrypted
    models that have been trained on plain-text data](https://oreil.ly/HBUBj). In
    this case, as shown in [Figure 14-5](#filepos1578515), you have full access to
    the unencrypted training data, but you want the users of your application to be
    able to receive private predictions. This provides privacy to the users, who upload
    encrypted data and receive an encrypted prediction.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: TFE 的第二种有用场景是您希望为[在明文数据上训练的加密模型提供服务](https://oreil.ly/HBUBj)。在这种情况下，如图 [14-5](#filepos1578515)
    所示，您可以完全访问未加密的训练数据，但希望应用程序的用户能够接收私密预测。这为上传加密数据并接收加密预测的用户提供了隐私保护。
- en: '![](images/00079.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00079.jpg)'
- en: Figure 14-5\. Trusted parties when encrypting a trained model
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14-5\. 在加密训练模型时的受信任方
- en: 'This method may be the best fit with today’s machine learning pipelines, as
    models can be trained as normal and converted to an encrypted version. It can
    also be used for models that have been trained using DP. The main difference from
    unencrypted models is that multiple servers are required: each one hosts a share
    of the original model. If anyone views a share of the model on one server or one
    share of the data that is sent to any one server, it reveals nothing about the
    model or the data.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法可能是当今机器学习管道的最佳选择，因为模型可以像正常训练一样进行训练并转换为加密版本。它还可用于使用 DP 训练的模型。与非加密模型的主要区别在于需要多个服务器：每个服务器都托管原始模型的一部分。如果任何人在一个服务器上查看模型的一部分或发送到任何一个服务器的数据的一部分，都不会泄露关于模型或数据的任何信息。
- en: 'Keras models can be converted to TFE models via:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 可通过以下方式将 Keras 模型转换为 TFE 模型：
- en: '`tfe_model``=``tfe``.``keras``.``models``.``clone_model``(``model``)`'
  id: totrans-118
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`tfe_model``=``tfe``.``keras``.``models``.``clone_model``(``model``)`'
- en: 'In this scenario, the following steps need to be carried out:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，需要执行以下步骤：
- en: Load and preprocess the data locally on the client.
  id: totrans-120
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在客户端上加载和预处理数据。
- en: Encrypt the data on the client.
  id: totrans-121
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在客户端对数据进行加密。
- en: Send the encrypted data to the servers.
  id: totrans-122
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 将加密数据发送到服务器。
- en: Make a prediction on the encrypted data.
  id: totrans-123
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在加密数据上进行预测。
- en: Send the encrypted prediction to the client.
  id: totrans-124
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 将加密预测结果发送到客户端。
- en: Decrypt the prediction on the client and show the result to the user.
  id: totrans-125
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在客户端解密预测结果并显示给用户。
- en: TFE provides [a series of notebooks](https://oreil.ly/r0cKP) showing how to
    serve private predictions.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: TFE提供了一系列笔记本，展示如何提供私密预测结果的方法，详见[这里](https://oreil.ly/r0cKP)。
- en: Other Methods for Data Privacy
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 其他数据隐私方法
- en: There are many other techniques for increasing privacy for the people who have
    their data included in machine learning models. Simply scrubbing text data for
    names, addresses, phone numbers, and so on, can be surprisingly easy using regular
    expressions and named-entity recognition models.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他技术可为包含在机器学习模型中的个人增强隐私保护。仅仅使用正则表达式和命名实体识别模型对文本数据进行清理（如姓名、地址、电话号码等），可能会非常简单。
- en: K-ANONYMITY
  id: totrans-129
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: K-匿名性
- en: '[K-anonymity](https://oreil.ly/sxQet), often simply known as anonymization,
    is not a good candidate for increasing privacy in machine learning pipelines.
    K-anonymity requires that each individual in a dataset is indistinguishable from
    k – 1 others with respect to their quasi-identifiers (data that can indirectly
    identify individuals, such as gender, race, and zip code). This is achieved by
    aggregating or removing data until the dataset satisfies this requirement. This
    removal of data generally causes a large decrease in the accuracy of machine learning
    models.[10](#filepos1589595)'
  id: totrans-130
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[K-匿名性](https://oreil.ly/sxQet)，通常简称为匿名化，在增强机器学习流程中的隐私保护方面并不是一个良好的选择。K-匿名性要求数据集中的每个个体在其准标识符（如性别、种族和邮政编码等间接识别个体的数据）方面与其他k-1个个体不可区分。这通常通过聚合或删除数据来实现，直到数据集满足此要求。这种数据的移除通常会显著降低机器学习模型的准确性。[10](#filepos1589595)'
- en: Summary
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要
- en: When you’re working with personal or sensitive data, choose the data privacy
    solution that best fits your needs regarding who is trusted, what level of model
    performance is required, and what consent you have obtained from users.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 当您处理个人或敏感数据时，请选择最适合您需求的数据隐私解决方案，考虑谁可信、需要何种模型性能水平以及从用户那里获得了什么同意。
- en: All of the techniques described in this chapter are extremely new, and their
    production use is not yet widespread. Don’t assume that using one of the frameworks
    described in this chapter ensures complete privacy for your users. There is always
    a substantial additional engineering effort involved in adding privacy to a machine
    learning pipeline. The field of privacy-preserving machine learning is evolving
    rapidly, and new research is being undertaken right now. We encourage you to look
    for improvements in this field and support open source projects around data privacy,
    such as [PySyft](https://oreil.ly/rj0_c) and [TFE](https://oreil.ly/L5zik).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 本章描述的所有技术都非常新颖，它们的实际应用还不广泛。不要认为使用本章描述的任何框架可以完全保护用户的隐私。为了在机器学习流程中增加隐私，通常需要大量的额外工程努力。隐私保护机器学习领域正在迅速发展，目前正在进行新的研究。我们鼓励您寻找该领域的改进，并支持诸如[PySyft](https://oreil.ly/rj0_c)和[TFE](https://oreil.ly/L5zik)等数据隐私的开源项目。
- en: The goals of data privacy and machine learning are often well aligned, in that
    we want to learn about a whole population and make predictions that are equally
    good for everyone, rather than learning only about one individual. Adding privacy
    can stop a model from overfitting to one person’s data. We expect that, in the
    future, privacy will be designed into machine learning pipelines from the start
    whenever models are trained on personal data.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 数据隐私和机器学习的目标通常是高度一致的，即我们希望了解整个人群并做出对每个人都同样有利的预测，而不是仅仅了解个体。添加隐私可以阻止模型过度拟合于某个人的数据。我们期望未来，在模型训练个人数据时，隐私将从一开始就被设计进机器学习流程中。
- en: '[1  ](#filepos1531175) Cynthia Dwork, “Differential Privacy,” in Encyclopedia
    of Cryptography and Security, ed. Henk C. A. van Tilborg and Sushil Jajodia (Boston:
    Springer, 2006).'
  id: totrans-135
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[1  ](#filepos1531175) Cynthia Dwork，《差分隐私》，载于《密码学与安全百科全书》，编辑：Henk C. A. van
    Tilborg 和 Sushil Jajodia，波士顿：Springer，2006年。'
- en: '[2  ](#filepos1532052) Cynthia Dwork and Aaron Roth, “The Algorithmic Foundations
    of Differential Privacy,” Foundations and Trends in Theoretical Computer Science
    9, no.3–4: 211–407, (2014), https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf.'
  id: totrans-136
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[2  ](#filepos1532052) Cynthia Dwork 和 Aaron Roth，《差分隐私的算法基础》，《理论计算机科学基础和趋势》，第9卷，第3-4期：211–407，（2014），https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf。'
- en: '[3  ](#filepos1536527) More details on the math behind this can be found in
    Dwork and Roth, “The Algorithmic Foundations of Differential Privacy.”'
  id: totrans-137
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[3  ](#filepos1536527) 更多关于此背后数学的细节可以在Dwork和Roth的论文中找到，“差分隐私的算法基础”。'
- en: '[4  ](#filepos1536858) Further details may be found in Justin Hsu et al., “Differential
    Privacy: An Economic Method for Choosing Epsilon” (Paper presentation, 2014 IEEE
    Computer Security Foundations Symposium, Vienna, Austria, February 17, 2014),
    [https://arxiv.org/pdf/1402.3329.pdf](https://arxiv.org/pdf/1402.3329.pdf).'
  id: totrans-138
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[4  ](#filepos1536858) 更多细节可以在Justin Hsu等人的论文中找到，“差分隐私：选择ε的经济方法”（论文展示，2014年IEEE计算机安全基础对称研讨会，奥地利维也纳，2014年2月17日），[https://arxiv.org/pdf/1402.3329.pdf](https://arxiv.org/pdf/1402.3329.pdf)。'
- en: '[5  ](#filepos1537664) Nicholas Carlini et al., “The Secret Sharer,” July 2019\.
    [https://arxiv.org/pdf/1802.08232.pdf](https://arxiv.org/pdf/1802.08232.pdf).'
  id: totrans-139
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[5  ](#filepos1537664) Nicholas Carlini等人，“秘密共享者”，2019年7月。[https://arxiv.org/pdf/1802.08232.pdf](https://arxiv.org/pdf/1802.08232.pdf)。'
- en: '[6  ](#filepos1538511) Nicolas Papernot et al., “Semi-Supervised Knowledge
    Transfer for Deep Learning from Private Training Data,” October 2016, [https://arxiv.org/abs/1610.05755](https://arxiv.org/abs/1610.05755).'
  id: totrans-140
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[6  ](#filepos1538511) Nicolas Papernot等人，“半监督知识转移用于来自私有训练数据的深度学习”，2016年10月，[https://arxiv.org/abs/1610.05755](https://arxiv.org/abs/1610.05755)。'
- en: '[7  ](#filepos1564641) Robin C. Geyer et al., “Differentially Private Federated
    Learning: A Client Level Perspective,” December 2017, [https://arxiv.org/abs/1712.07557](https://arxiv.org/abs/1712.07557).'
  id: totrans-141
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[7  ](#filepos1564641) Robin C. Geyer等人，“差分私有联邦学习：客户端视角”，2017年12月，[https://arxiv.org/abs/1712.07557](https://arxiv.org/abs/1712.07557)。'
- en: '[8  ](#filepos1565208) This is covered in more detail in the paper by H. Brendan
    McMahan et al.,“Communication-Efficient Learning of Deep Networks from Decentralized
    Data,” Proceedings of the 20th International Conference on Artificial Intelligence
    and Statistics, PMLR 54 (2017): 1273–82, [https://arxiv.org/pdf/1602.05629.pdf](https://arxiv.org/pdf/1602.05629.pdf).'
  id: totrans-142
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[8  ](#filepos1565208) 这一点在H. Brendan McMahan等人的论文中有更详细的讨论，“分散数据中深度网络的通信有效学习”，发表于第20届人工智能与统计学国际会议论文集，PMLR
    54 (2017): 1273–82，[https://arxiv.org/pdf/1602.05629.pdf](https://arxiv.org/pdf/1602.05629.pdf)。'
- en: '[9  ](#filepos1567200) For more details on system design for FL, refer to the
    paper by Keith Bonawitz et al.,“Towards Federated Learning at Scale: System Design”
    (Presentation, Proceedings of the 2nd SysML Conference, Palo Alto, CA, 2019),
    [https://arxiv.org/pdf/1902.01046.pdf](https://arxiv.org/pdf/1902.01046.pdf).'
  id: totrans-143
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[9  ](#filepos1567200) 关于FL系统设计的更多细节，请参阅Keith Bonawitz等人的论文，“朝着规模化的联邦学习：系统设计”（演示，第2届SysML会议论文集，加利福尼亚州帕洛阿尔托，2019年），[https://arxiv.org/pdf/1902.01046.pdf](https://arxiv.org/pdf/1902.01046.pdf)。'
- en: '[10  ](#filepos1583584) In addition, individuals in “anonymized” datasets can
    be reidentified using outside information; see Luc Rocher et al., “Estimating
    the Success of Re-identifications in Incomplete Datasets Using Generative Models,”
    Nature Communications 10, Article no. 3069 (2019), [https://www.nature.com/articles/s41467-019-10933-3](https://www.nature.com/articles/s41467-019-10933-3).'
  id: totrans-144
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[10  ](#filepos1583584) 此外，“匿名化”数据集中的个体可以使用外部信息重新识别；详见Luc Rocher等人的论文，“使用生成模型估算不完整数据集中重新识别的成功率”，自然通讯10,
    文章号 3069 (2019)，[https://www.nature.com/articles/s41467-019-10933-3](https://www.nature.com/articles/s41467-019-10933-3)。'
