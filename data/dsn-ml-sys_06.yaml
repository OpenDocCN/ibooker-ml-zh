- en: Chapter 6\. Model Development and Offline Evaluation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第六章。模型开发与离线评估
- en: In [Chapter 4](ch04.xhtml#training_data), we discussed how to create training
    data for your model, and in [Chapter 5](ch05.xhtml#feature_engineering), we discussed
    how to engineer features from that training data. With the initial set of features,
    we’ll move to the ML algorithm part of ML systems. For me, this has always been
    the most fun step, as it allows me to play around with different algorithms and
    techniques, even the latest ones. This is also the first step where I can see
    all the hard work I’ve put into data and feature engineering transformed into
    a system whose outputs (predictions) I can use to evaluate the success of my effort.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第四章](ch04.xhtml#training_data)中，我们讨论了如何为您的模型创建训练数据；在[第五章](ch05.xhtml#feature_engineering)中，我们讨论了如何从这些训练数据中进行特征工程。通过初始的特征集，我们将进入
    ML 系统的 ML 算法部分。对我来说，这总是最有趣的一步，因为它允许我尝试不同的算法和技术，甚至是最新的技术。这也是我可以看到自己在数据和特征工程上投入的所有辛勤工作转化为一个系统的第一步，其输出（预测）我可以用来评估我的努力成功的第一步。
- en: To build an ML model, we first need to select the ML model to build. There are
    so many ML algorithms out there, with more actively being developed. This chapter
    starts with six tips for selecting the best algorithms for your task.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个 ML 模型，我们首先需要选择要构建的 ML 模型。有很多 ML 算法可供选择，而且还在积极开发中。本章从选择最适合您任务的最佳算法的六个提示开始。
- en: The section that follows discusses different aspects of model development, such
    as debugging, experiment tracking and versioning, distributed training, and AutoML.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的部分讨论了模型开发的不同方面，如调试、实验跟踪与版本控制、分布式训练和自动化机器学习。
- en: Model development is an iterative process. After each iteration, you’ll want
    to compare your model’s performance against its performance in previous iterations
    and evaluate how suitable this iteration is for production. The last section of
    this chapter is dedicated to how to evaluate your model before deploying it to
    production, covering a range of evaluation techniques including perturbation tests,
    invariance tests, model calibration, and slide-based evaluation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 模型开发是一个迭代过程。每次迭代后，您都希望将模型的性能与之前迭代的性能进行比较，并评估这次迭代是否适合投入生产。本章的最后一节专门讨论了在将模型部署到生产环境之前如何评估您的模型，涵盖了一系列评估技术，包括扰动测试、不变性测试、模型校准和基于幻灯片的评估。
- en: I expect that most readers already have an understanding of common ML algorithms
    such as linear models, decision trees, *k*-nearest neighbors, and different types
    of neural networks. This chapter will discuss techniques surrounding these algorithms
    but won’t go into details of how they work. Because this chapter deals with ML
    algorithms, it requires a lot more ML knowledge than other chapters. If you’re
    not familiar with them, I recommend taking an online course or reading a book
    on ML algorithms before reading this chapter. Readers wanting a quick refresh
    on basic ML concepts might find helpful the section “Basic ML Reviews” in the
    [book’s GitHub repository](https://oreil.ly/designing-machine-learning-systems-code).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我期望大多数读者已经了解常见的 ML 算法，如线性模型、决策树、*k* 最近邻居和不同类型的神经网络。本章将讨论围绕这些算法的技术，但不会详细介绍它们的工作原理。因为本章涉及
    ML 算法，所以需要比其他章节更多的 ML 知识。如果您对此不熟悉，我建议在阅读本章之前参加在线课程或阅读一本关于 ML 算法的书籍。希望快速复习基本 ML
    概念的读者可能会发现[书的 GitHub 仓库](https://oreil.ly/designing-machine-learning-systems-code)中“基本
    ML 复习”部分有所帮助。
- en: Model Development and Training
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型开发与训练
- en: In this section, we’ll discuss necessary aspects to help you develop and train
    your model, including how to evaluate different ML models for your problem, creating
    ensembles of models, experiment tracking and versioning, and distributed training,
    which is necessary for the scale at which models today are usually trained at.
    We’ll end this section with the more advanced topic of AutoML—using ML to automatically
    choose a model best for your problem.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论帮助您开发和训练模型的必要方面，包括如何为您的问题评估不同的 ML 模型、创建模型集合、实验跟踪与版本控制以及分布式训练，这对于目前通常训练模型的规模是必要的。我们将以更高级别的
    AutoML 主题结束本节——使用 ML 自动选择最适合您问题的模型。
- en: Evaluating ML Models
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估 ML 模型
- en: There are many possible solutions to any given problem. Given a task that can
    leverage ML in its solution, you might wonder what ML algorithm you should use
    for it. For example, should you start with logistic regression, an algorithm that
    you’re already familiar with? Or should you try out a new fancy model that is
    supposed to be the new state of the art for your problem? A more senior colleague
    mentioned that gradient-boosted trees have always worked for her for this task
    in the past—should you listen to her advice?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 针对任何给定问题，都有许多可能的解决方案。面对一个可以利用机器学习来解决的任务，你可能会想知道应该使用哪种机器学习算法。例如，你应该从你已经熟悉的逻辑回归开始吗？还是应该尝试一个新的看起来是你问题的最新技术的新模型？一位资深同事提到，梯度提升树对她过去这个任务总是有效——你应该听她的建议吗？
- en: If you had unlimited time and compute power, the rational thing to do would
    be to try all possible solutions and see what is best for you. However, time and
    compute power are limited resources, and you have to be strategic about what models
    you select.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有无限的时间和计算能力，理性的做法是尝试所有可能的解决方案，看看哪个对你最有利。然而，时间和计算能力都是有限资源，你必须对选择的模型有战略性的考虑。
- en: When talking about ML algorithms, many people think in terms of classical ML
    algorithms versus neural networks. There are a lot of interests and media coverage
    for neural networks, especially deep learning, which is understandable given that
    most of the AI progress in the last decade happened due to neural networks getting
    bigger and deeper.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 谈到机器学习算法时，许多人会想到经典的机器学习算法与神经网络之间的对比。神经网络，尤其是深度学习，受到了大量的兴趣和媒体报道，这是可以理解的，因为过去十年中大部分人工智能的进展都是由于神经网络变得更大、更深。
- en: These interests and coverage might give off the impression that deep learning
    is replacing classical ML algorithms. However, even though deep learning is finding
    more use cases in production, classical ML algorithms are not going away. Many
    recommender systems still rely on collaborative filtering and matrix factorization.
    Tree-based algorithms, including gradient-boosted trees, still power many classification
    tasks with strict latency requirements.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这些兴趣和报道可能会给人一种深度学习正在取代经典机器学习算法的印象。然而，尽管深度学习在生产中发现了更多的用例，但经典机器学习算法并没有消失。许多推荐系统仍然依赖协同过滤和矩阵分解。基于树的算法，包括梯度提升树，仍然驱动着许多有严格延迟要求的分类任务。
- en: Even in applications where neural networks are deployed, classic ML algorithms
    are still being used in tandem. For example, neural networks and decision trees
    might be used together in an ensemble. A k-means clustering model might be used
    to extract features to input into a neural network. Vice versa, a pretrained neural
    network (like BERT or GPT-3) might be used to generate embeddings to input into
    a logistic regression model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在部署神经网络的应用中，经典的机器学习算法仍然在同时使用。例如，神经网络和决策树可能会在集成中一起使用。一个k-means聚类模型可能被用来提取特征，以输入到神经网络中。反之亦然，一个预训练的神经网络（如BERT或GPT-3）可能被用来生成嵌入，以输入到逻辑回归模型中。
- en: When selecting a model for your problem, you don’t choose from every possible
    model out there, but usually focus on a set of models suitable for your problem.
    For example, if your boss tells you to build a system to detect toxic tweets,
    you know that this is a text classification problem—given a piece of text, classify
    whether it’s toxic or not—and common models for text classification include naive
    Bayes, logistic regression, recurrent neural networks, and transformer-based models
    such as BERT, GPT, and their variants.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在为你的问题选择模型时，你并不是从所有可能的模型中选择，而是通常集中在一组适合你问题的模型上。例如，如果你的老板告诉你要建立一个检测有毒推文的系统，你知道这是一个文本分类问题——给定一段文本，分类它是否有毒——常见的文本分类模型包括朴素贝叶斯、逻辑回归、循环神经网络以及基于变压器的模型如BERT、GPT及其变种。
- en: If your client wants you to build a system to detect fraudulent transactions,
    you know that this is the classic abnormality detection problem—fraudulent transactions
    are abnormalities that you want to detect—and common algorithms for this problem
    are many, including *k*-nearest neighbors, isolation forest, clustering, and neural
    networks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的客户希望你建立一个检测欺诈交易的系统，你知道这是经典的异常检测问题——欺诈交易是你想要检测的异常——这个问题的常见算法有许多，包括*k*-最近邻算法、孤立森林、聚类和神经网络。
- en: Knowledge of common ML tasks and the typical approaches to solve them is essential
    in this process.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对常见的机器学习任务及其解决方法的了解在这个过程中是至关重要的。
- en: Different types of algorithms require different numbers of labels as well as
    different amounts of compute power. Some take longer to train than others, whereas
    some take longer to make predictions. Non-neural network algorithms tend to be
    more explainable (e.g., what features contributed the most to an email being classified
    as spam) than neural networks.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 不同类型的算法需要不同数量的标签以及不同数量的计算资源。有些训练时间更长，而有些则更长时间做出预测。非神经网络算法往往更易于解释（例如，哪些特征对将电子邮件分类为垃圾邮件做出了最大贡献）比神经网络更易于解释。
- en: When considering what model to use, it’s important to consider not only the
    model’s performance, measured by metrics such as accuracy, F1 score, and log loss,
    but also its other properties, such as how much data, compute, and time it needs
    to train, what’s its inference latency, and interpretability. For example, a simple
    logistic regression model might have lower accuracy than a complex neural network,
    but it requires less labeled data to start, it’s much faster to train, it’s much
    easier to deploy, and it’s also much easier to explain why it’s making certain
    predictions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑使用哪种模型时，重要的是不仅要考虑模型的性能，例如准确度、F1分数和对数损失，还要考虑其它属性，比如需要训练的数据量、计算资源和时间、推断延迟以及可解释性。例如，一个简单的逻辑回归模型可能比复杂的神经网络精度要低，但它需要更少的标注数据开始训练，训练速度更快，部署起来更容易，并且解释其为何做出特定预测也更容易。
- en: Comparing ML algorithms is out of the scope of this book. No matter how good
    a comparison is, it will be outdated as soon as new algorithms come out. Back
    in 2016, LSTM-RNNs were all the rage and the backbone of the architecture seq2seq
    (Sequence-to-Sequence) that powered many NLP tasks from machine translation to
    text summarization to text classification. However, just two years later, recurrent
    architectures were largely replaced by transformer architectures for NLP tasks.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 比较机器学习算法不在本书的范围之内。无论比较多么出色，一旦新算法出现，它就会过时。回到2016年，LSTM-RNN曾风靡一时，是支持许多NLP任务的seq2seq（序列到序列）架构的骨干，从机器翻译到文本摘要到文本分类。然而，仅仅两年后，循环架构在NLP任务中大部分被变压器架构所取代。
- en: To understand different algorithms, the best way is to equip yourself with basic
    ML knowledge and run experiments with the algorithms you’re interested in. To
    keep up to date with so many new ML techniques and models, I find it helpful to
    monitor trends at major ML conferences such as NeurIPS, ICLR, and ICML, as well
    as following researchers whose work has a high signal-to-noise ratio on Twitter.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解不同的算法，最好的方法是装备自己基础的机器学习知识，并运行你感兴趣的算法的实验。为了跟上如此多的新机器学习技术和模型，我发现监视像NeurIPS、ICLR和ICML这样的主要机器学习会议的趋势，以及在Twitter上关注那些工作具有高信噪比的研究人员是有帮助的。
- en: Six tips for model selection
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型选择的六个建议
- en: Without getting into specifics of different algorithms, here are six tips that
    might help you decide what ML algorithms to work on next.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 不涉及具体的不同算法，这里是六个建议，可能帮助你决定下一步要研究的机器学习算法。
- en: Avoid the state-of-the-art trap
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 避免陷入最新技术的陷阱
- en: While helping companies as well as recent graduates get started in ML, I usually
    have to spend a nontrivial amount of time steering them away from jumping straight
    into state-of-the-art models. I can see why people want state-of-the-art models.
    Many believe that these models would be the best solutions for their problems—why
    try an old solution if you believe that a newer and superior solution exists?
    Many business leaders also want to use state-of-the-art models because they want
    to make their businesses appear cutting edge. Developers might also be more excited
    getting their hands on new models than getting stuck into the same old things
    over and over again.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在帮助公司和最近毕业生入门机器学习时，我通常要花不少时间引导他们不要直接跳进最新的模型中。我可以理解为什么人们想要最新的模型。许多人认为这些模型会是解决问题的最佳方案——如果你相信存在更新且更优越的解决方案，为什么要尝试一个旧的解决方案呢？许多业务领导也想使用最新的模型，因为他们希望使他们的业务显得前沿。开发者可能更喜欢尝试新模型，而不是一遍又一遍地陷入相同的老问题中。
- en: Researchers often only evaluate models in academic settings, which means that
    a model being state of the art often means that *it performs better than existing
    models on some static datasets*. It doesn’t mean that this model will be fast
    enough or cheap enough for *you* to implement. It doesn’t even mean that this
    model will perform better than other models on *your* data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员通常只在学术环境中评估模型，这意味着一个模型被视为最先进通常意味着*它在某些静态数据集上表现优于现有模型*。这并不意味着这个模型将足够快或便宜以便*你*实施。甚至这也不意味着这个模型将在*你*的数据上比其他模型表现更好。
- en: While it’s essential to stay up to date with new technologies and beneficial
    to evaluate them for your business, the most important thing to do when solving
    a problem is finding solutions that can solve that problem. If there’s a solution
    that can solve your problem that is much cheaper and simpler than state-of-the-art
    models, use the simpler solution.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管保持与新技术的同步是必要的，并且评估其对你的业务的益处是有益的，但解决问题时最重要的是找到能解决问题的解决方案。如果有一个比最先进模型更便宜和更简单的解决方案能够解决你的问题，那么使用简单的解决方案是最重要的。
- en: Start with the simplest models
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从最简单的模型开始
- en: Zen of Python states that “simple is better than complex,” and this principle
    is applicable to ML as well. Simplicity serves three purposes. First, simpler
    models are easier to deploy, and deploying your model early allows you to validate
    that your prediction pipeline is consistent with your training pipeline. Second,
    starting with something simple and adding more complex components step-by-step
    makes it easier to understand your model and debug it. Third, the simplest model
    serves as a baseline to which you can compare your more complex models.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Python之禅指出，“简单胜于复杂”，这个原则在机器学习中同样适用。简单性有三个目的。首先，简单模型更容易部署，早期部署你的模型可以验证你的预测管道与训练管道一致。其次，从简单开始，逐步添加更复杂的组件使得理解和调试你的模型更容易。第三，最简单的模型作为基准，可以用来比较你更复杂的模型。
- en: Simplest models are not always the same as models with the least effort. For
    example, pretrained BERT models are complex, but they require little effort to
    get started with, especially if you use a ready-made implementation like the one
    in Hugging Face’s Transformer. In this case, it’s not a bad idea to use the complex
    solution, given that the community around this solution is well developed enough
    to help you get through any problems you might encounter. However, you might still
    want to experiment with simpler solutions to ensure that pretrained BERT is indeed
    better than those simpler solutions for your problem. Pretrained BERT might be
    low effort to start with, but it can be quite high effort to improve upon. Whereas
    if you start with a simpler model, there’ll be a lot of room for you to improve
    upon your model.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的模型并不总是与最小努力的模型相同。例如，预训练BERT模型很复杂，但是如果你使用像Hugging Face的Transformer中那样的现成实现，那么开始使用它们几乎没有什么难度。在这种情况下，使用复杂的解决方案并不是一个坏主意，因为这个解决方案周围的社区已经发展得足够好，可以帮助你解决可能遇到的任何问题。然而，你可能仍然希望尝试更简单的解决方案，以确保预训练BERT确实比这些更简单的解决方案更适合你的问题。预训练BERT可能开始起步较轻松，但是改进起来可能需要相当大的努力。而如果你从一个更简单的模型开始，你将有很多空间来改进你的模型。
- en: Avoid human biases in selecting models
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 避免在选择模型时存在人类偏见
- en: 'Imagine an engineer on your team is assigned the task of evaluating which model
    is better for your problem: a gradient-boosted tree or a pretrained BERT model.
    After two weeks, this engineer announces that the best BERT model outperforms
    the best gradient-boosted tree by 5%. Your team decides to go with the pretrained
    BERT model.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你团队中的一名工程师被分配了评估哪个模型更适合你的问题的任务：梯度提升树还是预训练BERT模型。两周后，这位工程师宣布最好的BERT模型比最好的梯度提升树表现提升了5%。你的团队决定选择预训练BERT模型。
- en: A few months later, however, a seasoned engineer joins your team. She decides
    to look into gradient-boosted trees again and finds out that this time, the best
    gradient-boosted tree outperforms the pretrained BERT model you currently have
    in production. What happened?
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 几个月后，然而，一位经验丰富的工程师加入了你的团队。她决定重新研究梯度提升树，发现这一次，最优的梯度提升树优于你当前生产中的预训练BERT模型。发生了什么事情？
- en: There are a lot of human biases in evaluating models. Part of the process of
    evaluating an ML architecture is to experiment with different features and different
    sets of hyperparameters to find the best model of that architecture. If an engineer
    is more excited about an architecture, they will likely spend a lot more time
    experimenting with it, which might result in better-performing models for that
    architecture.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估模型时存在许多人为偏见。评估 ML 架构的过程之一是尝试不同的特征和不同的超参数组合，以找到该架构的最佳模型。如果工程师对某个架构更感兴趣，他们可能会花更多时间进行实验，这可能会导致该架构的模型表现更好。
- en: When comparing different architectures, it’s important to compare them under
    comparable setups. If you run 100 experiments for an architecture, it’s not fair
    to only run a couple of experiments for the architecture you’re evaluating it
    against. You might need to run 100 experiments for the other architecture too.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在比较不同架构时，重要的是在可比较的设置下进行比较。如果你为一个架构运行了 100 次实验，那么仅仅为正在评估的另一个架构运行几次实验是不公平的。你可能也需要为另一个架构运行
    100 次实验。
- en: Because the performance of a model architecture depends heavily on the context
    it’s evaluated in—e.g., the task, the training data, the test data, the hyperparameters,
    etc.—it’s extremely difficult to make claims that a model architecture is better
    than another architecture. The claim might be true in a context, but unlikely
    true for all possible contexts.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因为模型架构的性能在其评估的上下文中高度依赖于各种因素，例如任务、训练数据、测试数据、超参数等，所以很难断言一个模型架构比另一个更好。在某个特定上下文中可能是真的，但在所有可能的上下文中不太可能是真的。
- en: Evaluate good performance now versus good performance later
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 现在评估好的性能与以后评估好的性能对比
- en: The best model now does not always mean the best model two months from now.
    For example, a tree-based model might work better now because you don’t have a
    ton of data yet, but two months from now, you might be able to double your amount
    of training data, and your neural network might perform much better.^([1](ch06.xhtml#ch01fn150))
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在最好的模型并不总是意味着两个月后最好的模型。例如，基于树的模型现在可能效果更好，因为你还没有大量数据，但两个月后，你可能能够增加两倍的训练数据，你的神经网络可能会表现得更好。^([1](ch06.xhtml#ch01fn150))
- en: A simple way to estimate how your model’s performance might change with more
    data is to use [learning curves](https://oreil.ly/9QZLa). A learning curve of
    a model is a plot of its performance—e.g., training loss, training accuracy, validation
    accuracy—against the number of training samples it uses, as shown in [Figure 6-1](#the_learning_curves_of_a_naive_bayes_mo).
    The learning curve won’t help you estimate exactly how much performance gain you
    can get from having more training data, but it can give you a sense of whether
    you can expect any performance gain at all from more training data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 估计你的模型在有更多数据时性能如何变化的一种简单方法是使用[学习曲线](https://oreil.ly/9QZLa)。模型的学习曲线是其性能的绘图，例如训练损失、训练准确率、验证准确率，以及它使用的训练样本数量，如[图
    6-1](#the_learning_curves_of_a_naive_bayes_mo)所示。学习曲线不能帮助你准确估计通过增加训练数据可以获得多少性能提升，但它可以让你知道是否可以期待从更多训练数据中获得任何性能提升。
- en: '![](Images/dmls_0601.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_0601.png)'
- en: 'Figure 6-1\. The learning curves of a naive Bayes model and an SVM model. Source:
    [scikit-learn](https://oreil.ly/QA52c)'
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-1\. 朴素贝叶斯模型和 SVM 模型的学习曲线。来源：[scikit-learn](https://oreil.ly/QA52c)
- en: A situation that I’ve encountered is when a team evaluates a simple neural network
    against a collaborative filtering model for making recommendations. When evaluating
    both models offline, the collaborative filtering model outperformed. However,
    the simple neural network can update itself with each incoming example, whereas
    the collaborative filtering has to look at all the data to update its underlying
    matrix. The team decided to deploy both the collaborative filtering model and
    the simple neural network. They used the collaborative filtering model to make
    predictions for users, and continually trained the simple neural network in production
    with new, incoming data. After two weeks, the simple neural network was able to
    outperform the collaborative filtering model.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我遇到过的一种情况是，当一个团队评估一个简单的神经网络和一个协同过滤模型来进行推荐时。在离线评估两个模型时，协同过滤模型表现更好。然而，简单的神经网络可以随着每个新样本的到来进行更新，而协同过滤必须查看所有数据来更新其基础矩阵。团队决定部署协同过滤模型和简单神经网络。他们使用协同过滤模型为用户做出预测，并持续在生产环境中用新的输入数据训练简单的神经网络。两周后，简单神经网络能够超越协同过滤模型。
- en: While evaluating models, you might want to take into account their potential
    for improvements in the near future, and how easy/difficult it is to achieve those
    improvements.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估模型时，你可能希望考虑它们在不久的将来改进的潜力，以及实现这些改进的难易程度。
- en: Evaluate trade-offs
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估权衡
- en: There are many trade-offs you have to make when selecting models. Understanding
    what’s more important in the performance of your ML system will help you choose
    the most suitable model.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择模型时，你必须做出许多权衡。了解在你的机器学习系统性能中什么更重要将帮助你选择最合适的模型。
- en: One classic example of trade-off is the false positives and false negatives
    trade-off. Reducing the number of false positives might increase the number of
    false negatives, and vice versa. In a task where false positives are more dangerous
    than false negatives, such as fingerprint unlocking (unauthorized people shouldn’t
    be classified as authorized and given access), you might prefer a model that makes
    fewer false positives. Similarly, in a task where false negatives are more dangerous
    than false positives, such as COVID-19 screening (patients with COVID-19 shouldn’t
    be classified as no COVID-19), you might prefer a model that makes fewer false
    negatives.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一个经典的权衡例子是假阳性和假阴性之间的权衡。减少假阳性的数量可能会增加假阴性的数量，反之亦然。在假阳性比假阴性更为危险的任务中，例如指纹解锁（未经授权的人不应被分类为已授权并获得访问权限），你可能更喜欢一个能减少假阳性的模型。同样，在假阴性比假阳性更为危险的任务中，例如COVID-19筛查（COVID-19患者不应被错误地分类为非COVID-19），你可能更喜欢一个能减少假阴性的模型。
- en: Another example of trade-off is compute requirement and accuracy—a more complex
    model might deliver higher accuracy but might require a more powerful machine,
    such as a GPU instead of a CPU, to generate predictions with acceptable inference
    latency. Many people also care about the interpretability and performance trade-off.
    A more complex model can give a better performance, but its results are less interpretable.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个权衡的例子是计算需求和准确性——一个更复杂的模型可能会提供更高的准确性，但可能需要一个更强大的机器，例如GPU而非CPU，以在可接受的推断延迟下生成预测。许多人也关心可解释性和性能之间的权衡。一个更复杂的模型可以提供更好的性能，但其结果的解释性较差。
- en: Understand your model’s assumptions
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 了解你的模型假设
- en: The statistician George Box said in 1976 that “all models are wrong, but some
    are useful.” The real world is intractably complex, and models can only approximate
    using assumptions. Every single model comes with its own assumptions. Understanding
    what assumptions a model makes and whether our data satisfies those assumptions
    can help you evaluate which model works best for your use case.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学家乔治·博克斯在1976年曾说过：“所有模型都是错误的，但有些是有用的。” 现实世界异常复杂，模型只能基于假设进行近似。每个模型都有其自己的假设。了解一个模型做出了哪些假设，以及我们的数据是否满足这些假设，有助于评估哪种模型最适合你的使用场景。
- en: 'Following are some of the common assumptions. It’s not meant to be an exhaustive
    list, but just a demonstration:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些常见的假设。这不是详尽无遗的列表，而只是一个演示：
- en: Prediction assumption
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 预测假设
- en: Every model that aims to predict an output *Y* from an input *X* makes the assumption
    that it’s possible to predict *Y* based on *X*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 每个旨在从输入*X*预测输出*Y*的模型都假设，基于*X*可以预测*Y*是可能的。
- en: IID
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 独立同分布（IID）
- en: Neural networks assume that the examples are [independent and identically distributed](https://oreil.ly/hXRr2),
    which means that all the examples are independently drawn from the same joint
    distribution.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络假设示例是[独立同分布的](https://oreil.ly/hXRr2)，这意味着所有示例都是从相同的联合分布独立抽取的。
- en: Smoothness
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 平滑性
- en: Every supervised machine learning method assumes that there’s a set of functions
    that can transform inputs into outputs such that similar inputs are transformed
    into similar outputs. If an input *X* produces an output *Y*, then an input close
    to *X* would produce an output proportionally close to *Y*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 每个监督学习方法假设存在一组函数，可以将输入转换为输出，使得相似的输入被转换为相似的输出。如果一个输入*X*产生输出*Y*，那么接近*X*的输入会产生与*Y*成比例的输出。
- en: Tractability
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 可计算性
- en: Let *X* be the input and *Z* be the latent representation of *X*. Every generative
    model makes the assumption that it’s tractable to compute the probability *P*(*Z*|*X*).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 设*X*为输入，*Z*为*X*的潜在表示。每个生成模型都假设可以计算概率*P*(*Z*|*X*)。
- en: Boundaries
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 边界
- en: A linear classifier assumes that decision boundaries are linear.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 线性分类器假设决策边界是线性的。
- en: Conditional independence
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 条件独立
- en: A naive Bayes classifier assumes that the attribute values are independent of
    each other given the class.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器假设给定类别时属性值是相互独立的。
- en: Normally distributed
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 正态分布
- en: Many statistical methods assume that data is normally distributed.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 许多统计方法假设数据服从正态分布。
- en: Ensembles
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成方法
- en: When considering an ML solution to your problem, you might want to start with
    a system that contains just one model (the process of selecting one model for
    your problem was discussed earlier in the chapter). After developing one single
    model, you might think about how to continue improving its performance. One method
    that has consistently given a performance boost is to use an ensemble of multiple
    models instead of just an individual model to make predictions. Each model in
    the ensemble is called a *base learner*. For example, for the task of predicting
    whether an email is SPAM or NOT SPAM, you might have three different models. The
    final prediction for each email is the majority vote of all three models. So if
    at least two base learners output SPAM, the email will be classified as SPAM.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑解决您的问题的机器学习解决方案时，您可能希望从包含一个模型的系统开始（在本章早些时候讨论了选择解决方案的过程）。开发出一个单一模型后，您可以考虑如何进一步提高其性能。一种始终有效的方法是使用多个模型的集成来进行预测，而不仅仅是一个模型。集成中的每个模型称为*基学习器*。例如，对于预测电子邮件是否为垃圾邮件的任务，您可能有三种不同的模型。每封电子邮件的最终预测是所有三个模型的多数投票。因此，如果至少两个基学习器输出垃圾邮件，则该邮件将被分类为垃圾邮件。
- en: Twenty out of 22 winning solutions on Kaggle competitions in 2021, as of August
    2021, use ensembles.^([2](ch06.xhtml#ch01fn151)) As of January 2022, 20 top solutions
    on [SQuAD 2.0](https://oreil.ly/odo12), the Stanford Question Answering Dataset,
    are ensembles, as shown in [Figure 6-2](#as_of_january_twozerotwotwocomma_the_to).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 截至2021年8月，Kaggle比赛中的22个获胜解决方案中，有20个使用了集成方法。^([2](ch06.xhtml#ch01fn151)) 截至2022年1月，[SQuAD
    2.0](https://oreil.ly/odo12)，斯坦福问答数据集的前20个解决方案都是集成方法，如[图6-2](#as_of_january_twozerotwotwocomma_the_to)所示。
- en: Ensembling methods are less favored in production because ensembles are more
    complex to deploy and harder to maintain. However, they are still common for tasks
    where a small performance boost can lead to a huge financial gain, such as predicting
    click-through rate for ads.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法在生产环境中不太受青睐，因为集成模型部署复杂且难以维护。然而，在某些任务中，集成模型仍然很常见，因为稍微提高的性能可能带来巨大的财务收益，比如预测广告的点击率。
- en: '![](Images/dmls_0602.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_0602.png)'
- en: Figure 6-2\. As of January 2022, the top 20 solutions on [SQuAD 2.0](https://oreil.ly/odo12)
    are all ensembles
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-2。截至2022年1月，[SQuAD 2.0](https://oreil.ly/odo12)的前20个解决方案全部是集成方法。
- en: We’ll go over an example to give you the intuition of why ensembling works.
    Imagine you have three email spam classifiers, each with an accuracy of 70%. Assuming
    that each classifier has an equal probability of making a correct prediction for
    each email, and that these three classifiers are not correlated, we’ll show that
    by taking the majority vote of these three classifiers, we can get an accuracy
    of 78.4%.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一个例子来说明集成方法为何有效。想象一下，您有三个电子邮件垃圾分类器，每个分类器的准确率为70%。假设每个分类器对每封电子邮件的正确预测具有相等的概率，并且这三个分类器之间没有相关性，我们将展示通过这三个分类器的多数投票，我们可以获得78.4%的准确率。
- en: For each email, each classifier has a 70% chance of being correct. The ensemble
    will be correct if at least two classifiers are correct. [Table 6-1](#possible_outcomes_of_the_ensemble_that)
    shows the probabilities of different possible outcomes of the ensemble given an
    email. This ensemble will have an accuracy of 0.343 + 0.441 = 0.784, or 78.4%.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 每封电子邮件，每个分类器都有70%的正确概率。如果至少有两个分类器是正确的，那么集成将是正确的。[Table 6-1](#possible_outcomes_of_the_ensemble_that)
    展示了给定电子邮件的集成不同可能结果的概率。这个集成将有0.343 + 0.441 = 0.784，即78.4%的准确率。
- en: Table 6-1\. Possible outcomes of the ensemble that takes the majority vote from
    three classifiers
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表6-1\. 从三个分类器中采用多数投票法的集成可能结果
- en: '| Outputs of three models | Probability | Ensemble’s output |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 三个模型的输出 | 概率 | 集成输出 |'
- en: '| --- | --- | --- |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| All three are correct | 0.7 * 0.7 * 0.7 = 0.343 | Correct |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 三个都正确 | 0.7 * 0.7 * 0.7 = 0.343 | 正确 |'
- en: '| Only two are correct | (0.7 * 0.7 * 0.3) * 3 = 0.441 | Correct |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 仅两个正确 | (0.7 * 0.7 * 0.3) * 3 = 0.441 | 正确 |'
- en: '| Only one is correct | (0.3 * 0.3 * 0.7) * 3 = 0.189 | Wrong |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 仅一个正确 | (0.3 * 0.3 * 0.7) * 3 = 0.189 | 错误 |'
- en: '| None are correct | 0.3 * 0.3 * 0.3 = 0.027 | Wrong |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 无一个正确 | 0.3 * 0.3 * 0.3 = 0.027 | 错误 |'
- en: This calculation only holds if the classifiers in an ensemble are uncorrelated.
    If all classifiers are perfectly correlated—all three of them make the same prediction
    for every email—the ensemble will have the same accuracy as each individual classifier.
    When creating an ensemble, the less correlation there is among base learners,
    the better the ensemble will be. Therefore, it’s common to choose very different
    types of models for an ensemble. For example, you might create an ensemble that
    consists of one transformer model, one recurrent neural network, and one gradient-boosted
    tree.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在集成中的分类器不相关时，此计算才有效。如果所有分类器完全相关，即它们每个都对每封电子邮件做出相同预测，那么集成的准确性将与每个单独分类器的准确性相同。创建集成时，基础学习器之间的相关性越低，集成效果越好。因此，通常选择非常不同类型的模型进行集成。例如，可以创建一个由一个Transformer模型、一个循环神经网络和一个梯度提升树组成的集成。
- en: 'There are three ways to create an ensemble: bagging, boosting, and stacking.
    In addition to helping boost performance, according to several survey papers,
    ensemble methods such as boosting and bagging, together with resampling, have
    shown to help with imbalanced datasets.^([3](ch06.xhtml#ch01fn152)) We’ll go over
    each of these three methods, starting with bagging.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 创建集成有三种方法：bagging、boosting和stacking。根据几篇调查论文显示，除了帮助提升性能外，集成方法如boosting和bagging，连同重采样，还有助于处理不平衡数据集。^([3](ch06.xhtml#ch01fn152))
    我们将逐一介绍这三种方法，首先是bagging。
- en: Bagging
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Bagging
- en: Bagging, shortened from *bootstrap aggregating*, is designed to improve both
    the training stability and accuracy of ML algorithms.^([4](ch06.xhtml#ch01fn153))
    It reduces variance and helps to avoid overfitting.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging，缩写自*bootstrap aggregating*，旨在提高机器学习算法的训练稳定性和准确性。^([4](ch06.xhtml#ch01fn153))
    它减少了方差并有助于避免过拟合。
- en: Given a dataset, instead of training one classifier on the entire dataset, you
    sample with replacement to create different datasets, called bootstraps, and train
    a classification or regression model on each of these bootstraps. Sampling with
    replacement ensures that each bootstrap is created independently from its peers.
    [Figure 6-3](#bagging_illustrationdot_source_sirakorn) shows an illustration of
    bagging.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 给定数据集，与在整个数据集上训练一个分类器不同，您可以使用有放回抽样创建不同的数据集，称为bootstrap，并在每个bootstrap上训练分类或回归模型。有放回抽样确保每个bootstrap都是独立创建的。[Figure 6-3](#bagging_illustrationdot_source_sirakorn)
    展示了bagging的示意图。
- en: '![](Images/dmls_0603.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_0603.png)'
- en: 'Figure 6-3\. Bagging illustration. Source: Adapted from an image by [Sirakorn](https://oreil.ly/KEAPl)'
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-3\. Bagging示意图。来源：根据[Sirakorn](https://oreil.ly/KEAPl)的图像适配
- en: If the problem is classification, the final prediction is decided by the majority
    vote of all models. For example, if 10 classifiers vote SPAM and 6 models vote
    NOT SPAM, the final prediction is SPAM.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果问题是分类，最终预测由所有模型的多数投票决定。例如，如果有10个分类器投票SPAM，6个模型投票NOT SPAM，最终预测是SPAM。
- en: If the problem is regression, the final prediction is the average of all models’
    predictions.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果问题是回归，最终预测是所有模型预测的平均值。
- en: Bagging generally improves unstable methods, such as neural networks, classification
    and regression trees, and subset selection in linear regression. However, it can
    mildly degrade the performance of stable methods such as *k*-nearest neighbors.^([5](ch06.xhtml#ch01fn154))
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging通常可以改善不稳定方法，比如神经网络、分类和回归树，以及线性回归中的子集选择。然而，它可能会轻微降低稳定方法（比如*k*最近邻）的性能。^([5](ch06.xhtml#ch01fn154))
- en: A random forest is an example of bagging. A random forest is a collection of
    decision trees constructed by both bagging and feature randomness, where each
    tree can pick only from a random subset of features to use.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是Bagging的一个例子。随机森林由Bagging和特征随机性构建的决策树集合组成，其中每棵树只能从一个随机特征子集中选择。
- en: Boosting
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提升
- en: Boosting is a family of iterative ensemble algorithms that convert weak learners
    to strong ones. Each learner in this ensemble is trained on the same set of samples,
    but the samples are weighted differently among iterations. As a result, future
    weak learners focus more on the examples that previous weak learners misclassified.
    [Figure 6-4](#boosting_illustrationdot_source_sirakor) shows an illustration of
    boosting, which involves the steps that follow.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 提升是一族迭代集成算法，将弱学习器转化为强学习器。这个集成中的每个学习器都是在同一组样本上训练的，但是在迭代中样本的权重不同。因此，未来的弱学习器更加关注之前弱学习器误分类的例子。[图 6-4](#boosting_illustrationdot_source_sirakor)显示了提升的示意图，展示了随后的步骤。
- en: '![](Images/dmls_0604.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_0604.png)'
- en: 'Figure 6-4\. Boosting illustration. Source: Adapted from an image by [Sirakorn](https://oreil.ly/h5cuS)'
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-4\. 提升示例。来源：根据[Sirakorn](https://oreil.ly/h5cuS)的图片改编。
- en: You start by training the first weak classifier on the original dataset.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，在原始数据集上训练第一个弱分类器。
- en: Samples are reweighted based on how well the first classifier classifies them,
    e.g., misclassified samples are given higher weight.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据第一个分类器分类效果对样本进行了重新加权，例如，误分类的样本被赋予更高的权重。
- en: Train the second classifier on this reweighted dataset. Your ensemble now consists
    of the first and the second classifiers.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个重新加权的数据集上训练第二个分类器。你的集成现在包括第一个和第二个分类器。
- en: Samples are weighted based on how well the ensemble classifies them.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 样本的权重基于集成对其进行分类的效果来确定。
- en: Train the third classifier on this reweighted dataset. Add the third classifier
    to the ensemble.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个重新加权的数据集上训练第三个分类器。将第三个分类器添加到集成中。
- en: Repeat for as many iterations as needed.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如有需要，重复多次迭代。
- en: Form the final strong classifier as a weighted combination of the existing classifiers—classifiers
    with smaller training errors have higher weights.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 形成最终的强分类器，作为现有分类器的加权组合 —— 训练误差较小的分类器具有较高的权重。
- en: An example of a boosting algorithm is a gradient boosting machine (GBM), which
    produces a prediction model typically from weak decision trees. It builds the
    model in a stage-wise fashion like other boosting methods do, and it generalizes
    them by allowing optimization of an arbitrary differentiable loss function.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 提升算法的一个例子是梯度提升机（GBM），它通常通过弱决策树生成预测模型。它像其他提升方法一样，以逐阶段的方式构建模型，并通过允许优化任意可微损失函数来泛化它们。
- en: XGBoost, a variant of GBM, used to be the algorithm of choice for many winning
    teams of ML competitions.^([6](ch06.xhtml#ch01fn155)) It’s been used in a wide
    range of tasks from classification, ranking, to the discovery of the Higgs Boson.^([7](ch06.xhtml#ch01fn156))
    However, many teams have been opting for [LightGBM](https://oreil.ly/1qyWf), a
    distributed gradient boosting framework that allows parallel learning, which generally
    allows faster training on large datasets.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost，GBM的一种变体，曾是许多机器学习比赛中获胜团队首选的算法。^([6](ch06.xhtml#ch01fn155)) 它被广泛应用于从分类、排名到发现希格斯玻色子等多种任务。^([7](ch06.xhtml#ch01fn156))
    然而，许多团队开始选择[LightGBM](https://oreil.ly/1qyWf)，这是一个分布式梯度提升框架，支持并行学习，通常能更快地处理大规模数据集的训练。
- en: Stacking
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 堆叠
- en: 'Stacking means that you train base learners from the training data then create
    a meta-learner that combines the outputs of the base learners to output final
    predictions, as shown in [Figure 6-5](#a_visualization_of_a_stacked_ensemble_f).
    The meta-learner can be as simple as a heuristic: you take the majority vote (for
    classification tasks) or the average vote (for regression tasks) from all base
    learners. It can be another model, such as a logistic regression model or a linear
    regression model.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠意味着您从训练数据中训练基本学习器，然后创建一个元学习器，该元学习器将基本学习器的输出组合起来生成最终预测结果，如[图 6-5](#a_visualization_of_a_stacked_ensemble_f)所示。元学习器可以是一个简单的启发式方法：对于分类任务，您可以采用多数投票，对于回归任务，您可以采用平均投票。也可以是另一个模型，例如逻辑回归模型或线性回归模型。
- en: '![](Images/dmls_0605.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_0605.png)'
- en: Figure 6-5\. A visualization of a stacked ensemble from three base learners
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-5\. 三个基本学习器的堆叠集成可视化
- en: For more great advice on how to create an ensemble, refer to the awesome [ensemble
    guide](https://oreil.ly/Nu6G6) by one of Kaggle’s legendary teams, MLWave.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 关于如何创建集成模型的更多出色建议，请参考Kaggle传奇团队MLWave的精彩[集成指南](https://oreil.ly/Nu6G6)。
- en: Experiment Tracking and Versioning
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验追踪和版本控制
- en: During the model development process, you often have to experiment with many
    architectures and many different models to choose the best one for your problem.
    Some models might seem similar to each other and differ in only one hyperparameter—such
    as one model using a learning rate of 0.003 and another model using a learning
    rate of 0.002—and yet their performances are dramatically different. It’s important
    to keep track of all the definitions needed to re-create an experiment and its
    relevant artifacts. An artifact is a file generated during an experiment—examples
    of artifacts can be files that show the loss curve, evaluation loss graph, logs,
    or intermediate results of a model throughout a training process. This enables
    you to compare different experiments and choose the one best suited for your needs.
    Comparing different experiments can also help you understand how small changes
    affect your model’s performance, which, in turn, gives you more visibility into
    how your model works.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型开发过程中，通常需要尝试许多不同的架构和模型，以选择最适合您问题的模型。有些模型可能看起来相似，只有一个超参数不同，比如一个模型使用学习率为0.003，另一个模型使用学习率为0.002，但它们的性能可能截然不同。重要的是要记录所有需要重新创建实验及其相关文档的定义。文档是在实验过程中生成的文件，例如显示损失曲线、评估损失图、日志或模型在训练过程中的中间结果的文件。这使您能够比较不同的实验，并选择最适合您需求的实验。比较不同的实验还可以帮助您理解如何通过小的变化影响模型的性能，进而更好地了解您的模型如何工作。
- en: The process of tracking the progress and results of an experiment is called
    experiment tracking. The process of logging all the details of an experiment for
    the purpose of possibly recreating it later or comparing it with other experiments
    is called versioning. These two go hand in hand with each other. Many tools originally
    set out to be experiment tracking tools, such as MLflow and Weights & Biases,
    have grown to incorporate versioning. Many tools originally set out to be versioning
    tools, such as [DVC](https://oreil.ly/f3sBp), have also incorporated experiment
    tracking.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪实验进展和结果的过程称为实验追踪。为了可能在以后重新创建实验或与其他实验比较而记录实验的所有详细信息的过程称为版本控制。这两者是密切相关的。许多最初旨在成为实验追踪工具的工具，如MLflow和Weights
    & Biases，现在已经发展为包含版本控制功能。许多最初旨在成为版本控制工具的工具，如[DVC](https://oreil.ly/f3sBp)，现在也已经包含了实验追踪功能。
- en: Experiment tracking
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实验追踪
- en: A large part of training an ML model is babysitting the learning processes.
    Many problems can arise during the training process, including loss not decreasing,
    overfitting, underfitting, fluctuating weight values, dead neurons, and running
    out of memory. It’s important to track what’s going on during training not only
    to detect and address these issues but also to evaluate whether your model is
    learning anything useful.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 训练机器学习模型的重要部分是监控学习过程。在训练过程中可能会出现许多问题，包括损失不降低、过拟合、欠拟合、权重值波动、死神经元和内存耗尽等。重要的是要跟踪训练过程中发生的情况，不仅用于检测和解决这些问题，还用于评估模型是否学到了有用的东西。
- en: 'When I just started getting into ML, all I was told to track was loss and speed.
    Fast-forward several years, and people are tracking so many things that their
    experiment tracking boards look both beautiful and terrifying at the same time.
    Following is just a short list of things you might want to consider tracking for
    each experiment during its training process:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当我刚开始接触机器学习时，所有人都告诉我要追踪的只有损失和速度。几年后，人们追踪的项目变得如此之多，以至于他们的实验追踪面板看起来既美观又可怕。以下是您在每个实验的训练过程中可能想要考虑追踪的事项的简短列表：
- en: The *loss curve* corresponding to the train split and each of the eval splits.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对应于训练集和每个评估集的*损失曲线*。
- en: The *model performance metrics* that you care about on all nontest splits, such
    as accuracy, F1, perplexity.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您关心的模型性能指标，在所有非测试集上，例如准确率、F1 值、困惑度。
- en: The log of *corresponding sample, prediction, and ground truth label*. This
    comes in handy for ad hoc analytics and sanity check.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*相应样本、预测和实际标签的日志*。这对于临时分析和检查数据的一致性非常有用。'
- en: The *speed* of your model, evaluated by the number of steps per second or, if
    your data is text, the number of tokens processed per second.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的模型*速度*，通过每秒步骤数或者如果您的数据是文本，则是每秒处理的标记数来评估。
- en: '*System performance metrics* such as memory usage and CPU/GPU utilization.
    They’re important to identify bottlenecks and avoid wasting system resources.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*系统性能指标*，如内存使用情况和CPU/GPU利用率。它们对于识别瓶颈并避免浪费系统资源非常重要。'
- en: The values over time of any *parameter and hyperparameter* whose changes can
    affect your model’s performance, such as the learning rate if you use a learning
    rate schedule; gradient norms (both globally and per layer), especially if you’re
    clipping your gradient norms; and weight norm, especially if you’re doing weight
    decay.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随时间变化的任何*参数和超参数*的值，这些变化可能会影响您模型的性能，例如如果使用学习率调度，则学习率；梯度范数（全局和每层），特别是如果剪裁梯度范数；以及权重范数，特别是如果进行权重衰减。
- en: In theory, it’s not a bad idea to track everything you can. Most of the time,
    you probably don’t need to look at most of them. But when something does happen,
    one or more of them might give you clues to understand and/or debug your model.
    In general, tracking gives you observability into the state of your model.^([8](ch06.xhtml#ch01fn157))
    However, in practice, due to the limitations of tooling today, it can be overwhelming
    to track too many things, and tracking less important things can distract you
    from tracking what is really important.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，跟踪所有可能的事项并不是一个坏主意。大多数情况下，您可能不需要查看大多数追踪项。但是当发生问题时，其中一个或多个可能会为您提供理解和/或调试模型的线索。总体上，追踪使您能够观察模型状态。^([8](ch06.xhtml#ch01fn157))
    然而，在实践中，由于当前工具的限制，跟踪太多事项可能会让人不知所措，而追踪不重要的事项可能会让您分心，而忽视真正重要的事项。
- en: Experiment tracking enables comparison across experiments. By observing how
    a certain change in a component affects the model’s performance, you gain some
    understanding into what that component does.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 实验追踪使得可以跨实验进行比较。通过观察某个组件变化如何影响模型性能，您可以理解该组件的功能。
- en: A simple way to track your experiments is to automatically make copies of all
    the code files needed for an experiment and log all outputs with their timestamps.^([9](ch06.xhtml#ch01fn158))
    Using third-party experiment tracking tools, however, can give you nice dashboards
    and allow you to share your experiments with your coworkers.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪实验的简单方法是自动复制所有实验所需的代码文件，并记录所有输出及其时间戳。^([9](ch06.xhtml#ch01fn158)) 然而，使用第三方实验追踪工具可以为您提供漂亮的仪表板，并允许您与同事共享您的实验。
- en: Versioning
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 版本控制
- en: Imagine this scenario. You and your team spent the last few weeks tweaking your
    model, and one of the runs finally showed promising results. You wanted to use
    it for more extensive tests, so you tried to replicate it using the set of hyperparameters
    you’d noted down somewhere, only to find out that the results weren’t quite the
    same. You remembered that you’d made some changes to the code between that run
    and the next, so you tried your best to undo the changes from memory because your
    reckless past self had decided that the change was too minimal to be committed.
    But you still couldn’t replicate the promising result because there are just too
    many possible ways to make changes.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下这种情况。你和你的团队在过去几周里不断调整你们的模型，最后一次运行显示出了有希望的结果。你想要用它进行更广泛的测试，所以你试图使用你某处记录下来的超参数集来复制它，但却发现结果并不完全相同。你记得在那次运行和下一次之间做了一些代码更改，所以你尽力凭记忆撤销那些更改，因为你那时的冲动自己认为这些变化太微小，不值得提交。但你仍然无法复制出有希望的结果，因为有太多可能的方式可以进行更改。
- en: This problem could have been avoided if you versioned your ML experiments. ML
    systems are part code, part data, so you need to not only version your code but
    your data as well. Code versioning has more or less become a standard in the industry.
    However, at this point, data versioning is like flossing. Everyone agrees it’s
    a good thing to do, but few do it.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对机器学习实验进行了版本控制，这个问题本可以避免。机器学习系统既是代码，也是数据，因此您不仅需要对代码进行版本控制，还需要对数据进行版本控制。代码版本控制已经成为行业标准。然而，目前来说，数据版本控制就像使用牙线一样。每个人都认为这是一个好习惯，但很少有人这样做。
- en: There are a few reasons why data versioning is challenging. One reason is that
    because data is often much larger than code, we can’t use the same strategy that
    people usually use to version code to version data.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 数据版本控制面临几个挑战。其中一个原因是，因为数据通常比代码要大得多，所以我们不能使用通常用于版本控制代码的同样策略来版本控制数据。
- en: For example, code versioning is done by keeping track of all the changes made
    to a codebase. A change is known as a diff, short for difference. Each change
    is measured by line-by-line comparison. A line of code is usually short enough
    for line-by-line comparison to make sense. However, a line of your data, especially
    if it’s stored in a binary format, can be indefinitely long. Saying that this
    line of 1,000,000 characters is different from the other line of 1,000,000 characters
    isn’t going to be that helpful.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，代码版本控制通过跟踪对代码库的所有更改来进行。一个更改被称为diff，简称差异。每个更改都通过逐行比较来衡量。代码行通常很短，逐行比较是有意义的。然而，你的数据行，特别是如果它以二进制格式存储，可能长度无限。说这一行有100万个字符不同于另一行100万个字符，这并没有多大帮助。
- en: Code versioning tools allow users to revert to a previous version of the codebase
    by keeping copies of all the old files. However, a dataset used might be so large
    that duplicating it multiple times might be unfeasible.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 代码版本控制工具允许用户通过保留所有旧文件的副本来恢复到先前的代码库版本。然而，使用的数据集可能非常大，多次复制可能是不可行的。
- en: Code versioning tools allow for multiple people to work on the same codebase
    at the same time by duplicating the codebase on each person’s local machine. However,
    a dataset might not fit into a local machine.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 代码版本控制工具允许多人同时在同一份代码库上工作，每个人在本地机器上复制代码库。然而，数据集可能无法适应本地机器。
- en: Second, there’s still confusion in what exactly constitutes a diff when we version
    data. Would diffs mean changes in the content of any file in your data repository,
    only when a file is removed or added, or when the checksum of the whole repository
    has changed?
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个问题是，关于数据版本化到底包含什么内容仍然存在混淆。当我们对数据版本进行处理时，diff是否意味着在数据仓库中的任何文件内容发生了更改，只有在添加或删除文件时，还是只有整个仓库的校验和发生了更改时？
- en: As of 2021, data versioning tools like DVC only register a diff if the checksum
    of the total directory has changed and if a file is removed or added.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 截至2021年，像DVC这样的数据版本工具只有在整个目录的校验和发生了更改，或者文件被添加或删除时才会注册diff。
- en: 'Another confusion is in how to resolve merge conflicts: if developer 1 uses
    data version X to train model A and developer 2 uses data version Y to train model
    B, it doesn’t make sense to merge data versions X and Y to create Z, since there’s
    no model corresponding with Z.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个困惑是如何解决合并冲突：如果开发者1使用数据版本X来训练模型A，而开发者2使用数据版本Y来训练模型B，将数据版本X和Y合并成Z是没有意义的，因为没有与Z对应的模型。
- en: Third, if you use user data to train your model, regulations like General Data
    Protection Regulation (GDPR) might make versioning this data complicated. For
    example, regulations might mandate that you delete user data if requested, making
    it legally impossible to recover older versions of your data.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果您使用用户数据来训练模型，像《通用数据保护条例》（GDPR）这样的法规可能会使版本控制变得复杂化。例如，法规可能要求您在被请求时删除用户数据，这使得恢复旧版本数据在法律上变得不可能。
- en: Aggressive experiment tracking and versioning helps with reproducibility, but
    it doesn’t ensure reproducibility. The frameworks and hardware you use might introduce
    nondeterminism to your experiment results,^([10](ch06.xhtml#ch01fn159)) making
    it impossible to replicate the result of an experiment without knowing everything
    about the environment your experiment runs in.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 激进的实验跟踪和版本控制有助于实现可复现性，但并不保证完全可复现。您使用的框架和硬件可能会给您的实验结果引入不确定性^([10](ch06.xhtml#ch01fn159))，导致在不了解实验运行环境的情况下无法复制实验结果。
- en: The way we have to run so many experiments right now to find the best possible
    model is the result of us treating ML as a black box. Because we can’t predict
    which configuration will work best, we have to experiment with multiple configurations.
    However, I hope that as the field progresses, we’ll gain more understanding into
    different models and can reason about what model will work best instead of running
    hundreds or thousands of experiments.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 目前我们必须运行如此多的实验来找到最佳模型的原因是我们把机器学习视为一个黑盒子。因为我们无法预测哪种配置会最有效，所以我们必须尝试多种配置。然而，我希望随着领域的发展，我们能够更深入地理解不同的模型，并能够推理出哪种模型将最有效，而不是运行数百或数千次实验。
- en: Distributed Training
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式训练
- en: As models are getting bigger and more resource-intensive, companies care a lot
    more about training at scale.^([11](ch06.xhtml#ch01fn160)) Expertise in scalability
    is hard to acquire because it requires having regular access to massive compute
    resources. Scalability is a topic that merits a series of books. This section
    covers some notable issues to highlight the challenges of doing ML at scale and
    provide a scaffold to help you plan the resources for your project accordingly.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型变得更大、更加资源密集，公司对规模化训练越来越重视^([11](ch06.xhtml#ch01fn160))。在可扩展性方面的专业知识难以获取，因为它需要定期接触大量计算资源。可扩展性是一个值得一系列书籍探讨的主题。本节旨在强调大规模机器学习的挑战，并提供一个支架，帮助您合理规划项目资源。
- en: It’s common to train a model using data that doesn’t fit into memory. It’s especially
    common when dealing with medical data such as CT scans or genome sequences. It
    can also happen with text data if you work for teams that train large language
    models (cue OpenAI, Google, NVIDIA, Cohere).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型时使用超出内存容量的数据是很常见的。特别是处理医疗数据（如CT扫描或基因组序列）时尤其常见。如果您在团队中为大型语言模型（如OpenAI、Google、NVIDIA、Cohere）工作，这也可能发生在文本数据中。
- en: When your data doesn’t fit into memory, your algorithms for preprocessing (e.g.,
    zero-centering, normalizing, whitening), shuffling, and batching data will need
    to run out of core and in parallel.^([12](ch06.xhtml#ch01fn161)) When a sample
    of your data is large, e.g., one machine can handle a few samples at a time, you
    might only be able to work with a small batch size, which leads to instability
    for gradient descent-based optimization.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当您的数据无法全部加载到内存中时，预处理算法（如零中心化、归一化、白化）、数据打乱和批处理将需要在核心外和并行运行^([12](ch06.xhtml#ch01fn161))。当数据样本较大时，例如一台机器一次只能处理少量样本，您可能只能使用较小的批量大小，这会导致基于梯度下降的优化不稳定。
- en: In some cases, a data sample is so large it can’t even fit into memory and you
    will have to use something like gradient checkpointing, a technique that leverages
    the memory footprint and compute trade-off to make your system do more computation
    with less memory. According to the authors of the open source package gradient-checkpointing,
    “For feed-forward models we were able to fit more than 10x larger models onto
    our GPU, at only a 20% increase in computation time.”^([13](ch06.xhtml#ch01fn162))
    Even when a sample fits into memory, using checkpointing can allow you to fit
    more samples into a batch, which might allow you to train your model faster.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，数据样本非常大，甚至无法完全载入内存，这时你需要使用类似梯度检查点的技术，这种技术利用内存占用和计算效率之间的平衡来使系统在更少的内存中执行更多的计算。根据开源软件包梯度检查点的作者说：“对于前向传播模型，我们能够将比之前大10倍以上的模型装载到我们的GPU上，只需增加20%的计算时间。”^([13](ch06.xhtml#ch01fn162))
    即使一个样本能够装入内存，使用检查点技术也可以让你将更多的样本放入一个批次中，这可能使你能够更快地训练你的模型。
- en: Data parallelism
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据并行化
- en: 'It’s now the norm to train ML models on multiple machines. The most common
    parallelization method supported by modern ML frameworks is data parallelism:
    you split your data on multiple machines, train your model on all of them, and
    accumulate gradients. This gives rise to a couple of issues.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用多台机器来训练机器学习模型已经成为常态。现代机器学习框架支持的最常见并行化方法是数据并行：将数据分割在多台机器上，训练所有机器上的模型，并累积梯度。这导致了一些问题的出现。
- en: A challenging problem is how to accurately and effectively accumulate gradients
    from different machines. As each machine produces its own gradient, if your model
    waits for all of them to finish a run—synchronous stochastic gradient descent
    (SGD)—stragglers will cause the entire system to slow down, wasting time and resources.^([14](ch06.xhtml#ch01fn163))
    The straggler problem grows with the number of machines, as the more workers,
    the more likely that at least one worker will run unusually slowly in a given
    iteration. However, there have been many algorithms that effectively address this
    problem.^([15](ch06.xhtml#ch01fn164))
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有挑战性的问题是如何准确有效地累积来自不同机器的梯度。由于每台机器产生自己的梯度，如果你的模型等待它们全部完成一次运行——同步随机梯度下降（SSGD），落后者会导致整个系统减速，浪费时间和资源。^([14](ch06.xhtml#ch01fn163))
    落后者问题随着机器数量的增加而增长，因为工作机器越多，在给定迭代中至少有一台机器运行异常缓慢的可能性就越大。然而，已经有许多算法有效地解决了这个问题。^([15](ch06.xhtml#ch01fn164))
- en: If your model updates the weight using the gradient from each machine separately—asynchronous
    SGD—gradient staleness might become a problem because the gradients from one machine
    have caused the weights to change before the gradients from another machine have
    come in.^([16](ch06.xhtml#ch01fn165))
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的模型使用每台机器单独计算的梯度来更新权重——异步随机梯度下降（ASGD），梯度陈旧可能会成为一个问题，因为一台机器的梯度导致权重在另一台机器的梯度到来之前已经改变。^([16](ch06.xhtml#ch01fn165))
- en: The difference between synchronous SGD and asynchronous SGD is illustrated in
    [Figure 6-6](#synchronous_sgd_versus_asynchronous_sgd).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 同步 SGD 和异步 SGD 的区别如图 [6-6](#synchronous_sgd_versus_asynchronous_sgd) 所示。
- en: '![](Images/dmls_0606.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_0606.png)'
- en: 'Figure 6-6\. Synchronous SGD versus asynchronous SGD for data parallelism.
    Source: Adapted from an image by Jim Dowling^([17](ch06.xhtml#ch01fn166))'
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-6\. 同步 SGD 和异步 SGD 在数据并行化中的比较。来源：Jim Dowling 的图片改编^([17](ch06.xhtml#ch01fn166))
- en: In theory, asynchronous SGD converges but requires more steps than synchronous
    SGD. However, in practice, when the number of weights is large, gradient updates
    tend to be sparse, meaning most gradient updates only modify small fractions of
    the parameters, and it’s less likely that two gradient updates from different
    machines will modify the same weights. When gradient updates are sparse, gradient
    staleness becomes less of a problem and the model converges similarly for both
    synchronous and asynchronous SGD.^([18](ch06.xhtml#ch01fn167))
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，异步 SGD 可以收敛，但需要比同步 SGD 更多的步骤。然而，在实践中，当权重数量很大时，梯度更新往往是稀疏的，这意味着大多数梯度更新只修改参数的小部分，不太可能两台不同机器的梯度更新同时修改同一组权重。当梯度更新稀疏时，梯度陈旧问题就不再是一个大问题，模型在同步和异步
    SGD 下的收敛情况相似。^([18](ch06.xhtml#ch01fn167))
- en: Another problem is that spreading your model on multiple machines can cause
    your batch size to be very big. If a machine processes a batch size of 1,000,
    then 1,000 machines process a batch size of 1M (OpenAI’s GPT-3 175B uses a batch
    size of 3.2M in 2020).^([19](ch06.xhtml#ch01fn168)) To oversimplify the calculation,
    if training an epoch on a machine takes 1M steps, training on 1,000 machines might
    take only 1,000 steps. An intuitive approach is to scale up the learning rate
    to account for more learning at each step, but we also can’t make the learning
    rate too big as it will lead to unstable convergence. In practice, increasing
    the batch size past a certain point yields diminishing returns.^([20](ch06.xhtml#ch01fn169))
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是将模型分布在多台机器上可能会导致批量大小非常大。如果一台机器处理 1,000 的批次大小，那么 1,000 台机器将处理 1M（例如，OpenAI
    的 GPT-3 在 2020 年使用的批量大小为 3.2M）^([19](ch06.xhtml#ch01fn168))。简化计算，如果在一台机器上训练一个
    epoch 需要 1M 步，那么在 1,000 台机器上训练可能只需要 1,000 步。一个直观的方法是增加学习率，以便在每一步骤中进行更多学习，但我们也不能把学习率设置得太大，因为这会导致不稳定的收敛。实际上，将批量大小增加到一定点后会产生收益递减的效果^([20](ch06.xhtml#ch01fn169))。
- en: Last but not least, with the same model setup, the main worker sometimes uses
    a lot more resources than other workers. If that’s the case, to make the most
    use out of all machines, you need to figure out a way to balance out the workload
    among them. The easiest way, but not the most effective way, is to use a smaller
    batch size on the main worker and a larger batch size on other workers.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但并非最不重要的是，对于相同的模型设置，主要工作节点有时可能会使用比其他工作节点更多的资源。如果是这种情况，为了充分利用所有机器的资源，您需要找到一种方法来平衡它们之间的工作负载。最简单但不是最有效的方法是在主要工作节点上使用较小的批量大小，在其他工作节点上使用较大的批量大小。
- en: Model parallelism
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型并行
- en: With data parallelism, each worker has its own copy of the whole model and does
    all the computation necessary for its copy of the model. Model parallelism is
    when different components of your model are trained on different machines, as
    shown in [Figure 6-7](#data_parallelism_and_model_parallelismd). For example,
    machine 0 handles the computation for the first two layers while machine 1 handles
    the next two layers, or some machines can handle the forward pass while several
    others handle the backward pass.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据并行中，每个工作节点都有整个模型的副本，并且执行其模型副本所需的所有计算。模型并行是指在不同的机器上训练模型的不同组件，如图 [6-7](#data_parallelism_and_model_parallelismd)
    所示。例如，机器 0 处理前两层的计算，而机器 1 处理接下来的两层，或者某些机器处理前向传播，而其他机器处理反向传播。
- en: '![](Images/dmls_0607.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_0607.png)'
- en: 'Figure 6-7\. Data parallelism and model parallelism. Source: Adapted from an
    image by Jure Leskovec^([21](ch06.xhtml#ch01fn170))'
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-7\. 数据并行和模型并行。来源：改编自 Jure Leskovec 的一幅图^([21](ch06.xhtml#ch01fn170))
- en: Model parallelism can be misleading because in some cases parallelism doesn’t
    mean that different parts of the model in different machines are executed in parallel.
    For example, if your model is a massive matrix and the matrix is split into two
    halves on two machines, then these two halves might be executed in parallel. However,
    if your model is a neural network and you put the first layer on machine 1 and
    the second layer on machine 2, and layer 2 needs outputs from layer 1 to execute,
    then machine 2 has to wait for machine 1 to finish first to run.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 模型并行有时可能会误导，因为在某些情况下，并行不意味着在不同机器上执行模型的不同部分。例如，如果您的模型是一个庞大的矩阵，并且将该矩阵分成两半在两台机器上，那么这两半可能会并行执行。但是，如果您的模型是一个神经网络，并且将第一层放在机器
    1 上，第二层放在机器 2 上，而第二层需要来自第一层的输出才能执行，则机器 2 必须等待机器 1 先完成。
- en: '*Pipeline parallelism* is a clever technique to make different components of
    a model on different machines run more in parallel. There are multiple variants
    to this, but the key idea is to break the computation of each machine into multiple
    parts. When machine 1 finishes the first part of its computation, it passes the
    result onto machine 2, then continues to the second part, and so on. Machine 2
    now can execute its computation on the first part while machine 1 executes its
    computation on the second part.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '*管道并行* 是一种巧妙的技术，可以使模型的不同组件在不同的机器上更多地并行运行。有多种变体，但其关键思想是将每台机器的计算分成多个部分。当机器 1
    完成其计算的第一部分时，它将结果传递给机器 2，然后继续进行第二部分的计算，依此类推。现在，机器 2 可以在第一部分上执行其计算，而机器 1 则在第二部分上执行其计算。'
- en: To make this concrete, consider you have four different machines and the first,
    second, third, and fourth layers are on machine 1, 2, 3, and 4 respectively. With
    pipeline parallelism, each mini-batch is broken into four micro-batches. Machine
    1 computes the first layer on the first micro-batch, then machine 2 computes the
    second layer on machine 1’s results while machine 1 computes the first layer on
    the second micro-batch, and so on. [Figure 6-8](#pipeline_parallelism_for_a_neural_netwo)
    shows what pipeline parallelism looks like on four machines; each machine runs
    both the forward pass and the backward pass for one component of a neural network.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更加具体化，假设你有四台不同的机器，第一、第二、第三和第四层分别在机器1、2、3和4上。使用管道并行，每个小批次被分成四个微批次。机器1在第一个微批次上计算第一层，然后机器2在机器1的结果上计算第二层，同时机器1在第二个微批次上计算第一层，依此类推。图
    [Figure 6-8](#pipeline_parallelism_for_a_neural_netwo) 显示了在四台机器上使用管道并行的神经网络的情况。
- en: '![](Images/dmls_0608.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_0608.png)'
- en: 'Figure 6-8\. Pipeline parallelism for a neural network on four machines; each
    machine runs both the forward pass (F) and the backward pass (B) for one component
    of the neural network. Source: Adapted from an image by Huang et al.^([22](ch06.xhtml#ch01fn171))'
  id: totrans-159
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 6-8\. 神经网络在四台机器上的管道并行性；每台机器同时运行前向传递（F）和反向传递（B）来处理神经网络的一个组件。来源：根据黄等人的图片改编^([22](ch06.xhtml#ch01fn171))
- en: Model parallelism and data parallelism aren’t mutually exclusive. Many companies
    use both methods for better utilization of their hardware, even though the setup
    to use both methods can require significant engineering effort.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 模型并行和数据并行并不是互斥的。许多公司同时使用这两种方法以更好地利用他们的硬件，尽管配置同时使用这两种方法可能需要大量的工程努力。
- en: AutoML
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AutoML
- en: There’s a joke that a good ML researcher is someone who will automate themselves
    out of job, designing an AI algorithm intelligent enough to design itself. It
    was funny until the TensorFlow Dev Summit 2018, where Jeff Dean took the stage
    and declared that Google intended on replacing ML expertise with 100 times more
    computational power, introducing AutoML to the excitement and horror of the community.
    Instead of paying a group of 100 ML researchers/engineers to fiddle with various
    models and eventually select a suboptimal one, why not use that money on compute
    to search for the optimal model? A screenshot from the recording of the event
    is shown in [Figure 6-9](#jeff_dean_unveiling_googleapostrophes_a).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个笑话说好的机器学习研究员是那些能够设计出足够智能的 AI 算法来自动化自己工作的人。这在 TensorFlow Dev Summit 2018 中变得有趣，当时
    Jeff Dean 上台宣布谷歌打算用100倍的计算能力来替代机器学习专家，向社区引入了 AutoML，令人兴奋和震惊。与其支付一组100名机器学习研究员/工程师来玩弄各种模型，最终选择一个次优模型，为什么不用这笔钱投入计算资源来搜索最优模型呢？活动录像中的截图显示了这一事件
    [Figure 6-9](#jeff_dean_unveiling_googleapostrophes_a)。
- en: '![](Images/dmls_0609.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_0609.png)'
- en: Figure 6-9\. Jeff Dean unveiling Google’s AutoML at TensorFlow Dev Summit 2018
  id: totrans-164
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 6-9\. Jeff Dean 在 TensorFlow Dev Summit 2018 上揭示谷歌的 AutoML
- en: 'Soft AutoML: Hyperparameter tuning'
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 软 AutoML：超参数调整
- en: AutoML refers to automating the process of finding ML algorithms to solve real-world
    problems. One mild form, and the most popular form, of AutoML in production is
    hyperparameter tuning. A hyperparameter is a parameter supplied by users whose
    value is used to control the learning process, e.g., learning rate, batch size,
    number of hidden layers, number of hidden units, dropout probability, *β*[1] and
    *β*[2] in Adam optimizer, etc. Even quantization—e.g., whether to use 32 bits,
    16 bits, or 8 bits to represent a number or a mixture of these representations—can
    be considered a hyperparameter to tune.^([23](ch06.xhtml#ch01fn172))
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: AutoML 是指自动化寻找解决实际问题的机器学习算法的过程。在生产中，一种温和的形式，也是最流行的形式，是超参数调整。超参数是由用户提供的参数，其值用于控制学习过程，例如学习率、批量大小、隐藏层数量、隐藏单元数量、dropout
    概率，Adam 优化器中的 *β*[1] 和 *β*[2] 等等。甚至量化——例如使用32位、16位或8位来表示数字或这些表示的混合——也可以被视为需要调整的超参数^([23](ch06.xhtml#ch01fn172))。
- en: With different sets of hyperparameters, the same model can give drastically
    different performances on the same dataset. Melis et al. showed in their 2018
    paper [“On the State of the Art of Evaluation in Neural Language Models”](https://oreil.ly/AY2lF)
    that weaker models with well-tuned hyperparameters can outperform stronger, fancier
    models. The goal of hyperparameter tuning is to find the optimal set of hyperparameters
    for a given model within a search space—the performance of each set evaluated
    on a validation set.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不同的超参数集合，同一模型在同一数据集上的性能可能会大相径庭。Melis 等人在他们2018年的论文 [“On the State of the Art
    of Evaluation in Neural Language Models”](https://oreil.ly/AY2lF) 中表明，经过良好调优的弱模型可能会胜过更强大、更花哨的模型。超参数调优的目标是在一个搜索空间内找到给定模型的最优超参数集合，每个集合的性能都在验证集上评估。
- en: Despite knowing its importance, many still ignore systematic approaches to hyperparameter
    tuning in favor of a manual, gut-feeling approach. The most popular is arguably
    graduate student descent (GSD), a technique in which a graduate student fiddles
    around with the hyperparameters until the model works.^([24](ch06.xhtml#ch01fn173))
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管许多人知道超参数调优的重要性，仍然有很多人忽视系统化的方法，而更倾向于凭感觉进行手动调整。其中最流行的可能是研究生下降法（GSD），这是一种技术，研究生会调整超参数直到模型正常工作。^([24](ch06.xhtml#ch01fn173))
- en: 'However, more and more people are adopting hyperparameter tuning as part of
    their standard pipelines. Popular ML frameworks either come with built-in utilities
    or have third-party utilities for hyperparameter tuning—for example, scikit-learn
    with auto-sklearn,^([25](ch06.xhtml#ch01fn174)) TensorFlow with Keras Tuner, and
    Ray with [Tune](https://oreil.ly/uulrC). Popular methods for hyperparameter tuning
    include random search,^([26](ch06.xhtml#ch01fn175)) grid search, and Bayesian
    optimization.^([27](ch06.xhtml#ch01fn176)) The book *AutoML: Methods, Systems,
    Challenges* by the AutoML group at the University of Freiburg dedicates its [first
    chapter](https://oreil.ly/LfqJm) (which you can read online for free) to hyperparameter
    optimization.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，越来越多的人将超参数调优作为标准流程的一部分。流行的机器学习框架通常都带有内置工具或者有第三方工具来进行超参数调优，例如，scikit-learn
    自带的 auto-sklearn^([25](ch06.xhtml#ch01fn174))，TensorFlow 的 Keras Tuner，以及 Ray
    的 [Tune](https://oreil.ly/uulrC)。流行的超参数调优方法包括随机搜索^([26](ch06.xhtml#ch01fn175))，网格搜索，以及贝叶斯优化^([27](ch06.xhtml#ch01fn176))。《AutoML:
    Methods, Systems, Challenges》一书由弗莱堡大学的 AutoML 小组编写，将其 [第一章](https://oreil.ly/LfqJm)（可以免费在线阅读）专注于超参数优化。'
- en: When tuning hyperparameters, keep in mind that a model’s performance might be
    more sensitive to the change in one hyperparameter than another, and therefore
    sensitive hyperparameters should be more carefully tuned.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在调整超参数时，请记住模型的性能可能对某些超参数的更改更为敏感，因此对敏感超参数应更加小心地调整。
- en: Warning
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: It’s crucial to never use your test split to tune hyperparameters. Choose the
    best set of hyperparameters for a model based on its performance on a validation
    split, then report the model’s final performance on the test split. If you use
    your test split to tune hyperparameters, you risk overfitting your model to the
    test split.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对不要使用测试集来调整超参数。选择基于验证集性能的最佳超参数集合，然后报告模型在测试集上的最终性能。如果使用测试集来调整超参数，会导致模型对测试集过拟合。
- en: 'Hard AutoML: Architecture search and learned optimizer'
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Hard AutoML：架构搜索和学习优化器
- en: 'Some teams take hyperparameter tuning to the next level: what if we treat other
    components of a model or the entire model as hyperparameters. The size of a convolution
    layer or whether or not to have a skip layer can be considered a hyperparameter.
    Instead of manually putting a pooling layer after a convolutional layer or ReLu
    (rectified linear unit) after linear, you give your algorithm these building blocks
    and let it figure out how to combine them. This area of research is known as architectural
    search, or neural architecture search (NAS) for neural networks, as it searches
    for the optimal model architecture.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一些团队将超参数调优推向了新的高度：如果我们将模型的其他组件或整个模型视为超参数呢？卷积层的大小或是否具有跳跃层都可以视为超参数。与其在卷积层之后手动放置池化层或在线性层后放置
    ReLu（修正线性单元），不如给算法提供这些构建块，让算法自行决定如何组合它们。这一研究领域被称为架构搜索，或神经架构搜索（NAS）用于神经网络，因为它寻找最优的模型架构。
- en: 'A NAS setup consists of three components:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: NAS 设置由三个组件组成：
- en: A search space
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索空间
- en: Defines possible model architectures—i.e., building blocks to choose from and
    constraints on how they can be combined.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 定义可能的模型架构 —— 即可以选择的构建模块以及它们组合方式的约束条件。
- en: A performance estimation strategy
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 性能估计策略
- en: To evaluate the performance of a candidate architecture without having to train
    each candidate architecture from scratch until convergence. When we have a large
    number of candidate architectures, say 1,000, training all of them until convergence
    can be costly.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 评估候选架构的性能，而无需对每个候选架构从头开始训练直至收敛。当我们有大量候选架构时，比如 1,000 个，训练所有候选架构直至收敛可能成本高昂。
- en: A search strategy
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索策略
- en: To explore the search space. A simple approach is random search—randomly choosing
    from all possible configurations—which is unpopular because it’s prohibitively
    expensive even for NAS. Common approaches include reinforcement learning (rewarding
    the choices that improve the performance estimation) and evolution (adding mutations
    to an architecture, choosing the best-performing ones, adding mutations to them,
    and so on).^([28](ch06.xhtml#ch01fn177))
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 探索搜索空间的方法。一种简单的方法是随机搜索 —— 从所有可能的配置中随机选择 —— 这在NAS中即使不受欢迎也是 prohibitively expensive。常见的方法包括强化学习（奖励改进性能估计的选择）和进化算法（对架构添加突变，选择表现最佳的，对它们再添加突变，依此类推）^([28](ch06.xhtml#ch01fn177))。
- en: For NAS, the search space is discrete—the final architecture uses only one of
    the available options for each layer/operation,^([29](ch06.xhtml#ch01fn178)) and
    you have to provide the set of building blocks. The common building blocks are
    various convolutions of different sizes, linear, various activations, pooling,
    identity, zero, etc. The set of building blocks varies based on the base architecture,
    e.g., convolutional neural networks or transformers.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 对于NAS，搜索空间是离散的 —— 每一层/操作最终的架构仅使用可用选项中的一个^([29](ch06.xhtml#ch01fn178))，你需要提供构建模块的集合。常见的构建模块包括不同尺寸的各种卷积、线性操作、各种激活函数、池化、恒等映射、零操作等等。构建模块的集合根据基础架构而异，例如卷积神经网络或者变换器。
- en: In a typical ML training process, you have a model and then a learning procedure,
    an algorithm that helps your model find the set of parameters that minimize a
    given objective function for a given set of data. The most common learning procedure
    for neural networks today is gradient descent, which leverages an optimizer to
    specify how to update a model’s weights given gradient updates.^([30](ch06.xhtml#ch01fn179))
    Popular optimizers are, as you probably already know, Adam, Momentum, SGD, etc.
    In theory, you can include optimizers as building blocks in NAS and search for
    one that works best. In practice, this is difficult to do, since optimizers are
    sensitive to the setting of their hyperparameters, and the default hyperparameters
    don’t often work well across architectures.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的机器学习训练过程中，你有一个模型，然后有一个学习过程 —— 一种算法，帮助你的模型找到最小化给定数据集上给定目标函数的参数集。如今神经网络最常见的学习过程是梯度下降，它利用优化器指定如何根据梯度更新模型的权重^([30](ch06.xhtml#ch01fn179))。流行的优化器包括，如你可能已知，Adam、Momentum、SGD
    等等。理论上，你可以将优化器作为NAS的构建模块并寻找最适合的优化器。实际上，这很难做到，因为优化器对其超参数的设置很敏感，并且默认的超参数通常在不同架构间表现不佳。
- en: 'This leads to an exciting research direction: what if we replace the functions
    that specify the update rule with a neural network? How much to update the model’s
    weights will be calculated by this neural network. This approach results in learned
    optimizers, as opposed to hand-designed optimizers.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了一个激动人心的研究方向：如果我们用神经网络替换指定更新规则的函数会怎样？模型权重的更新量可以通过这个神经网络计算出来。这种方法产生了学习优化器，而不是手工设计的优化器。
- en: Since learned optimizers are neural networks, they need to be trained. You can
    train your learned optimizer on the same dataset you’re training the rest of your
    neural network on, but this requires you to train an optimizer every time you
    have a task.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 由于学习优化器是神经网络，它们需要训练。你可以在训练神经网络的同时在相同数据集上训练学习优化器，但这需要每次有任务时都训练一个优化器。
- en: Another approach is to train a learned optimizer once on a set of existing tasks—using
    aggregated loss on those tasks as the loss function and existing designed optimizers
    as the learning rule—and use it for every new task after that. For example, Metz
    et al. constructed a set of thousands of tasks to train learned optimizers. Their
    learned optimizer was able to generalize to both new datasets and domains as well
    as new architectures.^([31](ch06.xhtml#ch01fn180)) And the beauty of this approach
    is that the learned optimizer can then be used to train a better-learned optimizer,
    an algorithm that improves on itself.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是在一组现有任务上训练一个学习优化器——使用这些任务的聚合损失作为损失函数和现有设计的优化器作为学习规则——然后在每个新任务中使用它。例如，Metz等人构建了一组数千个任务来训练学习优化器。他们的学习优化器能够推广到新数据集、领域以及新架构。^([31](ch06.xhtml#ch01fn180))
    这种方法的美妙之处在于，学习优化器随后可以用来训练一个更好的学习优化器，即改进自身的算法。
- en: Whether it’s architecture search or meta-learning learning rules, the up-front
    training cost is expensive enough that only a handful of companies in the world
    can afford to pursue them. However, it’s important for people interested in ML
    in production to be aware of the progress in AutoML for two reasons. First, the
    resulting architectures and learned optimizers can allow ML algorithms to work
    off-the-shelf on multiple real-world tasks, saving production time and cost, during
    both training and inferencing. For example, EfficientNets, a family of models
    produced by Google’s AutoML team, surpass state-of-the-art accuracy with up to
    10x better efficiency.^([32](ch06.xhtml#ch01fn181)) Second, they might be able
    to solve many real-world tasks previously impossible with existing architectures
    and optimizers.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是架构搜索还是元学习学习规则，前期培训成本都足够昂贵，以至于全球只有少数几家公司能够承担这些成本。然而，对于有兴趣将ML应用于生产中的人们来说，了解AutoML的进展是很重要的，原因有两点。首先，产生的架构和学习优化器可以让ML算法即插即用地应用于多个实际任务，节省生产中的时间和成本，包括培训和推断。例如，由Google的AutoML团队生成的EfficientNets模型系列，在效率上超越了现有技术的精度，高达10倍。^([32](ch06.xhtml#ch01fn181))
    其次，它们可能能够解决许多以前依靠现有架构和优化器无法解决的实际任务。
- en: Model Offline Evaluation
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型离线评估
- en: 'One common but quite difficult question I often encounter when helping companies
    with their ML strategies is: “How do I know that our ML models are any good?”
    In one case, a company deployed ML to detect intrusions to 100 surveillance drones,
    but they had no way of measuring how many intrusions their system failed to detect,
    and they couldn’t decide if one ML algorithm was better than another for their
    needs.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我在帮助公司制定ML策略时经常遇到的一个常见但相当困难的问题是：“我怎么知道我们的ML模型是否有效？” 例如，某公司部署了ML来检测100架监视无人机的入侵，但他们无法衡量系统未能检测到多少次入侵，也无法确定哪种ML算法更适合他们的需求。
- en: Lacking a clear understanding of how to evaluate your ML systems is not necessarily
    a reason for your ML project to fail, but it might make it impossible to find
    the best solution for your need, and make it harder to convince your managers
    to adopt ML. You might want to partner with the business team to develop metrics
    for model evaluation that are more relevant to your company’s business.^([37](ch06.xhtml#ch01fn186))
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 缺乏对如何评估您的ML系统进行清晰理解并不一定会导致您的ML项目失败，但可能会使您无法找到最适合您需求的最佳解决方案，并使说服管理层采纳ML变得更加困难。您可能需要与业务团队合作，开发更符合公司业务的模型评估指标。^([37](ch06.xhtml#ch01fn186))
- en: Ideally, the evaluation methods should be the same during both development and
    production. But in many cases, the ideal is impossible because during development,
    you have ground truth labels, but in production, you don’t.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，开发和生产中的评估方法应该是一致的。但在许多情况下，这种理想是不可能的，因为在开发过程中，您有地面实况标签，但在生产中则没有。
- en: For certain tasks, it’s possible to infer or approximate labels in production
    based on users’ feedback, as covered in the section [“Natural Labels”](ch04.xhtml#natural_labels).
    For example, for the recommendation task, it’s possible to infer if a recommendation
    is good by whether users click on it. However, there are many biases associated
    with this.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些任务，可以根据用户反馈推断或近似标签，如在“自然标签”一节中所述。例如，对于推荐任务，可以通过用户是否点击来推断推荐是否有效。但是，这涉及许多偏见。
- en: For other tasks, you might not be able to evaluate your model’s performance
    in production directly and might have to rely on extensive monitoring to detect
    changes and failures in your ML system’s performance. We’ll cover monitoring in
    [Chapter 8](ch08.xhtml#data_distribution_shifts_and_monitoring).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他任务，您可能无法直接评估您模型在生产中的表现，并可能需要依赖广泛的监控来检测ML系统性能的变化和失败。我们将在[第8章](ch08.xhtml#data_distribution_shifts_and_monitoring)中讨论监控。
- en: Once your model is deployed, you’ll need to continue monitoring and testing
    your model in production. In this section, we’ll discuss methods to evaluate your
    model’s performance before it’s deployed. We’ll start with the baselines against
    which we will evaluate our models. Then we’ll cover some of the common methods
    to evaluate your model beyond overall accuracy metrics.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的模型部署，您将需要继续监控和测试您模型在生产中的表现。在本节中，我们将讨论在部署之前评估模型性能的方法。我们将从评估我们模型的基线开始。然后我们将介绍一些常见的方法来评估您模型的整体准确性之外的性能。
- en: Baselines
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基线
- en: Someone once told me that her new generative model achieved the FID score of
    10.3 on ImageNet.^([38](ch06.xhtml#ch01fn187)) I had no idea what this number
    meant or whether her model would be useful for my problem.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 曾经有人告诉我，她的新生成模型在ImageNet上达到了10.3的FID分数。^([38](ch06.xhtml#ch01fn187)) 我不知道这个数字意味着什么，或者她的模型对我的问题是否有用。
- en: Another time, I helped a company implement a classification model where the
    positive class appears 90% of the time. An ML engineer on the team told me, all
    excited, that their initial model achieved an F1 score of 0.90\. I asked him how
    it was compared to random. He had no idea. It turned out that because for his
    task the POSITIVE class accounts for 90% of the labels, if his model randomly
    outputs the positive class 90% of the time, its F1 score would also be around
    0.90.^([39](ch06.xhtml#ch01fn188)) His model might as well be making predictions
    at random.^([40](ch06.xhtml#ch01fn189))
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 另一次，我帮助一家公司实现了一个分类模型，其中正类在90%的时间内出现。团队中的一个ML工程师兴奋地告诉我，他们的初始模型实现了0.90的F1分数。我问他这与随机相比如何。他毫无头绪。事实证明，因为对于他的任务，正类占标签的90%，如果他的模型随机输出正类的概率为90%，其F1分数也会约为0.90。^([39](ch06.xhtml#ch01fn188))
    他的模型可能与随机预测无异。^([40](ch06.xhtml#ch01fn189))
- en: 'Evaluation metrics, by themselves, mean little. When evaluating your model,
    it’s essential to know the baseline you’re evaluating it against. The exact baselines
    should vary from one use case to another, but here are the five baselines that
    might be useful across use cases:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标本身意义不大。在评估您的模型时，了解您正在评估的基线至关重要。确切的基线应因用例而异，但以下五个基线可能对各种用例有所帮助：
- en: Random baseline
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 随机基线
- en: If our model just predicts at random, what’s the expected performance? The predictions
    are generated at random following a specific distribution, which can be the uniform
    distribution or the task’s label distribution.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的模型只是随机预测，预期性能如何？预测是根据特定分布随机生成的，可以是均匀分布或任务的标签分布。
- en: For example, consider the task that has two labels, NEGATIVE that appears 90%
    of the time and POSITIVE that appears 10% of the time. [Table 6-2](#fone_and_accuracy_scores_of_a_baseline)
    shows the F1 and accuracy scores of baseline models making predictions at random.
    However, as an exercise to see how challenging it is for most people to have an
    intuition for these values, try to calculate these raw numbers in your head before
    looking at the table.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个具有两个标签的任务，其中NEGATIVE出现的时间为90%，而POSITIVE出现的时间为10%。[表6-2](#fone_and_accuracy_scores_of_a_baseline)显示了随机预测基准模型的F1和准确度分数。然而，作为一种练习，看看大多数人对这些值缺乏直觉，尝试在查看表格之前在头脑中计算这些原始数字。
- en: Table 6-2\. F1 and accuracy scores of a baseline model predicting at random
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 表6-2\. 预测随机的基准模型的F1和准确度分数
- en: '| Random distribution | Meaning | F1 | Accuracy |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 随机分布 | 含义 | F1 | 准确度 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Uniform random | Predicting each label with equal probability (50%) | 0.167
    | 0.5 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 均匀随机 | 预测每个标签的概率相等（50%） | 0.167 | 0.5 |'
- en: '| Task’s label distribution | Predicting NEGATIVE 90% of the time, and POSITIVE
    10% of the time | 0.1 | 0.82 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 任务标签分布 | 90% 的时间预测为NEGATIVE，10% 的时间预测为POSITIVE | 0.1 | 0.82 |'
- en: Simple heuristic
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 简单启发式
- en: Forget ML. If you just make predictions based on simple heuristics, what performance
    would you expect? For example, if you want to build a ranking system to rank items
    on a user’s newsfeed with the goal of getting that user to spend more time on
    the newsfeed, how much time would a user spend if you just rank all the items
    in reverse chronological order, showing the latest one first?
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 忘记机器学习。如果你只是基于简单的启发式进行预测，你会期望什么样的表现？例如，如果你想建立一个排名系统，以排列用户新闻提要中的项目，目标是让用户在新闻提要上花更多时间，如果你只是按时间顺序逆序排列所有项目，先显示最新的项目，用户会花费多少时间？
- en: Zero rule baseline
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 零规则基线
- en: The zero rule baseline is a special case of the simple heuristic baseline when
    your baseline model always predicts the most common class.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 零规则基线是简单启发式基线的特殊情况，当你的基准模型总是预测最常见的类时。
- en: For example, for the task of recommending the app a user is most likely to use
    next on their phone, the simplest model would be to recommend their most frequently
    used app. If this simple heuristic can predict the next app accurately 70% of
    the time, any model you build has to outperform it significantly to justify the
    added complexity.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于推荐用户下一个手机应用的任务，最简单的模型可能是推荐他们最常用的应用。如果这个简单的启发式能够准确预测下一个应用达到70%的时间，任何你建立的模型必须显著优于它，以证明增加的复杂性是合理的。
- en: Human baseline
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 人类基线
- en: In many cases, the goal of ML is to automate what would have been otherwise
    done by humans, so it’s useful to know how your model performs compared to human
    experts. For example, if you work on a self-driving system, it’s crucial to measure
    your system’s progress compared to human drivers, because otherwise you might
    never be able to convince your users to trust this system. Even if your system
    isn’t meant to replace human experts and only to aid them in improving their productivity,
    it’s still important to know in what scenarios this system would be useful to
    humans.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，机器学习的目标是自动化人类可能会完成的任务，因此了解你的模型与人类专家相比的表现是非常有用的。例如，如果你在一个自动驾驶系统上工作，比较你的系统与人类驾驶员的进展至关重要，因为否则你可能永远无法说服用户信任这个系统。即使你的系统并不意味着取代人类专家，只是帮助他们提高生产力，了解在什么情况下这个系统对人类有用仍然很重要。
- en: Existing solutions
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现有解决方案
- en: In many cases, ML systems are designed to replace existing solutions, which
    might be business logic with a lot of if/else statements or third-party solutions.
    It’s crucial to compare your new model to these existing solutions. Your ML model
    doesn’t always have to be better than existing solutions to be useful. A model
    whose performance is a little bit inferior can still be useful if it’s much easier
    or cheaper to use.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，机器学习系统被设计来取代现有的解决方案，这可能是带有大量if/else语句或第三方解决方案的业务逻辑。将你的新模型与这些现有解决方案进行比较至关重要。你的机器学习模型并不总是比现有解决方案更好才能有用。如果一个模型的表现稍逊色，但使用起来更简单或更便宜，它仍然可以是有用的。
- en: When evaluating a model, it’s important to differentiate between “a good system”
    and “a useful system.” A good system isn’t necessarily useful, and a bad system
    isn’t necessarily useless. A self-driving vehicle might be good if it’s a significant
    improvement from previous self-driving systems, but it might not be useful if
    it doesn’t perform at least as well as human drivers. In some cases, even if an
    ML system drives better than an average human, people might still not trust it,
    which renders it not useful. On the other hand, a system that predicts what word
    a user will type next on their phone might be considered bad if it’s much worse
    than a native speaker. However, it might still be useful if its predictions can
    help users type faster some of the time.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估模型时，区分“好系统”和“有用系统”非常重要。一个好系统未必有用，一个坏系统未必无用。如果一个自动驾驶车辆较之前的系统有显著提升，那么它可能是好的，但如果它的表现不如人类驾驶员，那么它可能就不是有用的。在某些情况下，即使一个机器学习系统比普通人驾驶员表现更好，人们仍然可能不信任它，这使得它不具有用处。另一方面，如果一个系统预测用户在手机上接下来会输入的单词比母语者差得多，那么它可能被认为是坏的。然而，如果它的预测有助于用户更快地输入某些时间，它可能仍然是有用的。
- en: Evaluation Methods
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估方法
- en: In academic settings, when evaluating ML models, people tend to fixate on their
    performance metrics. However, in production, we also want our models to be robust,
    fair, calibrated, and overall make sense. We’ll introduce some evaluation methods
    that help with measuring these characteristics of a model.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在学术设置中，评估机器学习模型时，人们往往会专注于性能指标。然而，在实际生产中，我们还希望我们的模型具有稳健性、公平性、校准性，并且整体上是合理的。我们将介绍一些评估方法来帮助衡量模型的这些特性。
- en: Perturbation tests
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扰动测试
- en: A group of my students wanted to build an app to predict whether someone has
    COVID-19 through their cough. Their best model worked great on the training data,
    which consisted of two-second long cough segments collected by hospitals. However,
    when they deployed it to actual users, this model’s predictions were close to
    random.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我的一群学生想要开发一个通过咳嗽声预测某人是否患有 COVID-19 的应用程序。他们的最佳模型在训练数据上表现出色，这些数据由医院收集的两秒钟长的咳嗽片段组成。然而，当他们将其部署到实际用户时，该模型的预测结果接近随机。
- en: One of the reasons is that actual users’ coughs contain a lot of noise compared
    to the coughs collected in hospitals. Users’ recordings might contain background
    music or nearby chatter. The microphones they use are of varying quality. They
    might start recording their coughs as soon as recording is enabled or wait for
    a fraction of a second.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个原因是，实际用户的咳嗽声与在医院收集的咳嗽声相比含有大量噪音。用户的录音可能包含背景音乐或附近的喋喋不休。他们使用的麦克风质量参差不齐。他们可能会在启用录音后立即开始记录他们的咳嗽声，或者等待几分之一秒后开始。
- en: Ideally, the inputs used to develop your model should be similar to the inputs
    your model will have to work with in production, but it’s not possible in many
    cases. This is especially true when data collection is expensive or difficult
    and the best available data you have access to for training is still very different
    from your real-world data. The inputs your models have to work with in production
    are often noisy compared to inputs in development.^([41](ch06.xhtml#ch01fn190))
    The model that performs best on training data isn’t necessarily the model that
    performs best on noisy data.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，用于开发模型的输入应该与生产环境中模型将要处理的输入相似，但在许多情况下这是不可能的。特别是在数据收集昂贵或困难，并且你能够访问的最佳数据仍然与实际数据差异很大的情况下。生产环境中模型需要处理的输入通常与开发中的输入相比更加嘈杂。^([41](ch06.xhtml#ch01fn190))
    在训练数据上表现最好的模型不一定在嘈杂数据上表现最佳。
- en: To get a sense of how well your model might perform with noisy data, you can
    make small changes to your test splits to see how these changes affect your model’s
    performance. For the task of predicting whether someone has COVID-19 from their
    cough, you could randomly add some background noise or randomly clip the testing
    clips to simulate the variance in your users’ recordings. You might want to choose
    the model that works best on the perturbed data instead of the one that works
    best on the clean data.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解您的模型在嘈杂数据下的表现如何，您可以对测试集进行小的更改，看看这些更改如何影响模型的性能。例如，对于预测某人是否患有 COVID-19 的任务，您可以随机添加一些背景噪音或随机剪辑测试音频片段，以模拟用户录音的变化。您可能希望选择在扰动数据上表现最佳的模型，而不是在干净数据上表现最佳的模型。
- en: The more sensitive your model is to noise, the harder it will be to maintain
    it, since if your users’ behaviors change just slightly, such as they change their
    phones, your model’s performance might degrade. It also makes your model susceptible
    to adversarial attack.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 您的模型对噪声越敏感，维护起来就越困难，因为如果您的用户行为略有变化，例如他们更换手机，您的模型性能可能会下降。这也使您的模型容易受到对抗性攻击。
- en: Invariance tests
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不变性测试
- en: A Berkeley study found that between 2008 and 2015, 1.3 million creditworthy
    Black and Latino applicants had their mortgage applications rejected because of
    their races.^([42](ch06.xhtml#ch01fn191)) When the researchers used the income
    and credit scores of the rejected applications but deleted the race-identifying
    features, the applications were accepted.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 伯克利大学的一项研究发现，在 2008 年至 2015 年间，因为其种族而有 130 万名信用良好的黑人和拉丁裔申请者的抵押贷款申请被拒绝。^([42](ch06.xhtml#ch01fn191))
    当研究人员使用被拒绝申请的收入和信用评分，但删除了识别种族的特征时，这些申请被接受。
- en: Certain changes to the inputs shouldn’t lead to changes in the output. In the
    preceding case, changes to race information shouldn’t affect the mortgage outcome.
    Similarly, changes to applicants’ names shouldn’t affect their resume screening
    results nor should someone’s gender affect how much they should be paid. If these
    happen, there are biases in your model, which might render it unusable no matter
    how good its performance is.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 输入的某些变化不应导致输出的变化。在前述情况下，种族信息的变化不应影响抵押贷款的结果。同样，申请人姓名的变化不应影响其简历筛选结果，某人的性别也不应影响他们应该获得多少薪水。如果发生了这些情况，你的模型存在偏见，这可能导致其无法使用，无论其性能多么出色。
- en: 'To avoid these biases, one solution is to do the same process that helped the
    Berkeley researchers discover the biases: keep the inputs the same but change
    the sensitive information to see if the outputs change. Better, you should exclude
    the sensitive information from the features used to train the model in the first
    place.^([43](ch06.xhtml#ch01fn192))'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 要避免这些偏见，一个解决方案是执行与伯克利研究人员发现偏见相同的过程：保持输入不变，但改变敏感信息，看看输出是否改变。更好的做法是，在训练模型时首先排除敏感信息不作为特征使用。^([43](ch06.xhtml#ch01fn192))
- en: Directional expectation tests
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 方向性期望测试
- en: Certain changes to the inputs should, however, cause predictable changes in
    outputs. For example, when developing a model to predict housing prices, keeping
    all the features the same but increasing the lot size shouldn’t decrease the predicted
    price, and decreasing the square footage shouldn’t increase it. If the outputs
    change in the opposite expected direction, your model might not be learning the
    right thing, and you need to investigate it further before deploying it.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，输入的某些变化应导致输出的可预测的变化。例如，当开发一个预测房价的模型时，保持所有特征不变但增加土地面积不应导致预测价格下降，减少平方英尺不应导致价格上升。如果输出以相反预期的方向改变，你的模型可能没有学习到正确的内容，你需要在部署之前进一步调查它。
- en: Model calibration
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型校准
- en: Model calibration is a subtle but crucial concept to grasp. Imagine that someone
    makes a prediction that something will happen with a probability of 70%. What
    this prediction means is that out of all the times this prediction is made, the
    predicted outcome matches the actual outcome 70% of the time. If a model predicts
    that team A will beat team B with a 70% probability, and out of the 1,000 times
    these two teams play together, team A only wins 60% of the time, then we say that
    this model isn’t calibrated. A calibrated model should predict that team A wins
    with a 60% probability.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 模型校准是一个微妙但关键的概念。想象一下，有人预测某件事情发生的概率是70%。这个预测意味着，在所有这个预测被做出的情况下，预测的结果与实际结果一致的次数为70%。如果一个模型预测A队以70%的概率击败B队，而在这两支队伍共同比赛的1,000次中，A队只赢了60%的时间，那么我们称这个模型没有校准。一个校准的模型应该预测A队以60%的概率获胜。
- en: Model calibration is often overlooked by ML practitioners, but it’s one of the
    most important properties of any predictive system. To quote Nate Silver in his
    book *The Signal and the Noise*, calibration is “one of the most important tests
    of a forecast—I would argue that it is the single most important one.”
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 模型校准经常被机器学习从业者忽视，但它是任何预测系统的最重要属性之一。引用内特·西尔弗在他的书《信号与噪声》中的话，校准是“一个预测的最重要的测试之一——我会认为这是最重要的一个测试。”
- en: We’ll walk through two examples to show why model calibration is important.
    First, consider the task of building a recommender system to recommend what movies
    users will likely watch next. Suppose user A watches romance movies 80% of the
    time and comedy 20% of the time. If your recommender system shows exactly the
    movies A will most likely watch, the recommendations will consist of only romance
    movies because A is much more likely to watch romance than any other type of movies.
    You might want a more calibrated system whose recommendations are representative
    of users’ actual watching habits. In this case, they should consist of 80% romance
    and 20% comedy.^([44](ch06.xhtml#ch01fn193))
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过两个示例来展示为什么模型校准很重要。首先，考虑建立推荐系统来推荐用户接下来可能观看的电影的任务。假设用户A80%的时间观看浪漫电影，20%的时间观看喜剧。如果你的推荐系统精确地展示A将最有可能观看的电影，那么推荐将仅包含浪漫电影，因为A更有可能观看浪漫电影而不是其他类型的电影。你可能希望一个更加校准的系统，其推荐是用户实际观看习惯的代表。在这种情况下，推荐应包含80%的浪漫电影和20%的喜剧电影。^([44](ch06.xhtml#ch01fn193))
- en: Second, consider the task of building a model to predict how likely it is that
    a user will click on an ad. For the sake of simplicity, imagine that there are
    only two ads, ad A and ad B. Your model predicts that this user will click on
    ad A with a 10% probability and on ad B with an 8% probability. You don’t need
    your model to be calibrated to rank ad A above ad B. However, if you want to predict
    how many clicks your ads will get, you’ll need your model to be calibrated. If
    your model predicts that a user will click on ad A with a 10% probability but
    in reality the ad is only clicked on 5% of the time, your estimated number of
    clicks will be way off. If you have another model that gives the same ranking
    but is better calibrated, you might want to consider the better calibrated one.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，考虑建立一个模型来预测用户点击广告的可能性。为了简单起见，假设只有两个广告，广告A和广告B。你的模型预测这个用户点击广告A的概率是10%，点击广告B的概率是8%。你并不需要你的模型来校准以便将广告A排在广告B之上。然而，如果你想预测广告将会获得多少点击次数，你就需要你的模型进行校准。如果你的模型预测一个用户点击广告A的概率是10%，但实际上广告只被点击了5%的时间，你估计的点击次数就会大大偏差。如果你有另一个给出相同排名但更好校准的模型，你可能要考虑更好校准的那个模型。
- en: 'To measure a model’s calibration, a simple method is counting: you count the
    number of times your model outputs the probability *X* and the frequency *Y* of
    that prediction coming true, and plot *X* against *Y*. The graph for a perfectly
    calibrated model will have *X* equal *Y* at all data points. In scikit-learn,
    you can plot the calibration curve of a binary classifier with the method `sklearn.calibration.calibration_curve`,
    as shown in [Figure 6-11](#the_calibration_curves_of_different_mod).'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 要衡量模型的校准性，一个简单的方法是计数：你计算模型输出概率 *X* 的次数和该预测实际发生的频率 *Y*，并将 *X* 对 *Y* 进行绘图。一个完全校准的模型的图表将在所有数据点上都有
    *X* 等于 *Y*。在scikit-learn中，你可以使用方法 `sklearn.calibration.calibration_curve` 绘制二元分类器的校准曲线，如[图6-11](#the_calibration_curves_of_different_mod)所示。
- en: '![](Images/dmls_0611.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_0611.png)'
- en: 'Figure 6-11\. The calibration curves of different models on a toy task. The
    logistic regression model is the best calibrated model because it directly optimizes
    logistic loss. Source: [scikit-learn](https://oreil.ly/Tnts7)'
  id: totrans-238
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-11\. 不同模型在一个玩具任务上的校准曲线。逻辑回归模型是最佳校准模型，因为它直接优化逻辑损失。来源：[scikit-learn](https://oreil.ly/Tnts7)
- en: To calibrate your models, a common method is [Platt scaling](https://oreil.ly/pQ0TQ),
    which is implemented in scikit-learn with `sklearn.calibration.CalibratedClassifierCV`.
    Another good open source implementation by Geoff Pleiss can be found on [GitHub](https://oreil.ly/e1Meh).
    For readers who want to learn more about the importance of model calibration and
    how to calibrate neural networks, Lee Richardson and Taylor Pospisil have an [excellent
    blog post](https://oreil.ly/wPUkU) based on their work at Google.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 要对你的模型进行校准，一个常见的方法是[Platt缩放](https://oreil.ly/pQ0TQ)，在scikit-learn中实现为 `sklearn.calibration.CalibratedClassifierCV`。另一个由Geoff
    Pleiss优秀的开源实现可以在[GitHub](https://oreil.ly/e1Meh)找到。对于想要了解模型校准重要性以及如何校准神经网络的读者，Lee
    Richardson和Taylor Pospisil在Google的工作基础上撰写了一篇[优秀的博客文章](https://oreil.ly/wPUkU)。
- en: Confidence measurement
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 置信度测量
- en: Confidence measurement can be considered a way to think about the usefulness
    threshold for each individual prediction. Indiscriminately showing all a model’s
    predictions to users, even the predictions that the model is unsure about, can,
    at best, cause annoyance and make users lose trust in the system, such as an activity
    detection system on your smartwatch that thinks you’re running even though you’re
    just walking a bit fast. At worst, it can cause catastrophic consequences, such
    as a predictive policing algorithm that flags an innocent person as a potential
    criminal.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 置信度测量可以被视为考虑每个单独预测的有用性阈值的一种方式。无差别地向用户展示模型不确定的所有预测，甚至是模型不确定的预测，充其量会导致用户感到恼怒，并且失去对系统的信任，例如您智能手表上的活动检测系统认为您正在跑步，尽管您只是走得稍快。最坏的情况下，这可能会导致灾难性后果，例如预测性警务算法将一个无辜的人标记为潜在犯罪分子。
- en: If you only want to show the predictions that your model is certain about, how
    do you measure that certainty? What is the certainty threshold at which the predictions
    should be shown? What do you want to do with predictions below that threshold—discard
    them, loop in humans, or ask for more information from users?
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只想展示你的模型确信的预测，那么如何衡量这种确信度？在哪个确信度阈值下应该展示预测？你希望对低于该阈值的预测采取什么措施——丢弃它们、让人类介入，还是向用户索取更多信息？
- en: While most other metrics measure the system’s performance on average, confidence
    measurement is a metric for each individual sample. System-level measurement is
    useful to get a sense of overall performance, but sample-level metrics are crucial
    when you care about your system’s performance on every sample.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大多数其他指标衡量系统的平均性能，但置信度测量是针对每个单独样本的指标。系统级别的测量有助于了解总体性能，但样本级别的指标在你关心系统在每个样本上的表现时至关重要。
- en: Slice-based evaluation
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于切片的评估
- en: Slicing means to separate your data into subsets and look at your model’s performance
    on each subset separately. A common mistake that I’ve seen in many companies is
    that they are focused too much on coarse-grained metrics like overall F1 or accuracy
    on the entire data and not enough on sliced-based metrics. This can lead to two
    problems.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 切片意味着将数据分成子集，并分别查看模型在每个子集上的表现。我在许多公司中看到的一个常见错误是，它们过于专注于像整体 F1 或整体准确率这样的粗粒度指标，而不够关注基于切片的指标。这可能会导致两个问题。
- en: 'One is that their model performs differently on different slices of data when
    the model should perform the same. For example, their data has two subgroups,
    one majority and one minority, and the majority subgroup accounts for 90% of the
    data:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个问题是，他们的模型在数据的不同切片上表现不同，而实际上应该表现相同。例如，他们的数据有两个子组，一个是多数，一个是少数，而多数子组占数据的 90%：
- en: Model A achieves 98% accuracy on the majority subgroup but only 80% on the minority
    subgroup, which means its overall accuracy is 96.2%.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型 A 在多数子组上达到 98% 的准确率，但在少数子组上仅达到 80% 的准确率，这意味着其整体准确率为 96.2%。
- en: Model B achieves 95% accuracy on the majority and 95% on the minority, which
    means its overall accuracy is 95%.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型 B 在多数和少数上都达到了 95% 的准确率，这意味着它的整体准确率为 95%。
- en: These two models are compared in [Table 6-3](#two_modelsapostrophe_performance_on_the).
    Which model would you choose?
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个模型在 [表 6-3](#two_modelsapostrophe_performance_on_the) 中进行了比较。你会选择哪个模型？
- en: Table 6-3\. Two models’ performance on the majority and minority subgroups
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6-3. 两个模型在多数和少数子组的表现
- en: '|  | Majority accuracy | Minority accuracy | Overall accuracy |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  | 多数准确率 | 少数准确率 | 整体准确率 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Model A | 98% | 80% | 96.2% |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 模型 A | 98% | 80% | 96.2% |'
- en: '| Model B | 95% | 95% | 95% |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 模型 B | 95% | 95% | 95% |'
- en: If a company focuses only on overall metrics, they might go with model A. They
    might be very happy with this model’s high accuracy until, one day, their end
    users discover that this model is biased against the minority subgroup because
    the minority subgroup happens to correspond to an underrepresented demographic
    group.^([45](ch06.xhtml#ch01fn194)) The focus on overall performance is harmful
    not only because of the potential public backlash, but also because it blinds
    the company to huge potential model improvements. If the company sees the two
    models’ slice-based performance, they might follow different strategies. For example,
    they might decide to improve model A’s performance on the minority subgroup, which
    leads to improving this model’s performance overall. Or they might keep both models
    the same but now have more information to make a better-informed decision on which
    model to deploy.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一家公司只关注整体指标，他们可能会选择模型 A。他们可能对这个模型的高准确率非常满意，直到有一天，他们的最终用户发现，这个模型对少数子组存在偏见，因为少数子组恰好对应一个代表性不足的人口群体。^([45](ch06.xhtml#ch01fn194))
    过度关注整体性能不仅可能导致潜在的公众反弹，还会使公司对巨大的潜在模型改进视而不见。如果公司看到两个模型的基于切片的表现，他们可能会采取不同的策略。例如，他们可能决定提高模型
    A 在少数子组上的表现，从而提高该模型的整体表现。或者他们可能保持两个模型不变，但现在有更多信息来做出更明智的部署决策。
- en: Another problem is that their model performs the same on different slices of
    data when the model should perform differently. Some subsets of data are more
    critical. For example, when you build a model for user churn prediction (predicting
    when a user will cancel a subscription or a service), paid users are more critical
    than nonpaid users. Focusing on a model’s overall performance might hurt its performance
    on these critical slices.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是，他们的模型在数据的不同切片上表现相同，而实际上应该表现不同。一些数据子集更为关键。例如，当你为用户流失预测建模（预测用户何时取消订阅或服务）时，付费用户比非付费用户更为关键。过于关注模型的整体表现可能会损害其在这些关键切片上的表现。
- en: A fascinating and seemingly counterintuitive reason why slice-based evaluation
    is crucial is [Simpson’s paradox](https://oreil.ly/clFB0), a phenomenon in which
    a trend appears in several groups of data but disappears or reverses when the
    groups are combined. This means that model B can perform better than model A on
    all data together, but model A performs better than model B on each subgroup separately.
    Consider model A’s and model B’s performance on group A and group B as shown in
    [Table 6-4](#an_example_of_simpsonapostrophes_parado). Model A outperforms model
    B for both group A and B, but when combined, model B outperforms model A.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 切片评估至关重要的一个引人入胜且看似违反直觉的原因是 [辛普森悖论](https://oreil.ly/clFB0)，这是一种现象，多组数据表现出趋势，但在合并这些组时，这些趋势消失或反转。这意味着模型B在整体数据上可能比模型A表现更好，但在各个子组中，模型A却表现优于模型B。考虑模型A和模型B在A组和B组上的表现，如
    [表 6-4](#an_example_of_simpsonapostrophes_parado) 所示。模型A在A组和B组中都优于模型B，但合并后，模型B却优于模型A。
- en: Table 6-4\. An example of Simpson’s paradox^([a](ch06.xhtml#ch01fn195))
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: Table 6-4\. 辛普森悖论的一个例子^([a](ch06.xhtml#ch01fn195))
- en: '|  | Group A | Group B | Overall |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|  | A组 | B组 | 总体 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Model A | **93% (81/87)** | **73% (192/263)** | 78% (273/350) |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 模型A | **93% (81/87)** | **73% (192/263)** | 78% (273/350) |'
- en: '| Model B | 87% (234/270) | 69% (55/80) | **83% (289/350)** |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 模型B | 87% (234/270) | 69% (55/80) | **83% (289/350)** |'
- en: '| ^([a](ch06.xhtml#ch01fn195-marker)) Numbers from Charig et al.’s kidney stone
    treatment study in 1986: C. R. Charig, D. R. Webb, S. R. Payne, and J. E. Wickham,
    “Comparison of Treatment of Renal Calculi by Open Surgery, Percutaneous Nephrolithotomy,
    and Extracorporeal Shockwave Lithotripsy,” *British Medical Journal* (Clinical
    Research Edition) 292, no. 6524 (March 1986): 879–82, [*https://oreil.ly/X8oWr*](https://oreil.ly/X8oWr).
    |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| ^([a](ch06.xhtml#ch01fn195-marker)) 1986年查里格等人在肾结石治疗研究中的数据：C. R. Charig,
    D. R. Webb, S. R. Payne, and J. E. Wickham, “Comparison of Treatment of Renal
    Calculi by Open Surgery, Percutaneous Nephrolithotomy, and Extracorporeal Shockwave
    Lithotripsy,” *British Medical Journal*（临床研究版）292, no. 6524（1986年3月）：879–82，[*https://oreil.ly/X8oWr*](https://oreil.ly/X8oWr)。
    |'
- en: Simpson’s paradox is more common than you’d think. In 1973, Berkeley graduate
    statistics showed that the admission rate for men was much higher than for women,
    which caused people to suspect biases against women. However, a closer look into
    individual departments showed that the admission rates for women were actually
    higher than those for men in four out of six departments,^([46](ch06.xhtml#ch01fn196))
    as shown in [Table 6-5](#the_overall_graduate_admission_rate_for).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 辛普森悖论比你想象的更为常见。1973年，伯克利的研究数据显示，男性的录取率远高于女性，这引起了人们对女性是否受到偏见的怀疑。然而，进一步分析各个学科的数据后发现，在六个学科中，女性的录取率实际上比男性高，其中四个学科的情况如
    [表 6-5](#the_overall_graduate_admission_rate_for) 所示^([46](ch06.xhtml#ch01fn196))。
- en: Table 6-5\. Berkeley’s 1973 graduate admission data^([a](ch06.xhtml#idm46868208574464))
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: Table 6-5\. 1973年伯克利研究生录取数据^([a](ch06.xhtml#idm46868208574464))
- en: '|   | All | Men | Women |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|   | 全部 | 男性 | 女性 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Department | Applicants | Admitted | Applicants | Admitted | Applicants |
    Admitted |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 学科 | 申请人数 | 录取人数 | 申请人数 | 录取人数 | 申请人数 | 录取人数 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| **A** | 933 | 64% | ***825*** | 62% | 108 | **82%** |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| **A** | 933 | 64% | ***825*** | 62% | 108 | **82%** |'
- en: '| **B** | 585 | 63% | ***560*** | 63% | 25 | **68** **%** |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| **B** | 585 | 63% | ***560*** | 63% | 25 | **68** **%** |'
- en: '| **C** | 918 | 35% | 325 | **37** **%** | ***593*** | 34% |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| **C** | 918 | 35% | 325 | **37** **%** | ***593*** | 34% |'
- en: '| **D** | 792 | 34% | 417 | 33% | 375 | **35** **%** |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| **D** | 792 | 34% | 417 | 33% | 375 | **35** **%** |'
- en: '| **E** | 584 | 25% | 191 | **28** **%** | ***393*** | 24% |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| **E** | 584 | 25% | 191 | **28** **%** | ***393*** | 24% |'
- en: '| **F** | 714 | 6% | 373 | 6% | 341 | **7** **%** |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| **F** | 714 | 6% | 373 | 6% | 341 | **7** **%** |'
- en: '| **Total** | 12,763 | 41% | 8,442 | **44%** | 4,321 | 35% |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| **总计** | 12,763 | 41% | 8,442 | **44%** | 4,321 | 35% |'
- en: '| ^([a](ch06.xhtml#idm46868208574464-marker)) Data from Bickel et al. (1975)
    |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| ^([a](ch06.xhtml#idm46868208574464-marker)) Bickel等人（1975年）的数据 |'
- en: Regardless of whether you’ll actually encounter this paradox, the point here
    is that aggregation can conceal and contradict actual situations. To make informed
    decisions regarding what model to choose, we need to take into account its performance
    not only on the entire data, but also on individual slices. Slice-based evaluation
    can give you insights to improve your model’s performance both overall and on
    critical data and help detect potential biases. It might also help reveal non-ML
    problems. Once, our team discovered that our model performed great overall but
    very poorly on traffic from mobile users. After investigating, we realized that
    it was because a button was half hidden on small screens (e.g., phone screens).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你是否真的会遇到这种悖论，这里的重点是聚合可能会掩盖和与实际情况相矛盾。为了能够做出关于选择何种模型的明智决策，我们需要考虑它在整体数据上的表现，也要考虑它在各个切片上的表现。基于切片的评估可以帮助你深入了解和改进模型的整体性能以及关键数据，还可以帮助检测潜在的偏见。它还可能帮助揭示非机器学习问题。有一次，我们团队发现我们的模型整体表现很好，但在移动用户的流量上表现非常糟糕。经过调查，我们意识到这是因为在小屏幕上（例如手机屏幕）一个按钮被部分隐藏了。
- en: Even when you don’t think slices matter, understanding how your model performs
    in a more fine-grained way can give you confidence in your model to convince other
    stakeholders, like your boss or your customers, to trust your ML models.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你认为切片并不重要，了解你的模型在更细粒度方式下的表现，可以让你对你的模型有信心，从而说服其他利益相关者，如你的老板或客户，来信任你的机器学习模型。
- en: 'To track your model’s performance on critical slices, you’d first need to know
    what your critical slices are. You might wonder how to discover critical slices
    in your data. Slicing is, unfortunately, still more of an art than a science,
    requiring intensive data exploration and analysis. Here are the three main approaches:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟踪你的模型在关键切片上的表现，你首先需要知道你的关键切片是什么。你可能想知道如何在你的数据中发现关键切片。不幸的是，切片仍然更像是一门艺术而不是一门科学，需要进行深入的数据探索和分析。以下是三种主要方法：
- en: Heuristics-based
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 基于启发式的方法
- en: Slice your data using domain knowledge you have of the data and the task at
    hand. For example, when working with web traffic, you might want to slice your
    data along dimensions like mobile versus desktop, browser type, and locations.
    Mobile users might behave very differently from desktop users. Similarly, internet
    users in different geographic locations might have different expectations on what
    a website should look like.^([47](ch06.xhtml#ch01fn197))
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 利用你对数据和当前任务的领域知识对数据进行切片。例如，在处理网络流量时，你可能希望按照移动设备与桌面设备、浏览器类型和地理位置等维度对数据进行切片。移动用户的行为可能与桌面用户有很大不同。同样，不同地理位置的互联网用户可能对网站的期望也不同。^([47](ch06.xhtml#ch01fn197))
- en: Error analysis
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 错误分析
- en: Manually go through misclassified examples and find patterns among them. We
    discovered our model’s problem with mobile users when we saw that most of the
    misclassified examples were from mobile users.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 手动查看被误分类的示例，并找出它们之间的模式。当我们发现大多数被误分类的示例来自移动用户时，我们发现了我们模型在移动用户上的问题。
- en: Slice finder
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 切片发现器
- en: 'There has been research to systemize the process of finding slices, including
    Chung et al.’s [“Slice Finder: Automated Data Slicing for Model Validation”](https://oreil.ly/eypmq)
    in 2019 and covered in Sumyea Helal’s [“Subgroup Discovery Algorithms: A Survey
    and Empirical Evaluation”](https://oreil.ly/7yBJO) (2016). The process generally
    starts with generating slice candidates with algorithms such as beam search, clustering,
    or decision, then pruning out clearly bad candidates for slices, and then ranking
    the candidates that are left.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 已经有研究系统化了寻找切片的过程，包括2019年张等人的 [“切片发现器：自动化数据切片用于模型验证”](https://oreil.ly/eypmq)
    和Sumyea Helal在2016年的 [“子群发现算法：调查和实证评估”](https://oreil.ly/7yBJO) 中提到。这个过程通常从使用算法生成切片候选开始，如束搜索、聚类或决策，然后剔除明显不好的切片候选，并对剩下的候选进行排名。
- en: Keep in mind that once you have discovered these critical slices, you will need
    sufficient, correctly labeled data for each of these slices for evaluation. The
    quality of your evaluation is only as good as the quality of your evaluation data.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，一旦你发现了这些关键切片，你需要足够的、正确标记的数据来对每个切片进行评估。你的评估质量取决于你的评估数据的质量。
- en: Summary
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we’ve covered the ML algorithm part of ML systems, which many
    ML practitioners consider to be the most fun part of an ML project lifecycle.
    With the initial models, we can bring to life (in the form of predictions) all
    our hard work in data and feature engineering, and can finally evaluate our hypothesis
    (i.e., we can predict the outputs given the inputs).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了机器学习系统的机器学习算法部分，许多机器学习从业者认为这是机器学习项目生命周期中最有趣的部分。通过最初的模型，我们可以将我们在数据和特征工程上的辛勤工作变为现实（以预测的形式），并最终评估我们的假设（即，我们可以根据输入预测输出）。
- en: We started with how to select ML models best suited for our tasks. Instead of
    going into pros and cons of each individual model architecture—which is a fool’s
    errand given the growing pools of existing models—the chapter outlined the aspects
    you need to consider to make an informed decision on which model is best for your
    objectives, constraints, and requirements.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先讨论了如何选择最适合我们任务的机器学习模型。与其深入探讨每个单独模型架构的利弊——考虑到现有模型池越来越大，这种做法徒劳无功——本章概述了您需要考虑的方面，以便为您的目标、约束条件和需求做出明智的决策。
- en: We then continued to cover different aspects of model development. We covered
    not only individual models but also ensembles of models, a technique widely used
    in competitions and leaderboard-style research.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接着讨论了模型开发的不同方面。我们不仅涵盖了单个模型，还涉及了模型集成，这是比赛和排行榜风格研究中广泛使用的技术。
- en: During the model development phase, you might experiment with many different
    models. Intensive tracking and versioning of your many experiments are generally
    agreed to be important, but many ML engineers still skip it because doing it might
    feel like a chore. Therefore, having tools and appropriate infrastructure to automate
    the tracking and versioning process is essential. We’ll cover tools and infrastructure
    for ML production in [Chapter 10](ch10.xhtml#infrastructure_and_tooling_for_mlops).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型开发阶段，您可能会尝试许多不同的模型。对您的许多实验进行密集跟踪和版本控制通常被认为是重要的，但许多机器学习工程师仍然会跳过这一步，因为这样做可能会感觉像在做苦差事。因此，拥有工具和适当的基础设施来自动化跟踪和版本控制过程至关重要。我们将在[第10章](ch10.xhtml#infrastructure_and_tooling_for_mlops)中讨论用于机器学习生产的工具和基础设施。
- en: As models today are getting bigger and consuming more data, distributed training
    is becoming an essential skill for ML model developers, and we discussed techniques
    for parallelism including data parallelism, model parallelism, and pipeline parallelism.
    Making your models work on a large distributed system, like the one that runs
    models with hundreds of millions, if not billions, of parameters, can be challenging
    and require specialized system engineering expertise.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 由于现今模型越来越庞大且消耗数据量也越来越多，分布式训练成为机器学习模型开发人员必备的技能之一。我们讨论了并行技术，包括数据并行、模型并行和管道并行。让您的模型在大规模分布式系统上运行，比如那些运行具有数亿、甚至数十亿参数的模型，可能会面临挑战，需要专门的系统工程专业知识。
- en: We ended the chapter with how to evaluate your models to pick the best one to
    deploy. Evaluation metrics don’t mean much unless you have a baseline to compare
    them to, and we covered different types of baselines you might want to consider
    for evaluation. We also covered a range of evaluation techniques necessary to
    sanity check your models before further evaluating your models in a production
    environment.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们讨论了如何评估您的模型以选择最佳部署模型。评估指标如果没有基准进行比较，意义不大，我们涵盖了在进一步评估您的模型进入生产环境之前进行理智检查的各种评估技术。
- en: Often, no matter how good your offline evaluation of a model is, you still can’t
    be sure of your model’s performance in production until that model has been deployed.
    In the next chapter, we’ll go over how to deploy a model.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，无论您的模型的离线评估有多好，您仍然无法确保您的模型在生产环境中的表现，直到该模型已经部署。在下一章中，我们将讨论如何部署一个模型。
- en: ^([1](ch06.xhtml#ch01fn150-marker)) Andrew Ng has [a great lecture](https://oreil.ly/o6tGK)
    where he explains that if a learning algorithm suffers from high bias, getting
    more training data by itself won’t help much. Whereas if a learning algorithm
    suffers from high variance, getting more training data is likely to help.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch06.xhtml#ch01fn150-marker)) 安德鲁·吴在一场[精彩的讲座](https://oreil.ly/o6tGK)中解释说，如果一个学习算法遭受高偏差，仅仅增加训练数据本身并不会有太大帮助。然而，如果学习算法遭受高方差，增加训练数据可能会有所帮助。
- en: ^([2](ch06.xhtml#ch01fn151-marker)) I went through the winning solutions listed
    on Farid Rashidi’s [“Kaggle Solutions” web page](https://oreil.ly/vNrPx). One
    solution used 33 models (Giba, “1st Place-Winner Solution-Gilberto Titericz and
    Stanislav Semenov,” Kaggle, [*https://oreil.ly/z5od8*](https://oreil.ly/z5od8)).
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch06.xhtml#ch01fn151-marker)) 我查阅了Farid Rashidi的[“Kaggle解决方案”网页](https://oreil.ly/vNrPx)上列出的获胜解决方案。其中一种解决方案使用了33个模型（Giba，《第一名解决方案-Gilberto
    Titericz和Stanislav Semenov》，Kaggle，[*https://oreil.ly/z5od8*](https://oreil.ly/z5od8)）。
- en: '^([3](ch06.xhtml#ch01fn152-marker)) Mikel Galar, Alberto Fernandez, Edurne
    Barrenechea, Humberto Bustince, and Francisco Herrera, “A Review on Ensembles
    for the Class Imbalance Problem: Bagging-, Boosting-, and Hybrid-Based Approaches,”
    *IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and
    Reviews)* 42, no. 4 (July 2012): 463–84, [*https://oreil.ly/ZBlgE*](https://oreil.ly/ZBlgE);
    G. Rekha, Amit Kumar Tyagi, and V. Krishna Reddy, “Solving Class Imbalance Problem
    Using Bagging, Boosting Techniques, With and Without Using Noise Filtering Method,”
    *International Journal of Hybrid Intelligent Systems* 15, no. 2 (January 2019):
    67–76, [*https://oreil.ly/hchzU*](https://oreil.ly/hchzU).'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch06.xhtml#ch01fn152-marker)) Mikel Galar, Alberto Fernandez, Edurne Barrenechea,
    Humberto Bustince和Francisco Herrera，《类不平衡问题的集成方法综述：Bagging、Boosting和混合方法》，《IEEE
    Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)》42卷4期（2012年7月）：463–84，[*https://oreil.ly/ZBlgE*](https://oreil.ly/ZBlgE)；G.
    Rekha, Amit Kumar Tyagi和V. Krishna Reddy，《使用Bagging和Boosting技术解决类不平衡问题，使用和不使用噪声过滤方法的对比》，《国际混合智能系统期刊》15卷2期（2019年1月）：67–76，[*https://oreil.ly/hchzU*](https://oreil.ly/hchzU)。
- en: ^([4](ch06.xhtml#ch01fn153-marker)) Training stability here means less fluctuation
    in the training loss.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch06.xhtml#ch01fn153-marker)) 这里的训练稳定性意味着训练损失波动较小。
- en: '^([5](ch06.xhtml#ch01fn154-marker)) Leo Breiman, “Bagging Predictors,” *Machine
    Learning* 24 (1996): 123–40, [*https://oreil.ly/adzJu*](https://oreil.ly/adzJu).'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch06.xhtml#ch01fn154-marker)) Leo Breiman，《Bagging预测器》，《机器学习》24（1996年）：123–40，[*https://oreil.ly/adzJu*](https://oreil.ly/adzJu)。
- en: ^([6](ch06.xhtml#ch01fn155-marker)) “Machine Learning Challenge Winning Solutions,”
    [*https://oreil.ly/YjS8d*](https://oreil.ly/YjS8d).
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch06.xhtml#ch01fn155-marker)) “机器学习挑战获胜解决方案”，[*https://oreil.ly/YjS8d*](https://oreil.ly/YjS8d)。
- en: '^([7](ch06.xhtml#ch01fn156-marker)) Tianqi Chen and Tong He, “Higgs Boson Discovery
    with Boosted Trees,” *Proceedings of Machine Learning Research* 42 (2015): 69–80,
    [*https://oreil.ly/ysBYO*](https://oreil.ly/ysBYO).'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch06.xhtml#ch01fn156-marker)) Tianqi Chen和Tong He，《带增强树的希格斯玻色子发现》，《机器学习研究进展会议论文集》42（2015年）：69–80，[*https://oreil.ly/ysBYO*](https://oreil.ly/ysBYO)。
- en: ^([8](ch06.xhtml#ch01fn157-marker)) We’ll cover observability in [Chapter 8](ch08.xhtml#data_distribution_shifts_and_monitoring).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch06.xhtml#ch01fn157-marker)) 我们将在[第8章](ch08.xhtml#data_distribution_shifts_and_monitoring)中讨论可观察性。
- en: ^([9](ch06.xhtml#ch01fn158-marker)) I’m still waiting for an experiment tracking
    tool that integrates with Git commits and DVC commits.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch06.xhtml#ch01fn158-marker)) 我仍在等待一个与Git提交和DVC提交集成的实验跟踪工具。
- en: ^([10](ch06.xhtml#ch01fn159-marker)) Notable examples include atomic operations
    in CUDA where nondeterministic orders of operations lead to different floating
    point rounding errors between runs.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch06.xhtml#ch01fn159-marker)) 显著的例子包括CUDA中的原子操作，其中非确定性操作顺序导致不同运行之间的浮点舍入误差不同。
- en: ^([11](ch06.xhtml#ch01fn160-marker)) For products that serve a large number
    of users, you also have to care about scalability in serving a model, which is
    outside of the scope of an ML project so not covered in this book.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch06.xhtml#ch01fn160-marker)) 对于服务大量用户的产品，您还必须关注模型服务的可伸缩性，这超出了ML项目的范围，因此本书未涵盖此内容。
- en: ^([12](ch06.xhtml#ch01fn161-marker)) According to Wikipedia, “Out-of-core algorithms
    are algorithms that are designed to process data that are too large to fit into
    a computer’s main memory at once” (s.v. “External memory algorithm,” [*https://oreil.ly/apv5m*](https://oreil.ly/apv5m)).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch06.xhtml#ch01fn161-marker)) 根据维基百科，“Out-of-core算法是专为处理一次无法完全放入计算机主内存的数据而设计的”（参见“External
    memory algorithm”，[*https://oreil.ly/apv5m*](https://oreil.ly/apv5m)）。
- en: ^([13](ch06.xhtml#ch01fn162-marker)) Tim Salimans, Yaroslav Bulatov, and contributors,
    gradient-checkpointing repository, 2017, [*https://oreil.ly/GTUgC*](https://oreil.ly/GTUgC).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch06.xhtml#ch01fn162-marker)) Tim Salimans, Yaroslav Bulatov和贡献者，《梯度检查点存储库》，2017年，[*https://oreil.ly/GTUgC*](https://oreil.ly/GTUgC)。
- en: ^([14](ch06.xhtml#ch01fn163-marker)) Dipankar Das, Sasikanth Avancha, Dheevatsa
    Mudigere, Karthikeyan Vaidynathan, Srinivas Sridharan, Dhiraj Kalamkar, Bharat
    Kaul, and Pradeep Dubey, “Distributed Deep Learning Using Synchronous Stochastic
    Gradient Descent,” *arXiv*, February 22, 2016, [*https://oreil.ly/ma8Y6*](https://oreil.ly/ma8Y6).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch06.xhtml#ch01fn163-marker)) Dipankar Das, Sasikanth Avancha, Dheevatsa
    Mudigere, Karthikeyan Vaidynathan, Srinivas Sridharan, Dhiraj Kalamkar, Bharat
    Kaul 和 Pradeep Dubey，《使用同步随机梯度下降的分布式深度学习》，*arXiv*，2016年2月22日，[*https://oreil.ly/ma8Y6*](https://oreil.ly/ma8Y6)。
- en: ^([15](ch06.xhtml#ch01fn164-marker)) Jianmin Chen, Xinghao Pan, Rajat Monga,
    Samy Bengio, and Rafal Jozefowicz, “Revisiting Distributed Synchronous SGD,” ICLR
    2017, [*https://oreil.ly/dzVZ5*](https://oreil.ly/dzVZ5); Matei Zaharia, Andy
    Konwinski, Anthony D. Joseph, Randy Katz, and Ion Stoica, “Improving MapReduce
    Performance in Heterogeneous Environments,” 8th USENIX Symposium on Operating
    Systems Design and Implementation, [*https://oreil.ly/FWswd*](https://oreil.ly/FWswd);
    Aaron Harlap, Henggang Cui, Wei Dai, Jinliang Wei, Gregory R. Ganger, Phillip
    B. Gibbons, Garth A. Gibson, and Eric P. Xing, “Addressing the Straggler Problem
    for Iterative Convergent Parallel ML” (SoCC ’16, Santa Clara, CA, October 5–7,
    2016), [*https://oreil.ly/wZgOO*](https://oreil.ly/wZgOO).
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch06.xhtml#ch01fn164-marker)) Jianmin Chen, Xinghao Pan, Rajat Monga,
    Samy Bengio 和 Rafal Jozefowicz，《重温分布式同步 SGD》，ICLR 2017，[*https://oreil.ly/dzVZ5*](https://oreil.ly/dzVZ5)；Matei
    Zaharia, Andy Konwinski, Anthony D. Joseph, Randy Katz 和 Ion Stoica，《在异构环境中提升
    MapReduce 性能》，第8届USENIX操作系统设计与实现研讨会，[*https://oreil.ly/FWswd*](https://oreil.ly/FWswd)；Aaron
    Harlap, Henggang Cui, Wei Dai, Jinliang Wei, Gregory R. Ganger, Phillip B. Gibbons,
    Garth A. Gibson 和 Eric P. Xing，《解决并行机器学习迭代收敛中的拉滞问题》（SoCC '16，加利福尼亚州圣克拉拉，2016年10月5-7日），[*https://oreil.ly/wZgOO*](https://oreil.ly/wZgOO)。
- en: ^([16](ch06.xhtml#ch01fn165-marker)) Jeffrey Dean, Greg Corrado, Rajat Monga,
    Kai Chen, Matthieu Devin, Mark Mao, Marc’aurelio Ranzato, et al., “Large Scale
    Distributed Deep Networks,” NIPS 2012, [*https://oreil.ly/EWPun*](https://oreil.ly/EWPun).
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch06.xhtml#ch01fn165-marker)) Jeffrey Dean, Greg Corrado, Rajat Monga,
    Kai Chen, Matthieu Devin, Mark Mao, Marc’aurelio Ranzato 等，《大规模分布式深度网络》，NIPS 2012，[*https://oreil.ly/EWPun*](https://oreil.ly/EWPun)。
- en: ^([17](ch06.xhtml#ch01fn166-marker)) Jim Dowling, “Distributed TensorFlow,”
    O’Reilly Media, December 19, 2017, [*https://oreil.ly/VYlOP*](https://oreil.ly/VYlOP).
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch06.xhtml#ch01fn166-marker)) Jim Dowling，《分布式 TensorFlow》，O’Reilly Media，2017年12月19日，[*https://oreil.ly/VYlOP*](https://oreil.ly/VYlOP)。
- en: '^([18](ch06.xhtml#ch01fn167-marker)) Feng Niu, Benjamin Recht, Christopher
    Ré, and Stephen J. Wright, “Hogwild!: A Lock-Free Approach to Parallelizing Stochastic
    Gradient Descent,” 2011, [*https://oreil.ly/sAEbv*](https://oreil.ly/sAEbv).'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: ^([18](ch06.xhtml#ch01fn167-marker)) Feng Niu, Benjamin Recht, Christopher Ré
    和 Stephen J. Wright，《Hogwild！：一种无锁化并行随机梯度下降方法》，2011年，[*https://oreil.ly/sAEbv*](https://oreil.ly/sAEbv)。
- en: ^([19](ch06.xhtml#ch01fn168-marker)) Tom B. Brown, Benjamin Mann, Nick Ryder,
    Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al.,
    “Language Models Are Few-Shot Learners,” *arXiv*, May 28, 2020, [*https://oreil.ly/qjg2S*](https://oreil.ly/qjg2S).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: ^([19](ch06.xhtml#ch01fn168-marker)) Tom B. Brown, Benjamin Mann, Nick Ryder,
    Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan 等，《语言模型是少样本学习者》，*arXiv*，2020年5月28日，[*https://oreil.ly/qjg2S*](https://oreil.ly/qjg2S)。
- en: '^([20](ch06.xhtml#ch01fn169-marker)) Sam McCandlish, Jared Kaplan, Dario Amodei,
    and OpenAI Dota Team, “An Empirical Model of Large-Batch Training,” *arXiv*, December
    14, 2018, [*https://oreil.ly/mcjbV*](https://oreil.ly/mcjbV); Christopher J. Shallue,
    Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, and George
    E. Dahl, “Measuring the Effects of Data Parallelism on Neural Network Training,”
    *Journal of Machine Learning Research* 20 (2019): 1–49, [*https://oreil.ly/YAEOM*](https://oreil.ly/YAEOM).'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '^([20](ch06.xhtml#ch01fn169-marker)) Sam McCandlish, Jared Kaplan, Dario Amodei
    和 OpenAI Dota Team，《大批量训练的实证模型》，*arXiv*，2018年12月14日，[*https://oreil.ly/mcjbV*](https://oreil.ly/mcjbV)；Christopher
    J. Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig
    和 George E. Dahl，《数据并行对神经网络训练效果的衡量》，*Journal of Machine Learning Research*，20
    (2019): 1–49，[*https://oreil.ly/YAEOM*](https://oreil.ly/YAEOM)。'
- en: ^([21](ch06.xhtml#ch01fn170-marker)) Jure Leskovec, Mining Massive Datasets
    course, Stanford, lecture 13, 2020, [*https://oreil.ly/gZcja*](https://oreil.ly/gZcja).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: ^([21](ch06.xhtml#ch01fn170-marker)) Jure Leskovec，《挖掘大规模数据集》斯坦福课程，第13讲，2020年，[*https://oreil.ly/gZcja*](https://oreil.ly/gZcja)。
- en: '^([22](ch06.xhtml#ch01fn171-marker)) Yanping Huang, Youlong Cheng, Ankur Bapna,
    Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, et al., “GPipe: Easy Scaling
    with Micro-Batch Pipeline Parallelism,” *arXiv*, July 25, 2019, [*https://oreil.ly/wehkx*](https://oreil.ly/wehkx).'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: ^([22](ch06.xhtml#ch01fn171-marker)) Yanping Huang, Youlong Cheng, Ankur Bapna,
    Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee 等，《GPipe：微批量管道并行易扩展》，*arXiv*，2019年7月25日，[*https://oreil.ly/wehkx*](https://oreil.ly/wehkx)。
- en: ^([23](ch06.xhtml#ch01fn172-marker)) We’ll cover quantization in [Chapter 7](ch07.xhtml#model_deployment_and_prediction_service).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: ^([23](ch06.xhtml#ch01fn172-marker)) 我们将在[第7章](ch07.xhtml#model_deployment_and_prediction_service)中讨论量化问题。
- en: '^([24](ch06.xhtml#ch01fn173-marker)) GSD is a well-documented technique. See
    “How Do People Come Up With All These Crazy Deep Learning Architectures?,” Reddit,
    [*https://oreil.ly/5vEsH*](https://oreil.ly/5vEsH); “Debate About Science at Organizations
    like Google Brain/FAIR/DeepMind,” Reddit, [*https://oreil.ly/2K77r*](https://oreil.ly/2K77r);
    “Grad Student Descent,” *Science Dryad*, January 25, 2014, [*https://oreil.ly/dIR9r*](https://oreil.ly/dIR9r);
    and Guy Zyskind (@GuyZys), “Grad Student Descent: the preferred #nonlinear #optimization
    technique #machinelearning,” Twitter, April 27, 2015, [*https://oreil.ly/SW1or*](https://oreil.ly/SW1or).'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '^([24](ch06.xhtml#ch01fn173-marker)) GSD 是一种有详细文档记录的技术。参见“人们是如何提出所有这些疯狂的深度学习架构的？”，Reddit，[*https://oreil.ly/5vEsH*](https://oreil.ly/5vEsH)；“在
    Google Brain/FAIR/DeepMind 等组织中关于科学的辩论”，Reddit，[*https://oreil.ly/2K77r*](https://oreil.ly/2K77r)；“研究生下降”，*Science
    Dryad*，2014年1月25日，[*https://oreil.ly/dIR9r*](https://oreil.ly/dIR9r)；Guy Zyskind
    (@GuyZys)，“研究生下降：首选的 #非线性 #优化 技术 #机器学习”，Twitter，2015年4月27日，[*https://oreil.ly/SW1or*](https://oreil.ly/SW1or)。'
- en: ^([25](ch06.xhtml#ch01fn174-marker)) auto-sklearn 2.0 also provides basic model
    selection capacity.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: ^([25](ch06.xhtml#ch01fn174-marker)) auto-sklearn 2.0 也提供基本的模型选择能力。
- en: ^([26](ch06.xhtml#ch01fn175-marker)) Our team at NVIDIA developed [Milano](https://oreil.ly/FYWaU),
    a framework-agnostic tool for automatic hyperparameter tuning using random search.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: ^([26](ch06.xhtml#ch01fn175-marker)) 我们在 NVIDIA 开发了[Milano](https://oreil.ly/FYWaU)，一个框架不可知的工具，用于使用随机搜索进行自动超参数调整。
- en: ^([27](ch06.xhtml#ch01fn176-marker)) A common practice I’ve observed is to start
    with coarse-to-fine random search, then experiment with Bayesian or grid search
    once the search space has been significantly reduced.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: ^([27](ch06.xhtml#ch01fn176-marker)) 我观察到的一个常见做法是从粗到细的随机搜索开始，一旦搜索空间被显著缩减，再尝试贝叶斯或网格搜索。
- en: ^([28](ch06.xhtml#ch01fn177-marker)) Barret Zoph and Quoc V. Le, “Neural Architecture
    Search with Reinforcement Learning,” *arXiv*, November 5, 2016, [*https://oreil.ly/FhsuQ*](https://oreil.ly/FhsuQ);
    Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V. Le, “Regularized Evolution
    for Image Classifier Architecture Search,” AAAI 2019, [*https://oreil.ly/FWYjn*](https://oreil.ly/FWYjn).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: ^([28](ch06.xhtml#ch01fn177-marker)) Barret Zoph 和 Quoc V. Le，“用强化学习进行神经架构搜索”，*arXiv*，2016年11月5日，[*https://oreil.ly/FhsuQ*](https://oreil.ly/FhsuQ)；Esteban
    Real, Alok Aggarwal, Yanping Huang 和 Quoc V. Le，“用于图像分类器架构搜索的正则化进化”，AAAI 2019，[*https://oreil.ly/FWYjn*](https://oreil.ly/FWYjn)。
- en: '^([29](ch06.xhtml#ch01fn178-marker)) You can make the search space continuous
    to allow differentiation, but the resulting architecture has to be converted into
    a discrete architecture. See [“DARTS: Differentiable Architecture Search”](https://oreil.ly/sms2H)
    (Liu et al. 2018).'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '^([29](ch06.xhtml#ch01fn178-marker)) 您可以使搜索空间连续以允许差异化，但结果架构必须转换为离散架构。参见[“DARTS:
    Differentiable Architecture Search”](https://oreil.ly/sms2H)（Liu 等，2018年）。'
- en: ^([30](ch06.xhtml#ch01fn179-marker)) We cover learning procedures and optimizers
    in more detail in the section “Basic ML Reviews” in the [book’s GitHub repository](https://oreil.ly/designing-machine-learning-systems-code).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: ^([30](ch06.xhtml#ch01fn179-marker)) 我们在[书的 GitHub 仓库](https://oreil.ly/designing-machine-learning-systems-code)中更详细地涵盖了学习过程和优化器的内容。
- en: '^([31](ch06.xhtml#ch01fn180-marker)) Luke Metz, Niru Maheswaranathan, C. Daniel
    Freeman, Ben Poole, and Jascha Sohl-Dickstein, “Tasks, Stability, Architecture,
    and Compute: Training More Effective Learned Optimizers, and Using Them to Train
    Themselves,” *arXiv*, September 23, 2020, [*https://oreil.ly/IH7eT*](https://oreil.ly/IH7eT).'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: ^([31](ch06.xhtml#ch01fn180-marker)) Luke Metz, Niru Maheswaranathan, C. Daniel
    Freeman, Ben Poole 和 Jascha Sohl-Dickstein，“任务、稳定性、架构和计算：训练更有效的学习优化器，并使用它们自我训练”，*arXiv*，2020年9月23日，[*https://oreil.ly/IH7eT*](https://oreil.ly/IH7eT)。
- en: '^([32](ch06.xhtml#ch01fn181-marker)) Mingxing Tan and Quoc V. Le, “EfficientNet:
    Improving Accuracy and Efficiency through AutoML and Model Scaling,” *Google AI
    Blog*, May 29, 2019, [*https://oreil.ly/gonEn*](https://oreil.ly/gonEn).'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: ^([32](ch06.xhtml#ch01fn181-marker)) Mingxing Tan 和 Quoc V. Le，“EfficientNet：通过AutoML和模型缩放提高准确性和效率”，*Google
    AI Blog*，2019年5月29日，[*https://oreil.ly/gonEn*](https://oreil.ly/gonEn)。
- en: ^([33](ch06.xhtml#ch01fn182-marker)) Samantha Murphy, “The Evolution of Facebook
    News Feed,” *Mashable*, March 12, 2013, [*https://oreil.ly/1HMXh*](https://oreil.ly/1HMXh).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: ^([33](ch06.xhtml#ch01fn182-marker)) Samantha Murphy，“Facebook News Feed 的演变”，*Mashable*，2013年3月12日，[*https://oreil.ly/1HMXh*](https://oreil.ly/1HMXh)。
- en: ^([34](ch06.xhtml#ch01fn183-marker)) Iveta Ryšavá, “What Mark Zuckerberg’s News
    Feed Looked Like in 2006,” Newsfeed.org, January 14, 2016, [*https://oreil.ly/XZT6Q*](https://oreil.ly/XZT6Q).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: ^([34](ch06.xhtml#ch01fn183-marker)) Iveta Ryšavá，《马克·扎克伯格2006年的新闻动态》，Newsfeed.org，2016年1月14日，[*https://oreil.ly/XZT6Q*](https://oreil.ly/XZT6Q)。
- en: '^([35](ch06.xhtml#ch01fn184-marker)) Martin Zinkevich, “Rules of Machine Learning:
    Best Practices for ML Engineering,” Google, 2019, [*https://oreil.ly/YtEsN*](https://oreil.ly/YtEsN).'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: ^([35](ch06.xhtml#ch01fn184-marker)) Martin Zinkevich，《机器学习规则：ML工程的最佳实践》，Google，2019年，[*https://oreil.ly/YtEsN*](https://oreil.ly/YtEsN)。
- en: ^([36](ch06.xhtml#ch01fn185-marker)) We’ll go in depth about how often to update
    your models in [Chapter 9](ch09.xhtml#continual_learning_and_test_in_producti).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: ^([36](ch06.xhtml#ch01fn185-marker)) 我们将深入探讨如何在[第9章](ch09.xhtml#continual_learning_and_test_in_producti)中更新您的模型频率。
- en: ^([37](ch06.xhtml#ch01fn186-marker)) See the section [“Business and ML Objectives”](ch02.xhtml#business_and_ml_objectives).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ^([37](ch06.xhtml#ch01fn186-marker)) 请参阅部分 [“业务和机器学习目标”](ch02.xhtml#business_and_ml_objectives)。
- en: ^([38](ch06.xhtml#ch01fn187-marker)) Fréchet inception distance, a common metric
    for measuring the quality of synthesized images. The smaller the value, the higher
    the quality is supposed to be.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: ^([38](ch06.xhtml#ch01fn187-marker)) Fréchet inception distance，一种衡量合成图像质量的常用指标。数值越小，质量越高。
- en: ^([39](ch06.xhtml#ch01fn188-marker)) The accuracy, in this case, would be around
    0.80.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: ^([39](ch06.xhtml#ch01fn188-marker)) 在这种情况下，准确率大约为0.80。
- en: ^([40](ch06.xhtml#ch01fn189-marker)) Revisit the section [“Using the right evaluation
    metrics”](ch04.xhtml#using_the_right_evaluation_metrics) for a refresh on the
    asymmetry of F1.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: ^([40](ch06.xhtml#ch01fn189-marker)) 重新查看部分 [“使用正确的评估指标”](ch04.xhtml#using_the_right_evaluation_metrics)，了解F1的不对称性。
- en: ^([41](ch06.xhtml#ch01fn190-marker)) Other examples of noisy data include images
    with different lighting or texts with accidental typos or intentional text modifications
    such as typing “long” as “loooooong.”
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: ^([41](ch06.xhtml#ch01fn190-marker)) 其他噪声数据的示例包括光照不同的图像或意外拼写错误或意图修改文本，例如将“long”打成“loooooong”。
- en: ^([42](ch06.xhtml#ch01fn191-marker)) Khristopher J. Brooks, “Disparity in Home
    Lending Costs Minorities Millions, Researchers Find,” *CBS News*, November 15,
    2019, [*https://oreil.ly/TMPVl*](https://oreil.ly/TMPVl).
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: ^([42](ch06.xhtml#ch01fn191-marker)) Khristopher J. Brooks，《少数族裔居住贷款成本差异化问题的研究》，*CBS新闻*，2019年11月15日，[*https://oreil.ly/TMPVl*](https://oreil.ly/TMPVl)。
- en: ^([43](ch06.xhtml#ch01fn192-marker)) It might also be mandated by law to exclude
    sensitive information from the model training process.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: ^([43](ch06.xhtml#ch01fn192-marker)) 在模型训练过程中，可能也会被法律要求排除敏感信息。
- en: ^([44](ch06.xhtml#ch01fn193-marker)) For more information on calibrated recommendations,
    check out the paper [“Calibrated Recommendations”](https://oreil.ly/yueHR) by
    Harald Steck in 2018 based on his work at Netflix.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: ^([44](ch06.xhtml#ch01fn193-marker)) 欲了解更多关于校准推荐的信息，请参阅2018年由Harald Steck在Netflix工作期间发表的论文
    [“校准推荐”](https://oreil.ly/yueHR)。
- en: ^([45](ch06.xhtml#ch01fn194-marker)) Maggie Zhang, “Google Photos Tags Two African-Americans
    As Gorillas Through Facial Recognition Software,” *Forbes*, July 1, 2015, [*https://oreil.ly/VYG2j*](https://oreil.ly/VYG2j).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: ^([45](ch06.xhtml#ch01fn194-marker)) Maggie Zhang，《Google照片通过面部识别软件将两名非洲裔美国人标记为大猩猩》，*福布斯*，2015年7月1日，[*https://oreil.ly/VYG2j*](https://oreil.ly/VYG2j)。
- en: '^([46](ch06.xhtml#ch01fn196-marker)) P. J. Bickel, E. A. Hammel, and J. W.
    O’Connell, “Sex Bias in Graduate Admissions: Data from Berkeley,” *Science* 187
    (1975): 398–404, [*https://oreil.ly/TeR7E*](https://oreil.ly/TeR7E).'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '^([46](ch06.xhtml#ch01fn196-marker)) P. J. Bickel, E. A. Hammel 和 J. W. O’Connell，《伯克利的研究生录取中的性别偏见：数据来自科学》187
    (1975): 398–404，[*https://oreil.ly/TeR7E*](https://oreil.ly/TeR7E)。'
- en: ^([47](ch06.xhtml#ch01fn197-marker)) For readers interested in learning more
    about UX design across cultures, Jenny Shen has a [great post](https://oreil.ly/MAJVB).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: ^([47](ch06.xhtml#ch01fn197-marker)) 对于希望了解跨文化用户体验设计的读者，Jenny Shen在她的[优秀文章](https://oreil.ly/MAJVB)中有更多信息。
