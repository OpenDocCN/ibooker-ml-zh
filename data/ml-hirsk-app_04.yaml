- en: Chapter 2\. Interpretable and Explainable Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章 可解释和可解释的机器学习
- en: Scientists have been fitting models to data to learn more about observed patterns
    for centuries. Explainable machine learning models and post hoc explanation of
    ML models present an incremental, but important, advance in this long-standing
    practice. Because ML models learn about nonlinear, faint, and interacting signals
    more easily than traditional linear models, humans using explainable ML models
    and post hoc explanation techniques can now also learn about nonlinear, faint,
    and interacting signals in their data with more ease.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 科学家们几个世纪以来一直在将模型拟合到数据中，以更多地了解观察到的模式。可解释的机器学习模型和ML模型的事后解释呈现了这种长期实践中的渐进但重要的进展。因为ML模型更容易学习非线性、微弱和交互信号，所以使用可解释的ML模型和事后解释技术的人现在也能更轻松地学习他们数据中的非线性、微弱和交互信号。
- en: In this chapter, we’ll dig into important ideas for interpretation and explanation
    before tackling major explainable modeling and post hoc explanation techniques.
    We’ll cover the major pitfalls of post hoc explanation too—many of which can be
    overcome by using explainable models and post hoc explanation *together*. Next
    we’ll discuss applications of explainable models and post hoc explanation, like
    model documentation and actionable recourse for wrong decisions, that increase
    accountability for AI systems. The chapter will close with a case discussion of
    the so called “A-level scandal” in the United Kingdom (UK), where an explainable,
    highly documented model made unaccountable decisions, resulting in a nationwide
    AI incident. The discussion of explainable models and post hoc explanation continues
    in Chapters [6](ch06.html#unique_chapter_id_6) and [7](ch07.html#unique_chapter_id_7),
    where we explore two in-depth code examples related to these topics.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将深入探讨在进行主要可解释建模和事后解释技术之前，关于解释和说明的重要思想。我们还将涵盖事后解释的主要陷阱——其中许多可以通过同时使用可解释模型和事后解释来克服。接下来，我们将讨论可解释模型和事后解释的应用，如模型文档和对错误决策的可行回溯，这些增加了对AI系统的问责。本章将以讨论英国的所谓“A级考试丑闻”作为结尾，这是一个全国性的AI事件，其中一个可解释性强、高度文档化的模型做出了不负责任的决策。关于可解释模型和事后解释的讨论将在第[6](ch06.html#unique_chapter_id_6)章和第[7](ch07.html#unique_chapter_id_7)章继续进行，我们将探讨与这些主题相关的两个深入的代码示例。
- en: Important Ideas for Interpretability and Explainability
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重要的解释性和可解释性思想
- en: 'Before jumping into the techniques for training explainable models and generating
    post hoc explanations, we need to discuss the big ideas behind the math and code.
    We’ll start by affirming that transparency does not equate to trust. We can trust
    things that we don’t understand, and understand things we don’t trust. Stated
    even more simply: transparency enables understanding, and understanding is different
    from trust. In fact, greater understanding of poorly built ML systems may actually
    decrease trust.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入讨论训练可解释模型和生成事后解释技术之前，我们需要讨论数学和代码背后的重要思想。我们将从确认透明性不等同于信任开始。我们可以信任我们不理解的事物，并理解我们不信任的事物。更简单地说：透明性促进理解，而理解与信任是不同的。事实上，对于建立不良ML系统的更深入理解实际上可能会降低信任。
- en: 'Trustworthiness in AI is defined by the National Institute of Standards and
    Technology (NIST) using various characteristics: validity, reliability, safety,
    managed bias, security, resiliency, transparency, accountability, explainability,
    interpretability, and enhanced privacy. Transparency makes it easier to achieve
    other desirable trustworthiness characteristics and makes debugging easier. However,
    human operators must take these additional governance steps. Trustworthiness is
    often achieved in practice through testing, monitoring, and appeal processes (see
    Chapters [1](ch01.html#unique_chapter_id_1) and [3](ch03.html#unique_chapter_id_3)).
    Increased transparency enabled by explainable ML models and post hoc explanation
    should facilitate the kinds of diagnostics and debugging that help to make traditional
    linear models trustworthy. It also means that regulated applications that have
    relied on linear models for decades due to per-consumer explanation and general
    documentation requirements are now likely ripe for disruption with accurate and
    transparent ML models.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的可信度由国家标准技术研究所（NIST）定义，使用各种特征：有效性、可靠性、安全性、管理偏差、安全性、弹性、透明度、责任制、可解释性、可解释性和增强隐私。透明度使得实现其他理想的可信度特征更加容易，并且使调试变得更加简单。然而，人类操作员必须采取这些额外的治理步骤。可信度通常通过测试、监控和上诉流程实现（参见第[1](ch01.html#unique_chapter_id_1)章和第[3](ch03.html#unique_chapter_id_3)章）。由可解释的ML模型和事后解释增强的透明度应该促进那些有助于使传统线性模型可信的诊断和调试。这也意味着几十年来依赖于线性模型的受监管应用，由于每个消费者的解释和一般文档要求，现在很可能对准确和透明的ML模型充满了机遇。
- en: Warning
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Being interpretable, explainable, or otherwise transparent does not make a model
    good or trustworthy. Being interpretable, explainable, or otherwise transparent
    enables humans to make a highly informed decision as to whether a model is good
    or trustworthy.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 是否可解释、可解释或其他透明并不意味着模型是好的或可信的。是否可解释、可解释或其他透明使人类能够对模型是否良好或可信做出高度的知情决策。
- en: Redress for subjects of wrong ML-based decisions via prescribed appeal and override
    processes is perhaps the most important trust-enhancing use of explainable models
    and post hoc explanation. The ability to logically appeal automated decisions
    is sometimes called *actionable recourse*. It’s very hard for consumers—or job
    applicants, patients, prisoners, or students—to appeal automated and unexplainable
    decisions. Explainable ML models and post hoc explanation techniques should allow
    for ML-based automated decisions to be understood by decision subjects, which
    is the first step in a logical appeal process. Once users can demonstrate that
    either their input data or the logic of their decision is wrong, operators of
    ML systems should be able to override the initial errant decision.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指定的上诉和覆盖流程对错用基于ML的决策的对象进行补偿，可能是提高信任的最重要的使用可解释模型和事后解释。合理上诉自动化决策的能力有时被称为*可操作的追索权*。对于消费者——或求职者、患者、囚犯或学生来说，要对自动化和不可解释的决策提出上诉是非常困难的。可解释的ML模型和事后解释技术应允许决策对象理解基于ML的自动化决策，这是逻辑上诉流程的第一步。一旦用户能够证明他们的输入数据或决策逻辑是错误的，ML系统的操作者应该能够覆盖最初的错误决策。
- en: Note
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Mechanisms that enable appeal and override of automated decisions should always
    be deployed with high-risk ML systems.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 使得申诉和覆盖自动决策的机制应始终与高风险的ML系统一起部署。
- en: 'It’s also important to differentiate between interpretability and explanation.
    In the groundbreaking study [“Psychological Foundations of Explainability and
    Interpretability in Artificial Intelligence”](https://oreil.ly/yz6GF), researchers
    at NIST were able to differentiate between interpretability and explainability
    using widely accepted notions of human cognition. According to NIST researchers,
    the similar, but not identical, concepts of interpretation and explanation are
    defined as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 区分解释性和解释之间的差异也很重要。在开创性研究中[“解释性与人工智能中的解释能力的心理基础”](https://oreil.ly/yz6GF)，NIST的研究人员能够使用广泛接受的人类认知概念来区分解释性和解释能力。根据NIST的研究人员，解释性和解释能力的类似但不完全相同的概念被定义如下：
- en: Interpretation
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 解释
- en: A high-level, meaningful mental representation that contextualizes a stimulus
    and leverages human background knowledge. An interpretable model should provide
    users with a description of what a data point or model output means *in context*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一种高层次、有意义的心理表征，将刺激置于上下文中并利用人类背景知识。一个可解释的模型应该为用户提供数据点或模型输出在*上下文中*的描述。
- en: Explanation
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 说明
- en: A low-level, detailed mental representation that seeks to describe a complex
    process. An ML explanation is a description of how a particular model mechanism
    or output *came to be*.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一种低层次、详细的心理表征，试图描述一个复杂过程。机器学习解释是描述特定模型机制或输出*形成过程*的说明。
- en: Note
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注释
- en: Interpretability is a much higher bar to reach than explainability. Achieving
    interpretability means putting an ML mechanism or result in context, and this
    is rarely achieved through models or post hoc explanations. Interpretability is
    usually achieved through plainly written explanations, compelling visualizations,
    or interactive graphical user interfaces. Interpretability usually requires working
    with experts both within the applicable domain and in user interaction and experience,
    as well as other interdisciplinary professionals.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 解释性远比可解释性更高的门槛。实现解释性意味着将机器学习机制或结果置于背景中，这通常无法通过模型或事后解释来实现。解释性通常通过简明的文字说明、引人入胜的可视化或交互式图形用户界面来实现。解释性通常需要与适用领域内和用户互动经验中的专家以及其他跨学科专业人士合作。
- en: 'Let’s get more granular now, and touch on some of what makes an ML model interpretable
    or explainable. It’s very difficult for models achieve transparency if their input
    data is a mess. Let’s start our discussion there. When considering input data
    and transparency, think about the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们更加详细地探讨一些使机器学习模型可解释或可解释的因素。如果输入数据混乱，模型要想实现透明度就非常困难。我们从这里开始讨论。在考虑输入数据和透明度时，请考虑以下内容：
- en: Explainable feature engineering
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释的特征工程
- en: We should avoid overly complex feature engineering if our goal is transparency.
    While deep features from autoencoders, principal components, or high-degree interactions
    might make our model perform better in test data, explaining such features is
    going to be difficult even if we feed them into an otherwise explainable model.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的目标是透明度，我们应避免过于复杂的特征工程。尽管来自自编码器、主成分或高阶交互的深度特征可能使我们的模型在测试数据中表现更好，但即使我们将它们输入到一个本质上可解释的模型中，解释这些特征也将是困难的。
- en: Meaningful features
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有意义的特征
- en: Using a feature as an input to some model function assumes it is related to
    the function output, i.e., the model’s predictions. Using nonsensical or loosely
    related features because they improve test data performance violates a fundamental
    assumption of the way explaining models works. For example, if along with other
    features we use eye color to predict credit default, the model will likely train
    to some convergence criterion and we can calculate Shapley additive explanation
    (SHAP) values for eye color. But eye color has no real validity in this context
    and is not be causally related to credit default. While eye color may serve as
    a proxy for underlying systemic biases, claiming that eye color explains credit
    default is invalid. Apply commonsense—or even better, causal—discovery approaches
    to increase the validity of models and associated explanations.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使用特征作为某些模型函数的输入假设它与函数输出，即模型的预测相关。因为它们提高了测试数据性能，使用无意义或松散相关的特征违反了解释模型运作方式的基本假设。例如，如果除了其他特征外，我们使用眼睛颜色来预测信用违约，模型可能会收敛到某个收敛标准，我们可以计算Shapley增加解释（SHAP）值来评估眼睛颜色。但在这种情况下，眼睛颜色在此语境中没有真正的有效性，与信用违约也没有因果关系。虽然眼睛颜色可能作为潜在系统偏见的代理，但声称眼睛颜色解释信用违约是无效的。应用常识或更好的因果发现方法来增加模型及相关解释的有效性。
- en: Monotonic features
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 单调特征
- en: Monotonicity helps with explainability. When possible, use features that have
    a monotonic relationship with the target variable. If necessary, apply a feature
    engineering technique such as binning to induce monotonicity.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 单调性有助于解释性。在可能的情况下，使用与目标变量呈单调关系的特征。如果需要，应用像分箱这样的特征工程技术来诱导单调性。
- en: 'If we can rely on our data being usable for explainability and interpretability
    purposes, then we can use concepts like additive independence of inputs, constraints,
    linearity and smoothness, prototypes, sparsity, and summarization to ensure our
    model is as transparent as possible:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们可以依赖我们的数据用于解释和可解释性目的，那么我们可以使用添加性输入的概念、约束、线性和平滑性、原型、稀疏性和总结，以确保我们的模型尽可能透明：
- en: Additivity of inputs
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 添加性输入
- en: Keeping inputs in an ML model separate, or limiting their interaction to a small
    group, is crucial for transparency. Traditional ML models are (in)famous for combining
    and recombining input features into an undecipherable tangle of high-degree interactions.
    In stark contrast, traditional linear models treat inputs in an independent and
    additive fashion. The output decision from a linear model is typically the simple
    linear combination of learned model parameters and input feature values. Of course,
    performance quality is often noticeably worse for traditional linear models as
    compared to traditional opaque ML models.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入保持在ML模型中分离，或将它们的互动限制在一个小组内，对于透明度至关重要。传统的ML模型因将输入特征组合和重新组合成难以解开的高次交互而臭名昭著。与此形成鲜明对比的是，传统的线性模型以独立和添加的方式处理输入。从线性模型中得出的输出决策通常是学习模型参数和输入特征值的简单线性组合。当然，与传统的不透明ML模型相比，传统的线性模型的性能质量通常显著较差。
- en: Enter the generalized additive model (GAM). GAMs keep input features independent,
    enabling transparency, but also allow for arbitrarily complex modeling of each
    feature’s behavior, dramatically increasing performance quality. We’ll also be
    discussing GAM’s close descendants, GA2Ms and explainable boosting machines (EBMs),
    in the next section. They all work by keeping inputs independent and enabling
    visualization of the sometimes complex manner in which they treat those individual
    inputs. In the end, they retain high transparency because users don’t have to
    disentangle inputs and how they affect one another, and the output decision is
    still a linear combination of learned model parameters and some function applied
    to data input values.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 进入广义可加模型（GAM）。 GAM保持输入特征独立，实现透明度，但也允许对每个特征行为进行任意复杂建模，极大提高性能质量。我们还将在下一节讨论GAM的近亲GA2Ms和可解释增强机器（EBMs）。它们都通过保持输入独立并允许可视化处理这些单独输入的复杂方式来工作。最终，它们保持高透明度，因为用户不必解开输入及其相互影响的复杂性，输出决策仍然是学习模型参数的简单线性组合和应用于数据输入值的某些函数。
- en: Constraints
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 约束
- en: Traditional unexplainable ML models are revered for their flexibility. They
    can model almost any signal-generating function in training data. Yet, when it
    comes to transparency, modeling every nook and cranny of some observed response
    function usually isn’t a great idea. Due to overfitting, it turns out it’s also
    not a great idea for performance on unseen data either. Sometimes what we observe
    in training data is just noise, or even good old-fashioned wrong. So, instead
    of overfitting to bad data, it’s often a good idea to apply constraints that both
    increase transparency and help with performance on unseen data by forcing models
    to obey causal concepts instead of noise.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的不可解释ML模型因其灵活性而受到推崇。它们可以在训练数据中建模几乎任何信号生成函数。然而，当涉及透明度时，通常不是一个好主意去模拟观察到的响应函数的每一个细微之处。由于过拟合，结果表明，对未见数据的性能也不是一个好主意。有时，我们在训练数据中观察到的只是噪声，甚至是老式错误。因此，与其过度拟合错误数据，往往更明智的做法是应用约束，通过迫使模型遵守因果概念而增加透明度，并帮助处理未见数据的性能。
- en: Any number of constraints can be applied when training ML models, but some of
    the most helpful and widely available are sparsity, monotonicity, and interaction
    constraints. Sparsity constraints, often implemented by L1 regularization (which
    usually decreases the number of parameters or rules in a model), increases emphasis
    on a more manageable number of input parameters and internal learning mechanisms.
    Positive monotonic constraints mean that as a model’s input increases, its output
    must never decrease. Negative monotonic constraints ensure that as a model’s input
    increases, its output can never increase. Interaction constraints keep internal
    mechanisms of ML models from combining and recombining too many different features.
    These constraints can be used to encourage models to learn interpretable and explainable
    causal phenomena, as opposed to focusing on nonrobust inputs, arbitrary nonlinearities,
    and high-degree interactions that may be drivers of errors and bias in model outcomes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练 ML 模型时可以应用任意数量的约束，但其中一些最有帮助和广泛可用的包括稀疏性、单调性和交互约束。稀疏性约束通常通过 L1 正则化实现（通常减少模型中的参数或规则数量），增强对更可管理的输入参数和内部学习机制的重视。正单调性约束意味着随着模型的输入增加，其输出绝不能减少。负单调性约束确保随着模型的输入增加，其输出永远不会增加。交互约束防止
    ML 模型的内部机制过度组合和重新组合过多不同特征。这些约束可以用来鼓励模型学习可解释的因果现象，而不是关注不稳健的输入、任意的非线性和可能是模型结果错误和偏差驱动因素的高度交互。
- en: Linearity and smoothness
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 线性和平滑性
- en: Linear functions are monotonic by default and can be described by single numeric
    coefficients. Smooth functions are differentiable, meaning they can be summarized
    at any location with a derivative function or value. Basically, linear and smooth
    functions are better behaved and typically easier to summarize. In contrast, unconstrained
    and arbitrary ML functions can bounce around or bend themselves in knots in ways
    that defy human understanding, making the inherent summarization that needs to
    occur for explanation nearly impossible.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 线性函数默认是单调的，可以用单一数值系数描述。平滑函数是可微的，意味着可以通过导数函数或值在任何位置进行概述。基本上，线性和平滑函数行为更良好，通常更容易概述。相比之下，无约束和任意的
    ML 函数可能以不符合人类理解的方式跳跃或打结，使得需要进行的内在概述几乎不可能。
- en: Prototypes
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 原型
- en: Prototypes refer to well-understood data points (rows) or archetypal features
    (columns) that can be used to explain model outputs for some other previously
    unseen data. Prototypes appear in many places in ML, and have been used to explain
    and interpret model decisions for decades. Think of explaining a *k*-nearest neighbors
    prediction using the nearest neighbors or profiling clusters based on centroid
    locations—those are prototypes. Prototypes also end up being important for counterfactual
    explanations. Prototypes have even entered into the typically complex and opaque
    world of computer vision with [this-looks-like-that deep learning](https://oreil.ly/zO1kx).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 原型指的是可以用于解释先前未见数据的模型输出的良好理解数据点（行）或典型特征（列）。几十年来，原型已经出现在许多 ML 中，并且已被用来解释和解读模型决策。比如使用最近邻居来解释
    *k*-最近邻预测，或者基于质心位置对聚类进行描述—这些都是原型。原型在计算机视觉的典型复杂和不透明世界中也变得重要，比如[这个看起来像那个的深度学习](https://oreil.ly/zO1kx)。
- en: Sparsity
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏性
- en: ML models, for better or worse, can now be trained with [trillions of parameters](https://oreil.ly/mDwV5).
    Yet, it’s debatable whether their human operators can reason based on more than
    a few dozen of those parameters. The volume of information in a contemporary ML
    model will have to be summarized to be transparent. Generally, the fewer coefficients
    or rules in an ML model, the more sparse and explainable it is.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ML 模型，不管好坏，现在可以通过[数万亿个参数](https://oreil.ly/mDwV5)来训练。然而，它们的人类操作者能否根据超过几十个参数进行推理尚存在争议。当代
    ML 模型中的信息量必须被概述以保持透明。一般来说，ML 模型中的系数或规则越少，它就越稀疏和可解释。
- en: Summarization
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要
- en: Summarization can take many forms, including variable importance measures, surrogate
    models, and other post hoc ML approaches. Visualization is perhaps the most common
    vehicle for communicating summarized information about ML models, and approximation
    taken to compress information is the [Achilles’ heel](https://oreil.ly/Dzfit)
    for summarization. Additive, linear, smooth, and sparse models are generally easier
    to summarize, and post hoc explanation processes have a better chance of working
    well in these cases.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 总结可以采用多种形式，包括变量重要性度量、代理模型和其他事后机器学习方法。可视化可能是传达关于机器学习模型概述信息的最常见方式，而压缩信息的近似方法是总结的[致命弱点](https://oreil.ly/Dzfit)。加性、线性、平滑和稀疏模型通常更容易总结，而事后解释过程在这些情况下更有可能有效。
- en: Achieving transparency with ML takes some extra elbow grease as compared to
    popular and commoditized unexplainable approaches. Fear not. The recent paper
    [“Designing Inherently Interpretable Machine Learning Models”](https://oreil.ly/Zv6YO)
    and software packages like [InterpretML](https://oreil.ly/rML2n), [H2O](https://oreil.ly/ysEHE),
    and [PiML](https://oreil.ly/Y2EFl) provide a great framework for training and
    explaining transparent models, and the remaining sections of the chapter highlight
    the most effective technical approaches, and common gotchas, for us to consider
    next time we start to design an AI system.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 达到机器学习的透明度与流行和商品化的不可解释方法相比需要额外的努力。不要担心。最近的论文[“设计天然可解释的机器学习模型”](https://oreil.ly/Zv6YO)以及像[InterpretML](https://oreil.ly/rML2n)、[H2O](https://oreil.ly/ysEHE)和[PiML](https://oreil.ly/Y2EFl)等软件包提供了一个很好的框架，用于训练和解释透明模型。本章的其余部分将重点介绍最有效的技术方法和常见的陷阱，以便我们在下次设计AI系统时考虑。
- en: Explainable Models
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可解释模型
- en: For decades, many ML researchers and practitioners alike labored under a seemingly
    logical assumption that more complex models were more accurate. However, as pointed
    out by luminary professor Cynthia Rudin, in her impactful [“Stop Explaining Black
    Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models
    Instead”](https://oreil.ly/i4syT), “It is a myth that there is necessarily a trade-off
    between accuracy and interpretability.” We’ll dig into the tension around explainable
    models versus post hoc explanation later in the chapter. For now, let’s concentrate
    on the powerful idea of accurate and explainable models. They offer the potential
    for highly accurate decision making coupled with improved human learning from
    machine learning, actionable recourse processes and regulatory compliance, improved
    security, and better ability to address various issues, including inaccuracy and
    bias. These appealing characteristics make explainable ML models a general win-win
    for practitioners and consumers.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 几十年来，许多机器学习研究人员和从业者都努力工作在一个看似合乎逻辑的假设下，即更复杂的模型更准确。然而，正如杰出教授辛西娅·鲁丁在她影响深远的[“停止解释黑盒机器学习模型进行重大决策，改用可解释模型”](https://oreil.ly/i4syT)中指出的，“认为准确性和可解释性之间必然存在权衡是一种错误”。我们稍后将探讨关于可解释模型与事后解释之间的紧张关系。现在，让我们集中关注准确且可解释模型的强大理念。它们为高度准确的决策提供了潜力，同时改善了从机器学习中学习的人类、可操作的追索过程和监管合规性、改善安全性，以及更好地解决包括不准确性和偏见在内的各种问题。这些吸引人的特征使得可解释的机器学习模型对从业者和消费者来说都是一种普遍的双赢局面。
- en: We’re going to go over some of the most popular types of explainable ML models
    next. We’ll start with the large class of additive models, including penalized
    regression, GAMs, GA2Ms and EBMs. We’ll also be covering decision trees, constrained
    tree ensembles, and a litany of other options before moving on to post hoc explanation
    techniques.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍一些最流行的可解释机器学习模型类型。我们将从大类加性模型开始，包括惩罚回归、广义加性模型、广义可加性模型和解释性基础模型。我们还将涵盖决策树、约束树集成和其他选项，然后再讨论事后解释技术。
- en: Additive Models
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加性模型
- en: 'Perhaps the most widely used types of explainable ML models are those based
    on traditional linear models: penalized regression models, GAMs, and GA2Ms (or
    EBMs). These techniques use contemporary methods to augment traditional modeling
    approaches, often yielding noticeable improvements in performance. But they also
    keep interpretability high by treating input features in a separate and additive
    manner, or by allowing only a small number of interaction terms. They also rely
    on straightforward visualization techniques to enhance interpretability.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最广泛使用的可解释机器学习模型之一是基于传统线性模型的模型：惩罚回归模型、广义加性模型（GAMs）、以及GA2Ms（或EBMs）。这些技术通过现代方法增强了传统建模方法，通常显著提升了性能。但它们也通过单独和累加的方式处理输入特征，或仅允许少量交互项来保持解释性高。它们还依赖直观的可视化技术来增强可解释性。
- en: Penalized regression
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 惩罚回归
- en: We’ll start our discussion of explainable models with penalized regression.
    Penalized regression updates the typical 19th century approach to regression for
    the 21st century. These types of models usually produce linear, monotonic response
    functions with globally explainable results, like those of traditional linear
    models, but often with a boost in predictive performance. Penalized regression
    models eschew the assumption-laden [Normal Equation](https://oreil.ly/uXdeH) approaches
    to finding model parameters, and instead use more sophisticated constrained and
    iterative optimization procedures that allow for handling of correlation, feature
    selection, and treatment of outliers, all while using validation data to pick
    a better model automatically.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从惩罚回归开始讨论可解释模型。惩罚回归更新了19世纪的典型回归方法，使之适应21世纪。这类模型通常生成线性、单调响应函数，并产生类似传统线性模型的全局可解释结果，但通常具有预测性能的提升。惩罚回归模型避免了依赖假设的[正规方程](https://oreil.ly/uXdeH)方法来找到模型参数，而是采用更复杂的约束和迭代优化过程，允许在使用验证数据自动选择更好模型的同时处理相关性、特征选择和异常值处理。
- en: In [Figure 2-1](#elastic_net), we can see how a penalized regression model learns
    optimal coefficients for six input features over more than 80 training iterations.
    We can see at the beginning of the optimization procedure that all parameter values
    start very small, and grow as the procedure begins to converge. This happens because
    it’s typical to start the training procedure with large penalties on input features.
    These penalties are typically decreased as training proceeds to allow a small
    number of inputs to enter the model, to keep model parameters artificially small,
    or both. (Readers may also hear penalized regression coefficients referred to
    as “shrunken” on occasion.) At each iteration, as more features enter the model,
    or as coefficient values grow or change, the current model is applied to validation
    data. Training continues to a predefined number of iterations or until performance
    in validation data ceases to improve.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图2-1](#elastic_net)中，我们可以看到惩罚回归模型在80多个训练迭代中学习了六个输入特征的最优系数。我们可以看到在优化过程开始时，所有参数值都非常小，并且随着过程的收敛而增长。这是因为通常在训练过程中会在输入特征上设置大的惩罚。这些惩罚通常会随着训练的进行而减少，以允许少量输入进入模型，或者保持模型参数人为地小，或两者兼而有之。（读者在某些情况下也可能听到惩罚回归系数被称为“缩小”的说法。）在每次迭代中，当更多特征进入模型，或系数值增长或变化时，当前模型将应用于验证数据。训练将继续进行预定义数量的迭代，或者直到验证数据的性能停止改进为止。
- en: '![mlha 0201](assets/mlha_0201.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![mlha 0201](assets/mlha_0201.png)'
- en: Figure 2-1\. Regularization paths for selected features in an elastic net regression
    model ([digital, color version](https://oreil.ly/dR7Ty))
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1\. 弹性网络回归模型中选择特征的正则化路径（[数字，彩色版本](https://oreil.ly/dR7Ty)）
- en: 'In addition to a validation-based early stopping procedure, penalized regression
    addresses outliers, feature selection, correlated inputs, and nonlinearity by
    using, respectively, the iteratively reweighted least squares (IRLS) technique;
    two types of penalties, on the L1 and L2 parameter norms; and link functions:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基于验证的早停止程序外，惩罚回归还通过迭代重新加权最小二乘（IRLS）技术、L1和L2参数范数的两种类型的惩罚，以及链接函数，来处理异常值、特征选择、相关输入和非线性。
- en: IRLS
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: IRLS
- en: IRLS is a well-established procedure for minimizing the effect of outliers.
    It starts like an old-fashioned regression, but after that first iteration, the
    IRLS procedure checks which rows of input data are leading to large errors. It
    then reduces the weight of those rows in subsequent iterations of fitting model
    coefficients. IRLS continues this fitting and down-weighting procedure until model
    parameters converge to stable values.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: IRLS是一种旨在最小化异常值影响的成熟过程。它开始像传统的回归，但在第一次迭代后，IRLS过程检查哪些输入数据行导致较大的误差。然后在拟合模型系数的后续迭代中减少这些行的权重。IRLS持续执行这种拟合和降低权重的过程，直到模型参数收敛到稳定值。
- en: L1 norm penalty
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: L1范数惩罚
- en: Also known as least absolute shrinkage and selection operator (LASSO), L1 penalties
    keep the sum of the absolute model parameters to a minimum. This penalty has the
    effect of driving unnecessary regression parameters to zero, and selecting a small,
    representative subset of features for the regression model, while also avoiding
    the potential multiple comparison problems that arise in older stepwise feature
    selection. When used alone, L1 penalties are known to increase performance quality
    in situations where a large number of potentially correlated input features cause
    stepwise feature selection to fail.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 也称为最小绝对收缩和选择算子（LASSO），L1惩罚将绝对模型参数的总和最小化。这种惩罚效果使得不必要的回归参数趋向于零，并为回归模型选择一个小而代表性的特征子集，同时避免在旧的逐步特征选择中出现的潜在多重比较问题。当单独使用时，L1惩罚已知可以提高性能质量，特别是在大量潜在相关的输入特征导致逐步特征选择失败的情况下。
- en: L2 norm penalty
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: L2范数惩罚
- en: Also known as ridge or Tikhonov regression, L2 penalties minimize the sum of
    squared model parameters. L2 norm penalties stabilize model parameters, especially
    in the presence of correlation. Unlike L1 norm penalties, L2 norm penalties do
    not select features. Instead, they limit each feature’s impact on the overall
    model by keeping all model parameters smaller than they would be in the traditional
    solution. Smaller parameters make it harder for any one feature to dominate a
    model and for correlation to cause strange behavior during model training.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 也称为岭回归或Tikhonov回归，L2惩罚最小化模型参数的平方和。L2范数惩罚稳定模型参数，尤其是在存在相关性时。与L1范数惩罚不同，L2范数惩罚不选择特征。相反，它们通过保持所有模型参数小于传统解决方案中的参数来限制每个特征对整体模型的影响。较小的参数使得任何一个特征难以主导模型，并且在模型训练过程中，相关性不会引起奇怪的行为。
- en: Link functions
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 链接函数
- en: Link functions enable linear models to handle common distributions of training
    data, such as using a logit link function to fit a logistic regression to input
    data with two discrete outcomes. Other common and useful link functions include
    the Poisson link function for count data or an inverse link function for gamma-distributed
    outputs. Matching outcomes to their distribution family and link function, such
    as a binomial distribution and logit link function for logistic regression, is
    absolutely necessary for training deployable models. Many ML models and libraries
    outside of penalized regression packages do not support the necessary link functions
    and distribution families to address fundamental assumptions in training data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 链接函数使线性模型能够处理训练数据的常见分布，例如使用logit链接函数将逻辑回归拟合到具有两个离散结果的输入数据中。其他常见且有用的链接函数包括Poisson链接函数用于计数数据或伽马分布输出的反向链接函数。将结果与其分布族和链接函数匹配，例如二项分布和逻辑回归的logit链接函数，对于训练可部署的模型是绝对必要的。许多机器学习模型和库在惩罚回归软件包之外不支持必要的链接函数和分布族，以解决训练数据中的基本假设。
- en: 'Contemporary penalized regression techniques usually combine the following:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当代惩罚回归技术通常结合以下内容：
- en: Validation-based early stopping for improved generalization
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于验证的早期停止以提高泛化能力
- en: IRLS to handle outliers
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理异常值的IRLS
- en: L1 penalties for feature selection purposes
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征选择目的的L1惩罚
- en: L2 penalties for robustness
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于稳健性的L2惩罚
- en: Link functions for various target or error distributions
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种目标或误差分布的链接函数
- en: Readers can learn more about penalized regression in [*Elements of Statistical
    Learning*](https://oreil.ly/FQjuv) (Springer), but for our purposes, it’s more
    important to know when we might want to try penalized regression. Penalized regression
    has been applied widely across many research disciplines, but it is a great fit
    for business data with many features, even datasets with more features than rows,
    and for datasets with a lot of correlated variables. Penalized regression models
    also preserve the basic interpretability of traditional linear models, so we think
    of them when we have many correlated features or need maximum transparency. It’s
    also important to know that penalized regression techniques don’t always create
    confidence intervals, *t*-statistics, or *p*-values for regression parameters.
    These types of measures are typically only available through bootstrapping, which
    can require extra computing time. The R packages [elasticnet](https://oreil.ly/aooJV)
    and [glmnet](https://oreil.ly/lFcpJ) are maintained by the inventors of the LASSO
    and elastic net regression techniques, and the [H2O generalized linear model](https://oreil.ly/Oeywm)
    sticks closely to the implementation of the original software, while allowing
    for much-improved scalability.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 读者可以在[*统计学习的要素*](https://oreil.ly/FQjuv)（Springer）中了解更多关于惩罚回归的内容，但对于我们的目的来说，更重要的是知道何时可以尝试惩罚回归。
    惩罚回归已经广泛应用于许多研究领域，但对于具有许多特征的商业数据来说尤为适用，即使是拥有更多特征而不是行的数据集，以及具有大量相关变量的数据集。 惩罚回归模型还保留了传统线性模型的基本可解释性，所以我们在有许多相关特征或需要最大透明度时会考虑它们。
    此外，了解到惩罚回归技术并不总是为回归参数创建置信区间、*t*统计量或*p*值。 这些类型的测量通常只能通过自举法获得，这可能需要额外的计算时间。 R软件包[elasticnet](https://oreil.ly/aooJV)和[glmnet](https://oreil.ly/lFcpJ)由LASSO和弹性网回归技术的发明者维护，而[H2O广义线性模型](https://oreil.ly/Oeywm)紧密遵循原始软件的实现，同时允许大大改进的可扩展性。
- en: An even newer and extremely interesting twist to penalized regression models
    is the super-sparse linear integer model, or [SLIM](https://oreil.ly/2YPmFX1).
    SLIMs also rely on sophisticated optimization routines, with the objective of
    creating accurate models that only require simple arithmetic to evaluate. SLIMs
    are meant to train linear models that can be evaluated mentally by humans working
    in high-risk settings, such as healthcare. If we’re ever faced with an application
    that requires the highest interpretability and the need for field-workers to evaluate
    results quickly, we think of SLIMs. On the other hand, if we’re looking for better
    predictive performance, to match that of many unexplainable ML techniques, but
    to keep a high degree of interpretability, think of GAMs—which we’ll be covering
    next.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于惩罚回归模型的一个新而极其有趣的转折是超稀疏线性整数模型，或者[SLIM](https://oreil.ly/2YPmFX1)。 SLIMs还依赖于复杂的优化程序，其目标是创建只需要简单算术即可评估的准确模型。
    SLIMs旨在训练线性模型，这些模型可以由在高风险环境中工作的人类进行心算评估，比如医疗保健。 如果我们面临需要最高可解释性和需要现场工作人员快速评估结果的应用程序，我们就会想到SLIMs。
    另一方面，如果我们正在寻找更好的预测性能，以匹配许多不可解释的机器学习技术，但同时保持高度解释性，那么就考虑GAMs——我们将在接下来介绍。
- en: Generalized additive models
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 广义可加模型
- en: GAMs are a generalization of linear models that allow a coefficient and function
    to be fit to each model input, instead of just a coefficient for each input. Training
    models in this manner allows for each input variable to be treated in a separate
    but nonlinear fashion. Treating each input separately keeps interpretability high.
    Allowing for nonlinearity improves performance quality. Traditionally GAMs have
    relied on splines to fit nonlinear shape functions for each input, and most implementations
    of GAMs generate convenient plots of the fitted shape functions. Depending on
    our regulatory or internal documentation requirements, we may be able to use the
    shape functions directly in predictive models for increased performance. If not,
    we may be able to eyeball some fitted shape functions and switch them out for
    a more explainable polynomial, log, trigonometric, or other simple function of
    the input feature that may also increase prediction quality. An interesting twist
    to GAMs was introduced recently with [neural additive models (NAMs)](https://oreil.ly/nnCz5)
    and [GAMI-Nets](https://oreil.ly/G_wCc). In these models, an artificial neural
    network is used to fit the shape functions. We’ll continue the theme of estimating
    shape functions with ML when we discuss explainable boosting machines in the next
    section. The Rudin Group also recently put forward a variant of GAMs in which
    monotonic step functions are used as shape functions for maximum interpretability.
    Check out [“Fast Sparse Classification for Generalized Linear and Additive Models”](https://oreil.ly/tCnld)
    to see those in action.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: GAM 是线性模型的一种泛化，允许为每个模型输入拟合系数和函数，而不仅仅是每个输入的系数。以这种方式训练模型允许每个输入变量以独立但非线性的方式处理。单独处理每个输入有助于保持解释性高。允许非线性改进了性能质量。传统上，GAM
    依赖样条来拟合每个输入的非线性形状函数，并且大多数 GAM 实现生成了拟合的形状函数的便利图表。根据我们的监管或内部文档要求，我们可以直接在预测模型中使用形状函数以提高性能。如果不能，我们可以直接查看一些拟合的形状函数，并将其替换为更可解释的多项式、对数、三角或其他简单的输入特征函数，这也可能提高预测质量。最近，通过
    [神经加法模型 (NAMs)](https://oreil.ly/nnCz5) 和 [GAMI-Nets](https://oreil.ly/G_wCc)
    引入了 GAM 的一个有趣变化。在这些模型中，人工神经网络用于拟合形状函数。在下一节讨论可解释性增强机器时，我们将继续主题估计形状函数的 ML。Rudin
    Group 最近还提出了 GAM 的一个变体，其中使用单调阶跃函数作为形状函数以实现最大解释性。查看 [“快速稀疏分类广义线性和加法模型”](https://oreil.ly/tCnld)
    以查看它们的实际效果。
- en: Note
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We’ll use the term *shape function* to describe the learned nonlinear relationship
    that GAM-like models apply to each input and interaction feature. These shape
    functions may be traditional splines, or they can be fit by machine learning estimators
    like boosted trees or neural networks.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用术语 *形状函数* 来描述 GAM 类似模型对每个输入和交互特征应用的学习非线性关系。这些形状函数可能是传统的样条，也可以是由机器学习估算器如增强树或神经网络拟合的。
- en: The semi-official name for the ability to change out parts of a model to better
    match reality or human intuition is *model editing*. Being editable is another
    important aspect of many explainable models. Models often learn wrong or biased
    concepts from wrong or biased training data. Explainable models enable human users
    to spot bugs, and edit them out. The GAM family of models is particularly amenable
    to model editing, which is another advantage of these powerful modeling approaches.
    Readers can learn more about GAMs in [*Elements of Statistical Learning*](https://oreil.ly/5S49T).
    To try GAMs, look into the R [gam](https://oreil.ly/aSAJK) package or the more
    experimental [H2O](https://oreil.ly/5yoAd) or [pyGAM](https://oreil.ly/1h-Y7)
    implementations.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 能够更改模型部分以更好地符合现实或人类直觉的能力的半官方名称是 *模型编辑*。可编辑性是许多可解释模型的另一个重要方面。模型经常从错误或偏见的训练数据中学习错误或偏见的概念。可解释模型使人类用户能够发现错误并予以修正。GAM
    模型家族特别适合模型编辑，这是这些强大建模方法的另一个优势。读者可以在 [*统计学习元素*](https://oreil.ly/5S49T) 中了解更多关于
    GAM 的信息。要尝试 GAM，请查看 R 的 [gam](https://oreil.ly/aSAJK) 包或更实验性的 [H2O](https://oreil.ly/5yoAd)
    或 [pyGAM](https://oreil.ly/1h-Y7) 实现。
- en: GA2M and explainable boosting machines
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GA2M 和可解释性增强机器
- en: The GA2M and EBM represent straightforward and material improvements to GAMs.
    Let’s address the GA2M first. The “2” in GA2M refers to the consideration of a
    small group of pairwise interactions as inputs to the model. Choosing to include
    a small number of interaction terms in the GAM boosts performance without compromising
    interpretability. Interaction terms can be plotted as contours along with the
    standard two-dimensional input feature plots that tend to accompany GAMs. Some
    astute readers may already be familiar with the EBM, an important variant of GA2M.
    In an EBM, the shape functions for each input feature are trained iteratively
    using boosting. These response functions can be splines, decision trees, or even
    boosted ensembles of decision trees themselves. By training the additive model
    using boosting, we can usually get a more accurate final model than if we use
    the typical GAM backfitting method.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: GA2M和EBM代表了GAM的直接和实质性改进。首先让我们来讨论GA2M。GA2M中的“2”指的是将少量成对交互作用作为模型输入的考虑。选择在GAM中包含少量交互项可以提升性能，而不会牺牲可解释性。交互项可以作为等高线图与通常伴随GAM的二维输入特征图一起绘制。一些敏锐的读者可能已经熟悉EBM，这是GA2M的重要变体。在EBM中，每个输入特征的形状函数通过增强学习迭代地进行训练。这些响应函数可以是样条函数、决策树，甚至是决策树的增强集成。通过使用增强学习训练加法模型，通常可以获得比使用典型的GAM反向适配方法更准确的最终模型。
- en: Because of these advances, GA2Ms and EBMs now rival, or exceed, unexplainable
    ML models in performance quality on tabular data, while also presenting the obvious
    advantages of interpretability and model editing. If your next project is on structured
    data, try out EBMs using the [InterpretML](https://oreil.ly/GMsbK) package from
    Microsoft Research. EBMs put the final touches on our discussion of additive models.
    We’ll move on next to decision trees, yet another type of high-quality, high-interpretability
    model with a track record of success across statistics, data mining, and ML.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些进展，GA2Ms和EBMs现在在表格数据的性能质量上与不可解释的机器学习模型相媲美，甚至有所超越，同时还具备可解释性和模型编辑的明显优势。如果您的下一个项目是结构化数据，请尝试使用微软研究的[InterpretML](https://oreil.ly/GMsbK)包来尝试EBMs。EBMs为我们讨论加性模型画上了句号。接下来我们将转向决策树，这是另一种高质量、高可解释性模型，在统计学、数据挖掘和机器学习领域都取得了成功记录。
- en: Decision Trees
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树
- en: Decision trees are another type of popular predictive model. When used as single
    trees, and not as ensemble models, they learn highly interpretable flowcharts
    from training data and tend to exhibit better predictive performance than linear
    models. When used as ensembles, as with random forests and GBMs, they lose interpretability
    but tend to be even better predictors. In the following sections, we’ll discuss
    both single tree models and constrained decision tree ensembles that can retain
    some level of explainability.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是另一种流行的预测模型。当作为单棵树使用时，它们从训练数据中学习高度可解释的流程图，并且往往比线性模型表现出更好的预测性能。当作为集成模型，例如随机森林和GBMs时，它们失去了可解释性，但往往成为更好的预测器。在接下来的几节中，我们将讨论单棵树模型和能保留一定可解释性的约束决策树集成。
- en: Single decision trees
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单决策树
- en: Technically, decision trees are directed graphs in which each interior node
    corresponds to an input feature. There are graph edges to child nodes for values
    of the input feature that create the highest target purity, or increased predictive
    quality, in each child node. Each terminal node or leaf node represents a value
    of the target feature, given the values of the input features represented by the
    path from the root to the leaf. These paths can be visualized or explained with
    simple if-then rules.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，决策树是有向图，其中每个内部节点对应一个输入特征。对于创建最高目标纯度或增强预测质量的输入特征值，有图边指向子节点。每个终端节点或叶节点代表目标特征的一个值，给定由根到叶的路径表示的输入特征的值。这些路径可以用简单的if-then规则进行可视化或解释。
- en: In plainer language, decision trees are data-derived flowcharts, just like we
    see in [Figure 2-2](#dt). Decision trees are great for training interpretable
    models on structured data. They are beneficial when the goal is to understand
    relationships between the input and target variable with Boolean-like “if-then”
    logic. They are also great at finding interactions in training data. Parent-child
    relationships, especially near the top of the tree, tend to point toward feature
    interactions that can be used to better understand drivers of the modeling target,
    or they can be used as interaction terms to boost predictive accuracy for additive
    models. Perhaps their main advantage over more straightforward additive models
    is their ability to train directly on character values, missing data, nonstandardized
    data, and nonnormalized data. In this age of big (er, bad) data, decision trees
    enable the construction of models with minimal data preprocessing, which can help
    eliminate additional sources of human error from ML models.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 更直观地说，决策树就是数据导出的流程图，就像我们在[图2-2](#dt)中看到的那样。决策树非常适合在结构化数据上训练可解释模型。当目标是理解输入和目标变量之间的关系，并使用布尔逻辑“if-then”时，它们非常有益。它们也非常擅长在训练数据中找到交互作用。特别是在树的顶部附近，父子关系往往指向可以用来更好理解建模目标驱动因素的特征交互，或者它们可以用作交互项以提高加法模型的预测准确性。也许它们相对于更直接的加法模型的主要优势在于，它们能够直接在字符值、缺失数据、非标准化数据和非归一化数据上进行训练。在这个大数据时代，决策树能够构建具有最小数据预处理的模型，这有助于消除机器学习模型中的额外人为误差来源。
- en: '![mlha 0202](assets/mlha_0202.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![mlha 0202](assets/mlha_0202.png)'
- en: Figure 2-2\. A simple decision tree model forming a data-derived flowchart
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-2\. 一个简单的决策树模型形成的数据导出流程图
- en: With all this going for them, why shouldn’t we use decision trees? First of
    all, they’re only interpretable when they’re shallow. Decision trees with more
    than, say, five levels of if-then branches become difficult to interpret. They
    also tend to perform poorly on unstructured data like sound, images, video, and
    text. Unstructured data has become the domain of deep learning and neural networks.
    Like many models, decision trees require a high degree of tuning. Decision trees
    have many hyperparameters, or settings, which must be specified using human domain
    knowledge, grid searches, or other hyperparameter tuning methods. Such methods
    are time-consuming at a minimum, and at worst, sources of bias and overfitting.
    Single decision trees are also unstable, in that adding a few rows of data to
    training or validation data and retraining can lead to a completely rearranged
    flowchart. Such instability is an Achilles’ heel for many ML models.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这些优点都是决策树的利大于弊，为什么不应该使用决策树呢？首先，只有在决策树较浅时才能解释。一旦决策树的if-then分支超过五层，解释就变得困难。此外，它们在处理声音、图像、视频和文本等非结构化数据时性能通常较差。非结构化数据已经成为深度学习和神经网络的领域。与许多模型一样，决策树需要进行高度调整。决策树具有许多超参数或设置，必须使用人类领域知识、网格搜索或其他超参数调整方法来指定。这些方法至少是耗时的，最坏的情况下，可能会导致偏见和过拟合。单个决策树也不稳定，添加几行数据到训练或验证数据并重新训练可能会导致完全重新排列的流程图。这种不稳定性是许多机器学习模型的致命弱点。
- en: 'Decision trees make locally optimal, or *greedy*, decisions each time they
    make a new branch or if-then rule. Other ML models use different optimization
    strategies, but the end result is the same: a model that’s not the best model
    for the dataset, but instead is one good candidate for the best model out of many,
    many possible options. This issue of many possible models for any given dataset
    has at least two names, [*the multiplicity of good models* and *the Rashomon effect*](https://oreil.ly/nNwFY).
    (The name [*Rashomon*](https://oreil.ly/wW-k5) is from a famous film in which
    witnesses describe the same murder differently.) The Rashomon effect is linked
    to another nasty problem, known as [underspecification](https://oreil.ly/V_DaF),
    in which hyperparameter tuning and validation-data-based model selection leads
    to a model that looks good in test scenarios but then flops in the real world.
    How do we avoid these problems with decision trees? Mainly by training single
    trees that we can see and examine, and for which we can ensure that their logic
    will hold up when they are deployed.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树在每次创建新分支或者if-then规则时都会做出局部最优（或*贪婪*）决策。其他机器学习模型采用不同的优化策略，但最终结果都是一样的：一个不是数据集最佳模型，而是在众多可能选项中的一个良好候选模型。对于任何给定数据集来说，可能存在多个良好模型的问题至少有两个名称，[*良好模型的多样性*和*拉瑟蒙效应*](https://oreil.ly/nNwFY)。（[*拉瑟蒙*](https://oreil.ly/wW-k5)这个名字来源于一部著名电影，其中证人们对同一起谋杀案描述不同。）拉瑟蒙效应与另一个令人头痛的问题相关，即[欠指定性](https://oreil.ly/V_DaF)，在这种情况下，超参数调整和基于验证数据的模型选择导致模型在测试场景中看起来很好，但在实际世界中却失败。我们如何避免决策树的这些问题呢？主要是通过训练可以看到和检查的单棵树，并确保它们的逻辑在部署时能够保持稳定。
- en: There are many variations on the broader theme of decision trees. For example,
    linear tree models fit a linear model in each terminal node of a decision tree,
    adding predictive capacity to standard decision trees while keeping all the explainability
    intact. Professor Rudin’s research group has introduced [optimal sparse decision
    trees](https://oreil.ly/4kGpW) as a response to the issues of instability and
    underspecification. Another answer to these problems is manual constraints informed
    by human domain knowledge. We’ll address constrained decision tree ensembles next,
    but if readers are ready to try standard decision trees, there are many packages
    to try, and the R package [rpart](https://oreil.ly/XLE1H) is one of the best.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的更广泛主题有许多变体。例如，线性树模型在决策树的每个叶节点拟合线性模型，增加了标准决策树的预测能力，同时保持所有解释性。Rudin教授的研究小组引入了[最优稀疏决策树](https://oreil.ly/4kGpW)作为对不稳定性和欠指定性问题的一种响应。另一个解决这些问题的方法是基于人类领域知识的手动约束。接下来我们将讨论约束决策树集成，但如果读者准备尝试标准决策树，有许多可以尝试的包，其中R包[rpart](https://oreil.ly/XLE1H)是最好的之一。
- en: Constrained XGBoost models
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 有约束的XGBoost模型
- en: The popular gradient boosting package XGBoost now supports both [monotonic constraints](https://oreil.ly/39PhO)
    and [interaction constraints](https://oreil.ly/yVAtK). As described earlier, user-supplied
    monotonic constraints force the XGBoost model to preserve more explainable monotonic
    relationships between model inputs and model predictions. The interaction constraints
    can prevent XGBoost from endlessly recombining features. These software features
    turn what would normally be an opaque tree ensemble model replete with underspecification
    problems into a highly robust model capable of learning from data and accepting
    causally motivated constraints from expert human users. These newer training options,
    which enable human domain experts to use causal knowledge to set the direction
    of modeled relationships and to specify which input features should not interact,
    combined with XGBoost’s proven track record of scalability and performance, make
    this a hard choice to overlook when it comes to explainable ML. While not as directly
    interpretable as EBMs, because gradient boosting machines (GBMs) combine and recombine
    features into a tangle of nested if-then rules, constrained XGBoost models are
    amenable to a wide variety of post hoc explainable and visualization techniques.
    The explanation techniques often enable practitioners to confirm the causal knowledge
    they’ve injected into the model and to understand the inner workings of the complex
    ensemble.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在流行的梯度提升包 XGBoost 支持[单调约束](https://oreil.ly/39PhO)和[交互约束](https://oreil.ly/yVAtK)。如前所述，用户提供的单调约束强制
    XGBoost 模型保持更可解释的输入与预测之间的单调关系。交互约束可以防止 XGBoost 无限重组特征。这些软件功能将通常是一个充满不充分问题的不透明树集成模型转变为一个高度健壮的模型，能够从数据中学习，并接受专家用户提供的因果驱动约束。这些新的训练选项使人类领域专家能够使用因果知识来设置模型关系的方向，并指定哪些输入特征不应该互动，结合
    XGBoost 在可扩展性和性能上的成熟记录，使其在可解释机器学习领域成为一个不容忽视的强力选择。虽然不像EBMs那样直接可解释，因为梯度提升机（GBMs）将特征组合和重组成一系列嵌套的if-then规则，但受限的XGBoost模型适合各种事后可解释和可视化技术。解释技术通常使从业者能够确认他们注入模型的因果知识，并理解复杂集成的内部工作方式。
- en: An Ecosystem of Explainable Machine Learning Models
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可解释机器学习模型的生态系统
- en: 'Beyond additive and tree-based models, there is an entire ecosystem of explainable
    ML models. Some of these models have been known for decades, some represent tweaks
    to older approaches, and some are wholly new. Whether they’re new or old, they
    challenge the status quo of unexplainable ML—and that’s a good thing. If readers
    are surprised about all the different options for explainable models, imagine
    a customer’s surprise when we can explain the drivers of our AI system’s decisions,
    help them confirm or deny hypotheses about their business, and provide explanations
    to users, all while achieving high levels of predictive quality. Indeed, there
    are lots of options for explainable models, and there’s sure to be one that might
    suit our next project well. Instead of asking our colleagues, customers, or business
    partners to blindly trust an opaque ML pipeline, consider explainable neural networks;
    *k*-nearest neighbors; rule-based, causal, or graphical models; or even sparse
    matrix factorization next time:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 除了加法模型和基于树的模型之外，还有一个完整的可解释机器学习模型生态系统。其中一些模型已经为人熟知几十年，一些代表对旧方法的调整，还有一些是全新的。无论是新的还是旧的，它们都挑战了不可解释机器学习的现状——这是件好事。如果读者对可解释模型的各种选择感到惊讶，那么想象一下客户在我们能够解释AI系统决策驱动因素、帮助他们确认或否定关于他们业务的假设，并向用户提供解释的时候会有多么惊讶，并且还能保持高水平的预测质量。确实，有很多可解释模型的选择，肯定会有一个适合我们下一个项目的。与其要求我们的同事、客户或商业伙伴盲目信任不透明的机器学习管道，不如考虑下次使用可解释的神经网络；*k*-最近邻；基于规则、因果或图形的模型；甚至是稀疏矩阵分解：
- en: Causal models
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 因果模型
- en: Causal models, in which causal phenomena are linked to some prediction outcome
    of interest in a provable manner, are often seen as the gold standard of interpretability.
    Given that we can often look at a chart that defines how the model works and that
    they are composed using human domain or causal inference techniques, they’re almost
    automatically interpretable. They also don’t fit to noise in training data as
    badly as traditional unexplainable ML models. The only hard part of causal models
    is finding the data necessary to train them, or the training processes themselves.
    However, training capabilities for causal models continue to improve, and packages
    like [pyMC3](https://oreil.ly/XY448) for Bayesian models and [dowhy](https://oreil.ly/yacTr)
    for causal inference have been enabling practitioners to build and train causal
    models for years. More recently, both Microsoft and Uber released a [tutorial
    for causal inference](https://oreil.ly/wQu72) with real-world use cases using
    modeling libraries from both companies, [EconML](https://oreil.ly/Q448j) and [causalml](https://oreil.ly/Tx12a),
    respectively. If we care about stable, interpretable models, watch this space
    carefully. Once considered to be nearly impossible to train, causal models are
    slowly working their way into the mainstream.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 因果模型，其中因果现象以可证明的方式与某些感兴趣的预测结果相关联，通常被视为可解释性的黄金标准。鉴于我们经常可以查看定义模型工作方式的图表，并且它们是使用人类领域或因果推断技术组成的，它们几乎可以自动解释。它们也不像传统的不可解释的机器学习模型那样在训练数据中适应噪声。因果模型唯一困难的部分是找到训练它们所需的数据，或者训练过程本身。然而，因果模型的训练能力继续改善，像[pyMC3](https://oreil.ly/XY448)这样的贝叶斯模型和[dowhy](https://oreil.ly/yacTr)这样的因果推断软件包多年来一直使从业者能够构建和训练因果模型。更近期，微软和优步公司分别发布了使用它们公司的建模库[EconML](https://oreil.ly/Q448j)和[causalml](https://oreil.ly/Tx12a)进行因果推断的真实用例的[因果推断教程](https://oreil.ly/wQu72)。如果我们关心稳定的、可解释的模型，请认真关注这个领域。曾经被认为几乎不可能训练的因果模型正在逐渐进入主流。
- en: Explainable neural networks
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释神经网络
- en: '[Released](https://oreil.ly/xiKH_) and [refined](https://oreil.ly/jgu71) by
    Wells Fargo Bank risk management, explainable neural networks (XNNs) prove that
    with a little ingenuity and elbow grease, even the most unexplainable models can
    be made explainable and retain high degrees of performance quality. Explainable
    neural networks use the same principals as GAMs, GA2Ms, and EBMs to achieve both
    high interpretability and high predictive performance, but with some twists. Like
    GAMs, XNNs are simply additive combinations of shape functions. However, they
    add an additional indexed structure to GAMs. XNNs are an example of generalized
    additive *index* models (GAIMs), where GAM-like shape functions are fed by a lower
    projection layer that can learn interesting higher-degree interactions.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 由富国银行风险管理部门[发布](https://oreil.ly/xiKH_)并[完善](https://oreil.ly/jgu71)的可解释神经网络（XNNs）证明，即使对于最不可解释的模型，稍加独创和努力也能使其变得可解释并保持高度的性能质量。可解释神经网络使用与GAMs、GA2Ms和EBMs相同的原理来实现高解释性和高预测性能，但是有所不同。像GAMs一样，XNNs只是形状函数的加法组合。然而，它们在GAMs基础上添加了额外的索引结构。XNNs是广义可加性*索引*模型（GAIMs）的一个例子，其中类似于GAM的形状函数由能够学习有趣的高阶交互的低投影层输入。
- en: 'In an XNN, back-propagation is used to learn optimal combinations of variables
    (see c in [online XNN figure](https://oreil.ly/kBy92)) to act as inputs into shape
    functions learned via subnetworks (see b in online figure), which are then combined
    in an additive fashion with optimal weights to form the output of the network
    (see a in online figure). While likely not the simplest explainable model, XNNs
    have been refined to be more scalable, they automatically identify important interactions
    in training data, and post hoc explanations can be used to break down their predictions
    into locally accurate feature contributions. Another interesting type of transparent
    neural networks was put forward recently in [“Neural Additive Models: Interpretable
    Machine Learning with Neural Nets”](https://oreil.ly/ge-fk). NAMs appear similar
    to XNNs, except they forego the bottom layer that attempts to locate interaction
    terms.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在XNN中，使用反向传播来学习变量的最佳组合（见[c在线XNN图](https://oreil.ly/kBy92)）作为输入，这些变量通过子网络学习的形状函数（见在线图中的b）以加法方式与最优权重结合，形成网络输出（见在线图中的a）。虽然可能不是最简单的可解释模型，但XNN已经被改进以提高可扩展性，它们可以自动识别训练数据中的重要交互作用，并且事后解释可以用来将其预测分解为局部准确的特征贡献。最近提出的另一种有趣的透明神经网络类型在[“神经添加模型：用神经网络进行可解释机器学习”](https://oreil.ly/ge-fk)中提出。NAMs看起来与XNN类似，但它们放弃了试图定位交互项的底层。
- en: '*k*-nearest neighbors (*k*-NN)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*最近邻 (*k*-NN)'
- en: The *k*-NN method uses prototypes, or similar data points, to make predictions.
    Reasoning in this way does not require training and its simplicity often appeals
    to human users. If *k* is set to three, an inference on a new point involves finding
    the three nearest points to the new point and taking the average or modal label
    from those three points as the predicted outcome for the new point. This kind
    of logic is common in our day-to-day lives. Take the example of residential real-estate
    appraisal. Price per square foot for one home is often evaluated as the average
    price per square foot of three comparable houses. When we say, “it sounds like,”
    or “it feels like,” or “it looks like,” we’re probably using prototype data points
    to make inferences about things in our own life. The notion of interpretation
    by comparison to prototypes is what led the Rudin Group to take on unexplainable
    computer vision models with [this-looks-like-that](https://oreil.ly/qG7yT), a
    new type of deep learning that uses comparisons to prototypes to explain image
    classification predictions.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-NN 方法使用原型或类似的数据点来进行预测。以这种方式推理不需要训练，其简单性通常吸引人类用户。如果将*k*设为三，对新点的推断涉及查找最接近的三个点，并从这三个点中取平均标签或模态标签作为新点的预测结果。这种逻辑在我们日常生活中很常见。以住宅房地产评估为例。通常会将一个家庭每平方英尺的价格评估为三个可比房屋每平方英尺的平均价格。当我们说“听起来像”，“感觉像”或“看起来像”时，我们可能正在使用原型数据点来推断我们生活中的事物。通过与原型的比较进行解释的概念，促使鲁丁集团采用了[this-looks-like-that](https://oreil.ly/qG7yT)，这是一种新型深度学习，利用与原型的比较来解释图像分类预测。'
- en: Rule-based models
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则的模型
- en: Extracting predictive if-then rules from datasets is another long-running type
    of ML modeling. The simple Boolean logic of if-then rules is interpretable, as
    long as the number of rules, the number of branches in the rules, and the number
    of entities in the rules are constrained. [RuleFit](https://oreil.ly/xc3-B) and
    [skope-rules](https://oreil.ly/3w2BK) are two popular techniques that seek to
    find predictive and explainable rules from training data. Rule-based models are
    also the domain of the Rudin Group. Among their greatest hits for interpretable
    and high-quality rule-based predictors are [certifiable optimal rule lists (CORELS)](https://oreil.ly/BgWCt)
    and [scalable Bayesian rule lists](https://oreil.ly/RCu74). Code for these and
    other valuable contributions from the Rudin Group are available on their public
    [code page](https://oreil.ly/aYDTE).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据集中提取预测性的if-then规则是另一种长期存在的ML建模类型。if-then规则的简单布尔逻辑是可解释的，只要规则的数量、规则中分支的数量和规则中的实体数量受到限制。[RuleFit](https://oreil.ly/xc3-B)和[skope-rules](https://oreil.ly/3w2BK)是两种流行的技术，旨在从训练数据中找到预测性和可解释性规则。基于规则的模型也是鲁丁集团的领域。在可解释和高质量的基于规则的预测器中，他们的杰出作品包括[可证明最优规则列表（CORELS）](https://oreil.ly/BgWCt)和[可扩展贝叶斯规则列表](https://oreil.ly/RCu74)。鲁丁集团的这些及其他宝贵贡献的代码都可以在他们的公共[代码页面](https://oreil.ly/aYDTE)上找到。
- en: Sparse matrix factorization
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏矩阵分解
- en: Factoring a large data matrix into two smaller matrices is a common dimension
    reduction and unsupervised learning technique. Most older matrix factorization
    techniques reattribute the original columns of data across dozens of derived features,
    rendering the results of these techniques unexplainable. However, with the introduction
    of L1 penalties into matrix factorization, it’s now possible to extract new features
    from a large matrix of data, where only a few of the original columns have large
    weights on any new feature. By using [sparse principal components analysis (SPCA)](https://oreil.ly/xqWFw),
    we might find, for example, that when extracting a new feature from customer financial
    data, that the new feature is composed exclusively of the debt-to-income and revolving
    account balance features in our original dataset. We could then reason through
    this feature being related to consumer debt. Or if we find another new feature
    that has high weights for income, payments, and disbursements, then we could interpret
    that feature as relating to cash flow. [Nonnegative matrix factorization (NMF)](https://oreil.ly/CKydA)
    gives similar results but assumes that training data only takes on positive values.
    For unstructured data like term counts and pixel intensities, that assumption
    always holds. Hence, NMF can be used to find explainable summaries of topics in
    documents or to decompose images into explainable dictionaries of subcomponents.
    Whether we use SPCA or NMF, the resulting extracted features can be used as explainable
    summaries, archetypes for explainable comparisons, axes for visualization, or
    features in models. And it turns out that just like many explainable supervised
    learning models are special instances of GAMs, many unsupervised learning techniques
    are instances of [generalized low-rank models](https://oreil.ly/YKUDa), which
    we can try out in H2O.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 将大数据矩阵分解为两个较小的矩阵是一种常见的降维和无监督学习技术。大多数较旧的矩阵分解技术将数据的原始列重新分配到数十个派生特征中，使得这些技术的结果难以解释。然而，随着在矩阵分解中引入L1惩罚，现在可以从大量数据的矩阵中提取新特征，其中只有少数原始列在任何新特征上具有较大的权重。通过使用[sparse
    principal components analysis (SPCA)](https://oreil.ly/xqWFw)，例如，我们可能会发现，从客户财务数据中提取新特征时，这个新特征仅由原始数据集中的债务收入比和循环账户余额特征组成。我们可以因此推断出，这个特征与消费者债务相关。或者，如果我们发现另一个新特征在收入、支付和支出方面具有较高的权重，那么我们可以将这个特征解释为与现金流相关。[Nonnegative
    matrix factorization (NMF)](https://oreil.ly/CKydA)给出了类似的结果，但假设训练数据只取正值。对于像术语计数和像素强度这样的非结构化数据，这种假设始终成立。因此，NMF可用于找到文档中主题的可解释摘要，或将图像分解为可解释的子组件字典。无论我们使用SPCA还是NMF，所得的提取特征可以用作可解释的摘要、可解释比较的原型、可视化的轴或模型中的特征。事实证明，就像许多可解释的监督学习模型是广义加性模型的特殊实例一样，许多无监督学习技术都是[generalized
    low-rank models](https://oreil.ly/YKUDa)的实例，我们可以在H2O中尝试这些技术。
- en: Now that we’ve covered the basics of explainable models, we’ll move on to post
    hoc explanation techniques. But before we do, remember that it’s perfectly fine
    to use explainable models and post hoc explanation together. Explainable models
    are often used to incorporate domain knowledge into learning mechanisms, to address
    inherent assumptions or limitations in training data, or to build functional forms
    that humans have a chance of understanding. Post hoc explanation is often used
    for visualization and summarization. While post hoc explanation is often discussed
    in terms of increasing transparency for traditional opaque ML models, there are
    lots of reasons to question that application, which we’ll introduce in the following
    section. Using explainable models and post hoc explanation together, to improve
    upon and validate the other, may be the best general application for both technologies.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了可解释模型的基础知识，接下来我们将讨论后事因果解释技术。在此之前，请记住，同时使用可解释模型和后事因果解释是完全可以的。可解释模型通常用于将领域知识整合到学习机制中，以解决训练数据中固有的假设或限制，或构建人类能够理解的功能形式。后事因果解释通常用于可视化和总结。虽然后事因果解释通常被讨论为提高传统不透明机器学习模型透明度的手段，但我们将在接下来的部分介绍其应用存在的许多理由。同时使用可解释模型和后事因果解释，以改进和验证彼此，可能是这两种技术的最佳普遍应用。
- en: Post Hoc Explanation
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 后事因果解释
- en: 'We’ll start off by addressing global and local feature attribution measures,
    then move on to surrogate models and popular types of plots for describing model
    behavior, and touch on a few post hoc explanation techniques for unsupervised
    learning. We’ll also discuss the shortcomings of post hoc explanations, which
    can be broadly summarized in three points that readers should keep in mind as
    they form their own impressions of and practices with explanations:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先讨论全局和局部特征归因措施，然后转向替代模型和描述模型行为的流行类型绘图，还将触及几种无监督学习的事后解释技术。我们还将讨论事后解释的缺陷，读者在形成对解释的印象和实践时应牢记以下三点的广泛总结：
- en: If a model doesn’t make sense, its explanations won’t either. (We can’t explain
    nonsense.)
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个模型没有意义，它的解释也不会有意义。（我们无法解释无意义的东西。）
- en: ML models can easily grow so complex they can’t be summarized accurately.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习模型很容易变得如此复杂，以至于无法准确总结。
- en: It’s difficult to convey explanatory information about ML systems to a broad
    group of users and stakeholders.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向广大用户和利益相关者传达关于机器学习系统的解释信息是困难的。
- en: Despite these difficulties, post hoc explanation is almost always necessary
    for interpretability and transparency. Even models like logistic regression, thought
    of as highly transparent, must be summarized to meet regulatory obligations around
    transparency. For better or worse, we’re probably stuck with post hoc explanation
    and summarization. So let’s try to make it work as well as possible.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些困难，事后解释几乎总是必要的，以确保可解释性和透明性。即使像逻辑回归这样被认为高度透明的模型，也必须被总结，以满足透明度的监管义务。不管好坏，我们可能都无法摆脱事后解释和总结。因此，让我们尽可能使其运作良好。
- en: Warning
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Many models must be summarized in a post hoc fashion to be interpretable. Yet,
    ML explanations are often incorrect, and checking them requires rigorous testing
    and comparisons with an underlying explainable model.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 许多模型必须以事后方式总结才能被解释。然而，机器学习解释通常是不正确的，检查它们需要严格的测试和与基础可解释模型的比较。
- en: Feature Attribution and Importance
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征归因和重要性
- en: Feature attribution is one of the most central aspects of explaining ML models.
    Feature attribution methods tell us how much an input feature contributed to the
    predictions of a model, either globally (across an entire dataset) or locally
    (for one or a few rows of data). Feature attribution values can typically be either
    positive or negative.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 特征归因是解释机器学习模型中最核心的方面之一。特征归因方法告诉我们输入特征对模型预测的贡献有多大，无论是全局（整个数据集）还是局部（一行或几行数据）。特征归因值通常可以是正数或负数。
- en: When we discuss feature importance, we mean a global measure of how much each
    feature contributes toward a model’s predictions. Unlike feature attributions,
    feature importance values are typically always positive. That is, feature importance
    values measure how significantly a feature contributed to a model’s overall behavior
    on a dataset. Feature attributions, on the other hand, give us more detail on
    the feature’s contribution.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论特征重要性时，我们指的是每个特征对模型预测贡献的全局度量。与特征归因不同，特征重要性值通常始终为正数。也就是说，特征重要性值衡量了一个特征对模型在数据集上整体行为的显著性贡献。另一方面，特征归因为我们提供了关于特征贡献的更详细信息。
- en: While a handful of the more established global feature importance metrics do
    not arise from the aggregation of local measures, averaging (or otherwise aggregating)
    local feature attributions into global feature importances is common today. We’ll
    start by discussing newer methods for local feature attribution and then move
    on to global methods.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管一些更成熟的全局特征重要性指标不是由局部测量的聚合产生的，但是今天通常会将局部特征归因平均（或以其他方式聚合）为全局特征重要性。我们将首先讨论新的局部特征归因方法，然后转向全局方法。
- en: Note
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '*Global* explanations summarize a model mechanism or prediction over an entire
    dataset or large sample of data. *Local* explanations perform the same type of
    summarization, but for smaller segments of data, down to a single row or cell
    of data.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*全局*解释总结了整个数据集或大量数据样本上的模型机制或预测。*局部*解释执行相同类型的总结，但适用于数据的较小段落，甚至是单行或单元数据。'
- en: “Feature importance” can be misleading as a name. We’re just approximating one
    model’s idea of what is important. Consider, for example, a computer vision security
    system that relies on gradient-based methods to detect “important” aspects of
    video frames. Without proper training and deployment specifications, such systems
    will have a hard time picking up individuals wearing camouflage, as newer digital
    camouflage clothing is specifically designed to blend into various backgrounds,
    and to keep visual gradients between the fabric and different backgrounds smooth
    and undetectable. But isn’t people wearing camouflage one of the most important
    things to detect in a security application? For the rest of this section, keep
    in mind that “feature importance” is highly dependent on the model’s understanding
    and training.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: “特征重要性”这个名字可能会误导。我们只是在近似地描述一个模型认为重要的内容。例如，考虑一个依赖基于梯度的方法来检测视频帧中“重要”方面的计算机视觉安全系统。如果没有适当的培训和部署规范，这样的系统将很难检测到穿着数字伪装服装的个体，因为新款数字伪装服装专门设计成能够融入各种背景，并且保持布料与不同背景之间的视觉梯度平滑和不可检测。但是，在安全应用中，检测穿着伪装服装的人难道不是最重要的事情之一吗？在本节的其余部分，请记住，“特征重要性”高度依赖于模型的理解和训练。
- en: Local explanations and feature attribution
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本地解释和特征归因
- en: In some applications, it is crucial to determine which input features impacted
    a specific prediction—that is, to measure local feature attributions. Have a look
    at [Figure 2-3](#local) to see local feature attribution in action.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些应用中，确定哪些输入特征影响了特定预测是至关重要的，即测量本地特征归因。看看[图 2-3](#local) ，看看本地特征归因是如何发挥作用的。
- en: '![mlha 0203](assets/mlha_0203.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![mlha 0203](assets/mlha_0203.png)'
- en: Figure 2-3\. Local feature attribution for two explainable models for three
    individuals at the 10th, 50th, and 90th percentiles of predicted probabilities
    ([digital, color version](https://oreil.ly/4Y__H))
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-3\. 两个可解释模型的本地特征归因，适用于预测概率的第10、50和90百分位数的三个个体（[数字，彩色版本](https://oreil.ly/4Y__H)）
- en: '[Figure 2-3](#local) shows two types of local feature attribution values, for
    two different models, for three different customers in a credit lending example.
    The first customer sits at the 10th percentile of probability of default, and
    they are likely to receive the credit product on offer. The second customer sits
    at the 50th percentile, and are unlikely to receive the product on offer. The
    third customer sits at the 90th percentile of probability of default and provides
    an example of an extremely high-risk applicant. The two models being summarized
    are a penalized generalized linear model (GLM) and a monotonically constrained
    GBM. Since both models have relatively simple structures that make sense for the
    problem at hand, we’ve done a good job handling the caveats brought up in our
    opening remarks about post hoc explanation. These explanations should be trustworthy,
    and they are simple enough to be accurately summarized.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-3](#local) 展示了两种类型的本地特征归因值，分别对应两种不同模型，在信用贷款示例中针对三个不同客户的应用。第一个客户处于违约概率的第10百分位数，他们可能会接受提供的信贷产品。第二个客户处于第50百分位数，不太可能接受提供的产品。第三个客户处于违约概率的第90百分位数，并且是一个极高风险申请人的例子。总结的两个模型分别是惩罚的广义线性模型（GLM）和单调约束的GBM。由于这两个模型都具有相对简单的结构，对于手头的问题是有意义的，我们在处理关于事后解释的开场白中提出的警告时做得不错。这些解释应该是可信的，并且足够简单，能够准确地总结。'
- en: Note
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: One practical way to make explanations more interpretable is to compare them
    with some meaningful benchmark. That’s why we compare our feature attribution
    values to Pearson correlation and compare our partial dependence and individual
    conditional expectation (ICE) values to mean model predictions. This should enable
    viewers of the plot to compare more abstract explanatory values to a more understandable
    benchmark value.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 使解释更具可解释性的一个实际方法是将它们与某些有意义的基准进行比较。这就是为什么我们将我们的特征归因值与Pearson相关性进行比较，并将我们的部分依赖和个体条件期望（ICE）值与平均模型预测进行比较。这应该使图表的观看者能够将更抽象的解释值与更容易理解的基准值进行比较。
- en: 'How are we summarizing the models’ local behavior? For the penalized GLM, we’re
    multiplying the model coefficients by the values of the input feature for each
    applicant. For the GBM, we’re applying SHAP. Both of these local feature attribution
    techniques are additive and locally accurate—meaning they sum to the model prediction.
    Both are measured from an offset, the GLM intercept or the SHAP intercept, which
    are similar in value but not equal. (That is not accounted for in [Figure 2-3](#local).)
    And both values can be generated in the same space: either log odds or predicted
    probability, depending on how the calculation is performed.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何总结模型的本地行为？对于惩罚GLM模型，我们将模型系数乘以每个申请人输入特征的值。对于GBM，我们应用SHAP。这两种本地特征归因技术都是加法的并且在本地精确——意味着它们总结到模型预测值。它们都是从一个偏移值计算得出，GLM截距或SHAP截距，这些值在数值上相似但并不相等（这在[图 2-3](#local)中未考虑）。并且这两个值可以在相同的空间中生成：无论是对数几率还是预测概率，取决于计算的方式。
- en: What do the plotted values tell us? For the low-risk applicant at the 10th percentile
    of probability of default, we can see that their most recent bill amount, `BILL_AMT1`,
    is highly favorable and driving their prediction downward. The SHAP values for
    the same customer tell a slightly different story, but the GBM was trained on
    a different set of features. The SHAP values paint a story of the applicant being
    lower-risk on all considered attributes. For the applicant at the 50th percentile,
    we see most local feature attribution values staying close to their respective
    intercepts, and for the high-risk applicant nearly all local feature attribution
    values are positive, pushing predictions higher. Both models seem to agree that
    it’s the applicant’s recent payment statuses (`PAY_0`, `PAY_2`, and `PAY_3`) that
    are driving risk, with the GBM and SHAP also focusing on payment amount information.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制的值告诉我们什么？对于概率违约率排名为第10百分位的低风险申请人，我们可以看到他们的最新账单金额`BILL_AMT1`非常有利，并且推动了他们的预测向下。同一客户的SHAP值讲述了稍微不同的故事，但GBM是在不同的特征集上训练的。SHAP值描述了申请人在所有考虑的属性上都是较低风险的。对于第50百分位的申请人，我们看到大多数本地特征归因值都保持接近它们各自的截距，并且对于高风险申请人，几乎所有的本地特征归因值都是正值，推动了预测值的增加。两个模型似乎都认为是申请人最近的付款状况（`PAY_0`，`PAY_2`和`PAY_3`）在推动风险方面起到了作用，而GBM和SHAP也专注于付款金额信息。
- en: These are just two types of local feature attribution, and we’re going to discuss
    many others. But this small example likely brings up some questions that might
    get stuck in readers’ heads if we don’t address them before moving on. First and
    foremost, the act of generating explanations does not make these good models;
    it simply means we get to make a more informed decision about whether the models
    are good or not. Second, it’s not uncommon for two different models to give different
    explanations for the same row of data. But just because it’s common, doesn’t make
    it right. It’s not something we should just accept and move past. While the models
    in [Figure 2-3](#local) appear to show adequate agreement when operating on the
    same features, this is, unfortunately, a best-case scenario for post hoc explanation,
    and it happened because we picked relatively simple models and made sure the sign
    of the GLM coefficients and the direction of the monotonic constraints of the
    GBM agreed with domain expertise. For complex, unexplainable models trained on
    hundreds or more correlated features, we’ll likely see much less agreement between
    model explanations, and that should raise red flags.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这些仅是本地特征归因的两种类型，我们将讨论许多其他类型。但这个小例子可能会引起一些问题，如果我们在继续之前不加以解释的话，这些问题可能会困扰读者。首先，生成解释并不意味着这些模型就是好模型；这只是意味着我们可以更明智地决定这些模型是否良好。其次，不同的模型对相同数据行给出不同的解释并不罕见。但仅仅因为它常见，并不意味着它就是正确的。这不是我们应该接受并跳过的事情。虽然在[图 2-3](#local)中的模型在操作相同特征时似乎表现出了足够的一致性，但这很遗憾，这只是后续解释的最佳案例，因为我们选择了相对简单的模型，并确保了GLM系数的符号和GBM的单调约束方向与领域专业知识一致。对于训练在数百个或更多相关特征上的复杂、无法解释的模型，我们可能会看到模型解释之间的一致性大大降低，这应该引起警觉。
- en: This notion that different models should yield similar explanations on the same
    row of data is called *consistency*. Consistency is a reasonable goal for high-risk
    applications in order to bolster trust in outcomes and agreement among multiple
    important decision-making systems.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 不同模型在相同数据行上产生相似解释的概念被称为*一致性*。在高风险应用中，一致性是一个合理的目标，以增强对结果的信任和多个重要决策系统之间的一致性。
- en: 'As readers are probably starting to pick up, post hoc explanation is complex
    and fraught, and this is why we suggest pairing these techniques with explainable
    models, so the models can be used to check the explanations and vice versa. Nevertheless,
    we push onward to an outline of the major local explanation techniques—counterfactuals,
    gradient-based, occlusion, prototypes, SHAP values, and others:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 正如读者可能已经开始注意到的那样，事后解释是复杂而充满风险的，这就是为什么我们建议将这些技术与可解释模型配对使用，这样模型可以用来检查解释，反之亦然。尽管如此，我们仍然推进到主要的局部解释技术概述——反事实、基于梯度的、遮挡、原型、SHAP
    值等等：
- en: Counterfactuals
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 反事实
- en: 'Counterfactual explanations tell us what an input feature’s value would have
    to become to change the outcome of a model prediction. The bigger the swing of
    the prediction when changing an input variable by some standard amount, the more
    important that feature is, as measured from the counterfactual perspective. Check
    out [Section 9.3, “Counterfactual Explanations”](https://oreil.ly/9SNML), in Christoph
    Molnar’s *Interpretable Machine Learning* for more details. To try out counterfactual
    explanations, check out [“DiCE: Diverse Counterfactual Explanations for Machine
    Learning Classifiers”](https://oreil.ly/s_QaL) from Microsoft Research.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '反事实解释告诉我们，改变输入特征的值以改变模型预测结果所需的值。当通过某个标准量改变输入变量时，预测的摆动越大，从反事实的角度来衡量，该特征就越重要。更多详情，请查看克里斯托夫·莫尔纳尔的*可解释机器学习*中的[“反事实解释”](https://oreil.ly/9SNML)第
    9.3 节。要尝试反事实解释，请查看微软研究的[“DiCE: 机器学习分类器的多样反事实解释”](https://oreil.ly/s_QaL)。'
- en: Gradient-based feature attribution
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度的特征归因
- en: Think of gradients as regression coefficients for every little piece of a complex
    machine-learned function. In deep learning, gradient-based approaches to local
    explanations are common. When used for image or text data, gradients can often
    be overlaid on input images and text to create highly visual explanations depicting
    which parts of the input, if changed, would generate the largest changes in the
    model output. Various tweaks to this idea are said to result in improved explanations,
    such as [integrated gradients](https://oreil.ly/is_C-), [layer-wise relevance
    propagation](https://oreil.ly/XKJ4B), [deeplift](https://oreil.ly/6rhO0), and
    [Grad-CAM](https://oreil.ly/Zkfeh). For an excellent, highly technical review
    of gradient-based explanations, see Ancona et al.’s [“Towards Better Understanding
    of Gradient-based Attribution Methods for Deep Neural Networks”](https://oreil.ly/h7Lde).
    To see what can go wrong with these techniques, see [“Sanity Checks for Saliency
    Maps”](https://oreil.ly/a9fQA). We’ll return to these ideas in [Chapter 9](ch09.html#unique_chapter_id_9),
    when we train a deep learning model and compare various attribution techniques.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 将梯度视为复杂机器学习函数每个小部分的回归系数。在深度学习中，基于梯度的局部解释方法很常见。当用于图像或文本数据时，梯度通常可以叠加在输入图像和文本上，创建高度视觉化的解释，描绘如果改变输入的哪些部分会对模型输出产生最大变化。据说对这一想法的各种调整会导致改进的解释，例如[integrated
    gradients](https://oreil.ly/is_C-)、[layer-wise relevance propagation](https://oreil.ly/XKJ4B)、[deeplift](https://oreil.ly/6rhO0)和[Grad-CAM](https://oreil.ly/Zkfeh)。关于基于梯度的归因方法的优秀且高度技术性的审查，请参见安科纳等人的[“Towards
    Better Understanding of Gradient-based Attribution Methods for Deep Neural Networks”](https://oreil.ly/h7Lde)。要查看这些技术可能出现的问题，请参见[“Sanity
    Checks for Saliency Maps”](https://oreil.ly/a9fQA)。在我们训练深度学习模型并比较各种归因技术的[第 9 章](ch09.html#unique_chapter_id_9)中，我们将回到这些想法。
- en: Occlusion
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 遮挡
- en: 'Occlusion refers to the simple and powerful idea of removing features from
    a model prediction and tracking the resulting change in the prediction. A big
    change may mean the feature is important, a small change may mean it’s less important.
    Occlusion is the basis of SHAP, leave-one-feature-out (LOFO), and many other explanation
    approaches, including many in computer vision and natural language processing.
    Occlusion can be used to generate explanations in complex models when gradients
    are unavailable. Of course, it’s never simple mathematically to remove inputs
    from a model, and it takes a lot of care to generate relevant explanations from
    the results of feature removal. For an authoritative review of occlusion and feature
    removal techniques, see Covert, Lundberg, and Lee’s [“Explaining by Removing:
    A Unified Framework for Model Explanation”](https://oreil.ly/662aS), where they
    cover 25 explanation methods that can be linked back to occlusion and feature
    removal.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 遮蔽（occlusion）是一种简单而强大的概念，即从模型预测中移除特征并跟踪结果的变化。大的变化可能意味着该特征很重要，小的变化可能意味着它不那么重要。遮蔽是SHAP、leave-one-feature-out（LOFO）等许多解释方法的基础，包括计算机视觉和自然语言处理中的许多方法。当梯度不可用时，遮蔽可以用于生成复杂模型的解释。当然，从数学上讲，从模型中移除输入并生成相关解释需要非常谨慎。要了解遮蔽和特征移除技术的权威评估，请参阅Covert、Lundberg和Lee的[《通过移除来解释：模型解释的统一框架》](https://oreil.ly/662aS)，其中介绍了可以追溯到遮蔽和特征移除的25种解释方法。
- en: Prototypes
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 原型
- en: Prototypes are instances of data that are highly representative of larger amounts
    of data. Prototypes are used to explain by summarization and comparison. A common
    kind of prototype is *k*-means (or other) cluster centroids. These prototypes
    are an average representation of a similar group of data. They can be compared
    to other points in their own cluster and in other clusters based on distances
    and in terms of real-world similarity. Real-world data is often highly heterogeneous,
    and it can be difficult to find prototypes that represent an entire dataset well.
    *Criticisms* are data points that are not represented well by prototypes. Together,
    prototypes and criticisms create a set of points that can be leveraged for summarization
    and comparison purposes to better understand both datasets and ML models. Moreover,
    several types of ML models, like *k*-NN and this-looks-like-that deep learning,
    are based on the notion of prototypes, which enhances their overall interpretability.
    To learn more about prototypes, look into Molnar’s chapter on [prototypes and
    criticisms](https://oreil.ly/2IQYd).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 原型是数据的实例，极其代表大量数据。原型用于通过总结和比较来解释。一种常见的原型是*k*-means（或其他）聚类中心。这些原型是类似数据群体的平均表示。它们可以根据距离和实际相似性与其所在群体内部和其他群体中的其他点进行比较。现实世界的数据通常非常异质化，很难找到能够良好代表整个数据集的原型。*批评*是那些不能很好地被原型表示的数据点。原型和批评共同构成了一组点，可以用于总结和比较，从而更好地理解数据集和机器学习模型。此外，几种ML模型，如*k*-NN和this-looks-like-that深度学习，都基于原型的概念，从而增强了它们的整体可解释性。要了解更多关于原型的信息，请查阅Molnar的[原型和批评](https://oreil.ly/2IQYd)章节。
- en: There are various other local explanation techniques. Readers may have heard
    of [treeinterpreter](https://oreil.ly/VECj5) or [eli5](https://oreil.ly/xOkSX),
    which generate locally accurate, additive attribution values for ensembles of
    decision trees. [Alethia](https://oreil.ly/ZauV8) provides model summaries and
    local inference for rectified linear unit (ReLU) neural networks.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 还有各种其他的局部解释技术。读者可能听说过[treeinterpreter](https://oreil.ly/VECj5)或[eli5](https://oreil.ly/xOkSX)，它们为决策树集成生成局部准确的加法归因值。[Alethia](https://oreil.ly/ZauV8)为修正线性单元（ReLU）神经网络提供模型摘要和局部推理。
- en: Next, we’ll devote a section to the discussion of Shapley values, one of most
    popular and rigorous types of local explanations available to data scientists.
    Before moving on, we’ll remind readers once more that these post hoc explanation
    techniques, including SHAP, are not magic. While the ability to understand which
    features influence ML model decisions is an incredible breakthrough, there is
    a great deal of literature pointing toward problems with these techniques. To
    get the most out of them, approach them with a staid and scientific mindset. Do
    experiments. Use explainable models and simulated data to assess explanation quality
    and validity. Does the explanation technique we’ve selected give compelling explanations
    on random data? (If so, that’s bad.) Does the technique provide stable explanations
    when data is mildly perturbed? (That’s a good thing, usually.) Nothing in life
    or ML is perfect, and that certainly includes local post hoc explanation.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将专门讨论沙普利值，这是数据科学家可用的最受欢迎和严谨的本地解释类型之一。在继续之前，我们再次提醒读者，这些事后解释技术，包括SHAP，并非魔法。虽然理解哪些特征影响ML模型决策的能力是一项令人难以置信的突破，但有大量文献指出这些技术存在问题。要充分利用它们，要以冷静和科学的态度对待它们。做实验。使用可解释的模型和模拟数据来评估解释质量和有效性。我们选择的解释技术在随机数据上是否提供令人信服的解释？（如果是，那就不好了。）在数据轻微扰动时，该技术是否提供稳定的解释？（通常是好事。）生活中没有完美的东西，包括本地事后解释在内。
- en: Shapley values
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 沙普利值
- en: Shapley values were created by the Nobel laureate economist and mathematician
    Lloyd Shapley. Shapley additive explanations (SHAP) [unify](https://oreil.ly/ilEXW)
    approaches such as LIME, LOFO, treeinterpreter, deeplift, and others to generate
    accurate local feature importance values, and they can be aggregated or visualized
    to create consistent global explanations. Aside from their own Python package,
    [SHAP](https://oreil.ly/LP4zm), and various R packages, SHAP is supported in popular
    machine learning software frameworks like H2O, LightGBM, and XGBoost.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 沙普利值是由诺贝尔经济学奖得主和数学家劳埃德·沙普利创建的。沙普利添加解释（SHAP）[统一](https://oreil.ly/ilEXW)了像LIME、LOFO、treeinterpreter、deeplift等方法，以生成准确的本地特征重要性值，并且可以聚合或可视化以创建一致的全局解释。除了他们自己的Python包[SHAP](https://oreil.ly/LP4zm)，以及各种R包，SHAP在像H2O、LightGBM和XGBoost这样的流行机器学习软件框架中得到支持。
- en: 'SHAP starts out the same as many other explanation techniques, asking the intuitive
    question: what would the model prediction be for this row without this feature?
    So why is SHAP different from other types of local explanations? To be exact,
    in a system with as many complex interactions as a typical ML model, that simple
    question must be answered using an average of all possible sets of inputs that
    do not include the feature of interest. Those different groups of inputs are called
    *coalitions*. For a simple dataset with twenty columns, that means about half
    a million different model predictions on different coalitions are considered on
    the average. Now repeat that process of dropping and averaging for every prediction
    in our dataset, and we can see why SHAP takes into account more information than
    most other local feature attribution approaches.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP的起始方式与许多其他解释技术相同，提出了直观的问题：如果没有这个特征，这一行的模型预测会是什么？那么SHAP与其他类型的本地解释有何不同？确切地说，在典型的ML模型有如此多复杂交互的系统中，必须使用不包括感兴趣特征的所有可能输入集的平均值来回答这个简单问题。这些不同的输入组称为*联盟*。对于一个简单的包含20列的数据集来说，这意味着平均需要考虑大约五十万个不同的联盟上的模型预测。现在，对我们数据集中的每个预测重复这个删除和平均的过程，我们可以看到为什么SHAP比大多数其他本地特征归因方法考虑了更多信息。
- en: There are many different flavors of SHAP, but the most popular are Kernel SHAP,
    Deep SHAP, and Tree SHAP. Of these, Tree SHAP is less approximate, and Kernel
    and Deep SHAP are more approximate. Kernel SHAP has the advantage of being usable
    on any type of model, i.e., being *model-agnostic*. It’s like local interpretable
    model-agnostic explanations (LIME) combined with the coalitional game theory approach.
    However, with more than a few inputs, Kernel SHAP often requires untenable approximations
    to achieve tolerable runtimes. Kernel SHAP also requires the specification of
    *background data*, or data that is used by an explanation technique during the
    process of calculating explanations, which can have a large influence on the final
    explanation values. Deep SHAP also relies on approximations and may be less suitable
    than easier-to-compute gradient-based explanations, depending on the model and
    dataset at hand. On the other hand, Tree SHAP is fast and more exact. But as its
    name suggests, it’s only suitable for tree-based models.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP 有很多不同的变种，但最流行的是 Kernel SHAP、Deep SHAP 和 Tree SHAP。其中，Tree SHAP 较少使用近似，而
    Kernel 和 Deep SHAP 更多使用近似。Kernel SHAP 的优势在于可以用于任何类型的模型，即 *模型无关*。它就像本地可解释模型无关解释（LIME）结合联盟博弈理论方法。然而，对于超过少数输入，Kernel
    SHAP 经常需要不可持续的近似来达到可接受的运行时间。Kernel SHAP 还需要指定 *背景数据*，或者在计算解释过程中使用的数据，这对最终的解释值有很大影响。Deep
    SHAP 也依赖于近似，并且可能不如更容易计算的基于梯度的解释适用于手头的模型和数据集。另一方面，Tree SHAP 更快且更精确。但正如其名称所示，它只适用于基于树的模型。
- en: Note
  id: totrans-143
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Many explanation techniques rely on “background data,” which is data separate
    from the observations being explained that is used to support the calculation
    of the explanations. For example, when we compute SHAP values, we form coalitions
    by removing features from the data. When we have to evaluate the model on this
    coalition, we substitute the missing values by sampling from the background data.
    Background data can have a large effect on explanations and must be chosen carefully
    so as not to conflict with statistical assumptions of explanation techniques and
    to provide the right context for explanations.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 许多解释技术依赖于“背景数据”，这是与被解释的观测数据分开的数据，用于支持解释的计算。例如，当我们计算 SHAP 值时，我们通过从数据中移除特征来形成联盟。当我们在这个联盟上评估模型时，我们通过从背景数据中抽样来替换缺失值。背景数据可以对解释产生很大影响，必须谨慎选择，以避免与解释技术的统计假设冲突，并为解释提供正确的上下文。
- en: Two of the major places where data scientists tend to go wrong with Tree SHAP
    are in the interpretation of SHAP itself and in failing to understand the assumptions
    inherent in different parameterizations of the technique. For interpretation,
    let’s start with recognizing SHAP as an offset from the average model prediction.
    SHAP values are calculated in reference to that offset, and large SHAP values
    mean the feature causes the model prediction to depart from the average prediction
    in some noticeable way. Small SHAP values mean the feature doesn’t move the model
    prediction too far from the average prediction. We’re often tempted to read more
    into SHAP than is actually there. We tend to seek causal or counterfactual logic
    from SHAP values, and this is simply not possible. SHAP values are the weighted
    average of the feature’s contribution to model predictions across a vast number
    of coalitions. They don’t provide causal or counterfactual explanations, and if
    we’d like for them to be meaningful at all, the underlying model must also be
    meaningful.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家在 Tree SHAP 中通常容易犯错误的两个主要领域是解释 SHAP 本身和未能理解技术不同参数化中固有的假设。对于解释，让我们从认识 SHAP
    作为平均模型预测的偏移开始。SHAP 值是相对于该偏移计算的，较大的 SHAP 值意味着该特征导致模型预测与平均预测有显著偏离。较小的 SHAP 值意味着该特征不会使模型预测太远离平均预测。我们经常倾向于从
    SHAP 值中读取比实际存在的更多信息。我们倾向于从 SHAP 值中寻找因果或反事实逻辑，但这是不可能的。SHAP 值是特征对模型预测贡献的加权平均，跨大量联盟。它们不提供因果或反事实解释，如果我们希望它们有任何意义，底层模型也必须有意义。
- en: Note
  id: totrans-146
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: A SHAP value can be interpreted as the difference in model outcome away from
    the average prediction attributed to a certain input feature value.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP 值可以解释为与某个输入特征值相关的模型结果偏离平均预测的差异。
- en: Tree SHAP also asks users to make trade-offs. Based on how features that are
    missing from each coalition are filled in (the *perturbation method*), we choose
    between different philosophies of explanations and different shortcomings.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Tree SHAP 还要求用户进行权衡。根据每个联合中缺失特征的填充方式（*扰动方法*），我们在解释的哲学和不同的缺陷之间进行选择。
- en: If no background data is explicitly passed in, the default settings in Tree
    SHAP use `tree_path_dependent` perturbations, which use the number of training
    examples that went down each path of the tree to approximate the background data
    distribution. If background data is supplied to Tree SHAP, then this data is sampled
    from to fill in missing feature values in what are known as `interventional` feature
    perturbations. The additional flexibility of choosing a background dataset allows
    explanations to be more targeted, but choosing an appropriate background dataset
    can be a complex exercise, even for experienced practitioners. We’ll talk more
    about choosing an appropriate background dataset and the effects it can have in
    [Chapter 6](ch06.html#unique_chapter_id_6).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有显式传递背景数据，Tree SHAP 的默认设置将使用`树路径依赖`扰动，该扰动使用了树的每条路径上经过的训练示例数来近似背景数据分布。如果向
    Tree SHAP 提供了背景数据，则从该数据中采样以填充缺失的特征值，这称为`干预性`特征扰动。选择背景数据集的额外灵活性使得解释更加精确，但即使对于经验丰富的实践者而言，选择合适的背景数据集也可能是一个复杂的任务。我们将在第
    6 章详细讨论选择适当的背景数据集及其可能产生的影响。
- en: Besides the added complexity, the main shortcoming of `interventional` feature
    perturbations is that they create unrealistic data instances. This means that
    when we’re evaluating the attribution of a feature, we may be doing so on a bunch
    of fake observations that would never be observed in the real world. On the other
    hand, intervening allows us to skirt the need to worry about correlated features.
    In contrast, `tree_path_dependent` feature perturbations are more sensitive to
    correlated features, but they try to only consider data points that are realistic.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 除了增加的复杂性外，`干预性`特征扰动的主要缺点是它们会产生不现实的数据实例。这意味着当我们评估某个特征的归因时，我们可能是在一堆永远不会在真实世界中观察到的假观察中进行评估。另一方面，干预允许我们避开对相关特征的担忧。相比之下，`树路径依赖`特征扰动对相关特征更为敏感，但它们试图只考虑现实中可能存在的数据点。
- en: Warning
  id: totrans-151
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Due to general issues with correlation and information overload, good explanations
    usually require that the underlying model is trained on a smaller number of uncorrelated
    features with a direct relationship to the modeling target. As said by the authors
    of the excellent paper [“True to the Model or True to the Data?”](https://oreil.ly/ze8_z),
    including Scott Lundberg, the creator of SHAP: “Currently, the best case for feature
    attribution is when the features that are being perturbed are independent to start
    with.”'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 由于相关性和信息过载的一般问题，良好的解释通常要求基础模型在较少数量的与建模目标直接相关且不相关的特征上进行训练。正如优秀论文[“真实于模型还是真实于数据？”](https://oreil.ly/ze8_z)的作者所说，包括
    SHAP 的创始人 Scott Lundberg：“目前，特征归因的最佳情况是在特征起初是独立的情况下进行扰动。”
- en: This web of assumptions and limitations mean that we still have to be careful
    and thoughtful, even when using Tree SHAP. We can make things easier on us, though.
    Correlation is the enemy of many explainable models and post hoc explanation techniques,
    and SHAP is no different. The authors like to start with a reasonable number of
    input features that do not have serious multicollinearity issues. On a good day,
    we’d have found those features using a causal discovery approach as well. Then
    we’d use domain knowledge to apply monotonic constraints to input features in
    XGBoost. For general feature importance purposes, we’d use Tree SHAP with `tree_path_dependent`
    feature perturbation. For an application like credit scoring, where the proper
    context is defined by regulatory commentary, we might use `interventional` SHAP
    values and background data. For instance, certain regulatory commentary on the
    generation of explanations for credit denials in the US [suggests](https://oreil.ly/W0VxD)
    that we “identify the factors for which the applicant’s score fell furthest below
    the average score for each of those factors achieved by applicants whose total
    score was at or slightly above the minimum passing score.” This means our background
    dataset should be composed of applicants with predictions just above the cutoff
    to receive the credit product.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这些假设和限制的网状结构意味着即使在使用Tree SHAP时，我们仍然必须谨慎和深思熟虑。不过我们可以简化工作。相关性是许多可解释模型和事后解释技术的敌人，SHAP也不例外。作者喜欢从不具有严重多重共线性问题的合理数量的输入特征开始。在好的情况下，我们可以使用因果发现方法找到这些特征。然后，我们会使用领域知识在XGBoost中应用单调约束到输入特征上。对于一般的特征重要性目的，我们会使用带有`tree_path_dependent`特征扰动的Tree
    SHAP。对于像信用评分这样的应用，其适当的背景是由背景数据和`interventional` SHAP值定义的。例如，美国有关信用拒绝解释生成的某些监管评论[建议](https://oreil.ly/W0VxD)我们“确定申请人的分数远低于每个因素的平均分数的因素”，这意味着我们的背景数据集应该由刚刚超过最低及格分数的申请人组成。
- en: Critical applications of local explanations and feature importance
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 本地解释和特征重要性的关键应用
- en: Local feature attribution values’ most mission-critical application is likely
    meeting regulatory requirements. The primary requirement now in the US is to explain
    credit denials with *adverse action notices*. The key technical component for
    the adverse action reporting process are *reason codes*. Reason codes are plain-text
    explanations of a model prediction described in terms of a model’s input features.
    They are a step beyond local feature attributions, in which raw local feature
    attribution values are matched to reasons a product can be denied. Consumers should
    then be allowed to review the reason codes for their negative prediction and follow
    a prescribed appeal process if data inputs or decision factors are demonstrably
    wrong.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 本地特征归因值的最关键应用可能是满足监管要求。目前美国的主要要求是使用*不良行动通知*来解释信用拒绝。不良行动报告流程的关键技术组成部分是*原因代码*。原因代码是模型预测的纯文本解释，用模型的输入特征描述。它们是本地特征归因的进一步步骤，其中原始的本地特征归因值与产品可能被拒绝的原因匹配。消费者应该能够查看其负面预测的原因代码，并在数据输入或决策因素明显错误时按规定的申诉流程进行申诉。
- en: Adverse action reporting is a specific instance of a more high-level notion
    known as actionable recourse, where transparent model decisions are based on factors
    users have control over and can be appealed by model users and overridden by model
    operators. Many forthcoming and proposed regulations, such as those in [California](https://oreil.ly/Wc25G),
    [Washington, DC](https://oreil.ly/jh5xG), and the [EU](https://oreil.ly/kuoEI),
    are likely to introduce similar requirements for explanation or recourse. When
    working under regulatory scrutiny, or just to do the right thing when making important
    decisions for other human beings, we’ll want our explanations to be as accurate,
    consistent, and interpretable as possible. While we expect that local feature
    attributions will be one of the most convenient technical tools to generate the
    raw data needed to comply, we make the best explanations when combining local
    feature attributions with explainable models and other types of explanations,
    like those described in the subsequent sections of this chapter.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 不良行为报告是更高层次概念的一个具体实例，该概念被称为可操作的救济，其中透明的模型决策基于用户可以控制的因素，并且可以由模型用户上诉或被模型操作者覆盖。许多即将出台和提议的法规，如[加利福尼亚州](https://oreil.ly/Wc25G)、[华盛顿特区](https://oreil.ly/jh5xG)和[欧盟](https://oreil.ly/kuoEI)的法规，可能会引入类似的解释或救济要求。在面对监管审查时，或者只是在为其他人类做重要决策时，我们希望我们的解释尽可能准确、一致和可解释。虽然我们期望局部特征归因将成为生成所需原始数据的最方便技术工具之一，但当我们将局部特征归因与可解释模型和其他类型的解释结合使用时，我们会得到最佳的解释，正如本章后续部分所述。
- en: Global feature importance
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 全局特征重要性
- en: Global feature importance methods quantify the global contribution of each input
    feature to the predictions of a complex ML model over an entire dataset, not just
    for one individual or row of data. Global feature importance measures sometimes
    give insight into the average direction that a variable pushes a trained ML function,
    and sometimes they don’t. At their most basic, they simply state the magnitude
    of a feature’s relationship with the response as compared to other input features.
    This is hardly ever a bad thing to know, and since most global feature importance
    measures are older approaches, they are often expected by model validation teams.
    [Figure 2-4](#global) provides an example of feature importance plots, in which
    we are comparing the global feature importance of two models.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 全局特征重要性方法量化了复杂机器学习模型在整个数据集上每个输入特征对预测的全局贡献，而不仅仅是单个数据点或行的贡献。全局特征重要性测量有时能够揭示训练好的机器学习函数中变量推动方向的平均情况，但有时则不然。在最基本的情况下，它们仅仅陈述了某个特征与响应的关系幅度相对于其他输入特征的大小。了解这些信息几乎从未有坏处，而且由于大多数全局特征重要性测量方法都是较旧的方法，因此模型验证团队通常会预期这些信息。[图 2-4](#global)
    提供了特征重要性图的示例，其中我们比较了两个模型的全局特征重要性。
- en: '![mlha 0204](assets/mlha_0204.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![mlha 0204](assets/mlha_0204.png)'
- en: Figure 2-4\. Global feature importance for two explainable models compared to
    Pearson correlation ([digital, color version](https://oreil.ly/C2dF0))
  id: totrans-160
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-4\. 两个可解释模型的全局特征重要性与皮尔逊相关性的比较（[数字版，彩色版](https://oreil.ly/C2dF0)）
- en: 'Charts like this help us answer questions like the following:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 像这样的图表帮助我们回答以下问题：
- en: Does one ordering of feature importance make more sense than the other?
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 某一特征重要性排序是否比另一种更合理？
- en: Does this plot reflect patterns we know the models should have learned from
    training data?
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个图反映了我们从训练数据中应该学到的模式吗？
- en: Are the models placing too much emphasis on just one or two features?
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型是否过于强调某一个或两个特征？
- en: Global feature importance is a straightforward way to conduct such basic checks.
    In [Figure 2-4](#global), we compare feature importance to Pearson correlation
    to have some baseline understanding of which features should be important. Between
    Pearson correlation and the two models, we can see that everyone agrees that `PAY_0`
    is the most important feature. However, the GLM places nearly all of its decision-making
    importance on `PAY_0`, while the GBM spreads importance over a larger set of inputs.
    When models place too much emphasis on one feature, as the GLM in [Figure 2-4](#global)
    does, it can make them unstable in new data if the distribution of the most important
    feature drifts, and it makes adversarial manipulation of a model easy. For the
    GLM in [Figure 2-4](#global), a bad actor would only have to alter the value of
    a single feature, `PAY_0`, to drastically change the model’s predictions.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 全局特征重要性是进行基本检查的一种直接方法。在[图 2-4](#global)中，我们将特征重要性与皮尔逊相关性进行比较，以基本了解哪些特征应该是重要的。在皮尔逊相关性和两个模型之间，我们可以看到大家都认为`PAY_0`是最重要的特征。然而，GLM几乎将其决策重要性全部放在了`PAY_0`上，而GBM则在更大的输入集合上分散了重要性。当模型过分强调一个特征时，就像[图 2-4](#global)中的GLM那样，如果最重要特征的分布漂移，这可能使它们在新数据中不稳定，并且容易进行对抗性模型操纵。对于[图 2-4](#global)中的GLM，恶意行为者只需改变单个特征`PAY_0`的值，就能极大地改变模型的预测结果。
- en: Global feature importance metrics can be calculated in many ways. Many data
    scientists are first introduced to feature importance when learning about decision
    trees. A common feature importance method for decision trees is to sum up the
    change in the splitting criterion for every split in the tree based on a certain
    feature. For instance, if a decision tree (or tree ensemble) is trained to maximize
    information gain for each split, the feature importance assigned to some input
    feature is the total information gain associated with that feature every time
    it is used in the tree(s). Perturbation-based feature importance is another common
    type of feature importance measurement, and it’s a model-agnostic technique, meaning
    it can be used for almost all types of ML models. In perturbation-based feature
    importance, an input feature of interest is shuffled (sorted randomly) and predictions
    are made. The difference in some original score, usually the model prediction
    or something like mean squared error (MSE), before and after shuffling the feature
    of interest is the feature importance. Another similar approach is known as leave-one-feature-out
    (LOFO, or leave-one-covariate-out, LOCO). In the LOFO method, a feature is somehow
    dropped from the training or prediction of a model—say, by retraining without
    the feature and making predictions, or by setting the feature to missing and making
    predictions. The difference in the relevant score between the model with the feature
    of interest and without the feature of interest is taken to be the LOFO importance.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 全局特征重要性度量可以通过多种方式计算。许多数据科学家在学习决策树时首次接触特征重要性。决策树的一种常见特征重要性方法是根据某个特定特征在每个分裂中变化的分裂标准总和。例如，如果决策树（或树集合）被训练为最大化每个分裂的信息增益，那么分配给某个输入特征的特征重要性就是该特征在树（们）中每次使用时相关的总信息增益。基于扰动的特征重要性是另一种常见的特征重要性测量方法，它是一种与模型无关的技术，这意味着几乎可以用于所有类型的ML模型。在基于扰动的特征重要性中，会对感兴趣的输入特征进行洗牌（随机排序），并进行预测。通常是在洗牌感兴趣特征之前和之后的某些原始分数（通常是模型预测或类似均方误差（MSE）的东西）的差异即为特征重要性。另一种类似的方法被称为留一特征法（LOFO，或留一协变量法，LOCO）。在LOFO方法中，某个特征从模型的训练或预测中被某种方式移除——例如通过重新训练时不包含该特征并进行预测，或通过将特征设置为缺失并进行预测。在具有感兴趣特征和不具有感兴趣特征的模型之间相关得分的差异被视为LOFO重要性。
- en: While permutation and LOFO are typically used to measure the difference in predictions
    or the difference in an accuracy or error score, they have the advantage of being
    able to estimate the impact of a feature on nearly anything associated with a
    model. For instance, it’s quite possible to calculate permutation- or LOFO-based
    contributions to a fairness metric, allowing us to gain insight into which specific
    features are contributing to any detected sociological bias. This same motif can
    be reapplied for any number of measures of interest about a model—error functions,
    security, privacy, and more.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然排列和LOFO通常用于衡量预测差异或准确性或误差分数的差异，但它们的优势在于能够估计特征对模型相关任何事物的影响。例如，可以很容易地计算基于排列或LOFO的对公平度量的贡献，从而帮助我们洞察到哪些具体特征正在导致任何检测到的社会偏见。这一模式同样适用于关于模型的任何数量的感兴趣的测量标准——如误差函数、安全性、隐私等等。
- en: Note
  id: totrans-168
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Techniques like perturbation feature importance and LOFO can be used to estimate
    contributions to many quantities besides model predictions.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 类似扰动特征重要性和LOFO的技术可以用于估计除了模型预测之外的许多数量的贡献。
- en: Because these techniques are well-established, we can find a great deal of related
    information and software packages. For a great discussion on split-based feature
    importance, check out [Chapter 3](https://oreil.ly/P2gEb) of an *Introduction
    to Data Mining* (Pearson). Section 10.13.1 of [*Elements of Statistical Learning*](https://oreil.ly/jQOX6)
    introduces split-based feature importance, and Section 15.3.2 provides a brief
    introduction to permutation-based feature importance in the context of random
    forests. The R package [vip](https://oreil.ly/7Jo9s) provides a slew of *variable
    importance plots*, and we can try LOFO with the Python package [lofo-importance](https://oreil.ly/jP5jV).
    Of course, there are drawbacks and weaknesses to most global feature importance
    techniques. Split-based feature importance has serious consistency problems, and
    like so much of post hoc explainable AI (XAI), correlation will lead us astray
    with permutation-based and LOFO approaches. The paper [“There Is No Free Variable
    Importance”](https://oreil.ly/bx6QA) gets into more details of the sometimes disqualifying
    issues related to global feature importance. But, as is repeated many times in
    this chapter, constrained models with a reasonable number of noncorrelated and
    logical inputs will help us avoid the worst problems with global feature importance.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些技术已经得到很好的确认，我们可以找到大量相关的信息和软件包。关于基于分割的特征重要性的深入讨论，请查看《数据挖掘导论》（Pearson）的[第三章](https://oreil.ly/P2gEb)。《统计学习的要素》（Elements
    of Statistical Learning）的第10.13.1节介绍了基于分割的特征重要性，而第15.3.2节则简要介绍了随机森林背景下基于排列的特征重要性。R包[vip](https://oreil.ly/7Jo9s)提供了大量*变量重要性图*，我们也可以尝试Python包[lofo-importance](https://oreil.ly/jP5jV)进行LOFO分析。当然，大多数全局特征重要性技术都有其缺点和弱点。基于分割的特征重要性存在严重的一致性问题，而像大多数事后可解释AI（XAI）一样，基于排列和LOFO的方法也会因为相关性而导致我们偏离正确方向。文章[“没有免费的变量重要性”](https://oreil.ly/bx6QA)详细讨论了与全局特征重要性有关的有时排除问题。但是，正如本章中多次重复的那样，具有合理数量的非相关和逻辑输入的受限模型将帮助我们避免全局特征重要性的最严重问题。
- en: SHAP also has a role to play in global feature importance. SHAP is by nature
    a local feature attribution method, but it can be aggregated and visualized to
    create global feature importance information. Of the many benefits SHAP presents
    over more traditional feature importance measures, its interpretation is perhaps
    the most important. With split-based, permutation, and LOFO feature importance,
    oftentimes we only see a relative ordering of the importance of the input features,
    and maybe some qualitative notions of how a feature actually contributes to model
    predictions. With SHAP values, we can calculate the average absolute value of
    the feature attributions across a dataset, and this measure of feature importance
    has a clear and quantitative relationship with model predictions on individual
    observations. SHAP also provides many levels of granularity for feature importance.
    While SHAP can be directly aggregated into a feature importance value, that process
    can average out important local information. SHAP opens up the option of examining
    feature importance values anywhere from the most local level—a single row—to the
    global level. For instance, aggregating SHAP across important segments, like US
    states or different genders, or using the numerous visualizations in the SHAP
    package, can provide a view of feature importance that might be more informative
    and more representative than a single average absolute value. Like permutation
    and LOFO feature importance, SHAP can also be used to estimate importance of quantities
    besides model predictions. It’s capable of estimating contributions to [model
    errors](https://oreil.ly/oYG5d) and to fairness metrics, like [demographic parity](https://oreil.ly/4aHtK).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP 在全局特征重要性中也发挥了作用。SHAP 本质上是一种局部特征归因方法，但可以进行聚合和可视化，以创建全局特征重要性信息。与传统特征重要性措施相比，SHAP
    提供的许多优势中，其解释可能是最重要的。使用基于分割、置换和LOFO的特征重要性时，我们通常只看到输入特征重要性的相对顺序，也许会对特征如何实际贡献于模型预测有一些定性概念。通过SHAP值，我们可以计算整个数据集中特征归因的平均绝对值，这种特征重要性测量与单个观察结果上的模型预测有着清晰和定量的关系。SHAP
    还为特征重要性提供了许多粒度级别。虽然SHAP可以直接聚合成特征重要性值，但这一过程可能会平均掉重要的局部信息。SHAP提供了从最局部级别（单个行）到全局级别的特征重要性值的选项。例如，聚合SHAP跨重要部分（如美国各州或不同性别），或使用SHAP软件包中的众多可视化工具，可以提供比单一平均绝对值更具信息性和代表性的特征重要性视图。与置换和LOFO特征重要性一样，SHAP还可用于估计除了模型预测之外的重要性量。它能够估计对模型错误和公平性度量（如人口统计平等）的贡献。
- en: This concludes our discussion of feature importance. Whether it’s global or
    local, feature importance will probably be the first post hoc XAI technique we
    encounter when we’re building models. As this section shows, there’s a lot more
    to feature importance than just bar charts or running SHAP. To get the best results
    with feature importance, we’ll have to be familiar with the strengths and weaknesses
    of the many approaches. Next, we’ll be covering surrogate models—another intriguing
    explanation approach, but also one that requires thought and caution.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们关于特征重要性的讨论。无论是全局还是局部，特征重要性可能是我们在构建模型时首先遇到的后验XAI技术。正如本节所示，特征重要性远不止于条形图或运行SHAP。要在特征重要性方面取得最佳结果，我们必须熟悉许多方法的优缺点。接下来，我们将介绍替代模型——另一种引人注目的解释方法，但也需要深思熟虑。
- en: Surrogate Models
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 替代模型
- en: Surrogate models are simple models of complex models. If we can build a simple,
    interpretable model of a more complex model, we can use the explainable characteristics
    of the surrogate model to explain, summarize, describe, or debug the more complex
    model. Surrogate models are generally model agnostic. We can use them for almost
    any ML model. The problem with surrogate models is they are mostly a trick-of-the-trade
    technique, with few mathematical guarantees that they truly represent the more
    complex model they are attempting to summarize. That means we have to be careful
    when using surrogate models, and at a minimum, check that they are accurate and
    stable representations of the more complex models they seek to summarize. In practice,
    this often means looking at different types of accuracy and error measures on
    many different data partitions to ensure that fidelity to the more complex model’s
    predictions is high, and that it remains high in new data and stable during cross-validation.
    Surrogate models also have many names. Readers may have heard about *model compression*,
    *model distillation*, or *model extraction*. All of these either are surrogate
    modeling techniques or are closely related. Like feature importance, there are
    also many different types of surrogate models. In the sections that follow, we’ll
    start out with decision tree surrogate models, which are typically used to construct
    global explanations, then transition into LIME and anchors, which are typically
    used to generate local explanations.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 替代模型是复杂模型的简单模型。如果我们能构建一个简单且可解释的模型来代替更复杂的模型，我们可以利用替代模型的可解释特性来解释、总结、描述或调试更复杂的模型。替代模型通常是模型无关的。我们几乎可以将它们用于任何机器学习模型。替代模型的问题在于，它们大多是一种行业技巧，几乎没有数学上的保证能够真正代表它们试图总结的更复杂模型。这意味着在使用替代模型时，我们必须小心，并至少要检查它们是否是准确且稳定地代表了它们试图总结的更复杂模型。在实践中，这通常意味着在许多不同的数据分区上查看不同类型的准确性和误差测量，以确保对更复杂模型预测的忠实度高，并且在新数据中保持高并在交叉验证期间保持稳定。替代模型还有许多名称。读者可能听说过*模型压缩*、*模型蒸馏*或*模型提取*。所有这些要么是替代建模技术，要么与之密切相关。与特征重要性类似，还有许多不同类型的替代模型。在接下来的部分中，我们将首先介绍决策树替代模型，这些模型通常用于构建全局解释，然后过渡到LIME和anchors，这些模型通常用于生成局部解释。
- en: Decision tree surrogates
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策树替代
- en: Decision tree surrogate models are usually created by training a decision tree
    on the original inputs and predictions of a complex model. Feature importance,
    trends, and interactions displayed in the surrogate model are then assumed to
    be indicative of the internal mechanisms of the complex model. There are no theoretical
    guarantees that the simple surrogate model is highly representative of the more
    complex model. But, because of the structure of decision trees, these surrogate
    models create very interpretable flowcharts of a more complex model’s decision-making
    processes, as is visible in [Figure 2-5](#dt_surrogate). There are prescribed
    methods for training decision tree surrogate models, for example those explored
    in [“Extracting Tree-Structured Representations of Trained Networks”](https://oreil.ly/ewW3O)
    and [“Interpretability via Model Extraction”](https://oreil.ly/RnFep).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树替代模型通常是通过训练决策树使用复杂模型的原始输入和预测结果来创建的。在替代模型中显示的特征重要性、趋势和交互作用被认为是复杂模型内部机制的指示。没有理论保证简单的替代模型高度代表更复杂的模型。但是，由于决策树的结构，这些替代模型创建了非常可解释的流程图，显示了更复杂模型的决策过程，正如在[图2-5](#dt_surrogate)中可见。有关训练决策树替代模型的规定方法，例如在[“从训练网络中提取树结构表示”](https://oreil.ly/ewW3O)和[“通过模型提取实现可解释性”](https://oreil.ly/RnFep)中探讨的方法。
- en: '![mlha 0205](assets/mlha_0205.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![mlha 0205](assets/mlha_0205.png)'
- en: Figure 2-5\. A decision tree surrogate model creates a flowchart for a monotonic
    gradient boosting machine
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-5\. 决策树替代模型为单调梯度提升机创建了一个流程图
- en: Note
  id: totrans-179
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: Decision tree surrogate models can be highly interpretable when they create
    flowcharts of more complex models.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 当它们创建更复杂模型的流程图时，决策树替代模型可以是非常可解释的。
- en: In practice, it usually suffices to measure the fidelity of the surrogate tree’s
    predictions to the complex model’s predictions in the data partition of interest
    with metrics like logloss, root mean square error (RMSE), or R², and to measure
    the stability of those predictions with cross-validation. If a surrogate decision
    tree fails to provide high fidelity with respect to the more complex model, more
    sophisticated explainable models, like EBMs or XNNs, can be considered as surrogates
    instead.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，通常可以通过像对数损失、均方根误差（RMSE）或R²这样的度量指标来衡量代理树预测与数据分区中更复杂模型预测的保真度，并通过交叉验证来衡量这些预测的稳定性。如果代理决策树未能提供与更复杂模型相比的高保真度，可以考虑更复杂的可解释模型，如EBMs或XNNs，作为代理。
- en: The surrogate model in [Figure 2-5](#dt_surrogate) was trained on the same inputs
    as the more complex GBM it seeks to summarize, but instead of training on the
    original target that indicates payment delinquency, it is trained instead on the
    predictions of the GBM. When interpreting this tree, features that are higher
    or used more frequently are considered to be more important in the explained GBM.
    Features that are above and below one another can have strong interactions in
    the GBM, and other techniques discussed in this chapter, such as explainable boosting
    machines and a comparison between partial dependence and ICE, can be used to confirm
    the existence of those interactions in training data or in the GBM model.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-5](#dt_surrogate)中的代理模型是在与其要总结的更复杂的GBM相同的输入上训练的，但是与训练原始指示支付违约的目标不同，它是在GBM的预测上进行训练的。在解释这棵树时，被使用得更多或更频繁的特征被认为在解释GBM时更重要。在GBM中，相互位于上下的特征可能有强烈的相互作用，而本章还讨论了其他技术，如可解释性增强机器和偏依赖与ICE的比较，可以用来确认训练数据或GBM模型中存在这些相互作用。'
- en: Note
  id: totrans-183
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Decision tree surrogates can be used to find interactions for use in linear
    models or for LIMEs. EBMs and differences between partial dependence and ICE can
    also be used to find interactions in data and models.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树代理可以用来寻找与线性模型或LIMEs中使用的交互作用。EBMs和偏依赖与ICE之间的差异也可以用来在数据和模型中找到交互作用。
- en: The decision paths in the tree can also be used to gain some understanding of
    how the more complex GBM makes decisions. Tracing the decision path from the root
    node in [Figure 2-5](#dt_surrogate) to the average predictions at the bottom of
    the tree, we can see that those who have good statuses for their most recent (`PAY_0`)
    and second (`PAY_2`) most recent repayments and have somewhat large fifth most
    recent payment amounts (`PAY_AMT5`) are most likely not to have future delinquency,
    according to the original model. Those customers who have unfavorable most recent
    and fifth most recent repayment statuses appear most likely to have future payment
    problems. (The `PAY_3` splits exhibit a large amount of noise and are not interpreted
    here.) In both cases, the GBM appears to be considering both recent and past repayment
    behaviors to come to its decision about future payments. This prediction behavior
    is logical but should be confirmed by other means when possible. Like most surrogate
    models, decision tree surrogates are useful and highly interpretable, but should
    not be used for important explanation or interpretation tasks on their own.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 树中的决策路径也可以用来理解更复杂的GBM如何做出决策。从树的根节点到树底部的平均预测的决策路径追踪，我们可以看到，那些最近（`PAY_0`）和次近（`PAY_2`）还款状态良好，并且第五次最近的支付金额（`PAY_AMT5`）较大的客户，根据原始模型，最有可能不会有未来的违约问题。那些最近和第五最近还款状态不利的客户，似乎最可能有未来的支付问题。（`PAY_3`的分割显示了大量的噪音，这里不进行解释。）在这两种情况下，GBM似乎在考虑最近和过去的还款行为来做出关于未来支付的决策。这种预测行为是合乎逻辑的，但在可能的情况下应通过其他手段确认。与大多数代理模型一样，决策树代理是有用且高度可解释的，但不能单独用于重要的解释或解释任务。
- en: Linear models and local interpretable model-agnostic explanations
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性模型和局部可解释的模型无关解释
- en: LIME is one of the earliest, the most famous, and the most criticized post hoc
    explanation techniques. As the name indicates, it’s most often used for generating
    local explanations, by fitting a linear model to the predictions of some small
    region of a more complex model’s predictions. While this is its most common usage,
    it’s a reductionist take on the technique.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: LIME是最早、最著名且最受批评的事后解释技术之一。正如其名字所示，它通常用于生成局部解释，方法是将线性模型拟合到更复杂模型预测的某个小区域。虽然这是其最常见的用法，但这是该技术的简化视角。
- en: When first introduced in the 2016 article [“‘Why Should I Trust You?’ Explaining
    the Predictions of Any Classifier”](https://oreil.ly/e9WL2), LIME was presented
    as a framework with several admirable qualities. The most appealing of these was
    a sparsity requirement for local explanations. If our model has one thousand features
    and we apply SHAP, we will get back one thousand SHAP values for every prediction
    we want to explain. Even if SHAP is perfect for our data and model, we’ll still
    have to sort through one thousand values every time we want to explain a prediction.
    The framework of LIME circumvents this problem by requiring that generated explanations
    are sparse, meaning that they key into the small handful of locally important
    features instead of all the features included in the model to be explained.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 当首次在2016年的文章[“‘为什么要相信你？’解释任何分类器的预测”](https://oreil.ly/e9WL2)中介绍时，LIME被提出作为一个具有几个令人钦佩特质的框架。其中最吸引人的特性之一是对局部解释的稀疏性要求。如果我们的模型有一千个特征，我们应用SHAP，我们将得到每个预测的一千个SHAP值。即使SHAP对我们的数据和模型完美，我们每次想解释一个预测时仍需整理一千个值。LIME框架通过要求生成的解释是稀疏的来规避了这个问题，这意味着它们重点关注少数几个局部重要特征，而不是模型中包含的所有特征。
- en: Note
  id: totrans-189
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: A LIME value can be interpreted as the difference between a LIME prediction
    and the associated LIME intercept attributed to a certain input feature value.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: LIME值可以解释为LIME预测与相关输入特征值所归因的LIME截距之间的差异。
- en: The rest of the framework of LIME specifies fitting an interpretable surrogate
    model to some weighted local region of another model’s predictions. And that’s
    a more faithful description of the LIME framework—a locally weighted interpretable
    surrogate model with a penalty to induce sparsity, fit to some arbitrary, more
    complex model’s predictions. These ideas are useful and quite reasonable.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: LIME框架的其余部分指定了将可解释的替代模型拟合到另一个模型预测的加权局部区域。这是对LIME框架的更忠实的描述——一个带有罚项以诱导稀疏性的局部加权可解释替代模型，适用于某个任意的更复杂模型的预测。这些思想是有用且相当合理的。
- en: Warning
  id: totrans-192
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Always ensure LIME fits the underlying response function well with fit statistics
    and visualizations, and that the local model intercept is not explaining the most
    salient phenomenon driving a given prediction.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 始终确保LIME通过适当的拟合统计和可视化来很好地拟合基础响应函数，并且局部模型截距未解释某个给定预测中驱动最显著现象。
- en: 'It’s the popular implementation of LIME that gets inexperienced users into
    trouble and that presents security problems. For tabular data, the software package
    [lime](https://oreil.ly/dKYHV) asks users to select a row to be explained, generates
    a fairly simplistic sample of data based on a specified input dataset, weights
    the sample by the user-selected row, fits a LASSO regression between the weighted
    sample and the more complex model’s predictions on the sample, and finally, uses
    the LASSO regression coefficients to generate explanations for the user-specified
    row. There are a lot of potential issues in that implementation:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 正是LIME的流行实现让经验不足的用户陷入麻烦，并且带来安全问题。对于表格数据，软件包[lime](https://oreil.ly/dKYHV)要求用户选择要解释的行，基于指定的输入数据集生成相对简单的数据样本，通过用户选择的行对样本进行加权，将加权样本与更复杂模型在样本上的预测进行LASSO回归拟合，最后使用LASSO回归系数为用户指定的行生成解释。在这个实现中存在许多潜在问题：
- en: The sampling is a problem for real-time explanation because it requires data
    generation and fitting a model in the midst of a scoring pipeline, and it also
    opens users up to data poisoning attacks that can alter explanations.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于实时解释而言，采样是一个问题，因为它需要数据生成并在评分管道中拟合模型，并且还会使用户遭受可能改变解释的数据毒化攻击的风险。
- en: Generated LIME samples can contain large proportions of out-of-range data that
    can lead to unrealistic local feature importance values.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成的LIME样本可能包含大量超出范围数据，可能导致不现实的局部特征重要性值。
- en: Local feature importance values are offsets from the local GLM intercept, and
    this intercept can sometimes account for the most important local phenomena.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 局部特征重要性值是从局部GLM截距偏移而来的，有时这个截距可以解释最重要的局部现象。
- en: Extreme nonlinearity and high-degree interactions in the selected local region
    of predictions can cause LIME to fail completely.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选定预测局部区域内的极端非线性和高度交互可能会导致LIME完全失败。
- en: Because LIME can be used on almost any type of ML model to generate sparse explanations,
    it can still be a good tool in our kit if we’re willing to be patient and think
    through the LIME process. If we need to use LIME, we should plot the LIME predictions
    versus our more complex model predictions and analyze them with RMSE, R², or similar.
    We should be careful about the LIME intercept and make sure that it’s not explaining
    our prediction on its own, rendering the actual LIME values useless. To increase
    the fidelity of LIME, try LIME on discretized input features and on manually constructed
    interactions. (We can use decision tree surrogates to guess at those interactions.)
    Use cross-validation to estimate standard deviations or even confidence intervals
    for local feature contribution values. And keep in mind that poor fit or inaccuracy
    of local linear models is itself informative, often indicating extreme nonlinearity
    or high-degree interactions in that region of predictions.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 因为LIME可以用于几乎任何类型的机器学习模型来生成稀疏解释，所以如果我们愿意耐心地思考LIME的过程，它仍然可以是我们工具箱中的好工具。如果我们需要使用LIME，我们应该绘制LIME预测与我们更复杂的模型预测之间的图，并用RMSE、R²或类似方法分析它们。我们应该注意LIME截距，并确保它不是单独解释我们的预测，从而使实际的LIME值变得无用。为了增加LIME的准确性，尝试在离散化的输入特征和手动构建的交互上应用LIME（我们可以使用决策树替代模型来猜测这些交互）。使用交叉验证来估计局部特征贡献值的标准偏差甚至置信区间。请记住，局部线性模型的拟合不佳或不准确本身就具有信息性，通常表明预测区域内的极端非线性或高度交互。
- en: Anchors and rules
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Anchors和规则
- en: 'On the heels of LIME, probably with some lessons in mind, the same group of
    researchers released another model-agnostic local post hoc explanation technique
    named anchors. Anchors generates high-fidelity sets of plain-language rules to
    describe a machine learning model prediction, with a special focus on finding
    the most important features for the prediction at hand. Readers can learn more
    about anchors in [“Anchors: High-Precision Model-Agnostic Explanations”](https://oreil.ly/V1rFJ)
    and the software package [anchor](https://oreil.ly/KNGF3). While anchors is a
    prescribed technique with documented strengths and weaknesses, it’s just one special
    instance of using rule-based models as surrogate models. As discussed in the first
    part of the chapter, rule-based models have good learning capacity for nonlinearities
    and interactions, while still being generally interpretable. Many of the rule-based
    models highlighted previously could be evaluated as surrogate models.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '在LIME之后，可能有一些教训在心中，同一组研究人员发布了另一种模型无关的局部后处理解释技术，名为anchors。Anchors生成高保真度的简明语言规则集，用于描述机器学习模型预测，特别关注于找到该预测最重要的特征。读者可以在[“Anchors:
    High-Precision Model-Agnostic Explanations”](https://oreil.ly/V1rFJ)和软件包[anchor](https://oreil.ly/KNGF3)中了解更多关于anchors的信息。虽然anchors是一种有文档记录的技术，具有其优点和缺点，但它只是使用基于规则的模型作为替代模型的一个特殊实例。正如本章的第一部分讨论的那样，基于规则的模型对非线性和交互有良好的学习能力，同时仍然普遍可解释。以前突出显示的许多基于规则的模型可以作为替代模型进行评估。'
- en: Plots of Model Performance
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型性能图
- en: In addition to feature importance and surrogate models, partial dependence,
    individual conditional expectation, and accumulated local effects (ALE) plots
    have become popular for describing trained model behaviors with respect to input
    features. In this section, we’ll go over partial dependence and ICE, how partial
    dependence should really only be used with ICE, and discuss ALE as a more contemporary
    replacement for partial dependence.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 除了特征重要性和替代模型之外，部分依赖（partial dependence）、个体条件期望（individual conditional expectation）以及累积局部效应（ALE）图已经流行起来，用于描述训练模型关于输入特征的行为。在本节中，我们将讨论部分依赖和ICE，以及部分依赖真正应该只与ICE一起使用，并讨论ALE作为部分依赖的更现代替代品。
- en: Partial dependence and individual conditional expectation
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部分依赖和个体条件期望
- en: 'Partial dependence plots show us the estimated average manner in which machine-learned
    response functions change based on the values of one or two input features of
    interest, while averaging out the effects of all other input features. Remember
    the averaging-out part. We’ll circle back to that. Partial dependence plots can
    show the nonlinearity, nonmonotonicity, and two-way interactions in complex ML
    models and can be used to verify monotonicity of response functions trained under
    monotonic constraints. Partial dependence is introduced along with tree ensembles
    in [*Elements of Statistical Learning*](https://oreil.ly/35vig), Section 10.13\.
    ICE plots are a newer, local, and less well-known adaptation of partial dependence
    plots. They depict how a model behaves for a single row of data as one feature
    is changed. ICE pairs nicely with partial dependence in the same plot to provide
    more local information to augment the more global information provided by partial
    dependence. ICE plots were introduced in the paper [“Peeking Inside the Black
    Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation”](https://oreil.ly/Vuraz).
    There are lots of software packages for us to try partial dependence and ICE.
    For Python users, check out [PDPbox](https://oreil.ly/RbzII) and [PyCEbox](https://oreil.ly/KcdET).
    For R users, there are the [pdp](https://oreil.ly/pE0Ue) and [ICEbox](https://oreil.ly/QbsDA)
    packages. Also, many modeling libraries support partial dependence without having
    to use an external package.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 部分依赖图显示了我们基于感兴趣的一个或两个输入特征的值的机器学习响应函数的估计平均方式，同时平均其他所有输入特征的效果。记住平均化部分。我们将回到这一点。部分依赖图可以展示复杂机器学习模型中的非线性、非单调性和双向交互，并且可以用来验证在单调约束下训练的响应函数的单调性。部分依赖与树集成一起在[*统计学习要素*](https://oreil.ly/35vig)第10.13节中引入。ICE图是部分依赖图的一个更新、局部且较少为人知的适应。它们描述了当改变一个特征时模型如何为数据的单个行为。ICE与部分依赖很好地配对在同一图中，以提供更多局部信息，以增补部分依赖提供的更全局信息。ICE图首次在论文[“窥探黑匣子内部：通过个体条件期望图可视化统计学习”](https://oreil.ly/Vuraz)中引入。有许多软件包供我们尝试部分依赖和ICE。对于Python用户，请查看[PDPbox](https://oreil.ly/RbzII)和[PyCEbox](https://oreil.ly/KcdET)。对于R用户，有[pdp](https://oreil.ly/pE0Ue)和[ICEbox](https://oreil.ly/QbsDA)软件包。此外，许多建模库支持部分依赖，无需使用外部软件包。
- en: Partial dependence should be paired with ICE plots, as ICE plots can reveal
    inaccuracies in partial dependence due to the averaging-out of strong interactions
    or the presence of correlation. When ICE curves diverge from partial dependence
    curves, this can indicate strong interactions between input features, which is
    another advantage of using them together. We can then use EBMs or surrogate decision
    trees to confirm the existence of the interaction in training data or the model
    being explained. One more trick is to plot partial dependence and ICE with a histogram
    of the feature of interest. That gives good insight into whether any plotted prediction
    is trustworthy and supported by training data. In [Figure 2-6](#pd_ice), partial
    dependence, ICE, and a histogram of `PAY_0` are used to summarize the behavior
    of a monotonic GBM.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 应将部分依赖与ICE图配对使用，因为ICE图可以显示由于强交互或相关性导致部分依赖的平均化而产生的不准确性。当ICE曲线与部分依赖曲线分歧时，这可能表明输入特征之间存在强交互，这是一起使用它们的另一个优势。然后，我们可以使用EBMs或替代决策树来确认训练数据或被解释模型中的交互存在。另一个技巧是将部分依赖和ICE与感兴趣特征的直方图一起绘制。这可以深入了解任何绘制的预测是否可信，并得到训练数据的支持。在[图2-6](#pd_ice)中，部分依赖、ICE和`PAY_0`的直方图用于总结单调GBM的行为。
- en: '![mlha 0206](assets/mlha_0206.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![mlha 0206](assets/mlha_0206.png)'
- en: Figure 2-6\. Partial dependence and ICE for an important input variable with
    accompanying histogram and mean target value overlay for a monotonic gradient
    boosting machine ([digital, color version](https://oreil.ly/zFr70))
  id: totrans-208
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-6\. 对于一个重要的输入变量，部分依赖和ICE，附带直方图和均值目标值叠加，用于单调梯度增强机器 ([数字，彩色版本](https://oreil.ly/zFr70))
- en: Warning
  id: totrans-209
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Due to multiple known weaknesses, partial dependence should not be used without
    ICE, or ALE should be used in place of partial dependence.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 由于已知的多个弱点，不应在没有ICE的情况下使用部分依赖，或者应该使用ALE代替部分依赖。
- en: On the top, we can see a histogram of `PAY_0` and that there is simply not much
    data for customers who are more than two months late on their most recent payment.
    On the bottom, we see partial dependence and ICE curves for customers at the deciles
    of predicted probability of default. Partial dependence and ICE help us confirm
    the monotonicity of the constrained GBM’s response function for `PAY_0`. The model
    appears to behave reasonably, even when there is not much data to learn from for
    higher values of `PAY_0`. Probability of default increases in a monotonic fashion
    as customers become more late on their most recent payment, and probability of
    default is stable for higher values of `PAY_0`, even though there is almost no
    data to support the classifier in that region. It might be tempting to believe
    that every time we use monotonic constraints we would be protected against our
    ML models learning silly behaviors when there isn’t much training data for them
    to learn from, but this is not true. Yes, monotonic constraints help with stability
    and underspecification problems, and partial dependence and ICE help us spot these
    problems if they occur, but we got lucky here. The truth is we need to check all
    our models for unstable behavior in sparse domains of the training data, and be
    prepared to have specialized models, or even human case workers, ready to make
    good predictions for these difficult rows of data.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部，我们可以看到`PAY_0`的直方图，并且只有很少的数据是针对那些最近两个月内付款超过两个月的客户。在底部，我们看到了在预测违约概率十分位点上的部分依赖和ICE曲线。部分依赖和ICE帮助我们确认了`PAY_0`的受限GBM响应函数的单调性。即使在`PAY_0`较高值的数据较少时，模型似乎表现合理。随着客户最近付款越来越晚，违约概率呈单调增加，而在较高的`PAY_0`值时，违约概率稳定，尽管几乎没有数据支持该分类器在该区域内的行为。也许我们会认为每次使用单调约束时，我们都能保护我们的ML模型免受在没有足够训练数据的情况下学习愚蠢行为的影响，但事实并非如此。是的，单调约束有助于稳定性和欠确定性问题，部分依赖和ICE帮助我们发现这些问题（如果发生的话），但我们在这里算是幸运的。事实是，我们需要检查所有模型在训练数据稀疏区域内的不稳定行为，并准备好专门的模型，甚至是人工案例工作者，以便为这些困难的数据行做出良好的预测。
- en: Note
  id: totrans-212
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Comparing explainable model shape functions, partial dependence, ICE, or ALE
    plots with a histogram can give a basic qualitative measure of uncertainty in
    model outcomes, by enabling visual discovery of predictions that are based on
    only small amounts of training data.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较可解释模型的形状函数、部分依赖、ICE或ALE图与直方图，可以通过视觉发现仅基于少量训练数据的预测，从而提供模型结果不确定性的基本定性度量。
- en: 'Here’s one more word of advice before moving onto ALE plots: like feature importance,
    SHAP, LIME, and all other explanation techniques that operate on a background
    dataset, we have to think through issues of context with partial dependence and
    ICE. They both use sneaky implicit background data. For partial dependence, it’s
    whatever dataset we’re interested in with all the values of the feature being
    plotted set to a certain value. This alters patterns of interactions and correlation,
    and although it’s an exotic concern, it opens us up to data poisoning attacks
    as addressed in [“Fooling Partial Dependence via Data Poisoning”](https://oreil.ly/SVFmU).
    For ICE, the implicit background dataset is a single row of data with the feature
    of interest set to a certain value. Watch out for the ICE values being plotted
    being too unrealistic in combination with the rest of the observed data in that
    row.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在移动到ALE图之前，再给一个建议：就像特征重要性、SHAP、LIME以及所有操作于背景数据集上的解释技术一样，我们必须思考部分依赖和ICE的上下文问题。它们都使用了一些隐含的背景数据。对于部分依赖来说，这是我们感兴趣的任何数据集，其所有要绘制特征的值均设置为某个特定值。这改变了交互和相关性的模式，尽管这是一个异国情调的问题，但它使我们容易受到数据毒化攻击的影响，正如[“通过数据毒化愚弄部分依赖”](https://oreil.ly/SVFmU)中所述。对于ICE来说，隐含的背景数据集是单行数据，其感兴趣特征设置为某个特定值。请注意ICE值的绘制是否与该行中观察到的其余数据组合得太不现实。
- en: Accumulated local effect
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 累积局部效应
- en: ALE is a newer and highly rigorous method for representing the behavior of an
    ML model across the values of an input feature, introduced in [“Visualizing the
    Effects of Predictor Variables in Black Box Supervised Learning Models”](https://oreil.ly/TFFTK).
    Like partial dependence plots, ALE plots show the shape—i.e., nonlinearity or
    nonmonotonicity—of the relationship between predictions and input feature values.
    ALE plots are especially valuable when strong correlations exist in the training
    data, a situation where partial dependence is known to fail. ALE is also faster
    to calculate than partial dependence. Try it with [ALEPlot](https://oreil.ly/4o0wH)
    in R, and the Python edition [ALEPython](https://oreil.ly/_qKs8).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ALE是一种较新且高度严格的方法，用于表示ML模型在输入特征值的各个值上的行为，引入自["可视化黑盒监督学习模型中预测变量效应"](https://oreil.ly/TFFTK)。与部分依赖图类似，ALE图展示了预测与输入特征值之间的关系形状，即非线性或非单调性。当训练数据中存在强相关性时，ALE图尤为重要，而这种情况下部分依赖通常无法胜任。与部分依赖相比，ALE计算速度更快。在R中试试[ALEPlot](https://oreil.ly/4o0wH)，以及Python版的[ALEPython](https://oreil.ly/_qKs8)。
- en: Cluster Profiling
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[聚类分析](https://oreil.ly/_qKs8)'
- en: While a great deal of focus has been placed on explaining supervised learning
    models, sometimes we need to use unsupervised techniques. Feature extraction and
    clustering are two of the most common unsupervised learning tasks. We discussed
    how to make feature extraction more explainable with sparse methods like SPCA
    and NMF when we covered explainable models. And with the application of very much
    established post hoc methods of profiling, clustering can often be made more transparent
    too. The simplest approach is to use means and medians to describe cluster centroids,
    or to create prototypical members of a dataset based on clusters. From there,
    we can use concepts associated with prototypes such as summarization, comparison,
    and criticisms to better understand our clustering solution. Another technique
    is to apply feature extraction, particularly sparse methods, to project a higher-dimensional
    clustering solution into two or three dimensions for plotting. Once plotted on
    sparse interpretable axes, it’s easier to use our domain knowledge to understand
    and check a group of clusters. Distributions of features can also be employed
    to understand and describe clusters. The density of a feature within a cluster
    can be compared to its density in other clusters or to its overall distribution.
    Features with the most dissimilar distributions versus other clusters or the entire
    training data can be seen as more important to the clustering solution. Finally,
    surrogate models can be applied to explain clusters. Using the same inputs as
    the clustering algorithm and the cluster labels as the target, we fit an interpretable
    classifier like a decision tree to our clusters and use the surrogate model’s
    interpretable characteristics to gain insight into our clustering solution.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经花了大量精力解释监督学习模型，但有时我们需要使用无监督技术。特征提取和聚类是两种最常见的无监督学习任务。我们在讨论可解释模型时探讨了如何通过稀疏方法如SPCA和NMF使特征提取更易于理解。并且通过应用非常成熟的事后方法来进行特征提取，聚类也常常能更加透明。最简单的方法是使用均值和中位数描述聚类中心，或者根据聚类创建数据集的原型成员。从这里开始，我们可以利用原型相关的概念如总结、比较和批评来更好地理解我们的聚类解决方案。另一种技术是应用特征提取，特别是稀疏方法，将高维聚类解决方案投影到二维或三维用于绘图。一旦在稀疏可解释的轴上绘制，使用领域知识来理解和检查一组聚类就更容易了。还可以使用特征分布来理解和描述聚类。可以将特征在聚类内的密度与其在其他聚类或整体分布中的密度进行比较。与其他聚类或整个训练数据具有最不相似分布的特征可以视为聚类解决方案中更重要的特征。最后，可以应用替代模型来解释聚类。使用与聚类算法相同的输入和聚类标签作为目标，我们适配一个可解释的分类器如决策树到我们的聚类，并利用替代模型的可解释特性来深入了解我们的聚类解决方案。
- en: Stubborn Difficulties of Post Hoc Explanation in Practice
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[实践中的事后解释的顽固困难](https://oreil.ly/TFFTK)'
- en: 'Unless we’re careful, we can get into very murky areas with post hoc explanation.
    We’ve taken care to discuss the technical drawbacks of the techniques, but there
    is even more to consider when working with these techniques on real-world high-risk
    applications. As a refresher, Professor Rudin’s [“Stop Explaining Black Box Machine
    Learning Models for High Stakes Decisions and Use Interpretable Models Instead”](https://oreil.ly/vR4Xl)
    lays out the primary criticisms of post hoc explanation for opaque ML models and
    high-risk uses. According to Rudin, explanations for traditional ML models are:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 除非我们小心，否则后事后解释可能会陷入非常模糊的领域。我们已经注意到了讨论技术方法的缺陷，但在处理这些技术在现实世界高风险应用时，还有更多需要考虑的因素。作为一个提醒，鲁丁教授的
    [“停止解释黑盒机器学习模型用于高风险决策，而改用可解释模型”](https://oreil.ly/vR4Xl) 概述了后事后解释在不透明机器学习模型和高风险用途中的主要批评。根据鲁丁的观点，传统机器学习模型的解释：
- en: Premised on the wrong belief that unexplainable models are more accurate than
    explainable models
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立在错误的信念上，即不可解释的模型比可解释的模型更准确
- en: Not faithful enough to the actual inner workings of complex models
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不够忠实于复杂模型的实际内部工作
- en: Often nonsensical
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经常是荒谬的
- en: Difficult to calibrate against external data
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 难以校准对外部数据
- en: Unnecessarily complicated
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过于复杂
- en: That’s why in this chapter we advocate for the use of post hoc explanation with
    interpretable models, where the model and the explanations can act as process
    controls for one another. Even using explanations in this more risk-aware manner,
    there are still serious issues to address. This section will highlight the concerns
    we see the most in practice. We’ll close this section by highlighting the advantages
    of using explainable models and post hoc explanations in combination. But as the
    transparency case will show, even if we get things mostly right on the technical
    side of transparency, human factors are still immensely important to the final
    success, or failure, of a high-stakes ML application.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本章中，我们主张使用后事后解释与可解释模型结合使用，其中模型和解释可以相互作为过程控制。即使以这种更加风险感知的方式使用解释，仍然存在严重的问题需要解决。本节将重点介绍我们在实践中最常见的关注点。我们将通过强调在组合使用可解释模型和后事后解释的优势时，结束本节。但是，正如透明度案例将展示的那样，即使在技术透明度的技术方面大部分正确，人为因素对于最终的成功或失败仍然至关重要，尤其是在高风险的机器学习应用中。
- en: 'Christoph Molnar has not only been prolific in teaching us how to use explanations;
    he and coauthors have also been quite busy researching their drawbacks. If readers
    would like to dig into details of issues with common explanation approaches, we’d
    suggest both [“General Pitfalls of Model-Agnostic Interpretation Methods for Machine
    Learning Models”](https://oreil.ly/DeZ0J) and the earlier [*Limitations of Interpretable
    Machine Learning Methods*](https://oreil.ly/HYgJ6). Next we’ll outline the issues
    we see the most in practice—confirmation bias, context, correlation and local
    dependence, hacks, human interpretation, inconsistency, and explanation fidelity:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: Christoph Molnar 不仅在教我们如何使用解释方面很多产出；他和合著者们也很忙于研究它们的缺陷。如果读者希望深入了解常见解释方法的问题细节，我们建议阅读
    [“机器学习模型中模型无关解释方法的一般缺陷”](https://oreil.ly/DeZ0J) 和早期的 [*可解释机器学习方法的局限性*](https://oreil.ly/HYgJ6)。接下来，我们将概述我们在实践中最常见的问题：确认偏见、背景、相关性和局部依赖、黑客行为、人类解释、不一致以及解释的忠实度：
- en: Confirmation bias
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 确认偏见
- en: For most of this chapter, we’ve discussed increased transparency as a good thing.
    While it certainly is, increased human understanding of ML models, and the ability
    to intervene in the functioning of those models, does open cracks for confirmation
    bias to sneak into our ML workflow. For example, let’s say we’re convinced a certain
    interaction should be represented in a model based on past experience in similar
    projects. However, that interaction just isn’t appearing in our explainable model
    or post hoc explanation results. It’s extremely hard to know if our training data
    is biased, and missing the known important interaction, or if we’re biased. If
    we intervene in the mechanisms of our model to somehow inject this interaction,
    we could simply be succumbing to our own confirmation bias.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的大部分内容中，我们讨论了增加透明度作为一件好事。虽然这当然是，增加人类对机器学习模型的理解能力以及能够干预这些模型的能力确实为我们的机器学习工作流程开了一些存在确认偏差的裂缝。例如，假设我们坚信某种互动应该在模型中得到表现，基于类似项目的过去经验。然而，这种互动却没有出现在我们的可解释模型或事后解释的结果中。我们很难知道我们的训练数据是否存在偏见，是否遗漏了已知的重要互动，或者我们是否有偏见。如果我们干预我们模型的机制以某种方式注入这种互动，我们可能只是屈从于自己的确认偏差。
- en: Of course, a total lack of transparency also allows confirmation bias to run
    wild, as we can spin a model’s behavior in whatever way we like. The only real
    way to avoid confirmation bias is to stick to the scientific method and battle-tested
    scientific principles like transparency, verification, and reproducibility.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，完全缺乏透明度也会允许确认偏差肆意妄为，因为我们可以随意解释模型的行为。避免确认偏差的唯一真正方法是坚持科学方法和经过实践检验的科学原则，如透明度、验证性和可重复性。
- en: Context
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 背景
- en: '[“Do not explain without context”](https://oreil.ly/A-GxX), say Dr. Przemysław
    Biecek and team. In practice, this means using logical and realistic background
    data to generate explanations, and making sure background data cannot be manipulated
    by adversaries. Even with solid background data for explanations, we still need
    to ensure our underlying ML models are operating in a logical context as well.
    For us, this means a reasonable number of uncorrelated input features, all with
    direct relationships to the modeling target.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '[“不要缺乏背景的解释”](https://oreil.ly/A-GxX)，Przemysław Biecek 博士及其团队说道。在实践中，这意味着使用逻辑和现实的背景数据生成解释，并确保背景数据不能被对手操纵。即使对解释有坚实的背景数据，我们仍然需要确保我们的基础机器学习模型也在一个逻辑上下文中运行。对我们来说，这意味着有合理数量的不相关输入特征，所有这些特征都与建模目标有直接关系。'
- en: Correlation and dependencies
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 相关性和依赖关系
- en: While correlation may not prevent ML algorithms from training and generating
    accurate in silico predictions in many cases, it does make explanation and interpretation
    very difficult. In large datasets, there are typically many correlated features.
    Correlation violates the principle of independence, meaning we can’t realistically
    interpret features on their own. When we attempt to remove a feature, as many
    explanation techniques do, another correlated feature will swoop in and take its
    place in the model, nullifying the effect of the attempted removal and the removal’s
    intended meaning as an explanation tool. We also rely on perturbing features in
    explanations, but if features are correlated, it makes very little sense to perturb
    just one of them to derive an explanation. Worse, when dealing with ML models,
    they can learn local dependencies, meaning different correlation-like relationships,
    on a row-by-row basis. It’s almost impossible to think through the complexities
    of how correlation corrupts explanations, much less how complex local dependencies
    might do the same.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在许多情况下，相关性可能不会阻止机器学习算法进行训练并生成准确的计算机预测，但它确实使解释和解释非常困难。在大型数据集中，通常存在许多相关特征。相关性违反了独立性原则，这意味着我们不能真实地单独解释特征。当我们试图移除一个特征时，就像许多解释技术所做的那样，另一个相关的特征将会进入并取代它在模型中的位置，从而使尝试移除的效果和移除本意的解释工具无效。我们还依赖于扰动特征来解释，但如果特征之间存在相关性，仅仅扰动其中一个特征来导出解释是毫无意义的。更糟糕的是，在处理机器学习模型时，它们可以学习局部依赖关系，这意味着在逐行基础上存在不同的类似相关的关系。理解相关性如何破坏解释的复杂性几乎是不可能的，更不用说复杂的局部依赖可能会产生相同的影响了。
- en: Hacks
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Hacks
- en: 'Explanation techniques that use background data can be altered by adversaries.
    These include LIME and SHAP, as explored in [“Fooling LIME and SHAP: Adversarial
    Attacks on Post hoc Explanation Methods”](https://oreil.ly/ljDkp), and partial
    dependence, as described in [“Fooling Partial Dependence via Data Poisoning”](https://oreil.ly/MJUz7).
    While these hacks are likely an exotic concern for now, we don’t want to be part
    of the first major hack on ML explanations. Make sure that the code used to generate
    background data is kept secure and that background data cannot be unduly manipulated
    during explanation calculations. Data poisoning, whether against training data
    or background data, is easy for inside attackers. Even if background data is safe,
    explanations can still be misinterpreted in malicious ways. In what’s known as
    [*fairwashing*](https://oreil.ly/RBwSb), explanations for a sociologically biased
    ML model are made to look fair, abusing explanations to launder bias while still
    exposing model users to real harm.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '利用背景数据的解释技术可能会受到对手的修改。这些技术包括LIME和SHAP，如在[“Fooling LIME and SHAP: Adversarial
    Attacks on Post hoc Explanation Methods”](https://oreil.ly/ljDkp)中所探讨的，以及局部依赖，如在[“Fooling
    Partial Dependence via Data Poisoning”](https://oreil.ly/MJUz7)中所述。虽然这些黑客攻击目前可能是个奇特的担忧，但我们不希望成为ML解释领域的首次重大黑客攻击的一部分。确保用于生成背景数据的代码是安全的，并且在解释计算过程中背景数据不能被不当操纵。无论是针对训练数据还是背景数据，数据污染对内部攻击者来说都是容易的。即使背景数据是安全的，解释仍可能以恶意方式被误解释为所谓的“公平洗白”，即滥用解释来洗白偏见，同时仍然将模型用户暴露于真实危害之中。'
- en: Human interpretation
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 人类解释
- en: 'ML is difficult to understand, sometimes even for experienced practitioners
    and researchers. Yet, the audience for ML explanations is much broader than just
    industry experts. High-risk applications of ML often involve important decisions
    for other human beings. Even if those other human beings are highly educated,
    we cannot expect them to understand a partial dependence and ICE plot or an array
    of SHAP values. To get transparency right for high-stakes situations, we’ll need
    to work with psychologists, domain experts, designers, user interaction experts,
    and others. It will take extra time and product iterations, with extensive communications
    between technicians, domain experts, and users. Not doing this extra work can
    result in abject failure, even if technical transparency goals are met, as [“Case
    Study: Graded by Algorithm”](#case_study_algorithm) discusses.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '对于经验丰富的从业者和研究人员来说，机器学习有时候也很难理解。然而，ML解释的受众远不止行业专家。ML的高风险应用通常涉及对其他人类重要的决策。即使这些人高度受过教育，我们也不能指望他们理解局部依赖和ICE图或一系列SHAP值。为了在高风险情况下正确进行透明度的展示，我们需要与心理学家、领域专家、设计师、用户交互专家等人员合作。这将需要额外的时间和产品迭代，以及技术人员、领域专家和用户之间的广泛沟通。如果不进行这项额外工作，即使实现了技术透明度的目标，也可能导致彻底的失败，正如[“Case
    Study: Graded by Algorithm”](#case_study_algorithm)所讨论的那样。'
- en: Inconsistency
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 不一致
- en: Consistency refers to stable explanations across different models or data samples.
    Consistency is difficult to achieve and very important for high-stakes ML applications.
    In situations like credit or pretrial release decisions, people might be subject
    to multiple automated decisions and associated explanations, and especially so
    in a more automated future. If the explanations provide different reasons for
    the same outcome decision, this will complicate already difficult situations.
    To increase consistency, explanations need to key into real, generalizable phenomena
    in training data and in the application domain. To achieve consistency, we need
    to train our models on a reasonable number of independent features. The models
    themselves also need to be parsimonious, that is, constrained to obey real-world
    relationships. Conversely, consistent explanations are impossible for complex,
    underspecified, uninterpretable models with numerous and correlated inputs.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性指的是在不同模型或数据样本中稳定的解释。一致性难以实现，并且对于高风险的机器学习应用非常重要。在像信用或预审判释放决定等情况下，人们可能会受到多个自动化决策及其相关解释的影响，尤其在未来更加自动化的情况下如此。如果解释为同一结果决策提供了不同的原因，将会使本已困难的情况变得更加复杂。为了增强一致性，解释需要针对训练数据和应用领域中的真实、可泛化的现象进行关键性调整。为了实现一致性，我们需要在相当数量的独立特征上训练我们的模型。模型本身也需要是简约的，即受限于遵守现实世界的关系。相反，对于复杂、未明确规范、难以解释的具有大量相关输入的模型来说，一致的解释是不可能的。
- en: Measuring explanation quality
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量解释质量
- en: Imagine training a model, eyeballing the results, and then assuming it’s working
    properly and deploying it. That’s likely a bad idea. But that’s how we all work
    with post hoc explanations. Given all the technical concerns raised in previous
    sections, we obviously need to test explanations and see how they work for a given
    data source and application, just like we do with any other ML technique. Like
    our efforts to measure model quality before we deploy, we should be making efforts
    to measure explanation quality too. There are published proposals for such measurements
    and commonsense testing techniques we can apply. [“Towards Robust Interpretability
    with Self-Explaining Neural Networks”](https://oreil.ly/t4322) puts forward explicitness,
    i.e., whether explanations are immediately understandable; faithfulness, i.e.,
    whether explanations are true to known important factors; and stability, i.e.,
    whether explanations are consistent with neighboring data points. [“On the (In)fidelity
    and Sensitivity of Explanations”](https://oreil.ly/kSiSS) introduces related eponymous
    tests. Beyond these proposals for formal measurement, we can check that explainable
    model mechanisms and post hoc explanations confirm one another if we use both
    explainable models and explanations. If older trustworthy explanations are available,
    we can use those as benchmarks against which to test new explanations for fidelity.
    Also, stability tests, in which the data or model is perturbed in small ways,
    should generally not lead to major changes in post hoc explanations.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下训练模型，审视结果，然后假设它正常工作并部署它。这很可能是一个糟糕的主意。但这就是我们所有人如何处理事后解释的方式。鉴于前面章节中提出的所有技术问题，我们显然需要测试解释，并查看它们在给定数据源和应用程序中的工作情况，就像我们对任何其他机器学习技术一样。像在部署之前测量模型质量一样，我们应该也在努力测量解释质量。已经有提出这种测量和常识测试技术的公开建议。["Towards
    Robust Interpretability with Self-Explaining Neural Networks"](https://oreil.ly/t4322)提出了显式性，即解释是否立即可理解；忠实性，即解释是否符合已知重要因素；以及稳定性，即解释是否与相邻数据点一致。["On
    the (In)fidelity and Sensitivity of Explanations"](https://oreil.ly/kSiSS)介绍了相关的同名测试。除了这些正式测量提议之外，如果我们同时使用可解释模型和解释，我们可以检查可解释模型机制和事后解释是否相互确认。如果存在较老的可信解释，我们可以将其用作测试新解释忠实度的基准。此外，稳定性测试通常不应导致事后解释发生重大变化，这种测试中，数据或模型以微小方式扰动。
- en: Warning
  id: totrans-243
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Test explanations before deploying them in high-risk use cases. While the lack
    of ground truth for explanations is a difficult barrier, explanations should be
    compared to interpretable model mechanisms. Comparisons to benchmark explanations,
    explicitness and fidelity measures, perturbation, comparison to nearest neighbors,
    and simulated data can also be used to test explanation quality.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在高风险用例中部署之前，请测试解释。尽管解释缺乏地面真实性是一个困难的障碍，但应将解释与可解释的模型机制进行比较。可以使用与基准解释的比较、显式性和保真度测量、扰动、与最近邻的比较以及模拟数据来测试解释质量。
- en: There’s no denying the appeal of explaining any model, no matter how complex,
    simply by applying some postprocessing. But given all the technical and worldly
    problems we’ve just been over, hopefully we’ve convinced readers that traditional
    ML model explanation is kind of a pipe dream. Explaining the unexplainable might
    not be impossible, but it’s technically difficult today, and once we consider
    all the human factors required to achieve real-world transparency, it becomes
    even more difficult.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，通过应用一些后处理方法来解释任何模型的吸引力是不可否认的。但考虑到我们刚刚讨论过的所有技术和世俗问题，希望我们已经说服读者传统的机器学习模型解释有点空想。解释不可解释的可能并非不可能，但在今天技术上很困难，一旦我们考虑到实现现实世界透明度所需的所有人为因素，它变得更加困难。
- en: Pairing Explainable Models and Post Hoc Explanation
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结合可解释模型和事后解释
- en: As we end the chapter’s more technical discussions, we’d like to highlight new
    research that helps elucidate why explaining traditional ML models is so difficult
    and leave readers with an example of combining explainable models and post hoc
    explanations. Two recent papers relate the inherent complexity of traditional
    ML models to transparency difficulties. First, [“Assessing the Local Interpretability
    of Machine Learning Models”](https://oreil.ly/FQQd_) proxies complexity with the
    number of runtime operations associated with an ML model decision, and shows that
    as the number of operations increases, interpretability decreases. Second, [“Quantifying
    Model Complexity via Functional Decomposition for Better Post-Hoc Interpretability”](https://oreil.ly/DWTRF)
    uses number of features, interaction strength, and main effect complexity to measure
    the overall complexity of ML models, and shows that models that minimize these
    criteria are more reliably interpretable. In summary, complex models are hard
    to explain and simpler models are easier to explain, but certainly not easy. [Figure 2-7](#dt_shap)
    provides an example of augmenting a simple model with explanations, and why even
    that is difficult.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们结束本章更多的技术讨论，我们想要强调新的研究成果，帮助阐明为何解释传统 ML 模型如此困难，并给读者留下一个结合可解释模型和事后解释的示例。两篇最新论文将传统
    ML 模型的固有复杂性与透明度困难联系起来。首先，《评估机器学习模型的局部可解释性》（[链接](https://oreil.ly/FQQd_)）用 ML 模型决策关联的运行时操作数量作为复杂性的代理，并显示随着操作数量的增加，解释性降低。其次，《通过功能分解量化模型复杂性以获得更好的事后解释性》（[链接](https://oreil.ly/DWTRF)）利用特征数量、交互强度和主效应复杂性来衡量
    ML 模型的总体复杂性，并显示最小化这些标准的模型更容易解释。总结一下，复杂模型难以解释，简单模型更易解释，但绝对不容易。[图 2-7](#dt_shap)
    提供了一个增加解释的简单模型的示例，以及为什么即使这样做也很困难。
- en: '[Figure 2-7](#dt_shap) contains a trained three-level decision tree with a
    highlighted decision path, and the Tree SHAP values for a single row of data that
    follows that decision path. While [Figure 2-7](#dt_shap) looks simple, it’s actually
    illustrative of several fundamental problems in ML and ML explanation. Before
    we dive into the difficulties presented by [Figure 2-7](#dt_shap), let’s bask
    in the glory of a predictive model with its entire global decision-making mechanism
    on view, and for which we can generate numeric contributions of the input features
    to any model prediction. That level of transparency used to be reserved for linear
    models, but all the new approaches we’ve covered in this chapter make this level
    of transparency a reality for a much broader class of higher-capacity models.
    This means that if we’re careful we can train more sophisticated models, that
    learn more from data, and still be able to interpret and learn from the results
    ourselves. We can learn more from data, do so reliably, and learn more as humans
    from the results. That’s a huge breakthrough.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-7](#dt_shap) 包含一个经过训练的三级决策树，显示了一个突出显示的决策路径，以及沿着该决策路径进行数据单行的树 SHAP 值。尽管[图 2-7](#dt_shap)看起来简单，实际上它揭示了机器学习和机器学习解释中的几个基本问题。在我们深入讨论[图 2-7](#dt_shap)所呈现的困难之前，让我们沉浸在一个具有完整全局决策机制预测模型的荣耀中，并且我们可以生成输入特征对任何模型预测的数值贡献。这种透明度曾经仅限于线性模型，但我们在本章涵盖的所有新方法使得这种透明度对于更广泛的高容量模型成为现实。这意味着，如果我们小心的话，我们可以训练更复杂的模型，从数据中学习更多，并且仍然能够解释和从结果中学习。我们可以从数据中更多地学习，可靠地这样做，并且从结果中作为人类学到更多。这是一个巨大的突破。'
- en: '![mlha 0207](assets/mlha_0207.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![mlha 0207](assets/mlha_0207.png)'
- en: Figure 2-7\. A simple explainable model paired with post hoc explanation information
  id: totrans-250
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-7\. 简单的可解释模型与事后解释信息配对
- en: Note
  id: totrans-251
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: Use explainable models and post hoc explanations together to check one another
    and to maximize the transparency of ML models.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 使用可解释模型和事后解释相结合，相互验证，并最大化 ML 模型的透明度。
- en: Now let’s dig into the difficulties in [Figure 2-7](#dt_shap) while keeping
    in mind that these problems always exist when using ML models and post hoc XAI.
    (We can just see them and think through them in this simple case.) Notice that
    the decision path for the selected individual considers `PAY_0`, `PAY_6`, and
    `PAY_AMT1`. Now look at the Tree SHAP values. They give higher weight to `PAY_2`
    than `PAY_6`, and weigh `PAY_5` over `PAY_AMT1`, but `PAY_2` and `PAY_5` are not
    on the decision path. This occurs because the SHAP calculation takes into account
    artificial observations with different values for `PAY_0`, `PAY_AMT1`, and `PAY_6`,
    and those observations go down different decision paths. We’d see this behavior
    whether or not we used `tree_path_dependent` or `interventional` feature perturbations.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入探讨[图2-7](#dt_shap)中的困难，同时要记住，在使用机器学习模型和事后XAI时，这些问题总是存在的（我们可以在这种简单情况下看到它们并思考它们）。注意所选个体的决策路径考虑了`PAY_0`、`PAY_6`和`PAY_AMT1`。现在看看Tree
    SHAP值。它们给`PAY_2`比`PAY_6`更高的权重，并且比`PAY_AMT1`更重要，但`PAY_2`和`PAY_5`不在决策路径上。这是因为SHAP计算考虑了具有不同`PAY_0`、`PAY_AMT1`和`PAY_6`值的人工观测，而这些观测走不同的决策路径。无论我们使用`tree_path_dependent`还是`interventional`特征扰动，我们都会看到这种行为。
- en: This phenomenon is unintuitive, but it is correct and not the result of approximation
    error. We could have used a different package or approach and probably generated
    local explanations that were true to the single decision path highlighted in [Figure 2-7](#dt_shap),
    but then we wouldn’t have the deep theoretical support that accompanies Shapley
    values and SHAP. At least with SHAP, we know why our explanations show this effect.
    In general, explaining ML models is very hard, and for many different reasons.
    Always test explanations before deploying them in high-risk contexts, and make
    sure you understand the post hoc techniques you’re applying.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这种现象很不直观，但是它是正确的，不是近似误差的结果。我们本可以使用不同的软件包或方法，并可能生成与[图2-7](#dt_shap)中突出显示的单一决策路径一致的本地解释，但那样我们就无法获得伴随Shapley值和SHAP的深刻理论支持。至少通过SHAP，我们知道我们的解释显示了这种效果的原因。总体而言，解释机器学习模型非常困难，而且原因很多。在将解释部署到高风险环境之前，始终测试解释，并确保理解你所应用的事后技术。
- en: '[Chapter 6](ch06.html#unique_chapter_id_6) will go more into the details of
    how SHAP uses background data and calculates feature attributions based on different
    settings. We all need to understand these subtleties before using feature attribution
    methods like SHAP for high-risk applications. There’s so much to think about on
    just the technical side of explanations. The following case will dig into some
    of the human factors of explanations, which might be even more difficult to get
    right.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[第六章](ch06.html#unique_chapter_id_6)将更详细地讨论SHAP如何使用背景数据，并根据不同设置计算特征归因。在将像SHAP这样的特征归因方法用于高风险应用之前，我们都需要理解这些微妙之处。在解释的技术方面，有很多需要考虑的事情。接下来的案例将深入探讨一些解释中的人为因素，这可能更难以把握正确。'
- en: 'Case Study: Graded by Algorithm'
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究：通过算法评分
- en: Adding transparency into ML models is no easy task. Even if we get the technical
    aspects of explainable models and post hoc explanations right, there are still
    many human factors that must be handled carefully. The so-called [*A-level scandal*](https://oreil.ly/s54hO)
    in the UK is an object lesson in failing to understand human factors for high-risk
    ML-based decisions. As COVID lockdowns took hold across the United Kingdom in
    the spring of 2020, students, teachers, and government officials realized that
    standardized tests could not take place as usual. As a first attempt to remedy
    the problems with national standardized testing, teachers were asked to estimate
    student performance on the important A-level exams that determine college entrance
    and affect other important life outcomes. Unfortunately, teacher estimates were
    seen as implausibly positive, to the level that using the estimated student performance
    would be unfair to past and future students.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 向机器学习模型中增加透明度并非易事。即使我们在可解释模型的技术细节和事后解释方面做得很好，仍然有许多人为因素需要小心处理。在英国发生的所谓[*A级丑闻*](https://oreil.ly/s54hO)
    是一个未能理解高风险基于机器学习的决策中人为因素的典型案例。2020年春季，随着COVID封锁在英国全境蔓延，学生、教师和政府官员意识到无法像往常那样进行标准化测试。作为解决国家标准化测试问题的首次尝试，教师被要求估算对确定大学入学和影响其他重要生活结果的A级考试的学生表现。不幸的是，教师的估算被视为不可信，以至于使用估算的学生表现将对过去和未来的学生都不公平。
- en: To address teachers’ positive biases, the government Office of Qualifications
    and Examinations Regulation (Ofqual) decided to implement an algorithm to adjust
    teacher predictions. The statistical methodology of the adjustment algorithm was
    implemented by experts and a [model document](https://oreil.ly/0gM6i) was released
    after students received their grades. The algorithm was designed to generate a
    final distribution of grades that was similar to results in previous years. It
    preserved teacher rankings, but used past school performance to adjust grades
    downward. Students in Scotland were first to see the results. In that part of
    the UK, “35.6 percent of grades were adjusted down by a single grade, while 3.3
    percent went down by two grades, and 0.2 went down by three,” according to [ZDNet](https://oreil.ly/h47XJ).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决教师的正面偏见，英国资格和考试监管办公室（Ofqual）决定实施算法来调整教师的预测。 调整算法的统计方法由专家实施，并在学生收到成绩后发布了一个[模型文档](https://oreil.ly/0gM6i)。
    该算法旨在生成一个最终成绩分布，类似于以往几年的结果。 它保留了教师的排名，但使用过去的学校表现来调整成绩向下。 苏格兰的学生首先看到了结果。 根据[ZDNet](https://oreil.ly/h47XJ)，“35.6％的成绩降低了一个等级，而3.3％降低了两个等级，0.2％降低了三个等级。”
- en: Over the next few months, student outcry over likely bias against poorer schools
    and regions of the country caused a massive, slow-burning AI incident. Even though
    officials had seen the problems in Scotland, they applied the same process in
    England, but instated a free appeals process and the right for students to retest
    at a later date. In the end, irreparable damage to public trust could not be undone,
    and the transparent but biased notion of adjusting individual scores by past school
    performance was too much for many to stomach. In the end, the UK government decided
    to use the original teacher estimates. According to [Wired](https://oreil.ly/DoV4O),
    “the government has essentially passed the administrative buck to universities,
    who will now have to consider honouring thousands more offers—they have said that
    despite u-turn, it will not be possible to honour all original offers.” The same
    article also pointed out that teacher estimates of student performance had shown
    racial bias in the past. What a mess.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几个月里，学生对可能存在的对贫困学校和全国各地区的偏见发出了强烈抗议，引发了一场规模庞大、缓慢蔓延的AI事件。 尽管官员们已经看到了苏格兰的问题，他们在英格兰也应用了相同的流程，但同时设立了免费申诉流程以及学生有权在稍后的日期重新考试。
    最终，对公众信任造成的不可修复的损害，以及透明但带有偏见的根据过去学校表现调整个人分数的概念，让许多人无法接受。 最终，英国政府决定使用原始的教师估计值。
    根据[Wired](https://oreil.ly/DoV4O)，“政府实际上已将行政责任转移到了大学，他们现在必须考虑是否接受更多的申请——尽管进行了政策调整，但不可能兑现所有原始的录取要约。”
    同一篇文章还指出，过去教师对学生表现的估计显示了种族偏见。 这真是一团糟。
- en: Shockingly, other institutions have also adopted the idea of algorithmic scores
    for life-changing college entrance exams. The International Baccalaureate (IB)
    is an elite educational program that offers an advanced uniform curriculum for
    secondary school students all over the world. In the Spring of 2020, the IB used
    an algorithm for student scores that was [reported to be](https://oreil.ly/OT05d)
    “hastily deployed after canceling its usual springtime exams due to COVID-19\.
    The system used signals including a student’s grades on assignments and grades
    from past grads at their school.” Because of the timing, unanticipated negative
    scores were extremely hurtful to students applying to US colleges and universities,
    who reserve spaces for IB students based on past performance, but can cancel based
    on final performances, “shattering their plans for the fall and beyond.” Some
    students’ algorithmic scores were so low that they may have lost placement in
    prestigious universities in the US and their safety schools in their home country.
    What’s worse, and unlike the Ofqual algorithm, the IB was not forthcoming with
    how their algorithm worked, and appeals came with an almost $800 price tag.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 令人震惊的是，其他机构也采纳了算法评分的想法来改变人生的大学入学考试。国际文凭（IB）是一个精英教育项目，为全球中学生提供先进的统一课程。2020年春季，IB因COVID-19取消了春季考试后，匆忙部署了一个算法用于学生的评分，被[报道为](https://oreil.ly/OT05d)“急忙部署的”。该系统使用的信号包括学生的作业成绩以及他们学校过去的成绩。“由于时间问题，未预料到的负面评分对申请美国大学和大学的IB学生非常有害，这些大学根据过去的表现保留了空间，但可以根据最终表现取消，‘打碎了他们秋季及以后的计划。’”一些学生的算法分数如此之低，以至于他们可能失去了在美国著名大学及其本国的安全学校的位置。更糟糕的是，与Ofqual算法不同，IB没有公开他们的算法工作原理，而申诉费用则高达将近800美元。
- en: Putting aside the IB’s lack of transparency, there seem to be three major issues
    at play in these incidents. Scale is an inherent risk of ML, and these algorithms
    were used on many students across the globe. Large scale translates to high materiality,
    and transparency alone is not enough to offset issues of trust and bias. Understanding
    is not trust. Ofqual’s technical report and other [public analyses](https://oreil.ly/QAB8R)
    were over the heads of many students and parents. But what was not over their
    heads is that poorer areas have worse public schools, and that affected students
    twice in 2020—once in an overall manner like every year, and then again when their
    scores were adjusted downward. The second factor was the seriousness of the decision.
    College admittance plays a huge role in the rest of many people’s lives. The serious
    nature of the decision cranks up the materiality to an even higher degree—possibly
    to an impossible degree, where failure becomes guaranteed. ML is inherently probabilistic.
    It will be wrong. And when the stakes are this high, the public just might not
    accept it.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 撇开IB缺乏透明度不谈，这些事件中似乎存在三个主要问题。规模是ML的固有风险，这些算法被用于全球许多学生。大规模转化为高物质性，但单靠透明度并不足以抵消信任和偏见问题。理解并非信任。Ofqual的技术报告和其他[公共分析](https://oreil.ly/QAB8R)超出了许多学生和家长的理解能力。但对他们来说不难理解的是，贫困地区有更差的公立学校，这在2020年两次影响了受影响学生——一次是像往常一样的整体方式，然后是当他们的分数被调整向下时。第二个因素是决策的严重性。大学录取在许多人的生活中起着重要作用。决策的严重性将物质性提升到更高的程度——可能是不可能的程度，失败变得肯定。ML固有地是概率性的。它会出错。而当赌注这么高时，公众可能根本不会接受它。
- en: The third major issue at play here is the clear nature of disparate impact.
    For example, very small classes were not scored with the algorithm. Where are
    there the most very small classes? Private schools. A [Verge article](https://oreil.ly/eySQu)
    claims that “fee-paying private schools (also known as *independent schools*)
    disproportionately benefited from the algorithm used. These schools saw the amount
    of grades A and above increase by 4.7 percent compared to last year.” [ZDNet](https://oreil.ly/7mnEd)
    reported that the “pass rate for students undertaking higher courses in deprived
    locations across Scotland was reduced by 15.2%, in comparison to 6.9% in more
    affluent areas.” Adjusting by postal code or past school performance bakes in
    systemic biases, and students and parents understood this at an emotional level.
    As quoted in the [BBC](https://oreil.ly/vPQq1), Scotland’s Education Secretary
    realized in the end that the debacle left “young people feeling their future had
    been determined by statistical modeling rather than their own ability.” We should
    think about how we would feel if this incident had affected us or our children.
    Despite all the hype around automated decision making, almost no one wants to
    feel that their future is set by an algorithm.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这里存在的第三个主要问题是明显的不平等影响性质。例如，很小的班级没有通过算法进行评分。哪里有最小的班级？私立学校。一篇[Verge文章](https://oreil.ly/eySQu)声称，“付费私立学校（也称为*独立学校*）从使用的算法中不成比例地受益。与去年相比，这些学校的A级及以上成绩增加了4.7%。”[ZDNet](https://oreil.ly/7mnEd)报告称，“在苏格兰贫困地区进行更高课程的学生通过率减少了15.2%，而在更富裕地区为6.9%。”通过邮政编码或过去的学校表现调整，内含系统性偏见，学生和家长在情感上理解这一点。正如[BBC](https://oreil.ly/vPQq1)引述的那样，苏格兰的教育部长最终意识到，这场丑闻使“年轻人感觉他们的未来是由统计建模而不是他们自己的能力决定的。”我们应该想象如果这件事影响了我们或我们的孩子，我们会有什么感受。尽管围绕自动决策制定了如此多的宣传，几乎没有人希望感觉到他们的未来是由算法决定的。
- en: Although this may have been a doomed, impossibly high-materiality application
    of ML from the beginning, more could have been done to increase public trust.
    For example, Ofqual could have published the algorithm before applying it to students.
    They also could have taken public feedback on the algorithm before using it. Jeni
    Tennison, a UK-based open data advocate, [notes](https://oreil.ly/4Unct), “Part
    of the problem here is that these issues came out only after the grades were given
    to students, when we could have been having these discussions and been examining
    the algorithm and understanding the implications of it much, much earlier.” The
    take-home lessons here are that technical transparency is not the same as broad
    social understanding, and that understanding, if it can even be achieved, does
    not guarantee trust. Even if we’ve done a good job on technical transparency,
    as put forward in this chapter, there is still a great deal of work that must
    be done to ensure an ML system works as expected for users, or subjects. Finally,
    this is just one AI incident, and although it’s a big one, it shouldn’t cause
    us to overlook the smaller ones that are harming people right now, and we have
    to keep in mind that even more people will be harmed by AI systems in the future.
    As Tennison put it, “This has hit the headlines, because it affects so many people
    across the country, and it affects people who have a voice. There’s other automated
    decision making that goes on all the time, around benefits, for example, that
    affect lots of people who don’t have this strong voice.”
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这可能从一开始就是一个注定失败的、不可能达到高材料应用的ML，但可以做更多事情来增加公众的信任。例如，Ofqual可以在应用于学生之前发布算法。他们还可以在使用算法之前征求公众意见。英国开放数据倡导者Jeni
    Tennison在这里[指出](https://oreil.ly/4Unct)，“这里的问题的一部分是，这些问题只有在学生得到成绩之后才出现，当我们本可以在此之前进行这些讨论并检查算法并了解其影响的时候。”这里的主要教训是，技术透明并不等同于广泛社会的理解，即使可以实现理解，也不能保证信任。即使我们在技术透明方面做得很好，如本章提出的那样，仍然有很多工作需要做，以确保ML系统对用户或主体正常工作。最后，这只是一个AI事件，虽然这是一个大事件，但不应忽视那些正在伤害人们的更小的事件，并且我们必须记住，未来更多的人将受到AI系统的伤害。正如Tennison所说，“这件事成为头条新闻，因为它影响了全国如此多的人，影响了有声音的人。还有其他的自动决策制定一直在进行，比如围绕福利的，这些都影响到很多没有这么强大声音的人。”
- en: Resources
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: Further Reading
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 更多阅读
- en: '[*An Introduction to Machine Learning Interpretability* (O’Reilly)](https://oreil.ly/iyz08)'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*机器学习可解释性介绍*（O''Reilly）](https://oreil.ly/iyz08)'
- en: '[“Designing Inherently Interpretable Machine Learning Models”](https://oreil.ly/jbGNt)'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“设计固有可解释机器学习模型”](https://oreil.ly/jbGNt)'
- en: '[*Explanatory Model Analysis* (CRC Press)](https://oreil.ly/Yt_Xm)'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*解释性模型分析* (CRC Press)](https://oreil.ly/Yt_Xm)'
- en: '[“General Pitfalls of Model-Agnostic Interpretation Methods for Machine Learning
    Models”](https://oreil.ly/On9uS)'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“面向机器学习模型的通用解释方法的缺陷”](https://oreil.ly/On9uS)'
- en: '[*Interpretable Machine Learning*](https://oreil.ly/BHy1L)'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*可解释机器学习*](https://oreil.ly/BHy1L)'
- en: '[*Limitations of Interpretable Machine Learning Methods*](https://oreil.ly/VHMWh)'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*可解释机器学习方法的局限性*](https://oreil.ly/VHMWh)'
- en: '[“On the Art and Science of Explainable Machine Learning”](https://oreil.ly/myVr8)'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“可解释机器学习的艺术与科学”](https://oreil.ly/myVr8)'
- en: '[“Psychological Foundations of Explainability and Interpretability in Artificial
    Intelligence”](https://oreil.ly/HUomp)'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“人工智能中的可解释性和解释性心理基础”](https://oreil.ly/HUomp)'
- en: '[“When Not to Trust Your Explanations”](https://oreil.ly/9Oxa6)'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“何时不应信任你的解释”](https://oreil.ly/9Oxa6)'
