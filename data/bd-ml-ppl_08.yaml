- en: Chapter 8\. Model Deployment with TensorFlow Serving
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第8章. 使用TensorFlow Serving进行模型部署
- en: The deployment of your machine learning model is the last step before others
    can use your model and make predictions with it. Unfortunately, the deployment
    of machine learning models falls into a gray zone in today’s thinking of the division
    of labor in the digital world. It isn’t just a DevOps task since it requires some
    knowledge of the model architecture and its hardware requirements. At the same
    time, deploying machine learning models is a bit outside the comfort zone of machine
    learning engineers and data scientists. They know their models inside out but
    tend to struggle with the deployment of machine learning models. In this and the
    following chapter, we want to bridge the gap between the worlds and guide data
    scientists and DevOps engineers through the steps to deploy machine learning models.
    [Figure 8-1](#filepos766196) shows the position of the deployment step in a machine
    learning pipeline.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他人可以使用您的模型进行预测之前，部署您的机器学习模型是最后一步。不幸的是，机器学习模型的部署在当今的数字世界分工思维中存在灰色地带。它不仅仅是DevOps的任务，因为它需要一些对模型架构及其硬件需求的了解。同时，部署机器学习模型有点超出了机器学习工程师和数据科学家的舒适区。他们对自己的模型了如指掌，但在部署机器学习模型方面往往遇到困难。在本章和接下来的章节中，我们希望弥合这一差距，指导数据科学家和DevOps工程师完成部署机器学习模型的步骤。[图8-1](#filepos766196)显示了机器学习流水线中部署步骤的位置。
- en: '![](images/00026.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00026.jpg)'
- en: Figure 8-1\. Model deployments as part of ML pipelines
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图8-1. 作为ML流水线一部分的模型部署
- en: 'Machine learning models can be deployed in three main ways: with a model server,
    in a user’s browser, or on an edge device. The most common way today to deploy
    a machine learning model is with a model server, which we will focus on in this
    chapter. The client that requests a prediction submits the input data to the model
    server and in return receives a prediction. This requires that the client can
    connect with the model server.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型可以通过三种主要方式部署：使用模型服务器、在用户的浏览器中或者在边缘设备上。今天最常见的部署机器学习模型的方式是使用模型服务器，本章我们将重点讨论这一点。请求预测的客户端将输入数据提交给模型服务器，然后接收预测结果。这要求客户端能够连接到模型服务器。
- en: There are situations when you don’t want to submit the input data to a model
    server (e.g., when the input data is sensitive, or when there are privacy concerns).
    In these situations, you can deploy the machine learning model to a user’s browser.
    For example, if you want to determine whether an image contains sensitive information,
    you could classify the sensitivity level of the image before it is uploaded to
    a cloud server.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 有些情况下，你不希望将输入数据提交到模型服务器（例如当输入数据很敏感或存在隐私问题时）。在这些情况下，你可以将机器学习模型部署到用户的浏览器上。例如，如果你想要确定一张图片是否包含敏感信息，你可以在上传到云服务器之前对图片的敏感级别进行分类。
- en: 'However, there is also a third type of model deployment: deploying to edge
    devices. There are situations that don’t allow you to connect to a model server
    to make predictions (i.e., remote sensors or IoT devices). The number of applications
    being deployed to edge devices is increasing, making it a valid option for model
    deployments. In [Chapter 10](index_split_017.html#filepos1073133), we discuss
    how TensorFlow models can be converted to TFLite models, which can be executed
    on edge devices.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有第三种模型部署方式：部署到边缘设备。有些情况下，你无法连接到模型服务器进行预测（例如远程传感器或物联网设备）。部署到边缘设备的应用数量正在增加，使其成为模型部署的一个有效选项。在[第10章](index_split_017.html#filepos1073133)中，我们讨论了如何将TensorFlow模型转换为TFLite模型，以便在边缘设备上执行。
- en: In this chapter, we highlight TensorFlow’s Serving module, a simple and consistent
    way of deploying TensorFlow models through a model server. We will introduce its
    setup and discuss efficient deployment options. This is not the only way of deploying
    deep learning models; there are a few alternative options, which we discuss toward
    the end of this chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们突出了TensorFlow的Serving模块，这是通过模型服务器部署TensorFlow模型的一种简单且一致的方式。我们将介绍其设置并讨论高效的部署选项。这并不是部署深度学习模型的唯一方式；在本章末尾，我们还将讨论一些替代选项。
- en: Let’s start the chapter with how you shouldn’t set up a model server before
    we take a deep dive into TensorFlow Serving.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨TensorFlow Serving之前，让我们从如何不应该设置模型服务器开始这一章节。
- en: A Simple Model Server
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的模型服务器
- en: 'Most introductions to deploying machine learning models follow roughly the
    same workflow:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数关于部署机器学习模型的介绍大致遵循相同的工作流程：
- en: Create a web app with Python (i.e., with web frameworks like Flask or Django).
  id: totrans-11
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用 Python 创建 Web 应用程序（例如使用 Flask 或 Django）。
- en: Create an API endpoint in the web app, as we show in [Example 8-1](#filepos770161).
  id: totrans-12
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在 Web 应用程序中创建一个 API 端点，正如我们在 [示例 8-1](#filepos770161) 中展示的。
- en: Load the model structure and its weights.
  id: totrans-13
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 加载模型结构及其权重。
- en: Call the predict method on the loaded model.
  id: totrans-14
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在加载的模型上调用 predict 方法。
- en: Return the prediction results as an HTTP request.
  id: totrans-15
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 返回预测结果作为 HTTP 请求。
- en: Example 8-1\. Example setup of a Flask endpoint to infer model predictions
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 8-1\. 使用 Flask 端点设置来推断模型预测的示例
- en: '`import``json``from``flask``import``Flask``,``request``from``tensorflow.keras.models``import``load_model``from``utils``import``preprocess`![](images/00002.jpg)`model``=``load_model``(``''model.h5''``)`![](images/00075.jpg)`app``=``Flask``(``__name__``)``@app.route``(``''/classify''``,``methods``=``[``''POST''``])``def``classify``():``complaint_data``=``request``.``form``[``"complaint_data"``]``preprocessed_complaint_data``=``preprocess``(``complaint_data``)``prediction``=``model``.``predict``([``preprocessed_complaint_data``])`![](images/00064.jpg)`return``json``.``dumps``({``"score"``:``prediction``})`![](images/00055.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '`import``json``from``flask``import``Flask``,``request``from``tensorflow.keras.models``import``load_model``from``utils``import``preprocess`![](images/00002.jpg)`model``=``load_model``(``''model.h5''``)`![](images/00075.jpg)`app``=``Flask``(``__name__``)``@app.route``(``''/classify''``,``methods``=``[``''POST''``])``def``classify``():``complaint_data``=``request``.``form``[``"complaint_data"``]``preprocessed_complaint_data``=``preprocess``(``complaint_data``)``prediction``=``model``.``predict``([``preprocessed_complaint_data``])`![](images/00064.jpg)`return``json``.``dumps``({``"score"``:``prediction``})`![](images/00055.jpg)'
- en: '![](images/00002.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: Preprocessing to convert data structure.
  id: totrans-19
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 预处理以转换数据结构。
- en: '![](images/00075.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00075.jpg)'
- en: Load your trained model.
  id: totrans-21
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 加载您训练过的模型。
- en: '![](images/00064.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00064.jpg)'
- en: Perform the prediction.
  id: totrans-23
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 执行预测。
- en: '![](images/00055.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00055.jpg)'
- en: Return the prediction in an HTTP response.
  id: totrans-25
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 返回预测结果的 HTTP 响应。
- en: This setup is a quick and easy implementation, perfect for demonstration projects.
    However, we do not recommend using [Example 8-1](#filepos770161) to deploy machine
    learning models to production endpoints.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设置适用于演示项目的快速简易实现。然而，我们不建议将 [示例 8-1](#filepos770161) 用于将机器学习模型部署到生产端点。
- en: Next, let’s discuss why we don’t recommend deploying machine learning models
    with such a setup. The reason is our benchmark for our proposed deployment solution.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论为何不建议使用此类设置部署机器学习模型的原因。原因在于我们对提议的部署解决方案的基准。
- en: The Downside of Model Deployments with Python-Based APIs
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Python-Based API 部署模型的缺点
- en: While the [Example 8-1](#filepos770161) implementation can be sufficient for
    demonstration purposes, such deployments often face challenges. The challenges
    start with proper separation between the API and the data science code, a consistent
    API structure and the resulting inconsistent model versioning, and inefficient
    model inferences. We will take a closer look at these challenges in the following
    sections.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 [示例 8-1](#filepos770161) 的实现对演示目的可能足够，但这类部署通常面临挑战。挑战从 API 和数据科学代码之间的适当分离开始，一个一致的
    API 结构和由此产生的不一致的模型版本管理，以及低效的模型推断。我们将在接下来的部分更详细地探讨这些挑战。
- en: Lack of Code Separation
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 缺乏代码分离
- en: In [Example 8-1](#filepos770161), we assumed that the trained model was being
    deployed with the same API code base that it was also living in. This means that
    there would be no separation between the API code and the machine learning model,
    which can be problematic when data scientists want to update a model and such
    an update requires coordination with the API team. Such coordination also requires
    that the API and data science teams work in sync to avoid unnecessary delays on
    the model deployments.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 8-1](#filepos770161) 中，我们假设训练过的模型正在与同一 API 代码库一起部署。这意味着 API 代码和机器学习模型之间没有分离，当数据科学家想要更新模型时，这种更新就需要与
    API 团队协调。这种协调还要求 API 和数据科学团队同步工作，以避免在模型部署上造成不必要的延迟。
- en: An intertwined API and data science code base also creates ambiguity around
    API ownership.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 纠缠的 API 和数据科学代码库还会在 API 拥有权方面造成歧义。
- en: 'The lack of code separation also requires that the model has to be loaded in
    the same programming language as the API code. This mixing of backend and data
    science code can ultimately prevent your API team from upgrading your API backend.
    However, it also provides a good separation of responsibilities: the data scientists
    can focus on model training and the DevOps colleagues can focus on the deployment
    of the trained models.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 代码分离的缺乏还要求模型必须在与 API 代码相同的编程语言中加载。这种后端与数据科学代码的混合最终可能阻止 API 团队升级 API 后端。然而，它也提供了良好的责任分离：数据科学家可以专注于模型训练，DevOps
    同事可以专注于训练模型的部署。
- en: We highlight how you can separate your models from your API code effectively
    and simplify your deployment workflows in [“TensorFlow Serving”](#filepos782360).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强调了如何有效地将您的模型与 API 代码分离，并简化您的部署工作流程，详见 [“TensorFlow Serving”](#filepos782360)。
- en: Lack of Model Version Control
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 模型版本控制的缺乏
- en: '[Example 8-1](#filepos770161) doesn’t make any provision for different model
    versions. If you wanted to add a new version, you would have to create a new endpoint
    (or add some branching logic to the existing endpoint). This requires extra attention
    to keep all endpoints structurally the same, and it requires a lot of boilerplate
    code.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 8-1](#filepos770161) 没有为不同的模型版本做任何准备。如果您想要添加一个新版本，您将不得不创建一个新的端点（或在现有端点中添加一些分支逻辑）。这需要额外注意以保持所有端点在结构上相同，并且需要大量样板代码。'
- en: The lack of model version control also requires the API and the data science
    teams to coordinate which version is the default version and how to phase in new
    models.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 模型版本控制的缺乏还要求 API 和数据科学团队协调确定默认版本及如何逐步引入新模型。
- en: Inefficient Model Inference
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 低效的模型推理
- en: For any request to your prediction endpoint written in the Flask setup as shown
    in [Example 8-1](#filepos770161), a full round trip is performed. This means each
    request is preprocessed and inferred individually. The key reason why we argue
    that such a setup is only for demonstration purposes is that it is highly inefficient.
    During the training of your model, you will probably use a batching technique
    that allows you to compute multiple samples at the same time and then apply the
    gradient change for your batch to your network’s weights. You can apply the same
    technique when you want the model to make predictions. A model server can gather
    all requests during an acceptable timeframe or until the batch is full and ask
    the model for its predictions. This is an especially effective method when the
    inference runs on GPUs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于写入 Flask 设置中预测端点的任何请求，如 [示例 8-1](#filepos770161) 所示，都会执行一个完整的往返。这意味着每个请求都会单独进行预处理和推断。我们认为这种设置仅用于演示的关键原因是其效率极低。在训练模型时，您可能会使用批处理技术，允许您同时计算多个样本，然后将批处理的梯度变化应用于网络的权重。当您希望模型进行预测时，可以应用相同的技术。模型服务器可以在可接受的时间范围内或直到批处理满时收集所有请求，并询问模型其预测结果。当推理运行在
    GPU 上时，这是一种特别有效的方法。
- en: In [“Batching Inference Requests”](index_split_014.html#filepos949215), we introduce
    how you can easily set up such a batching behavior for your model server.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [“批处理推断请求”](index_split_014.html#filepos949215) 中，我们介绍了如何为您的模型服务器轻松设置这样的批处理行为。
- en: TensorFlow Serving
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving
- en: As you have seen in the earlier chapters of this book, TensorFlow comes with
    a fantastic ecosystem of extensions and tools. One of the earlier open source
    extensions was TensorFlow Serving. It allows you to deploy any TensorFlow graph,
    and you can make predictions from the graph through its standardized endpoints.
    As we discuss in a moment, TensorFlow Serving handles the model and version management
    for you, lets you serve models based on policies, and allows you to load your
    models from various sources. At the same time, it is focused on high-performance
    throughput for low-latency predictions. TensorFlow Serving is used internally
    at Google and has been adopted by a good number of corporations and startups.[1](index_split_015.html#filepos993442)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在本书的前几章中所见，TensorFlow 提供了一个出色的扩展和工具生态系统。早期的开源扩展之一是 TensorFlow Serving。它允许你部署任何
    TensorFlow 图，并通过其标准化的端点从图中进行预测。正如我们接下来会讨论的，TensorFlow Serving 为你处理模型和版本管理，让你根据策略提供模型，允许你从各种来源加载你的模型。同时，它专注于高性能吞吐量以进行低延迟的预测。TensorFlow
    Serving 在 Google 内部使用，并已被许多公司和初创企业采用。[1](index_split_015.html#filepos993442)
- en: TensorFlow Architecture Overview
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 架构概述
- en: TensorFlow Serving provides you the functionality to load models from a given
    source (e.g., AWS S3 buckets) and notifies the loader if the source has changed.
    As [Figure 8-2](#filepos784248) shows, everything behind the scenes of TensorFlow
    Serving is controlled by a model manager, which manages when to update the models
    and which model is used for the predictions. The rules for the inference determination
    are set by the policy which is managed by the model manager. Depending on your
    configuration, you can, for example, load one model at a time and have the model
    update automatically once the source module detects a newer version.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving 为您提供了从给定来源（例如 AWS S3 存储桶）加载模型并在源更改时通知加载器的功能。如 [Figure 8-2](#filepos784248)
    所示，TensorFlow Serving 的背后所有操作都由模型管理器控制，该管理器负责何时更新模型以及哪个模型用于预测。推断决策规则由策略管理，该策略也由模型管理器管理。根据您的配置，例如，您可以一次加载一个模型，并且一旦源模块检测到更新版本，模型将自动更新。
- en: '![](images/00038.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00038.jpg)'
- en: Figure 8-2\. Overview of the TensorFlow Serving architecture
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图8-2\. TensorFlow Serving 架构概述
- en: Exporting Models for TensorFlow Serving
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 导出 TensorFlow Serving 的模型
- en: Before we dive into the TensorFlow Serving configurations, let’s discuss how
    you can export your machine learning models so that they can be used by TensorFlow
    Serving.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入讨论 TensorFlow Serving 配置之前，让我们讨论一下如何导出您的机器学习模型，以便它们可以被 TensorFlow Serving
    使用。
- en: 'Depending on your type of TensorFlow model, the export steps are slightly different.
    The exported models have the same file structure as we see in [Example 8-2](#filepos798782).
    For Keras models, you can use:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的 TensorFlow 模型类型，导出步骤略有不同。导出的模型与我们在 [Example 8-2](#filepos798782) 中看到的文件结构相同。对于
    Keras 模型，您可以使用：
- en: '`saved_model_path``=``model``.``save``(``file``path``=``"./saved_models"``,``save_format``=``"tf"``)`'
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`saved_model_path``=``model``.``save``(``file``path``=``"./saved_models"``,``save_format``=``"tf"``)`'
- en: ADD A TIMESTAMP TO YOUR EXPORT PATH
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为您的导出路径添加时间戳
- en: 'It is recommended to add the timestamp of the export time to the export path
    for the Keras model when you are manually saving the model. Unlike the save method
    of `tf.Estimator`, `model.save()` doesn’t create the timestamped path automatically.
    You can create the file path easily with the following Python code:'
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当您手动保存模型时，建议为 Keras 模型的导出路径添加导出时间戳。与`tf.Estimator`的`save`方法不同，`model.save()`不会自动创建带时间戳的路径。您可以使用以下
    Python 代码轻松创建文件路径：
- en: '`import``time``ts``=``int``(``time``.``time``())``file``path``=``"./saved_models/{}"``.``format``(``ts``)``saved_model_path``=``model``.``save``(``file``path``=``file``path``,``save_format``=``"tf"``)`'
  id: totrans-53
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`import``time``ts``=``int``(``time``.``time``())``file``path``=``"./saved_models/{}"``.``format``(``ts``)``saved_model_path``=``model``.``save``(``file``path``=``file``path``,``save_format``=``"tf"``)`'
- en: 'For TensorFlow Estimator models, you need to first declare a `receiver function`:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 TensorFlow Estimator 模型，您需要首先声明一个`接收函数`：
- en: '`import``tensorflow``as``tf``def``serving_input_receiver_fn``():``# an example
    input feature``input_feature``=``tf``.``compat``.``v1``.``placeholder``(``dtype``=``tf``.``string``,``shape``=``[``None``,``1``],``name``=``"input"``)``fn``=``tf``.``estimator``.``export``.``build_raw_serving_input_receiver_fn``(``features``=``{``"input_feature"``:``input_feature``})``return``fn`'
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`import``tensorflow``as``tf``def``serving_input_receiver_fn``():``# an example
    input feature``input_feature``=``tf``.``compat``.``v1``.``placeholder``(``dtype``=``tf``.``string``,``shape``=``[``None``,``1``],``name``=``"input"``)``fn``=``tf``.``estimator``.``export``.``build_raw_serving_input_receiver_fn``(``features``=``{``"input_feature"``:``input_feature``})``return``fn`'
- en: 'Export the Estimator model with the `export_saved_model` method of the `Estimator`:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`Estimator`的`export_saved_model`方法导出 Estimator 模型：
- en: '`estimator``=``tf``.``estimator``.``Estimator``(``model_fn``,``"model"``,``params``=``{})``estimator``.``export_saved_model``(``export_dir_base``=``"saved_models/"``,``serving_input_receiver_fn``=``serving_input_receiver_fn``)`'
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`estimator``=``tf``.``estimator``.``Estimator``(``model_fn``,``"model"``,``params``=``{})``estimator``.``export_saved_model``(``export_dir_base``=``"saved_models/"``,``serving_input_receiver_fn``=``serving_input_receiver_fn``)`'
- en: 'Both export methods produce output which looks similar to the following example:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 两种导出方法产生的输出与以下示例类似：
- en: '`... INFO:tensorflow:Signatures INCLUDED in` `export` `for` `Classify: None
    INFO:tensorflow:Signatures INCLUDED in` `export` `for` `Regress: None INFO:tensorflow:Signatures
    INCLUDED in` `export` `for` `Predict:` `[``''serving_default''``]` `INFO:tensorflow:Signatures
    INCLUDED in` `export` `for` `Train: None INFO:tensorflow:Signatures INCLUDED in`
    `export` `for` `Eval: None INFO:tensorflow:No assets to save. INFO:tensorflow:No
    assets to write. INFO:tensorflow:SavedModel written to: saved_models/1555875926/saved_model.pb
    Model exported to:  b``''saved_models/1555875926''`'
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`... INFO:tensorflow:在导出中包含的签名: None INFO:tensorflow:在导出中包含的签名: None INFO:tensorflow:在导出中包含的签名:
    ` `[``''serving_default''``]` `INFO:tensorflow:在导出中包含的签名: None INFO:tensorflow:在导出中包含的签名:
    None INFO:tensorflow:没有要保存的资产。 INFO:tensorflow:没有要写入的资产。 INFO:tensorflow:SavedModel已写入:
    saved_models/1555875926/saved_model.pb 模型已导出至:  b``''saved_models/1555875926''`'
- en: 'In our model export examples, we specified the folder saved_models/ as the
    model destination. For every exported model, TensorFlow creates a directory with
    the timestamp of the export as its folder name:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的模型导出示例中，我们指定了文件夹saved_models/作为模型的目标位置。对于每个导出的模型，TensorFlow会创建一个以导出时间戳命名的目录：
- en: Example 8-2\. Folder and file structure of exported models
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 8-2\. 导出模型的文件夹和文件结构
- en: '`$` `tree saved_models/ saved_models/ └── 1555875926     ├── assets     │  
    └── saved_model.json     ├── saved_model.pb     └── variables         ├── checkpoint
            ├── variables.data-00000-of-00001         └── variables.index` `3` `directories,`
    `5` `files`'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`$` `tree saved_models/ saved_models/ └── 1555875926     ├── assets     │  
    └── saved_model.json     ├── saved_model.pb     └── variables         ├── checkpoint
            ├── variables.data-00000-of-00001         └── variables.index` `3` `directories,`
    `5` `files`'
- en: 'The folder contains the following files and subdirectories:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 文件夹包含以下文件和子目录：
- en: saved_model.pb
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: saved_model.pb
- en: The binary protocol buffer file contains the exported model graph structure
    as a `MetaGraphDef` object.
  id: totrans-65
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 二进制协议缓冲文件以`MetaGraphDef`对象形式包含导出模型图结构。
- en: variables
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 变量
- en: The folder contains the binary files with the exported variable values and checkpoints
    corresponding to the exported model graph.
  id: totrans-67
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 文件夹包含具有导出变量值的二进制文件以及对应于导出模型图的检查点。
- en: assets
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: assets
- en: This folder is created when additional files are needed to load the exported
    model. The additional file can include vocabularies, which saw in [Chapter 5](index_split_010.html#filepos397186).
  id: totrans-69
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当需要加载导出模型的其他文件时，会创建此文件夹。附加文件可以包括词汇表，这些词汇表在[第5章](index_split_010.html#filepos397186)中有介绍。
- en: Model Signatures
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 模型签名
- en: Model signatures identify the model graph’s inputs and outputs as well as the
    method of the graph signature. The definition of the input and output signatures
    allows us to map serving inputs to a given graph node for the inference. These
    mappings are useful if we want to update the model without changing the requests
    to the model server.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 模型签名标识模型图的输入和输出，以及图签名的方法。定义输入和输出签名允许我们将服务输入映射到给定图节点以进行推理。如果我们想要更新模型而不改变模型服务器的请求，这些映射就非常有用。
- en: 'In addition, the method of the model defines an expected pattern of inputs
    and outputs. At the moment, there are three supported signature types: predict,
    classify, or regress. We will take a closer look at the details in the following
    section.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，模型的方法定义了输入和输出的预期模式。目前支持三种签名类型：预测、分类或回归。我们将在下一节详细讨论这些细节。
- en: Signature Methods
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 签名方法
- en: The most flexible signature method is predict. If we don’t specify a different
    signature method, TensorFlow will use predict as the default method. [Example 8-3](#filepos802391)
    shows an example signature for the method predict. In the example, we are mapping
    the key `inputs` to the graph node with the name sentence. The prediction from
    the model is the output of the graph node y, which we are mapping to the output
    key `scores`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最灵活的签名方法是预测。如果我们没有指定不同的签名方法，TensorFlow将使用预测作为默认方法。[示例 8-3](#filepos802391)展示了方法预测的签名示例。在该示例中，我们将键`inputs`映射到名为sentence的图节点。模型的预测输出是图节点y的输出，我们将其映射到输出键`scores`。
- en: The predict method allows you to define additional output nodes. It is useful
    to add more inference outputs when you want to capture the output of an attention
    layer for visualizations or to debug a network node.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 预测方法允许您定义额外的输出节点。当您希望捕获注意力层输出以进行可视化或调试网络节点时，添加更多推理输出非常有用。
- en: Example 8-3\. Example model prediction signature
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 8-3\. 模型预测签名示例
- en: '`signature_def``:``{``key``:``"prediction_signature"``value``:``{``inputs``:``{``key``:``"inputs"``value``:``{``name``:``"sentence:0"``dtype``:``DT_STRING``tensor_shape``:``...``},``...``}``outputs``:``{``key``:``"scores"``value``:``{``name``:``"y:0"``dtype``:``...``tensor_shape``:``...``}``}``method_name``:``"tensorflow/serving/predict"``}``}`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`signature_def``:``{``key``:``"prediction_signature"``value``:``{``inputs``:``{``key``:``"inputs"``value``:``{``name``:``"sentence:0"``dtype``:``DT_STRING``tensor_shape``:``...``},``...``}``outputs``:``{``key``:``"scores"``value``:``{``name``:``"y:0"``dtype``:``...``tensor_shape``:``...``}``}``method_name``:``"tensorflow/serving/predict"``}``}`'
- en: Another signature method is classify. The method expects one input with the
    name inputs and provides two output tensors, classes and scores. At least one
    of the output tensors needs to be defined. In our example shown in [Example 8-4](#filepos808301),
    a classification model takes the input `sentence` and outputs the predicted `classes`
    together with the corresponding `scores`.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种签名方法是 `classify`。该方法期望一个名为 `inputs` 的输入，并提供两个输出张量，`classes` 和 `scores`。至少需要定义一个输出张量。在我们示例中显示的[示例 8-4](#filepos808301)中，分类模型接受输入
    `sentence` 并输出预测的 `classes` 以及相应的 `scores`。
- en: Example 8-4\. Example model classification signature
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 8-4\. 示例模型分类签名
- en: '`signature_def``:``{``key``:``"classification_signature"``value``:``{``inputs``:``{``key``:``"inputs"``value``:``{``name``:``"sentence:0"``dtype``:``DT_STRING``tensor_shape``:``...``}``}``outputs``:``{``key``:``"classes"``value``:``{``name``:``"y_classes:0"``dtype``:``DT_UINT16``tensor_shape``:``...``}``}``outputs``:``{``key``:``"scores"``value``:``{``name``:``"y:0"``dtype``:``DT_FLOAT``tensor_shape``:``...``}``}``method_name``:``"tensorflow/serving/classify"``}``}`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`signature_def``:``{``key``:``"classification_signature"``value``:``{``inputs``:``{``key``:``"inputs"``value``:``{``name``:``"sentence:0"``dtype``:``DT_STRING``tensor_shape``:``...``}``}``outputs``:``{``key``:``"classes"``value``:``{``name``:``"y_classes:0"``dtype``:``DT_UINT16``tensor_shape``:``...``}``}``outputs``:``{``key``:``"scores"``value``:``{``name``:``"y:0"``dtype``:``DT_FLOAT``tensor_shape``:``...``}``}``method_name``:``"tensorflow/serving/classify"``}``}`'
- en: The third available signature method is regress. This method takes only one
    input named inputs and provides only output with the name outputs. This signature
    method is designed for regression models. [Example 8-5](#filepos815898) shows
    an example of a regress signature.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种可用的签名方法是 `regress`。此方法仅接受名为 `inputs` 的输入，并仅提供名为 `outputs` 的输出。这种签名方法设计用于回归模型。[示例 8-5](#filepos815898)展示了回归签名的示例。
- en: Example 8-5\. Example model regression signature
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 8-5\. 示例模型回归签名
- en: '`signature_def``:``{``key``:``"regression_signature"``value``:``{``inputs``:``{``key``:``"inputs"``value``:``{``name``:``"input_tensor_0"``dtype``:``...``tensor_shape``:``...``}``}``outputs``:``{``key``:``"outputs"``value``:``{``name``:``"y_outputs_0"``dtype``:``DT_FLOAT``tensor_shape``:``...``}``}``method_name``:``"tensorflow/serving/regress"``}``}`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`signature_def``:``{``key``:``"regression_signature"``value``:``{``inputs``:``{``key``:``"inputs"``value``:``{``name``:``"input_tensor_0"``dtype``:``...``tensor_shape``:``...``}``}``outputs``:``{``key``:``"outputs"``value``:``{``name``:``"y_outputs_0"``dtype``:``DT_FLOAT``tensor_shape``:``...``}``}``method_name``:``"tensorflow/serving/regress"``}``}`'
- en: In [“URL structure”](index_split_014.html#filepos874335), we will see the signature
    methods again when we define the URL structure for our model endpoints.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在[“URL 结构”](index_split_014.html#filepos874335)中，当我们为模型端点定义 URL 结构时，我们将再次看到签名方法。
- en: Inspecting Exported Models
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 检查导出的模型
- en: After all the talk about exporting your model and the corresponding model signatures,
    let’s discuss how you can inspect the exported models before deploying them with
    TensorFlow Serving.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了如何导出模型及其对应模型签名之后，让我们讨论在部署 TensorFlow Serving 之前如何检查导出的模型。
- en: 'You can install the TensorFlow Serving Python API with the following `pip`
    command:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过以下 `pip` 命令安装 TensorFlow Serving Python API：
- en: '`$` `pip install tensorflow-serving-api`'
  id: totrans-88
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `pip install tensorflow-serving-api`'
- en: 'After the installation, you have access to a useful command-line tool called
    SavedModel Command Line Interface (CLI). This tool lets you:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 安装后，您可以使用一个称为 SavedModel 命令行接口（CLI）的实用命令行工具。此工具可以让您：
- en: Inspect the signatures of exported models
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 检查导出模型的签名
- en: This is very useful primarily when you don’t export the model yourself, and
    you want to learn about the inputs and outputs of the model graph.
  id: totrans-91
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 主要在您没有自行导出模型且希望了解模型图的输入和输出时非常有用。
- en: Test the exported models
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 测试导出的模型
- en: The CLI tools let you infer the model without deploying it with TensorFlow Serving.
    This is extremely useful when you want to test your model input data.
  id: totrans-93
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用 TensorFlow Serving 的 CLI 工具，您可以推断模型而无需部署它。这在您希望测试模型输入数据时非常有用。
- en: We’ll cover both use cases in the following two sections.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在以下两个部分中涵盖这两种用例。
- en: Inspecting the Model
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 检查模型
- en: '`saved_model_cli` helps you understand the model dependencies without inspecting
    the original graph code.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`saved_model_cli` 可帮助您了解模型的依赖关系，而无需检查原始图形代码。'
- en: 'If you don’t know the available tag-sets,[2](index_split_015.html#filepos993764)
    you can inspect the model with:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不知道可用的标签集合，[2](index_split_015.html#filepos993764) 您可以使用以下命令检查模型：
- en: '`$` `saved_model_cli show --dir saved_models/ The given SavedModel contains
    the following tag-sets: serve`'
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `saved_model_cli show --dir saved_models/ The given SavedModel contains
    the following tag-sets: serve`'
- en: If your model contains different graphs for different environments (e.g., a
    graph for a CPU or GPU inference), you will see multiple tags. If your model contains
    multiple tags, you need to specify a tag to inspect the details of the model.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的模型包含不同环境的不同图形（例如，用于 CPU 或 GPU 推断的图形），则会看到多个标签。如果模型包含多个标签，则需要指定一个标签以检查模型的详细信息。
- en: 'Once you know the `tag_set` you want to inspect, add it as an argument, and
    `saved_model_cli` will provide you the available model signatures. Our demo model
    has only one signature, which is called `serving_default`:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您知道要检查的 `tag_set`，请将其添加为参数，`saved_model_cli` 将提供可用的模型签名。我们的演示模型只有一个名为 `serving_default`
    的签名：
- en: '`$` `saved_model_cli show --dir saved_models/ --tag_set serve The given SavedModel`
    `''MetaGraphDef''` `contains` `''SignatureDefs''` `with the following keys: SignatureDef
    key:` `"serving_default"`'
  id: totrans-101
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `saved_model_cli show --dir saved_models/ --tag_set serve The given SavedModel`
    `''MetaGraphDef''` `contains` `''SignatureDefs''` `with the following keys: SignatureDef
    key:` `"serving_default"`'
- en: With the `tag_set` and `signature_def` information, you can now inspect the
    model’s inputs and outputs. To obtain the detailed information, add the `signature_def`
    to the CLI arguments.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 借助 `tag_set` 和 `signature_def` 信息，您现在可以检查模型的输入和输出。要获取详细信息，请将 `signature_def`
    添加到 CLI 参数中。
- en: 'The following example signature is taken from our model that was produced by
    our demonstration pipeline. In [Example 6-4](index_split_011.html#filepos565460),
    we defined our signature function, which takes serialized `tf.Example` records
    as inputs and provides the prediction through the output Tensor outputs, as shown
    in the following model signature:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例签名取自我们演示流水线生成的模型。在 [示例 6-4](index_split_011.html#filepos565460) 中，我们定义了签名函数，该函数将序列化的
    `tf.Example` 记录作为输入，并通过输出张量 `outputs` 提供预测结果，如以下模型签名所示：
- en: '`$` `saved_model_cli show --dir saved_models/` `\` `--tag_set serve --signature_def
    serving_default The given SavedModel SignatureDef contains the following input``(``s``)``:
      inputs``[``''examples''``]` `tensor_info:       dtype: DT_STRING       shape:`
    `(``-1``)` `name: serving_default_examples:0 The given SavedModel SignatureDef
    contains the following output``(``s``)``:   outputs``[``''outputs''``]` `tensor_info:
          dtype: DT_FLOAT       shape:` `(``-1, 1``)` `name: StatefulPartitionedCall_1:0
    Method name is: tensorflow/serving/predict`'
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `saved_model_cli show --dir saved_models/` `\` `--tag_set serve --signature_def
    serving_default The given SavedModel SignatureDef contains the following input``(``s``)``:
      inputs``[``''examples''``]` `tensor_info:       dtype: DT_STRING       shape:`
    `(``-1``)` `name: serving_default_examples:0 The given SavedModel SignatureDef
    contains the following output``(``s``)``:   outputs``[``''outputs''``]` `tensor_info:
          dtype: DT_FLOAT       shape:` `(``-1, 1``)` `name: StatefulPartitionedCall_1:0
    Method name is: tensorflow/serving/predict`'
- en: 'If you want to see all signatures regardless of the `tag_set` and `signature_def`,
    you can use the `--all` argument:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要查看所有签名而不考虑 `tag_set` 和 `signature_def`，可以使用 `--all` 参数：
- en: '`$` `saved_model_cli show --dir saved_models/ --all ...`'
  id: totrans-106
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `saved_model_cli show --dir saved_models/ --all ...`'
- en: After we investigated the model’s signature, we can now test the model inference
    before we deploy the machine learning model.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们调查模型签名后，我们现在可以在部署机器学习模型之前测试模型推理。
- en: Testing the Model
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 测试模型
- en: '`saved_model_cli` also lets you test the export model with sample input data.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`saved_model_cli` 还允许您使用示例输入数据测试导出模型。'
- en: 'You have three different ways to submit the sample input data for the model
    test inference:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 您有三种不同的方式提交模型测试推理的示例输入数据：
- en: '`--inputs`'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`--inputs`'
- en: The argument points at a NumPy file containing the input data formatted as NumPy
    `ndarray`.
  id: totrans-112
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 该参数指向一个包含以 NumPy `ndarray` 格式编码的输入数据的 NumPy 文件。
- en: '`--input_exprs`'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`--input_exprs`'
- en: The argument allows you to define a Python expression to specify the input data.
    You can use NumPy functionality in your expressions.
  id: totrans-114
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 该参数允许您定义 Python 表达式以指定输入数据。您可以在表达式中使用 NumPy 功能。
- en: '`--input_examples`'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`--input_examples`'
- en: The argument is expecting the input data formatted as a `tf.Example` data structure
    (see [Chapter 4](index_split_009.html#filepos295199)).
  id: totrans-116
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 该参数期望以 `tf.Example` 数据结构格式化的输入数据（请参见 [第四章](index_split_009.html#filepos295199)）。
- en: 'For testing the model, you can specify exactly one of the input arguments.
    Furthermore, `saved_model_cli` provides three optional arguments:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试模型，您可以准确指定一个输入参数。此外，`saved_model_cli` 提供了三个可选参数：
- en: '`--outdir`'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`--outdir`'
- en: '`saved_model_cli` will write any graph output to `stdout`. If you would rather
    write the output to a file, you can specify the target directory with `--outdir`.'
  id: totrans-119
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`saved_model_cli` 将任何图形输出写入 `stdout`。如果您更喜欢将输出写入文件，则可以使用 `--outdir` 指定目标目录。'
- en: '`--overwrite`'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`--overwrite`'
- en: If you opt for writing the output to a file, you can specify with `--overwrite`
    that the files can be overwritten.
  id: totrans-121
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果选择将输出写入文件，可以使用 `--overwrite` 指定文件可以被覆盖。
- en: '`--tf_debug`'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`--tf_debug`'
- en: If you want to further inspect the model, you can step through the model graph
    with the TensorFlow Debugger (TFDBG).
  id: totrans-123
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果您想进一步检查模型，可以使用 TensorFlow Debugger (TFDBG) 逐步查看模型图。
- en: '`$` `saved_model_cli run --dir saved_models/` `\` `--tag_set serve` `\` `--signature_def
    x1_x2_to_y` `\` `--input_examples` `''examples=[{"company": "HSBC", ...}]''`'
  id: totrans-124
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `saved_model_cli run --dir saved_models/` `\` `--tag_set serve` `\` `--signature_def
    x1_x2_to_y` `\` `--input_examples` `''examples=[{"company": "汇丰银行", ...}]''`'
- en: After all the introduction of how to export and inspect models, let’s dive into
    the TensorFlow Serving installation, setup, and operation.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍如何导出和检查模型后，让我们深入了解 TensorFlow Serving 的安装、设置和操作。
- en: Setting Up TensorFlow Serving
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 TensorFlow Serving
- en: There are two easy ways to get TensorFlow Serving installed on your serving
    instances. You can either run TensorFlow Serving on Docker or, if you run an Ubuntu
    OS on your serving instances, you can install the Ubuntu package.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种简单的方法可以在您的服务实例上安装 TensorFlow Serving。您可以在 Docker 上运行 TensorFlow Serving，或者如果您在服务实例上运行
    Ubuntu 操作系统，则可以安装 Ubuntu 包。
- en: Docker Installation
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 安装
- en: 'The easiest way of installing TensorFlow Serving is to download the prebuilt
    Docker image.[3](index_split_015.html#filepos994179) As you have seen in [Chapter 2](index_split_007.html#filepos83150),
    you can obtain the image by running:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 TensorFlow Serving 的最简单方法是下载预构建的 Docker 镜像。正如您在[第二章](index_split_007.html#filepos83150)中看到的那样，您可以通过运行以下命令获取该镜像：
- en: '`$` `docker pull tensorflow/serving`'
  id: totrans-130
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `docker pull tensorflow/serving`'
- en: If you are running the Docker container on an instance with GPUs available,
    you will need to download the latest build with GPU support.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在具有 GPU 的实例上运行 Docker 容器，则需要下载具有 GPU 支持的最新版本构建。
- en: '`$` `docker pull tensorflow/serving:latest-gpu`'
  id: totrans-132
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `docker pull tensorflow/serving:latest-gpu`'
- en: The Docker image with GPU support requires Nvidia’s Docker support for GPUs.
    The installation steps can be found on the [company’s website](https://oreil.ly/7N5uv).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 具有 GPU 支持的 Docker 镜像需要 Nvidia 的 Docker 支持。安装步骤可以在[公司的网站](https://oreil.ly/7N5uv)找到。
- en: Native Ubuntu Installation
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 本地 Ubuntu 安装
- en: If you want to run TensorFlow Serving without the overhead of running Docker,
    you can install Linux binary packages available for Ubuntu distributions.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要在没有运行 Docker 的开销的情况下运行 TensorFlow Serving，则可以安装适用于 Ubuntu 发行版的 Linux 二进制包。
- en: 'The installation steps are similar to other nonstandard Ubuntu packages. First,
    you need to add a new package source to the distribution’s source list or add
    a new list file to the `sources.list.d` directory by executing the following in
    your Linux terminal:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 安装步骤与其他非标准 Ubuntu 软件包类似。首先，您需要在 Linux 终端中执行以下操作，将一个新的软件包源添加到发行版的源列表中或者将一个新的列表文件添加到
    `sources.list.d` 目录中：
- en: '`$` `echo``"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt
    \``  stable tensorflow-model-server tensorflow-model-server-universal"``\``|`
    `sudo tee /etc/apt/sources.list.d/tensorflow-serving.list`'
  id: totrans-137
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `echo``"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt
    \``  stable tensorflow-model-server tensorflow-model-server-universal"``\``|`
    `sudo tee /etc/apt/sources.list.d/tensorflow-serving.list`'
- en: 'Before updating your package registry, you should add the packages’ public
    key to your distribution’s key chain:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在更新您的软件包注册表之前，您应将软件包的公钥添加到您发行版的密钥链中：
- en: '`$` `curl https://storage.googleapis.com/tensorflow-serving-apt/``\` `tensorflow-serving.release.pub.gpg`
    `|` `sudo apt-key add -`'
  id: totrans-139
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `curl https://storage.googleapis.com/tensorflow-serving-apt/``\` `tensorflow-serving.release.pub.gpg`
    `|` `sudo apt-key add -`'
- en: 'After updating your package registry, you can install TensorFlow Serving on
    your Ubuntu operating system:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在更新您的软件包注册表后，您可以在您的 Ubuntu 操作系统上安装 TensorFlow Serving：
- en: '`$` `apt-get update` `$` `apt-get install tensorflow-model-server`'
  id: totrans-141
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `apt-get update` `$` `apt-get install tensorflow-model-server`'
- en: TWO UBUNTU PACKAGES FOR TENSORFLOW SERVING
  id: totrans-142
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 用于 TensorFlow Serving 的两个 Ubuntu 软件包
- en: Google provides two Ubuntu packages for TensorFlow Serving! The earlier referenced
    `tensorflow-model-server` package is the preferred package, and it comes with
    specific CPU optimizations precompiled (e.g., AVX instructions).
  id: totrans-143
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 谷歌为 TensorFlow Serving 提供了两个 Ubuntu 软件包！前文提到的 `tensorflow-model-server` 软件包是首选软件包，并带有特定的
    CPU 优化预编译（例如，AVX 指令）。
- en: At the time of writing this chapter, a second package with the name `tensorflow-model-server-universal`
    is also provided. It doesn’t contain the precompiled optimizations and can, therefore,
    be run on old hardware (e.g., CPUs without the AVX instruction set).
  id: totrans-144
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在撰写本章时，还提供了名为 `tensorflow-model-server-universal` 的第二个软件包。它不包含预编译的优化，因此可以在旧硬件上运行（例如，没有
    AVX 指令集的 CPU）。
- en: Building TensorFlow Serving from Source
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 从源代码构建 TensorFlow Serving。
- en: It is recommended to run TensorFlow Serving with the prebuilt Docker image or
    Ubuntu packages. In some situations, you have to compile TensorFlow Serving, for
    example when you want to optimize the model serving for your underlying hardware.
    At the moment, you can only build TensorFlow Serving for Linux operating systems,
    and the build tool `bazel` is required. You can find detailed instructions in
    the [TensorFlow Serving documentation](https://oreil.ly/tUJTw).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 建议使用预构建的 Docker 镜像或 Ubuntu 软件包运行 TensorFlow Serving。在某些情况下，您需要编译 TensorFlow
    Serving，例如当您希望为底层硬件优化模型服务时。目前，您只能为 Linux 操作系统构建 TensorFlow Serving，需要使用构建工具 `bazel`。您可以在
    [TensorFlow Serving 文档](https://oreil.ly/tUJTw) 中找到详细的说明。
- en: OPTIMIZE YOUR TENSORFLOW SERVING INSTANCES
  id: totrans-147
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 优化您的 TensorFlow Serving 实例。
- en: If you build TensorFlow Serving from scratch, we highly recommend compiling
    the Serving version for the specific TensorFlow version of your models and available
    hardware of your serving instances.
  id: totrans-148
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果您从头开始构建 TensorFlow Serving，我们强烈建议为您的模型的特定 TensorFlow 版本和您服务实例的可用硬件编译 Serving
    版本。
- en: Configuring a TensorFlow Server
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 配置 TensorFlow 服务器。
- en: Out of the box, TensorFlow Serving can run in two different modes. First, you
    can specify a model, and have TensorFlow Serving always provide the latest model.
    Alternatively, you can specify a configuration file with all models and versions
    that you want to load, and have TensorFlow Serving load all the named models.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving 出厂设置可以在两种不同模式下运行。首先，您可以指定一个模型，并让 TensorFlow Serving 始终提供最新的模型。或者，您可以指定一个包含所有您想要加载的模型和版本的配置文件，并让
    TensorFlow Serving 加载所有命名模型。
- en: Single Model Configuration
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 单一模型配置。
- en: If you want to run TensorFlow Serving by loading a single model and switching
    to newer model versions when they are available, the single model configuration
    is preferred.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想通过加载单个模型并在可用时切换到更新的模型版本来运行 TensorFlow Serving，则首选单一模型配置。
- en: 'If you run TensorFlow Serving in a Docker environment, you can run the `tensorflow\serving`
    image with the following command:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在 Docker 环境中运行 TensorFlow Serving，则可以使用以下命令运行 `tensorflow\serving` 镜像：
- en: '`$` `docker run -p 8500:8500` `\` ![](images/00002.jpg) `-p 8501:8501` `\`
    `--mount` `type``=``bind``,source``=``/tmp/models,target``=``/models/my_model`
    `\` ![](images/00075.jpg) `-e` `MODEL_NAME``=``my_model` `\` ![](images/00064.jpg)
    `-e` `MODEL_BASE_PATH``=``/models/my_model` `\` `-t tensorflow/serving` ![](images/00055.jpg)'
  id: totrans-154
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `docker run -p 8500:8500` `\` ![](images/00002.jpg) `-p 8501:8501` `\`
    `--mount` `type``=``bind``,source``=``/tmp/models,target``=``/models/my_model`
    `\` ![](images/00075.jpg) `-e` `MODEL_NAME``=``my_model` `\` ![](images/00064.jpg)
    `-e` `MODEL_BASE_PATH``=``/models/my_model` `\` `-t tensorflow/serving` ![](images/00055.jpg)'
- en: '![](images/00002.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: Specify the default ports.
  id: totrans-156
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 指定默认端口。
- en: '![](images/00075.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00075.jpg)'
- en: Mount the model directory.
  id: totrans-158
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 挂载模型目录。
- en: '![](images/00064.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00064.jpg)'
- en: Specify your model.
  id: totrans-160
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 指定您的模型。
- en: '![](images/00055.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00055.jpg)'
- en: Specify the docker image.
  id: totrans-162
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 指定 Docker 镜像。
- en: By default, TensorFlow Serving is configured to create a representational state
    transfer (REST) and Google Remote Procedure Calls (gRPC) endpoint. By specifying
    both ports, 8500 and 8501, we expose the REST and gRPC capabilities.[4](index_split_015.html#filepos994597)
    The docker `run` command creates a mount between a folder on the host (source)
    and the container (target) filesystem. In [Chapter 2](index_split_007.html#filepos83150),
    we discussed how to pass environment variables to the docker container. To run
    the server in a single model configuration, you need to specify the model name
    `MODEL_NAME`.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，TensorFlow Serving 配置为创建一个表述状态传输（REST）和 Google 远程过程调用（gRPC）端点。通过指定端口 8500
    和 8501，我们暴露了 REST 和 gRPC 的能力。[4](index_split_015.html#filepos994597) Docker `run`
    命令在主机（源）和容器（目标）文件系统之间创建了一个挂载点。在[第 2 章](index_split_007.html#filepos83150)中，我们讨论了如何向
    Docker 容器传递环境变量。要在单个模型配置中运行服务器，您需要指定模型名称 `MODEL_NAME`。
- en: 'If you want to run the Docker image prebuilt for GPU images, you need to swap
    out the name of the docker image to the latest GPU build with:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要运行预先构建的用于 GPU 的 Docker 镜像，您需要将 docker 镜像的名称更换为最新的 GPU 构建版本：
- en: '`$` `docker run ...              -t tensorflow/serving:latest-gpu`'
  id: totrans-165
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `docker run ...              -t tensorflow/serving:latest-gpu`'
- en: 'If you have decided to run TensorFlow Serving without the Docker container,
    you can run it with the command:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您决定在没有 Docker 容器的情况下运行 TensorFlow Serving，您可以使用以下命令运行它：
- en: '`$` `tensorflow_model_server --port``=``8500``\` `--rest_api_port``=``8501``\`
    `--model_name``=``my_model` `\` `--model_base_path``=``/models/my_model`'
  id: totrans-167
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `tensorflow_model_server --port``=``8500``\` `--rest_api_port``=``8501``\`
    `--model_name``=``my_model` `\` `--model_base_path``=``/models/my_model`'
- en: 'In both scenarios, you should see output on your terminal that is similar to
    the following:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，您应该在终端上看到类似以下的输出：
- en: '`2019-04-26 03:51:20.304826: I tensorflow_serving/model_servers/ server.cc:82``]`
    `Building single TensorFlow model file config:   model_name: my_model model_base_path:
    /models/my_model 2019-04-26 03:51:20: I tensorflow_serving/model_servers/server_core.cc:461``]`
    `Adding/updating models. 2019-04-26 03:51:20: I tensorflow_serving/model_servers/
    server_core.cc:558``]``(``Re-``)``adding model: my_model ... 2019-04-26 03:51:34.507436:
    I tensorflow_serving/core/loader_harness.cc:86``]` `Successfully loaded servable
    version` `{``name: my_model version: 1556250435``}` `2019-04-26 03:51:34.516601:
    I tensorflow_serving/model_servers/server.cc:313``]` `Running gRPC ModelServer
    at 0.0.0.0:8500 ...` `[``warn``]` `getaddrinfo: address family` `for` `nodename
    not supported` `[``evhttp_server.cc : 237``]` `RAW: Entering the event loop ...
    2019-04-26 03:51:34.520287: I tensorflow_serving/model_servers/server.cc:333``]`
    `Exporting HTTP/REST API at:localhost:8501 ...`'
  id: totrans-169
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`2019-04-26 03:51:20.304826: I tensorflow_serving/model_servers/ server.cc:82``]`
    `构建单个 TensorFlow 模型文件配置：   model_name: my_model model_base_path: /models/my_model
    2019-04-26 03:51:20: I tensorflow_serving/model_servers/server_core.cc:461``]`
    `添加/更新模型。 2019-04-26 03:51:20: I tensorflow_serving/model_servers/ server_core.cc:558``]``(``重新``)``添加模型：my_model
    ... 2019-04-26 03:51:34.507436: I tensorflow_serving/core/loader_harness.cc:86``]`
    `成功加载可服务版本` `{``name: my_model version: 1556250435``}` `2019-04-26 03:51:34.516601:
    I tensorflow_serving/model_servers/server.cc:313``]` `在 0.0.0.0:8500 运行 gRPC ModelServer
    ...` `[``warn``]` `getaddrinfo：不支持节点名称的地址族` `[``evhttp_server.cc:237``]` `RAW：进入事件循环
    ... 2019-04-26 03:51:34.520287: I tensorflow_serving/model_servers/server.cc:333``]`
    `在 localhost:8501 导出 HTTP/REST API ...`'
- en: 'From the server output, you can see that the server loaded our model `my_model`
    successfully, and that created two endpoints: one REST and one gRPC endpoint.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 从服务器输出中，您可以看到服务器成功加载了我们的模型 `my_model`，并创建了两个端点：一个 REST 端点和一个 gRPC 端点。
- en: TensorFlow Serving makes the deployment of machine learning models extremely
    easy. One great advantage of serving models with TensorFlow Serving is the hot
    swap capability. If a new model is uploaded, the server’s model manager will detect
    the new version, unload the existing model, and load the newer model for inferencing.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving 使得部署机器学习模型变得极其简单。使用 TensorFlow Serving 提供模型的一个巨大优势是其热替换能力。如果上传了新模型，服务器的模型管理器将检测到新版本，卸载现有模型，并加载更新的模型以进行推理。
- en: 'Let’s say you update the model and export the new model version to the mounted
    folder on the host machine (if you are running with the docker setup) and no configuration
    change is required. The model manager will detect the newer model and reload the
    endpoints. It will notify you about the unloading of the older model and the loading
    of the newer model. In your terminal, you should find messages like:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您更新了模型并将新模型版本导出到主机机器上的挂载文件夹（如果您正在使用 Docker 设置），不需要进行配置更改。模型管理器将检测到更新的模型并重新加载端点。它将通知您有关旧模型卸载和新模型加载的消息。在您的终端中，您应该会找到类似以下的消息：
- en: '`2019-04-30 00:21:56.486988: I tensorflow_serving/core/basic_manager.cc:739``]`
    `Successfully reserved resources to load servable` `{``name: my_model version:
    1556583584``}` `2019-04-30 00:21:56.487043: I tensorflow_serving/core/loader_harness.cc:66``]`
    `Approving load` `for` `servable version` `{``name: my_model version: 1556583584``}`
    `2019-04-30 00:21:56.487071: I tensorflow_serving/core/loader_harness.cc:74``]`
    `Loading servable version` `{``name: my_model version: 1556583584``}` `... 2019-04-30
    00:22:08.839375: I tensorflow_serving/core/loader_harness.cc:119``]` `Unloading
    servable version` `{``name: my_model version: 1556583236``}` `2019-04-30 00:22:10.292695:
    I ./tensorflow_serving/core/simple_loader.h:294``]` `Calling MallocExtension_ReleaseToSystem``()`
    `after servable unload with 1262338988 2019-04-30 00:22:10.292771: I tensorflow_serving/core/loader_harness.cc:127``]`
    `Done unloading servable version` `{``name: my_model version: 1556583236``}`'
  id: totrans-173
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`2019-04-30 00:21:56.486988: I tensorflow_serving/core/basic_manager.cc:739`
    `]` `成功保留资源以加载可服务的` `{` `name: my_model version: 1556583584` `}` `2019-04-30 00:21:56.487043:
    I tensorflow_serving/core/loader_harness.cc:66` `]` `批准加载` `服务版本` `{` `name: my_model
    version: 1556583584` `}` `2019-04-30 00:21:56.487071: I tensorflow_serving/core/loader_harness.cc:74`
    `]` `加载服务版本` `{` `name: my_model version: 1556583584` `}` `... 2019-04-30 00:22:08.839375:
    I tensorflow_serving/core/loader_harness.cc:119` `]` `卸载服务版本` `{` `name: my_model
    version: 1556583236` `}` `2019-04-30 00:22:10.292695: I ./tensorflow_serving/core/simple_loader.h:294`
    `]` `在释放带有 1262338988 的服务卸载后调用 MallocExtension_ReleaseToSystem` `()` `2019-04-30
    00:22:10.292771: I tensorflow_serving/core/loader_harness.cc:127` `]` `完成卸载服务版本`
    `{` `name: my_model version: 1556583236` `}`'
- en: By default, TensorFlow Serving will load the model with the highest version
    number. If you use the export methods shown earlier in this chapter, all models
    will be exported in folders with the epoch timestamp as the folder name. Therefore,
    newer models will have a higher version number than older models.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，TensorFlow Serving 将加载具有最高版本号的模型。如果您使用本章前面展示的导出方法，所有模型将以时代时间戳作为文件夹名称导出。因此，新模型的版本号将高于旧模型。
- en: The same default model loading policy of TensorFlow Serving also allows model
    rollbacks. In case you want to roll back a model version, you can delete the model
    version from the base path. The model server will then detect the removal of the
    version with the next polling of the filesystem,[5](index_split_015.html#filepos994951)
    unload the deleted model, and load the most recent, existing model version.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving 的相同默认模型加载策略还允许模型回滚。如果您想回滚模型版本，可以从基本路径删除模型版本。模型服务器将在下一次轮询文件系统时检测到版本的删除，卸载已删除的模型，并加载最近的现有模型版本。
- en: Multiple Model Configuration
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 多模型配置
- en: 'You can also configure TensorFlow Serving to load multiple models at the same
    time. To do that, you need to create a configuration file to specify the models:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以配置 TensorFlow Serving 同时加载多个模型。为此，您需要创建一个配置文件来指定模型：
- en: '`model_config_list``{``config``{``name:``''my_model''``base_path:``''/models/my_model/''``model_platform:``''tensorflow''``}``config``{``name:``''another_model''``base_path:``''/models/another_model/''``model_platform:``''tensorflow''``}``}`'
  id: totrans-178
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`model_config_list` `{` `config` `{` `name:` `''my_model''` `base_path:` `''/models/my_model/''`
    `model_platform:` `''tensorflow''` `}` `config` `{` `name:` `''another_model''`
    `base_path:` `''/models/another_model/''` `model_platform:` `''tensorflow''` `}`
    `}`'
- en: The configuration file contains one or more `config` dictionaries, all listed
    below a `model_config_list` key.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 配置文件包含一个或多个 `config` 字典，所有这些都在 `model_config_list` 键下列出。
- en: 'In your Docker configuration, you can mount the configuration file and load
    the model server with the configuration file instead of a single model:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的 Docker 配置中，您可以挂载配置文件，并使用配置文件而不是单个模型加载模型服务器：
- en: '`$` `docker run -p 8500:8500` `\` `-p 8501:8501` `\` `--mount` `type``=``bind``,source``=``/tmp/models,target``=``/models/my_model`
    `\` `--mount` `type``=``bind``,source``=``/tmp/model_config,``\` `target``=``/models/model_config`
    `\` ![](images/00002.jpg) `-e` `MODEL_NAME``=``my_model` `\` `-t tensorflow/serving`
    `\` `             --model_config_file``=``/models/model_config` ![](images/00075.jpg)'
  id: totrans-181
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `docker run -p 8500:8500` `\` `-p 8501:8501` `\` `--mount` `type``=``bind``,source``=``/tmp/models,target``=``/models/my_model`
    `\` `--mount` `type``=``bind``,source``=``/tmp/model_config,``\` `target``=``/models/model_config`
    `\` ![](images/00002.jpg) `-e` `MODEL_NAME``=``my_model` `\` `-t tensorflow/serving`
    `\` `             --model_config_file``=``/models/model_config` ![](images/00075.jpg)'
- en: '![](images/00002.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: Mount the configuration file.
  id: totrans-183
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 挂载配置文件。
- en: '![](images/00075.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00075.jpg)'
- en: Specify the model configuration file.
  id: totrans-185
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 指定模型配置文件。
- en: 'If you use TensorFlow Serving outside of a Docker container, you can point
    the model server to the configuration file with the argument `model_config_file`,
    which loads and the configuration from the file:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在 Docker 容器之外使用 TensorFlow Serving，可以使用参数 `model_config_file` 将模型服务器指向配置文件，该文件加载并从文件中进行配置：
- en: '`$` `tensorflow_model_server --port``=``8500``\` `--rest_api_port``=``8501``\`
    `--model_config_file``=``/models/model_config`'
  id: totrans-187
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `tensorflow_model_server --port``=``8500``\` `--rest_api_port``=``8501``\`
    `--model_config_file``=``/models/model_config`'
- en: CONFIGURE SPECIFIC MODEL VERSIONS
  id: totrans-188
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 配置特定模型版本
- en: 'There are situations when you want to load not just the latest model version,
    but either all or specific model versions. For example, you may want to do model
    A/B testing, as we will discuss in [“Model A/B Testing with TensorFlow Serving”](#filepos911603),
    or provide a stable and a development model version. TensorFlow Serving, by default,
    always loads the latest model version. If you want to load a set of available
    model versions, you can extend the model configuration file with:'
  id: totrans-189
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 有些情况下，您不仅想加载最新的模型版本，而是所有或特定的模型版本。例如，您可能想进行模型 A/B 测试，如我们将在[“使用 TensorFlow Serving
    进行模型 A/B 测试”](#filepos911603)中讨论的那样，或者提供一个稳定版本和一个开发版本。TensorFlow Serving 默认始终加载最新的模型版本。如果您想加载一组可用的模型版本，您可以通过以下方式扩展模型配置文件：
- en: '`...``config``{``name:``''another_model''``base_path:``''/models/another_model/''``model_version_policy:``{all:``{``}``}``}``...`'
  id: totrans-190
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`...``config``{``name:``''another_model''``base_path:``''/models/another_model/''``model_version_policy``{``specific``{``versions:``1556250435``versions:``1556251435``}``}``}``...'
- en: 'If you want to specify specific model versions, you can define them as well:'
  id: totrans-191
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果您想指定特定的模型版本，您也可以定义它们：
- en: '`...``config``{``name:``''another_model''``base_path:``''/models/another_model/''``model_version_policy``{``specific``{``versions:``1556250435``versions:``1556251435``}``}``}``...`'
  id: totrans-192
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`...``config``{``name:``''another_model''``base_path:``''/models/another_model/''``model_version_policy``{``specific``{``versions:``1556250435``versions:``1556251435``}``}``}``...'
- en: 'You can even give the model version labels. The labels can be extremely handy
    later when you want to make predictions from the models. At the time of writing,
    version labels were only available through TensorFlow Serving’s gRPC endpoints:'
  id: totrans-193
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 您甚至可以提供模型版本的标签。这些标签在以后从模型进行预测时非常方便。在撰写本文时，版本标签仅通过 TensorFlow Serving 的 gRPC
    端点提供：
- en: '`...``model_version_policy``{``specific``{``versions:``1556250435``versions:``1556251435``}``}``version_labels``{``key:``''stable''``value:``1556250435``}``version_labels``{``key:``''testing''``value:``1556251435``}``...`'
  id: totrans-194
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`...``model_version_policy``{``specific``{``versions:``1556250435``versions:``1556251435``}``}``version_labels``{``key:``''stable''``value:``1556250435``}``version_labels``{``key:``''testing''``value:``1556251435``}``...'
- en: With the model version now configured, we can use those endpoints for the versions
    to run our model A/B test. If you are interested in how to infer these model versions,
    we recommend [“Model A/B Testing with TensorFlow Serving”](#filepos911603) for
    an example of a simple implementation.
  id: totrans-195
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在配置了模型版本，我们可以使用这些端点来运行我们的模型 A/B 测试版本。如果您想了解如何推断这些模型版本，我们建议参考[“使用 TensorFlow
    Serving 进行模型 A/B 测试”](#filepos911603)，这里有一个简单实现的例子。
- en: Starting with TensorFlow Serving 2.3, the version_label functionality will be
    available for REST endpoints in addition to the existing gRPC functionality of
    TensorFlow Serving.
  id: totrans-196
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 自 TensorFlow Serving 2.3 开始，版本标签功能将适用于 REST 端点，除了现有的 TensorFlow Serving 的 gRPC
    功能之外。
- en: REST Versus gRPC
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: REST 与 gRPC
- en: 'In [“Single Model Configuration”](index_split_013.html#filepos838316), we discussed
    how TensorFlow Serving allows two different API types: REST and gRPC. Both protocols
    have their advantages and disadvantages, and we would like to take a moment to
    introduce both before we dive into how you can communicate with these endpoints.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在[“单一模型配置”](index_split_013.html#filepos838316)中，我们讨论了TensorFlow Serving如何允许两种不同的API类型：REST和gRPC。这两种协议都有各自的优缺点，在我们深入介绍如何与这些端点通信之前，我们想先介绍一下它们。
- en: REST
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: REST
- en: REST is a communication “protocol” used by today’s web services. It isn’t a
    formal protocol, but more a communication style that defines how clients communicate
    with web services. REST clients communicate with the server using the standard
    HTTP methods like `GET`, `POST`, `DELETE`, etc. The payloads of the requests are
    often encoded as XML or JSON data formats.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: REST是当今Web服务使用的通信“协议”。它不是正式协议，而是定义客户端如何与Web服务通信的通信样式。REST客户端使用标准的HTTP方法（如`GET`，`POST`，`DELETE`等）与服务器通信。请求的有效负载通常编码为XML或JSON数据格式。
- en: gRPC
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: gRPC
- en: gRPC is a remote procedure protocol developed by Google. While gRPC supports
    different data formats, the standard data format used with gRPC is protocol buffer,
    which we used throughout this book. gRPC provides low-latency communication and
    smaller payloads if protocol buffers are used. gRPC was designed with APIs in
    mind. The downside is that the payloads are in a binary format, which can make
    a quick inspection difficult.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: gRPC是由Google开发的远程过程调用协议。虽然gRPC支持不同的数据格式，但与gRPC一起使用的标准数据格式是协议缓冲区，这在本书中被广泛使用。gRPC提供低延迟通信和较小的有效负载（如果使用协议缓冲区）。gRPC设计时考虑了API。缺点是有效负载以二进制格式存在，这可能使快速检查变得困难。
- en: WHICH PROTOCOL TO USE?
  id: totrans-203
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用哪种协议？
- en: On the one hand, it looks very convenient to communicate with the model server
    over REST. The endpoints are easy to infer, the payloads can be easily inspected,
    and the endpoints can be tested with `curl` requests or browser tools. REST libraries
    are widely available for all sorts of clients and often are already available
    on the client system (i.e., a mobile application).
  id: totrans-204
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一方面，通过REST与模型服务器进行通信看起来非常方便。端点易于推断，有效负载可以轻松检查，并且可以使用`curl`请求或浏览器工具测试端点。REST库广泛适用于各种客户端，并且通常已在客户端系统上可用（例如移动应用程序）。
- en: On the other hand, gRPC APIs have a higher burden of entry initially. gRPC libraries
    often need to be installed on the client side. However, they can lead to significant
    performance improvements depending on the data structures required for the model
    inference. If your model experiences many requests, the reduced payload size due
    to the protocol buffer serialization can be beneficial.
  id: totrans-205
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 另一方面，gRPC API在初始阶段具有更高的入门门槛。 gRPC库通常需要在客户端上安装。但是，根据模型推断所需的数据结构，它们可以带来显著的性能改进。如果您的模型接收到许多请求，则由于协议缓冲区序列化而减少的有效负载大小可能会有所帮助。
- en: Internally, TensorFlow Serving converts JSON data structures submitted via REST
    to `tf.Example` data structures, and this can lead to slower performance. Therefore,
    you might see better performance with gRPC requests if the conversion requires
    many type conversions (i.e., if you submit a large array with float values).
  id: totrans-206
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在内部，TensorFlow Serving将通过REST提交的JSON数据结构转换为`tf.Example`数据结构，这可能导致性能较慢。因此，如果转换需要许多类型转换（即，如果您提交包含浮点值的大数组），则通过gRPC请求可能会获得更好的性能。
- en: Making Predictions from the Model Server
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 从模型服务器获取预测
- en: Until now, we have entirely focused on the model server setup. In this section,
    we want to demonstrate how a client (e.g., a web app), can interact with the model
    server. All code examples concerning REST or gRPC requests are executed on the
    client side.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们完全专注于模型服务器的设置。在本节中，我们想展示客户端（例如Web应用程序）如何与模型服务器交互。所有关于REST或gRPC请求的代码示例都在客户端上执行。
- en: Getting Model Predictions via REST
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 通过REST获取模型预测
- en: 'To call the model server over REST, you’ll need a Python library to facilitate
    the communication for you. The standard library these days is `requests`. Install
    the library:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 要通过REST调用模型服务器，您需要一个Python库来为您简化通信。目前的标准库是`requests`。安装该库：
- en: '`$` `pip install requests`'
  id: totrans-211
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `pip install requests`'
- en: The following example showcases an example `POST` request.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的示例展示了一个`POST`请求的示例。
- en: '`import``requests``url``=``"http://some-domain.abc"``payload``=``{``"key_1"``:``"value_1"``}``r``=``requests``.``post``(``url``,``json``=``payload``)`![](images/00002.jpg)`print``(``r``.``json``())`![](images/00075.jpg)`#
    {''data'': ...}`'
  id: totrans-213
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`import``requests``url``=``"http://some-domain.abc"``payload``=``{``"key_1"``:``"value_1"``}``r``=``requests``.``post``(``url``,``json``=``payload``)`![](images/00002.jpg)`print``(``r``.``json``())`![](images/00075.jpg)`#
    {''data'': ...}`'
- en: '![](images/00002.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: Submit the request.
  id: totrans-215
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 提交请求。
- en: '![](images/00075.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00075.jpg)'
- en: View the HTTP response.
  id: totrans-217
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 查看 HTTP 响应。
- en: URL structure
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: URL 结构
- en: 'The URL for your HTTP request to the model server contains information about
    which model and which version you would like to infer:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 您的 HTTP 请求 URL 包含有关您想要推断的模型和版本的信息：
- en: '`http://``{``HOST``}``:``{``PORT``}``/v1/models/``{``MODEL_NAME``}``:``{``VERB``}`'
  id: totrans-220
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`http://``{``HOST``}``:``{``PORT``}``/v1/models/``{``MODEL_NAME``}``:``{``VERB``}`'
- en: HOST
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 主机
- en: The host is the IP address or domain name of your model server. If you run your
    model server on the same machine where you run your client code, you can set the
    host to `localhost`.
  id: totrans-222
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 主机是您模型服务器的 IP 地址或域名。如果您在同一台机器上运行模型服务器和客户端代码，您可以将主机设置为 `localhost`。
- en: PORT
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 端口
- en: You’ll need to specify the port in your request URL. The standard port for the
    REST API is 8501\. If this conflicts with other services in your service ecosystem,
    you can change the port in your server arguments during the startup of the server.
  id: totrans-224
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在您的请求 URL 中需要指定端口。REST API 的标准端口为 8501\. 如果这与您服务生态系统中的其他服务冲突，您可以在服务器启动期间通过服务器参数更改端口。
- en: MODEL_NAME
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 模型名称
- en: The model name needs to match the name of your model when you either set up
    your model configuration or started up the model server.
  id: totrans-226
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当您设置模型配置或启动模型服务器时，模型名称需要与您模型的名称匹配。
- en: VERB
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 动词
- en: 'The type of model is specified through the verb in the URL. You have three
    options: `predict`, `classify`, or `regress`. The verb corresponds to the signature
    methods of the endpoint.'
  id: totrans-228
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: URL 中的动词指定了模型的类型。您有三个选项：`predict`、`classify` 或 `regress`。动词对应于终点的签名方法。
- en: MODEL_VERSION
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 模型版本
- en: 'If you want to make predictions from a specific model version, you’ll need
    to extend the URL with the model version identifier:'
  id: totrans-230
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果您想要从特定模型版本进行预测，您需要使用模型版本标识符扩展 URL：
- en: '`http://``{``HOST``}``:``{``PORT``}``/v1/models/``{``MODEL_NAME``}[``/versions/``${``MODEL_VERSION``}``]``:``{``VERB``}`'
  id: totrans-231
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`http://``{``HOST``}``:``{``PORT``}``/v1/models/``{``MODEL_NAME``}[``/versions/``${``MODEL_VERSION``}``]``:``{``VERB``}`'
- en: Payloads
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 负载
- en: 'With the URL in place, let’s discuss the request payloads. TensorFlow Serving
    expects the input data as a JSON data structure, as shown in the following example:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在 URL 就绪后，让我们讨论请求负载。TensorFlow Serving 期望将输入数据作为 JSON 数据结构提交，如以下示例所示：
- en: '`{``"signature_name"``:``<string>``,``"instances"``:``<value>``}`'
  id: totrans-234
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`{``"signature_name"``:``<string>``,``"instances"``:``<value>``}`'
- en: The `signature_name` is not required. If it isn’t specified, the model server
    will infer the model graph signed with the default `serving` label.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要 `signature_name`。如果未指定，模型服务器将推断使用默认的 `serving` 标签签署的模型图。
- en: The input data is expected either as a list of objects or as a list of input
    values. To submit multiple data samples, you can submit them as a list under the
    `instances` key.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据预期为对象列表或输入值列表。要提交多个数据样本，您可以将它们作为列表提交在 `instances` 键下。
- en: 'If you want to submit one data example for the inference, you can use `inputs`
    and list all input values as a list. One of the keys, `instances` and `inputs`,
    has to be present, but never both at the same time:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要为推理提交一个数据示例，您可以使用 `inputs` 并将所有输入值列为列表。其中的一个键，`instances` 和 `inputs`，必须存在，但不能同时出现：
- en: '`{``"signature_name"``:``<string>``,``"inputs"``:``<value>``}`'
  id: totrans-238
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`{``"signature_name"``:``<string>``,``"inputs"``:``<value>``}`'
- en: '[Example 8-6](#filepos881325) shows an example of how to request a model prediction
    from our TensorFlow Serving endpoint. We only submit one data example for the
    inference in our example, but we could easily submit a list of data inputs representing
    multiple requests.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 8-6](#filepos881325) 展示了如何从我们的 TensorFlow 服务端点请求模型预测的示例。在我们的示例中，我们只提交了一个数据示例用于推理，但我们可以轻松地提交代表多个请求的数据输入列表。'
- en: Example 8-6\. Example model prediction request with a Python client
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 8-6\. 使用 Python 客户端进行模型预测请求的示例
- en: '`import``requests``def``get_rest_request``(``text``,``model_name``=``"my_model"``):``url``=``"http://localhost:8501/v1/models/{}:predict"``.``format``(``model_name``)`![](images/00002.jpg)`payload``=``{``"instances"``:``[``text``]}`![](images/00075.jpg)`response``=``requests``.``post``(``url``=``url``,``json``=``payload``)``return``response``rs_rest``=``get_rest_request``(``text``=``"classify
    my text"``)``rs_rest``.``json``()`'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '`import``requests``def``get_rest_request``(``text``,``model_name``=``"my_model"``):``url``=``"http://localhost:8501/v1/models/{}:predict"``.``format``(``model_name``)`![](images/00002.jpg)`payload``=``{``"instances"``:``[``text``]}`![](images/00075.jpg)`response``=``requests``.``post``(``url``=``url``,``json``=``payload``)``return``response``rs_rest``=``get_rest_request``(``text``=``"classify
    my text"``)``rs_rest``.``json``()`'
- en: '![](images/00002.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: Exchange `localhost` with an IP address if the server is not running on the
    same machine.
  id: totrans-243
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 将服务器未在同一台机器上运行时，将`localhost`替换为IP地址。
- en: '![](images/00075.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00075.jpg)'
- en: Add more examples to the `instance` list if you want to infer more samples.
  id: totrans-245
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果要推断更多示例，请在`instance`列表中添加更多示例。
- en: Using TensorFlow Serving via gRPC
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 通过gRPC使用TensorFlow Serving
- en: If you want to use the model with gRPC, the steps are slightly different from
    the REST API requests.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要使用gRPC模型，步骤与REST API请求略有不同。
- en: 'First, you establish a gRPC `channel`. The channel provides the connection
    to the gRPC server at a given host address and over a given port. If you require
    a secure connection, you need to establish a secure channel at this point. Once
    the channel is established, you’ll create a `stub`. A `stub` is a local object
    which replicates the available methods from the server:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，建立一个gRPC `channel`。该通道提供与给定主机地址上的gRPC服务器的连接，并通过给定端口。如果需要安全连接，则需要在此时建立安全通道。通道建立后，将创建一个`stub`。`stub`是一个本地对象，复制自服务器中可用的方法：
- en: '`import``grpc``from``tensorflow_serving.apis``import``predict_pb2``from``tensorflow_serving.apis``import``prediction_service_pb2_grpc``import``tensorflow``as``tf``def``create_grpc_stub``(``host``,``port``=``8500``):``hostport``=``"{}:{}"``.``format``(``host``,``port``)``channel``=``grpc``.``insecure_channel``(``hostport``)``stub``=``prediction_service_pb2_grpc``.``PredictionServiceStub``(``channel``)``return``stub`'
  id: totrans-249
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`import``grpc``from``tensorflow_serving.apis``import``predict_pb2``from``tensorflow_serving.apis``import``prediction_service_pb2_grpc``import``tensorflow``as``tf``def``create_grpc_stub``(``host``,``port``=``8500``):``hostport``=``"{}:{}"``.``format``(``host``,``port``)``channel``=``grpc``.``insecure_channel``(``hostport``)``stub``=``prediction_service_pb2_grpc``.``PredictionServiceStub``(``channel``)``return``stub`'
- en: 'Once the gRPC stub is created, we can set the model and the signature to access
    predictions from the correct model and submit our data for the inference:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 创建gRPC stub后，我们可以设置模型和签名，以访问正确模型的预测并提交我们的数据进行推理：
- en: '`def``grpc_request``(``stub``,``data_sample``,``model_name``=``''my_model''``,`
    `\` `signature_name``=``''classification''``):``request``=``predict_pb2``.``PredictRequest``()``request``.``model_spec``.``name``=``model_name``request``.``model_spec``.``signature_name``=``signature_name``request``.``inputs``[``''inputs''``]``.``CopyFrom``(``tf``.``make_tensor_proto``(``data_sample``,``shape``=``[``1``,``1``]))`![](images/00002.jpg)`result_future``=``stub``.``Predict``.``future``(``request``,``10``)`![](images/00075.jpg)`return``result_future`'
  id: totrans-251
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`def``grpc_request``(``stub``,``data_sample``,``model_name``=``''my_model''``,`
    `\` `signature_name``=``''classification''``):``request``=``predict_pb2``.``PredictRequest``()``request``.``model_spec``.``name``=``model_name``request``.``model_spec``.``signature_name``=``signature_name``request``.``inputs``[``''inputs''``]``.``CopyFrom``(``tf``.``make_tensor_proto``(``data_sample``,``shape``=``[``1``,``1``]))`![](images/00002.jpg)`result_future``=``stub``.``Predict``.``future``(``request``,``10``)`![](images/00075.jpg)`return``result_future`'
- en: '![](images/00002.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: '`inputs` is the name of the input of our neural network.'
  id: totrans-253
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`inputs`是我们神经网络输入的名称。'
- en: '![](images/00075.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00075.jpg)'
- en: 10 is the max time in seconds before the function times out.
  id: totrans-255
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 10是函数超时前的最大时间（秒数）。
- en: 'With the two function, now available, we can infer our example datasets with
    the two function calls:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有了这两个可用的函数，我们可以使用这两个函数调用推断我们的示例数据集：
- en: '`stub``=``create_grpc_stub``(``host``,``port``=``8500``)``rs_grpc``=``grpc_request``(``stub``,``data``)`'
  id: totrans-257
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`stub``=``create_grpc_stub``(``host``,``port``=``8500``)``rs_grpc``=``grpc_request``(``stub``,``data``)`'
- en: SECURE CONNECTIONS
  id: totrans-258
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 安全连接
- en: 'The `grpc` library also provides functionality to connect securely with the
    gRPC endpoints. The following example shows how to create a secure channel with
    gRPC from the client side:'
  id: totrans-259
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`grpc`库还提供了与gRPC端点安全连接的功能。以下示例显示如何从客户端端创建安全通道与gRPC：'
- en: '`import grpc` `cert``=` `open``(``client_cert_file,` `''rb''``)``.read``()``key``=`
    `open``(``client_key_file,` `''rb''``)``.read``()``ca_cert``=` `open``(``ca_cert_file,`
    `''rb''``)``.read``()``if` `ca_cert_file` `else``''''``credentials``=` `grpc.ssl_channel_credentials``(`
    `ca_cert, key, cert` `)``channel``=` `implementations.secure_channel``(``hostport,
    credentials``)`'
  id: totrans-260
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`import grpc` `cert``=` `open``(``client_cert_file,` `''rb''``)``.read``()``key``=`
    `open``(``client_key_file,` `''rb''``)``.read``()``ca_cert``=` `open``(``ca_cert_file,`
    `''rb''``)``.read``()``if` `ca_cert_file` `else``''''``credentials``=` `grpc.ssl_channel_credentials``(`
    `ca_cert, key, cert` `)``channel``=` `implementations.secure_channel``(``hostport,
    credentials``)`'
- en: On the server side, TensorFlow Serving can terminate secure connections if SSL
    is configured. To terminate secure connections, create an SSL configuration file
    as shown in the following example:[6](index_split_015.html#filepos995324)
  id: totrans-261
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在服务器端，如果配置了 SSL，TensorFlow Serving 可以终止安全连接。要终止安全连接，请按以下示例创建 SSL 配置文件：[6](index_split_015.html#filepos995324)
- en: '`server_key:` `"-----BEGIN PRIVATE KEY-----\n``              <your_ssl_key>\n``             
    -----END PRIVATE KEY-----"` `server_cert:` `"-----BEGIN CERTIFICATE-----\n``             
    <your_ssl_cert>\n``              -----END CERTIFICATE-----"` `custom_ca:` `""`
    `client_verify:` `false`'
  id: totrans-262
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`server_key:` `"-----BEGIN PRIVATE KEY-----\n``              <your_ssl_key>\n``             
    -----END PRIVATE KEY-----"` `server_cert:` `"-----BEGIN CERTIFICATE-----\n``             
    <your_ssl_cert>\n``              -----END CERTIFICATE-----"` `custom_ca:` `""`
    `client_verify:` `false`'
- en: 'Once you have created the configuration file, you can pass the file path to
    the TensorFlow Serving argument `--ssl_config_file` during the start of TensorFlow
    Serving:'
  id: totrans-263
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 创建了配置文件后，您可以在启动 TensorFlow Serving 时将文件路径传递给 `--ssl_config_file` 参数：
- en: '`$` `tensorflow_model_server --port``=``8500``\` `--rest_api_port``=``8501``\`
    `--model_name``=``my_model` `\` `--model_base_path``=``/models/my_model` `\` `--ssl_config_file``=``"<path_to_config_file>"`'
  id: totrans-264
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `tensorflow_model_server --port``=``8500``\` `--rest_api_port``=``8501``\`
    `--model_name``=``my_model` `\` `--model_base_path``=``/models/my_model` `\` `--ssl_config_file``=``"<path_to_config_file>"`'
- en: Getting predictions from classification and regression models
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 从分类和回归模型获取预测结果
- en: If you are interested in making predictions from classification and regression
    models, you can use the gRPC API.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对从分类和回归模型进行预测感兴趣，可以使用 gRPC API。
- en: 'If you would like to get predictions from a classification model, you will
    need to swap out the following lines:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想从分类模型获取预测结果，则需要替换以下行：
- en: '`from``tensorflow_serving.apis``import``predict_pb2``...``request``=``predict_pb2``.``PredictRequest``()`'
  id: totrans-268
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`from``tensorflow_serving.apis``import``predict_pb2``...``request``=``predict_pb2``.``PredictRequest``()`'
- en: 'with:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 'with:'
- en: '`from``tensorflow_serving.apis``import``classification_pb2``...``request``=``classification_pb2``.``ClassificationRequest``()`'
  id: totrans-270
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`from``tensorflow_serving.apis``import``classification_pb2``...``request``=``classification_pb2``.``ClassificationRequest``()`'
- en: 'If you want to get predictions from a regression model, you can use the following
    imports:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想从回归模型获取预测结果，可以使用以下导入语句：
- en: '`from``tensorflow_serving.apis``import``regression_pb2``...``regression_pb2``.``RegressionRequest``()`'
  id: totrans-272
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`from``tensorflow_serving.apis``import``regression_pb2``...``regression_pb2``.``RegressionRequest``()`'
- en: Payloads
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: Payloads
- en: gRPC API uses Protocol Buffers as the data structure for the API request. By
    using binary Protocol Buffer payloads, the API requests use less bandwidth compared
    to JSON payloads. Also, depending on the model input data structure, you might
    experience faster predictions as with REST endpoints. The performance difference
    is explained by the fact that the submitted JSON data will be converted to a `tf.Example`
    data structure. This conversion can slow down the model server inference, and
    you might encounter a slower inference performance than in the gRPC API case.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: gRPC API 使用协议缓冲作为 API 请求的数据结构。通过使用二进制协议缓冲有效减少了 API 请求所需的带宽，与 JSON 负载相比。此外，根据模型输入数据结构的不同，您可能会体验到更快的预测速度，如同
    REST 终端节点一样。性能差异在于，提交的 JSON 数据将被转换为 `tf.Example` 数据结构。这种转换可能会减慢模型服务器的推断速度，您可能会遇到比
    gRPC API 情况下更慢的推断性能。
- en: Your data submitted to the gRPC endpoints needs to be converted to the protocol
    buffer data structure. TensorFlow provides you a handy utility function to perform
    the conversion called `tf.make_tensor_proto`. It allows various data formats,
    including scalars, lists, NumPy scalars, and NumPy arrays. The function will then
    convert the given Python or NumPy data structures to the protocol buffer format
    for the inference.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 您提交给 gRPC 终端节点的数据需要转换为协议缓冲数据结构。TensorFlow 为您提供了一个便捷的实用函数来执行转换，称为 `tf.make_tensor_proto`。它允许各种数据格式，包括标量、列表、NumPy
    标量和 NumPy 数组。该函数将会把给定的 Python 或 NumPy 数据结构转换为推断所需的协议缓冲格式。
- en: Model A/B Testing with TensorFlow Serving
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TensorFlow Serving 进行模型 A/B 测试
- en: A/B testing is an excellent methodology to test different models in real-life
    situations. In this scenario, a certain percentage of clients will receive predictions
    from model version A and all other requests will be served by model version B.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: A/B 测试是在实际情况下测试不同模型的优秀方法论。在这种情况下，一定比例的客户将接收模型版本 A 的预测，所有其他请求将由模型版本 B 提供服务。
- en: We discussed earlier that you could configure TensorFlow Serving to load multiple
    model versions and then specify the model version in your REST request URL or
    gRPC specifications.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论过，您可以配置 TensorFlow Serving 加载多个模型版本，然后在您的 REST 请求 URL 或 gRPC 规范中指定模型版本。
- en: TensorFlow Serving doesn’t support server-side A/B testing, meaning that the
    model server will direct all client requests to a single endpoint to two model
    versions. But with a little tweak to our request URL, we can provide the appropriate
    support for random A/B testing from the client side:[7](index_split_015.html#filepos995729)
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving 不支持服务器端的 A/B 测试，这意味着模型服务器将所有客户端请求重定向到两个模型版本中的单一端点。但是通过稍微调整我们的请求
    URL，我们可以为客户端提供对随机 A/B 测试的适当支持：[7](index_split_015.html#filepos995729)
- en: '`from``random``import``random`![](images/00002.jpg)`def``get_rest_url``(``model_name``,``host``=``''localhost''``,``port``=``8501``,``verb``=``''predict''``,``version``=``None``):``url``=``"http://{}:{}/v1/models/{}/"``.``format``(``host``,``port``,``model_name``)``if``version``:``url``+=``"versions/{}"``.``format``(``version``)``url``+=``":{}"``.``format``(``verb``)``return``url``...``#
    Submit 10% of all requests from this client to version 1.``# 90% of the requests
    should go to the default models.``threshold``=``0.1``version``=``1``if``random``()``<``threshold``else``None`![](images/00075.jpg)`url``=``get_rest_url``(``model_name``=``''complaints_classification''``,``version``=``version``)`'
  id: totrans-280
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`from``random``import``random`![](images/00002.jpg)`def``get_rest_url``(``model_name``,``host``=``''localhost''``,``port``=``8501``,``verb``=``''predict''``,``version``=``None``):``url``=``"http://{}:{}/v1/models/{}/"``.``format``(``host``,``port``,``model_name``)``if``version``:``url``+=``"versions/{}"``.``format``(``version``)``url``+=``":{}"``.``format``(``verb``)``return``url``...``#
    从此客户端的所有请求中提交 10% 给版本 1。``# 90% 的请求应该转到默认模型。``threshold``=``0.1``version``=``1``if``random``()``<``threshold``else``None`![](images/00075.jpg)`url``=``get_rest_url``(``model_name``=``''complaints_classification''``,``version``=``version``)`'
- en: '![](images/00002.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: The `random` library will help us pick a model.
  id: totrans-282
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`random` 库将帮助我们选择一个模型。'
- en: '![](images/00075.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00075.jpg)'
- en: If `version = None`, TensorFlow Serving will infer with the default version.
  id: totrans-284
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果 `version = None`，TensorFlow Serving 将使用默认版本进行推断。
- en: As you can see, randomly changing the request URL for our model inference (in
    our REST API example), can provide you some basic A/B testing functionality. If
    you would like to extend these capabilities by performing the random routing of
    the model inference on the server side, we highly recommend routing tools like
    [Istio](https://istio.io) for this purpose. Originally designed for web traffic,
    Istio can be used to route traffic to specific models. You can phase in models,
    perform A/B tests, or create policies for data routed to specific models.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，随机更改我们模型推理的请求 URL（在我们的 REST API 示例中），可以为您提供一些基本的 A/B 测试功能。如果您希望通过在服务器端执行模型推理的随机路由来扩展这些能力，我们强烈建议使用像
    [Istio](https://istio.io) 这样的路由工具。Istio 最初设计用于网络流量，可用于将流量路由到特定模型，逐步引入模型，执行 A/B
    测试或创建数据路由到特定模型的策略。
- en: When you perform A/B tests with your models, it is often useful to request information
    about the model from the model server. In the following section, we will explain
    how you can request the metadata information from TensorFlow Serving.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用模型进行 A/B 测试时，通常有必要从模型服务器请求模型的信息。在接下来的部分中，我们将解释如何从 TensorFlow Serving 请求元数据信息。
- en: Requesting Model Metadata from the Model Server
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 从模型服务器请求模型元数据
- en: At the beginning of the book, we laid out the model life cycle and explained
    how we want to automate the machine learning life cycle. A critical component
    of the continuous life cycle is generating accuracy or general performance feedback
    about your model versions. We will take a deep dive into how to generate these
    feedback loops in [Chapter 13](index_split_020.html#filepos1489635), but for now,
    imagine that your model classifies some data (e.g., the sentiment of the text),
    and then asks the user to rate the prediction. The information of whether a model
    predicted something correctly or incorrectly is precious for improving future
    model versions, but it is only useful if we know which model version has performed
    the prediction.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的开头，我们阐述了模型生命周期，并解释了我们希望自动化机器学习生命周期的方式。连续生命周期的关键组成部分是生成关于模型版本的准确性或一般性能反馈。我们将深入研究如何生成这些反馈循环，详见[第
    13 章](index_split_020.html#filepos1489635)，但现在，想象一下，您的模型对某些数据进行分类（例如文本的情感），然后请用户对预测结果进行评分。模型是否正确预测了某些事物的信息对于改进未来的模型版本至关重要，但仅当我们知道哪个模型版本执行了预测时才有用。
- en: The metadata provided by the model server will contain the information to annotate
    your feedback loops.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务器提供的元数据将包含用于注释反馈循环的信息。
- en: REST Requests for Model Metadata
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 模型元数据的 REST 请求
- en: 'Requesting model metadata is straightforward with TensorFlow Serving. TensorFlow
    Serving provides you an endpoint for model metadata:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TensorFlow Serving 请求模型元数据非常直接。TensorFlow Serving 为模型元数据提供了一个端点：
- en: '`http://``{``HOST``}``:``{``PORT``}``/v1/models/``{``MODEL_NAME``}[``/versions/``{``MODEL_VERSION``}]``/metadata`'
  id: totrans-292
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`http://``{``HOST``}``:``{``PORT``}``/v1/models/``{``MODEL_NAME``}[``/versions/``{``MODEL_VERSION``}]``/metadata`'
- en: Similar to the REST API inference requests we discussed earlier, you have the
    option to specify the model version in the request URL, or if you don’t specify
    it, the model server will provide the information about the default model.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 类似我们之前讨论的 REST API 推断请求，您可以选择在请求 URL 中指定模型版本，或者如果不指定，模型服务器将提供关于默认模型的信息。
- en: As [Example 8-7](#filepos924489) shows, we can request model metadata with a
    single `GET` request.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 如[示例 8-7](#filepos924489) 所示，我们可以通过单个 `GET` 请求请求模型元数据。
- en: Example 8-7\. Example model metadata request with a python client
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 8-7\. 使用 Python 客户端请求模型元数据的示例
- en: '`import``requests``def``metadata_rest_request``(``model_name``,``host``=``"localhost"``,``port``=``8501``,``version``=``None``):``url``=``"http://{}:{}/v1/models/{}/"``.``format``(``host``,``port``,``model_name``)``if``version``:``url``+=``"versions/{}"``.``format``(``version``)``url``+=``"/metadata"`![](images/00002.jpg)`response``=``requests``.``get``(``url``=``url``)`![](images/00075.jpg)`return``response`'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '`import``requests``def``metadata_rest_request``(``model_name``,``host``=``"localhost"``,``port``=``8501``,``version``=``None``):``url``=``"http://{}:{}/v1/models/{}/"``.``format``(``host``,``port``,``model_name``)``if``version``:``url``+=``"versions/{}"``.``format``(``version``)``url``+=``"/metadata"`![](images/00002.jpg)`response``=``requests``.``get``(``url``=``url``)`![](images/00075.jpg)`return``response`'
- en: '![](images/00002.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: Append `/metadata` for model information.
  id: totrans-298
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在请求 URL 中追加 `/metadata` 获取模型信息。
- en: '![](images/00075.jpg)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00075.jpg)'
- en: Perform a `GET` request.
  id: totrans-300
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 执行 `GET` 请求。
- en: 'The model server will return the model specifications as a `model_spec` dictionary
    and the model definitions as a `metadata` dictionary:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务器将以 `model_spec` 字典和 `metadata` 字典的形式返回模型规格和模型定义：
- en: '`{``"model_spec"``:``{``"name"``:``"complaints_classification"``,``"signature_name"``:``""``,``"version"``:``"1556583584"``},``"metadata"``:``{``"signature_def"``:``{``"signature_def"``:``{``"classification"``:``{``"inputs"``:``{``"inputs"``:``{``"dtype"``:``"DT_STRING"``,``"tensor_shape"``:``{``...`'
  id: totrans-302
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`{``"model_spec"``:``{``"name"``:``"complaints_classification"``,``"signature_name"``:``""``,``"version"``:``"1556583584"``},``"metadata"``:``{``"signature_def"``:``{``"signature_def"``:``{``"classification"``:``{``"inputs"``:``{``"inputs"``:``{``"dtype"``:``"DT_STRING"``,``"tensor_shape"``:``{``...`'
- en: gRPC Requests for Model Metadata
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: gRPC 请求模型元数据
- en: 'Requesting model metadata with gRPC is almost as easy as the REST API case.
    In the gRPC case, you file a `GetModelMetadataRequest`, add the model name to
    the specifications, and submit the request via the `GetModelMetadata` method of
    the `stub`:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 gRPC 请求模型元数据与 REST API 情况几乎一样简单。在 gRPC 情况下，您需要提交一个 `GetModelMetadataRequest`，将模型名称添加到规范中，并通过
    `stub` 的 `GetModelMetadata` 方法提交请求。
- en: '`from``tensorflow_serving.apis``import``get_model_metadata_pb2``def``get_model_version``(``model_name``,``stub``):``request``=``get_model_metadata_pb2``.``GetModelMetadataRequest``()``request``.``model_spec``.``name``=``model_name``request``.``metadata_field``.``append``(``"signature_def"``)``response``=``stub``.``GetModelMetadata``(``request``,``5``)``return``response``.``model_spec``model_name``=``''complaints_classification''``stub``=``create_grpc_stub``(``''localhost''``)``get_model_version``(``model_name``,``stub``)``name``:``"complaints_classification"``version``{``value``:``1556583584``}`'
  id: totrans-305
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`from``tensorflow_serving.apis``import``get_model_metadata_pb2``def``get_model_version``(``model_name``,``stub``):``request``=``get_model_metadata_pb2``.``GetModelMetadataRequest``()``request``.``model_spec``.``name``=``model_name``request``.``metadata_field``.``append``(``"signature_def"``)``response``=``stub``.``GetModelMetadata``(``request``,``5``)``return``response``.``model_spec``model_name``=``''complaints_classification''``stub``=``create_grpc_stub``(``''localhost''``)``get_model_version``(``model_name``,``stub``)``name``:``"complaints_classification"``version``{``value``:``1556583584``}`'
- en: The gRPC response contains a `ModelSpec` object that contains the version number
    of the loaded model.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: gRPC 响应包含一个`ModelSpec`对象，其中包含加载模型的版本号。
- en: 'More interesting is the use case of obtaining the model signature information
    of the loaded models. With almost the same request functions, we can determine
    the model’s metadata. The only difference is that we don’t access the `model_spec`
    attribute of the response object, but the `metadata`. The information needs to
    be serialized to be human readable; therefore, we will use `SerializeToString`
    to convert the protocol buffer information:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '-   更有趣的是获取已加载模型的模型签名信息的用例。几乎使用相同的请求函数，我们可以确定模型的元数据。唯一的区别在于，我们不访问响应对象的`model_spec`属性，而是访问`metadata`。为了便于阅读，需要对信息进行序列化，因此我们将使用`SerializeToString`方法将协议缓冲区信息转换为人类可读的格式。'
- en: '`from``tensorflow_serving.apis``import``get_model_metadata_pb2``def``get_model_meta``(``model_name``,``stub``):``request``=``get_model_metadata_pb2``.``GetModelMetadataRequest``()``request``.``model_spec``.``name``=``model_name``request``.``metadata_field``.``append``(``"signature_def"``)``response``=``stub``.``GetModelMetadata``(``request``,``5``)``return``response``.``metadata``[``''signature_def''``]``model_name``=``''complaints_classification''``stub``=``create_grpc_stub``(``''localhost''``)``meta``=``get_model_meta``(``model_name``,``stub``)``print``(``meta``.``SerializeToString``()``.``decode``(``"utf-8"``,``''ignore''``))``#
    type.googleapis.com/tensorflow.serving.SignatureDefMap``# serving_default``# complaints_classification_input``#        
    input_1:0``#                2@``# complaints_classification_output(``# dense_1/Sigmoid:0``#               
    tensorflow/serving/predict`'
  id: totrans-308
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`from``tensorflow_serving.apis``import``get_model_metadata_pb2``def``get_model_meta``(``model_name``,``stub``):``request``=``get_model_metadata_pb2``.``GetModelMetadataRequest``()``request``.``model_spec``.``name``=``model_name``request``.``metadata_field``.``append``(``"signature_def"``)``response``=``stub``.``GetModelMetadata``(``request``,``5``)``return``response``.``metadata``[``''signature_def''``]``model_name``=``''complaints_classification''``stub``=``create_grpc_stub``(``''localhost''``)``meta``=``get_model_meta``(``model_name``,``stub``)``print``(``meta``.``SerializeToString``()``.``decode``(``"utf-8"``,``''ignore''``))``#
    type.googleapis.com/tensorflow.serving.SignatureDefMap``# serving_default``# complaints_classification_input``#        
    input_1:0``#                2@``# complaints_classification_output(``# dense_1/Sigmoid:0``#               
    tensorflow/serving/predict`'
- en: gRPC requests are more complex than REST requests; however, in applications
    with high performance requirements, they can provide faster prediction performances.
    Another way of increasing our model prediction performance is by batching our
    prediction requests.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: gRPC 请求比 REST 请求更复杂；然而，在对性能要求较高的应用中，它们可以提供更快的预测性能。通过批量处理预测请求，我们还可以增加模型的预测性能。
- en: Batching Inference Requests
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 批量推断请求
- en: Batching inference requests is one of the most powerful features of TensorFlow
    Serving. During model training, batching accelerates our training because we can
    parallelize the computation of our training samples. At the same time, we can
    also use the computation hardware efficiently if we match the memory requirements
    of our batches with the available memory of the GPU.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '-   TensorFlow Serving 中批量推断请求的使用案例是最强大的功能之一。在模型训练过程中，批处理加速了我们的训练，因为我们可以并行计算我们的训练样本。同时，如果我们将批次的内存需求与
    GPU 的可用内存匹配，还可以高效地使用计算硬件。'
- en: If you run TensorFlow Serving without the batching enabled, as shown in [Figure 8-3](#filepos950948),
    every client request is handled individually and in sequence. If you classify
    an image, for example, your first request will infer the model on your CPU or
    GPU before the second request, third request, and so on, will be classified. In
    this case, we under-utilize the available memory of the CPU or GPU.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在未启用批处理的情况下运行 TensorFlow Serving，如图 [8-3](#filepos950948) 所示，则每个客户端请求都会单独并按顺序处理。例如，如果您对图像进行分类，您的第一个请求将在您的CPU或GPU上推断模型，然后才是第二个请求、第三个请求等。在这种情况下，我们未充分利用CPU或GPU的可用内存。
- en: As shown in [Figure 8-4](#filepos951231), multiple clients can request model
    predictions, and the model server batches the different client requests into one
    “batch” to compute. Each request inferred through this batching step might take
    a bit longer than a single request because of the timeout or the limit of the
    batch. However, similar to our training phase, we can compute the batch in parallel
    and return the results to all clients after the completion of the batch computation.
    This will utilize the hardware more efficiently than single sample requests.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [8-4](#filepos951231) 所示，多个客户端可以请求模型预测，模型服务器将不同的客户端请求批处理为一个“批次”进行计算。通过这个批处理步骤推断每个请求可能需要比单个请求更长的时间，这是由于超时或批次限制。然而，与我们的训练阶段类似，我们可以并行计算批处理并在批处理计算完成后将结果返回给所有客户端。这比单个样本请求更有效地利用硬件。
- en: '![](images/00047.jpg)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00047.jpg)'
- en: Figure 8-3\. Overview of TensorFlow Serving without batching
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-3\. TensorFlow Serving 概述（未进行批处理）
- en: '![](images/00059.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00059.jpg)'
- en: Figure 8-4\. Overview of TensorFlow Serving with batching
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-4\. TensorFlow Serving 概述（带有批处理）
- en: Configuring Batch Predictions
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 配置批处理预测
- en: 'Batching predictions needs to be enabled for TensorFlow Serving and then configured
    for your use case. You have five configuration options:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 需要启用 TensorFlow Serving 的批处理预测，并为您的用例进行配置。您有五个配置选项：
- en: '`max_batch_size`'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_batch_size`'
- en: This parameter controls the batch size. Large batch sizes will increase the
    request latency and can lead to exhausting the GPU memory. Small batch sizes lose
    the benefit of using optimal computation resources.
  id: totrans-321
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 此参数控制批次大小。大批量大小将增加请求的延迟，并可能导致耗尽GPU内存。小批量大小则会失去使用最佳计算资源的好处。
- en: '`batch_timeout_micros`'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '`batch_timeout_micros`'
- en: This parameter sets the maximum wait time for filling a batch. This parameter
    is handy to cap the latency for inference requests.
  id: totrans-323
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 此参数设置填充批次的最大等待时间。此参数对于限制推理请求的延迟非常有用。
- en: '`num_batch_threads`'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '`num_batch_threads`'
- en: The number of threads configures how many CPU or GPU cores can be used in parallel.
  id: totrans-325
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 线程数量配置了可以并行使用多少个CPU或GPU核心。
- en: '`max_enqueued_batches`'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_enqueued_batches`'
- en: This parameter sets the maximum number of batches queued for predictions. This
    configuration is beneficial to avoid an unreasonable backlog of requests. If the
    maximum number is reached, requests will be returned with an error instead of
    being queued.
  id: totrans-327
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 此参数设置了用于预测排队的最大批次数。这种配置有助于避免请求的不合理积压。如果达到最大数量，请求将返回错误而不是排队。
- en: '`pad_variable_length_inputs`'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '`pad_variable_length_inputs`'
- en: This Boolean parameter determines if input tensors with variable lengths will
    be padded to the same lengths for all input tensors.
  id: totrans-329
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 此布尔参数确定是否对具有可变长度的输入张量进行填充，使所有输入张量的长度相同。
- en: As you can imagine, setting parameters for optimal batching requires some tuning
    and is application dependent. If you run online inferences, you should aim for
    limiting the latency. It is often recommended to set `batch_timeout_micros` initially
    to 0 and tune the timeout toward 10,000 microseconds. In contrast, batch requests
    will benefit from longer timeouts (milliseconds to a second) to constantly use
    the batch size for optimal performance. TensorFlow Serving will make predictions
    on the batch when either the `max_batch_size` or the timeout is reached.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 可想而知，为了实现最佳批处理设置参数需要进行一些调整，并且这取决于应用。如果您进行在线推理，应该旨在限制延迟。通常建议首先将`batch_timeout_micros`设置为
    0，然后将超时调整为 10,000 微秒。相反，批处理请求将受益于较长的超时时间（毫秒到秒），以始终使用最佳性能的批处理大小。当达到`max_batch_size`或超时时，TensorFlow
    Serving 将对批次进行预测。
- en: Set `num_batch_threads` to the number of CPU cores if you configure TensorFlow
    Serving for CPU-based predictions. If you configure a GPU setup, tune `max_batch_size`
    to get an optimal utilization of the GPU memory. While you tune your configuration,
    make sure that you set `max_enqueued_batches` to a huge number to avoid some requests
    being returned early without proper inference.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您为 CPU 预测配置 TensorFlow Serving，请将 `num_batch_threads` 设置为 CPU 核心数。如果您配置了 GPU
    设置，请调整 `max_batch_size` 以实现 GPU 内存的最佳利用率。在调整配置时，请确保将 `max_enqueued_batches` 设置为一个大数，以避免某些请求提前返回而没有进行适当推理。
- en: 'You can set the parameters in a text file, as shown in the following example.
    In our example, we create a configuration file called batching_parameters.txt
    and add the following content:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将参数设置在文本文件中，如下例所示。在我们的示例中，我们创建了一个名为 `batching_parameters.txt` 的配置文件，并添加了以下内容：
- en: '`max_batch_size` `{` `value:` `32``}` `batch_timeout_micros` `{` `value:` `5000``}`
    `pad_variable_length_inputs:` `true`'
  id: totrans-333
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`max_batch_size` `{` `value:` `32``}` `batch_timeout_micros` `{` `value:` `5000``}`
    `pad_variable_length_inputs:` `true`'
- en: If you want to enable batching, you need to pass two additional parameters to
    the Docker container running TensorFlow Serving. To enable batching, set `enable_batching`
    to true and set `batching_parameters_file` to the absolute path of the batching
    configuration file inside of the container. Please keep in mind that you have
    to mount the additional folder with the configuration file if it isn’t located
    in the same folder as the model versions.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想启用批处理，需要将两个额外的参数传递给运行 TensorFlow Serving 的 Docker 容器。要启用批处理，请将 `enable_batching`
    设置为 true，并将 `batching_parameters_file` 设置为容器内批处理配置文件的绝对路径。请记住，如果批处理配置文件不位于与模型版本相同的文件夹中，则必须挂载额外的文件夹。
- en: 'Here is a complete example of the `docker run` command that starts the TensorFlow
    Serving Docker container with batching enabled. The parameters will then be passed
    to the TensorFlow Serving instance:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用启用批处理的 `docker run` 命令的完整示例，它启动了带有批处理功能的 TensorFlow Serving Docker 容器。然后，参数将传递给
    TensorFlow Serving 实例：
- en: '`docker run -p 8500:8500` `\` `-p 8501:8501` `\` `--mount` `type``=``bind``,source``=``/path/to/models,target``=``/models/my_model`
    `\` `--mount` `type``=``bind``,source``=``/path/to/batch_config,target``=``/server_config`
    `\` `-e` `MODEL_NAME``=``my_model -t tensorflow/serving` `\` `--enable_batching``=``true`
    `--batching_parameters_file``=``/server_config/batching_parameters.txt`'
  id: totrans-336
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker run -p 8500:8500` `\` `-p 8501:8501` `\` `--mount` `type``=``bind``,source``=``/path/to/models,target``=``/models/my_model`
    `\` `--mount` `type``=``bind``,source``=``/path/to/batch_config,target``=``/server_config`
    `\` `-e` `MODEL_NAME``=``my_model -t tensorflow/serving` `\` `--enable_batching``=``true`
    `--batching_parameters_file``=``/server_config/batching_parameters.txt`'
- en: As explained earlier, the configuration of the batching will require additional
    tuning, but the performance gains should make up for the initial setup. We highly
    recommend enabling this TensorFlow Serving feature. It is especially useful for
    inferring a large number of data samples with offline batch processes.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前文所述，批处理的配置需要额外的调整，但性能增益应该弥补初始设置的成本。我们强烈建议启用此 TensorFlow Serving 功能。这对于使用离线批处理过程推断大量数据样本尤为有用。
- en: Other TensorFlow Serving Optimizations
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 其他 TensorFlow Serving 优化
- en: 'TensorFlow Serving comes with a variety of additional optimization features.
    Additional feature flags are:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving 提供了多种额外的优化功能。其他特性标志包括：
- en: '`--file_system_poll_wait_seconds=1`'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '`--file_system_poll_wait_seconds=1`'
- en: TensorFlow Serving will poll if a new model version is available. You can disable
    the feature by setting it to `1`. If you only want to load the model once and
    never update it, you can set it to `0`. The parameter expects an integer value.
    If you load models from cloud storage buckets, we highly recommend that you increase
    the polling time to avoid unnecessary cloud provider charges for the frequent
    list operations on the cloud storage bucket.
  id: totrans-341
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: TensorFlow Serving 会定期检查是否有新的模型版本可用。您可以通过将其设置为`1`来禁用此功能。如果您只想加载模型一次而不更新它，可以将其设置为`0`。该参数期望一个整数值。如果您从云存储桶加载模型，我们强烈建议您增加轮询时间，以避免因频繁列出云存储桶上的操作而产生不必要的云服务提供商费用。
- en: '`--tensorflow_session_parallelism=0`'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '`--tensorflow_session_parallelism=0`'
- en: TensorFlow Serving will automatically determine how many threads to use for
    a TensorFlow session. In case you want to set the number of a thread manually,
    you can overwrite it by setting this parameter to any positive integer value.
  id: totrans-343
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: TensorFlow Serving 将自动确定在 TensorFlow 会话中使用多少线程。如果您希望手动设置线程数，可以通过将此参数设置为任意正整数值来覆盖它。
- en: '`--tensorflow_intra_op_parallelism=0`'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '`--tensorflow_intra_op_parallelism=0`'
- en: This parameter sets the number of cores being used for running TensorFlow Serving.
    The number of available threads determines how many operations will be parallelized.
    If the value is `0`, all available cores will be used.
  id: totrans-345
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 此参数设置了用于运行 TensorFlow Serving 的核心数。可用线程数确定将并行化多少操作。如果值为`0`，将使用所有可用核心。
- en: '`--tensorflow_inter_op_parallelism=0`'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '`--tensorflow_inter_op_parallelism=0`'
- en: This parameter sets the number of available threads in a pool to execute TensorFlow
    ops. This is useful for maximizing the execution of independent operations in
    a TensorFlow graph. If the value is set to `0`, all available cores will be used
    and one thread per core will be allocated.
  id: totrans-347
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 此参数设置了在池中执行 TensorFlow 操作的可用线程数。这对于最大化 TensorFlow 图中独立操作的执行非常有用。如果值设置为`0`，将使用所有可用核心，并为每个核心分配一个线程。
- en: 'Similar to our earlier examples, you can pass the configuration parameter to
    the `docker run` command, as shown in the following example:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的示例类似，您可以将配置参数传递给`docker run`命令，如下例所示：
- en: '`docker run -p 8500:8500` `\` `-p 8501:8501` `\` `--mount` `type``=``bind``,source``=``/path/to/models,target``=``/models/my_model`
    `\` `-e` `MODEL_NAME``=``my_model -t tensorflow/serving` `\` `--tensorflow_intra_op_parallelism``=``4``\`
    `--tensorflow_inter_op_parallelism``=``4``\` `--file_system_poll_wait_seconds``=``10``\`
    `--tensorflow_session_parallelism``=``2`'
  id: totrans-349
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`docker run -p 8500:8500` `\` `-p 8501:8501` `\` `--mount` `type``=``bind``,source``=``/path/to/models,target``=``/models/my_model`
    `\` `-e` `MODEL_NAME``=``my_model -t tensorflow/serving` `\` `--tensorflow_intra_op_parallelism``=``4``\`
    `--tensorflow_inter_op_parallelism``=``4``\` `--file_system_poll_wait_seconds``=``10``\`
    `--tensorflow_session_parallelism``=``2`'
- en: The discussed configuration options can improve performance and avoid unnecessary
    cloud provider charges.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论的配置选项可以提高性能并避免不必要的云提供商费用。
- en: TensorFlow Serving Alternatives
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving 替代方案
- en: TensorFlow Serving is a great way of deploying machine learning models. With
    the TensorFlow `Estimator`s and Keras models, you should be covered for a large
    variety of machine learning concepts. However, if you would like to deploy a legacy
    model or if your machine learning framework of choice isn’t TensorFlow or Keras,
    here are a couple of options.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving 是部署机器学习模型的一个很好的方式。使用 TensorFlow 的`Estimator`和 Keras 模型，您应该可以涵盖大量的机器学习概念。但是，如果您想要部署传统模型或者您选择的机器学习框架不是
    TensorFlow 或 Keras，这里有一些选择。
- en: BentoML
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: BentoML
- en: '[BentoML](https://bentoml.org) is a framework-independent library that deploys
    machine learning models. It supports models trained through PyTorch, scikit-learn,
    TensorFlow, Keras, and XGBoost. For TensorFlow models, BentoML supports the `SavedModel`
    format. BentoML supports batching requests.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '[BentoML](https://bentoml.org) 是一个与框架无关的库，用于部署机器学习模型。它支持通过 PyTorch、scikit-learn、TensorFlow、Keras
    和 XGBoost 训练的模型。对于 TensorFlow 模型，BentoML 支持`SavedModel`格式。BentoML 支持批处理请求。'
- en: Seldon
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: Seldon
- en: The UK startup Seldon provides a variety of open source tools to manage model
    life cycles, and one of their core products is [Seldon Core](https://oreil.ly/Yx_U7).
    Seldon Core provides you a toolbox to wrap your models in a Docker image, which
    is then deployed via Seldon in a Kubernetes cluster.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 英国初创公司 Seldon 提供了多种开源工具来管理模型生命周期，其中核心产品之一是[Seldon Core](https://oreil.ly/Yx_U7)。Seldon
    Core 为您提供一个工具箱，用于将您的模型包装为 Docker 镜像，然后通过 Seldon 在 Kubernetes 集群中部署。
- en: At the time of writing this chapter, Seldon supported machine learning models
    trained with TensorFlow, scikit-learn, XGBoost, and even R.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本章时，Seldon 支持用 TensorFlow、scikit-learn、XGBoost 甚至 R 训练的机器学习模型。
- en: Seldon comes with its own ecosystem that allows building preprocessing into
    its own Docker images, which are deployed in conjunction with the deployment images.
    It also provides a routing service that allows you to perform A/B test or multiarm
    bandit experiments.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: Seldon 自带其生态系统，允许将预处理构建到其自己的 Docker 镜像中，这些镜像与部署镜像一起部署。它还提供了一个路由服务，允许您执行 A/B
    测试或多臂老虎机实验。
- en: Seldon is highly integrated with the Kubeflow environment and, similar to TensorFlow
    Serving, is a way to deploy models with Kubeflow on Kubernetes.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: Seldon 与 Kubeflow 环境高度集成，并且与 TensorFlow Serving 类似，是在 Kubernetes 上部署模型的一种方式。
- en: GraphPipe
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: GraphPipe
- en: '[GraphPipe](https://oreil.ly/w_U7U) is another way of deploying TensorFlow
    and non-TensorFlow models. Oracle drives the open source project. It allows you
    to deploy not just TensorFlow (including Keras) models, but also Caffe2 models
    and all machine learning models that can be converted to the Open Neural Network
    Exchange (ONNX) format.[8](index_split_015.html#filepos996089) Through the ONNX
    format, you can deploy PyTorch models with GraphPipe.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '[GraphPipe](https://oreil.ly/w_U7U) 是另一种部署 TensorFlow 和非 TensorFlow 模型的方法。Oracle
    推动这个开源项目。它允许您部署不仅仅是 TensorFlow（包括 Keras）模型，还可以是 Caffe2 模型和所有可以转换为开放神经网络交换格式（ONNX）的机器学习模型。[8](index_split_015.html#filepos996089)
    通过 ONNX 格式，您可以使用 GraphPipe 部署 PyTorch 模型。'
- en: Besides providing a model server for TensorFlow, PyTorch, etc., GraphPipe also
    provides client implementation for programming languages like Python, Java, and
    Go.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 除了为 TensorFlow、PyTorch 等提供模型服务器外，GraphPipe 还提供了 Python、Java 和 Go 等编程语言的客户端实现。
- en: Simple TensorFlow Serving
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的 TensorFlow 服务
- en: '[Simple TensorFlow Serving](https://stfs.readthedocs.io) is a development by
    Dihao Chen from 4Paradigm. The library supports more than just TensorFlow models.
    The current list of supported model frameworks includes ONNX, scikit-learn, XGBoost,
    PMML, and H2O. It supports multiple models, predictions on GPUs, and client code
    for a variety of languages.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '[简单 TensorFlow 服务](https://stfs.readthedocs.io) 是 4Paradigm 的陈迪豪开发的。这个库不仅仅支持
    TensorFlow 模型。当前支持的模型框架列表包括 ONNX、scikit-learn、XGBoost、PMML 和 H2O。它支持多个模型，在 GPU
    上进行预测，并提供各种语言的客户端代码。'
- en: One significant aspect of Simple TensorFlow Serving is that it supports authentication
    and encrypted connections to the model server. Authentication is currently not
    a feature of TensorFlow Serving, and supporting SSL or Transport Layer Security
    (TLS) requires a custom build of TensorFlow Serving.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 简单 TensorFlow 服务的一个重要方面是它支持对模型服务器的身份验证和加密连接。身份验证目前不是 TensorFlow 服务的功能，支持 SSL
    或传输层安全性（TLS）需要自定义构建 TensorFlow 服务。
- en: MLflow
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: MLflow
- en: '[MLflow](https://mlflow.org) supports the deployment of machine learning models,
    but that it is only one aspect of the tool created by DataBricks. MLflow is designed
    to manage model experiments through MLflow Tracking. The tool has a built-in model
    server, which provides REST API endpoints for the models managed through MLflow.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '[MLflow](https://mlflow.org) 支持部署机器学习模型，但这只是由 DataBricks 创建的工具的一个方面。MLflow
    旨在通过 MLflow Tracking 管理模型实验。该工具具有内置的模型服务器，为通过 MLflow 管理的模型提供 REST API 端点。'
- en: MLflow also provides interfaces to directly deploy models from MLflow to Microsoft’s
    Azure ML platform and AWS SageMaker.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: MLflow 还提供界面，可以直接将模型从 MLflow 部署到 Microsoft 的 Azure ML 平台和 AWS SageMaker。
- en: Ray Serve
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: Ray Serve
- en: The [Ray Project](https://ray.io) provides functionality to deploy machine learning
    models. Ray Serve is framework agnostic and it supports PyTorch, TensorFlow (incl.
    Keras), Scikit-Learn models, or custom model predictions. The library provides
    capabilities to batch requests and it allows the routing of traffic between models
    and their versions.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ray 项目](https://ray.io) 提供了部署机器学习模型的功能。Ray Serve 是框架无关的，支持 PyTorch、TensorFlow（包括
    Keras）、Scikit-Learn 模型或自定义模型预测。该库具备批处理请求的能力，并允许在模型及其版本之间进行流量路由。'
- en: Ray Serve is integrated in the Ray Project ecosystem and supports distributed
    computation setups.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: Ray Serve 已集成到 Ray 项目生态系统中，并支持分布式计算设置。
- en: Deploying with Cloud Providers
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 使用云服务提供商部署
- en: All model server solutions we have discussed up to this point have to be installed
    and managed by you. However, all primary cloud providers—Google Cloud, AWS, and
    Microsoft Azure—offer machine learning products, including the hosting of machine
    learning models.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论的所有模型服务器解决方案都必须由您安装和管理。但是，所有主要的云提供商——Google Cloud、AWS 和 Microsoft
    Azure——都提供包括托管机器学习模型在内的机器学习产品。
- en: In this section, we will guide you through an example deployment using Google
    Cloud’s AI Platform. Let’s start with the model deployment, and later we’ll explain
    how you can get predictions from the deployed model from your application client.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将指导您通过 Google Cloud 的 AI 平台进行示例部署。让我们从模型部署开始，稍后我们将解释如何从您的应用程序客户端获取部署模型的预测。
- en: Use Cases
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 使用案例
- en: Managed cloud deployments of machine learning models are a good alternative
    to running your model server instances if you want to deploy a model seamlessly
    and don’t want to worry about scaling the model deployment. All cloud providers
    offer deployment options with the ability to scale by the number of inference
    requests.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望无缝部署模型且不必担心扩展模型部署，则托管云部署是运行模型服务器实例的良好选择。所有云服务提供商都提供了通过推理请求数量扩展的部署选项。
- en: However, the flexibility of your model deployment comes at a cost. Managed services
    provide effortless deployments, but they cost a premium. For example, two model
    versions running full time (requiring two computation nodes) are more expensive
    than a comparable compute instance that is running a TensorFlow Serving instance.
    Another downside of managed deployments is the limitations of the products. Some
    cloud providers require that you deploy via their own software development kits,
    and others have limits on the node size and how much memory your model can take
    up. These limitations can be a severe restriction for sizeable machine learning
    models, especially if the models contain very many layers (i.e., language models).
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，模型部署的灵活性也伴随着成本。托管服务提供了无忧的部署方式，但其费用较高。例如，两个全天候运行的模型版本（需要两个计算节点）的费用比运行 TensorFlow
    Serving 实例的可比计算实例更高。托管部署的另一个缺点是产品的限制。一些云服务提供商要求您通过它们自己的软件开发工具包进行部署，其他提供商则对节点大小和模型占用内存的限制。对于庞大的机器学习模型来说，特别是包含很多层的模型（即语言模型），这些限制可能是严重的限制。
- en: Example Deployment with GCP
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 GCP 的部署示例
- en: In this section, we will walk you through one deployment with Google Cloud’s
    AI Platform. Instead of writing configuration files and executing terminal commands,
    we can set up model endpoints through a web UI.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将为您介绍如何在 Google Cloud 的 AI 平台上进行部署。与编写配置文件和执行终端命令不同，我们可以通过 Web UI 设置模型端点。
- en: LIMITS OF MODEL SIZE ON GCP’S AI PLATFORM
  id: totrans-380
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: GCP AI 平台的模型大小限制
- en: GCP’s endpoints are limited to model sizes up to 500 MB. However, if you deploy
    your endpoints via compute engines of type N1, the maximum model limit is increased
    to 2 GB. At the time of writing, this option was available as a beta feature.
  id: totrans-381
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: GCP 的端点限制模型大小最高为 500 MB。但是，如果您通过 N1 类型的计算引擎部署端点，则最大模型限制增加到 2 GB。在撰写本文时，此选项作为测试版功能可用。
- en: Model deployment
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 模型部署
- en: 'The deployment consists of three steps:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 部署包括三个步骤：
- en: Make the model accessible on Google Cloud.
  id: totrans-384
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使模型在 Google Cloud 上可访问。
- en: Create a new model instance with Google Cloud’s AI Platform.
  id: totrans-385
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 创建一个新的 Google Cloud AI 平台模型实例。
- en: Create a new version with the model instance.
  id: totrans-386
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 创建一个包含模型实例的新版本。
- en: The deployment starts with uploading your exported TensorFlow or Keras model
    to a storage bucket. As shown in [Figure 8-5](#filepos973443), you will need to
    upload the entire exported model. Once the upload of the model is done, please
    copy the complete path of the storage location.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 部署始于将您的导出 TensorFlow 或 Keras 模型上传到存储桶。如图 [8-5](#filepos973443) 所示，您需要上传整个导出模型。一旦模型上传完成，请复制存储位置的完整路径。
- en: '![](images/00069.jpg)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00069.jpg)'
- en: Figure 8-5\. Uploading the trained model to cloud storage
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-5\. 将训练好的模型上传到云存储
- en: Once you have uploaded your machine learning model, head over to the AI Platform
    of GCP to set up your machine learning model for deployment. If it is the first
    time that you are using the AI Platform in your GCP project, you’ll need to enable
    the API. The automated startup process by Google Cloud can take a few minutes.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您上传了您的机器学习模型，请转至 GCP 的 AI 平台，设置您的机器学习模型以进行部署。如果这是您在 GCP 项目中首次使用 AI 平台，则需要启用
    API。Google Cloud 的自动启动过程可能需要几分钟。
- en: As shown in [Figure 8-6](#filepos974518), you need to give the model a unique
    identifier. Once you have created the identifier, selected your preferred deployment
    region,[9](#filepos996412) and created an optional project description, continue
    with the setup by clicking Create.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [8-6](#filepos974518) 所示，您需要为模型提供一个唯一标识符。一旦创建了标识符，选择您首选的部署区域，[9](#filepos996412)
    并创建一个可选的项目描述，继续通过点击“创建”来完成设置。
- en: '![](images/00080.jpg)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00080.jpg)'
- en: Figure 8-6\. Creating a new model instance
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-6\. 创建一个新的模型实例
- en: Once the new model is registered, the model will be listed in the dashboard,
    as shown in [Figure 8-7](#filepos975091). You can create a new model version for
    the dashboard by clicking “Create version” in the overflow menu.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦注册新模型，该模型将显示在仪表板中，如 [图 8-7](#filepos975091) 所示。您可以通过在溢出菜单中点击 “创建版本” 来为仪表板创建新模型版本。
- en: '![](images/00090.jpg)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00090.jpg)'
- en: Figure 8-7\. Creating a new model version
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-7\. 创建新模型版本
- en: When you create a new model version, you configure a compute instance that runs
    your model. Google Cloud gives you a variety of configuration options, as shown
    in [Figure 8-8](#filepos976016). The `version name` is important since you’ll
    reference the `version name` later in the client setup. Please set the `Model
    URI` to the storage path you saved in the earlier step.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 创建新模型版本时，您需配置一个运行模型的计算实例。Google Cloud 提供多种配置选项，如 [图 8-8](#filepos976016) 所示。`版本名称`
    非常重要，因为稍后您将在客户端设置中引用 `版本名称`。请将 `模型 URI` 设置为您在先前步骤中保存的存储路径。
- en: Google Cloud AI Platform supports a variety of machine learning frameworks including
    XGBoost, scikit-learn, and custom prediction routines.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud AI 平台支持多种机器学习框架，包括 XGBoost、scikit-learn 和自定义预测例程。
- en: '![](images/00100.jpg)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00100.jpg)'
- en: Figure 8-8\. Setting up the instance details
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-8\. 设置实例详细信息
- en: 'GCP also lets you configure how your model instance scales if your model experiences
    a large number of inference requests. You can select between two scaling behaviors:
    manual scaling or autoscaling.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: GCP 还允许您配置模型实例在模型经历大量推理请求时的扩展方式。您可以选择手动扩展或自动扩展之间的两种扩展行为。
- en: Manual scaling gives you the option for setting the exact number of nodes available
    for the predictions of your model version. In contrast, autoscaling gives you
    the functionality to adjust the number of instances depending on the demand for
    your endpoint. If your nodes don’t experience any requests, the number of nodes
    could even drop to zero. Please note that if autoscaling drops the number of nodes
    to zero, it will take some time to reinstantiate your model version with the next
    request hitting the model version endpoint. Also, if you run inference nodes in
    the autoscaling mode, you’ll be billed in 10 min intervals.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 手动扩展为您提供了设置用于模型版本预测的节点数量的选项。相比之下，自动扩展功能使您能够根据端点需求调整实例数量。如果您的节点没有任何请求，节点数量甚至可能降至零。请注意，如果自动扩展将节点数量降至零，则需要一些时间来重新启动您的模型版本，以处理下一个请求命中模型版本端点。此外，如果在自动扩展模式下运行推理节点，则会按照
    10 分钟的间隔计费。
- en: Once the entire model version is configured, Google Cloud spins up the instances
    for you. If everything is ready for model predictions, you will see a green check
    icon next to the version name, as shown in [Figure 8-9](#filepos977533).
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦整个模型版本配置完成，Google Cloud 将为您启动实例。如果一切就绪以进行模型预测，则会在版本名称旁边看到绿色的检查图标，如 [图 8-9](#filepos977533)
    所示。
- en: '![](images/00112.jpg)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00112.jpg)'
- en: Figure 8-9\. Completing the deployment with a new version available
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-9\. 完成部署，新版本可用
- en: You can run multiple model versions simultaneously. In the control panel of
    the model version, you can set one version as the default version, and any inference
    request without a version specified will be routed to the designated “default
    version.” Just note that each model version will be hosted on an individual node
    and will accumulate GCP costs.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以同时运行多个模型版本。在模型版本的控制面板中，您可以将一个版本设置为默认版本，任何未指定版本的推理请求将路由到指定的 “默认版本”。请注意，每个模型版本将托管在单独的节点上，并会积累
    GCP 成本。
- en: Model inference
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 模型推理
- en: Since TensorFlow Serving has been battle tested at Google and is used heavily
    internally, it is also used behind the scenes at GCP. You’ll notice that the AI
    Platform isn’t just using the same model export format as we have seen with our
    TensorFlow Serving instances but the payloads have the same data structure as
    we have seen before.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 TensorFlow Serving 在 Google 内部经过了大量测试，并且在 GCP 幕后也得到了广泛使用，因此它也在 AI 平台中使用。您会注意到，AI
    平台不仅仅使用了我们在 TensorFlow Serving 实例中看到的相同模型导出格式，而且负载的数据结构与之前看到的相同。
- en: The only significant difference is the API connection. As you’ll see in this
    section, you’ll connect to the model version via the GCP API that is handling
    the request authentication.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的显著区别在于 API 连接。正如本节所述，您将通过处理请求认证的 GCP API 连接到模型版本。
- en: 'To connect with the Google Cloud API, you’ll need to install the library `google-api-python-client`
    with:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 要连接到Google Cloud API，您需要使用以下命令安装`google-api-python-client`库：
- en: '`$` `pip install google-api-python-client`'
  id: totrans-411
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `pip install google-api-python-client`'
- en: 'All Google services can be connected via a service object. The helper function
    in the following code snippet highlights how to create the service object. The
    Google API client takes a `service name` and a `service version` and returns an
    object that provides all API functionalities via methods from the returned object:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 所有Google服务都可以通过服务对象连接。下面代码片段中的辅助函数突出了如何创建服务对象。Google API客户端采用`服务名称`和`服务版本`，返回一个对象，通过返回的对象的方法提供所有API功能：
- en: '`import``googleapiclient.discovery``def``_connect_service``():``return``googleapiclient``.``discovery``.``build``(``serviceName``=``"ml"``,``version``=``"v1"``)`'
  id: totrans-413
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`import``googleapiclient.discovery``def``_connect_service``():``return``googleapiclient``.``discovery``.``build``(``serviceName``=``"ml"``,``version``=``"v1"``)`'
- en: 'Similar to our earlier REST and gRPC examples, we nest our inference data under
    a fixed `instances` key, which carries a list of input dictionaries. We have created
    a little helper function to generate the payloads. This function contains any
    preprocessing if you need to modify your input data before the inference:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前的REST和gRPC示例类似，我们将推断数据嵌套在一个固定的`instances`键下，其中包含一个输入字典列表。我们创建了一个小助手函数来生成有效负载。此函数包含任何预处理，如果需要在推断之前修改输入数据：
- en: '`def``_generate_payload``(``sentence``):``return``{``"instances"``:``[{``"sentence"``:``sentence``}]}`'
  id: totrans-415
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`def``_generate_payload``(``sentence``):``return``{``"instances"``:``[{``"sentence"``:``sentence``}]}`'
- en: With the service object created on the client side and the payload generated,
    it’s time to request the prediction from the Google Cloud–hosted machine learning
    model.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 在客户端创建的服务对象和生成的有效负载后，现在是时候从托管在Google Cloud上的机器学习模型请求预测了。
- en: 'The service object of the AI Platform service contains a predict method that
    accepts a `name` and a `body`. The name is a path string containing your GCP project
    name, model name, and, if you want to make predictions with a specific model version,
    version name. If you don’t specify a version number, the default model version
    will be used for the model inference. The body contains the inference data structure
    we generated earlier:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: AI平台服务的服务对象包含一个预测方法，接受`name`和`body`。`name`是一个路径字符串，包含您的GCP项目名称、模型名称，以及如果要使用特定模型版本进行预测，则版本名称。如果您不指定版本号，则将使用默认模型版本进行模型推断。`body`包含我们之前生成的推断数据结构：
- en: '`project``=``"``yourGCPProjectName``"``model_name``=``"demo_model"``version_name``=``"v1"``request``=``service``.``projects``()``.``predict``(``name``=``"projects/{}/models/{}/versions/{}"``.``format``(``project``,``model_name``,``version_name``),``body``=``_generate_payload``(``sentence``)``)``response``=``request``.``execute``()`'
  id: totrans-418
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`project``=``"``yourGCPProjectName``"``model_name``=``"demo_model"``version_name``=``"v1"``request``=``service``.``projects``()``.``predict``(``name``=``"projects/{}/models/{}/versions/{}"``.``format``(``project``,``model_name``,``version_name``),``body``=``_generate_payload``(``sentence``)``)``response``=``request``.``execute``()`'
- en: 'The Google Cloud AI Platform response contains the predict scores for the different
    categories similar to a REST response from a TensorFlow Serving instance:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud AI平台响应包含对不同类别的预测分数，类似于从TensorFlow Serving实例的REST响应：
- en: '`{``''predictions''``:``[``{``''label''``:``[``0.9000182151794434``,``0.02840868942439556``,``0.009750653058290482``,``0.06182243302464485``]}``]}`'
  id: totrans-420
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`{``''predictions''``:``[``{``''label''``:``[``0.9000182151794434``,``0.02840868942439556``,``0.009750653058290482``,``0.06182243302464485``]}``]}`'
- en: The demonstrated deployment option is a quick way of deploying a machine learning
    model without setting up an entire deployment infrastructure. Other cloud providers
    like AWS or Microsoft Azure offer similar model deployment services. Depending
    on your deployment requirements, cloud providers can be a good alternative to
    self-hosted deployment options. The downsides are potentially higher costs and
    the lack of completely optimizing the endpoints (i.e., by providing gRPC endpoints
    or batching functionality, as we discussed in [“Batching Inference Requests”](index_split_014.html#filepos949215)).
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 示范的部署选项是一种快速部署机器学习模型的方式，而无需设置整个部署基础设施。其他云提供商如AWS或Microsoft Azure提供类似的模型部署服务。根据您的部署要求，云提供商可能是自托管部署选项的良好替代方案。缺点可能包括更高的成本以及不完全优化端点（例如通过提供gRPC端点或批处理功能，正如我们在["批处理推断请求"](index_split_014.html#filepos949215)中讨论的）。
- en: Model Deployment with TFX Pipelines
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TFX Pipelines进行模型部署
- en: In the introduction to this chapter, in [Figure 8-1](index_split_013.html#filepos766196)
    we showed the deployment steps as one component of a machine learning pipeline.
    After discussing the inner workings of model deployments, and especially TensorFlow
    Serving, we want to connect the dots with our machine learning pipeline in this
    section.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的引言中，在[Figure 8-1](index_split_013.html#filepos766196)中展示了部署步骤作为机器学习流水线的一个组成部分。在讨论模型部署的内部工作原理，特别是
    TensorFlow Serving 后，我们希望在本节中将机器学习流水线与此连接起来。
- en: In [Figure 8-10](#filepos991098), you can see the steps for a continuous model
    deployment. We assume that you have TensorFlow Serving running and configured
    to load models from a given file location. Furthermore, we assume that TensorFlow
    Serving will load the models from an external file location (i.e., a cloud storage
    bucket or a mounted persistent volume). Both systems, the TFX pipeline and the
    TensorFlow Serving instance, need to have access to the same filesystem.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Figure 8-10](#filepos991098)中，您可以看到持续模型部署的步骤。我们假设您已经运行并配置了 TensorFlow Serving，以从给定的文件位置加载模型。此外，我们假设
    TensorFlow Serving 将从外部文件位置（例如云存储桶或挂载的持久卷）加载模型。TFX 管道和 TensorFlow Serving 实例两个系统需要访问相同的文件系统。
- en: '![](images/00007.jpg)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00007.jpg)'
- en: Figure 8-10\. Deployment of models produced from TFX pipelines
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-10\. TFX 管道生成的模型部署
- en: In [“TFX Pusher Component”](index_split_012.html#filepos758056), we discussed
    the `Pusher` component. The TFX component allows us to push validated models to
    a given location (e.g., a cloud storage bucket). TensorFlow Serving can pick up
    new model versions from the cloud storage location, unload the earlier model version,
    and load the latest version for the given model endpoint. This is the default
    model policy of TensorFlow Serving.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [“TFX Pusher Component”](index_split_012.html#filepos758056) 中，我们讨论了 `Pusher`
    组件。这个 TFX 组件允许我们将经过验证的模型推送到指定位置（例如，云存储桶）。TensorFlow Serving 可以从云存储位置获取新的模型版本，卸载之前的模型版本，并加载给定模型端点的最新版本。这是
    TensorFlow Serving 的默认模型策略。
- en: Due to the default model policy, we can build a simple continuous deployment
    setup with TFX and TensorFlow Serving fairly easily.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 基于默认的模型策略，我们可以相对容易地使用 TFX 和 TensorFlow Serving 构建一个简单的持续部署设置。
- en: Summary
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed how to set up TensorFlow Serving to deploy machine
    learning models and why a model server is a more scalable option than deploying
    machine learning models through a Flask web application. We stepped through the
    installation and configuration steps, introduced the two main communication options,
    REST and gRPC, and briefly discussed the advantages and disadvantages of both
    communication protocols.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了如何设置 TensorFlow Serving 来部署机器学习模型，以及为什么模型服务器比通过 Flask web 应用部署机器学习模型更具扩展性。我们详细介绍了安装和配置步骤，介绍了两种主要的通信选项，REST
    和 gRPC，并简要讨论了这两种通信协议的优缺点。
- en: Furthermore, we explained some of the great benefits of TensorFlow Serving,
    including the batching of model requests and the ability to obtain metadata about
    the different model versions. We also discussed how to set up a quick A/B test
    setup with TensorFlow Serving.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们解释了 TensorFlow Serving 的一些重要优势，包括模型请求的批处理和获取不同模型版本元数据的能力。我们还讨论了如何通过 TensorFlow
    Serving 快速设置 A/B 测试。
- en: We closed this chapter with a brief introduction of a managed cloud service,
    using Google Cloud AI Platform as an example. Managed cloud services provide you
    the ability to deploy machine learning models without managing your own server
    instances.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章结尾简要介绍了托管云服务的概念，以 Google Cloud AI Platform 为例。托管云服务允许您部署机器学习模型，而无需管理自己的服务器实例。
- en: In the next chapter, we will discuss enhancing our model deployments, for example,
    by loading models from cloud providers or by deploying TensorFlow Serving with
    Kubernetes.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论如何增强我们的模型部署，例如通过从云提供商加载模型，或者通过 Kubernetes 部署 TensorFlow Serving。
- en: '[1  ](index_split_013.html#filepos783246) For application use cases, visit
    [TensorFlow](https://oreil.ly/qCY6J).'
  id: totrans-434
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[1  ](index_split_013.html#filepos783246) 有关应用实例，请访问[TensorFlow](https://oreil.ly/qCY6J)。'
- en: '[2  ](index_split_013.html#filepos823381) Model tag sets are used to identify
    MetaGraphs for loading. A model could be exported with a graph specified for training
    and serving. Both MetaGraphs can be provided through different model tags.'
  id: totrans-435
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[2  ](index_split_013.html#filepos823381) 模型标签集用于识别用于加载的 MetaGraph。一个模型可以导出为指定用于训练和服务的图形。这两个
    MetaGraph 可以通过不同的模型标签提供。'
- en: '[3  ](index_split_013.html#filepos832218) If you haven’t installed or used
    `Docker` before, check out our brief introduction in [Appendix A](index_split_023.html#filepos1605424).'
  id: totrans-436
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[3  ](index_split_013.html#filepos832218) 如果您之前没有安装或使用过 `Docker`，请查看我们在 [附录
    A](index_split_023.html#filepos1605424) 中的简要介绍。'
- en: '[4  ](index_split_013.html#filepos841964) For a more detailed introduction
    to REST and gRPC, please check [“REST Versus gRPC”](index_split_014.html#filepos867025).'
  id: totrans-437
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[4  ](index_split_013.html#filepos841964) 想要更详细地了解 REST 和 gRPC，请查看 [“REST Versus
    gRPC”](index_split_014.html#filepos867025)。'
- en: '[5  ](index_split_013.html#filepos851409) The loading and unloading of models
    only works if the `file_system_poll_wait_seconds` is configured to be greater
    than 0\. The default configuration is 2s.'
  id: totrans-438
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[5  ](index_split_013.html#filepos851409) 模型的加载和卸载仅在 `file_system_poll_wait_seconds`
    配置为大于 0 时才有效。默认配置为 2 秒。'
- en: '[6  ](index_split_014.html#filepos903628) The SSL configuration file is based
    on the SSL configuration protocol buffer, which can be found in [the TensorFlow
    Serving API](https://oreil.ly/ZAEte).'
  id: totrans-439
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[6  ](index_split_014.html#filepos903628) SSL 配置文件基于 SSL 配置协议缓冲区，可以在 [TensorFlow
    Serving API](https://oreil.ly/ZAEte) 中找到。'
- en: '[7  ](index_split_014.html#filepos912610) A/B testing isn’t complete without
    the statistical test of the results you get from people interacting with the two
    models. The shown implementation just provides us the A/B testing backend.'
  id: totrans-440
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[7  ](index_split_014.html#filepos912610) A/B 测试如果没有对与两个模型交互产生的结果进行统计检验，是不完整的。所展示的实现仅提供了
    A/B 测试的后端。'
- en: '[8  ](index_split_014.html#filepos966587)  [ONNX](https://onnx.ai) is a way
    of describing machine learning models.'
  id: totrans-441
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[8  ](index_split_014.html#filepos966587)  [ONNX](https://onnx.ai) 是描述机器学习模型的一种方式。'
- en: '[9  ](#filepos974297) For the lowest prediction latencies, choose a region
    closest to the geographical region of the model requests.'
  id: totrans-442
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[9  ](#filepos974297) 为了达到最低的预测延迟，请选择距离模型请求地理区域最近的区域。'
