- en: Chapter 3\. Problem Representation Design Patterns
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章。问题表示设计模式
- en: '[Chapter 2](ch02.xhtml#data_representation_design_patterns) looked at design
    patterns that catalog the myriad ways in which inputs to machine learning models
    can be represented. This chapter looks at different types of machine learning
    problems and analyzes how the model architectures vary depending on the problem.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[第二章](ch02.xhtml#data_representation_design_patterns)讨论了设计模式，目的是列举机器学习模型输入的多种表示方式。本章将探讨不同类型的机器学习问题，并分析模型架构如何根据问题的不同而变化。'
- en: 'The input and the output types are two key factors impacting the model architecture.
    For instance, the output in supervised machine learning problems can vary depending
    on whether the problem being solved is a classification or regression problem.
    Special neural network layers exist for specific types of input data: convolutional
    layers for images, speech, text, and other data with spatiotemporal correlation,
    recurrent networks for sequential data, and so on. A huge literature has arisen
    around special techniques such as max pooling, attention, and so forth on these
    types of layers. In addition, special classes of solutions have been crafted for
    commonly occurring problems like recommendations (such as matrix factorization)
    or time-series forecasting (for example, ARIMA). Finally, a group of simpler models
    together with common idioms can be used to solve more complex problems—for example,
    text generation often involves a classification model whose outputs are postprocessed
    using a beam search algorithm.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 输入和输出类型是影响模型架构的两个关键因素。例如，在监督学习问题中，输出可以根据解决的问题是分类问题还是回归问题而有所不同。针对特定类型的输入数据存在特殊的神经网络层：用于图像、语音、文本以及具有时空相关性的其他数据的卷积层，用于序列数据的循环网络等等。围绕这些类型的层面已经出现了大量文献，专门讨论诸如最大池化、注意力等特殊技术。此外，针对常见问题已经形成了特殊类别的解决方案，如推荐问题（例如矩阵分解）或时间序列预测（例如ARIMA）。最后，一组更简单的模型以及常见的习惯用法可以用来解决更复杂的问题，例如文本生成通常涉及使用分类模型，其输出经过使用波束搜索算法进行后处理。
- en: To limit our discussion and stay away from areas of active research, we will
    ignore patterns and idioms associated with specialized machine learning domains.
    Instead, we will focus on regression and classification and examine patterns with
    problem representation in just these two types of ML models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了限制我们的讨论并避开正在研究的领域，我们将忽略与专门的机器学习领域相关的模式和习惯用法。相反，我们将专注于回归和分类，并分析在这两种类型的机器学习模型中，问题表示的模式。
- en: The *Reframing* design pattern takes a solution that is intuitively a regression
    problem and poses it as a classification problem (and vice versa). The *Multilabel*design
    pattern handles the case that training examples can belong to more than one class.
    The *Cascade* design pattern addresses situations where a machine learning problem
    can be profitably broken into a series (or cascade) of ML problems. The *Ensemble*
    design pattern solves a problem by training multiple models and aggregating their
    responses. The *Neutral Class* design pattern looks at how to handle situations
    where experts disagree. The *Rebalancing* design pattern recommends approaches
    to handle highly skewed or imbalanced data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*重构*设计模式将一个直观上是回归问题的解决方案转化为分类问题（反之亦然）。*多标签*设计模式处理训练示例可以属于多个类的情况。*级联*设计模式处理可以将机器学习问题有利地分解成一系列（或级联）机器学习问题的情况。*集成*设计模式通过训练多个模型并聚合它们的响应来解决问题。*中立类*设计模式探讨如何处理专家意见不一致的情况。*再平衡*设计模式建议处理高度倾斜或不平衡数据的方法。'
- en: 'Design Pattern 5: Reframing'
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计模式 5：重构
- en: The Reframingdesign patternrefers to changing the representation of the output
    of a machine learning problem. For example, we could take something that is intuitively
    a regression problem and instead pose it as a classification problem (and vice
    versa).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*重构*设计模式指的是改变机器学习问题输出表示的方法。例如，我们可以将一个直观上是回归问题的东西改变为分类问题（反之亦然）。'
- en: Problem
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: The first step of building any machine learning solution is framing the problem.
    Is this a supervised learning problem? Or unsupervised? What are the features?
    If it is a supervised problem, what are the labels? What amount of error is acceptable?
    Of course, the answers to these questions must be considered in context with the
    training data, the task at hand, and the metrics for success.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 建立任何机器学习解决方案的第一步是确立问题框架。这是一个监督学习问题吗？还是非监督学习？特征是什么？如果是监督问题，标签是什么？什么样的误差是可接受的？当然，这些问题的答案必须与训练数据、手头任务和成功的度量标准结合考虑。
- en: 'For example, suppose we wanted to build a machine learning model to predict
    future rainfall amounts in a given location. Starting broadly, would this be a
    regression or classification task? Well, since we’re trying to predict rainfall
    amount (for example, 0.3 cm), it makes sense to consider this as a time-series
    forecasting problem: given the current and historical climate and weather patterns,
    what amount of rainfall should we expect in a given area in the next 15 minutes?
    Alternately, because the label (the amount of rainfall) is a real number, we could
    build a regression model. As we start to develop and train our model, we find
    (perhaps not surprisingly) that weather prediction is harder than it sounds. Our
    predicted rainfall amounts are all off because, for the same set of features,
    it sometimes rains 0.3 cm and other times it rains 0.5 cm. What should we do to
    improve our predictions? Should we add more layers to our network? Or engineer
    more features? Perhaps more data will help? Maybe we need a different loss function?'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们想构建一个机器学习模型来预测给定位置未来降水量。从宽泛的角度来看，这将是一个回归任务还是分类任务？嗯，因为我们试图预测降水量（例如，0.3厘米），考虑这个问题作为时间序列预测问题是有道理的：在当前和历史气候和天气模式的基础上，我们应该预期在下一个15分钟内在某个区域内降水量是多少？或者，因为标签（降水量）是一个实数，我们可以构建一个回归模型。当我们开始开发和训练我们的模型时，我们发现（也许不足为奇）天气预测比听起来更难。我们预测的降水量都不准确，因为对于相同的特征集，有时会下0.3厘米的雨，有时会下0.5厘米的雨。我们应该怎么做来改进我们的预测？我们应该给我们的网络添加更多层吗？或者工程化更多特征？也许更多的数据会有帮助？也许我们需要一个不同的损失函数？
- en: Any of these adjustments could improve our model. But wait. Is regression the
    only way we can pose this task? Perhaps we can reframe our machine learning objective
    in a way that improves our task performance.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这些调整中的任何一个都可以改进我们的模型。但是等等。回归难道是我们唯一可以提出这个任务的方式吗？也许我们可以重新构思我们的机器学习目标，以改善我们的任务表现。
- en: Solution
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: The core issue here is that rainfall is probabilistic. For the same set of features,
    it sometimes rains 0.3 cm and other times it rains 0.5 cm. Yet, even if a regression
    model were able to learn the two possible amounts, it is limited to predicting
    only a single number.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的核心问题是降水是概率性的。对于相同的特征集，有时会下0.3厘米的雨，有时会下0.5厘米的雨。然而，即使回归模型能够学习这两种可能的量，它也仅限于预测一个单一的数字。
- en: Instead of trying to predict the amount of rainfall as a regression task, we
    can reframe our objective as a classification problem. There are different ways
    this can be accomplished. One approach is to model a discrete probability distribution,
    as shown in [Figure 3-1](#instead_of_predicting_precipitation_as). Instead of
    predicting rainfall as a real-valued output, we model the output as a multiclass
    classification giving the probability that the rainfall in the next 15 minutes
    is within a certain range of rainfall amounts.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是试图将降水量作为回归任务进行预测，我们可以重新构思我们的目标，将其作为一个分类问题。有不同的方法可以实现这一点。一种方法是建模离散概率分布，如[图3-1](#instead_of_predicting_precipitation_as)所示。与其预测降水量作为实值输出，我们将输出建模为一个多类分类，给出下一个15分钟内降水概率在一定降水量范围内的概率。
- en: '![Instead of predicting precipitation as a regression output, we can instead
    model discrete probability distribution using a multiclass classification. ](Images/mldp_0301.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![而不是将降水预测为回归输出，我们可以使用多类分类来建模离散概率分布。](Images/mldp_0301.png)'
- en: Figure 3-1\. Instead of predicting precipitation as a regression output, we
    can instead model discrete probability distribution using a multiclass classification.
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-1。而不是将降水预测为回归输出，我们可以使用多类分类来建模离散概率分布。
- en: Both the regression approach and this reframed-as-classification approach give
    a prediction of the rainfall for the next 15 minutes. However, the classification
    approach allows the model to capture the probability distribution of rainfall
    of different quantities instead of having to choose the mean of the distribution.
    Modeling a distribution in this way is advantageous since precipitation does not
    exhibit the typical bell-shaped curve of a normal distribution and instead follows
    a [Tweedie distribution](https://oreil.ly/C8JfK), which allows for a preponderance
    of points at zero. Indeed, that’s the approach taken in a [Google Research paper](https://oreil.ly/PGAEw)
    that predicts precipitation rates in a given location using a 512-way categorical
    distribution. Other reasons that modeling a distribution can be advantageous is
    when the distribution is bimodal, or even when it is normal but with a large variance.
    A recent paper that beats all benchmarks at [predicting protein folding structure](https://oreil.ly/-Hi3k)
    also predicts the distance between amino acids as a 64-way classification problem
    where the distances are bucketized into 64 bins.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这种回归方法和这种重新构思为分类方法都能预测接下来15分钟的降雨情况。然而，分类方法允许模型捕捉不同数量降雨的概率分布，而不是必须选择分布的平均值。以这种方式建模分布是有利的，因为降水不展示正态分布的典型钟形曲线，而是遵循[Tweedie分布](https://oreil.ly/C8JfK)，这种分布允许在零点处有大量数据点。实际上，这是谷歌研究论文中采用的方法，该论文使用512种分类分布预测给定位置的降水率。另一个建模分布有利的原因是当分布是双峰时，甚至是当分布是正态分布但方差很大时。最近一篇打破了所有[预测蛋白质折叠结构的基准](https://oreil.ly/-Hi3k)的论文也将氨基酸之间的距离预测为64种分类问题，其中距离被分桶为64个区间。
- en: Another reason to reframe a problem is when the objective is better in the other
    type of model. For example, suppose we are trying to build a recommendation system
    for videos. A natural way to frame this problem is as a classification problem
    of predicting whether a user is likely to watch a certain video. This framing,
    however, can lead to a recommendation system that prioritizes click bait. It might
    be better to reframe this into a regression problem of predicting the fraction
    of the video that will be watched.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重新构思问题的原因是当目标在另一类型的模型中表现更好时。例如，假设我们试图构建一个视频推荐系统。将这个问题自然地框定为分类问题，即预测用户是否可能观看某个视频，可能会导致推荐系统优先考虑点击诱饵。将这个问题重新构思为预测将会观看的视频的分数，可能会更好。
- en: Why It Works
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么有效
- en: Changing the context and reframing the task of a problem can help when building
    a machine learning solution. Instead of learning a single real number, we relax
    our prediction target to be instead a discrete probability distribution. We lose
    a little precision due to bucketing, but gain the expressiveness of a full probability
    density function (PDF). The discretized predictions provided by the classification
    model are more adept at learning a complex target than the more rigid regression
    model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 改变上下文和重新构思问题的任务可以帮助构建机器学习解决方案。我们不再学习单一实数，而是将预测目标放宽为离散概率分布。由于分桶，我们失去了一些精度，但获得了完整概率密度函数（PDF）的表达能力。分类模型提供的离散预测比更为严格的回归模型更擅长学习复杂的目标。
- en: An added advantage of this classification framing is that we obtain posterior
    probability distribution of our predicted values, which provides more nuanced
    information. For example, suppose the learned distribution is bimodal. By modeling
    a classification as a discrete probability distribution, the model is able to
    capture the bimodal structure of the predictions, as [Figure 3-2](#reframing_a_classification_task_to_mode)
    illustrates. Whereas, if only predicting a single numeric value, this information
    would be lost. Depending on the use case, this could make the task easier to learn
    and substantially more advantageous.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分类框架的另一个优势是我们获得了预测值的后验概率分布，这提供了更细致的信息。例如，假设学习的分布是双峰的。通过将分类建模为离散概率分布，模型能够捕捉到预测的双峰结构，正如[图3-2](#reframing_a_classification_task_to_mode)所示。相反，如果只预测单个数值，这些信息将会丢失。根据使用情况，这可能会使任务更容易学习并且更加有利。
- en: '![Reframing a classification task to model a probability distribution allows
    the predictions to capture bimodal output. The prediction is not limited to single
    value as in a regression.](Images/mldp_0302.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![将分类任务重新框架为建模概率分布允许预测捕捉双峰输出。预测不限于回归中的单个值。](Images/mldp_0302.png)'
- en: Figure 3-2\. Reframing a classification task to model a probability distribution
    allows the predictions to capture bimodal output. The prediction is not limited
    to a single value as in a regression.
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-2\. 将分类任务重新框架为建模概率分布允许预测捕捉双峰输出。预测不限于回归中的单个值。
- en: Capturing uncertainty
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 捕捉不确定性
- en: Let’s look again at the natality dataset and the task of predicting baby weight.
    Since baby weight is a positive real value, this is intuitively a regression problem.
    However, notice that for a given set of inputs, `weight_pounds` (the label) can
    take many different values. We see that the distribution of babies’ weights for
    a specific set of input values (male babies born to 25-year-old mothers at 38
    weeks) approximately follows a normal distribution centered at about 7.5 pounds.
    The code to produce the graph in [Figure 3-3](#given_a_specific_set_of_inputs_left_par)
    can be found in the [repository](https://github.com/GoogleCloudPlatform/ml-design-patterns/03_problem_representation/reframing.ipynb)
    for this book.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次看一下出生数据集和预测婴儿体重的任务。由于婴儿体重是一个正实数值，这在直觉上是一个回归问题。然而，请注意，对于给定的输入集合，`weight_pounds`（标签）可以取多种不同的值。我们看到，对于特定的输入值集合（例如，在38周时出生的25岁母亲的男婴），婴儿体重的分布大致遵循以约7.5磅为中心的正态分布。生成图3-3中图表的代码可以在此书的[存储库](https://github.com/GoogleCloudPlatform/ml-design-patterns/03_problem_representation/reframing.ipynb)中找到。
- en: '![Given a specific set of inputs (for example, male babies born to 25-year-old
    mothers at 38 weeks) takes a range of values, approximately following a normal
    distribution centered at 7.5 lbs. ](Images/mldp_0303.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![对于特定的输入集合（例如，在38周时出生的25岁母亲的男婴）会取一系列值，大致遵循以7.5磅为中心的正态分布。](Images/mldp_0303.png)'
- en: Figure 3-3\. Given a specific set of inputs (for example, male babies born to
    25-year-old mothers at 38 weeks) the weight_pounds variable takes a range of values,
    approximately following a normal distribution centered at 7.5 lbs.
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-3\. 对于特定的输入集合（例如，在38周时出生的25岁母亲的男婴），`weight_pounds`变量会取一系列值，大致遵循以7.5磅为中心的正态分布。
- en: However, notice the width of the distribution—even though the distribution peaks
    at 7.5 pounds, there is a nontrivial likelihood (actually 33%) that a given baby
    is less than 6.5 pounds or more than 8.5 pounds! The width of this distribution
    indicates the irreducible error inherent to the problem of predicting baby weight.
    Indeed, the best root mean square error we can obtain on this problem, if we frame
    it as a regression problem, is the standard deviation of the distribution seen
    in [Figure 3-3](#given_a_specific_set_of_inputs_left_par).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 但请注意分布的宽度——即使分布的峰值在7.5磅，有相当可观的可能性（实际上是33%）某个婴儿的体重小于6.5磅或大于8.5磅！这个分布的宽度显示了预测婴儿体重问题中固有的不可减少的误差。事实上，如果我们将其视为回归问题，那么在这个问题上我们可以获得的最佳均方根误差是[图3-3](#given_a_specific_set_of_inputs_left_par)中所见分布的标准偏差。
- en: If we frame this as a regression problem, we’d have to state the prediction
    result as 7.5 +/- 1.0 (or whatever the standard deviation is). Yet, the width
    of the distribution will vary for different combinations of inputs, and so learning
    the width is another machine learning problem in and of itself. For example, at
    the 36th week, for mothers of the same age, the standard deviation is 1.16 pounds.
    *Quantiles regression*, covered later in the pattern discussion, tries to do exactly
    this but in a nonparametric way.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将其视为回归问题，我们必须将预测结果表述为7.5 +/- 1.0（或者标准偏差是多少）。然而，对于不同的输入组合，这个分布的宽度将有所不同，因此学习这个宽度本身也是另一个机器学习问题。例如，在第36周，对于同龄母亲的母婴来说，标准偏差是1.16磅。*分位数回归*，稍后在模式讨论中涵盖，试图以非参数化方式做到这一点。
- en: Tip
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Had the distribution been multimodal (with multiple peaks), the case for reframing
    the problem as a classification would be even stronger. However, it is helpful
    to realize that because of the law of large numbers, as long as we capture all
    of the relevant inputs, many of the distributions we will encounter on large datasets
    will be bell-shaped, although other distributions are possible. The wider the
    bell curve, and the more this width varies at different values of inputs, the
    more important it is to capture uncertainty and the stronger the case for reframing
    the regression problem as a classification one.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果分布是多模态的（具有多个峰值），将问题重新定义为分类任务的情况将更为有力。然而，有助于认识到，由于大数定律的存在，只要我们捕捉到所有相关的输入，我们在大型数据集上会遇到的许多分布将是钟形的，尽管其他分布也是可能的。钟形曲线越宽，而在不同输入值处这种宽度变化越大，捕捉不确定性的重要性也越大，这就更加支持将回归问题重新定义为分类问题。
- en: By reframing the problem, we train the model as a multiclass classification
    that learns a discrete probability distribution for the given training examples.
    These discretized predictions are more flexible in terms of capturing uncertainty
    and better able to approximate the complex target than a regression model. At
    inference time, the model then predicts a collection of probabilities corresponding
    to these potential outputs. That is, we obtain a discrete PDF giving the relative
    likelihood of any specific weight. Of course, care has to be taken here—classification
    models can be hugely uncalibrated (such as the model being overly confident and
    wrong).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重新定义问题，我们训练模型作为一个多类别分类，学习给定训练示例的离散概率分布。这些离散化的预测更灵活，能够捕捉不确定性，并且比回归模型更能逼近复杂的目标。在推理时，模型预测与这些潜在输出相对应的一系列概率。也就是说，我们获得了一个离散的概率分布，给出了任何特定权重的相对可能性。当然，在这里需要注意——分类模型可能会出现严重的不校准（例如模型过于自信而错误）。
- en: Changing the objective
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 改变目标
- en: In some scenarios, reframing a classification task as a regression could be
    beneficial. For example, suppose we had a large movie database with customer ratings
    on a scale from 1 to 5, for all movies that the user had watched and rated. Our
    task is to build a machine learning model that will be used to serve recommendations
    to our users.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些场景中，将分类任务重新定义为回归任务可能会有益。例如，假设我们有一个大型电影数据库，其中包含用户对所有观看并评价的电影的评分（评分从1到5）。我们的任务是构建一个机器学习模型，用于为用户提供推荐。
- en: Viewed as a classification task, we could consider building a model that takes
    as input a `user_id`, along with that user’s previous video watches and ratings,
    and predicts which movie from our database to recommend next. However, it is possible
    to reframe this problem as a regression. Instead of the model having a categorical
    output corresponding to a movie in our database, our model could instead carry
    out multitask learning, with the model learning a number of key characteristics
    (such as income, customer segment, and so on) of users who are likely to watch
    a given movie.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将其视为分类任务，我们可以考虑构建一个模型，该模型以`user_id`及其用户先前观看的视频和评分作为输入，并预测从我们的数据库中推荐哪部电影。然而，我们也可以将这个问题重新构造为回归问题。模型不再具有与数据库中电影对应的分类输出，而是可以进行多任务学习，模型学习用户可能观看给定电影的若干关键特征（如收入、客户段等）。
- en: Reframed as a regression task, the model now predicts the user-space representation
    for a given movie. To serve recommendations, we choose the set of movies that
    are closest to the known characteristics of a user. In this way, instead of the
    model providing the probability that a user will like a movie as in a classification,
    we would get a cluster of movies that have been watched by users like this user.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 将其重新构造为回归任务，模型现在预测给定电影的用户空间表示。为了提供推荐，我们选择那些与用户已知特征最接近的电影集合。通过这种方式，与分类任务中模型提供用户可能会喜欢某部电影的概率不同，我们将得到一组被这类用户观看过的电影。
- en: By reframing the classification problem of recommending movies to be a regression
    of user characteristics, we gain the ability to easily adapt our recommendation
    model to recommend trending videos, or classic movies, or documentaries without
    having to train a separate classification model each time.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将推荐电影的分类问题重新定义为用户特征的回归问题，我们能够轻松地调整我们的推荐模型，以推荐流行视频、经典电影或纪录片，而无需每次训练一个单独的分类模型。
- en: This type of model approach is also useful when the numerical representation
    has an intuitive interpretation; for example, a latitude and longitude pair can
    be used instead of urban area predictions. Suppose we wanted to predict which
    city will experience the next viral outbreak or which New York neighborhood will
    have a real estate pricing surge. It could be easier to predict the latitude and
    longitude and choose the city or neighborhood closest to that location, rather
    than predicting the city or neighborhood itself.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当数值表示具有直观解释时，这种模型方法也非常有用；例如，经度和纬度对可以用来代替对城市地区的预测。假设我们想要预测哪个城市将出现下一次病毒爆发，或者哪个纽约社区将出现房地产价格的激增。预测经度和纬度，然后选择最接近该位置的城市或社区可能比直接预测城市或社区本身更容易。
- en: Trade-Offs and Alternatives
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 折中和替代方案
- en: There is rarely just one way to frame a problem, and it is helpful to be aware
    of any trade-offs or alternatives of a given implementation. For example, bucketizing
    the output values of a regression is an approach to reframing the problem as a
    classification task. Another approach is multitask learning that combines both
    tasks (classification and regression) into a single model using multiple prediction
    heads. With any reframing technique, being aware of data limitations or the risk
    of introducing label bias is important.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 很少只有一种方式来构建问题，了解任何实现的折中或替代方案是有帮助的。例如，将回归输出值分桶化是将问题重新构建为分类任务的一种方法。另一种方法是使用多任务学习，将分类和回归任务结合到单个模型中，使用多个预测头部。对于任何重新构建技术，了解数据限制或引入标签偏差的风险是很重要的。
- en: Bucketized outputs
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分桶化输出
- en: The typical approach to reframing a regression task as a classification is to
    bucketize the output values. For example, if our model is to be used to indicate
    when a baby might need critical care upon birth, the categories in [Table 3-1](#bucketized_outputs_for_baby_weight)
    could be sufficient.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 将回归任务重构为分类任务的典型方法是对输出值进行分桶。例如，如果我们的模型用于指示婴儿在出生时可能需要重视护理的时机，[表格 3-1](#bucketized_outputs_for_baby_weight)
    中的类别可能足够了。
- en: Table 3-1\. Bucketized outputs for baby weight
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Table 3-1\. 婴儿体重的分桶化输出
- en: '| Category | Description |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 描述 |'
- en: '| --- | --- |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| High birth weight | More than 8.8 lbs |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 高出生体重 | 大于 8.8 磅 |'
- en: '| Average birth weight | Between 5.5 lbs and 8.8 lbs |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 平均出生体重 | 5.5 磅至 8.8 磅之间 |'
- en: '| Low birth weight | Between 3.31 lbs and 5.5 lbs |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 低出生体重 | 3.31 磅至 5.5 磅之间 |'
- en: '| Very low birth weight | Less than 3.31 lbs |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 非常低出生体重 | 少于 3.31 磅 |'
- en: Our regression model now becomes a multiclass classification. Intuitively, it
    is easier to predict one out of four possible categorical cases than to predict
    a single value from the continuum of real numbers—just as it would be easier to
    predict a binary 0 versus 1 target for `is_underweight` instead of four separate
    categories `high_weight` versus `avg_weight` versus `low_weight` versus `very_low_weight`.
    By using categorical outputs, our model is incentivized less for getting arbitrarily
    close to the actual output value since we’ve essentially changed the output label
    to a range of values instead of a single real number.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的回归模型变成了多类分类。直观地说，预测四种可能的分类情况中的一种比预测一个连续的实数值要容易——就像预测二进制的 `is_underweight`
    目标 0 或 1 比预测 `高体重` versus `平均体重` versus `低体重` versus `非常低体重` 四种分开的类别要容易一样。通过使用分类输出，我们的模型更少地被激励于接近实际输出值，因为我们本质上已经将输出标签更改为数值范围而不是单个实数。
- en: In the [notebook](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/03_problem_representation/reframing.ipynb)
    accompanying this section, we train both a regression and a multiclass classification
    model. The regression model achieves an RMSE of 1.3 on the validation set while
    the classification model has an accuracy of 67%. Comparing these two models is
    difficult since one evaluation metric is RMSE and the other is accuracy. In the
    end, the design decision is governed by the use case. If medical decisions are
    based on bucketed values, then our model should be a classification using those
    buckets. However, if a more precise prediction of baby weight is needed, then
    it makes sense to use the regression model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节附带的[笔记本](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/03_problem_representation/reframing.ipynb)，我们训练了回归模型和多类别分类模型。回归模型在验证集上达到了1.3的RMSE，而分类模型的准确率为67%。由于一个评估指标是RMSE，另一个是准确率，因此比较这两个模型是困难的。最终，设计决策由使用案例决定。如果医疗决策基于分桶值，则我们的模型应该是使用这些桶的分类模型。然而，如果需要更精确地预测婴儿体重，那么使用回归模型是有意义的。
- en: Other ways of capturing uncertainty
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 捕捉不确定性的其他方法
- en: There are other ways to capture uncertainty in regression. A simple approach
    is to carry out quantile regression. For example, instead of predicting just the
    mean, we can estimate the conditional 10th, 20th, 30th, …, 90th percentile of
    what needs to be predicted. Quantile regression is an extension of linear regression.
    Reframing, on the other hand, can work with more complex machine learning models.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归中捕获不确定性还有其他方法。一个简单的方法是进行分位数回归。例如，我们可以估计需要预测的条件10th、20th、30th、…、90th百分位数，而不是仅预测均值。分位数回归是线性回归的扩展。另一方面，重构可以与更复杂的机器学习模型配合使用。
- en: 'Another, more sophisticated approach is to use a framework like [TensorFlow
    Probability](https://oreil.ly/AEtLG) to carry out regression. However, we have
    to explicitly model the distribution of the output. For example, if the output
    is expected to be normally distributed around a mean that’s dependent on the inputs,
    the model’s output layer would be:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种更复杂的方法是使用像[TensorFlow Probability](https://oreil.ly/AEtLG)这样的框架进行回归。然而，我们必须显式地对输出的分布进行建模。例如，如果预期输出以输入为依赖的均值正态分布，则模型的输出层将是：
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: On the other hand, if we know the variance increases with the mean, we might
    be able to model it using the lambda function. Reframing, on the other hand, doesn’t
    require us to model the posterior distribution.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果我们知道方差随着均值增加而增加，我们可能能够使用lambda函数对其进行建模。另一方面，重构并不要求我们对后验分布进行建模。
- en: Tip
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: When training any machine learning model, the data is key. More complex relationships
    typically require more training data examples to find those elusive patterns.
    With that in mind, it is important to consider how data requirements compare for
    regression or classification models. A common rule of thumb for classification
    tasks is that we should have 10 times the number of model features for each label
    category. For a regression model, the rule of thumb is 50 times the number of
    model features. Of course, these numbers are just rough heuristics and not precise.
    However, the intuition is that regression tasks typically require more training
    examples. Furthermore, this need for massive data only increases with the complexity
    of the task. Thus, there could be data limitations that should be considered when
    considering the type of model used or, in the case of classification, the number
    of label categories.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练任何机器学习模型时，数据至关重要。通常，更复杂的关系需要更多的训练数据示例来找到这些难以捉摸的模式。考虑到这一点，重要的是要考虑回归或分类模型的数据需求。分类任务的一个常见经验法则是，每个标签类别的模型特征应该有10倍的训练数据。对于回归模型，经验法则是模型特征的50倍。当然，这些数字只是粗略的启发式，而不是精确的。然而，直觉是回归任务通常需要更多的训练示例。此外，随着任务复杂性的增加，对大量数据的需求也会增加。因此，在考虑使用的模型类型或分类任务中标签类别数量时，应考虑数据限制。
- en: Precision of predictions
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测的精度
- en: When thinking of reframing a regression model as a multiclass classification,
    the width of the bins for the output label governs the precision of the classification
    model. In the case of our baby weight example, if we needed more precise information
    from the discrete probability density function, we would need to increase the
    number of bins of our categorical model. [Figure 3-4](#the_precision_of_the_multiclass_classif)
    shows how the discrete probability distributions would look as either a 4-way
    or 10-way classification.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑将回归模型重新构建为多类别分类时，输出标签的箱宽度决定了分类模型的精度。在我们的婴儿体重示例中，如果我们需要从离散概率密度函数中获取更精确的信息，则需要增加分类模型的箱数。[图 3-4](#the_precision_of_the_multiclass_classif)
    显示了离散概率分布如何表现为 4 通道或 10 通道分类。
- en: '![The precision of the multiclass classification is controlled by the width
    of the bins for the label. ](Images/mldp_0304.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![多类别分类的精度由标签的分箱宽度控制。](Images/mldp_0304.png)'
- en: Figure 3-4\. The precision of the multiclass classification is controlled by
    the width of the bins for the label.
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-4\. 多类别分类的精度由标签的分箱宽度控制。
- en: The sharpness of the PDF indicates the precision of the task as a regression.
    A sharper PDF indicates a smaller standard deviation of the output distribution
    while a wider PDF indicates a larger standard deviation and thus more variance.
    For a very sharp density function, it’s better to stick with a regression model
    (see [Figure 3-5](#the_precision_of_the_regression_is_indi)).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: PDF 的尖锐度表示回归任务的精度。更尖锐的 PDF 表示输出分布的标准偏差较小，而更宽的 PDF 表示标准偏差较大，因此具有更多方差。对于非常尖锐的密度函数，最好使用回归模型（见
    [图 3-5](#the_precision_of_the_regression_is_indi)）。
- en: '![The precision of the regression is indicated by the sharpness of the probability
    density function for a fixed set of input values.](Images/mldp_0305.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![回归的精度由概率密度函数在固定输入值集合上的尖锐程度表示。](Images/mldp_0305.png)'
- en: Figure 3-5\. The precision of the regression is indicated by the sharpness of
    the probability density function for a fixed set of input values.
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-5\. 回归的精度由概率密度函数在固定输入值集合上的尖锐程度表示。
- en: Restricting the prediction range
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 限制预测范围
- en: Another reason to reframe the problem is when it is essential to restrict the
    range of the prediction output. Let’s say, for example, that realistic output
    values for a regression problem are in the range [3, 20]. If we train a regression
    model where the output layer is a linear activation function, there is always
    the possibility that the model predictions will fall outside this range. One way
    to limit the range of the output is to reframe the problem.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重新构建问题的原因是限制预测输出范围的必要性。例如，对于回归问题，实际输出值的合理范围为 [3, 20]。如果我们训练一个输出层为线性激活函数的回归模型，模型预测可能会超出此范围。限制输出范围的一种方法是重新构建问题。
- en: 'Make the activation function of the last-but-one layer a sigmoid function (which
    is typically associated with classification) so that it is in the range [0,1]
    and have the last layer scale these values to the desired range:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 使倒数第二层的激活函数为 sigmoid 函数（通常与分类相关联），使其处于区间 [0,1]，并让最后一层将这些值缩放到期望的范围内：
- en: '[PRE1]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We can verify (see the [notebook](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/03_problem_representation/reframing.ipynb)
    on GitHub for full code) that this model now emits numbers in the range [3, 20].
    Note that because the output is a sigmoid, the model will never actually hit the
    minimum and maximum of the range, and only get quite close to it. When we trained
    the model above on some random data, we got values in the range [3.03, 19.99].
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以验证（请查看 GitHub 上的 [笔记本](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/03_problem_representation/reframing.ipynb)
    获取完整代码），此模型现在输出的数字范围为 [3, 20]。请注意，因为输出是 sigmoid 函数，模型实际上永远不会达到范围的最小值和最大值，只会非常接近。当我们对一些随机数据进行训练时，得到的值在范围
    [3.03, 19.99] 内。
- en: Label bias
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标签偏差
- en: Recommendation systems like matrix factorization can be reframed in the context
    of neural networks, both as a regression or classification. One advantage to this
    change of context is that a neural network framed as a regression or classification
    model can incorporate many more additional features outside of just the user and
    item embeddings learned in matrix factorization. So it can be an appealing alternative.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 像矩阵分解这样的推荐系统可以在神经网络的背景下重新构建，可以作为回归或分类的形式。这种背景变化的一个优点是，作为回归或分类模型的神经网络可以整合更多除了仅仅用户和物品嵌入之外的附加特征。因此，这可以是一个吸引人的替代选择。
- en: However, it is important to consider the nature of the target label when reframing
    the problem. For example, suppose we reframed our recommendation model to a classification
    task that predicts the likelihood a user will click on a certain video thumbnail.
    This seems like a reasonable reframing since our goal is to provide content a
    user will select and watch. But be careful. This change of label is not actually
    in line with our prediction task. By optimizing for user clicks, our model will
    inadvertently promote click bait and not actually recommend content of use to
    the user.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当重构问题时要考虑目标标签的性质是很重要的。例如，假设我们将我们的推荐模型重构为一个分类任务，预测用户点击某个视频缩略图的可能性。这似乎是一个合理的重构，因为我们的目标是提供用户选择和观看的内容。但要小心。这种标签的变化实际上并不符合我们的预测任务。通过优化用户点击，我们的模型将无意中促进点击诱饵，并不会真正推荐对用户有用的内容。
- en: Instead, a more advantageous label would be video watch time, reframing our
    recommendation as a regression instead. Or perhaps we can modify the classification
    objective to predict the likelihood that a user will watch at least half the video
    clip. There is often more than one suitable approach, and it is important to consider
    the problem holistically when framing a solution.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，一个更有利的标签是视频观看时间，将我们的推荐重新构建为一个回归任务。或者也许我们可以修改分类目标，预测用户至少观看视频片段一半的可能性。通常有多种适当的方法，当构建解决方案时综合考虑问题是很重要的。
- en: Warning
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Be careful when changing the label and training task of your machine learning
    model, as it can inadvertently introduce label bias into your solution. Consider
    again the example of video recommendation we discussed in [“Why It Works”](#why_it_works-id00304).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当改变机器学习模型的标签和训练任务时要小心，因为这可能会无意中引入标签偏差到你的解决方案中。考虑我们在[“为什么有效”](#why_it_works-id00304)中讨论的视频推荐示例。
- en: Multitask learning
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多任务学习
- en: One alternative to reframing is multitask learning. Instead of trying to choose
    between regression or classification, do both! Generally speaking, multitask learning
    refers to any machine learning model in which more than one loss function is optimized.
    This can be accomplished in many different ways, but the two most common forms
    of multi task learning in neural networks is through hard parameter sharing and
    soft parameter sharing.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 重构的另一种选择是多任务学习。不要试图在回归或分类之间选择，两者都做！一般来说，多任务学习是指任何一个优化多个损失函数的机器学习模型。这可以通过许多不同的方式实现，但神经网络中最常见的两种形式是硬参数共享和软参数共享。
- en: Parameter sharing refers to the parameters of the neural network being shared
    between the different output tasks, such as regression and classification. Hard
    parameter sharing occurs when the hidden layers of the model are shared between
    all the output tasks. In soft parameter sharing, each label has its own neural
    network with its own parameters, and the parameters of the different models are
    encouraged to be similar through some form of regularization. [Figure 3-6](#two_common_implementations_of_multitask)
    shows the typical architecture for hard parameter sharing and soft parameter sharing.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 参数共享指的是神经网络的参数在不同输出任务之间共享，比如回归和分类。硬参数共享发生在模型的隐藏层在所有输出任务之间共享时。在软参数共享中，每个标签有自己的神经网络和自己的参数，通过某种形式的正则化，不同模型的参数被鼓励保持相似。[图 3-6](#two_common_implementations_of_multitask)展示了硬参数共享和软参数共享的典型架构。
- en: '![Two common implementations of multitask learning are through hard parameter
    sharing and soft parameter sharing.](Images/mldp_0306.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![多任务学习的两种常见实现方式是硬参数共享和软参数共享。](Images/mldp_0306.png)'
- en: Figure 3-6\. Two common implementations of multitask learning are through hard
    parameter sharing and soft parameter sharing.
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-6\. 多任务学习的两种常见实现方式是硬参数共享和软参数共享。
- en: 'In this context, we could have two heads to our model: one to predict a regression
    output and another to predict classification output. For example, [this paper](https://oreil.ly/sIjsF)
    trains a computer vision model using a classification output of softmax probabilities
    together with a regression output to predict bounding boxes. They show that this
    approach achieves better performance than related work that trains networks separately
    for the classification and localization tasks. The idea is that through parameter
    sharing, the tasks are learned simultaneously and the gradient updates from the
    two loss functions inform both outputs and result in a more generalizable model.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们的模型可以有两个头部：一个用于预测回归输出，另一个用于预测分类输出。例如，[这篇论文](https://oreil.ly/sIjsF)使用分类输出的softmax概率训练计算机视觉模型，同时使用回归输出预测边界框。他们表明，这种方法比单独为分类和定位任务训练网络的相关工作表现更好。其核心思想是通过参数共享，同时学习任务，并且两个损失函数的梯度更新会影响两个输出，从而产生更具泛化能力的模型。
- en: 'Design Pattern 6: Multilabel'
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计模式 6：多标签
- en: The Multilabel design pattern refers to problems where we can assign *more than
    one* label to a given training example. For neural networks, this design requires
    changing the activation function used in the final output layer of the model and
    choosing how our application will parse model output. Note that this is different
    from *multiclass* classification problems, where a single example is assigned
    exactly one label from a group of many (> 1) possible classes. You may also hear
    the Multilabel design pattern referred to as *multilabel, multiclass classification*
    since it involves choosing more than one label from a group of more than one possible
    class.When discussing this pattern, we’ll focus primarily on neural networks.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签设计模式指的是我们可以为给定的训练样本分配*多个*标签的问题。对于神经网络，这种设计需要改变模型最终输出层中使用的激活函数，并选择我们的应用程序如何解析模型输出。请注意，这与*多类别*分类问题不同，后者是指从许多（>
    1）可能的类别中为单个示例分配一个标签。您可能还会听到多标签设计模式称为*多标签，多类别分类*，因为它涉及从多个可能的类别中选择多个标签。在讨论此模式时，我们将主要关注神经网络。
- en: Problem
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Often, model prediction tasks involve applying a single classification to a
    given training example. This prediction is determined from *N* possible classes
    where *N* is greater than 1\. In this case, it’s common to use softmax as the
    activation function for the output layer. Using softmax, the output of our model
    is an N-element array, where the sum of all the values adds up to 1\. Each value
    indicates the probability that a particular training example is associated with
    the class at that index.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，模型预测任务涉及为给定的训练示例应用单一分类。此预测是从*N*个可能的类别中确定，其中*N*大于1。在这种情况下，常见做法是使用softmax作为输出层的激活函数。使用softmax，我们模型的输出是一个N元素数组，其中所有值的总和为1。每个值表示特定训练示例与该索引处类别相关的概率。
- en: 'For example, if our model is classifying images as cats, dogs, or rabbits,
    the softmax output might look like this for a given image: [`.89`, `.02`, `.09`].
    This means our model is predicting an 89% chance the image is a cat, 2% chance
    it’s a dog, and 9% chance it’s a rabbit. Because each image can have *only one
    possible label* in this scenario, we can take the argmax (index of the highest
    probability) to determine our model’s predicted class. The less-common scenario
    is when each training example can be assigned *more than one* label, which is
    what this pattern addresses.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们的模型将图像分类为猫、狗或兔子，则给定图像的softmax输出可能如下所示：[`.89`, `.02`, `.09`]。这意味着我们的模型预测图像是猫的概率为89%，是狗的概率为2%，是兔子的概率为9%。在这种情况下，每个图像只能有*一个可能的标签*，我们可以使用argmax（最高概率的索引）来确定模型预测的类别。较少见的场景是每个训练示例可以分配*多个*标签，这正是这种模式要解决的。
- en: The Multilabel design pattern exists for models trained on all data modalities.
    For image classification, in the earlier cat, dog, rabbit example, we could instead
    use training images that each depicted *multiple* animals, and could therefore
    have multiple labels. For text models, we can imagine a few scenarios where text
    can be labeled with multiple tags. Using the dataset of Stack Overflow questions
    on BigQuery as an example, we could build a model to predict the tags associated
    with a particular question. As an example, the question “How do I plot a pandas
    DataFrame?” could be tagged as “Python,” “pandas,” and “visualization.” Another
    multilabel text classification example is a model that identifies toxic comments.
    For this model, we might want to flag comments with multiple toxicity labels.
    A comment could therefore be labeled both “hateful” and “obscene.”
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签设计模式适用于在所有数据模态上训练的模型。对于图像分类，在前述猫、狗、兔子的示例中，我们可以使用每个描绘 *多个* 动物的训练图像，因此可以有多个标签。对于文本模型，我们可以想象几种情况，其中文本可以用多个标签进行标记。以
    BigQuery 上的 Stack Overflow 问题数据集为例，我们可以构建一个模型来预测与特定问题相关联的标签。例如，问题“如何绘制 pandas
    DataFrame？”可以被标记为“Python”、“pandas”和“visualization”。另一个多标签文本分类的例子是识别有毒评论的模型。对于这种模型，我们可能希望标记同时具有多个毒性标签的评论。因此，评论可能被标记为“恶意”和“淫秽”。
- en: This design pattern can also apply to tabular datasets. Imagine a healthcare
    dataset with various physical characteristics for each patient, like height, weight,
    age, blood pressure, and more. This data could be used to predict the presence
    of multiple conditions. For example, a patient could show risk of both heart disease
    and diabetes.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设计模式也适用于表格数据集。想象一下，一个包含各种患者的健康数据集，如身高、体重、年龄、血压等。这些数据可以用来预测多种病症的存在。例如，一个患者可能显示出患心脏病和糖尿病的风险。
- en: Solution
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: The solution for building models that can assign *more than one label* to a
    given training example is to use the *sigmoid* activation function in our final
    output layer. Rather than generating an array where all values sum to 1 (as in
    softmax), each *individual* value in a sigmoid array is a float between 0 and
    1\. That is to say, when implementing the Multilabel design pattern, our label
    needs to be multi-hot encoded. The length of the multi-hot array corresponds with
    the number of classes in our model, and each output in this label array will be
    a sigmoid value.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建能够给定训练示例分配 *多个标签* 的模型解决方案，我们在最终输出层使用 *Sigmoid* 激活函数。与生成所有值总和为1的数组（如Softmax）不同，Sigmoid
    数组中的每个 *独立* 值是介于0和1之间的浮点数。也就是说，在实现多标签设计模式时，我们的标签需要进行多热编码。多热数组的长度对应于模型中类的数量，标签数组中的每个输出将是一个
    Sigmoid 值。
- en: 'Building on the image example above, let’s say our training dataset included
    images with more than one animal. The sigmoid output for an image that contained
    a cat and a dog but not a rabbit might look like the following: [`.92`, `.85`,
    `.11`]. This output means the model is 92% confident the image contains a cat,
    85% confident it contains a dog, and 11% confident it contains a rabbit.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 延续上面的图像示例，假设我们的训练数据集包含了多种动物的图像。对于包含猫和狗但不包含兔子的图像，其 Sigmoid 输出可能如下所示：[`.92`, `.85`,
    `.11`]。这个输出意味着模型对图像包含猫的信心为92%，包含狗的信心为85%，包含兔子的信心为11%。
- en: 'A version of this model for 28×28-pixel images with sigmoid output might look
    like this, using the Keras `Sequential` API:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有 28×28 像素图像的模型的一个版本，其 Sigmoid 输出可能如下所示，使用 Keras `Sequential` API：
- en: '[PRE2]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The main difference in output between the sigmoid model here and the softmax
    example in the Problem section is that the softmax array is guaranteed to contain
    three values that sum to 1, whereas the sigmoid output will contain three values,
    each between 0 and 1.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 Sigmoid 模型与问题部分的 Softmax 示例之间的主要输出差异在于，Softmax 数组保证包含三个值，其总和为1，而 Sigmoid
    输出将包含三个值，每个值介于0和1之间。
- en: Trade-Offs and Alternatives
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 折衷与替代方案
- en: There are several special cases to consider when following the Multilabel design
    pattern and using sigmoid output. Next, we’ll explore how to structure models
    that have two possible label classes, how to make sense of sigmoid results, and
    other important considerations for Multilabel models.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在遵循多标签设计模式和使用 Sigmoid 输出时，有几个特殊情况需要考虑。接下来，我们将探讨如何构建具有两个可能标签类别的模型，如何理解 Sigmoid
    结果，以及多标签模型的其他重要考虑因素。
- en: Sigmoid output for models with two classes
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 两类模型的 Sigmoid 输出
- en: 'There are two types of models where the output can belong to two possible classes:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种类型的模型，其输出可以属于两种可能的类别：
- en: Each training example can be assigned *only one* class. This is also called
    *binary classification* and is a special type of multiclass classification problem.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个训练示例只能分配*一个*类别。这也称为*二元分类*，是一种特殊类型的多类分类问题。
- en: Some training examples could belong to *both* classes. This is a type of *multilabel
    classification* problem.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些训练示例可能属于*两种*类别。这是一种*多标签分类*问题。
- en: '[Figure 3-8](#understanding_the_distinction_between_m) shows the distinction
    between these classifications.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-8](#理解多类、多标签和二分类问题之间的区别)展示了这些分类之间的区别。'
- en: '![Understanding the distinction between multiclass, multilabel, and binary
    classification problems.](Images/mldp_0308.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![理解多类、多标签和二分类问题之间的区别。](Images/mldp_0308.png)'
- en: Figure 3-8\. Understanding the distinction between multiclass, multilabel, and
    binary classification problems.
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-8\. 理解多类、多标签和二分类问题之间的区别。
- en: 'The first case (binary classification) is unique in that it is the only type
    of single-label classification problem where we would consider using sigmoid as
    our activation function. For nearly any other multiclass classification problem
    (for example, classifying text into one of five possible categories), we would
    use softmax. However, when we only have two classes, softmax is redundant. Take
    for example a model that predicts whether or not a specific transaction is fraudulent.
    Had we used a softmax output in this example, here’s what a fraudulent model prediction
    might look like:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个案例（二分类）在于它是唯一一个我们会考虑使用sigmoid作为激活函数的单标签分类问题。对于几乎任何其他多类分类问题（例如，将文本分类为五个可能的类别之一），我们会使用softmax。然而，当我们只有两类时，softmax是多余的。例如，考虑一个模型，预测特定交易是否欺诈。如果我们在这个例子中使用softmax输出，那么一个欺诈模型的预测可能会像这样：
- en: '[PRE3]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this example, the first index corresponds with “not fraudulent” and the second
    index corresponds with “fraudulent.” This is redundant because we could also represent
    this with a single scalar value, and thus use a sigmoid output. The same prediction
    could be represented as simply `.98`. Because each input can only be assigned
    a single class, we can infer from this output of `.98` that the model has predicted
    a 98% chance of fraud and a 2% chance of nonfraud.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，第一个索引对应“非欺诈”，第二个索引对应“欺诈”。这是多余的，因为我们也可以用单一标量值表示，并因此使用sigmoid输出。相同的预测可以简单地表示为`.98`。由于每个输入只能分配一个类别，我们可以从`.98`的输出推断出，模型预测欺诈的可能性为98%，非欺诈的可能性为2%。
- en: 'Therefore, for binary classification models, it is optimal to use an output
    shape of `1` with a sigmoid activation function. Models with a single output node
    are also more efficient, since they will have fewer trainable parameters and will
    likely train faster. Here is what the output layer of a binary classification
    model would look like:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于二分类模型，使用输出形状为`1`和sigmoid激活函数是最优的。具有单个输出节点的模型也更有效率，因为其可训练参数较少，很可能训练速度更快。以下是二分类模型的输出层示意图：
- en: '[PRE4]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For the second case where a training example could belong to *both possible
    classes* and fits into the Multilabel design pattern, we’ll also want to use sigmoid,
    this time with a two-element output:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二种情况，其中一个训练示例可能属于*两种可能的类别*并且符合多标签设计模式，我们还将使用sigmoid，这次是具有两个元素的输出：
- en: '[PRE5]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Which loss function should we use?
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们应该使用哪种损失函数呢？
- en: 'Now that we know when to use sigmoid as an activation function in our model,
    how should we choose which loss function to use with it? For the binary classification
    case where our model has a one-element output, use binary cross-entropy loss.
    In Keras, we provide a loss function when we compile our model:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道在模型中何时使用sigmoid作为激活函数，接下来应该选择哪种损失函数呢？对于二分类情况，当我们的模型具有单元素输出时，使用二元交叉熵损失函数。在Keras中，我们在编译模型时指定损失函数：
- en: '[PRE6]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Interestingly, we also use binary cross-entropy loss for multilabel models with
    sigmoid output. This is because, as shown in [Figure 3-9](#understanding_the_multilabel_pattern_by),
    a multilabel problem with three classes is essentially three smaller binary classification
    problems.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，对于具有sigmoid输出的多标签模型，我们也使用二元交叉熵损失。这是因为，如图 3-9 所示，具有三个类别的多标签问题本质上是三个较小的二分类问题。
- en: '![Understanding the Multilabel pattern by breaking down the problem into smaller
    binary classification tasks.](Images/mldp_0309.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![通过将问题分解为较小的二元分类任务来理解多标签模式。](Images/mldp_0309.png)'
- en: Figure 3-9\. Understanding the Multilabel pattern by breaking down the problem
    into smaller binary classification tasks.
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-9。通过将问题分解为较小的二元分类任务来理解多标签模式。
- en: Parsing sigmoid results
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解析 sigmoid 结果
- en: To extract the predicted label for a model with softmax output, we can simply
    take the argmax (highest value index) of the output array to get the predicted
    class. Parsing sigmoid outputs is less straightforward. Instead of taking the
    class with the highest predicted probability, we need to evaluate the probability
    of each class in our output layer and consider the probability *threshold* for
    our use case. Both of these choices are largely dependent on the end user application
    of our model.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 要提取具有 softmax 输出模型的预测标签，我们可以简单地取输出数组的 argmax（最高值索引）来获取预测类别。解析 sigmoid 输出则不那么直接。我们不是取预测概率最高的类别，而是需要评估输出层中每个类别的概率，并考虑我们用例的概率*阈值*。这两种选择在很大程度上取决于我们模型的最终用户应用。
- en: Note
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: By threshold, we’re referring to the probability we’re comfortable with for
    confirming an input belongs to a particular class. For example, if we’re building
    a model to classify different types of animals in images, we might be comfortable
    saying an image has a cat even if the model is only 80% confident the image contains
    a cat. Alternatively, if we’re building a model that’s making healthcare predictions,
    we’ll likely want the model to be closer to 99% confident before confirming a
    specific medical condition is present or not. While thresholding is something
    we’ll need to consider for any type of classification model, it’s especially relevant
    to the Multilabel design pattern since we’ll need to determine thresholds for
    each class and they may be different.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 通过*阈值*，我们指的是我们对确认输入属于特定类别的概率感到满意的程度。例如，如果我们正在构建一个模型来分类图像中的不同动物类型，即使模型只有80%的信心图像包含猫，我们可能也会确认图像中有一只猫。另外，如果我们正在构建一个进行医疗预测的模型，我们可能希望在确认特定医疗状况是否存在之前，模型的信心水平接近99%。虽然对于任何类型的分类模型都需要考虑阈值设定，但它对多标签设计模式尤为重要，因为我们需要为每个类别确定阈值，而这些阈值可能不同。
- en: 'To look at a specific example, let’s take the Stack Overflow dataset in BigQuery
    and use it to build a model that predicts the tags associated with a Stack Overflow
    question given its title. We’ll limit our dataset to questions that contain only
    five tags to keep things simple:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看一个具体的例子，让我们使用 BigQuery 中的 Stack Overflow 数据集构建一个模型，该模型可以根据问题的标题预测与 Stack
    Overflow 问题相关联的标签。我们将限制我们的数据集仅包含五个标签的问题，以保持简单。
- en: '[PRE7]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output layer of our model would look like the following (full code for
    this section is available in the [GitHub repository](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/03_problem_representation/multilabel.ipynb)):'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的输出层如下所示（本节的完整代码在[GitHub 代码库](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/03_problem_representation/multilabel.ipynb)中可用）：
- en: '[PRE8]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s take the Stack Overflow question *“*What is the definition of a non-trainable
    parameter?*”* as an input example. Assuming our output indices correspond with
    the order of tags in our query, an output for that question might look like this:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以 Stack Overflow 的问题*“*什么是不可训练参数的定义？*”*作为输入示例。假设我们的输出索引与查询中标签的顺序相对应，那么该问题的输出可能如下所示：
- en: '[PRE9]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Our model is 95% confident this question should be tagged Keras, and 83% confident
    it should be tagged TensorFlow. When evaluating model predictions, we’ll need
    to iterate over every element in the output array and determine how we want to
    display those results to our end users. If 80% is our threshold for all tags,
    we’d show `Keras` *and* `TensorFlow` associated with this question. Alternatively,
    maybe we want to encourage users to add as many tags as possible and we want to
    show options for any tag with prediction confidence above 50%.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型对该问题应该被标记为 Keras 有95%的信心，应该被标记为 TensorFlow 有83%的信心。在评估模型预测时，我们需要遍历输出数组中的每个元素，并确定如何向最终用户显示这些结果。如果我们所有标签的阈值是80%，我们将显示与该问题相关联的`Keras`和`TensorFlow`。或者，也许我们希望鼓励用户尽可能添加更多标签，并且我们希望为任何预测置信度超过50%的标签显示选项。
- en: For examples like this one, where the goal is primarily to suggest possible
    tags with less emphasis on getting the tag *exactly* right, a typical rule of
    thumb is to use `n_specific_tag` / `n_total_examples` as a threshold for each
    class. Here, `n_specific_tag` is the number of examples with one tag in the dataset
    (for example, “pandas”), and `n_total_examples` is the total number of examples
    in the training set across all tags. This ensures that the model is doing better
    than guessing a certain label based on its occurrence in the training dataset.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像这样的示例，主要目标是提出可能的标签建议，而不是强调准确获取标签，一个典型的经验法则是对每个类别使用`n_specific_tag` / `n_total_examples`作为阈值。这里，`n_specific_tag`是数据集中具有一个标签的示例数量（例如，“pandas”），而`n_total_examples`是训练集中所有标签的总样本数。这确保模型比基于训练数据集中标签出现频率来猜测某个标签要好。
- en: Tip
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: For a more precise approach to thresholding, consider using S-Cut or optimizing
    for your model’s F-measure. Details on both can be found in [this paper](https://oreil.ly/oyR57).
    Calibrating the per-label probabilities is often helpful as well, especially when
    there are thousands of labels and you want to consider the top K of them (this
    is common in search and ranking problems).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更精确的阈值处理方法，可以考虑使用S-Cut或优化模型的F-measure。关于这两者的详细信息可以在[这篇论文](https://oreil.ly/oyR57)中找到。调整每个标签的概率通常也很有帮助，特别是在存在成千上万个标签并且想要考虑它们的前K个时（这在搜索和排名问题中很常见）。
- en: As you’ve seen, multilabel models provide more flexibility in how we parse predictions
    and require us to think carefully about the output for each class.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，多标签模型在我们解析预测时提供了更大的灵活性，并且需要我们仔细考虑每个类别的输出。
- en: Dataset considerations
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集考虑事项
- en: When dealing with single-label classification tasks, we can ensure our dataset
    is balanced by aiming for a relatively equal number of training examples for each
    class. Building a balanced dataset is more nuanced for the Multilabel design pattern.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理单标签分类任务时，我们可以通过确保数据集中每个类别的训练样本数量相对均衡来保持数据集的平衡。构建平衡的数据集对于多标签设计模式来说更加微妙。
- en: Taking the Stack Overflow dataset example, there will likely be many questions
    tagged as both `TensorFlow` and `Keras`. But there will also be questions about
    Keras that have nothing to do with TensorFlow. Similarly, we might see questions
    about plotting data that is tagged with both `matplotlib` and `pandas`, and questions
    about data preprocessing that are tagged both `pandas` and `scikit-learn`. In
    order for our model to learn what is unique to each tag, we’ll want to ensure
    the training dataset consists of varied combinations of each tag. If the majority
    of `matplotlib` questions in our dataset are also tagged `pandas`, the model won’t
    learn to classify `matplotlib` on its own. To account for this, think about the
    different relationships between labels that might be present in our model and
    count the number of training examples that belong to each overlapping combination
    of labels.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 以Stack Overflow数据集为例，可能会有许多问题同时被标记为`TensorFlow`和`Keras`。但也会有关于Keras的问题，与TensorFlow无关。同样地，我们可能会看到关于用`matplotlib`和`pandas`标记的数据绘图问题，以及关于数据预处理的问题，同时被标记为`pandas`和`scikit-learn`。为了使我们的模型学习到每个标签的独特内容，我们需要确保训练数据集包含各种标签组合。如果我们的数据集中大多数关于`matplotlib`的问题也同时被标记为`pandas`，那么模型将无法单独学习分类`matplotlib`。为了解决这个问题，考虑我们的模型中可能存在的标签之间的不同关系，并计算属于每个标签重叠组合的训练示例数量。
- en: 'When exploriing relationships between labels in our dataset, we may also encounter
    hierarchical labels. [ImageNet](https://oreil.ly/0VXtc), the popular image classification
    dataset, contains thousands of labeled images and is often used as a starting
    point for transfer learning on image models. All of the labels used in ImageNet
    are hierarchical, meaning all images have at least one label, and many images
    have more specific labels that are part of a hierarchy. Here’s an example of one
    label hierarchy in ImageNet:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索数据集中标签之间的关系时，我们可能还会遇到层次标签。[ImageNet](https://oreil.ly/0VXtc)，这个流行的图像分类数据集，包含数千个带标签的图像，并且通常作为图像模型迁移学习的起点。ImageNet中所有的标签都是层次化的，这意味着所有的图像至少有一个标签，而许多图像还具有更具体的层次标签，这些标签构成了一个层次结构。以下是ImageNet中一个标签层次结构的示例：
- en: animal → invertebrate → arthropod → arachnid → spider
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 动物 → 无脊椎动物 → 节肢动物 → 蜘蛛 → 蜘蛛
- en: 'Depending on the size and nature of the dataset, there are two common approaches
    for handling hierarchical labels:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 根据数据集的大小和特性，处理层次标签通常有两种常见方法：
- en: Use a flat approach and put every label in the same output array regardless
    of hierarchy, making sure you have enough examples of each “leaf node” label.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用扁平方法，并将每个“叶节点”标签放入相同的输出数组中，确保每个标签都有足够的示例。
- en: Use the Cascade design pattern. Build one model to identify higher-level labels.
    Based on the high-level classification, send the example to a separate model for
    a more specific classification task. For example, we might have an initial model
    that labels images as “Plant,” “Animal,” or “Person.” Depending on which labels
    the first model applies, we’ll send the image to different model(s) to apply more
    granular labels like “succulent” or “barbary lion.”
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用级联设计模式。构建一个模型来识别高级标签。根据高级分类，将示例发送到另一个模型进行更具体的分类任务。例如，我们可能有一个初始模型，将图像标记为“植物”、“动物”或“人物”。根据第一个模型应用的标签，将图像发送到不同的模型来应用更细粒度的标签，如“多肉植物”或“巴巴里狮”。
- en: The flat approach is more straightforward than following the Cascade design
    pattern since it only requires one model. However, this might cause the model
    to lose information about more detailed label classes since there will naturally
    be more training examples with the higher-level labels in our dataset.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 与级联设计模式相比，扁平方法更为直接，因为它只需要一个模型。然而，这可能会导致模型丢失有关更详细标签类的信息，因为我们的数据集中自然会有更多具有高级标签的训练示例。
- en: Inputs with overlapping labels
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 具有重叠标签的输入
- en: The Multilabel design pattern is also useful in cases where input data occasionally
    has overlapping labels. Let’s take an image model that’s classifying clothing
    items for a catalog as an example. If we have multiple people labeling each image
    in the training dataset, one labeler may label an image of a skirt as “maxi skirt,”
    while another identifies it as “pleated skirt.” Both are correct. However, if
    we build a multiclass classification model on this data, passing it multiple examples
    of the same image with different labels, we’ll likely encounter situations where
    the model labels similar images differently when making predictions. Ideally,
    we want a model that labels this image as both “maxi skirt” and “pleated skirt”
    as seen in [Figure 3-10](#using_input_from_multiple_labelers_to_c), rather than
    sometimes predicting only one of these labels.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签设计模式在输入数据偶尔具有重叠标签的情况下也非常有用。举个例子，我们拿服装分类目录的图像模型来说。如果我们训练数据集中的每个图像都有多个人标记，一个标注者可能会将一张裙子的图像标记为“长裙”，而另一个标记为“褶皱裙”。这两种标记都是正确的。然而，如果我们在这些数据上构建一个多类别分类模型，将具有不同标签的同一图像多次输入，当进行预测时可能会遇到模型在标记相似图像时产生不同预测的情况。理想情况下，我们希望模型像在[图 3-10](#using_input_from_multiple_labelers_to_c)中看到的那样，将这张图像标记为“长裙”和“褶皱裙”，而不是仅预测其中一种标签。
- en: '![Using input from multiple labelers to create overlapping labels in cases
    where multiple descriptions of an item are correct.](Images/mldp_0310.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![使用多个标注者的输入创建重叠标签，以处理多个对同一项的描述都正确的情况。](Images/mldp_0310.png)'
- en: Figure 3-10\. Using input from multiple labelers to create overlapping labels
    in cases where multiple descriptions of an item are correct.
  id: totrans-144
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-10\. 使用多个标注者的输入创建重叠标签，以处理多个对同一项的描述都正确的情况。
- en: The Multilabel design pattern solves this by allowing us to associate both overlapping
    labels with an image. In cases with overlapping labels where we have multiple
    labelers evaluating each image in our training dataset, we can choose the maximum
    number of labels we’d like labelers to assign to a given image, then take the
    most commonly chosen tags to associate with an image during training. The threshold
    for “most commonly chosen tags” will depend on our prediction task and the number
    of human labelers we have. For example, if we have 5 labelers evaluating every
    image and 20 possible tags for each image, we might encourage labelers to give
    each image 3 tags. From this list of 15 label “votes” per image, we could then
    choose the 2 to 3 with the most votes from the labelers. When evaluating this
    model, we need to take note of the average prediction confidence the model returns
    for each label and use this to iteratively improve our dataset and label quality.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签设计模式通过允许我们将重叠标签与图像关联来解决这个问题。在具有多个标签器评估训练数据集中每个图像的重叠标签的情况下，我们可以选择我们希望标签器为给定图像分配的最大标签数，然后在训练期间选择最常选择的标签与图像关联。对于“最常选择的标签”，阈值将取决于我们的预测任务和我们有多少人类标签器。例如，如果我们有5个标签器评估每个图像，并且每个图像有20个可能的标签，我们可能鼓励标签器为每个图像提供3个标签。从每个图像的这15个标签“投票”列表中，我们可以选择具有最多投票的2到3个标签。在评估此模型时，我们需要注意模型对每个标签返回的平均预测置信度，并使用这些数据迭代地改进我们的数据集和标签质量。
- en: One versus rest
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一对多
- en: 'Another technique for handling Multilabel classification is to train multiple
    binary classifiers instead of one multilabel model. This approach is called *one
    versus rest*. In the case of the Stack Overflow example where we want to tag questions
    as TensorFlow, Python, and pandas, we’d train an individual classifier for each
    of these three tags: Python or not, TensorFlow or not, and so forth. Then we’d
    choose a confidence threshold and tag the original input question with tags from
    each binary classifier above some threshold.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 处理多标签分类的另一种技术是训练多个二元分类器而不是一个多标签模型。这种方法称为*一对多*。在Stack Overflow示例中，我们想要将问题标记为TensorFlow、Python和pandas时，我们会为这三个标签分别训练一个单独的分类器：Python或非Python，TensorFlow或非TensorFlow等。然后，我们会选择一个置信度阈值，并使用每个二元分类器在某个阈值以上标记原始输入问题的标签。
- en: The benefit of one versus rest is that we can use it with model architectures
    that can only do binary classification, like SVMs. It may also help with rare
    categories since the model will be performing only one classification task at
    a time on each input, and it is possible to apply the Rebalancing design pattern.
    The disadvantage of this approach is the added complexity of training many different
    classifiers, requiring us to build our application in a way that generates predictions
    from each of these models rather than having just one.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一对多的好处在于我们可以将其与只能进行二元分类的模型架构（如SVM）一起使用。它也可能有助于处理罕见的类别，因为模型每次只对每个输入执行一个分类任务，可以应用重新平衡设计模式。这种方法的缺点在于训练许多不同分类器增加了复杂性，需要我们构建应用程序以一种生成来自每个模型的预测的方式，而不是只有一个模型。
- en: 'To summarize, use the Multilabel design pattern when your data falls into any
    of the following classification scenarios:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，在您的数据属于以下任何分类场景时，请使用多标签设计模式：
- en: A single training example can be associated with mutually exclusive labels.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个训练示例可以与互斥标签相关联。
- en: A single training example can have many hierarchical labels.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个训练示例可以具有许多层次标签。
- en: Labelers describe the same item in different ways, and each interpretation is
    accurate.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签器以不同方式描述同一项，并且每种解释都是准确的。
- en: When implementing a multilabel model, ensure combinations of overlapping labels
    are well represented in your dataset, and consider the threshold values you’re
    willing to accept for each possible label in your model. Using a sigmoid output
    layer is the most common approach for building models that can handle multilabel
    classification. Additionally, sigmoid output can also be applied to binary classification
    tasks where a training example can have only one out of two possible labels.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 当实现多标签模型时，请确保您的数据集中充分代表了重叠标签的组合，并考虑您在模型中愿意接受的每个可能标签的阈值值。使用sigmoid输出层是构建能够处理多标签分类的模型的最常见方法。此外，sigmoid输出也可以应用于只能有两种可能标签之一的二元分类任务。
- en: 'Design Pattern 7: Ensembles'
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计模式 7：集成
- en: The Ensembles design pattern refers to techniques in machine learning that combine
    multiple machine learning models and aggregate their results to make predictions.
    Ensembles can be an effective means to improve performance and produce predictions
    that are better than any single model.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 集成设计模式指的是机器学习中的一种技术，它结合多个机器学习模型并聚合它们的结果来进行预测。集成可以有效提高性能，产生比任何单一模型更好的预测结果。
- en: Problem
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Suppose we’ve trained our baby weight prediction model, engineering special
    features and adding additional layers to our neural network so that the error
    on our training set is nearly zero. Excellent, you say! However, when we look
    to use our model in production at the hospital or evaluate performance on the
    hold out test set, our predictions are all wrong. What happened? And, more importantly,
    how can we fix it?
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经训练了我们的婴儿体重预测模型，工程师设计了特殊特征并添加了额外的神经网络层，以使训练集上的错误几乎为零。太棒了，你说！然而，当我们试图在医院生产环境中使用我们的模型或评估保留测试集的性能时，我们的预测全都错了。发生了什么？更重要的是，我们如何修复它？
- en: 'No machine learning model is perfect. To better understand where and how our
    model is wrong, the error of an ML model can be broken down into three parts:
    the irreducible error, the error due to bias, and the error due to variance. The
    irreducible error is the inherent error in the model resulting from noise in the
    dataset, the framing of the problem, or bad training examples, like measurement
    errors or confounding factors. Just as the name implies, we can’t do much about
    *irreducible error*.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 没有机器学习模型是完美的。为了更好地理解我们的模型错在哪里以及如何修复，一个ML模型的错误可以分解为三个部分：不可减少的错误、由于偏差而引起的错误以及由于方差而引起的错误。不可减少的错误是模型固有的错误，由数据集中的噪声、问题的框架或糟糕的训练示例（如测量错误或混淆因素）引起。正如其名，我们对*不可减少的错误*无能为力。
- en: The other two, the bias and the variance, are referred to as the *reducible
    error*, and here is where we can influence our model’s performance. In short,
    the bias is the model’s inability to learn enough about the relationship between
    the model’s features and labels, while the variance captures the model’s inability
    to generalize on new, unseen examples. A model with high bias oversimplifies the
    relationship and is said to be *underfit*. A model with high variance has learned
    too much about the training data and is said to be *overfit*. Of course, the goal
    of any ML model is to have low bias and low variance, but in practice, it is hard
    to achieve both. This is known as the bias–variance trade-off. We can’t have our
    cake and eat it too. For example, increasing model complexity decreases bias but
    increases variance, while decreasing model complexity decreases variance but introduces
    more bias.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 另外两个，偏差和方差，被称为*可减少的错误*，这里是我们可以影响模型性能的地方。简而言之，偏差是模型在模型特征和标签之间学习不足的能力，而方差捕捉模型在新的、未见的示例上泛化能力不足的问题。高偏差的模型过度简化了关系，被称为*欠拟合*。高方差的模型对训练数据学习过多，被称为*过拟合*。当然，任何ML模型的目标是既低偏差又低方差，但在实践中，同时实现这两个是困难的。这被称为偏差-方差权衡。我们不能两全其美。例如，增加模型复杂性会降低偏差但增加方差，而减少模型复杂性会降低方差但引入更多偏差。
- en: '[Recent work](https://oreil.ly/PxUvs) suggests that when using modern machine
    learning techniques such as large neural networks with high capacity, this behavior
    is valid only up to a point. In observed experiments, there is an “interpolation
    threshold” beyond which very high capacity models are able to achieve zero training
    error as well as low error on unseen data. Of course, we need much larger datasets
    in order to avoid overfitting on high-capacity models.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[最近的研究工作](https://oreil.ly/PxUvs)表明，当使用现代机器学习技术，如具有高容量的大型神经网络时，这种行为只在某个阈值之内有效。在观察的实验中，存在一个“插值阈值”，超过这个阈值，非常高容量的模型能够在训练错误为零的同时，在未见数据上达到低错误率。当然，我们需要更大的数据集来避免在高容量模型上过拟合。'
- en: Is there a way to mitigate this bias–variance trade-off on small- and medium-scale
    problems?
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 是否有办法在小规模和中等规模问题上减轻这种偏差-方差权衡呢？
- en: Solution
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: '*Ensemble methods* are meta-algorithms that combine several machine learning
    models as a technique to decrease the bias and/or variance and improve model performance.
    Generally speaking, the idea is that combining multiple models helps to improve
    the machine learning results. By building several models with different inductive
    biases and aggregating their outputs, we hope to get a model with better performance.
    In this section, we’ll discuss some commonly used ensemble methods, including
    bagging, boosting, and stacking.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*集成方法* 是一种元算法，它将多个机器学习模型结合起来，以减少偏差和/或方差，并提高模型性能。一般来说，这种方法的思想是通过结合多个模型来改善机器学习结果。通过构建具有不同归纳偏差的多个模型并聚合它们的输出，我们希望得到性能更好的模型。在本节中，我们将讨论一些常用的集成方法，包括
    Bagging、Boosting 和 Stacking。'
- en: Bagging
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Bagging
- en: Bagging (short for bootstrap aggregating) is a type of parallel ensembling method
    and is used to address high variance in machine learning models. The bootstrap
    part of bagging refers to the datasets used for training the ensemble members.
    Specifically, if there are *k* submodels, then there are *k* separate datasets
    used for training each submodel of the ensemble. Each dataset is constructed by
    randomly sampling (with replacement) from the original training dataset. This
    means there is a high probability that any of the *k* datasets will be missing
    some training examples, but also any dataset will likely have repeated training
    examples. The aggregation takes place on the output of the multiple ensemble model
    members—either an average in the case of a regression task or a majority vote
    in the case of classification.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging（即自助聚合）是一种并行集成方法，用于解决机器学习模型中的高方差问题。Bagging 中的自助部分指的是用于训练集成成员的数据集。具体来说，如果有
    *k* 个子模型，则使用 *k* 个单独的数据集来训练集成的每个子模型。每个数据集通过从原始训练数据集中随机抽样（有放回地）构建而成。这意味着 *k* 个数据集中有很高的概率会缺少一些训练样本，但也可能会有重复的训练样本。聚合操作在多个集成模型成员的输出上进行——在回归任务中通常是平均值，在分类任务中通常是多数投票。
- en: 'A good example of a bagging ensemble method is the random forest: multiple
    decision trees are trained on randomly sampled subsets of the entire training
    data, then the tree predictions are aggregated to produce a prediction, as shown
    in [Figure 3-11](#bagging_is_good_for_decreasing_variance).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的 Bagging 集成方法的例子是随机森林：多个决策树在整个训练数据的随机抽样子集上进行训练，然后聚合树的预测结果以生成预测，如在[图 3-11](#bagging_is_good_for_decreasing_variance)中所示。
- en: '![Bagging is good for decreasing variance in machine learning model output.](Images/mldp_0311.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![Bagging 对于减少机器学习模型输出的方差非常有效。](Images/mldp_0311.png)'
- en: Figure 3-11\. Bagging is good for decreasing variance in machine learning model
    output.
  id: totrans-168
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-11\. Bagging 对于减少机器学习模型输出的方差非常有效。
- en: 'Popular machine learning libraries have implementations of bagging methods.
    For example, to implement a random Forest regression in scikit-learn to predict
    baby weight from our natality dataset:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 流行的机器学习库中都有 Bagging 方法的实现。例如，在 scikit-learn 中实现随机森林回归，以预测我们的出生数据集中婴儿的体重：
- en: '[PRE10]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Model averaging as seen in bagging is a powerful and reliable method for reducing
    model variance. As we’ll see, different ensemble methods combine multiple submodels
    in different ways, sometimes using different models, different algorithms, or
    even different objective functions. With bagging, the model and algorithms are
    the same. For example, with random forest, the submodels are all short decision
    trees.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Bagging 中看到的模型平均化是一种强大且可靠的降低模型方差的方法。正如我们将看到的，不同的集成方法以不同的方式结合多个子模型，有时使用不同的模型、不同的算法或甚至不同的目标函数。在
    Bagging 中，模型和算法都是相同的。例如，在随机森林中，子模型都是短决策树。
- en: Boosting
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Boosting
- en: Boosting is another Ensemble technique. However, unlike bagging, boosting ultimately
    constructs an ensemble model with *more* capacity than the individual member models.
    For this reason, boosting provides a more effective means of reducing bias than
    variance. The idea behind boosting is to iteratively build an ensemble of models
    where each successive model focuses on learning the examples the previous model
    got wrong. In short, boosting iteratively improves upon a sequence of weak learners
    taking a weighted average to ultimately yield a strong learner.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 提升是另一种集成技术。但与装袋不同的是，提升最终构建一个具有 *更多* 容量的集成模型，而不是单个成员模型。因此，提升提供了比方差更有效的减少偏差的手段。提升背后的思想是迭代地构建一个模型集合，每个后续模型专注于学习上一个模型错误的示例。简而言之，提升通过迭代改进一系列弱学习器的加权平均值，最终产生一个强学习器。
- en: 'At the start of the boosting procedure, a simple base model `f_0` is selected.
    For a regression task, the base model could just be the average target value:
    `f_0 = np.mean(Y_train)`. For the first iteration step, the residuals `delta_1`
    are measured and approximated via a separate model. This residual model can be
    anything, but typically it isn’t very sophisticated; we’d often use a weak learner
    like a decision tree. The approximation provided by the residual model is then
    added to the current prediction, and the process continues.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在提升过程开始时，选择一个简单的基础模型 `f_0`。对于回归任务，基础模型可以只是目标值的平均值：`f_0 = np.mean(Y_train)`。在第一次迭代步骤中，测量并通过一个独立模型近似残差
    `delta_1`。这个残差模型可以是任何模型，但通常不是非常复杂；我们通常会使用一个弱学习器，比如决策树。残差模型提供的近似值然后添加到当前预测中，并且该过程继续。
- en: After many iterations, the residuals tend toward zero and the prediction gets
    better and better at modeling the original training dataset. Notice that in [Figure 3-12](#boosting_converts_weak_learners_into_st)
    the residuals for each element of the dataset decrease with each successive iteration.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 经过多次迭代后，残差趋近于零，并且预测在建模原始训练数据集时变得越来越好。请注意，在 [图 3-12](#boosting_converts_weak_learners_into_st)
    中，数据集每个元素的残差随着每次迭代逐渐减小。
- en: '![Boosting converts weak learners into strong learners by iteratively improving
    the model prediction. ](Images/mldp_0312.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![提升通过迭代改进模型预测将弱学习器转化为强学习器。](Images/mldp_0312.png)'
- en: Figure 3-12\. Boosting converts weak learners into strong learners by iteratively
    improving the model prediction.
  id: totrans-177
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-12\. 提升通过迭代改进模型预测将弱学习器转化为强学习器。
- en: Some of the more well-known boosting algorithms are AdaBoost, Gradient Boosting
    Machines, and XGBoost, and they have easy-to-use implementations in popular machine
    learning frameworks like scikit-learn or TensorFlow.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 一些知名的提升算法包括 AdaBoost、Gradient Boosting Machines 和 XGBoost，在流行的机器学习框架如 scikit-learn
    或 TensorFlow 中都有易于使用的实现。
- en: 'The implementation in scikit-learn is also straightforward:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中的实现也很简单：
- en: '[PRE11]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Stacking
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 堆叠
- en: Stacking is an ensemble method that combines the outputs of a collection of
    models to make a prediction. The initial models, which are typically of different
    model types, are trained to completion on the full training dataset. Then, a secondary
    meta-model is trained using the initial model outputs as features. This second
    meta-model learns how to best combine the outcomes of the initial models to decrease
    the training error and can be any type of machine learning model.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠是一种集成方法，它结合了一系列模型的输出来进行预测。初始模型通常是不同类型的模型，并在完整的训练数据集上进行训练。然后，使用初始模型输出作为特征训练一个次级元模型。这第二个元模型学习如何最佳地组合初始模型的输出以减少训练误差，并且可以是任何类型的机器学习模型。
- en: 'To implement a stacking ensemble, we first train all the members of the ensemble
    on the training dataset. The following code calls a function, `fit_model`, that
    takes as arguments a model and the training dataset inputs `X_train` and label
    `Y_train`. This way *members* is a list containing all the trained models in our
    ensemble. The full code for this example can be found in the code [repository](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/03_prob%E2%81%A0lem_representation/ensemble_methods.ipynb)
    for this book:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 要实施堆叠集成，我们首先在训练数据集上训练集合中的所有成员。以下代码调用一个函数 `fit_model`，该函数接受模型和训练数据集输入 `X_train`
    和标签 `Y_train` 作为参数。这样 *members* 是一个包含我们集合中所有训练过的模型的列表。这个示例的完整代码可以在本书的代码 [存储库](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/03_prob%E2%81%A0lem_representation/ensemble_methods.ipynb)
    中找到。
- en: '[PRE12]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'These submodels are incorporated into a larger stacking ensemble model as individual
    inputs. Since these input models are trained alongside the secondary ensemble
    model, we fix the weights of these input models. This can be done by setting `layer.trainable`
    to `False` for the ensemble member models:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这些子模型被整合到一个更大的堆叠集成模型中作为单独的输入。由于这些输入模型是与次要集成模型一起训练的，我们对这些输入模型的权重进行了固定。可以通过将 `layer.trainable`
    设置为 `False` 来实现这一点，用于集成成员模型：
- en: '[PRE13]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We create the ensemble model stitching together the components using the Keras
    functional API:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Keras 函数式 API 创建集成模型，将组件连接在一起：
- en: '[PRE14]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In this example, the secondary model is a dense neural network with two hidden
    layers. Through training, this network learns how to best combine the results
    of the ensemble members when making predictions.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，次要模型是一个具有两个隐藏层的密集神经网络。通过训练，这个网络学习如何在进行预测时最好地结合集成成员的结果。
- en: Why It Works
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么有效
- en: Model averaging methods like bagging work because typically the individual models
    that make up the ensemble model will not all make the same errors on the test
    set. In an ideal situation, each individual model is off by a random amount, so
    when their results are averaged, the random errors cancel out, and the prediction
    is closer to the correct answer. In short, there is wisdom in the crowd.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 像 Bagging 这样的模型平均方法之所以有效，是因为通常组成集成模型的单个模型在测试集上不会都产生相同的误差。在理想情况下，每个单独的模型都会有一定随机的偏差，因此当它们的结果被平均时，随机误差会相互抵消，预测结果更接近正确答案。简而言之，众人的智慧。
- en: Boosting works well because the model is punished more and more according to
    the residuals at each iteration step. With each iteration, the ensemble model
    is encouraged to get better and better at predicting those hard-to-predict examples.
    Stacking works because it combines the best of both bagging and boosting. The
    secondary model can be thought of as a more sophisticated version of model averaging.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Boosting 之所以有效，是因为模型会根据每个迭代步骤的残差而越来越严厉地受到惩罚。随着每次迭代，集成模型被鼓励在预测那些难以预测的示例时变得越来越好。堆叠之所以有效，是因为它结合了
    Bagging 和 Boosting 的优点。次要模型可以被看作是模型平均的更复杂版本。
- en: Bagging
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Bagging
- en: 'More precisely, suppose we’ve trained *k* neural network regression models
    and average their results to create an ensemble model. If each model has error
    `error_i` on each example, where `error_i` is drawn from a zero-mean multivariate
    normal distribution with variance `var` and covariance `cov`, then the ensemble
    predictor will have an error:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 更确切地说，假设我们已经训练了 *k* 个神经网络回归模型，并对它们的结果进行平均以创建一个集成模型。如果每个模型在每个示例上的误差为 `error_i`，其中
    `error_i` 是从方差 `var` 和协方差 `cov` 的零均值多变量正态分布中抽取的，则集成预测器将具有以下误差：
- en: '[PRE15]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: If the errors `error_i` are perfectly correlated so that `cov = var`, then the
    mean square error of the ensemble model reduces to `var`. In this case, model
    averaging doesn’t help at all. On the other extreme, if the errors `error_i` are
    perfectly uncorrelated, then `cov = 0` and the mean square error of the ensemble
    model is `var/k`. So, the expected square error decreases linearly with the number
    *k* of models in the ensemble.^([1](ch03.xhtml#ch01fn12)) To summarize, on average,
    the ensemble will perform at least as well as any of the individual models in
    the ensemble. Furthermore, if the models in the ensemble make independent errors
    (for example, `cov = 0`), then the ensemble will perform significantly better.
    Ultimately, the key to success with bagging is model diversity.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果误差 `error_i` 完全相关，使得 `cov = var`，则集成模型的均方误差减少到 `var`。在这种情况下，模型平均根本没有帮助。另一极端是，如果误差
    `error_i` 完全不相关，则 `cov = 0`，集成模型的均方误差为 `var/k`。因此，预期的平方误差随着集成中模型数 *k* 的增加而线性减少。^([1](ch03.xhtml#ch01fn12))
    总之，平均而言，集成的表现至少与集成中的任何单个模型一样好。此外，如果集成中的模型产生独立的误差（例如，`cov = 0`），那么集成的性能将显著提高。归根结底，集成成功的关键在于模型的多样性。
- en: This also explains why bagging is typically less effective for more stable learners
    like k-nearest neighbors (kNN), naive Bayes, linear models, or support vector
    machines (SVMs) since the size of the training set is reduced through bootstrapping.
    Even when using the same training data, neural networks can reach a variety of
    solutions due to random weight initializations or random mini-batch selection
    or different hyperparameters, creating models whose errors are partially independent.
    Thus, model averaging can even benefit neural networks trained on the same dataset.
    In fact, one recommended solution to fix the high variance of neural networks
    is to train multiple models and aggregate their predictions.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这也解释了为什么对于稳定性较强的学习器如k最近邻（kNN）、朴素贝叶斯、线性模型或支持向量机（SVM），装袋通常效果较差，因为通过自助法抽样减少了训练集的大小。即使使用相同的训练数据，神经网络也能通过随机权重初始化、随机小批量选择或不同的超参数达到各种解决方案，创建部分独立误差的模型。因此，即使是在相同数据集上训练的神经网络，模型平均也能够带来好处。实际上，修复神经网络高方差的一个推荐解决方案是训练多个模型并聚合它们的预测。
- en: Boosting
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Boosting
- en: The boosting algorithm works by iteratively improving the model to reduce the
    prediction error. Each new weak learner corrects for the mistakes of the previous
    prediction by modeling the residuals `delta_i` of each step. The final prediction
    is the sum of the outputs from the base learner and each of the successive weak
    learners, as shown in [Figure 3-13](#boosting_iteratively_builds_a_strong_le).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 提升算法通过迭代改进模型以减少预测误差。每个新的弱学习器通过建模每一步的残差`delta_i`来纠正上一个预测的错误。最终预测是基础学习器和每个后续弱学习器输出之和，如[Figure 3-13](#boosting_iteratively_builds_a_strong_le)所示。
- en: '![Boosting iteratively builds a strong learner from a sequence of weak learners
    that model the residual error of the previous iteration.](Images/mldp_0313.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![Boosting通过一系列模型化上一次迭代残差误差的弱学习器来逐步构建强学习器。](Images/mldp_0313.png)'
- en: Figure 3-13\. Boosting iteratively builds a strong learner from a sequence of
    weak learners that model the residual error of the previous iteration.
  id: totrans-201
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 3-13\. Boosting通过一系列模型化上一次迭代残差误差的弱学习器来逐步构建强学习器。
- en: Thus, the resulting ensemble model becomes successively more and more complex,
    having more capacity than any one of its members. This also explains why boosting
    is particularly good for combating high bias. Recall, the bias is related to the
    model’s tendency to be underfit. By iteratively focusing on the hard-to-predict
    examples, boosting effectively decreases the bias of the resulting model.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，结果集成模型变得越来越复杂，具有比其任何一个成员更多的容量。这也解释了为什么提升特别适用于对抗高偏差。记住，偏差与模型倾向于欠拟合有关。通过迭代地关注难以预测的例子，提升有效地减少了生成模型的偏差。
- en: Stacking
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Stacking
- en: Stacking can be thought of as an extension of simple model averaging where we
    train *k* models to completion on the training dataset, then average the results
    to determine a prediction. Simple model averaging is similar to bagging, but the
    models in the ensemble could be of different types, while for bagging, the models
    are of the same type. More generally, we could modify the averaging step to take
    a weighted average, for example, to give more weight to one model in our ensemble
    over the others, as shown in [Figure 3-14](#the_simplest_form_of_model_averaging_av).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Stacking可以被看作是简单模型平均的扩展，我们在训练数据集上完全训练*k*个模型，然后平均结果以确定预测。简单模型平均类似于装袋，但集成中的模型可以是不同类型的，而对于装袋来说，模型是相同类型的。更一般地，我们可以修改平均步骤以进行加权平均，例如，在我们的集成中给一个模型更多的权重，如[Figure 3-14](#the_simplest_form_of_model_averaging_av)所示。
- en: '![The simplest form of model averaging averages the outputs of two or more
    different machine learning models. Alternatively, the average could be replaced
    with a weighted average where the weight might be based on the relative accuracy
    of the models.](Images/mldp_0314.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![最简单的模型平均将两个或更多不同的机器学习模型的输出进行平均。或者，平均值可以用基于模型相对精度的加权平均替换。](Images/mldp_0314.png)'
- en: Figure 3-14\. The simplest form of model averaging averages the outputs of two
    or more different machine learning models. Alternatively, the average could be
    replaced with a weighted average where the weight might be based on the relative
    accuracy of the models.
  id: totrans-206
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 3-14\. 最简单的模型平均将两个或更多不同的机器学习模型的输出进行平均。或者，平均值可以用基于模型相对精度的加权平均替换。
- en: You can think of stacking as a more advanced version of model averaging, where
    instead of taking an average or weighted average, we train a second machine learning
    model on the outputs to learn how best to combine the results to the models in
    our ensemble to produce a prediction as shown in [Figure 3-15](#stacking_is_an_ensemble_learning_techni).
    This provides all the benefits of decreasing variance as with bagging techniques
    but also controls for high bias.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将堆叠视为模型平均的更高级版本，其中我们不是取平均或加权平均，而是训练第二个机器学习模型来学习如何最好地组合我们集成模型中的结果以生成预测，如[图3-15](#stacking_is_an_ensemble_learning_techni)所示。这不仅提供了减少方差的所有好处，就像装袋技术一样，还控制了高偏差。
- en: '![Stacking is an ensemble learning technique that combines the outputs of several
    different ML models as the input to a secondary ML model that makes predictions.
    ](Images/mldp_0315.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![堆叠是一种集成学习技术，将几个不同的ML模型的输出作为二级ML模型的输入，该模型进行预测。](Images/mldp_0315.png)'
- en: Figure 3-15\. Stacking is an ensemble learning technique that combines the outputs
    of several different ML models as the input to a secondary ML model that makes
    predictions.
  id: totrans-209
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-15\. 堆叠是一种集成学习技术，将几个不同的ML模型的输出作为二级ML模型的输入，该模型进行预测。
- en: Trade-Offs and Alternatives
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 折衷和替代方案
- en: Ensemble methods have become quite popular in modern machine learning and have
    played a large part in winning well-known challenges, perhaps most notably the
    [Netflix Prize](https://oreil.ly/ybZ28). There is also a lot of theoretical evidence
    to back up the success demonstrated on these real-world challenges.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法在现代机器学习中变得非常流行，并在赢得众所周知的挑战中发挥了重要作用，也许最为著名的是[Netflix Prize](https://oreil.ly/ybZ28)。有大量的理论证据支持这些在真实世界挑战中展示的成功。
- en: Increased training and design time
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 增加的训练和设计时间
- en: 'One downside to ensemble learning is increased training and design time. For
    example, for a stacked ensemble model, choosing the ensemble member models can
    require its own level of expertise and poses its own questions: Is it best to
    reuse the same architectures or encourage diversity? If we do use different architectures,
    which ones should we use? And how many? Instead of developing a single ML model
    (which can be a lot of work on its own!), we are now developing *k* models. We’ve
    introduced an additional amount of overhead in our model development, not to mention
    maintenance, inference complexity, and resource usage if the ensemble model is
    to go into production. This can quickly become impractical as the number of models
    in the ensemble increases.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习的一个缺点是增加了训练和设计时间。例如，对于堆叠集成模型，选择集成成员模型可能需要自己的专业知识，并提出了自己的问题：是重新使用相同的架构还是鼓励多样性？如果我们使用不同的架构，应该选择哪些？以及数量是多少？不是开发单个ML模型（这本身可能就是很大的工作！），而是现在开发*k*个模型。我们在模型开发中引入了额外的开销，更不用说如果集成模型投入生产，维护、推理复杂性和资源使用问题了。随着集成模型中模型数量的增加，这很快可能变得不切实际。
- en: 'Popular machine learning libraries, like scikit-learn and TensorFlow, provide
    easy-to-use implementations for many common bagging and boosting methods, like
    random forest, AdaBoost, gradient boosting, and XGBoost. However, we should carefully
    consider whether the increased overhead associated with an ensemble method is
    worth it. Always compare accuracy and resource usage against a linear or DNN model.
    Note that distilling (see [“Design Pattern 11: Useful Overfitting”](ch04.xhtml#design_pattern_oneone_useful_overfittin))
    an ensemble of neural networks can often reduce complexity and improve performance.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 流行的机器学习库，如scikit-learn和TensorFlow，为许多常见的装袋和提升方法（如随机森林、AdaBoost、梯度提升和XGBoost）提供了易于使用的实现。然而，我们应该仔细考虑是否值得使用集成方法所带来的额外开销。始终将准确性和资源使用与线性或DNN模型进行比较。请注意，蒸馏（参见[“设计模式11：有用的过拟合”](ch04.xhtml#design_pattern_oneone_useful_overfittin)）神经网络的集成通常可以减少复杂性并提高性能。
- en: Dropout as bagging
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 退火作为装袋
- en: Techniques like dropout provide a powerful and effective alternative. Dropout
    is known as a regularization technique in deep learning but can be also understood
    as an approximation to bagging. Dropout in a neural network randomly (with a prescribed
    probability) “turns off” neurons of the network for each mini-batch of training,
    essentially evaluating a bagged ensemble of exponentially many neural networks.
    That being said, training a neural network with dropout is not exactly the same
    as bagging. There are two notable differences. First, in the case of bagging,
    the models are independent, while when training with dropout, the models share
    parameters. Second, in bagging, the models are trained to convergence on their
    respective training set. However, when training with dropout, the ensemble member
    models would only be trained for a single training step because different nodes
    are dropped out in each iteration of the training loop.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 像dropout这样的技术提供了一个强大且有效的替代方案。Dropout被称为深度学习中的一种正则化技术，但也可以理解为对bagging的一种近似。在神经网络中，dropout会随机（以预定的概率）在每个训练小批量中“关闭”网络的神经元，本质上评估了指数级许多神经网络的袋装集成。尽管如此，使用dropout训练神经网络并不完全等同于bagging。有两个显著的不同点。首先，在bagging的情况下，模型是独立的，而使用dropout训练时，模型共享参数。其次，在bagging中，模型被训练以收敛于各自的训练集。然而，在使用dropout训练时，集成成员模型只会在单个训练步骤中进行训练，因为在训练循环的每次迭代中会有不同的节点被丢弃。
- en: Decreased model interpretability
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 降低模型的可解释性
- en: Another point to keep in mind is model interpretability. Already in deep learning,
    effectively explaining why our model makes the predictions it does can be difficult.
    This problem is compounded with ensemble models. Consider, for example, decision
    trees versus the random forest. A decision tree ultimately learns boundary values
    for each feature that guide a single instance to the model’s final prediction.
    As such, it is easy to explain why a decision tree makes the predictions it did.
    The random forest, being an ensemble of many decision trees, loses this level
    of local interpretability.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要记住的点是模型的可解释性。在深度学习中，有效解释为什么我们的模型做出它所做的预测可能会很困难。这个问题在集成模型中更加严重。例如，考虑决策树与随机森林。决策树最终学习每个特征的边界值，指导单个实例到模型的最终预测。因此，解释决策树为何做出其预测是很容易的。而随机森林作为许多决策树的集成，失去了这种局部可解释性。
- en: Choosing the right tool for the problem
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择适合问题的正确工具
- en: It’s also important to keep in mind the bias–variance trade-off. Some ensemble
    techniques are better at addressing bias or variance than others ([Table 3-2](#a_summary_of_the_tradeoff_between_bias)).
    In particular, boosting is adapted for addressing high bias, while bagging is
    useful for correcting high variance. That being said, as we saw in the section
    on [“Bagging”](#baggin), combining two models with highly correlated errors will
    do nothing to help lower the variance. In short, using the wrong ensemble method
    for our problem won’t necessarily improve performance; it will just add unnecessary
    overhead.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 同样重要的是要牢记偏差和方差的权衡。一些集成技术比其他技术更擅长解决偏差或方差问题（见表格 3-2）。特别是，boosting适用于解决高偏差问题，而bagging则用于修正高方差问题。尽管如此，正如我们在“Bagging”章节中看到的，组合两个具有高度相关错误的模型不会帮助降低方差。简而言之，对于我们的问题使用错误的集成方法不一定会改善性能；它只会增加不必要的开销。
- en: Table 3-2\. A summary of the trade-off between bias and variance
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 3-2. 偏差和方差之间的权衡总结
- en: '| Problem | Ensemble solution |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 集成解决方案 |'
- en: '| --- | --- |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| High bias (underfitting) | Boosting |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 高偏差（欠拟合） | Boosting |'
- en: '| High variance (overfitting) | Bagging |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 高方差（过拟合） | Bagging |'
- en: Other ensemble methods
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 其他集成方法
- en: We’ve discussed some of the more common ensemble techniques in machine learning.
    The list discussed earlier is by no means exhaustive and there are different algorithms
    that fit with these broad categories. There are also other ensemble techniques,
    including many that incorporate a Bayesian approach or that combine neural architecture
    search and reinforcement learning, like Google’s AdaNet or AutoML techniques.
    In short, the Ensemble design pattern encompasses techniques that combine multiple
    machine learning models to improve overall model performance and can be particularly
    useful when addressing common training issues like high bias or high variance.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了一些常见的集成技术在机器学习中。之前讨论的列表绝非详尽无遗，这些广义分类下还有许多适合的算法。还有其他集成技术，包括一些结合贝叶斯方法或者结合神经架构搜索和强化学习的技术，比如谷歌的AdaNet或者AutoML技术。简而言之，集成设计模式涵盖了多种结合多个机器学习模型以提升整体模型性能的技术，特别适用于解决常见的训练问题，比如高偏差或高方差。
- en: 'Design Pattern 8: Cascade'
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计模式 8：级联
- en: The Cascade design pattern addresses situations where a machine learning problem
    can be profitably broken into a series of ML problems. Such a cascade often requires
    careful design of the ML experiment.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 级联设计模式解决了机器学习问题可以有利地分解成一系列机器学习问题的情况。这种级联往往需要仔细设计机器学习实验。
- en: Problem
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: What happens if we need to predict a value during both usual and unusual activity?
    The model will learn to ignore the unusual activity because it is rare. If the
    unusual activity is also associated with abnormal values, then trainability suffers.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要在正常活动和异常活动期间预测值，会发生什么？模型会学会忽略异常活动，因为它很少见。如果异常活动也与异常值相关联，则可训练性会受到影响。
- en: For example, suppose we are trying to train a model to predict the likelihood
    that a customer will return an item that they have purchased. If we train a single
    model, the resellers’ return behavior will be lost because there are millions
    of retail buyers (and retail transactions) and only a few thousand resellers.
    We don’t really know at the time that a purchase is being made whether this is
    a retail buyer or a reseller. However, by monitoring other marketplaces, we have
    identified when items bought from us are subsequently being resold, and so our
    training dataset has a label that identifies a purchase as having been done by
    a reseller.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们试图训练一个模型来预测顾客退货的可能性。如果我们只训练一个模型，经销商的退货行为将会被忽略，因为有数百万的零售买家（和零售交易），而只有几千个经销商。在购买时，我们不知道这是零售买家还是经销商。然而，通过监控其他市场，我们已经确定了我们销售的物品后续是否被转售，因此我们的训练数据集有一个标签，标识了由经销商购买的物品。
- en: One way to solve this problem is to overweight the reseller instances when training
    the model. This is suboptimal because we need to get the more common retail buyer
    use case as correct as possible. We do not want to trade off a lower accuracy
    on the retail buyer use case for a higher accuracy on the reseller use case. However,
    retail buyers and resellers behave very differently; for example, while retail
    buyers return items within a week or so, resellers return items only if they are
    unable to sell them, and so the returns may take place after several months. The
    business decision of stocking inventory is different for likely returns from retail
    buyers versus resellers. Therefore, it is necessary to get both types of returns
    as accurate as possible. Simply overweighting the reseller instances will not
    work.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是在训练模型时加大经销商实例的权重。这是次优的，因为我们需要尽可能准确地获取更常见的零售买家用例。我们不希望为了提高经销商用例的准确性而牺牲零售买家用例的准确性。然而，零售买家和经销商的行为非常不同；例如，零售买家在一周左右内退货，而经销商只有在无法销售时才退货，因此退货可能在数月后发生。从零售买家和经销商的角度来看，库存管理的业务决策是不同的。因此，有必要尽可能准确地获取这两种类型的退货。简单地加大经销商实例的权重是行不通的。
- en: 'An intuitive way to address this problem is by using the Cascade design pattern.
    We break the problem into four parts:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一个直观方法是使用级联设计模式。我们将问题分解为四个部分：
- en: Predicting whether a specific transaction is by a reseller
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测特定交易是否由经销商进行
- en: Training one model on sales to retail buyers
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个模型以销售给零售买家
- en: Training the second model on sales to resellers
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练第二个模型以销售给经销商
- en: In production, combining the output of the three separate models to predict
    return likelihood for every item purchased and the probability that the transaction
    is by a reseller
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在生产中，结合三个独立模型的输出来预测每个购买物品的退货可能性和交易可能由转售商进行的概率
- en: This allows for the possibility of different decisions on items likely to be
    returned depending on the type of buyer and ensures that the models in steps 2
    and 3 are as accurate as possible on their segment of the training data. Each
    of these models is relatively easy to train. The first is simply a classifier,
    and if the unusual activity is extremely rare, we can use the Rebalancing pattern
    to address it. The next two models are essentially classification models trained
    on different segments of the training data. The combination is deterministic since
    we choose which model to run based on whether the activity belonged to a reseller.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许在可能返回不同决策的项目上取得不同的决定，这取决于买家类型，并确保步骤2和步骤3模型在其训练数据段上尽可能准确。每个模型相对容易训练。第一个仅仅是一个分类器，如果异常活动非常罕见，我们可以使用重新平衡模式来处理。接下来的两个模型本质上是在不同训练数据段上训练的分类模型。该组合是确定性的，因为我们根据活动是否属于转售商选择运行哪个模型。
- en: The problem comes during prediction. At prediction time, we don’t have true
    labels, just the output of the first classification model. Based on the output
    of the first model, we will have to determine which of the two sales models we
    invoke. The problem is that we are training on labels, but at inference time,
    we will have to make decisions based on predictions. And predictions have errors.
    So, the second and third models will be required to make predictions on data that
    they might have never seen during training.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 问题出现在预测过程中。在预测时，我们没有真实标签，只有第一个分类模型的输出。基于第一个模型的输出，我们将不得不确定调用哪个销售模型。问题在于我们在标签上训练，但在推断时，我们必须根据预测做出决策。而预测存在误差。因此，第二个和第三个模型将需要对它们可能在训练期间从未见过的数据进行预测。
- en: As an extreme example, assume that the address that resellers provide is always
    in an industrial area of the city, whereas retail buyers can live anywhere. If
    the first (classification) model makes a mistake and a retail buyer is wrongly
    identified as a reseller, the cancellation prediction model that is invoked will
    not have the neighborhood where the customer lives in its vocabulary.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 举个极端的例子，假设转售商提供的地址总是在城市的工业区域，而零售买家可以住在任何地方。如果第一个（分类）模型出错，并且错误地将零售买家识别为转售商，那么调用的取消预测模型将不会在其词汇表中包含客户居住的邻域。
- en: How do we train a cascade of models where the output of one model is an input
    to the following model or determines the selection of subsequent models?
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何训练级联模型，其中一个模型的输出是后续模型的输入或决定后续模型的选择？
- en: Solution
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Any machine learning problem where the output of the one model is an input to
    the following model or determines the selection of subsequent models is called
    a *cascade*. Special care has to be taken when training a cascade of ML models.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 任何一个机器学习问题，其中一个模型的输出是后续模型的输入或决定后续模型的选择，被称为*级联*。在训练级联机器学习模型时必须特别小心。
- en: 'For example, a machine learning problem that sometimes involves unusual circumstances
    can be solved by treating it as a cascade of four machine learning problems:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，有时涉及异常情况的机器学习问题可以通过将其视为四个机器学习问题的级联来解决：
- en: A classification model to identify the circumstance
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用于识别情况的分类模型
- en: One model trained on unusual circumstances
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个在异常情况下训练的模型
- en: A separate model trained on typical circumstances
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个在典型情况下训练的独立模型
- en: A model to combine the output of the two separate models, because the output
    is a probabilistic combination of the two outputs
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个模型来组合两个独立模型的输出，因为输出是两个输出的概率组合
- en: This appears, at first glance, to be a specific case of the Ensemble design
    pattern, but is considered separately because of the special experiment design
    required when doing a cascade.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 乍看之下，这似乎是集成设计模式的一个特例，但由于进行级联时需要的特殊实验设计，它被单独考虑。
- en: As an example, assume that, in order to estimate the cost of stocking bicycles
    at stations, we wish to predict the distance between rental and return stations
    for bicycles in San Francisco. The goal of the model, in other words, is to predict
    the distance we need to transport the bicycle back to the rental location given
    features such as the time of day the rental starts, where the bicycle is being
    rented from, whether the renter is a subscriber or not, etc. The problem is that
    rentals that are longer than four hours involve extremely different renter behavior
    than shorter rentals, and the stocking algorithm requires both outputs (the probability
    that the rental is longer than four hours and the likely distance the bicycle
    needs to be transported). However, only a very small fraction of rentals involve
    such abnormal trips.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设为了估算在车站储存自行车的成本，我们希望预测旧金山租赁和返回站点之间的距离。换句话说，模型的目标是预测我们需要将自行车运回租赁地点的距离，给定特征，例如租赁开始的时间、租赁自行车的地点、租客是否为订阅者等。问题在于超过四小时的租赁涉及的租客行为与较短租赁大不相同，而库存算法需要两个输出（租赁超过四小时的概率和需要运输自行车的预计距离）。然而，只有极小一部分租赁涉及这样的异常行程。
- en: 'One way to solve this problem is to train a classification model to first classify
    trips based on whether they are Long or Typical (the [full code](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/03_problem_representation/cascade.ipynb)
    is in the code repository of this book):'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是首先训练一个分类模型来根据租赁是否为长期或典型进行分类（这本书的代码库中有[完整代码](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/03_problem_representation/cascade.ipynb)）：
- en: '[PRE16]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: It can be tempting to simply split the training dataset into two parts based
    on the actual duration of the rental and train the next two models, one on Long
    rentals and the other on Typical rentals. The problem is that the classification
    model just discussed will have errors. Indeed, evaluating the model on a held-out
    portion of the San Francisco bicycle data shows that the accuracy of the model
    is only around 75% (see [Figure 3-16](#the_accuracy_of_a_classification_model)).
    Given this, training a model on a perfect split of the data will lead to tears.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 简单地根据租赁的实际持续时间将训练数据集分为两部分，然后分别训练下两个模型（一个针对长租赁，另一个针对典型租赁）可能是很诱人的。问题在于前面讨论的分类模型会有误差。实际上，在旧金山自行车数据的保留部分上评估模型表明，模型的准确率仅约为
    75%（见图 3-16）。鉴于此，将模型训练在数据的完美分割将导致泪水。
- en: '![The accuracy of a classification model to predict atypical behavior is unlikely
    to be 100%.](Images/mldp_0316.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![用于预测非典型行为的分类模型的准确率不太可能达到 100%。](Images/mldp_0316.png)'
- en: Figure 3-16\. The accuracy of a classification model to predict atypical behavior
    is unlikely to be 100%.
  id: totrans-256
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-16。用于预测非典型行为的分类模型的准确率不太可能达到 100%。
- en: 'Instead, after training this classification model, we need to use the predictions
    of this model to create the training dataset for the next set of models. For example,
    we could create the training dataset for the model to predict the distance of
    Typical rentals using:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在训练完这个分类模型后，我们需要使用该模型的预测来创建下一个模型的训练数据集。例如，我们可以使用以下方式创建用于预测典型租赁距离的模型的训练数据集：
- en: '[PRE17]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, we should use this dataset to train the model to predict distances:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们应该使用这个数据集来训练预测距离的模型：
- en: '[PRE18]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Finally, our evaluation, prediction, etc. should take into account that we need
    to use three trained models, not just one. This is what we term the Cascade design
    pattern.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们的评估、预测等都应考虑到，我们需要使用三个经过训练的模型，而不仅仅是一个。这就是我们所称的级联设计模式。
- en: In practice, it can become hard to keep a Cascade workflow straight. Rather
    than train the models individually, it is better to automate the entire workflow
    using the Workflow Pipelines pattern ([Chapter 6](ch06_split_000.xhtml#reproducibility_design_patterns))
    as shown in [Figure 3-17](#a_pipeline_to_train_the_cascade_of_mode). The key is
    to ensure that training datasets for the two downstream models are created each
    time the experiment is run based on the predictions of upstream models.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，保持级联工作流的清晰度可能变得困难。与单独训练模型相比，最好使用工作流程管道模式自动化整个工作流程（如[第 6 章](ch06_split_000.xhtml#reproducibility_design_patterns)所示），如图
    3-17。关键是确保每次运行实验时，基于上游模型的预测创建两个下游模型的训练数据集。
- en: Although we introduced the Cascade pattern as a way of predicting a value during
    both usual and unusual activity, the Cascade pattern’s solution is capable of
    addressing a more general situation. The pipeline framework allows us to handle
    any situation where a machine learning problem can be profitably broken into a
    series (or cascade) of ML problems. Whenever the output of a machine learning
    model needs to be fed as the input to another model, the second model needs to
    be trained on the predictions of the first model. In all such situations, a formal
    pipeline experimentation framework will be helpful.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们介绍级联模式作为在通常和不寻常活动期间预测值的一种方式，但级联模式的解决方案能够处理更一般的情况。管道框架使我们能够处理任何可以将机器学习问题有利地分解为一系列（或级联）机器学习问题的情况。每当一个机器学习模型的输出需要作为另一个模型的输入时，第二个模型需要根据第一个模型的预测进行训练。在所有这些情况下，正式的管道实验框架将会很有帮助。
- en: '![A pipeline to train the cascade of models as a single job.](Images/mldp_0317.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![一个将级联模型作为单个作业进行训练的管道。](Images/mldp_0317.png)'
- en: Figure 3-17\. A pipeline to train the cascade of models as a single job.
  id: totrans-265
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-17\. 一个将级联模型作为单个作业进行训练的管道。
- en: Kubeflow Pipelines provides such a framework. Because it works with containers,
    the underlying machine learning models and glue code can be written in nearly
    any programming or scripting language. Here, we will wrap the BigQuery SQL models
    above into Python functions using the BigQuery client library. We could use TensorFlow
    or scikit-learn or even R to implement individual components.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow Pipelines 提供了这样一个框架。由于它与容器一起工作，底层的机器学习模型和粘合代码可以用几乎任何编程或脚本语言编写。在这里，我们将上述的
    BigQuery SQL 模型封装成 Python 函数，使用 BigQuery 客户端库。我们可以使用 TensorFlow 或 scikit-learn，甚至
    R 来实现各个组件。
- en: 'The pipeline code using Kubeflow Pipelines can be expressed quite simply as
    the following (the [full code](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/03_problem_representation/cascade.ipynb)
    can be found in the code repository of this book):'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Kubeflow Pipelines 的管道代码可以简单地表达如下（本书的[完整代码](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/03_problem_representation/cascade.ipynb)可以在代码存储库中找到）：
- en: '[PRE19]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The entire pipeline can be submitted for running, and different runs of the
    experiment tracked using the Pipelines framework.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 整个管道可以提交运行，并可以使用 Pipelines 框架跟踪实验的不同运行。
- en: Tip
  id: totrans-270
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If we are using TFX as our pipeline framework (we can run TFX on Kubeflow Pipelines),
    then it is not necessary to deploy the upstream models in order to use their output
    predictions in downstream models. Instead, we can use the TensorFlow Transform
    method `tft.apply_saved_model` as part of our preprocessing operations. The Transform
    design pattern is discussed in [Chapter 6](ch06_split_000.xhtml#reproducibility_design_patterns).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们正在使用 TFX 作为我们的管道框架（我们可以在 Kubeflow Pipelines 上运行 TFX），那么没有必要部署上游模型以使用它们的输出预测在下游模型中。相反，我们可以在预处理操作中使用
    TensorFlow Transform 方法 `tft.apply_saved_model`。转换设计模式在[第 6 章](ch06_split_000.xhtml#reproducibility_design_patterns)中有所讨论。
- en: Use of a pipeline-experiment framework is strongly suggested whenever we will
    have chained ML models. Such a framework will ensure that downstream models are
    retrained whenever upstream models are revised and that we have a history of all
    the previous training runs.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 强烈建议在我们将会有串联的机器学习模型时使用管道实验框架。这样的框架将确保在上游模型修订时重新训练下游模型，并且我们有所有先前训练运行的历史记录。
- en: Trade-Offs and Alternatives
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 折衷与替代方案
- en: Don’t go overboard with the Cascade design pattern—unlike many of the design
    patterns we cover in this book, Cascade is not necessarily a best practice. It
    adds quite a bit of complexity to your machine learning workflows and may actually
    result in poorer performance. Note that a pipeline-experiment framework is definitely
    best practice, but as much as possible, try to limit a pipeline to a single machine
    learning problem (ingest, preprocessing, data validation, transformation, training,
    evaluation, and deployment). Avoid having, as in the Cascade pattern, multiple
    machine learning models in the same pipeline.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 不要过度使用级联设计模式——与本书涵盖的许多设计模式不同，级联模式不一定是最佳实践。它会给你的机器学习工作流程增加相当多的复杂性，实际上可能导致性能下降。请注意，管道实验框架绝对是最佳实践，但尽可能地，尝试将管道限制在单个机器学习问题上（摄取、预处理、数据验证、转换、训练、评估和部署）。避免像级联模式中那样，在同一个管道中包含多个机器学习模型。
- en: Deterministic inputs
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 确定性输入
- en: 'Splitting an ML problem is usually a bad idea, since an ML model can/should
    learn combinations of multiple factors. For example:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，将机器学习问题拆分通常是一个不好的主意，因为机器学习模型可以/应该学习多个因素的组合。例如：
- en: If a condition can be known deterministically from the input (holiday shopping
    versus weekday shopping), we should just add the condition as one more input to
    the model.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个条件可以从输入中确定地知道（假期购物与工作日购物），我们应该将该条件作为模型的另一个输入添加进去。
- en: If the condition involves an extrema in just one input (some customers who live
    nearby versus far away, with the meaning of near/far needing to be learned from
    the data), we can use Mixed Input Representation to handle it.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果条件涉及仅一个输入的极值（一些居住在附近与远处的客户，其中“附近/远处”的含义需要从数据中学习），我们可以使用混合输入表示来处理它。
- en: The Cascade design pattern addresses an unusual scenario for which we do not
    have a categorical input, and for which extreme values need to be learned from
    multiple inputs.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 级联设计模式解决了一个不寻常的场景，其中我们没有分类输入，而且需要从多个输入中学习极值。
- en: Single model
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单一模型
- en: The Cascade design pattern should not be used for common scenarios where a single
    model will suffice. For example, suppose we are trying to learn a customer’s propensity
    to buy. We may think we need to learn different models for people who have been
    comparison shopping versus those who aren’t. We don’t really know who has been
    comparison shopping, but we can make an educated guess based on the number of
    visits, how long the item has been in the cart, and so on. This problem does not
    need the Cascade design pattern because it is common enough (a large fraction
    of customers will be comparison shopping) that the machine learning model should
    be able to learn it implicitly in the course of training. For common scenarios,
    train a single model.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在常见情况下不应使用级联设计模式，适用单一模型即可。例如，假设我们正在尝试学习客户的购买倾向。我们可能认为我们需要为那些进行比较购物和不进行比较购物的人学习不同的模型。我们真的不知道谁进行了比较购物，但我们可以根据访问次数、商品在购物车中停留的时间等进行合理猜测。这个问题不需要级联设计模式，因为它足够常见（大部分客户都会进行比较购物），机器学习模型应该能在训练过程中隐式地学习到它。对于常见情况，只需训练一个单一模型。
- en: Internal consistency
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内部一致性
- en: The Cascade is needed when we need to maintain internal consistency amongst
    the predictions of multiple models. Note that we are trying to do more than just
    predict the unusual activity. We are trying to predict returns, considering that
    there will be some reseller activity also. If the task is only to predict whether
    or not a sale is by a reseller, we’d use the Rebalancing pattern. The reason to
    use Cascade is that the imbalanced label output is needed as an input to subsequent
    models and is useful in and of itself.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们需要在多个模型的预测之间保持内部一致性时，需要级联。请注意，我们试图做的不仅仅是预测不寻常的活动。我们正在尝试预测回报，考虑到还会有一些转售活动。如果任务仅仅是预测销售是否由转售商进行，我们将使用再平衡模式。使用级联的原因是，不平衡的标签输出需要作为后续模型的输入，并且本身也很有用。
- en: Similarly, suppose that the reason we are training the model to predict a customer’s
    propensity to buy is to make a discounted offer. Whether or not we make the discounted
    offer, and the amount of discount, will very often depend on whether this customer
    is comparison shopping or not. Given this, we need internal consistency between
    the two models (the model for comparison shoppers and the model for propensity
    to buy). In this case, the Cascade design pattern might be needed.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，假设我们训练模型来预测客户购买的倾向是为了提供折扣优惠。我们是否提供折扣优惠，以及优惠的金额，很大程度上会取决于这位客户是否进行比较购物。鉴于此，我们需要在两个模型之间保持内部一致性（用于比较购物者和购买倾向的模型）。在这种情况下，可能需要级联设计模式。
- en: Pre-trained models
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预训练模型
- en: The Cascade is also needed when we wish to reuse the output of a pre-trained
    model as an input into our model. For example, let’s say we are building a model
    to detect authorized entrants to a building so that we can automatically open
    the gate. One of the inputs to our model might be the license plate of the vehicle.
    Instead of using the security photo directly in our model, we might find it simpler
    to use the output of an optical character recognition (OCR) model. It is critical
    that we recognize that OCR systems will have errors, and so we should not train
    our model with perfect license plate information. Instead, we should train the
    model on the actual output of the OCR system. Indeed, because different OCR models
    will behave differently and have different errors, it is necessary to retrain
    the model if we change the vendor of our OCR system.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们希望重用预训练模型的输出作为我们模型的输入时，级联也是必需的。例如，假设我们正在构建一个模型来检测建筑物的授权入口者，以便我们可以自动打开门。我们模型的一个输入可能是车辆的车牌。与其直接使用安全照片在我们的模型中，我们可能会发现使用光学字符识别（OCR）模型的输出更简单。重要的是要认识到OCR系统会有误差，因此我们不应该使用完美的车牌信息来训练我们的模型。相反，我们应该根据OCR系统的实际输出来训练模型。实际上，因为不同的OCR模型的行为不同且具有不同的错误，如果更改OCR系统的供应商，有必要重新训练模型。
- en: Tip
  id: totrans-287
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: A common scenario of using a pre-trained model as the first step of a pipeline
    is using an object-detection model followed by a fine-grained image classification
    model. For example, the object-detection model might find all handbags in the
    image, an intermediate step might crop the image to the bounding boxes of the
    detected objects, and the subsequent model might identify the type of handbag.
    We recommend using a Cascade so that the entire pipeline can be retrained whenever
    the object-detection model is updated (such as with a new version of the API).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练模型作为流水线的第一步的常见情况是使用对象检测模型，然后是精细图像分类模型。例如，对象检测模型可能会在图像中找到所有的手提包，中间步骤可能会裁剪到检测到的对象的边界框，并且随后的模型可能会识别手提包的类型。我们建议使用级联，以便在对象检测模型更新时（例如使用
    API 的新版本时）可以重新训练整个流水线。
- en: Reframing instead of Cascade
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重新构思而不是级联
- en: Note that in our example problem, we were trying to predict the likelihood that
    an item would be returned, and so this was a classification problem. Suppose instead
    we wish to predict hourly sales amounts. Most of the time, we will serve just
    retail buyers, but once in a while (perhaps four or five times a year), we will
    have a wholesale buyer.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在我们的示例问题中，我们试图预测物品退货的可能性，因此这是一个分类问题。假设我们希望预测每小时销售额。大多数情况下，我们只会服务零售买家，但偶尔（例如每年四到五次），我们会有批发买家。
- en: This is notionally a regression problem of predicting daily sales amounts where
    we have a confounding factor in the form of wholesale buyers. Reframing the regression
    problem to be a classification problem of different sales amounts might be a better
    approach. Although it will involve training a classification model for each sales
    amount bucket, it avoids the need to get the retail versus wholesale classification
    correct.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这在概念上是一个回归问题，即预测每日销售额，我们有一个混淆因素，即批发买家。重新构思回归问题，将其作为不同销售额分类问题可能是更好的方法。虽然这将涉及针对每个销售额桶训练分类模型，但可以避免需要正确区分零售与批发。
- en: Regression in rare situations
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在罕见情况下的回归
- en: 'The Cascade design pattern can be helpful when carrying out regression when
    some values are much more common than others. For example, we might want to predict
    the quantity of rainfall from a satellite image. It might be the case that on
    99% of the pixels, it doesn’t rain. In such a case, it can be helpful to create
    a stacked classification model followed by a regression model:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 当进行回归时，某些值比其他值常见时，级联设计模式可能是有帮助的。例如，我们可能想要从卫星图像预测降雨量。有可能在99%的像素上没有雨。在这种情况下，创建堆叠分类模型后跟回归模型可能是有帮助的：
- en: First, predict whether or not it is going to rain.
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，预测是否会下雨。
- en: For pixels where the model predicts rain is not likely, predict a rainfall amount
    of zero.
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于模型预测雨不太可能的像素，预测降雨量为零。
- en: Train a regression model to predict the rainfall amount on pixels where the
    model predicts that rain is likely.
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练回归模型，以预测模型预测降雨可能性的像素上的降雨量。
- en: 'It is critical to realize that the classification model is not perfect, and
    so the regression model has to be trained on the pixels that the classification
    model predicts as likely to be raining (and not just on pixels that correspond
    to rain in the labeled dataset). For complementary solutions to this problem,
    also see the discussions on [“Design Pattern 10: Rebalancing ”](#design_pattern_onezero_rebalancing)
    and [“Design Pattern 5: Reframing ”](#design_pattern_five_reframing).'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 必须认识到分类模型并不完美，所以回归模型必须根据分类模型预测可能下雨的像素进行训练（而不仅仅是标记数据集中对应于雨的像素）。关于这个问题的补充解决方案，请参见[“设计模式10：再平衡”](#design_pattern_onezero_rebalancing)和[“设计模式5：重新构架”](#design_pattern_five_reframing)的讨论。
- en: 'Design Pattern 9: Neutral Class'
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计模式9：中性类
- en: In many classification situations, creating a neutral class can be helpful.
    For example, instead of training a binary classifier that outputs the probability
    of an event, train a three-class classifier that outputs disjoint probabilities
    for Yes, No, and Maybe. Disjoint here means that the classes do not overlap. A
    training pattern can belong to only one class, and so there is no overlap between
    Yes and Maybe, for example. The Maybe in this case is the neutral class.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多分类情况下，创建一个中性类可能会有所帮助。例如，不是训练输出事件概率的二元分类器，而是训练一个三类分类器，为“是”、“否”和“或许”分别输出互斥的概率。这里的互斥意味着类别不重叠。一个训练模式只能属于一个类别，因此“是”和“或许”之间没有重叠。在这种情况下，“或许”就是中性类。
- en: Problem
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Imagine that we are trying to create a model that provides guidance on pain
    relievers. There are two choices, ibuprofen and acetaminophen,^([2](ch03.xhtml#ch01fn13))
    and it turns out in our historical dataset that acetaminophen tends to be prescribed
    preferentially to patients at risk of stomach problems, and ibuprofen tends to
    be prescribed preferentially to patients at risk of liver damage. Beyond that,
    things tend to be quite random; some physicians default to acetaminophen and others
    to ibuprofen.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我们试图创建一个关于止痛药的指导模型。有两种选择，布洛芬和对乙酰氨基酚，^([2](ch03.xhtml#ch01fn13)) 根据我们的历史数据集，对乙酰氨基酚倾向于优先给存在胃问题风险的患者开药，而布洛芬倾向于优先给存在肝损伤风险的患者开药。除此之外，情况相当随机；一些医生倾向于默认给对乙酰氨基酚，而另一些则倾向于布洛芬。
- en: Training a binary classifier on such a dataset will lead to poor accuracy because
    the model will need to get the essentially arbitrary cases correct.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在这样的数据集上训练二元分类器会导致准确率低，因为模型需要正确处理本质上是任意的案例。
- en: Solution
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Imagine a different scenario. Suppose the electronic record that captures the
    doctor’s prescriptions also asks them whether the alternate pain medication would
    be acceptable. If the doctor prescribes acetaminophen, the application asks the
    doctor whether the patient can use ibuprofen if they already have it in their
    medicine cabinet.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个不同的场景。假设电子记录捕捉到医生的处方，同时询问他们是否可以接受替代止痛药。如果医生开了对乙酰氨基酚，应用程序会问医生患者如果已经在药柜里有布洛芬，是否可以使用布洛芬。
- en: Based on the answer to the second question, we have a neutral class. The prescription
    might still be written as “acetaminophen,” but the record captures that the doctor
    was neutral for this patient. Note that this fundamentally requires us to design
    the data collection appropriately—we cannot manufacture a neutral class after
    the fact. We have to correctly design the machine learning problem. Correct design,
    in this case, starts with how we pose the problem in the first place.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 根据第二个问题的答案，我们得到一个中性类。处方可能仍然被写为“对乙酰氨基酚”，但记录显示医生对这位患者持中立态度。请注意，这从根本上要求我们适当设计数据收集——我们不能事后制造中性类。我们必须正确设计机器学习问题。在这种情况下，正确的设计从问题的提出开始。
- en: If all we have is a historical dataset, we would need to get a [labeling service](https://oreil.ly/OSZsi)
    involved. We could ask the human labelers to validate the doctor’s original choice
    and answer the question of whether an alternate pain medication would be acceptable.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只有历史数据集，我们需要引入一个[标记服务](https://oreil.ly/OSZsi)。我们可以请人工标记员验证医生的原始选择，并回答是否可以接受替代止痛药。
- en: Why It Works
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么它有效
- en: We can explore the mechanism by which this works by simulating the mechanism
    involved with a synthetic dataset. Then, we will show that something akin to this
    also happens in the real world with marginal cases.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过模拟涉及合成数据集的机制来探索其工作原理。接着，我们将展示类似的情况在边缘案例中也会发生于现实世界。
- en: Synthetic data
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 合成数据
- en: 'Let’s create a synthetic dataset of length *N* where 10% of the data represents
    patients with a history of jaundice. Since they are at risk of liver damage, their
    correct prescription is ibuprofen (the [full code](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/03_problem_representation/neutral.ipynb)
    is in GitHub):'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个长度为*N*的合成数据集，其中10%的数据代表有黄疸病史的患者。因为他们有肝损伤的风险，他们的正确处方是布洛芬（完整代码见GitHub：[链接](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/03_problem_representation/neutral.ipynb)）：
- en: '[PRE20]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Another 10% of the data will represent patients with a history of stomach ulcers;
    since they are at risk of stomach damage, their correct prescription is acetaminophen:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 另外10%的数据将代表有胃溃疡病史的患者；因为他们有胃损伤的风险，他们的正确处方是对乙酰氨基酚：
- en: '[PRE21]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The remaining patients will be arbitrarily assigned to either medication. Naturally,
    this random assignment will cause the overall accuracy of a model trained on just
    two classes to be low. In fact, we can calculate the upper bound on the accuracy.
    Because 80% of the training examples have random labels, the best that the model
    can do is to guess half of them correctly. So, the accuracy on that subset of
    the training examples will be 40%. The remaining 20% of the training examples
    have systematic labels, and an ideal model will learn this, so we expect that
    overall accuracy can be at best 60%.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的患者将被随机分配到两种药物中的任意一种。显然，这种随机分配将导致仅基于两个类别训练的模型的总体准确率较低。事实上，我们可以计算准确率的上限。因为80%的训练示例具有随机标签，模型在这些示例的最佳表现是猜对其中一半。因此，在这些训练示例的准确率将为40%。剩下的20%训练示例具有系统标签，并且理想模型将学会这一点，因此我们预计总体准确率最多可以达到60%。
- en: 'Indeed, training a model using scikit-learn as follows, we get an accuracy
    of 0.56:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，使用scikit-learn训练模型如下，我们得到了0.56的准确率：
- en: '[PRE22]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: If we create three classes, and put all the randomly assigned prescriptions
    into that class, we get, as expected, perfect (100%) accuracy. The purpose of
    the synthetic data was to illustrate that, provided there is random assignment
    at work, the Neutral Class design pattern can help us avoid losing model accuracy
    because of arbitrarily labeled data.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们创建三个类，并将所有随机分配的处方放入该类中，我们得到了预期的完美（100%）的准确率。合成数据的目的是说明，只要工作中有随机分配，中性类设计模式可以帮助我们避免因任意标记数据而失去模型准确性。
- en: In the real world
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在真实世界中
- en: In real-world situations, things may not be precisely random as in the synthetic
    dataset, but the arbitrary assignment paradigm still holds. For example, one minute
    after a baby is born, the baby is assigned an “Apgar score,” a number between
    1 and 10, with 10 being a baby that has come through the birthing process perfectly.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际情况中，情况可能不像合成数据集中那样精确随机，但是任意分配范式仍然适用。例如，婴儿出生后一分钟，婴儿会被分配一个“Apgar评分”，这是一个介于1到10之间的数字，其中10表示婴儿完美度过了分娩过程。
- en: 'Consider a model that is trained to predict whether or not a baby will come
    through the birthing process healthily, or will require immediate attention (the
    [full code](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/03_problem_representation/neutral.ipynb)
    is on GitHub):'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个模型，该模型训练用于预测一个婴儿是否能够健康地度过分娩过程，或者是否需要立即关注（完整代码见GitHub：[链接](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/03_problem_representation/neutral.ipynb)）：
- en: '[PRE23]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We are thresholding the Apgar score at 9 and treating babies whose Apgar score
    is 9 or 10 as healthy, and babies whose Apgar score is 8 or lower as requiring
    attention. The accuracy of this binary classification model when trained on the
    natality dataset and evaluated on held-out data is 0.56.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将Apgar评分阈值设定为9，并将Apgar评分为9或10的婴儿视为健康，将Apgar评分为8或更低的婴儿视为需要关注的对象。在natality数据集上训练并在留出数据上评估的这种二元分类模型的准确率为0.56。
- en: 'Yet, assigning an Apgar score involves a number of relatively subjective assessments,
    and whether a baby is assigned 8 or 9 often reduces to matters of physician preference.
    Such babies are neither perfectly healthy, nor do they need serious medical intervention.
    What if we create a neutral class to hold these “marginal” scores? This requires
    creating three classes, with an Apgar score of 10 defined as healthy, scores of
    8 to 9 defined as neutral, and lower scores defined as requiring attention:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，分配Apgar评分涉及多个相对主观的评估，而婴儿是否被分配为8或9往往减少到医生的偏好问题。这些婴儿既不是完全健康的，也不需要严重的医疗干预。如果我们创建一个中性类来容纳这些“边缘”评分，会怎样？这需要创建三个类别，Apgar评分为10被定义为健康，评分为8到9被定义为中性，低分被定义为需要关注：
- en: '[PRE24]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This model achieves an accuracy of 0.79 on a held-out evaluation dataset, much
    higher than the 0.56 that was achieved with two classes.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在保留评估数据集上达到了0.79的准确率，远高于两类模型达到的0.56的准确率。
- en: Trade-Offs and Alternatives
  id: totrans-326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 折衷和替代方案
- en: The Neutral Class design pattern is one to keep in mind at the beginning of
    a machine learning problem. Collect the right data, and we can avoid a lot of
    sticky problems down the line. Here are a few situations where having a neutral
    class can be helpful.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习问题的初期，中性类设计模式是需要牢记的一个模式。收集正确的数据，我们可以避免后续出现许多棘手的问题。以下是一些使用中性类有帮助的情况。
- en: When human experts disagree
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 当人类专家存在分歧时
- en: The neutral class is helpful in dealing with disagreements among human experts.
    Suppose we have human labelers to whom we show patient history and ask them what
    medication they would prescribe. We might have a clear signal for acetaminophen
    in some cases, a clear signal for ibuprofen in other cases, and a huge swath of
    cases for which human labelers disagree. The neutral class provides a way to deal
    with such cases.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 中性类对处理人类专家之间的分歧非常有帮助。假设我们有人类标记者，向他们展示病人的历史记录，并询问他们会开什么药。在某些情况下，我们可能会对乙酰氨基酚有清晰的信号，在其他情况下，对布洛芬有明确的信号，而在大部分情况下，人类标记者存在分歧。中性类为处理这些情况提供了一种方法。
- en: 'In the case of human labeling (unlike with the historical dataset of actual
    doctor actions where a patient was seen by only one doctor), every pattern is
    labeled by multiple experts. Therefore, we know a priori which cases humans disagree
    about. It might seem far simpler to simply discard such cases, and simply train
    a binary classifier. After all, it doesn’t matter what the model does on the neutral
    cases. This has two problems:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在人类标注的情况下（与只有一个医生看过患者的历史数据集不同），每个模式都由多位专家标记。因此，我们预先知道人类在哪些案例上存在分歧。简单地丢弃这些案例并简单地训练一个二元分类器似乎更加简单。毕竟，模型在中性案例上的表现无关紧要。这样做有两个问题：
- en: False confidence tends to affect the acceptance of the model by human experts.
    A model that outputs a neutral determination is often more acceptable to experts
    than a model that is wrongly confident in cases where the human expert would have
    chosen the alternative.
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 错误的自信往往会影响人类专家对模型的接受程度。一个输出中性决定的模型通常比在人类专家会选择另一种情况时错误地充满信心的模型更容易被专家接受。
- en: If we are training a cascade of models, then downstream models will be extremely
    sensitive to the neutral classes. If we continue to improve this model, downstream
    models could change dramatically from version to version.
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们正在训练一系列模型，那么下游模型将极其敏感于中性类。如果我们继续改进该模型，那么下游模型可能会从一个版本改变到另一个版本。
- en: Another alternative is to use the agreement among human labelers as the weight
    of a pattern during training. Thus, if 5 experts agree on a diagnosis, the training
    pattern gets a weight of 1, while if the experts are split 3 to 2, the weight
    of the pattern might be only 0.6\. This allows us to train a binary classifier,
    but overweight the classifier toward the “sure” cases. The drawback to this approach
    is that when the probability output by the model is 0.5, it is unclear whether
    it is because this reflects a situation where there was insufficient training
    data, or whether it is a situation where human experts disagree. Using a neutral
    class to capture areas of disagreement allows us to disambiguate the two situations.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是在训练期间使用人类标记者之间的一致性作为模式的权重。因此，如果有5位专家对一个诊断意见一致，那么训练模式将获得权重为1，而如果专家意见分歧为3比2，模式的权重可能仅为0.6。这样可以训练一个二元分类器，但是过多地偏向“确定”的案例。这种方法的缺点是，当模型输出的概率为0.5时，不清楚是因为训练数据不足，还是人类专家存在分歧。使用中性类捕捉分歧的领域允许我们消除这两种情况的歧义。
- en: Customer satisfaction
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 客户满意度
- en: 'The need for a neutral class also arises with models that attempt to predict
    customer satisfaction. If the training data consists of survey responses where
    customers grade their experience on a scale of 1 to 10, it might be helpful to
    bucket the ratings into three categories: 1 to 4 as bad, 8 to 10 as good, and
    5 to 7 is neutral. If, instead, we attempt to train a binary classifier by thresholding
    at 6, the model will spend too much effort trying to get essentially neutral responses
    correct.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 中性类的需求也出现在试图预测客户满意度的模型中。如果训练数据包括客户根据1到10的评分对其体验进行评级的调查响应，那么将评级分为三类可能会很有帮助：1到4为差评，8到10为好评，5到7为中性评价。如果我们尝试通过在6处设定阈值来训练二元分类器，模型将花费过多精力来正确预测实质上是中性的响应。
- en: As a way to improve embeddings
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作为改进嵌入的一种方式
- en: Suppose we are creating a pricing model for flights and wish to predict whether
    or not a customer will buy a flight at a certain price. To do this, we can look
    at historical transactions of flight purchases and abandoned shopping carts. However,
    suppose many of our transactions also include purchases by consolidators and travel
    agents—these are people who have contracted fares, and so the fares for them were
    not actually set dynamically. In other words, they don’t pay the currently displayed
    price.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在为航班创建一个定价模型，并希望预测客户是否会以某个价格购买航班。为了做到这一点，我们可以查看航班购买和放弃购物车的历史交易。然而，假设我们的许多交易也包括了代理人和旅行代理商的购买
    - 这些人已经签订了票价协议，因此对他们来说，票价并没有实际动态设定。换句话说，他们不会支付当前显示的价格。
- en: We could throw away all the nondynamic purchases and train the model only on
    customers who made the decision to buy or not buy based on the price being displayed.
    However, such a model will miss all the information held in the destinations that
    the consolidator or travel agent was interested in at various times—this will
    affect things like how airports and hotels are embedded. One way to retain that
    information while not affecting the pricing decision is to use a neutral class
    for these transactions.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以丢弃所有非动态购买，并仅在基于当前显示价格做出购买或不购买决策的客户上训练模型。然而，这样的模型会错过关于代理人或旅行代理商在不同时间点关注的目的地的所有信息
    - 这将影响到如何嵌入机场和酒店等因素。保留这些信息而不影响定价决策的一种方法是为这些交易使用一个中性类。
- en: Reframing with neutral class
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用中性类重新构架
- en: Suppose we are training an automated trading system that makes trades based
    on whether it expects a security to go up or down in price. Because of stock market
    volatility and the speed with which new information is reflected in stock prices,
    trying to trade on small predicted ups and downs is likely to lead to high trading
    costs and poor profits over time.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在训练一个基于预期安全性价格上升或下降而进行交易的自动交易系统。由于股市的波动性以及新信息在股价中反映的速度，试图在预测的小幅上涨和下跌上进行交易很可能会导致高昂的交易成本和长期的低利润。
- en: In such cases, it is helpful to consider what the end goal is. The end goal
    of the ML model is not to predict whether a stock will go up or down. We will
    be unable to buy every stock that we predict will go up, and unable to sell stocks
    that we don’t hold.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，考虑最终目标是很有帮助的。机器学习模型的最终目标不是预测股票将上涨还是下跌。我们无法购买每一只我们预测将上涨的股票，也无法出售我们不持有的股票。
- en: The better strategy might be to buy call options^([3](ch03.xhtml#ch01fn14))
    for the 10 stocks that are most likely to go up more than 5% over the next 6 months,
    and buy put options for stocks that are most likely to go down more than 5% over
    the next 6 months.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的策略可能是购买对最有可能在未来6个月内上涨超过5%的10只股票买入看涨期权^([3](ch03.xhtml#ch01fn14))，并为那些最有可能在未来6个月内下跌超过5%的股票买入看跌期权。
- en: 'The solution, then, is to create a training dataset consisting of three classes:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，解决方案是创建一个包含三种类别的训练数据集：
- en: Stocks that went up more than 5%—call.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上涨超过5%的股票 - 看涨。
- en: Stocks that went down more than 5%—put.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下跌超过5%的股票 - 看跌。
- en: The remaining stocks are in the neutral category.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 剩余的股票属于中性类别。
- en: Rather than train a regression model on how much stocks will go up, we can now
    train a classification model with these three classes and pick the most confident
    predictions from our model.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 不再训练回归模型来预测股票将上涨多少，而是可以用这三个类别训练分类模型，并从模型中选择最可信的预测。
- en: 'Design Pattern 10: Rebalancing'
  id: totrans-348
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计模式10：再平衡
- en: The Rebalancingdesign pattern provides various approaches for handling datasets
    that are inherently imbalanced. By this we mean datasets where one label makes
    up the majority of the dataset, leaving far fewer examples of other labels.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 重新平衡设计模式提供了处理固有不平衡数据集的各种方法。我们指的是数据集中一个标签占据大部分数据集，其他标签的示例明显较少。
- en: This design pattern does *not* address scenarios where a dataset lacks representation
    for a specific population or real-world environment. Cases like this can often
    only be solved by additional data collection. The Rebalancingdesign pattern primarily
    addresses how to build models with datasets where few examples exist for a specific
    class or classes.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 此设计模式并*不*解决数据集缺乏特定人群或现实环境表征的情况。这类情况通常只能通过额外的数据收集来解决。重新平衡设计模式主要解决的是如何使用少量类别或类别的数据集构建模型的问题。
- en: Problem
  id: totrans-351
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Machine learning models learn best when they are given a similar number of examples
    for each label class in a dataset. Many real-world problems, however, are not
    so neatly balanced. Take for example a fraud detection use case, where you are
    building a model to identify fraudulent credit card transactions. Fraudulent transactions
    are much rarer than regular transactions, and as such, there is less data on fraud
    cases available to train a model. The same is true for other problems like detecting
    whether someone will default on a loan, identifying defective products, predicting
    the presence of a disease given medical images, filtering spam emails, flagging
    error logs in a software application, and more.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型在数据集中每个标签类别给出相似数量的示例时，机器学习模型学习效果最佳。然而，许多现实世界的问题并不如此平衡。例如，考虑欺诈检测用例，您正在构建一个模型来识别欺诈信用卡交易。欺诈交易远比常规交易罕见，因此用于训练模型的欺诈案例数据较少。对于其他问题，如预测是否会有人贷款违约、识别有缺陷的产品、根据医学图像预测疾病的存在、过滤垃圾邮件、标记软件应用程序中的错误日志等，情况也是如此。
- en: Imbalanced datasets apply to many types of models, including binary classification,
    multiclass classification, multilabel classification, and regression. In regression
    cases, imbalanced datasets refer to data with outlier values that are either much
    higher or lower than the median in your dataset.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 不平衡数据集适用于许多类型的模型，包括二元分类、多类分类、多标签分类和回归。在回归情况下，不平衡数据集指的是数据中具有远高于或远低于数据集中位数的异常值。
- en: A common pitfall in training models with imbalanced label classes is relying
    on misleading accuracy values for model evaluation. If we train a fraud detection
    model and only 5% of our dataset contains fraudulent transactions, chances are
    our model will train to 95% accuracy without any modifications to the dataset
    or underlying model architecture. While this 95% accuracy number is *technically*
    correct, there’s a good chance the model is guessing the majority class (in this
    case, nonfraud) for each example. As such, it’s not learning anything about how
    to distinguish the minority class from other examples in our dataset.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练具有不平衡标签类别的模型时，常见的陷阱是依赖于误导性的准确率值进行模型评估。如果我们训练一个欺诈检测模型，而我们的数据集仅包含 5% 的欺诈交易，那么我们的模型有可能在没有对数据集或底层模型架构进行任何修改的情况下训练到
    95% 的准确率。虽然这个 95% 的准确率在技术上是正确的，但模型很有可能是在每个示例中猜测多数类（在本例中是非欺诈类）。因此，它并没有学习如何区分少数类与数据集中其他示例的差异。
- en: To avoid leaning too much on this misleading accuracy value, it’s worth looking
    at the model’s confusion matrix to see accuracy for each class. The confusion
    matrix for a poorly performing model trained on an imbalanced dataset often looks
    something like [Figure 3-18](#confusion_matrix_for_a_model_trained_on).
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免过度依赖这种误导性的准确率值，值得查看模型的混淆矩阵，以查看每个类别的准确率。在不平衡数据集上训练的表现不佳的模型的混淆矩阵通常看起来像图 3-18
    中描述的样子。
- en: '![Confusion matrix for a model trained on an imbalanced dataset without dataset
    or model adjustments.](Images/mldp_0318.png)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![在未对数据集或模型进行调整的情况下训练的不平衡数据集的混淆矩阵。](Images/mldp_0318.png)'
- en: Figure 3-18\. Confusion matrix for a model trained on an imbalanced dataset
    without dataset or model adjustments.
  id: totrans-357
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-18\. 在未对数据集或模型进行调整的情况下训练的不平衡数据集的混淆矩阵。
- en: In this example, the model correctly guesses the majority class 95% of the time,
    but only guesses the minority class correctly 12% of the time. Typically, the
    confusion matrix for a high performing model has percentages close to 100 down
    the diagonal.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，模型在大多数情况下能够正确猜测出主要类别，但只有12%的时间能够正确猜测出少数类别。通常，高性能模型的混淆矩阵在对角线上的百分比接近100%。
- en: Solution
  id: totrans-359
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'First, since accuracy can be misleading on imbalanced datasets, it’s important
    to choose an appropriate evaluation metric when building our model. Then, there
    are various techniques we can employ for handling inherently imbalanced datasets
    at both the dataset and model level. *Downsampling* changes the balance of our
    underlying dataset, while *weighting* changes how our model handles certain classes.
    *Upsampling* duplicates examples from our minority class, and often involves applying
    augmentations to generate additional samples. We’ll also look at approaches for
    *reframing* the problem: changing it to a regression task, analyzing our model’s
    error values for each example, or clustering.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，由于在不平衡数据集上准确度可能具有误导性，因此在构建我们的模型时选择适当的评估指标非常重要。然后，我们可以在数据集和模型级别上采用各种技术来处理固有的不平衡数据集。*降采样*改变了我们基础数据集的平衡，而*加权*改变了我们的模型如何处理某些类别。*过采样*从我们的少数类别中复制示例，并且通常涉及应用增强以生成额外的样本。我们还将研究*重新构架*问题的方法：将其改变为回归任务，分析我们模型对每个示例的错误值，或进行聚类。
- en: Choosing an evaluation metric
  id: totrans-361
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择评估指标
- en: For imbalanced datasets like the one in our fraud detection example, it’s best
    to use metrics like precision, recall, or F-measure to get a complete picture
    of how our model is performing. *Precision* measures the percentage of positive
    classifications that were correct out of all positive predictions made by the
    model. Conversely, *recall* measures the proportion of actual positive examples
    that were identified correctly by the model. The biggest difference between these
    two metrics is the denominator used to calculate them. For precision, the denominator
    is the total number of positive class predictions made by our model. For recall,
    it is the number of *actual* positive class examples present in our dataset.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像我们的欺诈检测示例中那样不平衡的数据集，最好使用精度、召回率或F-度量来全面了解我们的模型表现如何。*精度*衡量了模型正确预测出所有正预测中正确的积极分类的百分比。相反，*召回率*衡量了模型正确识别出的实际正例的比例。这两个指标之间的最大区别在于用于计算它们的分母。对于精度，分母是我们的模型做出的积极类别预测的总数。对于召回率，它是我们数据集中*实际*正类示例的数量。
- en: 'A perfect model would have both precision and recall of 1.0, but in practice,
    these two measures are often at odds with each other. The *F-measure* is a metric
    that ranges from 0 to 1 and takes both precision and recall into account. It is
    calculated as:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 完美的模型应该具有精度和召回率均为1.0，但实际上，这两个指标通常是互相对立的。*F-度量*是一个从0到1的度量，同时考虑了精度和召回率。它的计算公式如下：
- en: '[PRE25]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Let’s return to the fraud detection use case to see how each of these metrics
    plays out in practice. For this example, let’s say our test set contains a total
    of 1,000 examples, 50 of which should be labeled as fraudulent transactions. For
    these examples, our model predicts 930/950 nonfraudulent examples correctly, and
    15/50 fraudulent examples correctly. We can visualize these results in [Figure 3-19](#sample_predictions_for_a_fraud_detectio).
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到欺诈检测用例，看看这些指标在实践中如何发挥作用。例如，假设我们的测试集包含总共1,000个示例，其中有50个应标记为欺诈交易。对于这些示例，我们的模型正确预测出了930/950个非欺诈示例，以及15/50个欺诈示例。我们可以在[图3-19](#sample_predictions_for_a_fraud_detectio)中可视化这些结果。
- en: '![Sample predictions for a fraud detection model.](Images/mldp_0319.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![一个欺诈检测模型的样本预测。](Images/mldp_0319.png)'
- en: Figure 3-19\. Sample predictions for a fraud detection model.
  id: totrans-367
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-19\. 一个欺诈检测模型的样本预测。
- en: In this case, our model’s precision is 15/35 (42%), recall is 15/50 (30%), and
    F-measure is 35%. These do a much better job capturing our model’s inability to
    correctly identify fraudulent transactions compared to accuracy, which is 945/1000
    (94.5%). Therefore, for models trained on imbalanced datasets, metrics other than
    accuracy are preferred. In fact, accuracy may even go down when optimizing for
    these metrics, but that is OK since precision, recall, and F-score are a better
    indication of model performance in this case.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们模型的精度为 15/35（42%），召回率为 15/50（30%），而 F-度量为 35%。与准确率相比，这些指标更能有效捕捉到我们模型在正确识别欺诈交易方面的能力不足。准确率为
    945/1000（94.5%）。因此，在训练于不平衡数据集上的模型中，除准确率外的其他度量标准更受青睐。实际上，在优化这些指标时，准确率甚至可能会下降，但这没关系，因为在这种情况下，精度、召回率和
    F-度量更能体现模型的性能。
- en: Note that, when evaluating models trained on imbalanced datasets, we need to
    use *unsampled data* when calculating success metrics. This means that no matter
    how we modify our dataset for training per the solutions we’ll outline below,
    we should leave our test set as is so that it provides an accurate representation
    of the original dataset. In other words, our test set should have roughly the
    same class balance as the original dataset. For the example above, that would
    be 5% fraud/95% nonfraud.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在评估训练于不平衡数据集上的模型时，我们需要在计算成功度量标准时使用*未经采样的数据*。这意味着无论我们如何修改我们的数据集用于训练，我们将在下面概述的解决方案中，都应保持测试集的原样，以便它能够准确地反映原始数据集。换句话说，我们的测试集应该与原始数据集具有大致相同的类别平衡。例如，在上述示例中，这将是
    5% 的欺诈交易和 95% 的非欺诈交易。
- en: If we are looking for a metric that captures the performance of the model across
    all thresholds, average precision-recall is a [more informative](https://oreil.ly/5iJX2)
    metric than area under the ROC curve (AUC) for model evaluation. This is because
    average precision-recall places more emphasis on how many predictions the model
    got right out of the *total* number it assigned to the positive class. This gives
    more weight to the positive class, which is important for imbalanced datasets.
    The AUC, on the other hand, treats both classes equally and is less sensitive
    to model improvements, which isn’t optimal in situations with imbalanced data.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们正在寻找一个能够捕捉模型在所有阈值下性能的指标，平均精度-召回率是一个[更具信息性](https://oreil.ly/5iJX2)的指标，比
    ROC 曲线下面积（AUC）更好用于模型评估。这是因为平均精度-召回率更加强调模型在*总体*分配给正类的预测中有多少个预测是正确的。这更加重视正类，这对于不平衡数据集是很重要的。而AUC则平等对待两个类别，对于模型改进不太敏感，这在不平衡数据的情况下并不理想。
- en: Downsampling
  id: totrans-371
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**降采样**'
- en: Downsampling is a solution for handling imbalanced datasets by changing the
    underlying dataset, rather than the model. With downsampling, we decrease the
    number of examples from the majority class used during model training. To see
    how this works, let’s take a look at the [synthetic fraud detection dataset on
    Kaggle](https://oreil.ly/WqUM-).^([4](ch03.xhtml#ch01fn15)) Each example in the
    dataset contains various information about the transaction, including the transaction
    type, the amount of the transaction, and the account balance both before and after
    the transaction took place. The dataset contains 6.3 million examples, only 8,000
    of which are fraudulent transactions. That’s a mere 0.1% of the entire dataset.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 降采样是一种处理不平衡数据集的解决方案，通过改变底层数据集而不是模型。通过降采样，我们减少了在模型训练过程中使用的多数类示例数量。为了看看这是如何运作的，让我们来看看[Kaggle
    上的合成欺诈检测数据集](https://oreil.ly/WqUM-)。^([4](ch03.xhtml#ch01fn15)) 数据集中的每个示例都包含有关交易的各种信息，包括交易类型、交易金额以及交易发生前后的账户余额。该数据集包含
    630 万个示例，其中仅有 8,000 个是欺诈交易。这仅占整个数据集的 0.1%。
- en: While a large dataset can often improve a model’s ability to identify patterns,
    it’s less helpful when the data is significantly imbalanced. If we [train a model](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/03_problem_representation/rebalancing.ipynb)
    on this entire dataset (6.3M rows) without any modifications, chances are we’ll
    see a misleading accuracy of 99.9% as a result of the model randomly guessing
    the nonfraudulent class each time. We can solve for this by removing a large chunk
    of the majority class from the dataset.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大数据集通常能够提高模型识别模式的能力，但在数据显著不平衡的情况下，其帮助作用就不那么大了。如果我们[在整个数据集上训练模型](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/03_problem_representation/rebalancing.ipynb)（630
    万行），而没有进行任何修改，很可能会看到误导性的准确率达到 99.9%，因为模型每次随机猜测非欺诈类。我们可以通过移除数据集中大部分多数类示例来解决这个问题。
- en: 'We’ll take all 8,000 of the fraudulent examples and set them aside to use when
    training the model. Then, we’ll take a small, random sample of the nonfraudulent
    transactions. We’ll then combine with our 8,000 fraudulent examples, reshuffle
    the data, and use this new, smaller dataset to train a model. Here’s how we could
    implement this with pandas:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将所有8,000个欺诈示例分开设置，以便在训练模型时使用。然后，我们将随机抽取少量非欺诈交易。然后，我们将与我们的8,000个欺诈示例组合，重新洗牌数据，并使用这个新的、较小的数据集来训练模型。以下是我们如何可以用pandas实现这一点的方式：
- en: '[PRE26]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Following this, our dataset would contain 25% fraudulent transactions, much
    more balanced than the original dataset with only 0.1% in the minority class.
    It’s worth experimenting with the exact balance used when downsampling. Here we
    used a 25/75 split, but different problems might require closer to a 50/50 split
    to achieve decent accuracy.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们的数据集将包含25％的欺诈交易，比原始数据集中仅0.1％的少数类更加平衡。在进行下采样时，值得尝试不同的精确平衡。在这里，我们使用了一个25/75的分割，但是不同的问题可能需要接近50/50的分割才能达到良好的准确性。
- en: 'Downsampling is usually combined with the Ensemble pattern, following these
    steps:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: Downsampling通常与Ensemble模式结合使用，按以下步骤进行：
- en: Downsample the majority class and use all the instances of the minority class.
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对多数类进行下采样，并使用少数类的所有实例。
- en: Train a model and add it to the ensemble.
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个模型并将其添加到集成中。
- en: Repeat.
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复。
- en: During inference, take the median output of the ensemble models.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在推断期间，获取集成模型的中位数输出。
- en: We discussed a classification example here, but downsampling can also be applied
    to regression models where we’re predicting a numerical value. In this case, taking
    a random sample of majority class samples will be more nuanced since the majority
    “class” in our data includes a range of values rather than a single label.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里讨论了一个分类示例，但是下采样也可以应用于回归模型，其中我们预测数值。在这种情况下，由于我们的数据中的多数“类”包括一系列值而不是单个标签，从多数类样本中随机抽取样本将更加微妙。
- en: Weighted classes
  id: totrans-383
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加权类别
- en: Another approach to handling imbalanced datasets is to change the *weight* our
    model gives to examples from each class. Note that this is a different use of
    the term “weight” than the weights (or parameters) learned by our model during
    training, which you cannot set manually. By weighting *classes*, we tell our model
    to treat specific label classes with more importance during training. We’ll want
    our model to assign more weight to examples from the minority class. Exactly how
    much importance your model should give to certain examples is up to you, and is
    a parameter you can experiment with.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 处理不平衡数据集的另一种方法是改变模型为每个类别示例赋予的*权重*。请注意，这是与训练期间学习的权重（或参数）不同的“权重”用法，您无法手动设置。通过加权*类别*，我们告诉模型在训练期间对特定标签类别给予更多重视。我们希望模型为来自少数类的示例分配更多权重。您的模型应该为某些示例分配多少重要性，完全取决于您，这是一个可以进行实验的参数。
- en: 'In Keras, we can pass a `class_weights` parameter to our model when we train
    it with `fit()`. The parameter `class_weights` is a dict, mapping each class to
    the weight Keras should assign to examples from that class. But how should we
    determine the exact weights for each class? The class weight values should relate
    to the balance of each class in our dataset. For example, if the minority class
    accounts for only 0.1% of the dataset, a reasonable conclusion is that our model
    should treat examples from that class with 1000× more weight than the majority
    class. In practice, it’s common to divide this weight value by 2 for each class
    so that the average weight of an example is *1.0*. Therefore, given a dataset
    with 0.1% of values representing the minority class, we could calculate the class
    weights with the following code:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中，当我们使用`fit()`训练模型时，可以传递`class_weights`参数给我们的模型。`class_weights`参数是一个字典，将每个类映射到Keras应分配给该类示例的权重。但是，我们应该如何确定每个类的确切权重呢？类权重值应与数据集中每个类的平衡相关联。例如，如果少数类仅占数据集的0.1％，合理的结论是，我们的模型应该以比多数类高1000倍的权重处理该类的示例。在实践中，通常将该权重值除以2以使每个类的平均示例权重为*1.0*。因此，对于仅包含0.1％少数类值的数据集，我们可以使用以下代码计算类权重：
- en: '[PRE27]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We’d then pass these weights to our model during training:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在训练模型时传递这些权重：
- en: '[PRE28]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In BigQuery ML, we can set `AUTO_CLASS_WEIGHTS = True` in the `OPTIONS` block
    when creating our model to have different classes weighted based on their frequency
    of occurrence in the training data.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 在BigQuery ML中，我们可以在创建模型时的`OPTIONS`块中设置`AUTO_CLASS_WEIGHTS = True`，以便根据训练数据中的频率对不同类别进行加权。
- en: While it can be helpful to follow a heuristic of class balance for setting class
    weights, the business application of a model might also dictate the class weights
    we choose to assign. For example, let’s say we have a model classifying images
    of defective products. If the cost of shipping a defective product is 10 times
    that of incorrectly classifying a normal product, we would choose 10 as the weight
    for our minority class.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然遵循类平衡启发式设置类权重可能有所帮助，但模型的业务应用可能也会决定我们选择分配的类权重。例如，假设我们有一个分类缺陷产品图像的模型。如果运输缺陷产品的成本是错误分类正常产品的10倍，我们将选择10作为少数类的权重。
- en: Upsampling
  id: totrans-391
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 上采样
- en: Another common technique for handling imbalanced datasets is *upsampling*. With
    upsampling, we overrepresent our minority class by both replicating minority class
    examples and generating additional, synthetic examples. This is often done in
    combination with downsampling the majority class. This approach—combining downsampling
    and upsampling—was proposed in 2002 and referred to as Synthetic Minority Over-sampling
    Technique ([SMOTE](https://oreil.ly/CFJPz)). SMOTE provides an algorithm that
    constructs these synthetic examples by analyzing the feature space of minority
    class examples in the dataset and then generates similar examples within this
    feature space using a nearest neighbors approach. Depending on how many similar
    data points we choose to consider at once (also referred to as the number of nearest
    neighbors), the SMOTE approach randomly generates a new minority class example
    between these points.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 处理不平衡数据集的另一种常见技术是*上采样*。通过上采样，我们通过复制少数类示例和生成额外的合成示例来过度表示我们的少数类。通常与减少多数类同时进行。这种方法——结合减少和上采样——在2002年提出，并称为合成少数过采样技术（[SMOTE](https://oreil.ly/CFJPz)）。SMOTE
    提供了一种算法，通过分析数据集中少数类示例的特征空间来构造这些合成示例，然后使用最近邻方法在这个特征空间内生成类似的示例。根据我们选择同时考虑多少相似数据点（也称为最近邻数），SMOTE
    方法随机生成这些点之间的新的少数类示例。
- en: Let’s look at the Pima Indian [Diabetes Dataset](https://oreil.ly/ljqnc) to
    see how this works at a high level. 34% of this dataset contains examples of patients
    who had diabetes, so we’ll consider this our minority class. [Table 3-3](#a_subset_of_features_for_two_training_e)
    shows a subset of columns for two minority class examples.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以高层次来看 Pima 印第安人[糖尿病数据集](https://oreil.ly/ljqnc)来看看这是如何工作的。这个数据集中有34%的例子是患有糖尿病的患者，所以我们将其视为我们的少数类。[表格 3-3](#a_subset_of_features_for_two_training_e)显示了两个少数类样本的列的子集。
- en: Table 3-3\. A subset of features for two training examples from the minority
    class (has diabetes) in the Pima Indian Diabetes Dataset
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 3-3\. Pima 印第安人糖尿病数据集中少数类（患有糖尿病）的两个训练样本的特征子集
- en: '| Glucose | BloodPressure | SkinThickness | BMI |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| 葡萄糖 | 血压 | 皮肤厚度 | BMI |'
- en: '| --- | --- | --- | --- |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 148 | 72 | 35 | 33.6 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| 148 | 72 | 35 | 33.6 |'
- en: '| 183 | 64 | 0 | 23.3 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| 183 | 64 | 0 | 23.3 |'
- en: A new, synthetic example based on these two actual examples from the dataset
    might look like [Table 3-4](#a_synthetic_example_generated_from_the), calculating
    by the midpoint between each of these column values.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 基于数据集中这两个实际样本的新的合成示例可能看起来像[表格 3-4](#a_synthetic_example_generated_from_the)，通过计算每个列值之间的中点。
- en: Table 3-4\. A synthetic example generated from the two minority training examples
    using the SMOTE approach
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 3-4\. 使用 SMOTE 方法从两个少数训练样本生成的合成示例的合成示例
- en: '| Glucose | BloodPressure | SkinThickness | BMI |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| 葡萄糖 | 血压 | 皮肤厚度 | BMI |'
- en: '| --- | --- | --- | --- |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 165.5 | 68 | 17.5 | 28.4 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| 165.5 | 68 | 17.5 | 28.4 |'
- en: The SMOTE technique refers primarily to tabular data, but similar logic can
    be applied to image datasets. For example, if we’re building a model to distinguish
    between Bengal and Siamese cats and only 10% of our dataset contains images of
    Bengals, we can generate additional variations of the Bengal cats in our dataset
    through image augmentation using the Keras `ImageDataGenerator` class. With a
    few parameters, this class will generate multiple variations of the same image
    by rotating, cropping, adjusting brightness, and more.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: SMOTE 技术主要涉及表格数据，但类似的逻辑也可以应用于图像数据集。例如，如果我们正在构建一个模型来区分孟加拉和暹罗猫，并且我们的数据集中仅包含10%的孟加拉猫图像，我们可以使用
    Keras `ImageDataGenerator` 类通过图像增强生成数据集中孟加拉猫的附加变化。使用几个参数，这个类将通过旋转、裁剪、调整亮度等方式生成同一图像的多个变化。
- en: Trade-Offs and Alternatives
  id: totrans-405
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 权衡与替代方案
- en: 'There are a few other alternative solutions for building models with inherently
    imbalanced datasets, including reframing the problem and handling cases of anomaly
    detection. We’ll also explore several important considerations for imbalanced
    datasets: overall dataset size, the optimal model architectures for different
    problem types, and explaining minority class prediction.'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有固有不平衡数据集的构建模型，还有一些其他替代解决方案，包括重新构建问题和处理异常检测案例。我们还将探讨不平衡数据集的几个重要考虑因素：整体数据集大小，不同问题类型的最优模型架构以及解释少数类预测。
- en: Reframing and Cascade
  id: totrans-407
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重新构建和级联
- en: Reframing the problem is another approach for handling imbalanced datasets.
    First, we might consider switching the problem from classification to regression
    or vice versa utilizing the techniques described in the Reframing design pattern
    section and training a cascade of models. For example, let’s say we have a regression
    problem where the majority of our training data falls within a certain range,
    with a few outliers. Assuming we care about predicting outlier values, we could
    convert this to a classification problem by bucketing the majority of the data
    in one bucket and the outliers in another.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 重新构建问题是处理不平衡数据集的另一种方法。首先，我们可以考虑将问题从分类转换为回归或者利用重新构建设计模式部分中描述的技术来进行反向操作并训练一系列模型。例如，假设我们有一个回归问题，其中大多数训练数据落在某个范围内，有一些异常值。假设我们关心预测异常值，我们可以通过将大多数数据放在一个桶中，将异常值放在另一个桶中，将这个问题转换为分类问题。
- en: 'Imagine we’re building a model to predict baby weight using the BigQuery natality
    dataset. Using pandas, we can create a histogram of a sample of the baby weight
    data to see the weight distribution:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 想象我们正在构建一个使用BigQuery生育数据集预测婴儿体重的模型。使用pandas，我们可以创建一个样本的婴儿体重数据直方图以查看体重分布：
- en: '[PRE29]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[Figure 3-20](#a_histogram_depicting_the_distribution) shows the resulting
    histogram.'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-20](#a_histogram_depicting_the_distribution) 展示了得到的直方图。'
- en: '![A histogram depicting the distribution of baby weight for 10,000 examples
    in the BigQuery natality dataset.](Images/mldp_0320.png)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
  zh: '![展示BigQuery生育数据集中1万个例子的婴儿体重分布的直方图。](Images/mldp_0320.png)'
- en: Figure 3-20\. A histogram depicting the distribution of baby weight for 10,000
    examples in the BigQuery natality dataset.
  id: totrans-414
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-20\. 展示了BigQuery生育数据集中1万个例子的婴儿体重分布的直方图。
- en: 'If we count the number of babies weighing 3 lbs in the entire dataset, there
    are approximately 96,000 (.06% of the data). Babies weighing 12 lbs make up only
    .05% of the dataset. To get good regression performance over the entire range,
    we can combine downsampling with the Reframing and Cascade design patterns. First,
    we’ll split the data into three buckets: “underweight,” “average,” and “overweight.”
    We can do that with the following query:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们统计整个数据集中重量为3磅的婴儿数量，约为96,000个（占数据的0.06%）。重量为12磅的婴儿仅占数据集的0.05%。为了在整个范围内获得良好的回归性能，我们可以结合降采样和重新构建与级联设计模式。首先，我们将数据分成三个桶：“欠重”，“平均”和“超重”。我们可以使用以下查询完成：
- en: '[PRE31]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[Table 3-5](#the_percentage_of_each_weight_class_pre) shows the results.'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 3-5](#the_percentage_of_each_weight_class_pre) 展示了结果。'
- en: Table 3-5\. The percentage of each weight class present in the natality dataset
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3-5\. 生育数据集中每个重量类的百分比
- en: '| weight | num_examples | percent_of_dataset |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| 重量 | 样本数 | 数据集百分比 |'
- en: '| --- | --- | --- |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Average | 123781044 | 0.8981 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 123781044 | 0.8981 |'
- en: '| Underweight | 9649724 | 0.07 |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| 欠重 | 9649724 | 0.07 |'
- en: '| Overweight | 4395995 | 0.0319 |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| 超重 | 4395995 | 0.0319 |'
- en: 'For demo purposes, we’ll take 100,000 examples from each class to train a model
    on an updated, balanced dataset:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示目的，我们将从每个类别中取100,000个示例来训练更新的平衡数据集上的模型：
- en: '[PRE32]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can save the results of that query to a table, and with a more balanced
    dataset, we can now train a classification model to label babies as “underweight,”
    “average,” or “overweight”:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将该查询的结果保存到一个表中，并且通过一个更平衡的数据集，现在我们可以训练一个分类模型来标记婴儿为“欠重”，“平均”或“超重”：
- en: '[PRE33]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Another approach is to use the Cascade pattern, training three separate regression
    models for each class. Then, we can use our multidesign pattern solution by passing
    our initial classification model an example and using the result of that classification
    to decide which regression model to send the example to for numeric prediction.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用级联模式，为每个类别训练三个单独的回归模型。然后，我们可以使用我们的多设计模式解决方案，通过将我们的初始分类模型传递给一个例子并使用该分类的结果来决定将该例子发送到哪个回归模型进行数值预测。
- en: Anomaly detection
  id: totrans-429
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 异常检测
- en: 'There are two approaches to handling regression models for imbalanced datasets:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 处理不平衡数据集的回归模型有两种方法：
- en: Use the model’s error on a prediction as a signal.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用模型对预测的错误作为信号。
- en: Cluster incoming data and compare the distance of each new data point to existing
    clusters.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对传入数据进行聚类，并比较每个新数据点与现有聚类的距离。
- en: To better understand each solution, let’s say we’re training a model on data
    collected by a sensor to predict temperature in the future. In this case, we’d
    need the model output to be a numerical value.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解每种解决方案，我们假设正在训练一个模型，用传感器收集的数据预测未来的温度。在这种情况下，我们需要模型输出为数值。
- en: For the first approach—using error as a signal—after training a model, we would
    then compare the model’s predicted value with the actual value for the current
    point in time. If there was a significant difference between the predicted and
    actual current value, we could flag the incoming data point as an anomaly. Of
    course, this requires a model trained with good accuracy on enough historical
    data to rely on its quality for future predictions. The main caveat for this approach
    is that it requires us to have new data readily available, so that we can compare
    the incoming data with the model’s prediction. As a result, it works best for
    problems involving streaming or time-series data.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一种方法——使用错误作为信号——在训练模型后，我们将比较模型对当前时点的预测值与实际值。如果预测值与当前实际值之间存在显著差异，我们可以将传入的数据点标记为异常。当然，这需要模型在足够多的历史数据上训练得到良好的准确度，以便依赖其质量进行未来的预测。这种方法的主要限制是需要我们能够及时获得新数据，以便将传入数据与模型预测进行比较。因此，它最适合涉及流数据或时间序列数据的问题。
- en: 'In the second approach—clustering data—we start by building a model with a
    clustering algorithm, a modeling technique that organizes our data into clusters.
    Clustering is an *unsupervised learning* method, meaning it looks for patterns
    in the dataset without any knowledge of ground truth labels. A common clustering
    algorithm is k-means, which we can implement with BigQuery ML. The following shows
    how to train a k-means model on the BigQuery natality dataset using three features:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是——聚类数据——我们首先使用聚类算法构建模型，这是一种将数据组织成簇的建模技术。聚类是一种*无监督学习*方法，意味着它在没有任何地面真实标签知识的情况下查找数据集中的模式。一个常见的聚类算法是k均值，我们可以在BigQuery
    ML中实现它。以下展示了如何在BigQuery natality数据集上使用三个特征训练k均值模型：
- en: '[PRE34]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The resulting model will cluster our data into four groups. Once the model
    has been created, we can then generate predictions on new data and look at that
    prediction’s distance from existing clusters. If the distance is high, we can
    flag the data point as an anomaly. To generate a cluster prediction on our model,
    we can run the following query, passing it a made-up average example from the
    dataset:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 结果模型将把我们的数据聚类成四组。一旦模型创建完成，我们可以对新数据生成预测，并查看该预测与现有聚类的距离。如果距离很大，我们可以将数据点标记为异常。要在我们的模型上生成一个聚类预测，我们可以运行以下查询，传递给它数据集中的一个假设平均示例：
- en: '[PRE35]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The query results in [Table 3-6](#the_distance_between_our_average_weight) show
    us the distance between this data point and the model’s generated clusters, called
    centroids.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 查询结果在[表 3-6](#the_distance_between_our_average_weight)中显示了这个数据点与模型生成的称为质心的聚类之间的距离。
- en: Table 3-6\. The distance between our average weight example data point and each
    of the clusters generated by our k-means model
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3-6\. 我们平均体重示例数据点与由我们的k均值模型生成的每个聚类之间的距离
- en: '| CENTROID_ID | NEAREST_CENTROIDS_DISTANCE.CENTROID_ID | NEAREST_CENTROIDS_DISTANCE.DISTANCE
    |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| 质心ID | 最近质心距离.质心ID | 最近质心距离.距离 |'
- en: '| --- | --- | --- |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 4 | 4 | 0.29998627812137374 |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 4 | 0.29998627812137374 |'
- en: '| 1 | 1.2370167418282159 |  |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1.2370167418282159 |  |'
- en: '| 2 | 1.376651161584178 |  |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1.376651161584178 |  |'
- en: '| 3 | 1.6853517159990536 |  |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1.6853517159990536 |  |'
- en: This example clearly fits into centroid 4, as seen by the small distance (.29).
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例明显属于质心4，如距离小（.29）所示。
- en: We can compare this to the results we get if we send an outlier, underweight
    example to the model, as shown in [Table 3-7](#the_distance_between_our_underweight_ex).
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 将此与我们将异常值、体重不足的示例发送到模型后得到的结果进行比较，如[表 3-7](#the_distance_between_our_underweight_ex)所示。
- en: Table 3-7\. The distance between our underweight example data point and each
    of the clusters generated by our k-means model
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3-7\. 我们体重不足示例数据点与由我们的k均值模型生成的每个聚类之间的距离
- en: '| CENTROID_ID | NEAREST_CENTROIDS_DISTANCE.CENTROID_ID | NEAREST_CENTROIDS_DISTANCE.DISTANCE
    |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| CENTROID_ID | NEAREST_CENTROIDS_DISTANCE.CENTROID_ID | NEAREST_CENTROIDS_DISTANCE.DISTANCE
    |'
- en: '| --- | --- | --- |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 3 | 3 | 3.061985789261998 |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 3 | 3.061985789261998 |'
- en: '| 4 | 3.3124603501734966 |  |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 3.3124603501734966 |  |'
- en: '| 2 | 4.330205096751425 |  |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 4.330205096751425 |  |'
- en: '| 1 | 4.658614918595627 |  |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 4.658614918595627 |  |'
- en: Here, the distance between this example and each centroid is quite large. We
    could then use these high-distance values to conclude that this data point might
    be an anomaly. This unsupervised clustering approach is especially useful if we
    don’t know the labels for our data in advance. Once we’ve generated cluster predictions
    on enough examples, we could then build a supervised learning model using the
    predicted clusters as labels.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，该示例与每个质心之间的距离非常大。然后，我们可以使用这些高距离值来推断此数据点可能是异常值。如果我们事先不知道数据的标签，这种无监督聚类方法尤其有用。一旦我们对足够的示例生成了集群预测，我们就可以建立一个使用预测的集群作为标签的监督学习模型。
- en: Number of minority class examples available
  id: totrans-457
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可用的少数类示例数量
- en: While the minority class in our first fraud detection example only made up 0.1%
    of the data, the dataset was large enough that we still had 8,000 fraudulent data
    points to work with. For datasets with even fewer examples of the minority class,
    downsampling may make the resulting dataset too small for a model to learn from.
    There isn’t a hard-and-fast rule for determining how many examples is too few
    to use downsampling, since it largely depends on our problem and model architecture.
    A general rule of thumb is that if you only have hundreds of examples of the minority
    class, you might want to consider a solution other than downsampling for handling
    dataset imbalance.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们第一个欺诈检测示例中的少数类仅占数据的0.1%，但数据集足够大，我们仍然有8,000个欺诈数据点可供使用。对于甚至少有少数类示例的数据集，降采样可能会使结果数据集过小，不利于模型学习。对于决定使用降采样时的最小示例数量并没有硬性规则，因为这在很大程度上取决于我们的问题和模型架构。一个经验法则是，如果你只有几百个少数类示例，你可能需要考虑除降采样外的其他解决方案来处理数据集不平衡问题。
- en: It’s also worth noting that the natural effect of removing a subset of our majority
    class is losing some information stored in those examples. This might slightly
    decrease our model’s ability to identify the majority class, but often the benefits
    of downsampling still outweigh this.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，删除我们大多数类别的子集的自然效果是丢失这些示例中存储的一些信息。这可能会略微降低我们模型识别多数类别的能力，但通常情况下，降采样的好处仍然超过了这一点。
- en: Combining different techniques
  id: totrans-460
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结合不同的技术
- en: The downsampling and class weight techniques described above can be combined
    for optimal results. To do this, we start by downsampling our data until we find
    a balance that works for our use case. Then, based on the label ratios for the
    rebalanced dataset, use the method described in the weighted classes section to
    pass new weights to our model. Combining these approaches can be especially useful
    when we have an anomaly detection problem and care most about predictions for
    our minority class. For example, if we’re building a fraud detection model, we’re
    likely much more concerned about the transactions our model flags as “fraud” rather
    than the ones it flags as “nonfraud.” Additionally, as mentioned by SMOTE, the
    approach of generating synthetic examples from the minority class is often combined
    with removing a random sample of examples from the minority class.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 上面描述的降采样和类别权重技术可以结合以获得最佳结果。为此，我们首先通过降采样我们的数据，直到找到适合我们用例的平衡点。然后，基于重新平衡数据集的标签比率，使用加权类别部分描述的方法向我们的模型传递新的权重。当我们面临异常检测问题并且最关心少数类预测时，结合这些方法尤为有用。例如，如果我们正在构建一个欺诈检测模型，我们可能更关心我们的模型标记为“欺诈”的交易，而不是标记为“非欺诈”的交易。此外，正如SMOTE所提到的，从少数类生成合成示例的方法通常与从少数类中移除随机示例相结合。
- en: Downsampling is also often combined with the Ensemble design pattern. Using
    this approach, instead of entirely removing a random sample of our majority class,
    we use different subsets of it to train multiple models and then ensemble those
    models. To illustrate this, let’s say we have a dataset with 100 minority class
    examples and 1,000 majority examples. Rather than removing 900 examples from our
    majority class to perfectly balance the dataset, we’d randomly split the majority
    examples into 10 groups with 100 examples each. We’d then train 10 classifiers,
    each with the same 100 examples from our minority class and 100 different, randomly
    selected values from our majority class. The bagging technique illustrated in
    [Figure 3-11](#bagging_is_good_for_decreasing_variance) would work well for this
    approach.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 下采样通常也与集成设计模式结合使用。采用这种方法，我们不是完全移除多数类的随机样本，而是使用其不同子集来训练多个模型，然后集成这些模型。举个例子，假设我们有一个数据集，其中包含
    100 个少数类示例和 1,000 个多数类示例。与其从多数类中移除 900 个示例以完全平衡数据集，我们将多数示例随机分为 10 组，每组包含 100 个示例。然后我们会训练
    10 个分类器，每个分类器使用相同的少数类示例和从多数类随机选择的不同 100 个示例。图 3-11 中展示的装袋技术对这种方法非常有效。
- en: In addition to combining these data-centric approaches, we can also adjust the
    threshold for our classifier to optimize for precision or recall depending on
    our use case. If we care more that our model is correct whenever it makes a positive
    class prediction, we’d optimize our prediction threshold for recall. This can
    apply in any situation where we want to avoid false positives. Alternatively,
    if it is more costly to *miss* a potential positive classification even when we
    might get it wrong, we optimize our model for recall.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 除了结合这些数据中心的方法外，我们还可以根据使用案例调整分类器的阈值以优化精度或召回率。如果我们更关心模型在进行正类预测时的准确性，我们将优化召回率的预测阈值。这在我们希望避免假阳性的任何情况下都适用。或者，如果即使可能会出错，错过一个潜在的正分类也更加昂贵，我们就会优化模型的召回率。
- en: Choosing a model architecture
  id: totrans-464
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择模型架构
- en: Depending on our prediction task, there are different model architectures to
    consider when solving problems with the Rebalancing design pattern. If we’re working
    with tabular data and building a classification model for anomaly detection, [research](https://oreil.ly/EnAab)
    has shown that decision tree models perform well on these types of tasks. Tree-based
    models also work well on problems involving small and imbalanced datasets. XGBoost,
    scikit-learn, and TensorFlow all have methods for implementing decision tree models.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的预测任务，解决具有重新平衡设计模式的问题时，需要考虑不同的模型架构。如果我们正在处理表格数据并构建用于异常检测的分类模型，研究表明决策树模型在这类任务上表现良好。基于树的模型在处理小型和不平衡数据集的问题时也效果显著。XGBoost、scikit-learn
    和 TensorFlow 都有实现决策树模型的方法。
- en: 'We can implement a binary classifier in XGBoost with the following code:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码在 XGBoost 中实现一个二元分类器：
- en: '[PRE36]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We can use downsampling and class weights in each of these frameworks to further
    optimize our model using the Rebalancing design pattern. For example, to add weighted
    classes to our `XGBClassifier` above, we’d add a `scale_pos_weight` parameter,
    calculated based on the balance of classes in our dataset.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在每个框架中使用下采样和类权重进一步优化我们的模型，使用重新平衡设计模式。例如，要在上述`XGBClassifier`中添加加权类，我们将添加一个基于数据集中类别平衡计算的`scale_pos_weight`参数。
- en: If we’re detecting anomalies in time-series data, long short-term memory (LSTM)
    models work well for identifying patterns present in sequences. Clustering models
    are also an option for tabular data with imbalanced classes. For imbalanced datasets
    with image input, use deep learning architectures with downsampling, weighted
    classes, upsampling, or a combination of these techniques. For text data, however,
    generating synthetic data is [less straightforward](https://oreil.ly/2ai2k), and
    it’s best to rely on downsampling and weighted classes.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在时间序列数据中检测异常，长短期记忆（LSTM）模型很适合识别序列中存在的模式。聚类模型也是处理具有不平衡类别的表格数据的一个选项。对于具有图像输入的不平衡数据集，使用深度学习架构配合下采样、加权类、上采样或这些技术的组合。然而，对于文本数据来说，生成合成数据并不是那么直接，最好依赖于下采样和加权类。
- en: Regardless of the data modality we’re working with, it’s useful to experiment
    with different model architectures to see which performs best on our imbalanced
    data.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们处理的是哪种数据模态，都可以尝试不同的模型架构来确定在我们的不平衡数据上表现最佳的模型。
- en: Importance of explainability
  id: totrans-471
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解释可解释性的重要性
- en: When building models for flagging rare occurrences in data such as anomalies,
    it’s especially important to understand how our model is making predictions. This
    can both verify that the model is picking up on the correct signals to make its
    predictions and help explain the model’s behavior to end users. There are a few
    tools available to help us interpret models and explain predictions, including
    the open source framework [SHAP](https://github.com/slundberg/shap), the [What-If
    Tool](https://oreil.ly/Vf3D-), and [Explainable AI on Google Cloud](https://oreil.ly/lDocn).
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建用于标记数据中罕见事件（如异常）的模型时，特别重要的是理解我们的模型如何进行预测。 这既可以验证模型是否捕捉到正确的信号以进行预测，也可以帮助解释模型对最终用户的行为。
    有几种工具可供我们解释模型和预测，包括开源框架[SHAP](https://github.com/slundberg/shap)，[What-If Tool](https://oreil.ly/Vf3D-)和[Google
    Cloud上的可解释AI](https://oreil.ly/lDocn)。
- en: Model explanations can take many forms, one of which is called *attribution
    values*. Attribution values tell us how much each feature in our model influenced
    the model’s prediction. Positive attribution values mean a particular feature
    pushed our model’s prediction up, and negative attribution values mean the feature
    pushed our model’s prediction down. The higher the absolute value of an attribution,
    the bigger impact it had on our model’s prediction. In image and text models,
    attributions can show you the pixels or words that signaled your model’s prediction
    most. For tabular models, attributions provide numerical values for each feature,
    indicating its overall effect on the model’s prediction.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 模型解释可以采用多种形式，其中之一称为*归因值*。 归因值告诉我们模型中每个特征对模型预测的影响程度。 正面的归因值意味着某个特征推动了我们模型的预测结果上升，而负面的归因值则意味着该特征推动了我们模型的预测结果下降。
    归因值的绝对值越高，它对模型预测的影响越大。 在图像和文本模型中，归因值可以显示出对模型预测最具信号性的像素或单词。 对于表格模型，归因值为每个特征提供数值，指示其对模型预测的整体影响。
- en: After training a TensorFlow model on the synthetic fraud detection dataset from
    Kaggle and deploying it to Explainable AI on Google Cloud, let’s take a look at
    some examples of instance-level attributions. In [Figure 3-21](#feature_attributions_from_explainable_a),
    we see two example transactions that our model correctly identified as fraud,
    along with their feature attributions.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 在从Kaggle合成欺诈检测数据集上训练了TensorFlow模型并将其部署到Google Cloud上的可解释AI后，让我们看一些实例级归因的示例。
    在[图3-21](#feature_attributions_from_explainable_a)中，我们看到了两个例子交易，我们的模型正确识别为欺诈，并且显示它们的特征归因。
- en: In the first example where the model predicted a 99% chance of fraud, the old
    balance at the origin account before the transaction was made was the biggest
    indicator of fraud. In the second example, our model was 89% confident in its
    prediction of fraud with the amount of the transaction identified as the biggest
    signal of fraud. However, the balance at the origin account made our model *less
    confident* in its prediction of fraud and explains *why* the prediction confidence
    is slightly *lower* by 10 percentage points.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个例子中，模型预测有99%的欺诈可能性时，交易前的原始账户余额是欺诈的最大指标。 在第二个例子中，我们的模型对欺诈的预测有89%的信心，交易金额被确认为欺诈的最大信号。
    然而，原始账户余额使我们的模型在预测欺诈时*不太自信*，并且解释了预测置信度稍微*降低*了10个百分点的*原因*。
- en: Explanations are important for any type of machine learning model, but we can
    see how they are especially useful for models following the Rebalancing design
    pattern. When dealing with imbalanced data, it’s important to look beyond our
    model’s accuracy and error metrics to verify that it’s picking up on meaningful
    signals in our data.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 解释对任何类型的机器学习模型都很重要，但我们可以看到它们对遵循再平衡设计模式的模型特别有用。 在处理不平衡数据时，重要的是超越我们模型的准确性和误差度量，验证它是否捕捉到我们数据中的有意义信号。
- en: '![Feature attributions from Explainable AI for two correctly classified fraudulent
    transactions.](Images/mldp_0321.png)'
  id: totrans-477
  prefs: []
  type: TYPE_IMG
  zh: '![从可解释AI中获取的特征归因示例，用于两笔正确分类的欺诈交易。](Images/mldp_0321.png)'
- en: Figure 3-21\. Feature attributions from Explainable AI for two correctly classified
    fraudulent transactions.
  id: totrans-478
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-21. 从可解释AI中获取的特征归因，用于两笔正确分类的欺诈交易。
- en: Summary
  id: totrans-479
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter looked at different ways to represent a prediction task through
    the lens of model architecture and model output. Thinking about how you’ll apply
    your model can guide your decision on the type of model to build, and how to format
    your output for prediction. With this in mind, we started with the *Reframing*
    design pattern, which explores changing your problem from a regression task to
    a classification task (or vice versa) to improve the quality of your model. You
    can do this by reformatting the label column in your data. Next we explored the
    *Multilabel* design pattern, which addresses cases where an input to your model
    can be associated with more than one label. To handle this case, use the sigmoid
    activation function on your output layer with binary cross entropy loss.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了通过模型架构和模型输出的角度来表示预测任务的不同方式。考虑如何应用模型可以指导您在构建模型类型和格式化输出以进行预测时的决策。在这方面，我们从*Reframing*设计模式开始，探讨将问题从回归任务转换为分类任务（反之亦然）以提高模型质量的方法。您可以通过重新格式化数据中的标签列来实现这一点。接下来，我们探讨了*Multilabel*设计模式，该模式处理了输入模型可以与多个标签相关联的情况。为处理此类情况，可以在输出层使用sigmoid激活函数和二元交叉熵损失。
- en: Whereas the Reframing and Multilabel patterns focus on formatting model *output*,
    the *Ensemble* design pattern addresses model *architecture* and includes various
    methods for combining multiple models to improve upon machine learning results
    from a single model. Specifically, the Ensemble pattern includes bagging, boosting,
    and stacking—all different techniques for aggregating multiple models into one
    ML system. The *Cascade*design pattern is also a model-level approach, and involves
    breaking a machine learning problem into several smaller problems. Unlike ensemble
    models, the Cascade pattern requires outputs from an initial model to be inputs
    into downstream models. Because of the complexity cascade models can create, you
    should only use them when you have a scenario where the initial classification
    labels are disparate and equally important.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 而Reframing和Multilabel模式专注于格式化模型*输出*，*Ensemble*设计模式则涉及模型*架构*，包括各种组合多个模型以改进单一模型机器学习结果的方法。具体来说，Ensemble模式包括装袋、提升和堆叠等不同的技术，用于将多个模型聚合成一个ML系统。*Cascade*设计模式也是一种模型级方法，涉及将机器学习问题分解为几个较小的问题。与集成模型不同，Cascade模式要求将初始模型的输出作为下游模型的输入。由于级联模型可能带来的复杂性，只有在初始分类标签不同且同等重要的情况下才应使用它们。
- en: Next, we looked at the *Neutral Class*design pattern, which addresses problem
    representation at the output level. This pattern improves a binary classifier
    by adding a third “neutral” class. This is useful in cases where you want to capture
    arbitrary or less-polarizing classifications that don’t fall into either of the
    distinct binary categories. Finally, the *Rebalancing* design pattern provides
    solutions for cases where you have an inherently imbalanced dataset. This pattern
    proposes using downsampling, weighted classes, or specific reframing techniques
    to solve for datasets with imbalanced label classes.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们介绍了*Neutral Class*设计模式，该模式解决了输出级别的问题表示。该模式通过添加第三个“中性”类来改进二元分类器。在想要捕捉不属于两个明显二元类别之一的任意或较少极化分类的情况下，此模式非常有用。最后，*Rebalancing*设计模式提供了解决本质上不平衡数据集情况的解决方案。该模式建议使用降采样、加权类别或特定重构技术来解决标签类别不平衡的数据集。
- en: Chapters [2](ch02.xhtml#data_representation_design_patterns) and [3](#problem_representation_design_patterns)
    focused on the initial steps for structuring your machine learning problem, specifically
    formatting input data, model architecture options, and model output representation.
    In the next chapter, we’ll navigate the next step in the machine learning workflow—design
    patterns for training models.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 第[2](ch02.xhtml#data_representation_design_patterns)章和第[3](#problem_representation_design_patterns)章专注于结构化机器学习问题的初始步骤，具体包括格式化输入数据、模型架构选项和模型输出表示。在下一章中，我们将探讨机器学习工作流程的下一步——用于训练模型的设计模式。
- en: '^([1](ch03.xhtml#ch01fn12-marker)) For the explicit computation of these values,
    see Ian Goodfellow, Yoshua Bengio, and Aaron Courville, *Deep Learning* (Cambridge,
    MA: MIT Press, 2016), Ch. 7.'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch03.xhtml#ch01fn12-marker)) 要计算这些值，请参阅Ian Goodfellow、Yoshua Bengio和Aaron
    Courville的《深度学习》（剑桥，马萨诸塞州：MIT出版社，2016年），第7章。
- en: ^([2](ch03.xhtml#ch01fn13-marker)) This is just an example being used for illustrative
    purposes; please don’t take this as medical advice!
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch03.xhtml#ch01fn13-marker)) 这只是用于说明目的的示例，请不要将其视为医疗建议！
- en: ^([3](ch03.xhtml#ch01fn14-marker)) See [*https://oreil.ly/kDndF*](https://oreil.ly/kDndF)
    for a primer on call and put options.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch03.xhtml#ch01fn14-marker)) 请参阅[*https://oreil.ly/kDndF*](https://oreil.ly/kDndF)了解期权的基础知识。
- en: '^([4](ch03.xhtml#ch01fn15-marker)) The dataset was generated based on the PaySim
    research proposed in this paper: EdgarLopez-Rojas , Ahmad Elmir, and Stefan Axelsson,
    “PaySim: A financial mobile money simulator for fraud detection,” *28th European
    Modeling and Simulation Symposium,* EMSS, Larnaca, Cyprus (2016): 249–255.'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch03.xhtml#ch01fn15-marker)) 数据集基于本文提出的PaySim研究生成：EdgarLopez-Rojas，Ahmad
    Elmir和Stefan Axelsson，“PaySim：用于欺诈检测的金融移动支付模拟器”，*第28届欧洲建模与仿真研讨会*，EMSS，塞浦路斯拉纳卡（2016年）：249–255。
