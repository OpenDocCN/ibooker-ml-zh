- en: Chapter 11\. The Human Side of Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章 机器学习的人文面
- en: Throughout this book, we’ve covered many technical aspects of designing an ML
    system. However, ML systems aren’t just technical. They involve business decision
    makers, users, and, of course, developers of the systems. We’ve discussed stakeholders
    and their objectives in Chapters [1](ch01.xhtml#overview_of_machine_learning_systems)
    and [2](ch02.xhtml#introduction_to_machine_learning_system). In this chapter,
    we’ll discuss how users and developers of ML systems might interact with these
    systems.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们涵盖了设计机器学习系统的许多技术方面。然而，机器学习系统不仅仅是技术性的。它涉及到业务决策者、用户，当然还有系统的开发者。我们在第[1](ch01.xhtml#overview_of_machine_learning_systems)章和第[2](ch02.xhtml#introduction_to_machine_learning_system)章中讨论了利益相关者及其目标。在本章中，我们将讨论用户和机器学习系统的开发者如何与这些系统互动。
- en: We’ll first consider how user experience might be altered and affected due to
    the probabilistic nature of ML models. We’ll continue to discuss organizational
    structure to allow different developers of the same ML system to work together
    effectively. We’ll end the chapter with how ML systems can affect the society
    as a whole in the section [“Responsible AI”](#responsible_ai).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 首先考虑到由于机器学习模型的概率性质可能会改变和影响用户体验。我们将继续讨论组织结构，以允许同一机器学习系统的不同开发者有效地合作。我们将在章节[“负责任的人工智能”](#responsible_ai)结束本章。
- en: User Experience
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用户体验
- en: We’ve discussed at length how ML systems behave differently from traditional
    software systems. First, ML systems are probabilistic instead of deterministic.
    Usually, if you run the same software on the same input twice at different times,
    you can expect the same result. However, if you run the same ML system twice at
    different times on the exact same input, you might get different results.^([1](ch11.xhtml#ch01fn371))
    Second, due to this probabilistic nature, ML systems’ predictions are mostly correct,
    and the hard part is we usually don’t know for what inputs the system will be
    correct! Third, ML systems can also be large and might take an unexpectedly long
    time to produce a prediction.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们长期讨论了机器学习系统与传统软件系统的行为不同之处。首先，机器学习系统是概率性的而不是确定性的。通常情况下，如果你在不同时间两次运行相同输入的软件，你可以期望得到相同的结果。然而，如果你在不同时间两次运行完全相同输入的机器学习系统，你可能会得到不同的结果。^([1](ch11.xhtml#ch01fn371))
    其次，由于这种概率性质，机器学习系统的预测大多数情况下是正确的，困难的部分在于我们通常不知道对于什么样的输入系统会正确！第三，机器学习系统也可能很庞大，并且可能需要意料之外的长时间来产生预测。
- en: These differences mean that ML systems can affect user experience differently,
    especially for users that have so far been used to traditional software. Due to
    the relatively new usage of ML in the real world, how ML systems affect user experience
    is still not well studied. In this section, we’ll discuss three challenges that
    ML systems pose to good user experience and how to address them.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这些差异意味着机器学习系统可能会以不同的方式影响用户体验，特别是对于迄今为止习惯于传统软件的用户。由于机器学习在现实世界中的相对新用法，机器学习系统如何影响用户体验还没有得到很好的研究。在本节中，我们将讨论机器学习系统对良好用户体验提出的三个挑战以及如何应对它们。
- en: Ensuring User Experience Consistency
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确保用户体验的一致性
- en: When using an app or a website, users expect a certain level of consistency.
    For example, I’m used to Chrome having their “minimize” button on the top left
    corner on my MacBook. If Chrome moved this button to the right, I’d be confused,
    even frustrated.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用应用程序或网站时，用户期望有一定的一致性水平。例如，我习惯于Chrome在我MacBook的左上角有他们的“最小化”按钮。如果Chrome把这个按钮移到右边，我会感到困惑，甚至会感到沮丧。
- en: ML predictions are probabilistic and inconsistent, which means that predictions
    generated for one user today might be different from what will be generated for
    the same user the next day, depending on the context of the predictions. For tasks
    that want to leverage ML to improve users’ experience, the inconsistency in ML
    predictions can be a hindrance.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习预测具有概率性和不一致性，这意味着今天为一个用户生成的预测可能会与下一天为同一用户生成的预测不同，这取决于预测的背景。对于希望利用机器学习来改善用户体验的任务来说，机器学习预测的不一致性可能是一种阻碍。
- en: To make this concrete, consider a [case study](https://oreil.ly/qBLV2) published
    by Booking.com in 2020\. When you book accommodations on Booking.com, there are
    about 200 filters you can use to specify your preferences, such as “breakfast
    included,” “pet friendly,” and “non-smoking rooms.” There are so many filters
    that it takes time for users to find the filters that they want. The applied ML
    team at Booking.com wanted to use ML to automatically suggest filters that a user
    might want, based on the filters they’ve used in a given browsing session.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，考虑一下由Booking.com在2020年发布的[案例研究](https://oreil.ly/qBLV2)。当您在Booking.com上预订住宿时，有大约200个筛选器可供您选择，例如“包括早餐”，“宠物友好”和“无烟房间”。有这么多筛选器，用户需要一些时间才能找到他们想要的筛选器。Booking.com的应用ML团队希望使用ML来根据用户在特定浏览会话中使用的筛选器自动建议用户可能需要的筛选器。
- en: The challenge they encountered is that if their ML model kept suggesting different
    filters each time, users could get confused, especially if they couldn’t find
    a filter that they had already applied before. The team resolved this challenge
    by creating a rule to specify the conditions in which the system must return the
    same filter recommendations (e.g., when the user has applied a filter) and the
    conditions in which the system can return new recommendations (e.g., when the
    user changes their destination). This is known as the consistency–accuracy trade-off,
    since the recommendations deemed most accurate by the system might not be the
    recommendations that can provide user consistency.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 他们遇到的挑战是，如果他们的ML模型每次都建议不同的筛选器，用户可能会感到困惑，特别是如果他们找不到他们之前已经应用过的筛选器。团队通过创建规则来解决这一挑战，规定系统必须返回相同的筛选器推荐条件（例如，用户已应用筛选器时），以及系统可以返回新推荐条件的条件（例如，用户更改目的地时）。这被称为一致性和准确性的权衡，因为系统认为最准确的推荐可能并不是能够提供用户一致性的推荐。
- en: Combatting “Mostly Correct” Predictions
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对抗“大多正确”的预测
- en: In the previous section, we talked about the importance of ensuring the consistency
    of a model’s predictions. In this section, we’ll talk about how, in some cases,
    we want less consistency and more diversity in a model’s predictions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，我们谈到了确保模型预测一致性的重要性。在本节中，我们将讨论在某些情况下，我们希望模型的预测 less 一致性 and more diversity
    in a model’s predictions
- en: Since 2018, the large language model [GPT](https://oreil.ly/sY39d) and its successors,
    [GPT-2](https://oreil.ly/TttNU) and [GPT-3](https://oreil.ly/ug9P4), have been
    taking the world by storm. An advantage of these large language models is that
    they’re able to generate predictions for a wide range of tasks with little to
    no task-specific training data required. For example, you can use the requirements
    for a web page as an input to the model, and it’ll output the React code needed
    to create that web page, as shown in [Figure 11-1](#gpt_three_can_help_you_write_code_for_y).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自2018年以来，大型语言模型[GPT](https://oreil.ly/sY39d)及其后继者[GPT-2](https://oreil.ly/TttNU)和[GPT-3](https://oreil.ly/ug9P4)席卷了全球。这些大型语言模型的优势在于，它们能够在几乎不需要任务特定训练数据的情况下生成广泛任务的预测。例如，您可以将网页要求作为模型的输入，并输出所需的React代码来创建该网页，如[图11-1](#gpt_three_can_help_you_write_code_for_y)所示。
- en: '![](Images/dmls_1101.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_1101.png)'
- en: 'Figure 11-1\. GPT-3 can help you write code for your website. Source: Adapted
    from screenshots of a video by [Sharif Shameem](https://oreil.ly/VEuml)'
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-1. GPT-3可以帮助您为网站编写代码。来源：根据[Sharif Shameem](https://oreil.ly/VEuml)的视频截图调整
- en: However, a drawback of these models is that these predictions are not always
    correct, and it’s very expensive to fine-tune them on task-specific data to improve
    their predictions. These mostly correct predictions can be useful for users who
    can easily correct them. For example, in the case of customer support, for each
    customer request, ML systems can produce mostly correct responses and the human
    operators can quickly edit those responses. This can speed up the response compared
    to having to write the response from scratch.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些模型的一个缺点是，这些预测并不总是正确的，并且在任务特定数据上进行微调以改进它们的预测非常昂贵。这些大多正确的预测对于可以轻松纠正它们的用户是有用的。例如，在客户支持的情况下，对于每个客户请求，ML系统可以生成大多正确的响应，而人工操作员可以快速编辑这些响应。这可以加快响应速度，而不必从头开始编写响应。
- en: However, these mostly correct predictions won’t be very useful if users don’t
    know how to or can’t correct the responses. Consider the same task of using a
    language model to generate React code for a web page. The generated code might
    not work, or if it does, it might not render to a web page that meets the specified
    requirements. A React engineer might be able to fix this code quickly, but many
    users of this application might not know React. And this application might attract
    a lot of users who don’t know React—that’s why they needed this app in the first
    place!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果用户不知道如何或不能纠正响应，这些大部分正确的预测就不会很有用。考虑使用语言模型生成网页的 React 代码的相同任务。生成的代码可能无法工作，或者即使工作，也可能无法呈现出满足指定要求的网页。React
    工程师可能能够快速修复这些代码，但是这个应用的许多用户可能不了解 React。这个应用可能会吸引许多不了解 React 的用户，这也是他们最初需要此应用程序的原因！
- en: To overcome this, an approach is to show users multiple resulting predictions
    for the same input to increase the chance of at least one of them being correct.
    These predictions should be rendered in a way that even nonexpert users can evaluate
    them. In this case, given a set of requirements input by users, you can have the
    model produce multiple snippets of React code. The code snippets are rendered
    into visual web pages so that nonengineering users can evaluate which one is the
    best for them.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，一种方法是向用户展示同一输入的多个预测结果，以增加至少有一个预测结果正确的机会。这些预测结果应该以一种即使非专家用户也能评估的方式呈现出来。在这种情况下，根据用户输入的一组要求，您可以让模型生成多个
    React 代码片段。这些代码片段被呈现成视觉网页，以便非工程背景的用户可以评估哪一个对他们最好。
- en: This approach is very common and is sometimes called “human-in-the-loop” AI,
    as it involves humans to pick the best predictions or to improve on the machine-generated
    predictions. For readers interested in human-in-the-loop AI, I’d highly recommend
    Jessy Lin’s [“Rethinking Human-AI Interaction”](https://oreil.ly/6o4pu).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法非常常见，有时被称为“人在回路中的 AI”，因为它涉及人类来选择最佳预测结果或改进机器生成的预测结果。对于对人在回路 AI 感兴趣的读者，我强烈推荐
    Jessy Lin 的[“重新思考人工智能与人类互动”](https://oreil.ly/6o4pu)。
- en: Smooth Failing
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 平滑失败
- en: We’ve talked at length about the effect of an ML model’s inference latency on
    user experience in the section [“Computational priorities”](ch01.xhtml#computational_priorities).
    We’ve also discussed how to compress models and optimize them for faster inference
    speed in the section [“Model Compression”](ch07.xhtml#model_compression). However,
    normally fast models might still take time with certain queries. This can happen
    especially with models that deal with sequential data like language models or
    time-series models—e.g., the model takes longer to process long series than shorter
    series. What should we do with the queries where models take too long to respond?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在章节[“计算优先级”](ch01.xhtml#computational_priorities)中详细讨论了 ML 模型推断延迟对用户体验的影响。我们还讨论了如何压缩模型并优化其以获得更快的推断速度，在章节[“模型压缩”](ch07.xhtml#model_compression)中。然而，即使是通常速度快的模型在处理某些查询时可能仍然需要时间。这在处理序列数据（如语言模型或时间序列模型）的模型中尤为常见——例如，模型处理长序列比处理短序列需要更长的时间。对于模型响应时间过长的查询，我们应该怎么办？
- en: 'Some companies that I’ve worked with use a backup system that is less optimal
    than the main system but is guaranteed to generate predictions quickly. These
    systems can be heuristics or simple models. They can even be cached precomputed
    predictions. This means that you might have a rule that specifies: if the main
    model takes longer than *X* milliseconds to generate predictions, use the backup
    model instead. Some companies, instead of having this simple rule, have another
    model to predict how long it’ll take the main model to generate predictions for
    a given query, and route that prediction to either the main model or the backup
    model accordingly. Of course, this added model might also add extra inference
    latency to your system.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾与一些公司合作，它们使用一种备用系统，其效果不如主系统好，但能够快速生成预测结果。这些系统可以是启发式的或简单的模型。它们甚至可以是预先计算的缓存预测结果。这意味着您可能有一条规则，指定：如果主模型生成预测结果的时间超过*X*毫秒，则使用备用模型。有些公司并不采用这种简单的规则，而是使用另一个模型来预测主模型生成给定查询的预测结果需要多长时间，并根据该预测将其路由到主模型或备用模型。当然，这种额外的模型可能还会增加系统的推断延迟。
- en: 'This is related to the speed–accuracy trade-off: a model might have worse performance
    than another model but can do inference much faster. This less-optimal but fast
    model might give users worse predictions but might still be preferred in situations
    where latency is crucial. Many companies have to choose one model over another,
    but with a backup system, you can do both.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这与速度与准确性的权衡有关：一个模型可能比另一个模型的性能差，但可以进行推理速度更快。这种次优但快速的模型可能会给用户带来较差的预测结果，但在延迟至关重要的情况下可能仍然更受欢迎。许多公司不得不在一个模型和另一个模型之间进行选择，但有备份系统的情况下，可以同时实现两者。
- en: Team Structure
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 团队结构
- en: 'An ML project involves not only data scientists and ML engineers, but also
    other types of engineers such as DevOps engineers and platform engineers as well
    as nondeveloper stakeholders like subject matter experts (SMEs). Given a diverse
    set of stakeholders, the question is what is the optimal structure when organizing
    ML teams. We’ll focus on two aspects: cross-functional teams collaboration and
    the much debated role of an end-to-end data scientist.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个机器学习项目涉及不仅仅是数据科学家和机器学习工程师，还有其他类型的工程师，如DevOps工程师和平台工程师，以及非开发人员利益相关者，如主题专家（SMEs）。面对多样化的利益相关者，问题在于在组织机器学习团队时什么是最优结构。我们将关注两个方面：跨职能团队的协作以及端到端数据科学家角色的争议。
- en: Cross-functional Teams Collaboration
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跨职能团队协作
- en: SMEs (doctors, lawyers, bankers, farmers, stylists, etc.) are often overlooked
    in the design of ML systems, but many ML systems wouldn’t work without subject
    matter expertise. They’re not only users but also developers of ML systems.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: SME（医生、律师、银行家、农民、造型师等）在设计机器学习系统时经常被忽视，但许多机器学习系统在没有主题专家的情况下无法正常工作。他们不仅仅是用户，也是机器学习系统的开发者。
- en: 'Most people only think of subject matter expertise during the data labeling
    phase—e.g., you’d need trained professionals to label whether a CT scan of a lung
    shows signs of cancer. However, as training ML models becomes an ongoing process
    in production, labeling and relabeling might also become an ongoing process spanning
    the entire project lifecycle. An ML system would benefit a lot to have SMEs involved
    in the rest of the lifecycle, such as problem formulation, feature engineering,
    error analysis, model evaluation, reranking predictions, and user interface: how
    to best present results to users and/or to other parts of the system.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人只在数据标注阶段考虑主题专业知识 —— 比如，你需要训练有素的专业人员来标注肺部CT扫描是否显示癌症迹象。然而，随着训练机器学习模型成为生产中的持续过程，标注和重新标注可能也会成为跨整个项目生命周期的持续过程。机器学习系统在整个生命周期中都涉及主题专家的参与将极大地受益，例如问题定义、特征工程、错误分析、模型评估、重新排名预测以及用户界面：如何最好地向用户和/或系统的其他部分呈现结果。
- en: There are many challenges that arise from having multiple different profiles
    working on a project. For example, how do you explain ML algorithms’ limitations
    and capacities to SMEs who might not have engineering or statistical backgrounds?
    To build an ML system, we want everything to be versioned, but how do you translate
    domain expertise (e.g., if there’s a small dot in this region between X and Y
    then it might be a sign of cancer) into code and version that?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 多个不同背景的人员参与项目会带来许多挑战。例如，如何向没有工程或统计背景的主题专家解释机器学习算法的限制和能力？为了构建机器学习系统，我们希望所有东西都有版本控制，但如何将领域专业知识（例如，如果在X和Y之间的这个区域有一个小点，那么可能是癌症的迹象）转化为代码并进行版本控制呢？
- en: Good luck trying to get your doctor to use Git.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 想让你的医生使用Git可真是个好运气。
- en: It’s important to involve SMEs early on in the project planning phase and empower
    them to make contributions without having to burden engineers to give them access.
    For example, to help SMEs get more involved in the development of ML systems,
    many companies are building no-code/low-code platforms that allow people to make
    changes without writing code. Most of the no-code ML solutions for SMEs are currently
    at the labeling, quality assurance, and feedback stages, but more platforms are
    being developed to aid in other critical junctions such as dataset creation and
    views for investigating issues that require SME input.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在项目规划阶段早期就让主题专家参与并赋予他们无需依赖工程师即可做出贡献的能力是很重要的。例如，为了帮助主题专家更多地参与机器学习系统的开发，许多公司正在构建允许人们在不编写代码的情况下进行更改的无代码/低代码平台。目前大多数面向主题专家的无代码机器学习解决方案主要集中在标注、质量保证和反馈阶段，但正在开发更多的平台以帮助解决其他关键环节，如数据集创建和用于调查需要主题专家输入问题的视图。
- en: End-to-End Data Scientists
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 端到端数据科学家
- en: Through this book, I hope I’ve convinced you that ML production is not just
    an ML problem but also an infrastructure problem. To do MLOps, we need not only
    ML expertise but also Ops (operational) expertise, especially around deployment,
    containerization, job orchestration, and workflow management.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本书，我希望能说服你，ML的生产不仅是一个ML问题，而且也是一个基础设施问题。要进行MLOps，我们不仅需要ML专业知识，还需要运维（操作）专业知识，特别是关于部署、容器化、作业编排和工作流管理的专业知识。
- en: 'To be able to bring all these areas of expertise into an ML project, companies
    tend to follow one of the two following approaches: have a separate team to manage
    all the Ops aspects or include data scientists on the team and have them own the
    entire process.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够将所有这些专业领域融入到机器学习项目中，公司倾向于遵循以下两种方法之一：建立一个独立的团队来管理所有运维方面，或者将数据科学家纳入团队并让他们负责整个过程。
- en: Let’s take a closer look at how each of these approaches works in practice.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看这些方法在实践中是如何运作的。
- en: 'Approach 1: Have a separate team to manage production'
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 方法1：建立一个独立的团队来管理生产
- en: 'In this approach, the data science/ML team develops models in the dev environment.
    Then a separate team, usually the Ops/platform/ML engineering team, production­i⁠zes
    the models in prod. This approach makes hiring easier as it’s easier to hire people
    with one set of skills instead of people with multiple sets of skills. It might
    also make life easier for each person involved, as they only have to focus on
    one concern (e.g., developing models or deploying models). However, this approach
    has many drawbacks:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，数据科学/ML团队在开发环境中开发模型。然后，通常是运维/平台/ML工程团队将模型投入生产。这种方法使得招聘更容易，因为只需招聘具有一组技能的人，而不是多组技能的人。这也可能使每个参与者的生活更轻松，因为他们只需专注于一个问题（例如，开发模型或部署模型）。然而，这种方法有许多缺点：
- en: Communication and coordination overhead
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 沟通和协调开销
- en: A team can become blockers for other teams. According to Frederick P. Brooks,
    “What one programmer can do in one month, two programmers can do in two months.”
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 团队可能会成为其他团队的阻碍。根据Frederick P. Brooks的说法：“一个程序员在一个月内能完成的工作，两个程序员在两个月内才能完成。”
- en: Debugging challenges
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 调试挑战
- en: When something fails, you don’t know whether your team’s code or some other
    team’s code might have caused it. It might not have been because of your company’s
    code at all. You need cooperation from multiple teams to figure out what’s wrong.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当某些事情失败时，你不知道是你团队的代码还是其他团队的代码可能引起的。这可能根本不是因为你公司的代码。你需要从多个团队那里得到合作，才能找出问题所在。
- en: Finger-pointing
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 推卸责任
- en: Even when you’ve figured out what went wrong, each team might think it’s another
    team’s responsibility to fix it.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你已经弄清楚了问题出在哪里，每个团队可能会认为解决问题是另一个团队的责任。
- en: Narrow context
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 狭窄的上下文
- en: No one has visibility into the entire process to optimize/improve it. For example,
    the platform team has ideas on how to improve the infrastructure but they can
    only act on requests from data scientists, but data scientists don’t have to deal
    with infrastructure so they have less incentives to proactively make changes to
    it.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 没有人能够全面了解整个过程以进行优化/改进。例如，平台团队对如何改进基础设施有想法，但他们只能根据数据科学家的请求行动，而数据科学家不必处理基础设施，因此他们没有动力积极地对其进行变更。
- en: 'Approach 2: Data scientists own the entire process'
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 方法2：数据科学家负责整个过程
- en: In this approach, the data science team also has to worry about productionizing
    models. Data scientists become grumpy unicorns, expected to know everything about
    the process, and they might end up writing more boilerplate code than data science.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，数据科学团队还必须关注模型的产品化。数据科学家成为抱怨的独角兽，人们期望他们对整个过程了如指掌，他们可能最终写的代码比数据科学还多。
- en: 'About a year ago, I [tweeted](https://oreil.ly/DPpt0) about a set of skills
    I thought was important to become an ML engineer or data scientist, as shown in
    [Figure 11-2](#i_used_to_think_that_a_data_scientist_w). The list covers almost
    every part of the workflow: querying data, modeling, distributed training, and
    setting up endpoints. It even includes tools like Kubernetes and Airflow.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 大约一年前，我在[Twitter上发推](https://oreil.ly/DPpt0)，谈到成为机器学习工程师或数据科学家所需的一组重要技能，如[图11-2](#i_used_to_think_that_a_data_scientist_w)所示。该列表涵盖了工作流的几乎每个部分：数据查询、建模、分布式训练以及设置端点。甚至包括像Kubernetes和Airflow这样的工具。
- en: '![](Images/dmls_1102.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_1102.png)'
- en: Figure 11-2\. I used to think that a data scientist would need to know all these
    things
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-2。我曾经认为数据科学家需要了解所有这些事情
- en: The tweet seems to resonate with my audience. Eugene Yan also wrote about how
    “data scientists should be more end-to-end.”^([2](ch11.xhtml#custom_ch11fn1))
    Eric Colson, Stitch Fix’s chief algorithms officer (who previously was also VP
    data science and engineering at Netflix), wrote a post on “the power of the full-stack
    data science generalist and the perils of division of labor through function.”^([3](ch11.xhtml#custom_ch11fn2))
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这条推文似乎引起了我的观众共鸣。尤金·扬（Eugene Yan）还写道，“数据科学家应该更加端到端。”^([2](ch11.xhtml#custom_ch11fn1))
    Stitch Fix 的首席算法官埃里克·科尔森（Eric Colson）（之前也是 Netflix 的副总裁兼数据科学与工程主管）在一篇文章中写道，“全栈数据科学通才的力量以及职能分工的危险。”^([3](ch11.xhtml#custom_ch11fn2))
- en: When I wrote that tweet, I believed that Kubernetes was essential to the ML
    workflow. This sentiment came from the frustration at my own job—my life as an
    ML engineer would’ve been much easier if I was more fluent with K8s.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当我写那条推文时，我相信 Kubernetes 对机器学习工作流程至关重要。这种情绪来自于我对自己工作的挫败感——如果我对 K8s 更加熟练，作为一名
    ML 工程师的生活会更轻松。
- en: However, as I learned more about low-level infrastructure, I realized how unreasonable
    it is to expect data scientists to know about it. Infrastructure requires a very
    different set of skills from data science. In theory, you can learn both sets
    of skills. In practice, the more time you spend on one means the less time you
    spend on the other. I love Erik Bernhardsson’s analogy that expecting data scientists
    to know about infrastructure is like expecting app developers to know about how
    Linux kernels work.^([4](ch11.xhtml#ch01fn372)) I joined an ML company because
    I wanted to spend more time with data, not with spinning up AWS instances, writing
    Dockerfiles, scheduling/scaling clusters, or debugging YAML configuration files.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着我对底层基础设施了解的增加，我意识到期望数据科学家了解这些是多么不切实际的。基础设施需要与数据科学完全不同的技能集。理论上，你可以学习两种技能。实际上，你在其中一种上花费的时间越多，就会在另一种上花费的时间越少。我喜欢埃里克·伯恩哈德森（Erik
    Bernhardsson）的比喻，认为期望数据科学家了解基础设施就像期望应用开发者了解 Linux 内核一样。^([4](ch11.xhtml#ch01fn372))
    我加入了一个 ML 公司，是因为我想花更多时间处理数据，而不是花时间启动 AWS 实例、编写 Dockerfile、调度/扩展集群或调试 YAML 配置文件。
- en: For data scientists to own the entire process, we need good tools. In other
    words, we need good infrastructure.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据科学家来说，要想拥有整个流程，我们需要良好的工具。换句话说，我们需要良好的基础设施。
- en: What if we have an abstraction to allow data scientists to own the process end-to-end
    without having to worry about infrastructure?
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个抽象层，允许数据科学家在不必担心基础设施的情况下端到端地拥有整个过程会怎么样？
- en: What if I can just tell this tool, “Here’s where I store my data (S3), here
    are the steps to run my code (featurizing, modeling), here’s where my code should
    run (EC2 instances, serverless stuff like AWS Batch, Function, etc.), here’s what
    my code needs to run at each step (dependencies),” and then this tool manages
    all the infrastructure stuff for me?
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我只需告诉这个工具，“这是我存储数据的地方（S3），这是运行我的代码的步骤（特征化、建模），这是我的代码应该运行的地方（EC2 实例、AWS 批处理、函数等服务器端自动化），每个步骤需要什么样的代码运行环境（依赖关系）”，然后这个工具为我管理所有的基础设施事务？
- en: According to both Stitch Fix and Netflix, the success of a full-stack data scientist
    relies on the tools they have. They need tools that “abstract the data scientists
    from the complexities of containerization, distributed processing, automatic failover,
    and other advanced computer science concepts.”^([5](ch11.xhtml#ch01fn373))
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 Stitch Fix 和 Netflix 的说法，全栈数据科学家的成功取决于他们拥有的工具。他们需要能够“从容器化、分布式处理、自动故障转移和其他高级计算机科学概念的复杂性中抽象出数据科学家的工具。”^([5](ch11.xhtml#ch01fn373))
- en: In Netflix’s model, the specialists—people who originally owned a part of the
    project—first create tools that automate their parts, as shown in [Figure 11-3](#full_cycle_developers_at_netflixquotati).
    Data scientists can leverage these tools to own their projects end-to-end.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Netflix 的模式中，专家们——最初负责项目一部分的人——首先创建自动化其部分的工具，如 [图 11-3](#full_cycle_developers_at_netflixquotati)
    所示。数据科学家可以利用这些工具端到端地拥有他们的项目。
- en: '![](Images/dmls_1103.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_1103.png)'
- en: 'Figure 11-3\. Full-cycle developers at Netflix. Source: Adapted from an image
    by Netflix^([6](ch11.xhtml#ch01fn374))'
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-3\. Netflix 的全周期开发者。来源：根据 Netflix 的图片修改而来^([6](ch11.xhtml#ch01fn374))
- en: 'We’ve talked about how ML systems might affect user experience and how organizational
    structure might influence productivity of ML projects. In the second half of this
    chapter, we’ll focus on an even more crucial consideration: how ML systems might
    affect society and what ML system developers should do to ensure that the systems
    they develop do more good than harm.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了ML系统可能如何影响用户体验以及组织结构可能如何影响ML项目的生产力。在本章的后半部分，我们将集中讨论更为关键的问题：ML系统可能如何影响社会，以及ML系统开发者应该做什么来确保他们开发的系统产生更多的利益而非伤害。
- en: Responsible AI
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负责任的AI
- en: This section was written with generous contributions from [Abhishek Gupta](https://oreil.ly/AGJHF),
    founder and principal researcher at the [Montreal AI Ethics Institute](https://montrealethics.ai).
    His work focuses on applied technical and policy measures to build ethical, safe,
    and inclusive AI systems.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 本节内容由[Abhishek Gupta](https://oreil.ly/AGJHF)，蒙特利尔人工智能伦理研究所的创始人兼首席研究员慷慨贡献撰写。他的工作侧重于应用技术和政策措施，构建伦理、安全和包容性的AI系统。
- en: Note
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: The question of how to make intelligent systems responsible is relevant not
    only to ML systems but also general artificial intelligence (AI) systems. AI is
    a broader term that includes ML. Therefore, in this section, we use AI instead
    of ML.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如何使智能系统负责任的问题不仅与ML系统相关，还涉及到一般的人工智能（AI）系统。AI是一个比ML更广泛的术语。因此，在本节中，我们使用AI而不是ML。
- en: Responsible AI is the practice of designing, developing, and deploying AI systems
    with good intention and sufficient awareness to empower users, to engender trust,
    and to ensure fair and positive impact to society. It consists of areas like fairness,
    privacy, transparency, and accountability.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的AI是以良好的意图和足够的意识设计、开发和部署AI系统的实践，以赋予用户权力，建立信任，并确保对社会产生公平和积极影响。它包括公平性、隐私、透明度和问责性等领域。
- en: These terms are no longer just philosophical musings, but serious considerations
    for both policy makers and everyday practitioners. Given ML is being deployed
    into almost every aspect of our lives, failing to make our ML systems fair and
    ethical can lead to catastrophic consequences, as outlined in the book *Weapons
    of Math Destruction* (Cathy O’Neil, Crown Books, 2016), and through other case
    studies mentioned throughout this book.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这些术语不再只是哲学的沉思，而是政策制定者和日常从业者的严肃考虑。鉴于ML正在被部署到我们生活的几乎每个方面，未能使我们的ML系统公平和道德可能导致灾难性后果，正如《数学毁灭的武器》（Cathy
    O’Neil，Crown Books，2016）所概述的那样，以及本书中提到的其他案例研究。
- en: As developers of ML systems, you have the responsibility not only to think about
    how your systems will impact users and society at large, but also to help all
    stakeholders better realize their responsibilities toward the users by concretely
    implementing ethics, safety, and inclusivity into your ML systems. This section
    is a brief introduction to what can happen when insufficient efforts are spent
    to make ML systems responsible. We’ll start with two case studies of quite unfortunate
    and public failures of ML. We will then propose a preliminary framework for data
    scientists and ML engineers to select the tools and guidelines that best help
    with making your ML systems responsible.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 作为ML系统开发者，你不仅要考虑你的系统如何影响用户和整个社会，还要通过具体实施伦理、安全和包容性帮助所有利益相关者更好地认识他们对用户的责任。本节是关于未能充分努力使ML系统负责的情况简要介绍。我们将从两个ML非常不幸和公开的失败案例开始。然后，我们将为数据科学家和ML工程师提出一个初步框架，以选择最能帮助他们使其ML系统负责的工具和指南。
- en: '*Disclaimer:* Responsible AI is a complex topic with growing literature that
    deserves its own coverage and can easily span multiple books. This section is
    far from an exhaustive guide. We only aim to give ML developers an overview to
    effectively navigate the developments in this field. Those interested in further
    reading are highly recommended to check out the following resources:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*免责声明：* 负责任的AI是一个复杂的主题，有着日益增长的文献，值得专门报道，并且很容易涵盖多本书。本节远非详尽指南。我们只旨在为ML开发者提供一个概述，以有效地引导这一领域的发展。那些对进一步阅读感兴趣的人强烈建议查阅以下资源： '
- en: '[NIST Special Publication 1270: Towards a Standard for Identifying and Managing
    Bias in Artificial Intelligence](https://oreil.ly/Glvnp)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NIST特别出版物1270：面向标识和管理人工智能偏见的标准](https://oreil.ly/Glvnp)'
- en: ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT) [publications](https://facctconference.org)
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ACM 公平性、问责性和透明度会议（ACM FAccT）](https://facctconference.org)'
- en: Trustworthy ML’s list of [recommended resources and fundamental papers](https://oreil.ly/NmLxU)
    for researchers and practitioners who want to learn more about trustworthy ML
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “值得信赖的ML”列出了[推荐资源和基础论文](https://oreil.ly/NmLxU)，供希望了解更多关于值得信赖的ML的研究人员和实践者参考。
- en: Sara Hooker’s awesome [slide deck](https://oreil.ly/upBxx) on fairness, security,
    and governance in machine learning (2022)
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sara Hooker的出色[幻灯片](https://oreil.ly/upBxx)介绍了机器学习中的公平性、安全性和治理（2022年）。
- en: Timnit Gebru and Emily Denton’s [tutorials](https://oreil.ly/jdAyF) on fairness,
    accountability, transparency, and ethics (2020)
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Timnit Gebru和Emily Denton的[教程](https://oreil.ly/jdAyF)，讨论公平性、问责性、透明度和伦理（2020年）。
- en: 'Irresponsible AI: Case Studies'
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不负责任的AI：案例研究
- en: We’ll start this section off by looking at two failures of AI systems that led
    to severe harm for not only the users of these systems but also to the organizations
    who developed the systems. We’ll trace some of the places where the organizations
    went wrong and what the practitioners could have done to potentially anticipate
    these points of failure. These highlights will serve as background as we dive
    into the engineering framework for responsible AI.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从两个AI系统失败的案例开始，这些失败严重伤害了这些系统的用户，也损害了开发这些系统的组织。我们将追溯组织哪些地方出错，以及从业者可能预见到这些失败点的方法。这些重点将作为我们深入探讨负责任AI工程框架的背景。
- en: There are other interesting examples of “AI incidents” logged at the [AI Incident
    Database](https://incidentdatabase.ai). Keep in mind that while the following
    two examples and the ones logged at AI Incident Database are the ones that caught
    attention, there are many more instances of irresponsible AI that happen silently.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他有趣的“AI事件”例子记录在[AI事件数据库](https://incidentdatabase.ai)中。请记住，尽管以下两个例子以及AI事件数据库中记录的引起注意的例子，还有许多更多的不负责任的AI事件是悄无声息地发生的。
- en: 'Case study I: Automated grader’s biases'
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 案例研究 I：自动评分系统的偏见
- en: In the summer of 2020, the United Kingdom canceled A levels, the high-stakes
    exams that determine college placement, due to the COVID-19 pandemic. Ofqual,
    the regulatory body for education and examinations in the UK, sanctioned the use
    of an automated system to assign final A-level grades to students—without them
    taking the test. According to Jones and Safak from Ada Lovelace Institute, “Awarding
    students’ grades based on teacher assessment was originally rejected by Ofqual
    on the grounds of unfairness between schools, incomparability across generations
    and devaluing of results because of grade inflation. The fairer option, Ofqual
    surmised, was to combine previous attainment data and teacher assessment to assign
    grades, using a particular statistical model—an ‘algorithm.’”^([7](ch11.xhtml#ch01fn375))
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 2020年夏季，由于COVID-19大流行，英国取消了决定大学入学的重要考试A-levels。英国教育和考试监管机构Ofqual批准使用自动化系统为学生分配最终的A-level成绩——而不需要他们参加考试。据来自Ada
    Lovelace Institute的Jones和Safak称，“根据教师评估授予学生成绩最初因不公平的学校之间、代际之间的不可比性和因成绩膨胀导致结果贬值而被Ofqual拒绝。Ofqual推测，更公平的选择是结合先前的成绩数据和教师评估使用特定的统计模型——一种‘算法’。”^([7](ch11.xhtml#ch01fn375))
- en: The results published by this algorithm, however, turned out to be unjust and
    untrustworthy. They quickly led to public outcries to get rid of it, with hundreds
    of students chanting in protest.^([8](ch11.xhtml#ch01fn376))
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这一算法发布的结果被证明是不公正和不可信的。这很快引起了公众抗议，数百名学生高呼要求废除该算法^([8](ch11.xhtml#ch01fn376))。
- en: What caused the public outcries? The first glance seems to point at the algorithm’s
    poor performance. Ofqual stated that their model, tested on 2019 data, had about
    60% average accuracy across A-level subjects.^([9](ch11.xhtml#ch01fn377)) This
    means that they expected 40% of the grades assigned by this model to be different
    from the students’ actual grades.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 造成公众哀号的原因是什么？初看似乎指向算法性能差。Ofqual称他们的模型在2019年的数据测试中，在A-level各科目中平均准确率约为60%^([9](ch11.xhtml#ch01fn377))。这意味着他们预计，通过该模型分配的成绩中，大约有40%与学生实际成绩不同。
- en: While the model’s accuracy seems low, Ofqual defended their algorithm as being
    broadly comparable to the accuracy of human graders. When comparing an examiner’s
    grades with those made by a senior examiner, the agreement is also around 60%.^([10](ch11.xhtml#ch01fn378))
    The accuracy by both human examiners and the algorithm exposes the underlying
    uncertainty in assessing students at a single point in time,^([11](ch11.xhtml#ch01fn379))
    further fueling the frustration of the public.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管该模型的准确性似乎较低，Ofqual却辩称他们的算法与人工评分者的准确性基本相当。当将一名审查员的分数与一名资深审查员的分数进行比较时，一致性也约为60%。^([10](ch11.xhtml#ch01fn378))
    无论是人工审查员还是算法的准确性都暴露了在单一时间点评估学生时的潜在不确定性，^([11](ch11.xhtml#ch01fn379)) 进一步加剧了公众的不满情绪。
- en: 'If you’ve read this book thus far, you know that coarse-grained accuracy alone
    is nowhere close to being sufficient to evaluate a model’s performance, especially
    for a model whose performance can influence the future of so many students. A
    closer look into this algorithm reveals at least three major failures along the
    process of designing and developing this automated grading system:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你到目前为止已经阅读了这本书，你会知道仅仅粗粒度的准确性远远不足以评估模型的性能，尤其是对于一个可能影响到如此多学生未来的模型。仔细分析这一算法揭示了在设计和开发这一自动评分系统过程中至少有三个主要的失败：
- en: Failure to set the right objective
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置错误的目标
- en: Failure to perform fine-grained evaluation to discover potential biases
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未能进行细粒度评估以发现潜在偏见
- en: Failure to make the model transparent
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未能使模型透明化
- en: We’ll go into detail about each of these failures. Keep in mind that even if
    these failures are addressed, the public might still be upset with the auto-grading
    system.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将详细讨论每一个这些失败。请记住，即使这些问题得到解决，公众对自动评分系统仍可能感到不满。
- en: 'Failure 1: Setting the wrong objective'
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 失败1：设定了错误的目标
- en: We discussed in [Chapter 2](ch02.xhtml#introduction_to_machine_learning_system)
    how the objective of an ML project will affect the resulting ML system’s performance.
    When developing an automated system to grade students, you would’ve thought that
    the objective of this system would be “grading accuracy for students.”
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第2章](ch02.xhtml#introduction_to_machine_learning_system)中讨论了一个机器学习项目的目标将如何影响最终机器学习系统的性能。当开发一个自动评分系统来对学生进行评分时，你本以为这个系统的目标应该是“对学生的评分准确性”。
- en: However, the objective that Ofqual seemingly chose to optimize was “maintaining
    standards” across schools—fitting the model’s predicted grades to historical grade
    distributions from each school. For example, if school A had historically outperformed
    school B in the past, Ofqual wanted an algorithm that, on average, also gives
    students from school A higher grades than students from school B. Ofqual prioritized
    fairness between schools over fairness between students—they preferred a model
    that gets school-level results right over another model that gets each individual’s
    grades right.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Ofqual显然选择优化的目标是“保持各学校水平的标准”，即使这意味着模型的预测分数要符合每个学校的历史分数分布。例如，如果学校A在过去的历史上优于学校B，Ofqual希望一个算法在平均意义上也给学校A的学生比给学校B的学生更高的分数。Ofqual更注重学校间的公平性而非学生间的公平性——他们更倾向于一个能正确预测学校级结果的模型，而不是一个能正确预测每个个体成绩的模型。
- en: Due to this objective, the model disproportionately downgraded high-performing
    cohorts from historically low-performing schools. A students from classes where
    students had historically received straight Ds were downgraded to Bs and Cs.^([12](ch11.xhtml#ch01fn380))
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这一目标，该模型不成比例地降低了历史上表现优秀但学术表现较差学校的学生的成绩。那些曾经成绩一直是D的学生现在被评为B和C。^([12](ch11.xhtml#ch01fn380))
- en: Ofqual failed to take into account the fact that schools with more resources
    tend to outperform schools with fewer resources. By prioritizing schools’ historical
    performance over students’ current performance, this auto-grader punished students
    from low resource schools, which tend to have more students from underprivileged
    backgrounds.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Ofqual没有考虑到资源更多的学校往往表现优于资源较少的学校这一事实。通过优先考虑学校的历史表现而非学生当前表现，这个自动评分系统惩罚了来自资源较少学校、通常有更多弱势背景学生的学生。
- en: 'Failure 2: Insufficient fine-grained model evaluation to discover biases'
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 失败2：不足的细粒度模型评估未能发现偏见
- en: Bias against students from historically low-performing schools is only one of
    the many biases discovered about this model after the results were brought to
    the public. The automated grading system took into account teachers’ assessments
    as inputs but failed to address teachers’ inconsistency in evaluation across demographic
    groups. It also “does not take into consideration the impact of multiple disadvantages
    for some protected groups [under the] 2010 Equalities Act, who will be double/triple
    disadvantaged by low teacher expectations, [and] racial discrimination that is
    endemic in some schools.”^([13](ch11.xhtml#ch01fn381))
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 针对历史表现低的学校学生的偏见只是在成绩公布后公众得知的该模型的许多偏见之一。自动评分系统考虑了教师的评估作为输入，但未能解决教师在跨人群评估中的不一致性。它还“没有考虑到一些受到《2010年平等法》保护群体的多重劣势的影响，这些群体将因教师的低期望和某些学校普遍存在的种族歧视而受到双重/三重劣势。”^([13](ch11.xhtml#ch01fn381))
- en: Because the model took into account each school’s historical performance, Ofqual
    acknowledged that their model didn’t have enough data for small schools. For these
    schools, instead of using this algorithm to assign final grades, they only used
    teacher-assessed grades. In practice, this led to “better grades for private school
    students who tend to have smaller classes.”^([14](ch11.xhtml#ch01fn382))
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 由于该模型考虑了每所学校的历史表现，Ofqual 承认他们的模型对于小型学校的数据不足。对于这些学校，他们没有使用该算法来分配最终成绩，而是仅使用了教师评估的成绩。实际上，这导致了“私立学校学生获得更好的成绩，因为他们往往班级较小。”^([14](ch11.xhtml#ch01fn382))
- en: It might have been possible to discover these biases through the public release
    of the model’s predicted grades with fine-grained evaluation to understand their
    model’s performance for different slices of data—e.g., evaluating the model’s
    accuracy for schools of different sizes and for students from different backgrounds.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 可能可以通过公开模型预测成绩并进行细粒度评估来发现这些偏见，以了解模型在不同数据片段（例如，评估不同规模学校和不同背景学生的模型准确性）中的表现。
- en: 'Failure 3: Lack of transparency'
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '失败 3: 缺乏透明度'
- en: Transparency is the first step in building trust in systems, yet Ofqual failed
    to make important aspects of their auto-grader public before it was too late.
    For example, they didn’t let the public know that the objective of their system
    was to maintain fairness between schools until the day the grades were published.
    The public, therefore, couldn’t express their concern over this objective as the
    model was being developed.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 透明度是建立系统信任的第一步，然而Ofqual 在太晚之前未能公开其自动评分系统的重要方面。例如，他们直到成绩发布日才让公众知道他们系统的目标是维持学校之间的公平性。因此，公众在系统开发过程中无法表达对这一目标的担忧。
- en: Further, Ofqual didn’t let teachers know how their assessments would be used
    by the auto-grader until after the assessments and student ranking had been submitted.
    Ofqual’s rationale was to avoid teachers attempting to alter their assessments
    to influence the model’s predictions. Ofqual chose not to release the exact model
    being used until results day to ensure that everyone would find out their results
    at the same time.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Ofqual 在评估和学生排名提交后才让教师知道他们的评估将如何被自动评分系统使用。Ofqual 的理由是为了避免教师试图改变他们的评估以影响模型的预测。Ofqual
    选择在成绩公布日之前不公布确切的使用模型，以确保每个人在同一时间知道他们的成绩。
- en: These considerations came from good intention; however, Ofqual’s decision to
    keep their model development in the dark meant that their system didn’t get sufficient
    independent, external scrutiny. Any system that operates on the trust of the public
    should be reviewable by independent experts trusted by the public. The Royal Statistical
    Society (RSS), in their inquiry into the development of this auto-grader, expressed
    concerns over the composition of the “technical advisory group” that Ofqual put
    together to evaluate the model. RSS indicated that “without a stronger procedural
    basis to ensure statistical rigor, and greater transparency about the issues that
    Ofqual is examining,”^([15](ch11.xhtml#ch01fn383)) the legitimacy of Ofqual’s
    statistical model is questionable.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这些考虑是出于善意；然而，Ofqual 决定在暗中开发他们的模型意味着他们的系统没有得到足够的独立外部审查。任何依靠公众信任运行的系统都应该由公众信任的独立专家审查。皇家统计学会（RSS）在对这个自动评分器的开发进行调查时，对Ofqual组建的“技术咨询组”的构成表示担忧。RSS指出，“没有更强有力的程序基础来确保统计严谨性，以及Ofqual正在审查的问题更透明”，^([15](ch11.xhtml#ch01fn383))
    这引发了对Ofqual统计模型合法性的质疑。
- en: This case study shows the importance of transparency when building a model that
    can make a direct impact on the lives of so many people, and what the consequences
    can be for failing to disclose important aspects of your model at the right time.
    It also shows the importance of choosing the right objective to optimize, as the
    wrong objective (e.g., prioritizing fairness among schools) can not only lead
    you to choose a model that underperforms for the right objective, but also perpetuate
    biases.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 该案例研究显示了在构建可能对如此多人生产生直接影响的模型时透明度的重要性，以及在适当时未能披露模型重要方面可能导致的后果。它还显示了选择正确的优化目标的重要性，因为错误的目标（例如，优先考虑学校的公平性）不仅可能导致选择表现不佳的模型来达到正确的目标，而且可能会使偏见持续存在。
- en: It also exemplifies the currently mucky boundary between what should be automated
    by algorithms and what should not. There must be people in the UK government who
    think it’s OK for A-level grading to be automated by algorithms, but it’s also
    possible to argue that due to the potential for catastrophic consequences of the
    A-level grading, it should never have been automated in the first place. Until
    there is a clearer boundary, there will be more cases of misusing AI algorithms.
    A clearer boundary can only be achieved with more investments in time and resources
    as well as serious considerations from AI developers, the public, and the authorities.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 它还展示了当前在应该由算法自动化和不应该由算法自动化之间的混乱边界。在英国政府中必然有人认为让A-level成绩评分由算法自动化是可以接受的，但也可以争论说，由于A-level评分可能带来的灾难性后果，它本应该从一开始就不应该被自动化。在没有更清晰的界限之前，会有更多误用AI算法的情况发生。只有通过更多的时间和资源投资以及AI开发者、公众和当局的认真考虑，才能实现更清晰的界限。
- en: 'Case study II: The danger of “anonymized” data'
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 案例研究II： “匿名化”数据的危险
- en: This case study is interesting to me because here, the algorithm is not an explicit
    culprit. Rather it’s how the interface and collection of data is designed that
    allows the leakage of sensitive data. Since the development of ML systems relies
    heavily on the quality of data, it’s important for user data to be collected.
    The research community needs access to high-quality datasets to develop new techniques.
    Practitioners and companies require access to data to discover new use cases and
    develop new AI-powered products.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对我来说，这个案例研究很有趣，因为在这里，算法并不是明显的罪魁祸首。而是接口和数据收集的设计使得敏感数据泄露成为可能。由于ML系统的开发严重依赖数据的质量，收集用户数据变得非常重要。研究界需要访问高质量的数据集来开发新技术。从业者和公司需要访问数据来发现新的用例并开发新的AI驱动产品。
- en: However, collecting and sharing datasets might violate the privacy and security
    of the users whose data is part of these datasets. To protect users, there have
    been calls for anonymization of personally identifiable information (PII). According
    to the US Department of Labor, PII is defined as “any representation of information
    that permits the identity of an individual to whom the information applies to
    be reasonably inferred by either direct or indirect means” such as name, address,
    or telephone number.^([16](ch11.xhtml#ch01fn384))
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，收集和共享数据集可能会侵犯那些数据包含在内的用户的隐私和安全。为了保护用户，有人呼吁对可识别个人信息（PII）进行匿名化处理。根据美国劳工部的定义，PII被定义为“任何信息的表达形式，通过直接或间接手段可以合理推断出信息适用的个人身份”，如姓名、地址或电话号码。^([16](ch11.xhtml#ch01fn384))
- en: However, anonymization may not be a sufficient guarantee for preventing data
    misuse and erosion of privacy expectations. In 2018, online fitness tracker Strava
    published a heatmap showing the paths it records of its users around the world
    as they exercise, e.g., running, jogging, or swimming. The heatmap was aggregated
    from one billion activities recorded between 2015 and September 2017, covering
    27 billion kilometers of distance. Strava stated that the data used had been anonymized,
    and “excludes activities that have been marked as private and user-defined privacy
    zones.”^([17](ch11.xhtml#ch01fn385))
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，匿名化可能不足以防止数据滥用和隐私期望的侵蚀。2018年，在线健身追踪器Strava发布了一张热力图，显示了全球用户在运动（如跑步、慢跑或游泳）时记录的路径。这张热力图聚合了2015年到2017年9月间记录的10亿次活动数据，涵盖了270亿公里的距离。Strava声称使用的数据已经进行了匿名化，并且“排除了标记为私密以及用户定义的隐私区域之外的活动”。^([17](ch11.xhtml#ch01fn385))
- en: Since Strava was used by military personnel, their public data, despite anonymization,
    allowed people to discover patterns that expose activities of US military bases
    overseas, including the “forward operating bases in Afghanistan, Turkish military
    patrols in Syria, and a possible guard patrol in the Russian operating area of
    Syria.”^([18](ch11.xhtml#ch01fn386)) An example of these discriminating patterns
    is shown in [Figure 11-4](#image_created_based_on_analysis_done_by). Some analysts
    even suggested that the data could reveal the names and heart rates of individual
    Strava users.^([19](ch11.xhtml#ch01fn387))
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Strava曾被军事人员使用，尽管已经进行了匿名化处理，但公开数据使人们能够发现显示美国海外军事基地活动的模式，包括“阿富汗前沿作战基地、叙利亚的土耳其军事巡逻以及俄罗斯在叙利亚运营区可能的警戒巡逻”。这些歧视性模式的例子显示在[图11-4](#image_created_based_on_analysis_done_by)中。一些分析人士甚至建议这些数据可以揭示个别Strava用户的姓名和心率。^([19](ch11.xhtml#ch01fn387))
- en: So where did the anonymization go wrong? First, Strava’s default privacy setting
    was “opt-out,” meaning that it requires users to manually opt out if they don’t
    want their data to be collected. However, users have pointed out that these privacy
    settings aren’t always clear and can cause surprises to users.^([20](ch11.xhtml#ch01fn389))
    Some of the privacy settings can only be changed through the Strava website rather
    than in its mobile app. This shows the importance of educating users about your
    privacy settings. Better, data opt-in (data collecting isn’t by default), not
    opt-out, should be the default.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，匿名化出了什么问题？首先，Strava的默认隐私设置是“选择退出”，这意味着用户需要手动选择退出，如果他们不希望他们的数据被收集。然而，用户指出这些隐私设置并不总是清晰的，可能会给用户带来惊喜。^([20](ch11.xhtml#ch01fn389))有些隐私设置只能通过Strava网站而非其移动应用程序更改。这显示了教育用户关于隐私设置的重要性。更好的做法是默认选择数据（数据收集不是默认的），而不是选择退出。
- en: '![](Images/dmls_1104.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_1104.png)'
- en: Figure 11-4\. Image created based on analysis done by BBC News^([21](ch11.xhtml#ch01fn388))
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-4\. 根据BBC News的分析创建的图像^([21](ch11.xhtml#ch01fn388))
- en: 'When this issue with the Strava heatmap became public, some of the responsibilities
    were shifted toward users: e.g., how military personnel shouldn’t use non-military-issue
    devices with GPS tracking and how location services should be turned off.^([22](ch11.xhtml#ch01fn390))'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当Strava热力图的问题公开化后，一些责任被转移到了用户身上：例如，军事人员不应使用带有GPS跟踪的非军用设备，以及应该关闭位置服务。^([22](ch11.xhtml#ch01fn390))
- en: However, privacy settings and users’ choices only address the problem at a surface
    level. The underlying problem is that the devices we use today are constantly
    collecting and reporting data on us. This data has to be moved and stored somewhere,
    creating opportunities for it to be intercepted and misused. The data that Strava
    has is small compared to much more widely used applications like Amazon, Facebook,
    Google, etc. Strava’s blunder might have exposed military bases’ activities, but
    other privacy failures might cause even more dangers not only to individuals but
    also to society at large.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，隐私设置和用户的选择只是在表面层面解决问题。潜在问题是，我们今天使用的设备不断收集和报告关于我们的数据。这些数据必须移动和存储在某个地方，从而为其被截取和误用创造机会。与像亚马逊、Facebook、Google等更广泛使用的应用程序相比，Strava的数据规模较小。Strava的失误可能暴露了军事基地的活动，但其他隐私失误可能不仅对个人，而且对整个社会造成更大的危险。
- en: Collecting and sharing data is essential for the development of data-driven
    technologies like AI. However, this case study shows the hidden danger of collecting
    and sharing data, even when data is supposedly anonymized and was released with
    good intention. Developers of applications that gather user data must understand
    that their users might not have the technical know-how and privacy awareness to
    choose the right privacy settings for themselves, and so developers must proactively
    work to make the right settings the default, even at the cost of gathering less
    data.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 收集和分享数据对于像人工智能这样的数据驱动技术的发展至关重要。然而，这个案例研究显示了即使数据被假定为匿名化并出于良好意图发布，收集和分享数据也存在潜在的危险。收集用户数据的应用程序开发者必须明白，他们的用户可能没有技术知识和隐私意识来为自己选择正确的隐私设置，因此开发者必须积极努力使正确设置成为默认选项，即使这可能导致收集的数据更少。
- en: A Framework for Responsible AI
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 负责任人工智能的框架
- en: In this section, we will lay down the foundations for you, as an ML practitioner,
    to audit model behavior and set out guidelines that best help you meet the needs
    of your projects. This framework is not sufficient for every use case. There are
    certain applications where the use of AI might altogether be inappropriate or
    unethical (e.g., criminal sentencing decisions, predictive policing), regardless
    of which framework you follow.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将为您作为机器学习从业者奠定基础，以审计模型行为并制定最佳指南，以最好地满足项目需求。这个框架并不适用于每个用例。在某些应用中，使用人工智能可能完全不合适或不道德（例如，刑事判决决策，预测性执法），无论您遵循哪种框架。
- en: Discover sources for model biases
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 发现模型偏见的来源
- en: As someone who has been following the discussions around ML systems design,
    you know that biases can creep in your system through the entire workflow. Your
    first step is to discover how these biases can creep in. The following are some
    examples of the sources of data, but keep in mind that this list is far from being
    exhaustive. One of the reasons why biases are so hard to combat is that biases
    can come from any step during a project lifecycle.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个关注机器学习系统设计讨论的人，你知道偏见可能通过整个工作流程渗入你的系统中。你的第一步是发现这些偏见是如何渗入的。以下是一些数据来源的示例，但请记住，这个列表远非详尽无遗。偏见之所以难以对抗的其中一个原因是，它们可以来自项目生命周期的任何步骤。
- en: Training data
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据
- en: Is the data used for developing your model representative of the data your model
    will handle in the real world? If not, your model might be biased against the
    groups of users with less data represented in the training data.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 用于开发模型的数据是否代表了您的模型将在现实世界中处理的数据？如果不是，您的模型可能会对在训练数据中代表少量数据的用户群体存在偏见。
- en: Labeling
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 标注
- en: If you use human annotators to label your data, how do you measure the quality
    of these labels? How do you ensure that annotators follow standard guidelines
    instead of relying on subjective experience to label your data? The more annotators
    have to rely on their subjective experience, the more room for human biases.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用人类标注员为数据打标签，您如何衡量这些标签的质量？您如何确保标注员遵循标准指南而不是依靠主观经验为数据打标签？标注员越依赖他们的主观经验，人为偏见的空间就越大。
- en: Feature engineering
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程
- en: Does your model use any feature that contains sensitive information? Does your
    model cause a disparate impact on a subgroup of people? Disparate impact occurs
    “when a selection process has widely different outcomes for different groups,
    even as it appears to be neutral.”^([23](ch11.xhtml#ch01fn391)) This can happen
    when a model’s decision relies on information correlated with legally protected
    classes (e.g., ethnicity, gender, religious practice) even when this information
    isn’t used in training the model directly. For example, a hiring process can cause
    disparate impact by race if it leverages variables correlated with race such as
    zip code and high school diplomas. To mitigate this potential disparate impact,
    you might want to use disparate impact remover techniques proposed by Feldman
    et al. in [“Certifying and Removing Disparate Impact”](https://oreil.ly/a9vxm)
    or to use the function [`DisparateImpactRemover`](https://oreil.ly/6LyA8) implemented
    by [AI Fairness 360](https://oreil.ly/TjavU) (AIF360). You can also identify hidden
    bias in variables (which can then be removed from the training set) using the
    [Infogram method](https://oreil.ly/JFZCL), implemented in H2O.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 你的模型是否使用包含敏感信息的特征？你的模型是否对某个人群产生了不平等的影响？即使看起来是中立的，当选择过程对不同群体有着显著不同的结果时即发生“不平等影响”^([23](ch11.xhtml#ch01fn391))。例如，招聘过程如果利用与种族相关的变量（如邮政编码和高中学历）可能导致种族不平等影响。为了减少这种潜在的不平等影响，你可能需要使用
    Feldman 等人在 [“认证和消除不平等影响”](https://oreil.ly/a9vxm) 中提出的不平等影响消除技术，或者使用由 [AI Fairness
    360](https://oreil.ly/TjavU)（AIF360） 实现的 [`DisparateImpactRemover`](https://oreil.ly/6LyA8)
    函数。你还可以使用 H2O 中实施的 [Infogram 方法](https://oreil.ly/JFZCL) 识别变量中的隐藏偏见（然后从训练集中去除）。
- en: Model’s objective
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的目标
- en: Are you optimizing your model using an objective that enables fairness to all
    users? For example, are you prioritizing your model’s performance on all users,
    which skews your model toward the majority group of users?
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否使用能够对所有用户实现公平的目标来优化你的模型？例如，你是否优先考虑模型在所有用户上的表现，从而使你的模型偏向于多数用户群体？
- en: Evaluation
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 评估
- en: Are you performing adequate, fine-grained evaluation to understand your model’s
    performance on different groups of users? This is covered in the section [“Slice-based
    evaluation”](ch06.xhtml#slice_based_evaluation). Fair, adequate evaluation depends
    on the existence of fair, adequate evaluation data.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否进行了充分的、细致的评估，以了解模型在不同用户群体上的表现？这在 [“基于切片的评估”](ch06.xhtml#slice_based_evaluation)
    部分有详细介绍。公正、充分的评估取决于存在公正、充分的评估数据。
- en: Understand the limitations of the data-driven approach
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解数据驱动方法的局限性
- en: ML is a data-driven approach to solving problems. However, it’s important to
    understand that data isn’t enough. Data concerns people in the real world, with
    socioeconomic and cultural aspects to consider. We need to gain a better understanding
    of the blind spots caused by too much reliance on data. This often means crossing
    over disciplinary and functional boundaries, both within and outside the organization,
    so that we can account for the lived experiences of those who will be impacted
    by the systems that we build.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是一种数据驱动的解决问题的方法。然而，理解的数据并不足够。数据涉及到现实世界中的人们，需要考虑到社会经济和文化等方面。我们需要更好地理解由于过度依赖数据而造成的盲点。这通常意味着跨越学科和功能边界，无论是组织内还是组织外，以便考虑到那些将受到我们构建系统影响的人们的生活经验。
- en: As an example, to build an equitable automated grading system, it’s essential
    to work with domain experts to understand the demographic distribution of the
    student population and how socioeconomic factors get reflected in the historical
    performance data.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要构建一个公平的自动评分系统，与领域专家合作以了解学生群体的人口统计分布以及历史表现数据中反映的社会经济因素是至关重要的。
- en: Understand the trade-offs between different desiderata
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解不同期望之间的权衡
- en: When building an ML system, there are different properties you might want this
    system to have. For example, you might want your system to have low inference
    latency, which could be obtained by model compression techniques like pruning.
    You might also want your model to have high predictive accuracy, which could be
    achieved by adding more data. You might also want your model to be fair and transparent,
    which could require the model and the data used to develop this model to be made
    accessible for public scrutiny.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建机器学习系统时，可能希望该系统具备不同的特性。例如，您可能希望系统具有低推断延迟，这可以通过像修剪这样的模型压缩技术实现。您可能还希望模型具有高预测准确性，这可以通过增加数据量来实现。您可能还希望模型具有公平和透明性，这可能需要将用于开发此模型的模型和数据公开供公众审查。
- en: 'Often, ML literature makes the unrealistic assumption that optimizing for one
    property, like model accuracy, holds all others static. People might discuss techniques
    to improve a model’s fairness with the assumption that this model’s accuracy or
    latency will remain the same. However, in reality, improving one property can
    cause other properties to degrade. Here are two examples of these trade-offs:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，机器学习文献会做出一个不切实际的假设，即优化模型准确性这样一个特性，其他所有特性都保持不变。人们可能会讨论提高模型公平性的技术，假设这将不会影响模型的准确性或延迟。然而，在现实中，改进一个特性可能会导致其他特性的降低。以下是这些权衡的两个例子：
- en: Privacy versus accuracy trade-off
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私与准确性的权衡
- en: According to Wikipedia, differential privacy is “a system for publicly sharing
    information about a dataset by describing the patterns of groups within the dataset
    while withholding information about individuals in the dataset. The idea behind
    differential privacy is that if the effect of making an arbitrary single substitution
    in the database is small enough, the query result cannot be used to infer much
    about any single individual, and therefore provides privacy.”^([24](ch11.xhtml#ch01fn392))
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 根据维基百科，差分隐私是“通过描述数据集内组群的模式来公开共享有关数据集的信息，同时隐瞒数据集中个体的信息。差分隐私的理念在于，如果对数据库进行任意单个替换的影响足够小，则查询结果无法用于推断任何单个个体的信息，因此提供隐私保护。”^([24](ch11.xhtml#ch01fn392))
- en: Differential privacy is a popular technique used on training data for ML models.
    The trade-off here is that the higher the level of privacy that differential privacy
    can provide, the lower the model’s accuracy. However, this accuracy reduction
    isn’t equal for all samples. As pointed out by Bagdasaryan and Shmatikov (2019),
    “the accuracy of differential privacy models drops much more for the underrepresented
    classes and subgroups.”^([25](ch11.xhtml#ch01fn393))
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私是用于机器学习模型训练数据的一种流行技术。这里的权衡是，差分隐私能够提供的隐私保护级别越高，模型的准确性就越低。然而，这种准确性降低并不对所有样本都一样。正如Bagdasaryan和Shmatikov（2019）指出的那样，“差分隐私模型的准确性对于少数族裔和次群体的影响更大。”^([25](ch11.xhtml#ch01fn393))
- en: Compactness versus fairness trade-off
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 紧凑性与公平性的权衡
- en: In [Chapter 7](ch07.xhtml#model_deployment_and_prediction_service), we talked
    at length about various techniques for model compression such as pruning and quantization.
    We learned that it’s possible to reduce a model’s size significantly with minimal
    cost of accuracy, e.g., reducing a model’s parameter count by 90% with minimal
    accuracy cost.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 7 章](ch07.xhtml#model_deployment_and_prediction_service)中，我们详细讨论了诸如修剪和量化之类的模型压缩技术。我们了解到，可以在几乎不损失准确性的情况下显著减小模型的大小，例如将模型参数数量减少
    90%。
- en: The minimal accuracy cost is indeed minimal if it’s spread uniformly across
    all classes, but what if the cost is concentrated in only a few classes? In their
    2019 paper, “What Do Compressed Deep Neural Networks Forget?,” Hooker et al. found
    that “models with radically different numbers of weights have comparable top-line
    performance metrics but diverge considerably in behavior on a narrow subset of
    the dataset.”^([26](ch11.xhtml#ch01fn394)) For example, they found that compression
    techniques amplify algorithmic harm when the protected feature (e.g., sex, race,
    disability) is in the long tail of the distribution. This means that compression
    disproportionately impacts underrepresented features.^([27](ch11.xhtml#ch01fn395))
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果最小的准确性损失均匀分布在所有类别中，成本确实是最小的，但如果成本集中在少数类别中呢？在他们2019年的论文《压缩深度神经网络会忘记什么？》中，Hooker等人发现，“具有截然不同权重数量的模型在顶线性能指标上具有可比性，但在数据集的某个狭窄子集上的行为却有很大分歧。”^([26](ch11.xhtml#ch01fn394))
    例如，他们发现，当受保护特征（例如性别、种族、残疾）位于分布的长尾时，压缩技术会放大算法的伤害，这意味着压缩对代表性较低的特征影响更大。^([27](ch11.xhtml#ch01fn395))
- en: Another important finding from their work is that while all compression techniques
    they evaluated have a nonuniform impact, not all techniques have the same level
    of disparate impact. Pruning incurs a far higher disparate impact than is observed
    for the quantization techniques that they evaluated.^([28](ch11.xhtml#ch01fn396))
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的工作中另一个重要发现是，尽管他们评估的所有压缩技术对不均衡的影响都不同，但并非所有技术都具有相同水平的差异影响。与他们评估的量化技术相比，修剪技术造成的差异影响要高得多。^([28](ch11.xhtml#ch01fn396))
- en: Similar trade-offs continue to be discovered. It’s important to be aware of
    these trade-offs so that we can make informed design decisions for our ML systems.
    If you are working with a system that is compressed or differentially private,
    allocating more resources to auditing model behavior is recommended to avoid unintended
    harm.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 发现类似的权衡继续进行。了解这些权衡是非常重要的，这样我们才能为我们的ML系统做出明智的设计决策。如果您正在处理压缩或差分私有的系统，建议增加资源用于审计模型行为，以避免意外的伤害。
- en: Act early
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 早期行动
- en: Consider a new building being constructed downtown. A contractor has been called
    upon to build something that will stand for the next 75 years. To save costs,
    the contractor uses poor-quality cement. The owner doesn’t invest in supervision
    since they want to avoid overhead to be able to move fast. The contractor continues
    building on top of that poor foundation and finishes the building on time.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑正在市中心建造的新建筑。承包商被召集来建造一些将在未来75年内屹立不倒的东西。为了节省成本，承包商使用了劣质水泥。业主没有投资监督，因为他们想避免额外开销以便快速推进。承包商继续在这个不良基础上建造并按时完成了建筑。
- en: Within a year, cracks start showing up and it appears that the building might
    topple. The city decides that this building poses a safety risk and requests for
    it to be demolished. The contractor’s decision to save cost and the owner’s decision
    to save time in the beginning now end up costing the owner much more money and
    time.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 一年之内，裂缝开始出现，建筑物可能倾斜。市政府认为这栋建筑存在安全风险，并要求拆除它。承包商为了节省成本的决定以及业主为了节省时间的决定，现在导致业主付出更多的金钱和时间代价。
- en: You might encounter this narrative often in ML systems. Companies might decide
    to bypass ethical issues in ML models to save cost and time, only to discover
    risks in the future when they end up costing a lot more, such as the preceding
    case studies of Ofqual and Strava.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能经常在ML系统中遇到这样的叙事。公司可能会决定绕过ML模型中的道德问题以节省成本和时间，但最终却发现风险，这会导致像Ofqual和Strava的前述案例一样花费更多。
- en: The earlier in the development cycle of an ML system that you can start thinking
    about how this system will affect the life of users and what biases your system
    might have, the cheaper it will be to address these biases. A study by NASA shows
    that for software development, the cost of errors goes up by an order of magnitude
    at every stage of your project lifecycle.^([29](ch11.xhtml#ch01fn397))
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在ML系统的开发周期越早开始思考该系统将如何影响用户的生活以及您的系统可能存在的偏见，就越便宜解决这些偏见。NASA的一项研究显示，对于软件开发，错误成本在项目生命周期的每个阶段都会增加一个数量级。^([29](ch11.xhtml#ch01fn397))
- en: Create model cards
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建模型卡片
- en: Model cards are short documents accompanying trained ML models that provide
    information on how these models were trained and evaluated. Model cards also disclose
    the context in which models are intended to be used, as well as their limitations.^([30](ch11.xhtml#ch01fn398))
    According to the authors of the model card paper, “The goal of model cards is
    to standardize ethical practice and reporting by allowing stakeholders to compare
    candidate models for deployment across not only traditional evaluation metrics
    but also along the axes of ethical, inclusive, and fair considerations.”
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 模型卡是随训练好的ML模型一起提供的简短文档，提供了这些模型的训练和评估信息。模型卡还披露了模型预期使用的背景及其局限性。^([30](ch11.xhtml#ch01fn398))
    根据模型卡论文的作者，“模型卡的目标是通过允许利益相关者比较候选模型的部署，标准化道德实践和报告，不仅沿着传统评估指标的轴线，还沿着伦理、包容和公平考虑的轴线。”
- en: The following list has been adapted from content in the paper “Model Cards for
    Model Reporting” to show the information you might want to report for your models:^([31](ch11.xhtml#ch01fn399))
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表已从论文“模型报告的模型卡”中调整，以展示您可能希望为您的模型报告的信息：^([31](ch11.xhtml#ch01fn399))
- en: '*Model details*: Basic information about the model.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型细节*：关于模型的基本信息。'
- en: Person or organization developing model
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发模型的人或组织
- en: Model date
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型日期
- en: Model version
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型版本
- en: Model type
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型类型
- en: Information about training algorithms, parameters, fairness constraints or other
    applied approaches, and features
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于训练算法、参数、公平性约束或其他应用方法以及特征的信息
- en: Paper or other resource for more information
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多信息的论文或其他资源
- en: Citation details
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引用详情
- en: License
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许可证
- en: Where to send questions or comments about the model
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提出关于模型的问题或意见的地方
- en: '*Intended use*: Use cases that were envisioned during development.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预期用途*：在开发过程中设想的使用情况。'
- en: Primary intended uses
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主要预期用途
- en: Primary intended users
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主要预期用户
- en: Out-of-scope use cases
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不在范围内的使用案例
- en: '*Factors*: Factors could include demographic or phenotypic groups, environmental
    conditions, technical attributes, or others.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*因素*：因素可能包括人口统计学或表型群体、环境条件、技术属性或其他因素。'
- en: Relevant factors
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关因素
- en: Evaluation factors
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估因素
- en: '*Metrics*: Metrics should be chosen to reflect potential real-world impacts
    of the model.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*指标*：应选择反映模型可能在现实世界中产生影响的指标。'
- en: Model performance measures
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型性能指标
- en: Decision thresholds
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策阈值
- en: Variation approaches
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变化方法
- en: '*Evaluation data*: Details on the dataset(s) used for the quantitative analyses
    in the card.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*评估数据*：卡片中用于定量分析的数据集详细信息。'
- en: Datasets
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集
- en: Motivation
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动机
- en: Preprocessing
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理
- en: '*Training data*: May not be possible to provide in practice. When possible,
    this section should mirror Evaluation Data. If such detail is not possible, minimal
    allowable information should be provided here, such as details of the distribution
    over various factors in the training datasets.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*训练数据*：在实践中可能无法提供。如果可能，该部分应与评估数据部分相对应。如果无法提供此类详细信息，则应在此提供最少允许的信息，例如训练数据集中各种因素的分布详情。'
- en: '*Quantitative analyses*'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*定量分析*'
- en: Unitary results
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单元结果
- en: Intersectional results
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉结果
- en: '*Ethical considerations*'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*伦理考虑*'
- en: '*Caveats and recommendations*'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*注意事项和建议*'
- en: Model cards are a step toward increasing transparency into the development of
    ML models. They are especially important in cases where people who use a model
    aren’t the same people who developed this model.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 模型卡是增加透明度的一步，揭示了ML模型开发的过程。尤其是在使用模型的人与开发该模型的人不同的情况下，这一点尤为重要。
- en: Note that model cards will need to be updated whenever a model is updated. For
    models that update frequently, this can create quite an overhead for data scientists
    if model cards are created manually. Therefore, it’s important to have tools to
    automatically generate model cards, either by leveraging the model card generation
    feature of tools like [TensorFlow](https://oreil.ly/iQtrS), [Metaflow](https://oreil.ly/nucaZ),
    and [scikit-learn](https://oreil.ly/Yk16x) or by building this feature in-house.
    Because the information that should be tracked in a model’s card overlaps with
    the information that should be tracked by a model store, I wouldn’t be surprised
    if in the near future, model stores evolve to automatically generate model cards.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，每当模型更新时，模型卡片都需要更新。对于频繁更新的模型，如果模型卡片是手动创建的，这可能会给数据科学家带来很大的负担。因此，很重要能够利用像[TensorFlow](https://oreil.ly/iQtrS)，[Metaflow](https://oreil.ly/nucaZ)，和[scikit-learn](https://oreil.ly/Yk16x)这样的工具自动生成模型卡片，或者在内部构建此功能。因为应该在模型卡片中跟踪的信息与模型存储中应该跟踪的信息重叠，所以我不会感到意外，如果在不久的将来，模型存储会发展出自动生成模型卡片的能力。
- en: Establish processes for mitigating biases
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 建立减少偏见的过程
- en: Building responsible AI is a complex process, and the more ad hoc the process
    is, the more room there is for errors. It’s important for businesses to establish
    systematic processes for making their ML systems responsible.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 建立负责任的人工智能是一个复杂的过程，过程越临时化，出错的可能性就越大。企业建立系统化的过程来确保他们的机器学习系统负责任至关重要。
- en: You might want to create a portfolio of internal tools easily accessible by
    different stakeholders. Big corporations have tool sets that you can reference.
    For example, Google has published [recommended best practices for responsible
    AI](https://oreil.ly/0C30s) and IBM has open-sourced [AI Fairness 360](https://aif360.mybluemix.net),
    which contains a set of metrics, explanations, and algorithms to mitigate bias
    in datasets and models. You might also consider using third-party audits.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能希望创建一组对不同利益相关者易于访问的内部工具组合。大公司有您可以参考的工具集。例如，谷歌已经发布了[负责任AI的推荐最佳实践](https://oreil.ly/0C30s)，IBM开源了[AI
    Fairness 360](https://aif360.mybluemix.net)，其中包含一组用于减少数据集和模型偏见的度量标准、解释和算法。您也可以考虑使用第三方审计。
- en: Stay up-to-date on responsible AI
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保持对负责任AI的最新了解
- en: AI is a fast-moving field. New sources of biases in AI are constantly being
    discovered, and new challenges for responsible AI constantly emerge. Novel techniques
    to combat these biases and challenges are actively being developed. It’s important
    to stay up-to-date with the latest research in responsible AI. You might want
    to follow the [ACM FAccT Conference](https://oreil.ly/dkEeG), the [Partnership
    on AI](https://partnershiponai.org), the [Alan Turing Institute’s Fairness, Transparency,
    Privacy group](https://oreil.ly/5aiQh), and the [AI Now Institute](https://ainowinstitute.org).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: AI是一个快速发展的领域。AI中新的偏见来源不断被发现，负责任AI面临新的挑战不断涌现。正在积极开发新的技术来应对这些偏见和挑战是非常重要的。保持对负责任AI最新研究的跟踪是重要的。您可能希望关注[ACM
    FAccT会议](https://oreil.ly/dkEeG)，[Partnership on AI](https://partnershiponai.org)，[Alan
    Turing Institute的公平性、透明性、隐私组](https://oreil.ly/5aiQh)，以及[AI Now Institute](https://ainowinstitute.org)。
- en: Summary
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Despite the technical nature of ML solutions, designing ML systems can’t be
    confined in the technical domain. They are developed by humans, used by humans,
    and leave their marks in society. In this chapter, we deviated from the technical
    theme of the last eight chapters to focus on the human side of ML.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管ML解决方案具有技术性质，但设计ML系统不能仅限于技术领域。它们由人类开发，由人类使用，并在社会中留下影响。在本章中，我们偏离了过去八章的技术主题，专注于ML的人类方面。
- en: We first focused on how the probabilistic, mostly correct, and high-latency
    nature of ML systems can affect user experience in various ways. The probabilistic
    nature can lead to inconsistency in user experience, which can cause frustration—“Hey,
    I just saw this option right here, and now I can’t find it anywhere.” The mostly
    correct nature of an ML system might render it useless if users can’t easily fix
    these predictions to be correct. To counter this, you might want to show users
    multiple “most correct” predictions for the same input, in the hope that at least
    one of them will be correct.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先关注了概率性、大多数正确以及高延迟的机器学习系统如何在各种方式影响用户体验。概率性质可以导致用户体验的不一致性，这可能引起沮丧——“嘿，我刚刚看到这个选项在这里，现在我却找不到它了。”如果用户无法轻松地修正这些预测使其正确，那么大多数正确的机器学习系统可能会变得无用。为了解决这个问题，您可能希望向用户展示多个相同输入的“最正确”预测，希望至少有一个是正确的。
- en: 'Building an ML system often requires multiple skill sets, and an organization
    might wonder how to distribute these required skill sets: to involve different
    teams with different skill sets or to expect the same team (e.g., data scientists)
    to have all the skills. We explored the pros and cons of both approaches. The
    main cons of the first approach is overhead in communication. The main cons of
    the second approach is that it’s difficult to hire data scientists who can own
    the process of developing an ML system end-to-end. Even if they can, they might
    not be happy doing it. However, the second approach might be possible if these
    end-to-end data scientists are provided with sufficient tools and infrastructure,
    which was the focus of [Chapter 10](ch10.xhtml#infrastructure_and_tooling_for_mlops).'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 建立一个机器学习系统通常需要多种技能，组织可能会考虑如何分配这些所需的技能：是让不同技能的团队参与（例如，数据科学家），还是期望同一个团队具备所有技能。我们探讨了两种方法的利弊。第一种方法的主要缺点是沟通成本增加。第二种方法的主要缺点是很难雇佣能够负责整个机器学习系统开发过程的数据科学家。即使能够找到这样的人才，他们也可能不愿意做这件事。然而，如果这些全流程数据科学家提供了足够的工具和基础设施，第二种方法可能是可行的，这正是[第10章](ch10.xhtml#infrastructure_and_tooling_for_mlops)的重点。
- en: 'We ended the chapter with what I believe to be the most important topic of
    this book: responsible AI. Responsible AI is no longer just an abstraction, but
    an essential practice in today’s ML industry that merits urgent actions. Incorporating
    ethics principles into your modeling and organizational practices will not only
    help you distinguish yourself as a professional and cutting-edge data scientist
    and ML engineer but also help your organization gain trust from your customers
    and users. It will also help your organization obtain a competitive edge in the
    market as more and more customers and users emphasize their need for responsible
    AI products and services.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以我认为是本书最重要的话题之一结束了这一章：负责任的人工智能。负责任的人工智能不再只是一个抽象概念，而是当今机器学习行业中至关重要的实践，值得紧急行动。将伦理原则融入到您的建模和组织实践中，不仅有助于您在专业和尖端数据科学家和机器学习工程师中脱颖而出，还有助于您的组织赢得客户和用户的信任。这还将有助于您的组织在市场上获得竞争优势，因为越来越多的客户和用户强调他们对负责任人工智能产品和服务的需求。
- en: It is important to not treat this responsible AI as merely a checkbox ticking
    activity that we undertake to meet compliance requirements for our organization.
    It’s true that the framework proposed in this chapter will help you meet the compliance
    requirements for your organization, but it won’t be a replacement for critical
    thinking on whether a product or service should be built in the first place.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 对待这种负责任的人工智能不能仅仅当作我们为了满足组织的合规要求而进行的一个勾选活动。这一章提出的框架确实可以帮助您满足组织的合规要求，但这不会取代对产品或服务是否应该首先构建进行深思熟虑的关键思维。
- en: ^([1](ch11.xhtml#ch01fn371-marker)) Sometimes, you can get different results
    if you run the same model on the same input twice *at the exact same time*.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch11.xhtml#ch01fn371-marker)) 有时，如果你同时运行相同的模型在*完全相同的时间*，可能会得到不同的结果。
- en: ^([2](ch11.xhtml#custom_ch11fn1-marker)) Eugene Yan, “Unpopular Opinion—Data
    Scientists Should be More End-to-End,” EugeneYan.com, August 9, 2020, [*https://oreil.ly/A6oPi*](https://oreil.ly/A6oPi).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch11.xhtml#custom_ch11fn1-marker)) Eugene Yan，“不受欢迎的观点——数据科学家应更全流程”，EugeneYan.com，2020年8月9日，[*https://oreil.ly/A6oPi*](https://oreil.ly/A6oPi)。
- en: '^([3](ch11.xhtml#custom_ch11fn2-marker)) Eric Colson, “Beware the Data Science
    Pin Factory: The Power of the Full-Stack Data Science Generalist and the Perils
    of Division of Labor Through Function,” MultiThreaded, March 11, 2019, [*https://oreil.ly/m6WWu*](https://oreil.ly/m6WWu).'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch11.xhtml#custom_ch11fn2-marker)) Eric Colson，“小心数据科学的引脚工厂：全栈数据科学通才的力量及分工带来的危害”，MultiThreaded，2019年3月11日，[*https://oreil.ly/m6WWu*](https://oreil.ly/m6WWu).
- en: ^([4](ch11.xhtml#ch01fn372-marker)) Erik Bernhardsson on Twitter (@bernhardsson),
    July 20, 2021, [*https://oreil.ly/7X4J9*](https://oreil.ly/7X4J9).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch11.xhtml#ch01fn372-marker)) Erik Bernhardsson在Twitter上的发言（@bernhardsson），2021年7月20日，[*https://oreil.ly/7X4J9*](https://oreil.ly/7X4J9).
- en: ^([5](ch11.xhtml#ch01fn373-marker)) Colson, “Beware the Data Science Pin Factory.”
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch11.xhtml#ch01fn373-marker)) Colson，“小心数据科学的引脚工厂。”
- en: ^([6](ch11.xhtml#ch01fn374-marker)) “Full Cycle Developers at Netflix—Operate
    What You Build,” *Netflix Technology Blog*, May 17, 2018, [*https://oreil.ly/iYgQs*](https://oreil.ly/iYgQs).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch11.xhtml#ch01fn374-marker)) “Netflix全周期开发者——运行你所构建的”，*Netflix Technology
    Blog*，2018年5月17日，[*https://oreil.ly/iYgQs*](https://oreil.ly/iYgQs).
- en: ^([7](ch11.xhtml#ch01fn375-marker)) Elliot Jones and Cansu Safak, “Can Algorithms
    Ever Make the Grade?” *Ada Lovelace Institute Blog*, 2020, [*https://oreil.ly/ztTxR*](https://oreil.ly/ztTxR).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch11.xhtml#ch01fn375-marker)) Elliot Jones和Cansu Safak，“算法能否成绩优异？” *Ada
    Lovelace Institute Blog*，2020年，[*https://oreil.ly/ztTxR*](https://oreil.ly/ztTxR).
- en: ^([8](ch11.xhtml#ch01fn376-marker)) Tom Simonite, “Skewed Grading Algorithms
    Fuel Backlash Beyond the Classroom,” *Wired*, August 19, 2020, [*https://oreil.ly/GFRet*](https://oreil.ly/GFRet).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch11.xhtml#ch01fn376-marker)) Tom Simonite，“偏斜的评分算法引发课堂之外的反弹”，*Wired*，2020年8月19日，[*https://oreil.ly/GFRet*](https://oreil.ly/GFRet).
- en: '^([9](ch11.xhtml#ch01fn377-marker)) Ofqual, “Awarding GCSE, AS & A Levels in
    Summer 2020: Interim Report,” Gov.uk, August 13, 2020, [*https://oreil.ly/r22iz*](https://oreil.ly/r22iz).'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch11.xhtml#ch01fn377-marker)) Ofqual，“2020年夏季颁发GCSE、AS和A级考试成绩的临时报告”，Gov.uk，2020年8月13日，[*https://oreil.ly/r22iz*](https://oreil.ly/r22iz).
- en: ^([10](ch11.xhtml#ch01fn378-marker)) Ofqual, “Awarding GCSE, AS & A levels.”
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch11.xhtml#ch01fn378-marker)) Ofqual，“颁发GCSE、AS和A级考试成绩。”
- en: ^([11](ch11.xhtml#ch01fn379-marker)) Jones and Safak, “Can Algorithms Ever Make
    the Grade?”
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch11.xhtml#ch01fn379-marker)) Jones and Safak，“算法能否成绩优异？”
- en: ^([12](ch11.xhtml#ch01fn380-marker)) Jones and Safak, “Can Algorithms Ever Make
    the Grade?”
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch11.xhtml#ch01fn380-marker)) Jones and Safak，“算法能否成绩优异？”
- en: ^([13](ch11.xhtml#ch01fn381-marker)) Ofqual, “Awarding GCSE, AS & A Levels.”
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch11.xhtml#ch01fn381-marker)) Ofqual，“颁发GCSE、AS & A Level资格证书。”
- en: ^([14](ch11.xhtml#ch01fn382-marker)) Jones and Safak, “Can Algorithms Ever Make
    the Grade?”
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch11.xhtml#ch01fn382-marker)) Jones and Safak，“算法能否成绩优异？”
- en: '^([15](ch11.xhtml#ch01fn383-marker)) “Royal Statistical Society Response to
    the House of Commons Education Select Committee Call for Evidence: The Impact
    of COVID-19 on Education and Children’s Services Inquiry,” Royal Statistical Society,
    June 8, 2020, [*https://oreil.ly/ernho*](https://oreil.ly/ernho).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch11.xhtml#ch01fn383-marker)) “皇家统计学会对下议院教育选择委员会关于COVID-19对教育和儿童服务影响的调查的回应”，皇家统计学会，2020年6月8日，[*https://oreil.ly/ernho*](https://oreil.ly/ernho).
- en: ^([16](ch11.xhtml#ch01fn384-marker)) “Guidance on the Protection of Personal
    Identifiable Information,” US Department of Labor, [*https://oreil.ly/FokAV*](https://oreil.ly/FokAV).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch11.xhtml#ch01fn384-marker)) “个人可识别信息保护指导”，美国劳工部，[*https://oreil.ly/FokAV*](https://oreil.ly/FokAV).
- en: ^([17](ch11.xhtml#ch01fn385-marker)) Sasha Lekach, “Strava’s Fitness Heatmap
    Has a Major Security Problem for the Military,” *Mashable*, January 28, 2018,
    [*https://oreil.ly/9ogYx*](https://oreil.ly/9ogYx).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch11.xhtml#ch01fn385-marker)) Sasha Lekach，“Strava的健身热图对军事存在重大安全问题”，*Mashable*，2018年1月28日，[*https://oreil.ly/9ogYx*](https://oreil.ly/9ogYx).
- en: ^([18](ch11.xhtml#ch01fn386-marker)) Jeremy Hsu, “The Strava Heat Map and the
    End of Secrets,” *Wired*, January 29, 2018, [*https://oreil.ly/mB0GD*](https://oreil.ly/mB0GD).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ^([18](ch11.xhtml#ch01fn386-marker)) Jeremy Hsu，“Strava热图和秘密的终结”，*Wired*，2018年1月29日，[*https://oreil.ly/mB0GD*](https://oreil.ly/mB0GD).
- en: ^([19](ch11.xhtml#ch01fn387-marker)) Matt Burgess, “Strava’s Heatmap Data Lets
    Anyone See the Names of People Exercising on Military Bases,” *Wired*, January
    30, 2018, [*https://oreil.ly/eJPdj*](https://oreil.ly/eJPdj).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ^([19](ch11.xhtml#ch01fn387-marker)) Matt Burgess，“Strava的热图数据让任何人看到在军事基地上锻炼的人的姓名”，*Wired*，2018年1月30日，[*https://oreil.ly/eJPdj*](https://oreil.ly/eJPdj).
- en: ^([20](ch11.xhtml#ch01fn389-marker)) Matt Burgess, “Strava’s Heatmap Data Lets
    Anyone See”; Rosie Spinks, “Using a Fitness App Taught Me the Scary Truth About
    Why Privacy Settings Are a Feminist Issue,” *Quartz*, August 1, 2017, [*https://oreil.ly/DO3WR*](https://oreil.ly/DO3WR).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ^([20](ch11.xhtml#ch01fn389-marker)) Matt Burgess，“Strava 的热图数据让任何人都能看到”；Rosie
    Spinks，“使用健身应用教会我关于隐私设置为何是女权主义问题的可怕真相”，*Quartz*，2017 年 8 月 1 日，[*https://oreil.ly/DO3WR*](https://oreil.ly/DO3WR)。
- en: ^([21](ch11.xhtml#ch01fn388-marker)) “Fitness App Strava Lights Up Staff at
    Military Bases,” *BBC News*, January 29, 2018, [*https://oreil.ly/hXwpN*](https://oreil.ly/hXwpN).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ^([21](ch11.xhtml#ch01fn388-marker)) “健身应用 Strava 照亮军事基地的工作人员”，*BBC News*，2018
    年 1 月 29 日，[*https://oreil.ly/hXwpN*](https://oreil.ly/hXwpN)。
- en: ^([22](ch11.xhtml#ch01fn390-marker)) Matt Burgess, “Strava’s Heatmap Data Lets
    Anyone See.”
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ^([22](ch11.xhtml#ch01fn390-marker)) Matt Burgess，“Strava 的热图数据让任何人都能看到”。
- en: ^([23](ch11.xhtml#ch01fn391-marker)) Michael Feldman, Sorelle Friedler, John
    Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian, “Certifying and Removing
    Disparate Impact,” *arXiv*, July 16, 2015, [*https://oreil.ly/FjSve*](https://oreil.ly/FjSve).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ^([23](ch11.xhtml#ch01fn391-marker)) Michael Feldman, Sorelle Friedler, John
    Moeller, Carlos Scheidegger 和 Suresh Venkatasubramanian，“认证和消除不公平影响”，*arXiv*，2015
    年 7 月 16 日，[*https://oreil.ly/FjSve*](https://oreil.ly/FjSve)。
- en: ^([24](ch11.xhtml#ch01fn392-marker)) Wikipedia, s.v. “Differential privacy,”
    [*https://oreil.ly/UcxzZ*](https://oreil.ly/UcxzZ).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ^([24](ch11.xhtml#ch01fn392-marker)) Wikipedia，“差分隐私”，[*https://oreil.ly/UcxzZ*](https://oreil.ly/UcxzZ)。
- en: ^([25](ch11.xhtml#ch01fn393-marker)) Eugene Bagdasaryan and Vitaly Shmatikov,
    “Differential Privacy Has Disparate Impact on Model Accuracy,” *arXiv*, May 28,
    2019, [*https://oreil.ly/nrJGK*](https://oreil.ly/nrJGK).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ^([25](ch11.xhtml#ch01fn393-marker)) Eugene Bagdasaryan 和 Vitaly Shmatikov，“差分隐私对模型准确性的不同影响”，*arXiv*，2019
    年 5 月 28 日，[*https://oreil.ly/nrJGK*](https://oreil.ly/nrJGK)。
- en: ^([26](ch11.xhtml#ch01fn394-marker)) Sarah Hooker, Aaron Courville, Gregory
    Clark, Yann Dauphin, and Andrea Frome, “What Do Compressed Deep Neural Networks
    Forget?” *arXiv*, November 13, 2019, [*https://oreil.ly/bgfFX*](https://oreil.ly/bgfFX).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ^([26](ch11.xhtml#ch01fn394-marker)) Sarah Hooker, Aaron Courville, Gregory
    Clark, Yann Dauphin 和 Andrea Frome，“压缩深度神经网络忘记了什么？”*arXiv*，2019 年 11 月 13 日，[*https://oreil.ly/bgfFX*](https://oreil.ly/bgfFX)。
- en: ^([27](ch11.xhtml#ch01fn395-marker)) Sara Hooker, Nyalleng Moorosi, Gregory
    Clark, Samy Bengio, and Emily Denton, “Characterising Bias in Compressed Models,”
    *arXiv*, October 6, 2020, [*https://oreil.ly/ZTI72*](https://oreil.ly/ZTI72).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ^([27](ch11.xhtml#ch01fn395-marker)) Sara Hooker, Nyalleng Moorosi, Gregory
    Clark, Samy Bengio 和 Emily Denton，“压缩模型中的偏差特征”，*arXiv*，2020 年 10 月 6 日，[*https://oreil.ly/ZTI72*](https://oreil.ly/ZTI72)。
- en: ^([28](ch11.xhtml#ch01fn396-marker)) Hooker et al., “Characterising Bias in
    Compressed Models.”
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ^([28](ch11.xhtml#ch01fn396-marker)) Hooker 等人，“压缩模型中的偏差特征”。
- en: ^([29](ch11.xhtml#ch01fn397-marker)) Jonette M. Stecklein, Jim Dabney, Brandon
    Dick, Bill Haskins, Randy Lovell, and Gregory Moroney, “Error Cost Escalation
    Through the Project Life Cycle,” NASA Technical Reports Server (NTRS), [*https://oreil.ly/edzaB*](https://oreil.ly/edzaB).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ^([29](ch11.xhtml#ch01fn397-marker)) Jonette M. Stecklein, Jim Dabney, Brandon
    Dick, Bill Haskins, Randy Lovell 和 Gregory Moroney，“项目生命周期中的错误成本升级”，NASA 技术报告服务器（NTRS），[*https://oreil.ly/edzaB*](https://oreil.ly/edzaB)。
- en: ^([30](ch11.xhtml#ch01fn398-marker)) Margaret Mitchell, Simone Wu, Andrew Zaldivar,
    Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah
    Raji, and Timnit Gebru, “Model Cards for Model Reporting,” *arXiv*, October 5,
    2018, [*https://oreil.ly/COpah*](https://oreil.ly/COpah).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ^([30](ch11.xhtml#ch01fn398-marker)) Margaret Mitchell, Simone Wu, Andrew Zaldivar,
    Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah
    Raji 和 Timnit Gebru，“模型报告的模型卡”，*arXiv*，2018 年 10 月 5 日，[*https://oreil.ly/COpah*](https://oreil.ly/COpah)。
- en: ^([31](ch11.xhtml#ch01fn399-marker)) Mitchell et al., “Model Cards for Model
    Reporting.”
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ^([31](ch11.xhtml#ch01fn399-marker)) Mitchell 等人，“模型报告的模型卡”。
