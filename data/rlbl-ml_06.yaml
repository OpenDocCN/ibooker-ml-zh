- en: Chapter 5\. Evaluating Model Validity and Quality
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章。评估模型的有效性和质量
- en: 'OK, so our model developers have created a model that they say is ready to
    go into production. Or we have an updated version of a model that needs to be
    swapped in to replace a currently running version of that model in production.
    Before we flip the switch and start using this new model in a critical setting,
    we need to answer two broad questions. The first establishes model *validity*:
    will the new model break our system? The second addresses model *quality*: is
    the new model any good?'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，所以我们的模型开发人员已经创建了一个他们认为可以投入生产的模型。或者我们有一个更新版本的模型需要替换当前在生产中运行的版本。在我们打开开关并在关键环境中开始使用这个新模型之前，我们需要回答两个广泛的问题。第一个是确定模型的*有效性*：新模型会破坏我们的系统吗？第二个是关于模型的*质量*：新模型是否好用？
- en: These are simple questions to ask but may require deep investigation to answer,
    often necessitating collaboration among folks with various areas of expertise.
    From an organizational perspective, it is important for us to develop and follow
    robust processes to ensure that these investigations are carried out carefully
    and thoroughly. Channeling our inner Thomas Edison, it is reasonable to say that
    model development is 1% inspiration and 99% verification.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是简单的问题，但可能需要深入调查才能回答，通常需要各种专业领域的人员合作。从组织的角度来看，对我们来说很重要的是制定并遵循健全的流程，以确保这些调查被仔细和彻底地执行。借用我们内心的托马斯·爱迪生，可以说模型开发是1%的灵感和99%的验证。
- en: This chapter dives into questions of both validity and quality, and provides
    enough background to allow MLOps folks to engage with both of these issues. We
    will also spend time talking about how to build processes, automation, and a strong
    culture around ensuring that these issues are treated with the appropriate attention,
    care, and rigor that practical deployment demands.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章深入探讨了有效性和质量的问题，并提供了足够的背景，使得MLOps从业者能够参与这两个问题。我们还将花时间讨论如何构建流程、自动化以及围绕确保这些问题得到适当关注、关怀和严谨对待的强大文化。
- en: '[Figure 5-1](#a_simplified_view_of_the_repeated_cycle) outlines the basic steps
    of model development and the role that quality plays in it. While this chapter
    focuses on evaluation and verification methods, it is important to note that these
    processes will likely be repeated over time in an iterative fashion. See [Chapter 10](ch10.xhtml#continuous_ml)
    for more on these topics.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-1](#a_simplified_view_of_the_repeated_cycle) 概述了模型开发的基本步骤以及质量在其中的角色。虽然本章重点介绍了评估和验证方法，但重要的是要注意这些过程可能会在时间上以迭代的方式重复进行。更多信息请参阅[第十章](ch10.xhtml#continuous_ml)。'
- en: '![A simplified view of the repeated cycle of model development](Images/reml_0501.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![模型开发重复周期的简化视图](Images/reml_0501.png)'
- en: Figure 5-1\. A simplified view of the repeated cycle of model development
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-1。模型开发重复周期的简化视图
- en: Evaluating Model Validity
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型的有效性
- en: It has been said that all humans crave validation of one form or another, and
    we MLOps folks are no different. Indeed, *validity* is a core concept for MLOps,
    and in this context we cover the concept of whether a model will create system-level
    failures or crashes if put into production.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 有人说所有人类都渴望以某种方式获得验证，我们MLOps从业者也不例外。事实上，*有效性* 是MLOps的核心概念，在这个背景下，我们涵盖了模型是否在投入生产时会导致系统级故障或崩溃的概念。
- en: The kinds of things that we consider for validity checks are distinct from model
    quality issues. For example, a model could be horribly inaccurate, incorrectly
    guessing that every image shown should be given the label `chocolate pudding`
    without causing system-level crashes. Similarly, a model might be shown to have
    wonderful predictive performance in offline tests, but rely on a particular feature
    version that is not currently available in the production stack, or use an incompatible
    version of some ML package, or rarely give values of `NaN` that cause downstream
    consumers to crash. Testing for validity is a first step that allows us to be
    sure that a model will not cause catastrophic harm to our system.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑有效性检查的事物与模型质量问题是不同的。例如，一个模型可能准确度极低，错误地猜测每张显示的图像都应该被标记为`chocolate pudding`，但不会导致系统级崩溃。同样地，一个模型在离线测试中可能表现出色，但依赖于当前生产堆栈中不可用的特定功能版本，或者使用某个ML包的不兼容版本，或者偶尔产生导致下游消费者崩溃的`NaN`值。验证有效性是第一步，确保模型不会对我们的系统造成灾难性危害。
- en: 'Here are some things to test for when verifying that a model will not bring
    our system to its knees:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在验证模型不会使我们的系统陷入困境时，有些需要测试的事项如下：
- en: Is it the right model?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 是否是正确的模型？
- en: Surprisingly easy to overlook, it is important to have a foolproof method for
    ensuring that the version of the model we are intending to serve is the version
    we are actually using. Including timestamp information and other metadata within
    the model file is a useful backstop. This issue highlights the importance of automation
    and shows the difficulties that can arise with ad hoc manual processes.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶易被忽视的是，确保我们打算服务的模型版本确实是我们实际使用的版本是非常重要的。在模型文件中包含时间戳信息和其他元数据是一个有用的后备方案。这个问题突显了自动化的重要性，并展示了随意手动过程可能带来的困难。
- en: Will the model load in a production environment?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 模型能在生产环境中加载吗？
- en: To verify this, we create a copy of the production environment and simply try
    to load the model. This sounds pretty basic, but it is a good place to start because
    it is surprisingly easy for errors to occur at this stage. As you’ll learn in
    [Chapter 7](ch07.xhtml#training_systems), we are likely to take a trained version
    of a model and copy it to another location where it will be used either for offline
    scoring by a large batch process, or for online serving of live traffic on demand.
    In either case, the model is likely to be stored as a file or set of files in
    a particular format that are then moved, copied, or replicated for serving. This
    is necessary because models tend to be large, and we also want to have versioned
    checkpoints around that can be used as artifacts both for serving and for future
    analysis or as recovery options in case of an unforeseen crisis or error. The
    problem is that file formats tend to change slightly over time as new options
    are added, and there is always at least some chance that the format a model is
    saved in is not a format compatible with our current serving system.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证这一点，我们创建生产环境的副本，然后简单地尝试加载模型。这听起来很基础，但这是一个很好的起点，因为在这个阶段出现错误是令人惊讶地容易。正如你在[第7章](ch07.xhtml#training_systems)中所学到的，我们通常会获取模型的训练版本，并将其复制到另一个位置，该位置将用于离线批处理的评分，或者根据需求在线提供实时流量的服务。无论哪种情况，模型通常会以特定格式的文件或文件集合的形式存储，然后进行移动、复制或复制以供服务。这是必要的，因为模型往往体积较大，而且我们也希望有版本化的检查点，可以作为服务和未来分析的工件，或者在发生意外危机或错误时作为恢复选项。问题在于，随着时间的推移，文件格式往往会略有变化，而且模型保存的格式可能与我们当前的服务系统兼容的格式不同。
- en: Another issue that can create loading errors is that the model file may be too
    large to be loaded into available memory. This is especially possible in memory-constrained
    serving environments, such as on-device settings. However, it can also occur when
    model developers zealously increase model size to pursue additional accuracy,
    which is an increasingly common theme in contemporary ML. It is important to note
    that the size of the model file and the size of the model instantiated in memory
    are correlated, but often only loosely so, and are absolutely not identical. We
    cannot just look at the size of the model file to ensure that the resulting model
    is sufficiently small to be successfully loaded in our environment.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能导致加载错误的问题是，模型文件可能过大，无法加载到可用内存中。这在内存受限的服务环境中尤为可能，例如设备设置中。但是，当模型开发者热衷于增加模型大小以追求更高的准确性时，这也可能发生，这在当今的机器学习中越来越常见。重要的是要注意，模型文件的大小和内存中实例化的模型的大小是相关的，但通常只是松散相关，并且绝对不相同。我们不能仅仅查看模型文件的大小来确保生成的模型足够小，以便在我们的环境中成功加载。
- en: Can the model serve a result without crashing the infrastructure?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 模型能在不崩溃基础架构的情况下提供结果吗？
- en: 'Again, this seems like a straightforward requirement: if we feed the model
    one minimal request, does it give us a result back of any kind, or does the system
    fail? Note that we say “one minimal request” as opposed to many requests intentionally,
    because these sorts of tests are often best done with single examples and single
    requests to start. This both minimizes risk and makes debugging easier if failures
    do arise.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，这似乎是一个直截了当的要求：如果我们向模型提供一个最小的请求，它是否会给我们返回任何类型的结果，还是系统会失败？请注意，我们故意说“一个最小的请求”，而不是许多请求，因为这类测试通常最好从单个示例和单个请求开始。这既可以最小化风险，又可以使调试更容易，如果确实出现故障的话。
- en: 'Serving the result for one request might cause a failure, for several reasons:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为一个请求提供结果可能会导致失败，原因有几种：
- en: Platform version incompatibility
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 平台版本不兼容
- en: Especially when using third-party or open source platforms, the serving stack
    could easily be using a different version of the platform than the training stack
    used.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是在使用第三方或开源平台时，服务堆栈可能会轻易地使用与训练堆栈不同的平台版本。
- en: Feature version incompatibility
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 特征版本不兼容
- en: The code for generating features is often different in the training stack from
    the serving stack, especially when each stack has different memory, compute cost,
    or latency requirements. In such cases, it is easy for the code that generates
    a given feature to get out of sync in these different systems, causing failures—one
    form of a general class of problems often referred to as *training-serving skew*.
    For example, if a dictionary is used to map word tokens to integers, the serving
    stack might be using a stale version of that dictionary even after a newer one
    was created for the training stack.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 生成特征的代码在训练堆栈和服务堆栈中通常是不同的，特别是当每个堆栈具有不同的内存、计算成本或延迟要求时。在这种情况下，导致一个给定特征在这些不同系统中失去同步的代码可能导致失败——这是通常被称为*训练-服务偏差*问题类别的一种形式。例如，如果一个字典用于将单词标记映射到整数，那么服务堆栈在为训练堆栈创建新字典后可能仍在使用过时版本的字典。
- en: Corrupted model
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 损坏的模型
- en: Errors happen, jobs crash, and in the end our machines are physical devices.
    It is possible for model files to become corrupted in one way or another, either
    through error at write time or by having `NaN` values written to disk if there
    were not sufficient sanity checks in training.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 错误发生了，作业崩溃了，最终我们的机器是物理设备。模型文件可能因写入时的错误或在训练时未进行足够的健全性检查而出现某种方式的损坏，或者在磁盘上写入NaN值。
- en: Missing plumbing
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失的管道连接
- en: Imagine that a model developer creates a new version of a feature in training,
    but neglects to implement or hook up a pipeline that allows that feature to be
    used in the serving stack. In these cases, loading in a version of the model that
    relies on that feature will result in crashes or undesirable behavior.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 假设模型开发人员在训练中创建了一个功能的新版本，但忽略了实现或连接管道，使该功能可以在服务堆栈中使用。在这些情况下，加载依赖该功能的模型版本将导致崩溃或不良行为。
- en: Results out of range
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 结果超出范围
- en: Our downstream infrastructure might require the model predictions to be within
    a given range. For example, consider what might happen if a model is supposed
    to return a probability value between 0 and 1, not inclusive, but instead returns
    a score of exactly 0.0, or even –0.2\. Or consider a model that is supposed to
    return 1 of 100 classes to signify the most appropriate image label, but instead
    returns 101.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下游基础设施可能要求模型预测在给定范围内。例如，考虑一下，如果一个模型应返回一个介于0和1之间（不包括）的概率值，但实际上返回了0.0的分数，甚至是-0.2。或者考虑一个模型应返回100个类中的一个以表示最合适的图像标签，但实际上返回了101。
- en: Is the computational performance of the model within allowable bounds?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的计算性能是否在允许范围内？
- en: In systems that use online serving, models must return results on the fly, which
    typically means tight latency constraints must be met. For example, a model intended
    to do real-time language translation might have a budget of only a few hundred
    milliseconds to respond. A model used within the context of high-frequency stock
    trading might have significantly less.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用在线服务的系统中，模型必须即时返回结果，这通常意味着必须满足严格的延迟约束。例如，一个旨在进行实时语言翻译的模型可能只有几百毫秒的预算来响应。而在高频股票交易上使用的模型可能会更少。
- en: Because such constraints are often in tension with each other, for changes made
    by model developers in the search for increased accuracy, it is critical to measure
    latency before deployment. In doing so, we need to keep in mind that the production
    settings are likely to have peculiarities around hardware or networking that create
    bottlenecks that might differ from a development environment, so latency testing
    must be done as close to the production setting as possible. Similarly, even in
    offline serving situations, overall compute cost can be a significant limiting
    factor, and it is important to assess any cost changes before kicking off huge
    batch-processing jobs. Finally, as discussed previously, storage costs, such as
    size of the model in RAM, are another critical limitation and must be assessed
    prior to deployment. Checks like this can be automated, but it may also be useful
    to verify manually to consider trade-offs.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这些约束通常互相冲突，对于模型开发者在寻求提高准确性的更改中，测量部署前的延迟非常关键。在这样做时，我们需要牢记生产环境可能存在的硬件或网络方面的特殊性，这些可能会导致不同于开发环境的瓶颈，因此延迟测试必须尽可能接近生产环境进行。同样地，即使在离线服务的情况下，总体计算成本可能是一个重要的限制因素，在启动大批处理作业之前评估任何成本变化是很重要的。最后，正如之前讨论的，存储成本，例如模型在
    RAM 中的大小，是另一个关键限制，必须在部署之前进行评估。这样的检查可以自动化进行，但手动验证以考虑权衡也可能很有用。
- en: For online serving, does the model pass through a series of gradual canary levels
    in production?
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在线服务，模型是否会在生产环境中经过一系列渐进的金丝雀级别？
- en: 'Even after we have some confidence in a new model version based on the validation
    checks, we will not want to just flip a switch and have the new model suddenly
    take on the full production load. Instead, our collective stress level will be
    reduced if we first ask the model to serve just a tiny trickle of data, and then
    gradually increase the amount after we have assurance that the model is performing
    as expected in serving. This form of canary ramp-up is a place where model validation
    and model monitoring, as discussed in [Chapter 9](ch09.xhtml#monitoring_and_observability_for_models),
    overlap to a degree: our final validation step is a controlled ramp-up in production
    with careful monitoring.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们对新模型版本在验证检查基础上有了一些信心，我们也不希望只是翻转开关，让新模型突然承担全部生产负载。相反，如果我们首先要求模型仅服务一小部分数据，并在确保模型在服务中表现如预期后逐渐增加数据量，我们的整体压力将会减少。这种金丝雀渐进上升的方式是模型验证和模型监控的交集，正如在[第9章](ch09.xhtml#monitoring_and_observability_for_models)中讨论的那样：我们的最终验证步骤是在生产中进行控制的渐进升级，并进行仔细监控。
- en: Evaluating Model Quality
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型质量
- en: Ensuring that a model passes validation tests is important, but by itself does
    not answer whether the model is good enough to do its job well. Answering these
    kinds of questions takes us into the realm of evaluating model quality. Understanding
    these issues is important for model developers, of course, but is also critical
    for both organizational decision makers and for MLOps folks in charge of keeping
    systems running smoothly.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 确保模型通过验证测试是重要的，但这本身并不能回答模型是否足够好来完成其工作。回答这类问题将使我们进入评估模型质量的领域。当然，理解这些问题对于模型开发者很重要，但对于组织决策者以及负责保持系统平稳运行的
    MLOps 人员同样至关重要。
- en: Offline Evaluations
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 离线评估
- en: As discussed in the whirlwind tour of the model development lifecycle in [Chapter 2](ch02.xhtml#data_management_principles),
    model developers typically rely on offline evaluations, such as looking at accuracy
    on a held-out test set as a way to judge how good a model is. Clearly, this kind
    of evaluation has limitations, as we will talk about later in this chapter—after
    all, accuracy on held-out data is not the same thing as happiness or profit. Despite
    their limitations, these offline evaluations are the bread and butter of the development
    lifecycle because they offer a reasonable proxy while existing in a sweet spot
    of low cost and high efficiency that allows developers to test many changes in
    rapid succession.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[第2章](ch02.xhtml#data_management_principles)中讨论的模型开发生命周期的快速导览中所述，模型开发者通常依赖离线评估，例如查看在保留测试集上的准确性，作为评判模型好坏的一种方式。显然，这种评估方式有其局限性，正如我们在本章稍后将讨论的那样——毕竟，在保留数据上的准确性并不等同于幸福或利润。尽管存在局限性，这些离线评估是开发生命周期的主要内容，因为它们在成本低效率高的甜蜜点上提供了一个合理的代理，使开发者能够快速连续测试多个更改。
- en: 'Now, what is an evaluation? An *evaluation* consists of two key components:
    a performance metric and a data distribution. *Performance metrics* are things
    like accuracy, precision, recall, area under the ROC curve, and so on—we will
    talk about a few of these later on if they are not already familiar. *Data distributions*
    are things like “a held-out test set that was randomly sampled from the same source
    of data as the training data” that we have talked about before, but held-out test
    data is not the only distribution that might be important to look at. Others might
    include “images specifically from roads in snowy conditions” or “yarn store queries
    from users in Norway” or “protein sequences that have not previously been identified
    by biologists.”'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，什么是评估呢？*评估*由两个关键组成部分组成：性能指标和数据分布。*性能指标*包括准确率、精确率、召回率、ROC曲线下面积等等——如果这些还不熟悉，稍后我们会详细讨论其中几个。*数据分布*则是诸如“从与训练数据相同的数据源中随机抽样的留出测试集”，我们之前讨论过，但留出测试数据并非唯一重要的分布。其他可能包括“特定于雪天路况的图像”或“挪威用户的纱线店查询”或“生物学家尚未确认的蛋白质序列”等等。
- en: An evaluation is always composed of both a metric and a distribution together—the
    evaluation shows the model’s performance on the data in that distribution, as
    computed by the chosen metric. This is important to know because folks in the
    ML world sometimes use shorthand and say things like “this model has better accuracy”
    without clarifying what the distribution is. This sort of shorthand can be dangerous
    for our systems because it neglects to mention which distribution is used in the
    evaluation, and can lead to a culture in which important cases are not fully assessed.
    Indeed, many of the issues around fairness and bias that emerged in the late 2010s
    can likely be tracked down to not giving sufficient consideration to the specifics
    of the data distribution used at test time during model evaluations. Thus, when
    we hear a statement like “accuracy is better,” we can always add value by asking
    *on what distribution?*
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 评估总是由性能指标和数据分布两者共同组成——评估展示了模型在该分布中数据上的性能，由所选指标计算得出。这一点很重要，因为在机器学习领域，有时人们会使用简化说法，比如“这个模型的准确率更高”，而不澄清使用的数据分布是什么。这种简化说法对我们的系统是危险的，因为它忽略了在评估过程中使用的数据分布的具体情况，可能导致忽视重要案例的文化。实际上，2010年代末出现的许多关于公平性和偏见的问题很可能可以追溯到在模型评估中未充分考虑数据分布的具体情况。因此，当我们听到“准确率更高”的说法时，我们总是可以通过问“在哪个数据分布上？”来增加价值。
- en: Evaluation Distributions
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估分布
- en: Perhaps no question is more important in the understanding of an ML system than
    deciding how to create the evaluation data. Here are some of the most common distributions
    used, along with some factors to consider as their strengths and weaknesses.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解机器学习系统中，也许没有比决定如何创建评估数据更重要的问题了。以下是一些常用的分布以及一些考虑其优势和劣势的因素。
- en: Held-out test data
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 留出测试数据
- en: The most common evaluation distribution used is *held-out test data*, which
    we covered in [Chapter 3](ch03.xhtml#basic_introduction_to_models) when reviewing
    the typical model lifecycle. On the surface, this seems like an easy thing to
    think about—we randomly select some of our training data to be set aside and used
    only for evaluation. When each example in the training data has an equal and independent
    chance of being put into the held-out test data, we call this an *IID test set*.
    The *IID* term is statistics-speak that means *independently and identically distributed*.
    We can think of the IID test set process as basically flipping a (maybe biased)
    coin or rolling a die for each example in the training data, and holding each
    one out for the IID test set based on the result.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的评估分布是*留出测试数据*，我们在[第三章](ch03.xhtml#basic_introduction_to_models)中介绍了它，当时正在回顾典型的模型生命周期。表面上看，这似乎是一个容易考虑的事情——我们随机选择一些训练数据，将其保留并仅用于评估。当训练数据中的每个示例具有等概率且独立地放入留出测试数据时，我们称之为*IID测试集*。*IID*术语是统计学术语，意味着*独立同分布*。我们可以将IID测试集过程简单地理解为针对训练数据中的每个示例基于结果像是抛一枚（可能是有偏的）硬币或掷一颗骰子，并根据结果保留其中的每一个示例用于IID测试集。
- en: The use of IID test data is widely established, not because it is necessarily
    the most informative way to create test data, but because it is the way that respects
    the assumptions that underpin some of the theoretical guarantees for supervised
    ML. In practice, a purely IID test set might be inappropriate, though, because
    it might give an unrealistically rosy view of our model’s expected performance
    in deployment.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用IID测试数据已被广泛接受，这不是因为它一定是创建测试数据的最具信息性的方式，而是因为它尊重了支持一些监督机器学习理论保证的假设。然而，在实践中，一个纯粹的IID测试集可能不合适，因为它可能会给出一个在部署中模型预期性能的不现实的乐观视图。
- en: As an example, imagine we have a large set of stock-price data, and we want
    to train a model to predict these prices. If we create a purely IID test set,
    we might have training data from 12:01 and 12:03 from a given day in the training
    data, while data from 12:02 ends up in the test data. This would create a situation
    in which the model can make a better guess about 12:02 because it has seen the
    “future” of what 12:03 looks like. In reality, a model that is guessing about
    12:02 would be unable to have access to this kind of information, so we would
    need to be careful to create our evaluations in a way that does not allow the
    model to train on “future” data. Similar examples might exist in weather prediction,
    or yarn product purchase prediction.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 举例来说，想象我们有一个大量股价数据的数据集，我们希望训练一个模型来预测这些价格。如果我们创建一个纯粹的IID测试集，我们可能会在训练数据中有来自某一天12:01和12:03的数据，而12:02的数据却在测试数据中。这会造成这样一种情况：模型可以更好地猜测12:02，因为它已经看到了12:03的“未来”。实际上，一个关于12:02的模型是无法访问这种信息的，所以我们需要小心地创建我们的评估方式，不允许模型在“未来”数据上进行训练。类似的例子可能存在于天气预测或纱线产品购买预测中。
- en: The point here is not that IID test distributions are always bad, but rather
    that the details here really do matter. We need to apply careful reasoning and
    common sense to the creation of our evaluation data, rather than relying on fixed
    recipes.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的重点不是说IID测试分布总是不好，而是这些细节确实很重要。我们需要在创建评估数据时应用谨慎的推理和常识，而不是依赖固定的配方。
- en: Progressive validation
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 渐进式验证
- en: In systems with a time-based ordering to data—like our preceding stock-price
    prediction example—it can be quite helpful to use a progressive validation strategy,
    sometimes also called *backtesting*. The basic idea is to simulate how training
    and prediction would work in the real world, but playing the data through to the
    model in the same order that it originally appeared.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有基于时间排序数据的系统中——就像我们前面的股价预测示例一样——使用渐进式验证策略可能非常有帮助，有时也称为*回测*。基本思想是模拟训练和预测在真实世界中的工作方式，但是按照数据最初出现的顺序将数据通过模型。
- en: For each simulated time step, the model is shown the next example and asked
    to make its best prediction. That prediction is then recorded and incorporated
    to the aggregate evaluation metric, and only then is the example shown to the
    model for training. In this way, each example is first used for evaluation and
    then used for training. This helps us see the effects of temporal ordering, and
    ask questions like, “What would this model have done last year on election day?”
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个模拟时间步骤，模型展示下一个示例并要求做出最佳预测。然后记录该预测并将其合并到总体评估指标中，然后才将示例展示给模型进行训练。通过这种方式，每个示例首先用于评估，然后用于训练。这有助于我们看到时间顺序的影响，并提出诸如“去年选举日这个模型会怎么做？”的问题。
- en: The drawback is that this setup is somewhat awkward to adapt if our models require
    many passes over the data to train well. A second drawback is that we must be
    careful to make comparisons between models based on evaluation data from exactly
    the same time range. Finally, not all systems will operate in a setting in which
    the data can meaningfully be ordered in a canonical way, like time.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点是，如果我们的模型需要多次通过数据进行良好训练，这种设置有些难以适应。第二个缺点是，我们必须小心比较基于完全相同时间范围的评估数据的模型。最后，不是所有的系统都会在一个可以有意义地按照规范顺序排序数据的环境中运行，比如时间。
- en: Golden sets
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 黄金集
- en: In models that continually retrain and evaluate using some form of progressive
    validation, it can be difficult to know whether the model performance is changing
    or whether the data is getting easier or harder to predict on. One way to control
    this is to create a *golden set* of data that models are not ever allowed to train
    on, but that is from a specific point in time. For example, we might decide that
    the data from October 1 of last year might be set aside as golden set data, never
    to be used for training under any circumstance, but held aside.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在不断重新训练和使用某种形式的渐进验证的模型中，很难确定模型性能是否正在改变，还是数据变得更容易或更难预测。控制这种情况的一种方法是创建一组*金标准*数据，模型永远不允许在其上进行训练，但数据是从特定时间点获取的。例如，我们可能决定去年10月1日的数据被设置为金标准数据，无论何种情况下都不得用于训练，而是单独保留。
- en: When we set aside the golden set of data, we also keep with it either the results
    of running that set of data through our model or, in some cases, the result of
    having humans evaluate the golden set. We might sometimes treat these results
    as “correct” even if they are really just the predictions for those examples from
    a specific process and at a particular point in time.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们设置金标准数据时，还会将该数据集通过我们的模型运行的结果，或者在某些情况下，人类对金标准数据进行评估的结果与之保持一致。有时我们可能会把这些结果视为“正确的”，即使它们实际上只是从特定过程和特定时间点的这些示例的预测结果。
- en: Performance on golden set data like this can reveal any sudden changes in model
    quality, which can aid debugging greatly. Note that golden set evaluations are
    not particularly useful for judging absolute model quality, because their relevance
    to current performance diminishes as their time period recedes into the past.
    Another problem can arise if we are not able to keep golden set data around for
    very long (for example, to respect certain data privacy laws or to respond to
    requests for deletion or expiration of access). The primary benefit of golden
    sets is to identify changes or bugs, because, typically, model performance on
    golden set data changes only gradually as new training data is incorporated into
    the model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 金标准数据的性能评估能够显示模型质量的突然变化，这对于调试大有裨益。需要注意的是，金标准评估并不特别适用于评判模型的绝对质量，因为随着时间的推移，其对当前性能的相关性会逐渐降低。如果我们无法长时间保留金标准数据（例如，为了遵守某些数据隐私法律或响应删除或访问到期请求），还可能会出现另一个问题。金标准的主要好处在于识别变化或错误，因为通常情况下，随着新训练数据被整合到模型中，模型在金标准数据上的表现只会逐渐改变。
- en: Stress-test distributions
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 压力测试分布
- en: When deploying models in the real world, one worry is that the data they may
    encounter in reality may differ substantially from the data they were shown in
    training. (These issues are sometimes described by different names in the literature,
    including *covariate shift*, *nonstationarity*, or *training-serving skew*). For
    example, we might have a model trained largely on image data from North America
    and Western Europe, but that is then later applied in countries across the globe.
    This creates two possible problems. First, the model may not perform well on the
    new kinds of data. Second, and even more important, we may not know that the model
    would not perform well because the data was not represented in the source that
    supplied the (supposedly) IID test data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中部署模型时，一个担忧是它们在现实中遇到的数据可能与训练时展示的数据大不相同。（文献中有时将这些问题描述为不同的名称，包括*协变量转移*、*非稳态*或*训练-服务偏差*）。例如，我们可能有一个主要基于北美和西欧图像数据进行训练的模型，但后来将其应用于全球各国。这会产生两个可能的问题。首先，模型可能在新类型的数据上表现不佳。其次，更为重要的是，我们可能并不知道模型表现不佳的原因，因为数据在提供（据称为IID测试数据）的源中未被代表。
- en: Such issues are especially critical from a fairness and inclusion standpoint.
    Imagine we are building a model to predict the yarn color preferred by a user,
    given a provided portrait image. If our training data does not include portrait
    images with a wide range of skin tones, an IID test set might not have sufficient
    representation to uncover problems if the model does not do well for images of
    people with especially dark skin tones. (This example harkens back to seminal
    work by Buolamwini and Gebru.)^([1](ch05.xhtml#ch01fn39)) In cases like this,
    it’s important to create specific stress-test distributions in which carefully
    constructed test sets each probe for model performance on different skin tones.
    Similar logic applies to testing any other area of model performance that might
    be critical in practice, from snowy streets for navigation systems developed in
    temperate climates, to a broad range of accents and languages for speech-recognition
    systems developed in a majority English-speaking workplace.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这类问题在公平和包容的角度尤为关键。假设我们正在建立一个模型，根据提供的人像图像预测用户偏好的纱线颜色。如果我们的训练数据不包括具有各种肤色的人像图像，那么IID测试集可能无法充分代表，以揭示模型在具有特别深色皮肤人群的图像上表现不佳的问题。（这个例子回溯到Buolamwini和Gebru的开创性工作。）在这种情况下，创建特定的压力测试分布是非常重要的，其中精心构建的测试集各自探索模型在不同肤色上的性能。类似的逻辑适用于测试模型在实践中可能关键的任何其他性能领域，从适用于温带气候中开发的导航系统的雪地街道，到适用于多数英语工作场所中开发的语音识别系统的广泛语音和语言。
- en: Sliced analysis
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**分析切片**'
- en: One useful trick to consider is that any test set—even an IID test set—can be
    sliced to effectively create a variety of more targeted stress-test distributions.
    By *slicing*, we mean filtering the data based on the value of a certain feature.
    For example, we could slice images to look at performance on images with only
    snowy backgrounds, or stocks from companies that were only in their first week
    of trading on the market, or yarns that were only a shade of red. So long as we
    have at least some data that conforms to these conditions, we can evaluate performance
    on each of these cases through slicing. Of course, we need to take care not to
    slice too finely, in which case the amount of data we would be looking at would
    be too small to say anything meaningful in a statistical sense.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个有用的技巧需要考虑，即任何测试集——即使是IID测试集——都可以被切片，以有效地创建各种更有针对性的压力测试分布。通过*切片*，我们指的是根据某个特定特征的值对数据进行过滤。例如，我们可以切片图像，查看仅具有雪地背景的图像的性能，或者是市场上第一周交易的公司的股票，或者是仅为红色的纱线。只要我们至少有一些符合这些条件的数据，我们就可以通过切片评估每种情况下的性能。当然，我们需要注意不要切片得太细，否则我们所看到的数据量在统计意义上将会太小，无法得出任何有意义的结论。
- en: Counterfactual testing
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**对事实进行反事实测试**'
- en: One way to understand a model’s performance at a deeper level involves learning
    what the model would have predicted if the data had been different. This is sometimes
    called *counterfactual testing* because the data that we end up feeding to the
    model runs counter to the actual data in some way. For example, we might ask what
    the model would predict if the dog in a given image were not on a grassy background,
    but instead shown against a background of snow or of clouds.^([2](ch05.xhtml#ch01fn40))
    We might ask if the model would have recommended a higher credit score if the
    applicant had lived in a different city, or if the model would have predicted
    a different review score if the pronoun for the lead actor in a movie had been
    switched from *he* to *she*.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 了解模型更深层次性能的一种方法涉及学习，即如果数据不同，模型将如何进行预测。这有时被称为*反事实测试*，因为我们最终提供给模型的数据与某种方式上的实际数据相悖。例如，我们可能会问，如果给定图像中的狗不是在草地背景上，而是在雪地或云层背景下，模型会预测什么。我们可能会问，如果申请人住在不同的城市，模型是否会推荐更高的信用评分，或者如果电影中主演的代词从*他*变为*她*，模型是否会预测出不同的评分。
- en: The trick here is that we might not have any examples that match any of these
    scenarios, in which case we would take the step of creating synthetic counterfactual
    examples by manipulating or altering examples that we do have access to. This
    tends to be most effective when we want to test that a given alteration does *not*
    substantially change model prediction. Each of these tests might reveal something
    interesting about the model and the kinds of information sources it relies on,
    allowing us to use judgment about whether it is appropriate model behavior and
    whether we need to address any issues prior to launch.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的诀窍在于，我们可能没有符合这些情景的任何示例，如果是这样的话，我们会采取创建合成反事实示例的步骤，通过操纵或改变我们可以访问到的示例来进行。当我们想要测试某个给定的改变是否*不会*显著改变模型预测时，这种方法往往非常有效。每一个这样的测试都可能揭示关于模型及其依赖的信息源的一些有趣的事情，从而使我们能够判断模型行为是否恰当，以及是否需要在发布前解决任何问题。
- en: A Few Useful Metrics
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一些有用的指标
- en: In some corners of the ML world, there is a tendency to look at a single metric
    as the standard way to view a model’s performance on a given task. For example,
    accuracy was, for years, the one way that models were evaluated on ImageNet held-out
    test data. And indeed, this mindset is most often seen in benchmarking or competition
    settings, in which the use of a single metric simplifies comparisons between different
    approaches. However, in real-world ML, it is often ill-advised to myopically consider
    only a single metric. It is better to think of each metric as a particular perspective
    or vantage point. Just as there is no one best place to watch the sun rise, there
    is no one best metric to evaluate a given model, and the most effective approach
    is often to consider a diverse range of metrics and evaluations, each of which
    has its own strengths, weaknesses, blind spots, and peculiarities.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习领域的某些角落，有一种倾向于将单一指标视为评估给定任务上模型性能的标准方法。例如，多年来，准确率一直是评估ImageNet留存测试数据上模型的唯一方式。事实上，在基准测试或竞赛设置中，这种心态通常最为常见，因为单一指标简化了不同方法之间的比较。然而，在现实世界的机器学习中，只单纯考虑一个指标通常是不明智的。最有效的方法往往是考虑多样化的指标和评估，每个指标都有其独特的优势、劣势、盲点和特异性，就像没有一个最佳的观日出的地方一样，也没有一个最佳的指标来评估一个给定的模型。
- en: 'Here, we will try to build up our intuition around some of the more common
    metrics. We divide them into three broad categories:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将试图建立对一些较常见指标的直觉。我们将它们分为三大类别：
- en: Canary metrics
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 金丝雀指标
- en: Are great at indicating that something is wrong with a model, but are not so
    effective at distinguishing a good model from a better one.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 擅长指出模型存在问题，但不太有效地区分好模型和更好模型。
- en: Classification metrics
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 分类指标
- en: Help us understand the impact of a given model on downstream tasks and decisions,
    but require fiddly tuning that can make comparisons between models more difficult.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 帮助我们理解模型对下游任务和决策的影响，但需要复杂的调整，可能会使模型之间的比较更加困难。
- en: Regression and ranking metrics
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 回归和排名指标
- en: Avoid this tuning and make comparisons easier to reason about, but may miss
    specific trade-offs that might be available when some errors are less costly than
    others.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 避免这种调整，使得比较更容易理解，但可能会忽视某些错误成本较低的具体折衷方案。
- en: Canary metrics
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 金丝雀指标
- en: As we’ve noted, this set of metrics offers a useful way to tell when something
    is horribly wrong with our model. Like the fabled canary in the coal mine, if
    any of these metrics is not singing as expected, then we definitely have a problem
    to deal with. On the flip side, if these metrics look good, that does not necessarily
    mean that all is well or that our model is perfect. These metrics are just a first
    line of detection for potential issues.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所指出的，这组指标为我们提供了一种有用的方法来判断我们的模型是否存在严重问题。就像传说中的煤矿中的金丝雀一样，如果任何一个指标的表现不如预期，那么我们肯定有问题需要解决。另一方面，如果这些指标看起来不错，并不一定意味着一切都好或者我们的模型完美无缺。这些指标只是潜在问题检测的第一道防线。
- en: Bias
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 偏差
- en: Here we use *bias* in the statistical sense rather than the ethical sense. Statistical
    bias is an easy concept—if we add up all the things we expect to see based on
    the model’s predictions, and then add up all the things we actually see in the
    data, do we get the same amount? In an ideal world, we would, and typically a
    well-functioning model will show very low bias, meaning a very low difference
    between the total expected and observed values for a given class of predictions.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用的*偏差*是指统计意义上的，而不是道德意义上的。统计偏差是一个简单的概念——如果我们根据模型的预测加总所有我们期望看到的东西，然后加总实际数据中我们实际看到的东西，结果是否相同？在理想的情况下，应该是相同的，通常一个运行良好的模型将展现出非常低的偏差，意味着对于给定的预测类别，预期值和观察到值之间的差异非常小。
- en: One of the nice qualities of bias as a metric is that unlike most other metrics,
    there is a “correct” value of 0.00 that we do expect most models to achieve. Differences
    here of even a few percent in one direction or another are often a sign that something
    is wrong. Bias often couples well with sliced analysis as an evaluation strategy
    to uncover problems. We can use sliced analysis to identify particular parts of
    the data that the model is performing badly on as a way to begin debugging and
    improving the overall model performance.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个评估指标，偏差的一个好处是，与大多数其他指标不同，我们期望大多数模型能够达到的“正确”值是0.00。这里偏差的偏差甚至一两个百分点的差异通常都是某些问题存在的迹象。偏差经常与切片分析结合起来作为一种评估策略，以揭示问题。我们可以使用切片分析来识别模型在某些数据部分上表现不佳的地方，作为开始调试和改善整体模型性能的方法。
- en: The drawback of bias as a metric is that it is trivial to create a model with
    perfectly zero bias, but that is a terrible model. As a thought experiment, this
    could be done by having a model that just returns the average observed value for
    every example—zero bias in aggregate, but totally uninformative. A pathologically
    bad model like this can be detected by looking at bias on more fine-grained slices,
    but the larger point remains. Bias as a metric is a great canary, but just having
    zero bias is not by itself indicative of a high-quality model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 以偏差作为评估指标的缺点在于，创造一个完全零偏差的模型是微不足道的，但那是一个糟糕的模型。作为一种思想实验，可以通过让模型只返回每个示例的平均观察值来实现这一点——总体上偏差为零，但完全没有信息价值。像这样病态糟糕的模型可以通过在更细粒度切片上观察偏差来检测，但更重要的问题仍然存在。偏差作为一个指标是一个很好的指示器，但仅仅有零偏差并不能表明模型质量高。
- en: Calibration
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 校准
- en: When we have a model that predicts a probability value or a regression estimate,
    like a probability that a user will click a given product or a numerical prediction
    of tomorrow’s temperature, creating a *calibration plot* can provide significant
    insight into the overall quality of the model. This is done essentially in two
    steps, which can be thought of roughly as first bucketing our evaluation data
    in a set of buckets and then computing the model’s bias in each of these buckets.
    Often, the bucketing is done by model score—for example, the examples that are
    in the lowest tenth of the model’s predictions go in one bucket, the next lowest
    tenth in the next bucket, and so on—in a way that brings to mind the idea of sliced
    analysis discussed previously. In this way, calibration can be seen as an extension
    of the approach of combining bias and sliced analysis, in a systematic way.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有一个预测概率值或回归估计的模型，比如用户点击给定产品的概率或明天温度的数值预测时，创建一个*校准图*可以对模型的整体质量提供重要的洞察。这主要可以分为两步进行，大致可以理解为首先将我们的评估数据分桶，并计算每个桶中模型的偏差。通常，分桶是通过模型分数来完成的——例如，模型预测中最低十分之一的示例放入一个桶中，接下来的十分之一放入下一个桶中，以此类推——这种方式让人想起之前讨论的切片分析的概念。这样，校准可以被看作是结合偏差和切片分析方法的一种系统扩展。
- en: Calibration plots can show systemic effects such as overprediction or underprediction
    in different areas, and can be an especially useful visualization to help understand
    how a model performs near the limits of its output range. In general, calibration
    plots can help show areas where a model may systematically overpredict or underpredict,
    by plotting the observed rates of occurrence versus their predicted probabilities.
    This can be useful to help detect situations where the model’s scores are either
    more or less reliable. For example, the plot in [Figure 5-2](#an_example_calibration_plot)
    shows a model that gives good calibration in the middle ranges, but does not do
    as well at the extremes, overpredicting on actual low probability examples and
    underpredicting on actual high-probability examples.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 校准图可以显示系统效应，如在不同区域的过度预测或欠预测，并且可以是特别有用的可视化工具，帮助理解模型在其输出范围极限附近的表现。总体而言，校准图可以帮助显示模型可能系统性地过度预测或欠预测的区域，通过绘制观察到的发生率与它们的预测概率。这对于检测模型评分更可靠或不太可靠的情况非常有用。例如，在
    [图 5-2](#an_example_calibration_plot) 中的图表显示，该模型在中间范围内有良好的校准，但在极端情况下表现不佳，在实际低概率示例上过度预测，在实际高概率示例上欠预测。
- en: '![An example calibration plot](Images/reml_0502.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![一个示例校准图](Images/reml_0502.png)'
- en: Figure 5-2\. An example calibration plot
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-2\. 一个示例校准图
- en: Classification metrics
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类指标
- en: When we think of model evaluation metrics, classification metrics like accuracy
    are often the first ones that come to mind. Broadly speaking, a *classification
    metric* helps measure whether we’ve correctly identified that a given example
    belongs to a specific category (or *class*). Class labels are typically discrete—things
    like `click` or `no_click`, or `lambs_wool`, `cashmere`, `acrylic`, `merino_wool`—and
    we tend to judge a prediction as a binary correct or incorrect on getting a given
    class label right.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们考虑模型评估指标时，分类指标如准确度通常是首先想到的。广义上说，*分类指标* 帮助衡量我们是否正确地识别了给定示例属于特定类别（或 *类* ）。类别标签通常是离散的——比如
    `click` 或 `no_click`，或者 `lambs_wool`、`cashmere`、`acrylic`、`merino_wool`——我们倾向于将预测判断为二进制的正确或错误，是否正确得到了给定的类别标签。
- en: 'Because models typically report a score for a given label, such as `lambs_wool:
    0.6`, `cashmere: 0.2`, `acrylic: 0.1`, `merino_wool: 0.1`, we need to invoke a
    decision rule of some kind to decide when we are going to predict a given label.
    This might involve a threshold, like “predict acrylic whenever the score for `acrylic`
    is above 0.41 for a given image,” or it might ask which class label gets the highest
    score out of all available options. Decision rules like these are a choice on
    the part of the model developer, and are often set by taking into account the
    potential costs of different kinds of mistakes. For example, it may be significantly
    more costly to miss identifying stop signs than to miss identifying merino wool
    products.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '因为模型通常为给定的标签报告一个分数，比如 `lambs_wool: 0.6`、`cashmere: 0.2`、`acrylic: 0.1`、`merino_wool:
    0.1`，我们需要调用某种决策规则来决定何时预测一个给定的标签。这可能涉及一个阈值，比如“每当 `acrylic` 的得分在给定图像中超过 0.41 时，预测为亚克力”，或者询问哪个类别标签在所有可用选项中得分最高。这些决策规则是模型开发者的选择，通常是通过考虑不同类型错误的潜在成本来设置的。例如，错过识别停止标志可能比错过识别美利奴羊毛产品显著更为昂贵。'
- en: With that background, let’s look at a couple of classic metrics.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些背景知识，让我们看一些经典的指标。
- en: Accuracy
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 准确度
- en: In conversation, many folks use the term *accuracy* to mean a general sense
    of goodness, but accuracy also has a formal definition that shows the fraction
    of predictions for which the model was correct. This satisfies an intuitive desire—we
    want to know how often our model was right. However, this intuition can sometimes
    be misleading without appropriate context.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在对话中，很多人使用 *准确度* 这个术语来表示一种总体的良好感，但准确度也有一个正式的定义，显示了模型预测正确的比例。这满足了我们的直觉需求——我们想知道我们的模型多频繁地正确。然而，这种直觉有时候在缺乏适当的背景信息时可能会误导我们。
- en: To place an accuracy metric into context, we need to have some understanding
    of how good a naive model that always predicts the most prevalent class would
    be, and also to understand relative costs of different types of errors. For example,
    99% accuracy sounds pretty good, but may be completely terrible if the goal is
    figuring out when it is safe to cross a busy street—we would be almost sure to
    be in an accident quite soon. Similarly, 0.1% accuracy sounds horrible but would
    be an amazingly good performance if the goal was to predict winning lottery number
    combinations. So, when we hear an accuracy value quoted, the first question should
    always be, “What is the base rate of each class?” It is also worth noting that
    seeing 100% accuracy—or perfect performance on any metric—is most often cause
    for concern rather than celebration, as this may indicate overfitting, label leakage,
    or other problems.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 要将准确度度量放入背景中，我们需要理解总是预测最普遍类别的天真模型有多好，还要了解不同类型错误的相对成本。例如，99% 的准确率听起来很不错，但如果目标是弄清楚何时安全地穿过繁忙的街道，那可能会很糟糕——我们很可能很快就会发生事故。类似地，0.1%
    的准确率听起来很糟糕，但如果目标是预测中奖的彩票号码组合，那将是惊人的表现。因此，当我们听到一个准确率值时，第一个问题应该始终是：“每个类别的基础率是多少？”还值得注意的是，看到100%
    的准确率或任何指标的完美表现通常更多是引起关注而不是庆祝，因为这可能表明过度拟合、标签泄露或其他问题。
- en: Precision and recall
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 精确率和召回率
- en: 'These two metrics are often paired together, and are related in an important
    way. Both metrics have a notion of a *positive*, which we can think of as “the
    thing we are trying hard to find.” This can be finding spam for a spam classifier,
    or yarn products that match the user’s interest for a yarn store model. These
    metrics answer the following related questions:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个指标通常成对出现，并且在一个重要的方面相关。这两个指标都有一个*正例*的概念，我们可以将其视为“我们努力寻找的东西”。这可以是为垃圾邮件分类器找到垃圾邮件，或者为毛线商店模型找到与用户兴趣匹配的毛线产品。这些指标回答以下相关的问题：
- en: Precision
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 精确率
- en: When we said an example was a positive, how often was that indeed the case?
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们说一个例子是正例时，这种情况确实发生了多少次？
- en: Recall
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率
- en: Out of all of the positive examples in our dataset, how many of them were identified
    by our model?
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据集中的所有正例中，我们的模型识别了多少个？
- en: These questions are especially useful to ask and answer when positives and negatives
    are not evenly split in the data. Unlike accuracy, the intuitions of what a precision
    of 90% or a recall of 95% might mean scale reasonably well even if positives are
    just a small fraction of the overall data.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据中正例和负例不均匀分布时，这些问题特别有用。与准确率不同，90% 的精确率或者95% 的召回率意味着什么的直觉在一定程度上是合理的，即使正例只是总体数据的一小部分。
- en: That said, it is important to notice that the metrics are in tension with each
    other in an interesting way. If our model does not have sufficient precision,
    we may be able to increase its precision by increasing the threshold it uses to
    make a decision. This would cause the model to say “positive” only when it is
    even more sure, and for reasonable models would result in higher precision. However,
    this would also mean that the model refrains from saying “positive” more often,
    meaning that it identifies fewer of the total possible number of positives and
    results in lower recall because of the increased precision. We could also trade
    off in the other direction, lowering thresholds to increase recall at the cost
    of less precision. This means that it is critical to consider these metrics together,
    rather than in isolation.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，重要的是注意这些指标之间存在一种有趣的紧张关系。如果我们的模型没有足够的精确度，我们可以通过增加它用于做出决策的阈值来提高其精确度。这会导致模型仅在更加确信时才说“正例”，对于合理的模型，这将导致更高的精确度。然而，这也意味着模型更少地说“正例”，即它识别的正例总数更少，召回率更低。我们也可以在另一个方向进行权衡，降低阈值以增加召回率，但代价是精确度降低。这意味着必须考虑这些指标的综合使用，而不是孤立地考虑它们。
- en: AUC ROC
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: AUC ROC
- en: This is sometimes just referred to as *area under the curve* (*AUC*). *ROC*
    is an abbreviation for *receiver operating characteristics*, a metric that was
    first developed to help measure and assess radar technology in the Second World
    War, but the acronym has become universally used.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 有时这仅仅被称为*曲线下面积*（*AUC*）。*ROC*是*接收者操作特征*的缩写，这是一个最初用于帮助测量和评估第二次世界大战中雷达技术的指标，但这个缩写已经普遍使用。
- en: Despite the confusing name, it has the lovely property of being a threshold-independent
    measure of model quality. Remember that accuracy, precision, and recall all rely
    on classification thresholds, which must be tuned. The choice of threshold can
    impact the value of the metric substantially, making comparisons between models
    tricky. AUC ROC takes this threshold tuning step out of the metric computation.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管名称令人困惑，AUC ROC 具有独立于阈值的模型质量度量的优良特性。请记住，准确率、精度和召回率都依赖于分类阈值，这些阈值必须进行调整。阈值的选择可以显著影响度量的值，使得模型之间的比较变得棘手。AUC
    ROC 将这个阈值调整步骤从度量计算中剔除。
- en: 'Conceptually, AUC ROC is computed by creating a plot showing the true-positive
    rate and the false-positive rate for a given model at every possible classification
    threshold, and then finding the area under that plotted curved line; see [Figure 5-3](#an_roc_curve_demonstrating_performance)
    for an example. (This sounds expensive, but efficient algorithms can be used for
    this computation that don’t involve actually running a lot of evaluations with
    different thresholds.) When the area under this curve is scaled to a range from
    0 to 1, this value also ends up giving the answer to the following question: “If
    we randomly choose one positive example and one negative from our data, what is
    the probability that our model gives a higher prediction score to the positive
    example rather than the negative?”'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，AUC ROC 是通过创建一个图表来计算的，该图表显示了给定模型在每个可能的分类阈值下的真正率和假正率，然后找出该曲线下的面积；参见[图5-3](#an_roc_curve_demonstrating_performance)作为示例。（听起来很昂贵，但可以使用高效算法来进行此计算，而无需实际运行大量不同阈值的评估。）当这条曲线下的面积缩放到0到1的范围时，这个值也提供了以下问题的答案：“如果我们从数据中随机选择一个正例和一个负例，那么我们的模型给出高分预测分数给正例而不是负例的概率是多少？”
- en: No metric is perfect, though, and AUC ROC does have a weakness. It is vulnerable
    to being fooled by model improvements that change the relative ordering of examples
    far away from any reasonable decision threshold, such as pushing an already low-ranked
    negative example even lower.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，没有完美的指标，AUC ROC 也有其弱点。它容易被模型改进所欺骗，这些改进改变了远离任何合理决策阈值的示例的相对排序，例如将已经排名低的负例进一步降低。
- en: '![An ROC curve demonstrating performance of a model on a set of classifications](Images/reml_0503.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![展示模型在一组分类上性能的 ROC 曲线](Images/reml_0503.png)'
- en: Figure 5-3\. An ROC curve demonstrating performance of a model on a set of classifications
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-3. 展示模型在一组分类上性能的 ROC 曲线
- en: Precision/recall curves
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 精度/召回率曲线
- en: Just as an ROC curve maps out the space of trade-offs between true-positive
    rate and false-positive rate at different decision thresholds, many folks plot
    precision/recall curves that map out the space of trade-offs between precision
    and recall at different decision thresholds. This can be useful to get an overall
    sense of comparison between two models across a range of possible trade-offs.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 ROC 曲线在不同决策阈值下映射出真正率和假正率之间的权衡空间一样，许多人绘制精度/召回率曲线，这些曲线在不同决策阈值下映射出精度和召回率之间的权衡空间。这对于在一系列可能的权衡中获取两个模型的整体比较是有用的。
- en: Unlike the AUC ROC, the computed area under the precision/recall curve does
    not have a theoretically grounded statistical meaning, but is often used in practice
    as a quick way to summarize the information nonetheless. In cases of strong levels
    of class imbalance, there is a case to be made that the area under the precision/recall
    curve is a more informative metric.^([3](ch05.xhtml#ch01fn41))
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 与 AUC ROC 不同，计算的精度/召回率曲线下的面积没有理论上的统计含义，但在实践中常被用作概述信息的快速方式。在强烈的类别不平衡情况下，可以认为精度/召回率曲线下的面积是一种更具信息性的度量。^([3](ch05.xhtml#ch01fn41))
- en: Regression metrics
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回归指标
- en: Unlike classification metrics, regression metrics do not rely on the idea of
    a decision threshold. Instead, they look at the raw numerical output that represents
    a model’s prediction, like predicted price for a given skein of yarn, number of
    seconds a user might spend reading a description, or the probability that a given
    picture contains a puppy. They are most often used when the target value itself
    is continuously valued, but have utility in discrete valued label settings like
    click-through prediction as well.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 与分类度量不同，回归度量不依赖于决策阈值的概念。相反，它们查看代表模型预测的原始数值输出，例如给定毛线的预测价格，用户可能花在阅读描述上的秒数，或者给定图片包含小狗的概率。它们通常在目标值本身是连续值时使用最多，但在离散值标签设置中也有用，如点击预测。
- en: Mean squared error and mean absolute error
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 均方误差和平均绝对误差
- en: When comparing predictions from a model to a ground-truth value, the first metric
    we might look at is the difference between our prediction and reality. For example,
    in one case, our model might predict 4.3 stars for an example that had 4 stars
    in reality, and in another case it might predict 4.7 stars for an example that
    had 5 stars in reality. If we were to aggregate those values without thinking
    about it, so we could look at averages over many values, we would run into the
    mild annoyance that in the first example the difference was 0.3 and in the second
    it was –0.3, so our average error would appear to be 0, which feels misleading
    for a model that is clearly imperfect.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当将模型的预测与实际值进行比较时，我们可能首先关注的指标是我们的预测与现实之间的差异。例如，在某种情况下，我们的模型可能预测某个例子实际上有4颗星，但实际上只有4.3颗星，而在另一种情况下，它可能预测某个例子实际上有5颗星，但实际上只有4.7颗星。如果我们不加思考地聚合这些值，这样我们可以查看许多值的平均值，我们将遇到一个轻微的烦恼，在第一个例子中差异是0.3，在第二个例子中是-0.3，因此我们的平均误差看起来是0，这对于一个明显不完美的模型来说可能会误导。
- en: One fix for this is to take the absolute value of each difference—creating a
    metric called *mean absolute error* (*MAE*) to average these values across examples.
    Another fix is to square the errors—raising them to the power of two—to create
    a metric called *mean squared error* (*MSE*). Both metrics have the useful quality
    that a value of 0.0 shows a perfect model. MSE penalizes larger errors much more
    than smaller errors, which can be useful in domains where you do not want to make
    big mistakes. MSE can be less useful if the data contains noise or outlier examples
    that are better ignored, in which case MAE is likely a better metric. It can be
    especially useful to compute both metrics and see if they yield qualitatively
    different results for a comparison between two models, which can provide clues
    into a deeper level of understanding their differences.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个修复方法是取每个差异的绝对值——创建一个称为*平均绝对误差*（*MAE*）的度量来对这些值进行平均。另一个修复方法是平方误差——将它们提升到二次幂——以创建一个称为*均方误差*（*MSE*）的度量。这两个度量具有一个有用的特性，即值为0.0表示一个完美的模型。MSE比较大的误差要比较小的误差惩罚更严厉，这在不希望犯大错误的领域中可能是有用的。如果数据包含噪声或更好地忽略的异常值示例，那么MAE可能是一个更好的度量。特别是计算这两个度量并查看它们是否为两个模型之间的比较提供了质量上不同的结果，这可以提供有关深入理解它们差异的线索。
- en: Log loss
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Log loss
- en: Some people think of *log loss* as an abbreviation for *logistic loss*, because
    it is derived from the *logit* function, which is equivalent to the log of the
    odds ratio between two possible outcomes. A more convenient way to think of it
    might be as *the loss we want to use when we think about our model outputs as
    actual probabilities*. Probabilities are not just numbers restricted to the range
    from 0.0 to 1.0, although this is an important detail. Probabilities also meaningfully
    describe the chance that a given thing will happen to be true.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人将*对数损失*视为*逻辑损失*的缩写，因为它源自*logit*函数，它等同于两种可能结果之间的对数比率。更方便的思考方式可能是将其视为*我们在考虑我们的模型输出为实际概率时想使用的损失*。概率不仅仅是限制在0.0到1.0范围内的数字，尽管这是一个重要的细节。概率还有意义地描述了某个事情发生的可能性是真实的机会。
- en: Log loss is great for probabilities because it will highlight the difference
    between a prediction of 0.99, 0.999, and 0.9999, and will penalize each more confident
    prediction significantly more if it turns out to be incorrect—and the same thing
    happens at the other end of the range for predictions like 0.01, 0.001, and 0.0001\.
    If we do indeed care about using the model outputs as probabilities, this is quite
    helpful. For example, if we are creating a risk-prediction model predicting the
    chance of an accident, there is an enormous difference between an operation being
    99% reliable and 99.99% reliable—and we could end up making very bad pricing decisions
    if our metrics did not highlight these differences. In other settings, we might
    just loosely care how likely a picture is to contain a kitten, and 0.01 and 0.001
    probabilities might both be best interpreted as “basically unlikely,” in which
    case log loss would be a poor choice of final metric. Lastly, it is important
    to note that log loss can give infinite values (which show up as `NaN` values
    and destroy averages) if our models were to predict values of exactly 1.0 or 0.0
    and be in error.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Log loss 在概率方面表现出色，因为它能够凸显出预测为 0.99、0.999 和 0.9999 之间的差异，并且如果预测错误，将更严重地惩罚每个更自信的预测结果——对于预测为
    0.01、0.001 和 0.0001 等极端概率也是如此。如果我们确实关心使用模型输出作为概率，这一点非常有帮助。例如，如果我们正在创建一个风险预测模型，预测事故发生的机率，那么操作的可靠性为
    99% 和 99.99% 之间存在巨大差异——如果我们的指标不能凸显出这些差异，我们可能会做出非常糟糕的定价决策。在其他情境下，我们可能只是大致关心一张图片中包含小猫的可能性，而
    0.01 和 0.001 的概率可能都最好解释为“基本不太可能”，在这种情况下，对于最终指标来说，对数损失将是一个不合适的选择。最后，需要注意的是，如果我们的模型预测的值恰好是
    1.0 或 0.0 并且出错，对数损失可能会产生无限的值（显示为 `NaN` 值并破坏平均值）。
- en: Operationalizing Verification and Evaluation
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将验证和评估操作化
- en: We have just taken a whirlwind tour through the world of evaluating model validity
    and model quality. How do we turn this knowledge into something actionable?
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚快速浏览了评估模型有效性和模型质量的世界。我们如何将这些知识转化为可操作的内容？
- en: Assessing model validity is something that anyone who cares about production
    should know how to do. This is possible, even if you don’t do model evaluation
    daily, with a combination of training, checklists/processes, and automated support
    code for the simpler cases (which itself saves human expertise and judgment for
    more demanding cases).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 评估模型的有效性是任何关心生产的人都应该知道如何做的事情。即使你不是每天进行模型评估，也可以通过结合培训、检查清单/流程以及简单情况下的自动化支持代码（这本身可以为更复杂的情况节省人类的专业知识和判断）来实现这一点。
- en: For questions of model quality evaluations, things are perhaps a little more
    ambiguous. Obviously, it is highly useful for MLOps folks to have a working knowledge
    of the various distributions and metrics that are most critical for assessing
    model quality for our system. An organization may go through a few phases.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型质量评估的问题，情况可能更加模糊。显然，对于 MLOps 人员来说，熟悉对于我们系统中评估模型质量最关键的各种分布和指标是非常有用的。一个组织可能会经历几个阶段。
- en: In the earliest days of model development for an organization, the biggest questions
    are often much more around getting something working rather than about how to
    evaluate it. This can lead to relatively coarse strategies for evaluation. For
    example, the main problems in developing the first version of a yarn store product
    recommendation model are much more likely to be around creating a data pipeline
    and a serving stack, and model developers might not have bandwidth to choose carefully
    between varying classification or ranking metrics. So our first standard evaluation
    might just be AUC ROC for predicting user clicks within a held-out test set.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在组织进行模型开发的最早阶段，最大的问题通常更多地集中在使某事起作用上，而不是如何评估它。这可能导致相对粗糙的评估策略。例如，在开发羊毛店产品推荐模型的第一个版本时，主要问题更有可能是围绕创建数据管道和服务堆栈，而模型开发人员可能没有精力仔细选择各种分类或排名指标之间的差异。因此，我们的第一个标准评估可能只是预测用户在保留测试集中点击的
    AUC ROC。
- en: As the organization develops, a greater understanding develops of the drawbacks
    or blind spots that a given evaluation might have. Typically, this results in
    additional metrics or distributions being developed that help shed light on important
    areas of model performance. For example, we might notice a cold-start problem
    in which new products are not represented in the held-out set, or we might decide
    to look at calibration and bias metrics across a range of slices by country or
    product listing type to understand more about our model’s performance.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 随着组织的发展，对给定评估可能存在的缺陷或盲点的理解逐渐加深。通常情况下，这导致开发额外的度量或分布，帮助揭示模型性能的重要领域。例如，我们可能会注意到一个冷启动问题，即新产品未在留存集中表示，或者我们可能决定查看校准和偏差指标，跨国家或产品列表类型的范围，以更多了解我们模型的性能。
- en: At a later stage, the organization may start to go back and question basic assumptions,
    such as whether the chosen metrics reflect the business goals with sufficient
    veracity. For example, in our imaginary yarn store, we may come to realize that
    optimizing for clicks is not actually equivalent to optimizing for long-term user
    satisfaction. This may require a full reworking of the evaluation stack and careful
    reconsideration of all associated metrics.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在后期阶段，组织可能开始回顾和质疑基本假设，例如所选择的度量标准是否充分反映了业务目标的真实性。例如，在我们想象中的纱线店中，我们可能会意识到优化点击并非等同于优化长期用户满意度。这可能需要全面重构评估堆栈，并仔细重新考虑所有相关度量。
- en: Are these questions within the realm of model developers or MLOps folks? Opinions
    here may vary, but we believe that a healthy organization will encourage multiple
    points of view and rich discussions on these questions.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题是否属于模型开发人员或MLOps人员的领域？对此的看法可能有所不同，但我们认为，一个健康的组织将鼓励多种观点，并对这些问题进行充分讨论。
- en: Conclusion
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This chapter has focused on establishing an initial viewpoint of model validity
    and model quality, both of which are critical to assess before moving a new version
    of a model into production.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点是建立模型有效性和模型质量的初始观点，这两者在将模型新版本投入生产之前都至关重要。
- en: Validity tests help establish that a new version of the model will not break
    our system. These include establishing compatibility with code and formats, and
    making sure that the resource requirements of computation, memory, and latency
    are all within acceptable limits.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 有效性测试有助于确保模型的新版本不会破坏我们的系统。这些测试包括与代码和格式的兼容性，以及确保计算、内存和延迟的资源需求都在可接受的限度内。
- en: Quality tests help give assurance that a new version of the model will improve
    predictive performance. This most often involves assessment of the model’s performance
    on some form of held-out or test data, with the appropriate choice of evaluation
    metric suited for the application task.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 质量测试有助于确保模型的新版本将提高预测性能。这通常涉及评估模型在某种形式的留存或测试数据上的表现，选择适合应用任务的评估指标。
- en: Together, these two forms of testing establish a decent level of trust in a
    model and will be a reasonable starting point for many statically trained ML systems.
    However, systems that deploy ML in a continuous loop will require additional verification,
    as detailed in [Chapter 10](ch10.xhtml#continuous_ml).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种测试形式共同确立了对模型的相当程度的信任，并将是许多静态训练的机器学习系统的合理起点。然而，将机器学习部署在连续循环中的系统将需要额外的验证，详细信息见[第10章](ch10.xhtml#continuous_ml)。
- en: '^([1](ch05.xhtml#ch01fn39-marker)) See [“Gender Shades: Intersectional Accuracy
    Disparities in Commercial Gender Classification”](https://oreil.ly/g37lG) by Joy
    Buolamwini and Timnit Gebru.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch05.xhtml#ch01fn39-marker)) 详见Joy Buolamwini和Timnit Gebru的[“性别阴影：商业性别分类中的交叉准确性差异”](https://oreil.ly/g37lG)。
- en: '^([2](ch05.xhtml#ch01fn40-marker)) See, for example, [“Noise or Signal: The
    Role of Image Backgrounds in Object Recognition”](https://arxiv.org/abs/2006.09994)
    by Kai Xiao. The [What-If Tool](https://oreil.ly/M07ff) is also an excellent example
    of tooling that allows for counterfactual probing.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch05.xhtml#ch01fn40-marker)) 例如，详见Kai Xiao的[“噪声还是信号：图像背景在对象识别中的作用”](https://arxiv.org/abs/2006.09994)。[What-If
    Tool](https://oreil.ly/M07ff)也是一个允许进行反事实探索的优秀工具的例子。
- en: '^([3](ch05.xhtml#ch01fn41-marker)) See, for example: [“Precision-Recall Curve
    Is More Informative Than ROC in Imbalanced Data: Napkin Math & More,”](https://oreil.ly/wZA17)
    by Tam D. Tran-The.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch05.xhtml#ch01fn41-marker)) 详见Tam D. Tran-The的[“在不平衡数据中，精确度-召回曲线比ROC更具信息性：餐巾数学及更多”](https://oreil.ly/wZA17)。
