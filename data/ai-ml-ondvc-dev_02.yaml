- en: Chapter 2\. Introduction to Computer Vision
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章\. 计算机视觉简介
- en: While this book isn’t designed to teach you all of the fundamentals of architecting
    and training machine learning models, I do want to cover some basic scenarios
    so that the book can still work as a standalone. If you want to learn more about
    the model creation process with TensorFlow, I recommend my book, *AI and Machine
    Learning for Coders,*, published by O’Reilly, and if you want to go deeper than
    that, Aurelien Geron’s excellent book [*Hands-on Machine Learning with Scikit-Learn,
    Keras, and TensorFlow*](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)
    (O’Reilly) is a must!
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本书并非旨在教授你所有架构和训练机器学习模型的基础知识，但我确实想涵盖一些基本情景，以使本书仍然可以作为独立学习的一部分。如果你想了解更多有关使用
    TensorFlow 创建模型的过程，请参考我的书籍，*AI and Machine Learning for Coders*，由 O'Reilly 出版，如果你想深入了解，Aurelien
    Geron 的优秀著作 [*Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow*](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)（O'Reilly）是必读的！
- en: In this chapter, we’ll go beyond the very fundamental model you created in [Chapter 1](ch01.html#introduction_to_ai_and_machine_learning)
    and look at two more sophisticated ones, where you will deal with computer vision—namely
    how computers can “see” objects. Similar to the terms “artificial intelligence”
    and “machine learning,” the phrases “computer vision” and “seeing” might lead
    one to misunderstand what is fundamentally going on in the model.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将超越你在[第一章](ch01.html#introduction_to_ai_and_machine_learning)中创建的非常基础的模型，并且探讨两种更复杂的模型，这些模型涉及计算机视觉——即计算机如何“看见”物体。类似于“人工智能”和“机器学习”这些术语，术语“计算机视觉”和“看见”可能会让人误解模型的基本运作方式。
- en: Computer vision is a huge field, and for the purposes of this book and this
    chapter, we’ll focus narrowly on a couple of core scenarios, where we will use
    technology to parse the contents of images, either labeling the primary content
    of an image, or finding items within an image.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉是一个广阔的领域，针对本书和本章的目的，我们将狭义地专注于几个核心情景，在这些情景中，我们将使用技术来解析图像的内容，无论是标记图像的主要内容，还是在图像中找到物品。
- en: It’s not really about “vision” or “seeing,” but more having a structured algorithm
    that allows a computer to parse the pixels of an image. It doesn’t “understand”
    the image any more than it understands the meaning of a sentence when it parses
    the words into individual strings!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不真正涉及“视觉”或“看见”，而更多地是使用结构化算法，允许计算机解析图像的像素。当计算机将单词解析为独立字符串时，它并不“理解”图像的含义！
- en: If we were to try to do that task using the traditional rules-based approach,
    we would end up with many lines of code for even the simplest images. Machine
    learning is a key player here; as you’ll see in this chapter, by using the same
    code pattern we had in [Chapter 1](ch01.html#introduction_to_ai_and_machine_learning),
    but getting a little deeper, we can create models that can parse the contents
    of images...with just a few lines of code. So let’s get started.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们试图使用传统的基于规则的方法来执行这个任务，即使对于最简单的图像，我们也会得到许多行代码。在这里，机器学习是一个关键角色；正如你将在本章中看到的那样，通过使用我们在[第一章](ch01.html#introduction_to_ai_and_machine_learning)中的相同代码模式，但稍微深入一些，我们可以创建能够仅用几行代码解析图像内容的模型。所以让我们开始吧。
- en: Using Neurons for Vision
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用神经元进行视觉处理
- en: In the example you coded in [Chapter 1](ch01.html#introduction_to_ai_and_machine_learning),
    you saw how a neural network could “fit” itself to the desired parameters for
    a linear equation when given some examples of the points on a line in that equation.
    Represented visually, our neural network could look like [Figure 2-1](#fitting_an_x_to_a_y_with_a_neural_netwo).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在你在[第一章](ch01.html#introduction_to_ai_and_machine_learning)编写的示例中，你看到了神经网络如何在给定线上某些点的示例时，“拟合”自身以符合线性方程的期望参数。从视觉上表示，我们的神经网络可以看起来像[图
    2-1](#fitting_an_x_to_a_y_with_a_neural_netwo)。
- en: '![](assets/aiml_0201.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0201.png)'
- en: Figure 2-1\. Fitting an X to a Y with a neural network
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-1\. 使用神经网络将 X 拟合到 Y
- en: This was the simplest possible neural network, where there was only one layer,
    and that layer had only one neuron.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是可能的最简单的神经网络，只有一个层，该层只有一个神经元。
- en: Note
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In fact, I cheated a little bit when creating that example, because neurons
    in a dense layer are fundamentally linear in nature as they learn a weight and
    a bias, so a single one is enough for a linear equation!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，在创建该示例时我稍作作弊，因为密集层中的神经元本质上是线性的，它们学习一个权重和一个偏置，所以一个神经元足以表示线性方程！
- en: 'But when we coded this, recall that we created a `Sequential`, and that `Sequential`
    contained a `Dense`, like this:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 但是当我们编写代码时，请回忆我们创建了一个`Sequential`，并且该`Sequential`包含一个`Dense`，就像这样：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We can use the same pattern of code if we want to have more layers; for example,
    if we want to represent a neural network like that in [Figure 2-2](#a_slightly_more_advanced_neural_network),
    it would be quite easy to do.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望拥有更多层，我们可以使用相同的代码模式；例如，如果我们想要表示像[图 2-2](#a_slightly_more_advanced_neural_network)中的神经网络，这将非常容易实现。
- en: '![](assets/aiml_0202.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0202.png)'
- en: Figure 2-2\. A slightly more advanced neural network
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-2\. 稍微更高级的神经网络
- en: First, let’s consider the elements within the diagram of [Figure 2-2](#a_slightly_more_advanced_neural_network).
    Each vertical arrangement of neurons should be considered a *layer*. [Figure 2-3](#the_layers_within_the_neural_network)
    shows us the layers within this model.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们考虑[图 2-2](#a_slightly_more_advanced_neural_network)中的图表元素。每个垂直排列的神经元都应被视为一个*层*。[图 2-3](#the_layers_within_the_neural_network)向我们展示了该模型中的层。
- en: '![](assets/aiml_0203.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0203.png)'
- en: Figure 2-3\. The layers within the neural network
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-3\. 神经网络中的层
- en: 'To code these, we simply list them within the `Sequential` definition, updating
    our code to look like this:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要编写这些代码，我们只需将它们列在`Sequential`定义中，更新我们的代码如下：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We simply define our layers as a comma-separated list of definitions, and put
    that within the `Sequential`, so as you can see here, there’s a `Dense` with two
    units, followed by a `Dense` with one unit, and we get the architecture shown
    in [Figure 2-3](#the_layers_within_the_neural_network).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需将层定义为逗号分隔的列表，并将其放在`Sequential`中，如你所见，这里有一个包含两个单位的`Dense`，后跟一个包含一个单位的`Dense`，我们得到了[图 2-3](#the_layers_within_the_neural_network)中展示的架构。
- en: But in each of these cases our *output* layer is just a single value. There’s
    one neuron at the output, and given that the neuron can only learn a weight and
    a bias, it’s not very useful for understanding the contents of an image, because
    even the simplest image has too much content to be represented by just a single
    value.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 但在这些情况下，我们的*输出*层只有一个单一值。输出处有一个神经元，鉴于该神经元只能学习一个权重和一个偏差，对于理解图像内容来说并不是很有用，因为即使是最简单的图像也包含了太多内容，无法仅通过单一值来表示。
- en: So what if we have *multiple* neurons on the output? Consider the model in [Figure 2-4](#a_neural_network_with_multiple_outputs).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果我们在输出上有*多个*神经元会怎么样？考虑一下[图 2-4](#a_neural_network_with_multiple_outputs)中的模型。
- en: '![](assets/aiml_0204.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0204.png)'
- en: Figure 2-4\. A neural network with multiple outputs
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-4\. 具有多个输出的神经网络
- en: Now we have multiple inputs and multiple outputs. To design something to recognize
    and parse the contents in an image (as you’ll recall, this is how we’re defining
    computer vision), what if we then assign the output neurons to classes that we
    want to recognize?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有多个输入和多个输出。为了设计能够识别和解析图像内容的东西（你会回忆起，这就是我们定义计算机视觉的方式），如果我们将输出神经元分配给我们想要识别的类别会怎么样呢？
- en: What do we mean by that? Well, much like learning a language, you learn it word
    by word, so in learning how to parse an image, we have to limit the number of
    things that the computer can understand how to “see.” So, for example, if we want
    to start simple and have the computer recognize the difference between a cat and
    a dog, we could create this “vocabulary” of two image types (cats or dogs) and
    assign an output neuron to each. The term *class* is typically used here, not
    to be confused with classes in object-oriented programming.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是什么意思呢？很像学习一门语言，你需要逐字逐句学习，所以在学习如何解析图像时，我们必须限制计算机能够“看到”的事物数量。因此，例如，如果我们想要从简单开始，并让计算机识别猫和狗之间的差异，我们可以创建这样一个包含两种图像类型（猫或狗）的“词汇表”，并为每个类型分配一个输出神经元。这里通常使用术语*类*，请不要与面向对象编程中的类概念混淆。
- en: And because you have a fixed number of “classes” that you want your model to
    recognize, the term that is used is typically *classification* or *image classification*,
    and your model may also be called a *classifier*.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因为你要让模型识别的“类别”数量是固定的，所以通常使用的术语是*分类*或*图像分类*，你的模型也可能被称为*分类器*。
- en: So, to recognize cats or dogs, we could update [Figure 2-4](#a_neural_network_with_multiple_outputs)
    to look like [Figure 2-5](#updating_for_cats_or_dogs).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，要识别猫或狗，我们可以更新[图 2-4](#a_neural_network_with_multiple_outputs)，使其看起来像[图 2-5](#updating_for_cats_or_dogs)。
- en: '![](assets/aiml_0205.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0205.png)'
- en: Figure 2-5\. Updating for cats or dogs
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-5\. 更新以适应猫或狗
- en: So we feed a picture into the neural network, and at the end it has two neurons.
    These will each output a number, and we want that number to represent whether
    the network thinks it “sees” a dog or a cat. This methodology can then be expanded
    beyond this for other classes, so if you want to recognize different animals,
    then you just need additional output neurons that represent their classes. But
    let’s stick with two for the time being, to keep it simple.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将一张图片输入神经网络，最后它有两个神经元。这些神经元将分别输出一个数字，我们希望这个数字表示网络认为它“看到”了一只狗或一只猫。这种方法可以扩展到其他类别，所以如果你想识别不同的动物，那么你只需添加表示它们类别的额外输出神经元。但让我们暂时只保持两个。
- en: So now our problem becomes, how do we represent our data so that the computer
    will start matching our input images to our desired output neurons?
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的问题变成了，如何表示我们的数据，以便计算机开始将我们的输入图像与我们期望的输出神经元匹配？
- en: One method is to use something called *one-hot encoding*. This, at first, looks
    a bit onerous and wasteful, but when you understand the underlying concept and
    how it matches a neural network architecture, it begins to make sense. The idea
    behind this type of encoding is to have an array of values that is the size of
    our number of classes. Each entry in this array is zero, except for the one representing
    the class that you want. In that case you set it to 1.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是使用称为 *one-hot encoding* 的东西。起初，这看起来有点繁琐和浪费，但当你理解其背后的概念以及它如何匹配神经网络架构时，它开始变得合理。这种编码背后的理念是拥有一个大小等于我们类别数的值数组。这个数组中的每个条目都是零，除了表示你想要的类别的那个条目，你将它设置为
    1。
- en: So, for example, if you look at [Figure 2-5](#updating_for_cats_or_dogs), there
    are two output neurons—one for a cat, and one for a dog. Thus, if we want to represent
    “what a cat looks like,” we can represent this as [1,0], and similarly if we want
    to represent “what a dog looks like,” we can encode that to [0,1]. At this point
    you’re probably thinking how wasteful this would look when you’re recognizing
    more classes—say 1,000, where each label on your data will have 999 0s and a single
    1.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，例如，如果你看 [图 2-5](#updating_for_cats_or_dogs)，有两个输出神经元——一个代表猫，一个代表狗。因此，如果我们想要表示“猫的样子”，我们可以表示为
    [1,0]，类似地，如果我们想要表示“狗的样子”，我们可以编码为 [0,1]。此时，你可能会想到当你识别更多类别时，比如 1,000 个类别，每个数据标签都会有
    999 个 0 和一个单独的 1，这看起来是多么浪费。
- en: It’s definitely not efficient, but you’ll only store data like this for your
    images while you are training the model, and you can effectively throw it away
    when you’re done. The output layers of your model will have neurons that match
    this encoding, so when you read them, you’ll know which ones represent which class.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这绝对不是高效的，但在训练模型时，你只会暂时存储这样的数据图像，而在完成后你可以有效地丢弃它们。你的模型的输出层将有与此编码匹配的神经元，因此当你读取它们时，你会知道哪些代表哪个类别。
- en: So, if we update our diagram from [Figure 2-5](#updating_for_cats_or_dogs) with
    these encodings of what a cat looks like and what a dog looks like, then if we
    feed in an image of, say, a cat, we’d want the outputs to look like those encodings,
    as in [Figure 2-6](#using_one_hot_encodings_to_label_a_cat).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果我们更新我们的图表，从 [图 2-5](#updating_for_cats_or_dogs) 这里更新了猫和狗的样式编码，然后如果我们输入一张猫的图像，我们希望输出看起来像这些编码，就像
    [图 2-6](#using_one_hot_encodings_to_label_a_cat)。
- en: '![](assets/aiml_0206.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0206.png)'
- en: Figure 2-6\. Using one-hot encodings to label a cat
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-6\. 使用 one-hot 编码标记一只猫
- en: Now the neural network is behaving the way that we’d want it to. We feed in
    the image of a cat, and the output neurons respond with the encoding [1,0] indicating
    that it “sees” a cat. This gives us the basis of the data representation that
    can be used to train a network. So, if we have a bunch of images of cats and dogs,
    for example, and we label these images accordingly, then, over time, it’s possible
    that a neural network can “fit” itself to these input contents and these labels,
    so that future ones will give the same output.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在神经网络的行为符合我们的期望。我们输入一张猫的图片，输出神经元响应编码 [1,0] 表示它“看到”了一只猫。这为我们提供了可以用来训练网络的数据表示的基础。因此，例如，如果我们有一堆猫和狗的图像，并相应地标记这些图像，那么随着时间的推移，神经网络可能会“适应”这些输入内容和这些标签，使得未来的图像输出相同。
- en: In fact, what will happen is that the output neurons, instead of giving a 1
    or a 0, will give a value between 0 and 1\. This also just happens to be a *probability*
    value. So if you train a neural network on images using one-hot-encoded labels,
    and output one neuron per class, you’ll end up having a model that parses an image
    and returns a list of probabilities of what it can see, something like [Figure 2-7](#parsing_the_contents_of_an_image).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，输出神经元会输出一个介于0和1之间的值，而这也恰好是一个*概率*值。因此，如果你用一个独热编码标签训练图像的神经网络，并输出每个类别一个神经元，你最终会得到一个能解析图像并返回它所能看到的东西概率列表的模型，就像[图 2-7](#parsing_the_contents_of_an_image)一样。
- en: '![](assets/aiml_0207.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0207.png)'
- en: Figure 2-7\. Parsing the contents of an image
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-7\. 解析图像内容
- en: Here, you can see that the model determined that there was a 98.82% chance it
    was looking at a banana, with smaller chances that it was looking at a Granny
    Smith apple or a fig. And while it’s obvious that this is a banana, when this
    app looked at the image, it was extracting features from the image, and some of
    those features may be present in an apple—such as the skin texture, or maybe the
    color.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到模型确定它正在看一只香蕉的概率为98.82%，而看到格兰尼史密斯苹果或无花果的概率较小。虽然很明显这是一根香蕉，但当这款应用看图时，它正在从图像中提取特征，其中一些特征可能也存在于苹果中，比如皮肤质地或颜色。
- en: So, as you can imagine, if you want to train a model to see, you’ll need lots
    of examples of images, and these need to be labelled according to their classes.
    Thankfully there are some basic datasets that limit the scope to make it easy
    to learn, and we’ll look at building a classifier from scratch for these next.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，你可以想象，如果你想训练一个能看的模型，你需要大量的图像示例，并且这些图像需要按类别标记。幸运的是，有一些基本数据集限定了范围，使得学习变得简单，接下来我们将从头开始构建一个分类器。 '
- en: 'Your First Classifier: Recognizing Clothing Items'
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你的第一个分类器：识别衣物
- en: For our first example, let’s consider what it takes to recognize items of clothing
    in an image. Consider, for example, the items in [Figure 2-8](#examples_of_clothing).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 作为我们的第一个例子，让我们考虑如何在图像中识别衣物。例如，考虑[图 2-8](#examples_of_clothing)中的物品。
- en: '![](assets/aiml_0208.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0208.png)'
- en: Figure 2-8\. Examples of clothing
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-8\. 衣物示例
- en: There are a number of different clothing items here, and you can recognize them.
    You understand what a shirt is, or a coat, or a dress. But how would you explain
    this to somebody who has never seen clothing? How about a shoe? There are two
    shoes in this image, but how would you describe that to somebody? This is another
    area where the rules-based programming we spoke about in [Chapter 1](ch01.html#introduction_to_ai_and_machine_learning)
    can fall down. Sometimes it’s just infeasible to describe something with rules.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了多种不同的衣物，你可以认出它们。你知道什么是衬衫，或者外套，或者裙子。但是如果要向从未见过衣物的人解释，你该如何描述鞋子？这张图里有两只鞋，但你要如何描述给别人听？这也是我们之前在[第一章](ch01.html#introduction_to_ai_and_machine_learning)提到的基于规则的编程可能失败的另一个领域。有时候用规则来描述某些事物是不可行的。
- en: Of course, computer vision is no exception. But consider how you learned to
    recognize all these items—by seeing lots of different examples and gaining experience
    with how they’re used. Can we do the same with a computer? The answer is yes,
    but with limitations. Let’s take a look at a first example of how to teach a computer
    to recognize items of clothing, using a well-known dataset called Fashion MNIST.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，计算机视觉也不例外。但是想一想你是如何学会认识所有这些物品的——通过看大量不同的例子，并获得它们使用方式的经验。我们能否用同样的方式教会计算机？答案是肯定的，但有限制。让我们来看一个第一个示例，如何教计算机识别衣物，使用一个名为时尚
    MNIST 的知名数据集。
- en: 'The Data: Fashion MNIST'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据：时尚 MNIST
- en: One of the foundational datasets for learning and benchmarking algorithms is
    the Modified National Institute of Standards and Technology (MNIST) database by
    Yann LeCun, Corinna Cortes, and Christopher Burges. This dataset is comprised
    of images of 70,000 handwritten digits from 0 to 9\. The images are 28 × 28 grayscale.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 用于学习和基准测试算法的基础数据集之一是由 Yann LeCun、Corinna Cortes 和 Christopher Burges 创建的 Modified
    National Institute of Standards and Technology（MNIST）数据库。该数据集包含了70,000张0到9的手写数字图像。这些图像是28
    × 28的灰度图像。
- en: '[Fashion MNIST](https://oreil.ly/GmmUB) is designed to be a drop-in replacement
    for MNIST that has the same number of records, the same image dimensions, and
    the same number of classes—so, instead of images of the digits 0 through 9, Fashion
    MNIST contains images of 10 different types of clothing.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[时尚 MNIST](https://oreil.ly/GmmUB)旨在成为 MNIST 的替代品，记录数、图像尺寸和类别数都与其相同——因此，与数字
    0 到 9 的图像不同，时尚 MNIST 包含 10 种不同类型的服装图像。'
- en: You can see an example of the contents of the dataset in [Figure 2-9](#exploring_the_fashion_mnist_dataset).
    Here, three lines are dedicated to each clothing item type.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 [图 2-9](#exploring_the_fashion_mnist_dataset) 中看到数据集内容的示例。这里，每种服装类型都有三行。
- en: '![](assets/aiml_0209.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0209.png)'
- en: Figure 2-9\. Exploring the Fashion MNIST dataset
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-9\. 探索时尚 MNIST 数据集
- en: It has a nice variety of clothing, including shirts, trousers, dresses, and
    lots of types of shoes! As you may notice, it’s grayscale, so each picture consists
    of a certain number of pixels with values between 0 and 255\. This makes the dataset
    simpler to manage.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 它包含了各种服装，包括衬衫、裤子、连衣裙和各种类型的鞋子！正如您可能注意到的那样，它是灰度的，因此每张图片由一定数量的像素组成，其值在 0 到 255
    之间。这使得数据集更容易管理。
- en: You can see a close-up of a particular image from the dataset in [Figure 2-10](#closeup_of_an_image_in_the_fashion_mnis).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 [图 2-10](#closeup_of_an_image_in_the_fashion_mnis) 中看到数据集中特定图像的放大视图。
- en: '![](assets/aiml_0210.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0210.png)'
- en: Figure 2-10\. Close-up of an image in the Fashion MNIST dataset
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-10\. 时尚 MNIST 数据集中图像的放大视图
- en: Like any image, it’s a rectangular grid of pixels. In this case the grid size
    is 28 × 28, and each pixel is simply a value between 0 and 255, as mentioned previously.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何图像一样，它是一个矩形像素网格。在这种情况下，网格尺寸为 28 × 28，每个像素只是介于 0 到 255 之间的值，如前所述。
- en: A Model Architecture to Parse Fashion MNIST
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于解析时尚 MNIST 的模型架构
- en: Let’s now take a look at how you can use these pixel values with the architecture
    for computer vision that we saw previously.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何将这些像素值与先前看到的计算机视觉架构结合使用。
- en: You can see a representation of this in [Figure 2-11](#a_neural_network_architecture_to_recogn).
    Note that there are 10 classes of fashion in Fashion MNIST, so we’ll need an output
    layer of 10 neurons. Note that to make it easier to fit on the page, I’ve rotated
    the architecture so that the output layer of 10 neurons is at the bottom, instead
    of on the right.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 [图 2-11](#a_neural_network_architecture_to_recogn) 中看到这一表示。请注意，时尚 MNIST
    中有 10 类服装，因此我们需要一个包含 10 个神经元的输出层。请注意，为了更容易适应页面，我已经旋转了架构，使得 10 个神经元的输出层位于底部，而不是右侧。
- en: The number of neurons “above” these, currently set to 20 to fit onto the page,
    may change as you write your code. But the idea is that we will feed the pixels
    of the image into these neurons.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: “上面”的神经元数量，当前设置为 20 以适应页面，可能会随着您编写代码而改变。但是，我们的想法是将图像的像素馈送到这些神经元中。
- en: '![](assets/aiml_0211.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0211.png)'
- en: Figure 2-11\. A neural network architecture to recognize fashion images
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-11\. 用于识别时尚图像的神经网络架构
- en: Given that our image is rectangular, and 28 × 28 pixels in size, we’ll need
    to represent it in the same way the neurons in a layer are represented, i.e.,
    a one-dimensional array, so we can follow a process called “flattening” the image,
    so that it becomes, instead of 28 × 28, a 784 × 1 array. It then has a similar
    “shape” to the input neurons, so we can begin to feed it in. See [Figure 2-12](#training_a_neural_network_with_fashion).
    Note that because the ankle boot image from [Figure 2-10](#closeup_of_an_image_in_the_fashion_mnis)
    is class “9” in Fashion MNIST, we’ll also train by saying that’s the neuron that
    should light up in one-hot encoding. We start counting from 0, so the neuron for
    class 9 is the 10th, or rightmost one, in [Figure 2-12](#training_a_neural_network_with_fashion).
    The reason for the term “Dense” being given to this layer type should now be more
    visually apparent!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们的图像是矩形的，尺寸为 28 × 28 像素，我们需要以与图层中神经元表示方式相同的方式来表示它，即一维数组，因此我们可以遵循所谓的“展平”图像的过程，这样它就变成了
    784 × 1 的数组。然后它与输入神经元具有类似的“形状”，因此我们可以开始将其馈送进去。参见 [图 2-12](#training_a_neural_network_with_fashion)。请注意，由于来自
    [图 2-10](#closeup_of_an_image_in_the_fashion_mnis) 的踝靴图像在时尚 MNIST 中是类别“9”，我们还将通过说这是应该点亮的神经元来进行训练。我们从
    0 开始计数，因此类别 9 的神经元是 [图 2-12](#training_a_neural_network_with_fashion) 中最右侧的第 10
    个。现在，“Dense”这个层类型的术语之所以得到这个名称，应该更加直观了！
- en: '![](assets/aiml_0212.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0212.png)'
- en: Figure 2-12\. Training a neural network with Fashion MNIST
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-12\. 使用时尚 MNIST 训练神经网络
- en: Given that there are 60,000 images in the training set, the training loop that
    we mentioned in [Chapter 1](ch01.html#introduction_to_ai_and_machine_learning)
    will take place. First, every neuron in the network will be randomly initialized.
    Then, for each of the 60,000 labeled images, a classification will be made. The
    accuracy and loss on this classification will help an optimizer tweak the values
    of the neurons, and we’ll try again and so on. Over time, the internal weights
    and biases in the neurons will be tuned to match the training data. Let’s now
    explore this in code.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于训练集中有 60,000 张图像，在 [第 1 章](ch01.html#introduction_to_ai_and_machine_learning)
    中提到的训练循环将会发生。首先，网络中的每个神经元将被随机初始化。然后，对于这 60,000 个带标签的图像中的每一个，将进行分类。这次分类的准确性和损失将帮助优化器调整神经元的值，然后我们再次尝试，依此类推。随着时间的推移，神经元内部的权重和偏置将被调整以匹配训练数据。现在让我们在代码中来探索这一点。
- en: Coding the Fashion MNIST Model
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码时尚 MNIST 模型
- en: 'The model architecture described earlier is shown here:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 先前描述的模型架构在这里展示：
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: It’s really that simple! There are a few new concepts here, so let’s explore
    them.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这么简单！这里有一些新概念，让我们来探索一下。
- en: First of all, we can see that we’re using a `Sequential`. Recall that this allows
    us to define the layers in our network using a list. Each element in that list
    defines a layer type (in this case a `Flatten` followed by two `Dense` layers),
    as well as details about the layer, such as the number of neurons and the activation
    function.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以看到我们使用了 `Sequential`。回想一下，这允许我们使用列表定义网络中的层。列表中的每个元素定义了一个层类型（在本例中是一个 `Flatten`，后跟两个
    `Dense` 层），以及关于层的细节，如神经元数量和激活函数。
- en: 'The first layer is:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层是：
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This demonstrates part of the power of layers—you don’t just define the model
    architecture using them, you can also have functionality encapsulated within the
    layer. So here your input shape of 28 × 28 gets flattened into the 784 × 1 that
    you need to feed into the neural network.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这展示了层的部分功能——你不仅可以使用它们定义模型架构，还可以将功能封装在层内。因此，这里你的输入形状从 28 × 28 扁平化为 784 × 1，以便输入到神经网络中。
- en: After that you have the two layers that we showed in the diagram in [Figure 2-12](#training_a_neural_network_with_fashion),
    a `Dense` layer with 20 neurons, and another `Dense` layer with 10 neurons.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，你有两个层，正如我们在 [图 2-12](#training_a_neural_network_with_fashion) 中展示的，一个具有
    20 个神经元的 `Dense` 层，以及另一个具有 10 个神经元的 `Dense` 层。
- en: But there’s something new here too—the `activation` parameter. This defines
    an activation function, which executes on a layer at the end of the processing
    of that layer. Activation functions can help the network recognize more complex
    patterns and also change the behavior of information as it flows from layer to
    layer, helping the network to learn better and faster.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 但这里也有一些新东西——`activation` 参数。这定义了一个激活函数，它在处理层的末尾执行。激活函数可以帮助网络识别更复杂的模式，并改变信息在层与层之间流动的行为，有助于网络更好地学习和更快地学习。
- en: They’re optional, but they’re very useful and often recommended to use.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 它们是可选的，但它们非常有用，通常建议使用。
- en: 'On the 20-neuron layer, the activation function is `tf.nn.relu`, where *relu*
    stands for “rectified linear unit.” This is a pretty fancy term that effectively
    equates to—if the value is less than zero, set it to zero; otherwise, keep it
    as it is. Kind of like:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在 20 个神经元的层上，激活函数是 `tf.nn.relu`，其中 *relu* 代表“修正线性单元”。这是一个相当花哨的术语，实际上等同于——如果值小于零，则将其设置为零；否则保持不变。有点像：
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: How this helps is that, if any of the neurons in the layer return a *negative*
    value, that could cancel out the *positive* value on another neuron, effectively
    ignoring what it learned. So, instead of doing lots of checking on every neuron
    at every iteration, we simply have an activation function on the layer do it for
    us.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 它如何帮助的是，如果任何一个层中的神经元返回一个 *负* 值，那可能会取消另一个神经元的 *正* 值，从而忽略它所学到的内容。因此，我们在每次迭代中不需要对每个神经元进行大量检查，而是在层上简单地使用激活函数来执行。
- en: Similarly the output layer has an activation called `softmax`. The idea here
    is that our output layer has 10 neurons in it. Ideally they would all contain
    zero, except for one of them, which would have the value 1\. That one would be
    our class. In reality this rarely happens, and each neuron will have a value in
    it. The biggest one will be our best candidate for the classification of the input
    image. However, in order to report a *probability*, we would want the values of
    each of the neurons to add up to 1, and their values scaled appropriately. Instead
    of writing the code to handle this, we can simply apply a `softmax` activation
    function to the layer, and it will do that for us!
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，输出层具有一个称为`softmax`的激活函数。这里的想法是，我们的输出层有10个神经元。理想情况下，它们应该全部为零，除了其中一个神经元为1，表示我们的类别。实际情况很少发生，每个神经元都会有一个值。最大的那个值将是我们输入图像分类的最佳候选项。然而，为了报告一个*概率*，我们希望每个神经元的值加起来为1，并且它们的值应该经过适当的缩放。我们可以简单地在层上应用`softmax`激活函数来处理这些，而不需要编写处理代码！
- en: 'This is just the model architecture. Now let’s explore the full code, including
    getting the data, compiling the model, and then performing the training:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是模型架构。现在让我们来探索完整的代码，包括获取数据、编译模型，然后进行训练：
- en: '[PRE5]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Remember earlier when I mentioned that traditional coding to parse the contents
    of images, even simple ones like Fashion MNIST, could have many thousands of lines
    of code to handle it, but machine learning can do it in just a few lines? Well,
    here they are!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得之前我提到过传统编码解析图像内容的方式吗，即使是像Fashion MNIST这样简单的图像，也可能需要成千上万行代码来处理，但机器学习只需要几行代码就可以完成？好了，这里它们就是！
- en: 'First is getting the data. The Fashion MNIST dataset is built into TensorFlow,
    so we can get it easily like this:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要获取数据。Fashion MNIST数据集已经集成在TensorFlow中，所以我们可以这样轻松地获取它：
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: After executing this line of code, `training_images` will have our 60,000 training
    images and `training_labels` will have their associated labels. Additionally `val_images`
    and `val_labels` will have 10,000 images and their associated labels. We’ll hold
    them back and not use them when training, so we can have a set of data that the
    neural network hasn’t previously “seen” when we explore its efficacy.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 执行完这行代码后，`training_images`将包含我们的60,000个训练图像，`training_labels`将包含它们对应的标签。此外，`val_images`和`val_labels`将包含10,000个图像及其对应的标签。在训练时，我们不会使用它们，这样我们就可以得到一组神经网络以前没有“见过”的数据，从而探索它的有效性。
- en: 'Next are these lines:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是这些行：
- en: '[PRE7]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Using NumPy in Python is powerful in that if you divide an array by a value,
    you’ll divide every item in that array by the value. But why are we dividing by
    255?
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中使用NumPy非常强大，如果你将一个数组除以一个值，那么数组中的每个项目都将被该值除以。但是为什么我们要除以255呢？
- en: This process is called *normalization*, which again is quite a fancy term that
    means to set a value to something between 0 and 1\. Our pixels are between 0 and
    255, so by dividing by 255, we’ll normalize them. Why normalize? The math within
    a `Dense` works best when values are between 0 and 1, so that errors don’t massively
    inflate when they are greater. You might remember for the y = 2x − 1 example in
    [Chapter 1](ch01.html#introduction_to_ai_and_machine_learning) we didn’t normalize.
    That was a trivial example that didn’t need it, but for the most part you will
    need to normalize your data prior to feeding it into a neural network!
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程称为*归一化*，这是一个相当花哨的术语，意味着将一个值设置为介于0和1之间的某个值。我们的像素值介于0和255之间，因此通过除以255，我们将它们归一化。为什么要归一化呢？在`Dense`中的数学运算最好在值介于0和1之间时进行，这样当值较大时错误不会显著增加。你可能还记得在[第1章](ch01.html#introduction_to_ai_and_machine_learning)中的y
    = 2x − 1的例子中我们没有进行归一化。那是一个不需要归一化的简单例子，但大多数情况下，在将数据馈送到神经网络之前，你需要对数据进行归一化！
- en: 'Then, after defining the model architecture, you compile your model, specifying
    the loss function and the optimizer:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在定义模型架构之后，你编译模型，指定损失函数和优化器：
- en: '[PRE8]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: These are different from the `sgd` and `mean_squared_error` you used in [Chapter 1](ch01.html#introduction_to_ai_and_machine_learning).
    TensorFlow has a library of these functions and you can generally pick from these
    to experiment and see what works best for your model. There are some constraints
    here, most notably in the `loss` function. Given that this model is going to have
    more than one output neuron, and these neurons are giving us classes or *categories*
    of output, we will want to use a *categorical* loss function to measure them effectively,
    and for this I chose `sparse_categorical_crossentropy`. Understanding how each
    of these work is beyond the scope of this book, but it’s good to experiment with
    the different loss functions and optimizers that you can find on [TensorFlow.org](http://TensorFlow.org).
    For the optimizer, I chose `adam`, which is an enhanced version of `sgd` that
    internally tunes itself for better performance.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这些与您在[第1章](ch01.html#introduction_to_ai_and_machine_learning)中使用的`sgd`和`mean_squared_error`不同。
    TensorFlow有一个这些函数的库，通常可以从中选择进行实验，并看看哪种对您的模型效果最好。 这里有一些约束条件，最显著的是`loss`函数。 鉴于该模型将具有多个输出神经元，并且这些神经元为我们提供输出的类别或*类别*，我们将希望使用*分类*损失函数有效地对它们进行测量，因此我选择了`sparse_categorical_crossentropy`。
    理解这些函数的工作原理超出了本书的范围，但是尝试不同的损失函数和优化器是很好的。 对于优化器，我选择了`adam`，这是`sgd`的增强版本，内部调整自身以获得更好的性能。
- en: Note also that I use another parameter—`metrics=['accuracy']`—which asks TensorFlow
    to report on accuracy while training. As we are doing categorical model training,
    where we want the classifier to tell us what it thinks it sees, we can use basic
    accuracy, i.e., how many of the training images it got “right” with its guess,
    and report on that along with the loss value. By specifying metrics at compile
    time, TensorFlow will report this back to us.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，我使用了另一个参数—`metrics=['accuracy']`—这要求TensorFlow在训练时报告准确性。 因为我们正在进行分类模型训练，我们希望分类器告诉我们它认为它看到了什么，因此我们可以使用基本准确性，即它在猜测中有多少训练图像“正确”，并在此基础上报告损失值。
    通过在编译时指定指标，TensorFlow将向我们报告这些。
- en: 'Finally we can fit the training values to the training data with:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将训练值与训练数据拟合：
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: I set it to do the whole training loop (make a guess, evaluate and measure the
    loss, optimize, repeat) 20 times by setting `epochs` to 20, and asking it to fit
    the training images to the training labels.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我将其设置为通过将`epochs`设置为20来进行整个训练循环（进行猜测，评估和测量损失，优化，重复），并要求将训练图像与训练标签拟合。
- en: 'As it trains, you’ll see output like this:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，您将看到如下输出：
- en: '[PRE10]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Note the accuracy: after only three loops, it’s already at 94.5% accuracy on
    the training set! I trained this using Google Colab, and we could see that each
    loop, despite processing 60,000 images, only took two seconds. Finally, you see
    the values 1875/1875, and you might be wondering what they are? When training
    you don’t have to process one image at a time, and TensorFlow supports batching
    to make things faster. Fashion MNIST defaults to each batch having 32 images,
    so it trains with one batch of images at a time. This gives you 1875 batches of
    images to make up 60,000 images (i.e., 60,000 divided by 32 = 1875).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 注意准确率：仅经过三个循环，训练集的准确率已经达到94.5％！ 我使用Google Colab进行了训练，我们可以看到每个循环，尽管处理了60,000张图像，但仅需两秒。
    最后，您将看到值为1875/1875，并且您可能想知道它们是什么？ 在训练过程中，您不必一次处理一张图像，TensorFlow支持批处理以加快速度。 Fashion
    MNIST默认每个批次包含32张图像，因此每次一批次地进行训练。 这为您提供了1875批次的图像，以组成60,000张图像（即60,000除以32 = 1875）。
- en: 'By the time you reach epoch 20, you’ll see that the accuracy is now over 97%:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当您达到第20个epoch时，您将看到准确率已经超过97％：
- en: '[PRE11]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: So, with just a few lines of code, and less than a minute of training, you now
    have a model that can recognize Fashion MNIST images with greater than 97% accuracy.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，仅需几行代码和不到一分钟的训练，您现在拥有了一个可以以超过97％的准确率识别Fashion MNIST图像的模型。
- en: 'Remember earlier that you also held back 10,000 images as the validation dataset?
    You can pass them to the model now to see how the model parses them. Note that
    it has never seen these images before, so it’s a great way to test if your model
    is really accurate—if it can classify images that it hasn’t previously seen with
    a high level of accuracy. You can do this by calling `model.evaluate`, passing
    it the set of images and labels:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得之前您还留了10,000张图片作为验证数据集吗？现在可以将它们传递给模型，看看模型如何解析它们。请注意，它以前从未见过这些图片，所以这是测试您的模型真实准确性的一个好方法——如果它能够高准确度地分类之前未见过的图片。您可以通过调用`model.evaluate`来实现这一点，将图片和标签传递给它：
- en: '[PRE12]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: From this you can see that your model is 96% accurate on data it hadn’t previously
    seen, telling you that you have a really good model for predicting fashion data.
    A concept in machine learning called *overfitting* is what you are looking to
    avoid here. Overfitting is what happens when your model becomes really good at
    understanding its training data, but not so good at understanding other data.
    This would be indicated by a large difference between the training accuracy and
    the validation accuracy. Think of it as if you were teaching an intelligent being
    what shoes were, but only ever showed it high-heeled shoes. It would then “think”
    that all shoes are high-heeled, and if you subsequently showed it a pair of sneakers,
    it would be overfit to high-heeled shoes. You want to avoid this practice in neural
    networks too, but we can see that we’re doing well here with only a small difference
    between training and validation accuracy!
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里您可以看到，您的模型在之前未见过的数据上准确率达到了96%，这告诉您您有一个非常好的模型来预测时尚数据。机器学习中的一个概念叫做*过拟合*，您希望在这里避免它。过拟合是指当您的模型在理解其训练数据方面表现非常出色时，在理解其他数据方面表现不佳。这将通过训练准确率和验证准确率之间的较大差异来指示。可以将其类比为如果您教会一个智能体高跟鞋是什么，但只向它展示过高跟鞋。那么它会“认为”所有鞋子都是高跟鞋，如果随后向它展示一双运动鞋，它就会对高跟鞋过拟合。在神经网络中也要避免这种实践，但我们可以看到我们在这里的训练和验证准确率之间只有很小的差异！
- en: This shows you how you can create a simple model to learn how to “see” contents
    of an image, but it relies on very simple, monochrome images where the data is
    the only thing in the picture, centered within the frame of the image. The models
    to recognize real-world images will need to be far more complicated than this
    one, but it’s possible to build them using something called a “convolutional neural
    network.” Going into how they work in detail is beyond the scope of this book,
    but check out the other books I mentioned at the top of this chapter for more
    in-depth coverage.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这向您展示了如何创建一个简单的模型来学习如何“看到”图像的内容，但它依赖于非常简单的单色图像，其中数据是图片中唯一的东西，并位于图像框架的中心。识别真实世界图像的模型需要比这个模型复杂得多，但可以使用一种称为“卷积神经网络”的东西来构建它们。详细介绍它们的工作原理超出了本书的范围，但请查阅本章开头提到的其他书籍，以获取更详尽的覆盖范围。
- en: One thing you *can* do though, without going further in depth into model architecture
    types, is something called t*ransfer learning*, and we’ll explore that next.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 但有一件事情*确实*可以做，而不必进一步深入了解模型架构类型，那就是所谓的*迁移学习*，我们将在接下来探讨它。
- en: Transfer Learning for Computer Vision
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机视觉的迁移学习
- en: Consider the architecture for Fashion MNIST discussed previously in [Figure 2-12](#training_a_neural_network_with_fashion).
    It’s already looking quite sophisticated and complicated despite the relative
    simplicity of the data it’s designed to classify. Then, extend this to larger
    images, more classes, color, and other levels of sophistication. You’ll end up
    with really complex architectures in order to handle them. For example, [Table 2-1](#mobilenet_description)
    is a chart describing the layers of an architecture called *MobileNet* which,
    as its name suggests, is designed to be mobile-friendly, low in battery consumption
    while high in performance.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑之前讨论过的Fashion MNIST的架构，如[图 2-12](#training_a_neural_network_with_fashion)所示。尽管其设计用于分类的数据相对简单，但它已经看起来相当复杂和复杂。然后，将其扩展到更大的图像、更多的类别、颜色和其他复杂程度。您将需要设计非常复杂的架构来处理它们。例如，[表 2-1](#mobilenet_description)描述了一种被称为*MobileNet*的架构的层，正如其名称所示，它旨在对移动设备友好，低耗电高性能。
- en: Table 2-1\. MobileNet description
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2-1\. MobileNet 描述
- en: '| Input | Operator | *t* | *c* | *n* | *s* |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 输入 | 运算符 | *t* | *c* | *n* | *s* |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 224² × 3 | conv2d | – | 32 | 1 | 2 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 224² × 3 | 卷积层 | – | 32 | 1 | 2 |'
- en: '| 112² × 32 | bottleneck | 1 | 16 | 1 | 1 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 112² × 32 | 瓶颈层 | 1 | 16 | 1 | 1 |'
- en: '| 112² × 16 | bottleneck | 6 | 24 | 2 | 2 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 112² × 16 | 瓶颈层 | 6 | 24 | 2 | 2 |'
- en: '| 56² × 24 | bottleneck | 6 | 32 | 3 | 2 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 56² × 24 | 瓶颈层 | 6 | 32 | 3 | 2 |'
- en: '| 28² × 32 | bottleneck | 6 | 64 | 4 | 2 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 28² × 32 | 瓶颈层 | 6 | 64 | 4 | 2 |'
- en: '| 14² × 64 | bottleneck | 6 | 96 | 3 | 1 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 14² × 64 | 瓶颈层 | 6 | 96 | 3 | 1 |'
- en: '| 14² × 96 | bottleneck | 6 | 160 | 3 | 2 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 14² × 96 | 瓶颈层 | 6 | 160 | 3 | 2 |'
- en: '| 7² × 160 | bottleneck | 6 | 320 | 1 | 1 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 7² × 160 | 瓶颈层 | 6 | 320 | 1 | 1 |'
- en: '| 7² × 320 | conv2d 1x1 | – | 1280 | 1 | 1 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 7² × 320 | 1x1卷积 | – | 1280 | 1 | 1 |'
- en: '| 7² × 1280 | avgpool 7x7 | – | – | 1 | – |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 7² × 1280 | avgpool 7x7 | – | – | 1 | – |'
- en: '| 1 × 1 × 1280 | conv2d 1x1 | – | k | – |   |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 1 × 1 × 1280 | 1x1卷积 | – | k | – |   |'
- en: Here you can see that there are many layers, mostly of type “bottleneck” (which
    use convolutions), that take a color image of size 224 × 224 × 3 (where the image
    is 224 × 224 pixels, and three bytes are required for color), and break it down
    into 1,280 values that are called “feature vectors.” These vectors can then be
    fed into a classifier for the 1,000 images that make up the MobileNet model. It’s
    designed to work with a version of the [ImageNet database](https://oreil.ly/qnBpY)
    that was created for the ImageNet Large Scale Visual Recognition Challenge (ILSVRC),
    which used 1,000 classes of image.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您可以看到有许多层，主要是“瓶颈”类型（使用卷积），它们接收尺寸为224 × 224 × 3的彩色图像（图像为224 × 224像素，需要三个字节的颜色），并将其分解为称为“特征向量”的1,280个值。然后，这些向量可以馈送到一个分类器中，用于MobileNet模型的1,000张图像。它被设计用于与为ImageNet大规模视觉识别挑战（ILSVRC）创建的一个版本的[ImageNet数据库](https://oreil.ly/qnBpY)一起工作，该数据库使用1,000类图像。
- en: Designing and training a model like this is a very complex undertaking.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 设计和训练这样的模型是一个非常复杂的任务。
- en: But *reusing* the model and what it learned is possible, even if you want to
    use it to recognize images *other* than the 1,000 it was trained to recognize.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，即使您希望将其用于识别*不同*于其训练识别的1,000种图像，*重复使用*模型和它学到的内容也是可能的。
- en: 'The logic goes like this: if a model like MobileNet, trained on hundreds of
    thousands of images to recognize thousands of classes, works very well, then it
    has become very effective at *generally* spotting what’s in an image. If we take
    the values that it learned in its internal parameters and apply them to a *different*
    set of images, they will likely work very well given their general nature.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑是这样的：如果像MobileNet这样的模型，经过数十万张图像的训练来识别成千上万个类别，效果非常好，那么它已经非常有效地*通常*能够识别图像中的内容。如果我们采用它在内部参数中学到的值，并将其应用于*不同*的图像集合，由于其通用性，它们很可能会表现得非常好。
- en: So, for example, if we go back to [Table 2-1](#mobilenet_description) and say
    that we want to create a model that recognizes only three different classes of
    image, instead of the 1,000 that it already recognizes, then we could use everything
    that was learned by MobileNet to get the 1,280 feature vectors and feed them into
    our own output of only three neurons for our three classes.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，例如，如果我们回到[表 2-1](#mobilenet_description)，并且说我们想创建一个仅能识别三种不同图像类别的模型，而不是已经识别的1,000个类别，那么我们可以使用MobileNet学到的所有内容来获得1,280个特征向量，并将它们馈送到我们自己的仅含三个神经元的输出层，用于我们的三类。
- en: Fortunately, this is very simple to do because of the existence of *TensorFlow
    Hub*, a repository for models and model architectures that are pretrained.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，由于*TensorFlow Hub*的存在，这非常容易实现，它是一个预训练模型和模型架构的存储库。
- en: 'You can include TensorFlow Hub in your code by importing it:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过导入TensorFlow Hub来在您的代码中包含它：
- en: '[PRE13]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'So, for example, if I want to use MobileNet v2, I can use code like this:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，例如，如果我想使用MobileNet v2，我可以使用这样的代码：
- en: '[PRE14]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here I define that I want to use MobileNet and take its feature vectors. There’s
    lots of different types of architectures for MobileNet tuned in different ways
    in TensorFlow Hub, leading to numbers like `035_224` in the URL. I won’t go into
    detail on them here, but the 224 represents the image dimensions that we want
    to use. Look back to [Table 2-1](#mobilenet_description) and you’ll see that the
    MobileNet image was 224 × 224.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我定义我想使用MobileNet并获取其特征向量。在TensorFlow Hub中，有许多不同类型的MobileNet架构，以不同的方式进行调整，导致URL中的`035_224`之类的数字。我在这里不会详细介绍它们，但224表示我们要使用的图像尺寸。回到[表 2-1](#mobilenet_description)，您会看到MobileNet图像为224
    × 224。
- en: 'The important thing is that I want to load a model that is already trained
    from Hub. It outputs feature vectors that I can classify, so my model will look
    like this:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，我想加载一个已经从Hub训练好的模型。它输出特征向量，我可以对其进行分类，所以我的模型会像这样：
- en: '[PRE15]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note the `trainable=False` setting in the first line. This means that we will
    reuse the model but will not edit it in any way—just use what it already learned.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意第一行中的 `trainable=False` 设置。这意味着我们将重用模型，但不会对其进行任何编辑，只是使用它已经学到的知识。
- en: So my model is really only two lines of code. It’s a `Sequential` with the feature
    vector followed by a `Dense` containing three neurons. Everything that was learned
    in the many hours training MobileNet with ImageNet data is available to me; I
    don’t need to retrain!
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我的模型实际上只有两行代码。它是一个 `Sequential` 模型，包含特征向量，后跟一个包含三个神经元的 `Dense` 层。在使用 MobileNet
    和 ImageNet 数据进行多小时训练后学到的所有内容现在都可以用于我的用途；我不需要重新训练！
- en: Using this and the Beans dataset, which classifies three types of bean disease
    in plants, I can now create a classifier with this very simple code that recognizes
    even very complex images. [Figure 2-13](#complex_images_with_transfer_learning)
    shows the output of this, and the code to get it is available in the download
    for this book.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此方法和豆子数据集，该数据集可以对植物中的三种豆类病进行分类，我现在可以用这个非常简单的代码创建一个分类器，即使识别非常复杂的图像。[图 2-13](#complex_images_with_transfer_learning)显示了其输出，获取该输出的代码可以在本书的下载中找到。
- en: '![](assets/aiml_0213.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0213.png)'
- en: Figure 2-13\. Complex images with transfer learning
  id: totrans-151
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-13\. 使用迁移学习处理复杂图像
- en: Given the power that you get from transfer learning to quickly build very complex
    models, the main focus of model creation in this book will be to use transfer
    learning. As such, I hope this was a useful introduction!
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于通过迁移学习快速构建非常复杂模型所带来的强大功能，本书中模型创建的主要重点将是使用迁移学习。希望这是一个有用的介绍！
- en: Summary
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter you got an introduction to computer vision and saw what it really
    means—writing code to help a computer parse the contents of images. You learned
    how a neural network can be architected to recognize multiple classes, before
    building one from scratch that could recognize 10 fashion items. After that you
    got introduced to the concept of transfer learning, where you can take existing
    models that have been pretrained on millions of images to recognize many classes
    and use their internal variables to apply them to your scenario. From this, you
    saw how models can be downloaded from TensorFlow Hub and reused to give you very
    complex models in very few lines of code. As an example, you saw a classifier
    for bean disease in plants that was written using a model that was defined with
    just two layers! This will form the methodology that you’ll primarily use in this
    book, because the focus from here on out will be *using* models in mobile apps.
    We’ll start that journey in [Chapter 3](ch03.html#introduction_to_ml_kit) with
    an introduction to ML Kit, a framework that helps you rapidly prototype or use
    turnkey ML scenarios across Android and iOS.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将了解计算机视觉的简介，并了解其真正含义——编写代码来帮助计算机解析图像内容。您将学习如何设计神经网络以识别多个类别，在从零开始构建一个可以识别10种时尚物品的网络之前。接下来，您将了解到迁移学习的概念，即可以使用已在数百万张图像上预训练过的现有模型来识别多种类别，并使用它们的内部变量来应用到您的场景中。从中您将看到如何从
    TensorFlow Hub 下载模型并在极少的代码行中重复使用它们提供的非常复杂的模型。例如，您将看到一个用仅定义了两个层的模型编写的植物豆病分类器！这将是本书中您主要使用的方法论，因为从现在开始的重点将是在移动应用中*使用*模型。我们将在[第三章](ch03.html#introduction_to_ml_kit)中开始这段旅程，介绍
    ML Kit，这是一个框架，可以帮助您在 Android 和 iOS 上快速原型或使用即插即用的机器学习场景。
