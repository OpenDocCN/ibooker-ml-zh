- en: Chapter 15\. K-Nearest Neighbors
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 15 章 K 近邻算法
- en: 15.0 Introduction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 15.0 简介
- en: The k-nearest neighbors (KNN) classifier is one of the simplest yet most commonly
    used classifiers in supervised machine learning. KNN is often considered a lazy
    learner; it doesn’t technically train a model to make predictions. Instead an
    observation is predicted to be the same class as that of the largest proportion
    of the *k* nearest observations.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: k-最近邻（KNN）分类器是监督机器学习中最简单但最常用的分类器之一。KNN通常被认为是一种惰性学习器；它不会技术上训练一个模型来进行预测。相反，一个观测值被预测为与
    *k* 个最近观测值中最大比例的类相同。
- en: For example, if an observation with an unknown class is surrounded by an observation
    of class 1, then the observation is classified as class 1\. In this chapter we
    will explore how to use scikit-learn to create and use a KNN classifier.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果一个具有未知类的观测值被一个类为 1 的观测值所包围，则该观测值将被分类为类 1。在本章中，我们将探讨如何使用 scikit-learn 创建和使用
    KNN 分类器。
- en: 15.1 Finding an Observation’s Nearest Neighbors
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 15.1 寻找一个观测值的最近邻居
- en: Problem
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You need to find an observation’s *k* nearest observations (neighbors).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要找到一个观测值的 *k* 个最近邻居。
- en: Solution
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use scikit-learn’s `NearestNeighbors`:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 的 `NearestNeighbors`：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Discussion
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: In our solution we used the dataset of iris flowers. We created an observation,
    `new_observation`, with some values and then found the two observations that are
    closest to our observation. `indices` contains the locations of the observations
    in our dataset that are closest, so `X[indices]` displays the values of those
    observations. Intuitively, distance can be thought of as a measure of similarity,
    so the two closest observations are the two flowers most similar to the flower
    we created.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的解决方案中，我们使用了鸢尾花数据集。我们创建了一个观测值，`new_observation`，具有一些值，然后找到了最接近我们观测值的两个观测值。
    `indices` 包含了最接近我们数据集中的观测值的位置，所以 `X[indices]` 显示了这些观测值的值。直观地，距离可以被看作是相似性的度量，因此两个最接近的观测值是与我们创建的花最相似的两朵花。
- en: 'How do we measure distance? scikit-learn offers a wide variety of distance
    metrics, <math display="inline"><mi>d</mi></math>, including Euclidean:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何衡量距离？scikit-learn 提供了多种距离度量方式，<math display="inline"><mi>d</mi></math>，包括欧几里得距离：
- en: <math display="block"><mrow><msub><mi>d</mi> <mrow><mi>e</mi><mi>u</mi><mi>c</mi><mi>l</mi><mi>i</mi><mi>d</mi><mi>e</mi><mi>a</mi><mi>n</mi></mrow></msub>
    <mo>=</mo> <msqrt><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt></mrow></math>
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>d</mi> <mrow><mi>e</mi><mi>u</mi><mi>c</mi><mi>l</mi><mi>i</mi><mi>d</mi><mi>e</mi><mi>a</mi><mi>n</mi></mrow></msub>
    <mo>=</mo> <msqrt><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt></mrow></math>
- en: 'and Manhattan distance:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 和曼哈顿距离：
- en: <math display="block"><mrow><msub><mi>d</mi> <mrow><mi>m</mi><mi>a</mi><mi>n</mi><mi>h</mi><mi>a</mi><mi>t</mi><mi>t</mi><mi>a</mi><mi>n</mi></mrow></msub>
    <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <mfenced close="|" open="|" separators=""><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>-</mo> <msub><mi>y</mi> <mi>i</mi></msub></mfenced></mrow></math>
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>d</mi> <mrow><mi>m</mi><mi>a</mi><mi>n</mi><mi>h</mi><mi>a</mi><mi>t</mi><mi>t</mi><mi>a</mi><mi>n</mi></mrow></msub>
    <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <mfenced close="|" open="|" separators=""><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>-</mo> <msub><mi>y</mi> <mi>i</mi></msub></mfenced></mrow></math>
- en: 'By default, `NearestNeighbors` uses Minkowski distance:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`NearestNeighbors` 使用闵可夫斯基距离：
- en: <math display="block"><mrow><msub><mi>d</mi> <mrow><mi>m</mi><mi>i</mi><mi>n</mi><mi>k</mi><mi>o</mi><mi>w</mi><mi>s</mi><mi>k</mi><mi>i</mi></mrow></msub>
    <mo>=</mo> <msup><mfenced close=")" open="(" separators=""><munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover> <msup><mfenced
    close="|" open="|" separators=""><msub><mi>x</mi> <mi>i</mi></msub> <mo>-</mo><msub><mi>y</mi>
    <mi>i</mi></msub></mfenced> <mi>p</mi></msup></mfenced> <mrow><mn>1</mn><mo>/</mo><mi>p</mi></mrow></msup></mrow></math>
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>d</mi> <mrow><mi>m</mi><mi>i</mi><mi>n</mi><mi>k</mi><mi>o</mi><mi>w</mi><mi>s</mi><mi>k</mi><mi>i</mi></mrow></msub>
    <mo>=</mo> <msup><mfenced close=")" open="(" separators=""><munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover> <msup><mfenced
    close="|" open="|" separators=""><msub><mi>x</mi> <mi>i</mi></msub> <mo>-</mo><msub><mi>y</mi>
    <mi>i</mi></msub></mfenced> <mi>p</mi></msup></mfenced> <mrow><mn>1</mn><mo>/</mo><mi>p</mi></mrow></msup></mrow></math>
- en: where <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math> and <math
    display="inline"><msub><mi>y</mi><mi>i</mi></msub></math> are the two observations
    we are calculating the distance between. Minkowski includes a hyperparameter,
    <math display="inline"><mi>p</mi></math>, where <math display="inline"><mi>p</mi></math>
    = 1 is Manhattan distance and <math display="inline"><mi>p</mi></math> = 2 is
    Euclidean distance, and so on. By default in scikit-learn <math display="inline"><mi>p</mi></math>
    = 2.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math>和<math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math>是我们计算距离的两个观测值。闵可夫斯基距离包括一个超参数<math
    display="inline"><mi>p</mi></math>，其中<math display="inline"><mi>p</mi></math>=1
    是曼哈顿距离，<math display="inline"><mi>p</mi></math>=2 是欧几里得距离，等等。在 scikit-learn 中，默认情况下<math
    display="inline"><mi>p</mi></math>=2。
- en: 'We can set the distance metric using the `metric` parameter:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `metric` 参数设置距离度量：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `distance` variable we created contains the actual distance measurement
    to each of the two nearest neighbors:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的 `distance` 变量包含了到两个最近邻居的实际距离测量：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In addition, we can use `kneighbors_graph` to create a matrix indicating each
    observation’s nearest neighbors:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以使用 `kneighbors_graph` 创建一个矩阵，指示每个观测值的最近邻居：
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: When we are finding nearest neighbors or using any learning algorithm based
    on distance, it is important to transform features so that they are on the same
    scale. This is because the distance metrics treat all features as if they were
    on the same scale, but if one feature is in millions of dollars and a second feature
    is in percentages, the distance calculated will be biased toward the former. In
    our solution we addressed this potential issue by standardizing the features using
    `StandardScaler`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们寻找最近邻居或使用基于距离的任何学习算法时，重要的是要转换特征，使它们处于相同的尺度上。这是因为距离度量将所有特征都视为处于相同的尺度上，但如果一个特征是以百万美元计算的，而第二个特征是以百分比计算的，那么计算出的距离将偏向于前者。在我们的解决方案中，我们通过使用
    `StandardScaler` 对特征进行了标准化，以解决这个潜在的问题。
- en: 15.2 Creating a K-Nearest Neighbors Classifier
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 15.2 创建 K 近邻分类器
- en: Problem
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Given an observation of unknown class, you need to predict its class based on
    the class of its neighbors.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个未知类别的观测值，你需要根据其邻居的类别预测其类别。
- en: Solution
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解答
- en: 'If the dataset is not very large, use `KNeighborsClassifier`:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据集不太大，使用 `KNeighborsClassifier`：
- en: '[PRE7]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Discussion
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'In KNN, given an observation, <math display="inline"><msub><mi>x</mi><mi>u</mi></msub></math>,
    with an unknown target class, the algorithm first identifies the <math display="inline"><mi>k</mi></math>
    closest observations (sometimes called <math display="inline"><msub><mi>x</mi><mi>u</mi></msub></math>’s
    *neighborhood*) based on some distance metric (e.g., Euclidean distance), then
    these <math display="inline"><mi>k</mi></math> observations “vote” based on their
    class, and the class that wins the vote is <math display="inline"><msub><mi>x</mi><mi>u</mi></msub></math>’s
    predicted class. More formally, the probability <math display="inline"><msub><mi>x</mi><mi>u</mi></msub></math>
    of some class <math display="inline"><mi>j</mi></math> is:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在 KNN 中，给定一个观测值，<math display="inline"><msub><mi>x</mi><mi>u</mi></msub></math>，其目标类别未知，算法首先基于某种距离度量（例如欧几里得距离）确定最近的
    <math display="inline"><mi>k</mi></math> 个观测值（有时称为 <math display="inline"><msub><mi>x</mi><mi>u</mi></msub></math>
    的 *邻域*），然后这些 <math display="inline"><mi>k</mi></math> 个观测值基于它们的类别“投票”，获胜的类别就是
    <math display="inline"><msub><mi>x</mi><mi>u</mi></msub></math> 的预测类别。更正式地，某个类别
    <math display="inline"><mi>j</mi></math> 的概率 <math display="inline"><msub><mi>x</mi><mi>u</mi></msub></math>
    为：
- en: <math display="block"><mrow><mfrac><mn>1</mn> <mi>k</mi></mfrac> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>∈</mo><mi>ν</mi></mrow></munderover> <mi>I</mi> <mrow><mo>(</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo> <mi>j</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mfrac><mn>1</mn> <mi>k</mi></mfrac> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>∈</mo><mi>ν</mi></mrow></munderover> <mi>I</mi> <mrow><mo>(</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>=</mo> <mi>j</mi> <mo>)</mo></mrow></mrow></math>
- en: 'where ν is the <math display="inline"><mi>k</mi></math> observation in <math
    display="inline"><msub><mi>x</mi><mi>u</mi></msub></math>’s neighborhood, <math
    display="inline"><msub><mi>y</mi><mi>i</mi></msub></math> is the class of the
    *i*th observation, and <math display="inline"><mi>I</mi></math> is an indicator
    function (i.e., 1 is true, 0 otherwise). In scikit-learn we can see these probabilities
    using `predict_proba`:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ν 是 <math display="inline"><mi>k</mi></math> 个观测值在 <math display="inline"><msub><mi>x</mi><mi>u</mi></msub></math>
    的邻域中，<math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math> 是第 *i* 个观测值的类别，<math
    display="inline"><mi>I</mi></math> 是一个指示函数（即，1 为真，0 其他）。在 scikit-learn 中，我们可以使用
    `predict_proba` 查看这些概率：
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The class with the highest probability becomes the predicted class. For example,
    in the preceding output, the first observation should be class 1 (*Pr* = 0.6)
    while the second observation should be class 2 (*Pr* = 1), and this is just what
    we see:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 概率最高的类别成为预测类别。例如，在前面的输出中，第一观测值应该是类别 1 (*Pr* = 0.6)，而第二观测值应该是类别 2 (*Pr* = 1)，这正是我们所看到的：
- en: '[PRE11]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`KNeighborsClassifier` contains a number of important parameters to consider.
    First, `metric` sets the distance metric used. Second, `n_jobs` determines how
    many of the computer’s cores to use. Because making a prediction requires calculating
    the distance from a point to every single point in the data, using multiple cores
    is highly recommended. Third, `algorithm` sets the method used to calculate the
    nearest neighbors. While there are real differences in the algorithms, by default
    `KNeighborsClassifier` attempts to auto-select the best algorithm so you often
    don’t need to worry about this parameter. Fourth, by default `KNeighborsClassifier`
    works how we described previously, with each observation in the neighborhood getting
    one vote; however, if we set the `weights` parameter to `distance`, the closer
    observations’ votes are weighted more than observations farther away. Intuitively
    this make sense, since more similar neighbors might tell us more about an observation’s
    class than others.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`KNeighborsClassifier` 包含许多重要参数需要考虑。首先，`metric` 设置使用的距离度量。其次，`n_jobs` 决定使用计算机的多少核心。因为做出预测需要计算一个点与数据中每个点的距离，推荐使用多个核心。第三，`algorithm`
    设置计算最近邻的方法。虽然算法之间存在实际差异，默认情况下 `KNeighborsClassifier` 尝试自动选择最佳算法，因此通常不需要担心这个参数。第四，默认情况下
    `KNeighborsClassifier` 的工作方式与我们之前描述的相同，每个邻域中的观测值获得一个投票；然而，如果我们将 `weights` 参数设置为
    `distance`，则更靠近的观测值的投票比更远的观测值更重要。直观上这很有道理，因为更相似的邻居可能会告诉我们更多关于一个观测值类别的信息。'
- en: Finally, because distance calculations treat all features as if they are on
    the same scale, it is important to standardize the features prior to using a KNN
    classifier.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，由于距离计算将所有特征视为在相同尺度上，因此在使用 KNN 分类器之前，标准化特征是很重要的。
- en: 15.3 Identifying the Best Neighborhood Size
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 15.3 确定最佳邻域大小
- en: Problem
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to select the best value for *k* in a k-nearest neighbors classifier.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你想在 k 最近邻分类器中选择最佳的 *k* 值。
- en: Solution
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解答
- en: 'Use model selection techniques like `GridSearchCV`:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用模型选择技术如 `GridSearchCV`：
- en: '[PRE13]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Discussion
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'The size of *k* has real implications in KNN classifiers. In machine learning
    we are trying to find a balance between bias and variance, and in few places is
    that as explicit as the value of *k*. If *k* = *n*, where *n* is the number of
    observations, then we have high bias but low variance. If *k* = 1, we will have
    low bias but high variance. The best model will come from finding the value of
    *k* that balances this bias-variance trade-off. In our solution, we used `GridSearchCV`
    to conduct five-fold cross-validation on KNN classifiers with different values
    of *k*. When that is completed, we can see the *k* that produces the best model:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*k* 的大小在KNN分类器中有着真实的影响。在机器学习中，我们试图在偏差和方差之间找到一个平衡点，*k* 的值在其中的显性展示是极其重要的。如果 *k*
    = *n*，其中 *n* 是观测数量，那么我们具有高偏差但低方差。如果 *k* = 1，我们将具有低偏差但高方差。最佳模型将通过找到能平衡这种偏差-方差权衡的*k*的值来获得。在我们的解决方案中，我们使用`GridSearchCV`对具有不同*k*值的KNN分类器进行了五折交叉验证。完成后，我们可以看到产生最佳模型的*k*值：'
- en: '[PRE14]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 15.4 Creating a Radius-Based Nearest Neighbors Classifier
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 15.4 创建基于半径的最近邻分类器
- en: Problem
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Given an observation of unknown class, you need to predict its class based on
    the class of all observations within a certain distance.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个未知类的观测值，您需要基于一定距离内所有观测值的类来预测其类别。
- en: Solution
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use `RadiusNeighborsClassifier`:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `RadiusNeighborsClassifier`：
- en: '[PRE16]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Discussion
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: In KNN classification, an observation’s class is predicted from the classes
    of its *k* neighbors. A less common technique is classification in a *radius-based
    nearest neighbor* (RNN) classifier, where an observation’s class is predicted
    from the classes of all observations within a given radius *r*.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在KNN分类中，一个观测的类别是根据其 *k* 个邻居的类别预测的。一种不太常见的技术是基于半径的最近邻（RNN）分类器，其中一个观测的类别是根据给定半径
    *r* 内所有观测的类别预测的。
- en: In scikit-learn, `RadiusNeighborsClassifier` is very similar to `KNeighbors​Classi⁠fier`,
    with the exception of two parameters. First, in `RadiusNeighbors​Clas⁠sifier`
    we need to specify the radius of the fixed area used to determine if an observation
    is a neighbor using `radius`. Unless there is some substantive reason for setting
    `radius` to some value, it’s best to treat it like any other hyperparameter and
    tune it during model selection. The second useful parameter is `outlier_label`,
    which indicates what label to give an observation that has no observations within
    the radius—​which itself can be a useful tool for identifying outliers.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，`RadiusNeighborsClassifier`与`KNeighbors​Classi⁠fier`非常相似，只有两个参数例外。首先，在`RadiusNeighbors​Clas⁠sifier`中，我们需要指定用于确定观测是否为邻居的固定区域半径使用`radius`。除非有设置`radius`到某个值的实质性原因，否则最好在模型选择过程中像其他超参数一样进行调整。第二个有用的参数是`outlier_label`，它指示如果半径内没有观测值，则给出一个观测的标签—这本身可以是一个用于识别异常值的有用工具。
- en: 15.5 Finding Approximate Nearest Neighbors
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 15.5 寻找近似最近邻
- en: Problem
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: 'You want to fetch nearest neighbors for big data at low latency:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望在低延迟下获取大数据的最近邻：
- en: Solution
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use an *approximate nearest neighbors* (ANN) based search with Facebook’s `faiss`
    library:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 *近似最近邻*（ANN）搜索，使用Facebook的 `faiss` 库：
- en: '[PRE18]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Discussion
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: KNN is a great approach to finding the most similar observations in a set of
    small data. However, as the size of our data increases, so does the time it takes
    to compute the distance between any one observation and all other points in our
    dataset. Large scale ML systems such as search or recommendation engines often
    use some form of vector similarity measure to retrieve similar observations. But
    at scale in real time, where we need results in less than 100 ms, KNN becomes
    infeasible to run.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: KNN是在小数据集中找到最相似观测的一个很好的方法。然而，随着数据集的增大，计算任意观测与数据集中所有其他点之间距离所需的时间也会增加。大规模的ML系统如搜索或推荐引擎通常使用某种形式的向量相似度测量来检索相似的观测。但在实时规模中，我们需要在不到100毫秒内获得结果，KNN变得不可行。
- en: ANN helps us overcome this problem by sacrificing some of the quality of the
    exact nearest neighbors search in favor of speed. This is to say that although
    the order and items in the first 10 nearest neighbors of an ANN search may not
    match the first 10 results from an exact KNN search, we get those first 10 nearest
    neighbors much faster.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ANN通过牺牲精确最近邻搜索的一些质量以换取速度来帮助我们克服这个问题。换句话说，虽然ANN搜索的前10个最近邻的顺序和项可能与精确KNN搜索的前10个结果不匹配，但我们能更快地得到这前10个最近邻。
- en: In this example, we use an ANN approach called inverted file index (IVF). This
    approach works by using clustering to limit the scope of the search space for
    our nearest neighbors search. IVF uses Voronoi tessellations to partition our
    search space into a number of distinct areas (or clusters). And when we go to
    find nearest neighbors, we visit a limited number of clusters to find similar
    observations, as opposed to conducting a comparison across every point in our
    dataset.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用了一种名为倒排文件索引（IVF）的ANN方法。这种方法通过使用聚类来限制最近邻搜索的范围。IVF使用Voronoi镶嵌将我们的搜索空间划分为多个不同的区域（或聚类）。当我们去查找最近邻时，我们访问了有限数量的聚类以找到相似的观测值，而不是对数据集中的每一个点进行比较。
- en: How Voronoi tessellations are created from data is best visualized using simple
    data. Take a scatter plot of random data visualized in two dimensions, as shown
    in [Figure 15-1](#fig1501).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如何从数据中创建Voronoi镶嵌最好通过简单的数据可视化。例如，取随机数据的散点图在二维中可视化，如[图 15-1](#fig1501)所示。
- en: '![mpc2 1501](assets/mpc2_1501.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![mpc2 1501](assets/mpc2_1501.png)'
- en: Figure 15-1\. A scatter plot of randomly generated two-dimensional data
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-1\. 一组随机生成的二维数据的散点图
- en: Using Voronoi tessellations, we can create a number of subspaces, each of which
    contains only a small subset of the total observations we want to search, as shown
    in [Figure 15-2](#fig1502).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Voronoi镶嵌，我们可以创建多个子空间，每个子空间只包含我们想要搜索的总观测的一个小子集，如[图 15-2](#fig1502)所示。
- en: '![mpc2 1502](assets/mpc2_1502.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![mpc2 1502](assets/mpc2_1502.png)'
- en: Figure 15-2\. Randomly generated two-dimensional data separated into a number
    of different subspaces
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-2\. 将随机生成的二维数据分割成多个不同子空间
- en: The `nlist` parameter in the `Faiss` library lets us define the number of clusters
    we want to create. An additional parameter, `nprobe`, can be used at query time
    to define the number of clusters we want to search to retrieve nearest neighbors
    for a given observation. Increasing both `nlist` and `nprobe` can result in higher
    quality neighbors at the cost of larger computational effort and thus a longer
    runtime for IVF indices. Decreasing each of these parameters will have the inverse
    effect, and your code will run faster but at the risk of returning lower quality
    results.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Faiss`库中，`nlist`参数允许我们定义要创建的聚类数。还可以在查询时使用一个额外的参数`nprobe`来定义要搜索的聚类数，以检索给定观测值的最近邻。增加`nlist`和`nprobe`都可以提高邻居的质量，但会增加计算成本，从而导致IVF索引的运行时间更长。减少这些参数会产生相反的效果，您的代码将运行得更快，但可能返回质量较低的结果。
- en: Notice this example returns the exact same output as the first recipe in this
    chapter. This is because we are working with very small data and using only three
    clusters, which makes it unlikely our ANN results will differ significantly from
    our KNN results.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，此示例返回与本章第一个配方完全相同的输出。这是因为我们处理的是非常小的数据，并且仅使用了三个聚类，这使得我们的ANN结果与我们的KNN结果没有显著差异。
- en: See Also
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Nearest Neighbor Indexes for Similarity Search (different ANN index types)](https://oreil.ly/DVqgn)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用于相似性搜索的最近邻索引（不同的ANN索引类型）](https://oreil.ly/DVqgn)'
- en: 15.6 Evaluating Approximate Nearest Neighbors
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 15.6 评估近似最近邻
- en: Problem
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: 'You want to see how your ANN compares to exact nearest neighbors (KNN):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 您想看看您的ANN与精确最近邻（KNN）的比较情况：
- en: Solution
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Compute the recall @k nearest neighbors of the ANN as compared to the KNN:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 计算ANN相对于KNN的Recall @k最近邻
- en: '[PRE20]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Discussion
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: '*Recall @k* is most simply defined as the number of items returned by the ANN
    at some *k* nearest neighbors that also appear in the exact nearest neighbors
    at the same *k*, divided by *k*. In this example, at 10 nearest neighbors we have
    100% recall, which means that our ANN is returning the same indices as our KNN
    at k=10 (though not necessarily in the same order).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*Recall @k* 最简单的定义是ANN在某个*k*个最近邻处返回的项目数，这些项目同时也出现在相同*k*个精确最近邻中，除以*k*。在这个例子中，在10个最近邻处，我们有100%的召回率，这意味着我们的ANN返回的索引与我们的KNN在k=10时是相同的（尽管不一定是相同的顺序）。'
- en: Recall is a common metric to use when evaluating ANNs against exact nearest
    neighbors.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估ANN与精确最近邻时，Recall是一个常用的度量标准。
- en: See Also
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Google’s note on ANN for its Vertex Matching Engine Service](https://oreil.ly/-COc9)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Google对其Vertex匹配引擎服务的ANN的注意事项](https://oreil.ly/-COc9)'
