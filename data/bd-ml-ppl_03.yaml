- en: Chapter 3\. Data Ingestion
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第三章\. 数据摄取
- en: With the basic TFX setup and the ML MetadataStore in place, in this chapter,
    we focus on how to ingest your datasets into a pipeline for consumption in various
    components, as shown in [Figure 3-1](#filepos156590).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 通过基本的 TFX 设置和 ML MetadataStore，本章重点介绍了如何将您的数据集摄取到流水线中，以便在各个组件中使用，如[图 3-1](#filepos156590)所示。
- en: '![](images/00030.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00030.jpg)'
- en: Figure 3-1\. Data ingestion as part of ML pipelines
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-1\. 作为 ML 流水线一部分的数据摄取
- en: TFX provides us components to ingest data from files or services. In this chapter,
    we outline the underlying concepts, explain ways to split the datasets into training
    and evaluation subsets, and demonstrate how to combine multiple data exports into
    one all-encompassing dataset. We then discuss some strategies to ingest different
    forms of data (structured, text, and images), which have proven helpful in previous
    use cases.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: TFX 为我们提供了从文件或服务摄取数据的组件。在本章中，我们概述了底层概念，解释了如何将数据集拆分为训练和评估子集，并演示了如何将多个数据导出组合成一个全面的数据集。然后，我们讨论了摄取不同形式数据（结构化数据、文本和图像）的一些策略，这些策略在之前的使用案例中被证明是有帮助的。
- en: Concepts for Data Ingestion
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 数据摄取的概念
- en: In this step of our pipeline, we read data files or request the data for our
    pipeline run from an external service (e.g., Google Cloud BigQuery). Before passing
    the ingested dataset to the next component, we divide the available data into
    separate datasets (e.g., training and validation datasets) and then convert the
    datasets into TFRecord files containing the data represented as `tf.Example` data
    structures.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的流水线的这一步骤中，我们从外部服务（例如 Google Cloud BigQuery）读取数据文件或请求数据以运行我们的流水线。在将摄取的数据传递给下一个组件之前，我们将可用数据分成单独的数据集（例如训练数据集和验证数据集），然后将数据集转换为包含以
    `tf.Example` 数据结构表示的数据的 TFRecord 文件。
- en: TFRECORD
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: TFRECORD
- en: 'TFRecord is a lightweight format optimized for streaming large datasets. While
    in practice, most TensorFlow users store serialized example Protocol Buffers in
    TFRecord files, the TFRecord file format actually supports any binary data, as
    shown in the following:'
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: TFRecord 是一种针对流式传输大型数据集进行优化的轻量级格式。实际上，大多数 TensorFlow 用户在 TFRecord 文件中存储序列化的示例
    Protocol Buffers，但 TFRecord 文件格式实际上支持任何二进制数据，如下所示：
- en: '`import``tensorflow``as``tf``with``tf``.``io``.``TFRecordWriter``(``"test.tfrecord"``)``as``w``:``w``.``write``(``b``"First
    record"``)``w``.``write``(``b``"Second record"``)``for``record``in``tf``.``data``.``TFRecordDataset``(``"test.tfrecord"``):``print``(``record``)``tf``.``Tensor``(``b``''First
    record''``,``shape``=``(),``dtype``=``string``)``tf``.``Tensor``(``b``''Second
    record''``,``shape``=``(),``dtype``=``string``)`'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`import``tensorflow``as``tf``with``tf``.``io``.``TFRecordWriter``(``"test.tfrecord"``)``as``w``:``w``.``write``(``b``"First
    record"``)``w``.``write``(``b``"Second record"``)``for``record``in``tf``.``data``.``TFRecordDataset``(``"test.tfrecord"``):``print``(``record``)``tf``.``Tensor``(``b``''First
    record''``,``shape``=``(),``dtype``=``string``)``tf``.``Tensor``(``b``''Second
    record''``,``shape``=``(),``dtype``=``string``)`'
- en: If TFRecord files contain `tf.Example` records, each record contains one or
    more features that would represent the columns in our data. The data is then stored
    in binary files, which can be digested efficiently. If you are interested in the
    internals of TFRecord files, we recommend the [TensorFlow documentation](https://oreil.ly/2-MuJ).
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果 TFRecord 文件包含 `tf.Example` 记录，每个记录都包含一个或多个特征，这些特征代表我们数据中的列。然后，数据以可以高效消化的二进制文件形式存储。如果您对
    TFRecord 文件的内部机制感兴趣，我们建议参阅[TensorFlow文档](https://oreil.ly/2-MuJ)。
- en: 'Storing your data as TFRecord and `tf.Examples` provides a few benefits:'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 将数据存储为 TFRecord 和 `tf.Examples` 提供了一些好处：
- en: The data structure is system independent since it relies on Protocol Buffers,
    a cross-platform, cross-language library, to serialize data.
  id: totrans-12
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 该数据结构与系统无关，因为它依赖于 Protocol Buffers，这是一个跨平台、跨语言的库，用于序列化数据。
- en: TFRecord is optimized for downloading or writing large amounts of data quickly.
  id: totrans-13
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: TFRecord 优化了快速下载或写入大量数据的能力。
- en: '`tf.Example`, the data structure representing every data row within TFRecord,
    is also the default data structure in the TensorFlow ecosystem and, therefore,
    is used in all TFX components.'
  id: totrans-14
  prefs:
  - PREF_OL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`tf.Example`，在 TFRecord 中代表每个数据行的数据结构，也是 TensorFlow 生态系统中的默认数据结构，因此在所有 TFX
    组件中都被使用。'
- en: The process of ingesting, splitting, and converting the datasets is performed
    by the `ExampleGen` component. As we see in the following examples, datasets can
    be read from local and remote folders as well as requested from data services
    like Google Cloud BigQuery.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 摄取、拆分和转换数据集的过程由 `ExampleGen` 组件执行。正如我们在下面的示例中看到的那样，数据集可以从本地和远程文件夹中读取，也可以从像 Google
    Cloud BigQuery 这样的数据服务中请求。
- en: Ingesting Local Data Files
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 摄取本地数据文件
- en: The `ExampleGen` component can ingest a few data structures, including comma-separated
    value files (CSVs), precomputed TFRecord files, and serialization outputs from
    Apache Avro and Apache Parquet.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '`ExampleGen` 组件可以摄取几种数据结构，包括逗号分隔值文件（CSV）、预计算的 TFRecord 文件以及 Apache Avro 和 Apache
    Parquet 的序列化输出。'
- en: Converting comma-separated data to tf.Example
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 将逗号分隔数据转换为 tf.Example
- en: 'Datasets for structured data or text data are often stored in CSV files. TFX
    provides functionality to read and convert these files to `tf.Example` data structures.
    The following code example demonstrates the ingestion of a folder containing the
    CSV data of our example project:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化数据或文本数据的数据集通常存储在 CSV 文件中。TFX 提供功能来读取和转换这些文件为 `tf.Example` 数据结构。以下代码示例演示了如何摄取包含我们示例项目
    CSV 数据的文件夹：
- en: '`import``os``from``tfx.components``import``CsvExampleGen``from``tfx.utils.dsl_utils``import``external_input``base_dir``=``os``.``getcwd``()``data_dir``=``os``.``path``.``join``(``os``.``pardir``,``"data"``)``examples``=``external_input``(``os``.``path``.``join``(``base_dir``,``data_dir``))`![](images/00002.jpg)`example_gen``=``CsvExampleGen``(``input``=``examples``)`![](images/00075.jpg)`context``.``run``(``example_gen``)`![](images/00064.jpg)'
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`import``os``from``tfx.components``import``CsvExampleGen``from``tfx.utils.dsl_utils``import``external_input``base_dir``=``os``.``getcwd``()``data_dir``=``os``.``path``.``join``(``os``.``pardir``,``"data"``)``examples``=``external_input``(``os``.``path``.``join``(``base_dir``,``data_dir``))`![](images/00002.jpg)`example_gen``=``CsvExampleGen``(``input``=``examples``)`![](images/00075.jpg)`context``.``run``(``example_gen``)`![](images/00064.jpg)'
- en: '![](images/00002.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: Define the data path.
  id: totrans-22
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 定义数据路径。
- en: '![](images/00075.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00075.jpg)'
- en: Instantiate the pipeline component.
  id: totrans-24
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 实例化管道组件。
- en: '![](images/00064.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00064.jpg)'
- en: Execute the component interactively.
  id: totrans-26
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 执行组件的交互操作。
- en: If you execute the component as part of an interactive pipeline, the metadata
    of the run will be shown in the Jupyter Notebook. The outputs of the component
    are shown in [Figure 3-2](#filepos172647), highlighting the storage locations
    of the training and the evaluation datasets.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您将组件作为交互式管道的一部分执行，则运行的元数据将显示在 Jupyter Notebook 中。组件的输出显示在 [图 3-2](#filepos172647)
    中，突出显示了训练和评估数据集的存储位置。
- en: '![](images/00086.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00086.jpg)'
- en: Figure 3-2\. `ExampleGen` component output
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3-2\. `ExampleGen` 组件输出
- en: FOLDER STRUCTURE
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 文件夹结构
- en: It is expected that the input path of ExampleGen only contains the data files.
    The component tries to consume all existing files within the path level. Any additional
    files (e.g., metadata files) can’t be consumed by the component and make the component
    step fail. The component is also not stepping through existing subdirectories
    unless it is configured as an input pattern.
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 预期 `ExampleGen` 的输入路径仅包含数据文件。组件尝试消耗路径级别内的所有现有文件。任何额外的文件（例如元数据文件）无法被组件消耗，并导致组件步骤失败。该组件也不会遍历现有的子目录，除非配置为输入模式。
- en: Importing existing TFRecord Files
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 导入现有的 TFRecord 文件
- en: 'Sometimes our data can’t be expressed efficiently as CSVs (e.g., when we want
    to load images for computer vision problems or large corpora for natural language
    processing problems). In these cases, it is recommended to convert the datasets
    to TFRecord data structures and then load the saved TFRecord files with the `ImportExampleGen`
    component. If you would like to perform the conversion of your data to TFRecord
    files as part of the pipeline, take a look at [Chapter 10](index_split_017.html#filepos1073133),
    in which we discuss the development of custom TFX components including a data
    ingestion component. TFRecord files can be ingested as shown in the following
    example:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们的数据无法有效表示为 CSV（例如，当我们想要加载用于计算机视觉问题的图像或用于自然语言处理问题的大语料库时）。在这些情况下，建议将数据集转换为
    TFRecord 数据结构，然后使用 `ImportExampleGen` 组件加载保存的 TFRecord 文件。如果您希望作为管道的一部分将数据转换为
    TFRecord 文件，请参阅 [第 10 章](index_split_017.html#filepos1073133)，我们将讨论自定义 TFX 组件的开发，包括数据摄取组件。TFRecord
    文件可以如以下示例所示进行摄取：
- en: '`import``os``from``tfx.components``import``ImportExampleGen``from``tfx.utils.dsl_utils``import``external_input``base_dir``=``os``.``getcwd``()``data_dir``=``os``.``path``.``join``(``os``.``pardir``,``"tfrecord_data"``)``examples``=``external_input``(``os``.``path``.``join``(``base_dir``,``data_dir``))``example_gen``=``ImportExampleGen``(``input``=``examples``)``context``.``run``(``example_gen``)`'
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`import``os``from``tfx.components``import``ImportExampleGen``from``tfx.utils.dsl_utils``import``external_input``base_dir``=``os``.``getcwd``()``data_dir``=``os``.``path``.``join``(``os``.``pardir``,``"tfrecord_data"``)``examples``=``external_input``(``os``.``path``.``join``(``base_dir``,``data_dir``))``example_gen``=``ImportExampleGen``(``input``=``examples``)``context``.``run``(``example_gen``)`'
- en: Since the datasets are already stored as `tf.Example` records within the TFRecord
    files, they can be imported and don’t need any conversion. The `ImportExampleGen`
    component handles this import step.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集已经以TFRecord文件中的`tf.Example`记录的形式存储，因此可以导入而无需转换。`ImportExampleGen`组件处理此导入步骤。
- en: Converting Parquet-serialized data to tf.Example
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 将Parquet序列化数据转换为tf.Example
- en: In [Chapter 2](index_split_007.html#filepos83150), we discussed the internal
    architecture of TFX components and the behavior of a component, which is driven
    by its `executor`. If we would like to load new file types into our pipeline,
    we can override the `executor_class` instead of writing a completely new component.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](index_split_007.html#filepos83150)中，我们讨论了TFX组件的内部架构和组件的行为，其驱动力是其`executor`。如果我们想要将新的文件类型加载到我们的流水线中，我们可以覆盖`executor_class`，而不是编写全新的组件。
- en: 'TFX includes `executor` classes for loading different file types, including
    Parquet serialized data. The following example shows how you can override the
    `executor_class` to change the loading behavior. Instead of using the `CsvExampleGen`
    or `ImportExampleGen` components, we will use a generic file loader component
    `FileBasedExampleGen`, which allows an override of the `executor_class`:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: TFX包括用于加载不同文件类型的`executor`类，包括Parquet序列化数据。以下示例显示了如何覆盖`executor_class`以更改加载行为。而不是使用`CsvExampleGen`或`ImportExampleGen`组件，我们将使用通用文件加载器组件`FileBasedExampleGen`，允许覆盖`executor_class`：
- en: '`from``tfx.components``import``FileBasedExampleGen`![](images/00002.jpg)`from``tfx.components.example_gen.custom_executors``import``parquet_executor`![](images/00075.jpg)`from``tfx.utils.dsl_utils``import``external_input``examples``=``external_input``(``parquet_dir_path``)``example_gen``=``FileBasedExampleGen``(``input``=``examples``,``executor_class``=``parquet_executor``.``Executor``)`![](images/00064.jpg)'
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`from``tfx.components``import``FileBasedExampleGen`![](images/00002.jpg)`from``tfx.components.example_gen.custom_executors``import``parquet_executor`![](images/00075.jpg)`from``tfx.utils.dsl_utils``import``external_input``examples``=``external_input``(``parquet_dir_path``)``example_gen``=``FileBasedExampleGen``(``input``=``examples``,``executor_class``=``parquet_executor``.``Executor``)`![](images/00064.jpg)'
- en: '![](images/00002.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: Import generic file loader component.
  id: totrans-41
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 导入通用文件加载器组件。
- en: '![](images/00075.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00075.jpg)'
- en: Import Parquet-specific executor.
  id: totrans-43
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 导入特定于Parquet的执行器。
- en: '![](images/00064.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00064.jpg)'
- en: Override the executor.
  id: totrans-45
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 覆盖执行器。
- en: Converting Avro-serialized data to tf.Example
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 将Avro序列化数据转换为tf.Example
- en: 'The concept of overriding the `executor_class` can, of course, be expanded
    to almost any other file type. TFX provides additional classes, as shown in the
    following example for loading Avro-serialized data:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖`executor_class`的概念当然可以扩展到几乎任何其他文件类型。TFX提供了额外的类，如下例所示，用于加载Avro序列化数据：
- en: '`from``tfx.components``import``FileBasedExampleGen`![](images/00002.jpg)`from``tfx.components.example_gen.custom_executors``import``avro_executor`![](images/00075.jpg)`from``tfx.utils.dsl_utils``import``external_input``examples``=``external_input``(``avro_dir_path``)``example_gen``=``FileBasedExampleGen``(``input``=``examples``,``executor_class``=``avro_executor``.``Executor``)`![](images/00064.jpg)'
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`from``tfx.components``import``FileBasedExampleGen`![](images/00002.jpg)`from``tfx.components.example_gen.custom_executors``import``avro_executor`![](images/00075.jpg)`from``tfx.utils.dsl_utils``import``external_input``examples``=``external_input``(``avro_dir_path``)``example_gen``=``FileBasedExampleGen``(``input``=``examples``,``executor_class``=``avro_executor``.``Executor``)`![](images/00064.jpg)'
- en: '![](images/00002.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: Import generic file loader component.
  id: totrans-50
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 导入通用文件加载器组件。
- en: '![](images/00075.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00075.jpg)'
- en: Import the Avro-specific executor.
  id: totrans-52
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 导入特定于Avro的执行器。
- en: '![](images/00064.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00064.jpg)'
- en: Override the executor.
  id: totrans-54
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 覆盖执行器。
- en: In case we want to load a different file type, we could write our custom executor
    specific to our file type and apply the same concepts of overriding the executor
    earlier. In [Chapter 10](index_split_017.html#filepos1073133), we will guide you
    through two examples of how to write your own custom data ingestion component
    and executor.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要加载不同的文件类型，可以编写特定于我们文件类型的自定义执行器，并应用之前覆盖执行器的相同概念。在[第10章](index_split_017.html#filepos1073133)中，我们将向您介绍如何编写自己的自定义数据摄入组件和执行器的两个示例。
- en: Converting your custom data to TFRecord data structures
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 将自定义数据转换为TFRecord数据结构
- en: Sometimes it is simpler to convert existing datasets to TFRecord data structures
    and then ingest them with the `ImportExampleGen` component as we discussed in
    [“Importing existing TFRecord Files”](#filepos173622). This approach is useful
    if our data is not available through a data platform that allows efficient data
    streaming. For example, if we are training a computer vision model and we load
    a large number of images into our pipeline, we have to convert the images to TFRecord
    data structures in the first place (more on this in the later section on [“Image
    Data for Computer Vision Problems”](#filepos277782)).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，将现有数据集转换为TFRecord数据结构，然后如我们在[“导入现有TFRecord文件”](#filepos173622)中讨论的那样使用`ImportExampleGen`组件导入，比较简单。如果我们的数据不通过允许高效数据流的数据平台可用，这种方法尤其有用。例如，如果我们正在训练计算机视觉模型并将大量图像加载到管道中，我们首先必须将图像转换为TFRecord数据结构（稍后在[“用于计算机视觉问题的图像数据”](#filepos277782)一节中详细介绍）。
- en: In the following example, we convert our structured data into TFRecord data
    structures. Imagine our data isn’t available in a CSV format, only in JSON or
    XML. The following example can be used (with small modifications) to convert these
    data formats before ingesting them to our pipeline with the `ImportExampleGen`
    component.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将结构化数据转换为TFRecord数据结构。想象一下，我们的数据不是以CSV格式提供的，只能使用JSON或XML格式。在使用`ImportExampleGen`组件将这些数据导入管道之前，可以使用以下示例（稍作修改）转换这些数据格式。
- en: 'To convert data of any type to TFRecord files, we need to create a `tf.Example`
    structure for every data record in the dataset. `tf.Example` is a simple but highly
    flexible data structure, which is a key-value mapping:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 要将任何类型的数据转换为TFRecord文件，我们需要为数据集中的每个数据记录创建一个`tf.Example`结构。`tf.Example`是一个简单但非常灵活的数据结构，它是一个键值映射：
- en: '`{``"string"``:``value``}`'
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`{``"string"``:``value``}`'
- en: In the case of TFRecord data structure, a `tf.Example` expects a `tf.Features`
    object, which accepts a dictionary of features containing key-value pairs. The
    key is always a string identifier representing the feature column, and the value
    is a `tf.train.Feature` object.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于TFRecord数据结构，`tf.Example`期望一个`tf.Features`对象，该对象接受包含键值对的特征字典。键始终是表示特征列的字符串标识符，值是`tf.train.Feature`对象。
- en: Example 3-1\. TFRecord data structure
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 示例3-1. TFRecord数据结构
- en: '`Record 1: tf.Example     tf.Features         ''column A'': tf.train.Feature
            ''column B'': tf.train.Feature         ''column C'': tf.train.Feature`'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`Record 1: tf.Example     tf.Features         ''column A'': tf.train.Feature
            ''column B'': tf.train.Feature         ''column C'': tf.train.Feature`'
- en: '`tf.train.Feature` allows three data types:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.train.Feature`允许三种数据类型：'
- en: '`tf.train.BytesList`'
  id: totrans-65
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`tf.train.BytesList`'
- en: '`tf.train.FloatList`'
  id: totrans-66
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`tf.train.FloatList`'
- en: '`tf.train.Int64List`'
  id: totrans-67
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`tf.train.Int64List`'
- en: 'To reduce code redundancy, we’ll define helper functions to assist with converting
    the data records into the correct data structure used by `tf.Example`:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少代码冗余，我们将定义辅助函数来帮助将数据记录转换为`tf.Example`所使用的正确数据结构：
- en: '`import``tensorflow``as``tf``def``_bytes_feature``(``value``):``return``tf``.``train``.``Feature``(``bytes_list``=``tf``.``train``.``BytesList``(``value``=``[``value``]))``def``_float_feature``(``value``):``return``tf``.``train``.``Feature``(``float_list``=``tf``.``train``.``FloatList``(``value``=``[``value``]))``def``_int64_feature``(``value``):``return``tf``.``train``.``Feature``(``int64_list``=``tf``.``train``.``Int64List``(``value``=``[``value``]))`'
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`import``tensorflow``as``tf``def``_bytes_feature``(``value``):``return``tf``.``train``.``Feature``(``bytes_list``=``tf``.``train``.``BytesList``(``value``=``[``value``]))``def``_float_feature``(``value``):``return``tf``.``train``.``Feature``(``float_list``=``tf``.``train``.``FloatList``(``value``=``[``value``]))``def``_int64_feature``(``value``):``return``tf``.``train``.``Feature``(``int64_list``=``tf``.``train``.``Int64List``(``value``=``[``value``]))`'
- en: With the helper functions in place, let’s take a look at how we could convert
    our demo dataset to files containing the TFRecord data structure. First, we need
    to read our original data file and convert every data record into a `tf.Example`
    data structure and then save all records in a TFRecord file. The following code
    example is an abbreviated version. The complete example can be found in the book’s
    [GitHub repository](https://oreil.ly/bmlp-git-convert_data_to_tfrecordspy) under
    chapters/data_ingestion.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在放置了辅助函数之后，让我们看看如何将演示数据集转换为包含TFRecord数据结构的文件。首先，我们需要读取原始数据文件，并将每个数据记录转换为`tf.Example`数据结构，然后将所有记录保存在TFRecord文件中。以下代码示例是简略版本。完整示例可以在书的[GitHub存储库](https://oreil.ly/bmlp-git-convert_data_to_tfrecordspy)的章节/data_ingestion中找到。
- en: '`import``csv``import``tensorflow``as``tf``original_data_file``=``os``.``path``.``join``(``os``.``pardir``,``os``.``pardir``,``"data"``,``"consumer-complaints.csv"``)``tfrecord_filename``=``"consumer-complaints.tfrecord"``tf_record_writer``=``tf``.``io``.``TFRecordWriter``(``tfrecord_filename``)`![](images/00002.jpg)`with``open``(``original_data_file``)``as``csv_file``:``reader``=``csv``.``DictReader``(``csv_file``,``delimiter``=``","``,``quotechar``=``''"''``)``for``row``in``reader``:``example``=``tf``.``train``.``Example``(``features``=``tf``.``train``.``Features``(``feature``=``{`![](images/00075.jpg)`"product"``:``_bytes_feature``(``row``[``"product"``]),``"sub_product"``:``_bytes_feature``(``row``[``"sub_product"``]),``"issue"``:``_bytes_feature``(``row``[``"issue"``]),``"sub_issue"``:``_bytes_feature``(``row``[``"sub_issue"``]),``"state"``:``_bytes_feature``(``row``[``"state"``]),``"zip_code"``:``_int64_feature``(``int``(``float``(``row``[``"zip_code"``]))),``"company"``:``_bytes_feature``(``row``[``"company"``]),``"company_response"``:``_bytes_feature``(``row``[``"company_response"``]),``"consumer_complaint_narrative"``:`
    `\` `_bytes_feature``(``row``[``"consumer_complaint_narrative"``]),``"timely_response"``:``_bytes_feature``(``row``[``"timely_response"``]),``"consumer_disputed"``:``_bytes_feature``(``row``[``"consumer_disputed"``]),``}))``tf_record_writer``.``write``(``example``.``SerializeToString``())`![](images/00064.jpg)`tf_record_writer``.``close``()`'
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`import``csv``import``tensorflow``as``tf``original_data_file``=``os``.``path``.``join``(``os``.``pardir``,``os``.``pardir``,``"data"``,``"consumer-complaints.csv"``)``tfrecord_filename``=``"consumer-complaints.tfrecord"``tf_record_writer``=``tf``.``io``.``TFRecordWriter``(``tfrecord_filename``)`![](images/00002.jpg)`with``open``(``original_data_file``)``as``csv_file``:``reader``=``csv``.``DictReader``(``csv_file``,``delimiter``=``","``,``quotechar``=``''"''``)``for``row``in``reader``:``example``=``tf``.``train``.``Example``(``features``=``tf``.``train``.``Features``(``feature``=``{`![](images/00075.jpg)`"product"``:``_bytes_feature``(``row``[``"product"``]),``"sub_product"``:``_bytes_feature``(``row``[``"sub_product"``]),``"issue"``:``_bytes_feature``(``row``[``"issue"``]),``"sub_issue"``:``_bytes_feature``(``row``[``"sub_issue"``]),``"state"``:``_bytes_feature``(``row``[``"state"``]),``"zip_code"``:``_int64_feature``(``int``(``float``(``row``[``"zip_code"``]))),``"company"``:``_bytes_feature``(``row``[``"company"``]),``"company_response"``:``_bytes_feature``(``row``[``"company_response"``]),``"consumer_complaint_narrative"``:`
    `\` `_bytes_feature``(``row``[``"consumer_complaint_narrative"``]),``"timely_response"``:``_bytes_feature``(``row``[``"timely_response"``]),``"consumer_disputed"``:``_bytes_feature``(``row``[``"consumer_disputed"``]),``}))``tf_record_writer``.``write``(``example``.``SerializeToString``())`![](images/00064.jpg)`tf_record_writer``.``close``()'
- en: '![](images/00002.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: Creates a `TFRecordWriter` object that saves to the path specified in `tfrecord_filename`
  id: totrans-73
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 创建了一个 `TFRecordWriter` 对象，保存在 `tfrecord_filename` 指定的路径中。
- en: '![](images/00075.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00075.jpg)'
- en: '`tf.train.Example` for every data record'
  id: totrans-75
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`tf.train.Example` 适用于每个数据记录'
- en: '![](images/00064.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00064.jpg)'
- en: Serializes the data structure
  id: totrans-77
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 序列化数据结构
- en: The generated TFRecord file consumer-complaints.tfrecord can now be imported
    with the `ImportExampleGen` component.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的 TFRecord 文件 consumer-complaints.tfrecord 现在可以使用 `ImportExampleGen` 组件导入。
- en: Ingesting Remote Data Files
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从远程数据文件中摄取
- en: 'The `ExampleGen` component can read files from remote cloud storage buckets
    like Google Cloud Storage or AWS Simple Storage Service (S3).[1](#filepos293747)
    TFX users can provide the bucket path to the `external_input` function, as shown
    in the following example:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`ExampleGen` 组件可以从像 Google Cloud Storage 或 AWS Simple Storage Service (S3)
    这样的远程云存储桶中读取文件。[1](#filepos293747) TFX 用户可以像下面的例子一样向 `external_input` 函数提供存储桶路径：'
- en: '`examples``=``external_input``(``"gs://example_compliance_data/"``)``example_gen``=``CsvExampleGen``(``input``=``examples``)`'
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`examples``=``external_input``(``"gs://example_compliance_data/"``)``example_gen``=``CsvExampleGen``(``input``=``examples``)`'
- en: Access to private cloud storage buckets requires setting up the cloud provider
    credentials. The setup is provider specific. AWS is authenticating users through
    a user-specific access key and access secret. To access private AWS S3 buckets,
    you need to create a user access key and secret.[2](#filepos294020) In contrast,
    the Google Cloud Platform (GCP) authenticates users through service accounts.
    To access private GCP Storage buckets, you need to create a service account file
    with the permission to access the storage bucket.[3](#filepos294365)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 访问私有云存储桶需要设置云服务提供商的凭证。设置是特定于提供商的。AWS 通过用户特定的访问密钥和访问密钥进行用户身份验证。要访问私有 AWS S3 存储桶，需要创建用户访问密钥和密钥。[2](#filepos294020)
    相反，Google Cloud Platform (GCP) 通过服务帐号对用户进行身份验证。要访问私有 GCP 存储桶，需要创建具有访问存储桶权限的服务帐号文件。[3](#filepos294365)
- en: Ingesting Data Directly from Databases
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 直接从数据库摄取数据
- en: TFX provides two components to ingest datasets directly from databases. In the
    following sections, we introduce the `BigQueryExampleGen` component to query data
    from BigQuery tables and the `PrestoExampleGen` component to query data from Presto
    databases.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: TFX 提供了两个组件，直接从数据库摄取数据集。在以下部分，我们介绍了 `BigQueryExampleGen` 组件用于查询 BigQuery 表中的数据，以及
    `PrestoExampleGen` 组件用于查询 Presto 数据库中的数据。
- en: Google Cloud BigQuery
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud BigQuery
- en: TFX provides a component to ingest data from Google Cloud’s BigQuery tables.
    This is a very efficient way of ingesting structured data if we execute our machine
    learning pipelines in the GCP ecosystem.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: TFX 提供了一个组件，用于从 Google Cloud 的 BigQuery 表中摄取数据。如果我们在 GCP 生态系统中执行机器学习流水线，这是摄取结构化数据非常高效的方式。
- en: GOOGLE CLOUD CREDENTIALS
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: GOOGLE CLOUD 凭证
- en: Executing the `BigQueryExampleGen` component requires that we have set the necessary
    Google Cloud credentials in our local environment. We need to create a service
    account with the required roles (at least BigQuery Data Viewer and BigQuery Job
    User). If you execute the component in the interactive context with Apache Beam
    or Apache Airflow, you have to specify the path to the service account credential
    file through the environment variable `GOOGLE_APPLICATION_CREDENTIALS`, as shown
    in the following code snippet. If you execute the component through Kubeflow Pipelines,
    you can provide the service account information through OpFunc functions introduced
    at [“OpFunc Functions”](index_split_019.html#filepos1422198).
  id: totrans-88
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 执行 `BigQueryExampleGen` 组件需要在本地环境中设置必要的 Google Cloud 凭证。我们需要创建一个带有所需角色（至少是 BigQuery
    数据查看器和 BigQuery 作业用户）的服务账号。如果在交互式环境中使用 Apache Beam 或 Apache Airflow 执行组件，则必须通过环境变量
    `GOOGLE_APPLICATION_CREDENTIALS` 指定服务账号凭证文件的路径，如下面的代码片段所示。如果通过 Kubeflow Pipelines
    执行组件，可以通过引入的 OpFunc 函数提供服务账号信息，详见[“OpFunc 函数”](index_split_019.html#filepos1422198)。
- en: 'You can do this in Python with the following:'
  id: totrans-89
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 您可以在 Python 中使用以下方式实现：
- en: '`import``os``os``.``environ``[``"GOOGLE_APPLICATION_CREDENTIALS"``]``=``"/path/to/credential_file.json"`'
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`import``os``os``.``environ``[``"GOOGLE_APPLICATION_CREDENTIALS"``]``=``"/path/to/credential_file.json"`'
- en: For more details, see the [Google Cloud documentation](https://oreil.ly/EPEs3).
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参阅[Google Cloud 文档](https://oreil.ly/EPEs3)。
- en: 'The following example shows the simplest way of querying our BigQuery tables:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的示例展示了查询我们的 BigQuery 表的最简单方法：
- en: '`from``tfx.components``import``BigQueryExampleGen``query``=``"""[PRE0]"""``example_gen``=``BigQueryExampleGen``(``query``=``query``)`'
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`from``tfx.components``import``BigQueryExampleGen``query``=``"""[PRE0]"""``example_gen``=``BigQueryExampleGen``(``query``=``query``)`'
- en: Of course, we can create more complex queries to select our data, for example,
    joining multiple tables.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们可以创建更复杂的查询来选择我们的数据，例如，连接多个表。
- en: CHANGES TO THE BIGQUERYEXAMPLEGEN COMPONENT
  id: totrans-95
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: BIGQUERYEXAMPLEGEN 组件的更改
- en: 'In TFX versions greater than 0.22.0, the BigQueryExampleGen component needs
    to be imported from `tfx.extensions.google_cloud_big_query`:'
  id: totrans-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在 TFX 版本大于 0.22.0 时，需要从 `tfx.extensions.google_cloud_big_query` 导入 `BigQueryExampleGen`
    组件：
- en: '`from``tfx.extensions.google_cloud_big_query.example_gen` `\` `import``component``as``big_query_example_gen_component``big_query_example_gen_component``.``BigQueryExampleGen``(``query``=``query``)`'
  id: totrans-97
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`from``tfx.extensions.google_cloud_big_query.example_gen` `\` `import``component``as``big_query_example_gen_component``big_query_example_gen_component``.``BigQueryExampleGen``(``query``=``query``)`'
- en: Presto databases
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Presto 数据库
- en: 'If we want to ingest data from a Presto database, we can use `PrestoExampleGen`.
    The usage is very similar to `BigQueryExampleGen`, in which we defined a database
    query and then executed the query. The `PrestoExampleGen` component requires additional
    configuration to specify the database’s connection details:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要从 Presto 数据库摄取数据，可以使用 `PrestoExampleGen`。其用法与 `BigQueryExampleGen` 非常类似，我们定义了一个数据库查询，然后执行该查询。`PrestoExampleGen`
    组件需要额外的配置来指定数据库的连接详细信息：
- en: '`from``proto``import``presto_config_pb2``from``presto_component.component``import``PrestoExampleGen``query``=``"""[PRE1]"""``presto_config``=``presto_config_pb2``.``PrestoConnConfig``(``host``=``''localhost''``,``port``=``8080``)``example_gen``=``PrestoExampleGen``(``presto_config``,``query``=``query``)`'
  id: totrans-100
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`from``proto``import``presto_config_pb2``from``presto_component.component``import``PrestoExampleGen``query``=``"""[PRE1]"""``presto_config``=``presto_config_pb2``.``PrestoConnConfig``(``host``=``''localhost''``,``port``=``8080``)``example_gen``=``PrestoExampleGen``(``presto_config``,``query``=``query``)`'
- en: PRESTOEXAMPLEGEN REQUIRES SEPARATE INSTALLATION
  id: totrans-101
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: PRESTOEXAMPLEGEN 需要单独安装
- en: 'Since TFX version 0.22, the PrestoExampleGen requires a separate installation
    process. After installing the `protoc` compiler,[4](#filepos294727) you can install
    the component from source with the steps below:'
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 自TFX版本0.22以来，PrestoExampleGen需要单独的安装过程。安装`protoc`编译器后，[4](#filepos294727)您可以按以下步骤从源代码安装组件：
- en: '`$` `git clone git@github.com:tensorflow/tfx.git` `&&``cd` `tfx/` `$` `git
    checkout v0.22.0` `$` `cd` `tfx/examples/custom_components/presto_example_gen`
    `$` `pip install -e .`'
  id: totrans-103
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `git clone git@github.com:tensorflow/tfx.git` `&&``cd` `tfx/` `$` `git
    checkout v0.22.0` `$` `cd` `tfx/examples/custom_components/presto_example_gen`
    `$` `pip install -e .`'
- en: After the installation, you will be able to import the PrestoExampleGen component
    and its protocol buffer definitions.
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 安装完成后，您将能够导入PrestoExampleGen组件及其协议缓冲区定义。
- en: Data Preparation
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备
- en: Each of the introduced `ExampleGen` components allows us to configure input
    settings (`input_config`) and output settings (`output_config`) for our dataset.
    If we would like to ingest datasets incrementally, we can define a span as the
    input configuration. At the same time, we can configure how the data should be
    split. Often we would like to generate a training set together with an evaluation
    and test set. We can define the details with the output configuration.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 每个引入的`ExampleGen`组件都允许我们配置数据集的输入设置（`input_config`）和输出设置（`output_config`）。如果我们想增量摄入数据集，可以将一个时间跨度定义为输入配置。同时，我们可以配置数据应如何拆分。通常，我们希望生成一个训练集以及一个评估和测试集。我们可以通过输出配置定义详细信息。
- en: Splitting Datasets
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 分割数据集
- en: Later in our pipeline, we will want to evaluate our machine learning model during
    the training and test it during the model analysis step. Therefore, it is beneficial
    to split the dataset into the required subsets.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的管道中稍后，我们希望在训练期间评估我们的机器学习模型，并在模型分析步骤中测试它。因此，将数据集拆分为所需的子集是有益的。
- en: Splitting one dataset into subsets
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个数据集拆分成子集
- en: 'The following example shows how we can extend our data ingestion by requiring
    a three-way split: training, evaluation, and test sets with a ratio of 6:2:2\.
    The ratio settings are defined through the `hash_buckets`:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例显示了如何通过需要三向拆分来扩展我们的数据摄入：训练、评估和测试集，比例为6:2:2。比例设置通过`hash_buckets`定义：
- en: '`from``tfx.components``import``CsvExampleGen``from``tfx.proto``import``example_gen_pb2``from``tfx.utils.dsl_utils``import``external_input``base_dir``=``os``.``getcwd``()``data_dir``=``os``.``path``.``join``(``os``.``pardir``,``"data"``)``output``=``example_gen_pb2``.``Output``(``split_config``=``example_gen_pb2``.``SplitConfig``(``splits``=``[`![](images/00002.jpg)`example_gen_pb2``.``SplitConfig``.``Split``(``name``=``''train''``,``hash_buckets``=``6``),`![](images/00075.jpg)`example_gen_pb2``.``SplitConfig``.``Split``(``name``=``''eval''``,``hash_buckets``=``2``),``example_gen_pb2``.``SplitConfig``.``Split``(``name``=``''test''``,``hash_buckets``=``2``)``]))``examples``=``external_input``(``os``.``path``.``join``(``base_dir``,``data_dir``))``example_gen``=``CsvExampleGen``(``input``=``examples``,``output_config``=``output``)`![](images/00064.jpg)`context``.``run``(``example_gen``)`'
  id: totrans-111
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`from``tfx.components``import``CsvExampleGen``from``tfx.proto``import``example_gen_pb2``from``tfx.utils.dsl_utils``import``external_input``base_dir``=``os``.``getcwd``()``data_dir``=``os``.``path``.``join``(``os``.``pardir``,``"data"``)``output``=``example_gen_pb2``.``Output``(``split_config``=``example_gen_pb2``.``SplitConfig``(``splits``=``[`![](images/00002.jpg)`example_gen_pb2``.``SplitConfig``.``Split``(``name``=``''train''``,``hash_buckets``=``6``),`![](images/00075.jpg)`example_gen_pb2``.``SplitConfig``.``Split``(``name``=``''eval''``,``hash_buckets``=``2``),``example_gen_pb2``.``SplitConfig``.``Split``(``name``=``''test''``,``hash_buckets``=``2``)``]))``examples``=``external_input``(``os``.``path``.``join``(``base_dir``,``data_dir``))``example_gen``=``CsvExampleGen``(``input``=``examples``,``output_config``=``output``)`![](images/00064.jpg)`context``.``run``(``example_gen``)`'
- en: '![](images/00002.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: Define preferred splits.
  id: totrans-113
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 定义首选拆分。
- en: '![](images/00075.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00075.jpg)'
- en: Specify the ratio.
  id: totrans-115
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 指定比例。
- en: '![](images/00064.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00064.jpg)'
- en: Add `output_config` argument.
  id: totrans-117
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 添加`output_config`参数。
- en: 'After the execution of the `example_gen` object, we can inspect the generated
    artifacts by printing the list of the artifacts:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行`example_gen`对象后，我们可以通过打印生成的艺术品列表来检查生成的艺术品：
- en: '`for``artifact``in``example_gen``.``outputs``[``''examples''``]``.``get``():``print``(``artifact``)``Artifact``(``type_name``:``ExamplesPath``,``uri``:``/``path``/``to``/``CsvExampleGen``/``examples``/``1``/``train``/``,``split``:``train``,``id``:``2``)``Artifact``(``type_name``:``ExamplesPath``,``uri``:``/``path``/``to``/``CsvExampleGen``/``examples``/``1``/``eval``/``,``split``:``eval``,``id``:``3``)``Artifact``(``type_name``:``ExamplesPath``,``uri``:``/``path``/``to``/``CsvExampleGen``/``examples``/``1``/``test``/``,``split``:``test``,``id``:``4``)`'
  id: totrans-119
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`for``artifact``in``example_gen``.``outputs``[``''examples''``]``.``get``():``print``(``artifact``)``Artifact``(``type_name``:``ExamplesPath``,``uri``:``/``path``/``to``/``CsvExampleGen``/``examples``/``1``/``train``/``,``split``:``train``,``id``:``2``)``Artifact``(``type_name``:``ExamplesPath``,``uri``:``/``path``/``to``/``CsvExampleGen``/``examples``/``1``/``eval``/``,``split``:``eval``,``id``:``3``)``Artifact``(``type_name``:``ExamplesPath``,``uri``:``/``path``/``to``/``CsvExampleGen``/``examples``/``1``/``test``/``,``split``:``test``,``id``:``4``)`'
- en: In the following chapter, we will discuss how we investigate the produced datasets
    for our data pipeline.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将讨论如何调查我们数据管道中产生的数据集。
- en: DEFAULT SPLITS
  id: totrans-121
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 默认拆分
- en: If we don’t specify any output configuration, the `ExampleGen` component splits
    the dataset into a training and evaluation split with a ratio of 2:1 by default.
  id: totrans-122
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果我们没有指定任何输出配置，`ExampleGen` 组件默认将数据集拆分为训练集和评估集，比例为 2:1。
- en: Preserving existing splits
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 保留现有的拆分
- en: In some situations, we have already generated the subsets of the datasets externally,
    and we would like to preserve these splits when we ingest the datasets. We can
    achieve this by providing an input configuration.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们已经在外部生成了数据集的子集，并且在摄取数据集时希望保留这些拆分。我们可以通过提供输入配置来实现这一点。
- en: 'For the following configuration, let’s assume that our dataset has been split
    externally and saved in subdirectories:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以下配置，让我们假设我们的数据集已经被外部拆分并保存在子目录中：
- en: '`└── data    ├── train     │   └─ 20k-consumer-complaints-training.csv    
    ├──` `eval` `│   └─ 4k-consumer-complaints-eval.csv     └──` `test` `└─ 2k-consumer-complaints-test.csv`'
  id: totrans-126
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`└── data    ├── train     │   └─ 20k-consumer-complaints-training.csv    
    ├──` `eval` `│   └─ 4k-consumer-complaints-eval.csv     └──` `test` `└─ 2k-consumer-complaints-test.csv`'
- en: 'We can preserve the existing input split by defining this input configuration:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过定义以下输入配置来保留现有的输入拆分：
- en: '`import``os``from``tfx.components``import``CsvExampleGen``from``tfx.proto``import``example_gen_pb2``from``tfx.utils.dsl_utils``import``external_input``base_dir``=``os``.``getcwd``()``data_dir``=``os``.``path``.``join``(``os``.``pardir``,``"data"``)``input``=``example_gen_pb2``.``Input``(``splits``=``[``example_gen_pb2``.``Input``.``Split``(``name``=``''train''``,``pattern``=``''train/*''``),`![](images/00002.jpg)`example_gen_pb2``.``Input``.``Split``(``name``=``''eval''``,``pattern``=``''eval/*''``),``example_gen_pb2``.``Input``.``Split``(``name``=``''test''``,``pattern``=``''test/*''``)``])``examples``=``external_input``(``os``.``path``.``join``(``base_dir``,``data_dir``))``example_gen``=``CsvExampleGen``(``input``=``examples``,``input_config``=``input``)`![](images/00075.jpg)'
  id: totrans-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`import``os``from``tfx.components``import``CsvExampleGen``from``tfx.proto``import``example_gen_pb2``from``tfx.utils.dsl_utils``import``external_input``base_dir``=``os``.``getcwd``()``data_dir``=``os``.``path``.``join``(``os``.``pardir``,``"data"``)``input``=``example_gen_pb2``.``Input``(``splits``=``[``example_gen_pb2``.``Input``.``Split``(``name``=``''train''``,``pattern``=``''train/*''``),`![](images/00002.jpg)`example_gen_pb2``.``Input``.``Split``(``name``=``''eval''``,``pattern``=``''eval/*''``),``example_gen_pb2``.``Input``.``Split``(``name``=``''test''``,``pattern``=``''test/*''``)``])``examples``=``external_input``(``os``.``path``.``join``(``base_dir``,``data_dir``))``example_gen``=``CsvExampleGen``(``input``=``examples``,``input_config``=``input``)`![](images/00075.jpg)'
- en: '![](images/00002.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: Set existing subdirectories.
  id: totrans-130
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 设置现有子目录。
- en: '![](images/00075.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00075.jpg)'
- en: Add the `input_config` argument.
  id: totrans-132
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 添加 `input_config` 参数。
- en: After defining the input configuration, we can pass the settings to the `ExampleGen`
    component by defining the `input_config` argument.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了输入配置后，我们可以通过定义 `input_config` 参数将设置传递给 `ExampleGen` 组件。
- en: Spanning Datasets
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集跨度
- en: One of the significant use cases for machine learning pipelines is that we can
    update our machine learning models when new data becomes available. For this scenario,
    the `ExampleGen` component allows us to use spans. Think of a span as a snapshot
    of data. Every hour, day, or week, a batch extract, transform, load (ETL) process
    could make such a data snapshot and create a new span.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习流水线的一个重要用例是当新数据可用时，我们可以更新我们的机器学习模型。对于这种情况，`ExampleGen` 组件允许我们使用 spans。将
    span 视为数据的快照。每小时、每天或每周，批量提取、转换、加载 (ETL) 过程可以生成这样的数据快照并创建一个新的 span。
- en: 'A span can replicate the existing data records. As shown in the following,
    export-1 contains the data from the previous export-0 as well as newly created
    records that were added since the export-0 export:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 跨度可以复制现有的数据记录。如下所示，`export-1`包含了前一个`export-0`的数据，以及自`export-0`导出后新增的记录：
- en: '`└── data     ├──` `export``-0     │   └─ 20k-consumer-complaints.csv     ├──`
    `export``-1     │   └─ 24k-consumer-complaints.csv     └──` `export``-2        
    └─ 26k-consumer-complaints.csv`'
  id: totrans-137
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`└── data     ├──` `export``-0     │   └─ 20k-consumer-complaints.csv     ├──`
    `export``-1     │   └─ 24k-consumer-complaints.csv     └──` `export``-2        
    └─ 26k-consumer-complaints.csv`'
- en: 'We can now specify the patterns of the spans. The input configuration accepts
    a `{SPAN}` placeholder, which represents the number (0, 1, 2, …) shown in our
    folder structure. With the input configuration, the `ExampleGen` component now
    picks up the “latest” span. In our example, this would be the data available under
    folder export-2:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以指定跨度的模式。输入配置接受一个`{SPAN}`占位符，表示在我们的文件夹结构中显示的数字（0、1、2等）。通过输入配置，`ExampleGen`组件现在选择“最新”的跨度。在我们的示例中，这将是文件夹`export-2`下的可用数据：
- en: '`from``tfx.components``import``CsvExampleGen``from``tfx.proto``import``example_gen_pb2``from``tfx.utils.dsl_utils``import``external_input``base_dir``=``os``.``getcwd``()``data_dir``=``os``.``path``.``join``(``os``.``pardir``,``"data"``)``input``=``example_gen_pb2``.``Input``(``splits``=``[``example_gen_pb2``.``Input``.``Split``(``pattern``=``''export-{SPAN}/*''``)``])``examples``=``external_input``(``os``.``path``.``join``(``base_dir``,``data_dir``))``example_gen``=``CsvExampleGen``(``input``=``examples``,``input_config``=``input``)``context``.``run``(``example_gen``)`'
  id: totrans-139
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`from``tfx.components``import``CsvExampleGen``from``tfx.proto``import``example_gen_pb2``from``tfx.utils.dsl_utils``import``external_input``base_dir``=``os``.``getcwd``()``data_dir``=``os``.``path``.``join``(``os``.``pardir``,``"data"``)``input``=``example_gen_pb2``.``Input``(``splits``=``[``example_gen_pb2``.``Input``.``Split``(``pattern``=``''export-{SPAN}/*''``)``])``examples``=``external_input``(``os``.``path``.``join``(``base_dir``,``data_dir``))``example_gen``=``CsvExampleGen``(``input``=``examples``,``input_config``=``input``)``context``.``run``(``example_gen``)`'
- en: 'Of course, the input definitions can also define subdirectories if the data
    is already split:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果数据已经分割，输入定义也可以定义子目录：
- en: '`input``=``example_gen_pb2``.``Input``(``splits``=``[``example_gen_pb2``.``Input``.``Split``(``name``=``''train''``,``pattern``=``''export-{SPAN}/train/*''``),``example_gen_pb2``.``Input``.``Split``(``name``=``''eval''``,``pattern``=``''export-{SPAN}/eval/*''``)``])`'
  id: totrans-141
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`input``=``example_gen_pb2``.``Input``(``splits``=``[``example_gen_pb2``.``Input``.``Split``(``name``=``''train''``,``pattern``=``''export-{SPAN}/train/*''``),``example_gen_pb2``.``Input``.``Split``(``name``=``''eval''``,``pattern``=``''export-{SPAN}/eval/*''``)``])`'
- en: Versioning Datasets
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集版本化
- en: In machine learning pipelines, we want to track the produced models together
    with the used datasets, which were used to train the machine learning model. To
    do this, it is useful to version our datasets.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习流水线中，我们希望跟踪生成的模型以及用于训练机器学习模型的数据集。为了做到这一点，版本化我们的数据集非常有用。
- en: Data versioning allows us to track the ingested data in more detail. This means
    that we not only store the file name and path of the ingested data in the ML MetadataStore
    (because it’s currently supported by the TFX components) but also that we track
    more metainformation about the raw dataset, such as a hash of the ingested data.
    Such version tracking would allow us to verify that the dataset used during the
    training is still the dataset at a later point in time. Such a feature is critical
    for end-to-end ML reproducibility.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 数据版本控制允许我们更详细地跟踪摄入的数据。这意味着我们不仅在ML MetadataStore中存储摄入数据的文件名和路径（因为当前由TFX组件支持），还跟踪关于原始数据集的更多元信息，例如摄入数据的哈希值。这样的版本跟踪可以确保在训练期间使用的数据集在稍后的时间点仍然是相同的数据集。这种功能对端到端ML可重现性至关重要。
- en: However, such a feature is currently not supported by the TFX `ExampleGen` component.
    If you would like to version your datasets, you can use third-party data versioning
    tools and version the data before the datasets are ingested into the pipeline.
    Unfortunately, none of the available tools will write the metadata information
    to the TFX ML MetadataStore directly.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，TFX的`ExampleGen`组件目前不支持此功能。如果您希望对数据集进行版本化，可以使用第三方数据版本控制工具，并在数据集摄入流水线之前对数据进行版本化。不幸的是，目前没有可用的工具会直接将元数据信息写入TFX的ML
    MetadataStore中。
- en: 'If you would like to version your datasets, you can use one of the following
    tools:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望对数据集进行版本控制，可以使用以下其中之一的工具：
- en: '[Data Version Control (DVC)](https://dvc.org)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[数据版本控制（DVC）](https://dvc.org)'
- en: DVC is an open source version control system for machine learning projects.
    It lets you commit hashes of your datasets instead of the entire dataset itself.
    Therefore, the state of the dataset is tracked (e.g., via `git`), but the repository
    isn’t cluttered with the entire dataset.
  id: totrans-148
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: DVC是一个用于机器学习项目的开源版本控制系统。它允许您提交数据集的哈希而不是整个数据集本身。因此，数据集的状态被跟踪（例如通过`git`），但存储库不会因整个数据集而杂乱。
- en: '[Pachyderm](https://www.pachyderm.com)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[厚皮象](https://www.pachyderm.com)'
- en: Pachyderm is an open source machine learning platform running on Kubernetes.
    It originated with the concept of versioning for data (“Git for data”) but has
    now expanded into an entire data platform, including pipeline orchestration based
    on data versions.
  id: totrans-150
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Pachyderm是一个运行在Kubernetes上的开源机器学习平台。它最初是基于数据版本控制概念而起的（“数据的Git”），但现在已扩展为包括基于数据版本的流水线编排在内的整个数据平台。
- en: Ingestion Strategies
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 注入策略
- en: 'So far, we have discussed a variety of ways to ingest data into our machine
    learning pipelines. If you are starting with an entirely new project, it might
    be overwhelming to choose the right data ingestion strategy. In the following
    sections, we will provide a few suggestions for three data types: structured,
    text, and image data.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了多种将数据注入到机器学习流水线中的方法。如果您从头开始一个全新的项目，选择合适的数据注入策略可能会让人感到不知所措。在接下来的章节中，我们将为三种数据类型（结构化数据、文本数据和图像数据）提供一些建议。
- en: Structured Data
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化数据
- en: Structured data is often stored in a database or on a disk in file format, supporting
    tabular data. If the data exists in a database, we can either export it to CSVs
    or consume the data directly with the `PrestoExampleGen` or the `BigQueryExampleGen`
    components (if the services are available).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化数据通常存储在数据库中或以支持表格数据的文件格式存储在磁盘上。如果数据存在于数据库中，我们可以将其导出为CSV，或直接使用`PrestoExampleGen`或`BigQueryExampleGen`组件（如果服务可用）消耗数据。
- en: Data available on a disk stored in file formats supporting tabular data should
    be converted to CSVs and ingested into the pipeline with the `CsvExampleGen` component.
    Should the amount of data grow beyond a few hundred megabytes, you should consider
    converting the data into TFRecord files or store the data with Apache Parquet.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 存储在支持表格数据的文件格式中的磁盘上的数据应转换为CSV，并使用`CsvExampleGen`组件将其注入到流水线中。如果数据量超过几百兆字节，应考虑将数据转换为TFRecord文件或使用Apache
    Parquet存储数据。
- en: Text Data for Natural Language Problems
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言问题的文本数据
- en: Text corpora can snowball to a considerable size. To ingest such datasets efficiently,
    we recommend converting the datasets to TFRecord or Apache Parquet representations.
    Using performant data file types allows an efficient and incremental loading of
    the corpus documents. The ingestion of the corpora from a database is also possible;
    however, we recommend considering network traffic costs and bottlenecks.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 文本语料库的大小可能会迅速增加。为了有效地注入这些数据集，我们建议将数据集转换为TFRecord或Apache Parquet表示形式。使用性能良好的数据文件类型可以实现语料库文档的高效和增量加载。也可以从数据库中注入语料库，但建议考虑网络流量成本和瓶颈问题。
- en: Image Data for Computer Vision Problems
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉问题的图像数据
- en: 'We recommend converting image datasets from the image files to TFRecord files,
    but not to decode the images. Any decoding of highly compressed images only increases
    the amount of disk space needed to store the intermediate `tf.Example` records.
    The compressed images can be stored in `tf.Example` records as byte strings:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐将图像数据集从图像文件转换为TFRecord文件，但不要解码图像。对高度压缩的图像进行解码只会增加存储中间`tf.Example`记录所需的磁盘空间。压缩图像可以作为字节字符串存储在`tf.Example`记录中：
- en: '`import``tensorflow``as``tf``base_path``=``"/path/to/images"``filenames``=``os``.``listdir``(``base_path``)``def``generate_label_from_path``(``image_path``):``...``return``label``def``_bytes_feature``(``value``):``return``tf``.``train``.``Feature``(``bytes_list``=``tf``.``train``.``BytesList``(``value``=``[``value``]))``def``_int64_feature``(``value``):``return``tf``.``train``.``Feature``(``int64_list``=``tf``.``train``.``Int64List``(``value``=``[``value``]))``tfrecord_filename``=``''data/image_dataset.tfrecord''``with``tf``.``io``.``TFRecordWriter``(``tfrecord_filename``)``as``writer``:``for``img_path``in``filenames``:``image_path``=``os``.``path``.``join``(``base_path``,``img_path``)``try``:``raw_file``=``tf``.``io``.``read_file``(``image_path``)``except``FileNotFoundError``:``print``(``"File
    {} could not be found"``.``format``(``image_path``))``continue``example``=``tf``.``train``.``Example``(``features``=``tf``.``train``.``Features``(``feature``=``{``''image_raw''``:``_bytes_feature``(``raw_file``.``numpy``()),``''label''``:``_int64_feature``(``generate_label_from_path``(``image_path``))``}))``writer``.``write``(``example``.``SerializeToString``())`'
  id: totrans-160
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`import``tensorflow``as``tf``base_path``=``"/path/to/images"``filenames``=``os``.``listdir``(``base_path``)``def``generate_label_from_path``(``image_path``):``...``return``label``def``_bytes_feature``(``value``):``return``tf``.``train``.``Feature``(``bytes_list``=``tf``.``train``.``BytesList``(``value``=``[``value``]))``def``_int64_feature``(``value``):``return``tf``.``train``.``Feature``(``int64_list``=``tf``.``train``.``Int64List``(``value``=``[``value``]))``tfrecord_filename``=``''data/image_dataset.tfrecord''``with``tf``.``io``.``TFRecordWriter``(``tfrecord_filename``)``as``writer``:``for``img_path``in``filenames``:``image_path``=``os``.``path``.``join``(``base_path``,``img_path``)``try``:``raw_file``=``tf``.``io``.``read_file``(``image_path``)``except``FileNotFoundError``:``print``(``"File
    {} could not be found"``.``format``(``image_path``))``continue``example``=``tf``.``train``.``Example``(``features``=``tf``.``train``.``Features``(``feature``=``{``''image_raw''``:``_bytes_feature``(``raw_file``.``numpy``()),``''label''``:``_int64_feature``(``generate_label_from_path``(``image_path``))``}))``writer``.``write``(``example``.``SerializeToString``())`'
- en: The example code reads images from a provided path /path/to/images and stores
    the image as byte strings in the `tf.Example`. We aren’t preprocessing our images
    at this point in the pipeline. Even though we might save a considerable amount
    of disk space, we want to perform these tasks later in our pipeline. Avoiding
    the preprocessing at this point helps us to prevent bugs and potential training/serving
    skew later on.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 示例代码从提供的路径 `/path/to/images` 读取图像，并将图像存储为字节字符串放入 `tf.Example` 中。我们当前阶段不对图像进行预处理。尽管这样可能节省大量磁盘空间，但我们希望稍后在管道中执行这些任务。当前阶段避免预处理有助于避免后续训练/服务偏差的错误。
- en: We store the raw image together with labels in the `tf.Examples`. We derive
    the label for each image from the file name with the function `generate_label_from_path`
    in our example. The label generation is dataset specific; therefore, we haven’t
    included it in this example.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 `tf.Examples` 中存储原始图像及其标签。我们通过示例中的 `generate_label_from_path` 函数从文件名中派生每个图像的标签。由于标签生成是特定于数据集的，因此我们没有在此示例中包含它。
- en: After converting the images to TFRecord files, we can consume the datasets efficiently
    with the `ImportExampleGen` component and apply the same strategies we discussed
    in [“Importing existing TFRecord Files”](#filepos173622).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 将图像转换为 TFRecord 文件后，我们可以使用 `ImportExampleGen` 组件高效地消费数据集，并应用我们在[“导入现有 TFRecord
    文件”](#filepos173622)中讨论的相同策略。
- en: Summary
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed various ways of ingesting data into our machine
    learning pipeline. We highlighted the consumption of datasets stored on a disk
    as well as in databases. In the process, we also discussed that ingested data
    records, which are converted to `tf.Example` (store in TFRecord files) for the
    consumption of the downstream components.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了将数据输入到机器学习管道中的各种方法。我们强调了从磁盘和数据库中存储的数据集的消费。在此过程中，我们还讨论了转换为 `tf.Example`（存储在
    TFRecord 文件中）的摄取数据记录，以便下游组件消费。
- en: In the following chapter, we will take a look at how we can consume the generated
    `tf.Example` records in our data validation step of the pipeline.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将看看如何在管道的数据验证步骤中消费生成的 `tf.Example` 记录。
- en: '[1  ](#filepos216082) Reading files from AWS S3 requires Apache Beam 2.19 or
    higher, which is supported since TFX version 0.22.'
  id: totrans-167
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[1  ](#filepos216082) 从 AWS S3 读取文件需要 Apache Beam 2.19 或更高版本，这是 TFX 版本 0.22
    开始支持的。'
- en: '[2  ](#filepos217805) See the documentation for more details on [managing AWS
    Access Keys](https://oreil.ly/Dow7L).'
  id: totrans-168
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[2  ](#filepos217805) 有关如何管理 AWS 访问密钥的更多详细信息，请参阅[文档](https://oreil.ly/Dow7L)。'
- en: '[3  ](#filepos218126) See the documentation for more details on [how to create
    and manage service accounts](https://oreil.ly/6y8WX).'
  id: totrans-169
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[3  ](#filepos218126) 有关如何创建和管理服务帐户的更多详细信息，请参阅[文档](https://oreil.ly/6y8WX)。'
- en: '[4  ](#filepos229210) Visit the proto-lens GitHub for [details on the `protoc`
    installation](https://oreil.ly/h6FtO).'
  id: totrans-170
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[4  ](#filepos229210) 访问 proto-lens GitHub 了解 [关于 `protoc` 安装的详细信息](https://oreil.ly/h6FtO)。'
