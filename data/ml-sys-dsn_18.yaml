- en: 15 Serving and inference optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 15 服务和推理优化
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Challenges that may arise during the serving and inference stage
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在服务阶段和推理阶段可能出现的挑战
- en: Tools and frameworks that will come in handy
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将派上用场的工具和框架
- en: Optimizing inference pipelines
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化推理管道
- en: Making your machine learning (ML) model run in a production environment is among
    the final steps required for reaching an efficient operating lifecycle of your
    system. Some ML practitioners demonstrate low interest in this aspect of the craft,
    preferring instead to focus on developing and training their models. This might
    be a false move, however, as the model can only be useful if it’s deployed and
    effectively utilized in production. In this chapter, we discuss the challenges
    of deploying and serving ML models, as well as review different methods of optimizing
    the inference process.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中运行你的机器学习（ML）模型是达到系统高效运行生命周期所需的最后一步。一些机器学习从业者对此方面表现出较低的兴趣，更愿意专注于模型开发和训练。然而，这可能是一个错误的决定，因为模型只有在部署并有效利用于生产中时才能发挥作用。在本章中，我们讨论了部署和推理机器学习模型的挑战，以及回顾了优化推理过程的不同方法。
- en: As we mentioned in chapter 10, an inference pipeline is a sequence (in most
    cases) or a more complicated acyclic graph of steps that takes raw data as input
    and produces predictions as output. Along with the ML model itself, the inference
    pipeline includes steps like feature computation, data preprocessing, output postprocessing,
    and others. Preprocessing and postprocessing are somewhat generic terms, as they
    can be built differently depending on the specific requirements of a given system.
    Let’s take a few examples. A typical computer vision pipeline often starts with
    image resizing and normalization; a typical language processing pipeline, in turn,
    initiates with tokenization, while a typical recommender system pipeline kicks
    off by pulling user features from a feature store.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第10章中提到的，推理管道是一个序列（在大多数情况下）或更复杂的无环图，它以原始数据为输入，产生预测作为输出。除了ML模型本身外，推理管道还包括特征计算、数据预处理、输出后处理等步骤。预处理和后处理是有些通用的术语，因为它们可以根据特定系统的具体要求以不同的方式构建。让我们举几个例子。典型的计算机视觉管道通常从图像缩放和归一化开始；典型的语言处理管道，反过来，从标记化开始；而典型的推荐系统管道则从从特征存储中提取用户特征开始。
- en: The role of properly tuned inference may vary from domain to domain as well.
    High-frequency trading companies or adtech businesses are on a constant hunt for
    the brightest talents to refine their systems of extremely low latency. Mobile
    app and Internet of Things (IoT) developers put efficient inference at the top
    of their priority list, chasing more efficient battery consumption and thus improved
    user experience. Those who utilize high-load backend in their products are interested
    in keeping the load without spending a fortune on the infrastructure. At the same
    time, there are many scenarios when the model prediction is not a bottleneck.
    For example, once per week, we need to run a batch job to predict the next week’s
    sales, generate the report, and distribute it to the procurement department. In
    those cases, if the report is required to be in corporate emails by Monday morning,
    it doesn’t make much of a difference if it will take 10 minutes or 3 hours to
    compile, as long as it’s scheduled for Sunday night.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 正确调整推理的作用可能因领域而异。高频交易公司或广告技术企业一直在寻找最优秀的人才以优化其极低延迟的系统。移动应用和物联网（IoT）开发者将高效的推理置于优先事项之首，追求更高效的电池消耗，从而改善用户体验。那些在其产品中使用高负载后端的人感兴趣的是在不花费大量资金的情况下保持负载。同时，有许多场景下模型预测并不是瓶颈。例如，每周一次，我们需要运行一个批量作业来预测下周的销售情况，生成报告，并将其分发给采购部门。在这些情况下，如果报告需要在周一早上通过公司电子邮件发送，那么它是在周日晚上编译需要10分钟还是3小时，并没有太大的区别，只要它被安排在周日晚上即可。
- en: In general, this chapter is mostly focused on deep learning-based systems. And
    it shouldn’t come as a surprise, as heavy models require more engineering efforts
    before production deployment, while serving some lightweight solution like a logistic
    regression or a small tree ensemble is not too complicated as long as your feature
    infrastructure (described in chapter 11) is in place. At the same time, some principles
    and techniques described here apply to any ML system.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，本章主要关注基于深度学习的系统。这并不令人惊讶，因为重型模型在生产部署前需要更多的工程努力，而只要你的特征基础设施（在第11章中描述）到位，提供一些轻量级解决方案，如逻辑回归或小型树集成，并不会太复杂。同时，这里描述的一些原则和技术适用于任何机器学习系统。
- en: '15.1 Serving and inference: Challenges'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.1 服务和推理：挑战
- en: 'As always happens in system design, our first step lies in defining the requirements.
    There are several crucial factors to keep in mind, and we touch on each of them
    next:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在系统设计中经常发生的那样，我们的第一步在于定义需求。有几个关键因素需要牢记，我们将在下面逐一讨论：
- en: '*Latency**—*This defines our expectations of how prompt the system is in providing
    predictions. For real-time applications that require an immediate response, latency
    is generally measured in milliseconds (there are extreme cases where even 1 ms
    is too long!), while in some scenarios, waiting for hours or even days is totally
    acceptable.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*延迟**—*这定义了我们对系统在提供预测方面的响应速度的期望。对于需要即时响应的实时应用，延迟通常以毫秒计算（在某些极端情况下，即使1毫秒也太长了！），而在某些场景中，等待数小时甚至数天是完全可接受的。'
- en: '*Throughput**—*This refers to the number of tasks or amount of data that a
    system can process in a given time frame. In the world of ML, it implies the total
    number of predictions the model can produce per unit of time. Optimizing for throughput
    is often crucial in cases when you need to process large volumes of data over
    a short period, such as batch processing of large datasets.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*吞吐量**—*这指的是系统在给定时间内可以处理的任务数量或数据量。在机器学习的世界里，它意味着模型每单位时间可以产生的预测总数。在需要短时间内处理大量数据的场景中，如大型数据集的批量处理，优化吞吐量通常至关重要。'
- en: '*Scalability**—*We need to understand the number of predictions the system
    may face and how this number can either increase or decrease. Load patterns are
    often seasonal and may vary depending on the industry. For retail, it is common
    to see a spike in sales during the holiday season or on days of huge discounts
    such as Black Friday or Cyber Monday. For the adtech industry, the load may be
    higher due to higher activity on the internet. While some spikes, as mentioned
    here, are predictable, others can come out of the blue; the viral popularity of
    an app or a sudden usage burst by a large customer is not something we can prepare
    for in advance. The system should possess sufficient scalability to handle the
    peak load without compromising latency and throughput.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可扩展性**—*我们需要了解系统可能面临的预测数量以及这个数量如何增加或减少。负载模式通常是季节性的，可能因行业而异。对于零售业，在假日季节或像黑色星期五或网络星期一这样的巨大折扣日，通常会看到销售额的激增。对于广告技术行业，由于互联网上的活动增加，负载可能会更高。虽然这里提到的某些峰值是可以预测的，但其他一些可能会突然出现；一个应用程序的病毒式流行或一个大客户的突然使用激增是我们无法提前准备的。系统应具备足够的可扩展性，以处理峰值负载，而不会降低延迟和吞吐量。'
- en: '*Target platforms—*Your models can run on CPU-only or GPU-accelerated servers;
    in serverless environments like AWS Lambda or Cloudflare Workers; on desktop,
    mobile, or IoT devices; or even in the browser. Along with advantages, each platform
    has its own limitations and requirements, and we must have a deep understanding
    of those before building the system. If we are building a mobile app, we’ll need
    to consider the model’s size, as it should be small enough to fit into the app
    package yet efficient enough to run on a user’s device without draining the battery.
    If we are developing a backend system, there’s more freedom in choosing the hardware,
    which provides a vast array of opportunities in terms of the system’s complexity
    and size. If our target platform is exotic IoT hardware, there is a chance we
    can’t design a fancy model with all the bells and whistles, so sticking to the
    simplest architectures becomes the major technical requirement.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*目标平台—*您的模型可以在仅CPU或GPU加速的服务器上运行；在AWS Lambda或Cloudflare Workers等无服务器环境中；在桌面、移动或物联网设备上；甚至可以在浏览器中运行。除了优势之外，每个平台都有其自身的局限性和要求，在构建系统之前，我们必须对这些有深刻的理解。如果我们正在构建移动应用，我们需要考虑模型的大小，因为它应该足够小，可以放入应用包中，同时足够高效，可以在用户的设备上运行而不耗尽电池。如果我们正在开发后端系统，在硬件选择上我们有更多的自由度，这为系统的复杂性和规模提供了大量的机会。如果我们目标平台是异构的物联网硬件，我们可能无法设计一个功能齐全的复杂模型，因此坚持最简单的架构成为主要的技术要求。'
- en: 'Things can get more complicated when we have to use more than one platform
    for our product. In certain cases, we may decide to run small batches on user
    devices and send the rest to the backend or load the backend with finetuning before
    sending the model to a user’s device for inference. This way, the combined requirements
    of both platforms are followed. Furthermore, we may need to use various computing
    units within a single platform. Arseny once needed to speed up a system that ran
    on devices with low-end GPUs. Under the hood, the system used a small model to
    process multiple concurrent requests, which led to those cheap GPUs failing to
    handle the load. The solution Arseny came up with introduced a pool of mixed-device
    sessions: every time the GPU was overloaded, the following request was processed
    by a CPU, thus providing a more balanced load across devices and meeting the latency
    requirements.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们必须为我们的产品使用多个平台时，事情可能会变得更加复杂。在某些情况下，我们可能会决定在用户设备上运行小批量，其余的发送到后端，或者在将模型发送到用户设备进行推理之前，在后台进行微调。这样，就遵循了两个平台的综合要求。此外，我们可能需要在单个平台上使用各种计算单元。阿列克谢曾经需要加速一个运行在低端GPU设备上的系统。在底层，该系统使用一个小模型来处理多个并发请求，这导致那些廉价的GPU无法处理负载。阿列克谢提出的解决方案引入了一个混合设备会话池：每次GPU过载时，下一个请求由CPU处理，从而在设备之间提供更平衡的负载，并满足延迟要求。
- en: '*Cost—*ML systems are often the most resource-intensive solutions in a company,
    and with heavy generative models gaining more and more popularity, costs are standing
    out as a crucial factor like never before. With the cost of an ML infrastructure
    becoming an ever-growing concern, businesses are forced to look for smart decisions
    for inference pipelines, which can lead to massive financial benefits. It is important
    to understand the cost of the infrastructure and how it will scale based on an
    increasing load before building the system. It may even lead to scenarios when
    the infrastructure cost ends up being higher than the revenue generated by the
    system. In other cases, it will not be a concern if inference is performed on
    a user’s device. For mobile apps or IoT devices, for example, a growing user base
    will not affect the infrastructure cost in any significant manner (we can’t claim
    it will not affect it at all; if users download the models from your content delivery
    network, every new 1,000 users will cost you extra cents).'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*成本—*机器学习系统通常是公司中最资源密集型的解决方案，随着重型生成模型越来越受欢迎，成本已经成为一个至关重要的因素，就像以前从未出现过一样。随着机器学习基础设施成本成为一个不断增长的关注点，企业被迫寻找智能的推理管道决策，这可以带来巨大的财务效益。在构建系统之前，了解基础设施的成本以及它将如何根据增加的负载进行扩展是很重要的。甚至可能导致基础设施成本最终高于系统产生的收入。在其他情况下，如果推理是在用户设备上进行的，这可能不会成为一个问题。例如，对于移动应用或物联网设备，随着用户基础的不断增长，它不会以任何显著的方式影响基础设施成本（我们无法断言它根本不会影响；如果用户从您的内容分发网络下载模型，每增加1,000名新用户将使您额外花费几分钱）。'
- en: '*Reliability**—*Reliability can become a problem when you settle for a cheap
    option in your choice of inference platforms. Opting for the least expensive hardware
    vendor only to go through a sudden failure in the middle of a hot season may be
    at times worse than investing more in a time-tested, reliable solution. Think
    of all the disasters that may happen during an unexpected spike in the load, a
    bug in the model, or a hardware failure and, most importantly, how (and if) the
    system will tackle them.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可靠性**—*当你在选择推理平台时选择了便宜的选择，可靠性可能会成为一个问题。仅仅为了在炎热的季节中突然出现故障而选择最便宜的硬件供应商，有时可能比投资更多在经过时间考验的可靠解决方案上更糟糕。想想在意外负载高峰期间可能发生的所有灾难，模型中的错误，或硬件故障，最重要的是，系统将如何（以及是否）处理这些问题。'
- en: '*Flexibility**—*Even if a newly released system shows stable performance and
    efficiency, we can’t be certain about future requirements and ideas we’ll need
    to implement. Thus, the system should be flexible enough to digest changes and
    improvements. Those may include a new model (which may even be trained with a
    different framework!), additional preprocessing or postprocessing, new features,
    additional APIs, etc. Always keep in mind that the system will evolve and should
    be easy to modify without affecting the existing functionality.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*灵活性**—*即使一个新发布的系统显示出稳定的表现和效率，我们也不能确定未来的需求和我们将需要实施的想法。因此，系统应该足够灵活，能够消化变化和改进。这些可能包括一个新的模型（甚至可能使用不同的框架进行训练！），额外的预处理或后处理，新功能，额外的API等。始终记住，系统将不断发展，并且应该易于修改，而不会影响现有的功能。'
- en: '*Security and privacy**—*This subject spreads much farther than just one paragraph
    of text, but within the scope of this chapter, we can only mention that security
    requirements rely heavily on the target platform. For example, when your system
    operates fully within your backend, and the predictions never leave the perimeter
    of the organization, you barely need to think about security beyond your current
    protocols. On the other hand, if you are building a mobile app that will run on
    a user’s device, you need to heavily consider protecting the model from reverse
    engineering. In some cases, the model should be protected from users themselves,
    with one of the most notable examples being jailbreaks for large language models
    (LLMs) that became popular in 2023, when users tried to outsmart them to get answers
    to sensitive questions.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*安全和隐私**—*这个主题的范畴远不止一段文字，但在这个章节的范围内，我们只能提到安全需求在很大程度上依赖于目标平台。例如，当你的系统完全运行在你的后端，预测从未离开组织的边界时，你几乎不需要考虑超出当前协议的安全问题。另一方面，如果你正在开发一个将在用户设备上运行的移动应用，你需要认真考虑保护模型免受逆向工程。在某些情况下，模型甚至需要保护用户本身，其中最显著的例子是2023年流行的针对大型语言模型（LLMs）的越狱，当时用户试图通过它们来获取敏感问题的答案。'
- en: 15.2 Tradeoffs and patterns
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.2 权衡和模式
- en: The factors mentioned here are often conflicting and even mutually exclusive.
    For this reason, it is not possible to optimize for all of them simultaneously,
    meaning we’ll have to find compromises between those factors. However, there are
    certain patterns we can lean toward in finding a fair balance in various scenarios.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这里提到的一些因素往往是相互冲突的，甚至是相互排斥的。因此，我们无法同时优化所有这些因素，这意味着我们将在这些因素之间找到妥协。然而，在寻找各种场景中的公平平衡时，我们可以倾向于某些模式。
- en: 15.2.1 Tradeoffs
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.1 权衡
- en: Let’s start with *latency* and *throughput*. Both are very popular candidates
    for optimization, and improvements in one of them may lead to either improvements
    or degradation in the other.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从*延迟*和*吞吐量*开始。两者都是非常受欢迎的优化候选者，其中之一得到改进可能会带来另一个的改进或退化。
- en: Real production systems are often optimized for the best throughput for a given
    latency budget. The latency budget is dictated by the product or user experience
    needs (do users expect a real-time/near-real-time response, or are they tolerant
    of delays?), and given this budget, the aim is to maximize throughput (or minimize
    the number of required servers for expected throughput) by changing either the
    model architecture or inference design (e.g., with batching, as we describe later
    in section 15.3).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 真实的生产系统通常是在给定的延迟预算内优化最佳吞吐量。延迟预算由产品或用户体验需求决定（用户是否期望实时/近实时响应，或者他们是否可以容忍延迟？），在这个预算下，目标是通过改变模型架构或推理设计（例如，通过批处理，如我们在15.3节中描述的）来最大化吞吐量（或最小化预期吞吐量所需的服务器数量）。
- en: 'Let’s go through some examples. Imagine a simple deep learning model—a convolutional
    network like Resnet or a transformer like BERT. If you just reduce the number
    of blocks, it is very likely both your latency and throughput numbers will improve
    no matter what your inference setup is, so things are very straightforward (the
    model’s accuracy can drop, but that’s not the case in the current example). But
    imagine having two models with the same number of blocks and the same number of
    parameters per block but utilizing two different architectures: model A runs them
    in parallel with further aggregation (similar to the ResNeXt architecture), and
    model B runs every block sequentially. Model B will have a higher latency compared
    to model A due to the sequential nature of its architecture, but at the same time,
    you can run multiple instances of model B in parallel or run it with a large batch
    size, so the throughput of model A is not any worse than that of model B. Thus,
    parallelism is one of the internal factors that can affect latency and throughput
    in different ways. In practice, it means that the number of parameters or the
    number of blocks cannot be the definitive factor in comparing models, and our
    understanding of the architecture in the context of the inference platform is
    what helps us identify the bottlenecks, as seen in figure 15.1.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一些例子。想象一个简单的深度学习模型——一个像Resnet这样的卷积网络或者一个像BERT这样的Transformer。如果你只是减少块的数量，无论你的推理设置如何，你的延迟和吞吐量数字都有很大可能得到改善，所以事情非常直接（模型的准确率可能会下降，但在当前例子中并非如此）。但想象一下有两个模型，它们具有相同数量的块和每个块相同的参数数量，但使用了两种不同的架构：模型A以并行方式运行它们并进行进一步聚合（类似于ResNeXt架构），而模型B则按顺序运行每个块。由于模型B架构的顺序性，其延迟将高于模型A，但与此同时，你可以并行运行多个模型B的实例，或者以大批次大小运行它，因此模型A的吞吐量并不比模型B差。因此，并行性是影响延迟和吞吐量的内部因素之一。在实践中，这意味着参数数量或块的数量不能成为比较模型的最终因素，我们对架构在推理平台背景下的理解帮助我们识别瓶颈，如图15.1所示。
- en: '![figure](../Images/CH15_F01_Babushkin.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH15_F01_Babushkin.png)'
- en: Figure 15.1 An example of wide versus deep models. While they have the same
    number of parameters, the deep one is more limited in terms of parallel computation
  id: totrans-27
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图15.1 宽度与深度模型的示例。虽然它们具有相同的参数数量，但深度模型在并行计算方面更为受限
- en: 'Another tradeoff is related to model accuracy (or other ML-specific metrics
    used for the system) and correlated latency/throughput/costs. ML engineers are
    often tempted to solve the model’s imperfection by upgrading to a larger model.
    This can provide eventual benefits occasionally, but it is an expensive and poorly
    scalable way to fix the problem in the first place. At the same time, trying to
    solely optimize for cost by choosing the simplest model is not a good idea either.
    A dependency between compute costs and the value of the model is not linear, and
    in most cases, there is a sweet spot where the model is good enough, and the cost
    is relatively low, while radical solutions on the edges of the spectrum are less
    efficient. If we revisit section 9.1, one can be built with the same idea in mind:
    we can plot the model’s accuracy as a function of the compute costs and find a
    proper balance for our particular problem (see figure 15.2 for details).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个权衡与模型准确度（或用于系统的其他ML特定指标）以及相关的延迟/吞吐量/成本有关。ML工程师常常倾向于通过升级到更大的模型来解决模型的不足。这偶尔可以提供一些好处，但最初这是一种昂贵且扩展性差的解决问题的方式。同时，试图仅通过选择最简单的模型来优化成本也不是一个好主意。计算成本与模型价值之间的关系不是线性的，在大多数情况下，都有一个甜点，模型足够好，成本相对较低，而光谱边缘的激进解决方案效率较低。如果我们回顾第9.1节，可以以同样的想法构建：我们可以将模型的准确率作为计算成本的函数进行绘制，并为我们特定的问题找到合适的平衡（详见图15.2的详细信息）。
- en: '![figure](../Images/CH15_F02_Babushkin.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH15_F02_Babushkin.png)'
- en: 'Figure 15.2 Different sizes of models belonging to the same families and their
    respective accuracy values on the Imagenet dataset. The chart illustrates that
    larger models tend to have higher accuracy, but this effect tends to saturate.
    (Source: [https://mng.bz/QVzG](https://mng.bz/QVzG).)'
  id: totrans-30
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图15.2 同一家族的不同大小模型及其在ImageNet数据集上的相应准确度值。图表表明，较大的模型通常具有更高的准确度，但这种效应趋于饱和。（来源：[https://mng.bz/QVzG](https://mng.bz/QVzG)。）
- en: Finally, a tradeoff that might not be that obvious but is still worth mentioning
    is based on a link between research flexibility and production-level serving performance.
    On the one hand, we can benefit from building a model that is easy to migrate
    from the experimental sandbox directly to production. On the other hand, having
    a clear separation between research and production will allow the research team
    to experiment with new ideas without affecting the production system. But as we
    go further down the stretch, the difference between experimental code and real
    system inference will amplify, thus increasing the chance of facing a defect caused
    by the difference in the environments, which is hard to track without proper investments
    in monitoring and integration tests (please see chapter 13 for deployment aspects
    and chapter 14for observability).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一个可能不是那么明显但仍然值得提到的权衡是基于研究灵活性生产级服务性能之间的联系。一方面，我们可以从构建一个易于从实验沙盒直接迁移到生产的模型中受益。另一方面，研究团队和生产系统之间有明确的分离，将允许研究团队在不影响生产系统的情况下实验新想法。但随着我们进一步深入，实验代码和真实系统推理之间的差异将放大，从而增加面对由环境差异引起的缺陷的机会，没有适当的监控和集成测试投资，这将很难追踪（请参阅第13章关于部署方面和第14章关于可观察性的内容）。
- en: There are certain tools and frameworks that you may want to consider when facing
    the choice between either option. Because the route you take will significantly
    affect the sustainability of your system, the following section is fully dedicated
    to this very important subject.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当面临两种选择时，你可能需要考虑某些工具和框架。因为你选择的路线将显著影响你系统的可持续性，所以接下来的部分将完全致力于这个非常重要的主题。
- en: 15.2.2 Patterns
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.2 模式
- en: Once we have determined the tradeoffs we need to make for optimizing our model,
    it’s time to choose the right pattern to implement. We will once again limit ourselves
    to a small list and mention three diverse patterns. Although this list is not
    complete, it is highly likely that you will use one of these patterns in your
    system.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们确定了为了优化我们的模型需要做出的权衡，就到了选择合适的模式来实施的时候了。我们再次将范围限制在一个小的列表中，并提及三种不同的模式。尽管这个列表并不完整，但你很可能会在你的系统中使用这些模式之一。
- en: 'The first pattern worth mentioning is *batching*, a technique used to improve
    throughput, which needs to be considered in advance. If you know that the system
    can be used in a batch mode, you should design it accordingly. Batching is not
    just a binary property (to use or not to use) and features multiple nuances. Imagine
    a typical API for a web page: the client, whom we can’t control, sends some data,
    and the backend returns a prediction. That’s not a typical offline batch mode
    prediction, but there is room for dynamic batching: on the backend side, we wait
    for a short period (e.g., 20 ms or, say, 32 requests, whichever comes first) and
    collect all the requests that landed during this period, and then run a batch
    inference on them. This way, we can reduce the latency for the client but still
    keep the system responsive. This approach is implemented in frameworks like the
    Triton Inference Server by Nvidia and Tensorflow Serving. Another example of smart
    batching is related to language models: the typical input size for the model is
    dynamic, and running a batch inference on multiple inputs of various sizes requires
    padding to the size of the longest input. However, we can group inputs by size
    and run batch inference on each group separately, thus reducing the padding overhead
    (at the cost of smaller batches or a longer batch accumulation window). You can
    read more about this technique at Graphcore’s blog ([https://mng.bz/XVEv](https://mng.bz/XVEv)).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 值得首先提到的第一个模式是**批处理**，这是一种用于提高吞吐量的技术，需要提前考虑。如果你知道系统可以以批处理模式使用，你应该相应地设计它。批处理不仅仅是一个二元属性（使用或不使用），它具有多个细微差别。想象一下一个典型的网页API：客户端（我们无法控制）发送一些数据，后端返回一个预测。这并不是典型的离线批处理预测，但存在动态批处理的余地：在后端，我们等待一个短暂的时间（例如，20毫秒或，比如说，32个请求，哪个先到就先处理）并收集这个时间段内到达的所有请求，然后对它们进行批推理。这样，我们可以减少客户端的延迟，同时仍然保持系统的响应性。这种方法在Nvidia的Triton推理服务器和Tensorflow
    Serving等框架中得到实现。智能批处理的另一个例子与语言模型相关：模型的典型输入大小是动态的，对多个不同大小的输入进行批推理需要填充到最长输入的大小。然而，我们可以根据大小对输入进行分组，并分别对每个组进行批推理，从而减少填充开销（以较小的批次或更长的批次累积窗口为代价）。你可以在Graphcore的博客上了解更多关于这项技术的信息（[https://mng.bz/XVEv](https://mng.bz/XVEv))。
- en: 'The second pattern we’d like to talk about is *caching*, which has earned the
    status of the ultimate level of optimization (we’ll discuss inference optimization
    later in the chapter). Cache usage comes from the obvious idea: never compute
    the same thing twice. Sometimes it is as simple as in more traditional software
    systems: input data is associated with a key, and some key-value storage is used
    to keep previously computed results, so we don’t run expensive computations twice
    or more.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要讨论的第二种模式是**缓存**，它已经获得了优化终极级别的地位（我们将在本章后面讨论推理优化）。缓存的使用源于一个显而易见的思想：永远不要两次计算相同的东西。有时这就像在更传统的软件系统中一样简单：输入数据与一个键相关联，并使用一些键值存储来保存之前计算的结果，这样我们就不必重复进行昂贵的计算。
- en: Listing 15.1 Example of the simplest possible in-memory cache
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表15.1展示了最简单的内存缓存示例
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In reality, the distribution of inputs for an ML model may have a long tail,
    as most requests are unique. For example, according to Google, 15% of all search
    queries are totally unique ([https://mng.bz/yoeB](https://mng.bz/yoeB)). Given
    that cache time to live is typically way lower than the whole Google history,
    the share of unique (noncacheable) queries would be high within any reasonable
    window. Does it mean the cache is useless? Not necessarily. One reason is that
    even a low percentage of saved compute can provide a great benefit in terms of
    saved money. Another concept is the idea of using fuzzy caches: instead of checking
    the direct match of the key, we can relax the matching condition. For example,
    Arseny has seen a system where the cache key was based on a regular expression,
    so the result could be shared by multiple similar requests matching the same regex.
    Even more aggressive caching can be built based on the semantic similarity of
    the key (e.g., [https://github.com/zilliztech/GPTCache](https://github.com/zilliztech/GPTCache)
    uses such an approach to cache LLM queries). As practitioners who care about reliability,
    we recommend thinking twice about using such caching: its level of fuzziness is
    huge, and thus the cache would be prone to false cache hits.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实中，机器学习模型的输入分布可能有一个长尾，因为大多数请求都是唯一的。例如，根据谷歌的数据，所有搜索查询中有15%是完全独特的（[https://mng.bz/yoeB](https://mng.bz/yoeB)）。鉴于缓存的有效期通常远低于整个谷歌历史，任何合理的时间窗口内，独特（不可缓存的）查询的比例都会很高。这意味着缓存没有用吗？不一定。一个原因是，即使是很低的计算节省比例也能在节省资金方面带来巨大的好处。另一个概念是使用模糊缓存的想法：我们不必检查键的直接匹配，我们可以放宽匹配条件。例如，Arseny看到过一个系统，其缓存键基于正则表达式，因此结果可以被多个匹配相同正则表达式的相似请求共享。更激进的缓存可以基于键的语义相似性构建（例如，[https://github.com/zilliztech/GPTCache](https://github.com/zilliztech/GPTCache)使用这种方法来缓存LLM查询）。作为关注可靠性的从业者，我们建议在使用这种缓存时三思而后行：其模糊程度很高，因此缓存容易产生错误的缓存命中。
- en: 'A third pattern has recently emerged in the LLM world: *routing between models*.
    Inspired by the “mixture of experts” architecture ([https://mng.bz/M1qW](https://mng.bz/M1qW)),
    it proved to be a good step for optimization: some queries are hard (and should
    be processed by expensive in serving but advanced models); some are simpler (and
    thus can be delegated to less complex and cheaper models). Such an approach is
    mostly used for general LLMs, machine translation, and other—mostly natural language
    processing-specific—tasks. A good example of the implementation is finely presented
    in the paper “Leeroo Orchestrator: Elevating LLMs Performance Through Model” by
    Alireza Mohammadshahi et al. ([https://arxiv.org/abs/2401.13979v1](https://arxiv.org/abs/2401.13979v1)).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '最近在LLM领域出现了一种第三种模式：**模型间的路由**。受“专家混合”架构（[https://mng.bz/M1qW](https://mng.bz/M1qW)）的启发，这被证明是优化的一大步：一些查询很困难（并且应该由昂贵的服务端高级模型处理）；一些则更简单（因此可以委托给更复杂且成本更低的模型）。这种方法主要用于通用LLM、机器翻译和其他主要针对自然语言处理任务的领域。这种实现的一个很好的例子在Alireza
    Mohammadshahi等人撰写的论文“Leeroo Orchestrator: Elevating LLMs Performance Through Model”中有详细阐述（[https://arxiv.org/abs/2401.13979v1](https://arxiv.org/abs/2401.13979v1)）。'
- en: A variation of this pattern is to *use two models* (one is fast and imperfect;
    the other is slow and more accurate). With this combination, a quick response
    is generated by the first, smaller model (so it can be rendered with low latency)
    to be later replaced with the output of the second, heavier model. Unlike the
    previous pattern, it does not save the total compute required, but it does optimize
    a special kind of latency, which is time to initial response.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式的变体是*使用两个模型*（一个快速但不完美；另一个慢但更准确）。在这种组合中，第一个较小的模型（因此可以以低延迟渲染）快速响应，随后被第二个较重的模型的输出所取代。与之前的模式不同，它并不节省总的计算需求，但它优化了一种特殊的延迟，即初始响应时间。
- en: 15.3 Tools and frameworks
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.3 工具和框架
- en: The inference process is heavily engineering focused. Luckily, there are many
    tools and frameworks available for use. Following the approach taken for this
    book, we don’t aim to provide a comprehensive overview of every framework you
    might need to construct a reliable inference pipeline; instead, we focus on the
    principles and mention popular solutions for illustrative purposes.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 推理过程高度依赖于工程。幸运的是，有许多工具和框架可供使用。遵循本书采用的方法，我们不旨在提供每个可能需要构建可靠推理流程的框架的全面概述；相反，我们专注于原则，并提及一些流行的解决方案以供说明。
- en: 15.3.1 Choosing a framework
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.3.1 选择框架
- en: One common, although not immediately apparent, heuristic is to separate your
    training and inference frameworks. It is typical to use tools like Pandas, scikit-learn,
    and Keras as they offer flexibility and simplicity during research, prototyping,
    or training. However, they are not ideal for inference due to the inevitable tradeoff
    between flexibility and simplicity. This is why it’s a popular practice to train
    models in one framework and then convert them for further inference in another.
    Additionally, it’s essential to decouple the training framework from the inference
    framework as much as possible so that if you need to switch to a different training
    framework with new features, it won’t affect the inference pipeline. This is especially
    crucial for production systems expected to be operational and evolve for years.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见但并非立即显而易见的启发式方法是分离你的训练和推理框架。通常，在研究、原型设计或训练过程中，人们会使用像Pandas、scikit-learn和Keras这样的工具，因为它们在灵活性、简单性方面提供了便利。然而，由于灵活性和简单性之间不可避免的权衡，它们并不适合推理。这就是为什么在同一个框架中训练模型，然后将其转换为另一个框架进行进一步推理的做法变得流行。此外，尽可能地将训练框架与推理框架解耦是至关重要的，这样如果需要切换到具有新特性的不同训练框架，它就不会影响推理流程。这对于预期在几年内保持运行和演变的系统来说尤为重要。
- en: From the other perspective, some research-first frameworks like Torch tend to
    close the gap between research and production. The compilation functionality introduced
    in Torch 2.0 allows for making a fairly optimized inference pipeline from the
    same code used for training and relevant experiments. So, whether you want to
    use a universal framework or a combination of two or even more solutions for different
    purposes, both paradigms are viable, depending on which approach you choose (see
    figure 15.3 for information on the most popular frameworks).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从另一个角度来看，一些以研究为先导的框架，如Torch，倾向于缩小研究和生产之间的差距。Torch 2.0中引入的编译功能允许从用于训练和相关实验的相同代码中生成相当优化的推理流程。因此，无论您是想使用通用框架，还是为了不同的目的而结合两个甚至更多的解决方案，这两种范式都是可行的，具体取决于您选择哪种方法（有关最流行框架的信息，请参阅图15.3）。
- en: '![figure](../Images/CH15_F03_Babushkin.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH15_F03_Babushkin.png)'
- en: Figure 15.3 A wide spectrum of tools with their focus on either research or
    production serving goals
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图15.3 一系列工具，它们专注于研究或生产服务目标
- en: Achieving a balance between research flexibility and high performance in the
    production environment may require an interframework format. A popular choice
    for this purpose is ONNX, which is supported by many training frameworks for converting
    their models to ONNX. On the other hand, inference frameworks often work with
    the ONNX format or allow the conversion of ONNX models into their own format,
    making ONNX a common language in the ML world.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究灵活性和生产环境中的高性能之间取得平衡可能需要一种跨框架的格式。为此，ONNX是一个流行的选择，它被许多训练框架支持，用于将它们的模型转换为ONNX。另一方面，推理框架通常与ONNX格式一起工作，或者允许将ONNX模型转换为它们自己的格式，使ONNX成为ML世界中的通用语言。
- en: Here it should be mentioned that ONNX is not just a representation format but
    rather an ecosystem. It includes a runtime that can be used for inference and
    a set of tools for model conversion and optimization. ONNX Runtime is well-suited
    for a variety of backends and can run on almost any platform with specific optimizations
    for each of them. This makes it an excellent choice for multiplatform systems.
    Based on our experience, ONNX Runtime strikes a popular balance between flexibility
    (although less flexible than serving PyTorch or scikit-learn models directly)
    and performance (although less performant than tailoring a highly custom solution
    for the specific combination of models, hardware, and usage patterns).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这里应该提到，ONNX不仅仅是一种表示格式，而是一个生态系统。它包括一个可用于推理的运行时以及一组用于模型转换和优化的工具。ONNX运行时非常适合各种后端，并且几乎可以在任何平台上运行，并为每个平台提供特定的优化。这使得它成为多平台系统的绝佳选择。根据我们的经验，ONNX运行时在灵活性（尽管不如直接服务PyTorch或scikit-learn模型灵活）和性能（尽管不如为特定组合的模型、硬件和使用模式量身定制的解决方案性能好）之间取得了流行的平衡。
- en: For the server CPU inference, a popular engine is OpenVINO by Intel. For CUDA
    inference, TensorRT by Nvidia is commonly used. Both of them are optimized for
    their target hardware and are also available as ONNX Runtime backends, which allows
    them to be used in the same way as ONNX Runtime. Two more honorable mentions are
    TVM ([https://tvm.apache.org/](https://tvm.apache.org/)), which is a compiler
    for deep learning models capable of generating code for a variety of hardware
    targets, and AITemplate ([https://github.com/facebookincubator/AITemplate](https://github.com/facebookincubator/AITemplate)),
    which can be used even for running models on the less common AMD GPUs via the
    ROCm software stack.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于服务器CPU推理，一个流行的引擎是英特尔的开源VINO。对于CUDA推理，通常使用英伟达的TensorRT。这两个引擎都针对其目标硬件进行了优化，并且也作为ONNX运行时后端提供，这使得它们可以像ONNX运行时一样使用。另外两个值得提及的是TVM（[https://tvm.apache.org/](https://tvm.apache.org/)），这是一个能够为各种硬件目标生成代码的深度学习模型编译器，以及AITemplate（[https://github.com/facebookincubator/AITemplate](https://github.com/facebookincubator/AITemplate)），它甚至可以通过ROCm软件堆栈在不太常见的AMD
    GPU上运行模型。
- en: Those familiar with iOS model deployment may be aware of CoreML, an engine for
    iOS inference that uses its own format. Android developers usually opt for TensorFlow
    Lite, although there are more options available on Android thanks to its greater
    fragmentation. This list is far from complete, but it provides an idea of how
    large the variety of available options is.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于熟悉iOS模型部署的人来说，CoreML可能并不陌生，这是一个用于iOS推理的引擎，它使用自己的格式。Android开发者通常选择TensorFlow
    Lite，尽管由于Android的碎片化程度更高，Android上可用的选项更多。这个列表远非完整，但它提供了一个关于可选项多样性的概念。
- en: When we say something like “engine X runs on device Y,” it may not be precisely
    accurate. For example, even when the overall inference is meant to run on a GPU,
    some operations may be forwarded to the CPU because it produces more efficiency.
    GPUs excel at massively parallel computing, making them ideal for tasks like matrix
    multiplication or convolutions. However, some operations related to control flow
    or sparse data are better handled by the CPU. For instance, CoreML dynamically
    splits the execution graph between the CPU, GPU, and Apple Neural Engine to maximize
    efficiency.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们说“引擎X在设备Y上运行”时，这可能并不完全准确。例如，即使整体推理旨在在GPU上运行，某些操作可能也会转发到CPU，因为这样做效率更高。GPU擅长大规模并行计算，这使得它们非常适合矩阵乘法或卷积等任务。然而，与控制流或稀疏数据相关的某些操作更适合由CPU处理。例如，CoreML会动态地将执行图分割到CPU、GPU和Apple
    Neural Engine之间，以最大化效率。
- en: Various inference engines often come with optimizers that can be used to improve
    the model’s performance. For example, ONNX Runtime offers a set of optimizers
    that can reduce the model’s size by trimming unused parts of the graph (e.g.,
    those only used in training for loss computation) or by reducing the number of
    operations after fusing several operations into one. These optimizers are typically
    separate tools used to prepare the model for inference but are not part of the
    inference engine itself.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 各种推理引擎通常附带优化器，可用于提高模型性能。例如，ONNX运行时提供了一套优化器，可以通过修剪图中的未使用部分（例如，仅在训练中用于损失计算的）或通过将多个操作融合为一个来减少操作的数量。这些优化器通常是用于准备模型以供推理的独立工具，但不是推理引擎本身的一部分。
- en: 'By the time this book is published, there’s a chance this section will be outdated,
    as the field is evolving rapidly. For example, when we started writing the book,
    few people paid attention to LLMs, and now they are ubiquitous. People often debate
    which is a better inference engine for them—whether it’s VLLM ([https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm)),
    TGI ([https://mng.bz/aVG7](https://mng.bz/aVG7)), or GGML ([https://github.com/ggerganov/ggml](https://github.com/ggerganov/ggml)).
    LLM inference is overall a kind of specific topic—unlike most of the more traditional
    models, LLMs are often bound by large memory footprint and autoregressive paradigm
    (you can’t predict token T + 1 before token T is predicted, which makes parallelization
    barely possible without additional tricks). If you’re interested in LLM inference,
    we recommend reading the blog post at [https://vgel.me/posts/faster-inference/](https://vgel.me/posts/faster-inference/)
    and some truly profound pieces of research (e.g., at the moment of polishing this
    chapter, we were impressed with “PowerInfer: Fast Large Language Model Serving
    with a Consumer-grade GPU” by Song et al. ([https://arxiv.org/abs/2312.12456](https://arxiv.org/abs/2312.12456)).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '到这本书出版时，这个部分可能会过时，因为这个领域正在快速发展。例如，当我们开始写这本书时，很少有人关注大型语言模型（LLMs），而现在它们无处不在。人们经常争论哪种推理引擎对他们来说更好——是
    VLLM ([https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm))，TGI
    ([https://mng.bz/aVG7](https://mng.bz/aVG7))，还是 GGML ([https://github.com/ggerganov/ggml](https://github.com/ggerganov/ggml)))。LLM
    推理整体上是一个特定的主题——与大多数更传统的模型不同，LLMs 通常受限于大的内存占用和自回归范式（在预测了标记 T 之前，你不能预测标记 T + 1，这使得在没有额外技巧的情况下并行化几乎不可能）。如果你对
    LLM 推理感兴趣，我们建议阅读博客文章 [https://vgel.me/posts/faster-inference/](https://vgel.me/posts/faster-inference/)
    和一些真正深刻的研究成果（例如，在润色这一章节的时候，我们被 Song 等人撰写的“PowerInfer: 使用消费级 GPU 的快速大型语言模型服务”所打动
    ([https://arxiv.org/abs/2312.12456](https://arxiv.org/abs/2312.12456)))。'
- en: While the terms *inference engine* and *inference framework* are often used
    interchangeably, they are not completely identical. An inference engine is a runtime
    used for inference, while an inference framework is a more general term that may
    include an engine, a set of tools for model conversion and optimization, and other
    components that assist with various aspects of serving, such as batching, versioning,
    model registry, logging, and so on. For example, ONNX Runtime is an inference
    engine, while TorchServe ([https://pytorch.org/serve/](https://pytorch.org/serve/))
    is an inference framework.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 *推理引擎* 和 *推理框架* 这两个术语经常被互换使用，但它们并不完全相同。推理引擎是一个用于推理的运行时，而推理框架是一个更通用的术语，可能包括一个引擎、一组用于模型转换和优化的工具，以及其他辅助服务各个方面的组件，例如批处理、版本控制、模型注册、日志记录等。例如，ONNX
    Runtime 是一个推理引擎，而 TorchServe ([https://pytorch.org/serve/](https://pytorch.org/serve/)))
    是一个推理框架。
- en: There is no single answer to the question of whether you need a fully featured
    framework or just a small wrapper on top of the inference engine. From our experience,
    once your needs reach a high level of certainty and you generally tend to work
    with more advanced machinery while having enough human resources to maintain it,
    frameworks are the way to go. On the other hand, once you’re in a startup and
    need to ship the system somehow but know that your needs will only be formulated
    in the next several months, it makes sense to opt for the leaner way, which is
    to deploy the model with a simple combination of an inference engine and some
    communication layer (e.g., web framework) and postpone a more reliable solution
    for the next version.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于是否需要一个功能齐全的框架或仅仅是在推理引擎之上添加一个小的包装层，并没有一个唯一的答案。根据我们的经验，一旦你的需求达到高度确定性，并且你通常倾向于使用更先进的设备同时拥有足够的人力资源来维护它，框架就是最佳选择。另一方面，一旦你处于初创公司阶段，需要以某种方式交付系统，但又知道你的需求将在接下来的几个月内才会明确，选择更精简的方式，即使用推理引擎和一些通信层（例如，Web
    框架）的简单组合来部署模型，并将更可靠的解决方案推迟到下一个版本，这是有意义的。
- en: 15.3.2 Serverless inference
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.3.2 无服务器推理
- en: Serverless inference is an emerging approach that stands out from traditional
    server-based models. Popularized by AWS Lambda, this serverless paradigm has now
    found representation in multiple alternatives from major cloud providers such
    as Google Cloud Functions, Azure Functions, and Cloudflare Workers, as well as
    from startups like Banana.dev and Replicate. As of the current writing, major
    providers primarily offer CPU inference with limited GPU capabilities, although
    this is likely to change as startups continue to push the boundaries in this field.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 无服务器推理是一种新兴的方法，与传统基于服务器的模型相比，具有独特之处。AWS Lambda的推广使得这种无服务器范式现在在多个主要云服务提供商的替代方案中得到了体现，例如Google
    Cloud Functions、Azure Functions和Cloudflare Workers，以及Banana.dev和Replicate等初创公司。截至本文撰写时，主要提供商主要提供有限的GPU能力，主要提供CPU推理，尽管随着初创公司继续在该领域突破边界，这种情况可能会改变。
- en: 'More advanced products by major cloud providers like AWS Sagemaker jobs can
    be viewed as serverless as well. They share core serverless properties (managed
    infrastructure, pay-as-you-go pricing, autoscaling) but aim for another level
    of abstraction: slower functions and more containerized jobs running for extended
    periods.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 主要云服务提供商如AWS Sagemaker作业的更高级产品也可以被视为无服务器。它们共享核心无服务器属性（管理基础设施、按使用付费定价、自动扩展），但追求更高层次的抽象：运行时间较长的函数和更多容器化作业。
- en: It’s important to note that the term *serverless* can be somewhat misleading.
    It doesn’t imply the absence of servers in the system. Rather, it means that engineers
    have no direct control over the servers, as they are isolated from them by cloud
    providers, so they don’t need to concern themselves with server management.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，术语*无服务器*可能会有些误导。它并不意味着系统中没有服务器。相反，这意味着工程师无法直接控制服务器，因为它们被云服务提供商隔离开来，所以他们不需要担心服务器管理。
- en: 'Attitudes toward serverless inference are generally either strictly positive
    or strictly negative due to its notable advantages and disadvantages. Let’s highlight
    some of them:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于无服务器推理的态度通常是严格正面或严格负面，因为其显著的优缺点。让我们突出一些优点和缺点：
- en: '*No need to manage infrastructure or pay for idle resources**—*You pay only
    for what you use. While serverless advocates emphasize this, the reality is that
    some level of infrastructure management may still be required, although likely
    at a higher level and with reduced complexity.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*无需管理基础设施或为闲置资源付费**—*您只需为使用的资源付费。虽然无服务器倡导者强调这一点，但现实是可能仍需要一定程度的设施管理，尽管可能处于更高层次且复杂性降低。'
- en: '*Ease of scaling, especially for sporadic loads**—*However, it’s not a universal
    solution. Cloud providers may impose limitations on concurrent requests, preventing
    infinite and swift scaling. Additionally, the problem of “cold start,” where it
    can take several seconds for a large model to initialize, is a concern for applications
    with low latency requirements. Some serverless providers, like Runpod, offer more
    control in this regard, allowing you to set minimum and maximum numbers of workers
    and customize scaling rules. Cold start is viewed as a significant problem of
    serverless computing, so providers develop specific solutions to address it, like
    SnapStart by AWS ([https://mng.bz/gAWV](https://mng.bz/gAWV)) and Flashboot by
    Runpod ([https://mng.bz/eV9Q](https://mng.bz/eV9Q)). When aiming for serverless
    inference, cold start time also becomes a factor when choosing the inference engine,
    as we should aim for slimmer artifacts (like Docker images) and lower load time.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*易于扩展，特别是对于间歇性负载**—*然而，它并非万能的解决方案。云服务提供商可能会对并发请求施加限制，防止无限和快速扩展。此外，“冷启动”问题，即大型模型初始化可能需要几秒钟，对于低延迟要求的应用程序来说是一个关注点。一些无服务器提供商，如Runpod，在这方面提供了更多控制，允许您设置最小和最大工作员数量以及自定义扩展规则。冷启动被视为无服务器计算的一个重大问题，因此提供商开发了特定的解决方案来解决它，例如AWS的SnapStart([https://mng.bz/gAWV](https://mng.bz/gAWV))和Runpod的Flashboot([https://mng.bz/eV9Q](https://mng.bz/eV9Q))。当目标是无服务器推理时，冷启动时间也成为选择推理引擎时的一个因素，因为我们应旨在使用更精简的工件（如Docker镜像）和更低的加载时间。'
- en: '*Cost-effectiveness for low loads**—*Yet, for moderately low but consistent
    and predictable workloads, dedicated machines might be more cost-efficient than
    a serverless solution. The pricing and latency combination can also be perplexing.
    For instance, if a model takes 100 ms to process when warm and 5,000 ms when cold
    (i.e., right after a break), and the pricing is based on processing time, the
    cost for the cold request would be 50 times higher. Optimizing such scenarios
    isn’t always straightforward. A special case of low load is various testing environments:
    it is nice to avoid costs associated with more traditional architecture and only
    pay for rare test calls happening in your staging environment.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低负载的成本效益**——然而，对于中等低但一致且可预测的工作负载，专用机器可能比无服务器解决方案更经济高效。定价和延迟的组合也可能令人困惑。例如，如果模型在热状态下处理需要100毫秒，而在冷状态下（即休息后）需要5,000毫秒（即，冷请求），并且定价基于处理时间，那么冷请求的成本将是热请求的50倍。优化这种场景并不总是简单直接。低负载的一个特殊情况是各种测试环境：避免与更传统架构相关的成本，只为在您的预发布环境中发生的罕见测试调用付费是很不错的。'
- en: '*Harder to test locally**—*The overall complexity of the dev infrastructure
    tends to increase for the serverless inference, even though it can be cheaper,
    as mentioned earlier. It is not just “I can’t test it without an internet connection
    anymore.” Once the serverless inference is deep in our system, it may bring additional
    problems (e.g., there’s a need to redeploy test artifacts for trivial changes,
    make sure the test environment has proper permissions, etc.)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地测试更困难**——正如之前提到的，尽管无服务器推理可能更便宜，但它的整体开发基础设施复杂性往往会增加。这不仅仅是“没有互联网连接我就无法测试了。”一旦无服务器推理深入到我们的系统中，它可能会带来额外的问题（例如，需要重新部署测试工件以进行微小的更改，确保测试环境有适当的权限等）。'
- en: Some serverless providers, like Replicate, offer a wide range of pretrained
    foundation models that can be used out of the box. This is particularly advantageous
    when starting new projects, especially for prototyping or research purposes.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一些无服务器提供商，如Replicate，提供了一系列开箱即用的预训练基础模型。这在启动新项目时尤其有利，尤其是用于原型设计或研究目的。
- en: 'We’ve observed both successful and failed cases of serverless inference in
    small pet projects and high-load production environments. It’s unquestionably
    a viable option, but careful consideration and a thorough cost-benefit analysis
    are crucial before fully embracing it. The rule of thumb that we follow is as
    follows: consider using serverless inference when the autoscaling is a significant
    advantage (e.g., high variance in the number of requests) and the model itself
    is not excessively large (although even LLMs can sometimes be deployed in CPU-only
    serverless environment; [https://mng.bz/pxnz](https://mng.bz/pxnz)). Another good
    idea is to consider serverless inference when you are uncertain about the future
    load: it can somewhat fit multiple load patterns at the beginning of the project,
    giving you enough time to redesign the inference part once the situation becomes
    clearer.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到了在小型宠物项目和高压生产环境中无服务器推理的成功和失败案例。毫无疑问，这是一个可行的选择，但在完全接受它之前，仔细考虑和彻底的成本效益分析是至关重要的。我们遵循的规则如下：当自动扩展是一个显著优势（例如，请求数量的高变异性）且模型本身不是过大（尽管LLMs有时也可以在仅CPU的无服务器环境中部署；[https://mng.bz/pxnz](https://mng.bz/pxnz)）时，考虑使用无服务器推理。另一个好主意是在你对未来负载不确定时考虑无服务器推理：它可以在项目初期适应多种负载模式，给你足够的时间在情况变得清晰后重新设计推理部分。
- en: 15.4 Optimizing inference pipelines
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.4 优化推理管道
- en: Premature optimization is the root of all evil.— Donald Knuth
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 过早的优化是万恶之源。——唐纳德·克努特
- en: Optimizing inference pipelines is a wide topic that is not a part of ML system
    design per se; still, it’s a crucial part of ML system engineering that deserves
    a separate section, at least for listing common approaches and tools as a landscape
    overview. At its core, optimizing inference pipelines often boils down to a tradeoff
    between the model’s speed and accuracy and the required resource capacity, which
    implies numerous optimizing techniques that mainly depend on the model’s characteristics
    and architecture.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 优化推理管道是一个广泛的话题，它本身并不是机器学习系统设计的一部分；然而，它仍然是机器学习系统工程的一个关键部分，值得单独成章，至少应该列出常见的方法和工具作为概览。在本质上，优化推理管道通常归结于模型的速度和准确性与所需资源容量之间的权衡，这暗示了多种优化技术，这些技术主要取决于模型的特点和架构。
- en: 'A reasonable question that may pop up here is, “What would be the starting
    procedure for optimizing?” We asked a similar question of a number of ML engineers
    during job interviews and received a variety of answers that mentioned such terms
    as *model pruning*, *quantization* (see “Pruning and Quantization for Deep Neural
    Network Acceleration: A Survey” [https://arxiv.org/abs/2101.09671](https://arxiv.org/abs/2101.09671)),
    and *distillation* (see “Knowledge Distillation: A Survey,” [https://arxiv.org/abs/2006.05525](https://arxiv.org/abs/2006.05525)),
    with references to the state of the papers (we only mention a couple of surveys
    so you can use them as starting points). These techniques are widely known in
    the ML research community; they’re useful and often applicable, but they are only
    focused on model optimization without letting us control the whole picture. The
    most pragmatic answer was, “I would start with profiling.”'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可能在这里出现的合理问题是，“优化应该从哪个步骤开始？”我们在面试中向许多机器学习工程师提出了类似的问题，并收到了各种答案，提到了诸如*模型剪枝*、*量化*（参见“用于深度神经网络加速的剪枝和量化：综述”
    [https://arxiv.org/abs/2101.09671](https://arxiv.org/abs/2101.09671)）和*蒸馏*（参见“知识蒸馏：综述”
    [https://arxiv.org/abs/2006.05525](https://arxiv.org/abs/2006.05525)）等术语，并引用了论文的状态（我们只提到了几篇综述，以便您可以用它们作为起点）。这些技术在机器学习研究社区中广为人知；它们是有用的并且通常适用，但它们只关注模型优化，而没有让我们控制整个局面。最实际的回答是，“我会从性能分析开始。”
- en: 15.4.1 Starting with profiling
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.4.1 从性能分析开始
- en: '*Profiling* is a process of measuring the performance of the system and identifying
    bottlenecks. In contrast to the techniques mentioned earlier, profiling is a more
    general approach that can be applied to the whole system. Just like strategy comes
    before tactics, profiling is a great starting point that allows for the identification
    of the most promising directions for optimization and the selection of the most
    appropriate techniques.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*性能分析*是一个衡量系统性能和识别瓶颈的过程。与前面提到的技术相比，性能分析是一种更通用的方法，可以应用于整个系统。就像战略在战术之前一样，性能分析是一个很好的起点，它允许我们确定最有可能的优化方向，并选择最合适的技巧。'
- en: You might be surprised by how often the seemingly most obvious factors may not
    be the model’s weakest links. Let’s take latency as an example. There are cases
    when it is not the bottleneck (especially when the served model is not a recent
    generative thing but something more conventional), and the problem is hiding somewhere
    else (e.g., data preprocessing or network interactions). Furthermore, even if
    the model is the slowest part of the pipeline, it doesn’t automatically mean it
    should be the target for optimization. This may seem counterintuitive, but we
    should look for the most optimizable, not the slowest, part of the system. Imagine
    that a full run takes 200 ms, 120 ms of which is required for the model. But the
    fact is, the model is already optimized; it runs on GPU via a highly performant
    engine, and there is not much room for any more improvement. On the other hand,
    data preprocessing takes 80 ms, and it is an arbitrary Python code that can be
    optimized in many ways. In this case, it is better to start with data preprocessing,
    not the model.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会惊讶于看似最明显的因素可能并不是模型的瓶颈。以延迟为例，有些情况下它并不是瓶颈（特别是当提供的服务模型不是最新的生成式事物，而是更传统的事物时），问题可能隐藏在其他地方（例如，数据预处理或网络交互）。此外，即使模型是管道中最慢的部分，这也并不意味着它应该是优化的目标。这可能看起来有些反直觉，但我们应该寻找最可优化的部分，而不是最慢的部分。想象一下，整个运行需要200毫秒，其中120毫秒是模型所需的。但事实是，模型已经是最优化的了；它通过一个高性能的引擎在GPU上运行，没有太多改进的空间。另一方面，数据预处理需要80毫秒，这是一段任意的Python代码，可以通过多种方式优化。在这种情况下，最好从数据预处理开始，而不是模型。
- en: 'Another example comes from Arseny’s experience; he was once asked to reduce
    the latency of a system. The system was a relatively simple pipeline that ran
    tens of simple models sequentially. The first idea to improve timing was to replace
    the sequential run with a batch inference. However, profiling proved the opposite:
    the inference itself was insignificant (around 5%) compared to the time spent
    on data preprocessing, and a batch inference would not help. What helped was optimizing
    the preprocessing step, which could not benefit from batching and was eventually
    coupled with the initially designed sequential run. In the end, Arseny managed
    to speed up the system by 40% without touching the core model inference, and such
    elements as thread management, IO and serialization/deserialization functions,
    and cascade caching were the real low-hanging fruit.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子来自Arseny的经验；他曾经被要求降低一个系统的延迟。该系统是一个相对简单的流水线，依次运行了数十个简单的模型。提高时间的第一想法是将顺序运行替换为批量推理。然而，分析证明结果相反：与数据预处理花费的时间相比，推理本身（大约5%）微不足道，批量推理不会有所帮助。真正有帮助的是优化预处理步骤，它不能从批处理中受益，并且最终与最初设计的顺序运行相结合。最终，Arseny通过不触及核心模型推理，将系统速度提高了40%，而像线程管理、IO以及序列化和反序列化函数、级联缓存等元素才是真正的低垂之果。
- en: Here we should mention that profiling ML systems differs slightly from profiling
    regular software for reasons like extensive use of GPUs, asynchronous execution,
    using thin Python wrappers on top of high-performance native code, and so on.
    And since the whole process may include a larger number of variables, you should
    be careful when interpreting the results of profiling, as it is easy to get confused
    by the tricky nature of the problem.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们应该提到，由于广泛使用GPU、异步执行、在高性能原生代码之上使用薄的Python包装器等原因，对ML系统的分析略不同于对常规软件的分析。而且，由于整个过程可能包括更多的变量，因此在解释分析结果时应谨慎，因为很容易被问题的复杂性质所迷惑。
- en: 'GPU execution can be especially confusing. Typical CPU load is simple: the
    data is loaded into memory, and the CPU executes the code: done. There may be
    nuances related to CPU cache, single instruction, multiple data instructions,
    or concurrent execution, but in most cases, it is straightforward. Although the
    GPU is a separate device, it usually comes with built-in memory, and the data
    has to be copied to the GPU memory before the execution. The copying process itself
    can be a bottleneck, and it’s not always obvious how to measure it. The highly
    parallel nature of the GPU execution also makes for nonlinear effects. The simplest
    example is that if you run a model on a single image, it may take 100 ms, but
    if you run it on 64 images, the processing time will increase only to 200 ms.
    That is because the GPU is not fully utilized when processing just one item, which
    leads to a significant overhead of copying the data to the GPU memory. The same
    happens on a lower level of model architecture: reducing the number of filters
    in a convolutional layer may not reduce latency, as the GPU is not fully utilized
    and uses the same CUDA kernel under the hood. Overall, programming for CUDA and
    other general-purpose GPU frameworks is a separate and extremely deep rabbit hole;
    the only takeaway we would like to focus on is that the typical programmer’s intuition
    on what is fast and what is slow can be totally irrelevant for GPU-based computing.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: GPU执行可能会特别令人困惑。典型的CPU负载很简单：数据被加载到内存中，CPU执行代码：完成。可能会有与CPU缓存、单指令多数据指令或并发执行相关的细微差别，但在大多数情况下，它是直接的。尽管GPU是一个独立的设备，但它通常带有内置的内存，并且在执行之前必须将数据复制到GPU内存中。复制过程本身可能成为瓶颈，而且并不总是明显如何衡量它。GPU执行的高度并行性也导致了非线性效应。最简单的例子是，如果你在单个图像上运行一个模型，它可能需要100毫秒，但如果你在64个图像上运行它，处理时间只会增加到200毫秒。这是因为当只处理一个项目时，GPU并没有得到充分利用，这导致了将数据复制到GPU内存中的显著开销。在模型架构的较低级别也是如此：减少卷积层中的滤波器数量可能不会减少延迟，因为GPU没有得到充分利用，并且底层使用相同的CUDA内核。总的来说，为CUDA和其他通用GPU框架编程是一个独立且极其深奥的领域；我们唯一想强调的是，典型程序员对什么快什么慢的直觉对于基于GPU的计算可能是完全无关的。
- en: So a proper approach to profiling requires keeping a wide mix of tools at hand,
    starting from basic profilers like cProfile from a Python standard library, more
    advanced third-party tools like Scalene ([https://github.com/plasma-umass/scalene](https://github.com/plasma-umass/scalene)),
    memray ([https://github.com/bloomberg/memray](https://github.com/bloomberg/memray)),
    py-spy ([https://github.com/benfred/py-spy](https://github.com/benfred/py-spy)),
    and ML framework-specific tools (like PyTorch Profiler) and ending with low-level
    GPU profilers like nvprof ([https://mng.bz/OmqE](https://mng.bz/OmqE)). Finally,
    when working with exotic hardware like tensor processing units or IoT processors,
    you may need to use vendor-specific tools.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，适当的性能分析方法需要手头有一系列工具，从Python标准库中的基本分析器cProfile开始，到更高级的第三方工具，如Scalene ([https://github.com/plasma-umass/scalene](https://github.com/plasma-umass/scalene))、memray
    ([https://github.com/bloomberg/memray](https://github.com/bloomberg/memray))、py-spy
    ([https://github.com/benfred/py-spy](https://github.com/benfred/py-spy))，以及特定于机器学习框架的工具（如PyTorch
    Profiler），最后是低级GPU分析器如nvprof ([https://mng.bz/OmqE](https://mng.bz/OmqE)))。最后，当与如张量处理单元或物联网处理器等异构硬件一起工作时，您可能需要使用供应商特定的工具。
- en: 'After interpreting the profiling results, we can start optimizing the system,
    addressing the most visible bottlenecks. Typical approaches are arranged across
    various levels:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在解释了性能分析结果之后，我们可以开始优化系统，解决最明显的瓶颈。典型的方法分布在不同的层级：
- en: '*Model-related optimizations* like architecture changes, pruning, quantization,
    distillation, feature selection, etc.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*与模型相关的优化*，如架构变化、剪枝、量化、蒸馏、特征选择等。'
- en: '*Serving-related optimizations* like batching, caching, precomputation, etc.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*与服务器相关的优化*，如批处理、缓存、预计算等。'
- en: '*Code-related optimizations* like more effective low-level algorithms, using
    more effective paradigms (like vectorized algorithms instead of for loops), or
    rewriting in a faster library/framework (e.g., numba for numeric computation)
    or even language (e.g., replacing Python bottleneck with a faster wrapper on top
    of C++ or Rust alternative).'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*与代码相关的优化*，如更有效的低级算法，使用更有效的范式（如使用向量化算法而不是for循环），或者用更快的库/框架（例如，numba用于数值计算）甚至语言（例如，用C++或Rust的替代方案替换Python的瓶颈）重写。'
- en: '*Hardware-related optimizations* like using more powerful hardware, vertical/horizontal
    scaling (please refer to chapter 13), etc.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*与硬件相关的优化*，如使用更强大的硬件，垂直/水平扩展（请参阅第13章）等。'
- en: 15.4.2 The best optimizing is minimum optimizing
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.4.2 最佳优化是最小优化
- en: If we step away from the operating level and give an overview of optimizing
    from the overall design perspective, some of the problems that arise during the
    maintenance stage can be avoided if the system has been given a thorough treatment
    in accordance with the original requirements during the design stage. If we are
    aware of strict latency requirements, we should initially choose a model that
    is fast enough for the target platform. Of course, we can reduce the memory footprint
    with quantization or pruning or reduce latency with distillation, but it is better
    to start with a model close to the target requirements rather than trying to speed
    it up when the system is ready. From our experience, an urgent need for optimization
    is a result of poor choices at the beginning of the system’s life cycle (e.g.,
    heavy models were used as a baseline), an unexpected success case (a startup built
    a quick prototype and suddenly needs to scale), or a planned tech debt (“Okay,
    we’ve built something really suboptimal but fast for now; if it survives and helps
    us finding the product-market fit, we will clean it up”).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从操作级别退一步，从整体设计角度概述优化，如果在设计阶段根据原始要求对系统进行了彻底的处理，那么在维护阶段出现的一些问题可以避免。如果我们意识到严格的延迟要求，我们应该最初选择一个足够快的模型以适应目标平台。当然，我们可以通过量化或剪枝来减少内存占用，或者通过蒸馏来减少延迟，但最好从接近目标要求的模型开始，而不是在系统准备就绪时尝试加速它。根据我们的经验，迫切需要优化是系统生命周期初期（例如，使用重模型作为基线）的糟糕选择、意外成功案例（一家初创公司快速构建了一个原型并突然需要扩展）或计划中的技术债务（“好吧，我们构建了一些现在虽然不最优但很快的东西；如果它能生存并帮助我们找到产品市场匹配，我们将清理它”）的结果。
- en: Choosing the level of optimization is a crucial decision for effective inference.
    A founding engineer in a startup who needs to extend their minimal viable product
    to the second customer should usually just scale via straightforward renting of
    additional cloud computing resources. On the other hand, a platform engineer in
    a big tech company would likely benefit from reading “Algorithms for Modern Hardware”
    ([https://en.algorithmica.org/hpc/](https://en.algorithmica.org/hpc/)) and applying
    low-level optimizations at scale.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 选择优化级别是有效推理的关键决策。一个初创公司的创始工程师，需要将他们的最小可行产品扩展到第二个客户，通常只需通过简单租用额外的云计算资源进行扩展。另一方面，一个大科技公司的平台工程师可能会从阅读“现代硬件算法”（[https://en.algorithmica.org/hpc/](https://en.algorithmica.org/hpc/))并应用大规模的低级优化中受益。
- en: '15.5 Design document: Serving and inference'
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.5 设计文档：服务和推理
- en: A separate section of the design document dedicated to inference optimization
    should cover the anticipated actions during the maintenance stage. Let’s examine
    the commonalities and differences in inference optimization for the two rather
    divergent ML systems.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 设计文档中专门针对推理优化的一个部分应该涵盖维护阶段预期的操作。让我们来探讨两个相当不同的机器学习系统在推理优化方面的共性和差异。
- en: 15.5.1 Serving and inference for Supermegaretail
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.5.1 Supermegaretail的服务和推理
- en: Based on the key features and requirements of retail-focused ML systems, the
    solution for Supermegaretail will not require real-time involvement, allowing
    modification of the model in batches; still, it will involve a large scope of
    work.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 基于以零售为中心的机器学习系统的关键特性和要求，Supermegaretail的解决方案将不需要实时参与，允许批量修改模型；尽管如此，它仍将涉及大量工作。
- en: 'Design document: Supermegaretail'
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设计文档：Supermegaretail
- en: XII. Serving and inference
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: XII. 服务和推理
- en: The primary considerations for serving and inference are
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 服务和推理的主要考虑因素是
- en: Efficient batch throughput, as forecasts will be run daily, weekly, and monthly
    on large volumes of data
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高效的批量吞吐量，因为预测将每天、每周和每月在大量数据上运行
- en: Security of the sensitive inventory and sales data
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 敏感库存和销售数据的安全性
- en: Cost-effective architecture that can scale batch jobs
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可扩展的、成本效益的架构，可以扩展批量作业
- en: Monitoring data and prediction quality
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控数据和预测质量
- en: i. Serving architecture
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: i. 服务架构
- en: We will serve the batch demand forecasting jobs using Docker containers orchestrated
    by AWS Batch on EC2 machines. AWS Batch will allow for the definition of resource
    requirements, dynamic scaling of the required number of containers, and queuing
    of large workloads.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Docker容器在EC2机器上通过AWS Batch来处理批量需求预测任务。AWS Batch将允许定义资源需求，动态调整所需容器数量，以及排队处理大量工作负载。
- en: The batch jobs will be triggered on a schedule to process the input data from
    S3, run inferences, and output results back to S3\. A simple Flask API will allow
    on-demand batch inference requests if required.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 批量作业将按计划触发，以处理来自S3的输入数据，运行推理，并将结果输出回S3。如果需要，一个简单的Flask API将允许按需批量推理请求。
- en: All data transferring and processing will occur on secured AWS infrastructure,
    isolated from external access. Proper credentials will be used for authentication
    and authorization.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数据传输和处理都将发生在安全的AWS基础设施上，与外部访问隔离。将使用适当的凭证进行身份验证和授权。
- en: ii. Infrastructure
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ii. 基础设施
- en: The batch servers will use auto-scaling groups to match workload demands. Spot
    instances can be used to reduce costs for flexible batch jobs.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 批量服务器将使用自动扩展组来匹配工作负载需求。可以使用Spot实例来降低灵活批量作业的成本。
- en: No specialized hardware or optimizations are required at this stage, as batch
    throughput is the priority, and the batch nature allows ample parallelization.
    We will use the horizontal scalability options provided by AWS Batch and S3.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段不需要专门的硬件或优化，因为批量吞吐量是优先考虑的，批量性质允许充分的并行化。我们将使用AWS Batch和S3提供的水平扩展选项。
- en: iii. Monitoring
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: iii. 监控
- en: Key metrics to track for the batch jobs include
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 需要跟踪的批量作业的关键指标包括
- en: Job success rate, duration, and failure rate
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作业成功率、持续时间和失败率
- en: Number of rows processed per job
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个作业处理的行数
- en: 'Server utilization: CPU, memory, disk space'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器利用率：CPU、内存、磁盘空间
- en: Prediction accuracy compared to actual demand
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与实际需求相比的预测准确性
- en: Data validation checks and alerts
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据验证检查和警报
- en: This monitoring will help ensure the batch process remains efficient and scalable
    and produces high-quality predictions. We can assess optimization needs in the
    future based on production data.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这种监控将有助于确保批处理过程保持高效和可扩展，并产生高质量的预测。我们可以根据生产数据在未来评估优化需求。
- en: 15.5.2 Serving and inference for PhotoStock Inc.
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.5.2 PhotoStock Inc. 的服务和推理
- en: 'Search engine optimization includes two main components:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索引擎优化包括两个主要组件：
- en: Real-time processing of user requests, where the overall number of users is
    prone to seasonal drifts (e.g., the day/night difference) with possible drastic
    spikes.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时处理用户请求，其中用户总数容易受到季节性波动（例如，昼夜差异）的影响，可能出现剧烈的峰值。
- en: The index used for the photo search itself, which should be regularly updated.
    Similar to the Supermegaretail case, a real-time approach is not required but
    will require processing large amounts of data.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于照片搜索的索引本身，应定期更新。类似于Supermegaretail案例，不需要实时方法，但将需要处理大量数据。
- en: 'Design document: PhotoStock Inc.'
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设计文档：PhotoStock Inc.
- en: XII. Serving and inference
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: XII. 服务和推理
- en: 'Given that our search engine is based on vector similarity, there are two aspects
    we need to care about: generating vectors for searchable items (updating the index)
    and searching for user queries (querying the index). Those two aspects have different
    requirements and constraints, so we will design them separately.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的搜索引擎基于向量相似性，有两个方面我们需要关注：为可搜索项生成向量（更新索引）和搜索用户查询（查询索引）。这两个方面有不同的要求和约束，因此我们将分别设计它们。
- en: i. Index update
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: i. 索引更新
- en: Updating the index is a batch process that happens once a day (as mentioned
    in chapter 13). Other than regular updates, we also need to support the initial
    index creation or re-creation if the core model is updated. Although it’s a relatively
    rare event, it is important to have a process that can be run on demand.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 更新索引是一个每天发生一次的批处理过程（如第13章所述）。除了常规更新外，我们还需要支持在核心模型更新时进行初始索引创建或重新创建。尽管这是一个相对罕见的事件，但拥有一个可以按需运行的流程是很重要的。
- en: 'Both cases share the same characteristics:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种情况具有相同的特征：
- en: Mild latency requirements
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轻度延迟要求
- en: Strict throughput requirements
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 严格的吞吐量要求
- en: We need to be able to process a large number of items in a reasonable time at
    a reasonable cost. For the rough estimates, we should support reindexing ~10e5
    items per day within several hours. We also need to be able to reindex ~10e8 items
    within a reasonable time if the core model is updated.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在合理的时间内以合理的成本处理大量项目。对于粗略估计，我们应该支持在几小时内支持每天重新索引约10e5个项目。如果核心模型更新，我们还需要能够在合理的时间内重新索引约10e8个项目。
- en: ii. Index query
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ii. 索引查询
- en: Querying the index is a real-time process that engages every user query. We
    need to minimize the latency of the query so that our throughput requirements
    are not too high, as our average number of searches is 150,000 per day (please
    see chapter 12), which is about 2 queries per second. However, the number of queries
    is not evenly distributed, and we need to be able to handle peak loads of ~100
    queries per second, as well as upscale and downscale quickly in cases of traffic
    spikes.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 查询索引是一个实时过程，涉及每个用户查询。我们需要最小化查询的延迟，以便我们的吞吐量要求不是太高，因为我们的平均搜索量每天约为150,000次（请参阅第12章），大约每秒2个查询。然而，查询数量并不均匀分布，我们需要能够处理每秒约100个查询的峰值负载，以及在流量峰值时能够快速扩容和缩容。
- en: We suggest using the same model converted to ONNX for both batch and real-time
    inferences. It is not a requirement, but it will simplify the design and maintenance
    of the system. However, the inference process is different for each batch, and
    real-time should be separated given the different requirements.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议使用转换为ONNX的相同模型，用于批量和实时推理。这不是强制要求，但它将简化系统的设计和维护。然而，推理过程对每个批次都是不同的，鉴于不同的要求，实时推理应该分开。
- en: iii. Framework and hardware
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: iii. 框架和硬件
- en: From a software perspective, we will use Nvidia Triton Inference Server as a
    serving framework. It is a high-performance open source inference serving software
    that supports ONNX and has a lot of features that simplify the serving process.
    We will use the HTTP API of Triton Inference Server to communicate with it from
    our application. We will use the same model for batch and real-time inference,
    but the real-time inference will use a more latency-optimized version of the config
    (e.g., dynamic batching’s parameter `max_queue_delay_microseconds` should be under
    10 ms).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 从软件角度来看，我们将使用Nvidia Triton推理服务器作为服务框架。它是一个高性能的开源推理服务软件，支持ONNX，并具有许多简化服务过程的特性。我们将使用Triton推理服务器的HTTP
    API从我们的应用程序与其通信。我们将使用相同的模型进行批量和实时推理，但实时推理将使用更优化的配置版本（例如，动态批次的参数`max_queue_delay_microseconds`应低于10毫秒）。
- en: 'For batch inference, we will use a default solution by our cloud provider:
    AWS Sagemaker. It is a managed service that allows us to run the batch inference
    on configurable instances, it is easy to scale if needed, and it is integrated
    with other AWS services we use. We can consider using spot instances under the
    hood to reduce the cost of the batch inference. The batch job itself will be a
    simple script on top of the real-time inference that adds an IO layer, reading
    the data from the queue and writing the results to both S3 and the database.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对于批量推理，我们将使用云提供商的默认解决方案：AWS Sagemaker。它是一种托管服务，允许我们在可配置的实例上运行批量推理，如果需要，易于扩展，并且与其他我们使用的AWS服务集成。我们可以考虑在底层使用spot实例以降低批量推理的成本。批量作业本身将是在实时推理之上的简单脚本，它添加了一个IO层，从队列中读取数据并将结果写入S3和数据库。
- en: For real-time inference, it would be nice to have a serverless solution that
    can scale to zero when there are no queries. However, given our high load and
    low latency requirements, this may be hard to achieve using major providers like
    AWS Lambda; thus, we will use a more traditional approach with a pool of servers
    behind our load balancer. We will use AWS EC2 instances for the servers and AWS
    Application Load Balancer for the load balancer. We can use spot instances under
    the hood to reduce the cost of the real-time inference because each worker will
    be stateless, and we can easily replace it with a new one if it is terminated
    by AWS. We need to make sure that the system has a reasonable number of available
    workers guaranteed and enable additional scaling if needed.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实时推理，如果能有一个无服务器解决方案，在无查询时可以扩展到零，那将很理想。然而，鉴于我们高负载和低延迟的要求，使用像AWS Lambda这样的主要提供商可能难以实现；因此，我们将采用更传统的方案，在负载均衡器后面使用服务器池。我们将使用AWS
    EC2实例作为服务器，AWS应用程序负载均衡器作为负载均衡器。我们可以使用spot实例在底层降低实时推理的成本，因为每个工作器是无状态的，如果它被AWS终止，我们可以轻松地用一个新的替换它。我们需要确保系统有合理数量的可用工作器保证，并在需要时启用额外的扩展。
- en: An exact hardware configuration for both batch and real-time jobs is a subject
    of future experiments; obviously, we need to use GPU instances for both, but the
    exact type of GPU and other resources is not clear yet. We don’t expect heavy
    CPU usage given that the preprocessing is relatively simple, but we need to ensure
    we have enough CPU and avoid it becoming a bottleneck; thorough monitoring of
    resource usage (CPU/RAM/GPU) is required. The exact number of instances under
    the balancer is also subject to future experiments.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 批量和实时作业的精确硬件配置是未来实验的主题；显然，我们需要使用GPU实例，但确切的GPU类型和其他资源尚不明确。鉴于预处理相对简单，我们预计不会出现大量的CPU使用，但我们需要确保我们有足够的CPU并避免它成为瓶颈；需要彻底监控资源使用（CPU/RAM/GPU）。负载均衡器下的实例数量也是未来实验的主题。
- en: iv. Auxiliary infrastructure
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: iv. 辅助基础设施
- en: We will start model serving with a default float32 precision, but we will experiment
    with lower precision (e.g., float16) later to reduce the serving cost. Optimizing
    the model itself for latency can be done later as well, although at the moment
    we don’t expect specific bottlenecks, as the CLIP model is relatively simple.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用默认的float32精度开始模型服务，但稍后我们将尝试使用更低的精度（例如，float16）以降低服务成本。对模型本身进行延迟优化也可以稍后进行，尽管目前我们预计不会有特定的瓶颈，因为CLIP模型相对简单。
- en: Because queries are more popular than others, we can use a cache to reduce the
    load on the inference servers. We can use AWS Elasticache for that; it is a managed
    service that supports Redis and Memcached. We can use a simple key-value cache
    with a low time to live (the exact number is subject to data analysis). Caching
    is useful for the runtime inference, not for the batch inference; the batch inference
    should be responsible for updating the cache, though, if the key has changed.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 由于查询比其他操作更受欢迎，我们可以使用缓存来减轻推理服务器的负载。我们可以使用AWS Elasticache来实现这一点；它是一个支持Redis和Memcached的托管服务。我们可以使用一个简单的键值缓存，其生存时间（确切数字取决于数据分析）。缓存对于运行时推理很有用，但不适用于批量推理；尽管如此，如果键已更改，批量推理应负责更新缓存。
- en: 'We need to make sure that the system can scale to handle the peak load. This
    part of the design should be refined further with the site reliability engineering
    team and AWS experts. At the initial stage, we want to ensure autoscaling is enabled
    for the real-time inference and all relevant metrics (e.g., resource usage, #
    of requests, # of active instances) are monitored + alerts are configured.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要确保系统可以扩展以处理峰值负载。这部分设计应与站点可靠性工程团队和AWS专家进一步细化。在初始阶段，我们希望确保实时推理启用了自动扩展，并且所有相关指标（例如，资源使用情况、请求数、活动实例数）都进行了监控+配置了警报。
- en: No additional security measures are required for the serving system, given that
    it is not exposed to the internet and is only accessible from our application.
    We need to make sure that access to the serving system is restricted to our application
    only and that access is granted only to the required resources (e.g., we don’t
    need to give the serving system any access to the database).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 由于服务系统没有暴露在互联网上，并且只能从我们的应用程序访问，因此不需要额外的安全措施。我们需要确保对服务系统的访问仅限于我们的应用程序，并且仅授予所需的资源访问权限（例如，我们不需要给服务系统提供对数据库的任何访问权限）。
- en: Summary
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: While it may be tempting to limit your efforts to developing and training your
    system, inference optimization is just as important a step that will ensure stable
    performance when you hit the operation stage.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然将精力限制在开发和训练系统上可能很有吸引力，但推理优化同样是一个重要的步骤，它将确保你在进入运营阶段时获得稳定的性能。
- en: There are key factors that affect the way you will design the inference optimization
    process for your system. These include latency, throughput, scalability, target
    platforms, cost, reliability, flexibility, and security and privacy.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 影响你为系统设计推理优化过程的关键因素包括延迟、吞吐量、可扩展性、目标平台、成本、可靠性、灵活性和安全性与隐私。
- en: The aforementioned factors can be conflicting or even mutually exclusive. Hence,
    it would be impossible to optimize for all of them simultaneously, which will
    inevitably force you to go for tradeoffs to finetune your model in the best way
    possible.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上述因素可能存在冲突，甚至相互排斥。因此，同时优化所有这些因素是不可能的，这不可避免地会迫使你进行权衡，以尽可能最佳地微调你的模型。
- en: Remember to train your model on one framework and then convert it for further
    inference in another. This step is especially important, as it won’t affect the
    inference pipeline once you need to switch to a different framework with new features.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记得在一个框架上训练你的模型，然后再将其转换为在另一个框架上进行进一步推理。这一步尤其重要，因为一旦你需要切换到具有新特性的不同框架，它就不会影响推理流程。
- en: One of your primary goals will be achieving the balance between research flexibility
    and high performance in the production environment. Bear in mind, however, that
    it will require an interframework medium.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的主要目标之一将是实现研究灵活性和生产环境中高性能之间的平衡。然而，请记住，这需要跨框架的介质。
- en: The most important point is that the best optimizing is minimum optimizing.
    Some of the problems that arise during the maintenance stage can be avoided if
    the system has been properly designed in accordance with the initial requirements.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最重要的是，最佳优化是最小优化。如果在维护阶段，系统已经根据初始要求得到适当设计，那么可以避免一些问题的出现。
