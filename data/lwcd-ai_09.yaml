- en: Chapter 9\. Next Steps in Your AI Journey
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 9 章\. AI 旅程中的下一步
- en: Throughout the course of this book, you have learned how data can drive decision
    making in your business with an enterprise ML workflow, how to understand your
    data with an eye toward building ML models, and what tools are available for building
    ML models. You have discovered how to use AutoML to train your regression and
    classification models, how to create custom low-code models using SQL in BigQuery
    ML, how to create custom code models using the scikit-learn and TensorFlow framework,
    and then finally how to improve your custom model performance with further feature
    engineering and hyperparameter tuning. Hopefully, you have found this journey
    to be equally enlightening and enjoyable. For many, that should be more than enough
    to enable you to infuse ML into your problem-solving processes.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的学习过程中，您已经了解了如何通过企业ML工作流程驱动业务决策的数据，如何通过理解数据来构建ML模型，并了解了用于构建ML模型的工具。您已经学会了如何使用AutoML训练回归和分类模型，如何在BigQuery
    ML中使用SQL创建自定义低代码模型，如何使用scikit-learn和TensorFlow框架创建自定义代码模型，以及如何通过进一步的特征工程和超参数调整提高自定义模型的性能。希望您觉得这段旅程同样启发和愉快。对于许多人来说，这应该足以让您能够将ML融入到解决问题的流程中去。
- en: For others, this is only the beginning of a longer journey into ML and AI. This
    chapter explores where to go next. You will learn about other important topics
    in data science and ML operations (or MLOps). You will also be pointed toward
    many wonderful resources to grow your knowledge beyond this book.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他人来说，这只是进入机器学习和人工智能更长旅程的开始。本章探讨了接下来的方向。您将了解数据科学和ML运维（或MLOps）中的其他重要主题。还会指引您去很多超越本书的知识资源。
- en: Going Deeper into Data Science
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探讨数据科学
- en: There is no universally agreed-upon definition for *data science* or a *data
    scientist*. A decent approximation of such a definition could be that *data science*
    is the discipline that uses various tools from other disciplines to extract insights
    from datasets. These various tools come from other areas such as mathematics,
    statistics, computer science, and occasionally different areas depending on the
    problem at hand.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据科学*或*数据科学家*并没有普遍认可的定义。这样的定义的一个合理近似可能是，*数据科学*是利用来自其他学科的各种工具从数据集中提取洞见的学科。这些各种工具来自于数学、统计学、计算机科学，以及根据手头问题的不同情况，可能还涉及其他领域。'
- en: All of the datasets that you worked with in this book have been *structured*
    datasets—datasets with a well-defined schema. Most business problems involve structured
    data, and you picked up a wonderful skill set for exploring structured data. However,
    *unstructured* datasets are becoming increasingly important as ML becomes more
    mature as a discipline. Recall from [Chapter 2](ch02.html#data_is_the_first_step)
    that unstructured data includes images, videos, sound files, and text. A large
    amount of research over the past decade has gone into various aspects of ML for
    unstructured data.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中您所使用的所有数据集都是*结构化*数据集——具有明确定义模式的数据集。大多数业务问题涉及结构化数据，并且您已经掌握了探索结构化数据的精湛技能。然而，随着ML学科的成熟，*非结构化*数据集变得越来越重要。回顾[第
    2 章](ch02.html#data_is_the_first_step)，非结构化数据包括图像、视频、声音文件和文本。在过去十年中，大量研究已经涉及ML在非结构化数据方面的各个方面。
- en: A type of AI that has become increasingly important recently is *generative
    AI*, referring to models that generate various types of data such as images, videos,
    and so on. Recently, generative AI has become a very popular and fast-growing
    field, with image generation models such as [Midjourney](http://www.midjourney.com)
    and [Craiyon](http://craiyon.com), and chatbots (contextual text generation) such
    as [ChatGPT](https://openai.com/blog/chatgpt) and [Bard](http://bard.google.com).
    Additionally, generative AI capabilities are being included in many commercial
    products such as [Bing](https://www.bing.com) (ChatGPT), [Google Search](http://google.com)
    ([Search Generative Experience](https://oreil.ly/szZB7)), and [Amazon CodeWhisperer](https://aws.amazon.com/codewhisperer).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 最近变得越来越重要的一种AI类型是*生成式AI*，指的是能够生成各种类型数据，如图像、视频等的模型。最近，生成式AI已成为一个非常流行和快速增长的领域，例如图像生成模型如[Midjourney](http://www.midjourney.com)和[Craiyon](http://craiyon.com)，以及聊天机器人（上下文文本生成）如[ChatGPT](https://openai.com/blog/chatgpt)和[Bard](http://bard.google.com)。此外，生成式AI的能力已经被纳入许多商业产品中，如[Bing](https://www.bing.com)（ChatGPT）、[Google
    Search](http://google.com)（[Search Generative Experience](https://oreil.ly/szZB7)）和[Amazon
    CodeWhisperer](https://aws.amazon.com/codewhisperer)。
- en: The more complex models become, the harder they are to easily understand. For
    example, when you learned about linear regression in [Chapter 6](ch06.html#using_bigquery_ml_to_train_a_linear_reg),
    you saw that the weights of the model gave a clear understanding of the importance
    of the individual features. For even a neural network with only one hidden layer,
    there is no longer an easy-to-describe connection between the weights of the model
    and the importance of the features being used. This becomes even more difficult
    with the very large models that are used for problems with unstructured data and
    generative models.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 模型变得越复杂，理解起来就越困难。例如，在您学习[第六章](ch06.html#using_bigquery_ml_to_train_a_linear_reg)中的线性回归时，您会发现模型的权重清楚地显示了各个特征的重要性。即使是只有一个隐藏层的神经网络，模型的权重与使用的特征的重要性之间也不再有简单的描述连接。在处理非结构化数据和生成模型问题时，使用非常大的模型会变得更加困难。
- en: This section dives a little deeper into various resources and offers additional
    resources to explore these topics if you so choose.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将更深入地探讨各种资源，并提供额外的资源，以便您选择探索这些主题。
- en: Working with Unstructured Data
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理非结构化数据
- en: Unstructured data is defined as data without a schema. Some classic examples
    were mentioned before, such as images and text. Recall that ML models are ultimately
    mathematical functions that take numeric inputs and have numeric outputs, which
    you then interpret. How do you interpret an image or a sentence as numeric input?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 非结构化数据被定义为没有模式的数据。一些经典的例子如图像和文本。请记住，ML模型最终是将数值输入并具有数值输出的数学函数，然后您进行解释。如何将图像或句子解释为数值输入？
- en: Working with image data
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理图像数据
- en: For images, the story is simpler than you may expect. Every image is represented
    as an array of pixel values. For example, consider the pixelated image of a handwritten
    digit in [Figure 9-1](#a_low_resolution_grayscale_image_of_a_h). The image on
    the left is a low-resolution version of the handwritten digit 2\. That image consists
    of a 12 × 12 grid of blocks known as pixels. The *pixel values* for this grayscale
    image range between 0 and 255\. 0 represents black, 255 represents white, and
    values in between represent different values of gray. In the second image, you
    can see the actual pixel values for the image as an array.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像，情况比您预期的要简单。每个图像都表示为像素值数组。例如，考虑[图9-1](#a_low_resolution_grayscale_image_of_a_h)中手写数字的像素化图像。左侧的图像是手写数字2的低分辨率版本。该图像由一个12×12的像素块网格组成。这种灰度图像的*像素值*范围在0到255之间。0代表黑色，255代表白色，中间的值代表不同的灰度值。在第二幅图像中，您可以看到图像的实际像素值作为数组。
- en: '![A low-resolution grayscale image of a handwritten 2 and the corresponding
    pixel values](assets/lcai_0901.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![手写数字2的低分辨率灰度图像及其相应的像素值](assets/lcai_0901.png)'
- en: Figure 9-1\. A low-resolution grayscale image of a handwritten 2 and the corresponding
    pixel values.
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-1\. 手写数字2的低分辨率灰度图像及其相应的像素值。
- en: 'For color images, the idea is very similar. Color images consist of three *channels*:
    red, green, and blue. For each pixel, there is a value between 0 and 255 for each
    of these channels. The values for these channels together are usually called *RGB
    values*. For example, white is represented by `[255,255,255]` and yellow is represented
    by `[255,255,0]`. There are a lot of simple tools, such as the one at [RapidTables](https://oreil.ly/gby4M),
    that allow you to explore different colors and see their RGB values.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于彩色图像，思路非常相似。彩色图像由三个*通道*组成：红色、绿色和蓝色。对于每个像素，每个通道的值介于0和255之间。这些通道的值通常被称为*RGB值*。例如，白色由`[255,255,255]`表示，黄色由`[255,255,0]`表示。有很多简单的工具，比如在[RapidTables](https://oreil.ly/gby4M)上的工具，可以让您探索不同的颜色并查看它们的RGB值。
- en: Now that you understand images as either two-dimensional (black-and-white images)
    or three-dimensional arrays (color images) of numeric values, you may now have
    an inkling of how you might use these values in ML. These are your numeric inputs
    for your image models.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您理解了图像可以作为数值值的二维（黑白图像）或三维数组（彩色图像），您现在可能对如何在ML中使用这些值有所了解。这些是您图像模型的数值输入。
- en: The “hello world” example for image classification is the problem of handwritten
    digit recognition using what is known as the MNIST (Modified National Institute
    of Standards and Technology) dataset. This is a dataset of 60,000 training images
    and 10,000 test images that are well balanced between the 10 handwritten digits
    (0 through 9). These images are 28 × 28 grayscale images. You can see an example
    of one of these images in [Figure 9-2](#an_example_of_a_handwritten_seven_in_th).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类的“hello world”示例是使用所谓的MNIST（修改过的国家标准与技术研究所）数据集进行手写数字识别的问题。这是一个包含60,000个训练图像和10,000个测试图像的数据集，这些图像在10个手写数字（0到9）之间很好地平衡。这些图像是28×28的灰度图像。你可以在[图9-2](#an_example_of_a_handwritten_seven_in_th)中看到其中一个示例图像。
- en: '![An example of a handwritten 7 in the MNIST dataset](assets/lcai_0902.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![MNIST数据集中手写数字7的示例](assets/lcai_0902.png)'
- en: Figure 9-2\. An example of a handwritten 7 in the MNIST dataset.
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-2\. MNIST数据集中手写数字7的示例。
- en: 'Classifying each image as the corresponding number is an example of a multiclass
    classification problem. We did not explore beyond two classes in the examples
    in this book, but the rough idea is similar. The model will predict a probability
    for each digit, and the most likely digit will be taken as the predicted label.
    You can use linear classification and neural network classifiers in a similar
    manner to what you used in [Chapter 7](ch07.html#training_custom_ml_models_in_python),
    but there are also additional tools, such as convolutional layers, that are very
    useful when working with image data. These additional tools are beyond the scope
    of this book, but here are a couple of useful resources for those who want to
    learn more:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 将每个图像分类为相应的数字是多类别分类问题的一个示例。我们在本书的示例中没有探讨超过两个类别，但大致思路是相似的。模型将预测每个数字的概率，最可能的数字将被视为预测标签。你可以像在[第7章](ch07.html#training_custom_ml_models_in_python)中使用线性分类和神经网络分类器一样使用它们，但也有其他附加工具，如卷积层，在处理图像数据时非常有用。这些额外的工具超出了本书的范围，但以下是一些想要进一步学习的有用资源：
- en: '[Kaggle competition and tutorials for working with the MNIST dataset](https://oreil.ly/Vvp1d)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kaggle竞赛和教程，用于处理MNIST数据集](https://oreil.ly/Vvp1d)'
- en: '[Google’s ML Crash Course on Image Classification](https://oreil.ly/_ea_0)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Google的ML崩溃课程，关于图像分类](https://oreil.ly/_ea_0)'
- en: Working with text data
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理文本数据
- en: Another common type of unstructured data that you may encounter in your ML models
    is text data. For example, what if you wanted to use comments in reviews to understand
    why your customers gave certain positive or negative ratings? You need to have
    a method to turn text data into numeric data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的ML模型中可能会遇到的另一种常见的非结构化数据类型是文本数据。例如，如果你想要使用评论中的文本数据来理解为什么你的客户给出了某些积极或消极的评分，你需要有一种方法将文本数据转化为数值数据。
- en: The simplest way of performing this task is to use one-hot encoding like you
    have already done for categorical data in previous problems. For example, you
    could have the vocabulary `['red', 'blue', 'green']`. If you have the word `'blue'`,
    then the corresponding value would be `[0,1,0]`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此任务的最简单方法是像在前面的问题中对分类数据使用的一样使用独热编码。例如，你可以有词汇表`['red', 'blue', 'green']`。如果有单词`'blue'`，则相应的值将是`[0,1,0]`。
- en: This can become tricky very quickly, however. If every different word has a
    different corresponding value, then you can end up with a very high-dimensional
    feature. Certain words will appear rarely, or not at all, in the training set,
    so the model may struggle to learn the meaning of those words.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这很快就会变得棘手。如果每个不同的单词都有一个不同的对应值，那么你可能会得到一个非常高维的特征。某些单词在训练集中可能很少出现，甚至根本不出现，因此模型可能会难以学习这些单词的含义。
- en: One strategy is to use *n-grams* instead of individual words. n-grams are continuous
    sequences of *n* words. For example, in the sentence *The cow jumped over the
    moon*, the 2-grams (or *bigrams*) are `['The cow','cow jumped', 'jumped over',
    'over the', 'the moon']`. For spam detection, 3-grams and 4-grams tend to be more
    useful features than individual words. Intuitively we can see this in an explicit
    example. A spam email may have a sentence like `"You have won the lottery and
    are now rich!"` as part of the body. 1-grams and 2-grams will look at fragments
    of the sentence that are too granular to capture context, except for `"now rich!"`,
    which could tip off a person or model that the email is spam. For example, `"won"`
    ,`"the"`, and `"lottery"` are missing context individually needed to confirm the
    email is spam. On the other hand, the 3-gram `"won the lottery"` would throw up
    an immediate red flag for most people looking out for spam emails.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一个策略是使用*n-grams*而不是单个单词。 n-grams 是*n*个连续单词的序列。例如，在句子*The cow jumped over the
    moon*中，2-grams（或*bigrams*）是`['The cow', 'cow jumped', 'jumped over', 'over the',
    'the moon']`。对于垃圾邮件检测，3-grams 和 4-grams 往往比单个单词更有用。直观地，我们可以在一个明确的例子中看到这一点。垃圾邮件可能包含像“你赢得了彩票，现在变得富有！”这样的句子作为正文的一部分。1-grams
    和 2-grams 将查看句子的片段，这些片段对于捕获上下文来说太细粒度了，除了“now rich!”之外，这可能会提示人或模型该邮件是垃圾邮件。例如，“won”，“the”和“lottery”缺少单独需要确认该邮件是垃圾邮件的上下文。另一方面，3-gram
    “won the lottery”对于大多数警惕寻找垃圾邮件的人来说会立即引起警觉。
- en: Another strategy is to use word embeddings. A *word embedding* is a representation
    of a word in some number of dimensions to try to capture its meaning and relationship
    to other words. For example, the word *king* could be represented as `[0.5, 0.7]`.
    Ideally, a word embedding will place words that are similar close to each other.
    For example, as shown in [Figure 9-3](#an_example_two_dimensional_word_embeddi),
    the embedding for *dog* and *puppy* will be close to each other, as will *cat*
    and *kitten*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个策略是使用词嵌入。*词嵌入*是单词在某些维度上的表示，旨在捕捉其含义和与其他单词的关系。例如，单词*king*可以表示为`[0.5, 0.7]`。理想情况下，词嵌入将会将相似的单词放置在彼此附近。例如，如[Figue 9-3](#an_example_two_dimensional_word_embeddi)所示，*dog*和*puppy*的嵌入将彼此靠近，*cat*和*kitten*也将如此。
- en: '![An example two-dimensional word embedding of “dog,” “puppy,” “cat,” and “kitten”](assets/lcai_0903.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![“dog”、“puppy”、“cat”和“kitten”的示例二维词嵌入](assets/lcai_0903.png)'
- en: Figure 9-3\. An example two-dimensional word embedding of “dog,” “puppy,” “cat,”
    and “kitten.”
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-3\. “dog”、“puppy”、“cat”和“kitten”的示例二维词嵌入。
- en: Also note that the distance and direction between the pair “dog” and “puppy”
    and the pair “cat” and “kitten” are very similar. With a word embedding, you would
    expect a similar relationship between any animal and their babies (say, “sheep”
    and “lamb”).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，“dog”和“puppy”以及“cat”和“kitten”之间的距离和方向非常相似。使用词嵌入，您会期望在任何动物及其幼崽之间（比如“sheep”和“lamb”）之间有类似的关系。
- en: Word embeddings are models in their own right and are often trained within the
    context of a specific problem. More generally, different preprocessing models
    and tools exist for converting words and sentence fragments into numeric inputs.
    BERT (Bidirectional Encoder Representations from Transformers) preprocessing is
    a popular way of converting text input into numeric input for input into your
    model.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入本身就是模型，并且通常在特定问题的上下文中进行训练。更一般地，存在不同的预处理模型和工具，用于将单词和句子片段转换为数值输入。BERT（双向编码器转换器表示）预处理是将文本输入转换为模型输入的一种流行方式。
- en: 'To learn more about working with text in ML models, here are a few useful resources:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于在ML模型中处理文本的信息，请参考以下几个有用的资源：
- en: '[Word embeddings in Keras/TensorFlow](https://oreil.ly/WQesS)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Keras/TensorFlow中的词嵌入](https://oreil.ly/WQesS)'
- en: '[Preprocessing data in TensorFlow using BERT](https://oreil.ly/Sqn-7)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用BERT在TensorFlow中预处理数据](https://oreil.ly/Sqn-7)'
- en: '[Working with text data in scikit-learn](https://oreil.ly/4EgWi)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在scikit-learn中处理文本数据](https://oreil.ly/4EgWi)'
- en: '[Yelp review dataset](https://oreil.ly/-NlFC): A great dataset for working
    with text data to predict review scores'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Yelp 评论数据集](https://oreil.ly/-NlFC)：一个用于处理文本数据以预测评论分数的优秀数据集。'
- en: Generative AI
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成式AI
- en: The classification models we have discussed so far in this book were discriminative
    models. *Discriminative* models have the goal of predicting which class an instance
    belongs to. For example, predicting whether a transaction is fraudulent or legitimate.
    Generative models are in some sense the inverse problem. The model generates the
    instance from the label instead. For example, given the label “a cat playing a
    banjo,” an image of a cat playing a banjo is generated.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书中讨论的分类模型是判别模型。*判别*模型的目标是预测实例属于哪个类别。例如，预测交易是欺诈还是合法。生成模型在某种程度上是相反的问题。该模型从标签生成实例。例如，给定标签“一只弹奏班卓琴的猫”，将生成一张弹奏班卓琴的猫的图像。
- en: There are free tools, such as [craiyon.com](http://craiyon.com), which can be
    used to have fun playing around with these sorts of models. An example of this
    is shown in [Figure 9-4](#an_image_generated_using_the_prompt_quo).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 有免费工具，比如[craiyon.com](http://craiyon.com)，可以用来玩弄这些模型。其中一个示例显示在[图9-4](#an_image_generated_using_the_prompt_quo)中。
- en: '![An image generated using the prompt “a cat playing a banjo.” This image was
    generated using craiyon.com.](assets/lcai_0904.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![使用提示“一只弹奏班卓琴的猫”生成的图像。该图像使用craiyon.com生成。](assets/lcai_0904.png)'
- en: Figure 9-4\. An image generated using the prompt “a cat playing a banjo.” This
    image was generated using [craiyon.com](http://craiyon.com).
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-4\. 使用提示“一只弹奏班卓琴的猫”生成的图像。该图像使用[craiyon.com](http://craiyon.com)生成。
- en: Generative AI has become a major topic of discussion recently due to its use
    in chatbots. ChatGPT and Bard are two such examples of generative AI being used
    for chatbots. Underlying these products are large language models, or LLMs. The
    term *LLM* is a bit vague, but it is used for language models trained on large
    datasets with massive amounts of parameters. For example, GPT-3.5, the original
    model underlying ChatGPT, had more than 175 billion parameters and a corpus of
    over a half trillion tokens.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 生成人工智能近来因其在聊天机器人中的应用成为了讨论的主要话题。ChatGPT 和 Bard 就是生成人工智能应用于聊天机器人的两个例子。这些产品的核心是大型语言模型，或称为LLMs。术语*LLM*有点模糊，但指的是在大规模数据集上训练的参数极为庞大的语言模型。例如，ChatGPT
    的原始模型 GPT-3.5 就有超过1750亿个参数，并拥有超过五百万亿个标记。
- en: There are a lot of interesting conversations about the role of generative AI
    in society and how we as a society should be interacting with it. A deep conversation
    about the ethics of responsible use of generative AI is beyond the scope and purpose
    of this book, but these are important conversations moving forward.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 关于生成人工智能在社会中的角色以及我们作为社会应如何与其互动，有很多有趣的讨论。关于负责任使用生成人工智能的伦理深入讨论超出了本书的范围和目的，但这些是未来重要的讨论。
- en: If you’re interested in learning more about generative AI, [ChatGPT 101 on Coursera](https://oreil.ly/KrRFm)
    is a great resource to explore the use and implications of this new technology.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对深入了解生成人工智能感兴趣，[Coursera上的ChatGPT 101](https://oreil.ly/KrRFm)是一个探索这种新技术使用和影响的好资源。
- en: Explainable AI
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可解释人工智能（Explainable AI）
- en: Chapters [4](ch04.html#use_automl_to_predict_advertising_media) and [5](ch05.html#using_automl_to_detect_fraudulent_trans)
    introduced feature attributions for your models, and [Chapter 6](ch06.html#using_bigquery_ml_to_train_a_linear_reg)
    introduced the field of *explainable AI*, or XAI, in general for structured data.
    Many XAI techniques also exist for unstructured data such as images and text data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 第4章和第5章介绍了用于您的模型的特征归因，而第6章介绍了*可解释人工智能*或XAI，通常用于结构化数据。许多XAI技术也适用于非结构化数据，如图像和文本数据。
- en: XAI techniques can be broken down into intrinsic and post hoc techniques. *Intrinsic*
    techniques leverage the structure of the model to give explanations for predictions.
    Certain types of models, such as linear models and decision trees, are intrinsically
    explainable. For linear models, explored in detail in Chapters [6](ch06.html#using_bigquery_ml_to_train_a_linear_reg)
    and [7](ch07.html#training_custom_ml_models_in_python), the weights give the relative
    importance of the features.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: XAI技术可以分为内在和事后技术。*内在*技术利用模型的结构来解释预测。某些类型的模型，如线性模型和决策树，本质上是可解释的。对于线性模型，本书的第6章和第7章详细探讨了这一点，权重给出了特征的相对重要性。
- en: '*Post hoc* techniques, on the other hand, use the model’s predictions to understand
    its behavior. These techniques are applied after a model has been trained, and
    often they are trained on part of the evaluation dataset. Post hoc techniques
    can be broken down into two main categories: local and global techniques. *Local*
    techniques focus on a single instance and attempt to explain why a specific prediction
    was made for that specific instance. *Global* techniques focus on the model’s
    behavior over an entire dataset. In general, local techniques can be turned into
    global techniques by aggregating the results of a local technique over the dataset.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*事后*技术，另一方面，使用模型的预测来理解其行为。这些技术是在模型训练完成后应用的，通常它们是在评估数据集的一部分上进行训练的。事后技术可以分为两大类：局部和全局技术。*局部*技术专注于单个实例，并试图解释为什么对于该特定实例做出了特定的预测。*全局*技术专注于模型在整个数据集上的行为。一般来说，局部技术可以通过对数据集中的局部技术的结果进行聚合来转化为全局技术。'
- en: 'Both local and global techniques can be further broken down into model-agnostic
    and model-specific techniques. *Model-agnostic* techniques tend to alter the data
    and understand how it changes the predictions being made. A great example of this
    is the feature attributions that you saw in Chapters [4](ch04.html#use_automl_to_predict_advertising_media)
    and [5](ch05.html#using_automl_to_detect_fraudulent_trans) when using AutoML.
    How are these feature attributions computed? They are computed using a technique
    called *permutation feature importance*, or PFI. PFI is computed in the following
    manner: first compute the loss of the model for the dataset you want to use. Next
    compute the loss again when you permute the first column of data in place. That
    is, you permute the first column while leaving the other columns alone (see [Table 9-1](#permuting_a_column_in_placesemicolon_in)).
    The difference between the loss on the original dataset and the dataset with the
    first column permuted gives a “score” for the importance of the first feature.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 局部和全局技术都可以进一步分为与模型无关和与模型相关的技术。*与模型无关*的技术倾向于改变数据并理解它如何改变正在进行的预测。一个很好的例子是您在第[4](ch04.html#use_automl_to_predict_advertising_media)和第[5](ch05.html#using_automl_to_detect_fraudulent_trans)章节中看到的特征归因，当使用AutoML时。这些特征归因是如何计算的？它们是使用称为*排列特征重要性*或PFI的技术计算的。PFI的计算方式如下：首先计算您想要使用的数据集的模型损失。接下来，在原地排列数据的第一列时再次计算损失。也就是说，您在保持其他列不变的情况下排列第一列（参见[表 9-1](#permuting_a_column_in_placesemicolon_in)）。原始数据集的损失与第一列数据集排列后的损失之间的差异给出了第一个特征重要性的“分数”。
- en: Table 9-1\. Permuting a column in place; in this example, Column A is the column
    being permuted
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 9-1\. 在原地对一列进行排列；在此示例中，列A是被排列的列
- en: '| Column A | Column B | Column C |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 列A | 列B | 列C |'
- en: '| --- | --- | --- |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | 4 | 7 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 4 | 7 |'
- en: '| 2 | 5 | 8 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 5 | 8 |'
- en: '| 3 | 6 | 9 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 6 | 9 |'
- en: '| Column A | Column B | Column C |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 列A | 列B | 列C |'
- en: '| --- | --- | --- |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 3 | 4 | 7 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 4 | 7 |'
- en: '| 1 | 5 | 8 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 5 | 8 |'
- en: '| 2 | 6 | 9 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 6 | 9 |'
- en: Repeat this process for every column. The PFI for a column is the normalized
    score received from this process. *Normalized* here means that the scores are
    rescaled so that all of the normalized scores add up to one. PFI is easy to interpret
    and to compute in practice, but it does depend on some randomness due to the permutation.
    Often, when using PFI, you will compute the score for a column with multiple different
    permutations and average it out to try to minimize the effect of randomness.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 重复此过程以处理每一列。一列的PFI是从此过程中得到的归一化分数。*归一化*在这里意味着重新缩放分数，使所有归一化分数相加为一。PFI在实践中易于解释和计算，但它确实依赖于一些由于排列而引起的随机性。通常情况下，在使用PFI时，您将对一列进行多次不同排列并对其进行平均以尝试减少随机性的影响。
- en: The other type of technique is a *model-specific* technique. For example, *directional
    feature importances/contributions* are specific to tree-based models like decision
    trees, random forests, and gradient-boosted trees. On the other hand, a popular
    technique for neural networks is the technique of integrated gradients. *Integrated
    gradients* take advantage of the fact that neural networks are differentiable
    models. *Differentiability* is an important mathematical property that is also
    leveraged when minimizing loss functions using the gradient descent algorithm.
    The exact mathematics is beyond the scope of this book, but the idea is fairly
    straightforward.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种技术是*模型特定*的技术。例如，*方向特征重要性/贡献*专门针对基于树的模型，如决策树、随机森林和梯度提升树。另一方面，神经网络的一种流行技术是综合梯度技术。*综合梯度*利用神经网络是可微模型的事实。*可微性*是一种重要的数学性质，在使用梯度下降算法最小化损失函数时也被利用。确切的数学内容超出了本书的范围，但其思想非常直观。
- en: Take the example of an image classification model. Suppose you have trained
    a model to classify images based on their main subject. Your model takes in a
    picture of a fireboat and correctly predicts that the image was of a fireboat.
    But why did it make this prediction? Roughly speaking, the integrated gradients
    method looks at how the predictions change as you change the features and accumulate
    those changes for each individual feature. For image models, those features are
    the individual pixel values. For integrated gradients, you define a starting point
    or a *baseline*. In image models, that baseline is often a completely black image,
    though other baseline images (such as a completely white image or random noise)
    can be used depending on the circumstance. You start off with the predicted probability
    of the label for fireboat for the purely black image and then brighten up the
    pixels proportionally until you get the original image in steps. For each step,
    you compute how much the predicted probability of fireboat has changed based on
    the pixel values. This can be done by computing the *gradient*, a mathematical
    tool for understanding the rate of change of an output based on many inputs. Finally,
    you accumulate (or *integrate)* these rates of change for every pixel. The pixels
    with the highest accumulated rates of change correspond to the most important
    pixels for the prediction. In Figures [9-5](#the_baseline_image_on_the_left_with_the)
    and [9-6](#the_image_of_the_fireboatsemicolon_the), you can see this interpolation
    of images and the corresponding pixel importances using integrated gradients.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以图像分类模型为例。假设你已经训练了一个模型来基于其主题对图像进行分类。你的模型接收到一张火船的图片，并正确预测这张图片是火船。但是为什么它会做出这个预测呢？粗略地说，综合梯度方法观察当你改变特征并累积每个特征的变化时预测如何改变。对于图像模型来说，这些特征就是单个像素值。对于综合梯度来说，你定义一个起始点或*基线*。在图像模型中，基线通常是完全黑色的图像，尽管在特定情况下可以使用其他基线图像（如完全白色的图像或随机噪声）。你从纯黑图像的火船标签的预测概率开始，然后按比例增加像素直到得到原始图像。对于每一步，你计算像素值变化对火船预测概率的影响。这可以通过计算*梯度*来完成，梯度是理解输出随多个输入变化率的数学工具。最后，你累积（或*积分*）每个像素的这些变化率。具有最高累积变化率的像素对应于预测中最重要的像素。在图9-5和图9-6中，你可以看到这些插值图像及使用综合梯度确定的像素重要性。
- en: '![The baseline image on the left with the original image of the fireboat on
    the right. The middle image is an interpolation between the two.](assets/lcai_0905.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![左侧是基线图像，右侧是火船的原始图像。中间图像是两者之间的插值。](assets/lcai_0905.png)'
- en: Figure 9-5\. The baseline image on the left with the original image of the fireboat
    on the right. The middle image is an interpolation between the two.
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-5\. 左侧是基线图像，右侧是火船的原始图像。中间图像是两者之间的插值。
- en: '![The image of the fireboat; the most important pixels, as identified by integrated
    gradients, are brighter. Note that integrated gradients highlight the streams
    of water from the boat as the explanation of why this is a picture of a fireboat.](assets/lcai_0906.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![火船图像；通过综合梯度识别的最重要像素更亮。请注意，综合梯度突出了船上的水流，解释了这张图片是火船的原因。](assets/lcai_0906.png)'
- en: Figure 9-6\. The image of the fireboat; the most important pixels, as identified
    by integrated gradients, are brighter. Note that integrated gradients highlight
    the streams of water from the boat as the explanation of why this is a picture
    of a fireboat.
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-6\. 消防船图像；由综合梯度标识的最重要像素更亮。请注意，综合梯度突出了从船上的水流作为解释，说明为什么这是一幅消防船的图片。
- en: 'To learn more about XAI, [*Interpretable Machine Learning: A Guide for Making
    Black Box Models Explainable*](https://oreil.ly/4CyIF) by Christoph Molnar is
    a wonderful resource.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于XAI的信息，[*可解释机器学习：使黑匣子模型可解释的指南*](https://oreil.ly/4CyIF) 由Christoph Molnar提供了一个很好的资源。
- en: ML Operations
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ML运营
- en: In [Chapter 7](ch07.html#training_custom_ml_models_in_python) you trained an
    ML model for predicting customer churn and were able to serve predictions in your
    notebook environment. However, this is only one step in the journey to an ML model
    being put into production. As you saw, there was work to be done to ensure that
    the data was ready for training. But this does not take into account the underlying
    compute infrastructure needed for both training and serving models, managing and
    optimizing the consumption of resources being used, thinking about how to make
    your model usable for consumers, and how to monitor that model over time. [Figure 9-7](#the_hidden_complexities_of_making_ml_mo)
    shows the relative effort involved in ML code compared with the other aspects
    of productionizing ML models.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](ch07.html#training_custom_ml_models_in_python)中，您训练了一个ML模型来预测客户流失，并能够在笔记本环境中提供预测。然而，这只是将ML模型投入生产过程中的一步。正如您所看到的，需要做一些工作来确保数据准备就绪。但是这并未考虑到需要进行培训和服务模型的基础计算基础设施，管理和优化正在使用的资源消耗，考虑如何使您的模型对消费者可用，以及如何随时间监控该模型。[图 9-7](#the_hidden_complexities_of_making_ml_mo)
    显示了与生产化ML模型相关的代码相比的相对努力程度。
- en: '![The hidden complexities of making ML models usable](assets/lcai_0907.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![使ML模型可用的隐藏复杂性](assets/lcai_0907.png)'
- en: Figure 9-7\. The hidden complexities of making ML models usable.
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-7\. 使ML模型可用的隐藏复杂性。
- en: ML operations, or MLOps, is the discipline of managing all of the different
    tasks beyond the model itself. This includes managing the infrastructure, deciding
    how the model is deployed and accessed, and monitoring and updating the model
    as appropriate.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ML运营，或MLOps，是管理模型本身之外的所有不同任务的学科。这包括管理基础设施，决定模型的部署和访问方式，以及在适当时监控和更新模型。
- en: MLOps is usually the responsibility of ML engineers, data engineers, and data
    scientists to manage. However, even if you are not in one of these roles, having
    a high-level understanding of the various concerns in using ML models in production
    is very valuable. As someone building models, you can share good notes on what
    tools and data sources you are using, what data preprocessing needs to be done,
    and what features the model expects at prediction time. Doing things like including
    the preprocessing logic in the model itself, using something like transformers
    in scikit-learn or preprocessing layers in Keras, can make the deployment of training
    and prediction pipelines easier for engineers if the model is going into production.
    At most companies, there will not be a single person doing everything, so good
    communication is key.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps通常是ML工程师、数据工程师和数据科学家的责任来管理。但是，即使你不在这些角色之一，了解在生产中使用ML模型的各种关注点也非常有价值。作为构建模型的人，你可以分享关于所使用的工具和数据源、需要进行的数据预处理以及模型在预测时期望的特征的良好笔记。像在模型本身中包含预处理逻辑这样的做法，使用像scikit-learn中的transformers或Keras中的预处理层，可以使工程师在模型投入生产时更容易地部署训练和预测流水线。在大多数公司，不会有一个人做所有事情，因此良好的沟通至关重要。
- en: 'For more details on topics around MLOps, see the following resources:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于MLOps相关主题的详细信息，请参阅以下资源：
- en: '[Introduction to MLOps](https://oreil.ly/E1K_3)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MLOps简介](https://oreil.ly/E1K_3)'
- en: '[MLOps: Continuous Delivery and Automation Pipelines in Machine Learning](https://oreil.ly/X6jv0)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MLOps：机器学习中的持续交付和自动化管道](https://oreil.ly/X6jv0)'
- en: '[*Designing Machine Learning Systems*](https://www.oreilly.com/library/view/designing-machine-learning/9781098107956)
    by Chip Huyen (O’Reilly, 2022)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*设计机器学习系统*](https://www.oreilly.com/library/view/designing-machine-learning/9781098107956)
    由Chip Huyen（O’Reilly，2022）撰写'
- en: Continuous Training and Evaluation
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持续培训和评估
- en: In [Chapter 5](ch05.html#using_automl_to_detect_fraudulent_trans), you trained
    a model using AutoML to classify transactions as either fraudulent or legitimate.
    Your model performed well on the dataset that was provided, but after the model
    was being used in production for a few months, your company’s support teams shared
    that they were getting more reports of fraudulent transactions after the fact
    from customers. Your model did not flag many of these fraudulent transactions
    though. What happened?
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 5 章](ch05.html#using_automl_to_detect_fraudulent_trans)中，您使用AutoML训练了一个模型，用于将交易分类为欺诈或合法。您的模型在提供的数据集上表现良好，但在使用几个月后的生产过程中，您公司的支持团队表示他们收到了更多关于事后客户报告的欺诈交易。尽管如此，您的模型并没有标记出许多这类欺诈交易。发生了什么？
- en: Over time, different types of *drift* can occur and affect the performance of
    your model. For example, there could be a new type of fraudulent transaction that
    was not present in your original training dataset. Your model would likely not
    pick up on this. This would be an example of *data drift*, a change in the underlying
    data distribution.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，不同类型的*漂移*可能会发生，并影响您模型的性能。例如，可能会出现新类型的欺诈交易，这些交易在您原始的训练数据集中并不存在。您的模型可能不会发现这一点。这将是*数据漂移*的一个例子，即基础数据分布的变化。
- en: Another type of drift, called *concept drift*, can occur. Concept drift is where
    the relationship between the features and the labels change over time. A great
    example of this is in demand forecasting for retail. Shopping trends change over
    time, and the exact same product will sell differently at different times depending
    on the current trends. The product did not change, but the relationship between
    the product and the sales has changed.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种漂移类型被称为*概念漂移*。概念漂移是指特征与标签之间的关系随时间变化的情况。零售需求预测中有一个很好的例子。购物趋势随时间改变，同一产品在不同时间销售情况不同，这取决于当前的趋势。产品本身没有变化，但产品与销售之间的关系发生了变化。
- en: '*Continuous training* is the process of retraining the model automatically
    based on some criterion. Most often, this criterion will depend on either time
    or model performance. For example, you may want to retrain the model weekly, or
    you may want to retrain the model when a certain percentage of fraudulent transactions
    are being missed and reported by customers. For most models being used in production,
    this is a very common and important practice.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*持续训练*是根据某些标准自动重新训练模型的过程。最常见的标准通常是时间或模型性能。例如，您可能希望每周重新训练模型，或者当某个特定比例的欺诈交易被错过并由客户报告时重新训练模型。对于大多数在生产中使用的模型来说，这是一个非常常见和重要的实践。'
- en: '*Continuous evaluation* is often an important component in deciding when to
    retrain a model and is an important part of monitoring your model’s performance.
    As the model is predicting results on new data, a sample of the new data is labeled.
    Often domain experts determine these labels. The model is then evaluated on this
    data against the new labels. If the performance of the model wanes over time,
    then that could be a sign that there has been some sort of drift and it may be
    time to retrain the model.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*持续评估*通常是决定何时重新训练模型的重要组成部分，也是监控模型性能的重要部分。当模型在新数据上预测结果时，会对新数据进行标记。通常这些标签由领域专家确定。然后模型根据这些数据与新标签进行评估。如果模型的性能随时间减弱，那么可能表明发生了某种漂移，可能是时候重新训练模型了。'
- en: If you are part of the intended audience for this book, then likely you will
    be working with others when thinking about these concerns. However, even if it
    is not your responsibility, it is important to be aware of the concerns and tools
    that others are working with. This allows you to make more informed decisions
    in your own approach to make their job easier and to make you a better collaborator.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您是本书预期读者的一部分，那么您在思考这些问题时可能会与其他人合作。然而，即使这不是您的责任，了解其他人正在使用的工具和问题也是很重要的。这可以让您在自己的方法上做出更明智的决策，使他人的工作更轻松，并让您成为更好的合作者。
- en: Summary
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: You have been on quite the ML journey working through this book! The hope is
    that you now feel comfortable thinking about how to turn your questions into ML
    projects and feel ready to start building models using the tools you used in this
    book. You can employ different resources and go in many directions from here,
    and you should feel empowered to follow those into topics of your interest. ML
    and AI is a rapidly growing field whose demand has exponentially grown over the
    past decade and likely will continue to do so in the years to come.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你在阅读这本书的过程中已经经历了一段很不错的机器学习之旅！希望现在你能够自如地思考如何将自己的问题转化为机器学习项目，并且已经准备好开始使用本书中介绍的工具构建模型了。从这里起，你可以利用不同的资源，探索多种方向，并且应该感到自信去追寻你感兴趣的领域。机器学习和人工智能是一个快速增长的领域，在过去十年里需求呈指数级增长，今后几年很可能会继续增长。
