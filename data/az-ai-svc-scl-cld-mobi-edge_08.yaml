- en: Chapter 5\. Using Azure Applied AI Services for Common Scenarios
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 5 章\. 使用 Azure 应用 AI 服务处理常见场景
- en: In the previous chapter we looked at the individual Cognitive Services you can
    use for specific tasks. Now, we’re going to focus on the high-level Applied AI
    Services that cover common scenarios like extracting information from documents
    or videos.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们看了可以用于特定任务的各个认知服务。现在，我们将专注于涵盖从文档或视频中提取信息等常见场景的高级应用 AI 服务。
- en: Azure Applied AI Services
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Azure 应用 AI 服务
- en: Individual Cognitive Services are powerful, but often you will want to combine
    multiple Cognitive Services to handle broader scenarios. If you’re making a chatbot,
    you might start with QnA Maker but then use LUIS to make the bot better at understanding
    what users are trying to achieve, and use the Speech Services to let people talk
    to your bot as well as type. Because that’s such a popular business scenario,
    Microsoft built the Azure Bot Service as an integrated environment that brings
    together all those tools.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 单个认知服务功能强大，但通常您会希望结合多个认知服务来处理更广泛的情景。如果您正在制作聊天机器人，您可能会从 QnA Maker 开始，然后使用 LUIS
    使机器人更好地理解用户试图实现的目标，并使用语音服务使人们能够与您的机器人交谈和输入。由于这是一个非常流行的业务场景，微软建立了 Azure Bot Service
    作为一个集成环境，将所有这些工具汇集在一起。
- en: The Bot Service is one of the Azure Applied AI Services that build on the core
    Cognitive Services, either combining multiple services or wrapping business logic
    and a UI around a single service to handle common business problems.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人服务是 Azure 应用 AI 服务之一，它基于核心认知服务，无论是结合多个服务还是将业务逻辑和 UI 包装在单个服务周围以处理常见的业务问题。
- en: For example, Azure Metrics Advisor builds on the Anomaly Detector API and provides
    a web-based workspace that simplifies ingesting data from multiple sources and
    configuring settings like how sensitive you want your model to be to outliers,
    as well as building a graph to explain how different metrics relate to each other.
    As you can see in [Figure 5-1](#although_it_uses_the_anomaly_detector_a), it also
    groups anomalies plus root cause analysis suggesting what’s likely behind them
    with other details into an incident where you can dig into the graphs and trees
    of data to do your own analysis. That makes it easier to see what’s going on with
    the metrics, when there’s an anomaly, and what you should do about it; you can
    also set up notifications to alert the relevant engineering, service, or business
    team.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Azure Metrics Advisor 基于异常检测器 API 构建，并提供一个基于 Web 的工作空间，简化了从多个来源摄取数据和配置设置（例如您希望模型对异常值敏感程度如何），以及构建解释不同指标如何相互关联的图表。正如您在
    [图 5-1](#although_it_uses_the_anomaly_detector_a) 中所看到的，它还将异常分组及根本原因分析建议与其他细节整合到一个事件中，您可以深入研究图表和数据树以进行自己的分析。这样可以更容易地看到指标的情况，在出现异常时，以及您应该采取什么措施；您还可以设置通知，以通知相关工程、服务或业务团队。
- en: '![Although it uses the Anomaly Detector API, Metrics Advisor wraps the Cognitive
    Services API with business logic and presents it inside a web workspace](Images/aasc_0501.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![虽然使用异常检测器 API，但 Metrics Advisor 将认知服务 API 与业务逻辑结合在一起，并将其呈现在 Web 工作空间内](Images/aasc_0501.png)'
- en: Figure 5-1\. Although it uses the Anomaly Detector API, Metrics Advisor wraps
    the Cognitive Services API with business logic and presents it inside a web workspace
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-1\. 虽然使用异常检测器 API，但 Metrics Advisor 将认知服务 API 与业务逻辑结合在一起，并将其呈现在 Web 工作空间内
- en: You can mark a data point as normal or anomalous to train the model as it gives
    you suggestions; you can also mark inflection points where a trend changes, note
    when time series data has seasonality (the acceptable temperature levels or delays
    in delivery might be very different in summer and winter), and give feedback on
    multiple continuous points, for cases where you can only see an anomaly in the
    context of the points around it.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将数据点标记为正常或异常，以训练模型，因为它会为您提供建议；您还可以标记趋势变化的拐点，注意时间序列数据的季节性（夏季和冬季的可接受温度水平或交付延迟可能非常不同），并对多个连续点进行反馈，以便在只能在其周围的点的上下文中看到异常的情况下处理。
- en: The time series data you can analyze with Azure Metrics Advisor is ideal for
    tracking business metrics, IoT monitoring, or any kind of AIOps; you can spot
    problems as they occur and prevent outages or damage to equipment. Samsung uses
    Metrics Advisor to monitor the health of its Smart TV service. With multivariate
    analysis, you can monitor multiple systems and data sources to cover complex scenarios
    like smart buildings, where you might need to include temperature, room occupancy,
    and different heating and cooling systems to understand if unusually low levels
    of electricity usage mean there’s a problem.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用Azure度量顾问分析时间序列数据，非常适合跟踪业务指标、物联网监控或任何类型的AIOps；你可以在问题发生时发现问题并防止停机或设备损坏。三星使用度量顾问来监控其智能电视服务的健康状况。通过多变量分析，你可以监控多个系统和数据源，覆盖像智能建筑这样的复杂场景，你可能需要包括温度、房间占用率以及不同的供暖和冷却系统，以了解异常低的电力使用是否意味着存在问题。
- en: But as well as the Metrics Advisor portal where you can see all this information,
    you still get a REST API that you can integrate with existing analysis tools or
    business applications, like a dashboard of key performance indicators that you
    build for stakeholders.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 除了度量顾问门户网站，你还可以使用REST API查看所有这些信息，这可以与现有的分析工具或业务应用程序集成，比如你为利益相关者建立的关键绩效指标仪表板。
- en: Warning
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: At the time of writing, Form Recognizer is the only Applied AI Service you can
    run in a container on an edge device or your own servers if you have documents
    that you’re not able to store in the cloud or you need to process documents somewhere
    with poor connectivity.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，表格识别器是唯一可以在边缘设备或自己的服务器上运行的应用AI服务，如果你有无法存储在云端的文件，或者需要在网络连接差的地方处理文件。
- en: Azure Video Analyzer
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Azure视频分析器
- en: You can extract a lot of information from videos. If you have a video library
    with thousands or millions of assets, using Azure Video Analyzer for Media you
    can extract metadata to index or control playback; use faces, emotions, and spoken
    words to make your videos searchable; or use the indexing to trigger automated
    actions. Put it all together and you can take a multi-hour video that covers a
    dozen topics, extract the people and topics, add captions, and put links that
    start the video playing in the right place next to other content that it helps
    to explain.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从视频中提取大量信息。如果你有一个拥有成千上万资产的视频库，使用Azure视频分析器可以提取元数据进行索引或控制播放；使用面部、情绪和口头语言来使你的视频可搜索；或者使用索引来触发自动化操作。将所有这些整合在一起，你可以处理多个小时的视频，涵盖十几个主题，提取人物和主题，添加字幕，并在其他内容旁边放置链接，从而帮助解释视频中的内容。
- en: Insights are presented in a hierarchy, starting with a summary of which insights
    have been discovered for the video and audio (like faces, emotions, sentiments,
    brands, topics, or keywords) and the timecode where they occur in the video. As
    you drill down you can get more detailed information by querying across different
    insight dimensions—for example, extracting transcripts from recognized speech,
    lists of individuals appearing in the video, and even OCR information from text
    shown in the video.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 洞察力以层次结构呈现，从汇总开始，显示了为视频和音频发现的洞察力（如面部、情绪、情感、品牌、主题或关键词），以及它们在视频中发生的时间码。随着深入挖掘，你可以通过查询不同的洞察力维度获得更详细的信息，例如从识别的语音中提取的转录、视频中出现的个人名单，甚至是视频中显示的OCR信息。
- en: The insights can cover video, audio, or both. Video insights can detect people
    and draw a bounding box around them in each frame to help you trace them, detect
    and group faces then extract thumbnail images, identify celebrities or individuals
    you’ve trained custom facial models for, identify objects and actions, and OCR
    text. You can use the information about people detected to do trend analysis (understanding
    how customers move around a store or how long they have to stand in a checkout
    line) or to help analyze critical events like accidents or robberies.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 洞察力可以涵盖视频、音频或两者。视频洞察力可以检测人物并在每一帧周围绘制边界框以帮助追踪他们，在检测和分组面孔后提取缩略图图像，识别名人或为其训练的自定义面部模型，识别物体和动作，并进行OCR文本。你可以利用检测到的人员信息进行趋势分析（了解客户在商店中移动的方式或他们在结账队列中等待的时间长短），或者帮助分析事故或抢劫等关键事件。
- en: 'There are also insights specific to produced video: identifying the opening
    or closing credits of a show, detecting keyframes and blank frames, and marking
    the scenes and shots that make up a video.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 视频制作中还有一些特定的见解：识别节目的开头或结尾片头，检测关键帧和空白帧，并标记组成视频的场景和镜头。
- en: 'Audio insights clean up noisy audio, detect language and transcribe audio (with
    the option of using custom language models), translate the transcript or turn
    it into captions, detect sounds like clapping (or silence) and emotions (based
    on both speech and other audio cues), or identify who speaks which words and generate
    statistics for how often each person speaks. Video Analyzer can also detect audio
    effects that aren’t speech: alarms, breaking glass, barking dogs, and so on.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 音频见解可以清理嘈杂的音频，检测语言并转录音频（可以使用自定义语言模型），翻译转录文本或将其转换为字幕，检测如鼓掌（或沉默）等声音以及情绪（基于语音和其他音频提示），或者识别谁说了哪些话并生成每个人说话频率的统计数据。视频分析器还可以检测非语音的音频效果：警报声、玻璃破碎声、狗叫声等。
- en: Combined insights extract keywords, brands, and sentiment from both speech and
    text shown on screen, and list the main topics covered in the video transcript.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 结合的见解从屏幕上显示的语音和文本中提取关键词、品牌和情感，并列出视频转录中涵盖的主要主题。
- en: Tip
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'Find a prebuilt solution that creates a custom video search with Video Analyzer,
    Azure Machine Learning’s Data Labeling AutoML Vision solution, Cognitive Services,
    and Azure Functions in this [GitHub repository](https://go.microsoft.com/fwlink/?linkid=2190161):
    it’s been trained to recognize breeds of dog, but you can use your own custom
    vision model.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个[GitHub存储库](https://go.microsoft.com/fwlink/?linkid=2190161)中找到一个预构建的解决方案，它使用Video
    Analyzer、Azure Machine Learning的数据标记AutoML Vision解决方案、认知服务和Azure Functions创建自定义视频搜索：它已经训练好可以识别狗的品种，但您也可以使用自己的自定义视觉模型。
- en: In an industrial scenario where you need to enforce workplace safety, manage
    visual inspections, or optimize processes like turning around planes at an airport
    gate, Azure Video Analyzer lets you build intelligent video applications that
    bring together IoT solutions (using an IoT Edge module and the architecture you
    can see in [Figure 5-2](#azure_video_analyzer_combines_iot_at_th)) and video analytics,
    without the complexity of building and operating a live video pipeline.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在需要执行工作场所安全、管理视觉检查或优化流程（如在机场登机口周转飞机）的工业场景中，Azure Video Analyzer使您能够构建智能视频应用程序，集成物联网解决方案（使用IoT
    Edge模块和您可以在[图5-2](#azure_video_analyzer_combines_iot_at_th)中看到的架构）和视频分析，无需建立和运行实时视频管道的复杂性。
- en: '![Azure Video Analyzer combines IoT at the edge with cloud AI services to help
    you understand what’s happening in a space](Images/aasc_0502.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![Azure Video Analyzer将IoT边缘与云AI服务结合起来，帮助您理解空间内发生的情况](Images/aasc_0502.png)'
- en: Figure 5-2\. Azure Video Analyzer combines IoT at the edge with cloud AI services
    to help you understand what’s happening in a space
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-2\. Azure Video Analyzer将IoT边缘与云AI服务结合起来，帮助您理解空间内发生的情况
- en: When a plane lands, airport staff have to get the passengers and baggage off
    the plane and coordinate connecting it to local power, aircraft cleaning, safety
    checks, refueling, restocking the catering, and loading cargo, baggage, and passengers
    for the outgoing flight. Tracking that with video analytics means the airport
    team knows if an aircraft turnaround is taking longer than usual, so they can
    allocate more staff or warn the airline about potential delays, or even detect
    safety issues as they happen. Video Analyzer can also produce metrics to track
    performance over time.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当飞机降落时，机场工作人员必须协调将乘客和行李从飞机上卸下，并连接到本地电源，进行飞机清洁、安全检查、加油、补给餐饮、以及为出港航班装载货物、行李和乘客。通过视频分析跟踪这些工作意味着机场团队可以知道飞机周转是否比平常花费更长时间，因此他们可以分配更多工作人员或者警告航空公司可能存在的延误，甚至在问题发生时检测到安全问题。视频分析器还可以生成指标以跟踪时间内的绩效表现。
- en: You can analyze live or recorded video from existing CCTV and RTSP (Real Time
    Streaming Protocol) IP cameras; if you’re working with live video, you can process
    it at the edge for high latency or record relevant video clips on the edge for
    limited bandwidth deployments. Video analytics use the Cognitive Services Custom
    Vision and Spatial Analysis APIs—plus your own custom models—to detect and track
    people and objects, trigger events or notifications when objects cross a line,
    recognize and caption speech, and mine those transcripts for insights to help
    you understand what’s happening.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以分析现有的闭路电视和RTSP（实时流媒体传输协议）IP摄像机的实时或录制视频；如果您处理实时视频，可以在边缘处进行高延迟处理，或在边缘上记录相关视频片段以适应带宽有限的部署。视频分析使用认知服务定制视觉和空间分析API，以及您自己的定制模型，用于检测和跟踪人员和物体，当物体越过某条线时触发事件或通知，识别和加标语音，并从这些转录中挖掘见解，帮助您理解发生了什么。
- en: You can also see the video and analytics in Power BI by inserting the [Video
    Analyzer player widget](https://go.microsoft.com/fwlink/?linkid=2190275), which
    makes a REST call to your Video Analyzer endpoint. You can get the embed code
    from the Video Analyzer portal by choosing Dashboard, Widget setup, and scrolling
    down to Option 2—using HTML; add your own token and insert it into a Power BI
    dashboard as a web content tile.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过插入[视频分析器播放器小部件](https://go.microsoft.com/fwlink/?linkid=2190275)在Power
    BI中查看视频和分析结果，该小部件通过REST调用您的视频分析器端点。您可以从视频分析器门户获取嵌入代码，选择仪表板，小部件设置，并向下滚动至选项2 - 使用HTML；添加您自己的令牌，并将其作为Web内容瓦片插入Power
    BI仪表板中。
- en: Cognitive Search
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 认知搜索
- en: Workers still spend an average of two weeks a year looking for information,
    and enterprise document search is rarely as good as web search engines. You can
    make document search more powerful by enriching the unstructured documents you
    want to index and search using machine learning to extract structure, transforming
    information and adding metadata. That lets you classify documents by the people
    or organizations mentioned in them or use text to search for objects in images.
    New fields are added to the source documents, enriching them with, for example,
    entity relationships. Turn a large set of documents into a graph of information,
    linked by the people, products, and other entities mentioned in them, and you
    can go beyond just finding documents to making it easier to understand what’s
    in them and how they’re related to each other.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 员工仍然平均每年花费两周的时间查找信息，企业文件搜索很少像网络搜索引擎那样有效。通过使用机器学习来丰富您想要索引和搜索的非结构化文档，提取结构，转换信息并添加元数据，您可以使文档搜索功能更强大。这使您可以根据文档中提到的人员或组织对文档进行分类，或使用文本搜索图像中的对象。新字段被添加到源文档中，通过例如实体关系对它们进行丰富。将大量文档转化为信息图，通过其中提到的人员、产品和其他实体进行链接，您不仅仅可以查找文档，还可以更轻松地理解文档内容及其彼此之间的关系。
- en: 'Azure Cognitive Search has a full-text, keyword-based search engine built on
    the efficient, widely used “Best Match 25” algorithm: that’s great for keywords
    but not so good at finding the document that best matches a natural language query
    like “how to add a user in Exchange” or “how do I book vacation time,” where lots
    of documents that don’t answer the question will contain the keywords. So it also
    uses semantic search, using large transformer-based language models (the same
    technology used in Bing). Semantic ranking puts documents that best match the
    meaning of search terms rather than the exact words at the top of search results.
    Semantic answers extract relevant sections from the top documents, rank them for
    how well they answer the query, and pull the best match out of a potentially long
    document and highlight it at the top of the results, and semantic captions use
    machine reading comprehension to highlight relevant words or phrases in the previews
    included with search results.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Azure认知搜索是基于高效、广泛使用的“最佳匹配25”算法构建的全文、基于关键字的搜索引擎：非常适合关键字，但在查找“如何在Exchange中添加用户”或“如何预订假期时间”等自然语言查询最匹配文档时效果不佳，因为很多不相关文档也包含这些关键字。因此，它还使用语义搜索，利用大型基于转换器的语言模型（与Bing使用的技术相同）。语义排序将最符合搜索词意义的文档排在搜索结果的顶部，语义答案从顶部文档中提取相关部分，根据其回答查询的效果对它们进行排名，并从可能很长的文档中提取最佳匹配并突出显示在结果的顶部，语义标题使用机器阅读理解突出显示搜索结果中包含的相关词或短语的预览中。
- en: Semantic search powers the search feature in Microsoft Docs, reranking the top
    results based on clusters of related concepts so you see a handful of very relevant
    results from all the documentation for that particular product or topic. In [Figure 5-3](#the_semantic_ranking_feature_in_cogniti),
    you can see how searching for a common word with multiple meanings suggests very
    different pages when you’re reading about the visualization tools or the admin
    console.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 语义搜索支持 Microsoft 文档中的搜索功能，根据相关概念群集重新排列顶部结果，因此您可以从所有关于特定产品或主题的文档中看到一些非常相关的结果。在
    [图 5-3](#the_semantic_ranking_feature_in_cogniti) 中，您可以看到当您在阅读可视化工具或管理员控制台相关内容时，搜索具有多重含义的常见词语建议非常不同的页面。
- en: Cognitive Search can handle PDFs, PowerPoints, Word documents, JPEGs, CSV and
    text files, and other business documents, pulling data from multiple sources including
    SharePoint Online indexer, Azure Files indexer, or by using Power Query connectors.
    It uses Cognitive Services OCR APIs to extract text from images and the Text Analytics
    APIs to extract key phrases, and it detects location, people, and organizations.
    It also includes auto-complete and spell correction, geospatial search, and faceting,
    which adds categories and filters to results.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 认知搜索可以处理 PDF、PowerPoint、Word 文档、JPEG、CSV 和文本文件以及其他业务文档，从多个来源提取数据，包括 SharePoint
    Online 索引器、Azure Files 索引器，或通过使用 Power Query 连接器。它使用认知服务 OCR API 从图像中提取文本，并使用文本分析
    API 提取关键短语，并检测位置、人员和组织。它还包括自动完成和拼写校正、地理空间搜索以及分面，这些功能为结果添加类别和过滤器。
- en: '![The semantic ranking feature in Cognitive Search suggests documents about
    using Power BI if you search for “key” when you’re reading about user features
    but finds information about authentication keys if you’re looking at admin documentation](Images/aasc_0503.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![在认知搜索中的语义排名功能建议，如果您在阅读有关用户功能时搜索“关键词”，则找到有关使用 Power BI 的文档，但如果您在查看管理员文档时查找有关身份验证密钥的信息](Images/aasc_0503.png)'
- en: Figure 5-3\. The semantic ranking feature in Cognitive Search suggests documents
    about using Power BI if you search for “key” when you’re reading about user features
    but finds information about authentication keys if you’re looking at admin documentation
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-3\. 在认知搜索中的语义排名功能建议，如果您在阅读有关用户功能时搜索“关键词”，则找到有关使用 Power BI 的文档，但如果您在查看管理员文档时查找有关身份验证密钥的信息。
- en: 'There are SDKs for .NET, Java, Python, and JavaScript, or you can work with
    Cognitive Search through an API for indexing, querying, and AI enrichment. These
    are the key properties in a query:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有 .NET、Java、Python 和 JavaScript 的 SDK，或者您可以通过用于索引、查询和 AI 丰富化的 API 使用认知搜索。以下是查询中的关键属性：
- en: queryType
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: queryType
- en: Set to “semantic” for semantic ranking and answers, “simple” or “full.”
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 设置为“语义”以进行语义排名和回答，“简单”或“完整”。
- en: searchFields
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: searchFields
- en: An ordered list of fields to apply semantic ranking on. Put the title or any
    summary fields first, then the URL, document body, and any other fields.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对要应用语义排名的字段的有序列表。首先放置标题或任何摘要字段，然后是 URL、文档正文和任何其他字段。
- en: queryLanguage
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: queryLanguage
- en: At the time of writing “en-us” is the only supported value, but this will allow
    multiple languages.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 写作时，“en-us” 是唯一支持的值，但这将允许多种语言。
- en: speller
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: speller
- en: Set to “lexicon” for spelling correction on query terms or “none.”
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 设置为“词典”以在查询项上进行拼写校正或“无”。
- en: answers
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: answers
- en: Set to “extractive” for semantic answers and captions or “none.”
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 设置为“抽取”以进行语义答案和标题或“无”。
- en: 'Suppose you want to highlight some information about the creator of the “imitation
    game” designed to test whether a computer is exhibiting intelligence behavior:
    Alan Turing. Here’s a query for getting semantically ranked results (specifying
    the fields to rank) with semantic answers and captions, and any spelling mistakes
    in the original query automatically corrected:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您希望突出显示关于“模拟游戏”的创建者的一些信息，该游戏旨在测试计算机是否表现出智能行为：阿兰·图灵。以下是用于获取语义排名结果（指定要排名的字段）、语义答案和标题以及自动更正原始查询中的任何拼写错误的查询：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Responses are returned as JSON, ready for use in your application. Here we’re
    showing the response for a search for Alan Turing’s birthplace:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 响应以 JSON 形式返回，可供您的应用程序使用。这里我们展示了搜索阿兰·图灵出生地的响应：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Cognitive Search isn’t just about being smart with query results: you can also
    augment the content you’re going to search. Add extra processing to further enhance
    documents using Cognitive Services like text translation or by calling another
    Applied AI Service like Form Recognizer through the Web API custom skill interface,
    as shown in [Figure 5-4](#you_can_add_cognitive_skills_from_azure); you can also
    write custom skills with Azure Machine Learning. Build a translation skill app
    with an HTTP trigger as an Azure Function, and whenever a new document is indexed
    you can automatically create a translation and have that indexed too.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 认知搜索不仅仅是智能查询结果的关键：您还可以通过使用文本翻译等认知服务或通过Web API自定义技能接口调用另一个应用AI服务（如表单识别器）来增强要搜索的内容。如图5-4所示，使用Azure机器学习编写自定义技能应用程序作为HTTP触发器，每当索引新文档时，您都可以自动创建翻译并将其索引化。
- en: '![You can add cognitive skills from Azure Cognitive Services to Cognitive Search
    when you create your index or connect them later](Images/aasc_0504.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![您可以在创建索引或稍后连接时将Azure认知搜索的认知技能添加到其中](Images/aasc_0504.png)'
- en: Figure 5-4\. You can add cognitive skills from Azure Cognitive Services to Cognitive
    Search when you create your index or connect them later
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-4。您可以在创建索引或稍后连接时将Azure认知搜索的认知技能添加到其中
- en: Tip
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Microsoft built the [JFK Files](https://go.microsoft.com/fwlink/?linkid=2190276)
    with Cognitive Search as an example of the information you can extract from large
    amounts of data in multiple formats. But if you were starting a project like that
    now, you might want to use the OpenAI Service from [Chapter 4](ch04.xhtml#using_azure_cognitive_services_to_build)
    in a skill to ask more complex questions than the JFK Files currently support.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Microsoft建立了[JFK文件](https://go.microsoft.com/fwlink/?linkid=2190276)，作为从多种格式的大量数据中提取信息的示例。但是，如果您现在开始像那样的项目，您可能希望在技能中使用第4章中的OpenAI服务来提出比JFK文件当前支持的更复杂的问题。
- en: Azure Form Recognizer
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Azure表单识别器
- en: Almost every organization has to deal with forms that customers, employees,
    or suppliers have filled out. If you want to automate those processes, you need
    to be able to extract the information from paper forms that might be printed or
    handwritten as data that you can store in a database and use to trigger a workflow.
    If you have legal and financial documents like contracts and insurance quotes,
    they often have tables of data that you need to extract. Even though they’re not
    forms, Azure Forms Recognizer can handle both, in over 70 languages.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎每个组织都必须处理客户、员工或供应商填写的表格。如果您希望自动化这些流程，您需要能够从可能是打印或手写的纸质表单中提取信息，以便将其存储在数据库中并用于触发工作流程。如果您有像合同和保险报价这样的法律和财务文件，则通常包含您需要提取的数据表。即使它们不是表单，Azure表单识别器也可以处理70多种语言的这两种情况。
- en: In the next chapter we’ll show you how business users can use this same service
    through the AI Builder feature in the Power Platform, where it’s called form processing,
    but developers can also build document processing into their own application by
    calling the Form Recognizer REST API or client library SDKs (for C#, Java, JavaScript,
    and Python). You can also work with the API and SDKs to train custom models and
    use those in Logic Apps, Microsoft Power Automate, and Microsoft Power Apps using
    connectors.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将展示业务用户如何通过Power平台中的AI Builder功能使用同一服务，这里称为表单处理，但开发人员也可以通过调用Form Recognizer
    REST API或客户端库SDK（如C＃，Java，JavaScript和Python）将文档处理集成到他们自己的应用程序中。您还可以使用API和SDK来训练自定义模型，并在Logic
    Apps、Microsoft Power Automate和Microsoft Power Apps中使用连接器。
- en: Form Recognizer uses the Cognitive Services OCR, Text Analytics, and Custom
    Text APIs to find the fields in forms and tables and extract the text or handwriting
    in each field as a key-value pair, so it can spot the name field on a passport
    and the name shown there.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 表单识别器使用认知服务OCR、文本分析和自定义文本API来查找表单和表格中的字段，并提取每个字段中的文本或手写文本作为键值对，因此它可以识别护照上的姓名字段和显示的姓名。
- en: Form Recognizer has prebuilt models for invoices, sales receipts, business cards,
    and ID cards, or you can train custom models on your own documents to extract
    text and layout information. That way you can ignore the address and shipping
    details at the top of a shipping form, the headings on a table, and the boilerplate
    information at the bottom, and just pull out the fields that you need, like the
    invoice number, items supplied, and price.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Form Recognizer 提供了预构建模型，用于发票、销售收据、名片和身份证，或者您可以自行训练自定义模型，以提取文本和布局信息。这样，您可以忽略发货表单顶部的地址和送货细节、表格上的标题以及底部的模板信息，只提取您需要的字段，如发票编号、提供的项目和价格。
- en: Tip
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Tip
- en: 'You can try out prebuilt, layout, and custom models in Form Recognizer with
    the Form Recognizer Sample Tool. You can try that out using the Form OCR Testing
    Tool [here](https://fott-2-1.azurewebsites.net) or run it in a Docker container
    using this command:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Form Recognizer 示例工具可以尝试预构建、布局和自定义模型。您可以通过 Form OCR 测试工具[此处](https://fott-2-1.azurewebsites.net)或者在
    Docker 容器中使用以下命令来试用：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The code for the tool is also available through the [OCR Form Tools](https://github.com/microsoft/OCR-Form-Tools).
    To use the tool, you have to provision a Form Recognizer resource from the Azure
    portal and copy the API key and endpoint into the relevant fields. If you want
    to train a model, you’ll also need Azure Blob storage for the training documents
    (or you can use local storage if you’re running the tool in a container). Pick
    a file to work with and you’ll see a preview of the fields, tables, and text extracted,
    with highlighting to show where they came from on the form. You can download the
    JSON output as a file. You can also deploy the Form Recognizer service in a container
    if you need to work with documents locally.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 工具的代码也可以通过[OCR Form Tools](https://github.com/microsoft/OCR-Form-Tools)获取。要使用该工具，您需要在
    Azure 门户中配置 Form Recognizer 资源，并复制 API 密钥和终结点到相应字段中。如果要训练模型，您还需要 Azure Blob 存储用于训练文档（或者如果在容器中运行工具，则可以使用本地存储）。选择要处理的文件，您将看到提取的字段、表格和文本的预览，并使用高亮显示显示它们在表单上的位置。您可以将
    JSON 输出下载为文件。如果需要本地处理文档，还可以在容器中部署 Form Recognizer 服务。
- en: For documents, Form Recognizer extracts tables—including complex tables that
    have merged cells or no visible borders—checkboxes and similar marks like radio
    buttons, and both the text and structure of the document. You can see what the
    data looks like in [Figure 5-5](#forms_recognizer_detects_tables_and_fie).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于文档，Form Recognizer 提取包括具有合并单元格或无可见边框的复杂表格在内的表格、复选框和类似的标记，以及文档的文本和结构。您可以在[图
    5-5](#forms_recognizer_detects_tables_and_fie)中查看数据的外观。
- en: 'The prebuilt models extract the important information for different types of
    forms: the business cards model looks for name, job title, address, email, company,
    and phone numbers; the official ID model finds the ID number, name, country expiration,
    and birth dates, but on receipts it extracts all of the text, the time and date,
    merchant information, line items, sales tax, and totals. The invoice model also
    extracts all the text and looks for fields like invoice ID, customer details,
    vendor details, ship to, bill to, total, tax, subtotal, and line items, because
    it might include contractual terms or other important details. You call these
    models using FormRecognizerClient, which returns JSON output with RecognizedForm,
    FormPage, documentResults, and pageResults sections.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 预构建模型可以从不同类型的表单中提取重要信息：名片模型查找姓名、职位、地址、电子邮件、公司和电话号码；官方身份证模型查找身份证号码、姓名、国家到期日期和出生日期，但在收据上提取所有文本、时间和日期、商户信息、项目明细、销售税和总数。发票模型也提取所有文本，并查找如发票编号、客户详细信息、供应商详细信息、发货地址、账单地址、总额、税费、小计和项目明细等字段，因为可能包含合同条款或其他重要细节。您可以使用
    FormRecognizerClient 调用这些模型，返回包含 RecognizedForm、FormPage、documentResults 和 pageResults
    部分的 JSON 输出。
- en: '![Forms Recognizer detects tables and fields in documents; this is the prebuilt
    model automatically extracting a table of transactions from a bank statement](Images/aasc_0505.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![Forms Recognizer 可以检测文档中的表格和字段；这是预构建模型自动从银行对账单中提取交易表格](Images/aasc_0505.png)'
- en: Figure 5-5\. Forms Recognizer detects tables and fields in documents; this is
    the prebuilt model automatically extracting a table of transactions from a bank
    statement
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-5\. Forms Recognizer 可以检测文档中的表格和字段；这是预构建模型自动从银行对账单中提取交易表格。
- en: 'Start by installing the Form Recognizer client:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 首先安装 Form Recognizer 客户端：
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Next, set variables for your endpoints and your subscription key. You’re now
    ready to call the FormRecognizerClient with either a custom model, a pretrained
    receipt model, or using the default recognition settings without a specific model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，设置你的端点和订阅密钥的变量。现在你可以调用FormRecognizerClient，使用定制模型、预训练收据模型，或者使用默认识别设置而不使用特定模型。
- en: 'The following code snippet logs into an endpoint and attempts to recognize
    the content in an uploaded receipt:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段登录到一个端点，并尝试识别上传收据中的内容：
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Warning
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Forms can be JPG, PNG, PDF, or TIFF files; you get the best results with text
    PDFs, but Form Recognizer can scan OCR text and handwriting in images and PDFs.
    Files must be less than 50 MB, page sizes can’t be larger than A3, images must
    be at least 50 × 50 pixels but no larger than 10,000 × 10,000, and only the first
    200 pages of long documents will be scanned.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 表单可以是JPG、PNG、PDF或TIFF文件；在文本PDF中获得最佳结果，但表单识别器可以扫描图像和PDF中的OCR文本和手写文字。文件大小必须小于50
    MB，页面尺寸不能大于A3，图像至少为50 × 50像素，但不能大于10,000 × 10,000像素，长文档只会扫描前200页。
- en: If you have a specific form type to handle, you can train a custom model with
    as few as five samples (and another example to test on). You can use the graphical
    interface in the Form Recognizer Sample Tool or call this with the REST API using
    FormTrainingClient.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有特定的表单类型要处理，你可以用至少五个样本（和另一个用于测试的示例）训练一个定制模型。你可以使用表单识别示例工具中的图形界面，或者使用FormTrainingClient通过REST
    API调用。
- en: 'The following code snippet will train a recognizer model on form images stored
    in an Azure storage account:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段将在存储在Azure存储账户中的表单图像上训练一个识别器模型：
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You don’t necessarily need to label your forms when you train them; Form Recognizer
    uses unsupervised learning to understand the layout and detect the relationships
    between fields and entries. For many forms, that will give good enough results.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练时，你不一定需要为你的表单进行标记；表单识别器使用无监督学习来理解布局，并检测字段和条目之间的关系。对于许多表单，这将产生足够好的结果。
- en: If you have more complex layouts, or fields that don’t have names that Form
    Recognizer can use (so it has no key to assign those values to), you can train
    it on labeled forms; again, you need five labeled forms of the same type, with
    the same structure. Form Recognizer still learns the layout of the form itself,
    but it uses your labels for the fields and tables.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有更复杂的布局，或者字段没有Form Recognizer可以使用的名称（因此它没有键来分配这些值），你可以在标记的表单上进行训练；同样，你需要五个相同类型、相同结构的标记表单。表单识别器仍然会学习表单本身的布局，但会使用你的标签来识别字段和表格。
- en: To try this out in the Form Recognizer Sample Tool, create a new custom project
    (you’ll need the details for your Azure Blob storage container with your training
    data and your Form Recognizer endpoint for the connections settings). Select the
    Tags Editor icon in the left pane where you see the list of forms (you can see
    this in [Figure 5-6](#label_the_fields_and_tables_on_your_exa)). This extracts
    the text and table layout information for the documents and draws bounding boxes
    around the text elements. Click the table/grid icon that appears on the form to
    preview any table information that’s been extracted.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 要在表单识别示例工具中尝试此功能，请创建一个新的自定义项目（你需要包含训练数据的Azure Blob存储容器的详细信息，以及你的Form Recognizer端点用于连接设置）。在左侧窗格中选择Tags
    Editor图标，这样你就可以看到表单列表（你可以在[图5-6](#label_the_fields_and_tables_on_your_exa)中看到这一点）。这将提取文档的文本和表格布局信息，并在文本元素周围绘制边界框。点击表格/网格图标，预览提取的任何表格信息。
- en: '![Label the fields and tables on your example forms in the Form Recognizer
    Sample Tool](Images/aasc_0506.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![在表单识别示例工具中，标记你的示例表单中的字段和表格](Images/aasc_0506.png)'
- en: Figure 5-6\. Label the fields and tables on your example forms in the Form Recognizer
    Sample Tool
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 在表单识别示例工具中，标记你的示例表单中的字段和表格。
- en: 'Create tags for the key-value pairs for each field you want to extract in the
    tag editor pane on the right (you can set the format and data type for each tag).
    You can also create tags for tables (or lists of data even if they’re not laid
    out as tables): click the icon to “Add a new table tag,” then choose whether tables
    will have a fixed or variable number of rows and whether the labels are on the
    rows or columns. Mark up the individual fields in the table with format and data
    type.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在右侧的标签编辑窗格中为要提取的每个字段的键值对创建标签（可以为每个标签设置格式和数据类型）。您还可以为表格创建标签（即使它们没有表格布局）：点击“添加新表格标签”图标，然后选择表格的行数是固定还是可变以及标签是在行还是列上。使用格式和数据类型标记表格中的各个字段。
- en: 'Then select the text element or table cell you want to apply the tag to in
    the main editor pane in the middle and the tag to use in the tag editor pane.
    If the field has a label, don’t include that: just the content in the field. If
    there are empty fields on the training form that will sometimes be filled in,
    label those with tags as well.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在中间的主编辑窗格中选择要应用标签的文本元素或表格单元格以及在标签编辑窗格中要使用的标签。如果字段有标签，请勿包含该标签：只需包含字段中的内容。如果训练表单上有空字段，有时会填写，请将其标记为标签。
- en: Choose the Train icon from the toolbar on the left of the window and click Train
    to train your custom model. When it’s finished, check the average accuracy and
    confidence values; if they’re low, you can label more documents and retrain the
    model. You can also click the Analyze icon in the toolbar to try out your model
    on a form (but don’t use one of the ones you trained on).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 从窗口左侧的工具栏中选择“训练”图标，然后单击“训练”以训练您的自定义模型。完成后，请检查平均准确度和置信度值；如果值较低，您可以标记更多文档并重新训练模型。您还可以在工具栏中单击“分析”图标，以在表单上尝试您的模型（但不要使用您训练过的表单之一）。
- en: Tip
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you have several different layouts for similar forms—invoices from different
    suppliers or shipping labels for different services—you can train custom models
    (with labels) for each of them and assign them to a composed model. That way you
    can call them all with a single model ID, and Form Recognizer runs a classifier
    to pick which model to use for the current form.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对类似表单的不同布局有多种不同的布局——来自不同供应商的发票或不同服务的运输标签——您可以为每个自定义模型（带标签）进行训练，并将它们分配给一个组合模型。这样，您可以通过单个模型
    ID 调用它们所有，并且表单识别器运行分类器来选择当前表单使用的模型。
- en: Azure Bot Service
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Azure 机器人服务
- en: 'The Azure Bot Service brings together all the different tools and services
    for building, testing, deploying, and managing custom chatbots—web applications
    with conversational interfaces—to one or more channels: that could be Facebook,
    Teams, embedded on your website, used in a call center, or exposed as a service
    through a voice assistant like Alexa or Google Assistant. See how it all fits
    together in [Figure 5-7](#the_components_that_make_up_a_conversat).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 机器人服务汇集了构建、测试、部署和管理自定义聊天机器人（具有对话界面的 Web 应用程序）的所有不同工具和服务，可以连接一个或多个渠道：可以是
    Facebook、Teams，在您的网站上嵌入，用于呼叫中心，或通过 Alexa 或 Google Assistant 作为服务公开。查看在[图 5-7](#the_components_that_make_up_a_conversat)中如何完美结合。
- en: You can create bots using the Microsoft Bot Framework SDK or with the Bot Framework
    Composer, an IDE with a visual design surface that runs on Windows, macOS, or
    Linux (or in the cloud as a web application) and allows you to publish the bot
    directly. You can start building a bot by dragging actions onto the canvas in
    the Bot Framework Composer and following the prompts to integrate with Cognitive
    Services, or create Power Virtual Agents and then extend them through the SDK.
    You can even use Power Virtual Agent topics as Bot Framework skills in a bot built
    in Composer.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 Microsoft Bot Framework SDK 或 Bot Framework Composer 创建机器人，后者是一个具有视觉设计界面的
    IDE，可在 Windows、macOS 或 Linux 上运行（或作为 Web 应用程序在云中运行），并允许您直接发布机器人。您可以通过在 Bot Framework
    Composer 的画布上拖动动作并按照提示与认知服务集成来开始构建机器人，或者创建 Power Virtual Agents，然后通过 SDK 进行扩展。您甚至可以将
    Power Virtual Agent 主题作为 Bot Framework Composer 中构建的机器人的 Bot Framework 技能使用。
- en: '![The components that make up a conversational AI experience](Images/aasc_0507.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![组成会话型 AI 体验的组件](Images/aasc_0507.png)'
- en: Figure 5-7\. The components that make up a conversational AI experience
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-7\. 组成会话型 AI 体验的组件
- en: The Bot Service simplifies using Cognitive Services like Speech, QnA Maker,
    Language Understanding, and Vision in a bot so you can deal with more complicated
    input and understand user needs better. You can select Bot Framework skills and
    components from a directory using the package manager in Composer or publish your
    own components on NuGet and npm, or as a private feed that other bots built in
    your organization can incorporate.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Bot 服务简化了使用认知服务如 Speech、QnA Maker、语言理解和视觉在机器人中的应用，从而您可以处理更复杂的输入并更好地理解用户需求。您可以通过
    Composer 中的包管理器选择 Bot 框架的技能和组件，或者发布您自己的组件到 NuGet 和 npm，或者作为私有订阅集成到您组织中构建的其他机器人中。
- en: You can track bot health and behavior with Bot Framework Analytics, which use
    Application Insights queries and Power BI dashboards.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 Bot Framework Analytics 跟踪机器人的健康状态和行为，这些使用 Application Insights 查询和 Power
    BI 仪表盘。
- en: If you haven’t built a bot before, the Bot Framework Composer includes templates
    for QnA bots, bots to manage a calendar, or a full enterprise assistant bot that
    includes multiple capabilities. Or you can use the open source [Virtual Assistant](https://go.microsoft.com/fwlink/?linkid=2190163)
    project template for C# and TypeScript, which includes business logic and handling
    user requests.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您之前没有构建过机器人，Bot Framework Composer 包含了 QnA 机器人、管理日历的机器人模板，或者包含多种能力的全功能企业助理机器人模板。或者您可以使用开源的
    [Virtual Assistant](https://go.microsoft.com/fwlink/?linkid=2190163) 项目模板，用于 C＃
    和 TypeScript，其中包含业务逻辑和处理用户请求。
- en: Immersive Reader
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Immersive Reader
- en: Dense, complex documents can be hard to follow.  Some people need bigger fonts
    of better contrast to read on screen quickly. Use the same accessibility tool
    that’s in Edge, Teams, and Word to make your own applications that host documents
    easier to read. Immersive Reader can read content aloud in a wide range of languages,
    translate it into even more (over 60 languages), or focus attention using highlighting
    and color to isolate content and make it more readable. You can see an example
    of how the Edge browser integrates Immersive Reader in [Figure 5-8](#the_reading_mode_in_edge_uses_immersive).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 密集复杂的文档可能难以理解。一些人需要更大的字体或更好的对比度来快速在屏幕上阅读。使用与 Edge、Teams 和 Word 中相同的辅助工具，可以使您自己的托管文档应用程序更易于阅读。Immersive
    Reader 可以以多种语言大声朗读内容，将其翻译成更多语言（超过 60 种），或者使用高亮显示和颜色聚焦注意力以隔离内容并使其更易读。您可以查看 Edge
    浏览器如何集成 Immersive Reader 的示例在 [Figure 5-8](#the_reading_mode_in_edge_uses_immersive)。
- en: '![The reading mode in Edge uses Immersive Reader to reduce distractions and
    make it easier to focus on the text, which can also be color coded to show grammar
    or even translated to a different language](Images/aasc_0508.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![Edge 中的阅读模式使用 Immersive Reader 减少干扰，使专注于文本变得更加容易，还可以进行颜色编码以显示语法，甚至可以翻译成不同的语言](Images/aasc_0508.png)'
- en: Figure 5-8\. The reading mode in Edge uses Immersive Reader to reduce distractions
    and make it easier to focus on the text, which can also be color coded to show
    grammar or even translated to a different language
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-8\. Edge 中的阅读模式使用 Immersive Reader 减少干扰，使专注于文本变得更加容易，还可以进行颜色编码以显示语法，甚至可以翻译成不同的语言。
- en: The Immersive Reader JavaScript library is a web application that you can integrate
    into a C#, JavaScript, Kotlin, Java (Android), or Swift app as an iframe; you’ll
    also need to build a button to close the Immersive Reader UI and configure Azure
    AD authentication. The service parses HTML documents that are marked up with ID
    tags.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Immersive Reader JavaScript 库是一个 Web 应用程序，您可以将其作为 iframe 集成到 C＃、JavaScript、Kotlin、Java（Android）或
    Swift 应用程序中；您还需要构建一个按钮来关闭 Immersive Reader UI 并配置 Azure AD 身份验证。该服务解析带有 ID 标签的
    HTML 文档。
- en: (While we’ve used Python for other examples in this book, here we’re using JavaScript
    to go with the Immersive Reader SDK.)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: （虽然我们在本书中的其他示例中使用了 Python，但在这里我们使用 JavaScript 来配合 Immersive Reader SDK。）
- en: You can use the SDK in web applications directly or load it via npm or Yarn
    for use in Node.js applications.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以直接在 Web 应用程序中使用 SDK，或者通过 npm 或 Yarn 加载它用于 Node.js 应用程序。
- en: 'In a web page, use the following line to load the library:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在网页中，使用以下行加载库：
- en: '[PRE6]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To use the library in your web page, add an HTML element to load the Immersive
    Reader launch button. This will invoke the SDK. You can either write Document
    Object Model parsing code to work with existing content on the page or load the
    content in chunks from other sources. Your content will need to be in a content
    string:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 要在您的网页中使用库，添加一个 HTML 元素来加载 Immersive Reader 启动按钮。这将调用 SDK。您可以编写文档对象模型解析代码以处理页面上的现有内容，或者从其他来源分块加载内容。您的内容需要是一个内容字符串：
- en: '[PRE7]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This launches the following function to display some text in Immersive Reader.
    You will need to pass it an authentication token and the URL of your service endpoint
    along with some content:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这会启动以下功能以在沉浸式阅读器中显示一些文本。您需要传递认证令牌和服务端点的URL以及一些内容：
- en: '[PRE8]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Use Transfer Learning to Train Vision, Speech, and Language Models in Minutes
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用迁移学习在几分钟内训练视觉、语音和语言模型。
- en: Many of the Cognitive Services APIs and  some of the Applied AI Services are
    ready to use as soon as you configure the resources; others have options for you
    to improve them by training with your own data. You can use prebuilt domains and
    dictionaries in LUIS that cover areas like music or calendar entries, and the
    service will learn from what your users ask. But you can also build custom dictionaries
    that cover the entities your users will be talking about, the things they will
    want to do, and the ways they’re likely to phrase that intent. Similarly, you
    can tell Microsoft Translator about product names and terminology you use in your
    business to make translations more useful, or train the Form Recognizer service
    on the form layouts you use frequently.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 许多认知服务API和部分应用AI服务在配置资源后即可使用；另一些服务可以通过使用自有数据进行训练来改进。你可以在LUIS中使用预建立的领域和词典，如音乐或日历条目，并且服务会从用户询问的内容中学习。但你也可以构建自定义词典，覆盖用户可能谈论的实体、他们希望完成的任务以及他们可能表达意图的方式。同样，你可以告诉Microsoft
    Translator有关你业务中使用的产品名称和术语，以使翻译更加实用，或者训练Form Recognizer服务识别你经常使用的表单布局。
- en: For vision and speech recognition, you can train a custom model. This uses transfer
    learning to take a model that has already been learned by a deep neural network
    using a large training set and then takes off one or more of the final layers
    that output predictions or classifications, replacing them with new layers trained
    on data that covers your specific task. That way, you can take advantage of a
    large model that’s been trained at scale and quickly adjust it to your problem
    with a relatively small amount of data.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于视觉和语音识别，你可以训练一个定制模型。这利用迁移学习，采用已经通过大规模训练集学习的深度神经网络模型，然后去掉一个或多个最终层，这些层用于输出预测或分类，替换为针对你特定任务训练的新层。这样，你可以利用大规模训练的大模型，并用相对较少的数据快速调整以解决你的问题。
- en: Creating a Custom Vision Model
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建定制视觉模型。
- en: If you need to recognize images of, say, the range of products your company
    makes and the types of damage that occur to them, or you want to be able to find
    your company logo even if it’s skewed because it’s on the side of a truck, the
    standard image tagging service might not be accurate enough.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要识别图片，比如你公司生产的产品范围及其可能遭受的损坏类型，或者你希望即使公司标志歪斜在卡车侧面也能找到，标准的图片标记服务可能不够精确。
- en: The Computer Vision API doesn’t know what a leaf infected with a specific disease
    looks like compared to a leaf from a plant that hasn’t been watered enough or
    a healthy egg sac in a fertilized chicken egg versus an egg that isn’t developing
    normally. It won’t be able to spot a circuit board that isn’t soldered correctly
    or detect whether the amount of foam in treated water from an industrial process
    shows that it’s safe for agricultural use.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉API无法区分受特定疾病感染的叶子与未充分浇水的植物叶子，或受精的鸡蛋中的健康卵囊与发育不正常的卵。它也无法检测电路板是否正确焊接，或者处理工业过程中产生的泡沫量是否适合农业用水。
- en: The Custom Vision Service lets you build your own custom classifier based on
    a relatively small set of labeled images that show exactly the objects, conditions,
    and concepts you need to recognize. You can use it to process images that customers
    send you, pair it with a cheap camera to replace or supplement expensive equipment
    like a spectrometer, export the model to a smartphone to give employees an app
    that gives them point-and-click answers, or use containers to embed the trained
    model in a drone or smart camera for real-time recognition.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 定制视觉服务允许您基于一小组标记图像构建自定义分类器，显示确切的对象、条件和概念。您可以用它处理客户发送的图像，将其与廉价摄像机配对，以替代或补充如分光计之类的昂贵设备，将模型导出到智能手机以为员工提供即点即答的应用程序，或使用容器将训练模型嵌入无人机或智能摄像头中进行实时识别。
- en: Custom Vision uses transfer learning, removing some of the final layers of a
    multilayer pretrained ResNet model for specific domains (food, landmarks, retail,
    adult, or a general image recognition classifier) and retraining on your own set
    of far fewer images that you upload to the portal and tag with the objects or
    scenes depicted. For the best results, your training set can be as small as 30
    to 50 images, ideally with a good range of camera angles, lighting, and background,
    variety in the size of the object, and both individual and grouped subjects. If
    the camera angle will be fixed, label common objects that will always be in the
    shot, like equipment in the background.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义视觉使用迁移学习，从多层预训练的 ResNet 模型中移除一些最终层，专门用于特定领域（食品、地标、零售、成人或一般图像识别分类器），然后重新训练您上传并标记对象或场景的少量图像。为了获得最佳结果，您的训练集可以仅包括
    30 到 50 张图像，最好涵盖不同的摄像机角度、光照和背景变化，物体大小的多样性，以及单个和分组主题。如果相机角度固定，请标记始终在镜头中的常见物体，如背景中的设备。
- en: You can create multiple models and layer them to improve discrimination in classes
    that are easy to confuse (like tomatoes and bell peppers or sandwiches and layer
    cakes). You can build a model for detecting objects or classifying them, and sort
    images into single categories (multiclass classification) or apply as many tags
    as match the image (multilabel classification). Create the model and upload images
    through the [Custom Vision portal](https://www.customvision.ai) (shown in [Figure 5-9](#the_custom_vision_portal_makes_it_easy)),
    or in code.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以创建多个模型并层叠它们以改善易混淆类别的区分度（如西红柿和甜椒或三明治和蛋糕层）。您可以构建用于检测对象或对其进行分类的模型，并将图像分类为单一类别（多类别分类）或应用与图像匹配的多个标签（多标签分类）。通过[自定义视觉门户](https://www.customvision.ai)（如图[5-9](#the_custom_vision_portal_makes_it_easy)所示），或者通过代码上传图像。
- en: '![The Custom Vision portal makes it easy to upload and tag your training images](Images/aasc_0509.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![自定义视觉门户](Images/aasc_0509.png)'
- en: Figure 5-9\. The Custom Vision portal makes it easy to upload and tag your training
    images
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-9. 自定义视觉门户使得上传和标记训练图像变得轻松
- en: Tune when predictions are considered to be correct by setting the Probability
    Threshold slider. Setting the threshold high favors precision over recall (classifications
    will be correct, but few of them will be found); setting it low will favor recall,
    so most of the classifications will be found but there will be false positives.
    Experiment with this and use the threshold value that best suits your project
    as a filter when you retrieve results from the model.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置概率阈值滑块来调整何时认为预测是正确的。将阈值设置高有利于精确度而不是召回率（分类会正确，但找到的数量较少）；将其设置低有利于召回率，因此大多数分类将被找到，但可能会有误报。尝试不同设置，并使用最适合您项目的阈值作为从模型检索结果时的过滤器。
- en: Warning
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: The responsible AI considerations we look at in [Chapter 7](ch07.xhtml#responsible_ai_development_and_use)
    apply perhaps most strongly to image recognition, and especially to facial recognition
    and spatial analysis. It’s important to have a balanced dataset that covers the
    range of objects you want to classify that doesn’t introduce any false correlations
    (like having rulers or coins to show scale), or your custom model will perform
    poorly. We look at this in more detail in [Chapter 8](ch08.xhtml#best_practices_for_machine_learning_pro)
    as part of machine learning best practices, along with understanding training
    performance and achieving accurate, reliable results.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第7章](ch07.xhtml#responsible_ai_development_and_use)中看到的负责任人工智能考虑事项可能最适用于图像识别，特别是面部识别和空间分析。重要的是要有一个平衡的数据集，涵盖您想要分类的对象范围，不要引入任何虚假的相关性（例如将尺子或硬币放置以显示比例），否则您的自定义模型将表现不佳。我们在[第8章](ch08.xhtml#best_practices_for_machine_learning_pro)中更详细地讨论这一点，作为机器学习最佳实践的一部分，还包括理解训练性能和实现准确可靠的结果。
- en: For challenging datasets or where you need very fine-grained classification,
    the Advanced Training option in the portal lets you specify how long you want
    the Custom Vision service to spend training the model. Once you’re happy with
    the accuracy of a model, you can publish it as a prediction API from the Performance
    tab in the portal and get the Prediction URL and Prediction-Key to call in your
    code.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有挑战性数据集或需要非常精细分类的情况，门户中的高级训练选项允许您指定自定义视觉服务在训练模型时要花费的时间长短。一旦您对模型的准确性感到满意，您可以从门户的性能选项卡将其发布为预测
    API，并获取预测 URL 和预测密钥以在您的代码中调用。
- en: 'The following code snippet shows how to build a custom vision model, first
    tagging and uploading data, then training it. Once trained, the model can be published
    and used to classify images. Once you have created a resource using Custom Vision
    in the Azure portal, start by importing the custom vision libraries:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了如何构建自定义视觉模型，首先标记和上传数据，然后进行训练。训练完成后，模型可以发布并用于分类图像。一旦您在 Azure 门户中使用自定义视觉创建资源，首先导入自定义视觉库：
- en: '[PRE9]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This will let you add specific libraries to your code, for training and prediction.
    You’ll need to get your various keys and endpoint details from the portal before
    adding them to your code.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使您能够向您的代码添加特定的库，用于训练和预测。在将它们添加到代码之前，您需要从门户获取各种密钥和终结点详细信息。
- en: 'Much of the Custom Vision service can be handled programmatically—for example,
    creating your training project:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义视觉服务的大部分功能可以通过编程方式处理，例如创建您的训练项目：
- en: '[PRE10]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You can now add your training tags to the project. Here we’re going to build
    a model that can distinguish Airbus aircraft from Boeing’s:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以将您的训练标签添加到项目中。我们将构建一个能够区分空客飞机和波音飞机的模型：
- en: '[PRE11]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You’ll need a set of training images for each tag, up to 64 images per uploaded
    batch:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 每个标签都需要一组训练图像，每次上传最多 64 张图像：
- en: '[PRE12]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we train the model. This can take some time to run:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们训练模型。这可能需要一些时间来运行：
- en: '[PRE13]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Once the model is trained, it’s ready to be published:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，就可以发布了：
- en: '[PRE14]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can now run a prediction against the model, using images of aircraft to
    test its operation:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以对模型运行预测，使用飞机的图像来测试其操作：
- en: '[PRE15]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This can all be built into one application or split across separate training
    and prediction apps.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些可以构建成一个应用程序，或分成单独的训练和预测应用程序。
- en: Running a model online has the advantage that you don’t need to rebuild your
    app when you update a model, but local deployment may be a better choice for image
    and video recognition where high latency can cause problems. If you want to embed
    your Custom Vision classifier in an app to run it locally on a device, you can
    export it as TensorFlow for Android, TensorFlow.js for the web and apps built
    using JavaScript frameworks, CoreML for iOS 11, ONNX for WinML, or as a Windows
    or Linux container (that can also run on ARM hardware) with a TensorFlow model
    and the services necessary to call the model from Python. This means using a compact
    domain that may be slightly less accurate than a standard domain; if you didn’t
    choose the compact domain to start with, you can convert to that, but you will
    have to retrain the model, so plan ahead if you want to use a Custom Vision model
    offline.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在线运行模型的优点是，在更新模型时，无需重建应用程序，但在高延迟可能会引起问题的图像和视频识别情况下，本地部署可能是更好的选择。如果您想将自定义视觉分类器嵌入应用程序以在设备上本地运行它，可以将其导出为适用于
    Android 的 TensorFlow、适用于 Web 和使用 JavaScript 框架构建的应用程序的 TensorFlow.js、适用于 iOS 11
    的 CoreML、适用于 WinML 的 ONNX 或作为 TensorFlow 模型和从 Python 调用模型所需的服务的 Windows 或 Linux
    容器（也可以在 ARM 硬件上运行）。这意味着使用一个紧凑的域可能略低于标准域的精度；如果您一开始没有选择紧凑域，可以转换为它，但您将不得不重新训练模型，因此如果想要离线使用自定义视觉模型，请提前计划。
- en: Custom Vision also supports Microsoft’s Vision AI DevKit, which is a $300 smart
    camera development and test platform, with all the hardware you need to deploy
    Custom Vision models and run them on the edge of your network.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义视觉还支持 Microsoft 的 Vision AI DevKit，这是一个 300 美元的智能摄像头开发和测试平台，拥有部署自定义视觉模型和在网络边缘运行它们所需的所有硬件。
- en: Creating a Custom Speech Model
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建自定义语音模型
- en: One of the difficulties with speech recognition is the many  different ways
    people speak. Speech styles, prosody, accents, and vocabulary vary, your field
    may have unusual terms, or you might need to recognize product names that could
    be confused for everyday words. The places speech is being recorded in can pose
    additional challenges; background noise at a drive-through or the acoustics of
    a mall or the reception desk in a building lobby are very different from someone
    speaking into their phone. Instead of using the default speech models, you can
    build a custom speech model for your specific tasks. Custom language models can
    be used to understand accents or work with specific vocabularies, building on
    top of the existing trained models.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 语音识别的一个难点在于人们说话的多种方式。语音风格、韵律、口音和词汇都有所不同，您的领域可能有不寻常的术语，或者您可能需要识别可能会与日常用语混淆的产品名称。语音录制的场所可能会带来额外的挑战；在驾车通道的背景噪音或商场的声学环境或大厦前台的接待处，这些都与某人在手机上说话完全不同。与使用默认语音模型不同，您可以为特定任务构建自定义语音模型。可以利用定制语言模型理解口音或处理特定词汇，这是在现有训练模型基础上的延伸。
- en: Tip
  id: totrans-144
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: You can customize the Custom Speech, Custom Commands (for voice-controlled apps),
    and Custom Voice (for text-to-speech) services using the no-code [Speech Studio
    visual environment](https://speech.microsoft.com) and then call them in your applications
    using the Speech SDK, Speech CLI, or the REST APIs.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用无代码的 [Speech Studio 可视化环境](https://speech.microsoft.com) 定制 Custom Speech、Custom
    Commands（用于语音控制应用程序）和 Custom Voice（用于文本转语音）服务，然后使用 Speech SDK、Speech CLI 或 REST
    API 在您的应用程序中调用它们。
- en: 'You can then add acoustic models to account for the complexities of varied
    environments where accurate recognition is essential: in vehicles, on the factory
    floor, or out in the field. Adding a custom acoustic model will definitely be
    necessary if you’re building code for use in a predictably noisy environment:
    using voice recognition in a car, or working with a specific device that might
    process sound in a particular way—and if you’re doing that, you will likely want
    to run the models locally in a container. You can also build custom language models,
    either for a specific technical vocabulary or to improve recognition on accented
    speech.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以添加声学模型，以考虑各种环境中准确识别的复杂性：在车辆、工厂车间或户外工作时。如果您在一个预期嘈杂环境中使用语音识别 —— 比如在车内使用语音识别，或者使用特定设备可能会以特定方式处理声音
    —— 那么肯定需要添加自定义声学模型，而且如果您这样做，您可能想要在容器中本地运行这些模型。您还可以构建自定义语言模型，无论是为了特定的技术词汇还是为了提高对口音语音的识别。
- en: For training, you can upload audio files or just text with sentences that contain
    jargon, technical terms, and other phrases specific to your area that might not
    be recognized correctly. Text training is faster—several hours rather than several
    days—so it’s worth starting with that to see if it improves recognition enough
    for your needs (and not all Azure regions have dedicated hardware for audio training).
    To get the best results, include text that uses your specific vocabulary in different
    sentences and contexts that cover the ways you expect the terms to be used. You
    can provide up to 1.5 GB of raw text data.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练，您可以上传音频文件，或只需包含行话、技术术语和其他特定于您领域的短语的句子的文本，这些短语可能无法被正确识别。文本训练更快速 —— 几个小时而不是几天
    —— 所以从这里开始看是否足以满足您的需求（并非所有 Azure 区域都有专用的音频训练硬件）。为了获得最佳结果，包括使用特定词汇的文本，在不同的句子和上下文中涵盖您期望这些术语被使用的方式。您可以提供高达
    1.5 GB 的原始文本数据。
- en: 'For audio training, you need five or more audio files recorded in the same
    conditions in which your code will be recognizing speech. That means people talking
    in the environment or into the device you plan to use. You can also use this method
    to tune speech recognition to a single voice, a useful technique for transcribing
    podcasts or other audio sources. But if you want to recognize multiple speakers,
    you need recordings of a diverse collection of voices: different accents, dialects,
    genders, and ages, and maybe even people recorded at different times of day or
    when they’re in a hurry, because stress affects speech patterns.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对于音频训练，您需要在与您的代码识别语音的相同条件下录制五个或更多音频文件。这意味着人们在环境中说话，或对着您计划使用的设备说话。您也可以使用此方法来调整语音识别到单一声音，这是转录播客或其他音频来源的有用技术。但如果您想识别多个说话者，则需要录制多种声音的录音：不同的口音、方言、性别和年龄，甚至可能是在一天中不同时间或急于时录制的人的声音，因为压力会影响语音模式。
- en: 'Data needs to be in 8 kHz or 16 kHz WAV files, using mono recordings. Split
    them up into 10- to 12-second chunks for the best results, starting and finishing
    with silence. Each file needs a unique name and should contain a single utterance:
    a query, a name, or a short sentence. Package the files in a single zipped folder
    that’s less than 2 GB, and upload that to the Speech Services web portal shown
    in [Figure 5-10](#upload_speech_samples_and_transcription).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 数据需要以 8 kHz 或 16 kHz 的 WAV 文件格式存在，并使用单声道录音。将它们拆分成 10 到 12 秒的片段以获得最佳结果，并以静音开头和结尾。每个文件需要一个唯一的名称，并且应包含一个单独的话语：查询、名称或简短句子。将文件打包到一个小于
    2 GB 的单个压缩文件夹中，然后上传到 Speech Services 网页门户中显示的 [图 5-10](#upload_speech_samples_and_transcription)。
- en: '![Upload speech samples and transcriptions in Speech Studio to create custom
    models](Images/aasc_0510.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![在 Speech Studio 中上传语音样本和转录以创建自定义模型](Images/aasc_0510.png)'
- en: Figure 5-10\. Upload speech samples and transcriptions in Speech Studio to create
    custom models
  id: totrans-151
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-10\. 在 Speech Studio 中上传语音样本和转录以创建自定义模型。
- en: 'Each file needs to be accompanied by a transcription in the correct format:
    a single line of text in a file that starts with the audio filename, then a tab,
    then the text. Once those are uploaded, use the Speech Services portal to apply
    Custom Speech and pick the zipped folder as your Adaptation Data. Run the import
    process to add your data to the Speech Service, where it’ll be processed automatically.
    Then you need a dataset to test it: that includes up to five hours of audio with
    a human-labeled transcript.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 每个文件都需要以正确格式的转录作为陪伴：文件中的单行文本，以音频文件名开头，然后是一个制表符，再接着是文本内容。上传完毕后，请使用 Speech Services
    门户应用自定义语音并选择压缩文件夹作为适应数据。运行导入过程将您的数据添加到 Speech Service 中，这些数据将自动处理。然后，您需要一个数据集来进行测试：其中包括长达五小时的音频和一个人工标记的转录。
- en: The accuracy of speech recognition is usually measured by the word error rate
    (WER). Count up all the mistakes the model makes—whether that’s adding a word
    that shouldn’t be there (insertions), missing a word that should (deletions),
    or recognizing the wrong word (substitutions)—and divide that by the number of
    words in the test transcript, then multiply by 100 to get the rate. If you expect
    to retrain your model frequently to improve accuracy, set up a CI/CD workflow
    to train and test models to see if the WER improves.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 语音识别的准确率通常通过词错误率（WER）来衡量。计算模型的所有错误，无论是多加一个不该有的词（插入），少了一个应该有的词（删除），还是识别错误的词（替换），然后除以测试转录中的单词数，再乘以
    100 得到比率。如果你希望频繁重新训练模型以提高准确率，请设置一个 CI/CD 工作流来训练和测试模型，以查看 WER 是否有所改善。
- en: Tip
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Use [this template](https://go.microsoft.com/fwlink/?linkid=2190278) to create
    a DevOps workflow to train, test, and release Azure Custom Speech models using
    GitHub Actions.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 [此模板](https://go.microsoft.com/fwlink/?linkid=2190278) 创建一个 DevOps 工作流，以使用
    GitHub Actions 训练、测试和发布 Azure 自定义语音模型。
- en: 'If you need to recognize what people are saying in a noisy environment or over
    lo-fi equipment like a walkie-talkie, you can also use your data to create a custom
    acoustic model: creating a new model from Azure’s base models, one for directed
    speech and one for conversational speech, adding your own acoustic data. Custom
    acoustic models can work with the default speech models or with a custom recognition
    model.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要在嘈杂的环境中或使用像对讲机这样的低保真设备识别人们在说什么，还可以使用您的数据创建一个自定义声学模型：从 Azure 的基础模型创建一个新模型，一个用于定向语音，一个用于对话语音，并添加您自己的声学数据。自定义声学模型可以与默认语音模型或自定义识别模型配合使用。
- en: Once your custom model is trained, deploy a custom endpoint you can call in
    your code.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的自定义模型训练完成，部署一个可以在您的代码中调用的自定义端点。
- en: Wrapping It Up
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we’ve looked at some of the most useful prebuilt cloud AI services
    that are available from Azure (or, for a selection of services, running in containers
    on your own infrastructure) that you can call from your own apps using APIs and
    SDKs.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了一些最有用的预构建云 AI 服务，这些服务可以从 Azure 调用（或在您自己的基础设施上的容器中运行部分服务），您可以使用 API
    和 SDK 从自己的应用程序中调用这些服务。
- en: But you don’t have to be a developer building applications from scratch to use
    Cognitive Services and Applied AI Services; several of them are also available
    inside Microsoft’s no-code and low-code tools, the Power Platform and Logic Apps.
    In the next chapter, we’ll look at how business users can work with developers
    or on their own to take advantage of these prebuilt cloud AI services for analysis,
    automation, and making low-code apps more powerful.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，你不必成为一个从头开始构建应用程序的开发者才能使用认知服务和应用AI服务；其中一些服务也可在微软的无代码和低代码工具中使用，例如 Power Platform
    和 Logic Apps。在下一章中，我们将看看业务用户如何与开发者合作或独立操作，利用这些预构建的云AI服务进行分析、自动化，并使低代码应用程序更加强大。
