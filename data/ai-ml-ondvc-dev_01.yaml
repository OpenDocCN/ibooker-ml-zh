- en: Chapter 1\. Introduction to AI and Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章\. 人工智能与机器学习简介
- en: You’ve likely picked this book up because you’re curious about artificial intelligence
    (AI), machine learning (ML), deep learning, and all of the new technologies that
    promise the latest and greatest breakthroughs. Welcome! In this book, my goal
    is to explain a little about how AI and ML work, and a lot about how you can put
    them to work for you in your mobile apps using technologies such as TensorFlow
    Lite, ML Kit, and Core ML. We’ll start light, in this chapter, by establishing
    what we actually *mean* when we describe artificial intelligence, machine learning,
    deep learning, and more.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能拿起这本书是因为对人工智能（AI）、机器学习（ML）、深度学习以及所有承诺最新和最伟大突破的新技术感到好奇。欢迎！在本书中，我的目标是解释一些关于AI和ML如何运作的知识，以及如何利用TensorFlow
    Lite、ML Kit和Core ML等技术在你的移动应用中使用它们。我们会从本章开始，轻松地建立起我们在描述人工智能、机器学习、深度学习等时实际上*指的是*什么。
- en: What Is Artificial Intelligence?
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是人工智能？
- en: 'In my experience, artificial intelligence, or AI, has become one of the most
    fundamentally misunderstood technologies of all time. Perhaps the reason for this
    is in its name—artificial intelligence evokes the *artificial* creation of an
    *intelligence.* Perhaps it’s in the use of the term widely in science fiction
    and pop culture, where AI is generally used to describe a robot that looks and
    sounds like a human. I remember the character Data from *Star Trek: The Next Generation*
    as the epitome of an artificial intelligence, and his story led him in a quest
    to be human, because he was intelligent and self-aware but lacked emotions. Stories
    and characters like this have likely framed the discussion of artificial intelligence.
    Others, such as nefarious AIs in various movies and books, have led to a fear
    of what AI can be.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我的经验，人工智能（AI）已经成为历史上最基本误解的技术之一。也许这个误解的原因在于其名称——人工智能唤起了*智能*的*人工*创造。也许它广泛在科幻小说和流行文化中使用的原因，AI通常用来描述一个看起来和听起来像人类的机器人。我记得《星际迷航：下一代》中的Data角色就是人工智能的典范，他的故事引导他追求成为人类，因为他聪明且自我意识，但缺乏情感。像这样的故事和角色可能塑造了人工智能的讨论。其他如电影和书籍中的邪恶AI则导致了对AI潜在危险的恐惧。
- en: Given how often AI is seen in these ways, it’s easy to come to the conclusion
    that they define AI. However, none of these are actual definitions or examples
    of what artificial intelligence is, at least in today’s terms. It’s not the artificial
    creation of intelligence—it’s the artificial *appearance* of intelligence. When
    you become an AI developer, you’re not building a new lifeform—you’re writing
    code that acts in a different way to traditional code, and that can very loosely
    emulate the way an intelligence reacts to something. A common example of this
    is to use deep learning for c*omputer vision* where, instead of writing code that
    tries to understand the contents of an image with a lot of if...then rules that
    parse the pixels, you can instead have a computer *learn* what the contents are
    by “looking” at lots of samples.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于AI经常被看作这些方式，很容易得出结论，它们定义了AI。然而，这些都不是人工智能的实际定义或例子，至少在当今的术语中不是。它不是智能的人工创造——而是智能*外观*的人工创造。当你成为AI开发者时，你并不是在构建一个新的生命形式——你正在编写与传统代码不同的代码，它可以非常宽松地模拟智能对某事物的反应方式。这的一个常见例子是使用深度学习进行*计算机视觉*，在这里，你不是编写试图通过大量的if...then规则解析像素来理解图像内容的代码，而是可以通过“观察”大量样本来*学习*图像的内容是什么。
- en: So, for example, say you want to write code to tell the difference between a
    T-shirt and a shoe ([Figure 1-1](#a_t_shirt_and_a_boot)).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，例如，假设你想编写代码来区分一件T恤和一只鞋（[图1-1](#a_t_shirt_and_a_boot)）。
- en: '![](assets/aiml_0101.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0101.png)'
- en: Figure 1-1\. A T-shirt and a shoe
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-1\. 一件T恤和一只鞋
- en: How would you do this? Well, you’d probably want to look for particular shapes.
    The distinct vertical lines in parallel on the T-shirt, with the body outline,
    are a good signal that it’s a T-shirt. The thick horizontal lines towards the
    bottom, the sole, are a good indication that it’s a shoe. But there’s a lot of
    code you would have to write to detect that. And that’s just for the general case—of
    course there would be many exceptions for nontraditional designs, such as a cutout
    T-shirt.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你会怎么做呢？嗯，你可能想要寻找特定的形状。T恤上平行的明显垂直线，以及身体轮廓，是它是T恤的一个明显信号。靠近底部的粗横线，也就是鞋底，是它是鞋子的一个很好的指示。但是，要编写很多代码来检测这些特征。而且这只是一般情况下的情况——当然，对于非传统设计，比如开衫T恤，会有很多例外情况。
- en: If you were to ask an intelligent being to pick between a shoe and a T-shirt,
    how would you do it? Assuming it had never seen them before, you’d show the being
    lots of examples of shoes and lots of examples of T-shirts, and it would just
    figure out what made a shoe a shoe, and what made a T-shirt a T-shirt. You wouldn’t
    need to give it lots of *rules* saying which one is which. *Artificial* Intelligence
    acts in the same way. Instead of figuring out all those rules and inputting them
    into a computer in order to tell that difference, you give the computer lots of
    examples of T-shirts and lots of examples of shoes, and it just figures out how
    to distinguish them.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你要求一个智能生物在鞋子和T恤之间做选择，你会怎么做呢？假设它以前从未见过它们，你会向它展示许多鞋子的例子和许多T恤的例子，它会自己弄清楚什么让鞋子成为鞋子，什么让T恤成为T恤。你不需要给它很多*规则*来告诉它哪个是哪个。*人工*智能的工作方式与此类似。与其弄清楚所有这些规则并将它们输入计算机以区分它们，你向计算机展示大量T恤和鞋子的例子，它会自己弄清楚如何区分它们。
- en: But the computer doesn’t do this by itself. It does it with code that you write.
    That code will look and feel *very* different from your typical code, and the
    framework by which the computer will learn to distinguish isn’t something that
    you’ll need to figure out how to write for yourself. There are several frameworks
    that already exist for this purpose. In this book you’ll learn how to use one
    of them, *TensorFlow,* to create applications like the one I just mentioned!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 但计算机不会单独完成这项任务。它需要你编写的代码来完成。那些代码看起来和感觉起来与你通常编写的代码*非常*不同，而计算机用来学习区分的框架并不需要你自己来编写。已经有几个为此目的而存在的框架。在这本书中，你将学习如何使用其中之一，*TensorFlow*，来创建像我刚提到的那种应用程序！
- en: TensorFlow is an end-to-end open source platform for ML. You’ll use many parts
    of it extensively in this book, from creating models that use ML and deep learning,
    to converting them to mobile-friendly formats with TensorFlow Lite and executing
    them on mobile devices, to serving them with TensorFlow-Serving. It also underpins
    technology such as ML Kit, which provides many common models as *turnkey scenarios*
    with a high-level API that’s designed around mobile scenarios.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow是一个端到端开源平台，用于机器学习。在本书中，你将广泛使用它的许多部分，从创建使用ML和深度学习的模型，到将它们转换为适合移动设备的格式并在TensorFlow
    Lite上执行，再到使用TensorFlow-Serving提供服务。它还支持诸如ML Kit之类的技术，提供许多常见模型作为*即插即用的场景*，并提供围绕移动场景设计的高级API。
- en: As you’ll see when reading this book, the techniques of AI aren’t particularly
    new or exciting. What *is* relatively new, and what made the current explosion
    in AI technologies possible, is increased, low-cost computing power, along with
    the availability of mass amounts of data. Having both is key to building a system
    using machine learning. But to demonstrate the concept, let’s start small, so
    it’s easier to grasp.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在阅读本书时会看到的那样，AI的技术并不特别新颖或激动人心。相对*新*的是，以及使当前AI技术爆发成为可能的是，增加的低成本计算能力，以及大量数据的可用性。拥有这两者对于使用机器学习构建系统至关重要。但为了演示这个概念，让我们从小处开始，这样更容易理解。
- en: What Is Machine Learning?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是机器学习？
- en: You might have noticed in the preceding scenario that I mentioned that an intelligent
    being would look at lots of examples of T-shirts and shoes and just figure out
    what the difference between them was, and in so doing, would *learn* how to differentiate
    between them. It had never previously been exposed to either, so it gains new
    knowledge about them by being *told* that these are T-shirts, and these are shoes.
    From that information, it then was able to move forward by learning something
    new.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述场景中，你可能注意到我提到过智能生物会查看大量T恤和鞋子的例子，并试图找出它们之间的区别，这样一来，它就*学会*了如何区分它们。它以前从未接触过这些，因此通过被*告知*这些是T恤和鞋子来获取了关于它们的新知识。有了这些信息，它就能继续学习新的东西。
- en: When programming a computer in the same way, the term m*achine learning* is
    used. Similar to artificial intelligence, the terminology can create the false
    impression that the computer is an intelligent entity that learns the way a human
    does, by studying, evaluating, theorizing, testing, and then remembering. And
    on a very surface level it does, but how it does it is far more mundane than how
    the human brain does it.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在以同样的方式编写计算机程序时，术语*machine learning*被使用。与人工智能类似，这种术语可能会给人一种错误印象，即计算机是一个像人类一样学习的智能实体，通过学习、评估、理论化、测试，然后记忆。在非常表面的层面上，它确实如此，但它的工作方式比人类大脑的方式要平凡得多。
- en: To wit, machine learning can be simply described as having code functions figure
    out their own parameters, instead of the human programmer supplying those parameters.
    They figure them out through trial and error, with a smart optimization process
    to reduce the overall error and drive the model towards better accuracy and performance.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，机器学习可以简单地描述为让代码函数自行找出它们自己的参数，而不是由人类程序员提供这些参数。它们通过试验和错误来找出这些参数，并通过智能优化过程来减少总体错误，从而推动模型朝着更高的准确性和性能前进。
- en: Now that’s a bit of a mouthful, so let’s explore what that looks like in practice.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这有点啰嗦，所以让我们看看实际操作中是什么样子。
- en: Moving from Traditional Programming to Machine Learning
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从传统编程转向机器学习
- en: To understand, in detail, the core difference between coding for machine learning
    and traditional coding, let’s go through an example.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要详细了解机器学习编码与传统编码的核心区别，让我们通过一个例子来说明。
- en: 'Consider the function that describes a line. You might remember this from high
    school geometry:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑描述一条线的函数。你可能还记得这个来自高中几何学的知识：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This describes that for something to be a line, every point y on the line can
    be derived by multiplying x by a value W (for weight) and adding a value B (for
    bias).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这说明了对于某物体是一条线，线上每个点y都可以通过将x乘以W值（权重）并加上B值（偏差）来推导得到。
- en: '(Note: AI literature tends to be very math heavy. Unnecessarily so, if you’re
    just getting started. This is one of a very few math examples I’ll use in this
    book!)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: （注意：人工智能文献往往非常数学化。如果你刚开始接触，这些可能显得有些多余。这是我在本书中使用的极少数数学示例之一！）
- en: Now, say you’re given two points on this line, let’s say they are at x = 2,
    y = 3 and x = 3, y = 5\. How would we write code that figures out the values of
    W and B that describe the line joining these two points?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设你给出了这条线上的两个点，假设它们在x = 2，y = 3和x = 3，y = 5。我们如何编写代码来找出描述连接这两点的线的W和B的值？
- en: Let’s start with W, which we call the weight, but in geometry, it’s also called
    the slope (sometimes gradient). See [Figure 1-2](#visualizing_a_line_segment_with_slope).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从W开始，我们称之为权重，但在几何学中，它也被称为斜率（有时候也称为梯度）。参见[图1-2](#visualizing_a_line_segment_with_slope)。
- en: '![](assets/aiml_0102.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0102.png)'
- en: Figure 1-2\. Visualizing a line segment with slope
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-2\. 可视化斜率的线段
- en: 'Calculating it is easy:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 计算它很容易：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'So if we fill in, we can see that the slope is:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们填写，我们可以看到斜率是：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Or, in code, in this case using Python:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，在代码中，在这种情况下使用Python：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This function sort of works. It’s naive because it ignores a divide by zero
    when the two x values are the same, but let’s just go with it for now.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数有点奇怪。它很天真，因为它忽略了当两个x值相同时的除零情况，但现在就这样继续吧。
- en: OK, so we’ve now figured out the W value. In order to get a function for the
    line, we also need to figure out the B. Back to high school geometry, we can use
    one of our points as an example.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，所以我们现在已经找出了W值。为了得到线的函数，我们还需要找出B值。回到高中几何学，我们可以使用其中一个点作为例子。
- en: 'So, assume we have:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，假设我们有：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can also say:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以说：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And we know that when x = 2, y = 3, and W = 2 we can backfill this function:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道当x = 2，y = 3时，W = 2，我们可以回填这个函数：
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This leads us to derive that B is −1.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致我们得出 B 是 −1。
- en: 'Again, in code, we would write:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，在代码中，我们将写：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'So, now, to determine any point on the line, given an x, we can easily say:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在，为了确定在给定 x 的线上的任何点，我们可以很容易地说：
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Or, for the complete listing:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，对于完整的列表：
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: From these, we could see that when x is 10, y will be 19.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些中，我们可以看到当 x 是 10 时，y 将是 19。
- en: You’ve just gone through a typical programming task. You had a problem to solve,
    and you could solve the problem by figuring out the *rules* and then expressing
    them in code. There was a *rule* to calculate W when given two points, and you
    created that code. You could then, once you’ve figured out W, derive another rule
    when using W and a single point to figure out B. Then, once you had W and B, you
    could write yet another rule to calculate y in terms of W, B, and a given x.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚经历了一个典型的编程任务。您有一个问题需要解决，而您可以通过弄清楚 *规则* 并将其表达为代码来解决问题。在给定两个点时，有一个 *规则* 可以计算出
    W，然后您创建了该代码。然后，一旦您弄清楚了 W，再使用 W 和一个单一点来计算 B 时，会产生另一个规则。然后，一旦您有了 W 和 B，您可以编写另一个规则，根据
    W、B 和给定的 x 计算 y。
- en: That’s traditional programming, which is now often referred to as rules-based
    programming. I like to summarize this with the diagram in [Figure 1-3](#traditional_programming).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是传统编程，现在常常被称为基于规则的编程。我喜欢在 [图1-3](#traditional_programming) 中总结这一点的图解。
- en: '![](assets/aiml_0103.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0103.png)'
- en: Figure 1-3\. Traditional programming
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-3\. 传统编程
- en: At its highest level, traditional programming involves creating *rules* that
    act on *data* and which give us *answers*. In the preceding scenario, we had data—two
    points on a line. We then figured out the rules that would act on this data to
    figure out the equation of that line. Then, given those rules, we could get answers
    for new items of data, so that we could, for example, plot that line.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在其最高层次上，传统编程涉及创建作用于 *数据* 的 *规则* 并为我们提供 *答案*。在前面的场景中，我们有了数据——线上的两个点。然后，我们找出了作用于这些数据以找出该线方程的规则。然后，给定这些规则，我们可以获得新数据项的答案，以便我们可以，例如，绘制那条线。
- en: The core job of the programmer in this scenario is to *figure out the rules*.
    That’s the value you bring to any problem—breaking it down into the rules that
    define it, and then expressing those rules in terms that a computer can understand
    using a coding language.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，程序员的核心工作是 *找出规则*。这是您为任何问题带来的价值—将其分解为定义它的规则，然后用编程语言表达这些规则。
- en: But you may not always be able to express those rules easily. Consider the scenario
    from earlier when we wanted to differentiate between a T-shirt and a shoe. One
    could not always figure out the rules to determine between them, and then express
    those rules in code. Here’s where machine learning can help, but before we go
    into it for a computer vision task like that, let’s consider how machine learning
    might be used to figure out the equation of a line as we worked out earlier.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 但您可能无法轻易表达这些规则。考虑之前的场景，当我们想要区分 T 恤和鞋子时。人们并非总能想出其规则，然后将这些规则表达为代码。这就是机器学习可以帮助的地方，但在我们进入计算机视觉任务的机器学习之前，让我们考虑机器学习如何用于计算这条线的方程，如我们之前拟定的那样。
- en: How Can a Machine Learn?
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器如何学习？
- en: Given the preceding scenario, where you as a programmer figured out the rules
    that make up a line, and the computer implemented them, let’s now see how a Machine
    Learning approach would be different.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于前面的情景，您作为程序员找出组成线的规则，计算机实现了这些规则，现在让我们看看机器学习方法将会有何不同。
- en: Let’s start by understanding how machine learning code is structured. While
    this is very much a “Hello World” problem, the overall structure of the code is
    very similar to what you’d see even in far more complex ones.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从理解机器学习代码的结构开始。虽然这在很大程度上是一个“Hello World”问题，但代码的整体结构非常类似于您在更复杂的问题中看到的代码。
- en: I like to draw a high-level architecture outlining the use of machine learning
    to solve a problem like this. Remember, in this case, we’ll have x and y values,
    so we want to figure out the W and B so that we have a line equation; once we
    have that equation, we can then get new y values given x ones.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢绘制一个高级架构，概述使用机器学习来解决这样的问题。记住，在这种情况下，我们会有 x 和 y 值，所以我们想要弄清楚 W 和 B，以便有一条线性方程；一旦我们有了这个方程式，我们就可以根据
    x 来获取新的 y 值。
- en: 'Step 1: Guess the answer'
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤1：猜测答案
- en: Yes, you read that right. To begin with, we have no idea what the answer might
    be, so a guess is as good as any other answer. In real terms this means we’ll
    pick random values for W and B. We’ll loop back to this step with more intelligence
    a little later, so subsequent values won’t be random, but we’ll start purely randomly.
    So, for example, let’s assume that our first “guess” is that W = 10 and B = 5.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，你没看错。首先，我们不知道答案可能是什么，所以一个猜测与其他任何答案一样好。在实际情况下，这意味着我们将为 W 和 B 选择随机值。稍后我们会以更多的智能回到这一步，所以后续值不会是随机的，但我们将从纯粹的随机开始。因此，例如，让我们假设我们的第一个“猜测”是
    W = 10 和 B = 5。
- en: 'Step 2: Measure the accuracy of our guess'
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '步骤 2: 测量我们猜测的准确性'
- en: Now that we have values for W and B, we can use these against our known data
    to see just how good, or how bad, those guesses are. So, we can use y = 10x +
    5 to figure out a y for each of our x values, compare that y against the “correct”
    value, and use that to derive how good or how bad our guess is. Obviously, for
    this situation our guess is really bad because our numbers would be way off. We’ll
    go into detail on that shortly, but for now, we realize that our guess is really
    bad, *and* we have a measure of how bad. This is often called the *loss*.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了 W 和 B 的值，我们可以将它们用在我们已知的数据上，看看我们的猜测有多好或多坏。因此，我们可以使用 y = 10x + 5 来计算每个
    x 值的 y，将这个 y 与“正确”值进行比较，并据此推断我们的猜测有多好或多坏。显然，在这种情况下，我们的猜测非常糟糕，因为我们的数字会大大偏离。稍后我们会详细介绍这一点，但现在我们意识到我们的猜测非常糟糕，并且我们有一个衡量指标。这通常被称为*损失*。
- en: 'Step 3: Optimize our guess'
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '步骤 3: 优化我们的猜测'
- en: Now that we have a guess, and we have intelligence about the results of that
    guess (or the loss), we have information that can help us create a new and better
    guess. This process is called *optimization.* If you’ve looked at any AI coding
    or training in the past and it was heavy on mathematics, it’s likely you were
    looking at optimization. Here’s where fancy calculus, in a process called g*radient
    descent,* can be used to help make a better guess. Optimization techniques like
    this figure out ways to make small adjustments to your parameters that drive towards
    minimal error. I’m not going to go into detail on that here, and while it’s a
    useful skill to understand how optimization works, the truth is that frameworks
    like TensorFlow implement them for you so you can just go ahead and use them.
    In time, it’s worth digging into them for more sophisticated models, allowing
    you to tweak their learning behavior. But for now, you’re safe just using a built-in
    optimizer. Once you’ve done this, you simply go to step 1\. Repeating this process,
    by definition, helps us over time and many loops, to figure out the parameters
    W and B.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个猜测，并且我们对该猜测结果（或损失）有了了解，这些信息可以帮助我们创建一个新的、更好的猜测。这个过程称为*优化*。如果你以前看过任何涉及到
    AI 编码或训练的内容，而且涉及数学较多，那么很可能你正在看优化过程。在这里，使用一种称为*梯度下降*的高级微积分过程可以帮助我们进行更好的猜测。虽然我在这里不会详细讨论这些内容，而且了解优化工作原理是一个有用的技能，但事实上，像
    TensorFlow 这样的框架已经为你实现了它们，所以你可以直接使用它们。随着时间的推移，你可以深入研究这些内容，以便为更复杂的模型调整它们的学习行为。但是现在，你可以放心地使用内置优化器。完成这一步后，你只需返回步骤
    1。重复这个过程，从定义上来说，帮助我们随着时间和许多循环，找出参数 W 和 B。
- en: And that’s why this process is called m*achine learning*. Over time, by making
    guesses, figuring out how good or how bad that guess might be, optimizing the
    next guess based on that intel, and then repeating it, the computer will “learn”
    the parameters for W and B (or indeed anything else), and from there, it will
    *figure out* the rules that make up our line. Visually, this might look like [Figure 1-4](#the_machine_learning_algorithm).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么这个过程被称为*机器学习*。随着时间的推移，通过做出猜测，找出该猜测有多好或多坏，根据这些情报优化下一个猜测，然后重复这个过程，计算机将“学习”参数
    W 和 B（或其他任何东西），然后，它将*找出*构成我们线条的规则。从视觉上看，这可能看起来像[图 1-4](#the_machine_learning_algorithm)。
- en: '![](assets/aiml_0104.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0104.png)'
- en: Figure 1-4\. The machine learning algorithm
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-4\. 机器学习算法
- en: Implementing machine learning in code
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在代码中实现机器学习
- en: That’s a lot of description, and a lot of theory. Let’s now take a look at what
    this would look like in code, so you can see it running for yourself. A lot of
    this code may look alien to you at first, but you’ll get the hang of it over time.
    I like to call this the “Hello World” of machine learning, as you use a very basic
    neural network (which I’ll explain a little later) to “learn” the parameters W
    and B for a line when given a few points on the line.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这是很多描述，也是很多理论。现在让我们看看这在代码中的样子，这样你就可以亲自看到它运行了。这段代码一开始可能对你来说有些陌生，但随着时间的推移，你会掌握它的。我喜欢把这称为机器学习的“Hello
    World”，因为你使用一个非常基础的神经网络（稍后我会解释一下）来“学习”给定线上几个点时的参数W和B。
- en: 'Here’s the code (a full notebook with this code sample can be found in the
    GitHub for this book):'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这是代码（此代码示例的完整笔记本可在本书的GitHub中找到）：
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This is written using the TensorFlow Keras APIs. Keras is an open source framework
    designed to make definition and training of models easier with a high level API.
    It became tightly integrated into TensorFlow in 2019 with the release of TensorFlow
    2.0.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用TensorFlow Keras API编写的。Keras是一个开源框架，旨在通过高级API使模型的定义和训练更加容易。它在2019年与TensorFlow
    2.0的发布中与TensorFlow紧密集成。
- en: Let’s explore this line by line.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐行探讨一下。
- en: First of all is the concept of a *model*. When creating code that learns details
    about data, we often use the term “model” to define the resultant object. A model,
    in this case, is roughly analogous to the get_y() function from the coded example
    earlier. What’s different here is that the model doesn’t *need* to be given the
    W and the B. It will figure them out for itself based on the given data, so you
    can just ask it for a y and give it an x, and it will give you its answer.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 首先是*模型*的概念。在创建关于数据细节的代码时，我们通常使用术语“模型”来定义结果对象。在这种情况下，模型大致相当于之前编码示例中的`get_y()`函数。这里不同的是，模型不需要自己提供W和B。它将根据给定的数据自行计算它们，因此你只需向它询问y并给它一个x，它就会给出它的答案。
- en: 'So our first line of code looks like this—it’s defining the model:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的第一行代码看起来像这样——它正在定义模型：
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: But what’s the rest of the code? Well, let’s start with the word `Dense`, which
    you can see within the first set of parentheses. You’ve probably seen pictures
    of neural networks that look a little like [Figure 1-5](#basic_neural_network).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 但是剩下的代码是什么呢？好吧，让我们从单词`Dense`开始，你可以在第一组括号内看到它。你可能已经看过类似于[图 1-5](#basic_neural_network)的神经网络的图片。
- en: '![](assets/aiml_0105.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0105.png)'
- en: Figure 1-5\. Basic neural network
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-5\. 基础神经网络
- en: You might notice that in [Figure 1-5](#basic_neural_network), each circle (or
    neuron) on the left is connected to each neuron on the right. Every neuron is
    connected to every other neuron in a dense manner. Hence the name `Dense`. Also,
    there are three stacked neurons on the left, and two stacked neurons on the right,
    and these form very distinct “layers” of neurons in sequence, where the first
    “layer” has three neurons, and the second has two neurons.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到，在[图 1-5](#basic_neural_network)中，左侧的每个圆圈（或神经元）都连接到右侧的每个神经元。每个神经元都以密集的方式连接到每个其他神经元。因此得名为`Dense`。此外，左侧有三个堆叠的神经元，右侧有两个堆叠的神经元，这些形成了序列中非常明显的“层”，其中第一“层”有三个神经元，第二“层”有两个神经元。
- en: 'So let’s go back to the code:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们回到代码中：
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This code is saying that we want a sequence of layers (`Sequential`), and within
    the parentheses, we will define those sequences of layers. The first in the sequence
    will be `Dense`, indicating a neural network like that in [Figure 1-5](#basic_neural_network).
    There are no other layers defined, so our `Sequential` just has one layer. This
    layer has only one unit, indicated by the `units=1` parameter, and the input shape
    to that unit is just a single value.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码表示我们想要一系列的层（`Sequential`），在括号内，我们将定义这些层的序列。序列中的第一个将是`Dense`，指示一个像[图 1-5](#basic_neural_network)中的神经网络。没有定义其他层，因此我们的`Sequential`只有一层。这一层只有一个单元，由`units=1`参数表示，该单元的输入形状只是一个单一的值。
- en: So our neural network will look like [Figure 1-6](#simplest_possible_neural_network).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的神经网络看起来像[图 1-6](#simplest_possible_neural_network)。
- en: '![](assets/aiml_0106.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0106.png)'
- en: Figure 1-6\. Simplest possible neural network
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-6\. 最简单的可能神经网络
- en: 'This is why I like to call this the “Hello World” of neural networks. It has
    one layer, and that layer has one neuron. That’s it. So, with that line of code,
    we’ve now defined our model architecture. Let’s move on to the next line:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么我喜欢称之为神经网络的“Hello World”。它只有一个层，该层只有一个神经元。就是这样。因此，通过这行代码，我们已经定义了我们的模型架构。让我们继续下一行：
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here we are specifying *built-in* functions to calculate the loss (remember
    step 2, where we wanted to see how good or how bad our guess was) and the optimizer
    (step 3, where we generate a new guess), so that we can improve on the parameters
    within the neuron for W and B.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们在指定*内置*函数来计算损失（记住第2步，我们想看看我们的猜测有多好或多坏）和优化器（第3步，我们生成一个新的猜测），以便我们可以改进神经元内W和B的参数。
- en: In this case `'sgd'` stands for “stochastic gradient descent,” which is beyond
    the scope of this book; in summary, it uses calculus alongside the mean squared
    error loss to figure out how to minimize loss, and once loss is minimized, we
    should have parameters that are accurate.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`'sgd'`代表“随机梯度下降”，这超出了本书的范围；总之，它使用微积分和均方误差损失来找出如何最小化损失，一旦损失被最小化，我们应该有准确的参数。
- en: 'Next, let’s define our data. Two points may not be enough, so I expanded it
    to six points for this example:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们定义我们的数据。两个点可能不足以，所以我扩展到六个点作为示例：
- en: '[PRE14]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `np` stands for “NumPy,” a Python library commonly used in data science
    and machine learning that makes handling of data very straightforward. You can
    learn more about NumPy at [*https://numpy.org*](https://numpy.org).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`np`代表“NumPy”，这是一个在数据科学和机器学习中常用的Python库，使数据处理非常简单。你可以在[*https://numpy.org*](https://numpy.org)了解更多关于NumPy的信息。'
- en: We’ll create an array of x values and their corresponding y values, so that
    given x = −1, y will be −3, when x is 0, y is −1, and so on. A quick inspection
    shows that you can see the relationship of y = 2x − 1 holds for these values.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个x值及其对应的y值的数组，这样当x = −1时，y将为−3；当x为0时，y为−1，依此类推。快速检查显示，你可以看到y = 2x − 1在这些值上成立。
- en: 'Next let’s do the loop that we had spoken about earlier—make a guess, measure
    how good or how bad that loss is, optimize for a new guess, and repeat. In TensorFlow
    parlance this is often called *fitting*—namely we have x’s and y’s, and we want
    to fit the x’s to the y’s, or, in other words, figure out the rule that gives
    us the correct y for a given x using the examples we have. The `epochs=500` parameter
    simply indicates that we’ll do the loop 500 times:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来让我们执行之前提到的循环 —— 进行猜测，测量那个损失值有多好或多坏，优化以获得新的猜测，并重复。在TensorFlow术语中，这通常被称为*拟合*
    —— 即我们有x和y，我们想要将x拟合到y，或者换句话说，找出给定x时正确y的规则，使用我们拥有的示例。`epochs=500`参数简单地表示我们将重复这个循环500次：
- en: '[PRE15]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'When you run code like this (you’ll see how to do this later in this chapter
    if you aren’t already familiar with it), you’ll see output like the following:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行像这样的代码（如果你对此不太熟悉，稍后在本章中你会看到如何做到这一点），你将看到如下输出：
- en: '[PRE16]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note the `loss` value. The unit doesn’t really matter, but what does is that
    it is getting smaller. Remember the lower the loss, the better your model will
    perform, and the closer its answers will be to what you expect. So the first guess
    was measured to have a loss of 32.4543, but by the fifth guess this was reduced
    to 13.3362.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`loss`值。单位并不重要，但重要的是它在变小。记住，损失越低，模型表现越好，其答案也会越接近你的预期。所以第一次猜测的损失为32.4543，但到第五次猜测时，这个值已降至13.3362。
- en: 'If we then look at the last 5 epochs of our 500, and explore the loss:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们然后查看我们500次中的最后5个周期，并探索损失：
- en: '[PRE17]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: It’s a *lot* smaller, on the order of 5.3 x 10^(-5).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 它小了很多，大约是5.3 x 10^(-5)的量级。
- en: 'This is indicating that the values of W and B that the neuron has figured out
    are only off by a tiny amount. It’s not zero, so we shouldn’t expect the exact
    correct answer. For example, assume we give it x = 10, like this:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明神经元找出的W和B的值只有微小的差异。虽然不是零，所以我们不应期望得到确切的正确答案。例如，假设我们给它x = 10，如下所示：
- en: '[PRE18]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The answer won’t be 19, but a value *very close* to 19, and it’s usually about
    18.98\. Why? Well, there are two main reasons. The first is that neural networks
    like this deal with probabilities, not certainties, so that the W and B that it
    figured out are ones that are *highly probable* to be correct but may not be 100%
    accurate. The second reason is that we only gave six points to the neural network.
    While those six points *are* linear, that’s not proof that every other point that
    we could possibly predict is necessarily on that line. The data could skew away
    from that line...there’s a very low probability that this is the case, but it’s
    nonzero. We didn’t *tell* the computer that this was a line, we just asked it
    to figure out the rule that matched the x’s to the y’s, and what it came up with
    looks like a line but isn’t guaranteed to be one.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 答案不会是19，而是一个非常接近19的值，通常大约是18.98。为什么？原因有两个。首先，像这样的神经网络处理概率而不是确定性，所以它找出的W和B是高度可能正确但可能不是100%准确的。第二个原因是我们只给了神经网络六个点。虽然这六个点*是*线性的，但这并不证明我们可能预测的每个其他点都在这条线上。数据可能会偏离这条线……这种情况的概率非常低，但并非零。我们没有*告诉*计算机这是一条直线，我们只是要求它找出与x和y相匹配的规则，而它找到的看起来像是一条直线但并不能保证是一条直线。
- en: This is something to watch out for when dealing with neural networks and machine
    learning—you will be dealing with probabilities like this!
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理神经网络和机器学习时，有一点需要注意——你将会处理到这样的概率！
- en: There’s also the hint in the method name on our model—notice that we didn’t
    ask it to *calculate* the y for x = 10.0, but instead to *predict* it. In this
    case a prediction (often called an inference) is reflective of the fact that the
    model will *try* to figure out what the value will be based on what it knows,
    but it may not always be correct.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 方法名称中也包含了我们模型的提示——请注意，我们没有要求它*计算* x = 10.0 时的y，而是要*预测*它。在这种情况下，预测（通常称为推断）反映了模型将根据其所知来尝试确定值将会是什么，但它可能并不总是正确的。
- en: Comparing Machine Learning with Traditional Programming
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较机器学习与传统编程
- en: 'Referring back to [Figure 1-3](#traditional_programming) for traditional programming,
    let’s now update it to show the difference between machine learning and traditional
    programming given what you just saw. Earlier we described traditional programming
    as follows: you figure out the rules for a given scenario, express them in code,
    have that code act on data, and get answers out. Machine learning is very similar,
    except that some of the process is reversed. See [Figure 1-7](#from_traditional_programming_to_machine).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾[图 1-3](#traditional_programming)，描述传统编程的方式是：你为给定的情景找出规则，将其表达为代码，让代码作用于数据，得出答案。机器学习非常类似，只是过程的某些部分是反向的。参见[图 1-7](#from_traditional_programming_to_machine)。
- en: '![](assets/aiml_0107.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0107.png)'
- en: Figure 1-7\. From traditional programming to machine learning
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-7\. 从传统编程到机器学习
- en: As you can see the key difference here is that with machine learning *you do
    not figure out the rules!* Instead you provide it answers and data, and the machine
    will figure out the rules for you. In the preceding example, we gave it the correct
    y values (aka the answers) for some given x values (aka the data), and the computer
    figured out the rules that fit the x to the y. We didn’t do any geometry, slope
    calculation, interception, or anything like that. The machine figured out the
    patterns that matched the x’s to the y’s.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的关键区别在于，你*不需要弄清楚规则*！相反，你提供答案和数据，机器会为你找出规则。在前面的例子中，我们为一些给定的x值（即数据）提供了正确的y值（即答案），计算机找出了适合将x映射到y的规则。我们没有进行任何几何、斜率计算、截距或类似的操作。机器找出了符合x和y的模式。
- en: That’s the *core* and *important* difference between machine learning and traditional
    programming, and it’s the cause of all of the excitement around machine learning
    because it opens up whole new scenarios of application. One example of this is
    computer vision—as we discussed earlier, trying to write the *rules* to figure
    out the difference between a T-shirt and a shoe would be much too difficult to
    do. But having a computer figure out how one matches to another makes this scenario
    possible, and from there, scenarios that are more important—such as interpreting
    X-rays or other medical scans, detecting atmospheric pollution, and a whole lot
    more—become possible. Indeed, research has shown that in many cases using these
    types of algorithms along with adequate data has led to computers being as good
    as, and sometimes better than, humans at particular tasks. For a bit of fun, check
    out [this blog post](https://oreil.ly/D2Ssu) about diabetic retinopathy—where
    researchers at Google trained a neural network on pre-diagnosed images of retinas
    and had the computer figure out what determines each diagnosis. The computer,
    over time, became as good as the best of the best at being able to diagnose the
    different types of diabetic retinopathy!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这是机器学习与传统编程的**核心**和**重要**区别，也是围绕机器学习所有兴奋的原因，因为它开辟了全新的应用场景。其中一个例子是计算机视觉——正如我们之前讨论的，尝试编写*规则*来区分T恤和鞋子之间的差异将会非常困难。但是让计算机找出如何匹配另一个使这种情景成为可能，并且从那里开始，更重要的场景——如解释X光或其他医学扫描图像，检测大气污染等——也变得可能。事实上，研究表明，在许多情况下，使用这些类型的算法和足够的数据已经使计算机在特定任务上与人类一样好，有时甚至更好。为了好玩，可以查看[这篇博文](https://oreil.ly/D2Ssu)，其中谷歌的研究人员使用预先诊断的视网膜图像对神经网络进行训练，并让计算机找出确定每种诊断的因素。随着时间的推移，计算机已经能够像最优秀的专家一样进行不同类型的糖尿病性视网膜病变的诊断！
- en: Building and Using Models on Mobile
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在移动设备上构建和使用模型
- en: Here you saw a very simple example of how you transition from rules-based programming
    to ML to solve a problem. But it’s not much use solving a problem if you can’t
    get it into your user’s hands, and with ML models on mobile devices running Android
    or iOS, you’ll do exactly that!
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您看到了一个非常简单的示例，展示了如何从基于规则的编程过渡到机器学习来解决问题。但是，如果您无法将其交付给用户，那么解决问题就没有多大用处了，而借助于在运行Android或iOS的移动设备上的ML模型，您将正好做到这一点！
- en: It’s a complicated and varied landscape, and in this book we’ll aim to make
    that easier for you, through a number of different methods.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个复杂而多样的领域，在本书中，我们将通过多种不同的方法来为您简化这一过程。
- en: For example, you might have a turnkey solution available to you, where an *existing*
    model will solve the problem for you, and you just want to learn how to do that.
    We’ll cover that for scenarios like face detection, where a model will detect
    faces in pictures for you, and you want to integrate that into your app.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可能已经有一个现成的解决方案，可以为您解决问题，而您只是想学习如何做到这一点。我们将覆盖像面部检测这样的场景，其中一个模型将为您检测图片中的面部，而您希望将其集成到您的应用程序中。
- en: Additionally, there are many scenarios where you don’t need to build a model
    from scratch, figuring out the architecture, and going through long and laborious
    training. A scenario called t*ransfer learning* can often be used, and this is
    where you are able to take parts of preexisting models and repurpose them. For
    example, Big Tech companies and researchers at top universities have access to
    data and computer power that you may not, and they have used that to build models.
    They’ve shared these models with the world so they can be reused and repurposed.
    You’ll explore that a lot in this book, starting in [Chapter 2](ch02.html#introduction_to_computer_vision).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有许多场景不需要从头开始构建模型，设计架构并进行漫长而费力的训练。经常可以使用一种称为**迁移学习**的场景，这是您可以利用预先存在的模型的部分并重新利用它们的地方。例如，大型科技公司和顶尖大学的研究人员可以访问您可能无法访问的数据和计算能力，并已经利用这些来构建模型。他们与世界分享了这些模型，以便它们可以被重复使用和重新利用。您将在本书中广泛探讨这一点，从[第二章](ch02.html#introduction_to_computer_vision)开始。
- en: Of course, you may also have a scenario where you need to build your own model
    from scratch. This can be done with TensorFlow, but we’ll only touch on that lightly
    here, instead focusing on mobile scenarios. The partner book to this one, called
    *AI and Machine Learning for Coders,* focuses heavily on that scenario, teaching
    you from first principles how models for various scenarios can be built from the
    ground up.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你可能还有需要从头开始构建自己模型的场景。这可以通过TensorFlow来完成，但我们在这里只是轻描淡写地提及一下，而是集中于移动场景。这本书的合作伙伴，《*面向编程人员的AI与机器学习*》，重点讲解这种场景，从基本原理开始教授你如何从零开始为各种情况构建模型。
- en: Summary
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you got an introduction to artificial intelligence and machine
    learning. Hopefully it helped cut through the hype so you can see, from a programmer’s
    perspective, what this is really all about, and from there you can identify the
    scenarios where AI and ML can be extraordinarily useful and powerful. You saw,
    in detail, how machine learning works, and the overall “loop” that a computer
    uses to learn how to fit values to each other, matching patterns and “learning”
    the rules that put them together. From there, the computer could act somewhat
    intelligently, lending us the term “artificial” intelligence. You learned about
    the terminology related to being a machine learning or artificial intelligence
    programmer, including models, predictions, loss, optimization, inference, and
    more.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，你将会对人工智能和机器学习有所了解。希望这能帮助你剖析炒作，从程序员的角度看清楚这一切真正的本质，从而识别出AI和ML可以极其有用和强大的场景。你详细了解了机器学习的工作原理以及计算机如何通过“循环”学习如何将值彼此拟合，匹配模式并“学习”将它们组合在一起的规则。从那里开始，计算机可以有些智能地行动，这使我们得以借用“人工”智能这一术语。你还学习了与成为机器学习或人工智能程序员相关的术语，包括模型、预测、损失、优化、推断等等。
- en: From [Chapter 3](ch03.html#introduction_to_ml_kit) onwards you’ll be using examples
    of these to implement machine learning models into mobile apps. But first, let’s
    explore building some more models of our own to see how it all works. In [Chapter 2](ch02.html#introduction_to_computer_vision)
    we’ll look into building some more sophisticated models for computer vision!
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 从[第三章](ch03.html#introduction_to_ml_kit)开始，你将会使用这些示例将机器学习模型实现到移动应用程序中。但首先，让我们探索构建一些更多模型的过程，看看这一切是如何运作的。在[第二章](ch02.html#introduction_to_computer_vision)中，我们将研究构建一些更复杂的计算机视觉模型！
