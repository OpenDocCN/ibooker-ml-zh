- en: Chapter 6\. Reproducibility Design Patterns
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 6 章。可重现性设计模式
- en: 'Software best practices such as unit testing assume that if we run a piece
    of code, it produces deterministic output:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 软件最佳实践，如单元测试，假设如果我们运行一段代码，它会产生确定性输出：
- en: '[PRE0]'
  id: totrans-2
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This sort of reproducibility is difficult in machine learning. During training,
    machine learning models are initialized with random values and then adjusted based
    on training data. A simple k-means algorithm implemented by scikit-learn requires
    setting the `random_state` in order to ensure the algorithm returns the same results
    each time:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，这种可重复性是困难的。在训练过程中，机器学习模型使用随机值初始化，然后根据训练数据进行调整。例如，由 scikit-learn 实施的简单
    k-means 算法需要设置 `random_state` 以确保算法每次返回相同结果：
- en: '[PRE1]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Beyond the random seed, there are many other artifacts that need to be fixed
    in order to ensure reproducibility during training. In addition, machine learning
    consists of different stages, such as training, deployment, and retraining. It
    is often important that some things are reproducible across these stages as well.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 除了随机种子外，还有许多其他需要修复的工件，以确保在训练期间的可重复性。此外，机器学习包括不同阶段，如训练、部署和重新训练。重要的是，在这些阶段之间某些事物的可重复性通常也很重要。
- en: In this chapter, we’ll look at design patterns that address different aspects
    of reproducibility. The *Transform* design pattern captures data preparation dependencies
    from the model training pipeline to reproduce them during serving. *Repeatable
    Splitting* captures the way data is split among training, validation, and test
    datasets to ensure that a training example that is used in training is never used
    for evaluation or testing even as the dataset grows. The *Bridged Schema* design
    pattern looks at how to ensure reproducibility when the training dataset is a
    hybrid of data conforming to different schema. The *Workflow Pipeline* design
    pattern captures all the steps in the machine learning process to ensure that
    as the model is retrained, parts of the pipeline can be reused. The *Feature Store*
    design pattern addresses reproducibility and reusability of features across different
    machine learning jobs. The *Windowed Inference* design pattern ensures that features
    that are calculated in a dynamic, time-dependent way can be correctly repeated
    between training and serving. *Versioning* of data and models is a prerequisite
    to handle many of the design patterns in this chapter.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到处理可重现性不同方面的设计模式。*Transform* 设计模式从模型训练流程中捕获数据准备依赖项，以在服务过程中重现它们。*Repeatable
    Splitting* 捕获数据在训练、验证和测试数据集之间的拆分方式，以确保训练中使用的训练样本不会在评估或测试中使用，即使数据集增长。*Bridged Schema*
    设计模式考虑了在训练数据集是符合不同架构的数据混合时如何确保可重复性。*Workflow Pipeline* 设计模式捕获机器学习过程中的所有步骤，以确保在重新训练模型时可以重复使用流水线的部分。*Feature
    Store* 设计模式解决了在不同机器学习作业中特征的可重复性和可重用性。*Windowed Inference* 设计模式确保以动态、时间依赖方式计算的特征在训练和服务之间能够正确重复。数据和模型的*版本控制*
    是本章中处理许多设计模式的先决条件。
- en: 'Design Pattern 21: Transform'
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '设计模式 21: Transform'
- en: The Transform design pattern makes moving an ML model to production much easier
    by keeping inputs, features, and transforms carefully separate.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Transform 设计模式通过精心分离输入、特征和转换，使将 ML 模型移至生产环境变得更加容易。
- en: Problem
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: 'The problem is that the *inputs* to a machine learning model are not the *features*
    that the machine learning model uses in its computations. In a text classification
    model, for example, the inputs are the raw text documents and the features are
    the numerical embedding representations of this text. When we train a machine
    learning model, we train it with features that are extracted from the raw inputs.
    Take this model that is trained to predict the duration of bicycle rides in London
    using BigQuery ML:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于机器学习模型的*输入*不是其在计算中使用的*特征*。例如，在文本分类模型中，输入是原始文本文档，而特征是该文本的数值嵌入表示。当我们训练一个机器学习模型时，我们使用从原始输入中提取出的特征进行训练。以
    BigQuery ML 预测伦敦自行车骑行时长的模型为例：
- en: '[PRE2]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This model has three features (`start_station_name`, `dayofweek`, and `hourofday`)
    computed from two inputs, `start_station_name` and `start_date`, as shown in [Figure 6-1](ch06_split_000.xhtml#the_model_has_three_features_computed_f).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型有三个特征（`start_station_name`、`dayofweek` 和 `hourofday`），这些特征是从两个输入 `start_station_name`
    和 `start_date` 计算得出的，如 [Figure 6-1](ch06_split_000.xhtml#the_model_has_three_features_computed_f)
    所示。
- en: '![The model has three features computed from two inputs.](Images/mldp_0601.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![该模型从两个输入计算出三个特征。](Images/mldp_0601.png)'
- en: Figure 6-1\. The model has three features computed from two inputs.
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1。该模型从两个输入计算出三个特征。
- en: 'But the SQL code above mixes up the inputs and features and doesn’t keep track
    of the transformations that were carried out. This comes back to bite when we
    try to predict with this model. Because the model was trained on three features,
    this is what the prediction signature has to look like:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 但上述 SQL 代码混淆了输入和特征，并且没有跟踪执行的转换。当我们尝试使用此模型进行预测时，这会对我们造成困扰。因为模型是基于三个特征进行训练的，这就是预测签名应该看起来的样子：
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that, at inference time, we have to know what features the model was trained
    on, how they should be interpreted, and the details of the transformations that
    were applied. We have to know that we need to send in `'3'` for dayofweek. That
    `'3'` …is that Tuesday or Wednesday? Depends on which library was used by the
    model, or what we consider the start of a week!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在推断时，我们必须知道模型训练的特征是什么，它们应该如何解释以及应用的转换的细节。我们必须知道我们需要发送 `'3'` 作为星期几。这个 `'3'`
    是星期二还是星期三？这取决于模型使用的库或我们认为一周的开始是什么！
- en: '*Training-serving skew*, caused by differences in any of these factors between
    the training and serving environments, is one of the key reasons why productionization
    of ML models is so hard.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*训练与服务之间的偏差*，由于这些因素在训练和服务环境之间的差异而引起，是 ML 模型投产困难的主要原因之一。'
- en: Solution
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: The solution is to explicitly capture the transformations applied to convert
    the model inputs into features. In BigQuery ML, this is done using the `TRANSFORM`
    clause. Using `TRANSFORM` ensures that these transformations are automatically
    applied during `ML.PREDICT`.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是明确捕捉应用于将模型输入转换为特征的转换。在 BigQuery ML 中，可以通过使用 `TRANSFORM` 子句来实现这一点。使用 `TRANSFORM`
    确保在 `ML.PREDICT` 期间自动应用这些转换。
- en: 'Given the support for `TRANSFORM`, the model above should be rewritten as:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于对 `TRANSFORM` 的支持，上述模型应进行如下重写：
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Notice how we have clearly separated out the inputs (in the `SELECT` clause)
    from the features (in the `TRANSFORM` clause). Now, prediction is much easier.
    We can simply send to the model the station name and a timestamp (the inputs):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们如何明确将输入（在 `SELECT` 子句中）与特征（在 `TRANSFORM` 子句中）分离开来。现在，预测变得更加容易。我们只需向模型发送站点名称和时间戳（即输入）即可：
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The model will then take care of carrying out the appropriate transformations
    to create the necessary features. It does so by capturing both the transformation
    logic and artifacts (such as scaling constants, embedding coefficients, lookup
    tables, and so on) to carry out the transformation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然后模型会负责执行适当的转换以创建必要的特征。它通过捕捉转换逻辑和工件（如缩放常数、嵌入系数、查找表等）来执行转换。
- en: As long as we carefully use only the raw inputs in the `SELECT` statement and
    put all subsequent processing of the input in the `TRANSFORM` clause, BigQuery
    ML will automatically apply these transformations during prediction.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 只要我们在 `SELECT` 语句中仔细使用原始输入并将所有后续处理放在 `TRANSFORM` 子句中，BigQuery ML 将在预测期间自动应用这些转换。
- en: Trade-Offs and Alternatives
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 折衷与替代方案
- en: The solution described above works because BigQuery ML keeps track of the transformation
    logic and artifacts for us, saves them in the model graph, and automatically applies
    the transformations during prediction.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 上述描述的解决方案之所以有效，是因为 BigQuery ML 为我们跟踪转换逻辑和工件，将它们保存在模型图中，并在预测期间自动应用这些转换。
- en: 'If we are using a framework where support for the Transform design pattern
    is not built in, we should design our model architecture in such a way that the
    transformations carried out during training are easy to reproduce during serving.
    We can do this by making sure to save the transformations in the model graph or
    by creating a repository of transformed features ([“Design Pattern 26: Feature
    Store”](ch06_split_001.xhtml#design_pattern_twosix_feature_store)).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用的框架不支持内置的 Transform 设计模式，我们应该设计我们的模型架构，以便在训练期间执行的转换在服务期间易于重现。我们可以通过确保将转换保存在模型图中或创建转换特征的存储库（[“设计模式26：特征存储”](ch06_split_001.xhtml#design_pattern_twosix_feature_store)）来实现这一点。
- en: Transformations in TensorFlow and Keras
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow 和 Keras 中的转换
- en: Assume that we are training an ML model to estimate taxi fare in New York and
    have six inputs (pickup latitude, pickup longitude, dropoff latitude, dropoff
    longitude, passenger count, and pickup time). TensorFlow supports the concept
    of feature columns, which are saved in the model graph. However, the API is designed
    assuming that the raw inputs are the same as the features.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在训练一个ML模型来估算纽约的出租车费用，并且有六个输入（上车纬度、上车经度、下车纬度、下车经度、乘客数量和上车时间）。TensorFlow支持特征列的概念，这些特征列保存在模型图中。但是，API设计时假设原始输入与特征相同。
- en: Let’s say that we want to scale the latitudes and longitudes (see [“Simple Data
    Representations”](ch02.xhtml#simple_data_representations) in [Chapter 2](ch02.xhtml#data_representation_design_patterns)
    for details), create a transformed feature that is the Euclidean distance, and
    extract the hour of day from the timestamp. We have to carefully design the model
    graph (see [Figure 6-2](ch06_split_000.xhtml#the_model_graph_for_the_taxi_fare_estim)),
    keeping the Transform concept firmly in mind. As we walk through the code below,
    notice how we set things up so that we clearly design three separate layers in
    our Keras model—the Inputs layer, the Transform layer, and a `DenseFeatures` layer.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要缩放纬度和经度（详见[“简单的数据表示”](ch02.xhtml#simple_data_representations)第[第二章](ch02.xhtml#data_representation_design_patterns)），创建一个转换后的特征作为欧几里德距离，并从时间戳中提取小时数。我们必须仔细设计模型图（参见[图 6-2](ch06_split_000.xhtml#the_model_graph_for_the_taxi_fare_estim)），牢记“转换”概念。当我们逐步分析下面的代码时，请注意我们如何设置事物，以便清晰地设计我们Keras模型中的三个单独层——输入层、转换层和`DenseFeatures`层。
- en: '![The model graph for the taxi fare estimation problem in Keras.](Images/mldp_0602.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![Keras中出租车费用估算问题的模型图。](Images/mldp_0602.png)'
- en: Figure 6-2\. The model graph for the taxi fare estimation problem in Keras.
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-2\. Keras中出租车费用估算问题的模型图。
- en: 'First, make every input to the Keras model an `Input` layer (the full code
    is in a notebook on [GitHub](https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/quests/serverlessml/06_feateng_keras/solution/taxifare_fc.ipynb)):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，将Keras模型的每个输入都作为一个`Input`层（完整代码在[GitHub](https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/quests/serverlessml/06_feateng_keras/solution/taxifare_fc.ipynb)的笔记本中）：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In [Figure 6-2](ch06_split_000.xhtml#the_model_graph_for_the_taxi_fare_estim),
    these are the boxes marked `dropoff_latitude`, `dropoff_longitude`, and so on.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 6-2](ch06_split_000.xhtml#the_model_graph_for_the_taxi_fare_estim)中，这些方框标记为`dropoff_latitude`、`dropoff_longitude`等。
- en: 'Second, maintain a dictionary of transformed features, and make every transformation
    either a Keras Preprocessing layer or a `Lambda` layer. Here, we scale the inputs
    using `Lambda` layers:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，维护一个转换特征的字典，并使每个转换成为Keras预处理层或`Lambda`层。在这里，我们使用`Lambda`层缩放输入：
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In [Figure 6-2](ch06_split_000.xhtml#the_model_graph_for_the_taxi_fare_estim),
    these are the boxes marked `scale_dropoff_latitude`, `scale_dropoff_longitude`,
    and so on.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 6-2](ch06_split_000.xhtml#the_model_graph_for_the_taxi_fare_estim)中，这些方框标记为`scale_dropoff_latitude`、`scale_dropoff_longitude`等。
- en: 'We will also have one `Lambda` layer for the Euclidean distance, which is computed
    from four of the `Input` layers (see [Figure 6-2](ch06_split_000.xhtml#the_model_graph_for_the_taxi_fare_estim)):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将有一个`Lambda`层来计算从四个`Input`层计算出的欧几里德距离（详见[图 6-2](ch06_split_000.xhtml#the_model_graph_for_the_taxi_fare_estim)）：
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Similarly, the column to create the hour of day from the timestamp is a `Lambda`
    layer:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，从时间戳创建小时数的列是一个`Lambda`层：
- en: '[PRE9]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Third, all these transformed layers will be concatenated into a `DenseFeatures`
    layer:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，所有这些转换层将被串联成一个`DenseFeatures`层：
- en: '[PRE10]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Because the constructor for `DenseFeatures` requires a set of feature columns,
    we will have to specify how to take each of the transformed values and convert
    them into an input to the neural network. We might use them as is, one-hot encode
    them, or choose to bucketize the numbers. For simplicity, let’s just use them
    all as is:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因为`DenseFeatures`的构造函数需要一组特征列，我们将不得不指定如何将每个转换后的值转换为神经网络的输入。我们可以原样使用它们、对它们进行独热编码，或者选择对数字进行分桶。为简单起见，让我们仅仅使用它们原样：
- en: '[PRE11]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Once we have a `DenseFeatures` input layer, we can build the rest of our Keras
    model as usual:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了`DenseFeatures`输入层，我们可以像往常一样构建剩余的Keras模型：
- en: '[PRE12]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The [complete example](https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/quests/serverlessml/06_feateng_keras/solution/taxifare_fc.ipynb)
    is on GitHub.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 完整示例在GitHub上的[这里](https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/quests/serverlessml/06_feateng_keras/solution/taxifare_fc.ipynb)。
- en: Notice how we set things up so that the first layer of the Keras model was `Inputs`.
    The second layer was the `Transform` layer. The third layer was the `DenseFeatures`
    layer that combined them. After this sequence of layers, the usual model architecture
    starts. Because the `Transform` layer is part of the model graph, the usual Serving
    Function and Batch Serving solutions (see [Chapter 5](ch05.xhtml#design_patterns_for_resilient_serving))
    will work as is.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们如何设置事物，使得Keras模型的第一层是`Inputs`。第二层是`Transform`层。第三层是`DenseFeatures`层，将它们组合在一起。在这些层序列之后，通常的模型架构开始。由于`Transform`层是模型图的一部分，通常的Serving功能和批量Serving解决方案（参见[第5章](ch05.xhtml#design_patterns_for_resilient_serving)）将如原样工作。
- en: Efficient transformations with tf.transform
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用`tf.transform`进行高效的转换
- en: One drawback to the above approach is that the transformations will be carried
    out during each iteration of training. This is not such a big deal if all we are
    doing is scaling by known constants. But what if our transformations are more
    computationally expensive? What if we want to scale using the mean and variance,
    in which case, we need to pass through all the data first to compute these variables?
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法的一个缺点是转换将在每次训练迭代期间执行。如果我们只是通过已知常数进行缩放，这并不是什么大问题。但是如果我们的转换计算更昂贵呢？如果我们想使用均值和方差进行缩放，那么我们需要首先通过所有数据来计算这些变量。
- en: Tip
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: It is helpful to differentiate between *instance-level* transformations that
    can be part of the model directly (where the only drawback is applying them on
    each training iteration) and *dataset-level* transformations, where we need a
    full pass to compute overall statistics or the vocabulary of a categorical variable.
    Such dataset-level transformations cannot be part of the model and have to be
    applied as a scalable preprocessing step, which produces the Transform, capturing
    the logic and the artifacts (mean, variance, vocabulary, and so on) to be attached
    to the model. For dataset-level transformations, use `tf.transform`.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 区分*实例级*变换和可以直接成为模型一部分的变换（其唯一缺点是在每次训练迭代时应用它们）以及*数据集级*变换是有帮助的。对于数据集级变换，我们需要完整地遍历数据以计算整体统计信息或分类变量的词汇表。这样的数据集级变换不能成为模型的一部分，必须作为可扩展的预处理步骤应用，这样就产生了Transform，捕捉了逻辑和制品（均值、方差、词汇等），以便附加到模型上。对于数据集级变换，请使用`tf.transform`。
- en: The `tf.transform` library (which is part of [TensorFlow Extended](https://oreil.ly/OznI3))
    provides an efficient way of carrying out transformations over a preprocessing
    pass through the data and saving the resulting features and transformation artifacts
    so that the transformations can be applied by TensorFlow Serving during prediction
    time.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.transform`库（作为[TensorFlow Extended](https://oreil.ly/OznI3)的一部分）提供了一种高效的方式，在数据的预处理传递中执行变换并保存生成的特征和转换制品，以便在预测时由TensorFlow
    Serving应用这些变换。'
- en: 'The first step is to define the transformation function. For example, to scale
    all the inputs to be zero mean and unit variance and bucketize them, we would
    create this preprocessing function (see the [full code](https://github.com/tensorflow/tfx/blob/master/tfx/examples/chicago_taxi_pipeline/taxi_utils_native_keras.py)
    on GitHub):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是定义转换函数。例如，要将所有输入缩放为零均值和单位方差并对其进行分桶，我们将创建此预处理函数（在GitHub上查看[完整代码](https://github.com/tensorflow/tfx/blob/master/tfx/examples/chicago_taxi_pipeline/taxi_utils_native_keras.py)）：
- en: '[PRE13]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Before training, the raw data is read and transformed using the prior function
    in Apache Beam:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练之前，使用Apache Beam中的先前函数读取并转换原始数据：
- en: '[PRE14]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The transformed data is then written out in a format suitable for reading by
    the training pipeline:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将转换后的数据写入适合训练流水线读取的格式：
- en: '[PRE15]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The Beam pipeline also stores the preprocessing function that needs to be run,
    along with any artifacts the function needs, into an artifact in TensorFlow graph
    format. In the case above, for example, this artifact would include the mean and
    variance for scaling the numbers, and the bucket boundaries for bucketizing numbers.
    The training function reads transformed data and, therefore, the transformations
    do not have to be repeated within the training loop.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Beam流水线还将预处理函数以及函数所需的任何制品存储为TensorFlow图格式的工件。例如，上述情况下，此工件将包括用于缩放数字的均值和方差，以及用于分桶数字的桶边界。训练函数读取转换后的数据，因此在训练循环内不必重复转换。
- en: 'The serving function needs to load in these artifacts and create a Transform
    layer:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 服务函数需要加载这些制品并创建一个Transform层：
- en: '[PRE16]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, the serving function can apply the Transform layer to the parsed input
    features and invoke the model with the transformed data to calculate the model
    output:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，服务功能可以将Transform层应用于解析的输入特征，并使用转换后的数据调用模型以计算模型输出：
- en: '[PRE17]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In this way, we are making sure to insert the transformations into the model
    graph for serving. At the same time, because the model training happens on the
    transformed data, our training loop does not have to carry out these transformations
    during each epoch.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们确保将转换插入模型图中以供服务。同时，因为模型训练是在转换后的数据上进行的，所以我们的训练循环不必在每个时期执行这些转换操作。
- en: Text and image transformations
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本和图像转换
- en: In text models, it is common to preprocess input text (such as to remove punctuation,
    stop words, capitalization, stemming, and so on) before providing the cleaned
    text as a feature to the model. Other common feature engineering on text inputs
    includes tokenization and regular expression matching. It is essential that the
    same cleanup or extraction steps be carried out at inference time.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本模型中，通常会对输入文本进行预处理（例如去除标点符号、停用词、大写、词干提取等），然后将清理后的文本作为特征提供给模型。其他常见的文本输入特征工程包括标记化和正则表达式匹配。在推理时，执行相同的清理或提取步骤至关重要。
- en: The need to capture transformations is important even if there is no explicit
    feature engineering as when using deep learning with images. Image models usually
    have an Input layer that accepts images of a specific size. Image inputs of a
    different size will have to be cropped, padded, or resampled to this fixed size
    before being fed into the model. Other common transformations in image models
    include color manipulations (gamma correction, grayscale conversion, and so on)
    and orientation correction. It is essential that such transformations be identical
    between what was carried out on the training dataset and what will be carried
    out during inference. The Transform pattern helps ensure this reproducibility.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在使用深度学习处理图像时，即使没有明确的特征工程，捕捉转换的需求也是重要的。图像模型通常具有一个接受特定大小图像的输入层。大小不同的图像必须在输入模型之前进行裁剪、填充或重新采样到固定大小。图像模型中其他常见的转换包括颜色操作（伽马校正、灰度转换等）和方向校正。这些转换在训练数据集和推理过程中必须保持一致。Transform模式有助于确保这种可重现性。
- en: With image models, there are some transformations (such as data augmentation
    by random cropping and zooming) that are applied only during training. These transformations
    do not need to be captured during inference. Such transformations will not be
    part of the Transform pattern.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像模型，有一些转换（如通过随机裁剪和缩放进行数据增强）仅在训练过程中应用。这些转换不需要在推理过程中捕获。这些转换不会成为Transform模式的一部分。
- en: Alternate pattern approaches
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 替代模式方法
- en: An alternative approach to solving the training-serving skew problem is to employ
    the Feature Store pattern. The feature store comprises a coordinated computation
    engine and repository of transformed feature data. The computation engine supports
    low-latency access for inference and batch creation of transformed features while
    the data repository provides quick access to transformed features for model training.
    The advantage of a feature store is there is no requirement for the transformation
    operations to fit into the model graph. For example, as long as the feature store
    supports Java, the preprocessing operations could be carried out in Java while
    the model itself could be written in PyTorch. The disadvantage of a feature store
    is that it makes the model dependent on the feature store and makes the serving
    infrastructure much more complex.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 解决训练-服务偏差问题的另一种方法是采用特征存储模式。特征存储包括协调计算引擎和转换特征数据的存储库。计算引擎支持推理的低延迟访问和批量创建转换特征，而数据存储库提供快速访问转换特征以进行模型训练。特征存储的优点是不需要转换操作符合模型图。例如，只要特征存储支持Java，预处理操作可以在Java中执行，而模型本身可以用PyTorch编写。特征存储的缺点是使模型依赖于特征存储，并使服务基础设施变得更加复杂。
- en: 'Another way to separate out the programming language and framework used for
    transformation of the features from the language used to write the model is to
    carry out the preprocessing in containers and use these custom containers as part
    of both the training and serving. This is discussed in [“Design Pattern 25: Workflow
    Pipeline”](ch06_split_000.xhtml#design_pattern_twofive_workflow_pipelin) and is
    adopted in practice by Kubeflow Serving.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种将特征转换的编程语言和框架与编写模型的语言分开的方法是在容器中进行预处理，并将这些自定义容器作为训练和服务的一部分。这在[“设计模式 25：工作流程管道”](ch06_split_000.xhtml#design_pattern_twofive_workflow_pipelin)中有所讨论，并且被Kubeflow
    Serving实践采纳。
- en: 'Design Pattern 22: Repeatable Splitting'
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计模式 22：可重复的分割
- en: To ensure that sampling is repeatable and reproducible, it is necessary to use
    a well-distributed column and a deterministic hash function to split the available
    data into training, validation, and test datasets.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保抽样是可重复和可再现的，必须使用一个分布良好的列和确定性哈希函数来将可用数据分割为训练、验证和测试数据集。
- en: Problem
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: 'Many machine learning tutorials will suggest splitting data randomly into training,
    validation, and test datasets using code similar to the following:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习教程会建议使用类似以下代码将数据随机分割为训练、验证和测试数据集：
- en: '[PRE18]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Unfortunately, this approach fails in many real-world situations. The reason
    is that it is rare that the rows are independent. For example, if we are training
    a model to predict flight delays, the arrival delays of flights on the same day
    will be highly correlated with one another. This leads to leakage of information
    between the training and testing dataset when some of the flights on any particular
    day are in the training dataset and some other flights on the same day are in
    the testing dataset. This leakage due to correlated rows is a frequently occurring
    problem, and one that we have to avoid when doing machine learning.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这种方法在许多实际情况下失败了。原因在于很少有行是独立的。例如，如果我们正在训练一个模型来预测航班延误，同一天的航班到达延误将高度相关。这会导致在训练数据集中有些航班在任何特定日期上的某些航班和测试数据集中有些其他航班之间泄漏信息。由于相关行导致的这种泄漏是一个经常发生的问题，我们在进行机器学习时必须避免。
- en: In addition, the `rand` function orders data differently each time it is run,
    so if we run the program again, we will get a different 80% of rows. This can
    play havoc if we are experimenting with different machine learning models with
    the goal of choosing the best one—we need to compare the model performance on
    the same test dataset. In order to address this, we need to set the random seed
    in advance or store the data after it is split. Hardcoding how the data is to
    be split is not a good idea because, when carrying out techniques like jackknifing,
    bootstrapping, cross-validation, and hyperparameter tuning, we will need to change
    this data split and do so in a way that allows us to do individual trials.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，`rand`函数每次运行时都会以不同的顺序排序数据，因此如果我们再次运行程序，将获得不同的80%行。如果我们正在尝试不同的机器学习模型，并希望选择最佳模型，这可能会带来麻烦——我们需要在相同的测试数据集上比较模型性能。为了解决这个问题，我们需要预先设置随机种子或在分割数据后存储数据。硬编码数据分割方式并不是一个好主意，因为在执行像自助法、交叉验证和超参数调整等技术时，我们需要改变这些数据分割方式，并且以允许我们进行单独试验的方式进行。
- en: For machine learning, we want lightweight, repeatable splitting of the data
    that works regardless of programming language or random seeds. We also want to
    ensure that correlated rows fall into the same split. For example, we do not want
    flights on January 2, 2019 in the test dataset if flights on that day are in the
    training dataset.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习，我们希望轻量级、可重复的数据分割，不受编程语言或随机种子的影响。我们还希望确保相关行进入同一分割中。例如，如果在训练数据集中有2019年1月2日的航班，我们不希望测试数据集中也有这一天的航班。
- en: Solution
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: First, we identify a column that captures the correlation relationship between
    rows. In our airline delay dataset, this is the `date` column. Then, we use the
    last few digits of a hash function on that column to split the data. For the airline
    delay problem, we can use the Farm Fingerprint hashing algorithm on the `date`
    column to split the available data into training, validation, and testing datasets.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要确定一个列，该列捕获行之间的相关关系。在我们的航空延误数据集中，这是`date`列。然后，我们使用哈希函数在该列上的最后几位数字来分割数据。对于航空延误问题，我们可以在`date`列上使用Farm
    Fingerprint哈希算法来将可用数据分割为训练、验证和测试数据集。
- en: Tip
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'For more on the Farm Fingerprint algorithm, support for other frameworks and
    languages, and the relationship between hashing and cryptography, please see [“Design
    Pattern 1: Hashed Feature”](ch02.xhtml#design_pattern_one_hashed_feature) in [Chapter 2](ch02.xhtml#data_representation_design_patterns).
    In particular, open source wrappers of the [Farm Hash](https://github.com/google/farmhash)
    algorithm are available in a number of languages ([including Python](https://oreil.ly/526Dc)),
    and so this pattern can be applied even if data is not in a data warehouse that
    supports a repeatable hash out of the box.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Farm Fingerprint算法的更多信息、其他框架和语言的支持以及哈希与加密的关系，请参阅[“设计模式1：哈希特征”](ch02.xhtml#design_pattern_one_hashed_feature)中的[第2章](ch02.xhtml#data_representation_design_patterns)。特别地，Farm
    Hash算法的开源封装可在多种语言中使用（[包括Python](https://oreil.ly/526Dc)），因此即使数据不在原生支持可重复哈希的数据仓库中，也可以应用此模式。
- en: 'Here is how to split the dataset based on the hash of the `date` column:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这是根据`date`列的哈希如何分割数据集的方法：
- en: '[PRE19]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: To split on the `date` column, we compute its hash using the `FARM_FINGERPRINT`
    function and then use the modulo function to find an arbitrary 80% subset of the
    rows. This is now repeatable—because the `FARM_FINGERPRINT` function returns the
    same value any time it is invoked on a specific date, we can be sure we will get
    the same 80% of data each time. As a result, all the flights on any given date
    will belong to the same split—train, validation, or test. This is repeatable regardless
    of the random seed.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 要根据`date`列进行分割，我们使用`FARM_FINGERPRINT`函数计算其哈希，然后使用模函数找出80%的行的任意子集。这样做是可重复的——因为`FARM_FINGERPRINT`函数在特定日期上每次调用时返回相同的值，我们可以确保每次都会得到相同的80%数据。因此，同一天的所有航班将属于同一分割——训练集、验证集或测试集。无论随机种子如何，这都是可重复的。
- en: If we want to split our data by `arrival_airport` (so that 80% of airports are
    in the training dataset, perhaps because we are trying to predict something about
    airport amenities), we would compute the hash on `arrival_airport` instead of
    `date`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想按`arrival_airport`分割数据集（例如因为我们试图预测机场设施的某些内容），我们将在`arrival_airport`上计算哈希，而不是在`date`上。
- en: 'It is also straightforward to get the validation data: change the `< 8` in
    the query above to `=8`, and for testing data, change it to `=9`. This way, we
    get 10% of samples in validation and 10% in testing.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 获取验证数据也很简单：将上述查询中的`< 8`更改为`=8`，对于测试数据，则更改为`=9`。这样，我们可以在验证集中获得10%的样本，在测试集中获得10%的样本。
- en: 'What are the considerations for choosing the column to split on? The `date`
    column has to have several characteristics for us to be able to use it as the
    splitting column:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 选择分割列的考虑因素是什么？`date`列必须具有多个特征，我们才能将其用作分割列：
- en: Rows at the same date tend to be correlated—again, this is the key reason why
    we want to ensure that all rows on the same date are in the same split.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相同日期的行往往具有相关性——这正是我们希望确保所有相同日期的行都在同一分割中的关键原因。
- en: '`date` is not an input to the model even though it is used as a criteria for
    splitting. Features extracted from `date` such as day of week or hour of day can
    be inputs, but we can’t use an actual input as the field with which to split because
    the trained model will not have seen 20% of the possible input values for the
    `date` column if we use 80% of the data for training.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`date`不是模型的输入，尽管它用作分割的标准。从`date`提取的特征，如星期几或每天的小时数，可以作为输入，但我们不能使用实际输入作为分割字段，因为训练过的模型将无法看到80%数据集中`date`列的所有可能输入值。'
- en: There have to be enough `date` values. Since we are computing the hash and finding
    the modulo with respect to 10, we need at least 10 unique hash values. The more
    unique values we have, the better. To be safe, a rule of thumb is to shoot for
    3–5× the denominator for the modulo, so in this case, we want 40 or so unique
    dates.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须有足够的`date`数值。因为我们正在计算哈希并对10取模，所以我们至少需要10个唯一的哈希数值。拥有更多唯一数值会更好。为了保险起见，一个经验法则是以模数的3-5倍为目标，因此在这种情况下，我们需要大约40个唯一日期。
- en: The label has to be well distributed among the dates. If it turns out that all
    the delays happened on January 1 and there were no delays the rest of the year,
    this wouldn’t work since the split datasets will be skewed. To be safe, look at
    a graph and make sure that all three splits have a similar distribution of labels.
    To be extra safe, ensure that the distributions of label by departure delay and
    other input values are similar across the three datasets.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签必须在日期之间良好分布。如果所有延误都发生在1月1日，并且全年没有延误，那么这种方法将不起作用，因为分割的数据集将会倾斜。为了安全起见，请查看图表，确保三个分割数据集的标签分布相似。为了更安全，请确保通过出发延误和其他输入值的标签分布在三个数据集中也相似。
- en: Tip
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'We can automate checking whether the label distributions are similar across
    the three datasets by using the Kolomogorov–Smirnov test: just plot the cumulative
    distribution functions of the label in the three datasets and find the maximum
    distance between each pair. The smaller the maximum distance, the better the split.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用Kolomogorov–Smirnov测试自动化检查标签在三个数据集中的分布是否相似：只需绘制标签在三个数据集中的累积分布函数，并找出每对之间的最大距离。最大距离越小，分割效果越好。
- en: Trade-Offs and Alternatives
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 折衷和替代方案
- en: Let’s look at a couple of variants of how we might do repeatable splitting and
    discuss the pros and cons of each. Let’s also examine how to extend this idea
    to do repeatable sampling, not just splitting.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下我们可能会如何进行可重复分割的几个变体，并讨论每种方法的利弊。让我们还讨论如何将这个想法扩展到不仅是分割而且是可重复抽样。
- en: Single query
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单一查询
- en: 'We don’t need three separate queries to generate training, validation, and
    test splits. We can do it in a single query as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要三个单独的查询来生成训练、验证和测试分割。我们可以在一个查询中完成如下操作：
- en: '[PRE20]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We can then use the `split_col` column to decide which of three datasets any
    particular row falls in. Using a single query decreases computational time but
    requires creating a new table or modifying the source table to add the extra `split_col`
    column.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以使用`split_col`列来确定每一行属于哪个数据集。使用单一查询减少了计算时间，但需要创建新表或修改源表以添加额外的`split_col`列。
- en: Random split
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机分割
- en: 'What if the rows are not correlated? In that case, we want a random, repeatable
    split but do not have a natural column to split by. We can hash the entire row
    of data by converting it to a string and hashing that string:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果行之间没有相关性怎么办？在这种情况下，我们希望进行随机的、可重复的分割，但没有自然的列可以进行分割。我们可以通过将整行数据转换为字符串并对该字符串进行哈希来对数据进行哈希化：
- en: '[PRE21]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note that if we have duplicate rows, they will always end up in the same split.
    This might be exactly what we desire. If not, then we will have to add a unique
    ID column to the `SELECT` query.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果我们有重复的行，它们将始终出现在相同的分割中。这可能正是我们所期望的。如果不是，则必须在`SELECT`查询中添加唯一ID列。
- en: Split on multiple columns
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于多列的分割
- en: 'We have talked about a single column that captures the correlation between
    rows. What if it is a combination of columns that capture when two rows are correlated?
    In such cases, simply concatenate the fields (this is a feature cross) before
    computing the hash. For example, suppose we only wish to ensure that flights from
    the same airport on the same day do not show up in different splits. In that case,
    we’d do the following:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了一个捕捉行之间相关性的单一列。如果是一组捕捉两行何时相关的列呢？在这种情况下，只需在计算哈希之前简单地连接字段（这是特征交叉）。例如，假设我们只希望确保同一天从同一机场出发的航班不会出现在不同的分割中。在这种情况下，我们将执行以下操作：
- en: '[PRE22]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: If we split on a feature cross of multiple columns, we *can* use `arrival_airport`
    as one of the inputs to the model, since there will be examples of any particular
    airport in both the training and test sets. On the other hand, if we had split
    only on `arrival_airport`, then the training and test sets will have a mutually
    exclusive set of arrival airports and, therefore, `arrival_airport` cannot be
    an input to the model.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们根据多列的特征交叉进行分割，我们可以将`arrival_airport`作为模型的输入之一，因为训练集和测试集中会有任何特定机场的示例。另一方面，如果我们仅根据`arrival_airport`进行分割，则训练集和测试集将具有互斥的到达机场集合，因此`arrival_airport`不能作为模型的输入。
- en: Repeatable sampling
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可重复抽样
- en: The basic solution is good if we want 80% of the entire dataset as training,
    but what if we want to play around with a smaller dataset than what we have in
    BigQuery? This is common for local development. The flights dataset is 70 million
    rows, and perhaps what we want is a smaller dataset of one million flights. How
    would we pick 1 in 70 flights, and then 80% of those as training?
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要 80% 的整个数据集作为训练，那么基本解决方案就很好了，但是如果我们想要使用比我们在 BigQuery 中拥有的较小的数据集进行玩耍，那怎么办？这在本地开发中很常见。航班数据集有
    70 百万行，也许我们想要的是一百万次航班的较小数据集。我们如何选择 70 次航班中的一次，然后选择其中的 80% 作为训练？
- en: 'What we can*not* do is something along the lines of:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能做的是：
- en: '[PRE23]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We cannot pick 1 in 70 rows and then pick 8 in 10\. If we are picking numbers
    that are divisible by 70, of course they are also going to be divisible by 10!
    That second modulo operation is useless.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能选择 70 行中的 1 行，然后选择 10 行中的 8 行。如果我们选择可以被 70 整除的数字，当然它们也会被 10 整除！那第二个取模操作是无用的。
- en: 'Here’s a better solution:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个更好的解决方案：
- en: '[PRE24]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In this query, the 700 is 70*10 and 560 is 70*8\. The first modulo operation
    picks 1 in 70 rows and the second modulo operation picks 8 in 10 of those rows.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个查询中，700 是 70*10，560 是 70*8\. 第一个取模操作在 70 行中选择了 1，第二个取模操作在这些行中的 10 个中选择了 8。
- en: 'For validation data, you’d replace `< 560` by the appropriate range:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 对于验证数据，你可以通过适当的范围替换 `< 560`：
- en: '[PRE25]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In the preceding code, our one million flights come from only 1/70th of the
    days in the dataset. This may be precisely what we want—for example, we may be
    modeling the full spectrum of flights on a particular day when experimenting with
    the smaller dataset. However, if what we want is 1/70th of the flights on any
    particular day, we’d have to use `RAND()` and save the result as a new table for
    repeatability. From this smaller table, we can sample 80% of dates using `FARM_FINGERPRINT()`.
    Because this new table is only one million rows and only for experimentation,
    the duplication may be acceptable.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们的一百万次航班来自数据集中仅有的 1/70。这可能正是我们想要的——例如，当我们使用较小的数据集进行实验时，可能是在特定一天上建模所有航班的全谱。然而，如果我们想要的是任何特定一天的航班的
    1/70，我们将不得不使用`RAND()`并将结果保存为新表以实现可重复性。从这个较小的表中，我们可以使用`FARM_FINGERPRINT()`来抽取 80%
    的日期。因为这个新表只有一百万行，而且只用于实验，所以重复可能是可以接受的。
- en: Sequential split
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 顺序拆分
- en: 'In the case of time-series models, a common approach is to use sequential splits
    of data. For example, to train a demand forecasting model where we train a model
    on the past 45 days of data to predict demand over the next 14 days, we’d train
    the model ([full code](https://github.com/GoogleCloudPlatform/bigquery-oreilly-book/blob/master/blogs/bqml_arima/bqml_arima.ipynb))
    by pulling the necessary data:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间序列模型的情况下，一种常见的方法是使用数据的顺序拆分。例如，在训练需求预测模型时，我们会使用过去 45 天的数据来预测接下来 14 天的需求，我们会通过获取必要的数据来训练模型（[完整代码](https://github.com/GoogleCloudPlatform/bigquery-oreilly-book/blob/master/blogs/bqml_arima/bqml_arima.ipynb)）：
- en: '[PRE26]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Such a sequential split of data is also necessary in fast-moving environments
    even if the goal is not to predict the future value of a time series. For example,
    in a fraud-detection model, bad actors adapt quickly to the fraud algorithm, and
    the model has to therefore be continually retrained on the latest data to predict
    future fraud. It is not sufficient to generate the evaluation data from a random
    split of the historical dataset because the goal is to predict behavior that the
    bad actors will exhibit in the future. The indirect goal is the same as that of
    a time-series model in that a good model will be able to train on historical data
    and predict future fraud. The data has to be split sequentially in terms of time
    to correctly evaluate this. For example ([full code](https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/blogs/bigquery_datascience/bigquery_tensorflow.ipynb)):'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 即使目标不是预测时间序列的未来值，快速变动的环境中也需要对数据进行顺序拆分。例如，在欺诈检测模型中，坏人会迅速适应欺诈算法，因此模型必须根据最新数据进行持续重新训练以预测未来的欺诈。从历史数据的随机拆分生成评估数据是不够的，因为目标是预测坏人将来会展示的行为。间接目标与时间序列模型的目标相同，一个好的模型将能够根据历史数据进行训练并预测未来的欺诈。数据必须按时间顺序进行拆分以正确评估这一点。例如（[完整代码](https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/blogs/bigquery_datascience/bigquery_tensorflow.ipynb)）：
- en: '[PRE27]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Another instance where a sequential split of data is needed is when there are
    high correlations between successive times. For example, in weather forecasting,
    the weather on consecutive days is highly correlated. Therefore, it is not reasonable
    to put October 12 in the training dataset and October 13 in the testing dataset
    because there will be considerable leakage (imagine, for example, that there is
    a hurricane on October 12). Further, weather is highly seasonal, and so it is
    necessary to have days from all seasons in all three splits. One way to properly
    evaluate the performance of a forecasting model is to use a sequential split but
    take seasonality into account by using the first 20 days of every month in the
    training dataset, the next 5 days in the validation dataset, and the last 5 days
    in the testing dataset.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 当连续时间之间存在高相关性时，需要顺序分割数据的另一个例子是。例如，在天气预报中，连续几天的天气高度相关。因此，将10月12日放入训练数据集，将10月13日放入测试数据集是不合理的，因为会有相当大的泄漏（例如，10月12日有飓风）。此外，天气具有很强的季节性，因此有必要在所有三个分割中都包含来自所有季节的天。评估预测模型性能的一种正确方法是使用顺序分割，但通过在训练数据集中使用每个月的前20天，在验证数据集中使用接下来的5天，在测试数据集中使用最后的5天来考虑季节性。
- en: In all these instances, repeatable splitting requires only that we place the
    logic used to create the split into version control and ensure that the model
    version is updated whenever this logic is changed.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些情况下，可重复分割只需要将用于创建分割的逻辑放入版本控制，并确保每次更改此逻辑时更新模型版本。
- en: Stratified split
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分层分割
- en: The example above of how weather patterns are different between different seasons
    is an example of a situation where the splitting needs to happen after the dataset
    is *stratified*. We needed to ensure that there were examples of all seasons in
    each split, and so we stratified the dataset in terms of months before carrying
    out the split. We used the first 20 days of every month in the training dataset,
    the next 5 days in the validation dataset, and the last 5 days in the testing
    dataset. Had we not been concerned with the correlation between successive days,
    we could have randomly split the dates within each month.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 季节变化在不同季节之间的天气模式差异的示例是分层之后数据集*分层*的一个情况的例子。在进行分割之前，我们需要确保每个分割中都有所有季节的示例，因此我们按月份对数据集进行了分层。在训练数据集中使用每个月的前20天，在验证数据集中使用接下来的5天，在测试数据集中使用最后的5天。如果我们不关心连续天数之间的相关性，我们可以在每个月内随机分割日期。
- en: The larger the dataset, the less concerned we have to be with stratification.
    On very large datasets, the odds are very high that the feature values will be
    well distributed among all the splits. Therefore, in large-scale machine learning,
    the need to stratify happens quite commonly only in the case of skewed datasets.
    For example, in the flights dataset, less than 1% of flights take off before 6
    a.m., and so the number of flights that meet this criterion may be quite small.
    If it is critical for our business use case to get the behavior of these flights
    correct, we should stratify the dataset based on departure hour and split each
    stratification evenly.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集越大，我们对分层的担忧就越少。在非常大的数据集上，特征值在所有分割中的分布可能非常均匀。因此，在大规模机器学习中，只有在数据集倾斜的情况下分层的需求才很常见。例如，在航班数据集中，不到1%的航班在上午6点之前起飞，因此满足这一标准的航班数量可能非常少。如果对于我们的业务用例来说，正确获取这些航班行为非常关键，我们应该根据起飞时间分层数据集，并均匀地分割每个分层。
- en: 'The departure time was an example of a skewed feature. In an imbalanced classification
    problem (such as fraud detection, where the number of fraud examples is quite
    small), we might want to stratify the dataset by the label and split each stratification
    evenly. This is also important if we have a multilabel problem and some of the
    labels are rarer than others. These are discussed in [“Design Pattern 10: Rebalancing
    ”](ch03.xhtml#design_pattern_onezero_rebalancing) in [Chapter 3](ch03.xhtml#problem_representation_design_patterns).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 出发时间是一个倾斜特征的例子。在不平衡分类问题（例如欺诈检测，其中欺诈示例数量相当少）中，我们可能希望根据标签分层数据集，并均匀地分割每个分层。如果我们有一个多标签问题，并且一些标签比其他标签更罕见，这也很重要。这些内容在[“设计模式10：重新平衡”](ch03.xhtml#design_pattern_onezero_rebalancing)的[第3章](ch03.xhtml#problem_representation_design_patterns)中讨论。
- en: Unstructured data
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非结构化数据
- en: Although we have focused in this section on structured data, the same principles
    apply to unstructured data such as images, video, audio, or free-form text as
    well. Just use the metadata to carry out the split. For example, if videos taken
    on the same day are correlated, use a video’s capture date from its metadata to
    split the videos among independent datasets. Similarly, if text reviews from the
    same person tend to be correlated, use the Farm Fingerprint of the `user_id` of
    the reviewer to repeatedly split reviews among the datasets. If the metadata is
    not available or there is no correlation between instances, encode the image or
    video using Base64 encoding and compute the fingerprint of the encoding.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本节重点放在结构化数据上，但相同的原则也适用于非结构化数据，如图像、视频、音频或自由格式文本。只需使用元数据进行拆分即可。例如，如果拍摄于同一天的视频之间存在关联，可以使用视频的元数据中的拍摄日期将视频拆分到独立数据集中。同样地，如果来自同一人的文本评论倾向于关联，可以使用评论者的`user_id`的
    Farm Fingerprint 重复将评论拆分到数据集中。如果元数据不可用或实例之间不存在相关性，则使用 Base64 编码对图像或视频进行编码，并计算编码的指纹。
- en: A natural way to split text datasets might be to use the hash of the text itself
    for splitting. However, this is akin to a random split and does not address the
    problem of correlations between reviews. For example, if a person uses the word
    “stunning” a lot in their negative reviews or if a person rates all Star Wars
    movies as bad, their reviews are correlated. Similarly, a natural way to split
    image or audio datasets might be to use the hash of the filename for splitting,
    but it does not address the problem of correlations between images or videos.
    It is worth thinking carefully about the best way to split a dataset. In our experience,
    many problems with poor performance of ML can be addressed by designing the data
    split (and data collection) with potential correlations in mind.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 对于拆分文本数据集的一个自然方法可能是使用文本本身的哈希值进行拆分。然而，这类似于随机拆分，并不能解决评论之间的关联问题。例如，如果一个人在他们的负面评价中经常使用“惊人”这个词，或者如果一个人将所有星球大战电影评为不好，那么他们的评论是相关的。类似地，对于拆分图像或音频数据集的一个自然方法可能是使用文件名的哈希值进行拆分，但不能解决图像或视频之间的关联问题。因此，需要仔细考虑拆分数据集的最佳方法。根据我们的经验，通过考虑可能存在的相关性来设计数据拆分（和数据收集），可以解决许多机器学习性能不佳的问题。
- en: When computing embeddings or pre-training autoencoders, we should make sure
    to first split the data and perform these pre-computations on the training dataset
    only. Because of this, splitting should not be done on the embeddings of the images,
    videos, or text unless these embeddings were created on a completely separate
    dataset.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算嵌入或预训练自编码器时，我们应确保首先对数据进行拆分，并仅在训练数据集上执行这些预计算。因此，在嵌入图像、视频或文本的数据之前，不应进行拆分，除非这些嵌入是在完全不同的数据集上创建的。
- en: 'Design Pattern 23: Bridged Schema'
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计模式 23：桥接模式
- en: The Bridged Schema design pattern provides ways to adapt the data used to train
    a model from its older, original data schema to newer, better data. This pattern
    is useful because when an input provider makes improvements to their data feed,
    it often takes time for enough data of the improved schema to be collected for
    us to adequately train a replacement model. The Bridged Schema pattern allows
    us to use as much of the newer data as is available, but augment it with some
    of the older data to improve model accuracy.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 桥接模式设计模式提供了将模型训练数据从其旧的原始数据模式适应到更新、更好数据的方法。这种模式很有用，因为当输入提供者改进其数据源时，通常需要一些时间来收集足够数量的改进后数据以充分训练替代模型。桥接模式允许我们使用尽可能多的新数据，但通过一些旧数据增强以提高模型准确性。
- en: Problem
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Consider a point-of-sale application that suggests how much to tip a delivery
    person. The application might use a machine learning model that predicts the tip
    amount, taking into account the order amount, delivery time, delivery distance,
    and so on. Such a model would be trained on the actual tips added by customers.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个点对点销售应用程序，该应用程序建议适当的小费金额给送货人。该应用可能使用一个机器学习模型来预测小费金额，考虑订单金额、交货时间、交货距离等因素。这样的模型将在顾客实际添加的小费基础上进行训练。
- en: Assume that one of the inputs to the model is the payment type. In the historical
    data, this has been recorded as “cash” or “card.” However, let’s say the payment
    system has been upgraded and it now provides more detail on the type of card (gift
    card, debit card, credit card) that was used. This is extremely useful information
    because the tipping behavior varies between the three types of cards.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 假设模型的一个输入是支付类型。在历史数据中，这些记录为“现金”或“卡”。但是，假设支付系统已升级，现在提供了更详细的卡类型信息（礼品卡、借记卡、信用卡）。这是非常有用的信息，因为小费行为在这三种卡类型之间有所不同。
- en: At prediction time, the newer information will always be available since we
    are always predicting tip amounts on transactions conducted after the payment
    system upgrade. Because the new information is extremely valuable, and it is already
    available in production to the prediction system, we would like to use it in the
    model as soon as possible.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测时，新信息始终可用，因为我们始终在预测支付系统升级后进行的交易的小费金额。因为新信息非常有价值，并且已经在预测系统的生产中可用，我们希望尽快在模型中使用它。
- en: We cannot train a new model exclusively on the newer data because the quantity
    of new data will be quite small, limited as it is to transactions after the payment
    system upgrade. Because the quality of an ML model is highly dependent on the
    amount of data used to train it, it is likely that a model trained with only the
    new data is going to fare poorly.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法仅使用新数据训练新模型，因为新数据的数量相当有限，仅限于支付系统升级后的交易。机器学习模型的质量高度依赖于用于训练的数据量，只用新数据训练的模型可能表现不佳。
- en: Solution
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: The solution is to bridge the schema of the old data to match the new data.
    Then, we train an ML model using as much of the new data as is available and augment
    it with the older data. There are two questions to answer. First, how will we
    square the fact that the older data has only two categories for payment type,
    whereas the new data has four categories? Second, how will the augmentation be
    done to create datasets for training, validation, and testing?
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是将旧数据的模式与新数据匹配。然后，我们使用尽可能多的新数据来训练ML模型，并用旧数据进行增强。有两个问题需要解决。首先，我们如何解决旧数据仅具有两种支付类型类别，而新数据有四种类别的问题？其次，如何进行增强以创建训练、验证和测试数据集？
- en: Bridged schema
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 跨桥模式
- en: Consider the case where the older data has two categories (cash and card). In
    the new schema, the card category is now much more granular (gift card, debit
    card, credit card). What we do know is that a transaction coded as “card” in the
    old data would have been one of these types but the actual type was not recorded.
    It’s possible to bridge the schema probabilistically or statically. The static
    method is what we recommend, but it is easier to understand if we walk through
    the probabilistic method first.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑旧数据有两个类别（现金和卡）。在新模式中，卡类别现在更加细化（礼品卡、借记卡、信用卡）。我们知道，在旧数据中编码为“卡”的交易可能是这些类型之一，但实际类型未记录。可以通过概率或静态方法进行模式转换。我们建议使用静态方法，但首先通过概率方法进行解释会更容易理解。
- en: Probabilistic method
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 概率方法
- en: Imagine that we estimate from the newer training data that of the card transactions,
    10% are gift cards, 30% are debit cards, and 60% are credit cards. Each time an
    older training example is loaded into the trainer program, we could choose the
    card type by generating a uniformly distributed random number in the range [0,
    100) and choosing a gift card when the random number is less than 10, a debit
    card if it is in [10, 40), and a credit card otherwise. Provided we train for
    enough epochs, any training example would be presented as all three categories,
    but proportional to the actual frequency of occurrence. The newer training examples,
    of course, would always have the actually recorded category.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们从新的训练数据中估计，卡交易中有10%是礼品卡，30%是借记卡，60%是信用卡。每次将旧的训练示例加载到训练程序中时，我们可以通过在[0, 100)范围内生成均匀分布的随机数来选择卡类型，并在随机数小于10时选择礼品卡，在[10,
    40)时选择借记卡，在其他情况下选择信用卡。只要我们训练足够的时期，任何训练示例都将按照实际发生频率显示所有三个类别。当然，新的训练示例始终会有实际记录的类别。
- en: The justification for the probabilistic approach is that we treat each older
    example as having happened hundreds of times. As the trainer goes through the
    data, in each epoch, we simulate one of those instances. In the simulation, we
    expect that 10% of the time that a card was used, the transaction would have occurred
    with a gift card. That’s why we pick “gift card” for the value of the categorical
    input 10% of the time. This is, of course, simplistic—just because gift cards
    are used 10% of the time overall, it is not the case that gift cards will be used
    10% of the time for any specific transaction. As an extreme example, maybe taxi
    companies disallow use of gift cards on airport trips, and so a gift card is not
    even a legal value for some historical examples. However, in the absence of any
    extra information, we will assume that the frequency distribution is the same
    for all the historical examples.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 概率方法的理由是，我们将每个老示例视为已发生了数百次。当训练器遍历数据时，在每个时期，我们模拟这些实例之一。在模拟中，我们期望当卡被使用时，交易有10%
    的概率使用礼品卡。这就是为什么我们选“礼品卡”作为分类输入值的10%。当然，这是简化的——仅因为礼品卡总体上使用频率为10%，并不意味着对于任何具体交易，礼品卡将会被使用10%
    的时间。作为一个极端的例子，也许出租车公司不允许在机场行程中使用礼品卡，因此某些历史示例中根本没有礼品卡作为合法值。然而，在没有任何额外信息的情况下，我们将假设频率分布对所有历史示例都是相同的。
- en: Static method
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 静态方法
- en: Categorical variables are usually one-hot encoded. If we follow the probabilistic
    approach above and train long enough, the average one-hot encoded value presented
    to the training program of a “card” in the older data will be [0, 0.1, 0.3, 0.6].
    The first 0 corresponds to the cash category. The second number is 0.1 because
    10% of the time, on card transactions, this number will be 1 and it will be zero
    in all other cases. Similarly, we have 0.3 for debit cards and 0.6 for credit
    cards.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 类别变量通常进行独热编码。如果我们采用上述的概率方法并且训练足够长时间，老数据中“卡”交易被呈现给训练程序的平均独热编码值将会是[0, 0.1, 0.3,
    0.6]。第一个0对应现金类别。第二个数字是0.1，因为在卡交易中，10% 的时间这个数字会是1，其他情况下为0。类似地，我们有0.3表示借记卡，0.6表示信用卡。
- en: To bridge the older data into the newer schema, we can transform the older categorical
    data into this representation where we insert the a priori probability of the
    new classes as estimated from the training data. The newer data, on the other
    hand, will have [0, 0, 1, 0] for a transaction that is known to have been paid
    by a debit card.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将老数据转换为新模式，我们可以将老的类别数据转换为这种表示形式，在这种表示中，我们插入从训练数据估计得到的新类别的先验概率。另一方面，新数据将会有[0,
    0, 1, 0]，表示这笔交易已知是通过借记卡支付的。
- en: We recommend the static method over the probabilistic method because it is effectively
    what happens if the probabilistic method runs for long enough. It is also much
    simpler to implement since every card payment from the old data will have the
    exact same value (the 4-element array [0, 0.1, 0.3, 0.6]). We can update the older
    data in one line of code, rather than writing a script to generate random numbers
    as in the probabilistic method. It is also computationally much less expensive.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们推荐静态方法而不是概率方法，因为如果概率方法运行足够长时间，实际上就是静态方法的结果。而且静态方法实现起来更简单，因为老数据中的每笔卡付款都有相同的值（4元素数组[0,
    0.1, 0.3, 0.6]）。我们只需一行代码就可以更新老数据，而不是像概率方法那样编写生成随机数的脚本。而且计算上也更加经济。
- en: Augmented data
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 增强数据
- en: 'In order to maximize use of the newer data, make sure to use only two splits
    of the data, which is discussed in [“Design Pattern 12: Checkpoints”](ch04.xhtml#design_pattern_onetwo_checkpoints)
    in [Chapter 4](ch04.xhtml#model_training_patterns). Let’s say that we have 1 million
    examples available with the old schema, but only 5,000 examples available with
    the new schema. How should we create the training and evaluation datasets?'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最大化使用新数据，请确保只使用数据的两个拆分部分，这在《设计模式12：检查点》（ch04.xhtml#design_pattern_onetwo_checkpoints）的第4章中有所讨论。假设我们有100万个示例可用于旧模式，但只有5000个示例可用于新模式。我们应该如何创建训练和评估数据集？
- en: Let’s take the evaluation dataset first. It is important to realize that the
    purpose of training an ML model is to make predictions on unseen data. The unseen
    data in our case will be exclusively data that matches the new schema. Therefore,
    we need to set aside a sufficient number of examples from the new data to adequately
    evaluate generalization performance. Perhaps we need 2,000 examples in our evaluation
    dataset in order to be confident that the model will perform well in production.
    The evaluation dataset will not contain any older examples that have been bridged
    to match the newer schema.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先考虑评估数据集。重要的是要意识到，训练 ML 模型的目的是对未见过的数据进行预测。在我们的情况下，未见过的数据将完全匹配新模式的数据。因此，我们需要从新数据中设置足够数量的示例，以充分评估泛化性能。也许我们需要在评估数据集中有
    2,000 个示例，以确保模型在生产环境中表现良好。评估数据集不会包含任何已经桥接以匹配新模式的旧示例。
- en: How do we know whether we need 1,000 examples in the evaluation dataset or 2,000?
    To estimate this number, compute the evaluation metric of the current production
    model (which was trained on the old schema) on subsets of its evaluation dataset
    and determine how large the subset has to be before the evaluation metric is consistent.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何知道我们是否需要 1,000 个或 2,000 个示例在评估数据集中？为了估计这个数字，计算当前生产模型的评估指标（该模型是在旧模式上训练的）在其评估数据集的子集上，并确定在评估指标一致之前子集必须有多大。
- en: 'Computing the evaluation metric on different subsets could be done as follows
    (as usual, the [full code is on GitHub](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/06_reproducibility/bridging_schema.ipynb)
    in the code repository for this book):'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同子集上计算评估指标可以如下完成（通常情况下，[完整代码在GitHub上](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/06_reproducibility/bridging_schema.ipynb)
    是本书代码库中的）：
- en: '[PRE28]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In the code above, we are trying out evaluation sizes of 100, 200, …, 5,000\.
    At each subset size, we are evaluating the model 25 times, each time on a different,
    randomly sampled subset of the full evaluation set. Because this is the evaluation
    set of the current production model (which we were able to train with one million
    examples), the evaluation dataset here might hold hundreds of thousands of examples.
    We can then compute the standard deviation of the evaluation metric over the 25
    subsets, repeat this on different evaluation sizes, and graph this standard deviation
    against the evaluation size. The resulting graph will be something like [Figure 6-3](ch06_split_000.xhtml#determine_the_number_of_evaluation_exam).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们尝试不同的评估大小，分别为 100、200、…、5,000\. 在每个子集大小上，我们对模型进行 25 次评估，每次使用完全评估集的不同随机抽样子集。因为这是当前生产模型的评估集（我们能够使用一百万个示例进行训练），这里的评估数据集可能包含数十万个示例。然后，我们可以计算
    25 个子集上评估指标的标准偏差，将这个过程重复应用于不同的评估大小，并将这些标准偏差绘制成图表。得到的图表将类似于 [图 6-3](ch06_split_000.xhtml#determine_the_number_of_evaluation_exam)。
- en: '![Determine the number of evaluation examples needed by evaluating the production
    model on subsets of varying sizes and tracking the variability of the evaluation
    metric by the size of the subset. Here, the standard deviation starts to plateau
    at around 2,000 examples.](Images/mldp_0603.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![通过评估生产模型在不同大小子集上的表现来确定所需的评估示例数量，并通过子集大小跟踪评估指标的变化情况。在这里，标准偏差在大约 2,000 个示例时开始趋于稳定。](Images/mldp_0603.png)'
- en: Figure 6-3\. Determine the number of evaluation examples needed by evaluating
    the production model on subsets of varying sizes and tracking the variability
    of the evaluation metric by the size of the subset. Here, the standard deviation
    starts to plateau at around 2,000 examples.
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-3\. 通过评估生产模型在不同大小子集上的表现来确定所需的评估示例数量，并通过子集大小跟踪评估指标的变化情况。在这里，标准偏差在大约 2,000
    个示例时开始趋于稳定。
- en: From [Figure 6-3](ch06_split_000.xhtml#determine_the_number_of_evaluation_exam),
    we see that the number of evaluation examples needs to be at least 2,000, and
    is ideally 3,000 or more. Let’s assume for the rest of this discussion that we
    choose to evaluate on 2,500 examples.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 从 [图 6-3](ch06_split_000.xhtml#determine_the_number_of_evaluation_exam) 中可以看出，所需的评估示例数量至少为
    2,000 个，最好是 3,000 个或更多。让我们假设在本讨论的其余部分中，我们选择在 2,500 个示例上进行评估。
- en: The training set would contain the remaining 2,500 new examples (the amount
    of new data available after withholding 2,500 for evaluation) augmented by some
    number of older examples that have been bridged to match the new schema. How do
    we know how many older examples we need? We don’t. This is a hyperparameter that
    we will have to tune. For example, on the tip problem, using grid search, we see
    from [Figure 6-4](ch06_split_000.xhtml#determine_the_number_of_older_examples)
    (the [notebook on GitHub](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/06_reproducibility/bridging_schema.ipynb)
    has the full details) that the evaluation metric drops steeply until 20,000 examples
    and then starts to plateau.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集将包含剩余的2,500个新样本（在保留2,500个用于评估后可用的新数据量）以及一些已经桥接以匹配新模式的较老样本。我们如何知道需要多少较老的例子？我们不知道。这是一个需要调整的超参数。例如，在梯度搜索中，我们从[图 6-4](ch06_split_000.xhtml#determine_the_number_of_older_examples)（GitHub上的[笔记本](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/06_reproducibility/bridging_schema.ipynb)详细介绍了全部细节）可以看出，在20,000个样本之前，评估指标急剧下降，然后开始趋于稳定。
- en: '![Determine the number of older examples to bridge by carrying out hyperparameter
    tuning. In this case, it is apparent that there are diminishing returns after
    20,000 bridged examples.](Images/mldp_0604.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![通过进行超参数调整确定桥接的较老例子数量。在这种情况下，很明显，在桥接20,000个例子之后，收益递减。](Images/mldp_0604.png)'
- en: Figure 6-4\. Determine the number of older examples to bridge by carrying out
    hyperparameter tuning. In this case, it is apparent that there are diminishing
    returns after 20,000 bridged examples.
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-4\. 通过进行超参数调整确定桥接的较老例子数量。在这种情况下，很明显，在桥接20,000个例子之后，收益递减。
- en: For best results, we should choose the smallest number of older examples that
    we can get away with—ideally, over time, as the number of new examples grows,
    we’ll rely less and less on bridged examples. At some point, we’ll be able to
    get rid of the older examples altogether.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得最佳结果，我们应该选择能够应付的最少数量的较老例子，理想情况下，随着新样本数量的增长，我们将越来越少依赖桥接的例子。在某个时刻，我们将能够完全摆脱较老的例子。
- en: It is worth noting that, on this problem, bridging does bring benefits because
    when we use no bridged examples, the evaluation metric is worse. If this is not
    the case, then the imputation method (the method of choosing the static value
    used for bridging) needs to be reexamined. We suggest an alternate imputation
    method (Cascade) in the next section.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，在这个问题上，桥接确实带来了好处，因为当我们没有使用桥接的例子时，评估指标会变差。如果不是这种情况，那么需要重新审视填充方法（用于桥接的静态值选择方法）。我们建议在下一节中使用另一种填充方法（级联）。
- en: Warning
  id: totrans-174
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: It is extremely important to compare the performance of the newer model trained
    on bridged examples against the older, unchanged model on the evaluation dataset.
    It might be the case that the new information does not yet have adequate value.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 非常重要的是，比较在桥接示例上训练的新模型与评估数据集上未改变的老模型的性能。可能情况是，新信息尚未具备足够的价值。
- en: Because we will be using the evaluation dataset to test whether or not the bridged
    model has value, it is critical that the evaluation dataset not be used during
    training or hyperparameter tuning. So, techniques like early stopping or checkpoint
    selection must be avoided. Instead, use regularization to control overfitting.
    The training loss will have to serve as the hyperparameter tuning metric. See
    the discussion of the Checkpoints design pattern in [Chapter 4](ch04.xhtml#model_training_patterns)
    for more details on how to conserve data by using only two splits.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们将使用评估数据集来测试桥接模型是否具有价值，所以非常重要的是评估数据集不在训练或超参数调整过程中使用。因此，必须避免使用早停止或检查点选择等技术。相反，使用正则化来控制过拟合。训练损失将作为超参数调整的度量。更多关于如何通过仅使用两个分割来节省数据的讨论，请参阅[第 4 章](ch04.xhtml#model_training_patterns)中关于检查点设计模式的详细信息。
- en: Trade-Offs and Alternatives
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 折衷和替代方案
- en: Let’s look at a commonly proposed approach that doesn’t work, a complex alternative
    to bridging, and an extension of the solution to a similar problem.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个通常提出但并不起作用的方法，一种复杂的桥接替代方案，以及解决类似问题的解决方案的扩展。
- en: Union schema
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 联合模式
- en: 'It can be tempting to simply create a union of the older and newer schemas.
    For example, we could define the schema for the payment type as having five possible
    values: cash, card, gift card, debit card, and credit card. This will make both
    the historical data and the newer data valid and is the approach that we would
    take in data warehousing to deal with changes like this. This way, the old data
    and the new data are valid as is and without any changes.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会诱人的是简单地创建旧版和新版模式的联合。例如，我们可以定义支付类型模式为具有五个可能值：现金、卡片、礼品卡、借记卡和信用卡。这样做将使历史数据和新数据都有效，并且是我们在数据仓库中处理此类更改时采取的方法。这样，旧数据和新数据都是有效的，无需任何更改。
- en: The backward-compatible, union-of-schemas approach doesn’t work for machine
    learning though.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于机器学习来说，向后兼容的、联合模式并不适用。
- en: At prediction time, we will never get the value “card” for the payment type
    because the input providers have all been upgraded. Effectively, all those training
    instances will have been for nought. For reproducibility (this is the reason that
    this pattern is classified as a reproducibility pattern), we need to bridge the
    older schema into the newer schema and can’t do a union of the two schemas.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测时，我们永远不会得到支付类型为“卡”的值，因为输入提供者已经全部升级了。事实上，所有这些训练实例都将是徒劳的。出于可重现性考虑（这是将此模式分类为可重现性模式的原因），我们需要将旧模式桥接到新模式中，而不能简单地将两个模式进行联合。
- en: Cascade method
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 级联方法
- en: Imputation in statistics is a set of techniques that can be used to replace
    missing data by some valid value. A common imputation technique is to replace
    a NULL value by the mean value of that column in the training data. Why do we
    choose the mean? Because, in the absence of any more information and assuming
    that the values are normally distributed, the most likely value is the mean.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学中的插补是一组可以用来替换缺失数据的技术。常见的插补技术是用该列在训练数据中的均值替换空值。为什么选择均值？因为在没有更多信息的情况下，并且假设值正态分布时，最可能的值是均值。
- en: The static method discussed in the main solution, of assigning a priori frequencies,
    is also an imputation method. We assume that the categorical variable is distributed
    according to a frequency chart (that we estimate from the training data) and impute
    the mean one-hot encoded value (according to that frequency distribution) to the
    “missing” categorical variable.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在主要解决方案中讨论的静态方法，即分配先验频率的方法，也是一种插补方法。我们假设分类变量按照频率表分布（我们从训练数据中估算出来），并且将均值的独热编码值（根据该频率分布）插补到“缺失”的分类变量中。
- en: 'Do we know any other way to estimate unknown values given some examples? Of
    course! Machine learning. What we can do is to train a cascade of models (see
    [“Design Pattern 8: Cascade ”](ch03.xhtml#design_pattern_eight_cascade) in [Chapter 3](ch03.xhtml#problem_representation_design_patterns)).
    The first model uses whatever new examples we have to train a machine learning
    model to predict the card type. If the original tips model had five inputs, this
    model will have four inputs. The fifth input (the payment type) will be the label
    for this model. Then, the output of the first model will be used to train the
    second model.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否知道其他方法可以根据一些示例来估计未知值？当然！机器学习。我们可以做的是训练一系列模型（参见[“设计模式 8：级联”](ch03.xhtml#design_pattern_eight_cascade)在[第三章](ch03.xhtml#problem_representation_design_patterns)）。第一个模型使用任何新示例来训练一个机器学习模型以预测卡类型。如果原始的技巧模型有五个输入，这个模型将有四个输入。第五个输入（支付类型）将是此模型的标签。然后，第一个模型的输出将用于训练第二个模型。
- en: In practice, the Cascade pattern adds too much complexity for something that
    is meant to be a temporary workaround until you have enough new data. The static
    method is effectively the simplest machine learning model—it’s the model we would
    get if we had uninformative inputs. We recommend the static approach and to use
    Cascade only if the static method doesn’t do well enough.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，级联模式为一些被认为是临时性的解决方案添加了太多复杂性，直到你有足够的新数据。静态方法实际上是最简单的机器学习模型——如果我们有无信息输入，那么这就是我们将得到的模型。我们建议采用静态方法，并且只有在静态方法效果不够好时才使用级联方法。
- en: Handling new features
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理新功能
- en: Another situation where bridging might be needed is when the input provider
    adds extra information to the input feed. For example, in our taxi fare example,
    we may start receiving data on whether the taxi’s wipers are on or whether the
    vehicle is moving. From this data, we can craft a feature on whether it was raining
    at the time the taxi trip started, the fraction of the trip time that the taxi
    is idle, and so on.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种需要桥接的情况是输入提供者向输入数据流添加额外信息。例如，在我们的出租车费用示例中，我们可以开始接收有关出租车是否打开雨刷或车辆是否移动的数据。根据这些数据，我们可以创建一个特征，表示出租车行程开始时是否下雨，以及出租车空闲时间的分数等。
- en: 'If we have new input features we want to start using immediately, we should
    bridge the older data (where this new feature will be missing) by imputing a value
    for the new feature. Recommended choices for the imputation value are:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有新的输入特征希望立即开始使用，我们应该通过插补新特征的值来桥接旧数据（其中这个新特征将缺失）。推荐的插补值选择有：
- en: The mean value of the feature if the feature is numeric and normally distributed
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果特征是数值且正态分布，则特征的平均值。
- en: The median value of the feature if the feature is numeric and skewed or has
    lots of outliers
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果特征是数值且存在偏斜或大量异常值，则特征的中位数值。
- en: The median value of the feature if the feature is categorical and sortable
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果特征是分类的且可排序，则特征的中位数值。
- en: The mode of the feature if the feature is categorical and not sortable
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果特征是分类的且不可排序，则特征的众数。
- en: The frequency of the feature being true if it is boolean
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果特征是布尔值，则特征为真的频率。
- en: If the feature is whether or not it was raining, it is boolean, and so the imputed
    value would be something like 0.02 if it rains 2% of the time in the training
    dataset. If the feature is the proportion of idle minutes, we could use the median
    value. The Cascade pattern approach remains viable for all these cases, but a
    static imputation is simpler and often sufficient.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果特征是是否下雨的布尔值，则插补值可能是像0.02这样的值，如果在训练数据集中下雨的时间占比为2%。如果特征是空闲分钟的比例，则可以使用中位数值。在所有这些情况下，级联模式方法仍然可行，但静态插补更简单且通常足够。
- en: Handling precision increases
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理精度增加
- en: When the input provider increases the precision of their data stream, follow
    the bridging approach to create a training dataset that consists of the higher-resolution
    data, augmented with some of the older data.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入提供者增加其数据流的精度时，按照桥接方法创建一个训练数据集，其中包含更高分辨率数据的一部分，并加上一些旧数据。
- en: For floating-point values, it is not necessary to explicitly bridge the older
    data to match the newer data’s precision. To see why, consider the case where
    some data was originally provided to one decimal place (e.g., 3.5 or 4.2) but
    is now being provided to two decimal places (e.g., 3.48 or 4.23). If we assume
    that 3.5 in the older data consists of values that would be uniformly distributed^([1](ch06_split_001.xhtml#ch01fn25))
    in [3.45, 3.55] in the newer data, the statically imputed value would be 3.5,
    which is precisely the value that is stored in the older data.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 对于浮点数值，不需要显式地调整旧数据以匹配新数据的精度。为了理解这一点，考虑一种情况：一些数据最初提供到小数点后一位（例如，3.5或4.2），但现在提供到小数点后两位（例如，3.48或4.23）。如果我们假设在旧数据中的3.5由在新数据中均匀分布于[3.45,
    3.55]的值组成，那么静态插补值将是3.5，这正是存储在旧数据中的值。
- en: For categorical values—for example, if the older data stored the location as
    a state or provincial code and the newer data provided the county or district
    code—use the frequency distribution of counties within states as described in
    the main solution to carry out static imputation.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类值——例如，如果旧数据将位置存储为州或省代码，而新数据提供县或区代码——使用主解决方案中描述的州内县的频率分布进行静态插补。
- en: 'Design Pattern 24: Windowed Inference'
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计模式 24：窗口推断
- en: The Windowed Inference design pattern handles models that require an ongoing
    sequence of instances in order to run inference. This pattern works by externalizing
    the model state and invoking the model from a stream analytics pipeline. This
    pattern is also useful when a machine learning model requires features that need
    to be computed from aggregates over time windows. By externalizing the state to
    a stream pipeline, the Windowed Inference design pattern ensures that features
    calculated in a dynamic, time-dependent way can be correctly repeated between
    training and serving. It is a way of avoiding training–serving skew in the case
    of temporal aggregate features.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口推理设计模式处理需要连续一系列实例才能运行推理的模型。这种模式通过外部化模型状态并从流分析管道调用模型来工作。当机器学习模型需要从时间窗口的聚合中计算特征时，此模式也非常有用。通过将状态外部化到流水线，窗口推理设计模式确保了在训练和服务之间正确重复计算动态、时间相关特征。这是避免在时间聚合特征情况下训练-服务偏差的一种方式。
- en: Problem
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Take a look at the arrival delays at Dallas Fort Worth (DFW) airport depicted
    for a couple of days in May 2010 in [Figure 6-5](ch06_split_000.xhtml#arrival_delays_at_dallas_fort_worth_lef)
    (the [full notebook](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/06_reproducibility/stateful_stream.ipynb)
    is on GitHub).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 查看2010年5月在[图 6-5](ch06_split_000.xhtml#arrival_delays_at_dallas_fort_worth_lef)上展示的达拉斯-沃思堡（DFW）机场的到达延误情况（完整的笔记本在GitHub上）。
- en: '![Arrival delays at Dallas Fort Worth (DFW) airport on May 10–11, 2010\. Abnormal
    arrival delays are marked with a dot.](Images/mldp_0605.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![2010年5月10日至11日达拉斯-沃思堡（DFW）机场的到达延误。异常到达延误用点标记。](Images/mldp_0605.png)'
- en: Figure 6-5\. Arrival delays at Dallas Fort Worth (DFW) airport on May 10–11,
    2010\. Abnormal arrival delays are marked with a dot.
  id: totrans-206
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-5\. 2010年5月10日至11日达拉斯-沃思堡（DFW）机场的到达延误。异常到达延误用点标记。
- en: The arrival delays exhibit considerable variability, but it is still possible
    to note unusually large arrival delays (marked by a dot). Note that the definition
    of “unusual” varies by context. Early in the morning (left corner of the plot),
    most flights are on time, so even the small spike is anomalous. By the middle
    of the day (after 12 p.m. on May 10), variability picks up and 25-minute delays
    are quite common, but a 75-minute delay is still unusual.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 到达延误表现出相当大的变异性，但仍然可以注意到异常的大到达延误（用点标记）。请注意，“异常”的定义根据上下文而异。在早晨（图表左上角），大多数航班准时到达，因此即使是小的峰值也是异常的。到了中午（5月10日下午12点后），变异性增加，25分钟的延误相当普遍，但75分钟的延误仍然是异常的。
- en: 'Whether or not a specific delay is anomalous depends on a time context, for
    example, on the arrival delays observed over the past two hours. To determine
    that a delay is anomalous requires that we first sort the dataframe based on the
    time (as in the graph in [Figure 6-5](ch06_split_000.xhtml#arrival_delays_at_dallas_fort_worth_lef)
    and shown below in pandas):'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 特定延误是否异常取决于时间上下文，例如过去两小时内观察到的到达延误。为了确定延误是否异常，我们首先需要根据时间对数据帧进行排序（如[图 6-5](ch06_split_000.xhtml#arrival_delays_at_dallas_fort_worth_lef)中的图表，并在pandas中显示）：
- en: '[PRE29]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then, we need to apply an anomaly detection function to sliding windows of
    two hours:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要对两小时的滑动窗口应用异常检测函数：
- en: '[PRE30]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The anomaly detection function, `is_anomaly`, can be quite sophisticated, but
    let’s take the simple case of discarding extrema and calling a data value an anomaly
    if it is more than four standard deviations from the mean in the two-hour window:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测函数`is_anomaly`可能非常复杂，但让我们简单地考虑丢弃极值并在两小时窗口中，如果数据值超过均值四个标准差，则将其视为异常：
- en: '[PRE31]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This works on historical (training) data because the entire dataframe is at
    hand. Of course, when running inference on our production model, we will not have
    the entire dataframe. In production, we will be receiving flight arrival information
    one by one, as each flight arrives. So, all that we will have is a single delay
    value at a timestamp:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这在历史（训练）数据上运行，因为整个数据帧都在手头上。当然，在生产模型上运行推理时，我们将不会有整个数据帧。在生产中，我们将逐个接收航班到达信息，每到一个航班就会有一个延误值。因此，我们将只有一个时间戳上的单个延误值：
- en: '[PRE32]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Given that the flight above (at 08:45 on February 3) is 19 minutes late, is
    that unusual or not? Commonly, to carry out ML inference on a flight, we only
    need the features of that flight. In this case, however, the model requires information
    about all flights to DFW airport between 06:45 and 08:45:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于上面的航班（在2月3日08:45）晚点了19分钟，这是否不寻常？ 通常，要对航班进行ML推断，我们只需要该航班的特征。 但是，在这种情况下，模型需要有关06:45到08:45之间所有飞往DFW机场的航班的信息：
- en: '[PRE33]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: It is not possible to carry out inference one flight at a time. We need to somehow
    provide the model information about all the previous flights.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 不可能一次进行单个航班的推断。 我们需要以某种方式向模型提供关于所有先前航班的信息。
- en: How do we carry out inference when the model requires not just one instance,
    but a sequence of instances?
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型不仅需要一个实例而是一系列实例时，我们如何进行推断？
- en: Solution
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'The solution is to carry out stateful stream processing—that is, stream processing
    that keeps track of the model state through time:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是进行有状态流处理，即通过时间跟踪模型状态进行流处理：
- en: A sliding window is applied to flight arrival data. The sliding window will
    be over 2 hours, but the window can be closed more often, such as every 10 minutes.
    In such a case, aggregate values will be calculated every 10 minutes over the
    previous 2 hours.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对航班到达数据应用滑动窗口。 滑动窗口将覆盖2小时，但窗口可以更频繁地关闭，例如每10分钟。 在这种情况下，将在过去2小时内每10分钟计算聚合值。
- en: The internal model state (this could be the list of flights) is updated with
    flight information every time a new flight arrives, thus building a 2-hour historical
    record of flight data.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内部模型状态（这可能是航班列表）每次新航班到达时都会更新航班信息，从而建立起2小时的历史航班数据记录。
- en: Every time the window is closed (every 10 minutes in our example), a time-series
    ML model is trained on the 2-hour list of flights. This model is then used to
    predict future flight delays and the confidence bounds of such predictions.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次窗口关闭时（例如我们的例子中每10分钟），将在2小时内的航班列表上训练时间序列ML模型。 然后，此模型用于预测未来的航班延误以及此类预测的置信区间。
- en: The time-series model parameters are externalized into a state variable. We
    could use a time-series model such as autoregressive integrated moving average
    (ARIMA) or long short-term memory (LSTMs), in which case, the model parameters
    would be the ARIMA model coefficients or the LSTM model weights. To keep the code
    understandable, we will use a zero-order regression model,^([2](ch06_split_001.xhtml#ch01fn26))
    and so our model parameters will be the average flight delay and the variance
    of the flight delays over the two-hour window.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间序列模型参数外部化为状态变量。 我们可以使用时间序列模型，如自回归积分滑动平均（ARIMA）或长短期记忆（LSTMs），在这种情况下，模型参数将是ARIMA模型系数或LSTM模型权重。
    为了保持代码易于理解，我们将使用零阶回归模型，^([2](ch06_split_001.xhtml#ch01fn26)) 因此，我们的模型参数将是平均航班延误和过去两小时窗口内航班延误的方差。
- en: When a flight arrives, its arrival delay can be classified as anomalous or not
    using the externalized model state—it is not necessary to have the full list of
    flights over the past 2 hours.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当航班到达时，可以使用外部化模型状态对其到达延误进行异常分类，而不必拥有过去2小时内所有航班的完整列表。
- en: 'We can use Apache Beam for streaming pipelines because then, the same code
    will work on both the historical data and on newly arriving data. In Apache Beam,
    the sliding window is set up as follows ([full code is on GitHub](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/06_reproducibility/find_anomalies_model.py)):'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用Apache Beam进行流水线处理，因为同样的代码将适用于历史数据和新到达的数据。 在Apache Beam中，滑动窗口设置如下（[完整代码在GitHub上](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/06_reproducibility/find_anomalies_model.py)）：
- en: '[PRE34]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The model is updated by combining all the flight data collected over the past
    two hours and passing it to a function that we call `ModelFn`:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 模型通过整合过去两小时收集的所有航班数据并传递给我们称之为`ModelFn`的函数来进行更新：
- en: '[PRE35]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '`ModelFn` updates the internal model state with flight information. Here, the
    internal model state will consist of a pandas dataframe that is updated with the
    flights in the window:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '`ModelFn`使用航班信息更新内部模型状态。 在这里，内部模型状态将包括一个带有窗口内航班的pandas dataframe：'
- en: '[PRE36]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Every time the window is closed, the output is extracted. The output here (we
    refer to it as externalized model state) consists of the model parameters:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 每次窗口关闭时，提取输出。 这里的输出（我们称之为外部化模型状态）包括模型参数：
- en: '[PRE37]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The externalized model state gets updated every 10 minutes based on a 2-hour
    rolling window:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 外部化模型状态每10分钟更新一次，基于2小时滚动窗口：
- en: '| Window close time | prediction | acceptable_deviation |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 窗口关闭时间 | 预测 | 可接受偏差 |'
- en: '| --- | --- | --- |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 2010-05-10T06:35:00 | -2.8421052631578947 | 10.48412597725367 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 2010-05-10T06:35:00 | -2.8421052631578947 | 10.48412597725367 |'
- en: '| 2010-05-10T06:45:00 | -2.6818181818181817 | 12.083729926046008 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 2010-05-10T06:45:00 | -2.6818181818181817 | 12.083729926046008 |'
- en: '| 2010-05-10T06:55:00 | -2.9615384615384617 | 11.765962341537781 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 2010-05-10T06:55:00 | -2.9615384615384617 | 11.765962341537781 |'
- en: 'The code to extract the model parameters shown above is similar to that of
    the pandas case, but it is done within a Beam pipeline. This allows the code to
    work in streaming, but the model state is available only within the context of
    the sliding window. In order to carry out inference on every arriving flight,
    we need to externalize the model state (similar to how we export the model weights
    out to a file in the Stateless Serving Function pattern to decouple it from the
    context of the training program where these weights are computed):'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 提取模型参数的代码与pandas情况类似，但在Beam管道内完成。这使代码可以在流式处理中工作，但模型状态仅在滑动窗口的上下文中可用。为了在每次到达的航班上进行推理，我们需要外部化模型状态（类似于在无状态服务函数模式中将模型权重导出到文件以将其与训练程序的上下文分离，其中这些权重是计算的）：
- en: '[PRE38]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This externalized state can be used to detect whether or not a given flight
    is an anomaly:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用此外部化状态检测给定航班是否异常：
- en: '[PRE39]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The `is_anomaly` function is then applied to every item in the last pane of
    the sliding window:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将`is_anomaly`函数应用于滑动窗口的最后一个项目：
- en: '[PRE40]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Trade-Offs and Alternatives
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 折中与替代方案
- en: The solution suggested above is computationally efficient in the case of high-throughput
    data streams but can be improved further if the ML model parameters can be updated
    online. This pattern is also applicable to stateful ML models such as recurrent
    neural networks and when a stateless model requires stateful input features.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 上述提到的解决方案在高吞吐量数据流的情况下具有计算效率，但如果可以在线更新机器学习模型参数，则可以进一步改进。这种模式也适用于状态化机器学习模型，如循环神经网络，以及当无状态模型需要有状态输入特征时。
- en: Reduce computational overhead
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 减少计算开销
- en: 'In the Problem section, we used the following pandas code:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在问题部分，我们使用了以下pandas代码：
- en: '[PRE41]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Whereas, in the Solution section, the Beam code was as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 而在解决方案部分，Beam代码如下所示：
- en: '[PRE42]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: There are meaningful differences between the rolling window in pandas and the
    sliding window in Apache Beam because of how often the `is_anomaly` function is
    called and how often the model parameters (mean and standard deviation) need to
    be computed. These are discussed below.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`is_anomaly`函数的调用频率以及模型参数（均值和标准差）的计算频率，pandas中的滚动窗口和Apache Beam中的滑动窗口存在有意义的区别。这些将在下文讨论。
- en: Per element versus over a time interval
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 按元素与按时间间隔
- en: In the pandas code, the `is_anomaly` function is being called on every instance
    in the dataset. The anomaly detection code computes the model parameters and applies
    it immediately to the last item in the window. In the Beam pipeline, the model
    state is also created on every sliding window, but the sliding window in this
    case is based on time. Therefore, the model parameters are computed just once
    every 10 minutes.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在pandas代码中，`is_anomaly`函数在数据集中的每个实例上都被调用。异常检测代码计算模型参数，并立即将其应用于窗口中的最后一个项目。在Beam管道中，模型状态也是在每个滑动窗口上创建的，但在这种情况下，滑动窗口是基于时间的。因此，模型参数每10分钟仅计算一次。
- en: 'The anomaly detection itself is carried out on every instance:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测本身在每个实例上执行：
- en: '[PRE43]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Notice that this carefully separates out computationally expensive training
    from computationally cheap inference. The computationally expensive part is carried
    out only once every 10 minutes while allowing every instance to be classified
    as being an anomaly or not.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这将计算昂贵的训练与计算廉价的推理仔细分离。昂贵的部分每10分钟仅执行一次，同时允许对每个实例进行异常分类。
- en: High-throughput data streams
  id: totrans-260
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 高吞吐量数据流
- en: Data volumes keep increasing, and much of that increase in data volume is due
    to real-time data. Consequently, this pattern has to be applied to high-throughput
    data streams—streams where the number of elements can be in excess of thousands
    of items per second. Think, for example, of clickstreams from websites or streams
    of machine activity from computers, wearable devices, or cars.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 数据量不断增加，其中大部分增加是由实时数据引起的。因此，这种模式必须应用于高吞吐量数据流——其中元素数量可能超过每秒数千个项目。例如，考虑来自网站点击流或来自计算机、可穿戴设备或汽车的机器活动流。
- en: The suggested solution using a streaming pipeline is advantageous in that it
    avoids retraining the model at every instance, something that the pandas code
    in the Problem statement does. However, the suggested solution gives back those
    gains by creating an in-memory dataframe of all the records received. If we receive
    5,000 items a second, then the in-memory dataframe over 10 minutes will contain
    3 million records. Because there are 12 sliding windows that will need to be maintained
    at any point in time (10-minute windows, each over 2 hours), the memory requirements
    can become considerable.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 使用流水线处理的建议解决方案具有优势，因为它避免了在每个实例中重新训练模型，这是问题声明中的 pandas 代码所做的事情。然而，建议的解决方案通过创建一个包含所有接收记录的内存数据框架来还原这些收益。如果我们每秒接收
    5,000 个项目，那么在 10 分钟内存数据框架将包含 3 百万条记录。因为任何时刻都需要维护 12 个滑动窗口（每个 10 分钟窗口，每个 2 小时），内存需求可能变得相当大。
- en: 'Storing all the received records in order to compute the model parameters at
    the end of the window can become problematic. When the data stream is high throughput,
    it becomes important to be able to update the model parameters with each element.
    This can be done by changing the `ModelFn` as follows ([full code is on GitHub](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/06_reproducibility/find_anomalies_model.py)):'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在窗口结束时计算模型参数，存储所有接收的记录可能会变得棘手。当数据流量大时，能够每个元素更新模型参数变得非常重要。可以通过如下更改`ModelFn`来实现此操作（[完整代码位于
    GitHub](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/06_reproducibility/find_anomalies_model.py)）：
- en: '[PRE44]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The key difference is that the only thing held in memory are three floating
    point numbers (`sum`, `sum²`, `count`) required to extract the output model state,
    not the entire dataframe of received instances. Updating the model parameters
    one instance at a time is called an *online update* and is something that can
    be done only if the model training doesn’t require iteration over the entire dataset.
    Therefore, in the above implementation, the variance is computed by maintaining
    a sum of x² so that we don’t need a second pass through the data after computing
    the mean.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 主要区别在于，仅在内存中保存了三个浮点数（`sum`、`sum²`、`count`），用于提取输出模型状态所需的模型参数，而不是接收实例的整个数据框架。逐个实例更新模型参数称为*在线更新*，只有在模型训练不需要对整个数据集进行迭代时才能完成。因此，在上述实现中，通过维护
    x² 的总和来计算方差，这样在计算均值后就不需要对数据进行第二次遍历。
- en: Streaming SQL
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 流式 SQL
- en: If our infrastructure consists of a high-performance SQL database that is capable
    of processing streaming data, it is possible to implement the Windowed Inference
    pattern in an alternative way by using an aggregation window ([full code is on
    GitHub](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/06_reproducibility/find_anomalies_model.py)).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的基础架构包括一个能够处理流数据的高性能 SQL 数据库，那么可以通过使用聚合窗口来实现 Windowed Inference 模式的另一种方式（[完整代码位于
    GitHub](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/06_reproducibility/find_anomalies_model.py)）。
- en: 'We pull out the flight data from BigQuery:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 BigQuery 中提取飞行数据：
- en: '[PRE45]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Then, we create the `model_state` by computing the model parameters over a
    time window specified as two hours preceding to one second preceding:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过计算一个时间窗口的模型参数来创建`model_state`，该窗口指定为前两小时到前一秒：
- en: '[PRE46]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Finally, we apply the anomaly detection algorithm to each instance:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们对每个实例应用异常检测算法：
- en: '[PRE47]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The result looks like [Table 6-1](ch06_split_000.xhtml#the_result_of_a_bigquery_query_determin),
    with the arrival delay of 54 minutes marked as an anomaly given that all the previous
    flights arrived early.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 结果看起来像[表 6-1](ch06_split_000.xhtml#the_result_of_a_bigquery_query_determin)，到达延迟
    54 分钟被标记为异常，因为所有先前的航班都提前到达。
- en: Table 6-1\. The result of a BigQuery query determining whether incoming flight
    data is an anomaly
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6-1\. BigQuery 查询结果，确定传入的航班数据是否异常
- en: '| scheduled_arrival_time | arrival_delay | prediction | acceptable_deviation
    | is_anomaly |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| scheduled_arrival_time | arrival_delay | prediction | acceptable_deviation
    | is_anomaly |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 2010-05-01T05:45:00 | -18.0 | -8.25 | 62.51399843235114 | false |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 2010-05-01T05:45:00 | -18.0 | -8.25 | 62.51399843235114 | false |'
- en: '| 2010-05-01T06:00:00 | -13.0 | -10.2 | 56.878818553131005 | false |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 2010-05-01T06:00:00 | -13.0 | -10.2 | 56.878818553131005 | false |'
- en: '| 2010-05-01T06:35:00 | -1.0 | -10.666 | 51.0790237442599 | false |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 2010-05-01T06:35:00 | -1.0 | -10.666 | 51.0790237442599 | false |'
- en: '| 2010-05-01T06:45:00 | -9.0 | -9.28576 | 48.86521793473886 | false |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 2010-05-01T06:45:00 | -9.0 | -9.28576 | 48.86521793473886 | false |'
- en: '| 2010-05-01T07:00:00 | **54.0** | -9.25 | 45.24220532707422 | **true** |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 2010-05-01T07:00:00 | **54.0** | -9.25 | 45.24220532707422 | **true** |'
- en: Unlike the Apache Beam solution, the efficiency of distributed SQL will allow
    us to calculate the 2-hour time window centered on each instance (instead of at
    a resolution of 10-minute windows). However, the drawback is that BigQuery tends
    to have relatively high latency (on the order of seconds), and so it cannot be
    used for real-time control applications.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 与Apache Beam解决方案不同，分布式SQL的效率将允许我们计算以每个实例为中心的2小时时间窗口（而不是以10分钟窗口的分辨率）。然而，其缺点是BigQuery往往具有相对较高的延迟（以秒计），因此不能用于实时控制应用程序。
- en: Sequence models
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 序列模型
- en: The Windowed Inference pattern of passing a sliding window of previous instances
    to an inference function is useful beyond anomaly detection or even time-series
    models. Specifically, it is useful in any class of models, such as Sequence models,
    that require a historical state. For example, a translation model needs to see
    several successive words before it can carry out the translation so that the translation
    takes into account the context of the word. After all, the translation of the
    words “left,” “Chicago,” and “road” vary between the sentences “I left Chicago
    by road” and “Turn left on Chicago Road.”
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 通过传递前几个实例的滑动窗口给推断函数的窗口推断模式，不仅对异常检测或甚至时间序列模型有用。特别是对于任何需要历史状态的模型类，如序列模型，它非常有用。例如，翻译模型需要看到几个连续的单词才能执行翻译，以便翻译考虑到单词的上下文。毕竟，“left”、“Chicago”和“road”的翻译在句子“I
    left Chicago by road”和“Turn left on Chicago Road”中有所不同。
- en: 'For performance reasons, the translation model will be set up to be stateless
    and require the user to provide the context. For example, if the model is stateless,
    instances of the model can be autoscaled in response to increased traffic, and
    can be invoked in parallel to obtain faster translations. Thus, the translation
    of the famous soliloquy from Shakespeare’s Hamlet into German might follow these
    steps, picking off in the middle where the bolded word is the one to be translated:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 为了性能原因，翻译模型将被设置为无状态，并要求用户提供上下文。例如，如果模型是无状态的，那么可以根据流量的增加自动扩展模型的实例，并且可以并行调用以获得更快的翻译。因此，翻译莎士比亚《哈姆雷特》中著名的独白到德语可能会按照这些步骤进行，从中间开始，加粗的单词是要翻译的单词：
- en: '| Input (9 words, 4 on either side) | Output |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 输入（9个单词，每边4个） | 输出 |'
- en: '| --- | --- |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| The undiscovered country, from **whose** bourn No traveller returns | dessen
    |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 未曾探索的国度，从**其**的彼岸，没有旅行者回来 | dessen |'
- en: '| undiscovered country, from whose **bourn** No traveller returns, puzzles
    | Bourn |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 未曾探索的国度，从其**彼岸**没有旅行者回来，这令 | Bourn |'
- en: '| country, from whose bourn **No** traveller returns, puzzles the | Kein |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 国度，从其彼岸**没有**旅行者回来，这令 | Kein |'
- en: '| from whose bourn No **traveller** returns, puzzles the will, | Reisender
    |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 从其彼岸没有**旅行者**回来，这令思 | Reisender |'
- en: The client, therefore, will need a streaming pipeline. The pipeline could take
    the input English text, tokenize it, send along nine tokens at a time, collect
    the outputs, and concatenate them into German sentences and paragraphs.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，客户端将需要一个流水线。该流水线可以接收输入的英文文本，将其标记化，每次发送九个标记，收集输出，并将其连接成德语句子和段落。
- en: Most sequence models, such as recurrent neural networks and LSTMs, require streaming
    pipelines for high-performance inference.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数序列模型，如循环神经网络和LSTM，需要流水线以实现高性能推断。
- en: Stateful features
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 有状态的特征
- en: The Windowed Inference pattern can be useful if an input feature to the model
    requires state, even if the model itself is stateless. For example, suppose we
    are training a model to predict arrival delays, and one of the inputs to the model
    is the departure delay. We might want to include, as an input to the model, the
    average departure delay of flights from that airport in the past two hours.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 即使模型本身是无状态的，如果模型的输入特征需要状态，窗口推理模式也可能很有用。例如，假设我们正在训练一个模型来预测到达延误，而模型的一个输入是出发延误。我们可能希望将过去两小时该机场航班的平均出发延误作为模型的一个输入。
- en: 'During training, we can create the dataset using a SQL window function:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间，我们可以使用 SQL 窗口函数来创建数据集：
- en: '[PRE48]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The training dataset now includes the average delay as just another feature:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据集现在包括平均延误作为另一个特征：
- en: '| Row | arrival_delay | departure_delay | departure_airport | hour_of_day |
    avg_depart_delay |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 行 | 到达延误 | 出发延误 | 出发机场 | 小时 | 平均出发延误 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 1 | -3.0 | -7.0 | LFT | 8 | **-4.0** |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 1 | -3.0 | -7.0 | LFT | 8 | **-4.0** |'
- en: '| 2 | 56.0 | 50.0 | LFT | 8 | **41.0** |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 56.0 | 50.0 | LFT | 8 | **41.0** |'
- en: '| 3 | -14.0 | -9.0 | LFT | 8 | **5.0** |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 3 | -14.0 | -9.0 | LFT | 8 | **5.0** |'
- en: '| 4 | -3.0 | 0.0 | LFT | 8 | **-2.0** |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 4 | -3.0 | 0.0 | LFT | 8 | **-2.0** |'
- en: During inference, though, we will need a streaming pipeline to compute this
    average departure delay so that we can supply it to the model. To limit training–serving
    skew, it is preferable to use the same SQL in a tumbling window function in a
    streaming pipeline, rather than trying to translate the SQL into Scala, Python,
    or Java.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在推理过程中，我们将需要一个流水线来计算这个平均出发延误，以便将其提供给模型。为了限制训练与服务之间的偏差，最好在流水线中使用相同的 SQL 作为一个滚动窗口函数，而不是尝试将
    SQL 翻译成 Scala、Python 或 Java。
- en: Batching prediction requests
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批处理预测请求
- en: 'Another scenario where we might want to use Windowed Inference even if the
    model is stateless is when the model is deployed on the cloud, but the client
    is embedded into a device or on-premises. In such cases, the network latency of
    sending inference requests one by one to a cloud-deployed model might be overwhelming.
    In this situation, [“Design Pattern 19: Two-Phase Predictions”](ch05.xhtml#design_pattern_onenine_two-phase_predic)
    from [Chapter 5](ch05.xhtml#design_patterns_for_resilient_serving) can be used
    where the first phase uses a pipeline to collect a number of requests and the
    second phase sends it to the service in one batch.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '另一种情况是，即使模型是无状态的，我们也可能希望使用窗口推理，例如当模型部署在云端，但客户端嵌入到设备或本地时。在这种情况下，将推理请求逐个发送到云端部署的模型可能会导致网络延迟过大。在这种情况下，可以使用
    [“设计模式 19: 两阶段预测”](ch05.xhtml#design_pattern_onenine_two-phase_predic) 来处理，其中第一阶段使用管道收集一些请求，第二阶段将其作为一个批次发送到服务端。'
- en: This is suitable only for latency-tolerant use cases. If we are collecting input
    instances over five minutes, then the client will have to be tolerant of up to
    five minutes delay in getting back the predictions.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 这仅适用于对延迟具有容忍性的用例。如果我们在五分钟内收集输入实例，那么客户端必须能够容忍获取预测结果的最多五分钟的延迟。
- en: 'Design Pattern 25: Workflow Pipeline'
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '设计模式 25: 工作流管道'
- en: In the Workflow Pipeline design pattern, we address the problem of creating
    an end-to-end reproducible pipeline by containerizing and orchestrating the steps
    in our machine learning process. The containerization might be done explicitly,
    or using a framework that simplifies the process.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在工作流管道设计模式中，我们通过将机器学习流程中的步骤容器化和编排来解决创建端到端可重复的管道的问题。容器化可以明确完成，也可以使用简化该过程的框架来完成。
- en: Problem
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: An individual data scientist may be able to run data preprocessing, training,
    and model deployment steps from end to end (depicted in [Figure 6-6](ch06_split_000.xhtml#the_steps_in_a_typical_end_to_end_ml_wo))
    within a single script or notebook. However, as each step in an ML process becomes
    more complex, and more people within an organization want to contribute to this
    code base, running these steps from a single notebook will not scale.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 一个数据科学家可能能够从头到尾（在 [Figure 6-6](ch06_split_000.xhtml#the_steps_in_a_typical_end_to_end_ml_wo)
    中描绘）在单个脚本或笔记本中运行数据预处理、训练和模型部署步骤。然而，随着机器学习过程中的每个步骤变得更加复杂，并且组织内更多的人希望为这个代码库做出贡献，从单个笔记本运行这些步骤将无法扩展。
- en: '![The steps in a typical end to end ML workflow. This is not meant to be all
    encompassing, but captures the most common steps in the ML development process.](Images/mldp_0606.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![典型端到端ML工作流程中的步骤。这并不是包罗万象，但捕捉了ML开发过程中最常见的步骤。](Images/mldp_0606.png)'
- en: Figure 6-6\. The steps in a typical end-to-end ML workflow. This is not meant
    to be all encompassing, but captures the most common steps in the ML development
    process.
  id: totrans-315
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-6\. 典型端到端ML工作流程中的步骤。这并不是包罗万象，但捕捉了ML开发过程中最常见的步骤。
- en: In traditional programming, *monolithic applications* are described as those
    where all of the application’s logic is handled by a single program. To test a
    small feature in a monolithic app, we must run the entire program. The same goes
    for deploying or debugging monolithic applications. Deploying a small bug fix
    for one piece of the program requires deploying the entire application, which
    can quickly become unwieldy. When the entire codebase is inextricably linked,
    it becomes difficult for individual developers to debug errors and work independently
    on different parts of the application. In recent years, monolithic apps have been
    replaced in favor of a *microservices* architecture where individual pieces of
    business logic are built and deployed as isolated (micro) packages of code. With
    microservices, a large application is split into smaller, more manageable parts
    so that developers can build, debug, and deploy pieces of an application independently.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的编程中，*单体应用程序*被描述为所有应用程序逻辑都由单个程序处理的情况。要在单体应用程序中测试一个小功能，我们必须运行整个程序。部署或调试单体应用程序也是如此。对一个程序的一个小错误修复进行部署需要部署整个应用程序，这很快就会变得笨拙。当整个代码库紧密耦合时，个别开发人员很难调试错误并独立于应用程序的不同部分工作。近年来，单体应用程序已被*微服务*架构取代，其中业务逻辑的各个部分作为独立的（微）代码包构建和部署。通过微服务，大型应用程序被拆分成更小、更易管理的部分，使开发人员可以独立地构建、调试和部署应用程序的各个部分。
- en: 'This monolith-versus-microservice discussion provides a good analogy for scaling
    ML workflows, enabling collaboration, and ensuring ML steps are reproducible and
    reusable across different workflows. When someone is building an ML model on their
    own, a “monolithic” approach may be faster to iterate on. It also often works
    because one person is actively involved in developing and maintaining each piece:
    data gathering and preprocessing, model development, training, and deployment.
    However, when scaling this workflow, different people or groups in an organization
    might be responsible for different steps. To scale the ML workflow, we need a
    way for the team building out the model to run trials independently of the data
    preprocessing step. We’ll also need to track the performance for each step of
    the pipeline and manage the output files generated by each part of the process.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 这个单体与微服务的讨论为扩展ML工作流程、促进协作以及确保ML步骤在不同工作流程中可重复和可重用提供了一个很好的类比。当某人独自构建ML模型时，“单体”方法可能更快地进行迭代。这也常常有效，因为一个人积极参与开发和维护每个部分：数据收集和预处理、模型开发、训练和部署。然而，当扩展此工作流程时，组织中的不同人员或团队可能负责不同的步骤。为了扩展ML工作流程，我们需要一种方法让正在构建模型的团队能够独立于数据预处理步骤运行试验。我们还需要跟踪管道每个步骤的性能，并管理每个过程部分生成的输出文件。
- en: Additionally, when initial development for each step is complete, we’ll want
    to schedule operations like retraining, or create event-triggered pipeline runs
    that are invoked in response to changes in your environment, like new training
    data being added to a bucket. In such cases, it’ll be necessary for the solution
    to allow us to run the entire workflow from end to end in one call while still
    being able to track output and trace errors from individual steps.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当每个步骤的初始开发完成后，我们希望安排像重新训练这样的操作，或者创建在环境更改（例如将新的训练数据添加到存储桶）时触发的事件驱动的流水线运行。在这种情况下，解决方案需要允许我们一次性运行整个工作流程，同时仍能够跟踪单个步骤的输出并追踪错误。
- en: Solution
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: To handle the problems that come with scaling machine learning processes, we
    can make each step in our ML workflow a separate, containerized service. Containers
    guarantee that we’ll be able to run the same code in different environments, and
    that we’ll see consistent behavior between runs. These individual containerized
    steps together are then chained together to make a *pipeline* that can be run
    with a REST API call. Because pipeline steps run in containers, we can run them
    on a development laptop, with on-premises infrastructure, or with a hosted cloud
    service. This pipeline workflow allows team members to build out pipeline steps
    independently. Containers also provide a reproducible way to run an entire pipeline
    end to end, since they guarantee consistency among library dependency versions
    and runtime environments. Additionally, because containerizing pipeline steps
    allows for a separation of concerns, individual steps can use different runtimes
    and language versions.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理机器学习流程的扩展带来的问题，我们可以将 ML 工作流程中的每个步骤制作成单独的容器化服务。容器保证我们能够在不同的环境中运行相同的代码，并且能够在运行之间保持一致的行为。这些单独的容器化步骤联合在一起形成一个*管道*，可以通过
    REST API 调用运行。由于管道步骤在容器中运行，我们可以在开发笔记本、本地基础设施或托管的云服务上运行它们。这种管道工作流程允许团队成员独立构建管道步骤。容器还提供了一种可重现的方式来端到端运行整个管道，因为它们保证了库依赖版本和运行环境之间的一致性。此外，因为容器化管道步骤允许关注点分离，各个步骤可以使用不同的运行时和语言版本。
- en: There are many tools for creating pipelines with both on-premise and cloud options
    available, including [Cloud AI Platform Pipelines](https://oreil.ly/nJo1p), [TensorFlow
    Extended](https://oreil.ly/OznI3) (TFX), [Kubeflow Pipelines](https://oreil.ly/BoegQ)
    (KFP), [MLflow](https://mlflow.org), and [Apache Airflow](https://oreil.ly/63_GG).
    To demonstrate the Workflow Pipeline design pattern here, we’ll define our pipeline
    with TFX and run it on Cloud AI Platform Pipelines, a hosted service for running
    ML pipelines on Google Cloud using Google Kubernetes Engine (GKE) as the underlying
    container infrastructure.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 创建管道的工具有很多选择，包括本地和云端选项，如[云 AI 平台管道](https://oreil.ly/nJo1p)，[TensorFlow Extended](https://oreil.ly/OznI3)（TFX），[Kubeflow
    管道](https://oreil.ly/BoegQ)（KFP），[MLflow](https://mlflow.org)和[Apache Airflow](https://oreil.ly/63_GG)。在这里展示
    Workflow Pipeline 设计模式，我们将使用 TFX 定义我们的管道，并在云 AI 平台管道上运行它，这是一个托管服务，可在 Google 云上使用
    Google Kubernetes Engine（GKE）作为底层容器基础设施来运行 ML 管道。
- en: Steps in TFX pipelines are known as *components*, and both pre-built and customizable
    components are available. Typically, the first component in a TFX pipeline is
    one that ingests data from an external source. This is referred to as an `ExampleGen`
    component where example refers to the machine learning terminology for a labeled
    instance used for training. [`ExampleGen`](https://oreil.ly/Sjx9F) components
    allow you to source data from CSV files, TFRecords, BigQuery, or a custom source.
    The `BigQueryExampleGen` component, for example, lets us connect data stored in
    BigQuery to our pipeline by specifying a query that will fetch the data. Then
    it will store that data as TFRecords in a GCS bucket so that it can be used by
    the next component. This is a component we customize by passing it a query. These
    `ExampleGen` components address the data collection phase of an ML workflow outlined
    in [Figure 6-6](ch06_split_000.xhtml#the_steps_in_a_typical_end_to_end_ml_wo).
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: TFX 管道中的步骤被称为*组件*，提供了预构建和可定制的组件。通常，TFX 管道中的第一个组件用于从外部源接收数据。这称为`ExampleGen`组件，其中“example”是机器学习术语，指用于训练的标记实例。[`ExampleGen`](https://oreil.ly/Sjx9F)组件允许您从
    CSV 文件、TFRecords、BigQuery 或自定义源获取数据。例如，`BigQueryExampleGen`组件允许我们通过指定查询来连接存储在
    BigQuery 中的数据到我们的管道，然后将该数据存储为 TFRecords 在 GCS 存储桶中，以便下一个组件使用。通过传递查询来定制此组件。这些`ExampleGen`组件解决了
    ML 工作流程中数据收集阶段的问题，如 [Figure 6-6](ch06_split_000.xhtml#the_steps_in_a_typical_end_to_end_ml_wo)
    所述。
- en: The next step of this workflow is data validation. Once we’ve ingested data,
    we can pass it to other components for transformation or analysis before training
    a model. The [`StatisticsGen`](https://oreil.ly/kX1QY) component takes data ingested
    from an `ExampleGen` step and generates summary statistics on the provided data.
    The [`SchemaGen`](https://oreil.ly/QpBlu) outputs the inferred schema from our
    ingested data. Utilizing the output of `SchemaGen`, the [`ExampleValidator`](https://oreil.ly/UD7Uh)
    performs anomaly detection on our dataset and checks for signs of data drift or
    potential training–serving skew.^([3](ch06_split_001.xhtml#ch01fn27)) The [`Transform`](https://oreil.ly/xsJYT)
    component also takes output from `SchemaGen` and is where we perform feature engineering
    to transform our data input into the right format for our model. This could include
    converting free-form text inputs into embeddings, normalizing numeric inputs,
    and more.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流程的下一步是数据验证。一旦我们摄入数据，我们可以将其传递给其他组件进行转换或分析，然后再训练模型。[`StatisticsGen`](https://oreil.ly/kX1QY)
    组件接收从`ExampleGen`步骤摄入的数据，并生成所提供数据的摘要统计信息。[`SchemaGen`](https://oreil.ly/QpBlu)
    从我们摄入的数据中输出推断的模式。利用`SchemaGen`的输出，[`ExampleValidator`](https://oreil.ly/UD7Uh)
    对我们的数据集执行异常检测，并检查数据漂移或潜在的训练–服务偏差迹象^([3](ch06_split_001.xhtml#ch01fn27))。[`Transform`](https://oreil.ly/xsJYT)
    组件还从`SchemaGen`输出，并在这里执行特征工程，将我们的数据输入转换为模型所需的正确格式。这可能包括将自由格式文本输入转换为嵌入、标准化数值输入等。
- en: Once our data is ready to be fed into a model, we can pass it to the [`Trainer`](https://oreil.ly/XFtR_)
    component. When we set up our `Trainer` component, we point to a function that
    defines our model code, and we can specify where we’d like to train the model.
    Here, we’ll show how to use Cloud AI Platform Training from this component. Finally,
    the [`Pusher`](https://oreil.ly/qP8GU) component handles model deployment. There
    are [many other](https://oreil.ly/gHv_z) pre-built components provided by TFX—we’ve
    only included a few here that we’ll use in our sample pipeline.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的数据准备好输入模型，我们可以将其传递给[`Trainer`](https://oreil.ly/XFtR_)组件。在设置`Trainer`组件时，我们指定一个定义模型代码的函数，并可以指定在哪里训练模型。在这里，我们将展示如何从这个组件使用
    Cloud AI Platform 进行训练。最后，[`Pusher`](https://oreil.ly/qP8GU)组件处理模型部署。TFX 提供了许多其他预构建组件——我们只在此处包含了一些在示例流水线中使用的组件。
- en: 'For this example, we’ll use the NOAA hurricane dataset in BigQuery to build
    a model that infers the SSHS code^([4](ch06_split_001.xhtml#ch01fn28)) for a hurricane.
    We’ll keep the features, components, and model code relatively short in order
    to focus on the pipeline tooling. The steps of our pipeline are outlined below,
    and roughly follow the workflow outlined in [Figure 6-6](ch06_split_000.xhtml#the_steps_in_a_typical_end_to_end_ml_wo):'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，我们将使用 BigQuery 中的 NOAA 飓风数据集构建一个推断 SSHS 代码^([4](ch06_split_001.xhtml#ch01fn28))
    的模型。为了专注于流水线工具，我们将保持特征、组件和模型代码相对简短。我们的流水线步骤如下所述，大致遵循 [Figure 6-6](ch06_split_000.xhtml#the_steps_in_a_typical_end_to_end_ml_wo)
    中概述的工作流程：
- en: 'Data collection: run a query to get the hurricane data from BigQuery.'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据收集：运行查询从 BigQuery 获取飓风数据。
- en: 'Data validation: use the `ExampleValidator` component to identify anomalies
    and check for data drift.'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据验证：使用`ExampleValidator`组件来识别异常并检查数据漂移。
- en: 'Data analysis and preprocessing: generate some statistics on the data and define
    the schema.'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据分析和预处理：在数据上生成一些统计信息并定义模式。
- en: 'Model training: train a `tf.keras` model on AI Platform.'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型训练：在 AI 平台上训练一个`tf.keras`模型。
- en: 'Model deployment: deploy the trained model to AI Platform Prediction.^([5](ch06_split_001.xhtml#ch01fn29))'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型部署：将训练好的模型部署到 AI 平台预测^([5](ch06_split_001.xhtml#ch01fn29))。
- en: When our pipeline is complete, we’ll be able to invoke the entire process outlined
    above with a single API call. Let’s start by discussing the scaffolding for a
    typical TFX pipeline and the process for running it on AI Platform.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的流水线完成时，我们将能够通过单个 API 调用来调用上述整个流程。让我们首先讨论典型 TFX 流水线的脚手架和在 AI 平台上运行它的过程。
- en: Building the TFX pipeline
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建 TFX 流水线。
- en: 'We’ll use the `tfx` command-line tools to create and invoke our pipeline. New
    invocations of a pipeline are known as *runs*, which are distinct from updates
    we make to the pipeline itself, like adding a new component. We can do both with
    the TFX CLI. We can define the scaffolding for our pipeline in a single Python
    script, which has two key parts:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `tfx` 命令行工具来创建和调用我们的管道。管道的新调用称为 *运行*，与我们对管道本身的更新不同，例如添加新组件。TFX CLI 可以完成这两种操作。我们可以在一个单独的
    Python 脚本中定义我们管道的支架，这个脚本有两个关键部分：
- en: An instance of [tfx.orchestration.pipeline](https://oreil.ly/62kf3) where we
    define our pipeline and the components it includes.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个实例化的 [tfx.orchestration.pipeline](https://oreil.ly/62kf3)，在其中我们定义我们的管道及其包含的组件。
- en: An instance of [kubeflow_dag_runner](https://oreil.ly/62kf3) from the [tfx](https://oreil.ly/62kf3)
    library. We’ll use this to create and run our pipeline. In addition to the Kubeflow
    runner, there’s also an API for running TFX pipelines with [Apache Beam](https://oreil.ly/hn0vF),
    which we could use to run our pipeline locally.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个来自 [tfx](https://oreil.ly/62kf3) 库的 [kubeflow_dag_runner](https://oreil.ly/62kf3)
    实例。我们将使用它来创建和运行我们的管道。除了 Kubeflow runner 外，还有一个用于使用 [Apache Beam](https://oreil.ly/hn0vF)
    在本地运行 TFX 管道的 API。
- en: 'Our pipeline (see [full code in GitHub](https://github.com/GoogleCloudPlatform/ml-design-patterns/tree/master/06_reproducibility/workflow_pipeline))
    will have the five steps or components defined above, and we can define our pipeline
    with the following:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的管道（参见 [GitHub 上的完整代码](https://github.com/GoogleCloudPlatform/ml-design-patterns/tree/master/06_reproducibility/workflow_pipeline)）将包括上述五个步骤或组件，我们可以通过以下方式定义我们的管道：
- en: '[PRE49]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'To use the `BigQueryExampleGen` component provided by TFX, we provide the query
    that will fetch our data. We can define this component in one line of code, where
    `query` is our BigQuery SQL query as a string:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 TFX 提供的 `BigQueryExampleGen` 组件，我们提供将获取数据的查询。我们可以在一行代码中定义这个组件，其中 `query`
    是我们的 BigQuery SQL 查询字符串：
- en: '[PRE50]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Another benefit of using pipelines is that it provides tooling to keep track
    of the input, output artifacts, and logs for each component. The output of the
    `statistics_gen` component, for example, is a summary of our dataset, which we
    can see in [Figure 6-7](ch06_split_001.xhtml#the_output_artifact_from_the_statistics).
    [`statistics_gen`](https://oreil.ly/wvq9n) is a pre-built component available
    in TFX that uses TF Data Validation to generate summary statistics on our dataset.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 使用管道的另一个好处是它提供了工具来跟踪每个组件的输入、输出结果和日志。例如，`statistics_gen` 组件的输出是我们数据集的摘要，我们可以在
    [图 6-7](ch06_split_001.xhtml#the_output_artifact_from_the_statistics) 中看到这些内容。[`statistics_gen`](https://oreil.ly/wvq9n)
    是 TFX 中的一个预构建组件，使用 TF Data Validation 生成我们数据集的摘要统计信息。
- en: '![The output artifact from the statistics_gen component in a TFX pipeline.](Images/mldp_0607.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![TFX 管道中 statistics_gen 组件的输出结果。](Images/mldp_0607.png)'
- en: Figure 6-7\. The output artifact from the statistics_gen component in a TFX
    pipeline.
  id: totrans-342
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-7\. TFX 管道中 statistics_gen 组件的输出结果。
- en: Running the pipeline on Cloud AI Platform
  id: totrans-343
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Cloud AI Platform 上运行管道
- en: 'We can run the TFX pipeline on Cloud AI Platform Pipelines, which will manage
    low-level details of the infrastructure for us. To deploy a pipeline to AI Platform,
    we package our pipeline code as a [Docker container](https://oreil.ly/rdXeb) and
    host it on [Google Container Registry](https://oreil.ly/m5wqD) (GCR).^([6](ch06_split_001.xhtml#ch01fn30))
    Once our containerized pipeline code has been pushed to GCR, we’ll create the
    pipeline using the TFX CLI:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 Cloud AI Platform Pipelines 上运行 TFX 管道，这将为我们管理基础设施的底层细节。要将管道部署到 AI Platform，我们将我们的管道代码打包为一个
    [Docker 容器](https://oreil.ly/rdXeb)，并将其托管在 [Google Container Registry](https://oreil.ly/m5wqD)（GCR）。^([6](ch06_split_001.xhtml#ch01fn30))
    一旦我们的容器化管道代码已推送到 GCR，我们将使用 TFX CLI 创建该管道：
- en: '[PRE51]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'In the command above, endpoint corresponds with the URL of our AI Platform
    Pipelines dashboard. When that completes, we’ll see the pipeline we just created
    in our pipelines dashboard. The `create` command creates a pipeline *resource*
    that we can invoke by creating a run:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述命令中，endpoint 对应着 AI Platform Pipelines 仪表板的 URL。完成后，我们将在管道仪表板中看到我们刚刚创建的管道。`create`
    命令创建一个可以通过创建运行来调用的管道 *资源*：
- en: '[PRE52]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: After running this command, we’ll be able to see a graph that updates in real
    time as our pipeline moves through each step. From the Pipelines dashboard, we
    can further examine individual steps to see any artifacts they generate, metadata,
    and more. We can see an example of the output for an individual step in [Figure 6-8](ch06_split_001.xhtml#output_of_the_schema_gen_component_for).
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此命令后，我们将能够看到一个图表，实时更新我们的管道通过每个步骤的情况。从管道仪表板，我们可以进一步检查单个步骤生成的任何工件、元数据等。我们可以在[图 6-8](ch06_split_001.xhtml#output_of_the_schema_gen_component_for)中看到一个单个步骤的输出示例。
- en: We could train our model directly in our containerized pipeline on GKE, but
    TFX provides a utility for using Cloud AI Platform Training as part of our process.
    TFX also has an extension for deploying our trained model to AI Platform Prediction.
    We’ll utilize both of these integrations in our pipeline. AI Platform Training
    lets us take advantage of specialized hardware for training our models, such as
    GPUs or TPUs, in a cost-effective way. It also provides an option to use distributed
    training, which can accelerate training time and minimize training cost. We can
    track individual training jobs and their output within the AI Platform console.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接在GKE上的容器化管道中训练我们的模型，但是TFX提供了一个实用程序，用于将Cloud AI平台训练作为我们过程的一部分。TFX还有一个扩展，用于将我们训练好的模型部署到AI平台预测中。我们将在我们的管道中利用这两个集成。AI平台训练使我们能够以成本效益的方式利用专门的硬件来训练我们的模型，例如GPU或TPU。它还提供了使用分布式训练的选项，这可以加快训练时间并减少训练成本。我们可以在AI平台控制台内跟踪单个训练作业及其输出。
- en: '![Output of the schema_gen component for an ML pipeline. The top menu bar shows
    the data available for each individual pipeline step.](Images/mldp_0608.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![ML管道中schema_gen组件的输出。顶部菜单栏显示每个单独管道步骤可用的数据。](Images/mldp_0608.png)'
- en: Figure 6-8\. Output of the schema_gen component for an ML pipeline. The top
    menu bar shows the data available for each individual pipeline step.
  id: totrans-351
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-8\. ML管道中schema_gen组件的输出。顶部菜单栏显示每个单独管道步骤可用的数据。
- en: Tip
  id: totrans-352
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: One advantage of building a pipeline with TFX or Kubeflow Pipelines is that
    we are not locked into Google Cloud. We can run the same code we’re demonstrating
    here with Google’s AI Platform Pipelines on [Azure ML Pipelines](https://oreil.ly/A5Rxe),
    [Amazon SageMaker](https://oreil.ly/H3p3Y), or on-premises.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TFX或Kubeflow Pipelines构建管道的一个优点是，我们不会被锁定在Google Cloud上。我们可以在[Azure ML Pipelines](https://oreil.ly/A5Rxe)、[Amazon
    SageMaker](https://oreil.ly/H3p3Y)或本地环境上运行我们在这里演示的相同代码。
- en: To implement a training step in TFX, we’ll use the [`Trainer` component](https://oreil.ly/TGKcP)
    and pass it information on the training data to use as model input, along with
    our model training code. TFX provides an extension for running the training step
    on AI Platform that we can use by importing `tfx.extensions.google_cloud_ai_platform.trainer`
    and providing details on our AI Platform training configuration. This includes
    our project name, region, and GCR location of the container with training code.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在TFX中实现训练步骤，我们将使用[`Trainer`组件](https://oreil.ly/TGKcP)，并传递关于要用作模型输入的训练数据以及我们的模型训练代码的信息。TFX提供了在AI平台上运行训练步骤的扩展，我们可以通过导入`tfx.extensions.google_cloud_ai_platform.trainer`并提供有关我们的AI平台训练配置的详细信息来使用它。这包括我们的项目名称、地区以及包含训练代码的容器的GCR位置。
- en: Similarly, TFX also has an AI Platform [`Pusher`](https://oreil.ly/wS6lc) [component](https://oreil.ly/bJavO)
    for deploying trained models to AI Platform Prediction. In order to use the `Pusher`
    component with AI Platform, we provide details on the name and version of our
    model, along with a serving function that tells AI Platform the format of input
    data it should expect for our model. With that, we have a complete pipeline that
    ingests data, analyzes it, runs data transformation, and finally trains and deploys
    the model using AI Platform.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，TFX还有一个AI平台[`Pusher`](https://oreil.ly/wS6lc) [组件](https://oreil.ly/bJavO)
    用于将训练好的模型部署到AI平台预测中。为了在AI平台上使用`Pusher`组件，我们提供有关我们模型的名称和版本的详细信息，以及一个服务函数，告诉AI平台它应该期望我们模型的输入数据格式。有了这些，我们就可以通过AI平台完成一个完整的管道，该管道摄取数据、分析数据、运行数据转换，最终使用AI平台进行模型训练和部署。
- en: Why It Works
  id: totrans-356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么它有效
- en: Without running our ML code as a pipeline, it would be difficult for others
    to reliably reproduce our work. They’d need to take our preprocessing, model development,
    training, and serving code and try to replicate the same environment where we
    ran it while taking into account library dependencies, authentication, and more.
    If there is logic controlling the selection of downstream components based on
    the output of upstream components, that logic will also have to be reliably replicated.
    The Workflow Pipeline design pattern lets others run and monitor our entire ML
    workflow from end to end in both on-premises and cloud environments, while still
    being able to debug the output of individual steps. Containerizing each step of
    the pipeline ensures that others will be able to reproduce both the environment
    we used to build it and the entire workflow captured in the pipeline. This also
    allows us to potentially reproduce the environment months later to support regulatory
    needs. With TFX and AI Platform Pipelines, the dashboard also gives us a UI for
    tracking the output artifacts produced from every pipeline execution. This is
    discussed further in [“Trade-Offs and Alternatives”](ch06_split_001.xhtml#trade-offs_and_alternatives-id00213).
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 不将我们的ML代码作为一个流水线运行，其他人可靠地复现我们的工作将会很困难。他们需要使用我们的预处理、模型开发、训练和服务代码，并尝试复制我们运行它的相同环境，同时考虑库依赖、认证等因素。如果有逻辑控制基于上游组件输出选择下游组件的逻辑，这个逻辑也必须能够可靠地复制。工作流程管道设计模式允许其他人在本地和云环境中运行和监控我们整个ML工作流程的端到端，并且能够调试单个步骤的输出。将管道的每个步骤容器化确保其他人能够复现我们用来构建它和在管道中捕获的整个工作流的环境。这也允许我们在数月后重现环境以支持监管需求。使用TFX和AI平台管道，仪表板还为我们提供了一个UI，用于跟踪每个管道执行生成的输出艺术品。这在[“权衡与替代方案”](ch06_split_001.xhtml#trade-offs_and_alternatives-id00213)中进一步讨论。
- en: Additionally, with each pipeline component in its own container, different team
    members can build and test separate pieces of a pipeline in parallel. This allows
    for faster development and minimizes the risks associated with a more monolithic
    ML process where steps are inextricably linked to one another. The package dependencies
    and code required to build out the data preprocessing step, for example, may be
    significantly different than those for model deployment. By building these steps
    as part of a pipeline, each piece can be built in a separate container with its
    own dependencies and incorporated into a larger pipeline when completed.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，每个管道组件在自己的容器中，不同团队成员可以并行构建和测试管道的不同部分。这允许更快的开发，并最小化与更单片化ML流程相关的风险，其中步骤彼此不可分割地联系在一起。例如，构建数据预处理步骤所需的包依赖和代码可能与模型部署的不同。通过将这些步骤作为管道的一部分构建，每个部分都可以在单独的容器中构建，具有自己的依赖项，并在完成后整合到更大的管道中。
- en: To summarize, the Workflow Pipeline pattern gives us the benefits that come
    with a directed acyclic graph (DAG), along with the pre-built components that
    come with pipeline frameworks like TFX. Because the pipeline is a DAG, we have
    the option of executing individual steps or running an entire pipeline from end
    to end. This also gives us logging and monitoring for each step of the pipeline
    across different runs, and allows for tracking artifacts from each step and pipeline
    execution in a centralized place. Pre-built components provide standalone, ready-to-use
    steps for common components of ML workflows, including training, evaluation, and
    inference. These components run as individual containers wherever we choose to
    run our pipeline.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，工作流管道模式为我们带来了有向无环图（DAG）的优势，以及像TFX这样的管道框架提供的预构建组件。因为管道是一个DAG，我们可以选择执行单个步骤或从头到尾运行整个管道。这还为我们提供了对管道的每个步骤在不同运行中的日志记录和监控，并允许在一个集中的地方跟踪每个步骤和管道执行的艺术品。预构建组件为ML工作流程的常见组件（包括训练、评估和推断）提供了独立、即用即用的步骤。这些组件作为单独的容器运行，无论我们选择在哪里运行我们的管道。
- en: Trade-Offs and Alternatives
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 权衡与替代方案
- en: 'The main alternative to using a pipeline framework is to run the steps of our
    ML workflow using a makeshift approach for keeping track of the notebooks and
    output associated with each step. Of course, there is some overhead involved in
    converting the different pieces of our ML workflow into an organized pipeline.
    In this section, we’ll look at some variations and extensions of the Workflow
    Pipeline design pattern: creating containers manually, automating a pipeline with
    tools for continuous integration and continuous delivery (CI/CD), processes for
    moving from a development to production workflow pipeline, and alternative tools
    for building and orchestrating pipelines. We’ll also explore how to use pipelines
    for metadata tracking.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 使用管道框架的主要替代方案是使用临时方法来运行我们的 ML 工作流的步骤，并为每个步骤跟踪笔记本和相关输出。当然，在将我们的 ML 工作流的不同部分转换为组织良好的管道时会涉及一些开销。在本节中，我们将探讨
    Workflow Pipeline 设计模式的一些变体和扩展：手动创建容器、使用持续集成和持续交付（CI/CD）工具自动化管道、从开发到生产工作流管道的流程以及构建和编排管道的替代工具。我们还将探讨如何使用管道进行元数据跟踪。
- en: Creating custom components
  id: totrans-362
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建自定义组件
- en: Instead of using pre-built or customizable TFX components to construct our pipeline,
    we can define our own containers to use as components, or convert a Python function
    to a component.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以定义自己的容器作为组件来构建我们的管道，而不是使用预构建或可自定义的 TFX 组件，或者将 Python 函数转换为组件。
- en: 'To use the [container-based components](https://oreil.ly/5ryEn) provided by
    TFX, we use the `create_container_component` method, passing it the inputs and
    outputs for our component and a base Docker image along with any entrypoint commands
    for the container. For example, the following container-based component invokes
    the command-line tool `bq` to download a BigQuery dataset:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用由 TFX 提供的基于容器的组件，我们使用`create_container_component`方法，向其传递我们组件的输入和输出，以及基础 Docker
    镜像和容器的任何入口点命令。例如，以下基于容器的组件调用命令行工具`bq`下载 BigQuery 数据集：
- en: '[PRE53]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: It’s best to use a base image that already has most of the dependencies we need.
    We’re using the Google Cloud SDK image, which provides us the `bq` command-line
    tool.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 最好使用已包含大部分所需依赖项的基础镜像。我们正在使用 Google Cloud SDK 镜像，该镜像提供了`bq`命令行工具。
- en: 'It is also possible to convert a custom Python function into a TFX component
    using the `@component` decorator. To demonstrate it, let’s say we have a step
    for preparing resources used throughout our pipeline where we create a Cloud Storage
    bucket. We can define this custom step using the following code:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以使用`@component`装饰器将自定义 Python 函数转换为 TFX 组件。为了演示它，假设我们有一个用于准备整个管道中使用的资源的步骤，例如创建一个
    Cloud Storage 存储桶。我们可以使用以下代码定义此自定义步骤：
- en: '[PRE54]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We can then add this component to our pipeline definition:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将此组件添加到我们的管道定义中：
- en: '[PRE55]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Integrating CI/CD with pipelines
  id: totrans-371
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 CI/CD 集成到管道中
- en: In addition to invoking pipelines via the dashboard or programmatically via
    the CLI or API, chances are we’ll want to automate runs of our pipeline as we
    productionize the model. For example, we may want to invoke our pipeline whenever
    a certain amount of new training data is available. Or we might want to trigger
    a pipeline run when the source code for the pipeline changes. Adding CI/CD to
    our Workflow Pipeline can help connect trigger events to pipeline runs.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通过仪表板或通过 CLI 或 API 编程方式调用管道外，我们可能希望在将模型投入生产时自动运行我们的管道。例如，当有一定量的新训练数据可用时，我们可能希望调用我们的管道。或者在管道的源代码更改时，我们可能希望触发管道运行。将
    CI/CD 集成到我们的工作流管道中可以帮助连接触发事件和管道运行。
- en: There are many managed services available for setting up triggers to run a pipeline
    when we want to retrain a model on new data. We could use a managed scheduling
    service to invoke our pipeline on a schedule. Alternatively, we could use a serverless
    event-based service like [Cloud Functions](https://oreil.ly/rVyzX) to invoke our
    pipeline when new data is added to a storage location. In our function, we could
    specify conditions—like a threshold for the amount of new data added to necessitate
    retraining—for creating a new pipeline run. Once enough new training data is available,
    we can instantiate a pipeline run for retraining and redeploying the model as
    demonstrated in [Figure 6-9](ch06_split_001.xhtml#a_cisoliduscd_workflow_using_cloud_func).
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多托管服务可用于设置触发器，以在我们需要基于新数据重新训练模型时运行流水线。我们可以使用托管调度服务按计划调用我们的流水线。或者，我们可以使用像 [Cloud
    Functions](https://oreil.ly/rVyzX) 这样的无服务器事件驱动服务，在存储位置添加新数据时调用我们的流水线。在我们的函数中，我们可以指定条件，例如添加新数据的阈值，以创建新的流水线运行。一旦有足够的新训练数据可用，我们就可以实例化一个流水线运行来进行重新训练和重新部署模型，如
    [Figure 6-9](ch06_split_001.xhtml#a_cisoliduscd_workflow_using_cloud_func) 所示。
- en: '![A CI/CD workflow using Cloud Functions to invoke a pipeline when enough new
    data is added to a storage location.](Images/mldp_0609.png)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
  zh: '![使用云函数调用流水线的 CI/CD 工作流，当存储位置新增足够的新数据时。](Images/mldp_0609.png)'
- en: Figure 6-9\. A CI/CD workflow using Cloud Functions to invoke a pipeline when
    enough new data is added to a storage location.
  id: totrans-375
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-9\. 使用云函数调用流水线的 CI/CD 工作流，当存储位置新增足够的新数据时。
- en: If we want to trigger our pipeline based on changes to source code, a managed
    CI/CD service like [Cloud Build](https://oreil.ly/kz8Aa) can help. When Cloud
    Build executes our code, it is run as a series of containerized steps. This approach
    fits well within the context of pipelines. We can connect Cloud Build to [GitHub
    Actions](https://oreil.ly/G2Xwv) or [GitLab Triggers](https://oreil.ly/m_dYr)
    on the repository where our pipeline code is located. When the code is committed,
    Cloud Build will then build the containers associated with our pipeline based
    on the new code and create a run.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望基于源代码更改触发我们的流水线，像 [Cloud Build](https://oreil.ly/kz8Aa) 这样的托管 CI/CD 服务可以帮助。当
    Cloud Build 执行我们的代码时，它作为一系列容器化步骤运行。这种方法很适合在流水线的上下文中使用。我们可以将 Cloud Build 连接到我们的流水线代码所在的
    GitHub Actions 或 GitLab Triggers。当代码提交时，Cloud Build 将基于新代码构建与我们的流水线相关联的容器，并创建一个运行。
- en: Apache Airflow and Kubeflow Pipelines
  id: totrans-377
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Apache Airflow 和 Kubeflow Pipelines
- en: In addition to TFX, [Apache Airflow](https://oreil.ly/rQlqK) and [Kubeflow Pipelines](https://oreil.ly/e_7zJ)
    are both alternatives for implementing the Workflow Pipeline pattern. Like TFX,
    both Airflow and KFP treat pipelines as a DAG where the workflow for each step
    is defined in a Python script. They then take this script and provide APIs to
    handle scheduling and orchestrating the graph on the specified infrastructure.
    Both Airflow and KFP are open source and can therefore run on-premises or in the
    cloud.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 TFX，[Apache Airflow](https://oreil.ly/rQlqK) 和 [Kubeflow Pipelines](https://oreil.ly/e_7zJ)
    都是实现工作流程管道模式的替代方案。与 TFX 一样，Airflow 和 KFP 都将流水线视为 DAG，其中每个步骤的工作流由 Python 脚本定义。然后，它们采用此脚本并提供
    API 来处理调度并在指定的基础设施上编排图形。Airflow 和 KFP 都是开源的，因此可以在本地或云端运行。
- en: It’s common to use Airflow for data engineering, so it’s worth considering for
    an organization’s data ETL tasks. However, while Airflow provides robust tooling
    for running jobs, it was built as a general-purpose solution and wasn’t designed
    with ML workloads in mind. KFP, on the other hand, was designed specifically for
    ML and operates at a lower level than TFX, providing more flexibility in how pipeline
    steps are defined. While TFX implements its own approach to orchestration, KFP
    lets us choose how to orchestrate our pipelines through its API. The relationship
    between TFX, KFP, and Kubeflow is summarized in [Figure 6-10](ch06_split_001.xhtml#the_relationship_between_tfxcomma_kubef).
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow 在数据工程中被广泛使用，因此对组织的数据 ETL 任务值得考虑。然而，虽然 Airflow 提供了强大的工具来运行作业，但它是作为通用解决方案构建的，并未考虑
    ML 工作负载。另一方面，KFP 是专为 ML 设计的，并且在比 TFX 更低的层次上操作，提供了更多在如何定义流水线步骤上的灵活性。虽然 TFX 实现了其自己的编排方式，但
    KFP 允许我们通过其 API 选择如何编排我们的流水线。TFX、KFP 和 Kubeflow 之间的关系总结在 [Figure 6-10](ch06_split_001.xhtml#the_relationship_between_tfxcomma_kubef)
    中。
- en: '![The relationship between TFX, Kubeflow Pipelines, Kubeflow, and underlying
    infrastructure. TFX operates at the highest level on top of Kubeflow Pipelines,
    with pre-built components offering specific approaches to common workflow steps.
    Kubeflow Pipelines provides an API for defining and orchestrating a ML pipeline,
    providing more flexibility in how each step is implemented. Both TFX and KFP run
    on Kubeflow, a platform for running container-based ML workloads on Kubernetes.
    All of the tools in this diagram are open source, so the underlying infrastructure
    where pipelines run is up to the user—some options include GKE, Anthos, Azure,
    AWS, or on-premises.](Images/mldp_0610.png)'
  id: totrans-380
  prefs: []
  type: TYPE_IMG
  zh: '![TFX、Kubeflow Pipelines、Kubeflow 和基础架构之间的关系。TFX 在 Kubeflow Pipelines 的顶层运行，具有预构建组件，提供常见工作流步骤的特定方法。Kubeflow
    Pipelines 提供 API 用于定义和编排 ML 管道，提供更灵活的实现每个步骤的方式。TFX 和 KFP 都在 Kubeflow 上运行，这是一个在
    Kubernetes 上运行基于容器的 ML 工作负载的平台。这个图中的所有工具都是开源的，因此管道运行的基础架构由用户决定，一些选项包括 GKE、Anthos、Azure、AWS
    或本地部署。](Images/mldp_0610.png)'
- en: Figure 6-10\. The relationship between TFX, Kubeflow Pipelines, Kubeflow, and
    underlying infrastructure. TFX operates at the highest level on top of Kubeflow
    Pipelines, with pre-built components offering specific approaches to common workflow
    steps. Kubeflow Pipelines provides an API for defining and orchestrating an ML
    pipeline, providing more flexibility in how each step is implemented. Both TFX
    and KFP run on Kubeflow, a platform for running container-based ML workloads on
    Kubernetes. All of the tools in this diagram are open source, so the underlying
    infrastructure where pipelines run is up to the user—some options include GKE,
    Anthos, Azure, AWS, or on-premises.
  id: totrans-381
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-10\. TFX、Kubeflow Pipelines、Kubeflow 和基础架构之间的关系。TFX 在 Kubeflow Pipelines
    的顶层运行，具有预构建组件，提供常见工作流步骤的特定方法。Kubeflow Pipelines 提供 API 用于定义和编排 ML 管道，提供更灵活的实现每个步骤的方式。TFX
    和 KFP 都在 Kubeflow 上运行，这是一个在 Kubernetes 上运行基于容器的 ML 工作负载的平台。这个图中的所有工具都是开源的，因此管道运行的基础架构由用户决定，一些选项包括
    GKE、Anthos、Azure、AWS 或本地部署。
- en: Development versus production pipelines
  id: totrans-382
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开发与生产管道
- en: The way a pipeline is invoked often changes as we move from development to production.
    We’ll likely want to build and prototype our pipeline from a notebook, where we
    can re-invoke our pipeline by running a notebook cell, debug errors, and update
    code all from the same environment. Once we’re ready to productionize, we can
    move our component code and pipeline definition to a single script. With our pipeline
    defined in a script, we’ll be able to schedule runs and make it easier for others
    in our organization to invoke the pipeline in a reproducible way. One tool available
    for productionizing pipelines is [Kale](https://github.com/kubeflow-kale/kale),
    which takes Jupyter notebook code and converts it into a script using the Kubeflow
    Pipelines API.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 从开发到生产，管道调用的方式通常会发生变化。我们可能希望从笔记本构建和原型化我们的管道，在那里我们可以通过运行笔记本单元重新调用我们的管道，调试错误，并从同一环境中更新代码。一旦准备好投入生产，我们可以将组件代码和管道定义移动到一个单一脚本中。在脚本中定义了我们的管道之后，我们可以安排运行，并使组织内的其他人以可重复的方式调用管道变得更加容易。用于将管道投入生产的工具之一是
    [Kale](https://github.com/kubeflow-kale/kale)，它使用 Kubeflow Pipelines API 将 Jupyter
    笔记本代码转换为脚本。
- en: A production pipeline also allows for *orchestration* of an ML workflow. By
    orchestration, we mean adding logic to our pipeline to determine which steps will
    be executed, and what the outcome of those steps will be. For example, we might
    decide we only want to deploy models to production that have 95% accuracy or higher.
    When newly available data triggers a pipeline run and trains an updated model,
    we can add logic to check the output of our evaluation component to execute the
    deployment component if the accuracy is above our threshold, or end the pipeline
    run if not. Both Airflow and Kubeflow Pipelines, discussed previously in this
    section, provide APIs for pipeline orchestration.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 生产管道还允许对 ML 工作流进行*编排*。编排意味着在我们的管道中添加逻辑以确定将执行哪些步骤，以及这些步骤的结果将是什么。例如，我们可能决定只想将准确率达到
    95% 或更高的模型部署到生产环境中。当新可用数据触发管道运行并训练更新的模型时，我们可以添加逻辑以检查我们评估组件的输出，如果准确度超过我们的阈值，则执行部署组件，否则结束管道运行。在本节前面讨论过的
    Airflow 和 Kubeflow Pipelines 都提供了管道编排的 API。
- en: Lineage tracking in ML pipelines
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ML 管道中的谱系跟踪
- en: One additional feature of pipelines is using them for tracking model metadata
    and artifacts, also known as *lineage tracking*. Each time we invoke a pipeline,
    a series of artifacts is generated. These artifacts could include dataset summaries,
    exported models, model evaluation results, metadata on specific pipeline invocations,
    and more. Lineage tracking lets us visualize the history of our model versions
    along with other associated model artifacts. In AI Platform Pipelines, for example,
    we can use the pipelines dashboard to see which data a model version was trained
    on, broken down both by data schema and date. [Figure 6-11](ch06_split_001.xhtml#the_lineage_explorer_section_of_the_ai)
    shows the Lineage Explorer dashboard for a TFX pipeline running on AI Platform.
    This allows us to track the input and output artifacts associated with a particular
    model.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 管道的另一个功能是使用它们来跟踪模型元数据和工件，也称为*血统跟踪*。每次我们调用管道时，都会生成一系列工件。这些工件可能包括数据集摘要、导出的模型、模型评估结果、特定管道调用的元数据等。血统跟踪允许我们可视化我们的模型版本历史及其他相关模型工件。例如，在
    AI 平台管道中，我们可以使用管道仪表板查看特定模型版本是在哪些数据上进行训练的，可以按数据模式和日期进行分解。[图 6-11](ch06_split_001.xhtml#the_lineage_explorer_section_of_the_ai)
    展示了 AI 平台上运行的 TFX 管道的 Lineage Explorer 仪表板。这使我们能够追踪与特定模型相关的输入和输出工件。
- en: '![The Lineage Explorer section of the AI Platform Pipelines dashboard for a
    TFX pipeline.](Images/mldp_0611.png)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![AI 平台管道的 TFX 管道的 Lineage Explorer 部分。](Images/mldp_0611.png)'
- en: Figure 6-11\. The Lineage Explorer section of the AI Platform Pipelines dashboard
    for a TFX pipeline.
  id: totrans-388
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-11\. AI 平台管道的 TFX 管道的 Lineage Explorer 部分。
- en: One benefit of using lineage tracking to manage artifacts generated during our
    pipeline run is that it supports both cloud-based and on-premises environments.
    This gives us flexibility in where models are trained and deployed, and where
    model metadata is stored. Lineage tracking is also an important aspect of making
    ML pipelines reproducible, since it allows for comparisons between metadata and
    artifacts from different pipeline runs.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 使用血统跟踪管理管道运行期间生成的工件的一个好处是，它支持云端和本地环境。这为我们在模型训练和部署的位置以及模型元数据存储的位置之间提供了灵活性。血统跟踪也是使
    ML 管道可复制的重要方面，因为它允许比较不同管道运行生成的元数据和工件。
- en: 'Design Pattern 26: Feature Store'
  id: totrans-390
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '设计模式 26: 特征存储'
- en: The *Feature Store* design pattern simplifies the management and reuse of features
    across projects by decoupling the feature creation process from the development
    of models using those features.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '*特征存储* 设计模式通过将特征创建过程与使用这些特征的模型开发过程解耦，简化了跨项目管理和重用特征的过程。'
- en: Problem
  id: totrans-392
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Good feature engineering is crucial for the success of many machine learning
    solutions. However, it is also one of the most time-consuming parts of model development.
    Some features require significant domain knowledge to calculate correctly, and
    changes in the business strategy can affect how a feature should be computed.
    To ensure such features are computed in a consistent way, it’s better for these
    features to be under the control of domain experts rather than ML engineers. Some
    input fields might allow for different choices of data representations (see [Chapter 2](ch02.xhtml#data_representation_design_patterns))
    to make them more amenable for machine learning. An ML engineer or data scientist
    will typically experiment with multiple different transformations to determine
    which are helpful and which aren’t, before deciding which features will be used
    in the final model. Many times, the data used for the ML model isn’t drawn from
    a single source. Some data may come from a data warehouse, some data may sit in
    a storage bucket as unstructured data, and other data may be collected in real
    time through streaming. The structure of the data may also vary between each of
    these sources, requiring each input to have its own feature engineering steps
    before it can be fed into a model. This development is often done on a VM or personal
    machine, causing the feature creation to be tied to the software environment where
    the model is built, and the more complex the model gets, the more complicated
    these data pipelines become.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 良好的特征工程对于许多机器学习解决方案的成功至关重要。然而，它也是模型开发中最耗时的部分之一。有些特征需要大量领域知识才能正确计算，而业务策略的变化可能会影响特征的计算方式。为了确保这些特征能够以一致的方式计算，最好由领域专家而不是机器学习工程师控制这些特征。一些输入字段可能允许采用不同的数据表示方式（见[第2章](ch02.xhtml#data_representation_design_patterns)），使其更适合机器学习。机器学习工程师或数据科学家通常会尝试多种不同的转换方法，以确定哪些是有帮助的，哪些不是，在决定最终模型中将使用哪些特征之前。许多情况下，用于机器学习模型的数据并非来自单一来源。某些数据可能来自数据仓库，某些数据可能作为非结构化数据存储在存储桶中，而其他数据可能通过流式传输实时收集。这些数据的结构也可能在各个来源之间有所不同，因此在将其输入模型之前，每个输入都需要进行自己的特征工程步骤。这种开发通常在虚拟机或个人机器上进行，导致特征创建与构建模型的软件环境紧密相关，而模型变得越复杂，这些数据管道就变得越复杂。
- en: 'An ad hoc approach where features are created as needed by ML projects may
    work for one-off model development and training, but as organizations scale, this
    method of feature engineering becomes impractical and significant problems arise:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 针对单次模型开发和训练，可以采用按需创建特征的临时方法，但随着组织规模扩大，这种特征工程方法变得不切实际，会带来重大问题：
- en: Ad hoc features aren’t easily reused. Features are re-created over and over
    again, either by individual users or within teams, or never leave the pipelines
    (or notebooks) in which they are created. This is particularly problematic for
    higher-level features that are complex to calculate. This could be because they
    are derived through expensive processes, such as pre-trained user or catalog item
    embeddings. Other times, it could be because the features are captured from upstream
    processes such as business priorities, availability of contracting, or market
    segmentations. Another source of complexity is when higher-level features, such
    as the number of orders by a customer in the past month, involve aggregations
    over time. Effort and time are wasted creating the same features from scratch
    for each new project.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 临时特征不容易重复使用。特征一遍又一遍地被重新创建，无论是由个别用户还是团队内部，或者永远停留在创建它们的管道（或笔记本电脑）中。这对于复杂计算的高级特征尤为棘手。这可能是因为它们通过昂贵的过程派生，比如预训练用户或目录项嵌入。其他时候，它可能是因为特征从业务优先事项、合同可用性或市场细分等上游过程中捕获。当高级特征，如客户过去一个月的订单数量，涉及到随时间的聚合时，也会引入复杂性。为每个新项目从头开始创建相同的特征，会浪费大量的努力和时间。
- en: Data governance is made difficult if each ML project computes features from
    sensitive data differently.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据治理如果每个机器学习项目都不同地从敏感数据计算特征，会变得困难。
- en: Ad hoc features aren’t easily shared between teams or across projects. In many
    organizations, the same raw data is used by multiple teams, but separate teams
    may define features differently and there is no easy access to feature documentation.
    This also hinders effective cross-collaboration of teams, leading to siloed work
    and unnecessarily duplicated effort.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 临时特征不容易在团队之间或跨项目共享。在许多组织中，多个团队使用相同的原始数据，但不同团队可能会以不同方式定义特征，并且没有易于访问的特征文档。这也阻碍了团队之间有效的跨部门合作，导致工作被隔离并且存在不必要的重复劳动。
- en: Ad hoc features used for training and serving are inconsistent—i.e., training–serving
    skew. Training is typically done using historical data with batch features that
    are created offline. However, serving is typically carried out online. If the
    feature pipeline for training differs at all from the pipeline used in production
    for serving (for example, different libraries, preprocessing code, or languages),
    then we run the risk of training–serving skew.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和服务中使用的临时特征不一致——即存在训练和服务的偏差。通常使用历史数据进行训练，批量特征在离线情况下创建。但是，服务通常在线进行。如果训练中的特征处理流程与生产环境中用于服务的流程有任何不同（例如，使用不同的库、预处理代码或语言），那么我们就存在训练和服务的偏差的风险。
- en: Productionizing features is difficult. When moving to production, there is no
    standardized framework to serve features for online ML models and to serve batch
    features for offline model training. Models are trained offline using features
    created in batch processes, but when served in production, these features are
    often created with an emphasis on low latency and less on high throughput. The
    framework for feature generation and storage is not flexible to handle both of
    these scenarios.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将特征投入生产是困难的。在进入生产环境时，没有标准化的框架用于为在线机器学习模型提供特征以及为离线模型训练提供批处理特征。模型通常是使用批处理过程中创建的特征进行离线训练，但在生产环境中服务时，这些特征通常更注重低延迟而不是高吞吐量。特征生成和存储框架不能灵活处理这两种场景。
- en: In short, the ad hoc approach to feature engineering slows model development
    and leads to duplicated effort and work stream inefficiency. Furthermore, feature
    creation is inconsistent between training and inference, running the risk of training–serving
    skew or data leakage by accidentally introducing label information into the model
    input pipeline.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 简言之，临时特征工程方法减慢了模型开发速度，并导致重复劳动和工作效率低下。此外，特征创建在训练和推断中不一致，存在意外将标签信息引入模型输入流程的风险。
- en: Solution
  id: totrans-401
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: The solution is to create a shared feature store, a centralized location to
    store and document feature datasets that will be used in building machine learning
    models and can be shared across projects and teams. The feature store acts as
    the interface between the data engineer’s pipelines for feature creation and the
    data scientist’s workflow building models using those features ([Figure 6-12](ch06_split_001.xhtml#a_feature_store_provides_a_bridge_betwe)).
    This way, there is a central repository to house precomputed features, which speeds
    development time and aids in feature discovery. This also allows the basic software
    engineering principles of versioning, documentation, and access control to be
    applied to the features that are created.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是创建一个共享特征存储库，一个集中存储和记录特征数据集的地方，这些特征数据集将用于构建机器学习模型，并可以在项目和团队之间共享。特征存储库充当数据工程师为特征创建而设计的流水线和数据科学家使用这些特征构建模型的工作流之间的接口（[图 6-12](ch06_split_001.xhtml#a_feature_store_provides_a_bridge_betwe)）。这样一来，就有一个中央仓库来存放预计算的特征，这加快了开发时间，并有助于特征的发现。这还允许将版本控制、文档编制和访问控制等基本软件工程原则应用于创建的特征。
- en: 'A typical feature store is built with two key design characteristics: tooling
    to process large feature data sets quickly, and a way to store features that supports
    both low-latency access (for inference) and large batch access (for model training).
    There is also a metadata layer that simplifies documentation and versioning of
    different feature sets and an API that manages loading and retrieving feature
    data.'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的特征存储库具有两个关键设计特点：工具化处理大型特征数据集的能力，以及支持低延迟访问（用于推断）和大批量访问（用于模型训练）的特征存储方式。还有一个元数据层，简化不同特征集的文档编制和版本控制，以及管理加载和检索特征数据的API。
- en: '![A feature store provides a bridge between raw data sources and model training
    and serving.](Images/mldp_0612.png)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
  zh: '![特征存储库提供了原始数据源和模型训练与服务之间的桥梁。](Images/mldp_0612.png)'
- en: Figure 6-12\. A feature store provides a bridge between raw data sources and
    model training and serving.
  id: totrans-405
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-12。特征存储提供了原始数据源和模型训练与服务之间的桥梁。
- en: The typical workflow of a data or ML engineer is to read raw data (structured
    or streaming) from a data source, apply various transformations on the data using
    their favorite processing framework, and store the transformed feature within
    the feature store. Rather than creating feature pipelines to support a single
    ML model, the Feature Store pattern decouples feature engineering from model development.
    In particular, tools like Apache Beam, Flink, or Spark are often used when building
    a feature store since they can handle processing data in batch as well as streaming.
    This also reduces the incidence of training–serving skew, since the feature data
    is populated by the same feature creation pipelines.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 数据或机器学习工程师的典型工作流程是从数据源（结构化或流式数据）读取原始数据，使用他们喜欢的处理框架对数据进行各种转换，并将转换后的特征存储在特征存储中。与创建支持单个机器学习模型的特征管道不同，特征存储模式将特征工程与模型开发分离。特别是在构建特征存储时经常使用Apache
    Beam、Flink或Spark等工具，因为它们可以处理批处理和流处理数据。这也减少了训练和服务偏差的发生，因为特征数据由相同的特征创建管道填充。
- en: After features are created, they are housed in a data store to be retrieved
    for training and serving. For serving feature retrieval, speed is optimized. A
    model in production backing some online application may need to produce real-time
    predictions within milliseconds, making low latency essential. However, for training,
    higher latency is not a problem. Instead the emphasis is on high throughput since
    historical features are pulled in large batches for training. A feature store
    addresses both these use cases by using different data stores for online and offline
    feature access. For example, a feature store may use Cassandra or Redis as a data
    store for online feature retrieval, and Hive or BigQuery for fetching historical,
    large batch feature sets.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 创建特征后，它们将存储在数据存储中，以便用于训练和服务。对于服务中的特征检索，速度被优化。生产中的模型可能需要在毫秒内生成实时预测，因此低延迟至关重要。但是，对于训练来说，较高的延迟并不是问题。相反，重点是高吞吐量，因为历史特征将以大批量用于训练。特征存储通过使用不同的数据存储解决了这两种用例，用于在线和离线特征访问。例如，特征存储可以使用Cassandra或Redis作为在线特征检索的数据存储，并使用Hive或BigQuery获取历史大批量特征集。
- en: In the end, a typical feature store will house many different feature sets containing
    features created from myriad raw data sources. The metadata layer is built in
    to document feature sets and provide a registry for easy feature discovery and
    cross collaboration among teams.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，典型的特征存储将包含许多不同的特征集，其中包含从各种原始数据源创建的特征。元数据层用于记录特征集并提供注册表，以便团队之间轻松发现特征并进行跨团队协作。
- en: Feast
  id: totrans-409
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Feast
- en: As an example of this pattern in action, consider [Feast](https://github.com/feast-dev),
    which is an open source feature store for machine learning developed by Google
    Cloud and [Gojek](https://oreil.ly/PszIn). It is built around [Google Cloud services](https://oreil.ly/ecJou)
    using Big Query for offline model training and Redis for low-latency, online serving
    ([Figure 6-13](ch06_split_001.xhtml#high_level_architecture_of_the_feast_fe)).
    Apache Beam is used for feature creation, which allows for consistent data pipelines
    for both batch and streaming.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 作为此模式在实际中的示例，请考虑[Feast](https://github.com/feast-dev)，这是由Google Cloud和[Gojek](https://oreil.ly/PszIn)开发的面向机器学习的开源特征存储。它围绕[Google
    Cloud服务](https://oreil.ly/ecJou)使用BigQuery进行离线模型训练和Redis进行低延迟的在线服务（参见图6-13中的高级架构）。Apache
    Beam用于特征创建，从而为批处理和流处理提供一致的数据管道。
- en: '![High level architecture of the Feast feature store. Feast is built around
    Google BigQuery, Redis, and Apache Beam.](Images/mldp_0613.png)'
  id: totrans-411
  prefs: []
  type: TYPE_IMG
  zh: '![Feast特征存储的高级架构。Feast围绕Google BigQuery、Redis和Apache Beam构建。](Images/mldp_0613.png)'
- en: Figure 6-13\. High-level architecture of the Feast feature store. Feast is built
    around Google BigQuery, Redis, and Apache Beam.
  id: totrans-412
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-13。Feast特征存储的高级架构。Feast围绕Google BigQuery、Redis和Apache Beam构建。
- en: To see how this works in practice, we’ll use a public BigQuery dataset containing
    information about taxi rides in New York City.^([7](ch06_split_001.xhtml#ch01fn32))
    Each row of the table contains a timestamp of the pickup, the pickup latitude
    and longitude, the dropoff latitude and longitude, the number of passengers, and
    the cost of the taxi ride. The goal of the ML model will be to predict the cost
    of the taxi ride, denoted `fare_amount`, using these characteristics.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解这在实践中是如何工作的，我们将使用一个公共的BigQuery数据集，其中包含关于纽约市出租车乘车的信息。^([7](ch06_split_001.xhtml#ch01fn32))
    表的每一行包含接送时间戳、接送点的纬度和经度、乘客数量以及出租车费用。ML模型的目标将是使用这些特征预测出租车费用，表示为`fare_amount`。
- en: This model benefits from engineering additional features from the raw data.
    For example, since taxi rides are based on the distance and duration of the trip,
    pre-computing the distance between the pickup and dropoff is a useful feature.
    Once this feature is computed on the dataset, we can store it within a feature
    set for future use.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型受益于从原始数据中工程化额外特征。例如，由于出租车乘车基于行程的距离和时长，预先计算接送点之间的距离是一个有用的特征。一旦在数据集上计算了这个特征，我们可以将其存储在特征集中以供将来使用。
- en: Adding feature data to Feast
  id: totrans-415
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 向Feast添加特征数据
- en: Data is stored in Feast using `FeatureSet`s. A `FeatureSet` contains the data
    schema and data source information, whether it is coming from a pandas dataframe
    or a streaming Kafka topic. `FeatureSet`s are how Feast knows where to source
    the data it needs for a feature, how to ingest it, and some basic characteristics
    about the data types. Groups of features can be ingested and stored together,
    and feature sets provide efficient storage and logical namespacing of data within
    these stores.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 数据使用`FeatureSet`在Feast中存储。`FeatureSet`包含数据架构和数据源信息，无论是来自pandas数据框还是流式Kafka主题。`FeatureSet`是Feast知道从哪里获取所需特征数据、如何摄取以及一些基本数据类型特征的方式。特征组可以一起摄取和存储，并且特征集在这些存储中提供高效的存储和逻辑命名空间。
- en: Once our feature set is registered, Feast will start an Apache Beam job to populate
    the feature store with data from the source. A feature set is used to generate
    both offline and online feature stores, which ensures developers train and serve
    their model with the same data. Feast ensures that the source data complies with
    the expected schema of the feature set.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的特征集被注册，Feast将启动一个Apache Beam作业，从源中填充特征存储。特征集用于生成离线和在线特征存储，确保开发人员用相同的数据训练和服务他们的模型。Feast确保源数据符合特征集的预期模式。
- en: There are four steps to ingest feature data into Feast, as shown in [Figure 6-14](ch06_split_001.xhtml#there_are_four_steps_to_ingesting_featu).
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 将特征数据导入Feast有四个步骤，如图6-14所示。
- en: '![There are four steps to ingesting feature data into Feast: create a FeatureSet,
    add entities and features, register the FeatureSet, and ingest feature data into
    the FeatureSet.](Images/mldp_0614.png)'
  id: totrans-419
  prefs: []
  type: TYPE_IMG
  zh: '![将特征数据导入Feast有四个步骤：创建一个FeatureSet，添加实体和特征，注册FeatureSet，并将特征数据导入FeatureSet。](Images/mldp_0614.png)'
- en: 'Figure 6-14\. There are four steps to ingesting feature data into Feast: create
    a FeatureSet, add entities and features, register the FeatureSet, and ingest feature
    data into the FeatureSet.'
  id: totrans-420
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-14。将特征数据导入Feast有四个步骤：创建一个FeatureSet，添加实体和特征，注册FeatureSet，并将特征数据导入FeatureSet。
- en: 'The four steps are as follows:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 这四个步骤如下：
- en: Create a `FeatureSet`. The feature set specifies the entities, features, and
    source.
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`FeatureSet`。特征集指定实体、特征和源。
- en: Add entities and features to the `FeatureSet`.
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向`FeatureSet`添加实体和特征。
- en: Register the `FeatureSet`. This creates a named feature set within Feast. The
    feature set contains no feature data.
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注册`FeatureSet`。这在Feast中创建了一个命名特征集。特征集不包含特征数据。
- en: Load feature data into the `FeatureSet`.
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将特征数据加载到`FeatureSet`中。
- en: A notebook with the full code for this example can be found in the [repository
    accompanying this book](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/06_reproducibility/feature_store.ipynb).
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例的完整代码可以在[附带本书的存储库](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/06_reproducibility/feature_store.ipynb)中找到。
- en: Creating a FeatureSet
  id: totrans-427
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 创建一个FeatureSet
- en: 'We connect to a Feast deployment by setting up a client with the Python SDK:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过设置一个Python SDK客户端连接到Feast部署：
- en: '[PRE56]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We can check that the client is connected by printing the existing feature
    sets with the command `client.list_feature_sets()`. If this is a new deployment,
    this will return an empty list. To create a new feature set, call the class `FeatureSet`
    and specify the feature set’s name:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查客户端是否连接，可以通过打印现有功能集合来实现，命令为 `client.list_feature_sets()`。如果这是一个新的部署，它将返回一个空列表。要创建一个新的功能集合，请调用
    `FeatureSet` 类并指定功能集合的名称：
- en: '[PRE57]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Adding entities and features to the FeatureSet
  id: totrans-432
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 向 FeatureSet 添加实体和特征
- en: In the context of Feast, `FeatureSets` consist of entities and features. Entities
    are used as keys to look up feature values and are used to join features between
    different feature sets when creating datasets for training or serving. The entity
    serves as an identifier for whatever relevant characteristic you have in the dataset.
    It is an object that can be modeled and store information. In the context of a
    ride-hailing or food delivery service, a relevant entity could be `customer_id`,
    `order_id`, `driver_id`, or `restaurant_id`. In the context of a churn model,
    an entity could be a `customer_id` or `segment_id`. Here, the entity is the `taxi_id`,
    a unique identifier for the taxi vendor of each trip.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Feast 的上下文中，`FeatureSets` 包括实体和特征。实体用作查找特征值的键，并在创建用于训练或服务的数据集时用于在不同特征集之间进行特征连接。实体作为数据集中任何相关特征的标识符。它是一个可以建模和存储信息的对象。在打车或食品配送服务的上下文中，相关实体可能是
    `customer_id`、`order_id`、`driver_id` 或 `restaurant_id`。在流失模型的上下文中，实体可以是 `customer_id`
    或 `segment_id`。这里的实体是 `taxi_id`，每次行程的出租车供应商的唯一标识符。
- en: At this stage, the feature set we created called `taxi_rides` contains no entities
    or features. We can use the Feast core client to specify these from a pandas dataframe
    that contains the raw data inputs and entities as shown in [Table 6-2](ch06_split_001.xhtml#the_taxi_ride_dataset_contains_informat).
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们创建的功能集称为 `taxi_rides`，不包含任何实体或特征。我们可以使用 Feast 核心客户端从包含原始数据输入和实体的 pandas
    数据帧中指定这些内容，如 [表 6-2](ch06_split_001.xhtml#the_taxi_ride_dataset_contains_informat)
    所示。
- en: Table 6-2\. The taxi ride dataset contains information about taxi rides in New
    York. The entity is the taxi_id, a unique identifier for the taxi vendor of each
    trip
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6-2\. 纽约出租车行程数据集包含有关出租车行程的信息。实体是 taxi_id，每次行程的出租车供应商的唯一标识符
- en: '| Row | pickup_datetime | pickup_lat | pickup_lon | dropoff_lat | dropoff_lon
    | num_pass | taxi_id | fare_amt |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| 行 | 接载时间 | 接载纬度 | 接载经度 | 卸载纬度 | 卸载经度 | 乘客数量 | 出租车ID | 费用金额 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 2020-05-31 11:29:48 UTC | 40.787403 | -73.955848 | 40.723042 | -73.993106
    | 2 | 0 | 15.3 |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2020-05-31 11:29:48 UTC | 40.787403 | -73.955848 | 40.723042 | -73.993106
    | 2 | 0 | 15.3 |'
- en: '| 2 | 2011-04-06 14:30:00 UTC | 40.645343 | -73.776698 | 40.71489 | -73.987242
    | 2 | 0 | 45.0 |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2011-04-06 14:30:00 UTC | 40.645343 | -73.776698 | 40.71489 | -73.987242
    | 2 | 0 | 45.0 |'
- en: '| 3 | 2020-04-24 13:11:06 UTC | 40.650105 | -73.785373 | 40.638858 | -73.9678
    | 2 | 2 | 32.1 |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 2020-04-24 13:11:06 UTC | 40.650105 | -73.785373 | 40.638858 | -73.9678
    | 2 | 2 | 32.1 |'
- en: '| 4 | 2020-02-20 09:07:00 UTC | 40.762365 | -73.925733 | 40.740118 | -73.986487
    | 2 | 1 | 21.3 |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 2020-02-20 09:07:00 UTC | 40.762365 | -73.925733 | 40.740118 | -73.986487
    | 2 | 1 | 21.3 |'
- en: 'The `pickup_datetime` timestamp here is important since it is necessary to
    retrieve batch features and is used to ensure time-correct joins for batch features.
    To create an additional feature, such as the Euclidean distance, load the dataset
    into a pandas dataframe and compute the feature:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 `pickup_datetime` 时间戳很重要，因为需要检索批处理特征，并且用于确保批处理特征的正确时间连接。要创建额外的特征，如欧几里得距离，请将数据集加载到
    pandas 数据框架中并计算该特征：
- en: '[PRE58]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We can add entities and features to the feature set with `.add(...)`. Alternatively,
    the method `.infer_fields_from_df(...)` will create the entities and features
    for our `FeatureSet` directly from the pandas dataframe. We simply specify the
    column name that represents the entity. The schema and data types for the features
    of the `FeatureSet` are then inferred from the dataframe:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `.add(...)` 向特征集添加实体和特征。或者，方法 `.infer_fields_from_df(...)` 将直接从 pandas
    数据帧为我们的 `FeatureSet` 创建实体和特征。我们只需指定表示实体的列名。然后，从数据帧中推断出 `FeatureSet` 的特征的模式和数据类型：
- en: '[PRE59]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Registering the FeatureSet
  id: totrans-446
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注册 FeatureSet
- en: 'Once the FeatureSet is created, we can register it with Feast using `client.apply(taxi_fs)`.
    To confirm that the feature set was correctly registered or to explore the contents
    of another feature set, we can retrieve it using `.get_feature_set(...)`:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 创建FeatureSet后，我们可以使用`client.apply(taxi_fs)`将其注册到Feast。要确认功能集已正确注册或探索另一个功能集的内容，我们可以使用`.get_feature_set(...)`检索它：
- en: '[PRE60]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'This returns a JSON object containing the data schema for the `taxi_rides`
    feature set:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回一个JSON对象，其中包含`taxi_rides`功能集的数据架构：
- en: '[PRE61]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Ingesting feature data into the FeatureSet
  id: totrans-451
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将功能数据摄入FeatureSet
- en: Once we are happy with our schema, we can ingest the dataframe feature data
    into Feast using `.ingest(...)`. We’ll specify the `FeatureSet`, called `taxi_fs`,
    and the dataframe from which to populate the feature data, called `taxi_df`.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们对架构满意，我们可以使用`.ingest(...)`将数据框架功能数据摄入Feast。我们将指定称为`taxi_fs`的`FeatureSet`和用于填充功能数据的名为`taxi_df`的数据框架。
- en: '[PRE62]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Progress during this ingestion step is printed to the screen showing that we’ve
    ingested 28,247 rows into the `taxi_rides` feature set within Feast:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 此摄取步骤期间的进度打印到屏幕上，显示我们已经在Feast中将28,247行摄入到`taxi_rides`功能集中：
- en: '[PRE63]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: At this stage, calling `client.list_feature_sets()` will now list the feature
    set `taxi_rides` we just created and return `[default/taxi_rides]`. Here, `default`
    refers to the project scope of the feature set within Feast. This can be changed
    when instantiating the feature set to keep certain feature sets within project
    access.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 在此阶段，调用`client.list_feature_sets()`现在将列出我们刚刚创建的功能集`taxi_rides`并返回`[default/taxi_rides]`。这里，`default`指的是Feast内功能集的项目范围。在实例化功能集时可以更改此设置，以保留某些功能集在项目访问范围内。
- en: Warning
  id: totrans-457
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Datasets may change over time, causing feature sets to change as well. In Feast,
    once a feature set is created, there are only a few changes that can be made.
    For example, the following changes are allowed:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可能随时间变化，从而导致功能集也发生变化。在Feast中，一旦创建功能集，只能进行少数更改。例如，允许进行以下更改：
- en: Adding new features.
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加新功能。
- en: Removing existing features. (Note that features are tombstoned and remain on
    record, so they are not removed completely. This will affect new features being
    able to take the names of previously deleted features.)
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除现有功能。（请注意，功能被删除并保留记录，因此它们并未完全删除。这将影响新功能能否使用先前删除的功能名称。）
- en: Changing features’ schemas.
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更改功能的架构。
- en: Changing the feature set’s source or the `max_age` of the feature set examples.
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更改功能集的来源或`max_age`的功能集示例。
- en: 'The following changes are *not* allowed:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 不允许以下更改：
- en: Changes to the feature set name.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更改功能集名称。
- en: Changes to entities.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更改实体。
- en: Changes to names of existing features.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更改现有功能的名称。
- en: Retrieving data from Feast
  id: totrans-467
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从Feast检索数据
- en: Once a feature set has been sourced with features, we can retrieve historical
    or online features. Users and production systems retrieve feature data through
    a Feast serving data access layer. Since Feast supports both offline and online
    store types, it’s common to have Feast deployments for both, as shown in [Figure 6-15](ch06_split_001.xhtml#feature_data_can_be_retrieved_either_of).
    The same feature data is contained within the two feature stores, ensuring consistency
    between training and serving.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦功能集已经通过功能进行了数据源化，我们可以检索历史或在线功能。用户和生产系统通过Feast服务数据访问层检索功能数据。由于Feast支持离线和在线存储类型，因此通常同时部署两者，如[图6-15](ch06_split_001.xhtml#feature_data_can_be_retrieved_either_of)所示。同一功能数据包含在两个功能存储中，确保训练和服务之间的一致性。
- en: '![Feature data can be retrieved either offline, using historical features for
    model training, or online, for serving.](Images/mldp_0615.png)'
  id: totrans-469
  prefs: []
  type: TYPE_IMG
  zh: '![功能数据可以从离线中检索，用于模型训练的历史功能，或在线用于服务。](Images/mldp_0615.png)'
- en: Figure 6-15\. Feature data can be retrieved either offline, using historical
    features for model training, or online, for serving.
  id: totrans-470
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-15。功能数据可以从离线中检索，用于模型训练的历史功能，或在线用于服务。
- en: 'These deployments are accessed via a separate online and batch client:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 这些部署可以通过单独的在线和批处理客户端访问：
- en: '[PRE64]'
  id: totrans-472
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Batch serving
  id: totrans-473
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 批量服务
- en: 'For training a model, historical feature retrieval is backed by BigQuery and
    accessed using `.get_batch_features(...)` with the batch serving client. In this
    case, we provide Feast with a pandas dataframe containing the entities and timestamps
    that feature data will be joined to. This allows Feast to produce a point-in-time
    correct dataset based on the features that have been requested:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型训练，历史特征检索由BigQuery支持，并使用批处理服务客户端`.get_batch_features(...)`访问。在这种情况下，我们提供一个包含实体和时间戳的pandas数据帧，特征数据将与之结合。这允许Feast基于请求的特征生成准确的时点数据集：
- en: '[PRE65]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'To retrieve historical features, the features in the feature set are referenced
    by the feature set name and the feature name, separated by a colon—for example,
    `taxi_rides:pickup_lat`:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 要检索历史特征，可以通过特征集名称和特征名称（以冒号分隔）引用特征集中的特征，例如，`taxi_rides:pickup_lat`：
- en: '[PRE66]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: The dataframe dataset now contains all features and the label for our model,
    pulled directly from the feature store.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 数据帧数据集现在包含我们模型的所有特征和标签，直接从特征存储中提取。
- en: Online serving
  id: totrans-479
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在线服务
- en: 'For online serving, Feast only stores the latest entity values, as opposed
    to historical serving where all historical values are stored. Online serving with
    Feast is built to be very low latency, and Feast provides a gRPC API backed by
    [Redis](https://redis.io). To retrieve online features, for example, when making
    online predictions with the trained model, we use `.get_online_features(...)`
    specifying the features we want to capture and the entity:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在线服务，Feast仅存储最新的实体数值，而不是所有历史数值。Feast的在线服务设计非常低延迟，并提供由[Redis](https://redis.io)支持的gRPC
    API。例如，在使用训练模型进行在线预测时，我们使用`.get_online_features(...)`指定要捕获的特征和实体：
- en: '[PRE67]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'This saves `online_features` as a list of maps where each item in the list
    contains the latest feature values for the provided entity, here, `taxi_id = 5`:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 这将`online_features`保存为一个映射列表，列表中的每个项包含提供的实体（例如，`taxi_id = 5`）的最新特征值：
- en: '[PRE68]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'To make an online prediction for this example, we pass the field values from
    the object returned in `online_features` as a pandas dataframe called `predict_df`
    to `model.predict`:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对这个示例进行在线预测，我们将从返回的对象中的字段值作为名为`predict_df`的pandas数据帧传递给`model.predict`：
- en: '[PRE69]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Why It Works
  id: totrans-486
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么它有效
- en: Feature stores work because they decouple feature engineering from feature usage,
    allowing feature development and creation to occur independently from the consumption
    of features during model development. As features are added to the feature store,
    they become available immediately for both training and serving and are stored
    in a single location. This ensures consistency between model training and serving.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 特征存储的工作原理在于将特征工程与特征使用解耦，允许在模型开发期间独立进行特征开发和创建。随着特征添加到特征存储中，它们立即可用于训练和服务，并存储在单个位置。这确保了模型训练和服务之间的一致性。
- en: For example, a model served as a customer-facing application may receive only
    10 input values from a client, but those 10 inputs may need to be transformed
    into many more features via feature engineering before being sent to a model.
    Those engineered features are maintained within the feature store. It is crucial
    that the pipeline for retrieving features during development is the same as when
    serving the model. A feature store ensures that consistency ([Figure 6-16](ch06_split_001.xhtml#a_feature_store_ensures_the_feature_eng)).
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，作为面向客户的应用程序提供服务的模型可能只从客户端接收到10个输入值，但这些10个输入值可能需要通过特征工程转换成更多特征。这些工程特征保存在特征存储中。在开发期间检索特征的管道与服务模型时的管道必须相同是至关重要的。特征存储确保了特征的一致性（[图6-16](ch06_split_001.xhtml#a_feature_store_ensures_the_feature_eng)）。
- en: Feast accomplishes this by using Beam on the backend for feature ingestion pipelines
    that write feature values into the feature sets, and uses Redis and BigQuery for
    online and offline (respectively) feature retrieval ([Figure 6-17](ch06_split_001.xhtml#feast_uses_beam_on_the_backend_for_feat)).^([8](ch06_split_001.xhtml#ch01fn33))
    As with any feature store, the ingestion pipeline also handles partial failure
    or race conditions that might cause some data to be in one storage but not the
    other.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: Feast通过在后端使用Beam进行特征摄入管道来实现这一点，将特征值写入特征集，并使用Redis和BigQuery进行在线和离线（分别）特征检索（[图6-17](ch06_split_001.xhtml#feast_uses_beam_on_the_backend_for_feat)）。^([8](ch06_split_001.xhtml#ch01fn33))
    与任何特征存储一样，摄入管道还处理可能导致某些数据在一个存储中而不在另一个存储中的部分失败或竞争条件。
- en: '![A feature store ensures the feature engineering pipelines are consistent
    between model training and serving. See also https://docs.feast.dev/.](Images/mldp_0616.png)'
  id: totrans-490
  prefs: []
  type: TYPE_IMG
  zh: '![特征存储确保特征工程流水线在模型训练和服务之间保持一致。另请参阅 https://docs.feast.dev/。](Images/mldp_0616.png)'
- en: Figure 6-16\. A feature store ensures the feature engineering pipelines are
    consistent between model training and serving. See also [*https://docs.feast.dev/*](https://docs.feast.dev/).
  id: totrans-491
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-16\. 特征存储确保特征工程流水线在模型训练和服务之间保持一致。另请参阅[*https://docs.feast.dev/*](https://docs.feast.dev/)。
- en: '![Feast uses Beam on the backend for feature creation and Redis and BigQuery
    for online and offline feature retrieval.](Images/mldp_0617.png)'
  id: totrans-492
  prefs: []
  type: TYPE_IMG
  zh: '![Feast在后端使用Beam进行特征创建，Redis和BigQuery用于在线和离线特征检索。](Images/mldp_0617.png)'
- en: Figure 6-17\. Feast uses Beam on the backend for feature ingestion and Redis
    and BigQuery for online and offline feature retrieval.
  id: totrans-493
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-17\. Feast在后端使用Beam进行特征摄入，Redis和BigQuery用于在线和离线特征检索。
- en: Different systems may produce data at different rates, and a feature store is
    flexible enough to handle those different cadences, both for ingestion and during
    retrieval ([Figure 6-18](ch06_split_001.xhtml#the_feature_store_design_pattern_can_ha)).
    For example, sensor data could be produced in real time, arriving every second,
    or there could be a monthly file that is generated from an external system reporting
    a summary of the last month’s transactions. Each of these need to be processed
    and ingested into the feature store. By the same token, there may be different
    time horizons for retrieving data from the feature store. For example, a user-facing
    online application may operate at very low latency using up-to-the-second features,
    whereas when training the model, features are pulled offline as a larger batch
    but with higher latency.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 不同系统可能以不同速率生成数据，特征存储库足够灵活，能够处理这些不同的节奏，无论是在摄入过程中还是在检索期间（[图 6-18](ch06_split_001.xhtml#the_feature_store_design_pattern_can_ha)）。例如，传感器数据可以实时产生，每秒到达，或者可能有一个每月生成的文件，由外部系统报告上个月交易的摘要。每一个这些都需要被处理并摄入到特征存储库中。同样地，从特征存储库检索数据可能有不同的时间视野。例如，用户面向的在线应用可以使用最低延迟，使用最新的秒级特征，而在训练模型时，特征以更大的批量离线拉取，但延迟较高。
- en: '![The Feature Store design pattern can handle both the requirements of data
    being highly scalable for large batches during training and extremely low latency
    for serving online applications.](Images/mldp_0618.png)'
  id: totrans-495
  prefs: []
  type: TYPE_IMG
  zh: '![特征存储设计模式可以处理数据在训练期间进行大批量高度扩展和为在线应用提供极低延迟的需求。](Images/mldp_0618.png)'
- en: Figure 6-18\. The Feature Store design pattern can handle both the requirements
    of data being highly scalable for large batches during training and extremely
    low latency for serving online applications.
  id: totrans-496
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-18\. 特征存储设计模式可以处理数据在训练期间进行大批量高度扩展和为在线应用提供极低延迟的需求。
- en: There is no single database that can handle both scaling to potentially terabytes
    of data *and* extremely low latency on the order of milliseconds. The feature
    store achieves this with separate online and offline feature stores and ensures
    that features are handled in a consistent fashion in both scenarios.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 没有单个数据库可以既扩展到可能的数据量级（可能达到几TB）*又*在毫秒级的极低延迟上处理。特征存储通过在线和离线特征存储的分开实现了这一点，并确保在两种情况下以一致的方式处理特征。
- en: Lastly, a feature store acts as a version-controlled repository for feature
    datasets, allowing the same CI/CD practices of code and model development to be
    applied to the feature engineering process. This means that new ML projects start
    with a process of feature selection from a catalog instead of having to do feature
    engineering from scratch, allowing organizations to achieve an economies-of-scale
    effect—as new features are created and added to the feature store, it becomes
    easier and faster to build new models that reuse those features.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，特征存储库充当特征数据集的版本控制存储库，允许将代码和模型开发的相同CI/CD实践应用于特征工程过程。这意味着新的机器学习项目从目录中进行特征选择，而不是从头开始进行特征工程，使组织能够实现规模经济效应——随着新特征的创建和添加到特征存储库中，构建重复使用这些特征的新模型变得更加简单和快速。
- en: Trade-Offs and Alternatives
  id: totrans-499
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 折衷与替代方案
- en: The Feast framework that we discussed is built on Google BigQuery, Redis, and
    Apache Beam. However, there are feature stores that rely on other tools and tech
    stacks. And, although a feature store is the recommended way to manage features
    at scale, `tf.transform` provides an alternative solution that addresses the issue
    of training–serving skew, but not feature reusability. There are also some alternative
    uses of a feature store that we have not yet detailed, such as how a feature store
    handles data from different sources and data arriving at different cadences.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论的 Feast 框架是建立在 Google BigQuery、Redis 和 Apache Beam 之上的。然而，也有一些特征存储依赖于其他工具和技术堆栈。虽然特征存储是规模化管理特征的推荐方式，`tf.transform`
    提供了一种解决训练-服务偏差问题的替代方案，但不能解决特征的可重用性问题。还有一些特征存储的替代用法尚未详细介绍，例如特征存储如何处理来自不同来源和以不同频率到达的数据。
- en: Alternative implementations
  id: totrans-501
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 替代实现
- en: Many large technology companies, like Uber, LinkedIn, Airbnb, Netflix, and Comcast,
    host their own version of a feature store, though the architectures and tools
    vary. Uber’s Michelangelo Palette is built around Spark/Scala using Hive for offline
    feature creation and Cassandra for online features. Hopsworks provides another
    open source feature store alternative to Feast and is built around dataframes
    using Spark and pandas with Hive for offline and MySQL Cluster for online feature
    access. Airbnb built their own feature store as part of their production ML framework
    called Zipline. It uses Spark and Flink for feature engineering jobs and Hive
    for feature storage.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 许多大型科技公司，如 Uber、LinkedIn、Airbnb、Netflix 和 Comcast，都拥有自己版本的特征存储，尽管架构和工具各不相同。Uber
    的 Michelangelo Palette 是围绕 Spark/Scala 构建的，使用 Hive 进行离线特征创建和 Cassandra 进行在线特征。Hopsworks
    提供了 Feast 的另一种开源特征存储替代方案，构建在使用 Spark 和 pandas 的数据框架上，使用 Hive 进行离线存储和 MySQL Cluster
    进行在线特征访问。Airbnb 在其生产 ML 框架 Zipline 中构建了自己的特征存储。它使用 Spark 和 Flink 进行特征工程作业，使用 Hive
    进行特征存储。
- en: 'Whichever tech stack is used, the primary components of the feature store are
    the same:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 无论使用哪种技术堆栈，特征存储的主要组件都是相同的：
- en: A tool to process large feature engineering jobs quickly, such as Spark, Flink
    or Beam.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于快速处理大规模特征工程作业的工具，例如 Spark、Flink 或 Beam。
- en: A storage component for housing the feature sets that are created, such as Hive,
    cloud storage (Amazon S3, Google Cloud Storage), BigQuery, Redis, BigTable, and/or
    Cassandra. The combination that Feast uses (BigQuery and Redis) is optimized for
    offline versus online (low-latency) feature retrieval.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于存储创建的特征集的存储组件，例如 Hive、云存储（Amazon S3、Google Cloud Storage）、BigQuery、Redis、BigTable
    和/或 Cassandra。Feast 使用的组合（BigQuery 和 Redis）针对离线与在线（低延迟）特征检索进行了优化。
- en: A metadata layer to record feature version information, documentation, and feature
    registry to simplify discovery and sharing of feature sets.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元数据层用于记录特征版本信息、文档和特征注册表，以简化特征集的发现和共享。
- en: An API for ingesting and retrieving features to/from the feature store.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于从特征存储区摄取和检索特征的 API。
- en: Transform design pattern
  id: totrans-508
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转换设计模式
- en: If feature engineering code is not the same during training and inference, there
    is a risk that the two code sources will not be consistent. This leads to training–serving
    skew, and model predictions may not be reliable since the features may not be
    the same. Feature stores get around this problem by having their feature engineering
    jobs write feature data to both an online and an offline database. And, while
    a feature store itself doesn’t perform the feature transformations, it provides
    a way to separate the upstream feature engineering steps from model serving and
    provide point in time correctness.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在训练和推理期间特征工程代码不同，存在两个代码源不一致的风险。这导致训练-服务偏差，因为特征可能不同，模型预测可能不可靠。特征存储通过使其特征工程作业将特征数据写入在线和离线数据库来解决此问题。特征存储本身并不执行特征转换，但提供了一种将上游特征工程步骤与模型服务分离并提供时点正确性的方式。
- en: The Transform design pattern discussed in this chapter also provides a way to
    keep feature transformations separate and reproducible. For example, `tf.transform`
    can be used to preprocess data using exactly the same code for both training a
    model and serving predictions in production, thus eliminating training–serving
    skew. This ensures that training and serving feature engineering pipelines are
    consistent.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章讨论的Transform设计模式还提供了一种保持特征转换分离和可复制的方法。例如，`tf.transform`可以用来使用完全相同的代码预处理数据，用于训练模型和在生产环境中提供预测，从而消除训练和服务之间的偏差。这确保了训练和服务特征工程流水线的一致性。
- en: However, the feature store provides an added advantage of feature reusability
    that `tf.transform` does not have. Although `tf.transform` pipelines ensure reproducibility,
    the features are created and developed only for that model and are not easily
    shared or reused by other models and pipelines.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，特征存储提供了`tf.transform`不具备的特征重用优势。虽然`tf.transform`流水线确保可复制性，但这些特征仅为该模型创建并开发，并不容易与其他模型和流水线共享或重用。
- en: On the other hand, `tf.transform` takes special care to ensure that feature
    creation during serving is carried out on accelerated hardware, since it is part
    of the serving graph. Feature stores typically do not provide this capability
    today.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`tf.transform` 特别注意确保在服务过程中使用加速硬件进行特征创建，因为它是服务图的一部分。目前特征存储通常不提供这种能力。
- en: 'Design Pattern 27: Model Versioning'
  id: totrans-513
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计模式27：模型版本管理
- en: In the Model Versioning design pattern, backward compatibility is achieved by
    deploying a changed model as a microservice with a different REST endpoint. This
    is a necessary prerequisite for many of the other patterns discussed in this chapter.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型版本设计模式中，通过将更改的模型部署为具有不同REST端点的微服务来实现向后兼容性。这是本章讨论的许多其他模式的必要前提。
- en: Problem
  id: totrans-515
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: As we’ve seen with *data drift* (introduced in [Chapter 1](ch01.xhtml#the_need_for_machine_learning_design_pa)),
    models can become stale over time and need to be updated regularly to make sure
    they reflect an organization’s changing goals, and the environment associated
    with their training data. Deploying model updates to production will inevitably
    affect the way models behave on new data, which presents a challenge—we need an
    approach for keeping production models up to date while still ensuring backward
    compatibility for existing model users.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*数据漂移*（见[第1章](ch01.xhtml#the_need_for_machine_learning_design_pa)介绍）中所看到的，随着时间推移，模型可能变得陈旧，并且需要定期更新，以确保其反映组织的变化目标以及与其训练数据相关的环境。将模型更新部署到生产环境将不可避免地影响模型在新数据上的行为方式，这提出了一个挑战——我们需要一种方法来保持生产模型的更新，同时确保现有模型用户的向后兼容性。
- en: Updates to an existing model might include changing a model’s architecture in
    order to improve accuracy, or retraining a model on more recent data to address
    drift. While these types of changes likely won’t require a different model output
    format, they will affect the prediction results users get from a model. As an
    example, let’s imagine we’re building a model that predicts the genre of a book
    from its description and uses the predicted genres to make recommendations to
    users. We trained our initial model on a dataset of older classic books, but now
    have access to new data on thousands of more recent books to use for training.
    Training on this updated dataset improves our overall model accuracy, but slightly
    reduces accuracy on older “classic” books. To handle this, we’ll need a solution
    that lets users choose an older version of our model if they prefer.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 对现有模型的更新可能包括更改模型架构以提高准确性，或者使用更近期数据重新训练模型以解决漂移问题。虽然这些更改可能不需要不同的模型输出格式，但它们会影响用户从模型获取的预测结果。例如，让我们想象一下，我们正在构建一个模型，根据书籍描述预测书籍的流派，并使用预测的流派向用户推荐。我们最初在一个旧经典书籍数据集上训练了我们的模型，但现在我们可以使用数千个更近期书籍的新数据来进行训练。在更新的数据集上训练可以提高我们整体模型的准确性，但在旧的“经典”书籍上略微降低了准确性。为了处理这个问题，我们需要一个解决方案，让用户可以选择我们模型的旧版本。
- en: Alternatively, our model’s end users might start to require more information
    on *how* the model is arriving at a specific prediction. In a medical use case,
    a doctor might need to see the regions in an x-ray that caused a model to predict
    the presence of disease rather than rely solely on the predicted label. In this
    case, the response from a deployed model would need to be updated to include these
    highlighted regions. This process is known as *explainability* and is discussed
    further in [Chapter 7](ch07.xhtml#responsible_ai).
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们模型的最终用户可能开始需要更多关于*模型如何*得出特定预测的信息。在医疗用例中，医生可能需要查看造成模型预测存在疾病的 X 光中的区域，而不仅仅依赖于预测的标签。在这种情况下，部署模型的响应需要更新以包含这些突出显示的区域。这个过程被称为*可解释性*，并且在[第
    7 章](ch07.xhtml#responsible_ai)中进一步讨论。
- en: When we deploy updates to our model, we’ll also likely want a way to track how
    the model is performing in production and compare this with previous iterations.
    We may also want a way to test a new model with only a subset of our users. Both
    performance monitoring and split testing, along with other possible model changes,
    will be difficult to solve by replacing a single production model each time we
    make updates. Doing this will break applications that are relying on our model
    output to match a specific format. To handle this, we’ll need a solution that
    allows us to continuously update our model without breaking existing users.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们部署模型更新时，我们很可能也希望有一种方式来跟踪模型在生产环境中的表现，并与以前的迭代进行比较。我们可能还希望以一种方式测试新模型，只涉及我们用户的一个子集。性能监控和分割测试以及其他可能的模型更改，将通过每次更新单个生产模型来解决将会很困难。这样做会破坏那些依赖我们模型输出匹配特定格式的应用程序。为了处理这个问题，我们需要一种解决方案，允许我们在不破坏现有用户的情况下持续更新我们的模型。
- en: Solution
  id: totrans-520
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: To gracefully handle updates to a model, deploy multiple model versions with
    different REST endpoints. This ensures backward compatibility—by keeping multiple
    versions of a model deployed at a given time, those users relying on older versions
    will still be able to use the service. Versioning also allows for fine-grained
    performance monitoring and analytics tracking across versions. We can compare
    accuracy and usage statistics, and use this to determine when to take a particular
    version offline. If we have a model update that we want to test with only a small
    subset of users, the Model Versioning design pattern makes it possible to perform
    A/B testing.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优雅地处理模型的更新，使用不同的 REST 端点部署多个模型版本。这确保了向后兼容性——通过在给定时间内部署多个版本的模型，依赖旧版本的用户仍然能够使用服务。版本控制还允许在版本间进行精细化的性能监控和分析跟踪。我们可以比较准确性和使用统计数据，并使用这些数据来确定何时将特定版本下线。如果我们有一个想要仅对少数用户进行测试的模型更新，模型版本设计模式使得进行
    A/B 测试成为可能。
- en: Additionally, with model versioning, each deployed version of our model is a
    microservice—thus decoupling changes to our model from our application frontend.
    To add support for a new version, our team’s application developers only need
    to change the name of the API endpoint pointing to the model. Of course, if a
    new model version introduces changes to the model’s response format, we’ll need
    to make changes to our app to accommodate this, but the model and application
    code are still separate. Data scientists or ML engineers can therefore deploy
    and test a new model version on our own without worrying about breaking our production
    app.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过模型版本控制，我们的每个部署版本都是一个微服务——因此将模型的更改与应用程序前端分离开来。要为新版本添加支持，我们团队的应用程序开发人员只需更改指向模型的
    API 端点的名称。当然，如果新模型版本引入了模型响应格式的更改，我们将需要修改应用程序以适应这些更改，但模型和应用程序代码仍然是分开的。因此，数据科学家或机器学习工程师可以在我们自己的环境中部署和测试新的模型版本，而不必担心破坏我们的生产应用程序。
- en: Types of model users
  id: totrans-523
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型用户类型
- en: When we refer to “end users” of our model, this includes two different groups
    of people. If we’re making our model API endpoint available to application developers
    outside our organization, these developers can be thought of as one type of model
    user. They are building applications that rely on our model for serving predictions
    to others. The backward compatibility benefit that comes with model versioning
    is most important for these users. If the format of our model’s response changes,
    application developers may want to use an older model version until they’ve updated
    their application code to support the latest response format.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们提到我们模型的“终端用户”时，这包括两类不同的人群。如果我们将模型 API 端点提供给组织外的应用程序开发人员使用，这些开发人员可以被视为一类模型用户。他们正在构建依赖我们模型为他人提供预测服务的应用程序。模型版本化所带来的向后兼容性好处对这些用户至关重要。如果我们模型响应的格式发生变化，应用程序开发人员可能希望使用旧的模型版本，直到他们更新其应用程序代码以支持最新的响应格式。
- en: The other group of end users refers to those using an application that calls
    our deployed model. This could be a doctor relying on our model to predict the
    presence of disease in an image, someone using our book recommendation app, our
    organization’s business unit analyzing the output of a revenue prediction model
    we built, and more. This group of users is less likely to run into backward compatibility
    issues, but may want the option to choose when to start using a new feature in
    our app. Also, if we can break users into distinct groups (i.e., based on their
    app usage), we can serve each group different model versions based on their preferences.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类终端用户指的是使用调用我们已部署模型的应用程序的人。这可以是依赖我们模型来预测图像中疾病存在的医生，使用我们的图书推荐应用程序的人，我们组织的业务单位分析我们构建的收入预测模型输出的人，等等。这类用户较少可能遇到向后兼容性问题，但可能希望在我们的应用程序中启用新功能时选择开始使用的时间。此外，如果我们能够将用户分为不同的组（即基于他们的应用使用情况），我们可以根据他们的偏好为每个组提供不同的模型版本。
- en: Model versioning with a managed service
  id: totrans-526
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用托管服务进行模型版本控制
- en: To demonstrate versioning, we’ll build a model that predicts flight delays and
    deploy this model to [Cloud AI Platform Prediction](https://oreil.ly/-GAVQ). Because
    we looked at TensorFlow’s SavedModel in previous chapters, we’ll use an XGBoost
    model here.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示版本控制，我们将构建一个预测航班延误的模型，并将此模型部署到 [Cloud AI Platform Prediction](https://oreil.ly/-GAVQ)。因为我们在前几章中已经看过
    TensorFlow 的 SavedModel，这里我们将使用 XGBoost 模型。
- en: 'Once we’ve trained our model, we can export it to get it ready for serving:'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完模型后，我们可以将其导出以准备服务：
- en: '[PRE70]'
  id: totrans-529
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: To deploy this model to AI Platform, we need to create a model version that
    will point to this `model.bst` in a Cloud Storage Bucket.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 要将此模型部署到 AI 平台，我们需要创建一个指向 Cloud Storage 存储桶中的 `model.bst` 的模型版本。
- en: 'In AI Platform, a model resource can have many versions associated with it.
    To create a new version using the gcloud CLI, we’ll run the following in a Terminal:'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AI 平台中，一个模型资源可以关联多个版本。要使用 gcloud CLI 创建新版本，我们将在终端中运行以下命令：
- en: '[PRE71]'
  id: totrans-532
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: With this model deployed, it’s now accessible via the endpoint */models/flight_delay_predictions/versions/v1*
    in an HTTPS URL tied to our project. Since this is the only version we’ve deployed
    so far, it’s considered the *default*. This means that if we don’t specify a version
    in our API request, the prediction service will use v1\. Now we can make predictions
    to our deployed model by sending it examples in the format our model expects—in
    this case, a 110-element array of dummy-coded airport codes (for the full code,
    see the [notebook on GitHub](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/06_reproducibility/model_versioning.ipynb)).
    The model returns sigmoid output, a float value between 0 and 1 indicating the
    likelihood a given flight was delayed more than 30 minutes.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 部署了此模型后，它可以通过 HTTPS URL 中的端点 */models/flight_delay_predictions/versions/v1*
    访问，这个端点绑定到我们的项目。由于这是我们目前唯一部署的版本，它被视为 *默认* 版本。这意味着如果我们在 API 请求中不指定版本，预测服务将使用 v1\.
    现在我们可以向部署的模型发送符合模型期望格式的示例来进行预测，本例中是一个包含 110 个元素的虚拟编码机场代码数组（完整代码请参见 [GitHub 上的笔记本](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/06_reproducibility/model_versioning.ipynb)）。该模型返回
    sigmoid 输出，一个介于 0 到 1 之间的浮点值，表示给定航班延误超过 30 分钟的可能性。
- en: 'To make a prediction request to our deployed model, we’ll use the following
    gcloud command, where *input.json* is a file with our newline delimited examples
    to send for prediction:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 要向已部署的模型发出预测请求，我们将使用以下 gcloud 命令，其中 *input.json* 是一个包含要发送给预测的新行分隔示例的文件：
- en: '[PRE72]'
  id: totrans-535
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'If we send five examples for prediction, we’ll get a five-element array back
    corresponding with the sigmoid output for each test example, like the following:'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们发送五个示例进行预测，我们将会收到一个五元素数组，对应每个测试示例的 sigmoid 输出，如下所示：
- en: '[PRE73]'
  id: totrans-537
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Now that we have a working model in production, let’s imagine that our data
    science team decides to change the model from XGBoost to TensorFlow since it results
    in improved accuracy and gives them access to additional tooling in the TensorFlow
    ecosystem. The model has the same input and output format, but its architecture
    and exported asset format has changed. Instead of a *.bst* file, our model is
    now in the TensorFlow SavedModel format. Ideally we can keep our underlying model
    assets separate from our application frontend—this will allow application developers
    to focus on our application’s functionality, rather than a change in model formatting
    that won’t affect the way end users interact with the model. This is where model
    versioning can help. We’ll deploy our TensorFlow model as a second version under
    the same `flight_delay_prediction` model resource. End users can upgrade to the
    new version for improved performance simply by changing the version name in the
    API endpoint.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的生产环境中已经有一个可用的模型，假设我们的数据科学团队决定将模型从 XGBoost 更改为 TensorFlow，因为这样可以提高准确性，并让他们能够使用
    TensorFlow 生态系统中的附加工具。该模型具有相同的输入和输出格式，但其架构和导出的资产格式已更改。现在我们的模型不再是一个 *.bst* 文件，而是以
    TensorFlow SavedModel 格式存储。理想情况下，我们可以将底层模型资产与应用程序前端分开管理，这样可以让应用程序开发人员专注于应用程序功能，而不是模型格式的变化，这种变化不会影响最终用户与模型的交互方式。这就是模型版本控制的用武之地。我们将把
    TensorFlow 模型作为第二个版本部署在同一个 `flight_delay_prediction` 模型资源下。用户可以通过更改 API 端点中的版本名称，轻松升级到新版本以获取更好的性能。
- en: To deploy our second version, we’ll export the model and copy it to a new subdirectory
    in the bucket we used previously. We can use the same deploy command as above,
    replacing the version name with v2 and pointing to the Cloud Storage location
    of the new model. As shown in [Figure 6-19](ch06_split_001.xhtml#the_dashboard_for_managing_models_and_v),
    we’re now able to see both deployed versions in our Cloud console.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署我们的第二个版本，我们将导出模型并复制到我们之前使用的存储桶中的新子目录中。我们可以使用与上述相同的部署命令，将版本名称替换为 v2，并指向新模型的
    Cloud Storage 位置。如 [图 6-19](ch06_split_001.xhtml#the_dashboard_for_managing_models_and_v)
    所示，我们现在可以在云控制台中看到已部署的两个版本。
- en: '![The dashboard for managing models and versions in the Cloud AI Platform console.
    ](Images/mldp_0619.png)'
  id: totrans-540
  prefs: []
  type: TYPE_IMG
  zh: '![云 AI 平台控制台中管理模型和版本的仪表板。](Images/mldp_0619.png)'
- en: Figure 6-19\. The dashboard for managing models and versions in the Cloud AI
    Platform console.
  id: totrans-541
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-19\. 云 AI 平台控制台中管理模型和版本的仪表板。
- en: Notice that we’ve also set *v2* as the new default version, so that if users
    don’t specify a version, they’ll get a response from *v2*. Since the input and
    output format of our model are the same, clients can upgrade without worrying
    about breaking changes.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们还将 *v2* 设置为新的默认版本，因此如果用户未指定版本，他们将从 *v2* 收到响应。由于我们的模型的输入和输出格式相同，客户可以升级而无需担心破坏性更改。
- en: Tip
  id: totrans-543
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Both Azure and AWS have similar model versioning services available. On Azure,
    model deployment and versioning is available with [Azure Machine Learning](https://oreil.ly/Q7NWh).
    In AWS, these services are available in [SageMaker](https://oreil.ly/r98Ve).
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 和 AWS 都提供类似的模型版本服务。在 Azure 上，模型部署和版本控制可通过 [Azure 机器学习](https://oreil.ly/Q7NWh)
    实现。在 AWS 上，这些服务则通过 [SageMaker](https://oreil.ly/r98Ve) 提供。
- en: An ML engineer deploying a new version of a model as an ML model endpoint may
    want to use an API gateway such as Apigee that determines which model version
    to call. There are various reasons for doing this, including split testing a new
    version. For split testing, maybe they want to test a model update with a randomly
    selected group of 10% of application users to track how it affects their overall
    engagement with the app. The API gateway determines which deployed model version
    to call given a user’s ID or IP address.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 ML 工程师，在将模型的新版本部署为 ML 模型端点时，可能希望使用 API 网关（例如 Apigee），该网关确定要调用哪个模型版本。进行此操作的原因有很多，包括通过分割测试测试新版本。对于分割测试，也许他们想要测试一个模型更新，观察它如何影响应用程序用户总体参与度的随机选择的10%用户组。API
    网关通过用户的 ID 或 IP 地址确定要调用的部署模型版本。
- en: With multiple model versions deployed, AI Platform allows for performance monitoring
    and analytics across versions. This lets us trace errors to a specific version,
    monitor traffic, and combine this with additional data we’re collecting in our
    application.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 部署多个模型版本后，AI平台允许在各个版本之间进行性能监控和分析。这使我们能够将错误追溯到特定版本，监控流量，并将其与我们应用程序中收集的其他数据结合起来。
- en: Trade-Offs and Alternatives
  id: totrans-547
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 折衷和替代方案
- en: While we recommend the Model Versioning design pattern over maintaining a single
    model version, there are a few implementation alternatives to the solution outlined
    above. Here, we’ll look at other serverless and open source tooling for this pattern
    and the approach of creating multiple serving functions. We’ll also discuss when
    to create an entirely new model resource instead of a version.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们推荐模型版本控制设计模式而不是维护单个模型版本，但在上述解决方案之外还有几种实施替代方案。在这里，我们将讨论其他无服务器和开源工具，以及创建多个服务功能的方法。我们还将讨论何时创建一个全新的模型资源而不是一个版本。
- en: Other serverless versioning tools
  id: totrans-549
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 其他无服务器版本控制工具
- en: We used a managed service specifically designed for versioning ML models, but
    we could achieve similar results with other serverless offerings. Under the hood,
    each model version is a stateless function with a specified input and output format,
    deployed behind a REST endpoint. We could therefore use a service like [Cloud
    Run](https://oreil.ly/KERBV), for example, to build and deploy each version in
    a separate container. Each container has a unique URL and can be invoked by an
    API request. This approach gives us more flexibility in how to configure the deployed
    model environment, letting us add functionality like server-side preprocessing
    for model inputs. In our flight example above, we may not want to require clients
    to one-hot encode categorical values. Instead, we could let clients pass the categorical
    values as strings, and handle preprocessing in our container.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了一个专门设计用于ML模型版本控制的托管服务，但我们也可以通过其他无服务器提供的服务达到类似的结果。在底层，每个模型版本都是一个具有指定输入和输出格式的无状态函数，部署在REST端点后面。因此，我们可以使用例如[Cloud
    Run](https://oreil.ly/KERBV)这样的服务，在单独的容器中构建和部署每个版本。每个容器都有唯一的URL，并且可以通过API请求调用。这种方法使我们在配置部署模型环境时具有更大的灵活性，让我们能够添加诸如模型输入的服务器端预处理功能。在我们上面的航班示例中，我们可能不希望客户需要对分类值进行独热编码。相反，我们可以让客户将分类值作为字符串传递，并在我们的容器中处理预处理。
- en: Why would we use a managed ML service like AI Platform Prediction instead of
    a more generalized serverless tool? Since AI Platform was built specifically for
    ML model deployment, it has built-in support for deploying models with GPUs optimized
    for ML. It also handles dependency management. When we deployed our XGBoost model
    above, we didn’t need to worry about installing the correct XGBoost version or
    other library dependencies.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们要使用像AI平台预测这样的托管ML服务，而不是更通用的无服务器工具？由于AI平台专门用于ML模型部署，它内置了针对ML优化的GPU支持。它还处理依赖管理。当我们部署我们的XGBoost模型时，我们无需担心安装正确的XGBoost版本或其他库依赖。
- en: TensorFlow Serving
  id: totrans-552
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow Serving
- en: Instead of using Cloud AI Platform or another cloud-based serverless offering
    for model versioning, we could use an open source tool like [TensorFlow Serving](https://oreil.ly/NzDA9).
    The recommended approach for implementing TensorFlow Serving is to use a Docker
    container via the latest [`tensorflow/serving`](https://oreil.ly/G0_Z7) Docker
    image. With Docker, we could then serve the model using whichever hardware we’d
    like, including GPUs. The TensorFlow Serving API has built-in support for model
    versioning, following a similar approach to the one discussed in the Solution
    section. In addition to TensorFlow Serving, there are also other open source model
    serving options, including [Seldon](https://oreil.ly/Cddpi) and [MLFlow](https://mlflow.org).
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是使用Cloud AI平台或其他基于云的无服务器提供的服务来进行模型版本控制，我们可以使用像[TensorFlow Serving](https://oreil.ly/NzDA9)这样的开源工具。实施TensorFlow
    Serving的推荐方法是通过最新的[`tensorflow/serving`](https://oreil.ly/G0_Z7) Docker镜像使用Docker容器。使用Docker，我们可以使用任何我们想要的硬件来提供模型，包括GPU。TensorFlow
    Serving API内置了对模型版本控制的支持，遵循与解决方案部分讨论的类似方法。除了TensorFlow Serving之外，还有其他开源模型服务选项，包括[Seldon](https://oreil.ly/Cddpi)和[MLFlow](https://mlflow.org)。
- en: Multiple serving functions
  id: totrans-554
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多个服务功能
- en: 'Another alternative to deploying multiple versions is to define multiple serving
    functions for a single version of an exported model. [“Design Pattern 16: Stateless
    Serving Function”](ch05.xhtml#design_pattern_onesix_stateless_serving) (introduced
    in [Chapter 5](ch05.xhtml#design_patterns_for_resilient_serving)) explained how
    to export a trained model as a stateless function for serving in production. This
    is especially useful when model inputs require preprocessing to transform data
    sent by the client into the format the model expects.'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 部署多个版本的另一种选择是为导出的模型的单个版本定义多个服务函数。[“设计模式16：无状态服务函数”](ch05.xhtml#design_pattern_onesix_stateless_serving)（介绍见[第5章](ch05.xhtml#design_patterns_for_resilient_serving)）解释了如何将训练好的模型导出为一个在生产中提供服务的无状态函数。当模型输入需要预处理以将客户端发送的数据转换为模型期望的格式时，这尤为有用。
- en: To handle requirements for different groups of model end users, we can define
    multiple serving functions when we export our model. These serving functions are
    part of *one* exported model version, and this model is deployed to a single REST
    endpoint. In TensorFlow, serving functions are implemented using model *signatures*,
    which define the input and output format a model is expecting. We can define multiple
    serving functions using the `@tf.function` decorator and pass each function an
    input signature.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理不同模型终端用户组的需求，我们可以在导出模型时定义多个服务函数。这些服务函数是*一个*导出模型版本的一部分，并且该模型被部署到单个REST端点。在TensorFlow中，服务函数使用模型*签名*实现，定义模型期望的输入和输出格式。我们可以使用`@tf.function`装饰器定义多个服务函数，并为每个函数传递一个输入签名。
- en: 'In the application code where we invoke our deployed model, we would determine
    which serving function to use based on the data sent from the client. For example,
    a request such as:'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用程序代码中，当我们调用部署的模型时，我们会根据从客户端发送的数据确定要使用哪个服务函数。例如，这样的请求：
- en: '[PRE74]'
  id: totrans-558
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'would be sent to the exported signature called `get_genre`, whereas a request
    like:'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 将会发送到名为`get_genre`的导出签名，而像以下这样的请求：
- en: '[PRE75]'
  id: totrans-560
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: would be sent to the exported signature called `get_genre_with_explanation`.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 将会发送到名为`get_genre_with_explanation`的导出签名。
- en: Deploying multiple signatures can, therefore, solve the backward compatibility
    problem. However, there is a significant difference—there is only one model, and
    when that model is deployed, all the signatures are simultaneously updated. In
    our original example of changing the model from providing just one genre to providing
    multiple genres, the model architecture changed. The multiple-signature approach
    wouldn’t work with that example since we have two different models. The multiple-signature
    solution is also not appropriate when we wish to keep different versions of the
    model separate and deprecate the older version over time.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，部署多个签名可以解决向后兼容性问题。然而，有一个显著的区别——只有一个模型，当部署该模型时，所有签名同时更新。在我们从提供单一流派到提供多个流派的原始示例中，模型架构已更改。多签名方法在这个例子中不适用，因为我们有两个不同的模型。当我们希望保持模型的不同版本分开并随时间淘汰旧版本时，多签名解决方案也不合适。
- en: Using multiple signatures is better than using multiple versions if you wish
    to maintain *both* model signatures going forward. In the scenario where there
    are some clients who simply want the best answer and other clients who want both
    the best answer and an explanation, there is an added benefit to updating all
    the signatures with a newer model instead of having to update versions one by
    one every time the model is retrained and redeployed.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望将来维护*两个*模型签名，使用多个签名比使用多个版本更好。在有些客户只想要最佳答案，而其他客户希望既有最佳答案又有解释的场景中，将所有签名与新模型一起更新具有额外的好处，而不是每次模型重新训练和重新部署时逐个更新版本。
- en: What are some scenarios where we might want to maintain both versions of the
    model? With a text classification model, we may have some clients that need to
    send raw text to the model, and others that are able to transform raw text into
    matrices before getting a prediction. Based on the request data from the client,
    the model framework can determine which serving function to use. Passing text
    embedding matrices to a model is less expensive than preprocessing raw text, so
    this is an example where multiple serving functions could reduce server-side processing
    time. It’s also worth noting that we can have multiple serving functions *with*
    multiple model versions, though there is a risk that this could create too much
    complexity.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 有哪些情况下我们可能希望保留模型的两个版本？对于文本分类模型，我们可能有一些客户需要将原始文本发送给模型，而另一些客户能够在获取预测之前将原始文本转换为矩阵。根据客户的请求数据，模型框架可以确定使用哪种服务函数。将文本嵌入矩阵传递给模型比预处理原始文本要便宜，所以这是一个多个服务函数能够减少服务器端处理时间的例子。值得注意的是，我们可以拥有多个服务函数
    *和* 多个模型版本，尽管这可能会增加太多复杂性的风险。
- en: New models versus new model versions
  id: totrans-565
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 新模型与新模型版本
- en: Sometimes it can be difficult to decide whether to create another model version
    or an entirely new model resource. We recommend creating a new model when a model’s
    prediction task changes. A new prediction task typically results in a different
    model output format, and changing this could result in breaking existing clients.
    If we’re unsure about whether to use a new version or model, we can think about
    whether we want existing clients to upgrade. If the answer is yes, chances are
    we have improved the model without changing the prediction task, and creating
    a new version will suffice. If we’ve changed the model in a way that would require
    users to decide whether they want the update, we’ll likely want to create a new
    model resource.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 有时很难决定是创建另一个模型版本还是完全新的模型资源。当模型的预测任务发生变化时，我们建议创建一个新模型。新的预测任务通常会导致不同的模型输出格式，更改这一点可能会导致破坏现有客户端。如果我们不确定是否要使用新版本或模型，我们可以考虑是否希望现有客户端升级。如果答案是肯定的，那么我们很可能已经改进了模型而没有改变预测任务，并且创建一个新版本就足够了。如果我们以一种需要用户决定是否要进行更新的方式更改了模型，那么我们可能会希望创建一个新的模型资源。
- en: 'To see this in practice, let’s return to our flight prediction model to see
    an example. The current model has defined what it considers a delay (30+ minutes
    late), but our end users may have different opinions on this. Some users think
    just 15 minutes late counts as delayed, whereas others think a flight is only
    delayed if it’s over an hour late. Let’s imagine that we’d now like our users
    to be able to incorporate their own definition of delayed rather than use ours.
    In this case we’d use [“Design Pattern 5: Reframing ”](ch03.xhtml#design_pattern_five_reframing)
    (discussed in [Chapter 3](ch03.xhtml#problem_representation_design_patterns))
    to change this to a regression model. The input format to this model is the same,
    but the output is now a numerical value representing the delay prediction.'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到这一实践，让我们返回到我们的航班预测模型，看一个例子。当前模型已经定义了它认为的延迟（晚30分钟以上），但是我们的最终用户可能对此有不同的看法。有些用户认为只要晚15分钟就算是延迟，而另一些用户认为只有晚一个小时以上才算延迟。让我们想象一下，我们现在希望我们的用户能够将他们自己对延迟的定义纳入考虑，而不是使用我们的定义。在这种情况下，我们将使用
    [“设计模式5：重新定义 ”](ch03.xhtml#design_pattern_five_reframing)（在 [第3章](ch03.xhtml#problem_representation_design_patterns)
    中讨论）来将其更改为回归模型。该模型的输入格式保持不变，但现在的输出是代表延迟预测的数值。
- en: The way our model users parse this response will obviously be different than
    the first version. With our latest regression model, app developers might choose
    to display the predicted delay when users search for flights, replacing something
    like “This flight is usually delayed more than 30 minutes” from the first version.
    In this scenario, the best solution is to create a new model *resource*, perhaps
    called `flight_model_regression`, to reflect the changes. This way, app developers
    can choose which to use, and we can continue to make performance updates to each
    model by deploying new versions.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的模型用户解析此响应时，与第一个版本显然是不同的。借助我们最新的回归模型，应用开发者可以选择在用户搜索航班时显示预测延迟，而不是像第一个版本中那样替换“这个航班通常延迟超过30分钟”。在这种情况下，最好的解决方案是创建一个新的模型
    *资源*，可能称为 `flight_model_regression`，以反映这些变化。这样，应用开发者可以选择使用哪一个，并且我们可以通过部署新版本继续对每个模型进行性能更新。
- en: Summary
  id: totrans-569
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter focused on design patterns that address different aspects of reproducibility.
    Starting with the *Transform* design, we saw how this pattern is used to ensure
    reproducibility of the data preparation dependencies between the model training
    pipeline and the model serving pipeline. This is achieved by explicitly capturing
    the transformations applied to convert the model inputs into the model features.
    The *Repeatable Splitting* design pattern captures the way data is split among
    training, validation, and test datasets to ensure that an example used in training
    is never used for evaluation or testing even as the dataset grows.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点讨论了解决可重复性不同方面的设计模式。从*Transform*设计开始，我们看到了该模式如何用于确保数据准备在模型训练管道和模型服务管道之间的可重复性。通过明确捕获应用的转换，将模型输入转换为模型特征。*Repeatable
    Splitting*设计模式捕获了数据在训练、验证和测试数据集之间的分割方式，以确保训练中使用的示例永远不会用于评估或测试，即使数据集在增长。
- en: The *Bridged Schema* design pattern looks at how to ensure reproducibility when
    a training dataset is a hybrid of newer data and older data with a different schema.
    This allows for combining two datasets with different schemas in a consistent
    way for training. Next, we discussed the *Windowed Inference* design pattern,
    which ensures that when features are calculated in a dynamic, time-dependent way,
    they can be correctly repeated between training and serving. This design pattern
    is particularly useful when machine learning models require features that are
    computed from aggregates over time windows.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: '*Bridged Schema*设计模式研究了在训练数据集是具有不同架构的新旧数据混合时如何确保可重复性。这允许以一致的方式组合两个具有不同架构的数据集进行训练。接下来，我们讨论了*Windowed
    Inference*设计模式，它确保在动态、时间相关方式计算特征时，这些特征在训练和服务之间能够正确重复。当机器学习模型需要从时间窗口聚合计算特征时，这个设计模式尤为有用。'
- en: The *Workflow Pipeline* design pattern addresses the problem of creating an
    end-to-end reproducible pipeline by containerizing and orchestrating the steps
    in our machine learning workflow. Next, we saw how the *Feature Store* design
    pattern can be used to address reproducibility and reusability of features across
    different machine learning jobs. Lastly, we looked at the *Model Versioning* design
    pattern, where backward compatibility is achieved by deploying a changed model
    as a microservice with a different REST endpoint.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '*Workflow Pipeline*设计模式解决了通过容器化和编排我们机器学习工作流中的步骤来创建端到端可重复的管道的问题。接下来，我们看到了*Feature
    Store*设计模式如何用于跨不同机器学习任务的特征的可重复性和可重用性。最后，我们看了*Model Versioning*设计模式，通过将更改后的模型部署为具有不同REST端点的微服务来实现向后兼容性。'
- en: In the next chapter, we look into design patterns that help carry out AI responsibly.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章中，我们将探讨设计模式，这些模式有助于负责任地执行人工智能。
- en: ^([1](ch06_split_000.xhtml#ch01fn25-marker)) Note that the overall probability
    distribution function doesn’t need to be uniform—all that we require is that the
    original bins are narrow enough for us to be able to approximate the probability
    distribution function by a staircase function. Where this assumption fails is
    when we have a highly skewed distribution that was inadequately sampled in the
    older data. In such cases, it is possible that 3.46 is more likely than 3.54,
    and this would need to be reflected in the bridged dataset.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch06_split_000.xhtml#ch01fn25-marker)) 注意，整体概率分布函数不需要是均匀的——我们只需要原始箱子足够窄，以便我们能够通过阶梯函数近似概率分布函数。当这种假设失败时，是因为旧数据中高度偏斜的分布未经充分采样。在这种情况下，可能会出现3.46比3.54更有可能的情况，这需要在桥接数据集中反映出来。
- en: ^([2](ch06_split_000.xhtml#ch01fn26-marker)) In other words, we are computing
    the average.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch06_split_000.xhtml#ch01fn26-marker)) 换句话说，我们正在计算平均值。
- en: '^([3](ch06_split_001.xhtml#ch01fn27-marker)) For more on data validation, see
    [“Design Pattern 30: Fairness Lens”](ch07.xhtml#design_pattern_threezero_fairness_lens)
    in [Chapter 7, *Responsible AI*](ch07.xhtml#responsible_ai).'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch06_split_001.xhtml#ch01fn27-marker)) 关于数据验证的更多信息，请参阅[“设计模式30：公平性镜头”](ch07.xhtml#design_pattern_threezero_fairness_lens)在[第7章，“负责任的人工智能”](ch07.xhtml#responsible_ai)中。
- en: ^([4](ch06_split_001.xhtml#ch01fn28-marker)) SSHS stands for [Saffir–Simpson
    Hurricane Scale](https://oreil.ly/62kf3), and is a scale from 1 to 5 used to measure
    the strength and severity of a hurricane. Note that the ML model does not forecast
    the severity of the hurricane at a later time. Instead, it simply learns the wind
    speed thresholds used in the Saffir–Simpson scale.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch06_split_001.xhtml#ch01fn28-marker)) SSHS代表[Saffir–Simpson飓风等级](https://oreil.ly/62kf3)，是一个用于测量飓风强度和严重程度的1到5级的等级。请注意，ML模型不预测飓风在稍后时间的严重程度，而是简单地学习Saffir–Simpson等级中使用的风速阈值。
- en: ^([5](ch06_split_001.xhtml#ch01fn29-marker)) While deployment is the last step
    in our example pipeline, production pipelines often include more steps, such as
    storing the model in a shared repository or executing a separate serving pipeline
    that does CI/CD and testing.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch06_split_001.xhtml#ch01fn29-marker)) 尽管部署是我们示例流水线中的最后一步，但生产流水线通常包括更多步骤，例如将模型存储在共享存储库中或执行单独的服务流水线，进行CI/CD和测试。
- en: ^([6](ch06_split_001.xhtml#ch01fn30-marker)) Note that in order to run TFX pipelines
    on AI Platform, you currently need to host your code on GCR and can’t use another
    container registry service like DockerHub.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch06_split_001.xhtml#ch01fn30-marker)) 请注意，为了在AI平台上运行TFX流水线，您目前需要将代码托管在GCR上，不能使用像DockerHub这样的其他容器注册服务。
- en: '^([7](ch06_split_001.xhtml#ch01fn32-marker)) The data is available in the BigQuery
    table: *bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2016*.'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch06_split_001.xhtml#ch01fn32-marker)) 数据存储在BigQuery表中：*bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2016*。
- en: '^([8](ch06_split_001.xhtml#ch01fn33-marker)) See the Gojek blog, “[Feast: Bridging
    ML Models and Data](https://oreil.ly/YVta5).”'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: '^([8](ch06_split_001.xhtml#ch01fn33-marker)) 参见Gojek博客，“[Feast: Bridging ML
    Models and Data](https://oreil.ly/YVta5)”。'
