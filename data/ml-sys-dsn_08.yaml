- en: 7 Validation schemas
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 验证方案
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Ensuring reliable evaluation
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保可靠的评估
- en: Standard validation schemas
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准验证方案
- en: Nontrivial validation schemas
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非平凡验证方案
- en: Split updating procedure
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分割更新程序
- en: Validation schemas as part of the design document
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证方案作为设计文档的一部分
- en: Building a robust evaluation process is essential for a machine learning (ML)
    system, and in this chapter, we will cover the process of building a proper validation
    schema to achieve confident estimates of system performance. We will touch upon
    typical validation schemas, as well as how to select the right validation based
    on the specifics of a given problem and what factors to consider when designing
    the evaluation process in the wild.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个稳健的评估过程对于机器学习（ML）系统至关重要，在本章中，我们将介绍构建适当的验证方案以实现系统性能自信估计的过程。我们将涉及典型的验证方案，以及如何根据给定问题的具体情况进行适当的验证选择，以及在设计野外评估过程时需要考虑哪些因素。
- en: A proper validation procedure aims to imitate what knowledge we are supposed
    to have and what knowledge can be dropped while operating in a real-life environment.
    This is somewhat connected to the overfitting problem or generalization, which
    we’ll cover in detail in chapter 9\.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 适当的验证程序旨在模仿我们在实际操作环境中应该拥有的知识，以及可以丢弃的知识。这与过度拟合问题或泛化问题有关，我们将在第9章中详细讨论。
- en: It also provides a reliable and robust estimation of a system’s performance,
    ideally with some theoretical guarantees. As an example, we guarantee that a real
    value will be in the range between the lower confidence bound and upper confidence
    bound 95 times out of 100 (this case will be covered in a campfire story from
    Valerii later in the chapter). It also helps detect and prevent data leaks, overfitting,
    and divergence between offline and online performance.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 它还提供了一个可靠且稳健的系统性能估计，理想情况下带有一些理论保证。例如，我们保证在100次中有95次真实值将在下限置信区间和上限置信区间之间（这种情况将在本章后面的篝火故事中介绍）。它还有助于检测和防止数据泄露、过度拟合以及离线和在线性能之间的差异。
- en: Performance estimation is the primary goal of validation. We use validation
    to estimate the model’s predictive power on unseen data, and the preferred schema
    is usually the one with the highest reliability and robustness (i.e., low bias/low
    variance).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 性能估计是验证的主要目标。我们使用验证来估计模型在未见数据上的预测能力，通常首选的方案是具有最高可靠性和稳健性的方案（即低偏差/低方差）。
- en: As long as we have a reliable and robust performance estimation, we can use
    it for various things, like hyperparameter optimization, architecture, algorithm,
    and feature selection. To some extent, there is a similarity to A/B testing where
    schema yielding lower variance provides higher sensitivity, which we will cover
    later in the chapter.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 只要我们有一个可靠且稳健的性能评估，我们就可以用它来做各种事情，比如超参数优化、架构、算法和特征选择。在某种程度上，它与A/B测试相似，其中产生较低方差的模式提供更高的敏感性，这一点我们将在本章后面进行讨论。
- en: 7.1 Reliable evaluation
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 可靠的评估
- en: 'When validating anything, it is almost always a good idea to build a stable
    and reliable pipeline that produces repeatable results (see figure 7.1). Standard
    advice that you most probably can find in the literature comes down to the following
    three classic conditions: all you need to do is to split the data into training,
    validation, and test datasets. A training set is used for model training, a validation
    set is designed to evaluate performance during training, and a test set is used
    to calculate final metrics. This three-set approach is well known to those familiar
    with competitive ML (e.g., challenges hosted by Kaggle) or academia. At the same
    time, there are subtle but important distinctions within applied ML that we will
    discuss further in this chapter.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当验证任何事物时，几乎总是一个好的主意，建立一个稳定可靠的流水线，产生可重复的结果（见图7.1）。你很可能在文献中找到的标准建议归结为以下三个经典条件：你所需要做的就是将数据分为训练集、验证集和测试集。训练集用于模型训练，验证集旨在评估训练过程中的性能，测试集用于计算最终指标。这种三集方法对于那些熟悉竞争性机器学习（例如，Kaggle举办的活动）或学术界的人来说是众所周知的。同时，在应用机器学习（ML）中存在一些微妙但重要的区别，我们将在本章中进一步讨论。
- en: '![figure](../Images/CH07_F01_Babushkin.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F01_Babushkin.png)'
- en: Figure 7.1 Basic high-level model development cycle
  id: totrans-15
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.1 基本高级模型开发周期
- en: 'There are some points to pay attention to:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 有几点需要注意：
- en: A simple train-validation-test split assumes that all three datasets come from
    the same distribution and that this will hold in the future. This is a strong
    assumption that has to be validated by itself. If this assumption doesn’t hold,
    there is no guarantee of future performance.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单的训练-验证-测试分割假设所有三个数据集都来自相同的分布，并且这种分布在未来将保持不变。这是一个必须自己验证的强假设。如果这个假设不成立，就无法保证未来的性能。
- en: 'There must be a repeatable use of the validation set to estimate model performance.
    Overestimating the model’s performance based on the validation set leads to a
    bias and overfit toward this set. Stop and think: when we talk about things like
    hyperparameter optimization, feature selection, or model selection from a high-level
    perspective, it is basically a part of the learning process as well. By induction,
    the test set can be abused in the same manner.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证集必须能够重复使用来估计模型性能。基于验证集高估模型性能会导致偏差和过度拟合。停下来思考一下：当我们从高层次的角度谈论超参数优化、特征选择或模型选择时，这基本上也是学习过程的一部分。通过归纳，测试集也可能被以同样的方式滥用。
- en: That is why using the same validation split over and over again for evaluation
    and searching for optimal hyperparameters or anything else will lead to biased/overfitted
    and nonrobust results. For this reason, instead of viewing validation as the thing
    done once at the very beginning, we view it as a continuous process to be done
    repeatedly once the context of the system changes (e.g., there are new sources
    of data, new features, potential feedback loops caused by model usage, etc.).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 正因如此，反复使用相同的验证分割进行评估和寻找最优超参数或其他任何东西会导致偏差/过度拟合和非鲁棒的结果。因此，我们不是将验证视为一开始就完成的事情，而是将其视为在系统环境发生变化（例如，有新的数据来源、新特征、模型使用可能引起的潜在反馈循环等）后需要反复进行的持续过程。
- en: We are never 100% sure what the world will bring next; that’s why we must expect
    the unexpected.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们永远无法100%确定世界会带来什么；这就是为什么我们必须预料到意外。
- en: 7.2 Standard schemas
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 标准模式
- en: As practice shows, you won’t need to reinvent the wheel when picking a validation
    schema for your ML system. Most of the standard schemas are time-tested and well-performing
    solutions that mostly require you to pick one that fits the requirements of your
    project. We will briefly cover these schemas in several subsections.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 实践表明，在选择机器学习系统的验证模式时，你不需要重新发明轮子。大多数标准模式都是经过时间考验且表现良好的解决方案，主要需要你选择一个适合你项目需求的模式。我们将在几个子节中简要介绍这些模式。
- en: Classic validation schemas are well implemented in the evergreen Python ML library
    scikit-learn, and all the relevant documentation is worth reading if you have
    doubts about your knowledge of the material. The information is available at [https://mng.bz/aV6B](https://mng.bz/aV6B).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的验证模式在永不过时的Python机器学习库scikit-learn中得到了很好的实现，如果你对材料的了解有疑问，所有相关文档都值得一读。信息可在[https://mng.bz/aV6B](https://mng.bz/aV6B)找到。
- en: 7.2.1 Holdout sets
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 保留集
- en: We’ll start by splitting the dataset into two or more chunks. Probably the golden
    classic mentioned in almost any book on ML is the training/validation/test split
    we discussed earlier.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先将数据集分为两个或更多部分。可能是在几乎所有关于机器学习的书籍中提到的黄金经典——我们之前讨论过的训练/验证/测试分割。
- en: With this approach, we partition data into three sets (it might be random or
    based on a specific criterion or strata) with different ratios—for example, 60/20/20
    (see figure 7.2). The percentage may vary depending on the number of samples and
    metrics (the amount of data, metric variance, sensitivity, robustness, and reliability
    requirements). Empirically, the bigger the full dataset, the smaller the share
    that’s dedicated to validation and testing, so the training set is growing faster.
    The test set (i.e., outer validation) is used for the final model evaluation and
    should never be used for any other purpose. Meanwhile, we can use the validation
    set (i.e., inner validation) primarily for model comparison or tuning hyperparameters.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 采用这种方法，我们将数据分为三个集合（可能是随机的，也可能基于特定的标准或层），具有不同的比例——例如，60/20/20（见图7.2）。百分比可能根据样本数量和指标（数据量、指标方差、敏感性、鲁棒性和可靠性要求）而变化。经验上，整个数据集越大，分配给验证和测试的部分就越小，因此训练集增长得更快。测试集（即外部验证）用于最终模型评估，绝不应用于其他任何目的。同时，我们可以主要使用验证集（即内部验证）进行模型比较或调整超参数。
- en: '![figure](../Images/CH07_F02_Babushkin.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F02_Babushkin.png)'
- en: Figure 7.2 Standard by-the-book data split
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.2 标准的按部就班的数据分割
- en: 7.2.2 Cross-validation
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 交叉验证
- en: The holdout validation is a good choice for computationally expensive models,
    such as deep learning models. It is easy to implement and doesn’t add much time
    to the learning loop.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于计算成本高的模型，如深度学习模型，保留法验证是一个不错的选择。它易于实现，并且不会给学习循环增加太多时间。
- en: But let’s remember that we take a single random subsample from all the data.
    We are not reusing all available data that might lead to biased evaluation or
    underutilization of available data. What’s the worst part? We get a single number
    that does not allow us to understand the distribution of the estimates.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 但让我们记住，我们从所有数据中抽取一个单独的随机子样本。我们并没有重用所有可能的数据，这可能导致评估偏差或未充分利用可用数据。最糟糕的部分是什么？我们得到一个单一的数字，它不允许我们了解估计值的分布。
- en: The silver bullet for resolving such a problem in statistics is a bootstrap
    procedure. In the validation case, it would look like randomly sampling train
    validation splits many times, training and evaluating the model each iteration.
    Training a model is time-consuming, and we want to iterate quickly for general
    parameter tweaking and experimentation. So how can we do it?
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学中解决此类问题的银弹是自助法。在验证情况下，它看起来像是多次随机采样训练验证分割，每次迭代训练和评估模型。训练模型是耗时的，我们希望快速迭代以进行一般参数调整和实验。那么我们如何做呢？
- en: We can use a similar but simplified sampling procedure called *cross-validation*.
    We can split data into K folds (usually five), exclude each of them one by one,
    fit the model to the K – 1 folds of data, and measure performance on the excluded
    fold. Hence, we get K estimates and can calculate their mean and standard deviation.
    As a result, we get five numbers instead of one, which is more representative
    (see figure 7.3).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用一个类似但简化的采样过程，称为*交叉验证*。我们可以将数据分成K折（通常为五折），逐个排除它们，将模型拟合到K-1折的数据，并在排除的折上测量性能。因此，我们得到K个估计值，可以计算它们的平均值和标准差。结果，我们得到五个数字而不是一个，这更具代表性（见图7.3）。
- en: '![figure](../Images/CH07_F03_Babushkin.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F03_Babushkin.png)'
- en: 'Figure 7.3 K-fold split: each sample is assigned to a fold, and each fold provides
    validation once and trains once in the rest of the training rounds.'
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.3 K折分割：每个样本被分配到一个折，每个折在剩余的训练轮次中提供一次验证和一次训练。
- en: 'There are several variations of cross-validation, including:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证有几种变体，包括：
- en: '*Stratified cross-validation* (we need to maintain the balance of classes).'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分层交叉验证*（我们需要保持类别的平衡）。'
- en: '*Repeated cross-validation* (we split into K folds N times, so that each object
    participates in the evaluation N times).'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*重复交叉验证*（我们将数据分成K折N次，这样每个对象参与评估N次）。'
- en: '*Grouped cross-validation* (when objects are similar within groups, we may
    want to avoid a leak; the entire group must be fully included either in the training
    sample or in the validation sample).'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分组交叉验证*（当组内对象相似时，我们可能希望避免泄露；整个组必须完全包含在训练样本或验证样本中）。'
- en: 'Suppose we predict the flow rate of oil at hundreds of wells. Wells are grouped
    based on their location: neighboring wells extract oil from the same oil field,
    so their production affects each other. For this case, a grouped K-fold is a reasonable
    choice. Finding a proper criterion for grouping samples while assigning them to
    folds is one of the key decisions for validation overall, and mistakes here greatly
    affect the result.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们预测数百个油井的油流量。油井根据其位置进行分组：相邻的油井从同一油田提取石油，因此它们的产量相互影响。在这种情况下，分组K折交叉验证是一个合理的选择。在将样本分配到折时找到一个合适的分组标准是验证过程中的一个关键决策，这里的错误会极大地影响结果。
- en: 7.2.3 The choice of K
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.3 K的选择
- en: 'The only question left is what number of folds to choose. The choice of K is
    dictated by three variables: bias, variance, and computation time. The rule of
    thumb is to use K = 5, which provides a good balance between bias and variance.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的唯一问题是选择多少折。K的选择受三个变量的影响：偏差、方差和计算时间。经验法则是使用K=5，它在偏差和方差之间提供了良好的平衡。
- en: An extreme case for K is a leave-one-out cross-validation when each fold contains
    a single sample of data; thus K is equal to the overall number of samples in the
    dataset. This schema is the worst in terms of computation time and variance, but
    it’s the best in terms of bias.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: K的一个极端情况是留一法交叉验证，其中每个折包含一个数据样本；因此K等于数据集中样本的总数。这种方案在计算时间和方差方面是最差的，但在偏差方面是最好的。
- en: 'There is a classic paper by Ron Kohavi from 1995 titled “A Study of Cross-Validation
    and Bootstrap for Accuracy Estimation and Model Selection” ([https://mng.bz/4pn5](https://mng.bz/4pn5))
    that provides the following guidelines:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Ron Kohavi 在 1995 年发表了一篇经典论文，题为“关于交叉验证和自助法在准确度估计和模型选择中的应用研究”（[https://mng.bz/4pn5](https://mng.bz/4pn5)），其中提供了以下指导原则：
- en: Increasing the number of folds reduces bias and improves performance estimation.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加折叠数量可以减少偏差并提高性能估计。
- en: At the same time, variance increases along with the number of folds due to a
    lower number of samples in each validation fold (the estimates become too noisy).
    With an assumption of consistent bias, the sensitivity of the validation schema
    is determined by variance.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同时，由于每个验证折叠中的样本数量减少（估计变得过于嘈杂），随着折叠数量的增加，方差也会增加。在假设偏差一致的情况下，验证方案敏感性由方差决定。
- en: Using repeated cross-validation for model comparison goals with K = 2 or K =
    3 repeated 10 to 20 times is a good idea. However, for the bias optimization,
    repeated K-fold isn’t helpful since estimates between different repeats already
    share consistent bias.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于模型比较目标，使用重复交叉验证（K = 2 或 K = 3，重复 10 到 20 次）是一个好主意。然而，对于偏差优化，重复 K 折叠并不有帮助，因为不同重复之间的估计已经共享一致的偏差。
- en: The number of required folds naturally decreases with the growth of the dataset
    size. The more data you have in each fold, the more representative it is.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着数据集大小的增长，所需的折叠数量自然会减少。每个折叠中的数据越多，它就越具有代表性。
- en: For simpler models (which is the case when dealing with baseline solutions)
    and well-behaved datasets, you expect both bias and variance to decrease with
    the number of folds.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于更简单的模型（在处理基线解决方案时通常是这种情况）和表现良好的数据集，你期望偏差和方差随着折叠数量的增加而减少。
- en: It is important to remember that the validation schema’s high sensitivity (i.e.,
    low variance) only matters when the changes we try to catch in the model’s performance
    are small.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住，验证方案的高敏感性（即低方差）只有在我们试图捕捉模型性能变化很小的情况下才有意义。
- en: 7.2.4 Time-series validation
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.4 时间序列验证
- en: When dealing with time-sensitive data, we can’t sample data randomly. Sales
    of products on neighboring days share some information with each other. Similarly,
    recent user actions provide a hint on some aspects of their later actions. But
    we can’t predict the past based on data from the future. In time series, the distribution
    of patterns is not uniform along the dataset, and we must figure out other kinds
    of validation schemas. How do we evaluate the model in this case?
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理时间敏感数据时，我们不能随机采样数据。相邻日期的产品销售之间共享一些信息。同样，最近的用户行为为他们的后续行为提供了一些线索。但我们不能根据未来的数据预测过去。在时间序列中，模式分布沿数据集不是均匀的，我们必须找出其他类型的验证方案。在这种情况下，我们如何评估模型？
- en: Validation schemas used in time-series data are similar to the holdout set and
    cross-validation but with nonrandom splitting by timestamp. The recommendations
    for choosing the number of folds and their size in rolling cross-validation are
    similar to the ordinal K-fold.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间序列数据中使用的验证方案类似于保留集和交叉验证，但通过时间戳进行非随机分割。在滚动交叉验证中选择折叠数量及其大小的建议类似于有序 K 折叠。
- en: 'Time-series validation adds several extra degrees of freedom that need to be
    considered. A great paper, “Evaluating Time Series Forecasting Models” by Cerqueira
    et al. ([https://arxiv.org/pdf/1905.11744.pdf](https://arxiv.org/pdf/1905.11744.pdf)),
    elaborates on the following points:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列验证增加了需要考虑的额外自由度。一篇优秀的论文，“评估时间序列预测模型”由 Cerqueira 等人撰写（[https://arxiv.org/pdf/1905.11744.pdf](https://arxiv.org/pdf/1905.11744.pdf)），详细阐述了以下观点：
- en: '*Window size*—The size of the testing set should reflect how far we make the
    forecast and how long the model will stay in production before retraining.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*窗口大小*—测试集的大小应该反映我们预测的距离以及模型在重新训练之前将保持生产状态的时间。'
- en: '*Training size*—There are two options in regard to the amount of data used
    for training: we either use all available history or limit the training size to
    one or two previous periods (those can be weeks, months, or years, depending on
    a given seasonality) and discard all previous history as irrelevant.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*训练大小*—关于用于训练的数据量，有两种选择：我们要么使用所有可用的历史数据，要么将训练大小限制在一到两个前期（这些可以是周、月或年，具体取决于给定的季节性）并丢弃所有以前的历史数据，因为它们是不相关的。'
- en: '*Seasonality*—There are patterns in data that depend on cycles of days, weeks,
    months, quarters, or years. We should select sizes of testing and training sets
    accordingly to capture these patterns. For example, to capture yearly patterns,
    the training data should include at least 2 years of history. Another example
    is a weekly seasonality in a testing set: to minimize variance between folds,
    each should contain the same days of the week (so we take whole weeks in each
    fold).'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*季节性*—数据中存在依赖于日、周、月、季度或年循环的模式。我们应该相应地选择测试集和训练集的大小，以捕捉这些模式。例如，为了捕捉年度模式，训练数据应至少包含2年的历史数据。另一个例子是测试集中的每周季节性：为了最小化折叠之间的方差，每个折叠应包含相同的日子（因此我们在每个折叠中取整个周）。'
- en: '*Gap*—There can be a gap between training and testing data, which pursues two
    goals. First, it prepares us for a lag in receiving new data (which leads to a
    lag in features), and second, it makes training and testing data less correlated,
    thus minimizing the risk of a leak. For instance, we may skip 2 to 3 days between
    training and testing sets in both cases.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*差距*—训练数据和测试数据之间可能存在差距，这追求两个目标。首先，它使我们为接收新数据的延迟（导致特征的延迟）做好准备，其次，它使训练数据和测试数据的相关性降低，从而最小化泄露的风险。例如，在两种情况下，我们可能在训练集和测试集之间跳过2到3天。'
- en: While time-series validation is one of the most defect-sensitive validation
    methods, relying solely on the simple “don’t look at future data while training”
    rule would be far too shortsighted. Following this rule can save you from 95%
    of typical mistakes; still, there are cases where you may need to break it. For
    example, ML applied to financial data (such as stock market time series) is known
    for its high bar in precise validation requirements. At the same time, some experts
    in the area highlight that trivial time-series validation, as shown in figure
    7.4, can lead to overfitting caused by limited data subsets (for more details,
    see “Backtesting Through Cross-Validation,” chapter 12 of Marcos Lopez de Prado’s
    *Advances in Financial Machine Learning*; Wiley). A similar reason to violate
    this rule may be rooted in your need to estimate how the model performs in anomaly
    scenarios. To get this signal, you can train the model on data from 2017 to 2019
    and 2021 to 2023 and later test it on data from the COVID period of 2020\. Such
    a split barely works as the default validation schema but still can be useful
    as auxiliary information.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然时间序列验证是最敏感的验证方法之一，但仅仅依靠简单的“训练时不要看未来数据”规则将会过于短视。遵循这个规则可以让你避免95%的典型错误；然而，仍然有一些情况下你可能需要打破这个规则。例如，应用于金融数据（如股市时间序列）的机器学习因其对精确验证要求的高标准而闻名。同时，该领域的某些专家强调，如图7.4所示，简单的时序验证可能导致由数据子集有限引起的过拟合（更多细节，请参阅Marcos
    Lopez de Prado的《金融机器学习进展》第12章“通过交叉验证进行回测”； Wiley）。违反此规则的一个类似原因可能源于你需要估计模型在异常情况下的表现。为了获得这个信号，你可以在2017年至2019年和2021年至2023年的数据上训练模型，然后在该2020年COVID时期的数据上进行测试。这种分割几乎作为默认的验证方案不起作用，但作为辅助信息仍然可能有用。
- en: '![figure](../Images/CH07_F04_Babushkin.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH07_F04_Babushkin.png)'
- en: Figure 7.4 Standard time-based split. The test dataset always follows the train
    one, so train samples are “past” and test is “future.”
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.4标准基于时间的分割。测试数据集始终跟随训练数据集，因此训练样本是“过去”，测试是“未来”。
- en: 'Sometimes you need to use a combination of different schemas. In the earlier
    example of flow rate prediction, we might combine grouped K-fold validation with
    time-series validation:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 有时你需要使用不同方案的组合。在早期流量预测的例子中，我们可能会结合分组K折验证和时间序列验证：
- en: '[PRE0]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Campfire story from Valerii
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Valerii的篝火故事
- en: When I was working in the dynamic pricing service of a large online retailer,
    we were set to build a sales forecast model that would predict sales volumes 1
    week ahead, along with postprocessing the predictions to determine optimal prices.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当我在一家大型在线零售商的动态定价服务部门工作时，我们旨在构建一个销售预测模型，该模型可以预测一周后的销售量，并处理预测的后处理以确定最佳价格。
- en: Initially, we took the previous week for validation. As new daily data became
    available, the validation week was shifted 1 day forward. However, it was observed
    that the performance metrics on the validation set showed significant fluctuations
    from day to day. This made it difficult to determine how the model’s quality was
    changing in the context of periodic feature additions and adjustments, as well
    as changes to prediction postprocessing.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，我们将上一周用于验证。随着新的每日数据变得可用，验证周被向前推进了1天。然而，观察到验证集上的性能指标每天都会出现显著的波动。这使得在周期性特征添加和调整以及预测后处理变化的背景下，很难确定模型质量的变化。
- en: We wanted to understand the fluctuations in the metrics, and after thoroughly
    investigating the issue, we discovered that the variety of products changed by
    15% week to week and by 40% month to month. Additionally, the sales dynamics of
    individual products were found to be highly heterogeneous (e.g., 10 units sold
    today but 0 units sold in the next 2 days). As a result, we relied on changes
    in the metric, which were caused by the daily updates to the validation set, rather
    than on actual changes in the model’s quality.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想了解指标波动的原因，经过彻底调查这个问题后，我们发现产品的种类每周变化了15%，每月变化了40%。此外，发现单个产品的销售动态高度异质（例如，今天售出10个单位，但在接下来的2天内售出0个单位）。因此，我们依赖于由每日更新的验证集引起的指标变化，而不是模型质量的实际变化。
- en: To address this issue, we implemented a “delayed shift” validation approach.
    Instead of updating the validation set daily, we updated it once a month while
    still using a 1-week validation period. This ensured that the data used for calculating
    metrics remained relatively fresh (no older than 1 month) while keeping the validation
    set fixed for an entire month. Consequently, the comparison between the two models
    became more meaningful, and the performance metrics became far less noisy.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们实施了一种“延迟移动”验证方法。我们不是每天更新验证集，而是每月更新一次，同时仍然使用一周的验证期。这确保了用于计算指标的数据保持相对新鲜（不超过1个月），同时在整个月份内保持验证集固定。因此，两个模型之间的比较变得更加有意义，性能指标也变得远不那么嘈杂。
- en: 7.3 Nontrivial schemas
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 非平凡模式
- en: We’ve reviewed standard validation schemas that cover most ML applications.
    Sometimes they are not enough to reflect the actual difference between seen and
    unseen data, even if you use a combination of them (e.g., time-based validation
    with group K-fold). As you know, inadequate validation leads to data leakage and,
    consequently, too optimistic model performance estimation (if not random!).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经审查了覆盖大多数机器学习应用的常规验证模式。有时，即使你使用它们的组合（例如，基于时间的验证与组K折交叉验证），它们也不足以反映已见和未见数据之间的实际差异。正如你所知，不充分的验证会导致数据泄露，从而导致模型性能估计过于乐观（如果不是随机的！）。
- en: Such situations require you to look for unorthodox processes. Let’s review some
    of them.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况需要你寻找非常规的过程。让我们回顾一些。
- en: 7.3.1 Nested validation
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.1 嵌套验证
- en: Nested validation is an approach used when we want to run hyperparameter optimization
    (or any other model selection procedure) as part of the learning process. We can’t
    just use an excluded fold or holdout set, which we will need for the final evaluation
    to estimate how good a given set of parameters is. Access to the score on the
    testing data while fitting any parameters is a direct way to overfitting.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌套验证是在我们希望在学习过程中运行超参数优化（或任何其他模型选择过程）时使用的一种方法。我们不能仅仅使用排除的折或保留集，这些我们将需要用于最终评估来估计给定参数集的好坏。在拟合任何参数的同时访问测试数据的分数是直接导致过拟合的方式。
- en: Instead, we use a fold-in-fold schema. We add an “inner” split of training data
    in each “outer” split to tune the parameters first. Then we fit the model on all
    available training folds with selected hyperparameters and make a prediction for
    the data that was not seen during hyperparameter tuning. Thus, we get two layers
    of validation, each of which can have its specific properties (e.g., we may prefer
    the inner layer to have lower variance and the outer layer to have a lower bias).
    We can apply nesting not only to cross-validation but also to time-series validation
    and ordinal holdout split (or mixed schemas of different natures) (see figures
    7.5 and 7.6).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们使用折叠内折叠的架构。在每个外部分割中添加一个“内部”的训练数据分割来首先调整参数。然后，我们使用选定的超参数在所有可用的训练折叠上拟合模型，并对在超参数调整期间未见过的数据进行预测。因此，我们得到两层验证，每一层都可以有其特定的属性（例如，我们可能更喜欢内部层具有较低的方差，而外部层具有较低的偏差）。我们不仅可以应用嵌套到交叉验证中，还可以应用到时间序列验证和有序保留分割（或不同性质的混合架构）中（见图7.5和7.6）。
- en: '![figure](../Images/CH07_F05_Babushkin.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH07_F05_Babushkin.png)'
- en: Figure 7.5 Example of nested cross-validation
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.5 嵌套交叉验证的示例
- en: '![figure](../Images/CH07_F06_Babushkin.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH07_F06_Babushkin.png)'
- en: 'Figure 7.6 Example of nested validation with mixed schemas: holdout split for
    the outer loop and K-fold for the inner loop'
  id: totrans-78
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.6 嵌套验证与混合架构的示例：外部循环的保留分割和内部循环的K折
- en: 7.3.2 Adversarial validation
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.2 对抗验证
- en: Instead of using a random subsample of data like in a standard holdout set,
    you may prefer to choose a different path. There is a technique called adversarial
    validation, a popular approach on ML competition platforms such as Kaggle. It
    applies an ML model for better validation of another ML model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 与在标准保留集使用随机子样本数据不同，你可能更倾向于选择不同的路径。有一种称为对抗验证的技术，在像Kaggle这样的机器学习竞赛平台上非常流行。它通过应用一个机器学习模型来更好地验证另一个机器学习模型。
- en: Adversarial validation numerically estimates whether two given datasets differ
    (those two may be sets of labeled and unlabeled data). And, if so, it can even
    quantify it on the sample level, making it possible to construct an arbitrary
    number of datasets, representative of each other, providing a perfect tool for
    estimation. An additional bonus is that it does not require data to be labeled.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗验证数值估计两个给定的数据集是否不同（这两个可能是有标签和无标签数据的集合）。如果确实如此，它甚至可以在样本级别上量化它，这使得构建任意数量的彼此代表的数据集成为可能，提供了一种完美的估计工具。一个额外的优点是它不需要对数据进行标记。
- en: 'The algorithm is simple:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 算法很简单：
- en: We combine datasets of interest (cutting off the target variable, if present),
    labeling the anchor dataset (the one we want to represent) as 1 and marking the
    rest as 0\.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将感兴趣的数据库集合并（如果存在，则截断目标变量），将锚定数据集（我们想要表示的数据集）标记为1，其余标记为0。
- en: We fit an auxiliary model on this concatenated dataset to solve the binary classification
    task (thus 0 and 1 marks).
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在这个连接的数据集上拟合一个辅助模型来解决二元分类任务（因此0和1标记）。
- en: If datasets are representative of each other and come from the same distribution,
    we could expect receiver operating characteristic area under the curve (ROC AUC)
    to be near 0.5\. If they are separable (e.g., ROC AUC is greater than 0.6), then
    we can use the output from the model as a measure of proximity.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果数据集彼此代表，并且来自相同的分布，我们预计接收者操作特征曲线下面积（ROC AUC）接近0.5。如果它们是可分离的（例如，ROC AUC大于0.6），那么我们可以使用模型的输出作为邻近度的度量。
- en: Note that while this trick was used in ML competitions for a long time (the
    first mention we found has been there since 2016, [http://fastml.com/adversarial-validation-part-one/](http://fastml.com/adversarial-validation-part-one/)),
    it was not part of more formal research until 2020 when it appeared in the paper
    “Adversarial Validation Approach to Concept Drift Problem in User Targeting Automation
    Systems at Uber” by Pan et al. ([https://arxiv.org/abs/2004.03045](https://arxiv.org/abs/2004.03045)).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，尽管这个技巧在机器学习竞赛中已经使用了很长时间（我们找到的第一个提及是在2016年，[http://fastml.com/adversarial-validation-part-one/](http://fastml.com/adversarial-validation-part-one/)），但它直到2020年才成为更正式研究的一部分，当时它出现在Pan等人撰写的论文“Adversarial
    Validation Approach to Concept Drift Problem in User Targeting Automation Systems
    at Uber”中（[https://arxiv.org/abs/2004.03045](https://arxiv.org/abs/2004.03045)）。
- en: We can use this kind of splitting in many cases. When we’re checking the similarity
    of labeled and unlabeled datasets, there are questions we should keep in mind.
    How different are their distributions? What features are the best predictors of
    this difference? Analyzing the model created by adversarial validation may answer
    these questions. We will also reuse this technique for a similar matter in chapter
    9.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在许多情况下使用这种分割。当我们检查标记和无标记数据集的相似性时，有一些问题我们应该记住。它们的分布有多不同？哪些特征是这种差异的最佳预测因子？分析由对抗性验证创建的模型可能回答这些问题。我们还将在此章节的第9节中重用这项技术。
- en: 7.3.3 Quantifying dataset leakage exploitation
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.3 量化数据集泄露利用
- en: We find an interesting validation technique in a paper by DeepMind titled “Improving
    Language Models by Retrieving from Trillions of Tokens (2021; [https://arxiv.org/abs/2112.04426](https://arxiv.org/abs/2112.04426)),
    which proposes a generative model trained on the next-word-prediction task.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在一篇由DeepMind撰写的论文中找到了一个有趣的验证技术，标题为“通过检索万亿个标记改进语言模型（2021；[https://arxiv.org/abs/2112.04426](https://arxiv.org/abs/2112.04426)），该论文提出了一种在下一个单词预测任务上训练的生成模型。
- en: The paper’s authors enhance the language model by conditioning it on a context
    retrieved from a large corpus based on local similarity with preceding tokens.
    This system memorizes the entire dataset and performs the nearest-neighbors search
    to find chunks of text in the history that are relevant to the recent sentences.
    But what if the sentences we try to continue are almost identical to those the
    model has seen in the training set? It looks like there is a high probability
    of encountering dataset leakage.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的作者通过将语言模型条件化于从大型语料库中检索到的上下文（基于与先前标记的局部相似性）来增强语言模型。该系统记忆整个数据集，并执行最近邻搜索以找到与最近句子相关的历史文本块。但如果我们尝试继续的句子几乎与模型在训练集中看到的句子相同呢？这似乎有很大的可能性会遇到数据集泄露。
- en: The authors discussed this problem in advance and proposed a noteworthy evaluation
    procedure. They developed a specific measure to quantify leakage exploitation.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们提前讨论了这个问题，并提出了一种值得注意的评估程序。他们开发了一种特定的度量来量化泄露利用。
- en: 'The general idea is the following:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 通用思想如下：
- en: Partition the dataset into training and validation sets as in the usual holdout
    validation.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集划分为训练集和验证集，如通常的保留验证。
- en: Split both into chunks of fixed length.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将两者都分割成固定长度的块。
- en: For each chunk in the validation set, retrieve N nearest neighbors from the
    training set based on chunk embeddings (here we will omit how chunks are transformed
    into embedding space, but you can find the details in the paper).
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于验证集中的每个块，根据块嵌入从训练集中检索N个最近的邻居（这里我们将省略块如何转换为嵌入空间，但你可以找到论文中的详细信息）。
- en: Calculate the ratio of tokens that are common in the two chunks (they use a
    score similar to the Jaccard Index); this gives us a score ranging from 0 (a chunk
    is totally different) to 1 (a chunk is a duplicate).
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算两个块中共同出现的标记的比例（他们使用与Jaccard指数类似的分数）；这给我们一个从0（块完全不同）到1（块是重复的）的分数。
- en: If this score exceeds a certain threshold, filter out this chunk from the training
    set.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果这个分数超过某个阈值，则从训练集中过滤掉这个块。
- en: This approach forces the model to retrieve useful information from similar texts
    and paraphrase it instead of copy-pasting it. You can use this procedure with
    any modern language model. It is a good example of an exotic technique that allows
    for minimizing data leakage and increasing the representativity of your dataset.
    A clear understanding of how the model will be applied will help you develop your
    own nontrivial validation schema if standard approaches are unsuitable.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法迫使模型从类似文本中检索有用信息并进行释义，而不是复制粘贴。你可以使用这个程序与任何现代语言模型一起使用。这是一个允许最小化数据泄露并增加数据集代表性的异国情调技术的良好例子。对模型如何应用有清晰的理解将帮助你开发自己的非平凡验证方案，如果标准方法不适用。
- en: 7.4 Split updating procedure
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 分割更新程序
- en: We spend as much time on the test data as on the training data.— Andrej Karpathy
  id: totrans-100
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们在测试数据上花费的时间与在训练数据上花费的时间一样多。—— 安德烈·卡帕西
- en: Regardless of which schema we use, we will probably apply it to a dynamically
    changing dataset. Periodically we get new data that may differ in distribution
    and include new patterns. How often should we update the test set to make sure
    our evaluation is always relevant?
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们使用哪种模式，我们可能都会将其应用于动态变化的数据库。定期我们会得到新的数据，这些数据可能在分布上有所不同，并包含新的模式。我们应该多久更新一次测试集以确保我们的评估始终相关？
- en: There are at least two goals we may want to reach while designing a split update
    procedure for new data. First, we want our test set to be representative of these
    new patterns. From this point of view, the evaluation process should be adaptive.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在为新数据设计分割更新程序时，我们可能希望达到至少两个目标。首先，我们希望我们的测试集能够代表这些新模式。从这个角度来看，评估过程应该是自适应的。
- en: 'Second, we want to see an evaluation dynamic: how has the model been changing
    through time with all updates in the architecture or features? For that, the estimates
    must be robust.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，我们想看到评估动态：模型在时间上是如何随着架构或特征的所有更新而变化的？为此，估计必须稳健。
- en: 'Some of the most common options are as follows (see figure 7.7):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些最常见的选项（见图7.7）：
- en: '![figure](../Images/CH07_F07_Babushkin.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_F07_Babushkin.png)'
- en: Figure 7.7 Common options for updating train/validation sets. The light data
    blocks are used for training, while the dark ones are used for validation.
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.7 更新训练/验证集的常见选项。浅色数据块用于训练，而深色数据块用于验证。
- en: '*Fixed shift*—When dealing with data that has a strong dependence on time and
    novelty, we are not interested in evaluating the model’s performance on data from
    a year ago or older due to the drastic change in target distribution. Instead,
    we would like to use only recent data for validation.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*固定偏移量*——当处理对时间和新颖性有强烈依赖的数据时，由于目标分布的剧烈变化，我们不会对一年前或更早的数据的性能感兴趣。相反，我们只想使用最近的数据进行验证。'
- en: For instance, we take the last 2 weeks as a validation set (starting from the
    last finished day) and update this set daily while retraining the model used for
    evaluation each time.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，我们将最后两周作为验证集（从最后完成的那天开始）并每天更新这个集合并重新训练用于评估的模型。
- en: '*Fixed ratio*—When dealing with images or text, we don’t gather new labels
    for data regularly. In contrast to the first case, we may have no strong dependence
    on data recency, meaning that newly added data may not be more important than
    the old data. Typically we expand the set of available data after receiving an
    extra portion of labels.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*固定比例*——当处理图像或文本时，我们不会定期为新数据收集标签。与第一种情况相比，我们可能对数据的时效性没有强烈的依赖，这意味着新添加的数据可能并不比旧数据更重要。通常，我们在收到额外部分标签后，会扩展可用数据集。'
- en: If we include newly labeled data only in the training set, we increase metrics
    due to more data available for the model. If we include this data only in the
    validation set, the model may miss some unseen patterns. The optimal solution
    is to keep the ratio between training and validation dataset sizes unchanged so
    that newly added data will be split accordingly.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果我们只将新标记的数据包含在训练集中，由于模型可用的数据更多，我们将增加指标。如果我们只将此数据包含在验证集中，模型可能会错过一些未见过的模式。最佳解决方案是保持训练集和验证集大小之间的比例不变，以便新添加的数据将相应地分割。
- en: '*Fixed set*—Sometimes, instead of a balanced subset of all currently available
    data, we would like to evaluate our model’s quality on an unchanged “golden set”
    used as a benchmark. This approach guarantees that the two models are still comparable
    in terms of any metric, even if there is a long modeling period between them.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*固定集*——有时，我们不想评估所有当前可用数据的平衡子集，而是想评估我们的模型质量在一个不变的“黄金集”上，这个集被用作基准。这种方法保证了两个模型在任何指标上都是可比较的，即使它们之间有一个很长的建模周期。'
- en: This fixed set can be sampled from the dataset before modeling or cherry-picked
    manually to contain a diverse range of hard cases and reference responses. It
    is not supposed to be updated by design to ensure consistent model comparison.
    If we extend this golden set in the future, we will treat it as a completely new
    benchmark.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个固定集可以在建模之前从数据集中采样，或者手动挑选以包含各种困难案例和参考响应。它不应该按设计更新，以确保一致的模型比较。如果我们将来扩展这个黄金集，我们将将其视为一个全新的基准。
- en: 'Remember: we should perform validation on the whole pipeline, including the
    dataset; inference on the test set should be the same as in production. If we
    want to compare models side by side accurately, we should somehow save previous
    versions of both datasets and models. Tools for data version control and model
    version control (such as DVC, Git LFS, or Deep Lake) may be of help.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 记住：我们应该在整个流水线上进行验证，包括数据集；在测试集上的推理应该与生产环境相同。如果我们想准确地对模型进行横向比较，我们应该以某种方式保存之前的数据集和模型的版本。数据版本控制和模型版本控制工具（如DVC、Git
    LFS或Deep Lake）可能会有所帮助。
- en: 'Once there are clues that the options here do not cover your particular use
    case, you may want to dive deeper into the literature dedicated to dynamic (nonstationary)
    data streams and concept drifts to get a holistic overview of related theory (e.g.,
    “Scarcity of Labels in Non-Stationary Data Streams: A Survey” [[https://mng.bz/gAXE](https://mng.bz/gAXE)]).
    We will also touch on the surface of the concept drift problem in chapter 11 as
    one of the underlying reasons why setting up a reliable validation schema is not
    easy.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这里的选择没有涵盖你的特定用例，你可能需要深入研究专门针对动态（非平稳）数据流和概念漂移的文献，以获得相关理论的全面概述（例如，“非平稳数据流中标签稀缺：综述”
    [[https://mng.bz/gAXE](https://mng.bz/gAXE)]）。我们还会在第11章中简要讨论概念漂移问题，作为设置可靠验证方案不容易的一个潜在原因。
- en: Campfire story from Valerii
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 瓦列里·的篝火故事
- en: When I was working in a big tech company, we would train a number of ML models
    on a local ML platform to catch spammers, scammers, scrapers, and other malevolent
    agents. However, the platform only produced point estimates when assessing model
    performance on the validation set. This turned out to be a problem, as offline
    estimation often was significantly different from online performance, creating
    either a considerable number of falsely banned users or wrong expectations.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当我在一家大型科技公司工作时，我们会在本地机器学习平台上训练多个机器学习模型来捕捉垃圾邮件发送者、诈骗者、抓取器和其他恶意代理。然而，该平台在评估验证集上的模型性能时只产生点估计。这最终成为一个问题，因为离线估计通常与在线性能有显著差异，导致大量误封用户或错误的期望。
- en: To illustrate the point estimate problem, let’s take a coin toss example.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明点估计问题，让我们以抛硬币的例子来说明。
- en: If we flip a fair coin 100 times, we can calculate the number of times it lands
    heads. That is our point estimate. If we do it again, we will end up with another
    number. If, instead, we say that 95 out of 100 times, we expect this number to
    be within the range of 40 to 60, this is a confidence interval. The lower confidence
    bound will be 40, meaning that we expect this number to be at least 40 in 95%
    of cases.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们公平地抛掷一枚硬币100次，我们可以计算出它落地正面的次数。这就是我们的点估计。如果我们再次这样做，我们最终会得到另一个数字。如果我们说，在100次中有95次，我们预计这个数字将在40到60的范围内，这是一个置信区间。下限置信度为40，这意味着我们预计在95%的情况下这个数字至少为40。
- en: The point estimate lacks robustness, as it does not consider an ever-present
    uncertainty, which is easy to illustrate graphically. The plots in the following
    figures demonstrate the variance of the two metrics, precision and recall, using
    the same threshold, ML classifier, and validation data generated by the same distribution
    on offline data.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 点估计缺乏稳健性，因为它没有考虑始终存在的不确定性，这很容易用图形来展示。以下图中的图表展示了使用相同阈值、机器学习分类器和由相同分布生成的离线数据生成的验证数据，两个指标（精确度和召回率）的方差。
- en: '![sidebar figure](../Images/CH07_UN01_Babushkin.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![侧边栏图片](../Images/CH07_UN01_Babushkin.png)'
- en: Distribution of precision and recall with sample size equal to 100,000; every
    point is an independent dataset.
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 样本大小等于100,000时的精确度和召回率分布；每个点代表一个独立的数据集。
- en: '![sidebar figure](../Images/CH07_UN02_Babushkin.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![侧边栏图片](../Images/CH07_UN02_Babushkin.png)'
- en: Distribution of precision and recall with sample size equal to 200,000; every
    point is an independent dataset.
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 样本大小等于200,000时的精确度和召回率分布；每个点代表一个独立的数据集。
- en: '![sidebar figure](../Images/CH07_UN03_Babushkin.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![侧边栏图片](../Images/CH07_UN03_Babushkin.png)'
- en: Distribution of precision and recall with sample size equal to 500,000; every
    point is an independent dataset
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 样本大小等于500,000时的精确度和召回率分布；每个点代表一个独立的数据集。
- en: It was no surprise that when we compared offline point estimates and online
    performance, they were almost always far apart. Even within offline evaluation,
    the variance was huge even when the validation data size was 500,000\. This situation
    lacks robustness and creates fragility in the whole system.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们比较离线点估计和在线性能时，它们几乎总是相去甚远。即使在离线评估中，即使验证数据大小为500,000，方差也非常大。这种情况缺乏稳健性，在整个系统中造成了脆弱性。
- en: With chunks of test data, it is easy to show uncertainty for precision, recall,
    or other metrics. Still, there are better ways to do this. The gold standard would
    be random sampling with replacement or, in other words, bootstrap. Unfortunately,
    bootstrap is very computationally expensive. For each bootstrap iteration (between
    10,000 and 100,000), we have to sample the multinomial distribution of length
    N (with the sample size reaching thousands or millions) and do this N times.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 使用测试数据块，很容易展示精确度、召回率或其他度量指标的不确定性。尽管如此，还有更好的方法来做这件事。黄金标准将是带有替换的随机抽样，换句话说，就是bootstrap。不幸的是，bootstrap的计算成本非常高。对于每个bootstrap迭代（在10,000到100,000之间），我们必须采样长度为N的多项式分布（样本大小达到数千或数百万），并且需要重复N次。
- en: This proved to be a problem. On the one hand, I couldn’t use the existing estimation
    solution provided by the platform, as it needed to be more reliable and robust.
    On the other hand, integrating bootstrap into every validation step was also impossible,
    as it would make even a single training loop run too long.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明了一个问题。一方面，我无法使用平台提供的现有估计解决方案，因为它需要更加可靠和稳健。另一方面，将bootstrap集成到每个验证步骤中也是不可能的，因为这会使单个训练循环运行时间过长。
- en: The solution came from math. Suppose we review each sample independently and
    run bootstrap in parallel. In that case, we can switch from multinomial sampling
    to binomial(n,1/n) and independently sample each observation for each bootstrap
    iteration. With N >> 100, sampling a Poisson with lambda parameter = 1 becomes
    a close approximation of binomial(n,1/n)—in other words, binomial(n,1/n) ~ Poisson(1)
    with N >>100\. (You can find more details at [https://mng.bz/OmyR](https://mng.bz/OmyR).)
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案来自数学。假设我们独立地审查每个样本并并行运行bootstrap。在这种情况下，我们可以从多项式抽样切换到二项式(n,1/n)并独立地对每个bootstrap迭代中的每个观测值进行抽样。当N
    >> 100时，具有lambda参数=1的泊松分布成为二项式(n,1/n)的近似——换句话说，当N >>100时，二项式(n,1/n) ~ 泊松(1)。（更多详细信息请见[https://mng.bz/OmyR](https://mng.bz/OmyR)。）
- en: No N exists in Poisson(1), making it completely independent of the data size
    and easy to parallel. This significantly increased speed (circa 100–1,000 times
    in my case with some additional tricks).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在泊松分布(1)中不存在N，这使得它与数据大小完全独立，并且易于并行处理。这显著提高了速度（在我的情况下，通过一些额外的技巧，速度大约提高了100-1,000倍）。
- en: We can pick a confidence bound to hold once we have distribution for the metric
    of interest. In the following figure, we can see a 99% lower confidence bound.
    On average, 99 times out of 100, recall will not be lower than 0.071\.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了感兴趣度量指标的分布，我们就可以选择一个置信区间。在下面的图中，我们可以看到一个99%的下限置信区间。平均而言，在100次中有99次，召回率不会低于0.071。
- en: '![sidebar figure](../Images/CH07_UN04_Babushkin.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![sidebar figure](../Images/CH07_UN04_Babushkin.png)'
- en: Distribution recall with every point being a bootstrapped original dataset;
    the red line is a 99% lower confidence bound.
  id: totrans-133
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 每个点都是一个bootstrap原始数据集的召回率分布；红色线是99%的下限置信区间。
- en: There is one more thing to take into consideration here. Some metrics, including
    precision and recall, depend on the threshold we pick to calculate them. The following
    figures demonstrate the distributions of precision and recall with and without
    some minor noise (normally distributed with mean = 0 and standard deviation –0.0125)
    applied to the samples.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还有一件事需要考虑。一些度量指标，包括精确度和召回率，取决于我们选择的阈值来计算它们。以下图展示了添加了一些轻微噪声（均值为0，标准差为-0.0125）的样本的精确度和召回率的分布情况。
- en: It is easy to see that the results with and without applied noise differ significantly,
    with increased recall and decreased precision in the latter. In a sense, these
    plots prove that in this case, the decision boundary margin is narrow and not
    robust. Adding some noise as a hyperparameter helps to estimate the distribution
    confidence intervals with increased trust in decision boundary robustness.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易看出，应用噪声与否的结果差异显著，后者召回率增加，精确度降低。从某种意义上说，这些图证明了在这种情况下，决策边界边缘很窄且不稳健。将一些噪声作为超参数添加有助于提高对决策边界稳健性的信任，从而估计分布置信区间。
- en: '![sidebar figure](../Images/CH07_UN05_Babushkin.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![sidebar figure](../Images/CH07_UN05_Babushkin.png)'
- en: Distribution of precision and recall with sample size of 200;000; every point
    is a bootstrapped original dataset, no noise added.
  id: totrans-137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 样本大小为200,000时的精确度和召回率的分布；每个点都是一个bootstrap原始数据集，未添加噪声。
- en: '![sidebar figure](../Images/CH07_UN06_Babushkin.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![sidebar figure](../Images/CH07_UN06_Babushkin.png)'
- en: Distribution of precision and recall with a sample size of 200,000; every point
    is a bootstrapped original dataset, noise added.
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 以200,000个样本大小的精确度和召回率分布；每个点都是一个重采样的原始数据集，添加了噪声。
- en: 'Estimating recall at a given precision/specificity is nothing new, but combined
    with Poisson bootstrap and noise addition, it created new metrics: bootstrapped
    lower confidence bound of recall at a given precision and bootstrapped lower confidence
    bound of recall at a given specificity. These metrics provided guaranteed (within
    a specific confidence level), reliable, and robust estimation of ML model performance.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定的精确度/特异性下估计召回率并不是什么新鲜事，但结合泊松重采样和噪声添加，它创造了新的指标：在给定精确度下的召回率的重采样下限和给定特异性下的召回率的重采样下限。这些指标提供了保证（在特定置信水平内），可靠且稳健的机器学习模型性能估计。
- en: '![sidebar figure](../Images/CH07_UN07_Babushkin.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![sidebar figure](../Images/CH07_UN07_Babushkin.png)'
- en: Metrics embedded into a native ML platform
  id: totrans-142
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 嵌入到原生机器学习平台中的指标
- en: '7.5 Design document: Choosing validation schemas'
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.5 设计文档：选择验证方案
- en: Time for another block of the design document, and this time we will fill in
    the information about preferred validation schemas for both Supermegaretail and
    PhotoStock Inc.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个设计文档块的时间到了，这次我们将填写关于Supermegaretail和PhotoStock Inc.首选验证方案的信息。
- en: 7.5.1 Validation schemas for Supermegaretail
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.1 Supermegaretail的验证方案
- en: We start with Supermegaretail.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从Supermegaretail开始。
- en: 'Design document: Supermegaretail'
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设计文档：Supermegaretail
- en: IV. Validation schema
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV. 验证方案
- en: i. Requirements
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: i. 需求
- en: What are the assumptions that we need to pay attention to when figuring out
    the evaluation process?
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定评估过程时，我们需要注意哪些假设？
- en: New data is coming daily.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新数据每日到来。
- en: Data can arrive with a delay of up to 48 hours.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可能延迟最多48小时到达。
- en: New labels (number of units sold) come with the new data.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新标签（销售单位数量）随新数据一起到来。
- en: Recent data is most probably more relevant for the prediction task.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 近期数据对于预测任务来说更有可能是相关的。
- en: The assortment matrix changes by 15% every month.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 商品组合矩阵每月变化15%。
- en: There’s seasonality present in the data (weekly/annual cycles).
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据中存在季节性（周/年周期）。
- en: Despite the fact that the data is naturally divided into categories, it is irrelevant
    to the choice of validation schema.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数据自然地分为类别，但这与验证方案的选择无关。
- en: ii. Inference
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ii. 推断
- en: After fixing a model (within the hyperparameter optimization procedure), we
    train it on the last 2 years of data and predict future demand for the next 4
    weeks. This process is fully reproduced in both inner and outer validation.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在固定一个模型（在超参数优化过程中）后，我们在过去两年数据上对其进行训练，并预测未来四周的需求。这个过程在内循环和外循环中完全重现。
- en: It is important to note that there should be a gap of 3 days between training
    and validation sets to be prepared for the fact that data may arrive with a delay.
    Subsequently, this will affect which features we can and cannot calculate when
    building a model.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，训练集和验证集之间应该有3天的差距，以应对数据可能延迟到达的事实。随后，这将影响我们在构建模型时可以和不能计算哪些特征。
- en: '![figure](../Images/CH07_UN08_Babushkin.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH07_UN08_Babushkin.png)'
- en: iii. Inner and outer loops
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: iii. 内循环和外循环
- en: We use two layers of validation. The outer loop is used for the final estimation
    of the model’s performance, while the inner loop is used for hyperparameter optimization.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用两层验证。外循环用于对模型性能的最终估计，而内循环用于超参数优化。
- en: First, for the outer loop, given that we are working with time series, rolling
    cross-validation is an obvious choice. We set K = 5 to train five models with
    optimal parameters. Since we are predicting 4 weeks ahead, the validation window
    size also consists of 28 days in all splits. There is a gap of 3 days between
    sets, and the step size is 7 days.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，对于外循环，鉴于我们处理的是时间序列数据，滚动交叉验证是一个明显的选择。我们设置K = 5，以训练具有最佳参数的五个模型。由于我们预测4周后的数据，验证窗口大小在所有分割中也包括28天。集合之间存在3天的差距，步长为7天。
- en: 'The following is an example of the outer loop:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个外循环的示例：
- en: 'First outer fold:'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一次外折：
- en: Data for the testing is from 2022-10-10 to 2022-11-06 (4 weeks).
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试数据的时间范围是2022-10-10至2022-11-06（4周）。
- en: Data for the training is from 2020-10-07 to 2022-10-06 (2 years).
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据的时间范围是2020-10-07至2022-10-06（2年）。
- en: 'Second outer fold:'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二次外折：
- en: Data for the testing is from 2022-10-03 to 2022-10-30\.
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试数据的时间范围是2022-10-03至2022-10-30。
- en: Data for the training is from 2020-09-29 to 2022-09-28\.
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据的时间范围是2020-09-29至2022-09-28。
- en: 'Fifth outer fold:'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第五次外折：
- en: Data for the testing is from 2022-09-12 to 2022-10-09\.
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试数据的时间范围是从2022-09-12到2022-10-09。
- en: Data for the training is from 2020-09-09 to 2022-09-08\.
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据的时间范围是从2020-09-09到2022-09-08。
- en: Second, for the inner loop, inside each “train set” of the outer validation,
    we perform additional rolling cross-validation with a three-fold split. Each inner
    loop training sample consists of a 2-year history as well to capture both annual
    and weekly seasonality. We use the inner loop to tune hyperparameters or for feature
    selection.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，对于内部循环，在外部验证的每个“训练集”内部，我们执行额外的滚动交叉验证，分为三折。每个内部循环的训练样本还包括2年的历史数据，以捕捉年度和周的季节性。我们使用内部循环来调整超参数或进行特征选择。
- en: 'The following is an example of the inner loop:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个内部循环的示例：
- en: 'Second fold of the outer loop:'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部循环的第二层折叠：
- en: Training data for the second outer fold is from 2020-10-03 to 2022-10-02\.
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二层外部折叠的训练数据是从2020-10-03到2022-10-02。
- en: 'First inner fold:'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一层内部折叠：
- en: Data for the testing is from 2022-09-05 to 2022-10-02 (4 weeks).
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试数据的时间范围是从2022-09-05到2022-10-02（4周）。
- en: Data for the training is from 2020-09-02 to 2022-09-01 (2 years).
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据的时间范围是从2020-09-02到2022-09-01（2年）。
- en: 'Second inner fold:'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二层内部折叠：
- en: Data for the testing is from 2022-08-29 to 2022-9-25\.
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试数据的时间范围是从2022-08-29到2022-09-25。
- en: Data for the training is from 2020-08-26 to 2022-08-25\.
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据的时间范围是从2020-08-26到2022-08-25。
- en: 'Third inner fold:'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三层内部折叠：
- en: Data for the testing is from 2022-08-22 to 2022-09-18\.
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试数据的时间范围是从2022-08-22到2022-09-18。
- en: Data for the training is from 2020-08-19 to 2022-08-18\.
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据的时间范围是从2020-08-19到2022-08-18。
- en: If the model does not require model tuning yet, we can skip the inner loop.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型尚不需要模型调整，我们可以跳过内部循环。
- en: '![figure](../Images/CH07_UN09_Babushkin.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH07_UN09_Babushkin.png)'
- en: iv. Update frequency
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: iv. 更新频率
- en: We update the split weekly along with new data and labels (so that each validation
    set always consists of a whole week). This will help us catch local changes and
    trends in model performance.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们每周更新拆分，同时添加新的数据和标签（这样每个验证集始终包含一个整周）。这将帮助我们捕捉模型性能的局部变化和趋势。
- en: Additionally, we have a separate holdout set as a benchmark (a “golden set”).
    We update it every 3 months. It helps us track long-term improvements in our system.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还有一个单独的保留集作为基准（一个“黄金集”）。我们每3个月更新一次。这有助于我们跟踪系统在长期内的改进。
- en: 7.5.2 Validation schemas for PhotoStock Inc.
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.2 PhotoStock Inc.的验证方案
- en: We will now add the information on validation schemas for PhotoStock Inc.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将添加有关PhotoStock Inc.验证方案的信息。
- en: 'Design document: PhotoStock Inc.'
  id: totrans-195
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设计文档：PhotoStock Inc.
- en: IV. Validation schema
  id: totrans-196
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV. 验证方案
- en: 'Search query is the main object of validation. There are four main caveats
    to keep in mind when planning a validation strategy for the PhotoStock Inc. search
    engine:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索查询是验证的主要对象。在为PhotoStock Inc.搜索引擎规划验证策略时，需要注意以下四个主要问题：
- en: Validation and test sets should be representative of the production data; in
    other words, they should represent real user queries.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证集和测试集应代表生产数据；换句话说，它们应代表真实用户的查询。
- en: Validation and test sets should be diverse; in other words, they should cover
    as wide a range of topics and contexts as possible.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证集和测试集应多样化；换句话说，它们应涵盖尽可能广泛的主题和上下文。
- en: Queries by the same user should only appear in either the training, validation,
    or test sets but not in multiple sets, so we avoid data leakage.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同一用户的查询应只出现在训练、验证或测试集中，而不是多个集中，这样我们就可以避免数据泄露。
- en: Duplicate queries should be removed from the dataset to avoid data leakage.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应从数据集中删除重复查询以避免数据泄露。
- en: 'Thus, we suggest using the following splitting strategy:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们建议使用以下拆分策略：
- en: Group queries by user; each query is assigned to a user once. If another user
    has the same query, it’s ignored.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按用户分组查询；每个查询只分配给一个用户。如果另一个用户有相同的查询，则忽略。
- en: Random-split users into training, validation, and test sets with a fixed ratio
    (to be determined; we don’t know what ratio is the best, but we can start with
    90/5/5).
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以固定的比例（待定；我们不知道什么比例是最好的，但我们可以从90/5/5开始）随机将用户拆分为训练、验证和测试集。
- en: Assign new users to their split once and never change it.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一次性将新用户分配到其拆分，然后不再更改。
- en: Random split assignment should address potential distribution skewness in the
    data. For example, we may guess there is a seasonality effect in searches (weekend
    users are amateurs, while weekday users are professionals) and there is some distribution
    drift over time (new topics emerge; old topics fade away). The random split should
    address those issues, although additional analysis is required to confirm that.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 随机分割分配应解决数据中的潜在分布偏斜问题。例如，我们可能猜测搜索中存在季节性效应（周末用户是业余爱好者，而工作日用户是专业人士），并且随着时间的推移存在一些分布漂移（新主题出现；旧主题消失）。随机分割应解决这些问题，尽管还需要额外的分析来确认这一点。
- en: 'To assign splits to users, we suggest using a deterministic bucketing approach:
    we split users into buckets based on their `user_id` `hash` and then assign each
    bucket to a split. This approach is universal because it allows the split ratio
    to change in the future. For example, if we want to increase the size of the validation
    set, we can just assign more buckets to the validation set from the training set.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将分割分配给用户，我们建议使用确定性桶式方法：我们根据用户的`user_id` `hash`将用户分成桶，然后将每个桶分配给一个分割。这种方法是通用的，因为它允许分割比例在未来发生变化。例如，如果我们想增加验证集的大小，我们只需将更多的桶从训练集分配到验证集即可。
- en: 'The following is an example of the bucketing approach:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个桶式方法的示例：
- en: '[PRE1]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For the initial project phase, we don’t plan to add more subsets (e.g., “the
    golden set”), although we can’t exclude this possibility in the future.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始项目阶段，我们计划不添加更多子集（例如，“黄金集”），尽管我们未来不能排除这种可能性。
- en: Summary
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Use validation schemas as a way to measure your model’s predictive power accurately.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将验证模式作为衡量模型预测能力准确性的方法。
- en: Try to avoid using the same validation split repeatedly for evaluation and searching
    for optimal hyperparameters, as it may lead to biased/overfitted and nonrobust
    results.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽量避免重复使用相同的验证分割进行评估和搜索最优超参数，因为这可能导致偏差/过拟合和非稳健的结果。
- en: Try to design a validation schema to reflect how the model is applied in practice.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试设计一个验证模式以反映模型在实际中的应用。
- en: 'When looking for a needed number of K folds, base your choice on the following
    three variables: bias, variance, and computation time.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在寻找所需的K折数量时，应根据以下三个变量进行选择：偏差、方差和计算时间。
- en: To do this, consider how the data differs between seen and unseen data (whether
    there are groups, classes, time, or other essential properties you should take
    into account).
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要做到这一点，考虑数据在已见和未见数据之间的差异（是否有组、类别、时间或其他你应该考虑的基本属性）。
- en: Design a nonstandard schema to fit a particular problem if necessary.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果有必要，设计一个非标准模式以适应特定问题。
- en: Remember that different schemas for different goals can work together nicely.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记住，不同的模式可以很好地协同工作以实现不同的目标。
